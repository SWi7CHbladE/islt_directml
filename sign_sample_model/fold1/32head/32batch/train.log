2024-02-03 05:52:01,435 Hello! This is Joey-NMT.
2024-02-03 05:52:01,454 Total params: 25618440
2024-02-03 05:52:01,456 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'gloss_output_layer.bias', 'gloss_output_layer.weight', 'sgn_embed.ln.bias', 'sgn_embed.ln.weight', 'sgn_embed.norm.norm.bias', 'sgn_embed.norm.norm.weight', 'txt_embed.norm.norm.bias', 'txt_embed.norm.norm.weight']
2024-02-03 05:52:02,655 cfg.name                           : sign_experiment
2024-02-03 05:52:02,655 cfg.data.data_path                 : ./data/Sports_dataset/1/
2024-02-03 05:52:02,655 cfg.data.version                   : phoenix_2014_trans
2024-02-03 05:52:02,656 cfg.data.sgn                       : sign
2024-02-03 05:52:02,656 cfg.data.txt                       : text
2024-02-03 05:52:02,656 cfg.data.gls                       : gloss
2024-02-03 05:52:02,656 cfg.data.train                     : excel_data.train
2024-02-03 05:52:02,656 cfg.data.dev                       : excel_data.dev
2024-02-03 05:52:02,656 cfg.data.test                      : excel_data.test
2024-02-03 05:52:02,656 cfg.data.feature_size              : 2560
2024-02-03 05:52:02,656 cfg.data.level                     : word
2024-02-03 05:52:02,657 cfg.data.txt_lowercase             : True
2024-02-03 05:52:02,657 cfg.data.max_sent_length           : 500
2024-02-03 05:52:02,657 cfg.data.random_train_subset       : -1
2024-02-03 05:52:02,657 cfg.data.random_dev_subset         : -1
2024-02-03 05:52:02,657 cfg.testing.recognition_beam_sizes : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
2024-02-03 05:52:02,657 cfg.testing.translation_beam_sizes : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
2024-02-03 05:52:02,657 cfg.testing.translation_beam_alphas : [-1, 0, 1, 2, 3, 4, 5]
2024-02-03 05:52:02,657 cfg.training.reset_best_ckpt       : False
2024-02-03 05:52:02,658 cfg.training.reset_scheduler       : False
2024-02-03 05:52:02,658 cfg.training.reset_optimizer       : False
2024-02-03 05:52:02,658 cfg.training.random_seed           : 42
2024-02-03 05:52:02,658 cfg.training.model_dir             : ./sign_sample_model/fold1/32head/32batch
2024-02-03 05:52:02,658 cfg.training.recognition_loss_weight : 1.0
2024-02-03 05:52:02,658 cfg.training.translation_loss_weight : 1.0
2024-02-03 05:52:02,658 cfg.training.eval_metric           : bleu
2024-02-03 05:52:02,658 cfg.training.optimizer             : adam
2024-02-03 05:52:02,658 cfg.training.learning_rate         : 0.0001
2024-02-03 05:52:02,659 cfg.training.batch_size            : 32
2024-02-03 05:52:02,659 cfg.training.num_valid_log         : 5
2024-02-03 05:52:02,659 cfg.training.epochs                : 50000
2024-02-03 05:52:02,659 cfg.training.early_stopping_metric : eval_metric
2024-02-03 05:52:02,659 cfg.training.batch_type            : sentence
2024-02-03 05:52:02,659 cfg.training.translation_normalization : batch
2024-02-03 05:52:02,659 cfg.training.eval_recognition_beam_size : 1
2024-02-03 05:52:02,659 cfg.training.eval_translation_beam_size : 1
2024-02-03 05:52:02,660 cfg.training.eval_translation_beam_alpha : -1
2024-02-03 05:52:02,660 cfg.training.overwrite             : True
2024-02-03 05:52:02,660 cfg.training.shuffle               : True
2024-02-03 05:52:02,660 cfg.training.use_cuda              : True
2024-02-03 05:52:02,660 cfg.training.translation_max_output_length : 40
2024-02-03 05:52:02,660 cfg.training.keep_last_ckpts       : 1
2024-02-03 05:52:02,660 cfg.training.batch_multiplier      : 1
2024-02-03 05:52:02,660 cfg.training.logging_freq          : 100
2024-02-03 05:52:02,660 cfg.training.validation_freq       : 2000
2024-02-03 05:52:02,661 cfg.training.betas                 : [0.9, 0.998]
2024-02-03 05:52:02,661 cfg.training.scheduling            : plateau
2024-02-03 05:52:02,661 cfg.training.learning_rate_min     : 1e-08
2024-02-03 05:52:02,661 cfg.training.weight_decay          : 0.0001
2024-02-03 05:52:02,661 cfg.training.patience              : 12
2024-02-03 05:52:02,661 cfg.training.decrease_factor       : 0.5
2024-02-03 05:52:02,661 cfg.training.label_smoothing       : 0.0
2024-02-03 05:52:02,661 cfg.model.initializer              : xavier
2024-02-03 05:52:02,662 cfg.model.bias_initializer         : zeros
2024-02-03 05:52:02,662 cfg.model.init_gain                : 1.0
2024-02-03 05:52:02,662 cfg.model.embed_initializer        : xavier
2024-02-03 05:52:02,662 cfg.model.embed_init_gain          : 1.0
2024-02-03 05:52:02,662 cfg.model.tied_softmax             : True
2024-02-03 05:52:02,662 cfg.model.encoder.type             : transformer
2024-02-03 05:52:02,662 cfg.model.encoder.num_layers       : 3
2024-02-03 05:52:02,662 cfg.model.encoder.num_heads        : 32
2024-02-03 05:52:02,662 cfg.model.encoder.embeddings.embedding_dim : 512
2024-02-03 05:52:02,663 cfg.model.encoder.embeddings.scale : False
2024-02-03 05:52:02,663 cfg.model.encoder.embeddings.dropout : 0.1
2024-02-03 05:52:02,663 cfg.model.encoder.embeddings.norm_type : batch
2024-02-03 05:52:02,663 cfg.model.encoder.embeddings.activation_type : softsign
2024-02-03 05:52:02,663 cfg.model.encoder.hidden_size      : 512
2024-02-03 05:52:02,663 cfg.model.encoder.ff_size          : 2048
2024-02-03 05:52:02,663 cfg.model.encoder.dropout          : 0.1
2024-02-03 05:52:02,663 cfg.model.decoder.type             : transformer
2024-02-03 05:52:02,664 cfg.model.decoder.num_layers       : 3
2024-02-03 05:52:02,664 cfg.model.decoder.num_heads        : 32
2024-02-03 05:52:02,664 cfg.model.decoder.embeddings.embedding_dim : 512
2024-02-03 05:52:02,664 cfg.model.decoder.embeddings.scale : False
2024-02-03 05:52:02,664 cfg.model.decoder.embeddings.dropout : 0.1
2024-02-03 05:52:02,664 cfg.model.decoder.embeddings.norm_type : batch
2024-02-03 05:52:02,664 cfg.model.decoder.embeddings.activation_type : softsign
2024-02-03 05:52:02,664 cfg.model.decoder.hidden_size      : 512
2024-02-03 05:52:02,664 cfg.model.decoder.ff_size          : 2048
2024-02-03 05:52:02,665 cfg.model.decoder.dropout          : 0.1
2024-02-03 05:52:02,665 Data set sizes: 
	train 2126,
	valid 707,
	test 708
2024-02-03 05:52:02,665 First training example:
	[GLS] A B C D E
	[TXT] another problem is that because of ipl and t20 world cup many players may not be available for the matches
2024-02-03 05:52:02,665 First 10 words (gls): (0) <si> (1) <unk> (2) <pad> (3) A (4) B (5) C (6) D (7) E
2024-02-03 05:52:02,665 First 10 words (txt): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) the (5) to (6) and (7) in (8) a (9) of
2024-02-03 05:52:02,665 Number of unique glosses (types): 8
2024-02-03 05:52:02,665 Number of unique words (types): 4355
2024-02-03 05:52:02,666 SignModel(
	encoder=TransformerEncoder(num_layers=3, num_heads=32),
	decoder=TransformerDecoder(num_layers=3, num_heads=32),
	sgn_embed=SpatialEmbeddings(embedding_dim=512, input_size=2560),
	txt_embed=Embeddings(embedding_dim=512, vocab_size=4355))
2024-02-03 05:52:02,669 EPOCH 1
2024-02-03 05:52:08,334 Epoch   1: Total Training Recognition Loss 345.23  Total Training Translation Loss 6515.62 
2024-02-03 05:52:08,334 EPOCH 2
2024-02-03 05:52:10,862 [Epoch: 002 Step: 00000100] Batch Recognition Loss:   2.737157 => Gls Tokens per Sec:     2089 || Batch Translation Loss: 103.887283 => Txt Tokens per Sec:     5920 || Lr: 0.000100
2024-02-03 05:52:13,604 Epoch   2: Total Training Recognition Loss 175.99  Total Training Translation Loss 5967.18 
2024-02-03 05:52:13,604 EPOCH 3
2024-02-03 05:52:18,669 [Epoch: 003 Step: 00000200] Batch Recognition Loss:   1.294156 => Gls Tokens per Sec:     2067 || Batch Translation Loss:  82.850487 => Txt Tokens per Sec:     5765 || Lr: 0.000100
2024-02-03 05:52:18,735 Epoch   3: Total Training Recognition Loss 142.82  Total Training Translation Loss 5875.75 
2024-02-03 05:52:18,735 EPOCH 4
2024-02-03 05:52:24,290 Epoch   4: Total Training Recognition Loss 153.93  Total Training Translation Loss 5735.68 
2024-02-03 05:52:24,291 EPOCH 5
2024-02-03 05:52:26,432 [Epoch: 005 Step: 00000300] Batch Recognition Loss:   1.995456 => Gls Tokens per Sec:     2393 || Batch Translation Loss: 116.541916 => Txt Tokens per Sec:     6320 || Lr: 0.000100
2024-02-03 05:52:29,026 Epoch   5: Total Training Recognition Loss 88.09  Total Training Translation Loss 5516.61 
2024-02-03 05:52:29,026 EPOCH 6
2024-02-03 05:52:34,008 [Epoch: 006 Step: 00000400] Batch Recognition Loss:   0.633196 => Gls Tokens per Sec:     2070 || Batch Translation Loss:  93.737320 => Txt Tokens per Sec:     5734 || Lr: 0.000100
2024-02-03 05:52:34,193 Epoch   6: Total Training Recognition Loss 63.51  Total Training Translation Loss 5298.47 
2024-02-03 05:52:34,194 EPOCH 7
2024-02-03 05:52:39,477 Epoch   7: Total Training Recognition Loss 55.37  Total Training Translation Loss 5090.12 
2024-02-03 05:52:39,478 EPOCH 8
2024-02-03 05:52:42,022 [Epoch: 008 Step: 00000500] Batch Recognition Loss:   0.229049 => Gls Tokens per Sec:     1951 || Batch Translation Loss:  57.559803 => Txt Tokens per Sec:     5432 || Lr: 0.000100
2024-02-03 05:52:44,810 Epoch   8: Total Training Recognition Loss 38.62  Total Training Translation Loss 4897.73 
2024-02-03 05:52:44,810 EPOCH 9
2024-02-03 05:52:50,080 [Epoch: 009 Step: 00000600] Batch Recognition Loss:   0.400001 => Gls Tokens per Sec:     1926 || Batch Translation Loss:  82.708542 => Txt Tokens per Sec:     5351 || Lr: 0.000100
2024-02-03 05:52:50,321 Epoch   9: Total Training Recognition Loss 31.28  Total Training Translation Loss 4714.32 
2024-02-03 05:52:50,322 EPOCH 10
2024-02-03 05:52:55,319 Epoch  10: Total Training Recognition Loss 27.23  Total Training Translation Loss 4516.06 
2024-02-03 05:52:55,319 EPOCH 11
2024-02-03 05:52:57,537 [Epoch: 011 Step: 00000700] Batch Recognition Loss:   0.334873 => Gls Tokens per Sec:     2165 || Batch Translation Loss:  37.341824 => Txt Tokens per Sec:     5832 || Lr: 0.000100
2024-02-03 05:53:00,590 Epoch  11: Total Training Recognition Loss 22.44  Total Training Translation Loss 4345.22 
2024-02-03 05:53:00,591 EPOCH 12
2024-02-03 05:53:05,506 [Epoch: 012 Step: 00000800] Batch Recognition Loss:   0.304941 => Gls Tokens per Sec:     2033 || Batch Translation Loss:  66.118210 => Txt Tokens per Sec:     5659 || Lr: 0.000100
2024-02-03 05:53:05,765 Epoch  12: Total Training Recognition Loss 18.50  Total Training Translation Loss 4180.48 
2024-02-03 05:53:05,766 EPOCH 13
2024-02-03 05:53:11,250 Epoch  13: Total Training Recognition Loss 15.95  Total Training Translation Loss 4015.75 
2024-02-03 05:53:11,251 EPOCH 14
2024-02-03 05:53:13,540 [Epoch: 014 Step: 00000900] Batch Recognition Loss:   0.261117 => Gls Tokens per Sec:     2029 || Batch Translation Loss:  60.934727 => Txt Tokens per Sec:     5930 || Lr: 0.000100
2024-02-03 05:53:16,414 Epoch  14: Total Training Recognition Loss 14.72  Total Training Translation Loss 3853.50 
2024-02-03 05:53:16,415 EPOCH 15
2024-02-03 05:53:21,197 [Epoch: 015 Step: 00001000] Batch Recognition Loss:   0.034085 => Gls Tokens per Sec:     2056 || Batch Translation Loss:  54.022602 => Txt Tokens per Sec:     5703 || Lr: 0.000100
2024-02-03 05:53:21,567 Epoch  15: Total Training Recognition Loss 13.76  Total Training Translation Loss 3705.96 
2024-02-03 05:53:21,567 EPOCH 16
2024-02-03 05:53:26,622 Epoch  16: Total Training Recognition Loss 13.24  Total Training Translation Loss 3572.05 
2024-02-03 05:53:26,623 EPOCH 17
2024-02-03 05:53:28,952 [Epoch: 017 Step: 00001100] Batch Recognition Loss:   0.160487 => Gls Tokens per Sec:     1886 || Batch Translation Loss:  29.803656 => Txt Tokens per Sec:     5376 || Lr: 0.000100
2024-02-03 05:53:32,125 Epoch  17: Total Training Recognition Loss 12.09  Total Training Translation Loss 3416.08 
2024-02-03 05:53:32,126 EPOCH 18
2024-02-03 05:53:37,021 [Epoch: 018 Step: 00001200] Batch Recognition Loss:   0.203289 => Gls Tokens per Sec:     1976 || Batch Translation Loss:  50.321594 => Txt Tokens per Sec:     5487 || Lr: 0.000100
2024-02-03 05:53:37,541 Epoch  18: Total Training Recognition Loss 11.47  Total Training Translation Loss 3265.37 
2024-02-03 05:53:37,541 EPOCH 19
2024-02-03 05:53:42,441 Epoch  19: Total Training Recognition Loss 11.60  Total Training Translation Loss 3123.40 
2024-02-03 05:53:42,442 EPOCH 20
2024-02-03 05:53:44,749 [Epoch: 020 Step: 00001300] Batch Recognition Loss:   0.064696 => Gls Tokens per Sec:     1874 || Batch Translation Loss:  39.049503 => Txt Tokens per Sec:     4961 || Lr: 0.000100
2024-02-03 05:53:48,012 Epoch  20: Total Training Recognition Loss 11.03  Total Training Translation Loss 3000.03 
2024-02-03 05:53:48,012 EPOCH 21
2024-02-03 05:53:52,772 [Epoch: 021 Step: 00001400] Batch Recognition Loss:   0.060500 => Gls Tokens per Sec:     1999 || Batch Translation Loss:  38.902382 => Txt Tokens per Sec:     5485 || Lr: 0.000100
2024-02-03 05:53:53,353 Epoch  21: Total Training Recognition Loss 10.72  Total Training Translation Loss 2887.47 
2024-02-03 05:53:53,354 EPOCH 22
2024-02-03 05:53:58,548 Epoch  22: Total Training Recognition Loss 12.17  Total Training Translation Loss 2761.62 
2024-02-03 05:53:58,549 EPOCH 23
2024-02-03 05:54:00,368 [Epoch: 023 Step: 00001500] Batch Recognition Loss:   0.066437 => Gls Tokens per Sec:     2287 || Batch Translation Loss:  39.023457 => Txt Tokens per Sec:     6159 || Lr: 0.000100
2024-02-03 05:54:03,141 Epoch  23: Total Training Recognition Loss 10.66  Total Training Translation Loss 2607.64 
2024-02-03 05:54:03,141 EPOCH 24
2024-02-03 05:54:07,829 [Epoch: 024 Step: 00001600] Batch Recognition Loss:   0.098066 => Gls Tokens per Sec:     1995 || Batch Translation Loss:  39.779305 => Txt Tokens per Sec:     5555 || Lr: 0.000100
2024-02-03 05:54:08,571 Epoch  24: Total Training Recognition Loss 9.16  Total Training Translation Loss 2458.03 
2024-02-03 05:54:08,571 EPOCH 25
2024-02-03 05:54:13,821 Epoch  25: Total Training Recognition Loss 9.50  Total Training Translation Loss 2337.03 
2024-02-03 05:54:13,821 EPOCH 26
2024-02-03 05:54:15,769 [Epoch: 026 Step: 00001700] Batch Recognition Loss:   0.152009 => Gls Tokens per Sec:     2055 || Batch Translation Loss:  40.887024 => Txt Tokens per Sec:     5504 || Lr: 0.000100
2024-02-03 05:54:19,009 Epoch  26: Total Training Recognition Loss 8.76  Total Training Translation Loss 2191.46 
2024-02-03 05:54:19,009 EPOCH 27
2024-02-03 05:54:23,829 [Epoch: 027 Step: 00001800] Batch Recognition Loss:   0.057792 => Gls Tokens per Sec:     1908 || Batch Translation Loss:  36.779453 => Txt Tokens per Sec:     5337 || Lr: 0.000100
2024-02-03 05:54:24,512 Epoch  27: Total Training Recognition Loss 8.33  Total Training Translation Loss 2076.80 
2024-02-03 05:54:24,512 EPOCH 28
2024-02-03 05:54:29,108 Epoch  28: Total Training Recognition Loss 8.58  Total Training Translation Loss 1955.01 
2024-02-03 05:54:29,109 EPOCH 29
2024-02-03 05:54:31,152 [Epoch: 029 Step: 00001900] Batch Recognition Loss:   0.115076 => Gls Tokens per Sec:     1881 || Batch Translation Loss:  15.024286 => Txt Tokens per Sec:     5394 || Lr: 0.000100
2024-02-03 05:54:34,604 Epoch  29: Total Training Recognition Loss 8.27  Total Training Translation Loss 1844.07 
2024-02-03 05:54:34,604 EPOCH 30
2024-02-03 05:54:38,944 [Epoch: 030 Step: 00002000] Batch Recognition Loss:   0.074978 => Gls Tokens per Sec:     2081 || Batch Translation Loss:  19.427673 => Txt Tokens per Sec:     5730 || Lr: 0.000100
2024-02-03 05:54:47,706 Hooray! New best validation result [eval_metric]!
2024-02-03 05:54:47,707 Saving new checkpoint.
2024-02-03 05:54:47,977 Validation result at epoch  30, step     2000: duration: 9.0329s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 5.85737	Translation Loss: 59230.51172	PPL: 469.60962
	Eval Metric: BLEU
	WER 6.58	(DEL: 0.07,	INS: 0.00,	SUB: 6.51)
	BLEU-4 0.84	(BLEU-1: 12.21,	BLEU-2: 4.28,	BLEU-3: 1.90,	BLEU-4: 0.84)
	CHRF 16.46	ROUGE 10.28
2024-02-03 05:54:47,979 Logging Recognition and Translation Outputs
2024-02-03 05:54:47,979 ========================================================================================================================
2024-02-03 05:54:47,979 Logging Sequence: 143_161.00
2024-02-03 05:54:47,979 	Gloss Reference :	A B+C+D+E
2024-02-03 05:54:47,980 	Gloss Hypothesis:	A B+C+E  
2024-02-03 05:54:47,980 	Gloss Alignment :	  S      
2024-02-03 05:54:47,980 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 05:54:47,980 	Text Reference  :	there is no response from them as yet  
2024-02-03 05:54:47,980 	Text Hypothesis :	***** ** ** ******** **** what a  proud
2024-02-03 05:54:47,981 	Text Alignment  :	D     D  D  D        D    S    S  S    
2024-02-03 05:54:47,981 ========================================================================================================================
2024-02-03 05:54:47,981 Logging Sequence: 63_21.00
2024-02-03 05:54:47,981 	Gloss Reference :	A B+C+D+E
2024-02-03 05:54:47,981 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 05:54:47,981 	Gloss Alignment :	         
2024-02-03 05:54:47,981 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 05:54:47,983 	Text Reference  :	*** ***** however the ***** teams will be ****** ******* announced only on  25th    october 2021 
2024-02-03 05:54:47,983 	Text Hypothesis :	the final of      the world cup   will be played between india     and  new zealand in      dubai
2024-02-03 05:54:47,983 	Text Alignment  :	I   I     S           I     S             I      I       S         S    S   S       S       S    
2024-02-03 05:54:47,983 ========================================================================================================================
2024-02-03 05:54:47,983 Logging Sequence: 122_147.00
2024-02-03 05:54:47,983 	Gloss Reference :	A B+C+D+E
2024-02-03 05:54:47,983 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 05:54:47,983 	Gloss Alignment :	         
2024-02-03 05:54:47,984 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 05:54:47,984 	Text Reference  :	chanu had   been working as         a tickets inspector in  the       indian railways
2024-02-03 05:54:47,985 	Text Hypothesis :	***** after the  press   conference a ******* table     was overjoyed with   him     
2024-02-03 05:54:47,985 	Text Alignment  :	D     S     S    S       S            D       S         S   S         S      S       
2024-02-03 05:54:47,985 ========================================================================================================================
2024-02-03 05:54:47,985 Logging Sequence: 87_196.00
2024-02-03 05:54:47,985 	Gloss Reference :	A B+C+D+E
2024-02-03 05:54:47,985 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 05:54:47,986 	Gloss Alignment :	         
2024-02-03 05:54:47,986 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 05:54:47,986 	Text Reference  :	my expression was  not against dhoni or   kohli
2024-02-03 05:54:47,986 	Text Hypothesis :	** i          love to  see     the   same time 
2024-02-03 05:54:47,986 	Text Alignment  :	D  S          S    S   S       S     S    S    
2024-02-03 05:54:47,987 ========================================================================================================================
2024-02-03 05:54:47,987 Logging Sequence: 114_153.00
2024-02-03 05:54:47,987 	Gloss Reference :	A B+C+D+E
2024-02-03 05:54:47,987 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 05:54:47,987 	Gloss Alignment :	         
2024-02-03 05:54:47,987 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 05:54:47,988 	Text Reference  :	****** another big     football news is     that the copa    america final
2024-02-03 05:54:47,988 	Text Hypothesis :	people were    shocked to       be   played at   the stadium in      china
2024-02-03 05:54:47,988 	Text Alignment  :	I      S       S       S        S    S      S        S       S       S    
2024-02-03 05:54:47,988 ========================================================================================================================
2024-02-03 05:54:48,960 Epoch  30: Total Training Recognition Loss 8.07  Total Training Translation Loss 1740.01 
2024-02-03 05:54:48,960 EPOCH 31
2024-02-03 05:54:54,235 Epoch  31: Total Training Recognition Loss 8.56  Total Training Translation Loss 1687.72 
2024-02-03 05:54:54,236 EPOCH 32
2024-02-03 05:54:56,196 [Epoch: 032 Step: 00002100] Batch Recognition Loss:   0.063576 => Gls Tokens per Sec:     1878 || Batch Translation Loss:  15.882830 => Txt Tokens per Sec:     5346 || Lr: 0.000100
2024-02-03 05:54:59,653 Epoch  32: Total Training Recognition Loss 8.27  Total Training Translation Loss 1519.32 
2024-02-03 05:54:59,653 EPOCH 33
2024-02-03 05:55:03,715 [Epoch: 033 Step: 00002200] Batch Recognition Loss:   0.038467 => Gls Tokens per Sec:     2206 || Batch Translation Loss:  19.303886 => Txt Tokens per Sec:     6220 || Lr: 0.000100
2024-02-03 05:55:04,451 Epoch  33: Total Training Recognition Loss 7.47  Total Training Translation Loss 1417.38 
2024-02-03 05:55:04,451 EPOCH 34
2024-02-03 05:55:09,904 Epoch  34: Total Training Recognition Loss 7.69  Total Training Translation Loss 1306.04 
2024-02-03 05:55:09,905 EPOCH 35
2024-02-03 05:55:11,700 [Epoch: 035 Step: 00002300] Batch Recognition Loss:   0.097880 => Gls Tokens per Sec:     1912 || Batch Translation Loss:  19.688354 => Txt Tokens per Sec:     5212 || Lr: 0.000100
2024-02-03 05:55:15,226 Epoch  35: Total Training Recognition Loss 7.76  Total Training Translation Loss 1201.06 
2024-02-03 05:55:15,227 EPOCH 36
2024-02-03 05:55:19,320 [Epoch: 036 Step: 00002400] Batch Recognition Loss:   0.234024 => Gls Tokens per Sec:     2129 || Batch Translation Loss:   7.513189 => Txt Tokens per Sec:     5799 || Lr: 0.000100
2024-02-03 05:55:20,424 Epoch  36: Total Training Recognition Loss 7.54  Total Training Translation Loss 1117.24 
2024-02-03 05:55:20,424 EPOCH 37
2024-02-03 05:55:25,525 Epoch  37: Total Training Recognition Loss 7.07  Total Training Translation Loss 1031.25 
2024-02-03 05:55:25,526 EPOCH 38
2024-02-03 05:55:27,295 [Epoch: 038 Step: 00002500] Batch Recognition Loss:   0.171931 => Gls Tokens per Sec:     1900 || Batch Translation Loss:  12.519168 => Txt Tokens per Sec:     5665 || Lr: 0.000100
2024-02-03 05:55:30,773 Epoch  38: Total Training Recognition Loss 7.22  Total Training Translation Loss 947.25 
2024-02-03 05:55:30,773 EPOCH 39
2024-02-03 05:55:35,394 [Epoch: 039 Step: 00002600] Batch Recognition Loss:   0.056456 => Gls Tokens per Sec:     1851 || Batch Translation Loss:  12.298859 => Txt Tokens per Sec:     5289 || Lr: 0.000100
2024-02-03 05:55:36,236 Epoch  39: Total Training Recognition Loss 7.07  Total Training Translation Loss 878.36 
2024-02-03 05:55:36,236 EPOCH 40
2024-02-03 05:55:41,356 Epoch  40: Total Training Recognition Loss 6.87  Total Training Translation Loss 823.69 
2024-02-03 05:55:41,357 EPOCH 41
2024-02-03 05:55:42,853 [Epoch: 041 Step: 00002700] Batch Recognition Loss:   0.068326 => Gls Tokens per Sec:     2143 || Batch Translation Loss:  10.828695 => Txt Tokens per Sec:     5711 || Lr: 0.000100
2024-02-03 05:55:46,736 Epoch  41: Total Training Recognition Loss 6.53  Total Training Translation Loss 737.55 
2024-02-03 05:55:46,737 EPOCH 42
2024-02-03 05:55:50,794 [Epoch: 042 Step: 00002800] Batch Recognition Loss:   0.085113 => Gls Tokens per Sec:     2090 || Batch Translation Loss:  11.688619 => Txt Tokens per Sec:     5840 || Lr: 0.000100
2024-02-03 05:55:51,982 Epoch  42: Total Training Recognition Loss 6.49  Total Training Translation Loss 669.94 
2024-02-03 05:55:51,983 EPOCH 43
2024-02-03 05:55:57,301 Epoch  43: Total Training Recognition Loss 6.63  Total Training Translation Loss 610.00 
2024-02-03 05:55:57,301 EPOCH 44
2024-02-03 05:55:59,036 [Epoch: 044 Step: 00002900] Batch Recognition Loss:   0.107715 => Gls Tokens per Sec:     1704 || Batch Translation Loss:   4.472294 => Txt Tokens per Sec:     4796 || Lr: 0.000100
2024-02-03 05:56:03,125 Epoch  44: Total Training Recognition Loss 6.30  Total Training Translation Loss 543.54 
2024-02-03 05:56:03,126 EPOCH 45
2024-02-03 05:56:06,610 [Epoch: 045 Step: 00003000] Batch Recognition Loss:   0.122335 => Gls Tokens per Sec:     2362 || Batch Translation Loss:   8.548654 => Txt Tokens per Sec:     6361 || Lr: 0.000100
2024-02-03 05:56:07,814 Epoch  45: Total Training Recognition Loss 6.28  Total Training Translation Loss 503.63 
2024-02-03 05:56:07,815 EPOCH 46
2024-02-03 05:56:13,398 Epoch  46: Total Training Recognition Loss 6.00  Total Training Translation Loss 464.30 
2024-02-03 05:56:13,399 EPOCH 47
2024-02-03 05:56:14,362 [Epoch: 047 Step: 00003100] Batch Recognition Loss:   0.037278 => Gls Tokens per Sec:     2994 || Batch Translation Loss:   6.909336 => Txt Tokens per Sec:     7199 || Lr: 0.000100
2024-02-03 05:56:18,015 Epoch  47: Total Training Recognition Loss 5.75  Total Training Translation Loss 421.75 
2024-02-03 05:56:18,015 EPOCH 48
2024-02-03 05:56:22,125 [Epoch: 048 Step: 00003200] Batch Recognition Loss:   0.023131 => Gls Tokens per Sec:     1986 || Batch Translation Loss:   5.525378 => Txt Tokens per Sec:     5479 || Lr: 0.000100
2024-02-03 05:56:23,428 Epoch  48: Total Training Recognition Loss 5.41  Total Training Translation Loss 373.43 
2024-02-03 05:56:23,428 EPOCH 49
2024-02-03 05:56:28,348 Epoch  49: Total Training Recognition Loss 5.33  Total Training Translation Loss 332.20 
2024-02-03 05:56:28,349 EPOCH 50
2024-02-03 05:56:29,753 [Epoch: 050 Step: 00003300] Batch Recognition Loss:   0.047770 => Gls Tokens per Sec:     1941 || Batch Translation Loss:   5.631779 => Txt Tokens per Sec:     5191 || Lr: 0.000100
2024-02-03 05:56:33,799 Epoch  50: Total Training Recognition Loss 5.31  Total Training Translation Loss 307.82 
2024-02-03 05:56:33,799 EPOCH 51
2024-02-03 05:56:37,730 [Epoch: 051 Step: 00003400] Batch Recognition Loss:   0.088233 => Gls Tokens per Sec:     2013 || Batch Translation Loss:   5.000405 => Txt Tokens per Sec:     5699 || Lr: 0.000100
2024-02-03 05:56:39,109 Epoch  51: Total Training Recognition Loss 5.14  Total Training Translation Loss 276.84 
2024-02-03 05:56:39,109 EPOCH 52
2024-02-03 05:56:44,153 Epoch  52: Total Training Recognition Loss 5.11  Total Training Translation Loss 266.30 
2024-02-03 05:56:44,153 EPOCH 53
2024-02-03 05:56:45,306 [Epoch: 053 Step: 00003500] Batch Recognition Loss:   0.035054 => Gls Tokens per Sec:     2224 || Batch Translation Loss:   3.090161 => Txt Tokens per Sec:     6150 || Lr: 0.000100
2024-02-03 05:56:49,554 Epoch  53: Total Training Recognition Loss 4.62  Total Training Translation Loss 236.03 
2024-02-03 05:56:49,555 EPOCH 54
2024-02-03 05:56:53,287 [Epoch: 054 Step: 00003600] Batch Recognition Loss:   0.043626 => Gls Tokens per Sec:     2101 || Batch Translation Loss:   3.882994 => Txt Tokens per Sec:     5719 || Lr: 0.000100
2024-02-03 05:56:54,881 Epoch  54: Total Training Recognition Loss 4.59  Total Training Translation Loss 216.32 
2024-02-03 05:56:54,881 EPOCH 55
2024-02-03 05:56:59,918 Epoch  55: Total Training Recognition Loss 4.46  Total Training Translation Loss 201.20 
2024-02-03 05:56:59,918 EPOCH 56
2024-02-03 05:57:01,317 [Epoch: 056 Step: 00003700] Batch Recognition Loss:   0.035817 => Gls Tokens per Sec:     1652 || Batch Translation Loss:   1.847587 => Txt Tokens per Sec:     5004 || Lr: 0.000100
2024-02-03 05:57:05,326 Epoch  56: Total Training Recognition Loss 4.39  Total Training Translation Loss 193.09 
2024-02-03 05:57:05,327 EPOCH 57
2024-02-03 05:57:09,300 [Epoch: 057 Step: 00003800] Batch Recognition Loss:   0.060083 => Gls Tokens per Sec:     1934 || Batch Translation Loss:   3.250468 => Txt Tokens per Sec:     5374 || Lr: 0.000100
2024-02-03 05:57:10,777 Epoch  57: Total Training Recognition Loss 4.26  Total Training Translation Loss 173.75 
2024-02-03 05:57:10,777 EPOCH 58
2024-02-03 05:57:15,314 Epoch  58: Total Training Recognition Loss 3.95  Total Training Translation Loss 157.38 
2024-02-03 05:57:15,315 EPOCH 59
2024-02-03 05:57:16,231 [Epoch: 059 Step: 00003900] Batch Recognition Loss:   0.047857 => Gls Tokens per Sec:     2445 || Batch Translation Loss:   2.127932 => Txt Tokens per Sec:     6592 || Lr: 0.000100
2024-02-03 05:57:20,519 Epoch  59: Total Training Recognition Loss 3.76  Total Training Translation Loss 147.58 
2024-02-03 05:57:20,520 EPOCH 60
2024-02-03 05:57:23,929 [Epoch: 060 Step: 00004000] Batch Recognition Loss:   0.028520 => Gls Tokens per Sec:     2180 || Batch Translation Loss:   2.047864 => Txt Tokens per Sec:     6047 || Lr: 0.000100
2024-02-03 05:57:32,323 Validation result at epoch  60, step     4000: duration: 8.3945s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 5.46036	Translation Loss: 69399.75781	PPL: 1350.34790
	Eval Metric: BLEU
	WER 6.15	(DEL: 0.07,	INS: 0.00,	SUB: 6.08)
	BLEU-4 0.83	(BLEU-1: 12.52,	BLEU-2: 4.15,	BLEU-3: 1.66,	BLEU-4: 0.83)
	CHRF 17.63	ROUGE 10.18
2024-02-03 05:57:32,324 Logging Recognition and Translation Outputs
2024-02-03 05:57:32,324 ========================================================================================================================
2024-02-03 05:57:32,325 Logging Sequence: 52_208.00
2024-02-03 05:57:32,325 	Gloss Reference :	A B+C+D+E
2024-02-03 05:57:32,325 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 05:57:32,325 	Gloss Alignment :	         
2024-02-03 05:57:32,325 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 05:57:32,327 	Text Reference  :	*** ***** *** ***** ** after seeing   dhoni play  within  3   hours 36  lakh people   downloaded candy crush
2024-02-03 05:57:32,327 	Text Hypothesis :	the tweet was taken to the   official candy crush account but now   why was  reshared by         rs    132  
2024-02-03 05:57:32,327 	Text Alignment  :	I   I     I   I     I  S     S        S     S     S       S   S     S   S    S        S          S     S    
2024-02-03 05:57:32,327 ========================================================================================================================
2024-02-03 05:57:32,327 Logging Sequence: 177_79.00
2024-02-03 05:57:32,327 	Gloss Reference :	A B+C+D+E
2024-02-03 05:57:32,327 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 05:57:32,328 	Gloss Alignment :	         
2024-02-03 05:57:32,328 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 05:57:32,329 	Text Reference  :	finally on     23rd      may  both sushil and         ajay were  arrested in  delhi' mundka area    
2024-02-03 05:57:32,329 	Text Hypothesis :	delhi   police announced such a    huge   controversy in   hansi as       per rules  for    covid-19
2024-02-03 05:57:32,329 	Text Alignment  :	S       S      S         S    S    S      S           S    S     S        S   S      S      S       
2024-02-03 05:57:32,329 ========================================================================================================================
2024-02-03 05:57:32,329 Logging Sequence: 107_94.00
2024-02-03 05:57:32,330 	Gloss Reference :	A B+C+D+E
2024-02-03 05:57:32,330 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 05:57:32,330 	Gloss Alignment :	         
2024-02-03 05:57:32,330 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 05:57:32,331 	Text Reference  :	and currently development officer of  the *** bengal tennis association bta said   
2024-02-03 05:57:32,331 	Text Hypothesis :	*** ********* india       had     won the t20 world  cup    for         new zealand
2024-02-03 05:57:32,331 	Text Alignment  :	D   D         S           S       S       I   S      S      S           S   S      
2024-02-03 05:57:32,331 ========================================================================================================================
2024-02-03 05:57:32,331 Logging Sequence: 114_153.00
2024-02-03 05:57:32,331 	Gloss Reference :	A B+C+D+E
2024-02-03 05:57:32,331 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 05:57:32,332 	Gloss Alignment :	         
2024-02-03 05:57:32,332 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 05:57:32,332 	Text Reference  :	another big football news is    that the  copa america final       
2024-02-03 05:57:32,333 	Text Hypothesis :	******* he  had      not  known as   well and  spain   respectively
2024-02-03 05:57:32,333 	Text Alignment  :	D       S   S        S    S     S    S    S    S       S           
2024-02-03 05:57:32,333 ========================================================================================================================
2024-02-03 05:57:32,333 Logging Sequence: 52_36.00
2024-02-03 05:57:32,333 	Gloss Reference :	A B+C+D+E
2024-02-03 05:57:32,333 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 05:57:32,333 	Gloss Alignment :	         
2024-02-03 05:57:32,334 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 05:57:32,334 	Text Reference  :	*** ***** *** *** *** ***** **** recently dhoni ** was   travellin on    an   indigo flight 
2024-02-03 05:57:32,335 	Text Hypothesis :	the tweet was not the first time and      dhoni is going to        start from 16th   october
2024-02-03 05:57:32,335 	Text Alignment  :	I   I     I   I   I   I     I    S              I  S     S         S     S    S      S      
2024-02-03 05:57:32,335 ========================================================================================================================
2024-02-03 05:57:34,058 Epoch  60: Total Training Recognition Loss 3.88  Total Training Translation Loss 141.75 
2024-02-03 05:57:34,058 EPOCH 61
2024-02-03 05:57:39,596 Epoch  61: Total Training Recognition Loss 3.51  Total Training Translation Loss 133.50 
2024-02-03 05:57:39,597 EPOCH 62
2024-02-03 05:57:40,526 [Epoch: 062 Step: 00004100] Batch Recognition Loss:   0.032569 => Gls Tokens per Sec:     2244 || Batch Translation Loss:   2.819468 => Txt Tokens per Sec:     6479 || Lr: 0.000100
2024-02-03 05:57:44,831 Epoch  62: Total Training Recognition Loss 3.39  Total Training Translation Loss 121.26 
2024-02-03 05:57:44,831 EPOCH 63
2024-02-03 05:57:48,416 [Epoch: 063 Step: 00004200] Batch Recognition Loss:   0.013995 => Gls Tokens per Sec:     2054 || Batch Translation Loss:   1.689043 => Txt Tokens per Sec:     5726 || Lr: 0.000100
2024-02-03 05:57:50,319 Epoch  63: Total Training Recognition Loss 3.38  Total Training Translation Loss 114.00 
2024-02-03 05:57:50,319 EPOCH 64
2024-02-03 05:57:55,638 Epoch  64: Total Training Recognition Loss 3.28  Total Training Translation Loss 107.19 
2024-02-03 05:57:55,639 EPOCH 65
2024-02-03 05:57:56,439 [Epoch: 065 Step: 00004300] Batch Recognition Loss:   0.043109 => Gls Tokens per Sec:     2403 || Batch Translation Loss:   1.080378 => Txt Tokens per Sec:     5860 || Lr: 0.000100
2024-02-03 05:58:00,967 Epoch  65: Total Training Recognition Loss 3.24  Total Training Translation Loss 98.65 
2024-02-03 05:58:00,968 EPOCH 66
2024-02-03 05:58:05,021 [Epoch: 066 Step: 00004400] Batch Recognition Loss:   0.016117 => Gls Tokens per Sec:     1755 || Batch Translation Loss:   1.792916 => Txt Tokens per Sec:     5052 || Lr: 0.000100
2024-02-03 05:58:06,659 Epoch  66: Total Training Recognition Loss 3.37  Total Training Translation Loss 101.26 
2024-02-03 05:58:06,659 EPOCH 67
2024-02-03 05:58:11,771 Epoch  67: Total Training Recognition Loss 3.09  Total Training Translation Loss 97.77 
2024-02-03 05:58:11,772 EPOCH 68
2024-02-03 05:58:12,620 [Epoch: 068 Step: 00004500] Batch Recognition Loss:   0.012328 => Gls Tokens per Sec:     1972 || Batch Translation Loss:   1.261576 => Txt Tokens per Sec:     5420 || Lr: 0.000100
2024-02-03 05:58:17,206 Epoch  68: Total Training Recognition Loss 2.98  Total Training Translation Loss 95.05 
2024-02-03 05:58:17,206 EPOCH 69
2024-02-03 05:58:20,336 [Epoch: 069 Step: 00004600] Batch Recognition Loss:   0.034202 => Gls Tokens per Sec:     2222 || Batch Translation Loss:   0.841238 => Txt Tokens per Sec:     6090 || Lr: 0.000100
2024-02-03 05:58:22,295 Epoch  69: Total Training Recognition Loss 2.82  Total Training Translation Loss 96.41 
2024-02-03 05:58:22,296 EPOCH 70
2024-02-03 05:58:27,793 Epoch  70: Total Training Recognition Loss 2.72  Total Training Translation Loss 85.69 
2024-02-03 05:58:27,793 EPOCH 71
2024-02-03 05:58:28,476 [Epoch: 071 Step: 00004700] Batch Recognition Loss:   0.053561 => Gls Tokens per Sec:     2345 || Batch Translation Loss:   1.234502 => Txt Tokens per Sec:     6106 || Lr: 0.000100
2024-02-03 05:58:32,969 Epoch  71: Total Training Recognition Loss 2.44  Total Training Translation Loss 79.47 
2024-02-03 05:58:32,970 EPOCH 72
2024-02-03 05:58:36,711 [Epoch: 072 Step: 00004800] Batch Recognition Loss:   0.046120 => Gls Tokens per Sec:     1815 || Batch Translation Loss:   1.612693 => Txt Tokens per Sec:     5139 || Lr: 0.000100
2024-02-03 05:58:38,423 Epoch  72: Total Training Recognition Loss 3.03  Total Training Translation Loss 76.23 
2024-02-03 05:58:38,424 EPOCH 73
2024-02-03 05:58:43,893 Epoch  73: Total Training Recognition Loss 2.37  Total Training Translation Loss 74.68 
2024-02-03 05:58:43,893 EPOCH 74
2024-02-03 05:58:44,660 [Epoch: 074 Step: 00004900] Batch Recognition Loss:   0.148070 => Gls Tokens per Sec:     1883 || Batch Translation Loss:   1.087617 => Txt Tokens per Sec:     5591 || Lr: 0.000100
2024-02-03 05:58:48,985 Epoch  74: Total Training Recognition Loss 2.54  Total Training Translation Loss 66.64 
2024-02-03 05:58:48,986 EPOCH 75
2024-02-03 05:58:52,249 [Epoch: 075 Step: 00005000] Batch Recognition Loss:   0.017701 => Gls Tokens per Sec:     2060 || Batch Translation Loss:   0.737269 => Txt Tokens per Sec:     5584 || Lr: 0.000100
2024-02-03 05:58:54,422 Epoch  75: Total Training Recognition Loss 2.37  Total Training Translation Loss 63.04 
2024-02-03 05:58:54,422 EPOCH 76
2024-02-03 05:58:59,931 Epoch  76: Total Training Recognition Loss 2.55  Total Training Translation Loss 58.59 
2024-02-03 05:58:59,932 EPOCH 77
2024-02-03 05:59:00,702 [Epoch: 077 Step: 00005100] Batch Recognition Loss:   0.028097 => Gls Tokens per Sec:     1549 || Batch Translation Loss:   1.001591 => Txt Tokens per Sec:     4628 || Lr: 0.000100
2024-02-03 05:59:05,103 Epoch  77: Total Training Recognition Loss 2.44  Total Training Translation Loss 57.69 
2024-02-03 05:59:05,103 EPOCH 78
2024-02-03 05:59:08,331 [Epoch: 078 Step: 00005200] Batch Recognition Loss:   0.030098 => Gls Tokens per Sec:     2006 || Batch Translation Loss:   0.936150 => Txt Tokens per Sec:     5455 || Lr: 0.000100
2024-02-03 05:59:10,551 Epoch  78: Total Training Recognition Loss 2.24  Total Training Translation Loss 55.22 
2024-02-03 05:59:10,551 EPOCH 79
2024-02-03 05:59:15,515 Epoch  79: Total Training Recognition Loss 2.03  Total Training Translation Loss 53.28 
2024-02-03 05:59:15,515 EPOCH 80
2024-02-03 05:59:15,999 [Epoch: 080 Step: 00005300] Batch Recognition Loss:   0.027310 => Gls Tokens per Sec:     2320 || Batch Translation Loss:   1.048000 => Txt Tokens per Sec:     6122 || Lr: 0.000100
2024-02-03 05:59:20,640 Epoch  80: Total Training Recognition Loss 2.21  Total Training Translation Loss 54.64 
2024-02-03 05:59:20,641 EPOCH 81
2024-02-03 05:59:23,768 [Epoch: 081 Step: 00005400] Batch Recognition Loss:   0.025754 => Gls Tokens per Sec:     2047 || Batch Translation Loss:   0.845777 => Txt Tokens per Sec:     5720 || Lr: 0.000100
2024-02-03 05:59:26,018 Epoch  81: Total Training Recognition Loss 2.25  Total Training Translation Loss 53.88 
2024-02-03 05:59:26,018 EPOCH 82
2024-02-03 05:59:31,435 Epoch  82: Total Training Recognition Loss 2.15  Total Training Translation Loss 56.37 
2024-02-03 05:59:31,436 EPOCH 83
2024-02-03 05:59:31,866 [Epoch: 083 Step: 00005500] Batch Recognition Loss:   0.042252 => Gls Tokens per Sec:     2238 || Batch Translation Loss:   1.106792 => Txt Tokens per Sec:     6508 || Lr: 0.000100
2024-02-03 05:59:36,599 Epoch  83: Total Training Recognition Loss 2.12  Total Training Translation Loss 55.88 
2024-02-03 05:59:36,600 EPOCH 84
2024-02-03 05:59:40,025 [Epoch: 084 Step: 00005600] Batch Recognition Loss:   0.031263 => Gls Tokens per Sec:     1797 || Batch Translation Loss:   0.594643 => Txt Tokens per Sec:     5231 || Lr: 0.000100
2024-02-03 05:59:42,016 Epoch  84: Total Training Recognition Loss 2.15  Total Training Translation Loss 63.61 
2024-02-03 05:59:42,016 EPOCH 85
2024-02-03 05:59:47,303 Epoch  85: Total Training Recognition Loss 2.19  Total Training Translation Loss 49.77 
2024-02-03 05:59:47,304 EPOCH 86
2024-02-03 05:59:47,633 [Epoch: 086 Step: 00005700] Batch Recognition Loss:   0.020464 => Gls Tokens per Sec:     2446 || Batch Translation Loss:   0.926779 => Txt Tokens per Sec:     6767 || Lr: 0.000100
2024-02-03 05:59:52,703 Epoch  86: Total Training Recognition Loss 1.91  Total Training Translation Loss 45.80 
2024-02-03 05:59:52,703 EPOCH 87
2024-02-03 05:59:55,787 [Epoch: 087 Step: 00005800] Batch Recognition Loss:   0.015020 => Gls Tokens per Sec:     1943 || Batch Translation Loss:   0.659439 => Txt Tokens per Sec:     5375 || Lr: 0.000100
2024-02-03 05:59:58,252 Epoch  87: Total Training Recognition Loss 2.26  Total Training Translation Loss 44.73 
2024-02-03 05:59:58,253 EPOCH 88
2024-02-03 06:00:03,266 Epoch  88: Total Training Recognition Loss 1.88  Total Training Translation Loss 41.26 
2024-02-03 06:00:03,266 EPOCH 89
2024-02-03 06:00:03,544 [Epoch: 089 Step: 00005900] Batch Recognition Loss:   0.018721 => Gls Tokens per Sec:     2316 || Batch Translation Loss:   0.344262 => Txt Tokens per Sec:     6307 || Lr: 0.000100
2024-02-03 06:00:08,704 Epoch  89: Total Training Recognition Loss 2.13  Total Training Translation Loss 44.51 
2024-02-03 06:00:08,705 EPOCH 90
2024-02-03 06:00:11,227 [Epoch: 090 Step: 00006000] Batch Recognition Loss:   0.010208 => Gls Tokens per Sec:     2313 || Batch Translation Loss:   0.604180 => Txt Tokens per Sec:     6348 || Lr: 0.000100
2024-02-03 06:00:19,795 Hooray! New best validation result [eval_metric]!
2024-02-03 06:00:19,796 Saving new checkpoint.
2024-02-03 06:00:20,064 Validation result at epoch  90, step     6000: duration: 8.8364s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 5.86722	Translation Loss: 76116.32031	PPL: 2712.76733
	Eval Metric: BLEU
	WER 4.95	(DEL: 0.07,	INS: 0.00,	SUB: 4.88)
	BLEU-4 0.85	(BLEU-1: 12.06,	BLEU-2: 3.91,	BLEU-3: 1.57,	BLEU-4: 0.85)
	CHRF 17.67	ROUGE 9.79
2024-02-03 06:00:20,065 Logging Recognition and Translation Outputs
2024-02-03 06:00:20,065 ========================================================================================================================
2024-02-03 06:00:20,066 Logging Sequence: 101_39.00
2024-02-03 06:00:20,066 	Gloss Reference :	A B+C+D+E
2024-02-03 06:00:20,066 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:00:20,066 	Gloss Alignment :	         
2024-02-03 06:00:20,066 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:00:20,067 	Text Reference  :	star batsmen and bowlers of the indian team were infected by the coronavirus
2024-02-03 06:00:20,067 	Text Hypothesis :	he   scored  2   runs    on the ****** **** **** ******** ** 5th series     
2024-02-03 06:00:20,067 	Text Alignment  :	S    S       S   S       S      D      D    D    D        D  S   S          
2024-02-03 06:00:20,067 ========================================================================================================================
2024-02-03 06:00:20,067 Logging Sequence: 105_139.00
2024-02-03 06:00:20,068 	Gloss Reference :	A B+C+D+E
2024-02-03 06:00:20,068 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:00:20,068 	Gloss Alignment :	         
2024-02-03 06:00:20,068 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:00:20,069 	Text Reference  :	**** and now he   has  finally achieved his dream by   defeating carlsen
2024-02-03 06:00:20,069 	Text Hypothesis :	when it  was four four years   old      him was   held in        him    
2024-02-03 06:00:20,069 	Text Alignment  :	I    S   S   S    S    S       S        S   S     S    S         S      
2024-02-03 06:00:20,069 ========================================================================================================================
2024-02-03 06:00:20,069 Logging Sequence: 85_17.00
2024-02-03 06:00:20,070 	Gloss Reference :	A B+C+D+E
2024-02-03 06:00:20,070 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:00:20,070 	Gloss Alignment :	         
2024-02-03 06:00:20,070 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:00:20,071 	Text Reference  :	in the ****** ********* ** ***** *** 2003 world     cup   symonds    scored an   unbeaten 143   against pakistan
2024-02-03 06:00:20,071 	Text Hypothesis :	** the sports authority of india sai also expressed their daughter's and    hand with     other january 2022    
2024-02-03 06:00:20,071 	Text Alignment  :	D      I      I         I  I     I   S    S         S     S          S      S    S        S     S       S       
2024-02-03 06:00:20,072 ========================================================================================================================
2024-02-03 06:00:20,072 Logging Sequence: 150_98.00
2024-02-03 06:00:20,072 	Gloss Reference :	A B+C+D+E
2024-02-03 06:00:20,072 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:00:20,072 	Gloss Alignment :	         
2024-02-03 06:00:20,072 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:00:20,073 	Text Reference  :	** chhetri was the ***** captain
2024-02-03 06:00:20,073 	Text Hypothesis :	so what    is  the first time   
2024-02-03 06:00:20,073 	Text Alignment  :	I  S       S       I     S      
2024-02-03 06:00:20,073 ========================================================================================================================
2024-02-03 06:00:20,073 Logging Sequence: 147_76.00
2024-02-03 06:00:20,073 	Gloss Reference :	A B+C+D+E
2024-02-03 06:00:20,073 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:00:20,073 	Gloss Alignment :	         
2024-02-03 06:00:20,074 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:00:20,075 	Text Reference  :	***** **** **** ***** **** *** however on 28 july   she   announced her   withdrawal from   the olympic games
2024-02-03 06:00:20,075 	Text Hypothesis :	there were also claim that two teams   on ** social media with      india and        others on  social  media
2024-02-03 06:00:20,075 	Text Alignment  :	I     I    I    I     I    I   S          D  S      S     S         S     S          S      S   S       S    
2024-02-03 06:00:20,075 ========================================================================================================================
2024-02-03 06:00:22,646 Epoch  90: Total Training Recognition Loss 2.10  Total Training Translation Loss 49.27 
2024-02-03 06:00:22,646 EPOCH 91
2024-02-03 06:00:28,054 Epoch  91: Total Training Recognition Loss 1.88  Total Training Translation Loss 55.85 
2024-02-03 06:00:28,055 EPOCH 92
2024-02-03 06:00:28,264 [Epoch: 092 Step: 00006100] Batch Recognition Loss:   0.031912 => Gls Tokens per Sec:     2308 || Batch Translation Loss:   1.022410 => Txt Tokens per Sec:     6332 || Lr: 0.000100
2024-02-03 06:00:32,987 Epoch  92: Total Training Recognition Loss 1.90  Total Training Translation Loss 51.81 
2024-02-03 06:00:32,988 EPOCH 93
2024-02-03 06:00:35,863 [Epoch: 093 Step: 00006200] Batch Recognition Loss:   0.077577 => Gls Tokens per Sec:     2004 || Batch Translation Loss:   0.762239 => Txt Tokens per Sec:     5667 || Lr: 0.000100
2024-02-03 06:00:38,230 Epoch  93: Total Training Recognition Loss 1.93  Total Training Translation Loss 40.46 
2024-02-03 06:00:38,231 EPOCH 94
2024-02-03 06:00:43,370 Epoch  94: Total Training Recognition Loss 1.90  Total Training Translation Loss 31.44 
2024-02-03 06:00:43,370 EPOCH 95
2024-02-03 06:00:43,478 [Epoch: 095 Step: 00006300] Batch Recognition Loss:   0.008695 => Gls Tokens per Sec:     2991 || Batch Translation Loss:   0.311242 => Txt Tokens per Sec:     6392 || Lr: 0.000100
2024-02-03 06:00:48,315 Epoch  95: Total Training Recognition Loss 1.80  Total Training Translation Loss 30.80 
2024-02-03 06:00:48,315 EPOCH 96
2024-02-03 06:00:51,285 [Epoch: 096 Step: 00006400] Batch Recognition Loss:   0.005393 => Gls Tokens per Sec:     1856 || Batch Translation Loss:   0.551039 => Txt Tokens per Sec:     5172 || Lr: 0.000100
2024-02-03 06:00:53,770 Epoch  96: Total Training Recognition Loss 1.57  Total Training Translation Loss 32.18 
2024-02-03 06:00:53,771 EPOCH 97
2024-02-03 06:00:58,615 Epoch  97: Total Training Recognition Loss 1.47  Total Training Translation Loss 34.17 
2024-02-03 06:00:58,615 EPOCH 98
2024-02-03 06:00:58,668 [Epoch: 098 Step: 00006500] Batch Recognition Loss:   0.005983 => Gls Tokens per Sec:     3077 || Batch Translation Loss:   0.470811 => Txt Tokens per Sec:     7154 || Lr: 0.000100
2024-02-03 06:01:03,936 Epoch  98: Total Training Recognition Loss 1.66  Total Training Translation Loss 32.44 
2024-02-03 06:01:03,936 EPOCH 99
2024-02-03 06:01:06,399 [Epoch: 099 Step: 00006600] Batch Recognition Loss:   0.011859 => Gls Tokens per Sec:     2210 || Batch Translation Loss:   0.920440 => Txt Tokens per Sec:     6081 || Lr: 0.000100
2024-02-03 06:01:08,738 Epoch  99: Total Training Recognition Loss 1.37  Total Training Translation Loss 32.04 
2024-02-03 06:01:08,738 EPOCH 100
2024-02-03 06:01:14,060 [Epoch: 100 Step: 00006700] Batch Recognition Loss:   0.005690 => Gls Tokens per Sec:     1997 || Batch Translation Loss:   0.523543 => Txt Tokens per Sec:     5555 || Lr: 0.000100
2024-02-03 06:01:14,061 Epoch 100: Total Training Recognition Loss 1.59  Total Training Translation Loss 33.09 
2024-02-03 06:01:14,061 EPOCH 101
2024-02-03 06:01:19,587 Epoch 101: Total Training Recognition Loss 1.35  Total Training Translation Loss 29.93 
2024-02-03 06:01:19,587 EPOCH 102
2024-02-03 06:01:22,075 [Epoch: 102 Step: 00006800] Batch Recognition Loss:   0.002295 => Gls Tokens per Sec:     2123 || Batch Translation Loss:   0.374057 => Txt Tokens per Sec:     6010 || Lr: 0.000100
2024-02-03 06:01:24,713 Epoch 102: Total Training Recognition Loss 1.32  Total Training Translation Loss 30.62 
2024-02-03 06:01:24,713 EPOCH 103
2024-02-03 06:01:30,167 [Epoch: 103 Step: 00006900] Batch Recognition Loss:   0.004813 => Gls Tokens per Sec:     1920 || Batch Translation Loss:   0.346736 => Txt Tokens per Sec:     5348 || Lr: 0.000100
2024-02-03 06:01:30,233 Epoch 103: Total Training Recognition Loss 1.46  Total Training Translation Loss 28.18 
2024-02-03 06:01:30,233 EPOCH 104
2024-02-03 06:01:35,400 Epoch 104: Total Training Recognition Loss 1.64  Total Training Translation Loss 35.51 
2024-02-03 06:01:35,400 EPOCH 105
2024-02-03 06:01:38,215 [Epoch: 105 Step: 00007000] Batch Recognition Loss:   0.015040 => Gls Tokens per Sec:     1787 || Batch Translation Loss:   0.820662 => Txt Tokens per Sec:     5173 || Lr: 0.000100
2024-02-03 06:01:40,969 Epoch 105: Total Training Recognition Loss 1.69  Total Training Translation Loss 58.74 
2024-02-03 06:01:40,969 EPOCH 106
2024-02-03 06:01:45,551 [Epoch: 106 Step: 00007100] Batch Recognition Loss:   0.024532 => Gls Tokens per Sec:     2251 || Batch Translation Loss:   1.033309 => Txt Tokens per Sec:     6247 || Lr: 0.000100
2024-02-03 06:01:45,696 Epoch 106: Total Training Recognition Loss 2.19  Total Training Translation Loss 55.29 
2024-02-03 06:01:45,696 EPOCH 107
2024-02-03 06:01:51,020 Epoch 107: Total Training Recognition Loss 1.72  Total Training Translation Loss 37.93 
2024-02-03 06:01:51,020 EPOCH 108
2024-02-03 06:01:53,110 [Epoch: 108 Step: 00007200] Batch Recognition Loss:   0.095004 => Gls Tokens per Sec:     2376 || Batch Translation Loss:   0.406345 => Txt Tokens per Sec:     6373 || Lr: 0.000100
2024-02-03 06:01:55,950 Epoch 108: Total Training Recognition Loss 1.53  Total Training Translation Loss 24.24 
2024-02-03 06:01:55,950 EPOCH 109
2024-02-03 06:02:01,210 [Epoch: 109 Step: 00007300] Batch Recognition Loss:   0.046315 => Gls Tokens per Sec:     1930 || Batch Translation Loss:   0.091369 => Txt Tokens per Sec:     5374 || Lr: 0.000100
2024-02-03 06:02:01,427 Epoch 109: Total Training Recognition Loss 1.25  Total Training Translation Loss 23.53 
2024-02-03 06:02:01,427 EPOCH 110
2024-02-03 06:02:06,764 Epoch 110: Total Training Recognition Loss 1.51  Total Training Translation Loss 22.67 
2024-02-03 06:02:06,764 EPOCH 111
2024-02-03 06:02:09,265 [Epoch: 111 Step: 00007400] Batch Recognition Loss:   0.007594 => Gls Tokens per Sec:     1884 || Batch Translation Loss:   0.241590 => Txt Tokens per Sec:     5334 || Lr: 0.000100
2024-02-03 06:02:12,006 Epoch 111: Total Training Recognition Loss 1.15  Total Training Translation Loss 19.01 
2024-02-03 06:02:12,006 EPOCH 112
2024-02-03 06:02:17,101 [Epoch: 112 Step: 00007500] Batch Recognition Loss:   0.030907 => Gls Tokens per Sec:     1961 || Batch Translation Loss:   0.356425 => Txt Tokens per Sec:     5547 || Lr: 0.000100
2024-02-03 06:02:17,343 Epoch 112: Total Training Recognition Loss 1.17  Total Training Translation Loss 19.30 
2024-02-03 06:02:17,343 EPOCH 113
2024-02-03 06:02:22,335 Epoch 113: Total Training Recognition Loss 1.09  Total Training Translation Loss 16.98 
2024-02-03 06:02:22,335 EPOCH 114
2024-02-03 06:02:24,873 [Epoch: 114 Step: 00007600] Batch Recognition Loss:   0.045685 => Gls Tokens per Sec:     1830 || Batch Translation Loss:   0.123925 => Txt Tokens per Sec:     5004 || Lr: 0.000100
2024-02-03 06:02:27,972 Epoch 114: Total Training Recognition Loss 1.03  Total Training Translation Loss 21.62 
2024-02-03 06:02:27,972 EPOCH 115
2024-02-03 06:02:32,723 [Epoch: 115 Step: 00007700] Batch Recognition Loss:   0.021219 => Gls Tokens per Sec:     2069 || Batch Translation Loss:   0.463309 => Txt Tokens per Sec:     5698 || Lr: 0.000100
2024-02-03 06:02:33,161 Epoch 115: Total Training Recognition Loss 1.10  Total Training Translation Loss 22.29 
2024-02-03 06:02:33,161 EPOCH 116
2024-02-03 06:02:38,337 Epoch 116: Total Training Recognition Loss 1.29  Total Training Translation Loss 24.11 
2024-02-03 06:02:38,337 EPOCH 117
2024-02-03 06:02:40,709 [Epoch: 117 Step: 00007800] Batch Recognition Loss:   0.006942 => Gls Tokens per Sec:     1852 || Batch Translation Loss:   0.338162 => Txt Tokens per Sec:     5337 || Lr: 0.000100
2024-02-03 06:02:43,735 Epoch 117: Total Training Recognition Loss 1.14  Total Training Translation Loss 24.90 
2024-02-03 06:02:43,736 EPOCH 118
2024-02-03 06:02:48,244 [Epoch: 118 Step: 00007900] Batch Recognition Loss:   0.004493 => Gls Tokens per Sec:     2145 || Batch Translation Loss:   0.331068 => Txt Tokens per Sec:     5967 || Lr: 0.000100
2024-02-03 06:02:48,662 Epoch 118: Total Training Recognition Loss 1.20  Total Training Translation Loss 25.31 
2024-02-03 06:02:48,663 EPOCH 119
2024-02-03 06:02:54,357 Epoch 119: Total Training Recognition Loss 1.16  Total Training Translation Loss 27.39 
2024-02-03 06:02:54,358 EPOCH 120
2024-02-03 06:02:56,101 [Epoch: 120 Step: 00008000] Batch Recognition Loss:   0.020890 => Gls Tokens per Sec:     2430 || Batch Translation Loss:   0.382352 => Txt Tokens per Sec:     6335 || Lr: 0.000100
2024-02-03 06:03:04,574 Validation result at epoch 120, step     8000: duration: 8.4730s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 5.49331	Translation Loss: 81394.82812	PPL: 4693.66943
	Eval Metric: BLEU
	WER 4.67	(DEL: 0.00,	INS: 0.00,	SUB: 4.67)
	BLEU-4 0.69	(BLEU-1: 10.65,	BLEU-2: 3.23,	BLEU-3: 1.36,	BLEU-4: 0.69)
	CHRF 17.03	ROUGE 8.97
2024-02-03 06:03:04,576 Logging Recognition and Translation Outputs
2024-02-03 06:03:04,576 ========================================================================================================================
2024-02-03 06:03:04,576 Logging Sequence: 109_16.00
2024-02-03 06:03:04,576 	Gloss Reference :	A B+C+D+E
2024-02-03 06:03:04,576 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:03:04,576 	Gloss Alignment :	         
2024-02-03 06:03:04,577 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:03:04,577 	Text Reference  :	however the  match  was rescheduled as * ****** two   kkr players -   
2024-02-03 06:03:04,578 	Text Hypothesis :	neeraj  also served as  well        as a silver medal for his     team
2024-02-03 06:03:04,578 	Text Alignment  :	S       S    S      S   S              I I      S     S   S       S   
2024-02-03 06:03:04,578 ========================================================================================================================
2024-02-03 06:03:04,578 Logging Sequence: 156_272.00
2024-02-03 06:03:04,578 	Gloss Reference :	A B+C+D+E
2024-02-03 06:03:04,578 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:03:04,578 	Gloss Alignment :	         
2024-02-03 06:03:04,578 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:03:04,580 	Text Reference  :	miny' original captain was kieron pollard nicholas pooran was  stand-in captain in place of   him 
2024-02-03 06:03:04,580 	Text Hypothesis :	they  also     own     abu dhabi  knight  riders   in     2010 and      ended   on 30th  july 2023
2024-02-03 06:03:04,580 	Text Alignment  :	S     S        S       S   S      S       S        S      S    S        S       S  S     S    S   
2024-02-03 06:03:04,580 ========================================================================================================================
2024-02-03 06:03:04,580 Logging Sequence: 115_59.00
2024-02-03 06:03:04,580 	Gloss Reference :	A B+C+D+E
2024-02-03 06:03:04,581 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:03:04,581 	Gloss Alignment :	         
2024-02-03 06:03:04,581 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:03:04,581 	Text Reference  :	** *** **** ***** **** ****** ****** * she   now hosts   several cricket programmes
2024-02-03 06:03:04,582 	Text Hypothesis :	on the 15th march 2021 bumrah posted a video on  twitter which   went    viral     
2024-02-03 06:03:04,582 	Text Alignment  :	I  I   I    I     I    I      I      I S     S   S       S       S       S         
2024-02-03 06:03:04,582 ========================================================================================================================
2024-02-03 06:03:04,582 Logging Sequence: 63_35.00
2024-02-03 06:03:04,582 	Gloss Reference :	A B+C+D+E
2024-02-03 06:03:04,582 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:03:04,582 	Gloss Alignment :	         
2024-02-03 06:03:04,583 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:03:04,584 	Text Reference  :	companies interested in   buying the teams need to fill the  tender form by  paying rs 10  lakh
2024-02-03 06:03:04,584 	Text Hypothesis :	it        is         held at     the ***** **** ** **** time that   i    was held   at the time
2024-02-03 06:03:04,584 	Text Alignment  :	S         S          S    S          D     D    D  D    S    S      S    S   S      S  S   S   
2024-02-03 06:03:04,584 ========================================================================================================================
2024-02-03 06:03:04,584 Logging Sequence: 100_97.00
2024-02-03 06:03:04,585 	Gloss Reference :	A B+C+D+E
2024-02-03 06:03:04,585 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:03:04,585 	Gloss Alignment :	         
2024-02-03 06:03:04,585 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:03:04,586 	Text Reference  :	india had to play against pakistan in  the sahara cup   in **** canada
2024-02-03 06:03:04,586 	Text Hypothesis :	***** *** ** **** ******* who      won the ****** match in test series
2024-02-03 06:03:04,586 	Text Alignment  :	D     D   D  D    D       S        S       D      S        I    S     
2024-02-03 06:03:04,586 ========================================================================================================================
2024-02-03 06:03:08,132 Epoch 120: Total Training Recognition Loss 1.13  Total Training Translation Loss 27.74 
2024-02-03 06:03:08,133 EPOCH 121
2024-02-03 06:03:12,482 [Epoch: 121 Step: 00008100] Batch Recognition Loss:   0.017384 => Gls Tokens per Sec:     2187 || Batch Translation Loss:   0.375565 => Txt Tokens per Sec:     5983 || Lr: 0.000100
2024-02-03 06:03:13,088 Epoch 121: Total Training Recognition Loss 1.26  Total Training Translation Loss 24.86 
2024-02-03 06:03:13,089 EPOCH 122
2024-02-03 06:03:18,218 Epoch 122: Total Training Recognition Loss 1.24  Total Training Translation Loss 34.68 
2024-02-03 06:03:18,218 EPOCH 123
2024-02-03 06:03:20,319 [Epoch: 123 Step: 00008200] Batch Recognition Loss:   0.003942 => Gls Tokens per Sec:     1981 || Batch Translation Loss:   0.267288 => Txt Tokens per Sec:     5704 || Lr: 0.000100
2024-02-03 06:03:23,648 Epoch 123: Total Training Recognition Loss 1.34  Total Training Translation Loss 26.11 
2024-02-03 06:03:23,649 EPOCH 124
2024-02-03 06:03:28,522 [Epoch: 124 Step: 00008300] Batch Recognition Loss:   0.005184 => Gls Tokens per Sec:     1919 || Batch Translation Loss:   0.408571 => Txt Tokens per Sec:     5417 || Lr: 0.000100
2024-02-03 06:03:29,028 Epoch 124: Total Training Recognition Loss 1.21  Total Training Translation Loss 25.11 
2024-02-03 06:03:29,029 EPOCH 125
2024-02-03 06:03:34,405 Epoch 125: Total Training Recognition Loss 0.97  Total Training Translation Loss 19.63 
2024-02-03 06:03:34,405 EPOCH 126
2024-02-03 06:03:36,545 [Epoch: 126 Step: 00008400] Batch Recognition Loss:   0.010677 => Gls Tokens per Sec:     1870 || Batch Translation Loss:   0.142661 => Txt Tokens per Sec:     5493 || Lr: 0.000100
2024-02-03 06:03:39,863 Epoch 126: Total Training Recognition Loss 1.11  Total Training Translation Loss 18.89 
2024-02-03 06:03:39,863 EPOCH 127
2024-02-03 06:03:44,786 [Epoch: 127 Step: 00008500] Batch Recognition Loss:   0.036411 => Gls Tokens per Sec:     1867 || Batch Translation Loss:   0.337308 => Txt Tokens per Sec:     5260 || Lr: 0.000100
2024-02-03 06:03:45,559 Epoch 127: Total Training Recognition Loss 1.09  Total Training Translation Loss 21.47 
2024-02-03 06:03:45,560 EPOCH 128
2024-02-03 06:03:51,052 Epoch 128: Total Training Recognition Loss 1.00  Total Training Translation Loss 20.28 
2024-02-03 06:03:51,052 EPOCH 129
2024-02-03 06:03:52,635 [Epoch: 129 Step: 00008600] Batch Recognition Loss:   0.003910 => Gls Tokens per Sec:     2427 || Batch Translation Loss:   0.182430 => Txt Tokens per Sec:     6748 || Lr: 0.000100
2024-02-03 06:03:56,238 Epoch 129: Total Training Recognition Loss 1.04  Total Training Translation Loss 18.57 
2024-02-03 06:03:56,239 EPOCH 130
2024-02-03 06:04:00,775 [Epoch: 130 Step: 00008700] Batch Recognition Loss:   0.002892 => Gls Tokens per Sec:     1991 || Batch Translation Loss:   0.405676 => Txt Tokens per Sec:     5539 || Lr: 0.000100
2024-02-03 06:04:01,616 Epoch 130: Total Training Recognition Loss 1.07  Total Training Translation Loss 22.37 
2024-02-03 06:04:01,617 EPOCH 131
2024-02-03 06:04:07,223 Epoch 131: Total Training Recognition Loss 1.01  Total Training Translation Loss 20.52 
2024-02-03 06:04:07,224 EPOCH 132
2024-02-03 06:04:09,214 [Epoch: 132 Step: 00008800] Batch Recognition Loss:   0.012962 => Gls Tokens per Sec:     1804 || Batch Translation Loss:   0.222191 => Txt Tokens per Sec:     5096 || Lr: 0.000100
2024-02-03 06:04:12,665 Epoch 132: Total Training Recognition Loss 1.02  Total Training Translation Loss 19.04 
2024-02-03 06:04:12,666 EPOCH 133
2024-02-03 06:04:17,324 [Epoch: 133 Step: 00008900] Batch Recognition Loss:   0.002058 => Gls Tokens per Sec:     1904 || Batch Translation Loss:   0.223558 => Txt Tokens per Sec:     5332 || Lr: 0.000100
2024-02-03 06:04:18,084 Epoch 133: Total Training Recognition Loss 1.07  Total Training Translation Loss 18.70 
2024-02-03 06:04:18,085 EPOCH 134
2024-02-03 06:04:22,697 Epoch 134: Total Training Recognition Loss 0.92  Total Training Translation Loss 17.71 
2024-02-03 06:04:22,697 EPOCH 135
2024-02-03 06:04:24,715 [Epoch: 135 Step: 00009000] Batch Recognition Loss:   0.006810 => Gls Tokens per Sec:     1701 || Batch Translation Loss:   0.315533 => Txt Tokens per Sec:     4888 || Lr: 0.000100
2024-02-03 06:04:28,198 Epoch 135: Total Training Recognition Loss 0.96  Total Training Translation Loss 16.96 
2024-02-03 06:04:28,198 EPOCH 136
2024-02-03 06:04:32,173 [Epoch: 136 Step: 00009100] Batch Recognition Loss:   0.003072 => Gls Tokens per Sec:     2192 || Batch Translation Loss:   0.262861 => Txt Tokens per Sec:     5963 || Lr: 0.000100
2024-02-03 06:04:33,456 Epoch 136: Total Training Recognition Loss 0.85  Total Training Translation Loss 15.98 
2024-02-03 06:04:33,457 EPOCH 137
2024-02-03 06:04:38,938 Epoch 137: Total Training Recognition Loss 0.84  Total Training Translation Loss 17.59 
2024-02-03 06:04:38,938 EPOCH 138
2024-02-03 06:04:40,501 [Epoch: 138 Step: 00009200] Batch Recognition Loss:   0.009718 => Gls Tokens per Sec:     2151 || Batch Translation Loss:   0.232488 => Txt Tokens per Sec:     5795 || Lr: 0.000100
2024-02-03 06:04:44,099 Epoch 138: Total Training Recognition Loss 0.94  Total Training Translation Loss 21.94 
2024-02-03 06:04:44,099 EPOCH 139
2024-02-03 06:04:48,201 [Epoch: 139 Step: 00009300] Batch Recognition Loss:   0.014906 => Gls Tokens per Sec:     2086 || Batch Translation Loss:   0.236507 => Txt Tokens per Sec:     5821 || Lr: 0.000100
2024-02-03 06:04:49,269 Epoch 139: Total Training Recognition Loss 1.14  Total Training Translation Loss 21.17 
2024-02-03 06:04:49,269 EPOCH 140
2024-02-03 06:04:54,851 Epoch 140: Total Training Recognition Loss 0.92  Total Training Translation Loss 21.94 
2024-02-03 06:04:54,852 EPOCH 141
2024-02-03 06:04:56,453 [Epoch: 141 Step: 00009400] Batch Recognition Loss:   0.015438 => Gls Tokens per Sec:     2000 || Batch Translation Loss:   0.433350 => Txt Tokens per Sec:     5916 || Lr: 0.000100
2024-02-03 06:05:00,235 Epoch 141: Total Training Recognition Loss 1.03  Total Training Translation Loss 22.27 
2024-02-03 06:05:00,236 EPOCH 142
2024-02-03 06:05:04,276 [Epoch: 142 Step: 00009500] Batch Recognition Loss:   0.047554 => Gls Tokens per Sec:     2077 || Batch Translation Loss:   0.497776 => Txt Tokens per Sec:     5853 || Lr: 0.000100
2024-02-03 06:05:05,301 Epoch 142: Total Training Recognition Loss 1.15  Total Training Translation Loss 25.38 
2024-02-03 06:05:05,301 EPOCH 143
2024-02-03 06:05:10,509 Epoch 143: Total Training Recognition Loss 0.88  Total Training Translation Loss 21.42 
2024-02-03 06:05:10,510 EPOCH 144
2024-02-03 06:05:12,135 [Epoch: 144 Step: 00009600] Batch Recognition Loss:   0.008728 => Gls Tokens per Sec:     1816 || Batch Translation Loss:   0.262740 => Txt Tokens per Sec:     5403 || Lr: 0.000100
2024-02-03 06:05:15,463 Epoch 144: Total Training Recognition Loss 0.82  Total Training Translation Loss 19.55 
2024-02-03 06:05:15,463 EPOCH 145
2024-02-03 06:05:19,726 [Epoch: 145 Step: 00009700] Batch Recognition Loss:   0.004647 => Gls Tokens per Sec:     1931 || Batch Translation Loss:   0.167166 => Txt Tokens per Sec:     5421 || Lr: 0.000100
2024-02-03 06:05:20,805 Epoch 145: Total Training Recognition Loss 0.75  Total Training Translation Loss 16.75 
2024-02-03 06:05:20,806 EPOCH 146
2024-02-03 06:05:25,648 Epoch 146: Total Training Recognition Loss 0.84  Total Training Translation Loss 14.87 
2024-02-03 06:05:25,649 EPOCH 147
2024-02-03 06:05:27,183 [Epoch: 147 Step: 00009800] Batch Recognition Loss:   0.033307 => Gls Tokens per Sec:     1879 || Batch Translation Loss:   0.302799 => Txt Tokens per Sec:     4935 || Lr: 0.000100
2024-02-03 06:05:31,337 Epoch 147: Total Training Recognition Loss 0.72  Total Training Translation Loss 19.18 
2024-02-03 06:05:31,338 EPOCH 148
2024-02-03 06:05:34,786 [Epoch: 148 Step: 00009900] Batch Recognition Loss:   0.003442 => Gls Tokens per Sec:     2367 || Batch Translation Loss:   0.248567 => Txt Tokens per Sec:     6542 || Lr: 0.000100
2024-02-03 06:05:36,395 Epoch 148: Total Training Recognition Loss 0.92  Total Training Translation Loss 20.42 
2024-02-03 06:05:36,395 EPOCH 149
2024-02-03 06:05:41,786 Epoch 149: Total Training Recognition Loss 0.94  Total Training Translation Loss 18.08 
2024-02-03 06:05:41,786 EPOCH 150
2024-02-03 06:05:43,133 [Epoch: 150 Step: 00010000] Batch Recognition Loss:   0.002414 => Gls Tokens per Sec:     2022 || Batch Translation Loss:   0.214243 => Txt Tokens per Sec:     5590 || Lr: 0.000100
2024-02-03 06:05:51,973 Validation result at epoch 150, step    10000: duration: 8.8401s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 5.82688	Translation Loss: 85106.18750	PPL: 6901.16895
	Eval Metric: BLEU
	WER 4.60	(DEL: 0.14,	INS: 0.00,	SUB: 4.46)
	BLEU-4 0.66	(BLEU-1: 11.12,	BLEU-2: 3.19,	BLEU-3: 1.29,	BLEU-4: 0.66)
	CHRF 17.15	ROUGE 9.19
2024-02-03 06:05:51,974 Logging Recognition and Translation Outputs
2024-02-03 06:05:51,974 ========================================================================================================================
2024-02-03 06:05:51,974 Logging Sequence: 78_43.00
2024-02-03 06:05:51,975 	Gloss Reference :	A B+C+D+E
2024-02-03 06:05:51,975 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:05:51,975 	Gloss Alignment :	         
2024-02-03 06:05:51,975 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:05:51,976 	Text Reference  :	however something happened which made    the  team very happy
2024-02-03 06:05:51,976 	Text Hypothesis :	******* ********* i        have  retired from all  high court
2024-02-03 06:05:51,976 	Text Alignment  :	D       D         S        S     S       S    S    S    S    
2024-02-03 06:05:51,976 ========================================================================================================================
2024-02-03 06:05:51,976 Logging Sequence: 98_97.00
2024-02-03 06:05:51,976 	Gloss Reference :	A B+C+D+E
2024-02-03 06:05:51,977 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:05:51,977 	Gloss Alignment :	         
2024-02-03 06:05:51,977 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:05:51,978 	Text Reference  :	** the teams    in the     series were india legends england legends sri lanka     legends
2024-02-03 06:05:51,978 	Text Hypothesis :	on the incident in england let    me   tell  you     earlier that    is  completly wrong  
2024-02-03 06:05:51,979 	Text Alignment  :	I      S           S       S      S    S     S       S       S       S   S         S      
2024-02-03 06:05:51,979 ========================================================================================================================
2024-02-03 06:05:51,979 Logging Sequence: 143_11.00
2024-02-03 06:05:51,979 	Gloss Reference :	A B+C+D+E
2024-02-03 06:05:51,979 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:05:51,980 	Gloss Alignment :	         
2024-02-03 06:05:51,980 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:05:51,981 	Text Reference  :	ronaldo has also become the first person to have 500    million followers on instagram he   is  the most loved footballer
2024-02-03 06:05:51,981 	Text Hypothesis :	******* *** **** ****** the ***** ****** ** fine amount was     held      in a         huge fan but that was   injured   
2024-02-03 06:05:51,982 	Text Alignment  :	D       D   D    D          D     D      D  S    S      S       S         S  S         S    S   S   S    S     S         
2024-02-03 06:05:51,982 ========================================================================================================================
2024-02-03 06:05:51,982 Logging Sequence: 179_386.00
2024-02-03 06:05:51,982 	Gloss Reference :	A B+C+D+E
2024-02-03 06:05:51,982 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:05:51,982 	Gloss Alignment :	         
2024-02-03 06:05:51,982 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:05:51,984 	Text Reference  :	and the federation or   sai  people  are their servants to pick      up    their passport
2024-02-03 06:05:51,984 	Text Hypothesis :	*** *** ********** many days whether she might have     a  different floor of    ipl     
2024-02-03 06:05:51,984 	Text Alignment  :	D   D   D          S    S    S       S   S     S        S  S         S     S     S       
2024-02-03 06:05:51,984 ========================================================================================================================
2024-02-03 06:05:51,984 Logging Sequence: 77_60.00
2024-02-03 06:05:51,984 	Gloss Reference :	A B+C+D+E
2024-02-03 06:05:51,985 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:05:51,985 	Gloss Alignment :	         
2024-02-03 06:05:51,985 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:05:51,985 	Text Reference  :	*** *** he    remained not out
2024-02-03 06:05:51,985 	Text Hypothesis :	she was happy to       see him
2024-02-03 06:05:51,986 	Text Alignment  :	I   I   S     S        S   S  
2024-02-03 06:05:51,986 ========================================================================================================================
2024-02-03 06:05:56,177 Epoch 150: Total Training Recognition Loss 0.81  Total Training Translation Loss 16.73 
2024-02-03 06:05:56,178 EPOCH 151
2024-02-03 06:05:59,801 [Epoch: 151 Step: 00010100] Batch Recognition Loss:   0.017625 => Gls Tokens per Sec:     2184 || Batch Translation Loss:   0.277967 => Txt Tokens per Sec:     6108 || Lr: 0.000100
2024-02-03 06:06:01,337 Epoch 151: Total Training Recognition Loss 0.81  Total Training Translation Loss 14.97 
2024-02-03 06:06:01,338 EPOCH 152
2024-02-03 06:06:07,206 Epoch 152: Total Training Recognition Loss 0.81  Total Training Translation Loss 19.36 
2024-02-03 06:06:07,206 EPOCH 153
2024-02-03 06:06:08,378 [Epoch: 153 Step: 00010200] Batch Recognition Loss:   0.003760 => Gls Tokens per Sec:     2186 || Batch Translation Loss:   0.127087 => Txt Tokens per Sec:     5961 || Lr: 0.000100
2024-02-03 06:06:12,795 Epoch 153: Total Training Recognition Loss 0.80  Total Training Translation Loss 16.34 
2024-02-03 06:06:12,795 EPOCH 154
2024-02-03 06:06:16,839 [Epoch: 154 Step: 00010300] Batch Recognition Loss:   0.001556 => Gls Tokens per Sec:     1917 || Batch Translation Loss:   0.090924 => Txt Tokens per Sec:     5375 || Lr: 0.000100
2024-02-03 06:06:18,226 Epoch 154: Total Training Recognition Loss 0.81  Total Training Translation Loss 13.17 
2024-02-03 06:06:18,227 EPOCH 155
2024-02-03 06:06:23,690 Epoch 155: Total Training Recognition Loss 0.83  Total Training Translation Loss 12.01 
2024-02-03 06:06:23,691 EPOCH 156
2024-02-03 06:06:24,824 [Epoch: 156 Step: 00010400] Batch Recognition Loss:   0.007581 => Gls Tokens per Sec:     2120 || Batch Translation Loss:   0.161268 => Txt Tokens per Sec:     6329 || Lr: 0.000100
2024-02-03 06:06:28,271 Epoch 156: Total Training Recognition Loss 0.75  Total Training Translation Loss 10.17 
2024-02-03 06:06:28,271 EPOCH 157
2024-02-03 06:06:32,296 [Epoch: 157 Step: 00010500] Batch Recognition Loss:   0.004461 => Gls Tokens per Sec:     1886 || Batch Translation Loss:   0.200919 => Txt Tokens per Sec:     5214 || Lr: 0.000100
2024-02-03 06:06:33,802 Epoch 157: Total Training Recognition Loss 0.70  Total Training Translation Loss 12.86 
2024-02-03 06:06:33,802 EPOCH 158
2024-02-03 06:06:38,923 Epoch 158: Total Training Recognition Loss 0.78  Total Training Translation Loss 10.67 
2024-02-03 06:06:38,923 EPOCH 159
2024-02-03 06:06:39,998 [Epoch: 159 Step: 00010600] Batch Recognition Loss:   0.003443 => Gls Tokens per Sec:     2085 || Batch Translation Loss:   0.183384 => Txt Tokens per Sec:     5713 || Lr: 0.000100
2024-02-03 06:06:44,156 Epoch 159: Total Training Recognition Loss 0.64  Total Training Translation Loss 14.08 
2024-02-03 06:06:44,156 EPOCH 160
2024-02-03 06:06:47,330 [Epoch: 160 Step: 00010700] Batch Recognition Loss:   0.015064 => Gls Tokens per Sec:     2371 || Batch Translation Loss:   0.276495 => Txt Tokens per Sec:     6423 || Lr: 0.000100
2024-02-03 06:06:49,201 Epoch 160: Total Training Recognition Loss 0.74  Total Training Translation Loss 13.99 
2024-02-03 06:06:49,201 EPOCH 161
2024-02-03 06:06:54,404 Epoch 161: Total Training Recognition Loss 0.73  Total Training Translation Loss 17.52 
2024-02-03 06:06:54,404 EPOCH 162
2024-02-03 06:06:55,543 [Epoch: 162 Step: 00010800] Batch Recognition Loss:   0.016096 => Gls Tokens per Sec:     1748 || Batch Translation Loss:   0.205587 => Txt Tokens per Sec:     5010 || Lr: 0.000100
2024-02-03 06:06:59,563 Epoch 162: Total Training Recognition Loss 0.75  Total Training Translation Loss 23.53 
2024-02-03 06:06:59,563 EPOCH 163
2024-02-03 06:07:03,100 [Epoch: 163 Step: 00010900] Batch Recognition Loss:   0.021036 => Gls Tokens per Sec:     2056 || Batch Translation Loss:   0.356261 => Txt Tokens per Sec:     5673 || Lr: 0.000100
2024-02-03 06:07:04,841 Epoch 163: Total Training Recognition Loss 0.75  Total Training Translation Loss 22.59 
2024-02-03 06:07:04,842 EPOCH 164
2024-02-03 06:07:10,475 Epoch 164: Total Training Recognition Loss 0.66  Total Training Translation Loss 20.33 
2024-02-03 06:07:10,475 EPOCH 165
2024-02-03 06:07:11,269 [Epoch: 165 Step: 00011000] Batch Recognition Loss:   0.024381 => Gls Tokens per Sec:     2421 || Batch Translation Loss:   1.206696 => Txt Tokens per Sec:     6681 || Lr: 0.000100
2024-02-03 06:07:15,600 Epoch 165: Total Training Recognition Loss 0.78  Total Training Translation Loss 19.48 
2024-02-03 06:07:15,601 EPOCH 166
2024-02-03 06:07:18,798 [Epoch: 166 Step: 00011100] Batch Recognition Loss:   0.005724 => Gls Tokens per Sec:     2253 || Batch Translation Loss:   0.237205 => Txt Tokens per Sec:     6241 || Lr: 0.000100
2024-02-03 06:07:20,674 Epoch 166: Total Training Recognition Loss 0.81  Total Training Translation Loss 14.70 
2024-02-03 06:07:20,674 EPOCH 167
2024-02-03 06:07:26,052 Epoch 167: Total Training Recognition Loss 0.57  Total Training Translation Loss 11.83 
2024-02-03 06:07:26,053 EPOCH 168
2024-02-03 06:07:26,781 [Epoch: 168 Step: 00011200] Batch Recognition Loss:   0.003781 => Gls Tokens per Sec:     2423 || Batch Translation Loss:   0.065867 => Txt Tokens per Sec:     6462 || Lr: 0.000100
2024-02-03 06:07:31,134 Epoch 168: Total Training Recognition Loss 0.63  Total Training Translation Loss 12.62 
2024-02-03 06:07:31,134 EPOCH 169
2024-02-03 06:07:34,869 [Epoch: 169 Step: 00011300] Batch Recognition Loss:   0.004846 => Gls Tokens per Sec:     1861 || Batch Translation Loss:   0.232061 => Txt Tokens per Sec:     5185 || Lr: 0.000100
2024-02-03 06:07:36,725 Epoch 169: Total Training Recognition Loss 0.61  Total Training Translation Loss 10.98 
2024-02-03 06:07:36,725 EPOCH 170
2024-02-03 06:07:41,420 Epoch 170: Total Training Recognition Loss 0.64  Total Training Translation Loss 11.49 
2024-02-03 06:07:41,420 EPOCH 171
2024-02-03 06:07:42,138 [Epoch: 171 Step: 00011400] Batch Recognition Loss:   0.012241 => Gls Tokens per Sec:     2232 || Batch Translation Loss:   0.231552 => Txt Tokens per Sec:     6021 || Lr: 0.000100
2024-02-03 06:07:46,506 Epoch 171: Total Training Recognition Loss 0.57  Total Training Translation Loss 13.13 
2024-02-03 06:07:46,507 EPOCH 172
2024-02-03 06:07:50,098 [Epoch: 172 Step: 00011500] Batch Recognition Loss:   0.002241 => Gls Tokens per Sec:     1892 || Batch Translation Loss:   0.238991 => Txt Tokens per Sec:     5197 || Lr: 0.000100
2024-02-03 06:07:52,001 Epoch 172: Total Training Recognition Loss 0.60  Total Training Translation Loss 14.66 
2024-02-03 06:07:52,001 EPOCH 173
2024-02-03 06:07:57,388 Epoch 173: Total Training Recognition Loss 0.66  Total Training Translation Loss 16.04 
2024-02-03 06:07:57,388 EPOCH 174
2024-02-03 06:07:58,229 [Epoch: 174 Step: 00011600] Batch Recognition Loss:   0.009716 => Gls Tokens per Sec:     1716 || Batch Translation Loss:   0.208708 => Txt Tokens per Sec:     5088 || Lr: 0.000100
2024-02-03 06:08:03,045 Epoch 174: Total Training Recognition Loss 0.62  Total Training Translation Loss 13.85 
2024-02-03 06:08:03,046 EPOCH 175
2024-02-03 06:08:06,718 [Epoch: 175 Step: 00011700] Batch Recognition Loss:   0.001212 => Gls Tokens per Sec:     1806 || Batch Translation Loss:   0.315953 => Txt Tokens per Sec:     5136 || Lr: 0.000100
2024-02-03 06:08:08,613 Epoch 175: Total Training Recognition Loss 0.79  Total Training Translation Loss 16.17 
2024-02-03 06:08:08,614 EPOCH 176
2024-02-03 06:08:13,613 Epoch 176: Total Training Recognition Loss 0.66  Total Training Translation Loss 19.58 
2024-02-03 06:08:13,613 EPOCH 177
2024-02-03 06:08:14,290 [Epoch: 177 Step: 00011800] Batch Recognition Loss:   0.011317 => Gls Tokens per Sec:     1896 || Batch Translation Loss:   0.191679 => Txt Tokens per Sec:     5975 || Lr: 0.000100
2024-02-03 06:08:18,972 Epoch 177: Total Training Recognition Loss 0.65  Total Training Translation Loss 16.57 
2024-02-03 06:08:18,972 EPOCH 178
2024-02-03 06:08:21,968 [Epoch: 178 Step: 00011900] Batch Recognition Loss:   0.004269 => Gls Tokens per Sec:     2161 || Batch Translation Loss:   0.098327 => Txt Tokens per Sec:     6095 || Lr: 0.000100
2024-02-03 06:08:24,063 Epoch 178: Total Training Recognition Loss 0.55  Total Training Translation Loss 12.73 
2024-02-03 06:08:24,064 EPOCH 179
2024-02-03 06:08:29,362 Epoch 179: Total Training Recognition Loss 0.60  Total Training Translation Loss 11.20 
2024-02-03 06:08:29,363 EPOCH 180
2024-02-03 06:08:30,004 [Epoch: 180 Step: 00012000] Batch Recognition Loss:   0.008966 => Gls Tokens per Sec:     1750 || Batch Translation Loss:   0.465456 => Txt Tokens per Sec:     5031 || Lr: 0.000100
2024-02-03 06:08:38,404 Validation result at epoch 180, step    12000: duration: 8.4006s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 6.38038	Translation Loss: 85788.80469	PPL: 7408.21729
	Eval Metric: BLEU
	WER 4.46	(DEL: 0.07,	INS: 0.00,	SUB: 4.38)
	BLEU-4 0.67	(BLEU-1: 9.66,	BLEU-2: 2.77,	BLEU-3: 1.20,	BLEU-4: 0.67)
	CHRF 17.06	ROUGE 8.24
2024-02-03 06:08:38,405 Logging Recognition and Translation Outputs
2024-02-03 06:08:38,405 ========================================================================================================================
2024-02-03 06:08:38,405 Logging Sequence: 180_138.00
2024-02-03 06:08:38,405 	Gloss Reference :	A B+C+D+E        
2024-02-03 06:08:38,406 	Gloss Hypothesis:	A B+C+D+E+B+C+B+E
2024-02-03 06:08:38,406 	Gloss Alignment :	  S              
2024-02-03 06:08:38,406 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:08:38,408 	Text Reference  :	** ioa president p  t   usha      constituted a seven-member panel which included world champions from various sports to     inquire into the allegations
2024-02-03 06:08:38,408 	Text Hypothesis :	on 4th may       at the wrestlers wanted      a ************ ***** ***** ******** great leader    when she     got    living in      bars and cafes      
2024-02-03 06:08:38,408 	Text Alignment  :	I  S   S         S  S   S         S             D            D     D     D        S     S         S    S       S      S      S       S    S   S          
2024-02-03 06:08:38,409 ========================================================================================================================
2024-02-03 06:08:38,409 Logging Sequence: 126_99.00
2024-02-03 06:08:38,409 	Gloss Reference :	A B+C+D+E
2024-02-03 06:08:38,409 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:08:38,409 	Gloss Alignment :	         
2024-02-03 06:08:38,409 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:08:38,410 	Text Reference  :	he dedicated the medal to         sprinter milkha singh   
2024-02-03 06:08:38,410 	Text Hypothesis :	** neeraj    is  very  particular about    his    wrestler
2024-02-03 06:08:38,410 	Text Alignment  :	D  S         S   S     S          S        S      S       
2024-02-03 06:08:38,410 ========================================================================================================================
2024-02-03 06:08:38,410 Logging Sequence: 90_146.00
2024-02-03 06:08:38,410 	Gloss Reference :	A B+C+D+E
2024-02-03 06:08:38,410 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:08:38,411 	Gloss Alignment :	         
2024-02-03 06:08:38,411 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:08:38,412 	Text Reference  :	**** similarly natasa  and ********** ***** ******** ** hardik both decided to  renew    their vows   
2024-02-03 06:08:38,412 	Text Hypothesis :	many people    slammed and criticized their decision by saying that india   and pakistan are   enemies
2024-02-03 06:08:38,412 	Text Alignment  :	I    S         S           I          I     I        I  S      S    S       S   S        S     S      
2024-02-03 06:08:38,412 ========================================================================================================================
2024-02-03 06:08:38,412 Logging Sequence: 79_198.00
2024-02-03 06:08:38,412 	Gloss Reference :	A B+C+D+E
2024-02-03 06:08:38,412 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:08:38,413 	Gloss Alignment :	         
2024-02-03 06:08:38,413 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:08:38,414 	Text Reference  :	****** *** **** ** will try to reschedule the     match before the finals   now  hope the team  tests negative
2024-02-03 06:08:38,415 	Text Hypothesis :	people say that we may  get a  true       glimpse of    vamika in  february 2022 when she turns 1     year    
2024-02-03 06:08:38,415 	Text Alignment  :	I      I   I    I  S    S   S  S          S       S     S      S   S        S    S    S   S     S     S       
2024-02-03 06:08:38,415 ========================================================================================================================
2024-02-03 06:08:38,415 Logging Sequence: 138_182.00
2024-02-03 06:08:38,415 	Gloss Reference :	A B+C+D+E  
2024-02-03 06:08:38,415 	Gloss Hypothesis:	A B+C+D+E+B
2024-02-03 06:08:38,415 	Gloss Alignment :	  S        
2024-02-03 06:08:38,415 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:08:38,416 	Text Reference  :	**** **** there is       a          mural of        marcus rashford's face  in      machester
2024-02-03 06:08:38,416 	Text Hypothesis :	they were seen  damaging spectators and   continues to     the        first penalty shoot-out
2024-02-03 06:08:38,417 	Text Alignment  :	I    I    S     S        S          S     S         S      S          S     S       S        
2024-02-03 06:08:38,417 ========================================================================================================================
2024-02-03 06:08:43,494 Epoch 180: Total Training Recognition Loss 0.61  Total Training Translation Loss 12.88 
2024-02-03 06:08:43,495 EPOCH 181
2024-02-03 06:08:46,442 [Epoch: 181 Step: 00012100] Batch Recognition Loss:   0.047337 => Gls Tokens per Sec:     2172 || Batch Translation Loss:   0.185808 => Txt Tokens per Sec:     5938 || Lr: 0.000100
2024-02-03 06:08:48,526 Epoch 181: Total Training Recognition Loss 0.62  Total Training Translation Loss 11.49 
2024-02-03 06:08:48,527 EPOCH 182
2024-02-03 06:08:53,866 Epoch 182: Total Training Recognition Loss 0.54  Total Training Translation Loss 13.96 
2024-02-03 06:08:53,866 EPOCH 183
2024-02-03 06:08:54,448 [Epoch: 183 Step: 00012200] Batch Recognition Loss:   0.011874 => Gls Tokens per Sec:     1497 || Batch Translation Loss:   0.889669 => Txt Tokens per Sec:     4628 || Lr: 0.000100
2024-02-03 06:08:58,484 Epoch 183: Total Training Recognition Loss 0.58  Total Training Translation Loss 16.75 
2024-02-03 06:08:58,484 EPOCH 184
2024-02-03 06:09:01,470 [Epoch: 184 Step: 00012300] Batch Recognition Loss:   0.017559 => Gls Tokens per Sec:     2060 || Batch Translation Loss:   0.144247 => Txt Tokens per Sec:     5950 || Lr: 0.000100
2024-02-03 06:09:03,265 Epoch 184: Total Training Recognition Loss 0.73  Total Training Translation Loss 14.40 
2024-02-03 06:09:03,266 EPOCH 185
2024-02-03 06:09:08,467 Epoch 185: Total Training Recognition Loss 0.75  Total Training Translation Loss 15.52 
2024-02-03 06:09:08,467 EPOCH 186
2024-02-03 06:09:08,833 [Epoch: 186 Step: 00012400] Batch Recognition Loss:   0.005289 => Gls Tokens per Sec:     2192 || Batch Translation Loss:   0.088485 => Txt Tokens per Sec:     6438 || Lr: 0.000100
2024-02-03 06:09:14,110 Epoch 186: Total Training Recognition Loss 0.77  Total Training Translation Loss 13.28 
2024-02-03 06:09:14,111 EPOCH 187
2024-02-03 06:09:17,212 [Epoch: 187 Step: 00012500] Batch Recognition Loss:   0.001591 => Gls Tokens per Sec:     1932 || Batch Translation Loss:   0.137430 => Txt Tokens per Sec:     5305 || Lr: 0.000100
2024-02-03 06:09:19,399 Epoch 187: Total Training Recognition Loss 0.74  Total Training Translation Loss 14.94 
2024-02-03 06:09:19,400 EPOCH 188
2024-02-03 06:09:24,640 Epoch 188: Total Training Recognition Loss 0.64  Total Training Translation Loss 10.25 
2024-02-03 06:09:24,640 EPOCH 189
2024-02-03 06:09:24,946 [Epoch: 189 Step: 00012600] Batch Recognition Loss:   0.007247 => Gls Tokens per Sec:     2098 || Batch Translation Loss:   0.203314 => Txt Tokens per Sec:     5941 || Lr: 0.000100
2024-02-03 06:09:30,139 Epoch 189: Total Training Recognition Loss 0.53  Total Training Translation Loss 9.53 
2024-02-03 06:09:30,140 EPOCH 190
2024-02-03 06:09:32,957 [Epoch: 190 Step: 00012700] Batch Recognition Loss:   0.005032 => Gls Tokens per Sec:     2103 || Batch Translation Loss:   0.300140 => Txt Tokens per Sec:     5790 || Lr: 0.000100
2024-02-03 06:09:35,536 Epoch 190: Total Training Recognition Loss 0.53  Total Training Translation Loss 9.37 
2024-02-03 06:09:35,536 EPOCH 191
2024-02-03 06:09:40,986 Epoch 191: Total Training Recognition Loss 0.55  Total Training Translation Loss 11.69 
2024-02-03 06:09:40,986 EPOCH 192
2024-02-03 06:09:41,333 [Epoch: 192 Step: 00012800] Batch Recognition Loss:   0.004866 => Gls Tokens per Sec:     1390 || Batch Translation Loss:   0.172557 => Txt Tokens per Sec:     4615 || Lr: 0.000100
2024-02-03 06:09:46,169 Epoch 192: Total Training Recognition Loss 0.42  Total Training Translation Loss 11.25 
2024-02-03 06:09:46,170 EPOCH 193
2024-02-03 06:09:49,085 [Epoch: 193 Step: 00012900] Batch Recognition Loss:   0.006487 => Gls Tokens per Sec:     1977 || Batch Translation Loss:   0.376419 => Txt Tokens per Sec:     5366 || Lr: 0.000100
2024-02-03 06:09:51,728 Epoch 193: Total Training Recognition Loss 0.46  Total Training Translation Loss 12.83 
2024-02-03 06:09:51,728 EPOCH 194
2024-02-03 06:09:56,678 Epoch 194: Total Training Recognition Loss 0.58  Total Training Translation Loss 13.70 
2024-02-03 06:09:56,679 EPOCH 195
2024-02-03 06:09:56,915 [Epoch: 195 Step: 00013000] Batch Recognition Loss:   0.024635 => Gls Tokens per Sec:     1362 || Batch Translation Loss:   0.217820 => Txt Tokens per Sec:     4421 || Lr: 0.000100
2024-02-03 06:10:02,072 Epoch 195: Total Training Recognition Loss 0.56  Total Training Translation Loss 11.78 
2024-02-03 06:10:02,072 EPOCH 196
2024-02-03 06:10:04,943 [Epoch: 196 Step: 00013100] Batch Recognition Loss:   0.004730 => Gls Tokens per Sec:     1919 || Batch Translation Loss:   0.184157 => Txt Tokens per Sec:     5243 || Lr: 0.000100
2024-02-03 06:10:07,587 Epoch 196: Total Training Recognition Loss 0.56  Total Training Translation Loss 11.08 
2024-02-03 06:10:07,588 EPOCH 197
2024-02-03 06:10:12,709 Epoch 197: Total Training Recognition Loss 0.52  Total Training Translation Loss 11.35 
2024-02-03 06:10:12,709 EPOCH 198
2024-02-03 06:10:12,771 [Epoch: 198 Step: 00013200] Batch Recognition Loss:   0.003008 => Gls Tokens per Sec:     2667 || Batch Translation Loss:   0.189639 => Txt Tokens per Sec:     6500 || Lr: 0.000100
2024-02-03 06:10:17,613 Epoch 198: Total Training Recognition Loss 0.47  Total Training Translation Loss 15.25 
2024-02-03 06:10:17,613 EPOCH 199
2024-02-03 06:10:20,445 [Epoch: 199 Step: 00013300] Batch Recognition Loss:   0.014426 => Gls Tokens per Sec:     1921 || Batch Translation Loss:   0.131800 => Txt Tokens per Sec:     5347 || Lr: 0.000100
2024-02-03 06:10:23,026 Epoch 199: Total Training Recognition Loss 0.78  Total Training Translation Loss 15.58 
2024-02-03 06:10:23,027 EPOCH 200
2024-02-03 06:10:28,304 [Epoch: 200 Step: 00013400] Batch Recognition Loss:   0.003037 => Gls Tokens per Sec:     2015 || Batch Translation Loss:   0.138963 => Txt Tokens per Sec:     5603 || Lr: 0.000100
2024-02-03 06:10:28,304 Epoch 200: Total Training Recognition Loss 0.71  Total Training Translation Loss 11.41 
2024-02-03 06:10:28,304 EPOCH 201
2024-02-03 06:10:33,297 Epoch 201: Total Training Recognition Loss 0.67  Total Training Translation Loss 10.47 
2024-02-03 06:10:33,297 EPOCH 202
2024-02-03 06:10:36,131 [Epoch: 202 Step: 00013500] Batch Recognition Loss:   0.005721 => Gls Tokens per Sec:     1832 || Batch Translation Loss:   0.150283 => Txt Tokens per Sec:     5183 || Lr: 0.000100
2024-02-03 06:10:38,792 Epoch 202: Total Training Recognition Loss 0.60  Total Training Translation Loss 11.79 
2024-02-03 06:10:38,793 EPOCH 203
2024-02-03 06:10:43,968 [Epoch: 203 Step: 00013600] Batch Recognition Loss:   0.004178 => Gls Tokens per Sec:     2024 || Batch Translation Loss:   0.361580 => Txt Tokens per Sec:     5607 || Lr: 0.000100
2024-02-03 06:10:44,125 Epoch 203: Total Training Recognition Loss 0.62  Total Training Translation Loss 18.91 
2024-02-03 06:10:44,125 EPOCH 204
2024-02-03 06:10:49,439 Epoch 204: Total Training Recognition Loss 0.77  Total Training Translation Loss 19.18 
2024-02-03 06:10:49,439 EPOCH 205
2024-02-03 06:10:51,883 [Epoch: 205 Step: 00013700] Batch Recognition Loss:   0.002825 => Gls Tokens per Sec:     2096 || Batch Translation Loss:   0.158030 => Txt Tokens per Sec:     5816 || Lr: 0.000100
2024-02-03 06:10:54,868 Epoch 205: Total Training Recognition Loss 0.90  Total Training Translation Loss 20.33 
2024-02-03 06:10:54,868 EPOCH 206
2024-02-03 06:10:59,959 [Epoch: 206 Step: 00013800] Batch Recognition Loss:   0.001085 => Gls Tokens per Sec:     2026 || Batch Translation Loss:   0.118274 => Txt Tokens per Sec:     5646 || Lr: 0.000100
2024-02-03 06:11:00,069 Epoch 206: Total Training Recognition Loss 0.62  Total Training Translation Loss 11.11 
2024-02-03 06:11:00,069 EPOCH 207
2024-02-03 06:11:05,394 Epoch 207: Total Training Recognition Loss 0.73  Total Training Translation Loss 8.41 
2024-02-03 06:11:05,394 EPOCH 208
2024-02-03 06:11:07,821 [Epoch: 208 Step: 00013900] Batch Recognition Loss:   0.022834 => Gls Tokens per Sec:     2007 || Batch Translation Loss:   0.120933 => Txt Tokens per Sec:     5786 || Lr: 0.000100
2024-02-03 06:11:10,626 Epoch 208: Total Training Recognition Loss 0.57  Total Training Translation Loss 8.58 
2024-02-03 06:11:10,627 EPOCH 209
2024-02-03 06:11:15,868 [Epoch: 209 Step: 00014000] Batch Recognition Loss:   0.001352 => Gls Tokens per Sec:     1937 || Batch Translation Loss:   0.085460 => Txt Tokens per Sec:     5414 || Lr: 0.000100
2024-02-03 06:11:24,402 Hooray! New best validation result [eval_metric]!
2024-02-03 06:11:24,403 Saving new checkpoint.
2024-02-03 06:11:24,671 Validation result at epoch 209, step    14000: duration: 8.8027s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 6.38447	Translation Loss: 87689.46094	PPL: 9025.00781
	Eval Metric: BLEU
	WER 4.24	(DEL: 0.00,	INS: 0.00,	SUB: 4.24)
	BLEU-4 0.86	(BLEU-1: 10.60,	BLEU-2: 3.53,	BLEU-3: 1.55,	BLEU-4: 0.86)
	CHRF 16.77	ROUGE 8.91
2024-02-03 06:11:24,671 Logging Recognition and Translation Outputs
2024-02-03 06:11:24,672 ========================================================================================================================
2024-02-03 06:11:24,672 Logging Sequence: 123_76.00
2024-02-03 06:11:24,672 	Gloss Reference :	A B+C+D+E
2024-02-03 06:11:24,672 	Gloss Hypothesis:	A B      
2024-02-03 06:11:24,672 	Gloss Alignment :	  S      
2024-02-03 06:11:24,672 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:11:24,673 	Text Reference  :	** here there are  
2024-02-03 06:11:24,673 	Text Hypothesis :	so what was   dhoni
2024-02-03 06:11:24,673 	Text Alignment  :	I  S    S     S    
2024-02-03 06:11:24,673 ========================================================================================================================
2024-02-03 06:11:24,673 Logging Sequence: 87_224.00
2024-02-03 06:11:24,673 	Gloss Reference :	A B+C+D+E
2024-02-03 06:11:24,674 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:11:24,674 	Gloss Alignment :	         
2024-02-03 06:11:24,674 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:11:24,675 	Text Reference  :	so i       lost my        cool and  it was my  natural reaction
2024-02-03 06:11:24,675 	Text Hypothesis :	** gambhir was  surprised that this -  did not india   proud   
2024-02-03 06:11:24,675 	Text Alignment  :	D  S       S    S         S    S    S  S   S   S       S       
2024-02-03 06:11:24,675 ========================================================================================================================
2024-02-03 06:11:24,675 Logging Sequence: 182_20.00
2024-02-03 06:11:24,675 	Gloss Reference :	A B+C+D+E
2024-02-03 06:11:24,676 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:11:24,676 	Gloss Alignment :	         
2024-02-03 06:11:24,676 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:11:24,678 	Text Reference  :	in 2019 june yuvraj shocked the world when   he     announced his international retirement many people were   sad  with the  news       
2024-02-03 06:11:24,678 	Text Hypothesis :	** **** **** ****** ******* *** even  though sunday was       a   holiday       wfi        kept their  office open so   much speculation
2024-02-03 06:11:24,678 	Text Alignment  :	D  D    D    D      D       D   S     S      S      S         S   S             S          S    S      S      S    S    S    S          
2024-02-03 06:11:24,678 ========================================================================================================================
2024-02-03 06:11:24,678 Logging Sequence: 116_133.00
2024-02-03 06:11:24,678 	Gloss Reference :	A B+C+D+E
2024-02-03 06:11:24,678 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:11:24,679 	Gloss Alignment :	         
2024-02-03 06:11:24,679 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:11:24,679 	Text Reference  :	he expressed his sadness in the caption
2024-02-03 06:11:24,679 	Text Hypothesis :	he is        his ******* ** *** video  
2024-02-03 06:11:24,679 	Text Alignment  :	   S             D       D  D   S      
2024-02-03 06:11:24,679 ========================================================================================================================
2024-02-03 06:11:24,680 Logging Sequence: 180_409.00
2024-02-03 06:11:24,680 	Gloss Reference :	A B+C+D+E
2024-02-03 06:11:24,680 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:11:24,680 	Gloss Alignment :	         
2024-02-03 06:11:24,680 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:11:24,682 	Text Reference  :	******* ******* **** ******** cricketer kapil dev       etc     have **** also shown support to the ******** wrestlers through their   tweets
2024-02-03 06:11:24,682 	Text Hypothesis :	several leaders from congress and       other political parties have come out  in    support of the athletes and       are     against singh 
2024-02-03 06:11:24,682 	Text Alignment  :	I       I       I    I        S         S     S         S            I    S    S             S      I        S         S       S       S     
2024-02-03 06:11:24,682 ========================================================================================================================
2024-02-03 06:11:24,873 Epoch 209: Total Training Recognition Loss 0.46  Total Training Translation Loss 8.32 
2024-02-03 06:11:24,873 EPOCH 210
2024-02-03 06:11:30,523 Epoch 210: Total Training Recognition Loss 0.36  Total Training Translation Loss 6.70 
2024-02-03 06:11:30,523 EPOCH 211
2024-02-03 06:11:33,013 [Epoch: 211 Step: 00014100] Batch Recognition Loss:   0.005194 => Gls Tokens per Sec:     1892 || Batch Translation Loss:   0.162727 => Txt Tokens per Sec:     5310 || Lr: 0.000100
2024-02-03 06:11:36,002 Epoch 211: Total Training Recognition Loss 0.42  Total Training Translation Loss 5.85 
2024-02-03 06:11:36,003 EPOCH 212
2024-02-03 06:11:41,070 [Epoch: 212 Step: 00014200] Batch Recognition Loss:   0.001498 => Gls Tokens per Sec:     1972 || Batch Translation Loss:   0.105403 => Txt Tokens per Sec:     5509 || Lr: 0.000100
2024-02-03 06:11:41,339 Epoch 212: Total Training Recognition Loss 0.40  Total Training Translation Loss 6.61 
2024-02-03 06:11:41,339 EPOCH 213
2024-02-03 06:11:46,588 Epoch 213: Total Training Recognition Loss 0.37  Total Training Translation Loss 6.59 
2024-02-03 06:11:46,589 EPOCH 214
2024-02-03 06:11:48,788 [Epoch: 214 Step: 00014300] Batch Recognition Loss:   0.001949 => Gls Tokens per Sec:     2111 || Batch Translation Loss:   0.043524 => Txt Tokens per Sec:     5766 || Lr: 0.000100
2024-02-03 06:11:51,930 Epoch 214: Total Training Recognition Loss 0.53  Total Training Translation Loss 8.17 
2024-02-03 06:11:51,930 EPOCH 215
2024-02-03 06:11:56,418 [Epoch: 215 Step: 00014400] Batch Recognition Loss:   0.010163 => Gls Tokens per Sec:     2191 || Batch Translation Loss:   0.064518 => Txt Tokens per Sec:     6112 || Lr: 0.000100
2024-02-03 06:11:56,772 Epoch 215: Total Training Recognition Loss 0.49  Total Training Translation Loss 10.43 
2024-02-03 06:11:56,772 EPOCH 216
2024-02-03 06:12:02,015 Epoch 216: Total Training Recognition Loss 0.55  Total Training Translation Loss 11.20 
2024-02-03 06:12:02,016 EPOCH 217
2024-02-03 06:12:04,315 [Epoch: 217 Step: 00014500] Batch Recognition Loss:   0.011395 => Gls Tokens per Sec:     1911 || Batch Translation Loss:   0.129430 => Txt Tokens per Sec:     5297 || Lr: 0.000100
2024-02-03 06:12:07,351 Epoch 217: Total Training Recognition Loss 0.50  Total Training Translation Loss 11.56 
2024-02-03 06:12:07,352 EPOCH 218
2024-02-03 06:12:12,300 [Epoch: 218 Step: 00014600] Batch Recognition Loss:   0.002883 => Gls Tokens per Sec:     1954 || Batch Translation Loss:   0.222128 => Txt Tokens per Sec:     5434 || Lr: 0.000100
2024-02-03 06:12:12,690 Epoch 218: Total Training Recognition Loss 0.66  Total Training Translation Loss 13.79 
2024-02-03 06:12:12,690 EPOCH 219
2024-02-03 06:12:18,037 Epoch 219: Total Training Recognition Loss 0.66  Total Training Translation Loss 11.83 
2024-02-03 06:12:18,037 EPOCH 220
2024-02-03 06:12:20,070 [Epoch: 220 Step: 00014700] Batch Recognition Loss:   0.004252 => Gls Tokens per Sec:     2082 || Batch Translation Loss:   0.064498 => Txt Tokens per Sec:     5619 || Lr: 0.000100
2024-02-03 06:12:23,325 Epoch 220: Total Training Recognition Loss 0.68  Total Training Translation Loss 11.11 
2024-02-03 06:12:23,325 EPOCH 221
2024-02-03 06:12:27,883 [Epoch: 221 Step: 00014800] Batch Recognition Loss:   0.005028 => Gls Tokens per Sec:     2088 || Batch Translation Loss:   0.191258 => Txt Tokens per Sec:     5796 || Lr: 0.000100
2024-02-03 06:12:28,431 Epoch 221: Total Training Recognition Loss 0.61  Total Training Translation Loss 10.78 
2024-02-03 06:12:28,432 EPOCH 222
2024-02-03 06:12:33,602 Epoch 222: Total Training Recognition Loss 0.63  Total Training Translation Loss 13.77 
2024-02-03 06:12:33,602 EPOCH 223
2024-02-03 06:12:35,494 [Epoch: 223 Step: 00014900] Batch Recognition Loss:   0.003183 => Gls Tokens per Sec:     2153 || Batch Translation Loss:   0.086655 => Txt Tokens per Sec:     5406 || Lr: 0.000100
2024-02-03 06:12:38,926 Epoch 223: Total Training Recognition Loss 1.25  Total Training Translation Loss 13.47 
2024-02-03 06:12:38,927 EPOCH 224
2024-02-03 06:12:43,350 [Epoch: 224 Step: 00015000] Batch Recognition Loss:   0.045413 => Gls Tokens per Sec:     2115 || Batch Translation Loss:   0.249695 => Txt Tokens per Sec:     5896 || Lr: 0.000100
2024-02-03 06:12:43,952 Epoch 224: Total Training Recognition Loss 5.50  Total Training Translation Loss 14.89 
2024-02-03 06:12:43,952 EPOCH 225
2024-02-03 06:12:49,378 Epoch 225: Total Training Recognition Loss 59.02  Total Training Translation Loss 58.12 
2024-02-03 06:12:49,379 EPOCH 226
2024-02-03 06:12:51,062 [Epoch: 226 Step: 00015100] Batch Recognition Loss:   0.228956 => Gls Tokens per Sec:     2378 || Batch Translation Loss:   0.660422 => Txt Tokens per Sec:     6463 || Lr: 0.000100
2024-02-03 06:12:54,440 Epoch 226: Total Training Recognition Loss 9.96  Total Training Translation Loss 23.32 
2024-02-03 06:12:54,441 EPOCH 227
2024-02-03 06:12:58,905 [Epoch: 227 Step: 00015200] Batch Recognition Loss:   0.032087 => Gls Tokens per Sec:     2059 || Batch Translation Loss:   0.092264 => Txt Tokens per Sec:     5731 || Lr: 0.000100
2024-02-03 06:12:59,669 Epoch 227: Total Training Recognition Loss 1.70  Total Training Translation Loss 10.49 
2024-02-03 06:12:59,670 EPOCH 228
2024-02-03 06:13:05,378 Epoch 228: Total Training Recognition Loss 0.84  Total Training Translation Loss 7.04 
2024-02-03 06:13:05,378 EPOCH 229
2024-02-03 06:13:07,182 [Epoch: 229 Step: 00015300] Batch Recognition Loss:   0.011114 => Gls Tokens per Sec:     2130 || Batch Translation Loss:   0.074508 => Txt Tokens per Sec:     5865 || Lr: 0.000100
2024-02-03 06:13:10,825 Epoch 229: Total Training Recognition Loss 0.66  Total Training Translation Loss 5.56 
2024-02-03 06:13:10,826 EPOCH 230
2024-02-03 06:13:15,572 [Epoch: 230 Step: 00015400] Batch Recognition Loss:   0.007214 => Gls Tokens per Sec:     1903 || Batch Translation Loss:   0.164969 => Txt Tokens per Sec:     5332 || Lr: 0.000100
2024-02-03 06:13:16,247 Epoch 230: Total Training Recognition Loss 0.67  Total Training Translation Loss 4.99 
2024-02-03 06:13:16,247 EPOCH 231
2024-02-03 06:13:21,462 Epoch 231: Total Training Recognition Loss 0.58  Total Training Translation Loss 5.79 
2024-02-03 06:13:21,463 EPOCH 232
2024-02-03 06:13:23,203 [Epoch: 232 Step: 00015500] Batch Recognition Loss:   0.009521 => Gls Tokens per Sec:     2118 || Batch Translation Loss:   0.065673 => Txt Tokens per Sec:     5745 || Lr: 0.000100
2024-02-03 06:13:26,894 Epoch 232: Total Training Recognition Loss 0.50  Total Training Translation Loss 4.49 
2024-02-03 06:13:26,895 EPOCH 233
2024-02-03 06:13:31,654 [Epoch: 233 Step: 00015600] Batch Recognition Loss:   0.001607 => Gls Tokens per Sec:     1864 || Batch Translation Loss:   0.078311 => Txt Tokens per Sec:     5221 || Lr: 0.000100
2024-02-03 06:13:32,519 Epoch 233: Total Training Recognition Loss 0.45  Total Training Translation Loss 3.75 
2024-02-03 06:13:32,519 EPOCH 234
2024-02-03 06:13:37,428 Epoch 234: Total Training Recognition Loss 0.47  Total Training Translation Loss 4.24 
2024-02-03 06:13:37,429 EPOCH 235
2024-02-03 06:13:39,066 [Epoch: 235 Step: 00015700] Batch Recognition Loss:   0.001425 => Gls Tokens per Sec:     2152 || Batch Translation Loss:   0.097096 => Txt Tokens per Sec:     5703 || Lr: 0.000100
2024-02-03 06:13:42,826 Epoch 235: Total Training Recognition Loss 0.46  Total Training Translation Loss 5.28 
2024-02-03 06:13:42,826 EPOCH 236
2024-02-03 06:13:46,509 [Epoch: 236 Step: 00015800] Batch Recognition Loss:   0.025408 => Gls Tokens per Sec:     2365 || Batch Translation Loss:   0.269251 => Txt Tokens per Sec:     6462 || Lr: 0.000100
2024-02-03 06:13:47,440 Epoch 236: Total Training Recognition Loss 0.38  Total Training Translation Loss 7.70 
2024-02-03 06:13:47,441 EPOCH 237
2024-02-03 06:13:52,846 Epoch 237: Total Training Recognition Loss 0.47  Total Training Translation Loss 5.93 
2024-02-03 06:13:52,846 EPOCH 238
2024-02-03 06:13:54,461 [Epoch: 238 Step: 00015900] Batch Recognition Loss:   0.001910 => Gls Tokens per Sec:     2026 || Batch Translation Loss:   0.585323 => Txt Tokens per Sec:     5959 || Lr: 0.000100
2024-02-03 06:13:58,264 Epoch 238: Total Training Recognition Loss 0.31  Total Training Translation Loss 8.36 
2024-02-03 06:13:58,265 EPOCH 239
2024-02-03 06:14:02,364 [Epoch: 239 Step: 00016000] Batch Recognition Loss:   0.002842 => Gls Tokens per Sec:     2108 || Batch Translation Loss:   0.029720 => Txt Tokens per Sec:     5917 || Lr: 0.000100
2024-02-03 06:14:10,804 Hooray! New best validation result [eval_metric]!
2024-02-03 06:14:10,805 Saving new checkpoint.
2024-02-03 06:14:11,078 Validation result at epoch 239, step    16000: duration: 8.7144s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 5.54243	Translation Loss: 88020.81250	PPL: 9341.01270
	Eval Metric: BLEU
	WER 3.96	(DEL: 0.00,	INS: 0.00,	SUB: 3.96)
	BLEU-4 0.95	(BLEU-1: 10.66,	BLEU-2: 3.46,	BLEU-3: 1.65,	BLEU-4: 0.95)
	CHRF 17.21	ROUGE 8.63
2024-02-03 06:14:11,079 Logging Recognition and Translation Outputs
2024-02-03 06:14:11,079 ========================================================================================================================
2024-02-03 06:14:11,079 Logging Sequence: 182_20.00
2024-02-03 06:14:11,079 	Gloss Reference :	A B+C+D+E
2024-02-03 06:14:11,079 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:14:11,079 	Gloss Alignment :	         
2024-02-03 06:14:11,080 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:14:11,081 	Text Reference  :	in 2019 june yuvraj shocked the     world when he     announced his     international retirement many people were sad with the news
2024-02-03 06:14:11,081 	Text Hypothesis :	** god  will decide my      destiny about my   return to        cricket post          retirement **** ****** **** *** **** *** ****
2024-02-03 06:14:11,082 	Text Alignment  :	D  S    S    S      S       S       S     S    S      S         S       S                        D    D      D    D   D    D   D   
2024-02-03 06:14:11,082 ========================================================================================================================
2024-02-03 06:14:11,082 Logging Sequence: 180_409.00
2024-02-03 06:14:11,082 	Gloss Reference :	A B+C+D+E
2024-02-03 06:14:11,082 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:14:11,082 	Gloss Alignment :	         
2024-02-03 06:14:11,082 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:14:11,084 	Text Reference  :	cricketer kapil dev    etc have  also shown support to     the wrestlers through their tweets 
2024-02-03 06:14:11,084 	Text Hypothesis :	********* he    played 18  tests 226  odis  and     denied the eight     more    in    support
2024-02-03 06:14:11,084 	Text Alignment  :	D         S     S      S   S     S    S     S       S          S         S       S     S      
2024-02-03 06:14:11,084 ========================================================================================================================
2024-02-03 06:14:11,084 Logging Sequence: 59_2.00
2024-02-03 06:14:11,085 	Gloss Reference :	A B+C+D+E
2024-02-03 06:14:11,085 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:14:11,085 	Gloss Alignment :	         
2024-02-03 06:14:11,085 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:14:11,086 	Text Reference  :	at   the 2020 tokyo olympics in    japan
2024-02-03 06:14:11,086 	Text Hypothesis :	well let me   tell  you      about them 
2024-02-03 06:14:11,086 	Text Alignment  :	S    S   S    S     S        S     S    
2024-02-03 06:14:11,086 ========================================================================================================================
2024-02-03 06:14:11,086 Logging Sequence: 118_100.00
2024-02-03 06:14:11,086 	Gloss Reference :	A B+C+D+E
2024-02-03 06:14:11,086 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:14:11,087 	Gloss Alignment :	         
2024-02-03 06:14:11,087 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:14:11,087 	Text Reference  :	while the     french were very heartbroken by *** this 
2024-02-03 06:14:11,088 	Text Hypothesis :	***** earlier india  was  just scored      by the match
2024-02-03 06:14:11,088 	Text Alignment  :	D     S       S      S    S    S              I   S    
2024-02-03 06:14:11,088 ========================================================================================================================
2024-02-03 06:14:11,088 Logging Sequence: 69_204.00
2024-02-03 06:14:11,088 	Gloss Reference :	A B+C+D+E
2024-02-03 06:14:11,088 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:14:11,088 	Gloss Alignment :	         
2024-02-03 06:14:11,089 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:14:11,089 	Text Reference  :	however i am hoping to     play     the   next  season'
2024-02-03 06:14:11,089 	Text Hypothesis :	******* * ** ****** bhogle directly asked about dhoni  
2024-02-03 06:14:11,089 	Text Alignment  :	D       D D  D      S      S        S     S     S      
2024-02-03 06:14:11,089 ========================================================================================================================
2024-02-03 06:14:12,086 Epoch 239: Total Training Recognition Loss 0.39  Total Training Translation Loss 7.20 
2024-02-03 06:14:12,086 EPOCH 240
2024-02-03 06:14:17,731 Epoch 240: Total Training Recognition Loss 0.39  Total Training Translation Loss 7.74 
2024-02-03 06:14:17,732 EPOCH 241
2024-02-03 06:14:19,101 [Epoch: 241 Step: 00016100] Batch Recognition Loss:   0.001119 => Gls Tokens per Sec:     2340 || Batch Translation Loss:   0.132841 => Txt Tokens per Sec:     6223 || Lr: 0.000100
2024-02-03 06:14:22,657 Epoch 241: Total Training Recognition Loss 0.35  Total Training Translation Loss 9.04 
2024-02-03 06:14:22,657 EPOCH 242
2024-02-03 06:14:27,118 [Epoch: 242 Step: 00016200] Batch Recognition Loss:   0.009583 => Gls Tokens per Sec:     1882 || Batch Translation Loss:   0.152655 => Txt Tokens per Sec:     5340 || Lr: 0.000100
2024-02-03 06:14:28,166 Epoch 242: Total Training Recognition Loss 0.35  Total Training Translation Loss 10.50 
2024-02-03 06:14:28,166 EPOCH 243
2024-02-03 06:14:33,357 Epoch 243: Total Training Recognition Loss 0.42  Total Training Translation Loss 12.54 
2024-02-03 06:14:33,358 EPOCH 244
2024-02-03 06:14:34,801 [Epoch: 244 Step: 00016300] Batch Recognition Loss:   0.012203 => Gls Tokens per Sec:     2107 || Batch Translation Loss:   0.153699 => Txt Tokens per Sec:     5827 || Lr: 0.000100
2024-02-03 06:14:38,464 Epoch 244: Total Training Recognition Loss 0.40  Total Training Translation Loss 12.53 
2024-02-03 06:14:38,464 EPOCH 245
2024-02-03 06:14:42,584 [Epoch: 245 Step: 00016400] Batch Recognition Loss:   0.003537 => Gls Tokens per Sec:     1998 || Batch Translation Loss:   0.188448 => Txt Tokens per Sec:     5679 || Lr: 0.000100
2024-02-03 06:14:43,690 Epoch 245: Total Training Recognition Loss 0.33  Total Training Translation Loss 11.05 
2024-02-03 06:14:43,691 EPOCH 246
2024-02-03 06:14:48,919 Epoch 246: Total Training Recognition Loss 0.39  Total Training Translation Loss 11.22 
2024-02-03 06:14:48,919 EPOCH 247
2024-02-03 06:14:50,297 [Epoch: 247 Step: 00016500] Batch Recognition Loss:   0.000790 => Gls Tokens per Sec:     2091 || Batch Translation Loss:   0.070626 => Txt Tokens per Sec:     5751 || Lr: 0.000100
2024-02-03 06:14:54,534 Epoch 247: Total Training Recognition Loss 0.36  Total Training Translation Loss 11.96 
2024-02-03 06:14:54,534 EPOCH 248
2024-02-03 06:14:57,949 [Epoch: 248 Step: 00016600] Batch Recognition Loss:   0.006842 => Gls Tokens per Sec:     2365 || Batch Translation Loss:   0.180854 => Txt Tokens per Sec:     6343 || Lr: 0.000100
2024-02-03 06:14:59,384 Epoch 248: Total Training Recognition Loss 0.39  Total Training Translation Loss 11.24 
2024-02-03 06:14:59,385 EPOCH 249
2024-02-03 06:15:04,821 Epoch 249: Total Training Recognition Loss 0.33  Total Training Translation Loss 9.24 
2024-02-03 06:15:04,821 EPOCH 250
2024-02-03 06:15:06,071 [Epoch: 250 Step: 00016700] Batch Recognition Loss:   0.009523 => Gls Tokens per Sec:     2106 || Batch Translation Loss:   0.256489 => Txt Tokens per Sec:     5945 || Lr: 0.000100
2024-02-03 06:15:10,293 Epoch 250: Total Training Recognition Loss 0.40  Total Training Translation Loss 8.27 
2024-02-03 06:15:10,294 EPOCH 251
2024-02-03 06:15:14,111 [Epoch: 251 Step: 00016800] Batch Recognition Loss:   0.022126 => Gls Tokens per Sec:     2072 || Batch Translation Loss:   0.146731 => Txt Tokens per Sec:     5776 || Lr: 0.000100
2024-02-03 06:15:15,310 Epoch 251: Total Training Recognition Loss 0.34  Total Training Translation Loss 9.83 
2024-02-03 06:15:15,310 EPOCH 252
2024-02-03 06:15:20,738 Epoch 252: Total Training Recognition Loss 0.41  Total Training Translation Loss 9.33 
2024-02-03 06:15:20,739 EPOCH 253
2024-02-03 06:15:21,828 [Epoch: 253 Step: 00016900] Batch Recognition Loss:   0.001066 => Gls Tokens per Sec:     2351 || Batch Translation Loss:   0.228317 => Txt Tokens per Sec:     6230 || Lr: 0.000100
2024-02-03 06:15:25,843 Epoch 253: Total Training Recognition Loss 0.39  Total Training Translation Loss 10.11 
2024-02-03 06:15:25,844 EPOCH 254
2024-02-03 06:15:29,340 [Epoch: 254 Step: 00017000] Batch Recognition Loss:   0.005559 => Gls Tokens per Sec:     2219 || Batch Translation Loss:   0.262843 => Txt Tokens per Sec:     5914 || Lr: 0.000100
2024-02-03 06:15:31,109 Epoch 254: Total Training Recognition Loss 0.34  Total Training Translation Loss 9.59 
2024-02-03 06:15:31,109 EPOCH 255
2024-02-03 06:15:36,383 Epoch 255: Total Training Recognition Loss 0.42  Total Training Translation Loss 10.19 
2024-02-03 06:15:36,383 EPOCH 256
2024-02-03 06:15:37,681 [Epoch: 256 Step: 00017100] Batch Recognition Loss:   0.002102 => Gls Tokens per Sec:     1850 || Batch Translation Loss:   0.206163 => Txt Tokens per Sec:     5132 || Lr: 0.000100
2024-02-03 06:15:41,598 Epoch 256: Total Training Recognition Loss 0.41  Total Training Translation Loss 10.89 
2024-02-03 06:15:41,598 EPOCH 257
2024-02-03 06:15:45,415 [Epoch: 257 Step: 00017200] Batch Recognition Loss:   0.001245 => Gls Tokens per Sec:     2013 || Batch Translation Loss:   0.059542 => Txt Tokens per Sec:     5553 || Lr: 0.000100
2024-02-03 06:15:46,899 Epoch 257: Total Training Recognition Loss 0.36  Total Training Translation Loss 7.64 
2024-02-03 06:15:46,900 EPOCH 258
2024-02-03 06:15:51,838 Epoch 258: Total Training Recognition Loss 0.41  Total Training Translation Loss 7.29 
2024-02-03 06:15:51,838 EPOCH 259
2024-02-03 06:15:53,131 [Epoch: 259 Step: 00017300] Batch Recognition Loss:   0.003160 => Gls Tokens per Sec:     1734 || Batch Translation Loss:   0.514049 => Txt Tokens per Sec:     4876 || Lr: 0.000100
2024-02-03 06:15:57,422 Epoch 259: Total Training Recognition Loss 0.30  Total Training Translation Loss 9.29 
2024-02-03 06:15:57,422 EPOCH 260
2024-02-03 06:16:01,149 [Epoch: 260 Step: 00017400] Batch Recognition Loss:   0.000894 => Gls Tokens per Sec:     1994 || Batch Translation Loss:   0.131339 => Txt Tokens per Sec:     5615 || Lr: 0.000100
2024-02-03 06:16:02,678 Epoch 260: Total Training Recognition Loss 0.31  Total Training Translation Loss 9.89 
2024-02-03 06:16:02,678 EPOCH 261
2024-02-03 06:16:07,751 Epoch 261: Total Training Recognition Loss 0.33  Total Training Translation Loss 8.21 
2024-02-03 06:16:07,752 EPOCH 262
2024-02-03 06:16:08,689 [Epoch: 262 Step: 00017500] Batch Recognition Loss:   0.000698 => Gls Tokens per Sec:     2220 || Batch Translation Loss:   0.083183 => Txt Tokens per Sec:     6456 || Lr: 0.000100
2024-02-03 06:16:12,864 Epoch 262: Total Training Recognition Loss 0.33  Total Training Translation Loss 7.46 
2024-02-03 06:16:12,865 EPOCH 263
2024-02-03 06:16:16,670 [Epoch: 263 Step: 00017600] Batch Recognition Loss:   0.001999 => Gls Tokens per Sec:     1935 || Batch Translation Loss:   0.073934 => Txt Tokens per Sec:     5569 || Lr: 0.000100
2024-02-03 06:16:18,039 Epoch 263: Total Training Recognition Loss 0.30  Total Training Translation Loss 6.78 
2024-02-03 06:16:18,040 EPOCH 264
2024-02-03 06:16:23,552 Epoch 264: Total Training Recognition Loss 0.35  Total Training Translation Loss 11.44 
2024-02-03 06:16:23,553 EPOCH 265
2024-02-03 06:16:24,457 [Epoch: 265 Step: 00017700] Batch Recognition Loss:   0.019118 => Gls Tokens per Sec:     2125 || Batch Translation Loss:   0.258357 => Txt Tokens per Sec:     5822 || Lr: 0.000100
2024-02-03 06:16:28,672 Epoch 265: Total Training Recognition Loss 0.25  Total Training Translation Loss 10.73 
2024-02-03 06:16:28,673 EPOCH 266
2024-02-03 06:16:32,329 [Epoch: 266 Step: 00017800] Batch Recognition Loss:   0.000413 => Gls Tokens per Sec:     1945 || Batch Translation Loss:   0.077389 => Txt Tokens per Sec:     5397 || Lr: 0.000100
2024-02-03 06:16:34,036 Epoch 266: Total Training Recognition Loss 0.32  Total Training Translation Loss 9.08 
2024-02-03 06:16:34,036 EPOCH 267
2024-02-03 06:16:39,209 Epoch 267: Total Training Recognition Loss 0.30  Total Training Translation Loss 9.33 
2024-02-03 06:16:39,209 EPOCH 268
2024-02-03 06:16:40,059 [Epoch: 268 Step: 00017900] Batch Recognition Loss:   0.002190 => Gls Tokens per Sec:     2075 || Batch Translation Loss:   0.118349 => Txt Tokens per Sec:     5623 || Lr: 0.000100
2024-02-03 06:16:44,499 Epoch 268: Total Training Recognition Loss 0.32  Total Training Translation Loss 11.22 
2024-02-03 06:16:44,499 EPOCH 269
2024-02-03 06:16:47,928 [Epoch: 269 Step: 00018000] Batch Recognition Loss:   0.002012 => Gls Tokens per Sec:     2028 || Batch Translation Loss:   0.236129 => Txt Tokens per Sec:     5668 || Lr: 0.000100
2024-02-03 06:16:56,733 Validation result at epoch 269, step    18000: duration: 8.8040s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 4.99901	Translation Loss: 88211.61719	PPL: 9527.97266
	Eval Metric: BLEU
	WER 3.96	(DEL: 0.00,	INS: 0.00,	SUB: 3.96)
	BLEU-4 0.92	(BLEU-1: 11.56,	BLEU-2: 3.64,	BLEU-3: 1.68,	BLEU-4: 0.92)
	CHRF 17.43	ROUGE 9.49
2024-02-03 06:16:56,734 Logging Recognition and Translation Outputs
2024-02-03 06:16:56,734 ========================================================================================================================
2024-02-03 06:16:56,735 Logging Sequence: 64_53.00
2024-02-03 06:16:56,735 	Gloss Reference :	A B+C+D+E
2024-02-03 06:16:56,735 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:16:56,735 	Gloss Alignment :	         
2024-02-03 06:16:56,735 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:16:56,736 	Text Reference  :	the bcci and ipl does not want to compromise on     the safety of the players
2024-02-03 06:16:56,736 	Text Hypothesis :	the bcci *** *** **** *** **** ** ********** posted a   lot    of *** rumours
2024-02-03 06:16:56,736 	Text Alignment  :	         D   D   D    D   D    D  D          S      S   S         D   S      
2024-02-03 06:16:56,736 ========================================================================================================================
2024-02-03 06:16:56,737 Logging Sequence: 165_252.00
2024-02-03 06:16:56,737 	Gloss Reference :	A B+C+D+E
2024-02-03 06:16:56,737 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:16:56,737 	Gloss Alignment :	         
2024-02-03 06:16:56,737 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:16:56,738 	Text Reference  :	**** ****** **** *** ** **** *** 4     rahul   dravid also has his own  superstitions
2024-02-03 06:16:56,738 	Text Hypothesis :	anil kumble came out to bowl the match between rcb    and  it  is  very protective   
2024-02-03 06:16:56,738 	Text Alignment  :	I    I      I    I   I  I    I   S     S       S      S    S   S   S    S            
2024-02-03 06:16:56,739 ========================================================================================================================
2024-02-03 06:16:56,739 Logging Sequence: 90_7.00
2024-02-03 06:16:56,739 	Gloss Reference :	A B+C+D+E
2024-02-03 06:16:56,739 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:16:56,739 	Gloss Alignment :	         
2024-02-03 06:16:56,739 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:16:56,740 	Text Reference  :	let me tell you  the exciting story   of     how they were  introduced
2024-02-03 06:16:56,740 	Text Hypothesis :	*** ** **** that is  how      reports played at  the  right time      
2024-02-03 06:16:56,740 	Text Alignment  :	D   D  D    S    S   S        S       S      S   S    S     S         
2024-02-03 06:16:56,741 ========================================================================================================================
2024-02-03 06:16:56,741 Logging Sequence: 137_307.00
2024-02-03 06:16:56,741 	Gloss Reference :	A B+C+D+E
2024-02-03 06:16:56,741 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:16:56,741 	Gloss Alignment :	         
2024-02-03 06:16:56,741 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:16:56,742 	Text Reference  :	so to be extra careful the player's flight was   escorted by the     flighter jets  
2024-02-03 06:16:56,742 	Text Hypothesis :	** ** ** ***** ******* the ******** match  ended in       a  stadium in       mumbai
2024-02-03 06:16:56,742 	Text Alignment  :	D  D  D  D     D           D        S      S     S        S  S       S        S     
2024-02-03 06:16:56,742 ========================================================================================================================
2024-02-03 06:16:56,743 Logging Sequence: 165_200.00
2024-02-03 06:16:56,743 	Gloss Reference :	A B+C+D+E
2024-02-03 06:16:56,743 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:16:56,743 	Gloss Alignment :	         
2024-02-03 06:16:56,743 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:16:56,744 	Text Reference  :	you may be wondering what bag remember in  2011 when india won    the world cup 
2024-02-03 06:16:56,744 	Text Hypothesis :	*** *** ** ********* **** but sehwag   did not  want to    change the ***** team
2024-02-03 06:16:56,744 	Text Alignment  :	D   D   D  D         D    S   S        S   S    S    S     S          D     S   
2024-02-03 06:16:56,745 ========================================================================================================================
2024-02-03 06:16:58,512 Epoch 269: Total Training Recognition Loss 0.32  Total Training Translation Loss 9.48 
2024-02-03 06:16:58,512 EPOCH 270
2024-02-03 06:17:03,713 Epoch 270: Total Training Recognition Loss 0.36  Total Training Translation Loss 8.82 
2024-02-03 06:17:03,713 EPOCH 271
2024-02-03 06:17:04,341 [Epoch: 271 Step: 00018100] Batch Recognition Loss:   0.024180 => Gls Tokens per Sec:     2410 || Batch Translation Loss:   0.021119 => Txt Tokens per Sec:     6013 || Lr: 0.000100
2024-02-03 06:17:09,073 Epoch 271: Total Training Recognition Loss 0.37  Total Training Translation Loss 8.27 
2024-02-03 06:17:09,073 EPOCH 272
2024-02-03 06:17:12,216 [Epoch: 272 Step: 00018200] Batch Recognition Loss:   0.003055 => Gls Tokens per Sec:     2161 || Batch Translation Loss:   0.069671 => Txt Tokens per Sec:     5827 || Lr: 0.000100
2024-02-03 06:17:14,293 Epoch 272: Total Training Recognition Loss 0.29  Total Training Translation Loss 7.97 
2024-02-03 06:17:14,293 EPOCH 273
2024-02-03 06:17:19,810 Epoch 273: Total Training Recognition Loss 0.29  Total Training Translation Loss 9.12 
2024-02-03 06:17:19,810 EPOCH 274
2024-02-03 06:17:20,434 [Epoch: 274 Step: 00018300] Batch Recognition Loss:   0.008310 => Gls Tokens per Sec:     2315 || Batch Translation Loss:   0.264178 => Txt Tokens per Sec:     6281 || Lr: 0.000100
2024-02-03 06:17:25,134 Epoch 274: Total Training Recognition Loss 0.45  Total Training Translation Loss 7.77 
2024-02-03 06:17:25,134 EPOCH 275
2024-02-03 06:17:28,143 [Epoch: 275 Step: 00018400] Batch Recognition Loss:   0.015384 => Gls Tokens per Sec:     2204 || Batch Translation Loss:   0.103606 => Txt Tokens per Sec:     6015 || Lr: 0.000100
2024-02-03 06:17:30,112 Epoch 275: Total Training Recognition Loss 0.41  Total Training Translation Loss 8.72 
2024-02-03 06:17:30,112 EPOCH 276
2024-02-03 06:17:35,493 Epoch 276: Total Training Recognition Loss 0.35  Total Training Translation Loss 11.48 
2024-02-03 06:17:35,494 EPOCH 277
2024-02-03 06:17:36,167 [Epoch: 277 Step: 00018500] Batch Recognition Loss:   0.001665 => Gls Tokens per Sec:     1908 || Batch Translation Loss:   0.489080 => Txt Tokens per Sec:     5624 || Lr: 0.000100
2024-02-03 06:17:40,953 Epoch 277: Total Training Recognition Loss 0.42  Total Training Translation Loss 11.02 
2024-02-03 06:17:40,954 EPOCH 278
2024-02-03 06:17:44,238 [Epoch: 278 Step: 00018600] Batch Recognition Loss:   0.006895 => Gls Tokens per Sec:     1971 || Batch Translation Loss:   0.191175 => Txt Tokens per Sec:     5324 || Lr: 0.000100
2024-02-03 06:17:46,334 Epoch 278: Total Training Recognition Loss 0.39  Total Training Translation Loss 10.23 
2024-02-03 06:17:46,334 EPOCH 279
2024-02-03 06:17:51,694 Epoch 279: Total Training Recognition Loss 0.28  Total Training Translation Loss 8.60 
2024-02-03 06:17:51,695 EPOCH 280
2024-02-03 06:17:52,283 [Epoch: 280 Step: 00018700] Batch Recognition Loss:   0.008229 => Gls Tokens per Sec:     1911 || Batch Translation Loss:   0.527267 => Txt Tokens per Sec:     6026 || Lr: 0.000100
2024-02-03 06:17:56,924 Epoch 280: Total Training Recognition Loss 0.36  Total Training Translation Loss 9.06 
2024-02-03 06:17:56,925 EPOCH 281
2024-02-03 06:17:59,905 [Epoch: 281 Step: 00018800] Batch Recognition Loss:   0.001417 => Gls Tokens per Sec:     2148 || Batch Translation Loss:   0.286164 => Txt Tokens per Sec:     5978 || Lr: 0.000100
2024-02-03 06:18:02,201 Epoch 281: Total Training Recognition Loss 0.31  Total Training Translation Loss 7.09 
2024-02-03 06:18:02,201 EPOCH 282
2024-02-03 06:18:07,378 Epoch 282: Total Training Recognition Loss 0.29  Total Training Translation Loss 6.89 
2024-02-03 06:18:07,379 EPOCH 283
2024-02-03 06:18:07,943 [Epoch: 283 Step: 00018900] Batch Recognition Loss:   0.019417 => Gls Tokens per Sec:     1705 || Batch Translation Loss:   0.099040 => Txt Tokens per Sec:     4798 || Lr: 0.000100
2024-02-03 06:18:12,414 Epoch 283: Total Training Recognition Loss 0.30  Total Training Translation Loss 10.20 
2024-02-03 06:18:12,414 EPOCH 284
2024-02-03 06:18:15,406 [Epoch: 284 Step: 00019000] Batch Recognition Loss:   0.005773 => Gls Tokens per Sec:     2056 || Batch Translation Loss:   0.154664 => Txt Tokens per Sec:     5458 || Lr: 0.000100
2024-02-03 06:18:17,696 Epoch 284: Total Training Recognition Loss 0.50  Total Training Translation Loss 12.87 
2024-02-03 06:18:17,696 EPOCH 285
2024-02-03 06:18:22,729 Epoch 285: Total Training Recognition Loss 0.78  Total Training Translation Loss 11.17 
2024-02-03 06:18:22,730 EPOCH 286
2024-02-03 06:18:23,044 [Epoch: 286 Step: 00019100] Batch Recognition Loss:   0.001612 => Gls Tokens per Sec:     2548 || Batch Translation Loss:   0.112082 => Txt Tokens per Sec:     6452 || Lr: 0.000100
2024-02-03 06:18:27,978 Epoch 286: Total Training Recognition Loss 2.29  Total Training Translation Loss 52.24 
2024-02-03 06:18:27,978 EPOCH 287
2024-02-03 06:18:31,109 [Epoch: 287 Step: 00019200] Batch Recognition Loss:   0.006929 => Gls Tokens per Sec:     1914 || Batch Translation Loss:   0.137142 => Txt Tokens per Sec:     5603 || Lr: 0.000100
2024-02-03 06:18:33,356 Epoch 287: Total Training Recognition Loss 6.82  Total Training Translation Loss 17.24 
2024-02-03 06:18:33,357 EPOCH 288
2024-02-03 06:18:38,437 Epoch 288: Total Training Recognition Loss 25.63  Total Training Translation Loss 16.56 
2024-02-03 06:18:38,437 EPOCH 289
2024-02-03 06:18:38,652 [Epoch: 289 Step: 00019300] Batch Recognition Loss:   0.001980 => Gls Tokens per Sec:     2991 || Batch Translation Loss:   0.408053 => Txt Tokens per Sec:     7477 || Lr: 0.000100
2024-02-03 06:18:43,397 Epoch 289: Total Training Recognition Loss 11.00  Total Training Translation Loss 17.05 
2024-02-03 06:18:43,397 EPOCH 290
2024-02-03 06:18:46,118 [Epoch: 290 Step: 00019400] Batch Recognition Loss:   0.001044 => Gls Tokens per Sec:     2144 || Batch Translation Loss:   0.179142 => Txt Tokens per Sec:     5699 || Lr: 0.000100
2024-02-03 06:18:48,743 Epoch 290: Total Training Recognition Loss 4.80  Total Training Translation Loss 5.92 
2024-02-03 06:18:48,744 EPOCH 291
2024-02-03 06:18:54,000 Epoch 291: Total Training Recognition Loss 1.48  Total Training Translation Loss 4.40 
2024-02-03 06:18:54,001 EPOCH 292
2024-02-03 06:18:54,206 [Epoch: 292 Step: 00019500] Batch Recognition Loss:   0.002682 => Gls Tokens per Sec:     2353 || Batch Translation Loss:   0.045865 => Txt Tokens per Sec:     6534 || Lr: 0.000100
2024-02-03 06:18:58,856 Epoch 292: Total Training Recognition Loss 0.57  Total Training Translation Loss 3.84 
2024-02-03 06:18:58,856 EPOCH 293
2024-02-03 06:19:01,604 [Epoch: 293 Step: 00019600] Batch Recognition Loss:   0.003425 => Gls Tokens per Sec:     2097 || Batch Translation Loss:   0.056014 => Txt Tokens per Sec:     5701 || Lr: 0.000100
2024-02-03 06:19:04,103 Epoch 293: Total Training Recognition Loss 0.41  Total Training Translation Loss 2.98 
2024-02-03 06:19:04,103 EPOCH 294
2024-02-03 06:19:09,289 Epoch 294: Total Training Recognition Loss 0.42  Total Training Translation Loss 3.16 
2024-02-03 06:19:09,290 EPOCH 295
2024-02-03 06:19:09,473 [Epoch: 295 Step: 00019700] Batch Recognition Loss:   0.004021 => Gls Tokens per Sec:     1768 || Batch Translation Loss:   0.022666 => Txt Tokens per Sec:     5735 || Lr: 0.000100
2024-02-03 06:19:14,785 Epoch 295: Total Training Recognition Loss 0.32  Total Training Translation Loss 4.00 
2024-02-03 06:19:14,785 EPOCH 296
2024-02-03 06:19:17,180 [Epoch: 296 Step: 00019800] Batch Recognition Loss:   0.001452 => Gls Tokens per Sec:     2339 || Batch Translation Loss:   0.194671 => Txt Tokens per Sec:     6481 || Lr: 0.000100
2024-02-03 06:19:19,761 Epoch 296: Total Training Recognition Loss 0.37  Total Training Translation Loss 5.05 
2024-02-03 06:19:19,762 EPOCH 297
2024-02-03 06:19:25,203 Epoch 297: Total Training Recognition Loss 0.33  Total Training Translation Loss 4.85 
2024-02-03 06:19:25,204 EPOCH 298
2024-02-03 06:19:25,285 [Epoch: 298 Step: 00019900] Batch Recognition Loss:   0.006808 => Gls Tokens per Sec:     2000 || Batch Translation Loss:   0.050368 => Txt Tokens per Sec:     4438 || Lr: 0.000100
2024-02-03 06:19:30,683 Epoch 298: Total Training Recognition Loss 0.25  Total Training Translation Loss 7.27 
2024-02-03 06:19:30,684 EPOCH 299
2024-02-03 06:19:33,430 [Epoch: 299 Step: 00020000] Batch Recognition Loss:   0.001895 => Gls Tokens per Sec:     1981 || Batch Translation Loss:   0.101296 => Txt Tokens per Sec:     5563 || Lr: 0.000100
2024-02-03 06:19:41,952 Validation result at epoch 299, step    20000: duration: 8.5223s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 5.05651	Translation Loss: 88882.99219	PPL: 10216.08789
	Eval Metric: BLEU
	WER 4.03	(DEL: 0.00,	INS: 0.00,	SUB: 4.03)
	BLEU-4 0.66	(BLEU-1: 10.22,	BLEU-2: 3.01,	BLEU-3: 1.29,	BLEU-4: 0.66)
	CHRF 17.39	ROUGE 8.43
2024-02-03 06:19:41,953 Logging Recognition and Translation Outputs
2024-02-03 06:19:41,954 ========================================================================================================================
2024-02-03 06:19:41,954 Logging Sequence: 153_218.00
2024-02-03 06:19:41,954 	Gloss Reference :	A B+C+D+E
2024-02-03 06:19:41,954 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:19:41,954 	Gloss Alignment :	         
2024-02-03 06:19:41,954 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:19:41,956 	Text Reference  :	the 2022 final match is        being         held    in the same melbourne stadium where the 1992 match was held    
2024-02-03 06:19:41,956 	Text Hypothesis :	the **** ***** world athletics championships started in *** **** ********* ******* qatar in  2022 world cup stadiums
2024-02-03 06:19:41,956 	Text Alignment  :	    D    D     S     S         S             S          D   D    D         D       S     S   S    S     S   S       
2024-02-03 06:19:41,956 ========================================================================================================================
2024-02-03 06:19:41,956 Logging Sequence: 168_115.00
2024-02-03 06:19:41,957 	Gloss Reference :	A B+C+D+E
2024-02-03 06:19:41,957 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:19:41,957 	Gloss Alignment :	         
2024-02-03 06:19:41,957 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:19:41,958 	Text Reference  :	**** ******* *** this has     sparked a    major discussion on social media   
2024-02-03 06:19:41,958 	Text Hypothesis :	many batsmen who have already know    that don't want       to be     pakistan
2024-02-03 06:19:41,958 	Text Alignment  :	I    I       I   S    S       S       S    S     S          S  S      S       
2024-02-03 06:19:41,958 ========================================================================================================================
2024-02-03 06:19:41,958 Logging Sequence: 180_82.00
2024-02-03 06:19:41,958 	Gloss Reference :	A B+C+D+E
2024-02-03 06:19:41,959 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:19:41,959 	Gloss Alignment :	         
2024-02-03 06:19:41,959 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:19:41,961 	Text Reference  :	let me   tell  you  about the *** *** **** ******* protest that has       been going  on since 23rd april 2023  
2024-02-03 06:19:41,961 	Text Hypothesis :	on  23rd march 2021 at    the 1st may 2023 earlier i       am   overjoyed and  agrees to stay  with the   matter
2024-02-03 06:19:41,961 	Text Alignment  :	S   S    S     S    S         I   I   I    I       S       S    S         S    S      S  S     S    S     S     
2024-02-03 06:19:41,961 ========================================================================================================================
2024-02-03 06:19:41,961 Logging Sequence: 56_14.00
2024-02-03 06:19:41,962 	Gloss Reference :	A B+C+D+E
2024-02-03 06:19:41,962 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:19:41,962 	Gloss Alignment :	         
2024-02-03 06:19:41,962 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:19:41,963 	Text Reference  :	*** people were glued to their screens during  the        match
2024-02-03 06:19:41,963 	Text Hypothesis :	the stumps were ***** ** asked about   dhoni's retirement plans
2024-02-03 06:19:41,963 	Text Alignment  :	I   S           D     D  S     S       S       S          S    
2024-02-03 06:19:41,963 ========================================================================================================================
2024-02-03 06:19:41,963 Logging Sequence: 62_135.00
2024-02-03 06:19:41,964 	Gloss Reference :	A B+C+D+E
2024-02-03 06:19:41,964 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:19:41,964 	Gloss Alignment :	         
2024-02-03 06:19:41,964 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:19:41,965 	Text Reference  :	******** *** *** ** *** ***** **** *** a grade player  is    the one who  
2024-02-03 06:19:41,965 	Text Hypothesis :	danushka got out on the first ball for a match between india and sri lanka
2024-02-03 06:19:41,965 	Text Alignment  :	I        I   I   I  I   I     I    I     S     S       S     S   S   S    
2024-02-03 06:19:41,965 ========================================================================================================================
2024-02-03 06:19:44,788 Epoch 299: Total Training Recognition Loss 0.32  Total Training Translation Loss 6.52 
2024-02-03 06:19:44,789 EPOCH 300
2024-02-03 06:19:50,021 [Epoch: 300 Step: 00020100] Batch Recognition Loss:   0.005004 => Gls Tokens per Sec:     2032 || Batch Translation Loss:   0.028174 => Txt Tokens per Sec:     5651 || Lr: 0.000100
2024-02-03 06:19:50,021 Epoch 300: Total Training Recognition Loss 0.23  Total Training Translation Loss 6.27 
2024-02-03 06:19:50,022 EPOCH 301
2024-02-03 06:19:55,036 Epoch 301: Total Training Recognition Loss 0.17  Total Training Translation Loss 6.84 
2024-02-03 06:19:55,037 EPOCH 302
2024-02-03 06:19:57,782 [Epoch: 302 Step: 00020200] Batch Recognition Loss:   0.000881 => Gls Tokens per Sec:     1892 || Batch Translation Loss:   0.177733 => Txt Tokens per Sec:     5619 || Lr: 0.000100
2024-02-03 06:20:00,209 Epoch 302: Total Training Recognition Loss 0.29  Total Training Translation Loss 9.83 
2024-02-03 06:20:00,209 EPOCH 303
2024-02-03 06:20:05,613 [Epoch: 303 Step: 00020300] Batch Recognition Loss:   0.006108 => Gls Tokens per Sec:     1938 || Batch Translation Loss:   0.175481 => Txt Tokens per Sec:     5403 || Lr: 0.000100
2024-02-03 06:20:05,664 Epoch 303: Total Training Recognition Loss 0.26  Total Training Translation Loss 11.41 
2024-02-03 06:20:05,665 EPOCH 304
2024-02-03 06:20:10,546 Epoch 304: Total Training Recognition Loss 0.27  Total Training Translation Loss 10.52 
2024-02-03 06:20:10,546 EPOCH 305
2024-02-03 06:20:13,011 [Epoch: 305 Step: 00020400] Batch Recognition Loss:   0.001188 => Gls Tokens per Sec:     2078 || Batch Translation Loss:   0.308438 => Txt Tokens per Sec:     5618 || Lr: 0.000100
2024-02-03 06:20:15,865 Epoch 305: Total Training Recognition Loss 0.31  Total Training Translation Loss 9.09 
2024-02-03 06:20:15,865 EPOCH 306
2024-02-03 06:20:20,724 [Epoch: 306 Step: 00020500] Batch Recognition Loss:   0.008317 => Gls Tokens per Sec:     2122 || Batch Translation Loss:   0.135888 => Txt Tokens per Sec:     5886 || Lr: 0.000100
2024-02-03 06:20:20,873 Epoch 306: Total Training Recognition Loss 0.27  Total Training Translation Loss 7.73 
2024-02-03 06:20:20,873 EPOCH 307
2024-02-03 06:20:26,299 Epoch 307: Total Training Recognition Loss 0.29  Total Training Translation Loss 6.50 
2024-02-03 06:20:26,299 EPOCH 308
2024-02-03 06:20:28,477 [Epoch: 308 Step: 00020600] Batch Recognition Loss:   0.004206 => Gls Tokens per Sec:     2278 || Batch Translation Loss:   0.037087 => Txt Tokens per Sec:     6475 || Lr: 0.000100
2024-02-03 06:20:31,524 Epoch 308: Total Training Recognition Loss 0.36  Total Training Translation Loss 7.05 
2024-02-03 06:20:31,525 EPOCH 309
2024-02-03 06:20:36,416 [Epoch: 309 Step: 00020700] Batch Recognition Loss:   0.001431 => Gls Tokens per Sec:     2076 || Batch Translation Loss:   0.193320 => Txt Tokens per Sec:     5787 || Lr: 0.000100
2024-02-03 06:20:36,630 Epoch 309: Total Training Recognition Loss 0.30  Total Training Translation Loss 7.32 
2024-02-03 06:20:36,630 EPOCH 310
2024-02-03 06:20:41,780 Epoch 310: Total Training Recognition Loss 0.29  Total Training Translation Loss 8.70 
2024-02-03 06:20:41,781 EPOCH 311
2024-02-03 06:20:44,224 [Epoch: 311 Step: 00020800] Batch Recognition Loss:   0.008548 => Gls Tokens per Sec:     1965 || Batch Translation Loss:   0.143716 => Txt Tokens per Sec:     5563 || Lr: 0.000100
2024-02-03 06:20:46,883 Epoch 311: Total Training Recognition Loss 0.31  Total Training Translation Loss 10.39 
2024-02-03 06:20:46,883 EPOCH 312
2024-02-03 06:20:52,239 [Epoch: 312 Step: 00020900] Batch Recognition Loss:   0.007254 => Gls Tokens per Sec:     1866 || Batch Translation Loss:   0.185502 => Txt Tokens per Sec:     5213 || Lr: 0.000100
2024-02-03 06:20:52,466 Epoch 312: Total Training Recognition Loss 0.25  Total Training Translation Loss 8.45 
2024-02-03 06:20:52,466 EPOCH 313
2024-02-03 06:20:57,780 Epoch 313: Total Training Recognition Loss 0.20  Total Training Translation Loss 5.33 
2024-02-03 06:20:57,781 EPOCH 314
2024-02-03 06:21:00,009 [Epoch: 314 Step: 00021000] Batch Recognition Loss:   0.002209 => Gls Tokens per Sec:     2083 || Batch Translation Loss:   0.065120 => Txt Tokens per Sec:     5730 || Lr: 0.000100
2024-02-03 06:21:02,809 Epoch 314: Total Training Recognition Loss 0.22  Total Training Translation Loss 5.17 
2024-02-03 06:21:02,809 EPOCH 315
2024-02-03 06:21:07,760 [Epoch: 315 Step: 00021100] Batch Recognition Loss:   0.011619 => Gls Tokens per Sec:     1986 || Batch Translation Loss:   0.071526 => Txt Tokens per Sec:     5585 || Lr: 0.000100
2024-02-03 06:21:08,089 Epoch 315: Total Training Recognition Loss 0.30  Total Training Translation Loss 8.46 
2024-02-03 06:21:08,089 EPOCH 316
2024-02-03 06:21:13,537 Epoch 316: Total Training Recognition Loss 0.33  Total Training Translation Loss 8.78 
2024-02-03 06:21:13,538 EPOCH 317
2024-02-03 06:21:15,810 [Epoch: 317 Step: 00021200] Batch Recognition Loss:   0.000398 => Gls Tokens per Sec:     1933 || Batch Translation Loss:   0.064512 => Txt Tokens per Sec:     5607 || Lr: 0.000100
2024-02-03 06:21:18,874 Epoch 317: Total Training Recognition Loss 0.30  Total Training Translation Loss 7.61 
2024-02-03 06:21:18,874 EPOCH 318
2024-02-03 06:21:23,970 [Epoch: 318 Step: 00021300] Batch Recognition Loss:   0.026191 => Gls Tokens per Sec:     1898 || Batch Translation Loss:   0.018833 => Txt Tokens per Sec:     5354 || Lr: 0.000100
2024-02-03 06:21:24,349 Epoch 318: Total Training Recognition Loss 0.27  Total Training Translation Loss 7.38 
2024-02-03 06:21:24,351 EPOCH 319
2024-02-03 06:21:29,490 Epoch 319: Total Training Recognition Loss 0.22  Total Training Translation Loss 6.35 
2024-02-03 06:21:29,491 EPOCH 320
2024-02-03 06:21:31,558 [Epoch: 320 Step: 00021400] Batch Recognition Loss:   0.003024 => Gls Tokens per Sec:     2048 || Batch Translation Loss:   0.165085 => Txt Tokens per Sec:     5786 || Lr: 0.000100
2024-02-03 06:21:34,841 Epoch 320: Total Training Recognition Loss 0.32  Total Training Translation Loss 6.81 
2024-02-03 06:21:34,842 EPOCH 321
2024-02-03 06:21:39,325 [Epoch: 321 Step: 00021500] Batch Recognition Loss:   0.002245 => Gls Tokens per Sec:     2122 || Batch Translation Loss:   0.086853 => Txt Tokens per Sec:     5936 || Lr: 0.000100
2024-02-03 06:21:39,823 Epoch 321: Total Training Recognition Loss 0.25  Total Training Translation Loss 7.88 
2024-02-03 06:21:39,823 EPOCH 322
2024-02-03 06:21:45,312 Epoch 322: Total Training Recognition Loss 0.28  Total Training Translation Loss 8.36 
2024-02-03 06:21:45,312 EPOCH 323
2024-02-03 06:21:47,176 [Epoch: 323 Step: 00021600] Batch Recognition Loss:   0.000664 => Gls Tokens per Sec:     2234 || Batch Translation Loss:   0.342888 => Txt Tokens per Sec:     6253 || Lr: 0.000100
2024-02-03 06:21:50,589 Epoch 323: Total Training Recognition Loss 0.29  Total Training Translation Loss 9.24 
2024-02-03 06:21:50,590 EPOCH 324
2024-02-03 06:21:55,046 [Epoch: 324 Step: 00021700] Batch Recognition Loss:   0.006322 => Gls Tokens per Sec:     2100 || Batch Translation Loss:   0.093413 => Txt Tokens per Sec:     5886 || Lr: 0.000100
2024-02-03 06:21:55,552 Epoch 324: Total Training Recognition Loss 0.34  Total Training Translation Loss 8.87 
2024-02-03 06:21:55,552 EPOCH 325
2024-02-03 06:22:01,055 Epoch 325: Total Training Recognition Loss 0.29  Total Training Translation Loss 7.60 
2024-02-03 06:22:01,056 EPOCH 326
2024-02-03 06:22:02,949 [Epoch: 326 Step: 00021800] Batch Recognition Loss:   0.005874 => Gls Tokens per Sec:     2113 || Batch Translation Loss:   0.626763 => Txt Tokens per Sec:     6023 || Lr: 0.000100
2024-02-03 06:22:06,251 Epoch 326: Total Training Recognition Loss 0.29  Total Training Translation Loss 9.17 
2024-02-03 06:22:06,252 EPOCH 327
2024-02-03 06:22:10,781 [Epoch: 327 Step: 00021900] Batch Recognition Loss:   0.004940 => Gls Tokens per Sec:     2030 || Batch Translation Loss:   0.054771 => Txt Tokens per Sec:     5682 || Lr: 0.000100
2024-02-03 06:22:11,384 Epoch 327: Total Training Recognition Loss 0.32  Total Training Translation Loss 8.17 
2024-02-03 06:22:11,385 EPOCH 328
2024-02-03 06:22:15,821 Epoch 328: Total Training Recognition Loss 0.22  Total Training Translation Loss 5.99 
2024-02-03 06:22:15,821 EPOCH 329
2024-02-03 06:22:17,752 [Epoch: 329 Step: 00022000] Batch Recognition Loss:   0.000804 => Gls Tokens per Sec:     1943 || Batch Translation Loss:   0.035442 => Txt Tokens per Sec:     5398 || Lr: 0.000100
2024-02-03 06:22:26,081 Validation result at epoch 329, step    22000: duration: 8.3284s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 4.25619	Translation Loss: 88827.30469	PPL: 10157.16895
	Eval Metric: BLEU
	WER 3.82	(DEL: 0.07,	INS: 0.00,	SUB: 3.75)
	BLEU-4 0.55	(BLEU-1: 10.00,	BLEU-2: 2.99,	BLEU-3: 1.16,	BLEU-4: 0.55)
	CHRF 17.08	ROUGE 8.45
2024-02-03 06:22:26,082 Logging Recognition and Translation Outputs
2024-02-03 06:22:26,082 ========================================================================================================================
2024-02-03 06:22:26,083 Logging Sequence: 85_58.00
2024-02-03 06:22:26,083 	Gloss Reference :	A B+C+D+E
2024-02-03 06:22:26,083 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:22:26,083 	Gloss Alignment :	         
2024-02-03 06:22:26,083 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:22:26,084 	Text Reference  :	symonds has been      an amazing player
2024-02-03 06:22:26,084 	Text Hypothesis :	******* was captained by south   africa
2024-02-03 06:22:26,084 	Text Alignment  :	D       S   S         S  S       S     
2024-02-03 06:22:26,084 ========================================================================================================================
2024-02-03 06:22:26,084 Logging Sequence: 71_120.00
2024-02-03 06:22:26,084 	Gloss Reference :	A B+C+D+E
2024-02-03 06:22:26,085 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:22:26,085 	Gloss Alignment :	         
2024-02-03 06:22:26,085 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:22:26,086 	Text Reference  :	on 3rd august 2022   kartikeya met  his family and  posted a   heartwarming picture with  his    mother
2024-02-03 06:22:26,086 	Text Hypothesis :	** his coach  sanjay was       born on  her    name for    the madhya       pradesh ranji trophy team  
2024-02-03 06:22:26,087 	Text Alignment  :	D  S   S      S      S         S    S   S      S    S      S   S            S       S     S      S     
2024-02-03 06:22:26,087 ========================================================================================================================
2024-02-03 06:22:26,087 Logging Sequence: 174_64.00
2024-02-03 06:22:26,087 	Gloss Reference :	A B+C+D+E
2024-02-03 06:22:26,087 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:22:26,087 	Gloss Alignment :	         
2024-02-03 06:22:26,088 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:22:26,089 	Text Reference  :	*** a      total     of 22 matches will  be  played  at              only 2     stadiums and not   all over india 
2024-02-03 06:22:26,089 	Text Hypothesis :	the sports authority of ** ******* india sai tweeted congratulations team india won      a   total of  9    medals
2024-02-03 06:22:26,090 	Text Alignment  :	I   S      S            D  D       S     S   S       S               S    S     S        S   S     S   S    S     
2024-02-03 06:22:26,090 ========================================================================================================================
2024-02-03 06:22:26,090 Logging Sequence: 93_2.00
2024-02-03 06:22:26,090 	Gloss Reference :	A B+C+D+E
2024-02-03 06:22:26,090 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:22:26,090 	Gloss Alignment :	         
2024-02-03 06:22:26,090 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:22:26,091 	Text Reference  :	********* ******** ********** wayne rooney was an    amazing football player ***** *** ******** ******* * ** **** *** *** *****
2024-02-03 06:22:26,092 	Text Hypothesis :	germany's football federation said  in     a   match against the      player could not continue playing i am sure you are proud
2024-02-03 06:22:26,092 	Text Alignment  :	I         I        I          S     S      S   S     S       S               I     I   I        I       I I  I    I   I   I    
2024-02-03 06:22:26,092 ========================================================================================================================
2024-02-03 06:22:26,092 Logging Sequence: 172_270.00
2024-02-03 06:22:26,092 	Gloss Reference :	A B+C+D+E
2024-02-03 06:22:26,092 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:22:26,093 	Gloss Alignment :	         
2024-02-03 06:22:26,093 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:22:26,094 	Text Reference  :	if ***** it  continues to *** rain   today the ****** tickets will   be  refunded
2024-02-03 06:22:26,094 	Text Hypothesis :	if after the oc        to the report then  the police had     broken the fir     
2024-02-03 06:22:26,094 	Text Alignment  :	   I     S   S            I   S      S         I      S       S      S   S       
2024-02-03 06:22:26,094 ========================================================================================================================
2024-02-03 06:22:29,642 Epoch 329: Total Training Recognition Loss 0.29  Total Training Translation Loss 7.28 
2024-02-03 06:22:29,643 EPOCH 330
2024-02-03 06:22:34,403 [Epoch: 330 Step: 00022100] Batch Recognition Loss:   0.018193 => Gls Tokens per Sec:     1897 || Batch Translation Loss:   0.053668 => Txt Tokens per Sec:     5304 || Lr: 0.000100
2024-02-03 06:22:35,132 Epoch 330: Total Training Recognition Loss 0.24  Total Training Translation Loss 6.14 
2024-02-03 06:22:35,132 EPOCH 331
2024-02-03 06:22:40,326 Epoch 331: Total Training Recognition Loss 0.23  Total Training Translation Loss 6.84 
2024-02-03 06:22:40,326 EPOCH 332
2024-02-03 06:22:41,973 [Epoch: 332 Step: 00022200] Batch Recognition Loss:   0.001670 => Gls Tokens per Sec:     2237 || Batch Translation Loss:   0.076751 => Txt Tokens per Sec:     6120 || Lr: 0.000100
2024-02-03 06:22:45,974 Epoch 332: Total Training Recognition Loss 0.22  Total Training Translation Loss 9.41 
2024-02-03 06:22:45,974 EPOCH 333
2024-02-03 06:22:50,463 [Epoch: 333 Step: 00022300] Batch Recognition Loss:   0.002401 => Gls Tokens per Sec:     1976 || Batch Translation Loss:   0.136183 => Txt Tokens per Sec:     5613 || Lr: 0.000100
2024-02-03 06:22:51,197 Epoch 333: Total Training Recognition Loss 0.28  Total Training Translation Loss 10.01 
2024-02-03 06:22:51,198 EPOCH 334
2024-02-03 06:22:56,690 Epoch 334: Total Training Recognition Loss 0.48  Total Training Translation Loss 8.46 
2024-02-03 06:22:56,691 EPOCH 335
2024-02-03 06:22:58,298 [Epoch: 335 Step: 00022400] Batch Recognition Loss:   0.000708 => Gls Tokens per Sec:     2135 || Batch Translation Loss:   0.110317 => Txt Tokens per Sec:     5745 || Lr: 0.000100
2024-02-03 06:23:01,776 Epoch 335: Total Training Recognition Loss 0.28  Total Training Translation Loss 11.36 
2024-02-03 06:23:01,777 EPOCH 336
2024-02-03 06:23:06,364 [Epoch: 336 Step: 00022500] Batch Recognition Loss:   0.006210 => Gls Tokens per Sec:     1900 || Batch Translation Loss:   0.169215 => Txt Tokens per Sec:     5301 || Lr: 0.000100
2024-02-03 06:23:07,227 Epoch 336: Total Training Recognition Loss 0.33  Total Training Translation Loss 10.70 
2024-02-03 06:23:07,227 EPOCH 337
2024-02-03 06:23:12,217 Epoch 337: Total Training Recognition Loss 0.25  Total Training Translation Loss 8.21 
2024-02-03 06:23:12,217 EPOCH 338
2024-02-03 06:23:14,113 [Epoch: 338 Step: 00022600] Batch Recognition Loss:   0.002269 => Gls Tokens per Sec:     1774 || Batch Translation Loss:   0.347584 => Txt Tokens per Sec:     5278 || Lr: 0.000100
2024-02-03 06:23:17,591 Epoch 338: Total Training Recognition Loss 0.24  Total Training Translation Loss 8.88 
2024-02-03 06:23:17,591 EPOCH 339
2024-02-03 06:23:21,469 [Epoch: 339 Step: 00022700] Batch Recognition Loss:   0.001850 => Gls Tokens per Sec:     2228 || Batch Translation Loss:   0.061070 => Txt Tokens per Sec:     6235 || Lr: 0.000100
2024-02-03 06:23:22,564 Epoch 339: Total Training Recognition Loss 0.33  Total Training Translation Loss 11.59 
2024-02-03 06:23:22,564 EPOCH 340
2024-02-03 06:23:27,928 Epoch 340: Total Training Recognition Loss 0.29  Total Training Translation Loss 7.49 
2024-02-03 06:23:27,928 EPOCH 341
2024-02-03 06:23:29,414 [Epoch: 341 Step: 00022800] Batch Recognition Loss:   0.003910 => Gls Tokens per Sec:     2155 || Batch Translation Loss:   0.059439 => Txt Tokens per Sec:     6696 || Lr: 0.000100
2024-02-03 06:23:33,101 Epoch 341: Total Training Recognition Loss 0.31  Total Training Translation Loss 5.74 
2024-02-03 06:23:33,101 EPOCH 342
2024-02-03 06:23:37,217 [Epoch: 342 Step: 00022900] Batch Recognition Loss:   0.001933 => Gls Tokens per Sec:     2039 || Batch Translation Loss:   0.110903 => Txt Tokens per Sec:     5679 || Lr: 0.000100
2024-02-03 06:23:38,299 Epoch 342: Total Training Recognition Loss 0.20  Total Training Translation Loss 4.20 
2024-02-03 06:23:38,299 EPOCH 343
2024-02-03 06:23:43,651 Epoch 343: Total Training Recognition Loss 0.18  Total Training Translation Loss 3.34 
2024-02-03 06:23:43,651 EPOCH 344
2024-02-03 06:23:45,138 [Epoch: 344 Step: 00023000] Batch Recognition Loss:   0.000401 => Gls Tokens per Sec:     1986 || Batch Translation Loss:   0.119396 => Txt Tokens per Sec:     5455 || Lr: 0.000100
2024-02-03 06:23:49,242 Epoch 344: Total Training Recognition Loss 0.17  Total Training Translation Loss 4.00 
2024-02-03 06:23:49,242 EPOCH 345
2024-02-03 06:23:53,136 [Epoch: 345 Step: 00023100] Batch Recognition Loss:   0.001519 => Gls Tokens per Sec:     2114 || Batch Translation Loss:   0.088896 => Txt Tokens per Sec:     5770 || Lr: 0.000100
2024-02-03 06:23:54,267 Epoch 345: Total Training Recognition Loss 0.26  Total Training Translation Loss 4.15 
2024-02-03 06:23:54,267 EPOCH 346
2024-02-03 06:23:59,547 Epoch 346: Total Training Recognition Loss 0.30  Total Training Translation Loss 4.87 
2024-02-03 06:23:59,548 EPOCH 347
2024-02-03 06:24:01,052 [Epoch: 347 Step: 00023200] Batch Recognition Loss:   0.003839 => Gls Tokens per Sec:     1915 || Batch Translation Loss:   0.042100 => Txt Tokens per Sec:     5590 || Lr: 0.000100
2024-02-03 06:24:04,674 Epoch 347: Total Training Recognition Loss 0.21  Total Training Translation Loss 6.60 
2024-02-03 06:24:04,674 EPOCH 348
2024-02-03 06:24:09,048 [Epoch: 348 Step: 00023300] Batch Recognition Loss:   0.001177 => Gls Tokens per Sec:     1846 || Batch Translation Loss:   0.071660 => Txt Tokens per Sec:     5167 || Lr: 0.000100
2024-02-03 06:24:10,174 Epoch 348: Total Training Recognition Loss 0.19  Total Training Translation Loss 4.99 
2024-02-03 06:24:10,174 EPOCH 349
2024-02-03 06:24:15,552 Epoch 349: Total Training Recognition Loss 0.18  Total Training Translation Loss 7.06 
2024-02-03 06:24:15,553 EPOCH 350
2024-02-03 06:24:16,805 [Epoch: 350 Step: 00023400] Batch Recognition Loss:   0.005260 => Gls Tokens per Sec:     2175 || Batch Translation Loss:   0.115580 => Txt Tokens per Sec:     6090 || Lr: 0.000100
2024-02-03 06:24:20,509 Epoch 350: Total Training Recognition Loss 0.25  Total Training Translation Loss 8.03 
2024-02-03 06:24:20,509 EPOCH 351
2024-02-03 06:24:23,895 [Epoch: 351 Step: 00023500] Batch Recognition Loss:   0.005791 => Gls Tokens per Sec:     2364 || Batch Translation Loss:   0.284592 => Txt Tokens per Sec:     6503 || Lr: 0.000100
2024-02-03 06:24:25,690 Epoch 351: Total Training Recognition Loss 0.32  Total Training Translation Loss 17.84 
2024-02-03 06:24:25,691 EPOCH 352
2024-02-03 06:24:31,197 Epoch 352: Total Training Recognition Loss 0.52  Total Training Translation Loss 14.61 
2024-02-03 06:24:31,197 EPOCH 353
2024-02-03 06:24:32,292 [Epoch: 353 Step: 00023600] Batch Recognition Loss:   0.003753 => Gls Tokens per Sec:     2340 || Batch Translation Loss:   0.175213 => Txt Tokens per Sec:     6329 || Lr: 0.000100
2024-02-03 06:24:36,542 Epoch 353: Total Training Recognition Loss 0.35  Total Training Translation Loss 9.65 
2024-02-03 06:24:36,543 EPOCH 354
2024-02-03 06:24:40,228 [Epoch: 354 Step: 00023700] Batch Recognition Loss:   0.002045 => Gls Tokens per Sec:     2128 || Batch Translation Loss:   0.149800 => Txt Tokens per Sec:     5949 || Lr: 0.000100
2024-02-03 06:24:41,718 Epoch 354: Total Training Recognition Loss 0.29  Total Training Translation Loss 8.55 
2024-02-03 06:24:41,719 EPOCH 355
2024-02-03 06:24:46,834 Epoch 355: Total Training Recognition Loss 0.28  Total Training Translation Loss 9.87 
2024-02-03 06:24:46,834 EPOCH 356
2024-02-03 06:24:47,977 [Epoch: 356 Step: 00023800] Batch Recognition Loss:   0.005920 => Gls Tokens per Sec:     2099 || Batch Translation Loss:   0.057680 => Txt Tokens per Sec:     5895 || Lr: 0.000100
2024-02-03 06:24:52,328 Epoch 356: Total Training Recognition Loss 0.33  Total Training Translation Loss 7.74 
2024-02-03 06:24:52,329 EPOCH 357
2024-02-03 06:24:55,898 [Epoch: 357 Step: 00023900] Batch Recognition Loss:   0.000814 => Gls Tokens per Sec:     2128 || Batch Translation Loss:   0.094683 => Txt Tokens per Sec:     5850 || Lr: 0.000100
2024-02-03 06:24:57,593 Epoch 357: Total Training Recognition Loss 0.22  Total Training Translation Loss 5.40 
2024-02-03 06:24:57,593 EPOCH 358
2024-02-03 06:25:02,874 Epoch 358: Total Training Recognition Loss 0.17  Total Training Translation Loss 5.13 
2024-02-03 06:25:02,874 EPOCH 359
2024-02-03 06:25:03,977 [Epoch: 359 Step: 00024000] Batch Recognition Loss:   0.005983 => Gls Tokens per Sec:     2033 || Batch Translation Loss:   0.026479 => Txt Tokens per Sec:     5455 || Lr: 0.000100
2024-02-03 06:25:12,008 Validation result at epoch 359, step    24000: duration: 8.0298s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 4.62987	Translation Loss: 89496.43750	PPL: 10888.18945
	Eval Metric: BLEU
	WER 3.75	(DEL: 0.14,	INS: 0.00,	SUB: 3.61)
	BLEU-4 0.49	(BLEU-1: 10.16,	BLEU-2: 2.80,	BLEU-3: 1.08,	BLEU-4: 0.49)
	CHRF 16.53	ROUGE 8.59
2024-02-03 06:25:12,009 Logging Recognition and Translation Outputs
2024-02-03 06:25:12,009 ========================================================================================================================
2024-02-03 06:25:12,009 Logging Sequence: 118_46.00
2024-02-03 06:25:12,009 	Gloss Reference :	A B+C+D+E
2024-02-03 06:25:12,009 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:25:12,009 	Gloss Alignment :	         
2024-02-03 06:25:12,010 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:25:12,011 	Text Reference  :	*** **** since there was a       tie the match went into the penalty shootout where each team gets a chance to score 5  times
2024-02-03 06:25:12,011 	Text Hypothesis :	the next day   he    was present to  the match **** **** *** ******* ******** ***** **** **** **** * ****** ** ***** as well 
2024-02-03 06:25:12,011 	Text Alignment  :	I   I    S     S         S       S             D    D    D   D       D        D     D    D    D    D D      D  D     S  S    
2024-02-03 06:25:12,011 ========================================================================================================================
2024-02-03 06:25:12,012 Logging Sequence: 115_59.00
2024-02-03 06:25:12,012 	Gloss Reference :	A B+C+D+E
2024-02-03 06:25:12,012 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:25:12,012 	Gloss Alignment :	         
2024-02-03 06:25:12,012 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:25:12,013 	Text Reference  :	** **** ****** she now hosts     several cricket  programmes
2024-02-03 06:25:12,013 	Text Hypothesis :	on 11th august she *** announced on      facebook that      
2024-02-03 06:25:12,013 	Text Alignment  :	I  I    I          D   S         S       S        S         
2024-02-03 06:25:12,013 ========================================================================================================================
2024-02-03 06:25:12,013 Logging Sequence: 128_172.00
2024-02-03 06:25:12,013 	Gloss Reference :	A B+C+D+E
2024-02-03 06:25:12,013 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:25:12,013 	Gloss Alignment :	         
2024-02-03 06:25:12,014 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:25:12,015 	Text Reference  :	******** the best test team can't be decided with just one  match and   it      should be     a    best of    three contest
2024-02-03 06:25:12,015 	Text Hypothesis :	shocking the **** **** **** ***** ** ******* **** bcci then the   three players who    worked hard to   reach the   finals 
2024-02-03 06:25:12,016 	Text Alignment  :	I            D    D    D    D     D  D       D    S    S    S     S     S       S      S      S    S    S     S     S      
2024-02-03 06:25:12,016 ========================================================================================================================
2024-02-03 06:25:12,016 Logging Sequence: 119_151.00
2024-02-03 06:25:12,016 	Gloss Reference :	A B+C+D+E
2024-02-03 06:25:12,016 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:25:12,016 	Gloss Alignment :	         
2024-02-03 06:25:12,016 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:25:12,017 	Text Reference  :	messi loved the  idea      and placed  the order  
2024-02-03 06:25:12,017 	Text Hypothesis :	when  i     gift something to  players and special
2024-02-03 06:25:12,017 	Text Alignment  :	S     S     S    S         S   S       S   S      
2024-02-03 06:25:12,017 ========================================================================================================================
2024-02-03 06:25:12,017 Logging Sequence: 61_196.00
2024-02-03 06:25:12,018 	Gloss Reference :	A B+C+D+E
2024-02-03 06:25:12,018 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:25:12,018 	Gloss Alignment :	         
2024-02-03 06:25:12,018 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:25:12,019 	Text Reference  :	another said people should refrain from spreading this video ** who knows if  that  video is  fake 
2024-02-03 06:25:12,019 	Text Hypothesis :	******* **** ****** ****** ******* as   per       the  video of the car   was going to    abu dhabi
2024-02-03 06:25:12,019 	Text Alignment  :	D       D    D      D      D       S    S         S          I  S   S     S   S     S     S   S    
2024-02-03 06:25:12,020 ========================================================================================================================
2024-02-03 06:25:16,519 Epoch 359: Total Training Recognition Loss 0.20  Total Training Translation Loss 5.92 
2024-02-03 06:25:16,519 EPOCH 360
2024-02-03 06:25:19,931 [Epoch: 360 Step: 00024100] Batch Recognition Loss:   0.001465 => Gls Tokens per Sec:     2178 || Batch Translation Loss:   0.040117 => Txt Tokens per Sec:     5907 || Lr: 0.000100
2024-02-03 06:25:21,442 Epoch 360: Total Training Recognition Loss 0.19  Total Training Translation Loss 5.91 
2024-02-03 06:25:21,443 EPOCH 361
2024-02-03 06:25:26,811 Epoch 361: Total Training Recognition Loss 0.24  Total Training Translation Loss 8.47 
2024-02-03 06:25:26,811 EPOCH 362
2024-02-03 06:25:27,666 [Epoch: 362 Step: 00024200] Batch Recognition Loss:   0.002948 => Gls Tokens per Sec:     2437 || Batch Translation Loss:   0.020947 => Txt Tokens per Sec:     6246 || Lr: 0.000100
2024-02-03 06:25:31,897 Epoch 362: Total Training Recognition Loss 0.22  Total Training Translation Loss 7.51 
2024-02-03 06:25:31,898 EPOCH 363
2024-02-03 06:25:35,867 [Epoch: 363 Step: 00024300] Batch Recognition Loss:   0.002242 => Gls Tokens per Sec:     1832 || Batch Translation Loss:   0.024781 => Txt Tokens per Sec:     5255 || Lr: 0.000100
2024-02-03 06:25:37,316 Epoch 363: Total Training Recognition Loss 0.30  Total Training Translation Loss 6.29 
2024-02-03 06:25:37,317 EPOCH 364
2024-02-03 06:25:41,945 Epoch 364: Total Training Recognition Loss 0.27  Total Training Translation Loss 5.54 
2024-02-03 06:25:41,945 EPOCH 365
2024-02-03 06:25:42,861 [Epoch: 365 Step: 00024400] Batch Recognition Loss:   0.005250 => Gls Tokens per Sec:     2100 || Batch Translation Loss:   0.095230 => Txt Tokens per Sec:     5985 || Lr: 0.000100
2024-02-03 06:25:47,446 Epoch 365: Total Training Recognition Loss 0.22  Total Training Translation Loss 6.74 
2024-02-03 06:25:47,447 EPOCH 366
2024-02-03 06:25:50,990 [Epoch: 366 Step: 00024500] Batch Recognition Loss:   0.002571 => Gls Tokens per Sec:     2007 || Batch Translation Loss:   0.061225 => Txt Tokens per Sec:     5680 || Lr: 0.000100
2024-02-03 06:25:52,738 Epoch 366: Total Training Recognition Loss 0.20  Total Training Translation Loss 8.95 
2024-02-03 06:25:52,739 EPOCH 367
2024-02-03 06:25:58,079 Epoch 367: Total Training Recognition Loss 0.19  Total Training Translation Loss 11.52 
2024-02-03 06:25:58,080 EPOCH 368
2024-02-03 06:25:58,856 [Epoch: 368 Step: 00024600] Batch Recognition Loss:   0.002262 => Gls Tokens per Sec:     2154 || Batch Translation Loss:   0.202541 => Txt Tokens per Sec:     6035 || Lr: 0.000100
2024-02-03 06:26:03,448 Epoch 368: Total Training Recognition Loss 0.29  Total Training Translation Loss 9.04 
2024-02-03 06:26:03,449 EPOCH 369
2024-02-03 06:26:06,794 [Epoch: 369 Step: 00024700] Batch Recognition Loss:   0.000698 => Gls Tokens per Sec:     2078 || Batch Translation Loss:   0.061124 => Txt Tokens per Sec:     5784 || Lr: 0.000100
2024-02-03 06:26:08,754 Epoch 369: Total Training Recognition Loss 0.30  Total Training Translation Loss 6.59 
2024-02-03 06:26:08,755 EPOCH 370
2024-02-03 06:26:14,078 Epoch 370: Total Training Recognition Loss 0.21  Total Training Translation Loss 6.71 
2024-02-03 06:26:14,079 EPOCH 371
2024-02-03 06:26:14,844 [Epoch: 371 Step: 00024800] Batch Recognition Loss:   0.001627 => Gls Tokens per Sec:     2092 || Batch Translation Loss:   0.075887 => Txt Tokens per Sec:     6408 || Lr: 0.000100
2024-02-03 06:26:19,039 Epoch 371: Total Training Recognition Loss 0.21  Total Training Translation Loss 5.79 
2024-02-03 06:26:19,039 EPOCH 372
2024-02-03 06:26:22,498 [Epoch: 372 Step: 00024900] Batch Recognition Loss:   0.001644 => Gls Tokens per Sec:     1964 || Batch Translation Loss:   0.034418 => Txt Tokens per Sec:     5558 || Lr: 0.000100
2024-02-03 06:26:24,268 Epoch 372: Total Training Recognition Loss 0.26  Total Training Translation Loss 6.67 
2024-02-03 06:26:24,268 EPOCH 373
2024-02-03 06:26:28,743 Epoch 373: Total Training Recognition Loss 0.34  Total Training Translation Loss 5.14 
2024-02-03 06:26:28,743 EPOCH 374
2024-02-03 06:26:29,340 [Epoch: 374 Step: 00025000] Batch Recognition Loss:   0.001974 => Gls Tokens per Sec:     2416 || Batch Translation Loss:   0.126330 => Txt Tokens per Sec:     6819 || Lr: 0.000100
2024-02-03 06:26:33,822 Epoch 374: Total Training Recognition Loss 0.16  Total Training Translation Loss 3.37 
2024-02-03 06:26:33,823 EPOCH 375
2024-02-03 06:26:37,142 [Epoch: 375 Step: 00025100] Batch Recognition Loss:   0.000203 => Gls Tokens per Sec:     1998 || Batch Translation Loss:   0.049808 => Txt Tokens per Sec:     5621 || Lr: 0.000100
2024-02-03 06:26:39,002 Epoch 375: Total Training Recognition Loss 0.19  Total Training Translation Loss 3.75 
2024-02-03 06:26:39,003 EPOCH 376
2024-02-03 06:26:44,481 Epoch 376: Total Training Recognition Loss 0.15  Total Training Translation Loss 4.32 
2024-02-03 06:26:44,482 EPOCH 377
2024-02-03 06:26:44,984 [Epoch: 377 Step: 00025200] Batch Recognition Loss:   0.010364 => Gls Tokens per Sec:     2555 || Batch Translation Loss:   0.041707 => Txt Tokens per Sec:     6691 || Lr: 0.000100
2024-02-03 06:26:49,614 Epoch 377: Total Training Recognition Loss 0.25  Total Training Translation Loss 7.32 
2024-02-03 06:26:49,615 EPOCH 378
2024-02-03 06:26:52,728 [Epoch: 378 Step: 00025300] Batch Recognition Loss:   0.006059 => Gls Tokens per Sec:     2108 || Batch Translation Loss:   0.119740 => Txt Tokens per Sec:     5837 || Lr: 0.000100
2024-02-03 06:26:54,820 Epoch 378: Total Training Recognition Loss 0.55  Total Training Translation Loss 8.55 
2024-02-03 06:26:54,820 EPOCH 379
2024-02-03 06:26:59,982 Epoch 379: Total Training Recognition Loss 1.59  Total Training Translation Loss 8.87 
2024-02-03 06:26:59,983 EPOCH 380
2024-02-03 06:27:00,565 [Epoch: 380 Step: 00025400] Batch Recognition Loss:   0.005244 => Gls Tokens per Sec:     1924 || Batch Translation Loss:   0.111591 => Txt Tokens per Sec:     5610 || Lr: 0.000100
2024-02-03 06:27:05,334 Epoch 380: Total Training Recognition Loss 6.04  Total Training Translation Loss 10.17 
2024-02-03 06:27:05,335 EPOCH 381
2024-02-03 06:27:08,593 [Epoch: 381 Step: 00025500] Batch Recognition Loss:   0.104452 => Gls Tokens per Sec:     1937 || Batch Translation Loss:   0.144336 => Txt Tokens per Sec:     5385 || Lr: 0.000100
2024-02-03 06:27:10,644 Epoch 381: Total Training Recognition Loss 7.17  Total Training Translation Loss 12.17 
2024-02-03 06:27:10,644 EPOCH 382
2024-02-03 06:27:15,674 Epoch 382: Total Training Recognition Loss 1.98  Total Training Translation Loss 10.82 
2024-02-03 06:27:15,675 EPOCH 383
2024-02-03 06:27:16,157 [Epoch: 383 Step: 00025600] Batch Recognition Loss:   0.021340 => Gls Tokens per Sec:     1996 || Batch Translation Loss:   0.101950 => Txt Tokens per Sec:     5035 || Lr: 0.000100
2024-02-03 06:27:21,243 Epoch 383: Total Training Recognition Loss 0.59  Total Training Translation Loss 7.13 
2024-02-03 06:27:21,243 EPOCH 384
2024-02-03 06:27:24,653 [Epoch: 384 Step: 00025700] Batch Recognition Loss:   0.049386 => Gls Tokens per Sec:     1831 || Batch Translation Loss:   0.106411 => Txt Tokens per Sec:     5129 || Lr: 0.000100
2024-02-03 06:27:26,769 Epoch 384: Total Training Recognition Loss 0.32  Total Training Translation Loss 4.26 
2024-02-03 06:27:26,770 EPOCH 385
2024-02-03 06:27:32,109 Epoch 385: Total Training Recognition Loss 0.31  Total Training Translation Loss 5.37 
2024-02-03 06:27:32,109 EPOCH 386
2024-02-03 06:27:32,517 [Epoch: 386 Step: 00025800] Batch Recognition Loss:   0.003530 => Gls Tokens per Sec:     1966 || Batch Translation Loss:   0.029813 => Txt Tokens per Sec:     5376 || Lr: 0.000100
2024-02-03 06:27:37,537 Epoch 386: Total Training Recognition Loss 0.22  Total Training Translation Loss 3.35 
2024-02-03 06:27:37,538 EPOCH 387
2024-02-03 06:27:40,793 [Epoch: 387 Step: 00025900] Batch Recognition Loss:   0.012857 => Gls Tokens per Sec:     1841 || Batch Translation Loss:   0.041255 => Txt Tokens per Sec:     5298 || Lr: 0.000100
2024-02-03 06:27:42,978 Epoch 387: Total Training Recognition Loss 0.19  Total Training Translation Loss 3.44 
2024-02-03 06:27:42,979 EPOCH 388
2024-02-03 06:27:48,270 Epoch 388: Total Training Recognition Loss 0.16  Total Training Translation Loss 4.54 
2024-02-03 06:27:48,270 EPOCH 389
2024-02-03 06:27:48,677 [Epoch: 389 Step: 00026000] Batch Recognition Loss:   0.002146 => Gls Tokens per Sec:     1575 || Batch Translation Loss:   0.011412 => Txt Tokens per Sec:     4778 || Lr: 0.000100
2024-02-03 06:27:57,260 Validation result at epoch 389, step    26000: duration: 8.5833s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 6.08510	Translation Loss: 90128.71875	PPL: 11627.22949
	Eval Metric: BLEU
	WER 3.61	(DEL: 0.00,	INS: 0.00,	SUB: 3.61)
	BLEU-4 0.60	(BLEU-1: 11.24,	BLEU-2: 3.18,	BLEU-3: 1.21,	BLEU-4: 0.60)
	CHRF 16.98	ROUGE 9.22
2024-02-03 06:27:57,262 Logging Recognition and Translation Outputs
2024-02-03 06:27:57,262 ========================================================================================================================
2024-02-03 06:27:57,262 Logging Sequence: 140_34.00
2024-02-03 06:27:57,262 	Gloss Reference :	A B+C+D+E
2024-02-03 06:27:57,262 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:27:57,262 	Gloss Alignment :	         
2024-02-03 06:27:57,262 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:27:57,264 	Text Reference  :	pant made his debut in  international cricket in   february 2017        against england india    won the match because of  pant
2024-02-03 06:27:57,264 	Text Hypothesis :	pant **** has named the biggest       odi     face of       uttarakhand the     biggest festival in  the state on      the same
2024-02-03 06:27:57,265 	Text Alignment  :	     D    S   S     S   S             S       S    S        S           S       S       S        S       S     S       S   S   
2024-02-03 06:27:57,265 ========================================================================================================================
2024-02-03 06:27:57,265 Logging Sequence: 142_111.00
2024-02-03 06:27:57,265 	Gloss Reference :	A B+C+D+E
2024-02-03 06:27:57,265 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:27:57,265 	Gloss Alignment :	         
2024-02-03 06:27:57,265 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:27:57,266 	Text Reference  :	oh god how embarassing will the young boy be   able to   pay such    a     huge amount
2024-02-03 06:27:57,267 	Text Hypothesis :	** *** *** *********** **** *** you   may know that such is  talking about this act   
2024-02-03 06:27:57,267 	Text Alignment  :	D  D   D   D           D    D   S     S   S    S    S    S   S       S     S    S     
2024-02-03 06:27:57,267 ========================================================================================================================
2024-02-03 06:27:57,267 Logging Sequence: 135_9.00
2024-02-03 06:27:57,267 	Gloss Reference :	A B+C+D+E
2024-02-03 06:27:57,267 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:27:57,267 	Gloss Alignment :	         
2024-02-03 06:27:57,267 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:27:57,268 	Text Reference  :	***** flew  to the tokyo olympics won a silver medal in      javelin  throw 
2024-02-03 06:27:57,268 	Text Hypothesis :	there needs to *** ***** ******** be  a ****** ***** neutral umpiring system
2024-02-03 06:27:57,268 	Text Alignment  :	I     S        D   D     D        S     D      D     S       S        S     
2024-02-03 06:27:57,269 ========================================================================================================================
2024-02-03 06:27:57,269 Logging Sequence: 118_444.00
2024-02-03 06:27:57,269 	Gloss Reference :	A B+C+D+E
2024-02-03 06:27:57,269 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:27:57,269 	Gloss Alignment :	         
2024-02-03 06:27:57,269 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:27:57,270 	Text Reference  :	what a        match but cheers to argentina for winning the  match    
2024-02-03 06:27:57,270 	Text Hypothesis :	this exciting match *** ****** ** ********* *** people  were surprised
2024-02-03 06:27:57,270 	Text Alignment  :	S    S              D   D      D  D         D   S       S    S        
2024-02-03 06:27:57,270 ========================================================================================================================
2024-02-03 06:27:57,270 Logging Sequence: 52_36.00
2024-02-03 06:27:57,270 	Gloss Reference :	A B+C+D+E
2024-02-03 06:27:57,271 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:27:57,271 	Gloss Alignment :	         
2024-02-03 06:27:57,271 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:27:57,272 	Text Reference  :	****** **** ******* ** *** *** **** ******* recently dhoni was travellin on  an   indigo flight
2024-02-03 06:27:57,272 	Text Hypothesis :	people were shocked to see him from chennai super    year  old pakistan  has down to     play  
2024-02-03 06:27:57,272 	Text Alignment  :	I      I    I       I  I   I   I    I       S        S     S   S         S   S    S      S     
2024-02-03 06:27:57,272 ========================================================================================================================
2024-02-03 06:28:02,265 Epoch 389: Total Training Recognition Loss 0.16  Total Training Translation Loss 4.52 
2024-02-03 06:28:02,266 EPOCH 390
2024-02-03 06:28:05,311 [Epoch: 390 Step: 00026100] Batch Recognition Loss:   0.001241 => Gls Tokens per Sec:     1944 || Batch Translation Loss:   0.128885 => Txt Tokens per Sec:     5478 || Lr: 0.000100
2024-02-03 06:28:07,740 Epoch 390: Total Training Recognition Loss 0.17  Total Training Translation Loss 4.09 
2024-02-03 06:28:07,741 EPOCH 391
2024-02-03 06:28:13,098 Epoch 391: Total Training Recognition Loss 0.14  Total Training Translation Loss 5.64 
2024-02-03 06:28:13,099 EPOCH 392
2024-02-03 06:28:13,275 [Epoch: 392 Step: 00026200] Batch Recognition Loss:   0.000731 => Gls Tokens per Sec:     2743 || Batch Translation Loss:   0.041836 => Txt Tokens per Sec:     6909 || Lr: 0.000100
2024-02-03 06:28:18,596 Epoch 392: Total Training Recognition Loss 0.19  Total Training Translation Loss 6.30 
2024-02-03 06:28:18,597 EPOCH 393
2024-02-03 06:28:21,512 [Epoch: 393 Step: 00026300] Batch Recognition Loss:   0.006024 => Gls Tokens per Sec:     1976 || Batch Translation Loss:   0.199516 => Txt Tokens per Sec:     5580 || Lr: 0.000100
2024-02-03 06:28:23,956 Epoch 393: Total Training Recognition Loss 0.16  Total Training Translation Loss 9.30 
2024-02-03 06:28:23,956 EPOCH 394
2024-02-03 06:28:29,407 Epoch 394: Total Training Recognition Loss 0.26  Total Training Translation Loss 10.77 
2024-02-03 06:28:29,408 EPOCH 395
2024-02-03 06:28:29,612 [Epoch: 395 Step: 00026400] Batch Recognition Loss:   0.014794 => Gls Tokens per Sec:     1571 || Batch Translation Loss:   0.524564 => Txt Tokens per Sec:     5318 || Lr: 0.000100
2024-02-03 06:28:34,894 Epoch 395: Total Training Recognition Loss 0.22  Total Training Translation Loss 8.57 
2024-02-03 06:28:34,895 EPOCH 396
2024-02-03 06:28:37,809 [Epoch: 396 Step: 00026500] Batch Recognition Loss:   0.000792 => Gls Tokens per Sec:     1923 || Batch Translation Loss:   0.264130 => Txt Tokens per Sec:     5355 || Lr: 0.000100
2024-02-03 06:28:40,485 Epoch 396: Total Training Recognition Loss 0.20  Total Training Translation Loss 8.48 
2024-02-03 06:28:40,486 EPOCH 397
2024-02-03 06:28:45,829 Epoch 397: Total Training Recognition Loss 0.27  Total Training Translation Loss 10.14 
2024-02-03 06:28:45,829 EPOCH 398
2024-02-03 06:28:45,896 [Epoch: 398 Step: 00026600] Batch Recognition Loss:   0.002168 => Gls Tokens per Sec:     2424 || Batch Translation Loss:   0.056071 => Txt Tokens per Sec:     7636 || Lr: 0.000100
2024-02-03 06:28:51,224 Epoch 398: Total Training Recognition Loss 0.26  Total Training Translation Loss 8.29 
2024-02-03 06:28:51,224 EPOCH 399
2024-02-03 06:28:53,414 [Epoch: 399 Step: 00026700] Batch Recognition Loss:   0.000991 => Gls Tokens per Sec:     2485 || Batch Translation Loss:   0.053082 => Txt Tokens per Sec:     6871 || Lr: 0.000100
2024-02-03 06:28:56,219 Epoch 399: Total Training Recognition Loss 0.41  Total Training Translation Loss 6.50 
2024-02-03 06:28:56,220 EPOCH 400
2024-02-03 06:29:01,729 [Epoch: 400 Step: 00026800] Batch Recognition Loss:   0.001180 => Gls Tokens per Sec:     1930 || Batch Translation Loss:   0.076698 => Txt Tokens per Sec:     5367 || Lr: 0.000100
2024-02-03 06:29:01,730 Epoch 400: Total Training Recognition Loss 0.29  Total Training Translation Loss 8.98 
2024-02-03 06:29:01,730 EPOCH 401
2024-02-03 06:29:06,902 Epoch 401: Total Training Recognition Loss 0.25  Total Training Translation Loss 6.04 
2024-02-03 06:29:06,903 EPOCH 402
2024-02-03 06:29:09,406 [Epoch: 402 Step: 00026900] Batch Recognition Loss:   0.005047 => Gls Tokens per Sec:     2075 || Batch Translation Loss:   0.143803 => Txt Tokens per Sec:     5828 || Lr: 0.000100
2024-02-03 06:29:12,123 Epoch 402: Total Training Recognition Loss 0.24  Total Training Translation Loss 4.91 
2024-02-03 06:29:12,123 EPOCH 403
2024-02-03 06:29:16,671 [Epoch: 403 Step: 00027000] Batch Recognition Loss:   0.003743 => Gls Tokens per Sec:     2302 || Batch Translation Loss:   0.033335 => Txt Tokens per Sec:     6438 || Lr: 0.000100
2024-02-03 06:29:16,712 Epoch 403: Total Training Recognition Loss 0.23  Total Training Translation Loss 3.40 
2024-02-03 06:29:16,712 EPOCH 404
2024-02-03 06:29:21,261 Epoch 404: Total Training Recognition Loss 0.18  Total Training Translation Loss 4.43 
2024-02-03 06:29:21,261 EPOCH 405
2024-02-03 06:29:23,399 [Epoch: 405 Step: 00027100] Batch Recognition Loss:   0.008564 => Gls Tokens per Sec:     2354 || Batch Translation Loss:   0.020985 => Txt Tokens per Sec:     6339 || Lr: 0.000100
2024-02-03 06:29:26,613 Epoch 405: Total Training Recognition Loss 0.23  Total Training Translation Loss 6.23 
2024-02-03 06:29:26,613 EPOCH 406
2024-02-03 06:29:31,729 [Epoch: 406 Step: 00027200] Batch Recognition Loss:   0.003236 => Gls Tokens per Sec:     2033 || Batch Translation Loss:   0.020710 => Txt Tokens per Sec:     5692 || Lr: 0.000100
2024-02-03 06:29:31,921 Epoch 406: Total Training Recognition Loss 0.27  Total Training Translation Loss 6.92 
2024-02-03 06:29:31,921 EPOCH 407
2024-02-03 06:29:37,207 Epoch 407: Total Training Recognition Loss 0.23  Total Training Translation Loss 10.40 
2024-02-03 06:29:37,207 EPOCH 408
2024-02-03 06:29:39,325 [Epoch: 408 Step: 00027300] Batch Recognition Loss:   0.011806 => Gls Tokens per Sec:     2302 || Batch Translation Loss:   0.309843 => Txt Tokens per Sec:     6313 || Lr: 0.000100
2024-02-03 06:29:42,360 Epoch 408: Total Training Recognition Loss 0.26  Total Training Translation Loss 9.76 
2024-02-03 06:29:42,361 EPOCH 409
2024-02-03 06:29:47,338 [Epoch: 409 Step: 00027400] Batch Recognition Loss:   0.011470 => Gls Tokens per Sec:     2040 || Batch Translation Loss:   0.058566 => Txt Tokens per Sec:     5677 || Lr: 0.000100
2024-02-03 06:29:47,586 Epoch 409: Total Training Recognition Loss 0.23  Total Training Translation Loss 6.93 
2024-02-03 06:29:47,586 EPOCH 410
2024-02-03 06:29:52,659 Epoch 410: Total Training Recognition Loss 0.20  Total Training Translation Loss 6.47 
2024-02-03 06:29:52,659 EPOCH 411
2024-02-03 06:29:54,995 [Epoch: 411 Step: 00027500] Batch Recognition Loss:   0.000506 => Gls Tokens per Sec:     2017 || Batch Translation Loss:   0.059383 => Txt Tokens per Sec:     5585 || Lr: 0.000100
2024-02-03 06:29:57,814 Epoch 411: Total Training Recognition Loss 0.18  Total Training Translation Loss 6.47 
2024-02-03 06:29:57,814 EPOCH 412
2024-02-03 06:30:02,546 [Epoch: 412 Step: 00027600] Batch Recognition Loss:   0.000418 => Gls Tokens per Sec:     2131 || Batch Translation Loss:   0.029376 => Txt Tokens per Sec:     5860 || Lr: 0.000100
2024-02-03 06:30:03,028 Epoch 412: Total Training Recognition Loss 0.22  Total Training Translation Loss 4.98 
2024-02-03 06:30:03,028 EPOCH 413
2024-02-03 06:30:08,219 Epoch 413: Total Training Recognition Loss 0.17  Total Training Translation Loss 4.43 
2024-02-03 06:30:08,219 EPOCH 414
2024-02-03 06:30:10,307 [Epoch: 414 Step: 00027700] Batch Recognition Loss:   0.000492 => Gls Tokens per Sec:     2223 || Batch Translation Loss:   0.061340 => Txt Tokens per Sec:     6015 || Lr: 0.000100
2024-02-03 06:30:13,715 Epoch 414: Total Training Recognition Loss 0.16  Total Training Translation Loss 4.78 
2024-02-03 06:30:13,715 EPOCH 415
2024-02-03 06:30:18,087 [Epoch: 415 Step: 00027800] Batch Recognition Loss:   0.006712 => Gls Tokens per Sec:     2248 || Batch Translation Loss:   0.091081 => Txt Tokens per Sec:     6259 || Lr: 0.000100
2024-02-03 06:30:18,600 Epoch 415: Total Training Recognition Loss 0.22  Total Training Translation Loss 7.17 
2024-02-03 06:30:18,601 EPOCH 416
2024-02-03 06:30:23,910 Epoch 416: Total Training Recognition Loss 0.21  Total Training Translation Loss 8.13 
2024-02-03 06:30:23,911 EPOCH 417
2024-02-03 06:30:25,742 [Epoch: 417 Step: 00027900] Batch Recognition Loss:   0.008818 => Gls Tokens per Sec:     2447 || Batch Translation Loss:   0.137011 => Txt Tokens per Sec:     6843 || Lr: 0.000100
2024-02-03 06:30:29,048 Epoch 417: Total Training Recognition Loss 0.25  Total Training Translation Loss 8.98 
2024-02-03 06:30:29,049 EPOCH 418
2024-02-03 06:30:33,826 [Epoch: 418 Step: 00028000] Batch Recognition Loss:   0.001238 => Gls Tokens per Sec:     2025 || Batch Translation Loss:   0.048223 => Txt Tokens per Sec:     5606 || Lr: 0.000100
2024-02-03 06:30:42,680 Validation result at epoch 418, step    28000: duration: 8.8533s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 5.54514	Translation Loss: 88655.45312	PPL: 9977.48145
	Eval Metric: BLEU
	WER 3.96	(DEL: 0.00,	INS: 0.00,	SUB: 3.96)
	BLEU-4 0.57	(BLEU-1: 10.24,	BLEU-2: 2.69,	BLEU-3: 1.13,	BLEU-4: 0.57)
	CHRF 16.94	ROUGE 8.59
2024-02-03 06:30:42,681 Logging Recognition and Translation Outputs
2024-02-03 06:30:42,681 ========================================================================================================================
2024-02-03 06:30:42,681 Logging Sequence: 79_33.00
2024-02-03 06:30:42,681 	Gloss Reference :	A B+C+D+E
2024-02-03 06:30:42,681 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:30:42,682 	Gloss Alignment :	         
2024-02-03 06:30:42,682 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:30:42,683 	Text Reference  :	result in cancellation and other hassles this  is   something the    bcci does    not want  and   is      taking all precautions
2024-02-03 06:30:42,684 	Text Hypothesis :	****** in ************ *** ***** the     photo went viral     people not  allowed to  drink water instead of     its minutes    
2024-02-03 06:30:42,684 	Text Alignment  :	D         D            D   D     S       S     S    S         S      S    S       S   S     S     S       S      S   S          
2024-02-03 06:30:42,684 ========================================================================================================================
2024-02-03 06:30:42,684 Logging Sequence: 113_75.00
2024-02-03 06:30:42,684 	Gloss Reference :	A B+C+D+E
2024-02-03 06:30:42,684 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:30:42,684 	Gloss Alignment :	         
2024-02-03 06:30:42,684 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:30:42,685 	Text Reference  :	mirza responded 'œpeople can date a person for years but break up  as     soon as   they get    married   
2024-02-03 06:30:42,686 	Text Hypothesis :	***** ********* ******** *** **** * ****** *** ***** *** if    her father is   high the  indian cricketers
2024-02-03 06:30:42,686 	Text Alignment  :	D     D         D        D   D    D D      D   D     D   S     S   S      S    S    S    S      S         
2024-02-03 06:30:42,686 ========================================================================================================================
2024-02-03 06:30:42,686 Logging Sequence: 96_165.00
2024-02-03 06:30:42,686 	Gloss Reference :	A B+C+D+E  
2024-02-03 06:30:42,686 	Gloss Hypothesis:	A B+E+C+D+E
2024-02-03 06:30:42,686 	Gloss Alignment :	  S        
2024-02-03 06:30:42,686 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:30:42,687 	Text Reference  :	7 runs were needed from 6  balls and india had    lost a   wicket
2024-02-03 06:30:42,687 	Text Hypothesis :	* **** **** ****** **** it is    was now   people to   see him   
2024-02-03 06:30:42,687 	Text Alignment  :	D D    D    D      D    S  S     S   S     S      S    S   S     
2024-02-03 06:30:42,688 ========================================================================================================================
2024-02-03 06:30:42,688 Logging Sequence: 91_133.00
2024-02-03 06:30:42,688 	Gloss Reference :	A B+C+D+E
2024-02-03 06:30:42,688 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:30:42,688 	Gloss Alignment :	         
2024-02-03 06:30:42,688 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:30:42,689 	Text Reference  :	all the   umpires in the tournament were    from     bangalesh'
2024-02-03 06:30:42,689 	Text Hypothesis :	*** there needs   to be  a          neutral umpiring system    
2024-02-03 06:30:42,689 	Text Alignment  :	D   S     S       S  S   S          S       S        S         
2024-02-03 06:30:42,689 ========================================================================================================================
2024-02-03 06:30:42,689 Logging Sequence: 105_139.00
2024-02-03 06:30:42,689 	Gloss Reference :	A B+C+D+E
2024-02-03 06:30:42,690 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:30:42,690 	Gloss Alignment :	         
2024-02-03 06:30:42,690 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:30:42,691 	Text Reference  :	** *** **** *** ********** ************** and  now he         has finally achieved his  dream   by defeating carlsen  
2024-02-03 06:30:42,691 	Text Hypothesis :	in the year old rameshbabu praggnanandhaa from the tournament and it      was      just because of a         wonderful
2024-02-03 06:30:42,691 	Text Alignment  :	I  I   I    I   I          I              S    S   S          S   S       S        S    S       S  S         S        
2024-02-03 06:30:42,691 ========================================================================================================================
2024-02-03 06:30:43,154 Epoch 418: Total Training Recognition Loss 0.24  Total Training Translation Loss 7.56 
2024-02-03 06:30:43,154 EPOCH 419
2024-02-03 06:30:48,496 Epoch 419: Total Training Recognition Loss 0.20  Total Training Translation Loss 6.13 
2024-02-03 06:30:48,496 EPOCH 420
2024-02-03 06:30:50,486 [Epoch: 420 Step: 00028100] Batch Recognition Loss:   0.000836 => Gls Tokens per Sec:     2127 || Batch Translation Loss:   0.024610 => Txt Tokens per Sec:     6047 || Lr: 0.000100
2024-02-03 06:30:53,615 Epoch 420: Total Training Recognition Loss 0.14  Total Training Translation Loss 5.39 
2024-02-03 06:30:53,616 EPOCH 421
2024-02-03 06:30:58,180 [Epoch: 421 Step: 00028200] Batch Recognition Loss:   0.000800 => Gls Tokens per Sec:     2084 || Batch Translation Loss:   0.058237 => Txt Tokens per Sec:     5787 || Lr: 0.000100
2024-02-03 06:30:58,742 Epoch 421: Total Training Recognition Loss 0.16  Total Training Translation Loss 7.07 
2024-02-03 06:30:58,742 EPOCH 422
2024-02-03 06:31:04,375 Epoch 422: Total Training Recognition Loss 0.19  Total Training Translation Loss 7.77 
2024-02-03 06:31:04,375 EPOCH 423
2024-02-03 06:31:06,421 [Epoch: 423 Step: 00028300] Batch Recognition Loss:   0.001848 => Gls Tokens per Sec:     1991 || Batch Translation Loss:   0.183533 => Txt Tokens per Sec:     5394 || Lr: 0.000100
2024-02-03 06:31:09,574 Epoch 423: Total Training Recognition Loss 0.26  Total Training Translation Loss 7.52 
2024-02-03 06:31:09,574 EPOCH 424
2024-02-03 06:31:14,398 [Epoch: 424 Step: 00028400] Batch Recognition Loss:   0.004541 => Gls Tokens per Sec:     1939 || Batch Translation Loss:   0.028176 => Txt Tokens per Sec:     5428 || Lr: 0.000100
2024-02-03 06:31:14,902 Epoch 424: Total Training Recognition Loss 0.19  Total Training Translation Loss 6.70 
2024-02-03 06:31:14,902 EPOCH 425
2024-02-03 06:31:19,622 Epoch 425: Total Training Recognition Loss 0.17  Total Training Translation Loss 7.27 
2024-02-03 06:31:19,622 EPOCH 426
2024-02-03 06:31:21,648 [Epoch: 426 Step: 00028500] Batch Recognition Loss:   0.003221 => Gls Tokens per Sec:     1975 || Batch Translation Loss:   0.073116 => Txt Tokens per Sec:     5377 || Lr: 0.000100
2024-02-03 06:31:25,058 Epoch 426: Total Training Recognition Loss 0.21  Total Training Translation Loss 5.28 
2024-02-03 06:31:25,058 EPOCH 427
2024-02-03 06:31:29,483 [Epoch: 427 Step: 00028600] Batch Recognition Loss:   0.005938 => Gls Tokens per Sec:     2098 || Batch Translation Loss:   0.098158 => Txt Tokens per Sec:     5922 || Lr: 0.000100
2024-02-03 06:31:30,162 Epoch 427: Total Training Recognition Loss 0.20  Total Training Translation Loss 7.49 
2024-02-03 06:31:30,163 EPOCH 428
2024-02-03 06:31:35,539 Epoch 428: Total Training Recognition Loss 0.25  Total Training Translation Loss 6.96 
2024-02-03 06:31:35,539 EPOCH 429
2024-02-03 06:31:37,110 [Epoch: 429 Step: 00028700] Batch Recognition Loss:   0.001332 => Gls Tokens per Sec:     2389 || Batch Translation Loss:   0.027578 => Txt Tokens per Sec:     6624 || Lr: 0.000100
2024-02-03 06:31:40,554 Epoch 429: Total Training Recognition Loss 0.24  Total Training Translation Loss 4.48 
2024-02-03 06:31:40,555 EPOCH 430
2024-02-03 06:31:44,947 [Epoch: 430 Step: 00028800] Batch Recognition Loss:   0.008498 => Gls Tokens per Sec:     2057 || Batch Translation Loss:   0.061888 => Txt Tokens per Sec:     5725 || Lr: 0.000100
2024-02-03 06:31:45,707 Epoch 430: Total Training Recognition Loss 0.18  Total Training Translation Loss 5.85 
2024-02-03 06:31:45,707 EPOCH 431
2024-02-03 06:31:51,099 Epoch 431: Total Training Recognition Loss 0.16  Total Training Translation Loss 6.34 
2024-02-03 06:31:51,100 EPOCH 432
2024-02-03 06:31:52,599 [Epoch: 432 Step: 00028900] Batch Recognition Loss:   0.001802 => Gls Tokens per Sec:     2457 || Batch Translation Loss:   0.026624 => Txt Tokens per Sec:     6502 || Lr: 0.000100
2024-02-03 06:31:55,973 Epoch 432: Total Training Recognition Loss 0.19  Total Training Translation Loss 6.92 
2024-02-03 06:31:55,973 EPOCH 433
2024-02-03 06:32:00,757 [Epoch: 433 Step: 00029000] Batch Recognition Loss:   0.000564 => Gls Tokens per Sec:     1873 || Batch Translation Loss:   0.074264 => Txt Tokens per Sec:     5223 || Lr: 0.000100
2024-02-03 06:32:01,607 Epoch 433: Total Training Recognition Loss 0.22  Total Training Translation Loss 9.41 
2024-02-03 06:32:01,607 EPOCH 434
2024-02-03 06:32:06,724 Epoch 434: Total Training Recognition Loss 0.27  Total Training Translation Loss 10.75 
2024-02-03 06:32:06,725 EPOCH 435
2024-02-03 06:32:08,397 [Epoch: 435 Step: 00029100] Batch Recognition Loss:   0.011604 => Gls Tokens per Sec:     2107 || Batch Translation Loss:   0.181018 => Txt Tokens per Sec:     5806 || Lr: 0.000100
2024-02-03 06:32:11,965 Epoch 435: Total Training Recognition Loss 0.32  Total Training Translation Loss 19.81 
2024-02-03 06:32:11,965 EPOCH 436
2024-02-03 06:32:16,529 [Epoch: 436 Step: 00029200] Batch Recognition Loss:   0.006115 => Gls Tokens per Sec:     1909 || Batch Translation Loss:   0.049776 => Txt Tokens per Sec:     5273 || Lr: 0.000100
2024-02-03 06:32:17,494 Epoch 436: Total Training Recognition Loss 0.25  Total Training Translation Loss 7.34 
2024-02-03 06:32:17,494 EPOCH 437
2024-02-03 06:32:22,332 Epoch 437: Total Training Recognition Loss 0.15  Total Training Translation Loss 5.53 
2024-02-03 06:32:22,332 EPOCH 438
2024-02-03 06:32:24,370 [Epoch: 438 Step: 00029300] Batch Recognition Loss:   0.000759 => Gls Tokens per Sec:     1605 || Batch Translation Loss:   0.058608 => Txt Tokens per Sec:     4614 || Lr: 0.000100
2024-02-03 06:32:27,706 Epoch 438: Total Training Recognition Loss 0.20  Total Training Translation Loss 5.09 
2024-02-03 06:32:27,706 EPOCH 439
2024-02-03 06:32:31,610 [Epoch: 439 Step: 00029400] Batch Recognition Loss:   0.002282 => Gls Tokens per Sec:     2191 || Batch Translation Loss:   0.120928 => Txt Tokens per Sec:     6140 || Lr: 0.000100
2024-02-03 06:32:32,435 Epoch 439: Total Training Recognition Loss 0.14  Total Training Translation Loss 3.43 
2024-02-03 06:32:32,436 EPOCH 440
2024-02-03 06:32:37,963 Epoch 440: Total Training Recognition Loss 0.16  Total Training Translation Loss 4.04 
2024-02-03 06:32:37,964 EPOCH 441
2024-02-03 06:32:39,640 [Epoch: 441 Step: 00029500] Batch Recognition Loss:   0.001347 => Gls Tokens per Sec:     1856 || Batch Translation Loss:   0.026511 => Txt Tokens per Sec:     5282 || Lr: 0.000100
2024-02-03 06:32:43,151 Epoch 441: Total Training Recognition Loss 0.17  Total Training Translation Loss 4.30 
2024-02-03 06:32:43,152 EPOCH 442
2024-02-03 06:32:47,308 [Epoch: 442 Step: 00029600] Batch Recognition Loss:   0.002776 => Gls Tokens per Sec:     2019 || Batch Translation Loss:   0.088475 => Txt Tokens per Sec:     5618 || Lr: 0.000100
2024-02-03 06:32:48,422 Epoch 442: Total Training Recognition Loss 0.15  Total Training Translation Loss 6.80 
2024-02-03 06:32:48,422 EPOCH 443
2024-02-03 06:32:53,438 Epoch 443: Total Training Recognition Loss 0.33  Total Training Translation Loss 6.18 
2024-02-03 06:32:53,439 EPOCH 444
2024-02-03 06:32:55,046 [Epoch: 444 Step: 00029700] Batch Recognition Loss:   0.001202 => Gls Tokens per Sec:     1893 || Batch Translation Loss:   0.034548 => Txt Tokens per Sec:     5481 || Lr: 0.000100
2024-02-03 06:32:58,705 Epoch 444: Total Training Recognition Loss 0.15  Total Training Translation Loss 3.99 
2024-02-03 06:32:58,705 EPOCH 445
2024-02-03 06:33:02,952 [Epoch: 445 Step: 00029800] Batch Recognition Loss:   0.002135 => Gls Tokens per Sec:     1939 || Batch Translation Loss:   0.026622 => Txt Tokens per Sec:     5423 || Lr: 0.000100
2024-02-03 06:33:04,139 Epoch 445: Total Training Recognition Loss 0.14  Total Training Translation Loss 3.96 
2024-02-03 06:33:04,139 EPOCH 446
2024-02-03 06:33:08,863 Epoch 446: Total Training Recognition Loss 0.12  Total Training Translation Loss 4.28 
2024-02-03 06:33:08,863 EPOCH 447
2024-02-03 06:33:10,107 [Epoch: 447 Step: 00029900] Batch Recognition Loss:   0.000807 => Gls Tokens per Sec:     2317 || Batch Translation Loss:   0.053961 => Txt Tokens per Sec:     6159 || Lr: 0.000100
2024-02-03 06:33:14,238 Epoch 447: Total Training Recognition Loss 0.18  Total Training Translation Loss 5.28 
2024-02-03 06:33:14,239 EPOCH 448
2024-02-03 06:33:17,998 [Epoch: 448 Step: 00030000] Batch Recognition Loss:   0.000527 => Gls Tokens per Sec:     2147 || Batch Translation Loss:   0.055916 => Txt Tokens per Sec:     6120 || Lr: 0.000100
2024-02-03 06:33:26,573 Validation result at epoch 448, step    30000: duration: 8.5745s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 5.37066	Translation Loss: 88564.58594	PPL: 9883.76172
	Eval Metric: BLEU
	WER 3.54	(DEL: 0.07,	INS: 0.00,	SUB: 3.47)
	BLEU-4 0.64	(BLEU-1: 10.43,	BLEU-2: 3.23,	BLEU-3: 1.31,	BLEU-4: 0.64)
	CHRF 16.80	ROUGE 8.93
2024-02-03 06:33:26,575 Logging Recognition and Translation Outputs
2024-02-03 06:33:26,575 ========================================================================================================================
2024-02-03 06:33:26,575 Logging Sequence: 177_16.00
2024-02-03 06:33:26,575 	Gloss Reference :	A B+C+D+E
2024-02-03 06:33:26,576 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:33:26,576 	Gloss Alignment :	         
2024-02-03 06:33:26,576 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:33:26,576 	Text Reference  :	***** *** ********** ******* *** ****** ****** ****** and severely injured 2    others
2024-02-03 06:33:26,577 	Text Hypothesis :	after the infections reduced the indian origin player was as       the     only time  
2024-02-03 06:33:26,577 	Text Alignment  :	I     I   I          I       I   I      I      I      S   S        S       S    S     
2024-02-03 06:33:26,577 ========================================================================================================================
2024-02-03 06:33:26,577 Logging Sequence: 165_200.00
2024-02-03 06:33:26,577 	Gloss Reference :	A B+C+D+E
2024-02-03 06:33:26,577 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:33:26,577 	Gloss Alignment :	         
2024-02-03 06:33:26,578 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:33:26,579 	Text Reference  :	you may be       wondering what     bag  remember in   2011    when india won     the world     cup   
2024-02-03 06:33:26,579 	Text Hypothesis :	*** *** whenever he        believed that his      team members to   his   batting and captaincy skills
2024-02-03 06:33:26,579 	Text Alignment  :	D   D   S        S         S        S    S        S    S       S    S     S       S   S         S     
2024-02-03 06:33:26,579 ========================================================================================================================
2024-02-03 06:33:26,579 Logging Sequence: 125_119.00
2024-02-03 06:33:26,580 	Gloss Reference :	A B+C+D+E
2024-02-03 06:33:26,580 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:33:26,580 	Gloss Alignment :	         
2024-02-03 06:33:26,580 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:33:26,581 	Text Reference  :	***** *** people must  not   post  such baseless comments  on   social media
2024-02-03 06:33:26,581 	Text Hypothesis :	after the match  virat kohli tried to   be       completed with the    loss 
2024-02-03 06:33:26,581 	Text Alignment  :	I     I   S      S     S     S     S    S        S         S    S      S    
2024-02-03 06:33:26,581 ========================================================================================================================
2024-02-03 06:33:26,581 Logging Sequence: 53_25.00
2024-02-03 06:33:26,582 	Gloss Reference :	A B+C+D+E
2024-02-03 06:33:26,582 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:33:26,582 	Gloss Alignment :	         
2024-02-03 06:33:26,582 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:33:26,583 	Text Reference  :	there is absolute chaos on the country as  residents are trying    to flee 
2024-02-03 06:33:26,583 	Text Hypothesis :	it    is not      known as the ******* t20 world     cup scheduled to start
2024-02-03 06:33:26,583 	Text Alignment  :	S        S        S     S      D       S   S         S   S            S    
2024-02-03 06:33:26,583 ========================================================================================================================
2024-02-03 06:33:26,584 Logging Sequence: 70_81.00
2024-02-03 06:33:26,584 	Gloss Reference :	A B+C+D+E
2024-02-03 06:33:26,584 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:33:26,584 	Gloss Alignment :	         
2024-02-03 06:33:26,584 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:33:26,585 	Text Reference  :	euro 2020 has many sponsors but coca-cola  is      the ***** official sponsor for the tournament
2024-02-03 06:33:26,585 	Text Hypothesis :	**** **** *** **** after    the infections reduced the brawl took     place   in  the mic       
2024-02-03 06:33:26,586 	Text Alignment  :	D    D    D   D    S        S   S          S           I     S        S       S       S         
2024-02-03 06:33:26,586 ========================================================================================================================
2024-02-03 06:33:27,780 Epoch 448: Total Training Recognition Loss 0.09  Total Training Translation Loss 6.41 
2024-02-03 06:33:27,781 EPOCH 449
2024-02-03 06:33:33,084 Epoch 449: Total Training Recognition Loss 0.18  Total Training Translation Loss 6.11 
2024-02-03 06:33:33,084 EPOCH 450
2024-02-03 06:33:34,262 [Epoch: 450 Step: 00030100] Batch Recognition Loss:   0.001639 => Gls Tokens per Sec:     2234 || Batch Translation Loss:   0.063618 => Txt Tokens per Sec:     5932 || Lr: 0.000100
2024-02-03 06:33:38,400 Epoch 450: Total Training Recognition Loss 0.20  Total Training Translation Loss 7.51 
2024-02-03 06:33:38,401 EPOCH 451
2024-02-03 06:33:42,088 [Epoch: 451 Step: 00030200] Batch Recognition Loss:   0.001651 => Gls Tokens per Sec:     2146 || Batch Translation Loss:   0.023128 => Txt Tokens per Sec:     5880 || Lr: 0.000100
2024-02-03 06:33:43,752 Epoch 451: Total Training Recognition Loss 0.18  Total Training Translation Loss 9.02 
2024-02-03 06:33:43,753 EPOCH 452
2024-02-03 06:33:49,032 Epoch 452: Total Training Recognition Loss 0.16  Total Training Translation Loss 6.37 
2024-02-03 06:33:49,032 EPOCH 453
2024-02-03 06:33:50,165 [Epoch: 453 Step: 00030300] Batch Recognition Loss:   0.001074 => Gls Tokens per Sec:     2263 || Batch Translation Loss:   0.033777 => Txt Tokens per Sec:     6304 || Lr: 0.000100
2024-02-03 06:33:54,430 Epoch 453: Total Training Recognition Loss 0.16  Total Training Translation Loss 6.93 
2024-02-03 06:33:54,431 EPOCH 454
2024-02-03 06:33:58,403 [Epoch: 454 Step: 00030400] Batch Recognition Loss:   0.013049 => Gls Tokens per Sec:     1952 || Batch Translation Loss:   0.054233 => Txt Tokens per Sec:     5504 || Lr: 0.000100
2024-02-03 06:33:59,752 Epoch 454: Total Training Recognition Loss 0.17  Total Training Translation Loss 4.28 
2024-02-03 06:33:59,752 EPOCH 455
2024-02-03 06:34:05,220 Epoch 455: Total Training Recognition Loss 0.11  Total Training Translation Loss 4.77 
2024-02-03 06:34:05,221 EPOCH 456
2024-02-03 06:34:06,187 [Epoch: 456 Step: 00030500] Batch Recognition Loss:   0.000239 => Gls Tokens per Sec:     2485 || Batch Translation Loss:   0.064457 => Txt Tokens per Sec:     6844 || Lr: 0.000100
2024-02-03 06:34:10,291 Epoch 456: Total Training Recognition Loss 0.15  Total Training Translation Loss 7.51 
2024-02-03 06:34:10,292 EPOCH 457
2024-02-03 06:34:14,194 [Epoch: 457 Step: 00030600] Batch Recognition Loss:   0.010072 => Gls Tokens per Sec:     1945 || Batch Translation Loss:   0.058900 => Txt Tokens per Sec:     5425 || Lr: 0.000100
2024-02-03 06:34:15,552 Epoch 457: Total Training Recognition Loss 0.22  Total Training Translation Loss 11.75 
2024-02-03 06:34:15,552 EPOCH 458
2024-02-03 06:34:20,187 Epoch 458: Total Training Recognition Loss 0.27  Total Training Translation Loss 13.11 
2024-02-03 06:34:20,188 EPOCH 459
2024-02-03 06:34:21,250 [Epoch: 459 Step: 00030700] Batch Recognition Loss:   0.002316 => Gls Tokens per Sec:     2110 || Batch Translation Loss:   0.048763 => Txt Tokens per Sec:     6137 || Lr: 0.000100
2024-02-03 06:34:24,927 Epoch 459: Total Training Recognition Loss 0.25  Total Training Translation Loss 5.09 
2024-02-03 06:34:24,927 EPOCH 460
2024-02-03 06:34:28,702 [Epoch: 460 Step: 00030800] Batch Recognition Loss:   0.000956 => Gls Tokens per Sec:     1969 || Batch Translation Loss:   0.027525 => Txt Tokens per Sec:     5395 || Lr: 0.000100
2024-02-03 06:34:30,383 Epoch 460: Total Training Recognition Loss 0.20  Total Training Translation Loss 3.94 
2024-02-03 06:34:30,383 EPOCH 461
2024-02-03 06:34:35,622 Epoch 461: Total Training Recognition Loss 0.13  Total Training Translation Loss 3.93 
2024-02-03 06:34:35,623 EPOCH 462
2024-02-03 06:34:36,541 [Epoch: 462 Step: 00030900] Batch Recognition Loss:   0.015204 => Gls Tokens per Sec:     2270 || Batch Translation Loss:   0.041909 => Txt Tokens per Sec:     6194 || Lr: 0.000100
2024-02-03 06:34:40,930 Epoch 462: Total Training Recognition Loss 0.14  Total Training Translation Loss 4.36 
2024-02-03 06:34:40,930 EPOCH 463
2024-02-03 06:34:45,026 [Epoch: 463 Step: 00031000] Batch Recognition Loss:   0.000953 => Gls Tokens per Sec:     1775 || Batch Translation Loss:   0.045902 => Txt Tokens per Sec:     4965 || Lr: 0.000100
2024-02-03 06:34:46,475 Epoch 463: Total Training Recognition Loss 0.16  Total Training Translation Loss 4.76 
2024-02-03 06:34:46,475 EPOCH 464
2024-02-03 06:34:51,064 Epoch 464: Total Training Recognition Loss 0.12  Total Training Translation Loss 3.53 
2024-02-03 06:34:51,064 EPOCH 465
2024-02-03 06:34:51,780 [Epoch: 465 Step: 00031100] Batch Recognition Loss:   0.003467 => Gls Tokens per Sec:     2686 || Batch Translation Loss:   0.036146 => Txt Tokens per Sec:     7180 || Lr: 0.000100
2024-02-03 06:34:56,557 Epoch 465: Total Training Recognition Loss 0.12  Total Training Translation Loss 4.06 
2024-02-03 06:34:56,558 EPOCH 466
2024-02-03 06:35:00,091 [Epoch: 466 Step: 00031200] Batch Recognition Loss:   0.001209 => Gls Tokens per Sec:     2013 || Batch Translation Loss:   0.337281 => Txt Tokens per Sec:     5763 || Lr: 0.000100
2024-02-03 06:35:01,778 Epoch 466: Total Training Recognition Loss 0.13  Total Training Translation Loss 3.68 
2024-02-03 06:35:01,779 EPOCH 467
2024-02-03 06:35:07,057 Epoch 467: Total Training Recognition Loss 0.14  Total Training Translation Loss 6.17 
2024-02-03 06:35:07,057 EPOCH 468
2024-02-03 06:35:07,839 [Epoch: 468 Step: 00031300] Batch Recognition Loss:   0.000722 => Gls Tokens per Sec:     2255 || Batch Translation Loss:   0.038194 => Txt Tokens per Sec:     6407 || Lr: 0.000100
2024-02-03 06:35:12,385 Epoch 468: Total Training Recognition Loss 0.18  Total Training Translation Loss 6.13 
2024-02-03 06:35:12,385 EPOCH 469
2024-02-03 06:35:15,764 [Epoch: 469 Step: 00031400] Batch Recognition Loss:   0.002079 => Gls Tokens per Sec:     2058 || Batch Translation Loss:   0.047744 => Txt Tokens per Sec:     5548 || Lr: 0.000100
2024-02-03 06:35:17,570 Epoch 469: Total Training Recognition Loss 0.23  Total Training Translation Loss 8.64 
2024-02-03 06:35:17,571 EPOCH 470
2024-02-03 06:35:23,054 Epoch 470: Total Training Recognition Loss 0.19  Total Training Translation Loss 8.69 
2024-02-03 06:35:23,054 EPOCH 471
2024-02-03 06:35:23,804 [Epoch: 471 Step: 00031500] Batch Recognition Loss:   0.002096 => Gls Tokens per Sec:     2136 || Batch Translation Loss:   0.203664 => Txt Tokens per Sec:     6077 || Lr: 0.000100
2024-02-03 06:35:27,951 Epoch 471: Total Training Recognition Loss 0.23  Total Training Translation Loss 8.74 
2024-02-03 06:35:27,951 EPOCH 472
2024-02-03 06:35:31,294 [Epoch: 472 Step: 00031600] Batch Recognition Loss:   0.003103 => Gls Tokens per Sec:     2031 || Batch Translation Loss:   0.057338 => Txt Tokens per Sec:     5456 || Lr: 0.000100
2024-02-03 06:35:33,456 Epoch 472: Total Training Recognition Loss 0.30  Total Training Translation Loss 12.15 
2024-02-03 06:35:33,456 EPOCH 473
2024-02-03 06:35:38,596 Epoch 473: Total Training Recognition Loss 0.30  Total Training Translation Loss 9.37 
2024-02-03 06:35:38,597 EPOCH 474
2024-02-03 06:35:39,342 [Epoch: 474 Step: 00031700] Batch Recognition Loss:   0.030343 => Gls Tokens per Sec:     1936 || Batch Translation Loss:   0.107562 => Txt Tokens per Sec:     5830 || Lr: 0.000100
2024-02-03 06:35:43,754 Epoch 474: Total Training Recognition Loss 0.30  Total Training Translation Loss 7.03 
2024-02-03 06:35:43,754 EPOCH 475
2024-02-03 06:35:46,677 [Epoch: 475 Step: 00031800] Batch Recognition Loss:   0.000514 => Gls Tokens per Sec:     2300 || Batch Translation Loss:   0.046203 => Txt Tokens per Sec:     6454 || Lr: 0.000100
2024-02-03 06:35:48,529 Epoch 475: Total Training Recognition Loss 0.30  Total Training Translation Loss 5.81 
2024-02-03 06:35:48,530 EPOCH 476
2024-02-03 06:35:54,094 Epoch 476: Total Training Recognition Loss 0.25  Total Training Translation Loss 3.57 
2024-02-03 06:35:54,094 EPOCH 477
2024-02-03 06:35:54,695 [Epoch: 477 Step: 00031900] Batch Recognition Loss:   0.000736 => Gls Tokens per Sec:     2132 || Batch Translation Loss:   0.052092 => Txt Tokens per Sec:     6201 || Lr: 0.000100
2024-02-03 06:35:59,283 Epoch 477: Total Training Recognition Loss 0.13  Total Training Translation Loss 4.00 
2024-02-03 06:35:59,283 EPOCH 478
2024-02-03 06:36:02,748 [Epoch: 478 Step: 00032000] Batch Recognition Loss:   0.000229 => Gls Tokens per Sec:     1867 || Batch Translation Loss:   0.078054 => Txt Tokens per Sec:     5136 || Lr: 0.000100
2024-02-03 06:36:10,999 Validation result at epoch 478, step    32000: duration: 8.2495s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 5.49256	Translation Loss: 89665.95312	PPL: 11081.58301
	Eval Metric: BLEU
	WER 3.54	(DEL: 0.00,	INS: 0.00,	SUB: 3.54)
	BLEU-4 0.47	(BLEU-1: 10.68,	BLEU-2: 3.02,	BLEU-3: 1.03,	BLEU-4: 0.47)
	CHRF 17.07	ROUGE 8.85
2024-02-03 06:36:10,999 Logging Recognition and Translation Outputs
2024-02-03 06:36:11,000 ========================================================================================================================
2024-02-03 06:36:11,000 Logging Sequence: 73_57.00
2024-02-03 06:36:11,000 	Gloss Reference :	A B+C+D+E
2024-02-03 06:36:11,000 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:36:11,000 	Gloss Alignment :	         
2024-02-03 06:36:11,000 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:36:11,002 	Text Reference  :	he made   this announcement on instagram with a  photo which had     raina standing along with  the   chefs      at   restaurant
2024-02-03 06:36:11,003 	Text Hypothesis :	** people were shocked      by this      news he was   just  because of    his      bare  hands these sandwiches were only      
2024-02-03 06:36:11,003 	Text Alignment  :	D  S      S    S            S  S         S    S  S     S     S       S     S        S     S     S     S          S    S         
2024-02-03 06:36:11,003 ========================================================================================================================
2024-02-03 06:36:11,003 Logging Sequence: 177_16.00
2024-02-03 06:36:11,003 	Gloss Reference :	A B+C+D+E
2024-02-03 06:36:11,003 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:36:11,003 	Gloss Alignment :	         
2024-02-03 06:36:11,004 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:36:11,004 	Text Reference  :	******* ***** ** *********** ******* ** *** ****** and    severely injured 2   others
2024-02-03 06:36:11,004 	Text Hypothesis :	however after an interesting history on the indian origin arrest   from    the police
2024-02-03 06:36:11,005 	Text Alignment  :	I       I     I  I           I       I  I   I      S      S        S       S   S     
2024-02-03 06:36:11,005 ========================================================================================================================
2024-02-03 06:36:11,005 Logging Sequence: 124_134.00
2024-02-03 06:36:11,005 	Gloss Reference :	A B+C+D+E
2024-02-03 06:36:11,005 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:36:11,005 	Gloss Alignment :	         
2024-02-03 06:36:11,005 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:36:11,007 	Text Reference  :	fans are now wondering why he       did      not     say     anything about dhoni as he   is very           close  to  dhoni 
2024-02-03 06:36:11,007 	Text Hypothesis :	**** *** *** ********* *** farewell messages started pouring in       for   dhoni as well as congratulatory wishes for jadeja
2024-02-03 06:36:11,007 	Text Alignment  :	D    D   D   D         D   S        S        S       S       S        S              S    S  S              S      S   S     
2024-02-03 06:36:11,008 ========================================================================================================================
2024-02-03 06:36:11,008 Logging Sequence: 117_50.00
2024-02-03 06:36:11,008 	Gloss Reference :	A B+C+D+E
2024-02-03 06:36:11,008 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:36:11,008 	Gloss Alignment :	         
2024-02-03 06:36:11,008 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:36:11,009 	Text Reference  :	** ******** ** ****** this was   krunal pandya's maiden odi 
2024-02-03 06:36:11,009 	Text Hypothesis :	on speaking to decide the  times and    13th     july   2023
2024-02-03 06:36:11,009 	Text Alignment  :	I  I        I  I      S    S     S      S        S      S   
2024-02-03 06:36:11,009 ========================================================================================================================
2024-02-03 06:36:11,009 Logging Sequence: 136_55.00
2024-02-03 06:36:11,010 	Gloss Reference :	A B+C+D+E
2024-02-03 06:36:11,010 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:36:11,010 	Gloss Alignment :	         
2024-02-03 06:36:11,010 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:36:11,011 	Text Reference  :	****** ****** she      had     earlier won     a        silver at      the     rio  de     janeiro  olympics in  2016       
2024-02-03 06:36:11,012 	Text Hypothesis :	famous indian champion players like    kidambi srikanth and    ashwini ponappa have tested positive for      the semi-finals
2024-02-03 06:36:11,012 	Text Alignment  :	I      I      S        S       S       S       S        S      S       S       S    S      S        S        S   S          
2024-02-03 06:36:11,012 ========================================================================================================================
2024-02-03 06:36:13,162 Epoch 478: Total Training Recognition Loss 0.13  Total Training Translation Loss 3.93 
2024-02-03 06:36:13,162 EPOCH 479
2024-02-03 06:36:18,692 Epoch 479: Total Training Recognition Loss 0.13  Total Training Translation Loss 4.97 
2024-02-03 06:36:18,693 EPOCH 480
2024-02-03 06:36:19,048 [Epoch: 480 Step: 00032100] Batch Recognition Loss:   0.019742 => Gls Tokens per Sec:     3162 || Batch Translation Loss:   0.008583 => Txt Tokens per Sec:     7162 || Lr: 0.000100
2024-02-03 06:36:24,005 Epoch 480: Total Training Recognition Loss 0.18  Total Training Translation Loss 6.86 
2024-02-03 06:36:24,005 EPOCH 481
2024-02-03 06:36:27,034 [Epoch: 481 Step: 00032200] Batch Recognition Loss:   0.000440 => Gls Tokens per Sec:     2113 || Batch Translation Loss:   0.044391 => Txt Tokens per Sec:     5845 || Lr: 0.000100
2024-02-03 06:36:29,126 Epoch 481: Total Training Recognition Loss 0.15  Total Training Translation Loss 5.75 
2024-02-03 06:36:29,126 EPOCH 482
2024-02-03 06:36:34,267 Epoch 482: Total Training Recognition Loss 0.14  Total Training Translation Loss 6.50 
2024-02-03 06:36:34,267 EPOCH 483
2024-02-03 06:36:34,725 [Epoch: 483 Step: 00032300] Batch Recognition Loss:   0.000773 => Gls Tokens per Sec:     2101 || Batch Translation Loss:   0.043896 => Txt Tokens per Sec:     5853 || Lr: 0.000100
2024-02-03 06:36:39,623 Epoch 483: Total Training Recognition Loss 0.23  Total Training Translation Loss 8.06 
2024-02-03 06:36:39,623 EPOCH 484
2024-02-03 06:36:42,629 [Epoch: 484 Step: 00032400] Batch Recognition Loss:   0.008621 => Gls Tokens per Sec:     2076 || Batch Translation Loss:   0.031906 => Txt Tokens per Sec:     5715 || Lr: 0.000100
2024-02-03 06:36:44,889 Epoch 484: Total Training Recognition Loss 0.16  Total Training Translation Loss 6.75 
2024-02-03 06:36:44,890 EPOCH 485
2024-02-03 06:36:50,006 Epoch 485: Total Training Recognition Loss 0.16  Total Training Translation Loss 5.62 
2024-02-03 06:36:50,006 EPOCH 486
2024-02-03 06:36:50,408 [Epoch: 486 Step: 00032500] Batch Recognition Loss:   0.004271 => Gls Tokens per Sec:     1997 || Batch Translation Loss:   0.026371 => Txt Tokens per Sec:     5617 || Lr: 0.000100
2024-02-03 06:36:55,057 Epoch 486: Total Training Recognition Loss 0.13  Total Training Translation Loss 7.05 
2024-02-03 06:36:55,057 EPOCH 487
2024-02-03 06:36:57,787 [Epoch: 487 Step: 00032600] Batch Recognition Loss:   0.001200 => Gls Tokens per Sec:     2228 || Batch Translation Loss:   0.062217 => Txt Tokens per Sec:     6033 || Lr: 0.000100
2024-02-03 06:37:00,415 Epoch 487: Total Training Recognition Loss 0.18  Total Training Translation Loss 5.45 
2024-02-03 06:37:00,415 EPOCH 488
2024-02-03 06:37:05,479 Epoch 488: Total Training Recognition Loss 0.13  Total Training Translation Loss 5.59 
2024-02-03 06:37:05,479 EPOCH 489
2024-02-03 06:37:05,743 [Epoch: 489 Step: 00032700] Batch Recognition Loss:   0.000428 => Gls Tokens per Sec:     2433 || Batch Translation Loss:   0.040596 => Txt Tokens per Sec:     6913 || Lr: 0.000100
2024-02-03 06:37:10,949 Epoch 489: Total Training Recognition Loss 0.12  Total Training Translation Loss 4.90 
2024-02-03 06:37:10,950 EPOCH 490
2024-02-03 06:37:14,047 [Epoch: 490 Step: 00032800] Batch Recognition Loss:   0.002077 => Gls Tokens per Sec:     1883 || Batch Translation Loss:   0.051037 => Txt Tokens per Sec:     5291 || Lr: 0.000100
2024-02-03 06:37:16,446 Epoch 490: Total Training Recognition Loss 0.14  Total Training Translation Loss 3.86 
2024-02-03 06:37:16,446 EPOCH 491
2024-02-03 06:37:21,274 Epoch 491: Total Training Recognition Loss 0.18  Total Training Translation Loss 13.93 
2024-02-03 06:37:21,275 EPOCH 492
2024-02-03 06:37:21,466 [Epoch: 492 Step: 00032900] Batch Recognition Loss:   0.000902 => Gls Tokens per Sec:     2536 || Batch Translation Loss:   0.121293 => Txt Tokens per Sec:     5744 || Lr: 0.000100
2024-02-03 06:37:26,710 Epoch 492: Total Training Recognition Loss 0.26  Total Training Translation Loss 5.80 
2024-02-03 06:37:26,710 EPOCH 493
2024-02-03 06:37:29,411 [Epoch: 493 Step: 00033000] Batch Recognition Loss:   0.004070 => Gls Tokens per Sec:     2100 || Batch Translation Loss:   0.614351 => Txt Tokens per Sec:     5928 || Lr: 0.000100
2024-02-03 06:37:31,956 Epoch 493: Total Training Recognition Loss 0.21  Total Training Translation Loss 7.46 
2024-02-03 06:37:31,956 EPOCH 494
2024-02-03 06:37:37,426 Epoch 494: Total Training Recognition Loss 0.21  Total Training Translation Loss 5.84 
2024-02-03 06:37:37,426 EPOCH 495
2024-02-03 06:37:37,555 [Epoch: 495 Step: 00033100] Batch Recognition Loss:   0.000440 => Gls Tokens per Sec:     2500 || Batch Translation Loss:   0.050192 => Txt Tokens per Sec:     6414 || Lr: 0.000100
2024-02-03 06:37:42,639 Epoch 495: Total Training Recognition Loss 0.16  Total Training Translation Loss 4.02 
2024-02-03 06:37:42,639 EPOCH 496
2024-02-03 06:37:45,119 [Epoch: 496 Step: 00033200] Batch Recognition Loss:   0.002474 => Gls Tokens per Sec:     2224 || Batch Translation Loss:   0.129118 => Txt Tokens per Sec:     6335 || Lr: 0.000100
2024-02-03 06:37:47,696 Epoch 496: Total Training Recognition Loss 0.13  Total Training Translation Loss 3.37 
2024-02-03 06:37:47,697 EPOCH 497
2024-02-03 06:37:52,831 Epoch 497: Total Training Recognition Loss 0.11  Total Training Translation Loss 3.81 
2024-02-03 06:37:52,831 EPOCH 498
2024-02-03 06:37:52,881 [Epoch: 498 Step: 00033300] Batch Recognition Loss:   0.000350 => Gls Tokens per Sec:     3333 || Batch Translation Loss:   0.029675 => Txt Tokens per Sec:     7666 || Lr: 0.000100
2024-02-03 06:37:58,055 Epoch 498: Total Training Recognition Loss 0.11  Total Training Translation Loss 4.42 
2024-02-03 06:37:58,056 EPOCH 499
2024-02-03 06:38:00,561 [Epoch: 499 Step: 00033400] Batch Recognition Loss:   0.001650 => Gls Tokens per Sec:     2172 || Batch Translation Loss:   0.021161 => Txt Tokens per Sec:     6025 || Lr: 0.000100
2024-02-03 06:38:03,368 Epoch 499: Total Training Recognition Loss 0.18  Total Training Translation Loss 4.01 
2024-02-03 06:38:03,369 EPOCH 500
2024-02-03 06:38:08,458 [Epoch: 500 Step: 00033500] Batch Recognition Loss:   0.006064 => Gls Tokens per Sec:     2089 || Batch Translation Loss:   0.113360 => Txt Tokens per Sec:     5810 || Lr: 0.000100
2024-02-03 06:38:08,458 Epoch 500: Total Training Recognition Loss 0.20  Total Training Translation Loss 7.76 
2024-02-03 06:38:08,458 EPOCH 501
2024-02-03 06:38:13,663 Epoch 501: Total Training Recognition Loss 0.23  Total Training Translation Loss 6.34 
2024-02-03 06:38:13,664 EPOCH 502
2024-02-03 06:38:16,235 [Epoch: 502 Step: 00033600] Batch Recognition Loss:   0.000958 => Gls Tokens per Sec:     2020 || Batch Translation Loss:   0.054750 => Txt Tokens per Sec:     5721 || Lr: 0.000100
2024-02-03 06:38:18,716 Epoch 502: Total Training Recognition Loss 0.26  Total Training Translation Loss 7.01 
2024-02-03 06:38:18,717 EPOCH 503
2024-02-03 06:38:24,312 [Epoch: 503 Step: 00033700] Batch Recognition Loss:   0.002572 => Gls Tokens per Sec:     1871 || Batch Translation Loss:   0.082091 => Txt Tokens per Sec:     5196 || Lr: 0.000100
2024-02-03 06:38:24,386 Epoch 503: Total Training Recognition Loss 0.17  Total Training Translation Loss 7.89 
2024-02-03 06:38:24,386 EPOCH 504
2024-02-03 06:38:29,543 Epoch 504: Total Training Recognition Loss 0.14  Total Training Translation Loss 6.80 
2024-02-03 06:38:29,544 EPOCH 505
2024-02-03 06:38:32,256 [Epoch: 505 Step: 00033800] Batch Recognition Loss:   0.002949 => Gls Tokens per Sec:     1888 || Batch Translation Loss:   0.060993 => Txt Tokens per Sec:     5236 || Lr: 0.000100
2024-02-03 06:38:34,982 Epoch 505: Total Training Recognition Loss 0.13  Total Training Translation Loss 4.43 
2024-02-03 06:38:34,983 EPOCH 506
2024-02-03 06:38:40,157 [Epoch: 506 Step: 00033900] Batch Recognition Loss:   0.001809 => Gls Tokens per Sec:     1993 || Batch Translation Loss:   0.036011 => Txt Tokens per Sec:     5527 || Lr: 0.000100
2024-02-03 06:38:40,348 Epoch 506: Total Training Recognition Loss 0.11  Total Training Translation Loss 3.02 
2024-02-03 06:38:40,348 EPOCH 507
2024-02-03 06:38:45,612 Epoch 507: Total Training Recognition Loss 0.15  Total Training Translation Loss 6.41 
2024-02-03 06:38:45,612 EPOCH 508
2024-02-03 06:38:48,007 [Epoch: 508 Step: 00034000] Batch Recognition Loss:   0.001338 => Gls Tokens per Sec:     2072 || Batch Translation Loss:   0.028199 => Txt Tokens per Sec:     5836 || Lr: 0.000100
2024-02-03 06:38:56,282 Validation result at epoch 508, step    34000: duration: 8.2753s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 5.57093	Translation Loss: 88030.91406	PPL: 9350.81641
	Eval Metric: BLEU
	WER 3.68	(DEL: 0.00,	INS: 0.00,	SUB: 3.68)
	BLEU-4 0.60	(BLEU-1: 10.08,	BLEU-2: 3.06,	BLEU-3: 1.21,	BLEU-4: 0.60)
	CHRF 16.94	ROUGE 8.54
2024-02-03 06:38:56,283 Logging Recognition and Translation Outputs
2024-02-03 06:38:56,283 ========================================================================================================================
2024-02-03 06:38:56,283 Logging Sequence: 92_117.00
2024-02-03 06:38:56,284 	Gloss Reference :	A B+C+D+E
2024-02-03 06:38:56,284 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:38:56,284 	Gloss Alignment :	         
2024-02-03 06:38:56,284 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:38:56,284 	Text Reference  :	vandana's brother shekhar filed a   complaint against the     2    men
2024-02-03 06:38:56,285 	Text Hypothesis :	********* ******* ******* ***** the girls     took    selfies with him
2024-02-03 06:38:56,285 	Text Alignment  :	D         D       D       D     S   S         S       S       S    S  
2024-02-03 06:38:56,285 ========================================================================================================================
2024-02-03 06:38:56,285 Logging Sequence: 70_219.00
2024-02-03 06:38:56,285 	Gloss Reference :	A B+C+D+E
2024-02-03 06:38:56,285 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:38:56,285 	Gloss Alignment :	         
2024-02-03 06:38:56,285 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:38:56,286 	Text Reference  :	after this ****** there was     another similar incident
2024-02-03 06:38:56,286 	Text Hypothesis :	doing this helped her   compete and     secure  medals  
2024-02-03 06:38:56,286 	Text Alignment  :	S          I      S     S       S       S       S       
2024-02-03 06:38:56,286 ========================================================================================================================
2024-02-03 06:38:56,286 Logging Sequence: 181_101.00
2024-02-03 06:38:56,287 	Gloss Reference :	A B+C+D+E
2024-02-03 06:38:56,287 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:38:56,287 	Gloss Alignment :	         
2024-02-03 06:38:56,287 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:38:56,288 	Text Reference  :	the cutest comment was by yuvraj' father yograj singh who  is  a      former cricketer and actor    
2024-02-03 06:38:56,288 	Text Hypothesis :	*** ****** ******* *** ** ******* ****** and    they  also own joburg super  kings     in  complaint
2024-02-03 06:38:56,288 	Text Alignment  :	D   D      D       D   D  D       D      S      S     S    S   S      S      S         S   S        
2024-02-03 06:38:56,288 ========================================================================================================================
2024-02-03 06:38:56,288 Logging Sequence: 112_117.00
2024-02-03 06:38:56,289 	Gloss Reference :	A B+C+D+E
2024-02-03 06:38:56,289 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:38:56,289 	Gloss Alignment :	         
2024-02-03 06:38:56,289 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:38:56,290 	Text Reference  :	the rpsg group previously owned the *** rising pune   supergiant in      2016  and 2017
2024-02-03 06:38:56,290 	Text Hypothesis :	the **** ***** bidding    for   the ipl media  rights for        2023-27 cycle was held
2024-02-03 06:38:56,290 	Text Alignment  :	    D    D     S          S         I   S      S      S          S       S     S   S   
2024-02-03 06:38:56,290 ========================================================================================================================
2024-02-03 06:38:56,291 Logging Sequence: 156_248.00
2024-02-03 06:38:56,291 	Gloss Reference :	A B+C+D+E
2024-02-03 06:38:56,291 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:38:56,291 	Gloss Alignment :	         
2024-02-03 06:38:56,291 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:38:56,292 	Text Reference  :	miny reached 1843 in 16 overs they     won   the    match this was      very shocking
2024-02-03 06:38:56,292 	Text Hypothesis :	**** this    led  to a  huge  argument sapna shobit were  also involved in   this    
2024-02-03 06:38:56,292 	Text Alignment  :	D    S       S    S  S  S     S        S     S      S     S    S        S    S       
2024-02-03 06:38:56,293 ========================================================================================================================
2024-02-03 06:38:59,603 Epoch 508: Total Training Recognition Loss 0.17  Total Training Translation Loss 7.52 
2024-02-03 06:38:59,603 EPOCH 509
2024-02-03 06:39:04,675 [Epoch: 509 Step: 00034100] Batch Recognition Loss:   0.000769 => Gls Tokens per Sec:     2001 || Batch Translation Loss:   0.162373 => Txt Tokens per Sec:     5571 || Lr: 0.000100
2024-02-03 06:39:04,851 Epoch 509: Total Training Recognition Loss 0.20  Total Training Translation Loss 7.45 
2024-02-03 06:39:04,852 EPOCH 510
2024-02-03 06:39:10,336 Epoch 510: Total Training Recognition Loss 0.19  Total Training Translation Loss 8.46 
2024-02-03 06:39:10,336 EPOCH 511
2024-02-03 06:39:12,392 [Epoch: 511 Step: 00034200] Batch Recognition Loss:   0.000795 => Gls Tokens per Sec:     2292 || Batch Translation Loss:   0.093661 => Txt Tokens per Sec:     6366 || Lr: 0.000100
2024-02-03 06:39:15,165 Epoch 511: Total Training Recognition Loss 0.22  Total Training Translation Loss 5.86 
2024-02-03 06:39:15,165 EPOCH 512
2024-02-03 06:39:20,363 [Epoch: 512 Step: 00034300] Batch Recognition Loss:   0.001285 => Gls Tokens per Sec:     1923 || Batch Translation Loss:   0.071128 => Txt Tokens per Sec:     5382 || Lr: 0.000100
2024-02-03 06:39:20,590 Epoch 512: Total Training Recognition Loss 0.13  Total Training Translation Loss 6.12 
2024-02-03 06:39:20,590 EPOCH 513
2024-02-03 06:39:25,905 Epoch 513: Total Training Recognition Loss 0.18  Total Training Translation Loss 5.64 
2024-02-03 06:39:25,906 EPOCH 514
2024-02-03 06:39:28,137 [Epoch: 514 Step: 00034400] Batch Recognition Loss:   0.012902 => Gls Tokens per Sec:     2040 || Batch Translation Loss:   0.127855 => Txt Tokens per Sec:     5919 || Lr: 0.000100
2024-02-03 06:39:30,745 Epoch 514: Total Training Recognition Loss 0.23  Total Training Translation Loss 3.92 
2024-02-03 06:39:30,746 EPOCH 515
2024-02-03 06:39:36,021 [Epoch: 515 Step: 00034500] Batch Recognition Loss:   0.000758 => Gls Tokens per Sec:     1864 || Batch Translation Loss:   0.015473 => Txt Tokens per Sec:     5241 || Lr: 0.000100
2024-02-03 06:39:36,391 Epoch 515: Total Training Recognition Loss 0.21  Total Training Translation Loss 2.98 
2024-02-03 06:39:36,392 EPOCH 516
2024-02-03 06:39:41,113 Epoch 516: Total Training Recognition Loss 0.11  Total Training Translation Loss 2.03 
2024-02-03 06:39:41,113 EPOCH 517
2024-02-03 06:39:43,147 [Epoch: 517 Step: 00034600] Batch Recognition Loss:   0.004881 => Gls Tokens per Sec:     2204 || Batch Translation Loss:   0.027654 => Txt Tokens per Sec:     6055 || Lr: 0.000100
2024-02-03 06:39:46,334 Epoch 517: Total Training Recognition Loss 0.12  Total Training Translation Loss 2.85 
2024-02-03 06:39:46,335 EPOCH 518
2024-02-03 06:39:51,285 [Epoch: 518 Step: 00034700] Batch Recognition Loss:   0.000372 => Gls Tokens per Sec:     1954 || Batch Translation Loss:   0.024470 => Txt Tokens per Sec:     5428 || Lr: 0.000100
2024-02-03 06:39:51,730 Epoch 518: Total Training Recognition Loss 0.17  Total Training Translation Loss 3.56 
2024-02-03 06:39:51,731 EPOCH 519
2024-02-03 06:39:56,923 Epoch 519: Total Training Recognition Loss 0.11  Total Training Translation Loss 4.26 
2024-02-03 06:39:56,923 EPOCH 520
2024-02-03 06:39:58,754 [Epoch: 520 Step: 00034800] Batch Recognition Loss:   0.000436 => Gls Tokens per Sec:     2361 || Batch Translation Loss:   0.464311 => Txt Tokens per Sec:     6604 || Lr: 0.000100
2024-02-03 06:40:01,765 Epoch 520: Total Training Recognition Loss 0.17  Total Training Translation Loss 9.01 
2024-02-03 06:40:01,765 EPOCH 521
2024-02-03 06:40:06,634 [Epoch: 521 Step: 00034900] Batch Recognition Loss:   0.001711 => Gls Tokens per Sec:     1953 || Batch Translation Loss:   0.059715 => Txt Tokens per Sec:     5387 || Lr: 0.000100
2024-02-03 06:40:07,328 Epoch 521: Total Training Recognition Loss 0.34  Total Training Translation Loss 13.67 
2024-02-03 06:40:07,328 EPOCH 522
2024-02-03 06:40:12,890 Epoch 522: Total Training Recognition Loss 0.25  Total Training Translation Loss 7.99 
2024-02-03 06:40:12,890 EPOCH 523
2024-02-03 06:40:14,777 [Epoch: 523 Step: 00035000] Batch Recognition Loss:   0.000639 => Gls Tokens per Sec:     2207 || Batch Translation Loss:   0.075762 => Txt Tokens per Sec:     6341 || Lr: 0.000100
2024-02-03 06:40:17,797 Epoch 523: Total Training Recognition Loss 0.24  Total Training Translation Loss 5.04 
2024-02-03 06:40:17,798 EPOCH 524
2024-02-03 06:40:22,381 [Epoch: 524 Step: 00035100] Batch Recognition Loss:   0.001320 => Gls Tokens per Sec:     2041 || Batch Translation Loss:   0.036704 => Txt Tokens per Sec:     5637 || Lr: 0.000100
2024-02-03 06:40:23,179 Epoch 524: Total Training Recognition Loss 0.21  Total Training Translation Loss 4.80 
2024-02-03 06:40:23,179 EPOCH 525
2024-02-03 06:40:28,766 Epoch 525: Total Training Recognition Loss 0.15  Total Training Translation Loss 4.89 
2024-02-03 06:40:28,766 EPOCH 526
2024-02-03 06:40:30,619 [Epoch: 526 Step: 00035200] Batch Recognition Loss:   0.003036 => Gls Tokens per Sec:     2161 || Batch Translation Loss:   0.022547 => Txt Tokens per Sec:     5933 || Lr: 0.000100
2024-02-03 06:40:33,765 Epoch 526: Total Training Recognition Loss 0.12  Total Training Translation Loss 4.70 
2024-02-03 06:40:33,765 EPOCH 527
2024-02-03 06:40:37,619 [Epoch: 527 Step: 00035300] Batch Recognition Loss:   0.001182 => Gls Tokens per Sec:     2386 || Batch Translation Loss:   0.037254 => Txt Tokens per Sec:     6520 || Lr: 0.000100
2024-02-03 06:40:38,584 Epoch 527: Total Training Recognition Loss 0.13  Total Training Translation Loss 3.53 
2024-02-03 06:40:38,584 EPOCH 528
2024-02-03 06:40:44,012 Epoch 528: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.35 
2024-02-03 06:40:44,013 EPOCH 529
2024-02-03 06:40:46,029 [Epoch: 529 Step: 00035400] Batch Recognition Loss:   0.000219 => Gls Tokens per Sec:     1907 || Batch Translation Loss:   0.013779 => Txt Tokens per Sec:     5194 || Lr: 0.000100
2024-02-03 06:40:49,536 Epoch 529: Total Training Recognition Loss 0.09  Total Training Translation Loss 2.63 
2024-02-03 06:40:49,537 EPOCH 530
2024-02-03 06:40:53,499 [Epoch: 530 Step: 00035500] Batch Recognition Loss:   0.001631 => Gls Tokens per Sec:     2280 || Batch Translation Loss:   0.029771 => Txt Tokens per Sec:     6361 || Lr: 0.000100
2024-02-03 06:40:54,260 Epoch 530: Total Training Recognition Loss 0.15  Total Training Translation Loss 2.51 
2024-02-03 06:40:54,260 EPOCH 531
2024-02-03 06:40:58,737 Epoch 531: Total Training Recognition Loss 0.15  Total Training Translation Loss 5.11 
2024-02-03 06:40:58,738 EPOCH 532
2024-02-03 06:41:00,094 [Epoch: 532 Step: 00035600] Batch Recognition Loss:   0.001504 => Gls Tokens per Sec:     2714 || Batch Translation Loss:   0.043355 => Txt Tokens per Sec:     7070 || Lr: 0.000100
2024-02-03 06:41:04,089 Epoch 532: Total Training Recognition Loss 0.19  Total Training Translation Loss 8.19 
2024-02-03 06:41:04,089 EPOCH 533
2024-02-03 06:41:08,354 [Epoch: 533 Step: 00035700] Batch Recognition Loss:   0.000783 => Gls Tokens per Sec:     2081 || Batch Translation Loss:   0.222402 => Txt Tokens per Sec:     5824 || Lr: 0.000100
2024-02-03 06:41:09,151 Epoch 533: Total Training Recognition Loss 0.19  Total Training Translation Loss 11.11 
2024-02-03 06:41:09,152 EPOCH 534
2024-02-03 06:41:14,708 Epoch 534: Total Training Recognition Loss 0.38  Total Training Translation Loss 9.07 
2024-02-03 06:41:14,708 EPOCH 535
2024-02-03 06:41:16,134 [Epoch: 535 Step: 00035800] Batch Recognition Loss:   0.003312 => Gls Tokens per Sec:     2470 || Batch Translation Loss:   0.237817 => Txt Tokens per Sec:     6682 || Lr: 0.000100
2024-02-03 06:41:19,775 Epoch 535: Total Training Recognition Loss 0.33  Total Training Translation Loss 8.02 
2024-02-03 06:41:19,776 EPOCH 536
2024-02-03 06:41:24,069 [Epoch: 536 Step: 00035900] Batch Recognition Loss:   0.000963 => Gls Tokens per Sec:     2029 || Batch Translation Loss:   0.155434 => Txt Tokens per Sec:     5701 || Lr: 0.000100
2024-02-03 06:41:24,913 Epoch 536: Total Training Recognition Loss 0.24  Total Training Translation Loss 8.74 
2024-02-03 06:41:24,913 EPOCH 537
2024-02-03 06:41:30,572 Epoch 537: Total Training Recognition Loss 0.34  Total Training Translation Loss 15.61 
2024-02-03 06:41:30,572 EPOCH 538
2024-02-03 06:41:32,209 [Epoch: 538 Step: 00036000] Batch Recognition Loss:   0.070577 => Gls Tokens per Sec:     2000 || Batch Translation Loss:   0.716373 => Txt Tokens per Sec:     5537 || Lr: 0.000100
2024-02-03 06:41:41,985 Validation result at epoch 538, step    36000: duration: 9.7765s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 5.24377	Translation Loss: 87780.78125	PPL: 9111.01074
	Eval Metric: BLEU
	WER 3.89	(DEL: 0.07,	INS: 0.00,	SUB: 3.82)
	BLEU-4 0.47	(BLEU-1: 10.77,	BLEU-2: 2.86,	BLEU-3: 1.05,	BLEU-4: 0.47)
	CHRF 17.17	ROUGE 8.95
2024-02-03 06:41:41,987 Logging Recognition and Translation Outputs
2024-02-03 06:41:41,987 ========================================================================================================================
2024-02-03 06:41:41,987 Logging Sequence: 109_16.00
2024-02-03 06:41:41,988 	Gloss Reference :	A B+C+D+E
2024-02-03 06:41:41,988 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:41:41,988 	Gloss Alignment :	         
2024-02-03 06:41:41,988 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:41:41,990 	Text Reference  :	however the match was  rescheduled as  two     kkr players -     
2024-02-03 06:41:41,990 	Text Hypothesis :	******* *** doing this helped      her compete and secure  medals
2024-02-03 06:41:41,990 	Text Alignment  :	D       D   S     S    S           S   S       S   S       S     
2024-02-03 06:41:41,990 ========================================================================================================================
2024-02-03 06:41:41,991 Logging Sequence: 146_120.00
2024-02-03 06:41:41,991 	Gloss Reference :	A B+C+D+E
2024-02-03 06:41:41,991 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:41:41,991 	Gloss Alignment :	         
2024-02-03 06:41:41,992 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:41:41,994 	Text Reference  :	****** bwf also announced that the doubles partners of the players infected with the virus would also          be            
2024-02-03 06:41:41,994 	Text Hypothesis :	dahiya did not  know      that *** ******* too      of *** ******* ******** **** *** ***** ***** uttarakhand's advertisements
2024-02-03 06:41:41,994 	Text Alignment  :	I      S   S    S              D   D       S           D   D       D        D    D   D     D     S             S             
2024-02-03 06:41:41,994 ========================================================================================================================
2024-02-03 06:41:41,995 Logging Sequence: 130_38.00
2024-02-03 06:41:41,995 	Gloss Reference :	A B+C+D+E
2024-02-03 06:41:41,995 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:41:41,995 	Gloss Alignment :	         
2024-02-03 06:41:41,996 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:41:41,997 	Text Reference  :	** **** and is * now   married to   the    oscar-winning film-maker dustin  lance black     
2024-02-03 06:41:41,997 	Text Hypothesis :	he said it  is a diver had     been diving from          the        world's best  goalkeeper
2024-02-03 06:41:41,998 	Text Alignment  :	I  I    S      I S     S       S    S      S             S          S       S     S         
2024-02-03 06:41:41,998 ========================================================================================================================
2024-02-03 06:41:41,998 Logging Sequence: 161_170.00
2024-02-03 06:41:41,998 	Gloss Reference :	A B+C+D+E
2024-02-03 06:41:41,998 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:41:41,999 	Gloss Alignment :	         
2024-02-03 06:41:41,999 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:41:42,001 	Text Reference  :	however when the bcci replaced him with rohit sharma he decided to step down as   test captain as  well     
2024-02-03 06:41:42,001 	Text Hypothesis :	******* **** *** **** ******** *** **** ***** ****** he wanted  to **** **** gift 35   people  wow wonderful
2024-02-03 06:41:42,001 	Text Alignment  :	D       D    D   D    D        D   D    D     D         S          D    D    S    S    S       S   S        
2024-02-03 06:41:42,001 ========================================================================================================================
2024-02-03 06:41:42,001 Logging Sequence: 89_14.00
2024-02-03 06:41:42,002 	Gloss Reference :	A B+C+D+E
2024-02-03 06:41:42,002 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:41:42,002 	Gloss Alignment :	         
2024-02-03 06:41:42,002 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:41:42,004 	Text Reference  :	***** ********* *** *** *** **** he     has   completed 23 years of   playing cricket
2024-02-03 06:41:42,004 	Text Hypothesis :	since pathirana was out for much longer dhoni had       a  word  with the     umpire 
2024-02-03 06:41:42,004 	Text Alignment  :	I     I         I   I   I   I    S      S     S         S  S     S    S       S      
2024-02-03 06:41:42,004 ========================================================================================================================
2024-02-03 06:41:46,573 Epoch 538: Total Training Recognition Loss 0.34  Total Training Translation Loss 10.82 
2024-02-03 06:41:46,573 EPOCH 539
2024-02-03 06:41:51,033 [Epoch: 539 Step: 00036100] Batch Recognition Loss:   0.002268 => Gls Tokens per Sec:     1918 || Batch Translation Loss:   0.044444 => Txt Tokens per Sec:     5335 || Lr: 0.000100
2024-02-03 06:41:52,153 Epoch 539: Total Training Recognition Loss 0.22  Total Training Translation Loss 3.95 
2024-02-03 06:41:52,153 EPOCH 540
2024-02-03 06:41:57,638 Epoch 540: Total Training Recognition Loss 0.12  Total Training Translation Loss 3.21 
2024-02-03 06:41:57,638 EPOCH 541
2024-02-03 06:41:59,184 [Epoch: 541 Step: 00036200] Batch Recognition Loss:   0.001902 => Gls Tokens per Sec:     2014 || Batch Translation Loss:   0.046596 => Txt Tokens per Sec:     5343 || Lr: 0.000100
2024-02-03 06:42:02,931 Epoch 541: Total Training Recognition Loss 0.17  Total Training Translation Loss 2.67 
2024-02-03 06:42:02,931 EPOCH 542
2024-02-03 06:42:06,794 [Epoch: 542 Step: 00036300] Batch Recognition Loss:   0.000380 => Gls Tokens per Sec:     2196 || Batch Translation Loss:   0.038161 => Txt Tokens per Sec:     6073 || Lr: 0.000100
2024-02-03 06:42:07,992 Epoch 542: Total Training Recognition Loss 0.11  Total Training Translation Loss 2.37 
2024-02-03 06:42:07,992 EPOCH 543
2024-02-03 06:42:13,280 Epoch 543: Total Training Recognition Loss 0.11  Total Training Translation Loss 2.61 
2024-02-03 06:42:13,280 EPOCH 544
2024-02-03 06:42:14,559 [Epoch: 544 Step: 00036400] Batch Recognition Loss:   0.000539 => Gls Tokens per Sec:     2379 || Batch Translation Loss:   0.027018 => Txt Tokens per Sec:     6439 || Lr: 0.000100
2024-02-03 06:42:17,980 Epoch 544: Total Training Recognition Loss 0.08  Total Training Translation Loss 3.34 
2024-02-03 06:42:17,980 EPOCH 545
2024-02-03 06:42:21,410 [Epoch: 545 Step: 00036500] Batch Recognition Loss:   0.000445 => Gls Tokens per Sec:     2426 || Batch Translation Loss:   0.020781 => Txt Tokens per Sec:     6762 || Lr: 0.000100
2024-02-03 06:42:22,509 Epoch 545: Total Training Recognition Loss 0.10  Total Training Translation Loss 2.20 
2024-02-03 06:42:22,509 EPOCH 546
2024-02-03 06:42:27,964 Epoch 546: Total Training Recognition Loss 0.12  Total Training Translation Loss 1.66 
2024-02-03 06:42:27,965 EPOCH 547
2024-02-03 06:42:29,319 [Epoch: 547 Step: 00036600] Batch Recognition Loss:   0.000252 => Gls Tokens per Sec:     2128 || Batch Translation Loss:   0.016644 => Txt Tokens per Sec:     5771 || Lr: 0.000100
2024-02-03 06:42:33,476 Epoch 547: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.74 
2024-02-03 06:42:33,477 EPOCH 548
2024-02-03 06:42:37,117 [Epoch: 548 Step: 00036700] Batch Recognition Loss:   0.001024 => Gls Tokens per Sec:     2242 || Batch Translation Loss:   0.022519 => Txt Tokens per Sec:     6197 || Lr: 0.000100
2024-02-03 06:42:38,443 Epoch 548: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.21 
2024-02-03 06:42:38,443 EPOCH 549
2024-02-03 06:42:43,983 Epoch 549: Total Training Recognition Loss 0.10  Total Training Translation Loss 3.55 
2024-02-03 06:42:43,984 EPOCH 550
2024-02-03 06:42:45,212 [Epoch: 550 Step: 00036800] Batch Recognition Loss:   0.000251 => Gls Tokens per Sec:     2215 || Batch Translation Loss:   0.024369 => Txt Tokens per Sec:     6346 || Lr: 0.000100
2024-02-03 06:42:49,157 Epoch 550: Total Training Recognition Loss 0.12  Total Training Translation Loss 5.37 
2024-02-03 06:42:49,158 EPOCH 551
2024-02-03 06:42:53,439 [Epoch: 551 Step: 00036900] Batch Recognition Loss:   0.014032 => Gls Tokens per Sec:     1869 || Batch Translation Loss:   0.054033 => Txt Tokens per Sec:     5211 || Lr: 0.000100
2024-02-03 06:42:54,827 Epoch 551: Total Training Recognition Loss 0.14  Total Training Translation Loss 9.10 
2024-02-03 06:42:54,827 EPOCH 552
2024-02-03 06:43:00,182 Epoch 552: Total Training Recognition Loss 0.24  Total Training Translation Loss 9.98 
2024-02-03 06:43:00,183 EPOCH 553
2024-02-03 06:43:01,228 [Epoch: 553 Step: 00037000] Batch Recognition Loss:   0.000350 => Gls Tokens per Sec:     2454 || Batch Translation Loss:   0.054667 => Txt Tokens per Sec:     6702 || Lr: 0.000100
2024-02-03 06:43:05,291 Epoch 553: Total Training Recognition Loss 0.19  Total Training Translation Loss 9.67 
2024-02-03 06:43:05,292 EPOCH 554
2024-02-03 06:43:09,422 [Epoch: 554 Step: 00037100] Batch Recognition Loss:   0.001702 => Gls Tokens per Sec:     1878 || Batch Translation Loss:   0.087809 => Txt Tokens per Sec:     5310 || Lr: 0.000100
2024-02-03 06:43:10,665 Epoch 554: Total Training Recognition Loss 0.24  Total Training Translation Loss 9.79 
2024-02-03 06:43:10,665 EPOCH 555
2024-02-03 06:43:16,181 Epoch 555: Total Training Recognition Loss 0.20  Total Training Translation Loss 7.17 
2024-02-03 06:43:16,182 EPOCH 556
2024-02-03 06:43:17,434 [Epoch: 556 Step: 00037200] Batch Recognition Loss:   0.003775 => Gls Tokens per Sec:     1848 || Batch Translation Loss:   0.048385 => Txt Tokens per Sec:     5124 || Lr: 0.000100
2024-02-03 06:43:21,367 Epoch 556: Total Training Recognition Loss 0.13  Total Training Translation Loss 5.76 
2024-02-03 06:43:21,368 EPOCH 557
2024-02-03 06:43:25,471 [Epoch: 557 Step: 00037300] Batch Recognition Loss:   0.003168 => Gls Tokens per Sec:     1850 || Batch Translation Loss:   0.019258 => Txt Tokens per Sec:     5219 || Lr: 0.000100
2024-02-03 06:43:26,788 Epoch 557: Total Training Recognition Loss 0.12  Total Training Translation Loss 3.90 
2024-02-03 06:43:26,788 EPOCH 558
2024-02-03 06:43:31,492 Epoch 558: Total Training Recognition Loss 0.14  Total Training Translation Loss 3.48 
2024-02-03 06:43:31,492 EPOCH 559
2024-02-03 06:43:32,649 [Epoch: 559 Step: 00037400] Batch Recognition Loss:   0.002850 => Gls Tokens per Sec:     1936 || Batch Translation Loss:   0.025890 => Txt Tokens per Sec:     5681 || Lr: 0.000100
2024-02-03 06:43:36,953 Epoch 559: Total Training Recognition Loss 0.17  Total Training Translation Loss 4.96 
2024-02-03 06:43:36,954 EPOCH 560
2024-02-03 06:43:40,818 [Epoch: 560 Step: 00037500] Batch Recognition Loss:   0.000331 => Gls Tokens per Sec:     1923 || Batch Translation Loss:   0.571921 => Txt Tokens per Sec:     5303 || Lr: 0.000100
2024-02-03 06:43:42,394 Epoch 560: Total Training Recognition Loss 0.11  Total Training Translation Loss 3.66 
2024-02-03 06:43:42,394 EPOCH 561
2024-02-03 06:43:47,368 Epoch 561: Total Training Recognition Loss 0.15  Total Training Translation Loss 3.52 
2024-02-03 06:43:47,368 EPOCH 562
2024-02-03 06:43:48,273 [Epoch: 562 Step: 00037600] Batch Recognition Loss:   0.000619 => Gls Tokens per Sec:     2302 || Batch Translation Loss:   0.027964 => Txt Tokens per Sec:     6663 || Lr: 0.000100
2024-02-03 06:43:52,662 Epoch 562: Total Training Recognition Loss 0.12  Total Training Translation Loss 4.03 
2024-02-03 06:43:52,662 EPOCH 563
2024-02-03 06:43:56,348 [Epoch: 563 Step: 00037700] Batch Recognition Loss:   0.001628 => Gls Tokens per Sec:     1973 || Batch Translation Loss:   0.015375 => Txt Tokens per Sec:     5436 || Lr: 0.000100
2024-02-03 06:43:57,991 Epoch 563: Total Training Recognition Loss 0.12  Total Training Translation Loss 5.45 
2024-02-03 06:43:57,991 EPOCH 564
2024-02-03 06:44:03,093 Epoch 564: Total Training Recognition Loss 0.16  Total Training Translation Loss 4.63 
2024-02-03 06:44:03,093 EPOCH 565
2024-02-03 06:44:04,196 [Epoch: 565 Step: 00037800] Batch Recognition Loss:   0.002696 => Gls Tokens per Sec:     1661 || Batch Translation Loss:   0.037397 => Txt Tokens per Sec:     5027 || Lr: 0.000100
2024-02-03 06:44:08,422 Epoch 565: Total Training Recognition Loss 0.09  Total Training Translation Loss 6.68 
2024-02-03 06:44:08,422 EPOCH 566
2024-02-03 06:44:11,632 [Epoch: 566 Step: 00037900] Batch Recognition Loss:   0.001530 => Gls Tokens per Sec:     2216 || Batch Translation Loss:   0.050606 => Txt Tokens per Sec:     6188 || Lr: 0.000100
2024-02-03 06:44:13,614 Epoch 566: Total Training Recognition Loss 0.13  Total Training Translation Loss 6.10 
2024-02-03 06:44:13,614 EPOCH 567
2024-02-03 06:44:18,601 Epoch 567: Total Training Recognition Loss 0.22  Total Training Translation Loss 5.37 
2024-02-03 06:44:18,601 EPOCH 568
2024-02-03 06:44:19,335 [Epoch: 568 Step: 00038000] Batch Recognition Loss:   0.001667 => Gls Tokens per Sec:     2401 || Batch Translation Loss:   0.155573 => Txt Tokens per Sec:     6449 || Lr: 0.000100
2024-02-03 06:44:27,982 Validation result at epoch 568, step    38000: duration: 8.6469s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 5.14993	Translation Loss: 87339.60938	PPL: 8702.95508
	Eval Metric: BLEU
	WER 3.75	(DEL: 0.00,	INS: 0.00,	SUB: 3.75)
	BLEU-4 0.47	(BLEU-1: 9.74,	BLEU-2: 2.82,	BLEU-3: 1.02,	BLEU-4: 0.47)
	CHRF 16.81	ROUGE 8.10
2024-02-03 06:44:27,984 Logging Recognition and Translation Outputs
2024-02-03 06:44:27,984 ========================================================================================================================
2024-02-03 06:44:27,984 Logging Sequence: 180_124.00
2024-02-03 06:44:27,985 	Gloss Reference :	A B+C+D+E
2024-02-03 06:44:27,985 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:44:27,985 	Gloss Alignment :	         
2024-02-03 06:44:27,985 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:44:27,987 	Text Reference  :	**** *** ***** ****** ********* ***** * demanding singh'     resignation dissolution of   wfi     committee and   appointing a  new    committee
2024-02-03 06:44:27,987 	Text Hypothesis :	they say seven female wrestlers filed a sexual    harassment complaint   against     brij bhushan sharan    singh at         cp police station  
2024-02-03 06:44:27,987 	Text Alignment  :	I    I   I     I      I         I     I S         S          S           S           S    S       S         S     S          S  S      S        
2024-02-03 06:44:27,987 ========================================================================================================================
2024-02-03 06:44:27,987 Logging Sequence: 67_16.00
2024-02-03 06:44:27,987 	Gloss Reference :	A B+C+D+E
2024-02-03 06:44:27,988 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:44:27,988 	Gloss Alignment :	         
2024-02-03 06:44:27,988 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:44:27,989 	Text Reference  :	* **** ********* **** ***** to help india's fight    against the covid-19 pandemic
2024-02-03 06:44:27,989 	Text Hypothesis :	i have dedicated this match to **** my      personal active  on  social   media   
2024-02-03 06:44:27,989 	Text Alignment  :	I I    I         I    I        D    S       S        S       S   S        S       
2024-02-03 06:44:27,989 ========================================================================================================================
2024-02-03 06:44:27,989 Logging Sequence: 79_57.00
2024-02-03 06:44:27,989 	Gloss Reference :	A B+C+D+E
2024-02-03 06:44:27,990 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:44:27,990 	Gloss Alignment :	         
2024-02-03 06:44:27,990 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:44:27,991 	Text Reference  :	****** *** ***** **** the    board organised the matches only   in      mumbai navi mumbai and **** ******* *** **** pune
2024-02-03 06:44:27,992 	Text Hypothesis :	krunal and rahul took charge and   put       on  their   family because of     his  lips   and both playing the same room
2024-02-03 06:44:27,992 	Text Alignment  :	I      I   I     I    S      S     S         S   S       S      S       S      S    S          I    I       I   I    S   
2024-02-03 06:44:27,992 ========================================================================================================================
2024-02-03 06:44:27,992 Logging Sequence: 109_161.00
2024-02-03 06:44:27,992 	Gloss Reference :	A B+C+D+E
2024-02-03 06:44:27,992 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:44:27,992 	Gloss Alignment :	         
2024-02-03 06:44:27,993 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:44:27,993 	Text Reference  :	he had ****** severe   pain in       his lower abdomen
2024-02-03 06:44:27,993 	Text Hypothesis :	he had tested positive for  covid-19 on  may   19     
2024-02-03 06:44:27,993 	Text Alignment  :	       I      S        S    S        S   S     S      
2024-02-03 06:44:27,994 ========================================================================================================================
2024-02-03 06:44:27,994 Logging Sequence: 135_136.00
2024-02-03 06:44:27,994 	Gloss Reference :	A B+C+D+E
2024-02-03 06:44:27,994 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:44:27,994 	Gloss Alignment :	         
2024-02-03 06:44:27,994 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:44:27,996 	Text Reference  :	***** ***** **** in 2018  maria was   diagnosed with cancer         and ******* required surgery she    is a       cancer suvivor
2024-02-03 06:44:27,996 	Text Hypothesis :	maria wrote that to cover the   costs of        his  transportation and medical care     maå‚ysa needed 15 million polish zlotys 
2024-02-03 06:44:27,996 	Text Alignment  :	I     I     I    S  S     S     S     S         S    S                  I       S        S       S      S  S       S      S      
2024-02-03 06:44:27,997 ========================================================================================================================
2024-02-03 06:44:32,863 Epoch 568: Total Training Recognition Loss 0.15  Total Training Translation Loss 4.95 
2024-02-03 06:44:32,864 EPOCH 569
2024-02-03 06:44:36,282 [Epoch: 569 Step: 00038100] Batch Recognition Loss:   0.000705 => Gls Tokens per Sec:     2060 || Batch Translation Loss:   0.035604 => Txt Tokens per Sec:     5675 || Lr: 0.000100
2024-02-03 06:44:38,327 Epoch 569: Total Training Recognition Loss 0.15  Total Training Translation Loss 6.01 
2024-02-03 06:44:38,327 EPOCH 570
2024-02-03 06:44:43,306 Epoch 570: Total Training Recognition Loss 0.18  Total Training Translation Loss 8.78 
2024-02-03 06:44:43,307 EPOCH 571
2024-02-03 06:44:44,136 [Epoch: 571 Step: 00038200] Batch Recognition Loss:   0.001760 => Gls Tokens per Sec:     1932 || Batch Translation Loss:   0.048807 => Txt Tokens per Sec:     5377 || Lr: 0.000100
2024-02-03 06:44:48,769 Epoch 571: Total Training Recognition Loss 0.12  Total Training Translation Loss 9.12 
2024-02-03 06:44:48,770 EPOCH 572
2024-02-03 06:44:51,797 [Epoch: 572 Step: 00038300] Batch Recognition Loss:   0.000191 => Gls Tokens per Sec:     2274 || Batch Translation Loss:   0.060650 => Txt Tokens per Sec:     6128 || Lr: 0.000100
2024-02-03 06:44:53,931 Epoch 572: Total Training Recognition Loss 0.15  Total Training Translation Loss 5.91 
2024-02-03 06:44:53,932 EPOCH 573
2024-02-03 06:44:59,463 Epoch 573: Total Training Recognition Loss 0.12  Total Training Translation Loss 5.24 
2024-02-03 06:44:59,464 EPOCH 574
2024-02-03 06:45:00,102 [Epoch: 574 Step: 00038400] Batch Recognition Loss:   0.000129 => Gls Tokens per Sec:     2261 || Batch Translation Loss:   0.049854 => Txt Tokens per Sec:     5918 || Lr: 0.000100
2024-02-03 06:45:04,797 Epoch 574: Total Training Recognition Loss 0.12  Total Training Translation Loss 4.63 
2024-02-03 06:45:04,798 EPOCH 575
2024-02-03 06:45:08,124 [Epoch: 575 Step: 00038500] Batch Recognition Loss:   0.001285 => Gls Tokens per Sec:     1994 || Batch Translation Loss:   0.042115 => Txt Tokens per Sec:     5476 || Lr: 0.000100
2024-02-03 06:45:09,968 Epoch 575: Total Training Recognition Loss 0.10  Total Training Translation Loss 3.49 
2024-02-03 06:45:09,968 EPOCH 576
2024-02-03 06:45:15,421 Epoch 576: Total Training Recognition Loss 0.13  Total Training Translation Loss 2.25 
2024-02-03 06:45:15,422 EPOCH 577
2024-02-03 06:45:16,075 [Epoch: 577 Step: 00038600] Batch Recognition Loss:   0.000924 => Gls Tokens per Sec:     1960 || Batch Translation Loss:   0.022142 => Txt Tokens per Sec:     5482 || Lr: 0.000100
2024-02-03 06:45:20,206 Epoch 577: Total Training Recognition Loss 0.10  Total Training Translation Loss 1.78 
2024-02-03 06:45:20,206 EPOCH 578
2024-02-03 06:45:23,502 [Epoch: 578 Step: 00038700] Batch Recognition Loss:   0.011040 => Gls Tokens per Sec:     1991 || Batch Translation Loss:   0.020907 => Txt Tokens per Sec:     5585 || Lr: 0.000100
2024-02-03 06:45:25,670 Epoch 578: Total Training Recognition Loss 0.12  Total Training Translation Loss 2.34 
2024-02-03 06:45:25,671 EPOCH 579
2024-02-03 06:45:30,977 Epoch 579: Total Training Recognition Loss 0.09  Total Training Translation Loss 1.74 
2024-02-03 06:45:30,977 EPOCH 580
2024-02-03 06:45:31,504 [Epoch: 580 Step: 00038800] Batch Recognition Loss:   0.001481 => Gls Tokens per Sec:     2129 || Batch Translation Loss:   0.069134 => Txt Tokens per Sec:     5804 || Lr: 0.000100
2024-02-03 06:45:36,126 Epoch 580: Total Training Recognition Loss 0.12  Total Training Translation Loss 2.51 
2024-02-03 06:45:36,126 EPOCH 581
2024-02-03 06:45:39,336 [Epoch: 581 Step: 00038900] Batch Recognition Loss:   0.000998 => Gls Tokens per Sec:     1994 || Batch Translation Loss:   0.019163 => Txt Tokens per Sec:     5655 || Lr: 0.000100
2024-02-03 06:45:41,510 Epoch 581: Total Training Recognition Loss 0.09  Total Training Translation Loss 4.25 
2024-02-03 06:45:41,510 EPOCH 582
2024-02-03 06:45:46,650 Epoch 582: Total Training Recognition Loss 0.29  Total Training Translation Loss 14.89 
2024-02-03 06:45:46,650 EPOCH 583
2024-02-03 06:45:47,076 [Epoch: 583 Step: 00039000] Batch Recognition Loss:   0.000880 => Gls Tokens per Sec:     2259 || Batch Translation Loss:   0.136120 => Txt Tokens per Sec:     5887 || Lr: 0.000100
2024-02-03 06:45:51,954 Epoch 583: Total Training Recognition Loss 0.44  Total Training Translation Loss 12.43 
2024-02-03 06:45:51,955 EPOCH 584
2024-02-03 06:45:55,101 [Epoch: 584 Step: 00039100] Batch Recognition Loss:   0.001339 => Gls Tokens per Sec:     1955 || Batch Translation Loss:   0.022255 => Txt Tokens per Sec:     5620 || Lr: 0.000100
2024-02-03 06:45:57,376 Epoch 584: Total Training Recognition Loss 0.23  Total Training Translation Loss 6.95 
2024-02-03 06:45:57,376 EPOCH 585
2024-02-03 06:46:02,963 Epoch 585: Total Training Recognition Loss 0.22  Total Training Translation Loss 4.70 
2024-02-03 06:46:02,963 EPOCH 586
2024-02-03 06:46:03,399 [Epoch: 586 Step: 00039200] Batch Recognition Loss:   0.000578 => Gls Tokens per Sec:     1843 || Batch Translation Loss:   0.080437 => Txt Tokens per Sec:     5542 || Lr: 0.000100
2024-02-03 06:46:08,310 Epoch 586: Total Training Recognition Loss 0.15  Total Training Translation Loss 5.77 
2024-02-03 06:46:08,311 EPOCH 587
2024-02-03 06:46:11,182 [Epoch: 587 Step: 00039300] Batch Recognition Loss:   0.000656 => Gls Tokens per Sec:     2087 || Batch Translation Loss:   0.033464 => Txt Tokens per Sec:     5738 || Lr: 0.000100
2024-02-03 06:46:13,541 Epoch 587: Total Training Recognition Loss 0.15  Total Training Translation Loss 6.20 
2024-02-03 06:46:13,542 EPOCH 588
2024-02-03 06:46:18,892 Epoch 588: Total Training Recognition Loss 0.20  Total Training Translation Loss 5.27 
2024-02-03 06:46:18,892 EPOCH 589
2024-02-03 06:46:19,161 [Epoch: 589 Step: 00039400] Batch Recognition Loss:   0.002711 => Gls Tokens per Sec:     2388 || Batch Translation Loss:   0.094325 => Txt Tokens per Sec:     6758 || Lr: 0.000100
2024-02-03 06:46:23,757 Epoch 589: Total Training Recognition Loss 0.15  Total Training Translation Loss 4.28 
2024-02-03 06:46:23,758 EPOCH 590
2024-02-03 06:46:26,902 [Epoch: 590 Step: 00039500] Batch Recognition Loss:   0.000468 => Gls Tokens per Sec:     1855 || Batch Translation Loss:   0.037581 => Txt Tokens per Sec:     5207 || Lr: 0.000100
2024-02-03 06:46:29,200 Epoch 590: Total Training Recognition Loss 0.15  Total Training Translation Loss 3.98 
2024-02-03 06:46:29,200 EPOCH 591
2024-02-03 06:46:34,197 Epoch 591: Total Training Recognition Loss 0.10  Total Training Translation Loss 3.61 
2024-02-03 06:46:34,197 EPOCH 592
2024-02-03 06:46:34,487 [Epoch: 592 Step: 00039600] Batch Recognition Loss:   0.005470 => Gls Tokens per Sec:     1667 || Batch Translation Loss:   0.040761 => Txt Tokens per Sec:     4878 || Lr: 0.000100
2024-02-03 06:46:39,498 Epoch 592: Total Training Recognition Loss 0.11  Total Training Translation Loss 3.84 
2024-02-03 06:46:39,498 EPOCH 593
2024-02-03 06:46:42,360 [Epoch: 593 Step: 00039700] Batch Recognition Loss:   0.010921 => Gls Tokens per Sec:     2014 || Batch Translation Loss:   0.096264 => Txt Tokens per Sec:     5717 || Lr: 0.000100
2024-02-03 06:46:44,829 Epoch 593: Total Training Recognition Loss 0.13  Total Training Translation Loss 4.16 
2024-02-03 06:46:44,830 EPOCH 594
2024-02-03 06:46:49,560 Epoch 594: Total Training Recognition Loss 0.09  Total Training Translation Loss 3.45 
2024-02-03 06:46:49,560 EPOCH 595
2024-02-03 06:46:49,658 [Epoch: 595 Step: 00039800] Batch Recognition Loss:   0.000831 => Gls Tokens per Sec:     3299 || Batch Translation Loss:   0.013043 => Txt Tokens per Sec:     7711 || Lr: 0.000100
2024-02-03 06:46:54,841 Epoch 595: Total Training Recognition Loss 0.10  Total Training Translation Loss 3.76 
2024-02-03 06:46:54,841 EPOCH 596
2024-02-03 06:46:57,752 [Epoch: 596 Step: 00039900] Batch Recognition Loss:   0.000839 => Gls Tokens per Sec:     1893 || Batch Translation Loss:   0.036295 => Txt Tokens per Sec:     5515 || Lr: 0.000100
2024-02-03 06:47:00,208 Epoch 596: Total Training Recognition Loss 0.09  Total Training Translation Loss 4.97 
2024-02-03 06:47:00,209 EPOCH 597
2024-02-03 06:47:05,195 Epoch 597: Total Training Recognition Loss 0.17  Total Training Translation Loss 6.07 
2024-02-03 06:47:05,196 EPOCH 598
2024-02-03 06:47:05,280 [Epoch: 598 Step: 00040000] Batch Recognition Loss:   0.001427 => Gls Tokens per Sec:     1928 || Batch Translation Loss:   0.107042 => Txt Tokens per Sec:     6602 || Lr: 0.000100
2024-02-03 06:47:13,849 Validation result at epoch 598, step    40000: duration: 8.5690s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 5.03155	Translation Loss: 88692.88281	PPL: 10016.34180
	Eval Metric: BLEU
	WER 3.39	(DEL: 0.00,	INS: 0.00,	SUB: 3.39)
	BLEU-4 0.68	(BLEU-1: 10.32,	BLEU-2: 2.95,	BLEU-3: 1.23,	BLEU-4: 0.68)
	CHRF 16.83	ROUGE 8.52
2024-02-03 06:47:13,850 Logging Recognition and Translation Outputs
2024-02-03 06:47:13,850 ========================================================================================================================
2024-02-03 06:47:13,850 Logging Sequence: 142_182.00
2024-02-03 06:47:13,850 	Gloss Reference :	A B+C+D+E
2024-02-03 06:47:13,851 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:47:13,851 	Gloss Alignment :	         
2024-02-03 06:47:13,851 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:47:13,852 	Text Reference  :	many indians commented that hope he   does  not    miss    the match as he is important while  playing against england
2024-02-03 06:47:13,852 	Text Hypothesis :	**** ******* ********* **** and  take stern action against the ***** ** ** ** ********* player if      found   guilty 
2024-02-03 06:47:13,852 	Text Alignment  :	D    D       D         D    S    S    S     S      S           D     D  D  D  D         S      S       S       S      
2024-02-03 06:47:13,852 ========================================================================================================================
2024-02-03 06:47:13,853 Logging Sequence: 58_97.00
2024-02-03 06:47:13,853 	Gloss Reference :	A B+C+D+E
2024-02-03 06:47:13,853 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:47:13,853 	Gloss Alignment :	         
2024-02-03 06:47:13,853 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:47:13,854 	Text Reference  :	*** the indian athletes have already bagged a    total of   28 medals
2024-02-03 06:47:13,854 	Text Hypothesis :	and his 4      days     from the     1st    ball was   held in dubai 
2024-02-03 06:47:13,854 	Text Alignment  :	I   S   S      S        S    S       S      S    S     S    S  S     
2024-02-03 06:47:13,854 ========================================================================================================================
2024-02-03 06:47:13,855 Logging Sequence: 62_76.00
2024-02-03 06:47:13,855 	Gloss Reference :	A B+C+D+E
2024-02-03 06:47:13,855 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:47:13,855 	Gloss Alignment :	         
2024-02-03 06:47:13,855 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:47:13,857 	Text Reference  :	*** ***** *** for t20 the male   player   is paid rs 3  lakh *** *** information is  not  available for women players
2024-02-03 06:47:13,857 	Text Hypothesis :	the match fee for *** the indian athletes is **** rs 65 lakh for the loss        are they get       rs  6     lakh   
2024-02-03 06:47:13,857 	Text Alignment  :	I   I     I       D       S      S           D       S       I   I   S           S   S    S         S   S     S      
2024-02-03 06:47:13,857 ========================================================================================================================
2024-02-03 06:47:13,857 Logging Sequence: 171_52.00
2024-02-03 06:47:13,858 	Gloss Reference :	A B+C+D+E
2024-02-03 06:47:13,858 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:47:13,858 	Gloss Alignment :	         
2024-02-03 06:47:13,858 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:47:13,860 	Text Reference  :	during the second half of the      match when gujarat titans  were batting sometime during  the 14th-15th over   
2024-02-03 06:47:13,860 	Text Hypothesis :	****** he  said   i    am stepping down  as   the     captain was  the     only     because of  a         message
2024-02-03 06:47:13,860 	Text Alignment  :	D      S   S      S    S  S        S     S    S       S       S    S       S        S       S   S         S      
2024-02-03 06:47:13,860 ========================================================================================================================
2024-02-03 06:47:13,860 Logging Sequence: 92_22.00
2024-02-03 06:47:13,860 	Gloss Reference :	A B+C+D+E
2024-02-03 06:47:13,860 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:47:13,861 	Gloss Alignment :	         
2024-02-03 06:47:13,861 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:47:13,862 	Text Reference  :	the team was devastated as they      were hoping to win and secure     a   gold   medal in the ******* finals   
2024-02-03 06:47:13,862 	Text Hypothesis :	*** it   was ********** ** difficult for  covid  to *** *** disqualify the afghan team  at the penalty shoot-out
2024-02-03 06:47:13,862 	Text Alignment  :	D   S        D          D  S         S    S         D   D   S          S   S      S     S      I       S        
2024-02-03 06:47:13,863 ========================================================================================================================
2024-02-03 06:47:19,309 Epoch 598: Total Training Recognition Loss 0.12  Total Training Translation Loss 5.12 
2024-02-03 06:47:19,310 EPOCH 599
2024-02-03 06:47:22,054 [Epoch: 599 Step: 00040100] Batch Recognition Loss:   0.001142 => Gls Tokens per Sec:     1950 || Batch Translation Loss:   0.167164 => Txt Tokens per Sec:     5469 || Lr: 0.000100
2024-02-03 06:47:24,640 Epoch 599: Total Training Recognition Loss 0.17  Total Training Translation Loss 7.98 
2024-02-03 06:47:24,640 EPOCH 600
2024-02-03 06:47:29,927 [Epoch: 600 Step: 00040200] Batch Recognition Loss:   0.001182 => Gls Tokens per Sec:     2011 || Batch Translation Loss:   0.102476 => Txt Tokens per Sec:     5593 || Lr: 0.000100
2024-02-03 06:47:29,928 Epoch 600: Total Training Recognition Loss 0.12  Total Training Translation Loss 10.26 
2024-02-03 06:47:29,928 EPOCH 601
2024-02-03 06:47:35,298 Epoch 601: Total Training Recognition Loss 0.28  Total Training Translation Loss 8.22 
2024-02-03 06:47:35,298 EPOCH 602
2024-02-03 06:47:37,843 [Epoch: 602 Step: 00040300] Batch Recognition Loss:   0.002676 => Gls Tokens per Sec:     2075 || Batch Translation Loss:   0.202655 => Txt Tokens per Sec:     5978 || Lr: 0.000100
2024-02-03 06:47:40,571 Epoch 602: Total Training Recognition Loss 0.66  Total Training Translation Loss 6.22 
2024-02-03 06:47:40,572 EPOCH 603
2024-02-03 06:47:45,782 [Epoch: 603 Step: 00040400] Batch Recognition Loss:   0.002427 => Gls Tokens per Sec:     2010 || Batch Translation Loss:   0.030348 => Txt Tokens per Sec:     5570 || Lr: 0.000100
2024-02-03 06:47:45,923 Epoch 603: Total Training Recognition Loss 1.13  Total Training Translation Loss 5.82 
2024-02-03 06:47:45,923 EPOCH 604
2024-02-03 06:47:51,314 Epoch 604: Total Training Recognition Loss 0.48  Total Training Translation Loss 4.56 
2024-02-03 06:47:51,314 EPOCH 605
2024-02-03 06:47:53,636 [Epoch: 605 Step: 00040500] Batch Recognition Loss:   0.000278 => Gls Tokens per Sec:     2167 || Batch Translation Loss:   0.038088 => Txt Tokens per Sec:     6226 || Lr: 0.000100
2024-02-03 06:47:56,185 Epoch 605: Total Training Recognition Loss 0.54  Total Training Translation Loss 4.19 
2024-02-03 06:47:56,186 EPOCH 606
2024-02-03 06:48:01,331 [Epoch: 606 Step: 00040600] Batch Recognition Loss:   0.006313 => Gls Tokens per Sec:     2004 || Batch Translation Loss:   0.013367 => Txt Tokens per Sec:     5559 || Lr: 0.000100
2024-02-03 06:48:01,500 Epoch 606: Total Training Recognition Loss 0.27  Total Training Translation Loss 4.67 
2024-02-03 06:48:01,500 EPOCH 607
2024-02-03 06:48:07,272 Epoch 607: Total Training Recognition Loss 0.16  Total Training Translation Loss 6.20 
2024-02-03 06:48:07,273 EPOCH 608
2024-02-03 06:48:09,620 [Epoch: 608 Step: 00040700] Batch Recognition Loss:   0.006789 => Gls Tokens per Sec:     2115 || Batch Translation Loss:   0.210849 => Txt Tokens per Sec:     6046 || Lr: 0.000100
2024-02-03 06:48:12,537 Epoch 608: Total Training Recognition Loss 0.16  Total Training Translation Loss 4.78 
2024-02-03 06:48:12,537 EPOCH 609
2024-02-03 06:48:17,670 [Epoch: 609 Step: 00040800] Batch Recognition Loss:   0.000678 => Gls Tokens per Sec:     1978 || Batch Translation Loss:   0.054434 => Txt Tokens per Sec:     5490 || Lr: 0.000100
2024-02-03 06:48:17,908 Epoch 609: Total Training Recognition Loss 0.21  Total Training Translation Loss 4.90 
2024-02-03 06:48:17,908 EPOCH 610
2024-02-03 06:48:23,043 Epoch 610: Total Training Recognition Loss 0.12  Total Training Translation Loss 6.86 
2024-02-03 06:48:23,044 EPOCH 611
2024-02-03 06:48:25,426 [Epoch: 611 Step: 00040900] Batch Recognition Loss:   0.001715 => Gls Tokens per Sec:     1978 || Batch Translation Loss:   0.018371 => Txt Tokens per Sec:     5584 || Lr: 0.000100
2024-02-03 06:48:28,057 Epoch 611: Total Training Recognition Loss 0.23  Total Training Translation Loss 6.49 
2024-02-03 06:48:28,057 EPOCH 612
2024-02-03 06:48:33,204 [Epoch: 612 Step: 00041000] Batch Recognition Loss:   0.001788 => Gls Tokens per Sec:     1942 || Batch Translation Loss:   0.138272 => Txt Tokens per Sec:     5352 || Lr: 0.000100
2024-02-03 06:48:33,624 Epoch 612: Total Training Recognition Loss 0.29  Total Training Translation Loss 6.37 
2024-02-03 06:48:33,624 EPOCH 613
2024-02-03 06:48:38,531 Epoch 613: Total Training Recognition Loss 0.50  Total Training Translation Loss 4.89 
2024-02-03 06:48:38,531 EPOCH 614
2024-02-03 06:48:40,912 [Epoch: 614 Step: 00041100] Batch Recognition Loss:   0.000367 => Gls Tokens per Sec:     1950 || Batch Translation Loss:   0.028618 => Txt Tokens per Sec:     5510 || Lr: 0.000100
2024-02-03 06:48:43,875 Epoch 614: Total Training Recognition Loss 0.21  Total Training Translation Loss 3.91 
2024-02-03 06:48:43,876 EPOCH 615
2024-02-03 06:48:48,966 [Epoch: 615 Step: 00041200] Batch Recognition Loss:   0.000476 => Gls Tokens per Sec:     1932 || Batch Translation Loss:   0.096266 => Txt Tokens per Sec:     5383 || Lr: 0.000100
2024-02-03 06:48:49,372 Epoch 615: Total Training Recognition Loss 0.14  Total Training Translation Loss 2.96 
2024-02-03 06:48:49,372 EPOCH 616
2024-02-03 06:48:54,682 Epoch 616: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.38 
2024-02-03 06:48:54,682 EPOCH 617
2024-02-03 06:48:56,842 [Epoch: 617 Step: 00041300] Batch Recognition Loss:   0.001021 => Gls Tokens per Sec:     2033 || Batch Translation Loss:   0.021337 => Txt Tokens per Sec:     5467 || Lr: 0.000100
2024-02-03 06:49:00,128 Epoch 617: Total Training Recognition Loss 0.09  Total Training Translation Loss 2.49 
2024-02-03 06:49:00,128 EPOCH 618
2024-02-03 06:49:04,620 [Epoch: 618 Step: 00041400] Batch Recognition Loss:   0.000915 => Gls Tokens per Sec:     2153 || Batch Translation Loss:   0.144234 => Txt Tokens per Sec:     5920 || Lr: 0.000100
2024-02-03 06:49:05,260 Epoch 618: Total Training Recognition Loss 0.11  Total Training Translation Loss 4.87 
2024-02-03 06:49:05,261 EPOCH 619
2024-02-03 06:49:10,538 Epoch 619: Total Training Recognition Loss 0.12  Total Training Translation Loss 6.14 
2024-02-03 06:49:10,538 EPOCH 620
2024-02-03 06:49:12,476 [Epoch: 620 Step: 00041500] Batch Recognition Loss:   0.001184 => Gls Tokens per Sec:     2230 || Batch Translation Loss:   0.520730 => Txt Tokens per Sec:     6395 || Lr: 0.000100
2024-02-03 06:49:15,371 Epoch 620: Total Training Recognition Loss 0.09  Total Training Translation Loss 8.97 
2024-02-03 06:49:15,372 EPOCH 621
2024-02-03 06:49:20,368 [Epoch: 621 Step: 00041600] Batch Recognition Loss:   0.000465 => Gls Tokens per Sec:     1904 || Batch Translation Loss:   0.103452 => Txt Tokens per Sec:     5328 || Lr: 0.000100
2024-02-03 06:49:20,813 Epoch 621: Total Training Recognition Loss 0.16  Total Training Translation Loss 7.25 
2024-02-03 06:49:20,814 EPOCH 622
2024-02-03 06:49:26,280 Epoch 622: Total Training Recognition Loss 0.10  Total Training Translation Loss 6.64 
2024-02-03 06:49:26,280 EPOCH 623
2024-02-03 06:49:28,071 [Epoch: 623 Step: 00041700] Batch Recognition Loss:   0.002115 => Gls Tokens per Sec:     2323 || Batch Translation Loss:   0.084046 => Txt Tokens per Sec:     6437 || Lr: 0.000100
2024-02-03 06:49:31,484 Epoch 623: Total Training Recognition Loss 0.11  Total Training Translation Loss 8.34 
2024-02-03 06:49:31,485 EPOCH 624
2024-02-03 06:49:36,001 [Epoch: 624 Step: 00041800] Batch Recognition Loss:   0.002175 => Gls Tokens per Sec:     2091 || Batch Translation Loss:   0.078008 => Txt Tokens per Sec:     5772 || Lr: 0.000100
2024-02-03 06:49:36,606 Epoch 624: Total Training Recognition Loss 0.09  Total Training Translation Loss 4.08 
2024-02-03 06:49:36,606 EPOCH 625
2024-02-03 06:49:41,815 Epoch 625: Total Training Recognition Loss 0.09  Total Training Translation Loss 3.82 
2024-02-03 06:49:41,816 EPOCH 626
2024-02-03 06:49:43,643 [Epoch: 626 Step: 00041900] Batch Recognition Loss:   0.001024 => Gls Tokens per Sec:     2191 || Batch Translation Loss:   0.024965 => Txt Tokens per Sec:     5911 || Lr: 0.000100
2024-02-03 06:49:46,992 Epoch 626: Total Training Recognition Loss 0.10  Total Training Translation Loss 3.78 
2024-02-03 06:49:46,993 EPOCH 627
2024-02-03 06:49:51,095 [Epoch: 627 Step: 00042000] Batch Recognition Loss:   0.002515 => Gls Tokens per Sec:     2240 || Batch Translation Loss:   0.019373 => Txt Tokens per Sec:     6291 || Lr: 0.000100
2024-02-03 06:49:59,464 Validation result at epoch 627, step    42000: duration: 8.3687s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 6.55289	Translation Loss: 89120.17969	PPL: 10470.89062
	Eval Metric: BLEU
	WER 3.68	(DEL: 0.00,	INS: 0.00,	SUB: 3.68)
	BLEU-4 0.40	(BLEU-1: 9.52,	BLEU-2: 2.48,	BLEU-3: 0.83,	BLEU-4: 0.40)
	CHRF 16.33	ROUGE 7.99
2024-02-03 06:49:59,465 Logging Recognition and Translation Outputs
2024-02-03 06:49:59,465 ========================================================================================================================
2024-02-03 06:49:59,465 Logging Sequence: 130_18.00
2024-02-03 06:49:59,465 	Gloss Reference :	A B+C+D+E
2024-02-03 06:49:59,465 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:49:59,466 	Gloss Alignment :	         
2024-02-03 06:49:59,466 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:49:59,467 	Text Reference  :	when he was only 14 years old he   participated in   the 2008 olympic games in  beijing china
2024-02-03 06:49:59,467 	Text Hypothesis :	**** he *** is   a  diver had been diving       from the **** ******* ***** age of      7    
2024-02-03 06:49:59,467 	Text Alignment  :	D       D   S    S  S     S   S    S            S        D    D       D     S   S       S    
2024-02-03 06:49:59,467 ========================================================================================================================
2024-02-03 06:49:59,468 Logging Sequence: 106_112.00
2024-02-03 06:49:59,468 	Gloss Reference :	A B+C+D+E
2024-02-03 06:49:59,468 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:49:59,468 	Gloss Alignment :	         
2024-02-03 06:49:59,468 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:49:59,469 	Text Reference  :	*** *** **** ***** ********** ****** * ***** ** this was    indian team's 7th  asia cup t20 win
2024-02-03 06:49:59,469 	Text Hypothesis :	now the deaf youth contingent bagged a total of 6    medals we     will   sold out  for rs  14 
2024-02-03 06:49:59,469 	Text Alignment  :	I   I   I    I     I          I      I I     I  S    S      S      S      S    S    S   S   S  
2024-02-03 06:49:59,470 ========================================================================================================================
2024-02-03 06:49:59,470 Logging Sequence: 109_161.00
2024-02-03 06:49:59,470 	Gloss Reference :	A B+C+D+E
2024-02-03 06:49:59,470 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:49:59,470 	Gloss Alignment :	         
2024-02-03 06:49:59,470 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:49:59,471 	Text Reference  :	he ******** **** **** had     severe pain in  his ******* *** lower     abdomen
2024-02-03 06:49:59,471 	Text Hypothesis :	he believed that this brought him    luck for his batting and captaincy skills 
2024-02-03 06:49:59,471 	Text Alignment  :	   I        I    I    S       S      S    S       I       I   S         S      
2024-02-03 06:49:59,471 ========================================================================================================================
2024-02-03 06:49:59,471 Logging Sequence: 136_202.00
2024-02-03 06:49:59,472 	Gloss Reference :	A B+C+D+E    
2024-02-03 06:49:59,472 	Gloss Hypothesis:	A B+C+B+C+B+D
2024-02-03 06:49:59,472 	Gloss Alignment :	  S          
2024-02-03 06:49:59,472 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:49:59,473 	Text Reference  :	* ***** ** *** *** ** ***** now    we      will  have to  wait for    updates
2024-02-03 06:49:59,473 	Text Hypothesis :	a video of the ipl is being played between india and  kkr on   social media  
2024-02-03 06:49:59,473 	Text Alignment  :	I I     I  I   I   I  I     S      S       S     S    S   S    S      S      
2024-02-03 06:49:59,473 ========================================================================================================================
2024-02-03 06:49:59,473 Logging Sequence: 140_2.00
2024-02-03 06:49:59,474 	Gloss Reference :	A B+C+D+E
2024-02-03 06:49:59,474 	Gloss Hypothesis:	A B+C+D  
2024-02-03 06:49:59,474 	Gloss Alignment :	  S      
2024-02-03 06:49:59,474 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:49:59,475 	Text Reference  :	* **** **** ********* ** ******** ************* indian batsman-wicket keeper rishabh  pant has    outstanding skills in    cricket
2024-02-03 06:49:59,475 	Text Hypothesis :	i have been embroiled in multiple controversies like   affairs        sex    scandals with models and         many   other issues 
2024-02-03 06:49:59,475 	Text Alignment  :	I I    I    I         I  I        I             S      S              S      S        S    S      S           S      S     S      
2024-02-03 06:49:59,475 ========================================================================================================================
2024-02-03 06:50:00,074 Epoch 627: Total Training Recognition Loss 0.10  Total Training Translation Loss 3.27 
2024-02-03 06:50:00,074 EPOCH 628
2024-02-03 06:50:05,653 Epoch 628: Total Training Recognition Loss 0.16  Total Training Translation Loss 1.94 
2024-02-03 06:50:05,654 EPOCH 629
2024-02-03 06:50:07,288 [Epoch: 629 Step: 00042100] Batch Recognition Loss:   0.000298 => Gls Tokens per Sec:     2351 || Batch Translation Loss:   0.010479 => Txt Tokens per Sec:     6212 || Lr: 0.000050
2024-02-03 06:50:10,835 Epoch 629: Total Training Recognition Loss 0.10  Total Training Translation Loss 1.23 
2024-02-03 06:50:10,835 EPOCH 630
2024-02-03 06:50:15,130 [Epoch: 630 Step: 00042200] Batch Recognition Loss:   0.000099 => Gls Tokens per Sec:     2124 || Batch Translation Loss:   0.010230 => Txt Tokens per Sec:     5939 || Lr: 0.000050
2024-02-03 06:50:16,039 Epoch 630: Total Training Recognition Loss 0.09  Total Training Translation Loss 1.15 
2024-02-03 06:50:16,040 EPOCH 631
2024-02-03 06:50:21,447 Epoch 631: Total Training Recognition Loss 0.10  Total Training Translation Loss 0.95 
2024-02-03 06:50:21,448 EPOCH 632
2024-02-03 06:50:23,147 [Epoch: 632 Step: 00042300] Batch Recognition Loss:   0.000178 => Gls Tokens per Sec:     2114 || Batch Translation Loss:   0.014014 => Txt Tokens per Sec:     5662 || Lr: 0.000050
2024-02-03 06:50:26,946 Epoch 632: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.01 
2024-02-03 06:50:26,946 EPOCH 633
2024-02-03 06:50:30,906 [Epoch: 633 Step: 00042400] Batch Recognition Loss:   0.000355 => Gls Tokens per Sec:     2241 || Batch Translation Loss:   0.022058 => Txt Tokens per Sec:     6155 || Lr: 0.000050
2024-02-03 06:50:31,887 Epoch 633: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.01 
2024-02-03 06:50:31,888 EPOCH 634
2024-02-03 06:50:37,431 Epoch 634: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.18 
2024-02-03 06:50:37,432 EPOCH 635
2024-02-03 06:50:38,939 [Epoch: 635 Step: 00042500] Batch Recognition Loss:   0.000504 => Gls Tokens per Sec:     2277 || Batch Translation Loss:   0.016414 => Txt Tokens per Sec:     6322 || Lr: 0.000050
2024-02-03 06:50:42,681 Epoch 635: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.44 
2024-02-03 06:50:42,681 EPOCH 636
2024-02-03 06:50:46,830 [Epoch: 636 Step: 00042600] Batch Recognition Loss:   0.000301 => Gls Tokens per Sec:     2100 || Batch Translation Loss:   0.007754 => Txt Tokens per Sec:     5876 || Lr: 0.000050
2024-02-03 06:50:47,631 Epoch 636: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.25 
2024-02-03 06:50:47,631 EPOCH 637
2024-02-03 06:50:52,834 Epoch 637: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.14 
2024-02-03 06:50:52,834 EPOCH 638
2024-02-03 06:50:54,298 [Epoch: 638 Step: 00042700] Batch Recognition Loss:   0.000999 => Gls Tokens per Sec:     2297 || Batch Translation Loss:   0.013389 => Txt Tokens per Sec:     6291 || Lr: 0.000050
2024-02-03 06:50:57,892 Epoch 638: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.31 
2024-02-03 06:50:57,892 EPOCH 639
2024-02-03 06:51:02,045 [Epoch: 639 Step: 00042800] Batch Recognition Loss:   0.000441 => Gls Tokens per Sec:     2059 || Batch Translation Loss:   0.011359 => Txt Tokens per Sec:     5682 || Lr: 0.000050
2024-02-03 06:51:03,214 Epoch 639: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.12 
2024-02-03 06:51:03,214 EPOCH 640
2024-02-03 06:51:08,604 Epoch 640: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.22 
2024-02-03 06:51:08,605 EPOCH 641
2024-02-03 06:51:10,263 [Epoch: 641 Step: 00042900] Batch Recognition Loss:   0.000097 => Gls Tokens per Sec:     1877 || Batch Translation Loss:   0.009833 => Txt Tokens per Sec:     5174 || Lr: 0.000050
2024-02-03 06:51:13,924 Epoch 641: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.10 
2024-02-03 06:51:13,924 EPOCH 642
2024-02-03 06:51:17,573 [Epoch: 642 Step: 00043000] Batch Recognition Loss:   0.002670 => Gls Tokens per Sec:     2325 || Batch Translation Loss:   0.018509 => Txt Tokens per Sec:     6476 || Lr: 0.000050
2024-02-03 06:51:18,688 Epoch 642: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.05 
2024-02-03 06:51:18,688 EPOCH 643
2024-02-03 06:51:24,212 Epoch 643: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.38 
2024-02-03 06:51:24,213 EPOCH 644
2024-02-03 06:51:25,705 [Epoch: 644 Step: 00043100] Batch Recognition Loss:   0.000314 => Gls Tokens per Sec:     2038 || Batch Translation Loss:   0.010953 => Txt Tokens per Sec:     5527 || Lr: 0.000050
2024-02-03 06:51:29,705 Epoch 644: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.93 
2024-02-03 06:51:29,706 EPOCH 645
2024-02-03 06:51:33,655 [Epoch: 645 Step: 00043200] Batch Recognition Loss:   0.000147 => Gls Tokens per Sec:     2084 || Batch Translation Loss:   0.014658 => Txt Tokens per Sec:     5651 || Lr: 0.000050
2024-02-03 06:51:35,063 Epoch 645: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.35 
2024-02-03 06:51:35,064 EPOCH 646
2024-02-03 06:51:40,179 Epoch 646: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.95 
2024-02-03 06:51:40,179 EPOCH 647
2024-02-03 06:51:41,294 [Epoch: 647 Step: 00043300] Batch Recognition Loss:   0.003080 => Gls Tokens per Sec:     2502 || Batch Translation Loss:   0.014259 => Txt Tokens per Sec:     6437 || Lr: 0.000050
2024-02-03 06:51:45,080 Epoch 647: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.05 
2024-02-03 06:51:45,080 EPOCH 648
2024-02-03 06:51:49,293 [Epoch: 648 Step: 00043400] Batch Recognition Loss:   0.001148 => Gls Tokens per Sec:     1916 || Batch Translation Loss:   0.011162 => Txt Tokens per Sec:     5359 || Lr: 0.000050
2024-02-03 06:51:50,508 Epoch 648: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.34 
2024-02-03 06:51:50,508 EPOCH 649
2024-02-03 06:51:55,854 Epoch 649: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.21 
2024-02-03 06:51:55,855 EPOCH 650
2024-02-03 06:51:57,192 [Epoch: 650 Step: 00043500] Batch Recognition Loss:   0.000187 => Gls Tokens per Sec:     1970 || Batch Translation Loss:   0.007279 => Txt Tokens per Sec:     5343 || Lr: 0.000050
2024-02-03 06:52:00,831 Epoch 650: Total Training Recognition Loss 0.10  Total Training Translation Loss 1.37 
2024-02-03 06:52:00,831 EPOCH 651
2024-02-03 06:52:04,728 [Epoch: 651 Step: 00043600] Batch Recognition Loss:   0.000105 => Gls Tokens per Sec:     2031 || Batch Translation Loss:   0.030841 => Txt Tokens per Sec:     5807 || Lr: 0.000050
2024-02-03 06:52:06,000 Epoch 651: Total Training Recognition Loss 0.09  Total Training Translation Loss 3.37 
2024-02-03 06:52:06,000 EPOCH 652
2024-02-03 06:52:11,477 Epoch 652: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.26 
2024-02-03 06:52:11,478 EPOCH 653
2024-02-03 06:52:12,877 [Epoch: 653 Step: 00043700] Batch Recognition Loss:   0.000390 => Gls Tokens per Sec:     1832 || Batch Translation Loss:   0.033913 => Txt Tokens per Sec:     5052 || Lr: 0.000050
2024-02-03 06:52:17,037 Epoch 653: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.79 
2024-02-03 06:52:17,037 EPOCH 654
2024-02-03 06:52:20,499 [Epoch: 654 Step: 00043800] Batch Recognition Loss:   0.004188 => Gls Tokens per Sec:     2265 || Batch Translation Loss:   0.028720 => Txt Tokens per Sec:     6320 || Lr: 0.000050
2024-02-03 06:52:22,010 Epoch 654: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.58 
2024-02-03 06:52:22,011 EPOCH 655
2024-02-03 06:52:27,189 Epoch 655: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.84 
2024-02-03 06:52:27,189 EPOCH 656
2024-02-03 06:52:28,132 [Epoch: 656 Step: 00043900] Batch Recognition Loss:   0.000263 => Gls Tokens per Sec:     2452 || Batch Translation Loss:   0.030356 => Txt Tokens per Sec:     6632 || Lr: 0.000050
2024-02-03 06:52:32,454 Epoch 656: Total Training Recognition Loss 0.07  Total Training Translation Loss 6.03 
2024-02-03 06:52:32,454 EPOCH 657
2024-02-03 06:52:36,272 [Epoch: 657 Step: 00044000] Batch Recognition Loss:   0.000461 => Gls Tokens per Sec:     1989 || Batch Translation Loss:   0.022554 => Txt Tokens per Sec:     5613 || Lr: 0.000050
2024-02-03 06:52:44,838 Validation result at epoch 657, step    44000: duration: 8.5657s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 5.17718	Translation Loss: 86940.08594	PPL: 8349.20410
	Eval Metric: BLEU
	WER 3.39	(DEL: 0.00,	INS: 0.00,	SUB: 3.39)
	BLEU-4 0.36	(BLEU-1: 9.62,	BLEU-2: 2.30,	BLEU-3: 0.83,	BLEU-4: 0.36)
	CHRF 16.50	ROUGE 8.08
2024-02-03 06:52:44,839 Logging Recognition and Translation Outputs
2024-02-03 06:52:44,840 ========================================================================================================================
2024-02-03 06:52:44,840 Logging Sequence: 101_234.00
2024-02-03 06:52:44,840 	Gloss Reference :	A B+C+D+E        
2024-02-03 06:52:44,840 	Gloss Hypothesis:	A B+C+D+B+C+D+E+C
2024-02-03 06:52:44,840 	Gloss Alignment :	  S              
2024-02-03 06:52:44,840 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:52:44,842 	Text Reference  :	now similarly in u-19 world cup     the     winning  runs    were scored      by    hitting a      six       
2024-02-03 06:52:44,842 	Text Hypothesis :	*** ********* ** **** the   taliban swiftly regained control of   afghanistan after us      troops withdrawal
2024-02-03 06:52:44,842 	Text Alignment  :	D   D         D  D    S     S       S       S        S       S    S           S     S       S      S         
2024-02-03 06:52:44,842 ========================================================================================================================
2024-02-03 06:52:44,842 Logging Sequence: 156_260.00
2024-02-03 06:52:44,843 	Gloss Reference :	A B+C+D+E
2024-02-03 06:52:44,843 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:52:44,843 	Gloss Alignment :	         
2024-02-03 06:52:44,843 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:52:44,846 	Text Reference  :	this was because of some fine batting from nicholas pooran pooran smacked an   unbeaten knock of  137  runs off 55      balls and led miny to   victory
2024-02-03 06:52:44,846 	Text Hypothesis :	**** the final   of **** **** ******* **** ******** the    mlc    was     held between  mi    new york miny and seattle orcas sor on  30th july 2023   
2024-02-03 06:52:44,846 	Text Alignment  :	D    S   S          D    D    D       D    D        S      S      S       S    S        S     S   S    S    S   S       S     S   S   S    S    S      
2024-02-03 06:52:44,846 ========================================================================================================================
2024-02-03 06:52:44,847 Logging Sequence: 78_16.00
2024-02-03 06:52:44,847 	Gloss Reference :	A B+C+D+E
2024-02-03 06:52:44,847 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:52:44,847 	Gloss Alignment :	         
2024-02-03 06:52:44,847 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:52:44,848 	Text Reference  :	********* *** the  full csk   team was very sad 
2024-02-03 06:52:44,848 	Text Hypothesis :	yesterday was sure you  think when he  kept 2021
2024-02-03 06:52:44,848 	Text Alignment  :	I         I   S    S    S     S    S   S    S   
2024-02-03 06:52:44,848 ========================================================================================================================
2024-02-03 06:52:44,848 Logging Sequence: 166_45.00
2024-02-03 06:52:44,849 	Gloss Reference :	A B+C+D+E
2024-02-03 06:52:44,849 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:52:44,849 	Gloss Alignment :	         
2024-02-03 06:52:44,849 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:52:44,850 	Text Reference  :	after an odi or a t20 match  the   next day is  a   rest day    
2024-02-03 06:52:44,850 	Text Hypothesis :	***** ** *** ** * the second world cups for odi and t20i formats
2024-02-03 06:52:44,850 	Text Alignment  :	D     D  D   D  D S   S      S     S    S   S   S   S    S      
2024-02-03 06:52:44,850 ========================================================================================================================
2024-02-03 06:52:44,851 Logging Sequence: 105_139.00
2024-02-03 06:52:44,851 	Gloss Reference :	A B+C+D+E
2024-02-03 06:52:44,851 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:52:44,851 	Gloss Alignment :	         
2024-02-03 06:52:44,851 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:52:44,852 	Text Reference  :	and now he has finally achieved his      dream by      defeating carlsen
2024-02-03 06:52:44,852 	Text Hypothesis :	*** *** ** *** then    the      pakistan team  started its       innings
2024-02-03 06:52:44,852 	Text Alignment  :	D   D   D  D   S       S        S        S     S       S         S      
2024-02-03 06:52:44,852 ========================================================================================================================
2024-02-03 06:52:46,220 Epoch 657: Total Training Recognition Loss 0.10  Total Training Translation Loss 3.66 
2024-02-03 06:52:46,220 EPOCH 658
2024-02-03 06:52:51,532 Epoch 658: Total Training Recognition Loss 0.08  Total Training Translation Loss 3.82 
2024-02-03 06:52:51,532 EPOCH 659
2024-02-03 06:52:52,710 [Epoch: 659 Step: 00044100] Batch Recognition Loss:   0.000929 => Gls Tokens per Sec:     1902 || Batch Translation Loss:   0.022405 => Txt Tokens per Sec:     5340 || Lr: 0.000050
2024-02-03 06:52:56,820 Epoch 659: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.57 
2024-02-03 06:52:56,820 EPOCH 660
2024-02-03 06:53:00,029 [Epoch: 660 Step: 00044200] Batch Recognition Loss:   0.001288 => Gls Tokens per Sec:     2344 || Batch Translation Loss:   0.017948 => Txt Tokens per Sec:     6461 || Lr: 0.000050
2024-02-03 06:53:01,522 Epoch 660: Total Training Recognition Loss 0.11  Total Training Translation Loss 1.45 
2024-02-03 06:53:01,523 EPOCH 661
2024-02-03 06:53:07,146 Epoch 661: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.15 
2024-02-03 06:53:07,146 EPOCH 662
2024-02-03 06:53:08,081 [Epoch: 662 Step: 00044300] Batch Recognition Loss:   0.000104 => Gls Tokens per Sec:     2225 || Batch Translation Loss:   0.009396 => Txt Tokens per Sec:     6128 || Lr: 0.000050
2024-02-03 06:53:12,763 Epoch 662: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.03 
2024-02-03 06:53:12,764 EPOCH 663
2024-02-03 06:53:16,326 [Epoch: 663 Step: 00044400] Batch Recognition Loss:   0.004663 => Gls Tokens per Sec:     2042 || Batch Translation Loss:   0.017261 => Txt Tokens per Sec:     5738 || Lr: 0.000050
2024-02-03 06:53:17,766 Epoch 663: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.96 
2024-02-03 06:53:17,766 EPOCH 664
2024-02-03 06:53:23,194 Epoch 664: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.97 
2024-02-03 06:53:23,195 EPOCH 665
2024-02-03 06:53:24,360 [Epoch: 665 Step: 00044500] Batch Recognition Loss:   0.000974 => Gls Tokens per Sec:     1572 || Batch Translation Loss:   0.017149 => Txt Tokens per Sec:     4910 || Lr: 0.000050
2024-02-03 06:53:28,689 Epoch 665: Total Training Recognition Loss 0.06  Total Training Translation Loss 0.90 
2024-02-03 06:53:28,689 EPOCH 666
2024-02-03 06:53:31,976 [Epoch: 666 Step: 00044600] Batch Recognition Loss:   0.001282 => Gls Tokens per Sec:     2164 || Batch Translation Loss:   0.003781 => Txt Tokens per Sec:     5907 || Lr: 0.000050
2024-02-03 06:53:33,999 Epoch 666: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.94 
2024-02-03 06:53:34,000 EPOCH 667
2024-02-03 06:53:39,454 Epoch 667: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.79 
2024-02-03 06:53:39,455 EPOCH 668
2024-02-03 06:53:40,200 [Epoch: 668 Step: 00044700] Batch Recognition Loss:   0.000407 => Gls Tokens per Sec:     2366 || Batch Translation Loss:   0.012515 => Txt Tokens per Sec:     6164 || Lr: 0.000050
2024-02-03 06:53:44,689 Epoch 668: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.45 
2024-02-03 06:53:44,690 EPOCH 669
2024-02-03 06:53:48,176 [Epoch: 669 Step: 00044800] Batch Recognition Loss:   0.000372 => Gls Tokens per Sec:     2021 || Batch Translation Loss:   0.021643 => Txt Tokens per Sec:     5710 || Lr: 0.000050
2024-02-03 06:53:49,989 Epoch 669: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.43 
2024-02-03 06:53:49,989 EPOCH 670
2024-02-03 06:53:55,422 Epoch 670: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.78 
2024-02-03 06:53:55,423 EPOCH 671
2024-02-03 06:53:56,058 [Epoch: 671 Step: 00044900] Batch Recognition Loss:   0.001399 => Gls Tokens per Sec:     2524 || Batch Translation Loss:   0.023444 => Txt Tokens per Sec:     6790 || Lr: 0.000050
2024-02-03 06:54:00,516 Epoch 671: Total Training Recognition Loss 0.10  Total Training Translation Loss 2.47 
2024-02-03 06:54:00,517 EPOCH 672
2024-02-03 06:54:04,061 [Epoch: 672 Step: 00045000] Batch Recognition Loss:   0.000623 => Gls Tokens per Sec:     1917 || Batch Translation Loss:   0.019711 => Txt Tokens per Sec:     5340 || Lr: 0.000050
2024-02-03 06:54:05,695 Epoch 672: Total Training Recognition Loss 0.11  Total Training Translation Loss 2.90 
2024-02-03 06:54:05,695 EPOCH 673
2024-02-03 06:54:10,634 Epoch 673: Total Training Recognition Loss 0.08  Total Training Translation Loss 4.14 
2024-02-03 06:54:10,635 EPOCH 674
2024-02-03 06:54:11,424 [Epoch: 674 Step: 00045100] Batch Recognition Loss:   0.000140 => Gls Tokens per Sec:     1715 || Batch Translation Loss:   0.021100 => Txt Tokens per Sec:     5220 || Lr: 0.000050
2024-02-03 06:54:16,026 Epoch 674: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.92 
2024-02-03 06:54:16,026 EPOCH 675
2024-02-03 06:54:19,349 [Epoch: 675 Step: 00045200] Batch Recognition Loss:   0.000124 => Gls Tokens per Sec:     2023 || Batch Translation Loss:   0.019153 => Txt Tokens per Sec:     5537 || Lr: 0.000050
2024-02-03 06:54:21,565 Epoch 675: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.68 
2024-02-03 06:54:21,566 EPOCH 676
2024-02-03 06:54:26,858 Epoch 676: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.16 
2024-02-03 06:54:26,859 EPOCH 677
2024-02-03 06:54:27,442 [Epoch: 677 Step: 00045300] Batch Recognition Loss:   0.001502 => Gls Tokens per Sec:     2197 || Batch Translation Loss:   0.021642 => Txt Tokens per Sec:     5980 || Lr: 0.000050
2024-02-03 06:54:32,173 Epoch 677: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.16 
2024-02-03 06:54:32,173 EPOCH 678
2024-02-03 06:54:35,576 [Epoch: 678 Step: 00045400] Batch Recognition Loss:   0.002707 => Gls Tokens per Sec:     1902 || Batch Translation Loss:   0.016341 => Txt Tokens per Sec:     5495 || Lr: 0.000050
2024-02-03 06:54:37,483 Epoch 678: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.09 
2024-02-03 06:54:37,483 EPOCH 679
2024-02-03 06:54:43,107 Epoch 679: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.25 
2024-02-03 06:54:43,107 EPOCH 680
2024-02-03 06:54:43,617 [Epoch: 680 Step: 00045500] Batch Recognition Loss:   0.000218 => Gls Tokens per Sec:     2205 || Batch Translation Loss:   0.016568 => Txt Tokens per Sec:     6138 || Lr: 0.000050
2024-02-03 06:54:48,479 Epoch 680: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.79 
2024-02-03 06:54:48,479 EPOCH 681
2024-02-03 06:54:51,619 [Epoch: 681 Step: 00045600] Batch Recognition Loss:   0.003494 => Gls Tokens per Sec:     2011 || Batch Translation Loss:   0.011189 => Txt Tokens per Sec:     5498 || Lr: 0.000050
2024-02-03 06:54:53,926 Epoch 681: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.87 
2024-02-03 06:54:53,926 EPOCH 682
2024-02-03 06:54:59,047 Epoch 682: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.65 
2024-02-03 06:54:59,047 EPOCH 683
2024-02-03 06:54:59,493 [Epoch: 683 Step: 00045700] Batch Recognition Loss:   0.000364 => Gls Tokens per Sec:     2157 || Batch Translation Loss:   0.007691 => Txt Tokens per Sec:     5813 || Lr: 0.000050
2024-02-03 06:55:04,633 Epoch 683: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.74 
2024-02-03 06:55:04,634 EPOCH 684
2024-02-03 06:55:07,827 [Epoch: 684 Step: 00045800] Batch Recognition Loss:   0.000230 => Gls Tokens per Sec:     1955 || Batch Translation Loss:   0.039301 => Txt Tokens per Sec:     5445 || Lr: 0.000050
2024-02-03 06:55:10,075 Epoch 684: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.93 
2024-02-03 06:55:10,075 EPOCH 685
2024-02-03 06:55:15,408 Epoch 685: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.24 
2024-02-03 06:55:15,409 EPOCH 686
2024-02-03 06:55:15,759 [Epoch: 686 Step: 00045900] Batch Recognition Loss:   0.000447 => Gls Tokens per Sec:     2292 || Batch Translation Loss:   0.110723 => Txt Tokens per Sec:     6398 || Lr: 0.000050
2024-02-03 06:55:20,637 Epoch 686: Total Training Recognition Loss 0.09  Total Training Translation Loss 3.79 
2024-02-03 06:55:20,637 EPOCH 687
2024-02-03 06:55:23,739 [Epoch: 687 Step: 00046000] Batch Recognition Loss:   0.000152 => Gls Tokens per Sec:     1932 || Batch Translation Loss:   0.056282 => Txt Tokens per Sec:     5354 || Lr: 0.000050
2024-02-03 06:55:32,128 Validation result at epoch 687, step    46000: duration: 8.3880s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 5.14707	Translation Loss: 86069.35156	PPL: 7627.25781
	Eval Metric: BLEU
	WER 3.47	(DEL: 0.00,	INS: 0.00,	SUB: 3.47)
	BLEU-4 0.49	(BLEU-1: 9.58,	BLEU-2: 2.55,	BLEU-3: 0.98,	BLEU-4: 0.49)
	CHRF 16.83	ROUGE 7.84
2024-02-03 06:55:32,128 Logging Recognition and Translation Outputs
2024-02-03 06:55:32,128 ========================================================================================================================
2024-02-03 06:55:32,129 Logging Sequence: 83_108.00
2024-02-03 06:55:32,129 	Gloss Reference :	A B+C+D+E
2024-02-03 06:55:32,129 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:55:32,129 	Gloss Alignment :	         
2024-02-03 06:55:32,129 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:55:32,131 	Text Reference  :	****** ***** eriksen' wife sabrina kvist  jensen rushed to the       pitch on      hearing   about  her husband
2024-02-03 06:55:32,131 	Text Hypothesis :	infact there was      no   one     around him    when   he collapsed his   shocked teammates rushed to  help   
2024-02-03 06:55:32,131 	Text Alignment  :	I      I     S        S    S       S      S      S      S  S         S     S       S         S      S   S      
2024-02-03 06:55:32,131 ========================================================================================================================
2024-02-03 06:55:32,131 Logging Sequence: 171_150.00
2024-02-03 06:55:32,131 	Gloss Reference :	A B+C+D+E
2024-02-03 06:55:32,132 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:55:32,132 	Gloss Alignment :	         
2024-02-03 06:55:32,132 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:55:32,134 	Text Reference  :	as per  rules players can be  fined for     purposely delaying a  match which   runs    on    a   fixed schedule
2024-02-03 06:55:32,134 	Text Hypothesis :	** then april 2019    she was filed playing with      him      as the   support support staff saw the   help    
2024-02-03 06:55:32,134 	Text Alignment  :	D  S    S     S       S   S   S     S       S         S        S  S     S       S       S     S   S     S       
2024-02-03 06:55:32,134 ========================================================================================================================
2024-02-03 06:55:32,134 Logging Sequence: 72_139.00
2024-02-03 06:55:32,134 	Gloss Reference :	A B+C+D+E
2024-02-03 06:55:32,134 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:55:32,134 	Gloss Alignment :	         
2024-02-03 06:55:32,135 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:55:32,136 	Text Reference  :	when shah was taking a u-turn all    those on   the bike    got    down and broke   the ****** windshield of   the car
2024-02-03 06:55:32,136 	Text Hypothesis :	**** **** *** ****** a ****** report said  that the taliban wanted icc  to  replace the afghan flag       with its own
2024-02-03 06:55:32,137 	Text Alignment  :	D    D    D   D        D      S      S     S        S       S      S    S   S           I      S          S    S   S  
2024-02-03 06:55:32,137 ========================================================================================================================
2024-02-03 06:55:32,137 Logging Sequence: 161_47.00
2024-02-03 06:55:32,137 	Gloss Reference :	A B+C+D+E
2024-02-03 06:55:32,137 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:55:32,137 	Gloss Alignment :	         
2024-02-03 06:55:32,137 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:55:32,138 	Text Reference  :	he requested confidentiality as he      was   planning to           make   an official announcement
2024-02-03 06:55:32,138 	Text Hypothesis :	** ********* *************** ** sanjana began her      professional career as a        model       
2024-02-03 06:55:32,138 	Text Alignment  :	D  D         D               D  S       S     S        S            S      S  S        S           
2024-02-03 06:55:32,138 ========================================================================================================================
2024-02-03 06:55:32,139 Logging Sequence: 118_284.00
2024-02-03 06:55:32,139 	Gloss Reference :	A B+C+D+E
2024-02-03 06:55:32,139 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:55:32,139 	Gloss Alignment :	         
2024-02-03 06:55:32,139 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:55:32,140 	Text Reference  :	**** ***** ****** the ***** second time   they won in    1986 under the leadership of   diego maradona
2024-02-03 06:55:32,141 	Text Hypothesis :	when messi lifted the world cup    trophy he   was given a    cloak to  wear       over his   jersey  
2024-02-03 06:55:32,141 	Text Alignment  :	I    I     I          I     S      S      S    S   S     S    S     S   S          S    S     S       
2024-02-03 06:55:32,141 ========================================================================================================================
2024-02-03 06:55:34,643 Epoch 687: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.00 
2024-02-03 06:55:34,644 EPOCH 688
2024-02-03 06:55:40,243 Epoch 688: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.66 
2024-02-03 06:55:40,243 EPOCH 689
2024-02-03 06:55:40,475 [Epoch: 689 Step: 00046100] Batch Recognition Loss:   0.000842 => Gls Tokens per Sec:     2771 || Batch Translation Loss:   0.012116 => Txt Tokens per Sec:     6377 || Lr: 0.000050
2024-02-03 06:55:45,593 Epoch 689: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.57 
2024-02-03 06:55:45,593 EPOCH 690
2024-02-03 06:55:48,647 [Epoch: 690 Step: 00046200] Batch Recognition Loss:   0.000178 => Gls Tokens per Sec:     1909 || Batch Translation Loss:   0.068508 => Txt Tokens per Sec:     5380 || Lr: 0.000050
2024-02-03 06:55:50,971 Epoch 690: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.75 
2024-02-03 06:55:50,971 EPOCH 691
2024-02-03 06:55:56,097 Epoch 691: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.43 
2024-02-03 06:55:56,098 EPOCH 692
2024-02-03 06:55:56,297 [Epoch: 692 Step: 00046300] Batch Recognition Loss:   0.000575 => Gls Tokens per Sec:     2412 || Batch Translation Loss:   0.029741 => Txt Tokens per Sec:     6387 || Lr: 0.000050
2024-02-03 06:56:01,724 Epoch 692: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.25 
2024-02-03 06:56:01,725 EPOCH 693
2024-02-03 06:56:04,824 [Epoch: 693 Step: 00046400] Batch Recognition Loss:   0.000614 => Gls Tokens per Sec:     1830 || Batch Translation Loss:   0.023885 => Txt Tokens per Sec:     5142 || Lr: 0.000050
2024-02-03 06:56:07,260 Epoch 693: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.64 
2024-02-03 06:56:07,260 EPOCH 694
2024-02-03 06:56:12,584 Epoch 694: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.88 
2024-02-03 06:56:12,584 EPOCH 695
2024-02-03 06:56:12,686 [Epoch: 695 Step: 00046500] Batch Recognition Loss:   0.000198 => Gls Tokens per Sec:     3168 || Batch Translation Loss:   0.026393 => Txt Tokens per Sec:     7733 || Lr: 0.000050
2024-02-03 06:56:17,786 Epoch 695: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.84 
2024-02-03 06:56:17,787 EPOCH 696
2024-02-03 06:56:20,761 [Epoch: 696 Step: 00046600] Batch Recognition Loss:   0.000128 => Gls Tokens per Sec:     1854 || Batch Translation Loss:   0.013067 => Txt Tokens per Sec:     5082 || Lr: 0.000050
2024-02-03 06:56:23,413 Epoch 696: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.57 
2024-02-03 06:56:23,414 EPOCH 697
2024-02-03 06:56:28,856 Epoch 697: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.54 
2024-02-03 06:56:28,856 EPOCH 698
2024-02-03 06:56:28,928 [Epoch: 698 Step: 00046700] Batch Recognition Loss:   0.000141 => Gls Tokens per Sec:     2253 || Batch Translation Loss:   0.022910 => Txt Tokens per Sec:     6817 || Lr: 0.000050
2024-02-03 06:56:34,031 Epoch 698: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.03 
2024-02-03 06:56:34,031 EPOCH 699
2024-02-03 06:56:36,788 [Epoch: 699 Step: 00046800] Batch Recognition Loss:   0.000281 => Gls Tokens per Sec:     1941 || Batch Translation Loss:   0.021208 => Txt Tokens per Sec:     5316 || Lr: 0.000050
2024-02-03 06:56:39,387 Epoch 699: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.90 
2024-02-03 06:56:39,388 EPOCH 700
2024-02-03 06:56:44,835 [Epoch: 700 Step: 00046900] Batch Recognition Loss:   0.000406 => Gls Tokens per Sec:     1952 || Batch Translation Loss:   0.017586 => Txt Tokens per Sec:     5428 || Lr: 0.000050
2024-02-03 06:56:44,836 Epoch 700: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.88 
2024-02-03 06:56:44,836 EPOCH 701
2024-02-03 06:56:50,073 Epoch 701: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.78 
2024-02-03 06:56:50,073 EPOCH 702
2024-02-03 06:56:52,332 [Epoch: 702 Step: 00047000] Batch Recognition Loss:   0.000417 => Gls Tokens per Sec:     2300 || Batch Translation Loss:   0.031735 => Txt Tokens per Sec:     6398 || Lr: 0.000050
2024-02-03 06:56:55,157 Epoch 702: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.18 
2024-02-03 06:56:55,158 EPOCH 703
2024-02-03 06:57:00,254 [Epoch: 703 Step: 00047100] Batch Recognition Loss:   0.000243 => Gls Tokens per Sec:     2055 || Batch Translation Loss:   0.012630 => Txt Tokens per Sec:     5706 || Lr: 0.000050
2024-02-03 06:57:00,328 Epoch 703: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.19 
2024-02-03 06:57:00,328 EPOCH 704
2024-02-03 06:57:05,535 Epoch 704: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.45 
2024-02-03 06:57:05,536 EPOCH 705
2024-02-03 06:57:07,836 [Epoch: 705 Step: 00047200] Batch Recognition Loss:   0.000278 => Gls Tokens per Sec:     2188 || Batch Translation Loss:   0.020221 => Txt Tokens per Sec:     5764 || Lr: 0.000050
2024-02-03 06:57:10,931 Epoch 705: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.72 
2024-02-03 06:57:10,932 EPOCH 706
2024-02-03 06:57:15,869 [Epoch: 706 Step: 00047300] Batch Recognition Loss:   0.000126 => Gls Tokens per Sec:     2107 || Batch Translation Loss:   0.018012 => Txt Tokens per Sec:     5831 || Lr: 0.000050
2024-02-03 06:57:16,099 Epoch 706: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.79 
2024-02-03 06:57:16,100 EPOCH 707
2024-02-03 06:57:21,452 Epoch 707: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.77 
2024-02-03 06:57:21,453 EPOCH 708
2024-02-03 06:57:23,683 [Epoch: 708 Step: 00047400] Batch Recognition Loss:   0.000686 => Gls Tokens per Sec:     2185 || Batch Translation Loss:   0.034439 => Txt Tokens per Sec:     6002 || Lr: 0.000050
2024-02-03 06:57:26,468 Epoch 708: Total Training Recognition Loss 0.08  Total Training Translation Loss 3.36 
2024-02-03 06:57:26,469 EPOCH 709
2024-02-03 06:57:31,594 [Epoch: 709 Step: 00047500] Batch Recognition Loss:   0.001742 => Gls Tokens per Sec:     1981 || Batch Translation Loss:   0.311779 => Txt Tokens per Sec:     5463 || Lr: 0.000050
2024-02-03 06:57:31,877 Epoch 709: Total Training Recognition Loss 0.09  Total Training Translation Loss 6.75 
2024-02-03 06:57:31,877 EPOCH 710
2024-02-03 06:57:37,099 Epoch 710: Total Training Recognition Loss 0.10  Total Training Translation Loss 7.10 
2024-02-03 06:57:37,100 EPOCH 711
2024-02-03 06:57:39,283 [Epoch: 711 Step: 00047600] Batch Recognition Loss:   0.000448 => Gls Tokens per Sec:     2201 || Batch Translation Loss:   0.058235 => Txt Tokens per Sec:     6062 || Lr: 0.000050
2024-02-03 06:57:42,237 Epoch 711: Total Training Recognition Loss 0.09  Total Training Translation Loss 2.52 
2024-02-03 06:57:42,237 EPOCH 712
2024-02-03 06:57:46,729 [Epoch: 712 Step: 00047700] Batch Recognition Loss:   0.000571 => Gls Tokens per Sec:     2225 || Batch Translation Loss:   0.022563 => Txt Tokens per Sec:     6197 || Lr: 0.000050
2024-02-03 06:57:47,058 Epoch 712: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.67 
2024-02-03 06:57:47,058 EPOCH 713
2024-02-03 06:57:52,400 Epoch 713: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.89 
2024-02-03 06:57:52,400 EPOCH 714
2024-02-03 06:57:54,473 [Epoch: 714 Step: 00047800] Batch Recognition Loss:   0.000302 => Gls Tokens per Sec:     2197 || Batch Translation Loss:   0.059990 => Txt Tokens per Sec:     6019 || Lr: 0.000050
2024-02-03 06:57:57,669 Epoch 714: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.33 
2024-02-03 06:57:57,670 EPOCH 715
2024-02-03 06:58:02,783 [Epoch: 715 Step: 00047900] Batch Recognition Loss:   0.000244 => Gls Tokens per Sec:     1923 || Batch Translation Loss:   0.016447 => Txt Tokens per Sec:     5327 || Lr: 0.000050
2024-02-03 06:58:03,179 Epoch 715: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.35 
2024-02-03 06:58:03,179 EPOCH 716
2024-02-03 06:58:08,268 Epoch 716: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.41 
2024-02-03 06:58:08,268 EPOCH 717
2024-02-03 06:58:10,457 [Epoch: 717 Step: 00048000] Batch Recognition Loss:   0.000612 => Gls Tokens per Sec:     2007 || Batch Translation Loss:   0.016870 => Txt Tokens per Sec:     5563 || Lr: 0.000050
2024-02-03 06:58:19,145 Validation result at epoch 717, step    48000: duration: 8.6880s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 4.30315	Translation Loss: 86269.92188	PPL: 7787.81592
	Eval Metric: BLEU
	WER 3.11	(DEL: 0.00,	INS: 0.00,	SUB: 3.11)
	BLEU-4 0.64	(BLEU-1: 10.51,	BLEU-2: 3.28,	BLEU-3: 1.23,	BLEU-4: 0.64)
	CHRF 17.04	ROUGE 8.98
2024-02-03 06:58:19,146 Logging Recognition and Translation Outputs
2024-02-03 06:58:19,147 ========================================================================================================================
2024-02-03 06:58:19,147 Logging Sequence: 155_56.00
2024-02-03 06:58:19,147 	Gloss Reference :	A B+C+D+E
2024-02-03 06:58:19,147 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:58:19,147 	Gloss Alignment :	         
2024-02-03 06:58:19,148 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:58:19,148 	Text Reference  :	***** ** ** ****** the        team would have   been  disqualified
2024-02-03 06:58:19,148 	Text Hypothesis :	there is no afghan government and  the   second world cup         
2024-02-03 06:58:19,148 	Text Alignment  :	I     I  I  I      S          S    S     S      S     S           
2024-02-03 06:58:19,149 ========================================================================================================================
2024-02-03 06:58:19,149 Logging Sequence: 161_170.00
2024-02-03 06:58:19,149 	Gloss Reference :	A B+C+D+E
2024-02-03 06:58:19,149 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:58:19,149 	Gloss Alignment :	         
2024-02-03 06:58:19,149 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:58:19,152 	Text Reference  :	* ** **** however when the bcci replaced him   with rohit sharma he         decided to   step  down as test captain as  well
2024-02-03 06:58:19,152 	Text Hypothesis :	i am very proud   of   the game and      there is   no    actual government and     that there is   a  part of      the game
2024-02-03 06:58:19,152 	Text Alignment  :	I I  I    S       S        S    S        S     S    S     S      S          S       S    S     S    S  S    S       S   S   
2024-02-03 06:58:19,152 ========================================================================================================================
2024-02-03 06:58:19,152 Logging Sequence: 88_13.00
2024-02-03 06:58:19,153 	Gloss Reference :	A B+C+D+E
2024-02-03 06:58:19,153 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:58:19,153 	Gloss Alignment :	         
2024-02-03 06:58:19,153 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:58:19,155 	Text Reference  :	while his fans      were elated   by  the victory there were some who are  jealous and want to   kill messi
2024-02-03 06:58:19,155 	Text Hypothesis :	***** his argentina team recently won the ******* ***** **** **** *** fifa world   cup 2022 held in   qatar
2024-02-03 06:58:19,155 	Text Alignment  :	D         S         S    S        S       D       D     D    D    D   S    S       S   S    S    S    S    
2024-02-03 06:58:19,155 ========================================================================================================================
2024-02-03 06:58:19,155 Logging Sequence: 103_25.00
2024-02-03 06:58:19,156 	Gloss Reference :	A B+C+D+E
2024-02-03 06:58:19,156 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:58:19,156 	Gloss Alignment :	         
2024-02-03 06:58:19,156 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:58:19,157 	Text Reference  :	*** ****** ** earlier ish news had  released a    video when  india   had won 6    medals
2024-02-03 06:58:19,158 	Text Hypothesis :	now before we start   let me   tell you      that these games started in  the year 1930  
2024-02-03 06:58:19,158 	Text Alignment  :	I   I      I  S       S   S    S    S        S    S     S     S       S   S   S    S     
2024-02-03 06:58:19,158 ========================================================================================================================
2024-02-03 06:58:19,158 Logging Sequence: 85_36.00
2024-02-03 06:58:19,158 	Gloss Reference :	A B+C+D+E
2024-02-03 06:58:19,158 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 06:58:19,159 	Gloss Alignment :	         
2024-02-03 06:58:19,159 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 06:58:19,160 	Text Reference  :	symonds has scored 2  centuries in   26   tests   that he played for his   country
2024-02-03 06:58:19,160 	Text Hypothesis :	******* *** it     is played    from 15th october 2023 in the    t20 world cup    
2024-02-03 06:58:19,160 	Text Alignment  :	D       D   S      S  S         S    S    S       S    S  S      S   S     S      
2024-02-03 06:58:19,160 ========================================================================================================================
2024-02-03 06:58:22,378 Epoch 717: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.19 
2024-02-03 06:58:22,379 EPOCH 718
2024-02-03 06:58:26,864 [Epoch: 718 Step: 00048100] Batch Recognition Loss:   0.000571 => Gls Tokens per Sec:     2156 || Batch Translation Loss:   0.016966 => Txt Tokens per Sec:     6024 || Lr: 0.000050
2024-02-03 06:58:27,279 Epoch 718: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.09 
2024-02-03 06:58:27,280 EPOCH 719
2024-02-03 06:58:31,829 Epoch 719: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.05 
2024-02-03 06:58:31,829 EPOCH 720
2024-02-03 06:58:33,868 [Epoch: 720 Step: 00048200] Batch Recognition Loss:   0.000205 => Gls Tokens per Sec:     2075 || Batch Translation Loss:   0.013267 => Txt Tokens per Sec:     6140 || Lr: 0.000050
2024-02-03 06:58:36,500 Epoch 720: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.12 
2024-02-03 06:58:36,500 EPOCH 721
2024-02-03 06:58:40,688 [Epoch: 721 Step: 00048300] Batch Recognition Loss:   0.000087 => Gls Tokens per Sec:     2271 || Batch Translation Loss:   0.009443 => Txt Tokens per Sec:     6315 || Lr: 0.000050
2024-02-03 06:58:41,095 Epoch 721: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.04 
2024-02-03 06:58:41,095 EPOCH 722
2024-02-03 06:58:46,448 Epoch 722: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.47 
2024-02-03 06:58:46,449 EPOCH 723
2024-02-03 06:58:48,511 [Epoch: 723 Step: 00048400] Batch Recognition Loss:   0.000537 => Gls Tokens per Sec:     2018 || Batch Translation Loss:   0.022543 => Txt Tokens per Sec:     5726 || Lr: 0.000050
2024-02-03 06:58:51,948 Epoch 723: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.19 
2024-02-03 06:58:51,949 EPOCH 724
2024-02-03 06:58:56,565 [Epoch: 724 Step: 00048500] Batch Recognition Loss:   0.000532 => Gls Tokens per Sec:     2026 || Batch Translation Loss:   0.033442 => Txt Tokens per Sec:     5700 || Lr: 0.000050
2024-02-03 06:58:57,059 Epoch 724: Total Training Recognition Loss 0.09  Total Training Translation Loss 6.62 
2024-02-03 06:58:57,059 EPOCH 725
2024-02-03 06:59:02,098 Epoch 725: Total Training Recognition Loss 0.08  Total Training Translation Loss 4.24 
2024-02-03 06:59:02,098 EPOCH 726
2024-02-03 06:59:04,274 [Epoch: 726 Step: 00048600] Batch Recognition Loss:   0.000417 => Gls Tokens per Sec:     1839 || Batch Translation Loss:   0.039108 => Txt Tokens per Sec:     5314 || Lr: 0.000050
2024-02-03 06:59:07,362 Epoch 726: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.11 
2024-02-03 06:59:07,362 EPOCH 727
2024-02-03 06:59:12,169 [Epoch: 727 Step: 00048700] Batch Recognition Loss:   0.000257 => Gls Tokens per Sec:     1912 || Batch Translation Loss:   0.016596 => Txt Tokens per Sec:     5373 || Lr: 0.000050
2024-02-03 06:59:12,814 Epoch 727: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.37 
2024-02-03 06:59:12,814 EPOCH 728
2024-02-03 06:59:18,184 Epoch 728: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.53 
2024-02-03 06:59:18,185 EPOCH 729
2024-02-03 06:59:19,820 [Epoch: 729 Step: 00048800] Batch Recognition Loss:   0.000766 => Gls Tokens per Sec:     2350 || Batch Translation Loss:   0.043804 => Txt Tokens per Sec:     6405 || Lr: 0.000050
2024-02-03 06:59:23,050 Epoch 729: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.84 
2024-02-03 06:59:23,050 EPOCH 730
2024-02-03 06:59:27,774 [Epoch: 730 Step: 00048900] Batch Recognition Loss:   0.003810 => Gls Tokens per Sec:     1913 || Batch Translation Loss:   0.064512 => Txt Tokens per Sec:     5282 || Lr: 0.000050
2024-02-03 06:59:28,556 Epoch 730: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.43 
2024-02-03 06:59:28,557 EPOCH 731
2024-02-03 06:59:33,377 Epoch 731: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.64 
2024-02-03 06:59:33,377 EPOCH 732
2024-02-03 06:59:35,418 [Epoch: 732 Step: 00049000] Batch Recognition Loss:   0.000707 => Gls Tokens per Sec:     1806 || Batch Translation Loss:   0.019435 => Txt Tokens per Sec:     4970 || Lr: 0.000050
2024-02-03 06:59:38,933 Epoch 732: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.26 
2024-02-03 06:59:38,933 EPOCH 733
2024-02-03 06:59:43,632 [Epoch: 733 Step: 00049100] Batch Recognition Loss:   0.000416 => Gls Tokens per Sec:     1888 || Batch Translation Loss:   0.012267 => Txt Tokens per Sec:     5280 || Lr: 0.000050
2024-02-03 06:59:44,410 Epoch 733: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.21 
2024-02-03 06:59:44,410 EPOCH 734
2024-02-03 06:59:49,458 Epoch 734: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.03 
2024-02-03 06:59:49,458 EPOCH 735
2024-02-03 06:59:51,372 [Epoch: 735 Step: 00049200] Batch Recognition Loss:   0.000649 => Gls Tokens per Sec:     1793 || Batch Translation Loss:   0.017828 => Txt Tokens per Sec:     5028 || Lr: 0.000050
2024-02-03 06:59:54,694 Epoch 735: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.15 
2024-02-03 06:59:54,694 EPOCH 736
2024-02-03 06:59:59,185 [Epoch: 736 Step: 00049300] Batch Recognition Loss:   0.000451 => Gls Tokens per Sec:     1940 || Batch Translation Loss:   0.014405 => Txt Tokens per Sec:     5315 || Lr: 0.000050
2024-02-03 07:00:00,247 Epoch 736: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.97 
2024-02-03 07:00:00,247 EPOCH 737
2024-02-03 07:00:05,547 Epoch 737: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.45 
2024-02-03 07:00:05,548 EPOCH 738
2024-02-03 07:00:07,372 [Epoch: 738 Step: 00049400] Batch Recognition Loss:   0.002944 => Gls Tokens per Sec:     1795 || Batch Translation Loss:   0.025536 => Txt Tokens per Sec:     4925 || Lr: 0.000050
2024-02-03 07:00:10,871 Epoch 738: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.03 
2024-02-03 07:00:10,872 EPOCH 739
2024-02-03 07:00:14,990 [Epoch: 739 Step: 00049500] Batch Recognition Loss:   0.000184 => Gls Tokens per Sec:     2077 || Batch Translation Loss:   0.019969 => Txt Tokens per Sec:     5691 || Lr: 0.000050
2024-02-03 07:00:16,071 Epoch 739: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.19 
2024-02-03 07:00:16,071 EPOCH 740
2024-02-03 07:00:21,129 Epoch 740: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.94 
2024-02-03 07:00:21,129 EPOCH 741
2024-02-03 07:00:22,659 [Epoch: 741 Step: 00049600] Batch Recognition Loss:   0.000213 => Gls Tokens per Sec:     2094 || Batch Translation Loss:   0.025452 => Txt Tokens per Sec:     6146 || Lr: 0.000050
2024-02-03 07:00:26,382 Epoch 741: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.41 
2024-02-03 07:00:26,383 EPOCH 742
2024-02-03 07:00:30,341 [Epoch: 742 Step: 00049700] Batch Recognition Loss:   0.000193 => Gls Tokens per Sec:     2143 || Batch Translation Loss:   0.014627 => Txt Tokens per Sec:     6022 || Lr: 0.000050
2024-02-03 07:00:31,413 Epoch 742: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.77 
2024-02-03 07:00:31,413 EPOCH 743
2024-02-03 07:00:36,879 Epoch 743: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.73 
2024-02-03 07:00:36,879 EPOCH 744
2024-02-03 07:00:38,399 [Epoch: 744 Step: 00049800] Batch Recognition Loss:   0.001514 => Gls Tokens per Sec:     1944 || Batch Translation Loss:   0.092434 => Txt Tokens per Sec:     5780 || Lr: 0.000050
2024-02-03 07:00:41,585 Epoch 744: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.73 
2024-02-03 07:00:41,586 EPOCH 745
2024-02-03 07:00:46,107 [Epoch: 745 Step: 00049900] Batch Recognition Loss:   0.000330 => Gls Tokens per Sec:     1821 || Batch Translation Loss:   0.021159 => Txt Tokens per Sec:     5171 || Lr: 0.000050
2024-02-03 07:00:47,207 Epoch 745: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.80 
2024-02-03 07:00:47,207 EPOCH 746
2024-02-03 07:00:52,587 Epoch 746: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.27 
2024-02-03 07:00:52,587 EPOCH 747
2024-02-03 07:00:53,874 [Epoch: 747 Step: 00050000] Batch Recognition Loss:   0.003386 => Gls Tokens per Sec:     2240 || Batch Translation Loss:   0.063495 => Txt Tokens per Sec:     6155 || Lr: 0.000050
2024-02-03 07:01:02,377 Validation result at epoch 747, step    50000: duration: 8.5037s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 3.53063	Translation Loss: 85560.20312	PPL: 7234.39111
	Eval Metric: BLEU
	WER 3.25	(DEL: 0.14,	INS: 0.00,	SUB: 3.11)
	BLEU-4 0.57	(BLEU-1: 9.60,	BLEU-2: 2.84,	BLEU-3: 1.12,	BLEU-4: 0.57)
	CHRF 16.90	ROUGE 8.17
2024-02-03 07:01:02,378 Logging Recognition and Translation Outputs
2024-02-03 07:01:02,379 ========================================================================================================================
2024-02-03 07:01:02,379 Logging Sequence: 154_33.00
2024-02-03 07:01:02,379 	Gloss Reference :	A B+C+D+E
2024-02-03 07:01:02,379 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 07:01:02,379 	Gloss Alignment :	         
2024-02-03 07:01:02,379 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 07:01:02,380 	Text Reference  :	the t20 world cup   will      now be    held in   uae oman
2024-02-03 07:01:02,380 	Text Hypothesis :	*** but that  never announced his knees any  risk of  7   
2024-02-03 07:01:02,380 	Text Alignment  :	D   S   S     S     S         S   S     S    S    S   S   
2024-02-03 07:01:02,381 ========================================================================================================================
2024-02-03 07:01:02,381 Logging Sequence: 155_56.00
2024-02-03 07:01:02,381 	Gloss Reference :	A B+C+D+E
2024-02-03 07:01:02,381 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 07:01:02,381 	Gloss Alignment :	         
2024-02-03 07:01:02,381 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 07:01:02,382 	Text Reference  :	** *** **** the **** *** *** **** ********** ** team would   have been  disqualified
2024-02-03 07:01:02,382 	Text Hypothesis :	if you wish the same but are very protective of the  shocked by   their shoes       
2024-02-03 07:01:02,382 	Text Alignment  :	I  I   I        I    I   I   I    I          I  S    S       S    S     S           
2024-02-03 07:01:02,382 ========================================================================================================================
2024-02-03 07:01:02,382 Logging Sequence: 125_119.00
2024-02-03 07:01:02,383 	Gloss Reference :	A B+C+D+E
2024-02-03 07:01:02,383 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 07:01:02,383 	Gloss Alignment :	         
2024-02-03 07:01:02,383 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 07:01:02,384 	Text Reference  :	people must not post  such baseless comments on    social media
2024-02-03 07:01:02,384 	Text Hypothesis :	****** in   a   world cup  this     is       about sri    lanka
2024-02-03 07:01:02,384 	Text Alignment  :	D      S    S   S     S    S        S        S     S      S    
2024-02-03 07:01:02,384 ========================================================================================================================
2024-02-03 07:01:02,384 Logging Sequence: 59_152.00
2024-02-03 07:01:02,384 	Gloss Reference :	A B+C+D+E
2024-02-03 07:01:02,385 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 07:01:02,385 	Gloss Alignment :	         
2024-02-03 07:01:02,385 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 07:01:02,386 	Text Reference  :	the organisers encouraged athletes to ***** * **** **** *** * use      the condoms in their home countries
2024-02-03 07:01:02,386 	Text Hypothesis :	*** once       she        returns  to india i will give you a suitable job as      a  token of   gratitude
2024-02-03 07:01:02,386 	Text Alignment  :	D   S          S          S           I     I I    I    I   I S        S   S       S  S     S    S        
2024-02-03 07:01:02,386 ========================================================================================================================
2024-02-03 07:01:02,387 Logging Sequence: 151_94.00
2024-02-03 07:01:02,387 	Gloss Reference :	A B+C+D+E
2024-02-03 07:01:02,387 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 07:01:02,387 	Gloss Alignment :	         
2024-02-03 07:01:02,387 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 07:01:02,389 	Text Reference  :	** *** suresh raina was also known as mr ipl    for his     amazing performance in the tournament
2024-02-03 07:01:02,389 	Text Hypothesis :	in ipl there  is    a   rule that  if a  person who allowed to      participate in the future    
2024-02-03 07:01:02,389 	Text Alignment  :	I  I   S      S     S   S    S     S  S  S      S   S       S       S                  S         
2024-02-03 07:01:02,389 ========================================================================================================================
2024-02-03 07:01:06,471 Epoch 747: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.28 
2024-02-03 07:01:06,471 EPOCH 748
2024-02-03 07:01:10,830 [Epoch: 748 Step: 00050100] Batch Recognition Loss:   0.000147 => Gls Tokens per Sec:     1852 || Batch Translation Loss:   0.015648 => Txt Tokens per Sec:     5080 || Lr: 0.000050
2024-02-03 07:01:12,032 Epoch 748: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.42 
2024-02-03 07:01:12,032 EPOCH 749
2024-02-03 07:01:17,286 Epoch 749: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.97 
2024-02-03 07:01:17,287 EPOCH 750
2024-02-03 07:01:18,540 [Epoch: 750 Step: 00050200] Batch Recognition Loss:   0.000083 => Gls Tokens per Sec:     2174 || Batch Translation Loss:   0.010577 => Txt Tokens per Sec:     5896 || Lr: 0.000050
2024-02-03 07:01:22,554 Epoch 750: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.33 
2024-02-03 07:01:22,554 EPOCH 751
2024-02-03 07:01:25,900 [Epoch: 751 Step: 00050300] Batch Recognition Loss:   0.000966 => Gls Tokens per Sec:     2392 || Batch Translation Loss:   0.015655 => Txt Tokens per Sec:     6606 || Lr: 0.000050
2024-02-03 07:01:27,125 Epoch 751: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.22 
2024-02-03 07:01:27,125 EPOCH 752
2024-02-03 07:01:31,935 Epoch 752: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.32 
2024-02-03 07:01:31,936 EPOCH 753
2024-02-03 07:01:33,091 [Epoch: 753 Step: 00050400] Batch Recognition Loss:   0.001113 => Gls Tokens per Sec:     2218 || Batch Translation Loss:   0.012236 => Txt Tokens per Sec:     5949 || Lr: 0.000050
2024-02-03 07:01:37,422 Epoch 753: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.24 
2024-02-03 07:01:37,422 EPOCH 754
2024-02-03 07:01:41,223 [Epoch: 754 Step: 00050500] Batch Recognition Loss:   0.000080 => Gls Tokens per Sec:     2064 || Batch Translation Loss:   0.009924 => Txt Tokens per Sec:     5884 || Lr: 0.000050
2024-02-03 07:01:42,637 Epoch 754: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.61 
2024-02-03 07:01:42,637 EPOCH 755
2024-02-03 07:01:47,946 Epoch 755: Total Training Recognition Loss 0.09  Total Training Translation Loss 12.74 
2024-02-03 07:01:47,946 EPOCH 756
2024-02-03 07:01:48,924 [Epoch: 756 Step: 00050600] Batch Recognition Loss:   0.000912 => Gls Tokens per Sec:     2456 || Batch Translation Loss:   0.036134 => Txt Tokens per Sec:     6823 || Lr: 0.000050
2024-02-03 07:01:53,234 Epoch 756: Total Training Recognition Loss 0.13  Total Training Translation Loss 3.97 
2024-02-03 07:01:53,234 EPOCH 757
2024-02-03 07:01:56,754 [Epoch: 757 Step: 00050700] Batch Recognition Loss:   0.000804 => Gls Tokens per Sec:     2182 || Batch Translation Loss:   0.018531 => Txt Tokens per Sec:     6096 || Lr: 0.000050
2024-02-03 07:01:58,236 Epoch 757: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.51 
2024-02-03 07:01:58,237 EPOCH 758
2024-02-03 07:02:03,735 Epoch 758: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.47 
2024-02-03 07:02:03,736 EPOCH 759
2024-02-03 07:02:04,707 [Epoch: 759 Step: 00050800] Batch Recognition Loss:   0.012212 => Gls Tokens per Sec:     2309 || Batch Translation Loss:   0.023723 => Txt Tokens per Sec:     6399 || Lr: 0.000050
2024-02-03 07:02:08,518 Epoch 759: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.43 
2024-02-03 07:02:08,518 EPOCH 760
2024-02-03 07:02:12,413 [Epoch: 760 Step: 00050900] Batch Recognition Loss:   0.000101 => Gls Tokens per Sec:     1909 || Batch Translation Loss:   0.015011 => Txt Tokens per Sec:     5176 || Lr: 0.000050
2024-02-03 07:02:13,981 Epoch 760: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.27 
2024-02-03 07:02:13,982 EPOCH 761
2024-02-03 07:02:19,069 Epoch 761: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.18 
2024-02-03 07:02:19,069 EPOCH 762
2024-02-03 07:02:20,299 [Epoch: 762 Step: 00051000] Batch Recognition Loss:   0.000242 => Gls Tokens per Sec:     1693 || Batch Translation Loss:   0.052088 => Txt Tokens per Sec:     5139 || Lr: 0.000050
2024-02-03 07:02:24,306 Epoch 762: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.25 
2024-02-03 07:02:24,306 EPOCH 763
2024-02-03 07:02:27,638 [Epoch: 763 Step: 00051100] Batch Recognition Loss:   0.000300 => Gls Tokens per Sec:     2183 || Batch Translation Loss:   0.016762 => Txt Tokens per Sec:     5901 || Lr: 0.000050
2024-02-03 07:02:29,672 Epoch 763: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.66 
2024-02-03 07:02:29,673 EPOCH 764
2024-02-03 07:02:35,015 Epoch 764: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.22 
2024-02-03 07:02:35,015 EPOCH 765
2024-02-03 07:02:35,933 [Epoch: 765 Step: 00051200] Batch Recognition Loss:   0.000390 => Gls Tokens per Sec:     1994 || Batch Translation Loss:   0.021302 => Txt Tokens per Sec:     5842 || Lr: 0.000050
2024-02-03 07:02:40,305 Epoch 765: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.15 
2024-02-03 07:02:40,305 EPOCH 766
2024-02-03 07:02:43,430 [Epoch: 766 Step: 00051300] Batch Recognition Loss:   0.000328 => Gls Tokens per Sec:     2304 || Batch Translation Loss:   0.007719 => Txt Tokens per Sec:     6311 || Lr: 0.000050
2024-02-03 07:02:45,575 Epoch 766: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.01 
2024-02-03 07:02:45,576 EPOCH 767
2024-02-03 07:02:50,827 Epoch 767: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.04 
2024-02-03 07:02:50,827 EPOCH 768
2024-02-03 07:02:51,596 [Epoch: 768 Step: 00051400] Batch Recognition Loss:   0.000118 => Gls Tokens per Sec:     2292 || Batch Translation Loss:   0.013530 => Txt Tokens per Sec:     6156 || Lr: 0.000050
2024-02-03 07:02:56,091 Epoch 768: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.07 
2024-02-03 07:02:56,092 EPOCH 769
2024-02-03 07:02:59,223 [Epoch: 769 Step: 00051500] Batch Recognition Loss:   0.000201 => Gls Tokens per Sec:     2249 || Batch Translation Loss:   0.011882 => Txt Tokens per Sec:     6158 || Lr: 0.000050
2024-02-03 07:03:01,059 Epoch 769: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.09 
2024-02-03 07:03:01,059 EPOCH 770
2024-02-03 07:03:06,628 Epoch 770: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.15 
2024-02-03 07:03:06,629 EPOCH 771
2024-02-03 07:03:07,514 [Epoch: 771 Step: 00051600] Batch Recognition Loss:   0.000120 => Gls Tokens per Sec:     1810 || Batch Translation Loss:   0.012807 => Txt Tokens per Sec:     4861 || Lr: 0.000050
2024-02-03 07:03:12,317 Epoch 771: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.05 
2024-02-03 07:03:12,317 EPOCH 772
2024-02-03 07:03:15,714 [Epoch: 772 Step: 00051700] Batch Recognition Loss:   0.000162 => Gls Tokens per Sec:     1999 || Batch Translation Loss:   0.096245 => Txt Tokens per Sec:     5672 || Lr: 0.000050
2024-02-03 07:03:17,550 Epoch 772: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.28 
2024-02-03 07:03:17,550 EPOCH 773
2024-02-03 07:03:22,691 Epoch 773: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.74 
2024-02-03 07:03:22,692 EPOCH 774
2024-02-03 07:03:23,349 [Epoch: 774 Step: 00051800] Batch Recognition Loss:   0.000320 => Gls Tokens per Sec:     2195 || Batch Translation Loss:   0.048235 => Txt Tokens per Sec:     5805 || Lr: 0.000050
2024-02-03 07:03:27,679 Epoch 774: Total Training Recognition Loss 0.11  Total Training Translation Loss 4.38 
2024-02-03 07:03:27,679 EPOCH 775
2024-02-03 07:03:31,026 [Epoch: 775 Step: 00051900] Batch Recognition Loss:   0.001842 => Gls Tokens per Sec:     1981 || Batch Translation Loss:   0.084127 => Txt Tokens per Sec:     5642 || Lr: 0.000050
2024-02-03 07:03:32,995 Epoch 775: Total Training Recognition Loss 0.18  Total Training Translation Loss 4.77 
2024-02-03 07:03:32,995 EPOCH 776
2024-02-03 07:03:38,192 Epoch 776: Total Training Recognition Loss 0.21  Total Training Translation Loss 3.50 
2024-02-03 07:03:38,192 EPOCH 777
2024-02-03 07:03:38,940 [Epoch: 777 Step: 00052000] Batch Recognition Loss:   0.002265 => Gls Tokens per Sec:     1714 || Batch Translation Loss:   0.033110 => Txt Tokens per Sec:     5020 || Lr: 0.000050
2024-02-03 07:03:47,376 Validation result at epoch 777, step    52000: duration: 8.4354s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 9.16867	Translation Loss: 85838.35938	PPL: 7446.44531
	Eval Metric: BLEU
	WER 3.68	(DEL: 0.00,	INS: 0.00,	SUB: 3.68)
	BLEU-4 0.70	(BLEU-1: 9.90,	BLEU-2: 3.03,	BLEU-3: 1.33,	BLEU-4: 0.70)
	CHRF 16.59	ROUGE 8.34
2024-02-03 07:03:47,378 Logging Recognition and Translation Outputs
2024-02-03 07:03:47,378 ========================================================================================================================
2024-02-03 07:03:47,378 Logging Sequence: 84_206.00
2024-02-03 07:03:47,378 	Gloss Reference :	A B+C+D+E
2024-02-03 07:03:47,378 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 07:03:47,378 	Gloss Alignment :	         
2024-02-03 07:03:47,379 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 07:03:47,379 	Text Reference  :	football fans in t he stadium too agreed with  the players and wished to  support the lgbtqia community 
2024-02-03 07:03:47,379 	Text Hypothesis :	******** **** ** * ** ******* *** ****** since the ******* *** game   was tied    the ******* tournament
2024-02-03 07:03:47,380 	Text Alignment  :	D        D    D  D D  D       D   D      S         D       D   S      S   S           D       S         
2024-02-03 07:03:47,380 ========================================================================================================================
2024-02-03 07:03:47,380 Logging Sequence: 126_263.00
2024-02-03 07:03:47,380 	Gloss Reference :	A B+C+D+E
2024-02-03 07:03:47,380 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 07:03:47,380 	Gloss Alignment :	         
2024-02-03 07:03:47,380 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 07:03:47,381 	Text Reference  :	indian airline indigo has announced unlimited free  year travel for the gold medalist
2024-02-03 07:03:47,381 	Text Hypothesis :	****** ******* ****** *** ********* everyone  hoped she  would  win a   gold medal   
2024-02-03 07:03:47,381 	Text Alignment  :	D      D       D      D   D         S         S     S    S      S   S        S       
2024-02-03 07:03:47,382 ========================================================================================================================
2024-02-03 07:03:47,382 Logging Sequence: 93_11.00
2024-02-03 07:03:47,382 	Gloss Reference :	A B+C+D+E
2024-02-03 07:03:47,382 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 07:03:47,382 	Gloss Alignment :	         
2024-02-03 07:03:47,382 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 07:03:47,383 	Text Reference  :	** he  also  played for   the manchester united       football team       
2024-02-03 07:03:47,383 	Text Hypothesis :	on 9th april 2022   there was an         announcement and      afghanistan
2024-02-03 07:03:47,383 	Text Alignment  :	I  S   S     S      S     S   S          S            S        S          
2024-02-03 07:03:47,383 ========================================================================================================================
2024-02-03 07:03:47,383 Logging Sequence: 155_62.00
2024-02-03 07:03:47,384 	Gloss Reference :	A B+C+D+E
2024-02-03 07:03:47,384 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 07:03:47,384 	Gloss Alignment :	         
2024-02-03 07:03:47,384 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 07:03:47,385 	Text Reference  :	however they were fortunate that the icc had drafted the match schedule well in advance with the afghan team in the     tournament   
2024-02-03 07:03:47,385 	Text Hypothesis :	however they **** ********* **** *** *** *** ******* *** ***** never    used a  video   of   the ****** **** ** taliban adminstration
2024-02-03 07:03:47,386 	Text Alignment  :	             D    D         D    D   D   D   D       D   D     S        S    S  S       S        D      D    D  S       S            
2024-02-03 07:03:47,386 ========================================================================================================================
2024-02-03 07:03:47,386 Logging Sequence: 93_267.00
2024-02-03 07:03:47,386 	Gloss Reference :	A B+C+D+E            
2024-02-03 07:03:47,386 	Gloss Hypothesis:	A B+C+B+C+D+E+B+C+D+E
2024-02-03 07:03:47,386 	Gloss Alignment :	  S                  
2024-02-03 07:03:47,387 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 07:03:47,388 	Text Reference  :	one girl who was wearing a    thong flashed her bottom next to  rooney     the image went viral on      snapchat 
2024-02-03 07:03:47,388 	Text Hypothesis :	*** **** *** *** ******* this is    why     he  walked into the tournament and never used a     penalty shoot-out
2024-02-03 07:03:47,388 	Text Alignment  :	D   D    D   D   D       S    S     S       S   S      S    S   S          S   S     S    S     S       S        
2024-02-03 07:03:47,389 ========================================================================================================================
2024-02-03 07:03:52,328 Epoch 777: Total Training Recognition Loss 0.18  Total Training Translation Loss 3.08 
2024-02-03 07:03:52,329 EPOCH 778
2024-02-03 07:03:55,048 [Epoch: 778 Step: 00052100] Batch Recognition Loss:   0.000409 => Gls Tokens per Sec:     2414 || Batch Translation Loss:   0.096118 => Txt Tokens per Sec:     6719 || Lr: 0.000050
2024-02-03 07:03:57,248 Epoch 778: Total Training Recognition Loss 0.08  Total Training Translation Loss 3.35 
2024-02-03 07:03:57,249 EPOCH 779
2024-02-03 07:04:02,729 Epoch 779: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.16 
2024-02-03 07:04:02,730 EPOCH 780
2024-02-03 07:04:03,254 [Epoch: 780 Step: 00052200] Batch Recognition Loss:   0.000491 => Gls Tokens per Sec:     2137 || Batch Translation Loss:   0.021245 => Txt Tokens per Sec:     6380 || Lr: 0.000050
2024-02-03 07:04:08,121 Epoch 780: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.86 
2024-02-03 07:04:08,121 EPOCH 781
2024-02-03 07:04:11,114 [Epoch: 781 Step: 00052300] Batch Recognition Loss:   0.000161 => Gls Tokens per Sec:     2139 || Batch Translation Loss:   0.017005 => Txt Tokens per Sec:     5700 || Lr: 0.000050
2024-02-03 07:04:13,356 Epoch 781: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.41 
2024-02-03 07:04:13,356 EPOCH 782
2024-02-03 07:04:18,816 Epoch 782: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.47 
2024-02-03 07:04:18,817 EPOCH 783
2024-02-03 07:04:19,161 [Epoch: 783 Step: 00052400] Batch Recognition Loss:   0.000116 => Gls Tokens per Sec:     2793 || Batch Translation Loss:   0.015961 => Txt Tokens per Sec:     7850 || Lr: 0.000050
2024-02-03 07:04:23,828 Epoch 783: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.82 
2024-02-03 07:04:23,828 EPOCH 784
2024-02-03 07:04:27,028 [Epoch: 784 Step: 00052500] Batch Recognition Loss:   0.000161 => Gls Tokens per Sec:     1951 || Batch Translation Loss:   0.022249 => Txt Tokens per Sec:     5624 || Lr: 0.000050
2024-02-03 07:04:29,216 Epoch 784: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.78 
2024-02-03 07:04:29,216 EPOCH 785
2024-02-03 07:04:34,682 Epoch 785: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.59 
2024-02-03 07:04:34,682 EPOCH 786
2024-02-03 07:04:35,009 [Epoch: 786 Step: 00052600] Batch Recognition Loss:   0.000496 => Gls Tokens per Sec:     2454 || Batch Translation Loss:   0.013465 => Txt Tokens per Sec:     6110 || Lr: 0.000050
2024-02-03 07:04:39,686 Epoch 786: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.20 
2024-02-03 07:04:39,687 EPOCH 787
2024-02-03 07:04:42,921 [Epoch: 787 Step: 00052700] Batch Recognition Loss:   0.000653 => Gls Tokens per Sec:     1881 || Batch Translation Loss:   0.018162 => Txt Tokens per Sec:     5324 || Lr: 0.000050
2024-02-03 07:04:45,070 Epoch 787: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.58 
2024-02-03 07:04:45,071 EPOCH 788
2024-02-03 07:04:49,733 Epoch 788: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.51 
2024-02-03 07:04:49,733 EPOCH 789
2024-02-03 07:04:50,039 [Epoch: 789 Step: 00052800] Batch Recognition Loss:   0.002934 => Gls Tokens per Sec:     1803 || Batch Translation Loss:   0.003889 => Txt Tokens per Sec:     4059 || Lr: 0.000050
2024-02-03 07:04:54,485 Epoch 789: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.35 
2024-02-03 07:04:54,485 EPOCH 790
2024-02-03 07:04:57,265 [Epoch: 790 Step: 00052900] Batch Recognition Loss:   0.000107 => Gls Tokens per Sec:     2098 || Batch Translation Loss:   0.017899 => Txt Tokens per Sec:     5648 || Lr: 0.000050
2024-02-03 07:04:59,745 Epoch 790: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.37 
2024-02-03 07:04:59,745 EPOCH 791
2024-02-03 07:05:04,873 Epoch 791: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.41 
2024-02-03 07:05:04,874 EPOCH 792
2024-02-03 07:05:05,267 [Epoch: 792 Step: 00053000] Batch Recognition Loss:   0.000096 => Gls Tokens per Sec:     1224 || Batch Translation Loss:   0.015409 => Txt Tokens per Sec:     3750 || Lr: 0.000050
2024-02-03 07:05:10,418 Epoch 792: Total Training Recognition Loss 0.05  Total Training Translation Loss 5.05 
2024-02-03 07:05:10,419 EPOCH 793
2024-02-03 07:05:13,460 [Epoch: 793 Step: 00053100] Batch Recognition Loss:   0.000179 => Gls Tokens per Sec:     1895 || Batch Translation Loss:   0.253264 => Txt Tokens per Sec:     5190 || Lr: 0.000050
2024-02-03 07:05:15,906 Epoch 793: Total Training Recognition Loss 0.20  Total Training Translation Loss 11.08 
2024-02-03 07:05:15,906 EPOCH 794
2024-02-03 07:05:20,840 Epoch 794: Total Training Recognition Loss 0.13  Total Training Translation Loss 6.58 
2024-02-03 07:05:20,841 EPOCH 795
2024-02-03 07:05:21,036 [Epoch: 795 Step: 00053200] Batch Recognition Loss:   0.004201 => Gls Tokens per Sec:     1658 || Batch Translation Loss:   0.045190 => Txt Tokens per Sec:     5539 || Lr: 0.000050
2024-02-03 07:05:26,396 Epoch 795: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.63 
2024-02-03 07:05:26,396 EPOCH 796
2024-02-03 07:05:29,323 [Epoch: 796 Step: 00053300] Batch Recognition Loss:   0.000820 => Gls Tokens per Sec:     1883 || Batch Translation Loss:   0.035437 => Txt Tokens per Sec:     5320 || Lr: 0.000050
2024-02-03 07:05:31,690 Epoch 796: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.49 
2024-02-03 07:05:31,690 EPOCH 797
2024-02-03 07:05:36,628 Epoch 797: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.61 
2024-02-03 07:05:36,629 EPOCH 798
2024-02-03 07:05:36,755 [Epoch: 798 Step: 00053400] Batch Recognition Loss:   0.000742 => Gls Tokens per Sec:     1270 || Batch Translation Loss:   0.025223 => Txt Tokens per Sec:     4333 || Lr: 0.000050
2024-02-03 07:05:41,578 Epoch 798: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.67 
2024-02-03 07:05:41,578 EPOCH 799
2024-02-03 07:05:44,280 [Epoch: 799 Step: 00053500] Batch Recognition Loss:   0.001309 => Gls Tokens per Sec:     2014 || Batch Translation Loss:   0.020677 => Txt Tokens per Sec:     5499 || Lr: 0.000050
2024-02-03 07:05:47,042 Epoch 799: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.49 
2024-02-03 07:05:47,042 EPOCH 800
2024-02-03 07:05:52,517 [Epoch: 800 Step: 00053600] Batch Recognition Loss:   0.000161 => Gls Tokens per Sec:     1942 || Batch Translation Loss:   0.015507 => Txt Tokens per Sec:     5401 || Lr: 0.000050
2024-02-03 07:05:52,517 Epoch 800: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.59 
2024-02-03 07:05:52,518 EPOCH 801
2024-02-03 07:05:57,636 Epoch 801: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.70 
2024-02-03 07:05:57,637 EPOCH 802
2024-02-03 07:06:00,271 [Epoch: 802 Step: 00053700] Batch Recognition Loss:   0.000530 => Gls Tokens per Sec:     1971 || Batch Translation Loss:   0.027719 => Txt Tokens per Sec:     5558 || Lr: 0.000050
2024-02-03 07:06:02,885 Epoch 802: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.70 
2024-02-03 07:06:02,885 EPOCH 803
2024-02-03 07:06:08,389 [Epoch: 803 Step: 00053800] Batch Recognition Loss:   0.000441 => Gls Tokens per Sec:     1903 || Batch Translation Loss:   0.023303 => Txt Tokens per Sec:     5295 || Lr: 0.000050
2024-02-03 07:06:08,464 Epoch 803: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.94 
2024-02-03 07:06:08,465 EPOCH 804
2024-02-03 07:06:13,352 Epoch 804: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.55 
2024-02-03 07:06:13,352 EPOCH 805
2024-02-03 07:06:16,353 [Epoch: 805 Step: 00053900] Batch Recognition Loss:   0.002623 => Gls Tokens per Sec:     1677 || Batch Translation Loss:   0.027813 => Txt Tokens per Sec:     4876 || Lr: 0.000050
2024-02-03 07:06:18,930 Epoch 805: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.36 
2024-02-03 07:06:18,930 EPOCH 806
2024-02-03 07:06:24,077 [Epoch: 806 Step: 00054000] Batch Recognition Loss:   0.000204 => Gls Tokens per Sec:     2003 || Batch Translation Loss:   0.016337 => Txt Tokens per Sec:     5602 || Lr: 0.000050
2024-02-03 07:06:32,402 Validation result at epoch 806, step    54000: duration: 8.3238s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 4.89031	Translation Loss: 86609.57812	PPL: 8067.46045
	Eval Metric: BLEU
	WER 3.04	(DEL: 0.00,	INS: 0.00,	SUB: 3.04)
	BLEU-4 0.45	(BLEU-1: 9.71,	BLEU-2: 2.78,	BLEU-3: 0.98,	BLEU-4: 0.45)
	CHRF 16.66	ROUGE 8.21
2024-02-03 07:06:32,403 Logging Recognition and Translation Outputs
2024-02-03 07:06:32,404 ========================================================================================================================
2024-02-03 07:06:32,404 Logging Sequence: 77_140.00
2024-02-03 07:06:32,404 	Gloss Reference :	A B+C+D+E
2024-02-03 07:06:32,404 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 07:06:32,404 	Gloss Alignment :	         
2024-02-03 07:06:32,404 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 07:06:32,405 	Text Reference  :	he was up against chennai's ravindra jadeja who   hit  multiple sixes    
2024-02-03 07:06:32,405 	Text Hypothesis :	** *** ** ******* however   social   media  users made furious  statement
2024-02-03 07:06:32,406 	Text Alignment  :	D  D   D  D       S         S        S      S     S    S        S        
2024-02-03 07:06:32,406 ========================================================================================================================
2024-02-03 07:06:32,406 Logging Sequence: 100_78.00
2024-02-03 07:06:32,406 	Gloss Reference :	A B+C+D+E
2024-02-03 07:06:32,406 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 07:06:32,406 	Gloss Alignment :	         
2024-02-03 07:06:32,406 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 07:06:32,408 	Text Reference  :	bcci had sent two separate indian teams to   compete in   the       cwg and          the     sahara  cup   
2024-02-03 07:06:32,408 	Text Hypothesis :	**** *** **** *** ******** ****** have  been the     bcci announced a   non-bailable warrant against sushil
2024-02-03 07:06:32,408 	Text Alignment  :	D    D   D    D   D        D      S     S    S       S    S         S   S            S       S       S     
2024-02-03 07:06:32,408 ========================================================================================================================
2024-02-03 07:06:32,408 Logging Sequence: 153_230.00
2024-02-03 07:06:32,408 	Gloss Reference :	A B+C+D+E
2024-02-03 07:06:32,408 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 07:06:32,408 	Gloss Alignment :	         
2024-02-03 07:06:32,409 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 07:06:32,410 	Text Reference  :	the imran khan-led the     pakistan side in  1992 and went on      to become the pm  
2024-02-03 07:06:32,410 	Text Hypothesis :	*** ***** new      zealand pakistan won  the toss and **** decided to ****** *** bowl
2024-02-03 07:06:32,410 	Text Alignment  :	D   D     S        S                S    S   S        D    S          D      D   S   
2024-02-03 07:06:32,410 ========================================================================================================================
2024-02-03 07:06:32,410 Logging Sequence: 154_25.00
2024-02-03 07:06:32,410 	Gloss Reference :	A B+C+D+E
2024-02-03 07:06:32,410 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 07:06:32,411 	Gloss Alignment :	         
2024-02-03 07:06:32,411 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 07:06:32,411 	Text Reference  :	bcci chief sourav ganguly on 28th june said that    the     
2024-02-03 07:06:32,411 	Text Hypothesis :	**** ***** ****** ******* ** in   the  49kg women's category
2024-02-03 07:06:32,411 	Text Alignment  :	D    D     D      D       D  S    S    S    S       S       
2024-02-03 07:06:32,412 ========================================================================================================================
2024-02-03 07:06:32,412 Logging Sequence: 81_394.00
2024-02-03 07:06:32,412 	Gloss Reference :	A B+C+D+E
2024-02-03 07:06:32,412 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 07:06:32,412 	Gloss Alignment :	         
2024-02-03 07:06:32,412 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 07:06:32,413 	Text Reference  :	the apex court is going  to  launch  a    strict investigation
2024-02-03 07:06:32,413 	Text Hypothesis :	*** **** ***** we eloped and players took 7      wickets      
2024-02-03 07:06:32,413 	Text Alignment  :	D   D    D     S  S      S   S       S    S      S            
2024-02-03 07:06:32,413 ========================================================================================================================
2024-02-03 07:06:32,570 Epoch 806: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.88 
2024-02-03 07:06:32,570 EPOCH 807
2024-02-03 07:06:38,055 Epoch 807: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.55 
2024-02-03 07:06:38,055 EPOCH 808
2024-02-03 07:06:40,222 [Epoch: 808 Step: 00054100] Batch Recognition Loss:   0.000564 => Gls Tokens per Sec:     2290 || Batch Translation Loss:   0.047268 => Txt Tokens per Sec:     6535 || Lr: 0.000050
2024-02-03 07:06:42,907 Epoch 808: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.16 
2024-02-03 07:06:42,908 EPOCH 809
2024-02-03 07:06:48,119 [Epoch: 809 Step: 00054200] Batch Recognition Loss:   0.001986 => Gls Tokens per Sec:     1948 || Batch Translation Loss:   0.045935 => Txt Tokens per Sec:     5408 || Lr: 0.000050
2024-02-03 07:06:48,336 Epoch 809: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.30 
2024-02-03 07:06:48,336 EPOCH 810
2024-02-03 07:06:53,478 Epoch 810: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.25 
2024-02-03 07:06:53,478 EPOCH 811
2024-02-03 07:06:55,781 [Epoch: 811 Step: 00054300] Batch Recognition Loss:   0.000260 => Gls Tokens per Sec:     2046 || Batch Translation Loss:   0.018012 => Txt Tokens per Sec:     5694 || Lr: 0.000050
2024-02-03 07:06:58,800 Epoch 811: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.87 
2024-02-03 07:06:58,801 EPOCH 812
2024-02-03 07:07:04,071 [Epoch: 812 Step: 00054400] Batch Recognition Loss:   0.002006 => Gls Tokens per Sec:     1896 || Batch Translation Loss:   0.019893 => Txt Tokens per Sec:     5286 || Lr: 0.000050
2024-02-03 07:07:04,376 Epoch 812: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.95 
2024-02-03 07:07:04,377 EPOCH 813
2024-02-03 07:07:09,446 Epoch 813: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.55 
2024-02-03 07:07:09,447 EPOCH 814
2024-02-03 07:07:11,766 [Epoch: 814 Step: 00054500] Batch Recognition Loss:   0.000112 => Gls Tokens per Sec:     2002 || Batch Translation Loss:   0.016924 => Txt Tokens per Sec:     5603 || Lr: 0.000050
2024-02-03 07:07:14,597 Epoch 814: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.46 
2024-02-03 07:07:14,597 EPOCH 815
2024-02-03 07:07:19,530 [Epoch: 815 Step: 00054600] Batch Recognition Loss:   0.000680 => Gls Tokens per Sec:     1993 || Batch Translation Loss:   0.054912 => Txt Tokens per Sec:     5521 || Lr: 0.000050
2024-02-03 07:07:19,971 Epoch 815: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.50 
2024-02-03 07:07:19,971 EPOCH 816
2024-02-03 07:07:25,365 Epoch 816: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.39 
2024-02-03 07:07:25,365 EPOCH 817
2024-02-03 07:07:27,673 [Epoch: 817 Step: 00054700] Batch Recognition Loss:   0.000225 => Gls Tokens per Sec:     1942 || Batch Translation Loss:   0.029636 => Txt Tokens per Sec:     5392 || Lr: 0.000050
2024-02-03 07:07:30,703 Epoch 817: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.74 
2024-02-03 07:07:30,703 EPOCH 818
2024-02-03 07:07:35,655 [Epoch: 818 Step: 00054800] Batch Recognition Loss:   0.000164 => Gls Tokens per Sec:     1953 || Batch Translation Loss:   0.032995 => Txt Tokens per Sec:     5448 || Lr: 0.000050
2024-02-03 07:07:36,044 Epoch 818: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.20 
2024-02-03 07:07:36,044 EPOCH 819
2024-02-03 07:07:41,064 Epoch 819: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.69 
2024-02-03 07:07:41,064 EPOCH 820
2024-02-03 07:07:43,277 [Epoch: 820 Step: 00054900] Batch Recognition Loss:   0.000484 => Gls Tokens per Sec:     1914 || Batch Translation Loss:   0.023482 => Txt Tokens per Sec:     5186 || Lr: 0.000050
2024-02-03 07:07:46,477 Epoch 820: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.54 
2024-02-03 07:07:46,477 EPOCH 821
2024-02-03 07:07:51,211 [Epoch: 821 Step: 00055000] Batch Recognition Loss:   0.000598 => Gls Tokens per Sec:     2009 || Batch Translation Loss:   0.035256 => Txt Tokens per Sec:     5546 || Lr: 0.000050
2024-02-03 07:07:51,740 Epoch 821: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.79 
2024-02-03 07:07:51,741 EPOCH 822
2024-02-03 07:07:56,885 Epoch 822: Total Training Recognition Loss 0.10  Total Training Translation Loss 2.92 
2024-02-03 07:07:56,885 EPOCH 823
2024-02-03 07:07:58,671 [Epoch: 823 Step: 00055100] Batch Recognition Loss:   0.000285 => Gls Tokens per Sec:     2331 || Batch Translation Loss:   0.058499 => Txt Tokens per Sec:     6529 || Lr: 0.000050
2024-02-03 07:08:02,072 Epoch 823: Total Training Recognition Loss 0.07  Total Training Translation Loss 8.32 
2024-02-03 07:08:02,073 EPOCH 824
2024-02-03 07:08:06,815 [Epoch: 824 Step: 00055200] Batch Recognition Loss:   0.001116 => Gls Tokens per Sec:     1972 || Batch Translation Loss:   0.164215 => Txt Tokens per Sec:     5477 || Lr: 0.000050
2024-02-03 07:08:07,468 Epoch 824: Total Training Recognition Loss 0.09  Total Training Translation Loss 7.35 
2024-02-03 07:08:07,468 EPOCH 825
2024-02-03 07:08:13,112 Epoch 825: Total Training Recognition Loss 0.10  Total Training Translation Loss 3.90 
2024-02-03 07:08:13,112 EPOCH 826
2024-02-03 07:08:15,129 [Epoch: 826 Step: 00055300] Batch Recognition Loss:   0.000613 => Gls Tokens per Sec:     1984 || Batch Translation Loss:   0.022877 => Txt Tokens per Sec:     5539 || Lr: 0.000050
2024-02-03 07:08:18,507 Epoch 826: Total Training Recognition Loss 0.10  Total Training Translation Loss 2.11 
2024-02-03 07:08:18,508 EPOCH 827
2024-02-03 07:08:22,720 [Epoch: 827 Step: 00055400] Batch Recognition Loss:   0.001273 => Gls Tokens per Sec:     2182 || Batch Translation Loss:   0.016484 => Txt Tokens per Sec:     6042 || Lr: 0.000050
2024-02-03 07:08:23,406 Epoch 827: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.67 
2024-02-03 07:08:23,407 EPOCH 828
2024-02-03 07:08:28,911 Epoch 828: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.99 
2024-02-03 07:08:28,912 EPOCH 829
2024-02-03 07:08:30,564 [Epoch: 829 Step: 00055500] Batch Recognition Loss:   0.000561 => Gls Tokens per Sec:     2326 || Batch Translation Loss:   0.029716 => Txt Tokens per Sec:     6540 || Lr: 0.000050
2024-02-03 07:08:34,172 Epoch 829: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.80 
2024-02-03 07:08:34,172 EPOCH 830
2024-02-03 07:08:38,565 [Epoch: 830 Step: 00055600] Batch Recognition Loss:   0.000400 => Gls Tokens per Sec:     2056 || Batch Translation Loss:   0.038399 => Txt Tokens per Sec:     5779 || Lr: 0.000050
2024-02-03 07:08:39,232 Epoch 830: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.30 
2024-02-03 07:08:39,233 EPOCH 831
2024-02-03 07:08:44,546 Epoch 831: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.25 
2024-02-03 07:08:44,547 EPOCH 832
2024-02-03 07:08:46,054 [Epoch: 832 Step: 00055700] Batch Recognition Loss:   0.000181 => Gls Tokens per Sec:     2444 || Batch Translation Loss:   0.012054 => Txt Tokens per Sec:     6570 || Lr: 0.000050
2024-02-03 07:08:49,671 Epoch 832: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.35 
2024-02-03 07:08:49,672 EPOCH 833
2024-02-03 07:08:54,226 [Epoch: 833 Step: 00055800] Batch Recognition Loss:   0.000095 => Gls Tokens per Sec:     1948 || Batch Translation Loss:   0.009089 => Txt Tokens per Sec:     5364 || Lr: 0.000050
2024-02-03 07:08:55,121 Epoch 833: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.13 
2024-02-03 07:08:55,122 EPOCH 834
2024-02-03 07:09:00,493 Epoch 834: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.05 
2024-02-03 07:09:00,494 EPOCH 835
2024-02-03 07:09:02,161 [Epoch: 835 Step: 00055900] Batch Recognition Loss:   0.000579 => Gls Tokens per Sec:     2112 || Batch Translation Loss:   0.018608 => Txt Tokens per Sec:     5898 || Lr: 0.000050
2024-02-03 07:09:05,478 Epoch 835: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.13 
2024-02-03 07:09:05,478 EPOCH 836
2024-02-03 07:09:09,781 [Epoch: 836 Step: 00056000] Batch Recognition Loss:   0.000546 => Gls Tokens per Sec:     2025 || Batch Translation Loss:   0.010042 => Txt Tokens per Sec:     5734 || Lr: 0.000050
2024-02-03 07:09:18,535 Validation result at epoch 836, step    56000: duration: 8.7537s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 4.59486	Translation Loss: 86979.69531	PPL: 8383.62500
	Eval Metric: BLEU
	WER 3.04	(DEL: 0.07,	INS: 0.00,	SUB: 2.97)
	BLEU-4 0.59	(BLEU-1: 9.62,	BLEU-2: 2.72,	BLEU-3: 1.12,	BLEU-4: 0.59)
	CHRF 16.54	ROUGE 8.13
2024-02-03 07:09:18,536 Logging Recognition and Translation Outputs
2024-02-03 07:09:18,536 ========================================================================================================================
2024-02-03 07:09:18,536 Logging Sequence: 92_59.00
2024-02-03 07:09:18,536 	Gloss Reference :	A B+C+D+E
2024-02-03 07:09:18,536 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 07:09:18,537 	Gloss Alignment :	         
2024-02-03 07:09:18,537 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 07:09:18,538 	Text Reference  :	after the indian team's defeat 2  men dancing and        bursting  crackers outside her house   
2024-02-03 07:09:18,538 	Text Hypothesis :	***** *** or     will   decide on to  avoid   purchasing duty-free alcohol  because the athletes
2024-02-03 07:09:18,538 	Text Alignment  :	D     D   S      S      S      S  S   S       S          S         S        S       S   S       
2024-02-03 07:09:18,538 ========================================================================================================================
2024-02-03 07:09:18,538 Logging Sequence: 81_73.00
2024-02-03 07:09:18,538 	Gloss Reference :	A B+C+D+E
2024-02-03 07:09:18,539 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 07:09:18,539 	Gloss Alignment :	         
2024-02-03 07:09:18,539 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 07:09:18,540 	Text Reference  :	buyers liked   the   ads  and  booked many       houses with the  builder sadly they     were duped
2024-02-03 07:09:18,540 	Text Hypothesis :	the    supreme court said that all    properties that   were left halfway by    amrapali will play 
2024-02-03 07:09:18,541 	Text Alignment  :	S      S       S     S    S    S      S          S      S    S    S       S     S        S    S    
2024-02-03 07:09:18,541 ========================================================================================================================
2024-02-03 07:09:18,541 Logging Sequence: 166_41.00
2024-02-03 07:09:18,541 	Gloss Reference :	A B+C+D+E
2024-02-03 07:09:18,541 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 07:09:18,541 	Gloss Alignment :	         
2024-02-03 07:09:18,541 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 07:09:18,542 	Text Reference  :	test match is played patiently till the   opponents are     all    out  
2024-02-03 07:09:18,542 	Text Hypothesis :	**** ***** ** ****** ********* 2    hours after     eriksen became viral
2024-02-03 07:09:18,542 	Text Alignment  :	D    D     D  D      D         S    S     S         S       S      S    
2024-02-03 07:09:18,542 ========================================================================================================================
2024-02-03 07:09:18,543 Logging Sequence: 171_33.00
2024-02-03 07:09:18,543 	Gloss Reference :	A B+C+D+E
2024-02-03 07:09:18,543 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 07:09:18,543 	Gloss Alignment :	         
2024-02-03 07:09:18,543 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 07:09:18,544 	Text Reference  :	the gujarat titans vs      chennai super kings match   was  held on *** 23rd    may 
2024-02-03 07:09:18,544 	Text Hypothesis :	the ******* match  between belgium and   and   gujarat will go   on 2nd october 2023
2024-02-03 07:09:18,545 	Text Alignment  :	    D       S      S       S       S     S     S       S    S       I   S       S   
2024-02-03 07:09:18,545 ========================================================================================================================
2024-02-03 07:09:18,545 Logging Sequence: 140_2.00
2024-02-03 07:09:18,545 	Gloss Reference :	A B+C+D+E
2024-02-03 07:09:18,545 	Gloss Hypothesis:	A B+C+D  
2024-02-03 07:09:18,545 	Gloss Alignment :	  S      
2024-02-03 07:09:18,545 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 07:09:18,546 	Text Reference  :	** *** ** **** indian batsman-wicket keeper rishabh pant     has    outstanding skills in    cricket
2024-02-03 07:09:18,546 	Text Hypothesis :	in let me tell you    why            have   a       talented player and         made   india proud  
2024-02-03 07:09:18,547 	Text Alignment  :	I  I   I  I    S      S              S      S       S        S      S           S      S     S      
2024-02-03 07:09:18,547 ========================================================================================================================
2024-02-03 07:09:19,498 Epoch 836: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.14 
2024-02-03 07:09:19,499 EPOCH 837
2024-02-03 07:09:24,956 Epoch 837: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.52 
2024-02-03 07:09:24,957 EPOCH 838
2024-02-03 07:09:26,710 [Epoch: 838 Step: 00056100] Batch Recognition Loss:   0.000151 => Gls Tokens per Sec:     1917 || Batch Translation Loss:   0.030545 => Txt Tokens per Sec:     5290 || Lr: 0.000050
2024-02-03 07:09:30,413 Epoch 838: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.61 
2024-02-03 07:09:30,414 EPOCH 839
2024-02-03 07:09:34,611 [Epoch: 839 Step: 00056200] Batch Recognition Loss:   0.000556 => Gls Tokens per Sec:     2037 || Batch Translation Loss:   0.037637 => Txt Tokens per Sec:     5648 || Lr: 0.000050
2024-02-03 07:09:35,648 Epoch 839: Total Training Recognition Loss 0.09  Total Training Translation Loss 3.89 
2024-02-03 07:09:35,649 EPOCH 840
2024-02-03 07:09:40,565 Epoch 840: Total Training Recognition Loss 0.07  Total Training Translation Loss 4.17 
2024-02-03 07:09:40,565 EPOCH 841
2024-02-03 07:09:42,304 [Epoch: 841 Step: 00056300] Batch Recognition Loss:   0.000597 => Gls Tokens per Sec:     1790 || Batch Translation Loss:   0.107366 => Txt Tokens per Sec:     5156 || Lr: 0.000050
2024-02-03 07:09:45,960 Epoch 841: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.31 
2024-02-03 07:09:45,960 EPOCH 842
2024-02-03 07:09:50,053 [Epoch: 842 Step: 00056400] Batch Recognition Loss:   0.000433 => Gls Tokens per Sec:     2051 || Batch Translation Loss:   0.009765 => Txt Tokens per Sec:     5812 || Lr: 0.000050
2024-02-03 07:09:51,201 Epoch 842: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.80 
2024-02-03 07:09:51,202 EPOCH 843
2024-02-03 07:09:56,519 Epoch 843: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.51 
2024-02-03 07:09:56,519 EPOCH 844
2024-02-03 07:09:57,858 [Epoch: 844 Step: 00056500] Batch Recognition Loss:   0.000294 => Gls Tokens per Sec:     2272 || Batch Translation Loss:   0.012983 => Txt Tokens per Sec:     6483 || Lr: 0.000050
2024-02-03 07:10:01,684 Epoch 844: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.38 
2024-02-03 07:10:01,684 EPOCH 845
2024-02-03 07:10:05,830 [Epoch: 845 Step: 00056600] Batch Recognition Loss:   0.001360 => Gls Tokens per Sec:     1986 || Batch Translation Loss:   0.020859 => Txt Tokens per Sec:     5645 || Lr: 0.000050
2024-02-03 07:10:06,749 Epoch 845: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.55 
2024-02-03 07:10:06,749 EPOCH 846
2024-02-03 07:10:12,254 Epoch 846: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.46 
2024-02-03 07:10:12,255 EPOCH 847
2024-02-03 07:10:13,606 [Epoch: 847 Step: 00056700] Batch Recognition Loss:   0.000082 => Gls Tokens per Sec:     2066 || Batch Translation Loss:   0.010672 => Txt Tokens per Sec:     5899 || Lr: 0.000050
2024-02-03 07:10:17,292 Epoch 847: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.39 
2024-02-03 07:10:17,293 EPOCH 848
2024-02-03 07:10:21,365 [Epoch: 848 Step: 00056800] Batch Recognition Loss:   0.000516 => Gls Tokens per Sec:     1982 || Batch Translation Loss:   0.008463 => Txt Tokens per Sec:     5446 || Lr: 0.000050
2024-02-03 07:10:22,640 Epoch 848: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.15 
2024-02-03 07:10:22,640 EPOCH 849
2024-02-03 07:10:27,806 Epoch 849: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.41 
2024-02-03 07:10:27,806 EPOCH 850
2024-02-03 07:10:29,025 [Epoch: 850 Step: 00056900] Batch Recognition Loss:   0.000775 => Gls Tokens per Sec:     2161 || Batch Translation Loss:   0.062334 => Txt Tokens per Sec:     5910 || Lr: 0.000050
2024-02-03 07:10:32,833 Epoch 850: Total Training Recognition Loss 0.07  Total Training Translation Loss 5.51 
2024-02-03 07:10:32,834 EPOCH 851
2024-02-03 07:10:37,242 [Epoch: 851 Step: 00057000] Batch Recognition Loss:   0.000522 => Gls Tokens per Sec:     1795 || Batch Translation Loss:   0.010667 => Txt Tokens per Sec:     5104 || Lr: 0.000050
2024-02-03 07:10:38,468 Epoch 851: Total Training Recognition Loss 0.13  Total Training Translation Loss 8.85 
2024-02-03 07:10:38,469 EPOCH 852
2024-02-03 07:10:43,632 Epoch 852: Total Training Recognition Loss 0.23  Total Training Translation Loss 4.87 
2024-02-03 07:10:43,633 EPOCH 853
2024-02-03 07:10:45,181 [Epoch: 853 Step: 00057100] Batch Recognition Loss:   0.000346 => Gls Tokens per Sec:     1655 || Batch Translation Loss:   0.048639 => Txt Tokens per Sec:     4965 || Lr: 0.000050
2024-02-03 07:10:48,932 Epoch 853: Total Training Recognition Loss 0.19  Total Training Translation Loss 2.50 
2024-02-03 07:10:48,932 EPOCH 854
2024-02-03 07:10:52,666 [Epoch: 854 Step: 00057200] Batch Recognition Loss:   0.000459 => Gls Tokens per Sec:     2076 || Batch Translation Loss:   0.016150 => Txt Tokens per Sec:     5874 || Lr: 0.000050
2024-02-03 07:10:54,061 Epoch 854: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.57 
2024-02-03 07:10:54,062 EPOCH 855
2024-02-03 07:10:59,309 Epoch 855: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.43 
2024-02-03 07:10:59,309 EPOCH 856
2024-02-03 07:11:00,271 [Epoch: 856 Step: 00057300] Batch Recognition Loss:   0.002647 => Gls Tokens per Sec:     2497 || Batch Translation Loss:   0.013368 => Txt Tokens per Sec:     6935 || Lr: 0.000050
2024-02-03 07:11:04,776 Epoch 856: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.38 
2024-02-03 07:11:04,776 EPOCH 857
2024-02-03 07:11:08,131 [Epoch: 857 Step: 00057400] Batch Recognition Loss:   0.000162 => Gls Tokens per Sec:     2290 || Batch Translation Loss:   0.016351 => Txt Tokens per Sec:     6309 || Lr: 0.000050
2024-02-03 07:11:09,781 Epoch 857: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.17 
2024-02-03 07:11:09,782 EPOCH 858
2024-02-03 07:11:15,477 Epoch 858: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.37 
2024-02-03 07:11:15,478 EPOCH 859
2024-02-03 07:11:16,590 [Epoch: 859 Step: 00057500] Batch Recognition Loss:   0.000336 => Gls Tokens per Sec:     2016 || Batch Translation Loss:   0.018272 => Txt Tokens per Sec:     5391 || Lr: 0.000050
2024-02-03 07:11:21,002 Epoch 859: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.54 
2024-02-03 07:11:21,003 EPOCH 860
2024-02-03 07:11:24,672 [Epoch: 860 Step: 00057600] Batch Recognition Loss:   0.000383 => Gls Tokens per Sec:     2025 || Batch Translation Loss:   0.022894 => Txt Tokens per Sec:     5611 || Lr: 0.000050
2024-02-03 07:11:26,459 Epoch 860: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.22 
2024-02-03 07:11:26,459 EPOCH 861
2024-02-03 07:11:32,043 Epoch 861: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.26 
2024-02-03 07:11:32,044 EPOCH 862
2024-02-03 07:11:33,200 [Epoch: 862 Step: 00057700] Batch Recognition Loss:   0.000119 => Gls Tokens per Sec:     1722 || Batch Translation Loss:   0.018470 => Txt Tokens per Sec:     5133 || Lr: 0.000050
2024-02-03 07:11:36,956 Epoch 862: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.57 
2024-02-03 07:11:36,956 EPOCH 863
2024-02-03 07:11:40,621 [Epoch: 863 Step: 00057800] Batch Recognition Loss:   0.000131 => Gls Tokens per Sec:     2008 || Batch Translation Loss:   0.019842 => Txt Tokens per Sec:     5557 || Lr: 0.000050
2024-02-03 07:11:42,445 Epoch 863: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.27 
2024-02-03 07:11:42,445 EPOCH 864
2024-02-03 07:11:47,681 Epoch 864: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.25 
2024-02-03 07:11:47,682 EPOCH 865
2024-02-03 07:11:48,590 [Epoch: 865 Step: 00057900] Batch Recognition Loss:   0.000270 => Gls Tokens per Sec:     2121 || Batch Translation Loss:   0.037151 => Txt Tokens per Sec:     5771 || Lr: 0.000050
2024-02-03 07:11:52,726 Epoch 865: Total Training Recognition Loss 0.08  Total Training Translation Loss 3.07 
2024-02-03 07:11:52,726 EPOCH 866
2024-02-03 07:11:55,794 [Epoch: 866 Step: 00058000] Batch Recognition Loss:   0.000268 => Gls Tokens per Sec:     2318 || Batch Translation Loss:   0.018132 => Txt Tokens per Sec:     6333 || Lr: 0.000050
2024-02-03 07:12:04,708 Validation result at epoch 866, step    58000: duration: 8.9123s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 4.89664	Translation Loss: 86527.97656	PPL: 7999.36768
	Eval Metric: BLEU
	WER 3.32	(DEL: 0.07,	INS: 0.00,	SUB: 3.25)
	BLEU-4 0.64	(BLEU-1: 10.40,	BLEU-2: 3.23,	BLEU-3: 1.33,	BLEU-4: 0.64)
	CHRF 17.18	ROUGE 8.84
2024-02-03 07:12:04,710 Logging Recognition and Translation Outputs
2024-02-03 07:12:04,710 ========================================================================================================================
2024-02-03 07:12:04,710 Logging Sequence: 135_100.00
2024-02-03 07:12:04,710 	Gloss Reference :	A B+C+D+E
2024-02-03 07:12:04,710 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 07:12:04,710 	Gloss Alignment :	         
2024-02-03 07:12:04,710 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 07:12:04,711 	Text Reference  :	and that her goal was  to raise the other half through the medal auction
2024-02-03 07:12:04,711 	Text Hypothesis :	*** **** *** they want to ***** win more  gold medals  by  west  indies 
2024-02-03 07:12:04,712 	Text Alignment  :	D   D    D   S    S       D     S   S     S    S       S   S     S      
2024-02-03 07:12:04,712 ========================================================================================================================
2024-02-03 07:12:04,712 Logging Sequence: 63_35.00
2024-02-03 07:12:04,712 	Gloss Reference :	A B+C+D+E
2024-02-03 07:12:04,712 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 07:12:04,712 	Gloss Alignment :	         
2024-02-03 07:12:04,712 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 07:12:04,714 	Text Reference  :	companies interested in buying the teams need to fill the         tender form    by  paying    rs    10  lakh 
2024-02-03 07:12:04,714 	Text Hypothesis :	********* ********** ** ****** *** ***** it   is held alternately in     england and australia every two years
2024-02-03 07:12:04,714 	Text Alignment  :	D         D          D  D      D   D     S    S  S    S           S      S       S   S         S     S   S    
2024-02-03 07:12:04,714 ========================================================================================================================
2024-02-03 07:12:04,714 Logging Sequence: 103_122.00
2024-02-03 07:12:04,714 	Gloss Reference :	A B+C+D+E
2024-02-03 07:12:04,715 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 07:12:04,715 	Gloss Alignment :	         
2024-02-03 07:12:04,715 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 07:12:04,716 	Text Reference  :	** *** *** ***** like india    australia canada pakistan nigeria in        africa and many more
2024-02-03 07:12:04,716 	Text Hypothesis :	as per the rules the  original target    of     215      was     truncated to     171 runs in  
2024-02-03 07:12:04,716 	Text Alignment  :	I  I   I   I     S    S        S         S      S        S       S         S      S   S    S   
2024-02-03 07:12:04,716 ========================================================================================================================
2024-02-03 07:12:04,716 Logging Sequence: 106_175.00
2024-02-03 07:12:04,717 	Gloss Reference :	A B+C+D+E
2024-02-03 07:12:04,717 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 07:12:04,717 	Gloss Alignment :	         
2024-02-03 07:12:04,717 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 07:12:04,718 	Text Reference  :	*** **** wonderful but what about the  deaf women's team  
2024-02-03 07:12:04,718 	Text Hypothesis :	she said it        is  a    good  luck for  the     finals
2024-02-03 07:12:04,718 	Text Alignment  :	I   I    S         S   S    S     S    S    S       S     
2024-02-03 07:12:04,718 ========================================================================================================================
2024-02-03 07:12:04,718 Logging Sequence: 83_57.00
2024-02-03 07:12:04,718 	Gloss Reference :	A B+C+D+E
2024-02-03 07:12:04,718 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 07:12:04,719 	Gloss Alignment :	         
2024-02-03 07:12:04,719 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 07:12:04,720 	Text Reference  :	*** ****** ******* ********** collapsed face    first on      the field he    was  completely unconscious
2024-02-03 07:12:04,720 	Text Hypothesis :	the danish players surrounded christian eriksen to    prevent the ***** media from capturing  pictures   
2024-02-03 07:12:04,720 	Text Alignment  :	I   I      I       I          S         S       S     S           D     S     S    S          S          
2024-02-03 07:12:04,720 ========================================================================================================================
2024-02-03 07:12:06,560 Epoch 866: Total Training Recognition Loss 0.08  Total Training Translation Loss 3.86 
2024-02-03 07:12:06,560 EPOCH 867
2024-02-03 07:12:11,699 Epoch 867: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.10 
2024-02-03 07:12:11,699 EPOCH 868
2024-02-03 07:12:12,652 [Epoch: 868 Step: 00058100] Batch Recognition Loss:   0.000498 => Gls Tokens per Sec:     1851 || Batch Translation Loss:   0.033622 => Txt Tokens per Sec:     5047 || Lr: 0.000050
2024-02-03 07:12:17,204 Epoch 868: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.29 
2024-02-03 07:12:17,204 EPOCH 869
2024-02-03 07:12:20,483 [Epoch: 869 Step: 00058200] Batch Recognition Loss:   0.000218 => Gls Tokens per Sec:     2149 || Batch Translation Loss:   0.016482 => Txt Tokens per Sec:     5987 || Lr: 0.000050
2024-02-03 07:12:22,464 Epoch 869: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.08 
2024-02-03 07:12:22,464 EPOCH 870
2024-02-03 07:12:27,508 Epoch 870: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.10 
2024-02-03 07:12:27,508 EPOCH 871
2024-02-03 07:12:28,261 [Epoch: 871 Step: 00058300] Batch Recognition Loss:   0.000606 => Gls Tokens per Sec:     2128 || Batch Translation Loss:   0.025554 => Txt Tokens per Sec:     6057 || Lr: 0.000050
2024-02-03 07:12:32,812 Epoch 871: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.93 
2024-02-03 07:12:32,813 EPOCH 872
2024-02-03 07:12:36,093 [Epoch: 872 Step: 00058400] Batch Recognition Loss:   0.004191 => Gls Tokens per Sec:     2071 || Batch Translation Loss:   0.037966 => Txt Tokens per Sec:     5857 || Lr: 0.000050
2024-02-03 07:12:37,889 Epoch 872: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.45 
2024-02-03 07:12:37,889 EPOCH 873
2024-02-03 07:12:43,399 Epoch 873: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.44 
2024-02-03 07:12:43,400 EPOCH 874
2024-02-03 07:12:44,184 [Epoch: 874 Step: 00058500] Batch Recognition Loss:   0.000374 => Gls Tokens per Sec:     1838 || Batch Translation Loss:   0.074169 => Txt Tokens per Sec:     5280 || Lr: 0.000050
2024-02-03 07:12:48,630 Epoch 874: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.53 
2024-02-03 07:12:48,631 EPOCH 875
2024-02-03 07:12:52,051 [Epoch: 875 Step: 00058600] Batch Recognition Loss:   0.001808 => Gls Tokens per Sec:     1939 || Batch Translation Loss:   0.054094 => Txt Tokens per Sec:     5558 || Lr: 0.000050
2024-02-03 07:12:53,694 Epoch 875: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.31 
2024-02-03 07:12:53,694 EPOCH 876
2024-02-03 07:12:58,984 Epoch 876: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.80 
2024-02-03 07:12:58,985 EPOCH 877
2024-02-03 07:12:59,578 [Epoch: 877 Step: 00058700] Batch Recognition Loss:   0.000396 => Gls Tokens per Sec:     2162 || Batch Translation Loss:   0.028595 => Txt Tokens per Sec:     5890 || Lr: 0.000050
2024-02-03 07:13:04,587 Epoch 877: Total Training Recognition Loss 0.06  Total Training Translation Loss 8.48 
2024-02-03 07:13:04,588 EPOCH 878
2024-02-03 07:13:07,552 [Epoch: 878 Step: 00058800] Batch Recognition Loss:   0.000465 => Gls Tokens per Sec:     2213 || Batch Translation Loss:   0.125024 => Txt Tokens per Sec:     5890 || Lr: 0.000050
2024-02-03 07:13:09,769 Epoch 878: Total Training Recognition Loss 0.09  Total Training Translation Loss 5.58 
2024-02-03 07:13:09,770 EPOCH 879
2024-02-03 07:13:15,133 Epoch 879: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.78 
2024-02-03 07:13:15,133 EPOCH 880
2024-02-03 07:13:15,662 [Epoch: 880 Step: 00058900] Batch Recognition Loss:   0.000373 => Gls Tokens per Sec:     2121 || Batch Translation Loss:   0.032281 => Txt Tokens per Sec:     5803 || Lr: 0.000050
2024-02-03 07:13:20,023 Epoch 880: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.39 
2024-02-03 07:13:20,023 EPOCH 881
2024-02-03 07:13:22,908 [Epoch: 881 Step: 00059000] Batch Recognition Loss:   0.000194 => Gls Tokens per Sec:     2188 || Batch Translation Loss:   0.022006 => Txt Tokens per Sec:     6152 || Lr: 0.000050
2024-02-03 07:13:24,813 Epoch 881: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.50 
2024-02-03 07:13:24,814 EPOCH 882
2024-02-03 07:13:30,212 Epoch 882: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.14 
2024-02-03 07:13:30,212 EPOCH 883
2024-02-03 07:13:30,565 [Epoch: 883 Step: 00059100] Batch Recognition Loss:   0.005273 => Gls Tokens per Sec:     2732 || Batch Translation Loss:   0.012981 => Txt Tokens per Sec:     6379 || Lr: 0.000050
2024-02-03 07:13:35,013 Epoch 883: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.79 
2024-02-03 07:13:35,013 EPOCH 884
2024-02-03 07:13:38,060 [Epoch: 884 Step: 00059200] Batch Recognition Loss:   0.000201 => Gls Tokens per Sec:     2019 || Batch Translation Loss:   0.015248 => Txt Tokens per Sec:     5442 || Lr: 0.000050
2024-02-03 07:13:40,480 Epoch 884: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.25 
2024-02-03 07:13:40,480 EPOCH 885
2024-02-03 07:13:45,689 Epoch 885: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.45 
2024-02-03 07:13:45,689 EPOCH 886
2024-02-03 07:13:45,993 [Epoch: 886 Step: 00059300] Batch Recognition Loss:   0.000259 => Gls Tokens per Sec:     2640 || Batch Translation Loss:   0.009120 => Txt Tokens per Sec:     6257 || Lr: 0.000050
2024-02-03 07:13:50,920 Epoch 886: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.59 
2024-02-03 07:13:50,921 EPOCH 887
2024-02-03 07:13:54,089 [Epoch: 887 Step: 00059400] Batch Recognition Loss:   0.000299 => Gls Tokens per Sec:     1920 || Batch Translation Loss:   0.033530 => Txt Tokens per Sec:     5310 || Lr: 0.000050
2024-02-03 07:13:56,341 Epoch 887: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.74 
2024-02-03 07:13:56,342 EPOCH 888
2024-02-03 07:14:01,573 Epoch 888: Total Training Recognition Loss 0.09  Total Training Translation Loss 6.49 
2024-02-03 07:14:01,573 EPOCH 889
2024-02-03 07:14:01,885 [Epoch: 889 Step: 00059500] Batch Recognition Loss:   0.000233 => Gls Tokens per Sec:     2065 || Batch Translation Loss:   0.087733 => Txt Tokens per Sec:     5477 || Lr: 0.000050
2024-02-03 07:14:06,660 Epoch 889: Total Training Recognition Loss 0.06  Total Training Translation Loss 4.81 
2024-02-03 07:14:06,660 EPOCH 890
2024-02-03 07:14:09,254 [Epoch: 890 Step: 00059600] Batch Recognition Loss:   0.000439 => Gls Tokens per Sec:     2283 || Batch Translation Loss:   0.013486 => Txt Tokens per Sec:     6439 || Lr: 0.000050
2024-02-03 07:14:11,564 Epoch 890: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.22 
2024-02-03 07:14:11,564 EPOCH 891
2024-02-03 07:14:17,220 Epoch 891: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.62 
2024-02-03 07:14:17,220 EPOCH 892
2024-02-03 07:14:17,416 [Epoch: 892 Step: 00059700] Batch Recognition Loss:   0.000164 => Gls Tokens per Sec:     2468 || Batch Translation Loss:   0.016522 => Txt Tokens per Sec:     5887 || Lr: 0.000050
2024-02-03 07:14:22,585 Epoch 892: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.50 
2024-02-03 07:14:22,585 EPOCH 893
2024-02-03 07:14:25,636 [Epoch: 893 Step: 00059800] Batch Recognition Loss:   0.000654 => Gls Tokens per Sec:     1889 || Batch Translation Loss:   0.058211 => Txt Tokens per Sec:     5455 || Lr: 0.000050
2024-02-03 07:14:28,171 Epoch 893: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.54 
2024-02-03 07:14:28,172 EPOCH 894
2024-02-03 07:14:33,304 Epoch 894: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.09 
2024-02-03 07:14:33,304 EPOCH 895
2024-02-03 07:14:33,445 [Epoch: 895 Step: 00059900] Batch Recognition Loss:   0.000432 => Gls Tokens per Sec:     2298 || Batch Translation Loss:   0.041469 => Txt Tokens per Sec:     5746 || Lr: 0.000050
2024-02-03 07:14:38,793 Epoch 895: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.84 
2024-02-03 07:14:38,794 EPOCH 896
2024-02-03 07:14:41,534 [Epoch: 896 Step: 00060000] Batch Recognition Loss:   0.002438 => Gls Tokens per Sec:     2012 || Batch Translation Loss:   0.023246 => Txt Tokens per Sec:     5640 || Lr: 0.000050
2024-02-03 07:14:50,194 Validation result at epoch 896, step    60000: duration: 8.6598s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 4.43501	Translation Loss: 86772.64062	PPL: 8205.25781
	Eval Metric: BLEU
	WER 3.25	(DEL: 0.07,	INS: 0.00,	SUB: 3.18)
	BLEU-4 0.71	(BLEU-1: 9.99,	BLEU-2: 2.90,	BLEU-3: 1.29,	BLEU-4: 0.71)
	CHRF 16.80	ROUGE 8.40
2024-02-03 07:14:50,195 Logging Recognition and Translation Outputs
2024-02-03 07:14:50,195 ========================================================================================================================
2024-02-03 07:14:50,196 Logging Sequence: 161_210.00
2024-02-03 07:14:50,196 	Gloss Reference :	A B+C+D+E
2024-02-03 07:14:50,196 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 07:14:50,196 	Gloss Alignment :	         
2024-02-03 07:14:50,196 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 07:14:50,197 	Text Reference  :	*** *** ******* **** we    hope    he     continues playing for          india
2024-02-03 07:14:50,197 	Text Hypothesis :	the two players took place between mumbai and       gujarat spokesperson runs 
2024-02-03 07:14:50,197 	Text Alignment  :	I   I   I       I    S     S       S      S         S       S            S    
2024-02-03 07:14:50,197 ========================================================================================================================
2024-02-03 07:14:50,197 Logging Sequence: 59_152.00
2024-02-03 07:14:50,197 	Gloss Reference :	A B+C+D+E
2024-02-03 07:14:50,198 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 07:14:50,198 	Gloss Alignment :	         
2024-02-03 07:14:50,198 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 07:14:50,199 	Text Reference  :	the organisers encouraged athletes to ***** * **** **** *** * use      the condoms in their home countries
2024-02-03 07:14:50,199 	Text Hypothesis :	*** once       she        returns  to india i will give you a suitable job as      a  token of   gratitude
2024-02-03 07:14:50,199 	Text Alignment  :	D   S          S          S           I     I I    I    I   I S        S   S       S  S     S    S        
2024-02-03 07:14:50,199 ========================================================================================================================
2024-02-03 07:14:50,200 Logging Sequence: 167_67.00
2024-02-03 07:14:50,200 	Gloss Reference :	A B+C+D+E
2024-02-03 07:14:50,200 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 07:14:50,200 	Gloss Alignment :	         
2024-02-03 07:14:50,200 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 07:14:50,202 	Text Reference  :	once infected the incubation period of  the virus is   1-2 weeks   after which symptoms begin   to **** appear
2024-02-03 07:14:50,202 	Text Hypothesis :	**** ******** *** ********** india  won the ***** toss and captain aaron finch had      elected to bowl first 
2024-02-03 07:14:50,202 	Text Alignment  :	D    D        D   D          S      S       D     S    S   S       S     S     S        S          I    S     
2024-02-03 07:14:50,202 ========================================================================================================================
2024-02-03 07:14:50,202 Logging Sequence: 77_13.00
2024-02-03 07:14:50,202 	Gloss Reference :	A B+C+D+E
2024-02-03 07:14:50,203 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 07:14:50,203 	Gloss Alignment :	         
2024-02-03 07:14:50,203 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 07:14:50,204 	Text Reference  :	*** this   was the 1st match that ended in      a   tie in    this season's ipl 
2024-02-03 07:14:50,204 	Text Hypothesis :	but sharma was the *** ***** **** ***** captain she was fined rs   12       lakh
2024-02-03 07:14:50,204 	Text Alignment  :	I   S              D   D     D    D     S       S   S   S     S    S        S   
2024-02-03 07:14:50,204 ========================================================================================================================
2024-02-03 07:14:50,204 Logging Sequence: 169_245.00
2024-02-03 07:14:50,204 	Gloss Reference :	A B+C+D+E
2024-02-03 07:14:50,205 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 07:14:50,205 	Gloss Alignment :	         
2024-02-03 07:14:50,205 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 07:14:50,206 	Text Reference  :	********** ******** ** **** ** *** mohammed shami has    said these trolls are spreading hate     through fake  accounts
2024-02-03 07:14:50,206 	Text Hypothesis :	pakistan's decision to stay in the pani     puri  seller on   the   street is  more      hygienic and     wears gloves  
2024-02-03 07:14:50,206 	Text Alignment  :	I          I        I  I    I  I   S        S     S      S    S     S      S   S         S        S       S     S       
2024-02-03 07:14:50,207 ========================================================================================================================
2024-02-03 07:14:52,562 Epoch 896: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.93 
2024-02-03 07:14:52,563 EPOCH 897
2024-02-03 07:14:58,050 Epoch 897: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.36 
2024-02-03 07:14:58,050 EPOCH 898
2024-02-03 07:14:58,128 [Epoch: 898 Step: 00060100] Batch Recognition Loss:   0.000491 => Gls Tokens per Sec:     2078 || Batch Translation Loss:   0.029979 => Txt Tokens per Sec:     6285 || Lr: 0.000050
2024-02-03 07:15:03,179 Epoch 898: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.32 
2024-02-03 07:15:03,179 EPOCH 899
2024-02-03 07:15:05,548 [Epoch: 899 Step: 00060200] Batch Recognition Loss:   0.000607 => Gls Tokens per Sec:     2297 || Batch Translation Loss:   0.016389 => Txt Tokens per Sec:     6033 || Lr: 0.000050
2024-02-03 07:15:08,369 Epoch 899: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.27 
2024-02-03 07:15:08,369 EPOCH 900
2024-02-03 07:15:13,738 [Epoch: 900 Step: 00060300] Batch Recognition Loss:   0.001470 => Gls Tokens per Sec:     1980 || Batch Translation Loss:   0.010251 => Txt Tokens per Sec:     5507 || Lr: 0.000050
2024-02-03 07:15:13,739 Epoch 900: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.16 
2024-02-03 07:15:13,739 EPOCH 901
2024-02-03 07:15:18,596 Epoch 901: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.12 
2024-02-03 07:15:18,597 EPOCH 902
2024-02-03 07:15:21,033 [Epoch: 902 Step: 00060400] Batch Recognition Loss:   0.000463 => Gls Tokens per Sec:     2131 || Batch Translation Loss:   0.007627 => Txt Tokens per Sec:     5709 || Lr: 0.000050
2024-02-03 07:15:23,895 Epoch 902: Total Training Recognition Loss 0.11  Total Training Translation Loss 1.28 
2024-02-03 07:15:23,895 EPOCH 903
2024-02-03 07:15:29,059 [Epoch: 903 Step: 00060500] Batch Recognition Loss:   0.000237 => Gls Tokens per Sec:     2028 || Batch Translation Loss:   0.015601 => Txt Tokens per Sec:     5647 || Lr: 0.000050
2024-02-03 07:15:29,130 Epoch 903: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.73 
2024-02-03 07:15:29,130 EPOCH 904
2024-02-03 07:15:34,599 Epoch 904: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.01 
2024-02-03 07:15:34,600 EPOCH 905
2024-02-03 07:15:36,742 [Epoch: 905 Step: 00060600] Batch Recognition Loss:   0.000095 => Gls Tokens per Sec:     2391 || Batch Translation Loss:   0.027646 => Txt Tokens per Sec:     6271 || Lr: 0.000050
2024-02-03 07:15:39,438 Epoch 905: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.76 
2024-02-03 07:15:39,438 EPOCH 906
2024-02-03 07:15:44,538 [Epoch: 906 Step: 00060700] Batch Recognition Loss:   0.000324 => Gls Tokens per Sec:     2022 || Batch Translation Loss:   0.028298 => Txt Tokens per Sec:     5677 || Lr: 0.000050
2024-02-03 07:15:44,635 Epoch 906: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.05 
2024-02-03 07:15:44,636 EPOCH 907
2024-02-03 07:15:49,571 Epoch 907: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.21 
2024-02-03 07:15:49,572 EPOCH 908
2024-02-03 07:15:51,950 [Epoch: 908 Step: 00060800] Batch Recognition Loss:   0.001964 => Gls Tokens per Sec:     2086 || Batch Translation Loss:   0.037208 => Txt Tokens per Sec:     5614 || Lr: 0.000050
2024-02-03 07:15:55,012 Epoch 908: Total Training Recognition Loss 0.09  Total Training Translation Loss 5.79 
2024-02-03 07:15:55,013 EPOCH 909
2024-02-03 07:16:00,292 [Epoch: 909 Step: 00060900] Batch Recognition Loss:   0.000850 => Gls Tokens per Sec:     1923 || Batch Translation Loss:   0.019577 => Txt Tokens per Sec:     5356 || Lr: 0.000050
2024-02-03 07:16:00,504 Epoch 909: Total Training Recognition Loss 0.10  Total Training Translation Loss 4.62 
2024-02-03 07:16:00,504 EPOCH 910
2024-02-03 07:16:05,502 Epoch 910: Total Training Recognition Loss 0.13  Total Training Translation Loss 3.24 
2024-02-03 07:16:05,502 EPOCH 911
2024-02-03 07:16:08,101 [Epoch: 911 Step: 00061000] Batch Recognition Loss:   0.017065 => Gls Tokens per Sec:     1848 || Batch Translation Loss:   0.016295 => Txt Tokens per Sec:     5291 || Lr: 0.000050
2024-02-03 07:16:10,877 Epoch 911: Total Training Recognition Loss 0.26  Total Training Translation Loss 3.55 
2024-02-03 07:16:10,877 EPOCH 912
2024-02-03 07:16:15,581 [Epoch: 912 Step: 00061100] Batch Recognition Loss:   0.000196 => Gls Tokens per Sec:     2124 || Batch Translation Loss:   0.044801 => Txt Tokens per Sec:     5880 || Lr: 0.000050
2024-02-03 07:16:15,973 Epoch 912: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.17 
2024-02-03 07:16:15,974 EPOCH 913
2024-02-03 07:16:21,317 Epoch 913: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.91 
2024-02-03 07:16:21,317 EPOCH 914
2024-02-03 07:16:23,661 [Epoch: 914 Step: 00061200] Batch Recognition Loss:   0.000470 => Gls Tokens per Sec:     1942 || Batch Translation Loss:   0.114283 => Txt Tokens per Sec:     5631 || Lr: 0.000050
2024-02-03 07:16:26,576 Epoch 914: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.99 
2024-02-03 07:16:26,577 EPOCH 915
2024-02-03 07:16:31,547 [Epoch: 915 Step: 00061300] Batch Recognition Loss:   0.003765 => Gls Tokens per Sec:     1978 || Batch Translation Loss:   0.095836 => Txt Tokens per Sec:     5489 || Lr: 0.000050
2024-02-03 07:16:32,012 Epoch 915: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.06 
2024-02-03 07:16:32,013 EPOCH 916
2024-02-03 07:16:37,271 Epoch 916: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.39 
2024-02-03 07:16:37,271 EPOCH 917
2024-02-03 07:16:39,184 [Epoch: 917 Step: 00061400] Batch Recognition Loss:   0.000801 => Gls Tokens per Sec:     2296 || Batch Translation Loss:   0.031497 => Txt Tokens per Sec:     6467 || Lr: 0.000050
2024-02-03 07:16:42,303 Epoch 917: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.47 
2024-02-03 07:16:42,304 EPOCH 918
2024-02-03 07:16:47,473 [Epoch: 918 Step: 00061500] Batch Recognition Loss:   0.000661 => Gls Tokens per Sec:     1871 || Batch Translation Loss:   0.015882 => Txt Tokens per Sec:     5253 || Lr: 0.000050
2024-02-03 07:16:47,837 Epoch 918: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.47 
2024-02-03 07:16:47,837 EPOCH 919
2024-02-03 07:16:52,525 Epoch 919: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.87 
2024-02-03 07:16:52,525 EPOCH 920
2024-02-03 07:16:54,479 [Epoch: 920 Step: 00061600] Batch Recognition Loss:   0.006586 => Gls Tokens per Sec:     2167 || Batch Translation Loss:   0.029935 => Txt Tokens per Sec:     5797 || Lr: 0.000050
2024-02-03 07:16:57,842 Epoch 920: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.99 
2024-02-03 07:16:57,843 EPOCH 921
2024-02-03 07:17:02,402 [Epoch: 921 Step: 00061700] Batch Recognition Loss:   0.000118 => Gls Tokens per Sec:     2086 || Batch Translation Loss:   0.013865 => Txt Tokens per Sec:     5781 || Lr: 0.000050
2024-02-03 07:17:03,131 Epoch 921: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.62 
2024-02-03 07:17:03,132 EPOCH 922
2024-02-03 07:17:08,618 Epoch 922: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.72 
2024-02-03 07:17:08,619 EPOCH 923
2024-02-03 07:17:10,765 [Epoch: 923 Step: 00061800] Batch Recognition Loss:   0.001269 => Gls Tokens per Sec:     1897 || Batch Translation Loss:   0.026144 => Txt Tokens per Sec:     5280 || Lr: 0.000050
2024-02-03 07:17:14,185 Epoch 923: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.38 
2024-02-03 07:17:14,185 EPOCH 924
2024-02-03 07:17:18,499 [Epoch: 924 Step: 00061900] Batch Recognition Loss:   0.000797 => Gls Tokens per Sec:     2168 || Batch Translation Loss:   0.006188 => Txt Tokens per Sec:     5997 || Lr: 0.000050
2024-02-03 07:17:19,161 Epoch 924: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.90 
2024-02-03 07:17:19,162 EPOCH 925
2024-02-03 07:17:24,551 Epoch 925: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.41 
2024-02-03 07:17:24,551 EPOCH 926
2024-02-03 07:17:26,239 [Epoch: 926 Step: 00062000] Batch Recognition Loss:   0.000118 => Gls Tokens per Sec:     2371 || Batch Translation Loss:   0.081061 => Txt Tokens per Sec:     6275 || Lr: 0.000050
2024-02-03 07:17:34,812 Validation result at epoch 926, step    62000: duration: 8.5723s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 4.27389	Translation Loss: 87416.82031	PPL: 8773.02734
	Eval Metric: BLEU
	WER 2.97	(DEL: 0.00,	INS: 0.00,	SUB: 2.97)
	BLEU-4 0.57	(BLEU-1: 9.84,	BLEU-2: 3.06,	BLEU-3: 1.21,	BLEU-4: 0.57)
	CHRF 16.74	ROUGE 8.33
2024-02-03 07:17:34,813 Logging Recognition and Translation Outputs
2024-02-03 07:17:34,813 ========================================================================================================================
2024-02-03 07:17:34,813 Logging Sequence: 162_86.00
2024-02-03 07:17:34,813 	Gloss Reference :	A B+C+D+E  
2024-02-03 07:17:34,813 	Gloss Hypothesis:	A B+C+D+E+B
2024-02-03 07:17:34,813 	Gloss Alignment :	  S        
2024-02-03 07:17:34,814 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 07:17:34,815 	Text Reference  :	*** *** **** ** ***** ** ****** ***** **** *** ****** do   you   know      how   people    responded to this
2024-02-03 07:17:34,815 	Text Hypothesis :	for the next 40 years no indian could beat his record time until paramjeet singh surpassed it        in 1998
2024-02-03 07:17:34,815 	Text Alignment  :	I   I   I    I  I     I  I      I     I    I   I      S    S     S         S     S         S         S  S   
2024-02-03 07:17:34,815 ========================================================================================================================
2024-02-03 07:17:34,815 Logging Sequence: 98_113.00
2024-02-03 07:17:34,815 	Gloss Reference :	A B+C+D+E
2024-02-03 07:17:34,815 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 07:17:34,816 	Gloss Alignment :	         
2024-02-03 07:17:34,816 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 07:17:34,817 	Text Reference  :	*** * **** ***** ** *** ** however      australia legends opted out due  to the covid situation 
2024-02-03 07:17:34,817 	Text Hypothesis :	any 3 days later on him an announcement and       it      was   a   part of the ***** tournament
2024-02-03 07:17:34,817 	Text Alignment  :	I   I I    I     I  I   I  S            S         S       S     S   S    S      D     S         
2024-02-03 07:17:34,817 ========================================================================================================================
2024-02-03 07:17:34,817 Logging Sequence: 178_25.00
2024-02-03 07:17:34,817 	Gloss Reference :	A B+C+D+E
2024-02-03 07:17:34,818 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 07:17:34,818 	Gloss Alignment :	         
2024-02-03 07:17:34,818 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 07:17:34,818 	Text Reference  :	on 5th may the ***** police ***** ******* filed       an  fir     
2024-02-03 07:17:34,818 	Text Hypothesis :	** *** *** the delhi police still haven't apprehended the wrestler
2024-02-03 07:17:34,819 	Text Alignment  :	D  D   D       I            I     I       S           S   S       
2024-02-03 07:17:34,819 ========================================================================================================================
2024-02-03 07:17:34,819 Logging Sequence: 147_148.00
2024-02-03 07:17:34,819 	Gloss Reference :	A B+C+D+E
2024-02-03 07:17:34,819 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 07:17:34,819 	Gloss Alignment :	         
2024-02-03 07:17:34,819 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 07:17:34,821 	Text Reference  :	*** ******** ** *** *** **** *** ********* *** *** ***** ** when  explained this to my teammates they      understood my  feelings   
2024-02-03 07:17:34,821 	Text Hypothesis :	the contents in the bag were not important the bag might be empty even      but  it is not       available for        the semi-finals
2024-02-03 07:17:34,821 	Text Alignment  :	I   I        I  I   I   I    I   I         I   I   I     I  S     S         S    S  S  S         S         S          S   S          
2024-02-03 07:17:34,821 ========================================================================================================================
2024-02-03 07:17:34,821 Logging Sequence: 61_326.00
2024-02-03 07:17:34,821 	Gloss Reference :	A B+C+D+E
2024-02-03 07:17:34,821 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 07:17:34,822 	Gloss Alignment :	         
2024-02-03 07:17:34,822 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 07:17:34,822 	Text Reference  :	*** ****** however   no confirmation has been made    yet            
2024-02-03 07:17:34,822 	Text Hypothesis :	the sports authority of india        sai then tweeted congratulations
2024-02-03 07:17:34,823 	Text Alignment  :	I   I      S         S  S            S   S    S       S              
2024-02-03 07:17:34,823 ========================================================================================================================
2024-02-03 07:17:38,476 Epoch 926: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.53 
2024-02-03 07:17:38,477 EPOCH 927
2024-02-03 07:17:42,772 [Epoch: 927 Step: 00062100] Batch Recognition Loss:   0.002575 => Gls Tokens per Sec:     2162 || Batch Translation Loss:   0.019826 => Txt Tokens per Sec:     5993 || Lr: 0.000050
2024-02-03 07:17:43,475 Epoch 927: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.72 
2024-02-03 07:17:43,475 EPOCH 928
2024-02-03 07:17:48,997 Epoch 928: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.63 
2024-02-03 07:17:48,997 EPOCH 929
2024-02-03 07:17:50,611 [Epoch: 929 Step: 00062200] Batch Recognition Loss:   0.000794 => Gls Tokens per Sec:     2381 || Batch Translation Loss:   0.017099 => Txt Tokens per Sec:     6625 || Lr: 0.000050
2024-02-03 07:17:53,946 Epoch 929: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.51 
2024-02-03 07:17:53,947 EPOCH 930
2024-02-03 07:17:58,363 [Epoch: 930 Step: 00062300] Batch Recognition Loss:   0.000227 => Gls Tokens per Sec:     2045 || Batch Translation Loss:   0.025638 => Txt Tokens per Sec:     5613 || Lr: 0.000050
2024-02-03 07:17:59,260 Epoch 930: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.46 
2024-02-03 07:17:59,260 EPOCH 931
2024-02-03 07:18:04,626 Epoch 931: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.02 
2024-02-03 07:18:04,626 EPOCH 932
2024-02-03 07:18:06,259 [Epoch: 932 Step: 00062400] Batch Recognition Loss:   0.000260 => Gls Tokens per Sec:     2255 || Batch Translation Loss:   0.025494 => Txt Tokens per Sec:     6426 || Lr: 0.000050
2024-02-03 07:18:09,567 Epoch 932: Total Training Recognition Loss 0.08  Total Training Translation Loss 4.12 
2024-02-03 07:18:09,567 EPOCH 933
2024-02-03 07:18:14,225 [Epoch: 933 Step: 00062500] Batch Recognition Loss:   0.008622 => Gls Tokens per Sec:     1905 || Batch Translation Loss:   0.016111 => Txt Tokens per Sec:     5316 || Lr: 0.000050
2024-02-03 07:18:14,958 Epoch 933: Total Training Recognition Loss 0.10  Total Training Translation Loss 4.83 
2024-02-03 07:18:14,959 EPOCH 934
2024-02-03 07:18:20,119 Epoch 934: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.77 
2024-02-03 07:18:20,120 EPOCH 935
2024-02-03 07:18:21,719 [Epoch: 935 Step: 00062600] Batch Recognition Loss:   0.000719 => Gls Tokens per Sec:     2204 || Batch Translation Loss:   0.022403 => Txt Tokens per Sec:     5981 || Lr: 0.000050
2024-02-03 07:18:25,403 Epoch 935: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.54 
2024-02-03 07:18:25,403 EPOCH 936
2024-02-03 07:18:29,915 [Epoch: 936 Step: 00062700] Batch Recognition Loss:   0.000263 => Gls Tokens per Sec:     1930 || Batch Translation Loss:   0.019506 => Txt Tokens per Sec:     5380 || Lr: 0.000050
2024-02-03 07:18:30,775 Epoch 936: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.77 
2024-02-03 07:18:30,775 EPOCH 937
2024-02-03 07:18:35,565 Epoch 937: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.46 
2024-02-03 07:18:35,565 EPOCH 938
2024-02-03 07:18:36,954 [Epoch: 938 Step: 00062800] Batch Recognition Loss:   0.000433 => Gls Tokens per Sec:     2420 || Batch Translation Loss:   0.075471 => Txt Tokens per Sec:     6696 || Lr: 0.000050
2024-02-03 07:18:40,863 Epoch 938: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.51 
2024-02-03 07:18:40,863 EPOCH 939
2024-02-03 07:18:44,789 [Epoch: 939 Step: 00062900] Batch Recognition Loss:   0.000541 => Gls Tokens per Sec:     2178 || Batch Translation Loss:   0.014647 => Txt Tokens per Sec:     5949 || Lr: 0.000050
2024-02-03 07:18:46,115 Epoch 939: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.70 
2024-02-03 07:18:46,116 EPOCH 940
2024-02-03 07:18:51,853 Epoch 940: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.34 
2024-02-03 07:18:51,854 EPOCH 941
2024-02-03 07:18:53,319 [Epoch: 941 Step: 00063000] Batch Recognition Loss:   0.000337 => Gls Tokens per Sec:     2186 || Batch Translation Loss:   0.028850 => Txt Tokens per Sec:     6359 || Lr: 0.000050
2024-02-03 07:18:56,999 Epoch 941: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.52 
2024-02-03 07:18:57,000 EPOCH 942
2024-02-03 07:19:00,978 [Epoch: 942 Step: 00063100] Batch Recognition Loss:   0.000706 => Gls Tokens per Sec:     2132 || Batch Translation Loss:   0.024000 => Txt Tokens per Sec:     5962 || Lr: 0.000050
2024-02-03 07:19:02,064 Epoch 942: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.35 
2024-02-03 07:19:02,065 EPOCH 943
2024-02-03 07:19:07,642 Epoch 943: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.76 
2024-02-03 07:19:07,642 EPOCH 944
2024-02-03 07:19:09,009 [Epoch: 944 Step: 00063200] Batch Recognition Loss:   0.000415 => Gls Tokens per Sec:     2160 || Batch Translation Loss:   0.014130 => Txt Tokens per Sec:     6100 || Lr: 0.000050
2024-02-03 07:19:12,801 Epoch 944: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.39 
2024-02-03 07:19:12,802 EPOCH 945
2024-02-03 07:19:16,717 [Epoch: 945 Step: 00063300] Batch Recognition Loss:   0.000585 => Gls Tokens per Sec:     2103 || Batch Translation Loss:   0.021855 => Txt Tokens per Sec:     5754 || Lr: 0.000050
2024-02-03 07:19:18,090 Epoch 945: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.98 
2024-02-03 07:19:18,090 EPOCH 946
2024-02-03 07:19:23,400 Epoch 946: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.72 
2024-02-03 07:19:23,400 EPOCH 947
2024-02-03 07:19:24,816 [Epoch: 947 Step: 00063400] Batch Recognition Loss:   0.000095 => Gls Tokens per Sec:     2035 || Batch Translation Loss:   0.014741 => Txt Tokens per Sec:     5844 || Lr: 0.000050
2024-02-03 07:19:28,370 Epoch 947: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.45 
2024-02-03 07:19:28,370 EPOCH 948
2024-02-03 07:19:32,692 [Epoch: 948 Step: 00063500] Batch Recognition Loss:   0.000155 => Gls Tokens per Sec:     1867 || Batch Translation Loss:   0.021333 => Txt Tokens per Sec:     5221 || Lr: 0.000050
2024-02-03 07:19:33,891 Epoch 948: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.35 
2024-02-03 07:19:33,892 EPOCH 949
2024-02-03 07:19:39,170 Epoch 949: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.02 
2024-02-03 07:19:39,171 EPOCH 950
2024-02-03 07:19:40,527 [Epoch: 950 Step: 00063600] Batch Recognition Loss:   0.003876 => Gls Tokens per Sec:     1939 || Batch Translation Loss:   0.028141 => Txt Tokens per Sec:     5298 || Lr: 0.000050
2024-02-03 07:19:44,168 Epoch 950: Total Training Recognition Loss 0.09  Total Training Translation Loss 3.75 
2024-02-03 07:19:44,168 EPOCH 951
2024-02-03 07:19:48,403 [Epoch: 951 Step: 00063700] Batch Recognition Loss:   0.000231 => Gls Tokens per Sec:     1868 || Batch Translation Loss:   0.016653 => Txt Tokens per Sec:     5221 || Lr: 0.000050
2024-02-03 07:19:49,554 Epoch 951: Total Training Recognition Loss 0.25  Total Training Translation Loss 5.32 
2024-02-03 07:19:49,554 EPOCH 952
2024-02-03 07:19:55,105 Epoch 952: Total Training Recognition Loss 0.22  Total Training Translation Loss 22.74 
2024-02-03 07:19:55,105 EPOCH 953
2024-02-03 07:19:56,634 [Epoch: 953 Step: 00063800] Batch Recognition Loss:   0.003437 => Gls Tokens per Sec:     1674 || Batch Translation Loss:   0.266542 => Txt Tokens per Sec:     5097 || Lr: 0.000050
2024-02-03 07:20:00,201 Epoch 953: Total Training Recognition Loss 0.13  Total Training Translation Loss 7.15 
2024-02-03 07:20:00,202 EPOCH 954
2024-02-03 07:20:04,087 [Epoch: 954 Step: 00063900] Batch Recognition Loss:   0.003149 => Gls Tokens per Sec:     1996 || Batch Translation Loss:   0.048714 => Txt Tokens per Sec:     5555 || Lr: 0.000050
2024-02-03 07:20:05,521 Epoch 954: Total Training Recognition Loss 0.09  Total Training Translation Loss 2.50 
2024-02-03 07:20:05,522 EPOCH 955
2024-02-03 07:20:10,892 Epoch 955: Total Training Recognition Loss 0.10  Total Training Translation Loss 1.77 
2024-02-03 07:20:10,893 EPOCH 956
2024-02-03 07:20:12,007 [Epoch: 956 Step: 00064000] Batch Recognition Loss:   0.001745 => Gls Tokens per Sec:     2158 || Batch Translation Loss:   0.022840 => Txt Tokens per Sec:     5736 || Lr: 0.000050
2024-02-03 07:20:20,379 Validation result at epoch 956, step    64000: duration: 8.3716s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 3.95247	Translation Loss: 86269.88281	PPL: 7787.78613
	Eval Metric: BLEU
	WER 3.18	(DEL: 0.07,	INS: 0.00,	SUB: 3.11)
	BLEU-4 0.60	(BLEU-1: 10.40,	BLEU-2: 3.04,	BLEU-3: 1.20,	BLEU-4: 0.60)
	CHRF 16.87	ROUGE 8.79
2024-02-03 07:20:20,380 Logging Recognition and Translation Outputs
2024-02-03 07:20:20,381 ========================================================================================================================
2024-02-03 07:20:20,381 Logging Sequence: 136_107.00
2024-02-03 07:20:20,381 	Gloss Reference :	A B+C+D+E
2024-02-03 07:20:20,381 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 07:20:20,381 	Gloss Alignment :	         
2024-02-03 07:20:20,382 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 07:20:20,382 	Text Reference  :	sindhu replied that she  had stop eating icecream because of                   her    training
2024-02-03 07:20:20,383 	Text Hypothesis :	****** it      is   held at  the  start  of       july    wicketkeeper-batsman rishab such    
2024-02-03 07:20:20,383 	Text Alignment  :	D      S       S    S    S   S    S      S        S       S                    S      S       
2024-02-03 07:20:20,383 ========================================================================================================================
2024-02-03 07:20:20,383 Logging Sequence: 59_152.00
2024-02-03 07:20:20,383 	Gloss Reference :	A B+C+D+E
2024-02-03 07:20:20,383 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 07:20:20,384 	Gloss Alignment :	         
2024-02-03 07:20:20,384 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 07:20:20,385 	Text Reference  :	the organisers encouraged athletes to ***** * **** **** *** * use      the condoms in their home countries
2024-02-03 07:20:20,385 	Text Hypothesis :	*** once       she        returns  to india i will give you a suitable job as      a  token of   gratitude
2024-02-03 07:20:20,385 	Text Alignment  :	D   S          S          S           I     I I    I    I   I S        S   S       S  S     S    S        
2024-02-03 07:20:20,385 ========================================================================================================================
2024-02-03 07:20:20,385 Logging Sequence: 52_36.00
2024-02-03 07:20:20,386 	Gloss Reference :	A B+C+D+E
2024-02-03 07:20:20,386 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 07:20:20,386 	Gloss Alignment :	         
2024-02-03 07:20:20,386 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 07:20:20,387 	Text Reference  :	*** *** recently dhoni was     travellin on an   indigo flight   
2024-02-03 07:20:20,387 	Text Hypothesis :	now csk has      been  flooded with      a  huge fan    following
2024-02-03 07:20:20,387 	Text Alignment  :	I   I   S        S     S       S         S  S    S      S        
2024-02-03 07:20:20,387 ========================================================================================================================
2024-02-03 07:20:20,387 Logging Sequence: 91_142.00
2024-02-03 07:20:20,387 	Gloss Reference :	A B+C+D+E
2024-02-03 07:20:20,387 	Gloss Hypothesis:	A B+C+D  
2024-02-03 07:20:20,388 	Gloss Alignment :	  S      
2024-02-03 07:20:20,388 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 07:20:20,389 	Text Reference  :	the icc has        penalised kaur by        deducting 75        of *** ***** *** *** *** her match   fee  
2024-02-03 07:20:20,389 	Text Hypothesis :	the *** bangladesh captain   was  suspended assistant secretary of the men's and are she was resumed there
2024-02-03 07:20:20,389 	Text Alignment  :	    D   S          S         S    S         S         S            I   I     I   I   I   S   S       S    
2024-02-03 07:20:20,389 ========================================================================================================================
2024-02-03 07:20:20,389 Logging Sequence: 134_54.00
2024-02-03 07:20:20,390 	Gloss Reference :	A B+C+D+E
2024-02-03 07:20:20,390 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 07:20:20,390 	Gloss Alignment :	         
2024-02-03 07:20:20,390 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 07:20:20,390 	Text Reference  :	** ****** *** the     news went viral    
2024-02-03 07:20:20,391 	Text Hypothesis :	so people are stunned at   the  situation
2024-02-03 07:20:20,391 	Text Alignment  :	I  I      I   S       S    S    S        
2024-02-03 07:20:20,391 ========================================================================================================================
2024-02-03 07:20:24,812 Epoch 956: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.61 
2024-02-03 07:20:24,812 EPOCH 957
2024-02-03 07:20:28,233 [Epoch: 957 Step: 00064100] Batch Recognition Loss:   0.000551 => Gls Tokens per Sec:     2246 || Batch Translation Loss:   0.047338 => Txt Tokens per Sec:     6132 || Lr: 0.000050
2024-02-03 07:20:29,976 Epoch 957: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.41 
2024-02-03 07:20:29,977 EPOCH 958
2024-02-03 07:20:35,240 Epoch 958: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.39 
2024-02-03 07:20:35,240 EPOCH 959
2024-02-03 07:20:36,115 [Epoch: 959 Step: 00064200] Batch Recognition Loss:   0.001367 => Gls Tokens per Sec:     2563 || Batch Translation Loss:   0.008920 => Txt Tokens per Sec:     6725 || Lr: 0.000050
2024-02-03 07:20:40,334 Epoch 959: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.26 
2024-02-03 07:20:40,335 EPOCH 960
2024-02-03 07:20:43,840 [Epoch: 960 Step: 00064300] Batch Recognition Loss:   0.000185 => Gls Tokens per Sec:     2121 || Batch Translation Loss:   0.020627 => Txt Tokens per Sec:     5884 || Lr: 0.000050
2024-02-03 07:20:45,279 Epoch 960: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.21 
2024-02-03 07:20:45,279 EPOCH 961
2024-02-03 07:20:50,704 Epoch 961: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.46 
2024-02-03 07:20:50,705 EPOCH 962
2024-02-03 07:20:51,781 [Epoch: 962 Step: 00064400] Batch Recognition Loss:   0.000234 => Gls Tokens per Sec:     1851 || Batch Translation Loss:   0.019498 => Txt Tokens per Sec:     5623 || Lr: 0.000050
2024-02-03 07:20:55,778 Epoch 962: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.38 
2024-02-03 07:20:55,778 EPOCH 963
2024-02-03 07:20:59,477 [Epoch: 963 Step: 00064500] Batch Recognition Loss:   0.000715 => Gls Tokens per Sec:     1966 || Batch Translation Loss:   0.079074 => Txt Tokens per Sec:     5621 || Lr: 0.000050
2024-02-03 07:21:00,938 Epoch 963: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.31 
2024-02-03 07:21:00,938 EPOCH 964
2024-02-03 07:21:06,062 Epoch 964: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.26 
2024-02-03 07:21:06,063 EPOCH 965
2024-02-03 07:21:06,919 [Epoch: 965 Step: 00064600] Batch Recognition Loss:   0.001895 => Gls Tokens per Sec:     2246 || Batch Translation Loss:   0.014357 => Txt Tokens per Sec:     6249 || Lr: 0.000050
2024-02-03 07:21:11,153 Epoch 965: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.46 
2024-02-03 07:21:11,153 EPOCH 966
2024-02-03 07:21:14,897 [Epoch: 966 Step: 00064700] Batch Recognition Loss:   0.000850 => Gls Tokens per Sec:     1923 || Batch Translation Loss:   0.026409 => Txt Tokens per Sec:     5362 || Lr: 0.000050
2024-02-03 07:21:16,768 Epoch 966: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.39 
2024-02-03 07:21:16,768 EPOCH 967
2024-02-03 07:21:22,284 Epoch 967: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.44 
2024-02-03 07:21:22,285 EPOCH 968
2024-02-03 07:21:23,081 [Epoch: 968 Step: 00064800] Batch Recognition Loss:   0.000217 => Gls Tokens per Sec:     2215 || Batch Translation Loss:   0.015672 => Txt Tokens per Sec:     6492 || Lr: 0.000050
2024-02-03 07:21:27,186 Epoch 968: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.59 
2024-02-03 07:21:27,187 EPOCH 969
2024-02-03 07:21:30,874 [Epoch: 969 Step: 00064900] Batch Recognition Loss:   0.001792 => Gls Tokens per Sec:     1885 || Batch Translation Loss:   0.118944 => Txt Tokens per Sec:     5374 || Lr: 0.000050
2024-02-03 07:21:32,441 Epoch 969: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.64 
2024-02-03 07:21:32,441 EPOCH 970
2024-02-03 07:21:37,541 Epoch 970: Total Training Recognition Loss 0.06  Total Training Translation Loss 4.67 
2024-02-03 07:21:37,541 EPOCH 971
2024-02-03 07:21:38,362 [Epoch: 971 Step: 00065000] Batch Recognition Loss:   0.000193 => Gls Tokens per Sec:     1953 || Batch Translation Loss:   0.022687 => Txt Tokens per Sec:     5202 || Lr: 0.000050
2024-02-03 07:21:42,922 Epoch 971: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.46 
2024-02-03 07:21:42,922 EPOCH 972
2024-02-03 07:21:46,120 [Epoch: 972 Step: 00065100] Batch Recognition Loss:   0.000335 => Gls Tokens per Sec:     2124 || Batch Translation Loss:   0.096370 => Txt Tokens per Sec:     5922 || Lr: 0.000050
2024-02-03 07:21:48,086 Epoch 972: Total Training Recognition Loss 0.07  Total Training Translation Loss 4.91 
2024-02-03 07:21:48,087 EPOCH 973
2024-02-03 07:21:53,083 Epoch 973: Total Training Recognition Loss 0.09  Total Training Translation Loss 4.28 
2024-02-03 07:21:53,083 EPOCH 974
2024-02-03 07:21:53,822 [Epoch: 974 Step: 00065200] Batch Recognition Loss:   0.001135 => Gls Tokens per Sec:     1950 || Batch Translation Loss:   0.091272 => Txt Tokens per Sec:     5547 || Lr: 0.000050
2024-02-03 07:21:58,516 Epoch 974: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.92 
2024-02-03 07:21:58,516 EPOCH 975
2024-02-03 07:22:01,993 [Epoch: 975 Step: 00065300] Batch Recognition Loss:   0.000377 => Gls Tokens per Sec:     1933 || Batch Translation Loss:   0.027935 => Txt Tokens per Sec:     5587 || Lr: 0.000050
2024-02-03 07:22:03,833 Epoch 975: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.70 
2024-02-03 07:22:03,833 EPOCH 976
2024-02-03 07:22:09,263 Epoch 976: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.52 
2024-02-03 07:22:09,263 EPOCH 977
2024-02-03 07:22:10,066 [Epoch: 977 Step: 00065400] Batch Recognition Loss:   0.000214 => Gls Tokens per Sec:     1598 || Batch Translation Loss:   0.027471 => Txt Tokens per Sec:     4788 || Lr: 0.000050
2024-02-03 07:22:14,826 Epoch 977: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.70 
2024-02-03 07:22:14,826 EPOCH 978
2024-02-03 07:22:17,922 [Epoch: 978 Step: 00065500] Batch Recognition Loss:   0.000244 => Gls Tokens per Sec:     2120 || Batch Translation Loss:   0.019098 => Txt Tokens per Sec:     5850 || Lr: 0.000050
2024-02-03 07:22:20,028 Epoch 978: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.38 
2024-02-03 07:22:20,028 EPOCH 979
2024-02-03 07:22:25,497 Epoch 979: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.72 
2024-02-03 07:22:25,497 EPOCH 980
2024-02-03 07:22:25,984 [Epoch: 980 Step: 00065600] Batch Recognition Loss:   0.006038 => Gls Tokens per Sec:     2305 || Batch Translation Loss:   0.022562 => Txt Tokens per Sec:     6327 || Lr: 0.000050
2024-02-03 07:22:30,370 Epoch 980: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.58 
2024-02-03 07:22:30,370 EPOCH 981
2024-02-03 07:22:33,782 [Epoch: 981 Step: 00065700] Batch Recognition Loss:   0.000567 => Gls Tokens per Sec:     1851 || Batch Translation Loss:   0.060218 => Txt Tokens per Sec:     5171 || Lr: 0.000050
2024-02-03 07:22:35,723 Epoch 981: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.26 
2024-02-03 07:22:35,723 EPOCH 982
2024-02-03 07:22:40,765 Epoch 982: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.30 
2024-02-03 07:22:40,766 EPOCH 983
2024-02-03 07:22:41,218 [Epoch: 983 Step: 00065800] Batch Recognition Loss:   0.001238 => Gls Tokens per Sec:     2129 || Batch Translation Loss:   0.023599 => Txt Tokens per Sec:     5455 || Lr: 0.000050
2024-02-03 07:22:46,072 Epoch 983: Total Training Recognition Loss 0.07  Total Training Translation Loss 4.70 
2024-02-03 07:22:46,072 EPOCH 984
2024-02-03 07:22:48,812 [Epoch: 984 Step: 00065900] Batch Recognition Loss:   0.000270 => Gls Tokens per Sec:     2278 || Batch Translation Loss:   0.028613 => Txt Tokens per Sec:     6115 || Lr: 0.000050
2024-02-03 07:22:51,338 Epoch 984: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.62 
2024-02-03 07:22:51,338 EPOCH 985
2024-02-03 07:22:56,678 Epoch 985: Total Training Recognition Loss 0.08  Total Training Translation Loss 3.83 
2024-02-03 07:22:56,678 EPOCH 986
2024-02-03 07:22:57,105 [Epoch: 986 Step: 00066000] Batch Recognition Loss:   0.000966 => Gls Tokens per Sec:     1882 || Batch Translation Loss:   0.044636 => Txt Tokens per Sec:     5238 || Lr: 0.000050
2024-02-03 07:23:05,769 Validation result at epoch 986, step    66000: duration: 8.6640s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 4.32312	Translation Loss: 87338.32812	PPL: 8701.79297
	Eval Metric: BLEU
	WER 2.97	(DEL: 0.00,	INS: 0.00,	SUB: 2.97)
	BLEU-4 0.48	(BLEU-1: 10.32,	BLEU-2: 2.75,	BLEU-3: 1.07,	BLEU-4: 0.48)
	CHRF 16.79	ROUGE 8.75
2024-02-03 07:23:05,770 Logging Recognition and Translation Outputs
2024-02-03 07:23:05,770 ========================================================================================================================
2024-02-03 07:23:05,770 Logging Sequence: 60_85.00
2024-02-03 07:23:05,771 	Gloss Reference :	A B+C+D+E
2024-02-03 07:23:05,771 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 07:23:05,771 	Gloss Alignment :	         
2024-02-03 07:23:05,771 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 07:23:05,772 	Text Reference  :	he had a  medical history of asthma and some heart issues which is why he passed away so  quickly   
2024-02-03 07:23:05,772 	Text Hypothesis :	** i   am very    grate   to see    and **** ***** ****** ***** ** *** ** media  for  his retirement
2024-02-03 07:23:05,773 	Text Alignment  :	D  S   S  S       S       S  S          D    D     D      D     D  D   D  S      S    S   S         
2024-02-03 07:23:05,773 ========================================================================================================================
2024-02-03 07:23:05,773 Logging Sequence: 72_59.00
2024-02-03 07:23:05,773 	Gloss Reference :	A B+C+D+E
2024-02-03 07:23:05,773 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 07:23:05,773 	Gloss Alignment :	         
2024-02-03 07:23:05,773 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 07:23:05,774 	Text Reference  :	after that sapna and   shobit  started arguing and ******* misbehaving with the           cricketer
2024-02-03 07:23:05,774 	Text Hypothesis :	then  in   2013  kochi tuskers were    removed and gambhir had         an   international matches  
2024-02-03 07:23:05,775 	Text Alignment  :	S     S    S     S     S       S       S           I       S           S    S             S        
2024-02-03 07:23:05,775 ========================================================================================================================
2024-02-03 07:23:05,775 Logging Sequence: 133_77.00
2024-02-03 07:23:05,775 	Gloss Reference :	A B+C+D+E
2024-02-03 07:23:05,775 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 07:23:05,775 	Gloss Alignment :	         
2024-02-03 07:23:05,775 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 07:23:05,776 	Text Reference  :	** ** the       group then welcomed australian prime minister anthony albanese on    his   arrival
2024-02-03 07:23:05,777 	Text Hypothesis :	it is difficult to    book so       that       who   who      came    the      first world cup    
2024-02-03 07:23:05,777 	Text Alignment  :	I  I  S         S     S    S        S          S     S        S       S        S     S     S      
2024-02-03 07:23:05,777 ========================================================================================================================
2024-02-03 07:23:05,777 Logging Sequence: 130_114.00
2024-02-03 07:23:05,777 	Gloss Reference :	A B+C+D+E
2024-02-03 07:23:05,777 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 07:23:05,777 	Gloss Alignment :	         
2024-02-03 07:23:05,777 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 07:23:05,778 	Text Reference  :	** *** *** ** ***** ** * **** ***** ** images of  him knitting went viral   
2024-02-03 07:23:05,778 	Text Hypothesis :	we are sad as there is a huge shock to we     had an  family   once olympics
2024-02-03 07:23:05,778 	Text Alignment  :	I  I   I   I  I     I  I I    I     I  S      S   S   S        S    S       
2024-02-03 07:23:05,779 ========================================================================================================================
2024-02-03 07:23:05,779 Logging Sequence: 173_107.00
2024-02-03 07:23:05,779 	Gloss Reference :	A B+C+D+E
2024-02-03 07:23:05,779 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 07:23:05,779 	Gloss Alignment :	         
2024-02-03 07:23:05,779 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 07:23:05,780 	Text Reference  :	india and      england   were    set to     play  a    5-match test     series
2024-02-03 07:23:05,780 	Text Hypothesis :	the   incident triggered outrage on  social media with people  pointing out   
2024-02-03 07:23:05,780 	Text Alignment  :	S     S        S         S       S   S      S     S    S       S        S     
2024-02-03 07:23:05,780 ========================================================================================================================
2024-02-03 07:23:11,072 Epoch 986: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.27 
2024-02-03 07:23:11,073 EPOCH 987
2024-02-03 07:23:14,107 [Epoch: 987 Step: 00066100] Batch Recognition Loss:   0.000588 => Gls Tokens per Sec:     1975 || Batch Translation Loss:   0.176053 => Txt Tokens per Sec:     5363 || Lr: 0.000050
2024-02-03 07:23:16,303 Epoch 987: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.12 
2024-02-03 07:23:16,304 EPOCH 988
2024-02-03 07:23:21,802 Epoch 988: Total Training Recognition Loss 0.10  Total Training Translation Loss 2.53 
2024-02-03 07:23:21,802 EPOCH 989
2024-02-03 07:23:22,124 [Epoch: 989 Step: 00066200] Batch Recognition Loss:   0.000425 => Gls Tokens per Sec:     2000 || Batch Translation Loss:   0.024945 => Txt Tokens per Sec:     4972 || Lr: 0.000050
2024-02-03 07:23:27,142 Epoch 989: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.89 
2024-02-03 07:23:27,143 EPOCH 990
2024-02-03 07:23:29,908 [Epoch: 990 Step: 00066300] Batch Recognition Loss:   0.000181 => Gls Tokens per Sec:     2109 || Batch Translation Loss:   0.025353 => Txt Tokens per Sec:     5839 || Lr: 0.000050
2024-02-03 07:23:32,148 Epoch 990: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.23 
2024-02-03 07:23:32,149 EPOCH 991
2024-02-03 07:23:37,550 Epoch 991: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.50 
2024-02-03 07:23:37,550 EPOCH 992
2024-02-03 07:23:37,757 [Epoch: 992 Step: 00066400] Batch Recognition Loss:   0.000275 => Gls Tokens per Sec:     2342 || Batch Translation Loss:   0.018835 => Txt Tokens per Sec:     6776 || Lr: 0.000050
2024-02-03 07:23:42,774 Epoch 992: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.29 
2024-02-03 07:23:42,774 EPOCH 993
2024-02-03 07:23:45,650 [Epoch: 993 Step: 00066500] Batch Recognition Loss:   0.002895 => Gls Tokens per Sec:     2004 || Batch Translation Loss:   0.020574 => Txt Tokens per Sec:     5475 || Lr: 0.000050
2024-02-03 07:23:48,243 Epoch 993: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.67 
2024-02-03 07:23:48,244 EPOCH 994
2024-02-03 07:23:53,468 Epoch 994: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.43 
2024-02-03 07:23:53,469 EPOCH 995
2024-02-03 07:23:53,641 [Epoch: 995 Step: 00066600] Batch Recognition Loss:   0.000166 => Gls Tokens per Sec:     1871 || Batch Translation Loss:   0.033051 => Txt Tokens per Sec:     5942 || Lr: 0.000050
2024-02-03 07:23:58,310 Epoch 995: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.67 
2024-02-03 07:23:58,310 EPOCH 996
2024-02-03 07:24:01,301 [Epoch: 996 Step: 00066700] Batch Recognition Loss:   0.000342 => Gls Tokens per Sec:     1843 || Batch Translation Loss:   0.028998 => Txt Tokens per Sec:     5070 || Lr: 0.000050
2024-02-03 07:24:03,814 Epoch 996: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.22 
2024-02-03 07:24:03,815 EPOCH 997
2024-02-03 07:24:08,942 Epoch 997: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.42 
2024-02-03 07:24:08,942 EPOCH 998
2024-02-03 07:24:09,028 [Epoch: 998 Step: 00066800] Batch Recognition Loss:   0.000135 => Gls Tokens per Sec:     1882 || Batch Translation Loss:   0.044653 => Txt Tokens per Sec:     5706 || Lr: 0.000050
2024-02-03 07:24:14,095 Epoch 998: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.24 
2024-02-03 07:24:14,095 EPOCH 999
2024-02-03 07:24:16,611 [Epoch: 999 Step: 00066900] Batch Recognition Loss:   0.000331 => Gls Tokens per Sec:     2128 || Batch Translation Loss:   0.026995 => Txt Tokens per Sec:     5966 || Lr: 0.000050
2024-02-03 07:24:19,346 Epoch 999: Total Training Recognition Loss 0.04  Total Training Translation Loss 4.20 
2024-02-03 07:24:19,347 EPOCH 1000
2024-02-03 07:24:24,382 [Epoch: 1000 Step: 00067000] Batch Recognition Loss:   0.000665 => Gls Tokens per Sec:     2111 || Batch Translation Loss:   0.030346 => Txt Tokens per Sec:     5872 || Lr: 0.000050
2024-02-03 07:24:24,383 Epoch 1000: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.78 
2024-02-03 07:24:24,383 EPOCH 1001
2024-02-03 07:24:29,873 Epoch 1001: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.99 
2024-02-03 07:24:29,873 EPOCH 1002
2024-02-03 07:24:32,141 [Epoch: 1002 Step: 00067100] Batch Recognition Loss:   0.000291 => Gls Tokens per Sec:     2329 || Batch Translation Loss:   0.021257 => Txt Tokens per Sec:     6461 || Lr: 0.000050
2024-02-03 07:24:34,971 Epoch 1002: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.77 
2024-02-03 07:24:34,971 EPOCH 1003
2024-02-03 07:24:40,128 [Epoch: 1003 Step: 00067200] Batch Recognition Loss:   0.000369 => Gls Tokens per Sec:     2031 || Batch Translation Loss:   0.021684 => Txt Tokens per Sec:     5660 || Lr: 0.000050
2024-02-03 07:24:40,180 Epoch 1003: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.13 
2024-02-03 07:24:40,180 EPOCH 1004
2024-02-03 07:24:45,476 Epoch 1004: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.31 
2024-02-03 07:24:45,477 EPOCH 1005
2024-02-03 07:24:47,882 [Epoch: 1005 Step: 00067300] Batch Recognition Loss:   0.000324 => Gls Tokens per Sec:     2092 || Batch Translation Loss:   0.016364 => Txt Tokens per Sec:     5885 || Lr: 0.000050
2024-02-03 07:24:50,541 Epoch 1005: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.55 
2024-02-03 07:24:50,541 EPOCH 1006
2024-02-03 07:24:55,972 [Epoch: 1006 Step: 00067400] Batch Recognition Loss:   0.000658 => Gls Tokens per Sec:     1898 || Batch Translation Loss:   0.016609 => Txt Tokens per Sec:     5281 || Lr: 0.000050
2024-02-03 07:24:56,103 Epoch 1006: Total Training Recognition Loss 0.09  Total Training Translation Loss 1.43 
2024-02-03 07:24:56,103 EPOCH 1007
2024-02-03 07:25:01,238 Epoch 1007: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.87 
2024-02-03 07:25:01,238 EPOCH 1008
2024-02-03 07:25:03,524 [Epoch: 1008 Step: 00067500] Batch Recognition Loss:   0.000654 => Gls Tokens per Sec:     2131 || Batch Translation Loss:   0.125428 => Txt Tokens per Sec:     5959 || Lr: 0.000050
2024-02-03 07:25:06,289 Epoch 1008: Total Training Recognition Loss 0.09  Total Training Translation Loss 3.47 
2024-02-03 07:25:06,289 EPOCH 1009
2024-02-03 07:25:11,498 [Epoch: 1009 Step: 00067600] Batch Recognition Loss:   0.001249 => Gls Tokens per Sec:     1949 || Batch Translation Loss:   0.017796 => Txt Tokens per Sec:     5409 || Lr: 0.000050
2024-02-03 07:25:11,706 Epoch 1009: Total Training Recognition Loss 0.11  Total Training Translation Loss 3.90 
2024-02-03 07:25:11,706 EPOCH 1010
2024-02-03 07:25:16,803 Epoch 1010: Total Training Recognition Loss 0.11  Total Training Translation Loss 3.35 
2024-02-03 07:25:16,804 EPOCH 1011
2024-02-03 07:25:19,181 [Epoch: 1011 Step: 00067700] Batch Recognition Loss:   0.001829 => Gls Tokens per Sec:     1983 || Batch Translation Loss:   0.060508 => Txt Tokens per Sec:     5234 || Lr: 0.000050
2024-02-03 07:25:22,352 Epoch 1011: Total Training Recognition Loss 0.21  Total Training Translation Loss 3.52 
2024-02-03 07:25:22,353 EPOCH 1012
2024-02-03 07:25:27,066 [Epoch: 1012 Step: 00067800] Batch Recognition Loss:   0.002084 => Gls Tokens per Sec:     2120 || Batch Translation Loss:   0.142893 => Txt Tokens per Sec:     5856 || Lr: 0.000050
2024-02-03 07:25:27,430 Epoch 1012: Total Training Recognition Loss 0.15  Total Training Translation Loss 5.53 
2024-02-03 07:25:27,430 EPOCH 1013
2024-02-03 07:25:32,641 Epoch 1013: Total Training Recognition Loss 0.08  Total Training Translation Loss 3.44 
2024-02-03 07:25:32,642 EPOCH 1014
2024-02-03 07:25:34,548 [Epoch: 1014 Step: 00067900] Batch Recognition Loss:   0.000469 => Gls Tokens per Sec:     2435 || Batch Translation Loss:   0.095419 => Txt Tokens per Sec:     7100 || Lr: 0.000050
2024-02-03 07:25:37,486 Epoch 1014: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.13 
2024-02-03 07:25:37,487 EPOCH 1015
2024-02-03 07:25:42,485 [Epoch: 1015 Step: 00068000] Batch Recognition Loss:   0.000276 => Gls Tokens per Sec:     1967 || Batch Translation Loss:   0.014477 => Txt Tokens per Sec:     5495 || Lr: 0.000050
2024-02-03 07:25:51,107 Validation result at epoch 1015, step    68000: duration: 8.6218s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 4.03844	Translation Loss: 87070.68750	PPL: 8463.23340
	Eval Metric: BLEU
	WER 2.97	(DEL: 0.00,	INS: 0.00,	SUB: 2.97)
	BLEU-4 0.56	(BLEU-1: 10.13,	BLEU-2: 2.74,	BLEU-3: 1.09,	BLEU-4: 0.56)
	CHRF 16.78	ROUGE 8.17
2024-02-03 07:25:51,108 Logging Recognition and Translation Outputs
2024-02-03 07:25:51,108 ========================================================================================================================
2024-02-03 07:25:51,108 Logging Sequence: 180_546.00
2024-02-03 07:25:51,108 	Gloss Reference :	A B+C+D+E
2024-02-03 07:25:51,109 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 07:25:51,109 	Gloss Alignment :	         
2024-02-03 07:25:51,109 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 07:25:51,110 	Text Reference  :	******* they  are also demanding that the report of the   oc   be made      public  
2024-02-03 07:25:51,110 	Text Hypothesis :	ranveer singh was also present   at   the ****** ** match here is something shocking
2024-02-03 07:25:51,110 	Text Alignment  :	I       S     S        S         S        D      D  S     S    S  S         S       
2024-02-03 07:25:51,110 ========================================================================================================================
2024-02-03 07:25:51,110 Logging Sequence: 142_148.00
2024-02-03 07:25:51,111 	Gloss Reference :	A B+C+D+E
2024-02-03 07:25:51,111 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 07:25:51,111 	Gloss Alignment :	         
2024-02-03 07:25:51,111 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 07:25:51,112 	Text Reference  :	now on the 10th november 2022 india is set   to  play england in the semi-final
2024-02-03 07:25:51,112 	Text Hypothesis :	*** ** *** **** ******** **** ***** a  young boy was  fined   rs 65  lakh      
2024-02-03 07:25:51,112 	Text Alignment  :	D   D  D   D    D        D    D     S  S     S   S    S       S  S   S         
2024-02-03 07:25:51,112 ========================================================================================================================
2024-02-03 07:25:51,112 Logging Sequence: 118_46.00
2024-02-03 07:25:51,112 	Gloss Reference :	A B+C+D+E
2024-02-03 07:25:51,113 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 07:25:51,113 	Gloss Alignment :	         
2024-02-03 07:25:51,113 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 07:25:51,115 	Text Reference  :	since there was a         tie the ******* match went into the penalty shootout where each  team gets   a chance to score 5          times 
2024-02-03 07:25:51,115 	Text Hypothesis :	***** ***** *** yesterday on  the support staff have won  the ******* match    in    qatar to   afford a ****** ** ***** time-bound manner
2024-02-03 07:25:51,115 	Text Alignment  :	D     D     D   S         S       I       S     S    S        D       S        S     S     S    S        D      D  D     S          S     
2024-02-03 07:25:51,115 ========================================================================================================================
2024-02-03 07:25:51,115 Logging Sequence: 180_345.00
2024-02-03 07:25:51,116 	Gloss Reference :	A B+C+D+E
2024-02-03 07:25:51,116 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 07:25:51,116 	Gloss Alignment :	         
2024-02-03 07:25:51,116 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 07:25:51,116 	Text Reference  :	i ** **** ******** haven't done  anything
2024-02-03 07:25:51,116 	Text Hypothesis :	i am very thankful to      their support 
2024-02-03 07:25:51,117 	Text Alignment  :	  I  I    I        S       S     S       
2024-02-03 07:25:51,117 ========================================================================================================================
2024-02-03 07:25:51,117 Logging Sequence: 108_138.00
2024-02-03 07:25:51,117 	Gloss Reference :	A B+C+D+E
2024-02-03 07:25:51,117 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 07:25:51,117 	Gloss Alignment :	         
2024-02-03 07:25:51,117 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 07:25:51,118 	Text Reference  :	**** *** khan is an uncapped  player   who   has      only played domestic cricket    
2024-02-03 07:25:51,118 	Text Hypothesis :	asia cup will be a  cricketer mohammed shami fielding with a      severe   competition
2024-02-03 07:25:51,119 	Text Alignment  :	I    I   S    S  S  S         S        S     S        S    S      S        S          
2024-02-03 07:25:51,119 ========================================================================================================================
2024-02-03 07:25:51,122 Training ended since there were no improvements inthe last learning rate step: 0.000050
2024-02-03 07:25:51,123 Best validation result at step    16000:   0.95 eval_metric.
2024-02-03 07:26:15,751 ------------------------------------------------------------
2024-02-03 07:26:15,751 [DEV] partition [RECOGNITION] experiment [BW]: 1
2024-02-03 07:26:24,399 finished in 8.6480s 
2024-02-03 07:26:24,400 ************************************************************
2024-02-03 07:26:24,400 [DEV] partition [RECOGNITION] results:
	New Best CTC Decode Beam Size: 1
	WER 3.96	(DEL: 0.00,	INS: 0.00,	SUB: 3.96)
2024-02-03 07:26:24,400 ************************************************************
2024-02-03 07:26:24,400 ------------------------------------------------------------
2024-02-03 07:26:24,401 [DEV] partition [RECOGNITION] experiment [BW]: 2
2024-02-03 07:26:32,676 finished in 8.2750s 
2024-02-03 07:26:32,676 ------------------------------------------------------------
2024-02-03 07:26:32,677 [DEV] partition [RECOGNITION] experiment [BW]: 3
2024-02-03 07:26:41,110 finished in 8.4330s 
2024-02-03 07:26:41,110 ------------------------------------------------------------
2024-02-03 07:26:41,110 [DEV] partition [RECOGNITION] experiment [BW]: 4
2024-02-03 07:26:49,403 finished in 8.2930s 
2024-02-03 07:26:49,404 ------------------------------------------------------------
2024-02-03 07:26:49,404 [DEV] partition [RECOGNITION] experiment [BW]: 5
2024-02-03 07:26:57,750 finished in 8.3460s 
2024-02-03 07:26:57,751 ------------------------------------------------------------
2024-02-03 07:26:57,751 [DEV] partition [RECOGNITION] experiment [BW]: 6
2024-02-03 07:27:06,126 finished in 8.3750s 
2024-02-03 07:27:06,127 ------------------------------------------------------------
2024-02-03 07:27:06,127 [DEV] partition [RECOGNITION] experiment [BW]: 7
2024-02-03 07:27:14,321 finished in 8.1941s 
2024-02-03 07:27:14,321 ------------------------------------------------------------
2024-02-03 07:27:14,321 [DEV] partition [RECOGNITION] experiment [BW]: 8
2024-02-03 07:27:22,535 finished in 8.2133s 
2024-02-03 07:27:22,535 ------------------------------------------------------------
2024-02-03 07:27:22,536 [DEV] partition [RECOGNITION] experiment [BW]: 9
2024-02-03 07:27:30,978 finished in 8.4420s 
2024-02-03 07:27:30,979 ------------------------------------------------------------
2024-02-03 07:27:30,979 [DEV] partition [RECOGNITION] experiment [BW]: 10
2024-02-03 07:27:39,444 finished in 8.4651s 
2024-02-03 07:27:39,444 ============================================================
2024-02-03 07:27:47,636 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 1 and Alpha: -1
	BLEU-4 0.95	(BLEU-1: 10.66,	BLEU-2: 3.46,	BLEU-3: 1.65,	BLEU-4: 0.95)
	CHRF 17.21	ROUGE 8.63
2024-02-03 07:27:47,637 ------------------------------------------------------------
2024-02-03 07:28:47,167 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 2 and Alpha: -1
	BLEU-4 1.00	(BLEU-1: 9.93,	BLEU-2: 3.29,	BLEU-3: 1.64,	BLEU-4: 1.00)
	CHRF 16.81	ROUGE 8.45
2024-02-03 07:28:47,168 ------------------------------------------------------------
2024-02-03 07:29:07,598 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 2 and Alpha: 1
	BLEU-4 1.03	(BLEU-1: 10.21,	BLEU-2: 3.39,	BLEU-3: 1.70,	BLEU-4: 1.03)
	CHRF 16.89	ROUGE 8.53
2024-02-03 07:29:07,599 ------------------------------------------------------------
2024-02-03 07:29:17,703 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 2 and Alpha: 2
	BLEU-4 1.09	(BLEU-1: 10.38,	BLEU-2: 3.47,	BLEU-3: 1.78,	BLEU-4: 1.09)
	CHRF 16.96	ROUGE 8.51
2024-02-03 07:29:17,704 ------------------------------------------------------------
2024-02-03 07:44:13,826 ************************************************************
2024-02-03 07:44:13,827 [DEV] partition [Recognition & Translation] results:
	Best CTC Decode Beam Size: 1
	Best Translation Beam Size: 2 and Alpha: 2
	WER 3.96	(DEL: 0.00,	INS: 0.00,	SUB: 3.96)
	BLEU-4 1.09	(BLEU-1: 10.38,	BLEU-2: 3.47,	BLEU-3: 1.78,	BLEU-4: 1.09)
	CHRF 16.96	ROUGE 8.51
2024-02-03 07:44:13,827 ************************************************************
2024-02-03 07:44:25,891 [TEST] partition [Recognition & Translation] results:
	Best CTC Decode Beam Size: 1
	Best Translation Beam Size: 2 and Alpha: 2
	WER 4.52	(DEL: 0.14,	INS: 0.00,	SUB: 4.38)
	BLEU-4 0.65	(BLEU-1: 9.47,	BLEU-2: 2.99,	BLEU-3: 1.23,	BLEU-4: 0.65)
	CHRF 16.94	ROUGE 7.93
2024-02-03 07:44:25,891 ************************************************************
