2024-02-05 19:16:01,710 Hello! This is Joey-NMT.
2024-02-05 19:16:01,719 Total params: 25642504
2024-02-05 19:16:01,720 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'gloss_output_layer.bias', 'gloss_output_layer.weight', 'sgn_embed.ln.bias', 'sgn_embed.ln.weight', 'sgn_embed.norm.norm.bias', 'sgn_embed.norm.norm.weight', 'txt_embed.norm.norm.bias', 'txt_embed.norm.norm.weight']
2024-02-05 19:16:02,868 cfg.name                           : sign_experiment
2024-02-05 19:16:02,868 cfg.data.data_path                 : ./data/Sports_dataset/8/
2024-02-05 19:16:02,868 cfg.data.version                   : phoenix_2014_trans
2024-02-05 19:16:02,868 cfg.data.sgn                       : sign
2024-02-05 19:16:02,869 cfg.data.txt                       : text
2024-02-05 19:16:02,869 cfg.data.gls                       : gloss
2024-02-05 19:16:02,869 cfg.data.train                     : excel_data.train
2024-02-05 19:16:02,869 cfg.data.dev                       : excel_data.dev
2024-02-05 19:16:02,869 cfg.data.test                      : excel_data.test
2024-02-05 19:16:02,869 cfg.data.feature_size              : 2560
2024-02-05 19:16:02,869 cfg.data.level                     : word
2024-02-05 19:16:02,870 cfg.data.txt_lowercase             : True
2024-02-05 19:16:02,870 cfg.data.max_sent_length           : 500
2024-02-05 19:16:02,870 cfg.data.random_train_subset       : -1
2024-02-05 19:16:02,870 cfg.data.random_dev_subset         : -1
2024-02-05 19:16:02,870 cfg.testing.recognition_beam_sizes : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
2024-02-05 19:16:02,870 cfg.testing.translation_beam_sizes : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
2024-02-05 19:16:02,870 cfg.testing.translation_beam_alphas : [-1, 0, 1, 2, 3, 4, 5]
2024-02-05 19:16:02,870 cfg.training.reset_best_ckpt       : False
2024-02-05 19:16:02,870 cfg.training.reset_scheduler       : False
2024-02-05 19:16:02,871 cfg.training.reset_optimizer       : False
2024-02-05 19:16:02,871 cfg.training.random_seed           : 42
2024-02-05 19:16:02,871 cfg.training.model_dir             : ./sign_sample_model/fold8/32head/16batch
2024-02-05 19:16:02,871 cfg.training.recognition_loss_weight : 1.0
2024-02-05 19:16:02,871 cfg.training.translation_loss_weight : 1.0
2024-02-05 19:16:02,871 cfg.training.eval_metric           : bleu
2024-02-05 19:16:02,871 cfg.training.optimizer             : adam
2024-02-05 19:16:02,872 cfg.training.learning_rate         : 0.0001
2024-02-05 19:16:02,872 cfg.training.batch_size            : 16
2024-02-05 19:16:02,872 cfg.training.num_valid_log         : 5
2024-02-05 19:16:02,872 cfg.training.epochs                : 50000
2024-02-05 19:16:02,872 cfg.training.early_stopping_metric : eval_metric
2024-02-05 19:16:02,872 cfg.training.batch_type            : sentence
2024-02-05 19:16:02,872 cfg.training.translation_normalization : batch
2024-02-05 19:16:02,872 cfg.training.eval_recognition_beam_size : 1
2024-02-05 19:16:02,872 cfg.training.eval_translation_beam_size : 1
2024-02-05 19:16:02,873 cfg.training.eval_translation_beam_alpha : -1
2024-02-05 19:16:02,873 cfg.training.overwrite             : True
2024-02-05 19:16:02,873 cfg.training.shuffle               : True
2024-02-05 19:16:02,873 cfg.training.use_cuda              : True
2024-02-05 19:16:02,873 cfg.training.translation_max_output_length : 40
2024-02-05 19:16:02,873 cfg.training.keep_last_ckpts       : 1
2024-02-05 19:16:02,873 cfg.training.batch_multiplier      : 1
2024-02-05 19:16:02,873 cfg.training.logging_freq          : 100
2024-02-05 19:16:02,874 cfg.training.validation_freq       : 2000
2024-02-05 19:16:02,874 cfg.training.betas                 : [0.9, 0.998]
2024-02-05 19:16:02,874 cfg.training.scheduling            : plateau
2024-02-05 19:16:02,874 cfg.training.learning_rate_min     : 1e-08
2024-02-05 19:16:02,874 cfg.training.weight_decay          : 0.0001
2024-02-05 19:16:02,874 cfg.training.patience              : 12
2024-02-05 19:16:02,874 cfg.training.decrease_factor       : 0.5
2024-02-05 19:16:02,874 cfg.training.label_smoothing       : 0.0
2024-02-05 19:16:02,875 cfg.model.initializer              : xavier
2024-02-05 19:16:02,875 cfg.model.bias_initializer         : zeros
2024-02-05 19:16:02,875 cfg.model.init_gain                : 1.0
2024-02-05 19:16:02,875 cfg.model.embed_initializer        : xavier
2024-02-05 19:16:02,875 cfg.model.embed_init_gain          : 1.0
2024-02-05 19:16:02,875 cfg.model.tied_softmax             : True
2024-02-05 19:16:02,875 cfg.model.encoder.type             : transformer
2024-02-05 19:16:02,876 cfg.model.encoder.num_layers       : 3
2024-02-05 19:16:02,876 cfg.model.encoder.num_heads        : 32
2024-02-05 19:16:02,876 cfg.model.encoder.embeddings.embedding_dim : 512
2024-02-05 19:16:02,876 cfg.model.encoder.embeddings.scale : False
2024-02-05 19:16:02,876 cfg.model.encoder.embeddings.dropout : 0.1
2024-02-05 19:16:02,876 cfg.model.encoder.embeddings.norm_type : batch
2024-02-05 19:16:02,876 cfg.model.encoder.embeddings.activation_type : softsign
2024-02-05 19:16:02,876 cfg.model.encoder.hidden_size      : 512
2024-02-05 19:16:02,876 cfg.model.encoder.ff_size          : 2048
2024-02-05 19:16:02,877 cfg.model.encoder.dropout          : 0.1
2024-02-05 19:16:02,877 cfg.model.decoder.type             : transformer
2024-02-05 19:16:02,877 cfg.model.decoder.num_layers       : 3
2024-02-05 19:16:02,877 cfg.model.decoder.num_heads        : 32
2024-02-05 19:16:02,877 cfg.model.decoder.embeddings.embedding_dim : 512
2024-02-05 19:16:02,877 cfg.model.decoder.embeddings.scale : False
2024-02-05 19:16:02,877 cfg.model.decoder.embeddings.dropout : 0.1
2024-02-05 19:16:02,877 cfg.model.decoder.embeddings.norm_type : batch
2024-02-05 19:16:02,878 cfg.model.decoder.embeddings.activation_type : softsign
2024-02-05 19:16:02,878 cfg.model.decoder.hidden_size      : 512
2024-02-05 19:16:02,878 cfg.model.decoder.ff_size          : 2048
2024-02-05 19:16:02,878 cfg.model.decoder.dropout          : 0.1
2024-02-05 19:16:02,878 Data set sizes: 
	train 2124,
	valid 708,
	test 708
2024-02-05 19:16:02,878 First training example:
	[GLS] A B C D E
	[TXT] how did she become a champion
2024-02-05 19:16:02,878 First 10 words (gls): (0) <si> (1) <unk> (2) <pad> (3) A (4) B (5) C (6) D (7) E
2024-02-05 19:16:02,878 First 10 words (txt): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) the (5) and (6) to (7) in (8) a (9) of
2024-02-05 19:16:02,879 Number of unique glosses (types): 8
2024-02-05 19:16:02,879 Number of unique words (types): 4402
2024-02-05 19:16:02,879 SignModel(
	encoder=TransformerEncoder(num_layers=3, num_heads=32),
	decoder=TransformerDecoder(num_layers=3, num_heads=32),
	sgn_embed=SpatialEmbeddings(embedding_dim=512, input_size=2560),
	txt_embed=Embeddings(embedding_dim=512, vocab_size=4402))
2024-02-05 19:16:02,882 EPOCH 1
2024-02-05 19:16:08,845 [Epoch: 001 Step: 00000100] Batch Recognition Loss:   0.905666 => Gls Tokens per Sec:     1342 || Batch Translation Loss: 100.148598 => Txt Tokens per Sec:     3758 || Lr: 0.000100
2024-02-05 19:16:10,754 Epoch   1: Total Training Recognition Loss 265.68  Total Training Translation Loss 12490.13 
2024-02-05 19:16:10,754 EPOCH 2
2024-02-05 19:16:14,385 [Epoch: 002 Step: 00000200] Batch Recognition Loss:   0.655709 => Gls Tokens per Sec:     1477 || Batch Translation Loss:  80.330894 => Txt Tokens per Sec:     4112 || Lr: 0.000100
2024-02-05 19:16:18,094 Epoch   2: Total Training Recognition Loss 97.03  Total Training Translation Loss 11630.19 
2024-02-05 19:16:18,095 EPOCH 3
2024-02-05 19:16:20,079 [Epoch: 003 Step: 00000300] Batch Recognition Loss:   0.454853 => Gls Tokens per Sec:     1372 || Batch Translation Loss:  64.913422 => Txt Tokens per Sec:     3796 || Lr: 0.000100
2024-02-05 19:16:25,956 Epoch   3: Total Training Recognition Loss 68.13  Total Training Translation Loss 11151.14 
2024-02-05 19:16:25,957 EPOCH 4
2024-02-05 19:16:26,018 [Epoch: 004 Step: 00000400] Batch Recognition Loss:   0.430961 => Gls Tokens per Sec:     1333 || Batch Translation Loss:  87.613167 => Txt Tokens per Sec:     3917 || Lr: 0.000100
2024-02-05 19:16:31,552 [Epoch: 004 Step: 00000500] Batch Recognition Loss:   0.174160 => Gls Tokens per Sec:     1446 || Batch Translation Loss:  82.482925 => Txt Tokens per Sec:     3995 || Lr: 0.000100
2024-02-05 19:16:33,304 Epoch   4: Total Training Recognition Loss 42.72  Total Training Translation Loss 10612.26 
2024-02-05 19:16:33,304 EPOCH 5
2024-02-05 19:16:36,765 [Epoch: 005 Step: 00000600] Batch Recognition Loss:   0.130911 => Gls Tokens per Sec:     1572 || Batch Translation Loss:  77.553612 => Txt Tokens per Sec:     4425 || Lr: 0.000100
2024-02-05 19:16:40,470 Epoch   5: Total Training Recognition Loss 37.73  Total Training Translation Loss 10079.92 
2024-02-05 19:16:40,471 EPOCH 6
2024-02-05 19:16:42,527 [Epoch: 006 Step: 00000700] Batch Recognition Loss:   0.062346 => Gls Tokens per Sec:     1364 || Batch Translation Loss:  87.654976 => Txt Tokens per Sec:     3681 || Lr: 0.000100
2024-02-05 19:16:48,045 Epoch   6: Total Training Recognition Loss 19.01  Total Training Translation Loss 9596.05 
2024-02-05 19:16:48,045 EPOCH 7
2024-02-05 19:16:48,145 [Epoch: 007 Step: 00000800] Batch Recognition Loss:   0.017362 => Gls Tokens per Sec:     1616 || Batch Translation Loss:  59.736057 => Txt Tokens per Sec:     4081 || Lr: 0.000100
2024-02-05 19:16:53,811 [Epoch: 007 Step: 00000900] Batch Recognition Loss:   0.117663 => Gls Tokens per Sec:     1412 || Batch Translation Loss:  65.282082 => Txt Tokens per Sec:     3879 || Lr: 0.000100
2024-02-05 19:16:55,670 Epoch   7: Total Training Recognition Loss 15.30  Total Training Translation Loss 9130.31 
2024-02-05 19:16:55,671 EPOCH 8
2024-02-05 19:16:59,286 [Epoch: 008 Step: 00001000] Batch Recognition Loss:   0.046793 => Gls Tokens per Sec:     1528 || Batch Translation Loss:  50.554142 => Txt Tokens per Sec:     4201 || Lr: 0.000100
2024-02-05 19:17:02,963 Epoch   8: Total Training Recognition Loss 13.02  Total Training Translation Loss 8671.56 
2024-02-05 19:17:02,963 EPOCH 9
2024-02-05 19:17:04,833 [Epoch: 009 Step: 00001100] Batch Recognition Loss:   0.287708 => Gls Tokens per Sec:     1540 || Batch Translation Loss:  82.607155 => Txt Tokens per Sec:     4143 || Lr: 0.000100
2024-02-05 19:17:10,749 Epoch   9: Total Training Recognition Loss 10.53  Total Training Translation Loss 8240.51 
2024-02-05 19:17:10,749 EPOCH 10
2024-02-05 19:17:10,887 [Epoch: 010 Step: 00001200] Batch Recognition Loss:   0.075005 => Gls Tokens per Sec:     1752 || Batch Translation Loss:  43.725620 => Txt Tokens per Sec:     4365 || Lr: 0.000100
2024-02-05 19:17:16,247 [Epoch: 010 Step: 00001300] Batch Recognition Loss:   0.045043 => Gls Tokens per Sec:     1492 || Batch Translation Loss:  62.724968 => Txt Tokens per Sec:     4084 || Lr: 0.000100
2024-02-05 19:17:17,809 Epoch  10: Total Training Recognition Loss 9.78  Total Training Translation Loss 7819.53 
2024-02-05 19:17:17,809 EPOCH 11
2024-02-05 19:17:21,818 [Epoch: 011 Step: 00001400] Batch Recognition Loss:   0.022493 => Gls Tokens per Sec:     1397 || Batch Translation Loss:  55.771404 => Txt Tokens per Sec:     3844 || Lr: 0.000100
2024-02-05 19:17:25,423 Epoch  11: Total Training Recognition Loss 9.50  Total Training Translation Loss 7384.04 
2024-02-05 19:17:25,423 EPOCH 12
2024-02-05 19:17:27,296 [Epoch: 012 Step: 00001500] Batch Recognition Loss:   0.067870 => Gls Tokens per Sec:     1581 || Batch Translation Loss:  50.721043 => Txt Tokens per Sec:     4394 || Lr: 0.000100
2024-02-05 19:17:32,996 Epoch  12: Total Training Recognition Loss 8.95  Total Training Translation Loss 7000.50 
2024-02-05 19:17:32,997 EPOCH 13
2024-02-05 19:17:33,259 [Epoch: 013 Step: 00001600] Batch Recognition Loss:   0.032241 => Gls Tokens per Sec:     1228 || Batch Translation Loss:  62.026375 => Txt Tokens per Sec:     3941 || Lr: 0.000100
2024-02-05 19:17:38,904 [Epoch: 013 Step: 00001700] Batch Recognition Loss:   0.023936 => Gls Tokens per Sec:     1417 || Batch Translation Loss:  43.995655 => Txt Tokens per Sec:     3878 || Lr: 0.000100
2024-02-05 19:17:40,481 Epoch  13: Total Training Recognition Loss 9.04  Total Training Translation Loss 6576.35 
2024-02-05 19:17:40,481 EPOCH 14
2024-02-05 19:17:44,024 [Epoch: 014 Step: 00001800] Batch Recognition Loss:   0.041191 => Gls Tokens per Sec:     1604 || Batch Translation Loss:  32.518673 => Txt Tokens per Sec:     4416 || Lr: 0.000100
2024-02-05 19:17:47,736 Epoch  14: Total Training Recognition Loss 7.55  Total Training Translation Loss 6162.87 
2024-02-05 19:17:47,737 EPOCH 15
2024-02-05 19:17:49,934 [Epoch: 015 Step: 00001900] Batch Recognition Loss:   0.039102 => Gls Tokens per Sec:     1384 || Batch Translation Loss:  27.397106 => Txt Tokens per Sec:     3911 || Lr: 0.000100
2024-02-05 19:17:55,331 Epoch  15: Total Training Recognition Loss 8.16  Total Training Translation Loss 5781.66 
2024-02-05 19:17:55,331 EPOCH 16
2024-02-05 19:17:55,680 [Epoch: 016 Step: 00002000] Batch Recognition Loss:   0.033931 => Gls Tokens per Sec:     1153 || Batch Translation Loss:  39.151669 => Txt Tokens per Sec:     3150 || Lr: 0.000100
2024-02-05 19:18:05,907 Hooray! New best validation result [eval_metric]!
2024-02-05 19:18:05,908 Saving new checkpoint.
2024-02-05 19:18:06,166 Validation result at epoch  16, step     2000: duration: 10.4860s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 5.60418	Translation Loss: 59462.84766	PPL: 379.61496
	Eval Metric: BLEU
	WER 6.85	(DEL: 0.00,	INS: 0.00,	SUB: 6.85)
	BLEU-4 0.56	(BLEU-1: 11.09,	BLEU-2: 3.85,	BLEU-3: 1.32,	BLEU-4: 0.56)
	CHRF 13.88	ROUGE 10.18
2024-02-05 19:18:06,167 Logging Recognition and Translation Outputs
2024-02-05 19:18:06,167 ========================================================================================================================
2024-02-05 19:18:06,167 Logging Sequence: 165_414.00
2024-02-05 19:18:06,167 	Gloss Reference :	A B+C+D+E
2024-02-05 19:18:06,168 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:18:06,168 	Gloss Alignment :	         
2024-02-05 19:18:06,168 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:18:06,169 	Text Reference  :	he felt sachin was lucky so   he   always  gave his sweater to give it to the     umpire 
2024-02-05 19:18:06,169 	Text Hypothesis :	he **** ****** *** has   been been brought for  his ******* ** **** ** ** amazing amazing
2024-02-05 19:18:06,169 	Text Alignment  :	   D    D      D   S     S    S    S       S        D       D  D    D  D  S       S      
2024-02-05 19:18:06,169 ========================================================================================================================
2024-02-05 19:18:06,170 Logging Sequence: 169_268.00
2024-02-05 19:18:06,170 	Gloss Reference :	A B+C+D+E
2024-02-05 19:18:06,170 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:18:06,170 	Gloss Alignment :	         
2024-02-05 19:18:06,170 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:18:06,171 	Text Reference  :	shami supports arshdeep and many  fans supported him     as     well 
2024-02-05 19:18:06,171 	Text Hypothesis :	***** the      bcci     is  known as   the       british empire games
2024-02-05 19:18:06,171 	Text Alignment  :	D     S        S        S   S     S    S         S       S      S    
2024-02-05 19:18:06,171 ========================================================================================================================
2024-02-05 19:18:06,172 Logging Sequence: 172_15.00
2024-02-05 19:18:06,172 	Gloss Reference :	A B+C+D+E
2024-02-05 19:18:06,172 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:18:06,172 	Gloss Alignment :	         
2024-02-05 19:18:06,172 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:18:06,174 	Text Reference  :	now in the final match on    28   may 2023 the two teams were up against each other at   the same  venue  
2024-02-05 19:18:06,174 	Text Hypothesis :	*** ** *** ***** ***** after this is  why  the *** ***** **** ** ******* **** most  most of  diego matches
2024-02-05 19:18:06,174 	Text Alignment  :	D   D  D   D     D     S     S    S   S        D   D     D    D  D       D    S     S    S   S     S      
2024-02-05 19:18:06,174 ========================================================================================================================
2024-02-05 19:18:06,174 Logging Sequence: 96_158.00
2024-02-05 19:18:06,175 	Gloss Reference :	A B+C+D+E
2024-02-05 19:18:06,175 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:18:06,175 	Gloss Alignment :	         
2024-02-05 19:18:06,175 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:18:06,176 	Text Reference  :	*** after  this pandya fell on   his knees in  disappointment
2024-02-05 19:18:06,176 	Text Hypothesis :	the couple are  very   very well for their fan following     
2024-02-05 19:18:06,176 	Text Alignment  :	I   S      S    S      S    S    S   S     S   S             
2024-02-05 19:18:06,176 ========================================================================================================================
2024-02-05 19:18:06,176 Logging Sequence: 152_73.00
2024-02-05 19:18:06,177 	Gloss Reference :	A B+C+D+E
2024-02-05 19:18:06,177 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:18:06,177 	Gloss Alignment :	         
2024-02-05 19:18:06,177 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:18:06,178 	Text Reference  :	**** **** ** **** ** eventually he  too      got  out by   shaheen afridi
2024-02-05 19:18:06,178 	Text Hypothesis :	they will be held in mumbai     and pakistan will be  held in      mumbai
2024-02-05 19:18:06,178 	Text Alignment  :	I    I    I  I    I  S          S   S        S    S   S    S       S     
2024-02-05 19:18:06,178 ========================================================================================================================
2024-02-05 19:18:11,788 [Epoch: 016 Step: 00002100] Batch Recognition Loss:   0.050895 => Gls Tokens per Sec:     1423 || Batch Translation Loss:  45.457775 => Txt Tokens per Sec:     3916 || Lr: 0.000100
2024-02-05 19:18:13,403 Epoch  16: Total Training Recognition Loss 7.65  Total Training Translation Loss 5398.06 
2024-02-05 19:18:13,403 EPOCH 17
2024-02-05 19:18:17,352 [Epoch: 017 Step: 00002200] Batch Recognition Loss:   0.014597 => Gls Tokens per Sec:     1459 || Batch Translation Loss:  31.337465 => Txt Tokens per Sec:     4032 || Lr: 0.000100
2024-02-05 19:18:20,950 Epoch  17: Total Training Recognition Loss 7.60  Total Training Translation Loss 5027.95 
2024-02-05 19:18:20,950 EPOCH 18
2024-02-05 19:18:23,305 [Epoch: 018 Step: 00002300] Batch Recognition Loss:   0.051939 => Gls Tokens per Sec:     1326 || Batch Translation Loss:  29.074175 => Txt Tokens per Sec:     3677 || Lr: 0.000100
2024-02-05 19:18:28,602 Epoch  18: Total Training Recognition Loss 7.88  Total Training Translation Loss 4673.04 
2024-02-05 19:18:28,603 EPOCH 19
2024-02-05 19:18:28,860 [Epoch: 019 Step: 00002400] Batch Recognition Loss:   0.037067 => Gls Tokens per Sec:     1882 || Batch Translation Loss:  39.396980 => Txt Tokens per Sec:     5043 || Lr: 0.000100
2024-02-05 19:18:34,207 [Epoch: 019 Step: 00002500] Batch Recognition Loss:   0.025024 => Gls Tokens per Sec:     1496 || Batch Translation Loss:  37.497128 => Txt Tokens per Sec:     4149 || Lr: 0.000100
2024-02-05 19:18:35,770 Epoch  19: Total Training Recognition Loss 7.37  Total Training Translation Loss 4277.78 
2024-02-05 19:18:35,770 EPOCH 20
2024-02-05 19:18:39,916 [Epoch: 020 Step: 00002600] Batch Recognition Loss:   0.087927 => Gls Tokens per Sec:     1409 || Batch Translation Loss:  37.032120 => Txt Tokens per Sec:     3862 || Lr: 0.000100
2024-02-05 19:18:43,416 Epoch  20: Total Training Recognition Loss 6.94  Total Training Translation Loss 3958.39 
2024-02-05 19:18:43,417 EPOCH 21
2024-02-05 19:18:45,726 [Epoch: 021 Step: 00002700] Batch Recognition Loss:   0.022354 => Gls Tokens per Sec:     1387 || Batch Translation Loss:  24.655195 => Txt Tokens per Sec:     3950 || Lr: 0.000100
2024-02-05 19:18:50,862 Epoch  21: Total Training Recognition Loss 7.33  Total Training Translation Loss 3596.51 
2024-02-05 19:18:50,862 EPOCH 22
2024-02-05 19:18:51,163 [Epoch: 022 Step: 00002800] Batch Recognition Loss:   0.074279 => Gls Tokens per Sec:     1867 || Batch Translation Loss:  19.311338 => Txt Tokens per Sec:     4913 || Lr: 0.000100
2024-02-05 19:18:56,797 [Epoch: 022 Step: 00002900] Batch Recognition Loss:   0.043924 => Gls Tokens per Sec:     1417 || Batch Translation Loss:  17.687277 => Txt Tokens per Sec:     3920 || Lr: 0.000100
2024-02-05 19:18:58,184 Epoch  22: Total Training Recognition Loss 6.89  Total Training Translation Loss 3275.39 
2024-02-05 19:18:58,184 EPOCH 23
2024-02-05 19:19:02,362 [Epoch: 023 Step: 00003000] Batch Recognition Loss:   0.097515 => Gls Tokens per Sec:     1417 || Batch Translation Loss:   6.351564 => Txt Tokens per Sec:     3913 || Lr: 0.000100
2024-02-05 19:19:05,802 Epoch  23: Total Training Recognition Loss 7.30  Total Training Translation Loss 2954.51 
2024-02-05 19:19:05,803 EPOCH 24
2024-02-05 19:19:08,193 [Epoch: 024 Step: 00003100] Batch Recognition Loss:   0.012531 => Gls Tokens per Sec:     1373 || Batch Translation Loss:  17.385902 => Txt Tokens per Sec:     3868 || Lr: 0.000100
2024-02-05 19:19:13,485 Epoch  24: Total Training Recognition Loss 6.66  Total Training Translation Loss 2680.72 
2024-02-05 19:19:13,486 EPOCH 25
2024-02-05 19:19:13,994 [Epoch: 025 Step: 00003200] Batch Recognition Loss:   0.020380 => Gls Tokens per Sec:     1262 || Batch Translation Loss:  15.754707 => Txt Tokens per Sec:     3854 || Lr: 0.000100
2024-02-05 19:19:19,534 [Epoch: 025 Step: 00003300] Batch Recognition Loss:   0.026360 => Gls Tokens per Sec:     1444 || Batch Translation Loss:  19.533564 => Txt Tokens per Sec:     3978 || Lr: 0.000100
2024-02-05 19:19:20,987 Epoch  25: Total Training Recognition Loss 7.08  Total Training Translation Loss 2383.74 
2024-02-05 19:19:20,987 EPOCH 26
2024-02-05 19:19:25,215 [Epoch: 026 Step: 00003400] Batch Recognition Loss:   0.112779 => Gls Tokens per Sec:     1420 || Batch Translation Loss:   5.503764 => Txt Tokens per Sec:     3926 || Lr: 0.000100
2024-02-05 19:19:28,621 Epoch  26: Total Training Recognition Loss 6.58  Total Training Translation Loss 2126.54 
2024-02-05 19:19:28,621 EPOCH 27
2024-02-05 19:19:30,810 [Epoch: 027 Step: 00003500] Batch Recognition Loss:   0.096274 => Gls Tokens per Sec:     1536 || Batch Translation Loss:  19.926826 => Txt Tokens per Sec:     4056 || Lr: 0.000100
2024-02-05 19:19:36,157 Epoch  27: Total Training Recognition Loss 6.00  Total Training Translation Loss 1890.02 
2024-02-05 19:19:36,158 EPOCH 28
2024-02-05 19:19:36,714 [Epoch: 028 Step: 00003600] Batch Recognition Loss:   0.048368 => Gls Tokens per Sec:     1295 || Batch Translation Loss:  13.842503 => Txt Tokens per Sec:     3358 || Lr: 0.000100
2024-02-05 19:19:41,977 [Epoch: 028 Step: 00003700] Batch Recognition Loss:   0.026813 => Gls Tokens per Sec:     1520 || Batch Translation Loss:  12.398520 => Txt Tokens per Sec:     4212 || Lr: 0.000100
2024-02-05 19:19:43,450 Epoch  28: Total Training Recognition Loss 6.68  Total Training Translation Loss 1660.51 
2024-02-05 19:19:43,450 EPOCH 29
2024-02-05 19:19:47,516 [Epoch: 029 Step: 00003800] Batch Recognition Loss:   0.027023 => Gls Tokens per Sec:     1496 || Batch Translation Loss:   8.420274 => Txt Tokens per Sec:     4110 || Lr: 0.000100
2024-02-05 19:19:50,863 Epoch  29: Total Training Recognition Loss 6.23  Total Training Translation Loss 1434.68 
2024-02-05 19:19:50,864 EPOCH 30
2024-02-05 19:19:53,411 [Epoch: 030 Step: 00003900] Batch Recognition Loss:   0.008876 => Gls Tokens per Sec:     1351 || Batch Translation Loss:   9.010634 => Txt Tokens per Sec:     3711 || Lr: 0.000100
2024-02-05 19:19:59,051 Epoch  30: Total Training Recognition Loss 5.55  Total Training Translation Loss 1270.01 
2024-02-05 19:19:59,052 EPOCH 31
2024-02-05 19:19:59,539 [Epoch: 031 Step: 00004000] Batch Recognition Loss:   0.023979 => Gls Tokens per Sec:     1648 || Batch Translation Loss:  10.335342 => Txt Tokens per Sec:     4355 || Lr: 0.000100
2024-02-05 19:20:11,156 Hooray! New best validation result [eval_metric]!
2024-02-05 19:20:11,157 Saving new checkpoint.
2024-02-05 19:20:11,550 Validation result at epoch  31, step     4000: duration: 12.0113s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 5.67530	Translation Loss: 66637.60156	PPL: 777.25867
	Eval Metric: BLEU
	WER 6.21	(DEL: 0.00,	INS: 0.00,	SUB: 6.21)
	BLEU-4 0.90	(BLEU-1: 13.04,	BLEU-2: 4.63,	BLEU-3: 1.79,	BLEU-4: 0.90)
	CHRF 16.97	ROUGE 10.51
2024-02-05 19:20:11,551 Logging Recognition and Translation Outputs
2024-02-05 19:20:11,551 ========================================================================================================================
2024-02-05 19:20:11,551 Logging Sequence: 112_165.00
2024-02-05 19:20:11,551 	Gloss Reference :	A B+C+D+E
2024-02-05 19:20:11,552 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:20:11,552 	Gloss Alignment :	         
2024-02-05 19:20:11,552 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:20:11,553 	Text Reference  :	**** **** ********* the **** *** **** *** ** ***** narendra modi   stadium will be   the  home for     the    ahmedabad-based franchise
2024-02-05 19:20:11,553 	Text Hypothesis :	they were objecting the same but they had to carry on       social media   that they have been planned around their           height   
2024-02-05 19:20:11,554 	Text Alignment  :	I    I    I             I    I   I    I   I  I     S        S      S       S    S    S    S    S       S      S               S        
2024-02-05 19:20:11,554 ========================================================================================================================
2024-02-05 19:20:11,554 Logging Sequence: 176_154.00
2024-02-05 19:20:11,554 	Gloss Reference :	A B+C+D+E
2024-02-05 19:20:11,554 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:20:11,554 	Gloss Alignment :	         
2024-02-05 19:20:11,555 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:20:11,555 	Text Reference  :	*** *** dahiya could potentially bring home india's second gold   medal   
2024-02-05 19:20:11,555 	Text Hypothesis :	she had won    a     silver      medal in   the     2012   london olympics
2024-02-05 19:20:11,556 	Text Alignment  :	I   I   S      S     S           S     S    S       S      S      S       
2024-02-05 19:20:11,556 ========================================================================================================================
2024-02-05 19:20:11,556 Logging Sequence: 94_2.00
2024-02-05 19:20:11,556 	Gloss Reference :	A B+C+D+E
2024-02-05 19:20:11,556 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:20:11,556 	Gloss Alignment :	         
2024-02-05 19:20:11,556 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:20:11,558 	Text Reference  :	the    icc     odi  men' world cup     2023 will   be     hosted  by   india   on 5th     october 2023 
2024-02-05 19:20:11,558 	Text Hypothesis :	indian cricket team is   in    colombo sri  lankan lankan batsmen were playing in colombo sri     lanka
2024-02-05 19:20:11,558 	Text Alignment  :	S      S       S    S    S     S       S    S      S      S       S    S       S  S       S       S    
2024-02-05 19:20:11,558 ========================================================================================================================
2024-02-05 19:20:11,559 Logging Sequence: 165_453.00
2024-02-05 19:20:11,559 	Gloss Reference :	A B+C+D+E
2024-02-05 19:20:11,559 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:20:11,559 	Gloss Alignment :	         
2024-02-05 19:20:11,559 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:20:11,560 	Text Reference  :	icc did not agree to sehwag' decision of        wearing a   numberless jersey    
2024-02-05 19:20:11,560 	Text Hypothesis :	*** he  has been  a  huge    fan      following in      the brand      ambassador
2024-02-05 19:20:11,560 	Text Alignment  :	D   S   S   S     S  S       S        S         S       S   S          S         
2024-02-05 19:20:11,560 ========================================================================================================================
2024-02-05 19:20:11,561 Logging Sequence: 139_46.00
2024-02-05 19:20:11,561 	Gloss Reference :	A B+C+D+E
2024-02-05 19:20:11,561 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:20:11,561 	Gloss Alignment :	         
2024-02-05 19:20:11,561 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:20:11,563 	Text Reference  :	everyone thought it would be a one sided match because  morocco is an  amateur team and       belgium ranks 2nd    in     the    world
2024-02-05 19:20:11,563 	Text Hypothesis :	******** ******* ** ***** ** * *** on    23rd  november 2022    he was known   as   argentina with    the   entire entire entire match
2024-02-05 19:20:11,563 	Text Alignment  :	D        D       D  D     D  D D   S     S     S        S       S  S   S       S    S         S       S     S      S      S      S    
2024-02-05 19:20:11,564 ========================================================================================================================
2024-02-05 19:20:17,340 [Epoch: 031 Step: 00004100] Batch Recognition Loss:   0.089352 => Gls Tokens per Sec:     1382 || Batch Translation Loss:  11.035307 => Txt Tokens per Sec:     3826 || Lr: 0.000100
2024-02-05 19:20:18,671 Epoch  31: Total Training Recognition Loss 5.31  Total Training Translation Loss 1099.32 
2024-02-05 19:20:18,671 EPOCH 32
2024-02-05 19:20:22,838 [Epoch: 032 Step: 00004200] Batch Recognition Loss:   0.076550 => Gls Tokens per Sec:     1479 || Batch Translation Loss:   9.553076 => Txt Tokens per Sec:     4044 || Lr: 0.000100
2024-02-05 19:20:25,866 Epoch  32: Total Training Recognition Loss 4.96  Total Training Translation Loss 962.76 
2024-02-05 19:20:25,866 EPOCH 33
2024-02-05 19:20:28,258 [Epoch: 033 Step: 00004300] Batch Recognition Loss:   0.018557 => Gls Tokens per Sec:     1472 || Batch Translation Loss:   4.415009 => Txt Tokens per Sec:     3987 || Lr: 0.000100
2024-02-05 19:20:33,335 Epoch  33: Total Training Recognition Loss 4.84  Total Training Translation Loss 832.85 
2024-02-05 19:20:33,335 EPOCH 34
2024-02-05 19:20:33,935 [Epoch: 034 Step: 00004400] Batch Recognition Loss:   0.024369 => Gls Tokens per Sec:     1472 || Batch Translation Loss:   6.544117 => Txt Tokens per Sec:     4293 || Lr: 0.000100
2024-02-05 19:20:39,481 [Epoch: 034 Step: 00004500] Batch Recognition Loss:   0.023748 => Gls Tokens per Sec:     1439 || Batch Translation Loss:   6.189412 => Txt Tokens per Sec:     4003 || Lr: 0.000100
2024-02-05 19:20:40,589 Epoch  34: Total Training Recognition Loss 4.56  Total Training Translation Loss 734.23 
2024-02-05 19:20:40,589 EPOCH 35
2024-02-05 19:20:44,675 [Epoch: 035 Step: 00004600] Batch Recognition Loss:   0.083799 => Gls Tokens per Sec:     1528 || Batch Translation Loss:   7.523927 => Txt Tokens per Sec:     4214 || Lr: 0.000100
2024-02-05 19:20:47,755 Epoch  35: Total Training Recognition Loss 4.61  Total Training Translation Loss 647.28 
2024-02-05 19:20:47,755 EPOCH 36
2024-02-05 19:20:50,053 [Epoch: 036 Step: 00004700] Batch Recognition Loss:   0.011147 => Gls Tokens per Sec:     1568 || Batch Translation Loss:   4.958719 => Txt Tokens per Sec:     4189 || Lr: 0.000100
2024-02-05 19:20:54,769 Epoch  36: Total Training Recognition Loss 4.30  Total Training Translation Loss 569.03 
2024-02-05 19:20:54,770 EPOCH 37
2024-02-05 19:20:55,332 [Epoch: 037 Step: 00004800] Batch Recognition Loss:   0.030523 => Gls Tokens per Sec:     1707 || Batch Translation Loss:   5.280664 => Txt Tokens per Sec:     4691 || Lr: 0.000100
2024-02-05 19:21:00,896 [Epoch: 037 Step: 00004900] Batch Recognition Loss:   0.117376 => Gls Tokens per Sec:     1438 || Batch Translation Loss:   5.065679 => Txt Tokens per Sec:     3978 || Lr: 0.000100
2024-02-05 19:21:02,024 Epoch  37: Total Training Recognition Loss 3.86  Total Training Translation Loss 503.10 
2024-02-05 19:21:02,024 EPOCH 38
2024-02-05 19:21:06,124 [Epoch: 038 Step: 00005000] Batch Recognition Loss:   0.013325 => Gls Tokens per Sec:     1542 || Batch Translation Loss:   2.705728 => Txt Tokens per Sec:     4241 || Lr: 0.000100
2024-02-05 19:21:09,254 Epoch  38: Total Training Recognition Loss 4.05  Total Training Translation Loss 463.25 
2024-02-05 19:21:09,255 EPOCH 39
2024-02-05 19:21:11,622 [Epoch: 039 Step: 00005100] Batch Recognition Loss:   0.035562 => Gls Tokens per Sec:     1555 || Batch Translation Loss:   2.965364 => Txt Tokens per Sec:     4138 || Lr: 0.000100
2024-02-05 19:21:16,314 Epoch  39: Total Training Recognition Loss 3.89  Total Training Translation Loss 442.07 
2024-02-05 19:21:16,314 EPOCH 40
2024-02-05 19:21:17,005 [Epoch: 040 Step: 00005200] Batch Recognition Loss:   0.019526 => Gls Tokens per Sec:     1507 || Batch Translation Loss:   2.060647 => Txt Tokens per Sec:     3983 || Lr: 0.000100
2024-02-05 19:21:22,590 [Epoch: 040 Step: 00005300] Batch Recognition Loss:   0.064800 => Gls Tokens per Sec:     1429 || Batch Translation Loss:   1.047823 => Txt Tokens per Sec:     3984 || Lr: 0.000100
2024-02-05 19:21:23,766 Epoch  40: Total Training Recognition Loss 3.83  Total Training Translation Loss 372.47 
2024-02-05 19:21:23,766 EPOCH 41
2024-02-05 19:21:28,060 [Epoch: 041 Step: 00005400] Batch Recognition Loss:   0.024934 => Gls Tokens per Sec:     1491 || Batch Translation Loss:   1.678124 => Txt Tokens per Sec:     4067 || Lr: 0.000100
2024-02-05 19:21:31,150 Epoch  41: Total Training Recognition Loss 3.63  Total Training Translation Loss 333.46 
2024-02-05 19:21:31,150 EPOCH 42
2024-02-05 19:21:33,757 [Epoch: 042 Step: 00005500] Batch Recognition Loss:   0.013325 => Gls Tokens per Sec:     1443 || Batch Translation Loss:   2.666793 => Txt Tokens per Sec:     3836 || Lr: 0.000100
2024-02-05 19:21:38,734 Epoch  42: Total Training Recognition Loss 3.23  Total Training Translation Loss 304.80 
2024-02-05 19:21:38,734 EPOCH 43
2024-02-05 19:21:39,567 [Epoch: 043 Step: 00005600] Batch Recognition Loss:   0.042253 => Gls Tokens per Sec:     1346 || Batch Translation Loss:   2.730907 => Txt Tokens per Sec:     3861 || Lr: 0.000100
2024-02-05 19:21:44,824 [Epoch: 043 Step: 00005700] Batch Recognition Loss:   0.031889 => Gls Tokens per Sec:     1518 || Batch Translation Loss:   1.274524 => Txt Tokens per Sec:     4179 || Lr: 0.000100
2024-02-05 19:21:45,768 Epoch  43: Total Training Recognition Loss 3.12  Total Training Translation Loss 271.39 
2024-02-05 19:21:45,768 EPOCH 44
2024-02-05 19:21:50,347 [Epoch: 044 Step: 00005800] Batch Recognition Loss:   0.008553 => Gls Tokens per Sec:     1415 || Batch Translation Loss:   2.240797 => Txt Tokens per Sec:     3884 || Lr: 0.000100
2024-02-05 19:21:53,310 Epoch  44: Total Training Recognition Loss 3.10  Total Training Translation Loss 270.82 
2024-02-05 19:21:53,311 EPOCH 45
2024-02-05 19:21:55,819 [Epoch: 045 Step: 00005900] Batch Recognition Loss:   0.005049 => Gls Tokens per Sec:     1533 || Batch Translation Loss:   1.643067 => Txt Tokens per Sec:     4330 || Lr: 0.000100
2024-02-05 19:22:00,584 Epoch  45: Total Training Recognition Loss 3.09  Total Training Translation Loss 254.40 
2024-02-05 19:22:00,584 EPOCH 46
2024-02-05 19:22:01,369 [Epoch: 046 Step: 00006000] Batch Recognition Loss:   0.004929 => Gls Tokens per Sec:     1531 || Batch Translation Loss:   3.145338 => Txt Tokens per Sec:     4290 || Lr: 0.000100
2024-02-05 19:22:12,124 Validation result at epoch  46, step     6000: duration: 10.7553s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 6.23954	Translation Loss: 75033.09375	PPL: 1797.79492
	Eval Metric: BLEU
	WER 6.21	(DEL: 0.00,	INS: 0.00,	SUB: 6.21)
	BLEU-4 0.84	(BLEU-1: 12.90,	BLEU-2: 4.32,	BLEU-3: 1.71,	BLEU-4: 0.84)
	CHRF 17.50	ROUGE 10.26
2024-02-05 19:22:12,125 Logging Recognition and Translation Outputs
2024-02-05 19:22:12,125 ========================================================================================================================
2024-02-05 19:22:12,125 Logging Sequence: 160_153.00
2024-02-05 19:22:12,126 	Gloss Reference :	A B+C+D+E
2024-02-05 19:22:12,126 	Gloss Hypothesis:	A B+C+D  
2024-02-05 19:22:12,126 	Gloss Alignment :	  S      
2024-02-05 19:22:12,126 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:22:12,128 	Text Reference  :	i have no hard feelings towards   rohit  sharma  and he will always have   my full support as he is  my   teammate
2024-02-05 19:22:12,128 	Text Hypothesis :	* **** ** **** bcci     president sourav ganguly and ** **** ****** others to step down    as ** odi t20i captain 
2024-02-05 19:22:12,128 	Text Alignment  :	D D    D  D    S        S         S      S           D  D    D      S      S  S    S          D  S   S    S       
2024-02-05 19:22:12,128 ========================================================================================================================
2024-02-05 19:22:12,128 Logging Sequence: 103_253.00
2024-02-05 19:22:12,128 	Gloss Reference :	A B+C+D+E
2024-02-05 19:22:12,129 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:22:12,129 	Gloss Alignment :	         
2024-02-05 19:22:12,129 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:22:12,129 	Text Reference  :	**** ***** ******* *** *** *** ***** ******** canada is 3rd   with 92 medals
2024-02-05 19:22:12,129 	Text Hypothesis :	this upset gambhir has won the first position for    a  total of   6  medals
2024-02-05 19:22:12,130 	Text Alignment  :	I    I     I       I   I   I   I     I        S      S  S     S    S        
2024-02-05 19:22:12,130 ========================================================================================================================
2024-02-05 19:22:12,130 Logging Sequence: 155_25.00
2024-02-05 19:22:12,130 	Gloss Reference :	A B+C+D+E
2024-02-05 19:22:12,130 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:22:12,130 	Gloss Alignment :	         
2024-02-05 19:22:12,130 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:22:12,131 	Text Reference  :	this is because taliban overthrew the  afghan government and    took          over the country
2024-02-05 19:22:12,132 	Text Hypothesis :	i    am so      what    a         huge fan    of         sports personalities for  the matches
2024-02-05 19:22:12,132 	Text Alignment  :	S    S  S       S       S         S    S      S          S      S             S        S      
2024-02-05 19:22:12,132 ========================================================================================================================
2024-02-05 19:22:12,132 Logging Sequence: 81_105.00
2024-02-05 19:22:12,132 	Gloss Reference :	A B+C+D+E
2024-02-05 19:22:12,132 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:22:12,132 	Gloss Alignment :	         
2024-02-05 19:22:12,132 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:22:12,134 	Text Reference  :	*** **** dhoni was tagged in  multiple such posts as he was  the brand ambassador
2024-02-05 19:22:12,134 	Text Hypothesis :	and told him   for the    t20 world    cup  is    up to play the brand ambassador
2024-02-05 19:22:12,134 	Text Alignment  :	I   I    S     S   S      S   S        S    S     S  S  S                        
2024-02-05 19:22:12,134 ========================================================================================================================
2024-02-05 19:22:12,134 Logging Sequence: 105_136.00
2024-02-05 19:22:12,134 	Gloss Reference :	A B+C+D+E
2024-02-05 19:22:12,134 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:22:12,135 	Gloss Alignment :	         
2024-02-05 19:22:12,135 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:22:12,135 	Text Reference  :	** *** **** ** **** *** ***** beating him   once   is  my  biggest dream 
2024-02-05 19:22:12,136 	Text Hypothesis :	as you want to know the image of      their mobile and why was     halted
2024-02-05 19:22:12,136 	Text Alignment  :	I  I   I    I  I    I   I     S       S     S      S   S   S       S     
2024-02-05 19:22:12,136 ========================================================================================================================
2024-02-05 19:22:17,867 [Epoch: 046 Step: 00006100] Batch Recognition Loss:   0.002709 => Gls Tokens per Sec:     1390 || Batch Translation Loss:   1.953916 => Txt Tokens per Sec:     3865 || Lr: 0.000100
2024-02-05 19:22:18,831 Epoch  46: Total Training Recognition Loss 3.23  Total Training Translation Loss 231.51 
2024-02-05 19:22:18,831 EPOCH 47
2024-02-05 19:22:22,979 [Epoch: 047 Step: 00006200] Batch Recognition Loss:   0.016567 => Gls Tokens per Sec:     1582 || Batch Translation Loss:   2.087783 => Txt Tokens per Sec:     4311 || Lr: 0.000100
2024-02-05 19:22:26,266 Epoch  47: Total Training Recognition Loss 3.31  Total Training Translation Loss 228.62 
2024-02-05 19:22:26,267 EPOCH 48
2024-02-05 19:22:29,058 [Epoch: 048 Step: 00006300] Batch Recognition Loss:   0.001881 => Gls Tokens per Sec:     1405 || Batch Translation Loss:   1.626924 => Txt Tokens per Sec:     3878 || Lr: 0.000100
2024-02-05 19:22:33,770 Epoch  48: Total Training Recognition Loss 2.74  Total Training Translation Loss 201.43 
2024-02-05 19:22:33,770 EPOCH 49
2024-02-05 19:22:34,634 [Epoch: 049 Step: 00006400] Batch Recognition Loss:   0.029490 => Gls Tokens per Sec:     1483 || Batch Translation Loss:   0.861567 => Txt Tokens per Sec:     4292 || Lr: 0.000100
2024-02-05 19:22:40,377 [Epoch: 049 Step: 00006500] Batch Recognition Loss:   0.021425 => Gls Tokens per Sec:     1393 || Batch Translation Loss:   1.146782 => Txt Tokens per Sec:     3810 || Lr: 0.000100
2024-02-05 19:22:41,414 Epoch  49: Total Training Recognition Loss 2.55  Total Training Translation Loss 177.93 
2024-02-05 19:22:41,415 EPOCH 50
2024-02-05 19:22:46,151 [Epoch: 050 Step: 00006600] Batch Recognition Loss:   0.028997 => Gls Tokens per Sec:     1402 || Batch Translation Loss:   1.407509 => Txt Tokens per Sec:     3926 || Lr: 0.000100
2024-02-05 19:22:48,855 Epoch  50: Total Training Recognition Loss 2.41  Total Training Translation Loss 172.11 
2024-02-05 19:22:48,855 EPOCH 51
2024-02-05 19:22:51,858 [Epoch: 051 Step: 00006700] Batch Recognition Loss:   0.020972 => Gls Tokens per Sec:     1332 || Batch Translation Loss:   1.600647 => Txt Tokens per Sec:     3663 || Lr: 0.000100
2024-02-05 19:22:56,611 Epoch  51: Total Training Recognition Loss 2.48  Total Training Translation Loss 179.08 
2024-02-05 19:22:56,611 EPOCH 52
2024-02-05 19:22:57,513 [Epoch: 052 Step: 00006800] Batch Recognition Loss:   0.011059 => Gls Tokens per Sec:     1509 || Batch Translation Loss:   1.705976 => Txt Tokens per Sec:     4187 || Lr: 0.000100
2024-02-05 19:23:03,001 [Epoch: 052 Step: 00006900] Batch Recognition Loss:   0.012915 => Gls Tokens per Sec:     1458 || Batch Translation Loss:   1.315951 => Txt Tokens per Sec:     4048 || Lr: 0.000100
2024-02-05 19:23:03,925 Epoch  52: Total Training Recognition Loss 2.34  Total Training Translation Loss 165.06 
2024-02-05 19:23:03,926 EPOCH 53
2024-02-05 19:23:08,503 [Epoch: 053 Step: 00007000] Batch Recognition Loss:   0.006304 => Gls Tokens per Sec:     1468 || Batch Translation Loss:   1.392996 => Txt Tokens per Sec:     4059 || Lr: 0.000100
2024-02-05 19:23:11,150 Epoch  53: Total Training Recognition Loss 2.51  Total Training Translation Loss 152.24 
2024-02-05 19:23:11,150 EPOCH 54
2024-02-05 19:23:14,164 [Epoch: 054 Step: 00007100] Batch Recognition Loss:   0.014460 => Gls Tokens per Sec:     1354 || Batch Translation Loss:   1.123867 => Txt Tokens per Sec:     3800 || Lr: 0.000100
2024-02-05 19:23:18,705 Epoch  54: Total Training Recognition Loss 2.29  Total Training Translation Loss 152.03 
2024-02-05 19:23:18,705 EPOCH 55
2024-02-05 19:23:19,538 [Epoch: 055 Step: 00007200] Batch Recognition Loss:   0.007499 => Gls Tokens per Sec:     1732 || Batch Translation Loss:   0.638661 => Txt Tokens per Sec:     4522 || Lr: 0.000100
2024-02-05 19:23:25,220 [Epoch: 055 Step: 00007300] Batch Recognition Loss:   0.009966 => Gls Tokens per Sec:     1404 || Batch Translation Loss:   1.162497 => Txt Tokens per Sec:     3973 || Lr: 0.000100
2024-02-05 19:23:25,944 Epoch  55: Total Training Recognition Loss 2.23  Total Training Translation Loss 144.10 
2024-02-05 19:23:25,944 EPOCH 56
2024-02-05 19:23:30,681 [Epoch: 056 Step: 00007400] Batch Recognition Loss:   0.012306 => Gls Tokens per Sec:     1436 || Batch Translation Loss:   0.674756 => Txt Tokens per Sec:     3920 || Lr: 0.000100
2024-02-05 19:23:33,556 Epoch  56: Total Training Recognition Loss 1.91  Total Training Translation Loss 140.63 
2024-02-05 19:23:33,557 EPOCH 57
2024-02-05 19:23:36,665 [Epoch: 057 Step: 00007500] Batch Recognition Loss:   0.015155 => Gls Tokens per Sec:     1338 || Batch Translation Loss:   1.826363 => Txt Tokens per Sec:     3656 || Lr: 0.000100
2024-02-05 19:23:41,387 Epoch  57: Total Training Recognition Loss 2.09  Total Training Translation Loss 138.94 
2024-02-05 19:23:41,387 EPOCH 58
2024-02-05 19:23:42,341 [Epoch: 058 Step: 00007600] Batch Recognition Loss:   0.008336 => Gls Tokens per Sec:     1595 || Batch Translation Loss:   1.180549 => Txt Tokens per Sec:     4419 || Lr: 0.000100
2024-02-05 19:23:48,131 [Epoch: 058 Step: 00007700] Batch Recognition Loss:   0.015186 => Gls Tokens per Sec:     1382 || Batch Translation Loss:   0.776001 => Txt Tokens per Sec:     3822 || Lr: 0.000100
2024-02-05 19:23:48,999 Epoch  58: Total Training Recognition Loss 2.14  Total Training Translation Loss 130.50 
2024-02-05 19:23:49,000 EPOCH 59
2024-02-05 19:23:53,509 [Epoch: 059 Step: 00007800] Batch Recognition Loss:   0.001509 => Gls Tokens per Sec:     1526 || Batch Translation Loss:   0.707295 => Txt Tokens per Sec:     4234 || Lr: 0.000100
2024-02-05 19:23:56,481 Epoch  59: Total Training Recognition Loss 1.95  Total Training Translation Loss 120.70 
2024-02-05 19:23:56,482 EPOCH 60
2024-02-05 19:23:59,578 [Epoch: 060 Step: 00007900] Batch Recognition Loss:   0.008575 => Gls Tokens per Sec:     1370 || Batch Translation Loss:   0.499225 => Txt Tokens per Sec:     3806 || Lr: 0.000100
2024-02-05 19:24:03,969 Epoch  60: Total Training Recognition Loss 2.24  Total Training Translation Loss 115.80 
2024-02-05 19:24:03,969 EPOCH 61
2024-02-05 19:24:05,274 [Epoch: 061 Step: 00008000] Batch Recognition Loss:   0.092788 => Gls Tokens per Sec:     1228 || Batch Translation Loss:   1.408222 => Txt Tokens per Sec:     3658 || Lr: 0.000100
2024-02-05 19:24:15,702 Validation result at epoch  61, step     8000: duration: 10.4279s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 6.46578	Translation Loss: 82008.84375	PPL: 3608.52832
	Eval Metric: BLEU
	WER 5.79	(DEL: 0.00,	INS: 0.00,	SUB: 5.79)
	BLEU-4 0.60	(BLEU-1: 11.46,	BLEU-2: 3.87,	BLEU-3: 1.32,	BLEU-4: 0.60)
	CHRF 17.12	ROUGE 9.95
2024-02-05 19:24:15,703 Logging Recognition and Translation Outputs
2024-02-05 19:24:15,704 ========================================================================================================================
2024-02-05 19:24:15,704 Logging Sequence: 180_236.00
2024-02-05 19:24:15,704 	Gloss Reference :	A B+C+D+E
2024-02-05 19:24:15,704 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:24:15,704 	Gloss Alignment :	         
2024-02-05 19:24:15,704 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:24:15,706 	Text Reference  :	however the  wrestlers returned to      the ****** protest site at   jantar mantar      with     thier demands
2024-02-05 19:24:15,706 	Text Hypothesis :	******* they were      also     outside the report this    and  they were   immediately released her   time   
2024-02-05 19:24:15,706 	Text Alignment  :	D       S    S         S        S           I      S       S    S    S      S           S        S     S      
2024-02-05 19:24:15,706 ========================================================================================================================
2024-02-05 19:24:15,706 Logging Sequence: 111_154.00
2024-02-05 19:24:15,706 	Gloss Reference :	A B+C+D+E  
2024-02-05 19:24:15,706 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-05 19:24:15,706 	Gloss Alignment :	  S        
2024-02-05 19:24:15,707 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:24:15,708 	Text Reference  :	** due  to    csk's slow      over   rate dhoni   was fined rs 12 lakh
2024-02-05 19:24:15,708 	Text Hypothesis :	on 10th april 2021  rajasthan royals as   captain was fined rs 12 lakh
2024-02-05 19:24:15,708 	Text Alignment  :	I  S    S     S     S         S      S    S                           
2024-02-05 19:24:15,708 ========================================================================================================================
2024-02-05 19:24:15,708 Logging Sequence: 118_314.00
2024-02-05 19:24:15,708 	Gloss Reference :	A B+C+D+E
2024-02-05 19:24:15,708 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:24:15,708 	Gloss Alignment :	         
2024-02-05 19:24:15,709 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:24:15,709 	Text Reference  :	** wow  even the president had come to   watch
2024-02-05 19:24:15,709 	Text Hypothesis :	so here are  the ********* *** **** same time 
2024-02-05 19:24:15,709 	Text Alignment  :	I  S    S        D         D   D    S    S    
2024-02-05 19:24:15,709 ========================================================================================================================
2024-02-05 19:24:15,710 Logging Sequence: 156_197.00
2024-02-05 19:24:15,710 	Gloss Reference :	A B+C+D+E  
2024-02-05 19:24:15,710 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-05 19:24:15,710 	Gloss Alignment :	  S        
2024-02-05 19:24:15,710 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:24:15,711 	Text Reference  :	******* seattle orcas sor   is  owned     by     many investors including satya    nadella microsoft ceo     
2024-02-05 19:24:15,712 	Text Hypothesis :	amazing to      see   these are different states if   they      have      invested in      the       athletes
2024-02-05 19:24:15,712 	Text Alignment  :	I       S       S     S     S   S         S      S    S         S         S        S       S         S       
2024-02-05 19:24:15,712 ========================================================================================================================
2024-02-05 19:24:15,712 Logging Sequence: 183_159.00
2024-02-05 19:24:15,712 	Gloss Reference :	A B+C+D+E
2024-02-05 19:24:15,712 	Gloss Hypothesis:	A B+C+D  
2024-02-05 19:24:15,712 	Gloss Alignment :	  S      
2024-02-05 19:24:15,712 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:24:15,714 	Text Reference  :	however an exception to this is virat kohli and his wife anushka sharma  who refuse  to share images of   their daughter
2024-02-05 19:24:15,714 	Text Hypothesis :	******* ** ********* ** **** ** ***** ***** *** *** **** fans    usually the picture of the   virus  will be    applied 
2024-02-05 19:24:15,714 	Text Alignment  :	D       D  D         D  D    D  D     D     D   D   D    S       S       S   S       S  S     S      S    S     S       
2024-02-05 19:24:15,714 ========================================================================================================================
2024-02-05 19:24:21,556 [Epoch: 061 Step: 00008100] Batch Recognition Loss:   0.012546 => Gls Tokens per Sec:     1367 || Batch Translation Loss:   0.991398 => Txt Tokens per Sec:     3726 || Lr: 0.000100
2024-02-05 19:24:22,298 Epoch  61: Total Training Recognition Loss 2.14  Total Training Translation Loss 121.87 
2024-02-05 19:24:22,298 EPOCH 62
2024-02-05 19:24:27,106 [Epoch: 062 Step: 00008200] Batch Recognition Loss:   0.027587 => Gls Tokens per Sec:     1448 || Batch Translation Loss:   0.937415 => Txt Tokens per Sec:     3937 || Lr: 0.000100
2024-02-05 19:24:29,952 Epoch  62: Total Training Recognition Loss 2.25  Total Training Translation Loss 109.87 
2024-02-05 19:24:29,952 EPOCH 63
2024-02-05 19:24:33,149 [Epoch: 063 Step: 00008300] Batch Recognition Loss:   0.026730 => Gls Tokens per Sec:     1352 || Batch Translation Loss:   0.660205 => Txt Tokens per Sec:     3736 || Lr: 0.000100
2024-02-05 19:24:37,781 Epoch  63: Total Training Recognition Loss 1.94  Total Training Translation Loss 96.61 
2024-02-05 19:24:37,781 EPOCH 64
2024-02-05 19:24:38,887 [Epoch: 064 Step: 00008400] Batch Recognition Loss:   0.058148 => Gls Tokens per Sec:     1520 || Batch Translation Loss:   0.834896 => Txt Tokens per Sec:     4301 || Lr: 0.000100
2024-02-05 19:24:44,499 [Epoch: 064 Step: 00008500] Batch Recognition Loss:   0.002967 => Gls Tokens per Sec:     1426 || Batch Translation Loss:   1.119718 => Txt Tokens per Sec:     3939 || Lr: 0.000100
2024-02-05 19:24:45,237 Epoch  64: Total Training Recognition Loss 1.91  Total Training Translation Loss 88.39 
2024-02-05 19:24:45,238 EPOCH 65
2024-02-05 19:24:50,152 [Epoch: 065 Step: 00008600] Batch Recognition Loss:   0.029972 => Gls Tokens per Sec:     1433 || Batch Translation Loss:   0.445759 => Txt Tokens per Sec:     3923 || Lr: 0.000100
2024-02-05 19:24:52,512 Epoch  65: Total Training Recognition Loss 2.02  Total Training Translation Loss 96.72 
2024-02-05 19:24:52,512 EPOCH 66
2024-02-05 19:24:55,719 [Epoch: 066 Step: 00008700] Batch Recognition Loss:   0.002673 => Gls Tokens per Sec:     1372 || Batch Translation Loss:   0.654528 => Txt Tokens per Sec:     3787 || Lr: 0.000100
2024-02-05 19:25:00,109 Epoch  66: Total Training Recognition Loss 1.99  Total Training Translation Loss 98.00 
2024-02-05 19:25:00,109 EPOCH 67
2024-02-05 19:25:01,213 [Epoch: 067 Step: 00008800] Batch Recognition Loss:   0.001646 => Gls Tokens per Sec:     1596 || Batch Translation Loss:   0.347029 => Txt Tokens per Sec:     4515 || Lr: 0.000100
2024-02-05 19:25:06,614 [Epoch: 067 Step: 00008900] Batch Recognition Loss:   0.006508 => Gls Tokens per Sec:     1478 || Batch Translation Loss:   0.470612 => Txt Tokens per Sec:     4065 || Lr: 0.000100
2024-02-05 19:25:07,212 Epoch  67: Total Training Recognition Loss 2.15  Total Training Translation Loss 88.49 
2024-02-05 19:25:07,212 EPOCH 68
2024-02-05 19:25:11,985 [Epoch: 068 Step: 00009000] Batch Recognition Loss:   0.001924 => Gls Tokens per Sec:     1492 || Batch Translation Loss:   0.623161 => Txt Tokens per Sec:     4076 || Lr: 0.000100
2024-02-05 19:25:14,639 Epoch  68: Total Training Recognition Loss 1.64  Total Training Translation Loss 87.58 
2024-02-05 19:25:14,640 EPOCH 69
2024-02-05 19:25:17,557 [Epoch: 069 Step: 00009100] Batch Recognition Loss:   0.006929 => Gls Tokens per Sec:     1537 || Batch Translation Loss:   0.256164 => Txt Tokens per Sec:     4179 || Lr: 0.000100
2024-02-05 19:25:21,992 Epoch  69: Total Training Recognition Loss 1.71  Total Training Translation Loss 79.80 
2024-02-05 19:25:21,992 EPOCH 70
2024-02-05 19:25:23,508 [Epoch: 070 Step: 00009200] Batch Recognition Loss:   0.007858 => Gls Tokens per Sec:     1215 || Batch Translation Loss:   0.381266 => Txt Tokens per Sec:     3291 || Lr: 0.000100
2024-02-05 19:25:28,978 [Epoch: 070 Step: 00009300] Batch Recognition Loss:   0.022894 => Gls Tokens per Sec:     1459 || Batch Translation Loss:   0.338679 => Txt Tokens per Sec:     4029 || Lr: 0.000100
2024-02-05 19:25:29,522 Epoch  70: Total Training Recognition Loss 1.95  Total Training Translation Loss 84.22 
2024-02-05 19:25:29,522 EPOCH 71
2024-02-05 19:25:34,326 [Epoch: 071 Step: 00009400] Batch Recognition Loss:   0.003948 => Gls Tokens per Sec:     1499 || Batch Translation Loss:   0.627664 => Txt Tokens per Sec:     4122 || Lr: 0.000100
2024-02-05 19:25:36,677 Epoch  71: Total Training Recognition Loss 1.56  Total Training Translation Loss 92.25 
2024-02-05 19:25:36,677 EPOCH 72
2024-02-05 19:25:39,751 [Epoch: 072 Step: 00009500] Batch Recognition Loss:   0.011671 => Gls Tokens per Sec:     1484 || Batch Translation Loss:   0.238773 => Txt Tokens per Sec:     4071 || Lr: 0.000100
2024-02-05 19:25:44,101 Epoch  72: Total Training Recognition Loss 1.87  Total Training Translation Loss 88.43 
2024-02-05 19:25:44,102 EPOCH 73
2024-02-05 19:25:45,403 [Epoch: 073 Step: 00009600] Batch Recognition Loss:   0.000867 => Gls Tokens per Sec:     1478 || Batch Translation Loss:   0.433190 => Txt Tokens per Sec:     4142 || Lr: 0.000100
2024-02-05 19:25:50,577 [Epoch: 073 Step: 00009700] Batch Recognition Loss:   0.036470 => Gls Tokens per Sec:     1542 || Batch Translation Loss:   0.566307 => Txt Tokens per Sec:     4256 || Lr: 0.000100
2024-02-05 19:25:51,000 Epoch  73: Total Training Recognition Loss 1.73  Total Training Translation Loss 83.67 
2024-02-05 19:25:51,000 EPOCH 74
2024-02-05 19:25:56,199 [Epoch: 074 Step: 00009800] Batch Recognition Loss:   0.050869 => Gls Tokens per Sec:     1400 || Batch Translation Loss:   0.799258 => Txt Tokens per Sec:     3860 || Lr: 0.000100
2024-02-05 19:25:58,687 Epoch  74: Total Training Recognition Loss 1.87  Total Training Translation Loss 72.31 
2024-02-05 19:25:58,687 EPOCH 75
2024-02-05 19:26:01,726 [Epoch: 075 Step: 00009900] Batch Recognition Loss:   0.049643 => Gls Tokens per Sec:     1528 || Batch Translation Loss:   1.208637 => Txt Tokens per Sec:     4309 || Lr: 0.000100
2024-02-05 19:26:05,698 Epoch  75: Total Training Recognition Loss 1.76  Total Training Translation Loss 73.37 
2024-02-05 19:26:05,699 EPOCH 76
2024-02-05 19:26:07,347 [Epoch: 076 Step: 00010000] Batch Recognition Loss:   0.004323 => Gls Tokens per Sec:     1215 || Batch Translation Loss:   0.315134 => Txt Tokens per Sec:     3541 || Lr: 0.000100
2024-02-05 19:26:18,238 Validation result at epoch  76, step    10000: duration: 10.8915s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 6.13198	Translation Loss: 85200.57031	PPL: 4963.39258
	Eval Metric: BLEU
	WER 5.37	(DEL: 0.00,	INS: 0.00,	SUB: 5.37)
	BLEU-4 0.41	(BLEU-1: 11.44,	BLEU-2: 3.66,	BLEU-3: 1.27,	BLEU-4: 0.41)
	CHRF 16.96	ROUGE 9.79
2024-02-05 19:26:18,239 Logging Recognition and Translation Outputs
2024-02-05 19:26:18,239 ========================================================================================================================
2024-02-05 19:26:18,239 Logging Sequence: 123_147.00
2024-02-05 19:26:18,239 	Gloss Reference :	A B+C+D+E
2024-02-05 19:26:18,239 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:26:18,239 	Gloss Alignment :	         
2024-02-05 19:26:18,239 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:26:18,240 	Text Reference  :	the former captain also owns the pontiac firebird trans am car   worth rs 68     lakh      
2024-02-05 19:26:18,240 	Text Hypothesis :	*** ****** ******* **** **** *** ******* ******** ***** ** dhoni has   an ardent bike-lover
2024-02-05 19:26:18,240 	Text Alignment  :	D   D      D       D    D    D   D       D        D     D  S     S     S  S      S         
2024-02-05 19:26:18,241 ========================================================================================================================
2024-02-05 19:26:18,241 Logging Sequence: 58_196.00
2024-02-05 19:26:18,241 	Gloss Reference :	A B+C+D+E
2024-02-05 19:26:18,241 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:26:18,241 	Gloss Alignment :	         
2024-02-05 19:26:18,241 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:26:18,242 	Text Reference  :	the talents and skills of    our athletes  knows no bounds 
2024-02-05 19:26:18,242 	Text Hypothesis :	the ******* *** ****** games are currently held  in england
2024-02-05 19:26:18,242 	Text Alignment  :	    D       D   D      S     S   S         S     S  S      
2024-02-05 19:26:18,242 ========================================================================================================================
2024-02-05 19:26:18,242 Logging Sequence: 168_184.00
2024-02-05 19:26:18,242 	Gloss Reference :	A B+C+D+E
2024-02-05 19:26:18,243 	Gloss Hypothesis:	A B+C+D  
2024-02-05 19:26:18,243 	Gloss Alignment :	  S      
2024-02-05 19:26:18,243 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:26:18,244 	Text Reference  :	people say that we may get a true glimpse of vamika  in     february 2022   when she turns 1  year   old  
2024-02-05 19:26:18,244 	Text Hypothesis :	****** *** **** ** *** *** * khan is      an amazing player and      rashid will be  going on social media
2024-02-05 19:26:18,245 	Text Alignment  :	D      D   D    D  D   D   D S    S       S  S       S      S        S      S    S   S     S  S      S    
2024-02-05 19:26:18,245 ========================================================================================================================
2024-02-05 19:26:18,245 Logging Sequence: 87_123.00
2024-02-05 19:26:18,245 	Gloss Reference :	A B+C+D+E
2024-02-05 19:26:18,245 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:26:18,245 	Gloss Alignment :	         
2024-02-05 19:26:18,245 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:26:18,246 	Text Reference  :	he said that he hoped kl rahul   would be fit     for the   upcoming world  cup  
2024-02-05 19:26:18,247 	Text Hypothesis :	** **** **** ** what  an intense game  of gambhir and video on       social media
2024-02-05 19:26:18,247 	Text Alignment  :	D  D    D    D  S     S  S       S     S  S       S   S     S        S      S    
2024-02-05 19:26:18,247 ========================================================================================================================
2024-02-05 19:26:18,247 Logging Sequence: 144_154.00
2024-02-05 19:26:18,247 	Gloss Reference :	A B+C+D+E
2024-02-05 19:26:18,247 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:26:18,247 	Gloss Alignment :	         
2024-02-05 19:26:18,247 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:26:18,249 	Text Reference  :	she also participated in the   rural olympic games  organised in    rajasthan a   few months
2024-02-05 19:26:18,249 	Text Hypothesis :	*** the  final        of their mlc   was     played between   their victory   and sri lanka 
2024-02-05 19:26:18,249 	Text Alignment  :	D   S    S            S  S     S     S       S      S         S     S         S   S   S     
2024-02-05 19:26:18,249 ========================================================================================================================
2024-02-05 19:26:23,484 [Epoch: 076 Step: 00010100] Batch Recognition Loss:   0.002027 => Gls Tokens per Sec:     1521 || Batch Translation Loss:   0.519987 => Txt Tokens per Sec:     4163 || Lr: 0.000100
2024-02-05 19:26:23,884 Epoch  76: Total Training Recognition Loss 1.30  Total Training Translation Loss 70.18 
2024-02-05 19:26:23,884 EPOCH 77
2024-02-05 19:26:28,900 [Epoch: 077 Step: 00010200] Batch Recognition Loss:   0.064962 => Gls Tokens per Sec:     1468 || Batch Translation Loss:   0.407748 => Txt Tokens per Sec:     4047 || Lr: 0.000100
2024-02-05 19:26:31,218 Epoch  77: Total Training Recognition Loss 1.57  Total Training Translation Loss 73.31 
2024-02-05 19:26:31,219 EPOCH 78
2024-02-05 19:26:34,310 [Epoch: 078 Step: 00010300] Batch Recognition Loss:   0.160092 => Gls Tokens per Sec:     1527 || Batch Translation Loss:   1.023327 => Txt Tokens per Sec:     4276 || Lr: 0.000100
2024-02-05 19:26:38,130 Epoch  78: Total Training Recognition Loss 1.65  Total Training Translation Loss 71.19 
2024-02-05 19:26:38,131 EPOCH 79
2024-02-05 19:26:39,616 [Epoch: 079 Step: 00010400] Batch Recognition Loss:   0.005596 => Gls Tokens per Sec:     1402 || Batch Translation Loss:   0.624701 => Txt Tokens per Sec:     3923 || Lr: 0.000100
2024-02-05 19:26:45,089 [Epoch: 079 Step: 00010500] Batch Recognition Loss:   0.126011 => Gls Tokens per Sec:     1458 || Batch Translation Loss:   0.545614 => Txt Tokens per Sec:     4061 || Lr: 0.000100
2024-02-05 19:26:45,417 Epoch  79: Total Training Recognition Loss 1.67  Total Training Translation Loss 69.41 
2024-02-05 19:26:45,417 EPOCH 80
2024-02-05 19:26:50,428 [Epoch: 080 Step: 00010600] Batch Recognition Loss:   0.024127 => Gls Tokens per Sec:     1485 || Batch Translation Loss:   0.938861 => Txt Tokens per Sec:     4104 || Lr: 0.000100
2024-02-05 19:26:52,733 Epoch  80: Total Training Recognition Loss 1.62  Total Training Translation Loss 62.20 
2024-02-05 19:26:52,733 EPOCH 81
2024-02-05 19:26:55,679 [Epoch: 081 Step: 00010700] Batch Recognition Loss:   0.003786 => Gls Tokens per Sec:     1630 || Batch Translation Loss:   0.256425 => Txt Tokens per Sec:     4413 || Lr: 0.000100
2024-02-05 19:26:59,928 Epoch  81: Total Training Recognition Loss 1.42  Total Training Translation Loss 62.72 
2024-02-05 19:26:59,929 EPOCH 82
2024-02-05 19:27:01,267 [Epoch: 082 Step: 00010800] Batch Recognition Loss:   0.009216 => Gls Tokens per Sec:     1614 || Batch Translation Loss:   0.239268 => Txt Tokens per Sec:     4137 || Lr: 0.000100
2024-02-05 19:27:06,685 [Epoch: 082 Step: 00010900] Batch Recognition Loss:   0.005880 => Gls Tokens per Sec:     1473 || Batch Translation Loss:   0.827575 => Txt Tokens per Sec:     4119 || Lr: 0.000100
2024-02-05 19:27:06,996 Epoch  82: Total Training Recognition Loss 1.62  Total Training Translation Loss 72.76 
2024-02-05 19:27:06,996 EPOCH 83
2024-02-05 19:27:12,210 [Epoch: 083 Step: 00011000] Batch Recognition Loss:   0.001157 => Gls Tokens per Sec:     1442 || Batch Translation Loss:   0.584644 => Txt Tokens per Sec:     3998 || Lr: 0.000100
2024-02-05 19:27:14,432 Epoch  83: Total Training Recognition Loss 1.38  Total Training Translation Loss 70.91 
2024-02-05 19:27:14,433 EPOCH 84
2024-02-05 19:27:18,085 [Epoch: 084 Step: 00011100] Batch Recognition Loss:   0.021870 => Gls Tokens per Sec:     1337 || Batch Translation Loss:   0.328556 => Txt Tokens per Sec:     3753 || Lr: 0.000100
2024-02-05 19:27:21,982 Epoch  84: Total Training Recognition Loss 1.55  Total Training Translation Loss 54.85 
2024-02-05 19:27:21,982 EPOCH 85
2024-02-05 19:27:23,296 [Epoch: 085 Step: 00011200] Batch Recognition Loss:   0.000674 => Gls Tokens per Sec:     1706 || Batch Translation Loss:   0.297581 => Txt Tokens per Sec:     4897 || Lr: 0.000100
2024-02-05 19:27:29,031 [Epoch: 085 Step: 00011300] Batch Recognition Loss:   0.115891 => Gls Tokens per Sec:     1392 || Batch Translation Loss:   0.284902 => Txt Tokens per Sec:     3821 || Lr: 0.000100
2024-02-05 19:27:29,265 Epoch  85: Total Training Recognition Loss 1.24  Total Training Translation Loss 57.09 
2024-02-05 19:27:29,265 EPOCH 86
2024-02-05 19:27:34,368 [Epoch: 086 Step: 00011400] Batch Recognition Loss:   0.006540 => Gls Tokens per Sec:     1490 || Batch Translation Loss:   0.680838 => Txt Tokens per Sec:     4115 || Lr: 0.000100
2024-02-05 19:27:36,479 Epoch  86: Total Training Recognition Loss 1.23  Total Training Translation Loss 58.42 
2024-02-05 19:27:36,480 EPOCH 87
2024-02-05 19:27:40,064 [Epoch: 087 Step: 00011500] Batch Recognition Loss:   0.001482 => Gls Tokens per Sec:     1384 || Batch Translation Loss:   0.239170 => Txt Tokens per Sec:     3882 || Lr: 0.000100
2024-02-05 19:27:43,923 Epoch  87: Total Training Recognition Loss 1.59  Total Training Translation Loss 71.32 
2024-02-05 19:27:43,923 EPOCH 88
2024-02-05 19:27:45,533 [Epoch: 088 Step: 00011600] Batch Recognition Loss:   0.013287 => Gls Tokens per Sec:     1442 || Batch Translation Loss:   0.355821 => Txt Tokens per Sec:     3924 || Lr: 0.000100
2024-02-05 19:27:51,020 [Epoch: 088 Step: 00011700] Batch Recognition Loss:   0.006866 => Gls Tokens per Sec:     1455 || Batch Translation Loss:   0.458386 => Txt Tokens per Sec:     4047 || Lr: 0.000100
2024-02-05 19:27:51,218 Epoch  88: Total Training Recognition Loss 1.41  Total Training Translation Loss 66.89 
2024-02-05 19:27:51,219 EPOCH 89
2024-02-05 19:27:56,397 [Epoch: 089 Step: 00011800] Batch Recognition Loss:   0.001591 => Gls Tokens per Sec:     1483 || Batch Translation Loss:   0.166180 => Txt Tokens per Sec:     4093 || Lr: 0.000100
2024-02-05 19:27:58,592 Epoch  89: Total Training Recognition Loss 1.54  Total Training Translation Loss 59.93 
2024-02-05 19:27:58,592 EPOCH 90
2024-02-05 19:28:02,159 [Epoch: 090 Step: 00011900] Batch Recognition Loss:   0.012697 => Gls Tokens per Sec:     1413 || Batch Translation Loss:   0.383623 => Txt Tokens per Sec:     3914 || Lr: 0.000100
2024-02-05 19:28:06,042 Epoch  90: Total Training Recognition Loss 1.08  Total Training Translation Loss 56.01 
2024-02-05 19:28:06,043 EPOCH 91
2024-02-05 19:28:07,619 [Epoch: 091 Step: 00012000] Batch Recognition Loss:   0.007416 => Gls Tokens per Sec:     1524 || Batch Translation Loss:   0.281402 => Txt Tokens per Sec:     3989 || Lr: 0.000100
2024-02-05 19:28:18,369 Validation result at epoch  91, step    12000: duration: 10.7490s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 6.20881	Translation Loss: 87324.04688	PPL: 6136.06006
	Eval Metric: BLEU
	WER 4.94	(DEL: 0.00,	INS: 0.00,	SUB: 4.94)
	BLEU-4 0.65	(BLEU-1: 11.18,	BLEU-2: 3.56,	BLEU-3: 1.37,	BLEU-4: 0.65)
	CHRF 17.39	ROUGE 9.49
2024-02-05 19:28:18,370 Logging Recognition and Translation Outputs
2024-02-05 19:28:18,371 ========================================================================================================================
2024-02-05 19:28:18,371 Logging Sequence: 168_56.00
2024-02-05 19:28:18,371 	Gloss Reference :	A B+C+D+E
2024-02-05 19:28:18,371 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:28:18,371 	Gloss Alignment :	         
2024-02-05 19:28:18,371 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:28:18,372 	Text Reference  :	fans have been  waiting to ****** ***** see   vamika for  a   long time      
2024-02-05 19:28:18,373 	Text Hypothesis :	**** they don't want    to change their child but    they are very protective
2024-02-05 19:28:18,373 	Text Alignment  :	D    S    S     S          I      I     S     S      S    S   S    S         
2024-02-05 19:28:18,373 ========================================================================================================================
2024-02-05 19:28:18,373 Logging Sequence: 161_74.00
2024-02-05 19:28:18,373 	Gloss Reference :	A B+C+D+E
2024-02-05 19:28:18,373 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:28:18,373 	Gloss Alignment :	         
2024-02-05 19:28:18,373 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:28:18,374 	Text Reference  :	*** ******** *** **** i  am        proud of       the     indian team's achievements
2024-02-05 19:28:18,374 	Text Hypothesis :	the incident was held in australia for   mandhana between india  and    kuwait      
2024-02-05 19:28:18,374 	Text Alignment  :	I   I        I   I    S  S         S     S        S       S      S      S           
2024-02-05 19:28:18,374 ========================================================================================================================
2024-02-05 19:28:18,375 Logging Sequence: 111_83.00
2024-02-05 19:28:18,375 	Gloss Reference :	A B+C+D+E
2024-02-05 19:28:18,375 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:28:18,375 	Gloss Alignment :	         
2024-02-05 19:28:18,375 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:28:18,376 	Text Reference  :	** **** and   the  other 10    team members are fined 25 of    the match fee or rs 6 lakh  
2024-02-05 19:28:18,377 	Text Hypothesis :	on 10th april 2022 deaf  women team ******* was able  to reach the ***** *** ** ** * finals
2024-02-05 19:28:18,377 	Text Alignment  :	I  I    S     S    S     S          D       S   S     S  S         D     D   D  D  D S     
2024-02-05 19:28:18,377 ========================================================================================================================
2024-02-05 19:28:18,377 Logging Sequence: 61_218.00
2024-02-05 19:28:18,377 	Gloss Reference :	A B+C+D+E
2024-02-05 19:28:18,377 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:28:18,377 	Gloss Alignment :	         
2024-02-05 19:28:18,378 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:28:18,378 	Text Reference  :	in   2020    a  woman had said at     the   press    conference
2024-02-05 19:28:18,378 	Text Hypothesis :	they rivalry is seen  the most during india pakistan cricket   
2024-02-05 19:28:18,379 	Text Alignment  :	S    S       S  S     S   S    S      S     S        S         
2024-02-05 19:28:18,379 ========================================================================================================================
2024-02-05 19:28:18,379 Logging Sequence: 94_123.00
2024-02-05 19:28:18,379 	Gloss Reference :	A B+C+D+E
2024-02-05 19:28:18,379 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:28:18,379 	Gloss Alignment :	         
2024-02-05 19:28:18,379 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:28:18,381 	Text Reference  :	the venue narendra modi stadium for the india-pakistan match has      been kept the same people can book  flights etc       
2024-02-05 19:28:18,381 	Text Hypothesis :	*** ***** icc      has  laws    for the finals         of    athletes room for  the **** ****** *** india has     devastated
2024-02-05 19:28:18,381 	Text Alignment  :	D   D     S        S    S               S              S     S        S    S        D    D      D   S     S       S         
2024-02-05 19:28:18,381 ========================================================================================================================
2024-02-05 19:28:24,075 [Epoch: 091 Step: 00012100] Batch Recognition Loss:   0.001299 => Gls Tokens per Sec:     1399 || Batch Translation Loss:   0.306255 => Txt Tokens per Sec:     3947 || Lr: 0.000100
2024-02-05 19:28:24,214 Epoch  91: Total Training Recognition Loss 1.49  Total Training Translation Loss 54.38 
2024-02-05 19:28:24,214 EPOCH 92
2024-02-05 19:28:29,494 [Epoch: 092 Step: 00012200] Batch Recognition Loss:   0.000874 => Gls Tokens per Sec:     1470 || Batch Translation Loss:   0.218672 => Txt Tokens per Sec:     4090 || Lr: 0.000100
2024-02-05 19:28:31,624 Epoch  92: Total Training Recognition Loss 1.56  Total Training Translation Loss 53.44 
2024-02-05 19:28:31,625 EPOCH 93
2024-02-05 19:28:35,278 [Epoch: 093 Step: 00012300] Batch Recognition Loss:   0.009591 => Gls Tokens per Sec:     1402 || Batch Translation Loss:   1.146510 => Txt Tokens per Sec:     3857 || Lr: 0.000100
2024-02-05 19:28:38,953 Epoch  93: Total Training Recognition Loss 1.14  Total Training Translation Loss 49.93 
2024-02-05 19:28:38,953 EPOCH 94
2024-02-05 19:28:40,428 [Epoch: 094 Step: 00012400] Batch Recognition Loss:   0.004012 => Gls Tokens per Sec:     1683 || Batch Translation Loss:   0.282664 => Txt Tokens per Sec:     4615 || Lr: 0.000100
2024-02-05 19:28:45,958 [Epoch: 094 Step: 00012500] Batch Recognition Loss:   0.024007 => Gls Tokens per Sec:     1443 || Batch Translation Loss:   0.342439 => Txt Tokens per Sec:     4012 || Lr: 0.000100
2024-02-05 19:28:46,064 Epoch  94: Total Training Recognition Loss 1.23  Total Training Translation Loss 69.76 
2024-02-05 19:28:46,064 EPOCH 95
2024-02-05 19:28:51,105 [Epoch: 095 Step: 00012600] Batch Recognition Loss:   0.020993 => Gls Tokens per Sec:     1556 || Batch Translation Loss:   0.614361 => Txt Tokens per Sec:     4295 || Lr: 0.000100
2024-02-05 19:28:53,300 Epoch  95: Total Training Recognition Loss 1.38  Total Training Translation Loss 61.00 
2024-02-05 19:28:53,301 EPOCH 96
2024-02-05 19:28:57,118 [Epoch: 096 Step: 00012700] Batch Recognition Loss:   0.002730 => Gls Tokens per Sec:     1363 || Batch Translation Loss:   0.533607 => Txt Tokens per Sec:     3802 || Lr: 0.000100
2024-02-05 19:29:00,726 Epoch  96: Total Training Recognition Loss 1.22  Total Training Translation Loss 47.73 
2024-02-05 19:29:00,726 EPOCH 97
2024-02-05 19:29:02,760 [Epoch: 097 Step: 00012800] Batch Recognition Loss:   0.001684 => Gls Tokens per Sec:     1260 || Batch Translation Loss:   0.152884 => Txt Tokens per Sec:     3593 || Lr: 0.000100
2024-02-05 19:29:08,180 [Epoch: 097 Step: 00012900] Batch Recognition Loss:   0.016979 => Gls Tokens per Sec:     1472 || Batch Translation Loss:   0.352363 => Txt Tokens per Sec:     4039 || Lr: 0.000100
2024-02-05 19:29:08,233 Epoch  97: Total Training Recognition Loss 0.94  Total Training Translation Loss 42.53 
2024-02-05 19:29:08,233 EPOCH 98
2024-02-05 19:29:13,342 [Epoch: 098 Step: 00013000] Batch Recognition Loss:   0.000613 => Gls Tokens per Sec:     1550 || Batch Translation Loss:   0.167570 => Txt Tokens per Sec:     4248 || Lr: 0.000100
2024-02-05 19:29:15,277 Epoch  98: Total Training Recognition Loss 1.20  Total Training Translation Loss 53.59 
2024-02-05 19:29:15,277 EPOCH 99
2024-02-05 19:29:18,831 [Epoch: 099 Step: 00013100] Batch Recognition Loss:   0.006574 => Gls Tokens per Sec:     1486 || Batch Translation Loss:   0.173782 => Txt Tokens per Sec:     4075 || Lr: 0.000100
2024-02-05 19:29:22,724 Epoch  99: Total Training Recognition Loss 1.20  Total Training Translation Loss 49.24 
2024-02-05 19:29:22,725 EPOCH 100
2024-02-05 19:29:24,456 [Epoch: 100 Step: 00013200] Batch Recognition Loss:   0.001883 => Gls Tokens per Sec:     1526 || Batch Translation Loss:   0.328272 => Txt Tokens per Sec:     4185 || Lr: 0.000100
2024-02-05 19:29:29,923 [Epoch: 100 Step: 00013300] Batch Recognition Loss:   0.007463 => Gls Tokens per Sec:     1460 || Batch Translation Loss:   0.507813 => Txt Tokens per Sec:     4050 || Lr: 0.000100
2024-02-05 19:29:29,923 Epoch 100: Total Training Recognition Loss 1.09  Total Training Translation Loss 54.03 
2024-02-05 19:29:29,923 EPOCH 101
2024-02-05 19:29:35,329 [Epoch: 101 Step: 00013400] Batch Recognition Loss:   0.003135 => Gls Tokens per Sec:     1480 || Batch Translation Loss:   0.113429 => Txt Tokens per Sec:     4090 || Lr: 0.000100
2024-02-05 19:29:37,203 Epoch 101: Total Training Recognition Loss 1.12  Total Training Translation Loss 57.07 
2024-02-05 19:29:37,203 EPOCH 102
2024-02-05 19:29:41,050 [Epoch: 102 Step: 00013500] Batch Recognition Loss:   0.005683 => Gls Tokens per Sec:     1394 || Batch Translation Loss:   0.308886 => Txt Tokens per Sec:     3888 || Lr: 0.000100
2024-02-05 19:29:44,705 Epoch 102: Total Training Recognition Loss 1.40  Total Training Translation Loss 48.19 
2024-02-05 19:29:44,705 EPOCH 103
2024-02-05 19:29:46,506 [Epoch: 103 Step: 00013600] Batch Recognition Loss:   0.027118 => Gls Tokens per Sec:     1511 || Batch Translation Loss:   0.259160 => Txt Tokens per Sec:     4262 || Lr: 0.000100
2024-02-05 19:29:52,205 Epoch 103: Total Training Recognition Loss 1.29  Total Training Translation Loss 36.63 
2024-02-05 19:29:52,205 EPOCH 104
2024-02-05 19:29:52,255 [Epoch: 104 Step: 00013700] Batch Recognition Loss:   0.000544 => Gls Tokens per Sec:     1667 || Batch Translation Loss:   0.108321 => Txt Tokens per Sec:     3979 || Lr: 0.000100
2024-02-05 19:29:57,543 [Epoch: 104 Step: 00013800] Batch Recognition Loss:   0.004206 => Gls Tokens per Sec:     1513 || Batch Translation Loss:   0.307678 => Txt Tokens per Sec:     4185 || Lr: 0.000100
2024-02-05 19:29:59,250 Epoch 104: Total Training Recognition Loss 1.29  Total Training Translation Loss 41.14 
2024-02-05 19:29:59,251 EPOCH 105
2024-02-05 19:30:03,218 [Epoch: 105 Step: 00013900] Batch Recognition Loss:   0.015876 => Gls Tokens per Sec:     1372 || Batch Translation Loss:   0.334841 => Txt Tokens per Sec:     3800 || Lr: 0.000100
2024-02-05 19:30:06,832 Epoch 105: Total Training Recognition Loss 1.28  Total Training Translation Loss 52.55 
2024-02-05 19:30:06,832 EPOCH 106
2024-02-05 19:30:08,693 [Epoch: 106 Step: 00014000] Batch Recognition Loss:   0.004924 => Gls Tokens per Sec:     1505 || Batch Translation Loss:   0.111230 => Txt Tokens per Sec:     4220 || Lr: 0.000100
2024-02-05 19:30:19,700 Validation result at epoch 106, step    14000: duration: 11.0067s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 5.99798	Translation Loss: 90039.48438	PPL: 8047.81934
	Eval Metric: BLEU
	WER 4.52	(DEL: 0.00,	INS: 0.00,	SUB: 4.52)
	BLEU-4 0.61	(BLEU-1: 11.34,	BLEU-2: 3.35,	BLEU-3: 1.26,	BLEU-4: 0.61)
	CHRF 17.08	ROUGE 9.18
2024-02-05 19:30:19,701 Logging Recognition and Translation Outputs
2024-02-05 19:30:19,701 ========================================================================================================================
2024-02-05 19:30:19,702 Logging Sequence: 177_50.00
2024-02-05 19:30:19,702 	Gloss Reference :	A B+C+D+E
2024-02-05 19:30:19,702 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:30:19,702 	Gloss Alignment :	         
2024-02-05 19:30:19,702 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:30:19,704 	Text Reference  :	***** a     similar reward of    rs    50000  was announced    for    information against his associate ajay   kumar
2024-02-05 19:30:19,704 	Text Hypothesis :	after rana' death   a      delhi court issued a   non-bailable arrest warrant     against *** ********* sushil kumar
2024-02-05 19:30:19,704 	Text Alignment  :	I     S     S       S      S     S     S      S   S            S      S                   D   D         S           
2024-02-05 19:30:19,704 ========================================================================================================================
2024-02-05 19:30:19,704 Logging Sequence: 136_175.00
2024-02-05 19:30:19,705 	Gloss Reference :	A B+C+D+E
2024-02-05 19:30:19,705 	Gloss Hypothesis:	A B+C+D  
2024-02-05 19:30:19,705 	Gloss Alignment :	  S      
2024-02-05 19:30:19,705 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:30:19,706 	Text Reference  :	*** *** after 49 years  india' hockey team beat britain and      qualified for the semi-finals
2024-02-05 19:30:19,706 	Text Hypothesis :	she had won   a  bronze medal  at     the  2012 london  olympics in        rio de  janeiro    
2024-02-05 19:30:19,706 	Text Alignment  :	I   I   S     S  S      S      S      S    S    S       S        S         S   S   S          
2024-02-05 19:30:19,707 ========================================================================================================================
2024-02-05 19:30:19,707 Logging Sequence: 126_159.00
2024-02-05 19:30:19,707 	Gloss Reference :	A B+C+D+E
2024-02-05 19:30:19,707 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:30:19,707 	Gloss Alignment :	         
2024-02-05 19:30:19,707 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:30:19,708 	Text Reference  :	despite multiple challenges and injuries you  did  not give up           
2024-02-05 19:30:19,708 	Text Hypothesis :	******* ******** ********** you all      know that a   very disappointing
2024-02-05 19:30:19,708 	Text Alignment  :	D       D        D          S   S        S    S    S   S    S            
2024-02-05 19:30:19,708 ========================================================================================================================
2024-02-05 19:30:19,708 Logging Sequence: 70_88.00
2024-02-05 19:30:19,708 	Gloss Reference :	A B+C+D+E
2024-02-05 19:30:19,709 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:30:19,709 	Gloss Alignment :	         
2024-02-05 19:30:19,709 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:30:19,710 	Text Reference  :	two coca-cola bottles were placed   on the table next to   the      mic 
2024-02-05 19:30:19,710 	Text Hypothesis :	*** ********* ronaldo has  admitted to the ***** **** most football team
2024-02-05 19:30:19,710 	Text Alignment  :	D   D         S       S    S        S      D     D    S    S        S   
2024-02-05 19:30:19,710 ========================================================================================================================
2024-02-05 19:30:19,710 Logging Sequence: 54_201.00
2024-02-05 19:30:19,710 	Gloss Reference :	A B+C+D+E
2024-02-05 19:30:19,710 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:30:19,710 	Gloss Alignment :	         
2024-02-05 19:30:19,711 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:30:19,714 	Text Reference  :	there is  a            huge demand  mostly from non-resident indians nris   who       are excited      to see   the     match and     they have booked the       hotel ******* ***** rooms
2024-02-05 19:30:19,714 	Text Hypothesis :	***** the middle-class fans usually decide at   the          last    moment depending on  availability of match tickets if    tickets are  not  be     available hotel booking would win  
2024-02-05 19:30:19,714 	Text Alignment  :	D     S   S            S    S       S      S    S            S       S      S         S   S            S  S     S       S     S       S    S    S      S               I       I     S    
2024-02-05 19:30:19,714 ========================================================================================================================
2024-02-05 19:30:25,226 Epoch 106: Total Training Recognition Loss 0.90  Total Training Translation Loss 49.27 
2024-02-05 19:30:25,226 EPOCH 107
2024-02-05 19:30:25,312 [Epoch: 107 Step: 00014100] Batch Recognition Loss:   0.006427 => Gls Tokens per Sec:     1882 || Batch Translation Loss:   0.436750 => Txt Tokens per Sec:     4271 || Lr: 0.000100
2024-02-05 19:30:30,678 [Epoch: 107 Step: 00014200] Batch Recognition Loss:   0.057529 => Gls Tokens per Sec:     1487 || Batch Translation Loss:   1.061386 => Txt Tokens per Sec:     4108 || Lr: 0.000100
2024-02-05 19:30:32,460 Epoch 107: Total Training Recognition Loss 1.11  Total Training Translation Loss 46.24 
2024-02-05 19:30:32,461 EPOCH 108
2024-02-05 19:30:36,230 [Epoch: 108 Step: 00014300] Batch Recognition Loss:   0.006004 => Gls Tokens per Sec:     1465 || Batch Translation Loss:   0.201093 => Txt Tokens per Sec:     4114 || Lr: 0.000100
2024-02-05 19:30:39,895 Epoch 108: Total Training Recognition Loss 1.16  Total Training Translation Loss 46.25 
2024-02-05 19:30:39,895 EPOCH 109
2024-02-05 19:30:41,875 [Epoch: 109 Step: 00014400] Batch Recognition Loss:   0.011458 => Gls Tokens per Sec:     1457 || Batch Translation Loss:   0.463274 => Txt Tokens per Sec:     3991 || Lr: 0.000100
2024-02-05 19:30:47,150 Epoch 109: Total Training Recognition Loss 1.09  Total Training Translation Loss 41.67 
2024-02-05 19:30:47,151 EPOCH 110
2024-02-05 19:30:47,268 [Epoch: 110 Step: 00014500] Batch Recognition Loss:   0.002435 => Gls Tokens per Sec:     2051 || Batch Translation Loss:   0.122849 => Txt Tokens per Sec:     4966 || Lr: 0.000100
2024-02-05 19:30:52,228 [Epoch: 110 Step: 00014600] Batch Recognition Loss:   0.003535 => Gls Tokens per Sec:     1613 || Batch Translation Loss:   0.196937 => Txt Tokens per Sec:     4464 || Lr: 0.000100
2024-02-05 19:30:54,057 Epoch 110: Total Training Recognition Loss 1.05  Total Training Translation Loss 37.38 
2024-02-05 19:30:54,058 EPOCH 111
2024-02-05 19:30:57,731 [Epoch: 111 Step: 00014700] Batch Recognition Loss:   0.004897 => Gls Tokens per Sec:     1525 || Batch Translation Loss:   0.108216 => Txt Tokens per Sec:     4184 || Lr: 0.000100
2024-02-05 19:31:01,301 Epoch 111: Total Training Recognition Loss 1.08  Total Training Translation Loss 47.72 
2024-02-05 19:31:01,301 EPOCH 112
2024-02-05 19:31:03,432 [Epoch: 112 Step: 00014800] Batch Recognition Loss:   0.042154 => Gls Tokens per Sec:     1390 || Batch Translation Loss:   0.183035 => Txt Tokens per Sec:     3816 || Lr: 0.000100
2024-02-05 19:31:09,065 Epoch 112: Total Training Recognition Loss 1.40  Total Training Translation Loss 43.70 
2024-02-05 19:31:09,065 EPOCH 113
2024-02-05 19:31:09,256 [Epoch: 113 Step: 00014900] Batch Recognition Loss:   0.001947 => Gls Tokens per Sec:     1684 || Batch Translation Loss:   0.108962 => Txt Tokens per Sec:     4542 || Lr: 0.000100
2024-02-05 19:31:14,534 [Epoch: 113 Step: 00015000] Batch Recognition Loss:   0.000653 => Gls Tokens per Sec:     1516 || Batch Translation Loss:   0.258125 => Txt Tokens per Sec:     4189 || Lr: 0.000100
2024-02-05 19:31:16,116 Epoch 113: Total Training Recognition Loss 1.13  Total Training Translation Loss 45.63 
2024-02-05 19:31:16,117 EPOCH 114
2024-02-05 19:31:19,749 [Epoch: 114 Step: 00015100] Batch Recognition Loss:   0.002642 => Gls Tokens per Sec:     1564 || Batch Translation Loss:   0.342585 => Txt Tokens per Sec:     4352 || Lr: 0.000100
2024-02-05 19:31:23,279 Epoch 114: Total Training Recognition Loss 1.04  Total Training Translation Loss 38.51 
2024-02-05 19:31:23,279 EPOCH 115
2024-02-05 19:31:25,431 [Epoch: 115 Step: 00015200] Batch Recognition Loss:   0.001322 => Gls Tokens per Sec:     1413 || Batch Translation Loss:   0.277618 => Txt Tokens per Sec:     3887 || Lr: 0.000100
2024-02-05 19:31:30,672 Epoch 115: Total Training Recognition Loss 0.99  Total Training Translation Loss 42.79 
2024-02-05 19:31:30,673 EPOCH 116
2024-02-05 19:31:30,972 [Epoch: 116 Step: 00015300] Batch Recognition Loss:   0.002975 => Gls Tokens per Sec:     1342 || Batch Translation Loss:   0.220549 => Txt Tokens per Sec:     4322 || Lr: 0.000100
2024-02-05 19:31:36,452 [Epoch: 116 Step: 00015400] Batch Recognition Loss:   0.000786 => Gls Tokens per Sec:     1456 || Batch Translation Loss:   0.201532 => Txt Tokens per Sec:     4002 || Lr: 0.000100
2024-02-05 19:31:38,098 Epoch 116: Total Training Recognition Loss 1.00  Total Training Translation Loss 44.09 
2024-02-05 19:31:38,099 EPOCH 117
2024-02-05 19:31:41,911 [Epoch: 117 Step: 00015500] Batch Recognition Loss:   0.001717 => Gls Tokens per Sec:     1511 || Batch Translation Loss:   0.200444 => Txt Tokens per Sec:     4156 || Lr: 0.000100
2024-02-05 19:31:45,511 Epoch 117: Total Training Recognition Loss 0.83  Total Training Translation Loss 39.16 
2024-02-05 19:31:45,511 EPOCH 118
2024-02-05 19:31:47,733 [Epoch: 118 Step: 00015600] Batch Recognition Loss:   0.002941 => Gls Tokens per Sec:     1405 || Batch Translation Loss:   0.411713 => Txt Tokens per Sec:     3979 || Lr: 0.000100
2024-02-05 19:31:52,955 Epoch 118: Total Training Recognition Loss 1.13  Total Training Translation Loss 41.96 
2024-02-05 19:31:52,955 EPOCH 119
2024-02-05 19:31:53,213 [Epoch: 119 Step: 00015700] Batch Recognition Loss:   0.001301 => Gls Tokens per Sec:     1873 || Batch Translation Loss:   0.303045 => Txt Tokens per Sec:     4578 || Lr: 0.000100
2024-02-05 19:31:58,232 [Epoch: 119 Step: 00015800] Batch Recognition Loss:   0.003015 => Gls Tokens per Sec:     1594 || Batch Translation Loss:   0.120365 => Txt Tokens per Sec:     4438 || Lr: 0.000100
2024-02-05 19:31:59,996 Epoch 119: Total Training Recognition Loss 1.07  Total Training Translation Loss 34.42 
2024-02-05 19:31:59,996 EPOCH 120
2024-02-05 19:32:04,225 [Epoch: 120 Step: 00015900] Batch Recognition Loss:   0.009592 => Gls Tokens per Sec:     1381 || Batch Translation Loss:   0.084605 => Txt Tokens per Sec:     3839 || Lr: 0.000100
2024-02-05 19:32:07,489 Epoch 120: Total Training Recognition Loss 0.92  Total Training Translation Loss 30.72 
2024-02-05 19:32:07,489 EPOCH 121
2024-02-05 19:32:09,790 [Epoch: 121 Step: 00016000] Batch Recognition Loss:   0.006432 => Gls Tokens per Sec:     1392 || Batch Translation Loss:   0.064303 => Txt Tokens per Sec:     3851 || Lr: 0.000100
2024-02-05 19:32:21,836 Validation result at epoch 121, step    16000: duration: 12.0461s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 5.72533	Translation Loss: 91201.91406	PPL: 9038.60059
	Eval Metric: BLEU
	WER 4.94	(DEL: 0.00,	INS: 0.00,	SUB: 4.94)
	BLEU-4 0.76	(BLEU-1: 11.72,	BLEU-2: 3.76,	BLEU-3: 1.54,	BLEU-4: 0.76)
	CHRF 17.76	ROUGE 9.71
2024-02-05 19:32:21,838 Logging Recognition and Translation Outputs
2024-02-05 19:32:21,838 ========================================================================================================================
2024-02-05 19:32:21,838 Logging Sequence: 163_116.00
2024-02-05 19:32:21,838 	Gloss Reference :	A B+C+D+E
2024-02-05 19:32:21,838 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:32:21,838 	Gloss Alignment :	         
2024-02-05 19:32:21,839 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:32:21,839 	Text Reference  :	people said that  she  looked similar to     virat
2024-02-05 19:32:21,839 	Text Hypothesis :	****** who  would even post   on      social media
2024-02-05 19:32:21,839 	Text Alignment  :	D      S    S     S    S      S       S      S    
2024-02-05 19:32:21,840 ========================================================================================================================
2024-02-05 19:32:21,840 Logging Sequence: 53_161.00
2024-02-05 19:32:21,840 	Gloss Reference :	A B+C+D+E
2024-02-05 19:32:21,840 	Gloss Hypothesis:	A B+C+D  
2024-02-05 19:32:21,840 	Gloss Alignment :	  S      
2024-02-05 19:32:21,840 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:32:21,842 	Text Reference  :	********* ********* rashid    has also been urging people to          donate to his rashid khan foundation and   afghanistan cricket association
2024-02-05 19:32:21,842 	Text Hypothesis :	meanwhile sunrisers hyderabad has **** said that   the    rescheduled runs   to *** ****** **** ********** reach the         trent   rockets    
2024-02-05 19:32:21,842 	Text Alignment  :	I         I         S             D    S    S      S      S           S         D   D      D    D          S     S           S       S          
2024-02-05 19:32:21,842 ========================================================================================================================
2024-02-05 19:32:21,843 Logging Sequence: 67_73.00
2024-02-05 19:32:21,843 	Gloss Reference :	A B+C+D+E
2024-02-05 19:32:21,843 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:32:21,843 	Gloss Alignment :	         
2024-02-05 19:32:21,843 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:32:21,844 	Text Reference  :	*** **** *** *** **** ** in   his   tweet he also  said  
2024-02-05 19:32:21,844 	Text Hypothesis :	but this can not want to have their names of their jersey
2024-02-05 19:32:21,844 	Text Alignment  :	I   I    I   I   I    I  S    S     S     S  S     S     
2024-02-05 19:32:21,844 ========================================================================================================================
2024-02-05 19:32:21,844 Logging Sequence: 137_44.00
2024-02-05 19:32:21,845 	Gloss Reference :	A B+C+D+E
2024-02-05 19:32:21,845 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:32:21,845 	Gloss Alignment :	         
2024-02-05 19:32:21,845 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:32:21,847 	Text Reference  :	let me tell you     the    rules that   qatar has announced for  the  fans travelling for     the world cup  
2024-02-05 19:32:21,847 	Text Hypothesis :	*** ** **** anushka sharma and   social media was taking    more than his  father     between the ***** hotel
2024-02-05 19:32:21,847 	Text Alignment  :	D   D  D    S       S      S     S      S     S   S         S    S    S    S          S           D     S    
2024-02-05 19:32:21,847 ========================================================================================================================
2024-02-05 19:32:21,847 Logging Sequence: 99_158.00
2024-02-05 19:32:21,848 	Gloss Reference :	A B+C+D+E
2024-02-05 19:32:21,848 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:32:21,848 	Gloss Alignment :	         
2024-02-05 19:32:21,848 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:32:21,849 	Text Reference  :	the incident occured in   dubai and it     was **** ******* ** extremely shameful
2024-02-05 19:32:21,849 	Text Hypothesis :	*** isn't    that    even since his family was very support of his       support 
2024-02-05 19:32:21,849 	Text Alignment  :	D   S        S       S    S     S   S          I    I       I  S         S       
2024-02-05 19:32:21,849 ========================================================================================================================
2024-02-05 19:32:26,978 Epoch 121: Total Training Recognition Loss 0.73  Total Training Translation Loss 32.19 
2024-02-05 19:32:26,978 EPOCH 122
2024-02-05 19:32:27,340 [Epoch: 122 Step: 00016100] Batch Recognition Loss:   0.001336 => Gls Tokens per Sec:     1556 || Batch Translation Loss:   0.104566 => Txt Tokens per Sec:     3714 || Lr: 0.000100
2024-02-05 19:32:33,055 [Epoch: 122 Step: 00016200] Batch Recognition Loss:   0.011885 => Gls Tokens per Sec:     1400 || Batch Translation Loss:   0.111233 => Txt Tokens per Sec:     3901 || Lr: 0.000100
2024-02-05 19:32:34,586 Epoch 122: Total Training Recognition Loss 0.95  Total Training Translation Loss 36.03 
2024-02-05 19:32:34,586 EPOCH 123
2024-02-05 19:32:38,849 [Epoch: 123 Step: 00016300] Batch Recognition Loss:   0.001784 => Gls Tokens per Sec:     1389 || Batch Translation Loss:   0.842671 => Txt Tokens per Sec:     3865 || Lr: 0.000100
2024-02-05 19:32:42,155 Epoch 123: Total Training Recognition Loss 1.02  Total Training Translation Loss 50.83 
2024-02-05 19:32:42,155 EPOCH 124
2024-02-05 19:32:44,318 [Epoch: 124 Step: 00016400] Batch Recognition Loss:   0.004427 => Gls Tokens per Sec:     1518 || Batch Translation Loss:   0.308593 => Txt Tokens per Sec:     4162 || Lr: 0.000100
2024-02-05 19:32:49,710 Epoch 124: Total Training Recognition Loss 1.21  Total Training Translation Loss 51.45 
2024-02-05 19:32:49,710 EPOCH 125
2024-02-05 19:32:50,098 [Epoch: 125 Step: 00016500] Batch Recognition Loss:   0.000847 => Gls Tokens per Sec:     1658 || Batch Translation Loss:   0.513737 => Txt Tokens per Sec:     4516 || Lr: 0.000100
2024-02-05 19:32:55,159 [Epoch: 125 Step: 00016600] Batch Recognition Loss:   0.007734 => Gls Tokens per Sec:     1581 || Batch Translation Loss:   0.251806 => Txt Tokens per Sec:     4360 || Lr: 0.000100
2024-02-05 19:32:56,710 Epoch 125: Total Training Recognition Loss 0.96  Total Training Translation Loss 46.69 
2024-02-05 19:32:56,710 EPOCH 126
2024-02-05 19:33:00,974 [Epoch: 126 Step: 00016700] Batch Recognition Loss:   0.001814 => Gls Tokens per Sec:     1408 || Batch Translation Loss:   0.718652 => Txt Tokens per Sec:     3902 || Lr: 0.000100
2024-02-05 19:33:04,275 Epoch 126: Total Training Recognition Loss 0.89  Total Training Translation Loss 35.00 
2024-02-05 19:33:04,275 EPOCH 127
2024-02-05 19:33:06,897 [Epoch: 127 Step: 00016800] Batch Recognition Loss:   0.002377 => Gls Tokens per Sec:     1283 || Batch Translation Loss:   0.190831 => Txt Tokens per Sec:     3623 || Lr: 0.000100
2024-02-05 19:33:11,713 Epoch 127: Total Training Recognition Loss 0.97  Total Training Translation Loss 33.36 
2024-02-05 19:33:11,713 EPOCH 128
2024-02-05 19:33:12,059 [Epoch: 128 Step: 00016900] Batch Recognition Loss:   0.001017 => Gls Tokens per Sec:     2081 || Batch Translation Loss:   0.127649 => Txt Tokens per Sec:     4636 || Lr: 0.000100
2024-02-05 19:33:17,746 [Epoch: 128 Step: 00017000] Batch Recognition Loss:   0.000596 => Gls Tokens per Sec:     1407 || Batch Translation Loss:   0.070600 => Txt Tokens per Sec:     3983 || Lr: 0.000100
2024-02-05 19:33:19,120 Epoch 128: Total Training Recognition Loss 0.99  Total Training Translation Loss 28.65 
2024-02-05 19:33:19,120 EPOCH 129
2024-02-05 19:33:22,937 [Epoch: 129 Step: 00017100] Batch Recognition Loss:   0.008180 => Gls Tokens per Sec:     1593 || Batch Translation Loss:   0.373488 => Txt Tokens per Sec:     4352 || Lr: 0.000100
2024-02-05 19:33:26,414 Epoch 129: Total Training Recognition Loss 0.72  Total Training Translation Loss 31.08 
2024-02-05 19:33:26,414 EPOCH 130
2024-02-05 19:33:28,863 [Epoch: 130 Step: 00017200] Batch Recognition Loss:   0.007317 => Gls Tokens per Sec:     1405 || Batch Translation Loss:   0.514864 => Txt Tokens per Sec:     3961 || Lr: 0.000100
2024-02-05 19:33:33,513 Epoch 130: Total Training Recognition Loss 0.89  Total Training Translation Loss 33.61 
2024-02-05 19:33:33,514 EPOCH 131
2024-02-05 19:33:34,168 [Epoch: 131 Step: 00017300] Batch Recognition Loss:   0.012828 => Gls Tokens per Sec:     1227 || Batch Translation Loss:   0.246008 => Txt Tokens per Sec:     3423 || Lr: 0.000100
2024-02-05 19:33:39,799 [Epoch: 131 Step: 00017400] Batch Recognition Loss:   0.011624 => Gls Tokens per Sec:     1421 || Batch Translation Loss:   0.083166 => Txt Tokens per Sec:     3919 || Lr: 0.000100
2024-02-05 19:33:41,155 Epoch 131: Total Training Recognition Loss 0.69  Total Training Translation Loss 29.37 
2024-02-05 19:33:41,155 EPOCH 132
2024-02-05 19:33:45,314 [Epoch: 132 Step: 00017500] Batch Recognition Loss:   0.033559 => Gls Tokens per Sec:     1481 || Batch Translation Loss:   0.569010 => Txt Tokens per Sec:     4054 || Lr: 0.000100
2024-02-05 19:33:48,508 Epoch 132: Total Training Recognition Loss 0.72  Total Training Translation Loss 42.24 
2024-02-05 19:33:48,508 EPOCH 133
2024-02-05 19:33:50,799 [Epoch: 133 Step: 00017600] Batch Recognition Loss:   0.001445 => Gls Tokens per Sec:     1537 || Batch Translation Loss:   0.226789 => Txt Tokens per Sec:     4388 || Lr: 0.000100
2024-02-05 19:33:55,936 Epoch 133: Total Training Recognition Loss 0.93  Total Training Translation Loss 43.58 
2024-02-05 19:33:55,936 EPOCH 134
2024-02-05 19:33:56,576 [Epoch: 134 Step: 00017700] Batch Recognition Loss:   0.004119 => Gls Tokens per Sec:     1377 || Batch Translation Loss:   0.169590 => Txt Tokens per Sec:     3922 || Lr: 0.000100
2024-02-05 19:34:01,654 [Epoch: 134 Step: 00017800] Batch Recognition Loss:   0.018500 => Gls Tokens per Sec:     1572 || Batch Translation Loss:   0.355973 => Txt Tokens per Sec:     4334 || Lr: 0.000100
2024-02-05 19:34:02,940 Epoch 134: Total Training Recognition Loss 0.64  Total Training Translation Loss 36.52 
2024-02-05 19:34:02,940 EPOCH 135
2024-02-05 19:34:07,342 [Epoch: 135 Step: 00017900] Batch Recognition Loss:   0.000332 => Gls Tokens per Sec:     1418 || Batch Translation Loss:   0.566239 => Txt Tokens per Sec:     3926 || Lr: 0.000100
2024-02-05 19:34:10,589 Epoch 135: Total Training Recognition Loss 0.75  Total Training Translation Loss 41.20 
2024-02-05 19:34:10,589 EPOCH 136
2024-02-05 19:34:13,099 [Epoch: 136 Step: 00018000] Batch Recognition Loss:   0.007281 => Gls Tokens per Sec:     1435 || Batch Translation Loss:   0.175598 => Txt Tokens per Sec:     3961 || Lr: 0.000100
2024-02-05 19:34:23,605 Validation result at epoch 136, step    18000: duration: 10.5053s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 6.85440	Translation Loss: 91452.40625	PPL: 9267.59570
	Eval Metric: BLEU
	WER 4.38	(DEL: 0.00,	INS: 0.00,	SUB: 4.38)
	BLEU-4 0.00	(BLEU-1: 11.19,	BLEU-2: 3.39,	BLEU-3: 1.05,	BLEU-4: 0.00)
	CHRF 17.35	ROUGE 9.25
2024-02-05 19:34:23,606 Logging Recognition and Translation Outputs
2024-02-05 19:34:23,606 ========================================================================================================================
2024-02-05 19:34:23,607 Logging Sequence: 179_309.00
2024-02-05 19:34:23,607 	Gloss Reference :	A B+C+D+E
2024-02-05 19:34:23,607 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:34:23,607 	Gloss Alignment :	         
2024-02-05 19:34:23,607 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:34:23,609 	Text Reference  :	before the ioa could send the notice wfi has asked phogat to explain her *** **** ** indiscipline
2024-02-05 19:34:23,609 	Text Hypothesis :	****** *** *** ***** **** *** if     she was not   travel to see     her ban will be extended    
2024-02-05 19:34:23,609 	Text Alignment  :	D      D   D   D     D    D   S      S   S   S     S         S           I   I    I  S           
2024-02-05 19:34:23,609 ========================================================================================================================
2024-02-05 19:34:23,609 Logging Sequence: 156_35.00
2024-02-05 19:34:23,609 	Gloss Reference :	A B+C+D+E
2024-02-05 19:34:23,609 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:34:23,610 	Gloss Alignment :	         
2024-02-05 19:34:23,610 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:34:23,612 	Text Reference  :	***** the first   season of mlc    began on  13th    july 2023 and ended on 30th july 2023 with six   teams      
2024-02-05 19:34:23,612 	Text Hypothesis :	after 2   players for    5  months they  got married in   2010 and ***** ** are  sold for  its  tough competition
2024-02-05 19:34:23,612 	Text Alignment  :	I     S   S       S      S  S      S     S   S       S    S        D     D  S    S    S    S    S     S          
2024-02-05 19:34:23,612 ========================================================================================================================
2024-02-05 19:34:23,612 Logging Sequence: 129_45.00
2024-02-05 19:34:23,612 	Gloss Reference :	A B+C+D+E
2024-02-05 19:34:23,613 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:34:23,613 	Gloss Alignment :	         
2024-02-05 19:34:23,613 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:34:23,614 	Text Reference  :	suga then  announced that    from 5    july onwards japan will be in   a  state of    emergency
2024-02-05 19:34:23,614 	Text Hypothesis :	the  tokyo olympics  started in   2020 but  will    have  to   be held at the   covid pandemic 
2024-02-05 19:34:23,615 	Text Alignment  :	S    S     S         S       S    S    S    S       S     S       S    S  S     S     S        
2024-02-05 19:34:23,615 ========================================================================================================================
2024-02-05 19:34:23,615 Logging Sequence: 56_17.00
2024-02-05 19:34:23,615 	Gloss Reference :	A B+C+D+E
2024-02-05 19:34:23,615 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:34:23,615 	Gloss Alignment :	         
2024-02-05 19:34:23,615 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:34:23,616 	Text Reference  :	*** ***** ******* it    was held at mumbai's wankhede stadium
2024-02-05 19:34:23,616 	Text Hypothesis :	the match started after a   lot  of them     as       well   
2024-02-05 19:34:23,616 	Text Alignment  :	I   I     I       S     S   S    S  S        S        S      
2024-02-05 19:34:23,616 ========================================================================================================================
2024-02-05 19:34:23,617 Logging Sequence: 152_73.00
2024-02-05 19:34:23,617 	Gloss Reference :	A B+C+D+E
2024-02-05 19:34:23,617 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:34:23,617 	Gloss Alignment :	         
2024-02-05 19:34:23,617 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:34:23,618 	Text Reference  :	**** ****** eventually he  too     got  out   by          shaheen afridi
2024-02-05 19:34:23,618 	Text Hypothesis :	even social media      was buzzing with match predictions and     memes 
2024-02-05 19:34:23,618 	Text Alignment  :	I    I      S          S   S       S    S     S           S       S     
2024-02-05 19:34:23,618 ========================================================================================================================
2024-02-05 19:34:28,859 Epoch 136: Total Training Recognition Loss 0.84  Total Training Translation Loss 32.23 
2024-02-05 19:34:28,860 EPOCH 137
2024-02-05 19:34:29,416 [Epoch: 137 Step: 00018100] Batch Recognition Loss:   0.004111 => Gls Tokens per Sec:     1727 || Batch Translation Loss:   0.462885 => Txt Tokens per Sec:     4764 || Lr: 0.000100
2024-02-05 19:34:35,077 [Epoch: 137 Step: 00018200] Batch Recognition Loss:   0.001157 => Gls Tokens per Sec:     1413 || Batch Translation Loss:   0.189284 => Txt Tokens per Sec:     3928 || Lr: 0.000100
2024-02-05 19:34:36,154 Epoch 137: Total Training Recognition Loss 0.80  Total Training Translation Loss 32.82 
2024-02-05 19:34:36,154 EPOCH 138
2024-02-05 19:34:40,751 [Epoch: 138 Step: 00018300] Batch Recognition Loss:   0.001154 => Gls Tokens per Sec:     1375 || Batch Translation Loss:   0.173252 => Txt Tokens per Sec:     3728 || Lr: 0.000100
2024-02-05 19:34:43,889 Epoch 138: Total Training Recognition Loss 1.04  Total Training Translation Loss 28.55 
2024-02-05 19:34:43,889 EPOCH 139
2024-02-05 19:34:46,473 [Epoch: 139 Step: 00018400] Batch Recognition Loss:   0.000817 => Gls Tokens per Sec:     1425 || Batch Translation Loss:   0.214437 => Txt Tokens per Sec:     4053 || Lr: 0.000100
2024-02-05 19:34:51,159 Epoch 139: Total Training Recognition Loss 0.68  Total Training Translation Loss 30.21 
2024-02-05 19:34:51,160 EPOCH 140
2024-02-05 19:34:51,840 [Epoch: 140 Step: 00018500] Batch Recognition Loss:   0.021515 => Gls Tokens per Sec:     1533 || Batch Translation Loss:   0.421303 => Txt Tokens per Sec:     4256 || Lr: 0.000100
2024-02-05 19:34:57,330 [Epoch: 140 Step: 00018600] Batch Recognition Loss:   0.003384 => Gls Tokens per Sec:     1457 || Batch Translation Loss:   0.484289 => Txt Tokens per Sec:     4069 || Lr: 0.000100
2024-02-05 19:34:58,443 Epoch 140: Total Training Recognition Loss 0.97  Total Training Translation Loss 33.43 
2024-02-05 19:34:58,443 EPOCH 141
2024-02-05 19:35:02,912 [Epoch: 141 Step: 00018700] Batch Recognition Loss:   0.000957 => Gls Tokens per Sec:     1433 || Batch Translation Loss:   0.224188 => Txt Tokens per Sec:     3945 || Lr: 0.000100
2024-02-05 19:35:05,991 Epoch 141: Total Training Recognition Loss 0.52  Total Training Translation Loss 42.90 
2024-02-05 19:35:05,991 EPOCH 142
2024-02-05 19:35:08,289 [Epoch: 142 Step: 00018800] Batch Recognition Loss:   0.007924 => Gls Tokens per Sec:     1637 || Batch Translation Loss:   0.197421 => Txt Tokens per Sec:     4600 || Lr: 0.000100
2024-02-05 19:35:13,135 Epoch 142: Total Training Recognition Loss 1.01  Total Training Translation Loss 45.60 
2024-02-05 19:35:13,135 EPOCH 143
2024-02-05 19:35:13,814 [Epoch: 143 Step: 00018900] Batch Recognition Loss:   0.005379 => Gls Tokens per Sec:     1652 || Batch Translation Loss:   0.062095 => Txt Tokens per Sec:     4383 || Lr: 0.000100
2024-02-05 19:35:19,357 [Epoch: 143 Step: 00019000] Batch Recognition Loss:   0.002107 => Gls Tokens per Sec:     1440 || Batch Translation Loss:   0.191387 => Txt Tokens per Sec:     3997 || Lr: 0.000100
2024-02-05 19:35:20,481 Epoch 143: Total Training Recognition Loss 0.61  Total Training Translation Loss 35.33 
2024-02-05 19:35:20,482 EPOCH 144
2024-02-05 19:35:24,939 [Epoch: 144 Step: 00019100] Batch Recognition Loss:   0.009141 => Gls Tokens per Sec:     1454 || Batch Translation Loss:   0.163995 => Txt Tokens per Sec:     4029 || Lr: 0.000100
2024-02-05 19:35:27,882 Epoch 144: Total Training Recognition Loss 0.65  Total Training Translation Loss 35.43 
2024-02-05 19:35:27,883 EPOCH 145
2024-02-05 19:35:30,665 [Epoch: 145 Step: 00019200] Batch Recognition Loss:   0.008095 => Gls Tokens per Sec:     1381 || Batch Translation Loss:   0.112994 => Txt Tokens per Sec:     3867 || Lr: 0.000100
2024-02-05 19:35:35,191 Epoch 145: Total Training Recognition Loss 0.61  Total Training Translation Loss 28.28 
2024-02-05 19:35:35,191 EPOCH 146
2024-02-05 19:35:35,825 [Epoch: 146 Step: 00019300] Batch Recognition Loss:   0.010477 => Gls Tokens per Sec:     1896 || Batch Translation Loss:   0.077574 => Txt Tokens per Sec:     4809 || Lr: 0.000100
2024-02-05 19:35:41,517 [Epoch: 146 Step: 00019400] Batch Recognition Loss:   0.000676 => Gls Tokens per Sec:     1402 || Batch Translation Loss:   0.239450 => Txt Tokens per Sec:     3938 || Lr: 0.000100
2024-02-05 19:35:42,397 Epoch 146: Total Training Recognition Loss 0.63  Total Training Translation Loss 23.42 
2024-02-05 19:35:42,397 EPOCH 147
2024-02-05 19:35:46,863 [Epoch: 147 Step: 00019500] Batch Recognition Loss:   0.011550 => Gls Tokens per Sec:     1470 || Batch Translation Loss:   0.596155 => Txt Tokens per Sec:     4080 || Lr: 0.000100
2024-02-05 19:35:49,854 Epoch 147: Total Training Recognition Loss 0.63  Total Training Translation Loss 28.41 
2024-02-05 19:35:49,854 EPOCH 148
2024-02-05 19:35:52,398 [Epoch: 148 Step: 00019600] Batch Recognition Loss:   0.000475 => Gls Tokens per Sec:     1542 || Batch Translation Loss:   0.151386 => Txt Tokens per Sec:     4312 || Lr: 0.000100
2024-02-05 19:35:57,031 Epoch 148: Total Training Recognition Loss 0.59  Total Training Translation Loss 31.37 
2024-02-05 19:35:57,032 EPOCH 149
2024-02-05 19:35:57,892 [Epoch: 149 Step: 00019700] Batch Recognition Loss:   0.001263 => Gls Tokens per Sec:     1489 || Batch Translation Loss:   0.106637 => Txt Tokens per Sec:     4274 || Lr: 0.000100
2024-02-05 19:36:03,486 [Epoch: 149 Step: 00019800] Batch Recognition Loss:   0.002513 => Gls Tokens per Sec:     1430 || Batch Translation Loss:   0.155850 => Txt Tokens per Sec:     3907 || Lr: 0.000100
2024-02-05 19:36:04,589 Epoch 149: Total Training Recognition Loss 0.88  Total Training Translation Loss 30.89 
2024-02-05 19:36:04,590 EPOCH 150
2024-02-05 19:36:09,263 [Epoch: 150 Step: 00019900] Batch Recognition Loss:   0.000875 => Gls Tokens per Sec:     1421 || Batch Translation Loss:   0.168710 => Txt Tokens per Sec:     3853 || Lr: 0.000100
2024-02-05 19:36:12,131 Epoch 150: Total Training Recognition Loss 1.21  Total Training Translation Loss 28.84 
2024-02-05 19:36:12,131 EPOCH 151
2024-02-05 19:36:15,024 [Epoch: 151 Step: 00020000] Batch Recognition Loss:   0.000325 => Gls Tokens per Sec:     1383 || Batch Translation Loss:   0.259193 => Txt Tokens per Sec:     3883 || Lr: 0.000100
2024-02-05 19:36:26,395 Validation result at epoch 151, step    20000: duration: 11.3699s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 5.38330	Translation Loss: 93203.25781	PPL: 11038.61133
	Eval Metric: BLEU
	WER 4.73	(DEL: 0.00,	INS: 0.00,	SUB: 4.73)
	BLEU-4 0.50	(BLEU-1: 10.36,	BLEU-2: 3.19,	BLEU-3: 1.13,	BLEU-4: 0.50)
	CHRF 16.30	ROUGE 9.14
2024-02-05 19:36:26,396 Logging Recognition and Translation Outputs
2024-02-05 19:36:26,396 ========================================================================================================================
2024-02-05 19:36:26,396 Logging Sequence: 120_7.00
2024-02-05 19:36:26,396 	Gloss Reference :	A B+C+D+E
2024-02-05 19:36:26,396 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:36:26,396 	Gloss Alignment :	         
2024-02-05 19:36:26,397 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:36:26,397 	Text Reference  :	** he          had    tested positive for    covid-19 on     may    19  
2024-02-05 19:36:26,397 	Text Hypothesis :	on questioning people about  the      murder the      police learnt that
2024-02-05 19:36:26,398 	Text Alignment  :	I  S           S      S      S        S      S        S      S      S   
2024-02-05 19:36:26,398 ========================================================================================================================
2024-02-05 19:36:26,398 Logging Sequence: 148_186.00
2024-02-05 19:36:26,398 	Gloss Reference :	A B+C+D+E
2024-02-05 19:36:26,398 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:36:26,398 	Gloss Alignment :	         
2024-02-05 19:36:26,398 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:36:26,400 	Text Reference  :	siraj also took four wickets in  1         over  thus       becoming the   record-holder for   most wickets in an    over in   odis
2024-02-05 19:36:26,400 	Text Hypothesis :	***** **** **** **** ******* the badminton world federation of       india is            being held every   2  years of   both more
2024-02-05 19:36:26,400 	Text Alignment  :	D     D    D    D    D       S   S         S     S          S        S     S             S     S    S       S  S     S    S    S   
2024-02-05 19:36:26,401 ========================================================================================================================
2024-02-05 19:36:26,401 Logging Sequence: 67_73.00
2024-02-05 19:36:26,401 	Gloss Reference :	A B+C+D+E
2024-02-05 19:36:26,401 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:36:26,401 	Gloss Alignment :	         
2024-02-05 19:36:26,401 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:36:26,402 	Text Reference  :	*** ****** in  his tweet he  also  said
2024-02-05 19:36:26,402 	Text Hypothesis :	she really let me  tell  you about it  
2024-02-05 19:36:26,402 	Text Alignment  :	I   I      S   S   S     S   S     S   
2024-02-05 19:36:26,402 ========================================================================================================================
2024-02-05 19:36:26,402 Logging Sequence: 164_526.00
2024-02-05 19:36:26,402 	Gloss Reference :	A B+C+D+E
2024-02-05 19:36:26,403 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:36:26,403 	Gloss Alignment :	         
2024-02-05 19:36:26,403 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:36:26,404 	Text Reference  :	*** you   are  aware that viacom18 bought the broadcast    rights *** of  ipl       
2024-02-05 19:36:26,404 	Text Hypothesis :	the first ball ended in   a        lot    of  broadcasting rights for the tournament
2024-02-05 19:36:26,404 	Text Alignment  :	I   S     S    S     S    S        S      S   S                   I   S   S         
2024-02-05 19:36:26,404 ========================================================================================================================
2024-02-05 19:36:26,404 Logging Sequence: 108_28.00
2024-02-05 19:36:26,404 	Gloss Reference :	A B+C+D+E  
2024-02-05 19:36:26,404 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-05 19:36:26,405 	Gloss Alignment :	  S        
2024-02-05 19:36:26,405 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:36:26,406 	Text Reference  :	the 10 teams bought 204 players including 67      foreign players after spending a  total of     rs 55170 crore   
2024-02-05 19:36:26,406 	Text Hypothesis :	the ** ***** ****** *** ******* ipl       matches will    be      held  only     in pune  mumbai at 25    capacity
2024-02-05 19:36:26,406 	Text Alignment  :	    D  D     D      D   D       S         S       S       S       S     S        S  S     S      S  S     S       
2024-02-05 19:36:26,407 ========================================================================================================================
2024-02-05 19:36:31,197 Epoch 151: Total Training Recognition Loss 0.62  Total Training Translation Loss 29.82 
2024-02-05 19:36:31,197 EPOCH 152
2024-02-05 19:36:32,165 [Epoch: 152 Step: 00020100] Batch Recognition Loss:   0.001510 => Gls Tokens per Sec:     1407 || Batch Translation Loss:   0.090646 => Txt Tokens per Sec:     4116 || Lr: 0.000100
2024-02-05 19:36:37,834 [Epoch: 152 Step: 00020200] Batch Recognition Loss:   0.000351 => Gls Tokens per Sec:     1411 || Batch Translation Loss:   0.385409 => Txt Tokens per Sec:     3852 || Lr: 0.000100
2024-02-05 19:36:38,701 Epoch 152: Total Training Recognition Loss 0.81  Total Training Translation Loss 40.67 
2024-02-05 19:36:38,701 EPOCH 153
2024-02-05 19:36:43,125 [Epoch: 153 Step: 00020300] Batch Recognition Loss:   0.000661 => Gls Tokens per Sec:     1519 || Batch Translation Loss:   0.202304 => Txt Tokens per Sec:     4187 || Lr: 0.000100
2024-02-05 19:36:45,881 Epoch 153: Total Training Recognition Loss 0.65  Total Training Translation Loss 35.55 
2024-02-05 19:36:45,881 EPOCH 154
2024-02-05 19:36:48,701 [Epoch: 154 Step: 00020400] Batch Recognition Loss:   0.000519 => Gls Tokens per Sec:     1448 || Batch Translation Loss:   0.169693 => Txt Tokens per Sec:     4017 || Lr: 0.000100
2024-02-05 19:36:53,101 Epoch 154: Total Training Recognition Loss 0.56  Total Training Translation Loss 30.47 
2024-02-05 19:36:53,102 EPOCH 155
2024-02-05 19:36:54,231 [Epoch: 155 Step: 00020500] Batch Recognition Loss:   0.004185 => Gls Tokens per Sec:     1277 || Batch Translation Loss:   0.043244 => Txt Tokens per Sec:     3511 || Lr: 0.000100
2024-02-05 19:36:59,635 [Epoch: 155 Step: 00020600] Batch Recognition Loss:   0.003613 => Gls Tokens per Sec:     1481 || Batch Translation Loss:   0.236167 => Txt Tokens per Sec:     4105 || Lr: 0.000100
2024-02-05 19:37:00,550 Epoch 155: Total Training Recognition Loss 0.51  Total Training Translation Loss 28.07 
2024-02-05 19:37:00,551 EPOCH 156
2024-02-05 19:37:05,207 [Epoch: 156 Step: 00020700] Batch Recognition Loss:   0.031692 => Gls Tokens per Sec:     1460 || Batch Translation Loss:   0.838195 => Txt Tokens per Sec:     4006 || Lr: 0.000100
2024-02-05 19:37:07,813 Epoch 156: Total Training Recognition Loss 0.76  Total Training Translation Loss 31.15 
2024-02-05 19:37:07,813 EPOCH 157
2024-02-05 19:37:10,398 [Epoch: 157 Step: 00020800] Batch Recognition Loss:   0.000407 => Gls Tokens per Sec:     1610 || Batch Translation Loss:   0.105858 => Txt Tokens per Sec:     4638 || Lr: 0.000100
2024-02-05 19:37:14,827 Epoch 157: Total Training Recognition Loss 0.67  Total Training Translation Loss 34.35 
2024-02-05 19:37:14,827 EPOCH 158
2024-02-05 19:37:15,905 [Epoch: 158 Step: 00020900] Batch Recognition Loss:   0.004711 => Gls Tokens per Sec:     1411 || Batch Translation Loss:   0.095395 => Txt Tokens per Sec:     3853 || Lr: 0.000100
2024-02-05 19:37:21,341 [Epoch: 158 Step: 00021000] Batch Recognition Loss:   0.002481 => Gls Tokens per Sec:     1468 || Batch Translation Loss:   0.118422 => Txt Tokens per Sec:     4073 || Lr: 0.000100
2024-02-05 19:37:22,087 Epoch 158: Total Training Recognition Loss 0.75  Total Training Translation Loss 33.41 
2024-02-05 19:37:22,087 EPOCH 159
2024-02-05 19:37:27,044 [Epoch: 159 Step: 00021100] Batch Recognition Loss:   0.005952 => Gls Tokens per Sec:     1389 || Batch Translation Loss:   0.069328 => Txt Tokens per Sec:     3918 || Lr: 0.000100
2024-02-05 19:37:29,750 Epoch 159: Total Training Recognition Loss 0.80  Total Training Translation Loss 21.17 
2024-02-05 19:37:29,750 EPOCH 160
2024-02-05 19:37:32,972 [Epoch: 160 Step: 00021200] Batch Recognition Loss:   0.001537 => Gls Tokens per Sec:     1316 || Batch Translation Loss:   1.002618 => Txt Tokens per Sec:     3636 || Lr: 0.000100
2024-02-05 19:37:37,412 Epoch 160: Total Training Recognition Loss 0.62  Total Training Translation Loss 34.96 
2024-02-05 19:37:37,412 EPOCH 161
2024-02-05 19:37:38,300 [Epoch: 161 Step: 00021300] Batch Recognition Loss:   0.000724 => Gls Tokens per Sec:     1804 || Batch Translation Loss:   0.198404 => Txt Tokens per Sec:     4840 || Lr: 0.000100
2024-02-05 19:37:44,140 [Epoch: 161 Step: 00021400] Batch Recognition Loss:   0.007121 => Gls Tokens per Sec:     1367 || Batch Translation Loss:   0.375808 => Txt Tokens per Sec:     3833 || Lr: 0.000100
2024-02-05 19:37:44,832 Epoch 161: Total Training Recognition Loss 0.81  Total Training Translation Loss 35.81 
2024-02-05 19:37:44,832 EPOCH 162
2024-02-05 19:37:49,343 [Epoch: 162 Step: 00021500] Batch Recognition Loss:   0.006962 => Gls Tokens per Sec:     1543 || Batch Translation Loss:   0.082019 => Txt Tokens per Sec:     4259 || Lr: 0.000100
2024-02-05 19:37:52,030 Epoch 162: Total Training Recognition Loss 0.74  Total Training Translation Loss 36.25 
2024-02-05 19:37:52,031 EPOCH 163
2024-02-05 19:37:55,417 [Epoch: 163 Step: 00021600] Batch Recognition Loss:   0.000294 => Gls Tokens per Sec:     1276 || Batch Translation Loss:   0.152268 => Txt Tokens per Sec:     3720 || Lr: 0.000100
2024-02-05 19:37:59,693 Epoch 163: Total Training Recognition Loss 0.75  Total Training Translation Loss 29.05 
2024-02-05 19:37:59,694 EPOCH 164
2024-02-05 19:38:00,852 [Epoch: 164 Step: 00021700] Batch Recognition Loss:   0.000472 => Gls Tokens per Sec:     1452 || Batch Translation Loss:   0.382574 => Txt Tokens per Sec:     3960 || Lr: 0.000100
2024-02-05 19:38:06,297 [Epoch: 164 Step: 00021800] Batch Recognition Loss:   0.001277 => Gls Tokens per Sec:     1466 || Batch Translation Loss:   0.092522 => Txt Tokens per Sec:     4081 || Lr: 0.000100
2024-02-05 19:38:06,971 Epoch 164: Total Training Recognition Loss 0.58  Total Training Translation Loss 23.53 
2024-02-05 19:38:06,971 EPOCH 165
2024-02-05 19:38:11,891 [Epoch: 165 Step: 00021900] Batch Recognition Loss:   0.000455 => Gls Tokens per Sec:     1431 || Batch Translation Loss:   0.095739 => Txt Tokens per Sec:     3947 || Lr: 0.000100
2024-02-05 19:38:14,441 Epoch 165: Total Training Recognition Loss 0.84  Total Training Translation Loss 20.02 
2024-02-05 19:38:14,441 EPOCH 166
2024-02-05 19:38:17,226 [Epoch: 166 Step: 00022000] Batch Recognition Loss:   0.000800 => Gls Tokens per Sec:     1581 || Batch Translation Loss:   0.134264 => Txt Tokens per Sec:     4338 || Lr: 0.000100
2024-02-05 19:38:27,827 Validation result at epoch 166, step    22000: duration: 10.6004s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 6.46466	Translation Loss: 93196.78906	PPL: 11031.48633
	Eval Metric: BLEU
	WER 4.45	(DEL: 0.00,	INS: 0.00,	SUB: 4.45)
	BLEU-4 0.77	(BLEU-1: 11.33,	BLEU-2: 3.65,	BLEU-3: 1.45,	BLEU-4: 0.77)
	CHRF 17.51	ROUGE 9.55
2024-02-05 19:38:27,828 Logging Recognition and Translation Outputs
2024-02-05 19:38:27,828 ========================================================================================================================
2024-02-05 19:38:27,828 Logging Sequence: 179_2.00
2024-02-05 19:38:27,829 	Gloss Reference :	A B+C+D+E    
2024-02-05 19:38:27,829 	Gloss Hypothesis:	A B+C+D+E+D+E
2024-02-05 19:38:27,829 	Gloss Alignment :	  S          
2024-02-05 19:38:27,829 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:38:27,829 	Text Reference  :	vinesh phogat is   a         well known   wrestler
2024-02-05 19:38:27,830 	Text Hypothesis :	the    duo    then travelled to   various winners 
2024-02-05 19:38:27,830 	Text Alignment  :	S      S      S    S         S    S       S       
2024-02-05 19:38:27,830 ========================================================================================================================
2024-02-05 19:38:27,830 Logging Sequence: 55_124.00
2024-02-05 19:38:27,830 	Gloss Reference :	A B+C+D+E
2024-02-05 19:38:27,830 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:38:27,830 	Gloss Alignment :	         
2024-02-05 19:38:27,830 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:38:27,831 	Text Reference  :	** *** * next to  him       with   the    patel jersey   was           ajaz patel
2024-02-05 19:38:27,831 	Text Hypothesis :	it was a huge fan following hardik pandya and   netizens congratulated the  tv   
2024-02-05 19:38:27,832 	Text Alignment  :	I  I   I S    S   S         S      S      S     S        S             S    S    
2024-02-05 19:38:27,832 ========================================================================================================================
2024-02-05 19:38:27,832 Logging Sequence: 148_105.00
2024-02-05 19:38:27,832 	Gloss Reference :	A B+C+D+E
2024-02-05 19:38:27,832 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:38:27,832 	Gloss Alignment :	         
2024-02-05 19:38:27,833 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:38:27,834 	Text Reference  :	later with amazing bowling by hardik pandya and kuldeep yadav sri lanka were     all out in   just   50    runs
2024-02-05 19:38:27,834 	Text Hypothesis :	***** **** ******* ******* ** ****** ****** the first   time  the most  followed the 6th over yuvraj singh said
2024-02-05 19:38:27,834 	Text Alignment  :	D     D    D       D       D  D      D      S   S       S     S   S     S        S   S   S    S      S     S   
2024-02-05 19:38:27,834 ========================================================================================================================
2024-02-05 19:38:27,834 Logging Sequence: 125_165.00
2024-02-05 19:38:27,835 	Gloss Reference :	A B+C+D+E
2024-02-05 19:38:27,835 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:38:27,835 	Gloss Alignment :	         
2024-02-05 19:38:27,835 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:38:27,836 	Text Reference  :	please do not target nadeem we speak to  each other and share   a   good bond   
2024-02-05 19:38:27,836 	Text Hypothesis :	****** ** *** ****** ****** ** ***** but that time  pat cummins for all  rounder
2024-02-05 19:38:27,836 	Text Alignment  :	D      D  D   D      D      D  D     S   S    S     S   S       S   S    S      
2024-02-05 19:38:27,836 ========================================================================================================================
2024-02-05 19:38:27,836 Logging Sequence: 77_52.00
2024-02-05 19:38:27,836 	Gloss Reference :	A B+C+D+E
2024-02-05 19:38:27,836 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:38:27,837 	Gloss Alignment :	         
2024-02-05 19:38:27,837 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:38:27,838 	Text Reference  :	kane williamson held down    the fort for hyderabad by   scoring     66  runs and ended the match in    a   tie  
2024-02-05 19:38:27,838 	Text Hypothesis :	**** ********** **** however the **** *** first     time afghanistan was set  to  help  he  still final and sixes
2024-02-05 19:38:27,839 	Text Alignment  :	D    D          D    S           D    D   S         S    S           S   S    S   S     S   S     S     S   S    
2024-02-05 19:38:27,839 ========================================================================================================================
2024-02-05 19:38:32,033 Epoch 166: Total Training Recognition Loss 0.40  Total Training Translation Loss 22.98 
2024-02-05 19:38:32,033 EPOCH 167
2024-02-05 19:38:33,360 [Epoch: 167 Step: 00022100] Batch Recognition Loss:   0.000530 => Gls Tokens per Sec:     1327 || Batch Translation Loss:   0.187099 => Txt Tokens per Sec:     3845 || Lr: 0.000100
2024-02-05 19:38:38,812 [Epoch: 167 Step: 00022200] Batch Recognition Loss:   0.000909 => Gls Tokens per Sec:     1464 || Batch Translation Loss:   0.162898 => Txt Tokens per Sec:     4030 || Lr: 0.000100
2024-02-05 19:38:39,458 Epoch 167: Total Training Recognition Loss 0.48  Total Training Translation Loss 23.80 
2024-02-05 19:38:39,458 EPOCH 168
2024-02-05 19:38:44,618 [Epoch: 168 Step: 00022300] Batch Recognition Loss:   0.002222 => Gls Tokens per Sec:     1380 || Batch Translation Loss:   0.103693 => Txt Tokens per Sec:     3839 || Lr: 0.000100
2024-02-05 19:38:46,981 Epoch 168: Total Training Recognition Loss 0.79  Total Training Translation Loss 22.51 
2024-02-05 19:38:46,981 EPOCH 169
2024-02-05 19:38:49,934 [Epoch: 169 Step: 00022400] Batch Recognition Loss:   0.000421 => Gls Tokens per Sec:     1517 || Batch Translation Loss:   0.229001 => Txt Tokens per Sec:     4326 || Lr: 0.000100
2024-02-05 19:38:54,381 Epoch 169: Total Training Recognition Loss 0.62  Total Training Translation Loss 30.43 
2024-02-05 19:38:54,381 EPOCH 170
2024-02-05 19:38:55,617 [Epoch: 170 Step: 00022500] Batch Recognition Loss:   0.007584 => Gls Tokens per Sec:     1490 || Batch Translation Loss:   0.437102 => Txt Tokens per Sec:     4037 || Lr: 0.000100
2024-02-05 19:39:00,900 [Epoch: 170 Step: 00022600] Batch Recognition Loss:   0.044553 => Gls Tokens per Sec:     1511 || Batch Translation Loss:   0.385981 => Txt Tokens per Sec:     4202 || Lr: 0.000100
2024-02-05 19:39:01,492 Epoch 170: Total Training Recognition Loss 0.54  Total Training Translation Loss 40.42 
2024-02-05 19:39:01,492 EPOCH 171
2024-02-05 19:39:06,383 [Epoch: 171 Step: 00022700] Batch Recognition Loss:   0.001122 => Gls Tokens per Sec:     1473 || Batch Translation Loss:   0.170462 => Txt Tokens per Sec:     4085 || Lr: 0.000100
2024-02-05 19:39:09,111 Epoch 171: Total Training Recognition Loss 0.71  Total Training Translation Loss 46.43 
2024-02-05 19:39:09,112 EPOCH 172
2024-02-05 19:39:12,413 [Epoch: 172 Step: 00022800] Batch Recognition Loss:   0.001065 => Gls Tokens per Sec:     1382 || Batch Translation Loss:   0.237408 => Txt Tokens per Sec:     3831 || Lr: 0.000100
2024-02-05 19:39:16,578 Epoch 172: Total Training Recognition Loss 0.48  Total Training Translation Loss 25.03 
2024-02-05 19:39:16,578 EPOCH 173
2024-02-05 19:39:18,067 [Epoch: 173 Step: 00022900] Batch Recognition Loss:   0.006649 => Gls Tokens per Sec:     1290 || Batch Translation Loss:   0.113181 => Txt Tokens per Sec:     3685 || Lr: 0.000100
2024-02-05 19:39:23,551 [Epoch: 173 Step: 00023000] Batch Recognition Loss:   0.002972 => Gls Tokens per Sec:     1455 || Batch Translation Loss:   0.213074 => Txt Tokens per Sec:     3984 || Lr: 0.000100
2024-02-05 19:39:24,096 Epoch 173: Total Training Recognition Loss 0.66  Total Training Translation Loss 21.46 
2024-02-05 19:39:24,096 EPOCH 174
2024-02-05 19:39:28,629 [Epoch: 174 Step: 00023100] Batch Recognition Loss:   0.000397 => Gls Tokens per Sec:     1606 || Batch Translation Loss:   0.811819 => Txt Tokens per Sec:     4449 || Lr: 0.000100
2024-02-05 19:39:30,894 Epoch 174: Total Training Recognition Loss 0.52  Total Training Translation Loss 24.85 
2024-02-05 19:39:30,895 EPOCH 175
2024-02-05 19:39:34,201 [Epoch: 175 Step: 00023200] Batch Recognition Loss:   0.004840 => Gls Tokens per Sec:     1404 || Batch Translation Loss:   0.108179 => Txt Tokens per Sec:     3791 || Lr: 0.000100
2024-02-05 19:39:38,522 Epoch 175: Total Training Recognition Loss 0.41  Total Training Translation Loss 26.62 
2024-02-05 19:39:38,523 EPOCH 176
2024-02-05 19:39:40,006 [Epoch: 176 Step: 00023300] Batch Recognition Loss:   0.000411 => Gls Tokens per Sec:     1350 || Batch Translation Loss:   0.119696 => Txt Tokens per Sec:     3835 || Lr: 0.000100
2024-02-05 19:39:45,448 [Epoch: 176 Step: 00023400] Batch Recognition Loss:   0.002890 => Gls Tokens per Sec:     1467 || Batch Translation Loss:   0.070665 => Txt Tokens per Sec:     4019 || Lr: 0.000100
2024-02-05 19:39:45,870 Epoch 176: Total Training Recognition Loss 0.52  Total Training Translation Loss 21.69 
2024-02-05 19:39:45,870 EPOCH 177
2024-02-05 19:39:51,084 [Epoch: 177 Step: 00023500] Batch Recognition Loss:   0.000441 => Gls Tokens per Sec:     1412 || Batch Translation Loss:   0.053166 => Txt Tokens per Sec:     3927 || Lr: 0.000100
2024-02-05 19:39:53,345 Epoch 177: Total Training Recognition Loss 0.75  Total Training Translation Loss 16.77 
2024-02-05 19:39:53,346 EPOCH 178
2024-02-05 19:39:56,367 [Epoch: 178 Step: 00023600] Batch Recognition Loss:   0.000474 => Gls Tokens per Sec:     1563 || Batch Translation Loss:   0.143014 => Txt Tokens per Sec:     4340 || Lr: 0.000100
2024-02-05 19:40:00,795 Epoch 178: Total Training Recognition Loss 0.56  Total Training Translation Loss 16.59 
2024-02-05 19:40:00,796 EPOCH 179
2024-02-05 19:40:02,136 [Epoch: 179 Step: 00023700] Batch Recognition Loss:   0.027519 => Gls Tokens per Sec:     1554 || Batch Translation Loss:   0.098789 => Txt Tokens per Sec:     4156 || Lr: 0.000100
2024-02-05 19:40:07,294 [Epoch: 179 Step: 00023800] Batch Recognition Loss:   0.000863 => Gls Tokens per Sec:     1547 || Batch Translation Loss:   0.085402 => Txt Tokens per Sec:     4313 || Lr: 0.000100
2024-02-05 19:40:07,755 Epoch 179: Total Training Recognition Loss 0.69  Total Training Translation Loss 21.00 
2024-02-05 19:40:07,755 EPOCH 180
2024-02-05 19:40:13,099 [Epoch: 180 Step: 00023900] Batch Recognition Loss:   0.002150 => Gls Tokens per Sec:     1393 || Batch Translation Loss:   0.161194 => Txt Tokens per Sec:     3865 || Lr: 0.000100
2024-02-05 19:40:15,563 Epoch 180: Total Training Recognition Loss 0.43  Total Training Translation Loss 34.21 
2024-02-05 19:40:15,563 EPOCH 181
2024-02-05 19:40:19,010 [Epoch: 181 Step: 00024000] Batch Recognition Loss:   0.000896 => Gls Tokens per Sec:     1393 || Batch Translation Loss:   0.160034 => Txt Tokens per Sec:     3909 || Lr: 0.000100
2024-02-05 19:40:30,134 Validation result at epoch 181, step    24000: duration: 11.1230s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 6.84828	Translation Loss: 92944.82812	PPL: 10757.32715
	Eval Metric: BLEU
	WER 4.87	(DEL: 0.00,	INS: 0.00,	SUB: 4.87)
	BLEU-4 0.42	(BLEU-1: 9.97,	BLEU-2: 3.09,	BLEU-3: 1.03,	BLEU-4: 0.42)
	CHRF 16.72	ROUGE 8.66
2024-02-05 19:40:30,136 Logging Recognition and Translation Outputs
2024-02-05 19:40:30,136 ========================================================================================================================
2024-02-05 19:40:30,136 Logging Sequence: 171_2.00
2024-02-05 19:40:30,136 	Gloss Reference :	A B+C+D+E  
2024-02-05 19:40:30,137 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-05 19:40:30,137 	Gloss Alignment :	  S        
2024-02-05 19:40:30,137 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:40:30,140 	Text Reference  :	as you might all know that the        ipl  is    about  to end  the finals are        on *** 28th may   
2024-02-05 19:40:30,140 	Text Hypothesis :	** *** ***** in  2011 2020 cricketers when india scored 35 runs in  26     deliveries on his left unsold
2024-02-05 19:40:30,140 	Text Alignment  :	D  D   D     S   S    S    S          S    S     S      S  S    S   S      S             I   S    S     
2024-02-05 19:40:30,140 ========================================================================================================================
2024-02-05 19:40:30,141 Logging Sequence: 119_33.00
2024-02-05 19:40:30,141 	Gloss Reference :	A B+C+D+E
2024-02-05 19:40:30,141 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:40:30,141 	Gloss Alignment :	         
2024-02-05 19:40:30,141 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:40:30,143 	Text Reference  :	he **** ********* *** *** wanted to    gift *** *** ****** ** 35 people wow wonderful
2024-02-05 19:40:30,143 	Text Hypothesis :	he then qualified for his wife   their gift for his father as he has    a   gift     
2024-02-05 19:40:30,143 	Text Alignment  :	   I    I         I   I   S      S          I   I   I      I  S  S      S   S        
2024-02-05 19:40:30,143 ========================================================================================================================
2024-02-05 19:40:30,144 Logging Sequence: 158_131.00
2024-02-05 19:40:30,144 	Gloss Reference :	A B+C+D+E
2024-02-05 19:40:30,144 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:40:30,144 	Gloss Alignment :	         
2024-02-05 19:40:30,145 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:40:30,146 	Text Reference  :	on 10th april 2023 there was a match between rcb     and     lsg in  bengaluru     
2024-02-05 19:40:30,146 	Text Hypothesis :	** **** ***** bcci kohli was * ***** ******* gambhir playing one day internationals
2024-02-05 19:40:30,146 	Text Alignment  :	D  D    D     S    S         D D     D       S       S       S   S   S             
2024-02-05 19:40:30,147 ========================================================================================================================
2024-02-05 19:40:30,147 Logging Sequence: 164_412.00
2024-02-05 19:40:30,147 	Gloss Reference :	A B+C+D+E
2024-02-05 19:40:30,147 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:40:30,147 	Gloss Alignment :	         
2024-02-05 19:40:30,148 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:40:30,151 	Text Reference  :	if  you   divide these     two figures you will be      shocked to know that each ball's   worth is      rs     50  lakhs    
2024-02-05 19:40:30,151 	Text Hypothesis :	the match was    presented the rights  of  ipl  matches yet     to **** **** **** complete the   various rights for marketing
2024-02-05 19:40:30,151 	Text Alignment  :	S   S     S      S         S   S       S   S    S       S          D    D    D    S        S     S       S      S   S        
2024-02-05 19:40:30,152 ========================================================================================================================
2024-02-05 19:40:30,152 Logging Sequence: 159_112.00
2024-02-05 19:40:30,152 	Gloss Reference :	A B+C+D+E    
2024-02-05 19:40:30,152 	Gloss Hypothesis:	A B+C+D+E+D+E
2024-02-05 19:40:30,152 	Gloss Alignment :	  S          
2024-02-05 19:40:30,153 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:40:30,155 	Text Reference  :	kohli had revealed that before the tournament he did not touch his bat  for a   month   yes 1    month  
2024-02-05 19:40:30,155 	Text Hypothesis :	***** *** you      all  know   the ********** ** *** *** ***** *** list of  ipl matches in  t20i captain
2024-02-05 19:40:30,155 	Text Alignment  :	D     D   S        S    S          D          D  D   D   D     D   S    S   S   S       S   S    S      
2024-02-05 19:40:30,155 ========================================================================================================================
2024-02-05 19:40:36,413 Epoch 181: Total Training Recognition Loss 1.05  Total Training Translation Loss 36.56 
2024-02-05 19:40:36,413 EPOCH 182
2024-02-05 19:40:38,923 [Epoch: 182 Step: 00024100] Batch Recognition Loss:   0.000838 => Gls Tokens per Sec:      861 || Batch Translation Loss:   0.164148 => Txt Tokens per Sec:     2539 || Lr: 0.000100
2024-02-05 19:40:44,625 [Epoch: 182 Step: 00024200] Batch Recognition Loss:   0.006966 => Gls Tokens per Sec:     1400 || Batch Translation Loss:   0.070189 => Txt Tokens per Sec:     3797 || Lr: 0.000100
2024-02-05 19:40:44,977 Epoch 182: Total Training Recognition Loss 0.61  Total Training Translation Loss 26.52 
2024-02-05 19:40:44,977 EPOCH 183
2024-02-05 19:40:50,466 [Epoch: 183 Step: 00024300] Batch Recognition Loss:   0.006293 => Gls Tokens per Sec:     1370 || Batch Translation Loss:   0.142127 => Txt Tokens per Sec:     3767 || Lr: 0.000100
2024-02-05 19:40:52,633 Epoch 183: Total Training Recognition Loss 0.83  Total Training Translation Loss 33.48 
2024-02-05 19:40:52,633 EPOCH 184
2024-02-05 19:40:55,723 [Epoch: 184 Step: 00024400] Batch Recognition Loss:   0.004679 => Gls Tokens per Sec:     1580 || Batch Translation Loss:   0.102491 => Txt Tokens per Sec:     4420 || Lr: 0.000100
2024-02-05 19:40:59,870 Epoch 184: Total Training Recognition Loss 0.73  Total Training Translation Loss 28.07 
2024-02-05 19:40:59,870 EPOCH 185
2024-02-05 19:41:01,227 [Epoch: 185 Step: 00024500] Batch Recognition Loss:   0.005598 => Gls Tokens per Sec:     1652 || Batch Translation Loss:   0.063528 => Txt Tokens per Sec:     4330 || Lr: 0.000100
2024-02-05 19:41:06,624 [Epoch: 185 Step: 00024600] Batch Recognition Loss:   0.002944 => Gls Tokens per Sec:     1482 || Batch Translation Loss:   0.146917 => Txt Tokens per Sec:     4162 || Lr: 0.000100
2024-02-05 19:41:06,932 Epoch 185: Total Training Recognition Loss 0.46  Total Training Translation Loss 23.98 
2024-02-05 19:41:06,932 EPOCH 186
2024-02-05 19:41:11,964 [Epoch: 186 Step: 00024700] Batch Recognition Loss:   0.001820 => Gls Tokens per Sec:     1511 || Batch Translation Loss:   0.400954 => Txt Tokens per Sec:     4136 || Lr: 0.000100
2024-02-05 19:41:14,280 Epoch 186: Total Training Recognition Loss 0.54  Total Training Translation Loss 32.39 
2024-02-05 19:41:14,280 EPOCH 187
2024-02-05 19:41:17,247 [Epoch: 187 Step: 00024800] Batch Recognition Loss:   0.001730 => Gls Tokens per Sec:     1672 || Batch Translation Loss:   0.121914 => Txt Tokens per Sec:     4489 || Lr: 0.000100
2024-02-05 19:41:20,750 Epoch 187: Total Training Recognition Loss 0.84  Total Training Translation Loss 27.39 
2024-02-05 19:41:20,750 EPOCH 188
2024-02-05 19:41:22,374 [Epoch: 188 Step: 00024900] Batch Recognition Loss:   0.001593 => Gls Tokens per Sec:     1430 || Batch Translation Loss:   0.058778 => Txt Tokens per Sec:     3868 || Lr: 0.000100
2024-02-05 19:41:27,901 [Epoch: 188 Step: 00025000] Batch Recognition Loss:   0.021875 => Gls Tokens per Sec:     1444 || Batch Translation Loss:   0.100687 => Txt Tokens per Sec:     3989 || Lr: 0.000100
2024-02-05 19:41:28,149 Epoch 188: Total Training Recognition Loss 0.74  Total Training Translation Loss 25.61 
2024-02-05 19:41:28,149 EPOCH 189
2024-02-05 19:41:33,444 [Epoch: 189 Step: 00025100] Batch Recognition Loss:   0.011639 => Gls Tokens per Sec:     1451 || Batch Translation Loss:   0.098742 => Txt Tokens per Sec:     4003 || Lr: 0.000100
2024-02-05 19:41:35,437 Epoch 189: Total Training Recognition Loss 0.54  Total Training Translation Loss 24.12 
2024-02-05 19:41:35,437 EPOCH 190
2024-02-05 19:41:38,962 [Epoch: 190 Step: 00025200] Batch Recognition Loss:   0.002052 => Gls Tokens per Sec:     1431 || Batch Translation Loss:   0.061181 => Txt Tokens per Sec:     4023 || Lr: 0.000100
2024-02-05 19:41:43,036 Epoch 190: Total Training Recognition Loss 0.55  Total Training Translation Loss 22.22 
2024-02-05 19:41:43,037 EPOCH 191
2024-02-05 19:41:44,600 [Epoch: 191 Step: 00025300] Batch Recognition Loss:   0.001485 => Gls Tokens per Sec:     1537 || Batch Translation Loss:   0.061673 => Txt Tokens per Sec:     4359 || Lr: 0.000100
2024-02-05 19:41:49,841 [Epoch: 191 Step: 00025400] Batch Recognition Loss:   0.000576 => Gls Tokens per Sec:     1523 || Batch Translation Loss:   0.199808 => Txt Tokens per Sec:     4218 || Lr: 0.000100
2024-02-05 19:41:50,007 Epoch 191: Total Training Recognition Loss 0.38  Total Training Translation Loss 21.90 
2024-02-05 19:41:50,007 EPOCH 192
2024-02-05 19:41:55,735 [Epoch: 192 Step: 00025500] Batch Recognition Loss:   0.002167 => Gls Tokens per Sec:     1355 || Batch Translation Loss:   0.083159 => Txt Tokens per Sec:     3753 || Lr: 0.000100
2024-02-05 19:41:57,762 Epoch 192: Total Training Recognition Loss 0.41  Total Training Translation Loss 27.31 
2024-02-05 19:41:57,762 EPOCH 193
2024-02-05 19:42:01,276 [Epoch: 193 Step: 00025600] Batch Recognition Loss:   0.000854 => Gls Tokens per Sec:     1458 || Batch Translation Loss:   0.236958 => Txt Tokens per Sec:     3973 || Lr: 0.000100
2024-02-05 19:42:05,241 Epoch 193: Total Training Recognition Loss 0.49  Total Training Translation Loss 25.52 
2024-02-05 19:42:05,242 EPOCH 194
2024-02-05 19:42:07,014 [Epoch: 194 Step: 00025700] Batch Recognition Loss:   0.000261 => Gls Tokens per Sec:     1401 || Batch Translation Loss:   0.083362 => Txt Tokens per Sec:     3818 || Lr: 0.000100
2024-02-05 19:42:12,592 [Epoch: 194 Step: 00025800] Batch Recognition Loss:   0.015759 => Gls Tokens per Sec:     1430 || Batch Translation Loss:   0.046993 => Txt Tokens per Sec:     3997 || Lr: 0.000100
2024-02-05 19:42:12,680 Epoch 194: Total Training Recognition Loss 0.53  Total Training Translation Loss 26.63 
2024-02-05 19:42:12,681 EPOCH 195
2024-02-05 19:42:18,130 [Epoch: 195 Step: 00025900] Batch Recognition Loss:   0.013362 => Gls Tokens per Sec:     1439 || Batch Translation Loss:   0.076994 => Txt Tokens per Sec:     3985 || Lr: 0.000100
2024-02-05 19:42:20,131 Epoch 195: Total Training Recognition Loss 0.70  Total Training Translation Loss 20.89 
2024-02-05 19:42:20,132 EPOCH 196
2024-02-05 19:42:23,707 [Epoch: 196 Step: 00026000] Batch Recognition Loss:   0.000608 => Gls Tokens per Sec:     1455 || Batch Translation Loss:   0.313212 => Txt Tokens per Sec:     4146 || Lr: 0.000100
2024-02-05 19:42:34,278 Validation result at epoch 196, step    26000: duration: 10.5710s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 6.54710	Translation Loss: 94426.23438	PPL: 12472.80469
	Eval Metric: BLEU
	WER 4.24	(DEL: 0.00,	INS: 0.00,	SUB: 4.24)
	BLEU-4 0.49	(BLEU-1: 10.80,	BLEU-2: 3.16,	BLEU-3: 1.12,	BLEU-4: 0.49)
	CHRF 17.00	ROUGE 9.18
2024-02-05 19:42:34,279 Logging Recognition and Translation Outputs
2024-02-05 19:42:34,279 ========================================================================================================================
2024-02-05 19:42:34,280 Logging Sequence: 166_243.00
2024-02-05 19:42:34,280 	Gloss Reference :	A B+C+D+E
2024-02-05 19:42:34,280 	Gloss Hypothesis:	A B+C+D  
2024-02-05 19:42:34,280 	Gloss Alignment :	  S      
2024-02-05 19:42:34,280 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:42:34,281 	Text Reference  :	*** ********* *********** ********* *** ***** ** icc     worked with members boards like bcci pcb   cricket australia etc 
2024-02-05 19:42:34,282 	Text Hypothesis :	the broadcast advertisers ticketing etc would be decided by     the  board   of     the  2    teams playing the       test
2024-02-05 19:42:34,282 	Text Alignment  :	I   I         I           I         I   I     I  S       S      S    S       S      S    S    S     S       S         S   
2024-02-05 19:42:34,282 ========================================================================================================================
2024-02-05 19:42:34,282 Logging Sequence: 59_152.00
2024-02-05 19:42:34,282 	Gloss Reference :	A B+C+D+E
2024-02-05 19:42:34,282 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:42:34,282 	Gloss Alignment :	         
2024-02-05 19:42:34,283 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:42:34,284 	Text Reference  :	the organisers encouraged athletes to    use the condoms      in    their home countries
2024-02-05 19:42:34,284 	Text Hypothesis :	in  2010       he         took     place as  the commonwealth games so    keep watching 
2024-02-05 19:42:34,284 	Text Alignment  :	S   S          S          S        S     S       S            S     S     S    S        
2024-02-05 19:42:34,284 ========================================================================================================================
2024-02-05 19:42:34,284 Logging Sequence: 145_52.00
2024-02-05 19:42:34,284 	Gloss Reference :	A B+C+D+E
2024-02-05 19:42:34,284 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:42:34,285 	Gloss Alignment :	         
2024-02-05 19:42:34,285 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:42:34,286 	Text Reference  :	her    name was   dropped despite having qualified as  she was the only female athlete
2024-02-05 19:42:34,286 	Text Hypothesis :	before the  start of      chennai super  kings     csk has won the **** ****** world  
2024-02-05 19:42:34,286 	Text Alignment  :	S      S    S     S       S       S      S         S   S   S       D    D      S      
2024-02-05 19:42:34,286 ========================================================================================================================
2024-02-05 19:42:34,286 Logging Sequence: 172_163.00
2024-02-05 19:42:34,287 	Gloss Reference :	A B+C+D+E
2024-02-05 19:42:34,287 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:42:34,287 	Gloss Alignment :	         
2024-02-05 19:42:34,287 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:42:34,288 	Text Reference  :	** if  the match starts anywhere between 730 pm     to 935 pm a full 20-over match can be played  
2024-02-05 19:42:34,288 	Text Hypothesis :	as per the ***** rules  if       hou     had failed to *** ** * **** ******* ***** *** be refunded
2024-02-05 19:42:34,289 	Text Alignment  :	I  S       D     S      S        S       S   S         D   D  D D    D       D     D      S       
2024-02-05 19:42:34,289 ========================================================================================================================
2024-02-05 19:42:34,289 Logging Sequence: 150_20.00
2024-02-05 19:42:34,289 	Gloss Reference :	A B+C+D+E
2024-02-05 19:42:34,289 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:42:34,289 	Gloss Alignment :	         
2024-02-05 19:42:34,289 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:42:34,290 	Text Reference  :	after   a   tough match india won the saff championship 2023 title  
2024-02-05 19:42:34,290 	Text Hypothesis :	however the one   thing has   won the **** indian       team members
2024-02-05 19:42:34,290 	Text Alignment  :	S       S   S     S     S             D    S            S    S      
2024-02-05 19:42:34,291 ========================================================================================================================
2024-02-05 19:42:38,032 Epoch 196: Total Training Recognition Loss 0.43  Total Training Translation Loss 20.54 
2024-02-05 19:42:38,032 EPOCH 197
2024-02-05 19:42:39,813 [Epoch: 197 Step: 00026100] Batch Recognition Loss:   0.001205 => Gls Tokens per Sec:     1438 || Batch Translation Loss:   0.215495 => Txt Tokens per Sec:     4072 || Lr: 0.000100
2024-02-05 19:42:45,475 [Epoch: 197 Step: 00026200] Batch Recognition Loss:   0.012492 => Gls Tokens per Sec:     1410 || Batch Translation Loss:   0.073124 => Txt Tokens per Sec:     3868 || Lr: 0.000100
2024-02-05 19:42:45,524 Epoch 197: Total Training Recognition Loss 0.66  Total Training Translation Loss 31.19 
2024-02-05 19:42:45,524 EPOCH 198
2024-02-05 19:42:51,184 [Epoch: 198 Step: 00026300] Batch Recognition Loss:   0.001148 => Gls Tokens per Sec:     1399 || Batch Translation Loss:   0.168768 => Txt Tokens per Sec:     3863 || Lr: 0.000100
2024-02-05 19:42:53,292 Epoch 198: Total Training Recognition Loss 0.81  Total Training Translation Loss 28.14 
2024-02-05 19:42:53,292 EPOCH 199
2024-02-05 19:42:56,996 [Epoch: 199 Step: 00026400] Batch Recognition Loss:   0.000850 => Gls Tokens per Sec:     1426 || Batch Translation Loss:   0.237897 => Txt Tokens per Sec:     3991 || Lr: 0.000100
2024-02-05 19:43:00,688 Epoch 199: Total Training Recognition Loss 0.53  Total Training Translation Loss 21.80 
2024-02-05 19:43:00,688 EPOCH 200
2024-02-05 19:43:02,190 [Epoch: 200 Step: 00026500] Batch Recognition Loss:   0.002359 => Gls Tokens per Sec:     1758 || Batch Translation Loss:   0.181406 => Txt Tokens per Sec:     4807 || Lr: 0.000100
2024-02-05 19:43:06,949 [Epoch: 200 Step: 00026600] Batch Recognition Loss:   0.002503 => Gls Tokens per Sec:     1677 || Batch Translation Loss:   0.152549 => Txt Tokens per Sec:     4657 || Lr: 0.000100
2024-02-05 19:43:06,949 Epoch 200: Total Training Recognition Loss 0.48  Total Training Translation Loss 25.58 
2024-02-05 19:43:06,949 EPOCH 201
2024-02-05 19:43:12,252 [Epoch: 201 Step: 00026700] Batch Recognition Loss:   0.020365 => Gls Tokens per Sec:     1509 || Batch Translation Loss:   0.166988 => Txt Tokens per Sec:     4157 || Lr: 0.000100
2024-02-05 19:43:14,023 Epoch 201: Total Training Recognition Loss 0.44  Total Training Translation Loss 29.10 
2024-02-05 19:43:14,024 EPOCH 202
2024-02-05 19:43:17,879 [Epoch: 202 Step: 00026800] Batch Recognition Loss:   0.000616 => Gls Tokens per Sec:     1391 || Batch Translation Loss:   0.075227 => Txt Tokens per Sec:     3843 || Lr: 0.000100
2024-02-05 19:43:21,672 Epoch 202: Total Training Recognition Loss 0.46  Total Training Translation Loss 18.98 
2024-02-05 19:43:21,673 EPOCH 203
2024-02-05 19:43:23,410 [Epoch: 203 Step: 00026900] Batch Recognition Loss:   0.003031 => Gls Tokens per Sec:     1567 || Batch Translation Loss:   0.187730 => Txt Tokens per Sec:     4211 || Lr: 0.000100
2024-02-05 19:43:28,879 Epoch 203: Total Training Recognition Loss 0.44  Total Training Translation Loss 17.69 
2024-02-05 19:43:28,880 EPOCH 204
2024-02-05 19:43:28,955 [Epoch: 204 Step: 00027000] Batch Recognition Loss:   0.006356 => Gls Tokens per Sec:     1081 || Batch Translation Loss:   0.134366 => Txt Tokens per Sec:     3662 || Lr: 0.000100
2024-02-05 19:43:34,634 [Epoch: 204 Step: 00027100] Batch Recognition Loss:   0.000614 => Gls Tokens per Sec:     1409 || Batch Translation Loss:   0.076725 => Txt Tokens per Sec:     3918 || Lr: 0.000100
2024-02-05 19:43:36,544 Epoch 204: Total Training Recognition Loss 0.65  Total Training Translation Loss 22.07 
2024-02-05 19:43:36,544 EPOCH 205
2024-02-05 19:43:40,463 [Epoch: 205 Step: 00027200] Batch Recognition Loss:   0.000376 => Gls Tokens per Sec:     1388 || Batch Translation Loss:   0.114227 => Txt Tokens per Sec:     3784 || Lr: 0.000100
2024-02-05 19:43:44,195 Epoch 205: Total Training Recognition Loss 0.50  Total Training Translation Loss 20.09 
2024-02-05 19:43:44,195 EPOCH 206
2024-02-05 19:43:45,866 [Epoch: 206 Step: 00027300] Batch Recognition Loss:   0.003539 => Gls Tokens per Sec:     1677 || Batch Translation Loss:   0.526535 => Txt Tokens per Sec:     4481 || Lr: 0.000100
2024-02-05 19:43:51,639 Epoch 206: Total Training Recognition Loss 0.33  Total Training Translation Loss 24.92 
2024-02-05 19:43:51,640 EPOCH 207
2024-02-05 19:43:51,737 [Epoch: 207 Step: 00027400] Batch Recognition Loss:   0.013092 => Gls Tokens per Sec:     1667 || Batch Translation Loss:   0.025344 => Txt Tokens per Sec:     3240 || Lr: 0.000100
2024-02-05 19:43:57,000 [Epoch: 207 Step: 00027500] Batch Recognition Loss:   0.021106 => Gls Tokens per Sec:     1520 || Batch Translation Loss:   0.104591 => Txt Tokens per Sec:     4230 || Lr: 0.000100
2024-02-05 19:43:58,559 Epoch 207: Total Training Recognition Loss 0.58  Total Training Translation Loss 31.65 
2024-02-05 19:43:58,559 EPOCH 208
2024-02-05 19:44:02,424 [Epoch: 208 Step: 00027600] Batch Recognition Loss:   0.001703 => Gls Tokens per Sec:     1429 || Batch Translation Loss:   0.164730 => Txt Tokens per Sec:     3912 || Lr: 0.000100
2024-02-05 19:44:06,173 Epoch 208: Total Training Recognition Loss 0.55  Total Training Translation Loss 29.87 
2024-02-05 19:44:06,173 EPOCH 209
2024-02-05 19:44:07,917 [Epoch: 209 Step: 00027700] Batch Recognition Loss:   0.000510 => Gls Tokens per Sec:     1652 || Batch Translation Loss:   0.064930 => Txt Tokens per Sec:     4634 || Lr: 0.000100
2024-02-05 19:44:13,295 Epoch 209: Total Training Recognition Loss 0.68  Total Training Translation Loss 29.44 
2024-02-05 19:44:13,295 EPOCH 210
2024-02-05 19:44:13,445 [Epoch: 210 Step: 00027800] Batch Recognition Loss:   0.001885 => Gls Tokens per Sec:     1622 || Batch Translation Loss:   0.100273 => Txt Tokens per Sec:     3993 || Lr: 0.000100
2024-02-05 19:44:19,067 [Epoch: 210 Step: 00027900] Batch Recognition Loss:   0.000230 => Gls Tokens per Sec:     1423 || Batch Translation Loss:   0.076136 => Txt Tokens per Sec:     3969 || Lr: 0.000100
2024-02-05 19:44:20,766 Epoch 210: Total Training Recognition Loss 0.58  Total Training Translation Loss 22.03 
2024-02-05 19:44:20,766 EPOCH 211
2024-02-05 19:44:24,282 [Epoch: 211 Step: 00028000] Batch Recognition Loss:   0.002268 => Gls Tokens per Sec:     1593 || Batch Translation Loss:   0.057622 => Txt Tokens per Sec:     4348 || Lr: 0.000100
2024-02-05 19:44:35,273 Validation result at epoch 211, step    28000: duration: 10.9902s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 5.68292	Translation Loss: 93456.27344	PPL: 11321.12793
	Eval Metric: BLEU
	WER 4.66	(DEL: 0.00,	INS: 0.00,	SUB: 4.66)
	BLEU-4 0.45	(BLEU-1: 10.31,	BLEU-2: 2.94,	BLEU-3: 1.00,	BLEU-4: 0.45)
	CHRF 16.84	ROUGE 9.01
2024-02-05 19:44:35,274 Logging Recognition and Translation Outputs
2024-02-05 19:44:35,274 ========================================================================================================================
2024-02-05 19:44:35,274 Logging Sequence: 156_288.00
2024-02-05 19:44:35,274 	Gloss Reference :	A B+C+D+E
2024-02-05 19:44:35,274 	Gloss Hypothesis:	A B+C+D  
2024-02-05 19:44:35,274 	Gloss Alignment :	  S      
2024-02-05 19:44:35,275 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:44:35,275 	Text Reference  :	***** pooran led the team to victory miny became winners of  the     1st season
2024-02-05 19:44:35,276 	Text Hypothesis :	india had    won the **** ** ******* **** ****** toss    and elected to  bat   
2024-02-05 19:44:35,276 	Text Alignment  :	I     S      S       D    D  D       D    D      S       S   S       S   S     
2024-02-05 19:44:35,276 ========================================================================================================================
2024-02-05 19:44:35,276 Logging Sequence: 98_135.00
2024-02-05 19:44:35,276 	Gloss Reference :	A B+C+D+E
2024-02-05 19:44:35,276 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:44:35,276 	Gloss Alignment :	         
2024-02-05 19:44:35,276 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:44:35,277 	Text Reference  :	however due to  the rise in   coronavirus cases the tournament was shifted
2024-02-05 19:44:35,277 	Text Hypothesis :	******* *** and it  was  very proud       for   the ********** *** series 
2024-02-05 19:44:35,277 	Text Alignment  :	D       D   S   S   S    S    S           S         D          D   S      
2024-02-05 19:44:35,277 ========================================================================================================================
2024-02-05 19:44:35,278 Logging Sequence: 161_47.00
2024-02-05 19:44:35,278 	Gloss Reference :	A B+C+D+E
2024-02-05 19:44:35,278 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:44:35,278 	Gloss Alignment :	         
2024-02-05 19:44:35,278 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:44:35,279 	Text Reference  :	** *** he requested confidentiality as     he  was  planning to   make an official announcement
2024-02-05 19:44:35,279 	Text Hypothesis :	it was a  few       minutes         before the game and      then goes on social   media       
2024-02-05 19:44:35,279 	Text Alignment  :	I  I   S  S         S               S      S   S    S        S    S    S  S        S           
2024-02-05 19:44:35,280 ========================================================================================================================
2024-02-05 19:44:35,280 Logging Sequence: 131_159.00
2024-02-05 19:44:35,280 	Gloss Reference :	A B+C+D+E
2024-02-05 19:44:35,280 	Gloss Hypothesis:	A B+C+D  
2024-02-05 19:44:35,280 	Gloss Alignment :	  S      
2024-02-05 19:44:35,280 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:44:35,281 	Text Reference  :	chanu also met biren singh following the  meeting singh   described chanu   as our      nation' pride   
2024-02-05 19:44:35,281 	Text Hypothesis :	we    also *** ***** ***** ********* have to      nongpok kakching  village in manipur' imphal  district
2024-02-05 19:44:35,282 	Text Alignment  :	S          D   D     D     D         S    S       S       S         S       S  S        S       S       
2024-02-05 19:44:35,282 ========================================================================================================================
2024-02-05 19:44:35,282 Logging Sequence: 137_167.00
2024-02-05 19:44:35,282 	Gloss Reference :	A B+C+D+E
2024-02-05 19:44:35,282 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:44:35,282 	Gloss Alignment :	         
2024-02-05 19:44:35,282 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:44:35,284 	Text Reference  :	however after 630 pm there will be  certain fan     zones where  beer   will be **** available and  nowhere else
2024-02-05 19:44:35,284 	Text Hypothesis :	******* ***** *** ** 3     if   the indian  skipper rohit sharma yuvraj will be held on        12th january 2022
2024-02-05 19:44:35,284 	Text Alignment  :	D       D     D   D  S     S    S   S       S       S     S      S              I    S         S    S       S   
2024-02-05 19:44:35,284 ========================================================================================================================
2024-02-05 19:44:39,013 Epoch 211: Total Training Recognition Loss 0.64  Total Training Translation Loss 21.25 
2024-02-05 19:44:39,014 EPOCH 212
2024-02-05 19:44:40,966 [Epoch: 212 Step: 00028100] Batch Recognition Loss:   0.000802 => Gls Tokens per Sec:     1517 || Batch Translation Loss:   0.052111 => Txt Tokens per Sec:     4191 || Lr: 0.000100
2024-02-05 19:44:45,979 Epoch 212: Total Training Recognition Loss 0.53  Total Training Translation Loss 19.10 
2024-02-05 19:44:45,979 EPOCH 213
2024-02-05 19:44:46,264 [Epoch: 213 Step: 00028200] Batch Recognition Loss:   0.002273 => Gls Tokens per Sec:     1127 || Batch Translation Loss:   0.595716 => Txt Tokens per Sec:     3246 || Lr: 0.000100
2024-02-05 19:44:51,943 [Epoch: 213 Step: 00028300] Batch Recognition Loss:   0.001186 => Gls Tokens per Sec:     1409 || Batch Translation Loss:   0.095266 => Txt Tokens per Sec:     3898 || Lr: 0.000100
2024-02-05 19:44:53,513 Epoch 213: Total Training Recognition Loss 0.74  Total Training Translation Loss 21.37 
2024-02-05 19:44:53,513 EPOCH 214
2024-02-05 19:44:57,442 [Epoch: 214 Step: 00028400] Batch Recognition Loss:   0.004628 => Gls Tokens per Sec:     1446 || Batch Translation Loss:   0.161001 => Txt Tokens per Sec:     4029 || Lr: 0.000100
2024-02-05 19:45:00,826 Epoch 214: Total Training Recognition Loss 0.71  Total Training Translation Loss 17.33 
2024-02-05 19:45:00,827 EPOCH 215
2024-02-05 19:45:02,842 [Epoch: 215 Step: 00028500] Batch Recognition Loss:   0.003514 => Gls Tokens per Sec:     1510 || Batch Translation Loss:   0.039136 => Txt Tokens per Sec:     4012 || Lr: 0.000100
2024-02-05 19:45:08,188 Epoch 215: Total Training Recognition Loss 0.49  Total Training Translation Loss 15.28 
2024-02-05 19:45:08,189 EPOCH 216
2024-02-05 19:45:08,471 [Epoch: 216 Step: 00028600] Batch Recognition Loss:   0.000598 => Gls Tokens per Sec:     1423 || Batch Translation Loss:   0.047002 => Txt Tokens per Sec:     4085 || Lr: 0.000100
2024-02-05 19:45:14,047 [Epoch: 216 Step: 00028700] Batch Recognition Loss:   0.001982 => Gls Tokens per Sec:     1435 || Batch Translation Loss:   0.286819 => Txt Tokens per Sec:     3938 || Lr: 0.000100
2024-02-05 19:45:15,763 Epoch 216: Total Training Recognition Loss 0.86  Total Training Translation Loss 25.64 
2024-02-05 19:45:15,763 EPOCH 217
2024-02-05 19:45:19,698 [Epoch: 217 Step: 00028800] Batch Recognition Loss:   0.008845 => Gls Tokens per Sec:     1465 || Batch Translation Loss:   0.336679 => Txt Tokens per Sec:     4126 || Lr: 0.000100
2024-02-05 19:45:22,976 Epoch 217: Total Training Recognition Loss 0.55  Total Training Translation Loss 32.91 
2024-02-05 19:45:22,976 EPOCH 218
2024-02-05 19:45:25,352 [Epoch: 218 Step: 00028900] Batch Recognition Loss:   0.010336 => Gls Tokens per Sec:     1313 || Batch Translation Loss:   0.130726 => Txt Tokens per Sec:     3693 || Lr: 0.000100
2024-02-05 19:45:30,701 Epoch 218: Total Training Recognition Loss 0.62  Total Training Translation Loss 28.33 
2024-02-05 19:45:30,702 EPOCH 219
2024-02-05 19:45:31,009 [Epoch: 219 Step: 00029000] Batch Recognition Loss:   0.001940 => Gls Tokens per Sec:     1565 || Batch Translation Loss:   0.836607 => Txt Tokens per Sec:     4298 || Lr: 0.000100
2024-02-05 19:45:36,583 [Epoch: 219 Step: 00029100] Batch Recognition Loss:   0.000434 => Gls Tokens per Sec:     1432 || Batch Translation Loss:   0.264208 => Txt Tokens per Sec:     3945 || Lr: 0.000100
2024-02-05 19:45:38,130 Epoch 219: Total Training Recognition Loss 0.45  Total Training Translation Loss 20.08 
2024-02-05 19:45:38,130 EPOCH 220
2024-02-05 19:45:41,754 [Epoch: 220 Step: 00029200] Batch Recognition Loss:   0.000544 => Gls Tokens per Sec:     1612 || Batch Translation Loss:   0.122029 => Txt Tokens per Sec:     4348 || Lr: 0.000100
2024-02-05 19:45:45,214 Epoch 220: Total Training Recognition Loss 0.77  Total Training Translation Loss 15.55 
2024-02-05 19:45:45,214 EPOCH 221
2024-02-05 19:45:47,314 [Epoch: 221 Step: 00029300] Batch Recognition Loss:   0.046856 => Gls Tokens per Sec:     1525 || Batch Translation Loss:   0.035005 => Txt Tokens per Sec:     4192 || Lr: 0.000100
2024-02-05 19:45:52,539 Epoch 221: Total Training Recognition Loss 0.54  Total Training Translation Loss 15.31 
2024-02-05 19:45:52,539 EPOCH 222
2024-02-05 19:45:52,952 [Epoch: 222 Step: 00029400] Batch Recognition Loss:   0.000142 => Gls Tokens per Sec:     1363 || Batch Translation Loss:   0.045645 => Txt Tokens per Sec:     3718 || Lr: 0.000100
2024-02-05 19:45:59,020 [Epoch: 222 Step: 00029500] Batch Recognition Loss:   0.000302 => Gls Tokens per Sec:     1315 || Batch Translation Loss:   0.051471 => Txt Tokens per Sec:     3649 || Lr: 0.000100
2024-02-05 19:46:00,513 Epoch 222: Total Training Recognition Loss 0.49  Total Training Translation Loss 23.32 
2024-02-05 19:46:00,513 EPOCH 223
2024-02-05 19:46:04,597 [Epoch: 223 Step: 00029600] Batch Recognition Loss:   0.002204 => Gls Tokens per Sec:     1450 || Batch Translation Loss:   0.239862 => Txt Tokens per Sec:     3944 || Lr: 0.000100
2024-02-05 19:46:07,917 Epoch 223: Total Training Recognition Loss 0.51  Total Training Translation Loss 19.59 
2024-02-05 19:46:07,917 EPOCH 224
2024-02-05 19:46:09,975 [Epoch: 224 Step: 00029700] Batch Recognition Loss:   0.000844 => Gls Tokens per Sec:     1594 || Batch Translation Loss:   0.064286 => Txt Tokens per Sec:     4458 || Lr: 0.000100
2024-02-05 19:46:15,030 Epoch 224: Total Training Recognition Loss 0.44  Total Training Translation Loss 19.58 
2024-02-05 19:46:15,031 EPOCH 225
2024-02-05 19:46:15,470 [Epoch: 225 Step: 00029800] Batch Recognition Loss:   0.000481 => Gls Tokens per Sec:     1461 || Batch Translation Loss:   0.109525 => Txt Tokens per Sec:     4235 || Lr: 0.000100
2024-02-05 19:46:20,678 [Epoch: 225 Step: 00029900] Batch Recognition Loss:   0.001487 => Gls Tokens per Sec:     1532 || Batch Translation Loss:   0.556110 => Txt Tokens per Sec:     4214 || Lr: 0.000100
2024-02-05 19:46:22,128 Epoch 225: Total Training Recognition Loss 0.32  Total Training Translation Loss 19.63 
2024-02-05 19:46:22,128 EPOCH 226
2024-02-05 19:46:26,225 [Epoch: 226 Step: 00030000] Batch Recognition Loss:   0.003173 => Gls Tokens per Sec:     1465 || Batch Translation Loss:   0.061715 => Txt Tokens per Sec:     4091 || Lr: 0.000100
2024-02-05 19:46:37,853 Validation result at epoch 226, step    30000: duration: 11.6274s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 7.65807	Translation Loss: 94322.59375	PPL: 12344.35059
	Eval Metric: BLEU
	WER 4.31	(DEL: 0.00,	INS: 0.00,	SUB: 4.31)
	BLEU-4 0.44	(BLEU-1: 10.22,	BLEU-2: 2.93,	BLEU-3: 0.96,	BLEU-4: 0.44)
	CHRF 17.10	ROUGE 8.66
2024-02-05 19:46:37,854 Logging Recognition and Translation Outputs
2024-02-05 19:46:37,855 ========================================================================================================================
2024-02-05 19:46:37,855 Logging Sequence: 146_102.00
2024-02-05 19:46:37,855 	Gloss Reference :	A B+C+D+E
2024-02-05 19:46:37,855 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:46:37,855 	Gloss Alignment :	         
2024-02-05 19:46:37,855 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:46:37,857 	Text Reference  :	famous indian champion players like kidambi     srikanth and      ashwini ponappa  have tested positive for coronavirus
2024-02-05 19:46:37,857 	Text Hypothesis :	****** ****** rohit    sharma  a    accompanied by       mohammed shami   fielding with a      caption  at  home       
2024-02-05 19:46:37,857 	Text Alignment  :	D      D      S        S       S    S           S        S        S       S        S    S      S        S   S          
2024-02-05 19:46:37,857 ========================================================================================================================
2024-02-05 19:46:37,857 Logging Sequence: 53_178.00
2024-02-05 19:46:37,857 	Gloss Reference :	A B+C+D+E
2024-02-05 19:46:37,858 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:46:37,858 	Gloss Alignment :	         
2024-02-05 19:46:37,858 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:46:37,859 	Text Reference  :	***** the money would help   all  those affected by       the  humanitarian crisis in  afghanistan
2024-02-05 19:46:37,859 	Text Hypothesis :	after the ***** match people will play  against  pakistan such a            huge   fan following  
2024-02-05 19:46:37,859 	Text Alignment  :	I         D     S     S      S    S     S        S        S    S            S      S   S          
2024-02-05 19:46:37,859 ========================================================================================================================
2024-02-05 19:46:37,859 Logging Sequence: 129_200.00
2024-02-05 19:46:37,860 	Gloss Reference :	A B+C+D+E  
2024-02-05 19:46:37,860 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-05 19:46:37,860 	Gloss Alignment :	  S        
2024-02-05 19:46:37,860 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:46:37,861 	Text Reference  :	*** ****** the   ioc    would lose  about  4  billion if   the olympics were   to  be       cancelled
2024-02-05 19:46:37,861 	Text Hypothesis :	now fouaad mirza became the   first indian to reach   some of  aerated  drinks and venomous abuses   
2024-02-05 19:46:37,861 	Text Alignment  :	I   I      S     S      S     S     S      S  S       S    S   S        S      S   S        S        
2024-02-05 19:46:37,862 ========================================================================================================================
2024-02-05 19:46:37,862 Logging Sequence: 77_2.00
2024-02-05 19:46:37,862 	Gloss Reference :	A B+C+D+E  
2024-02-05 19:46:37,862 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-05 19:46:37,862 	Gloss Alignment :	  S        
2024-02-05 19:46:37,862 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:46:37,864 	Text Reference  :	on    25th april     the           ipl match between    sunrisers hyderabad and   delhi capitals ended in a   tie    
2024-02-05 19:46:37,864 	Text Hypothesis :	india is   currently participating in  the   tournament against   sri       lanka while playing  out   of the taliban
2024-02-05 19:46:37,864 	Text Alignment  :	S     S    S         S             S   S     S          S         S         S     S     S        S     S  S   S      
2024-02-05 19:46:37,864 ========================================================================================================================
2024-02-05 19:46:37,864 Logging Sequence: 119_170.00
2024-02-05 19:46:37,864 	Gloss Reference :	A B+C+D+E
2024-02-05 19:46:37,865 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:46:37,865 	Gloss Alignment :	         
2024-02-05 19:46:37,865 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:46:37,866 	Text Reference  :	they said  it was    a  proud   moment messi is    a   big   hearted man
2024-02-05 19:46:37,866 	Text Hypothesis :	**** about 3  months of winning the    fifa  world cup messi met     ben
2024-02-05 19:46:37,866 	Text Alignment  :	D    S     S  S      S  S       S      S     S     S   S     S       S  
2024-02-05 19:46:37,866 ========================================================================================================================
2024-02-05 19:46:41,411 Epoch 226: Total Training Recognition Loss 0.40  Total Training Translation Loss 16.39 
2024-02-05 19:46:41,411 EPOCH 227
2024-02-05 19:46:44,030 [Epoch: 227 Step: 00030100] Batch Recognition Loss:   0.001246 => Gls Tokens per Sec:     1284 || Batch Translation Loss:   0.032206 => Txt Tokens per Sec:     3598 || Lr: 0.000050
2024-02-05 19:46:48,783 Epoch 227: Total Training Recognition Loss 0.26  Total Training Translation Loss 8.37 
2024-02-05 19:46:48,784 EPOCH 228
2024-02-05 19:46:49,218 [Epoch: 228 Step: 00030200] Batch Recognition Loss:   0.000255 => Gls Tokens per Sec:     1663 || Batch Translation Loss:   0.030048 => Txt Tokens per Sec:     4584 || Lr: 0.000050
2024-02-05 19:46:54,496 [Epoch: 228 Step: 00030300] Batch Recognition Loss:   0.000549 => Gls Tokens per Sec:     1516 || Batch Translation Loss:   0.017663 => Txt Tokens per Sec:     4175 || Lr: 0.000050
2024-02-05 19:46:55,869 Epoch 228: Total Training Recognition Loss 0.27  Total Training Translation Loss 5.24 
2024-02-05 19:46:55,869 EPOCH 229
2024-02-05 19:46:59,942 [Epoch: 229 Step: 00030400] Batch Recognition Loss:   0.000496 => Gls Tokens per Sec:     1494 || Batch Translation Loss:   0.047325 => Txt Tokens per Sec:     4146 || Lr: 0.000050
2024-02-05 19:47:03,129 Epoch 229: Total Training Recognition Loss 0.35  Total Training Translation Loss 4.20 
2024-02-05 19:47:03,130 EPOCH 230
2024-02-05 19:47:05,460 [Epoch: 230 Step: 00030500] Batch Recognition Loss:   0.009954 => Gls Tokens per Sec:     1477 || Batch Translation Loss:   0.011675 => Txt Tokens per Sec:     4068 || Lr: 0.000050
2024-02-05 19:47:10,277 Epoch 230: Total Training Recognition Loss 0.21  Total Training Translation Loss 3.95 
2024-02-05 19:47:10,277 EPOCH 231
2024-02-05 19:47:10,745 [Epoch: 231 Step: 00030600] Batch Recognition Loss:   0.000608 => Gls Tokens per Sec:     1713 || Batch Translation Loss:   0.030592 => Txt Tokens per Sec:     4987 || Lr: 0.000050
2024-02-05 19:47:16,354 [Epoch: 231 Step: 00030700] Batch Recognition Loss:   0.000144 => Gls Tokens per Sec:     1426 || Batch Translation Loss:   0.016607 => Txt Tokens per Sec:     3941 || Lr: 0.000050
2024-02-05 19:47:17,592 Epoch 231: Total Training Recognition Loss 0.28  Total Training Translation Loss 4.37 
2024-02-05 19:47:17,593 EPOCH 232
2024-02-05 19:47:21,807 [Epoch: 232 Step: 00030800] Batch Recognition Loss:   0.000214 => Gls Tokens per Sec:     1462 || Batch Translation Loss:   0.026221 => Txt Tokens per Sec:     4112 || Lr: 0.000050
2024-02-05 19:47:24,944 Epoch 232: Total Training Recognition Loss 0.44  Total Training Translation Loss 5.24 
2024-02-05 19:47:24,944 EPOCH 233
2024-02-05 19:47:27,207 [Epoch: 233 Step: 00030900] Batch Recognition Loss:   0.001399 => Gls Tokens per Sec:     1556 || Batch Translation Loss:   0.025199 => Txt Tokens per Sec:     4288 || Lr: 0.000050
2024-02-05 19:47:32,156 Epoch 233: Total Training Recognition Loss 0.28  Total Training Translation Loss 3.96 
2024-02-05 19:47:32,156 EPOCH 234
2024-02-05 19:47:32,650 [Epoch: 234 Step: 00031000] Batch Recognition Loss:   0.007387 => Gls Tokens per Sec:     1789 || Batch Translation Loss:   0.009975 => Txt Tokens per Sec:     4778 || Lr: 0.000050
2024-02-05 19:47:38,003 [Epoch: 234 Step: 00031100] Batch Recognition Loss:   0.000172 => Gls Tokens per Sec:     1495 || Batch Translation Loss:   0.015027 => Txt Tokens per Sec:     4141 || Lr: 0.000050
2024-02-05 19:47:39,439 Epoch 234: Total Training Recognition Loss 0.22  Total Training Translation Loss 5.27 
2024-02-05 19:47:39,439 EPOCH 235
2024-02-05 19:47:43,734 [Epoch: 235 Step: 00031200] Batch Recognition Loss:   0.003804 => Gls Tokens per Sec:     1453 || Batch Translation Loss:   0.013166 => Txt Tokens per Sec:     4119 || Lr: 0.000050
2024-02-05 19:47:46,489 Epoch 235: Total Training Recognition Loss 0.18  Total Training Translation Loss 4.51 
2024-02-05 19:47:46,489 EPOCH 236
2024-02-05 19:47:49,028 [Epoch: 236 Step: 00031300] Batch Recognition Loss:   0.000687 => Gls Tokens per Sec:     1419 || Batch Translation Loss:   0.019849 => Txt Tokens per Sec:     4084 || Lr: 0.000050
2024-02-05 19:47:53,743 Epoch 236: Total Training Recognition Loss 0.15  Total Training Translation Loss 6.59 
2024-02-05 19:47:53,743 EPOCH 237
2024-02-05 19:47:54,286 [Epoch: 237 Step: 00031400] Batch Recognition Loss:   0.006976 => Gls Tokens per Sec:     1771 || Batch Translation Loss:   0.006448 => Txt Tokens per Sec:     4838 || Lr: 0.000050
2024-02-05 19:47:59,700 [Epoch: 237 Step: 00031500] Batch Recognition Loss:   0.000372 => Gls Tokens per Sec:     1477 || Batch Translation Loss:   0.038629 => Txt Tokens per Sec:     4067 || Lr: 0.000050
2024-02-05 19:48:01,035 Epoch 237: Total Training Recognition Loss 0.25  Total Training Translation Loss 9.99 
2024-02-05 19:48:01,035 EPOCH 238
2024-02-05 19:48:05,592 [Epoch: 238 Step: 00031600] Batch Recognition Loss:   0.000350 => Gls Tokens per Sec:     1387 || Batch Translation Loss:   0.104881 => Txt Tokens per Sec:     3816 || Lr: 0.000050
2024-02-05 19:48:08,760 Epoch 238: Total Training Recognition Loss 0.30  Total Training Translation Loss 10.26 
2024-02-05 19:48:08,761 EPOCH 239
2024-02-05 19:48:11,164 [Epoch: 239 Step: 00031700] Batch Recognition Loss:   0.000229 => Gls Tokens per Sec:     1532 || Batch Translation Loss:   0.029104 => Txt Tokens per Sec:     4235 || Lr: 0.000050
2024-02-05 19:48:15,849 Epoch 239: Total Training Recognition Loss 0.36  Total Training Translation Loss 7.11 
2024-02-05 19:48:15,850 EPOCH 240
2024-02-05 19:48:16,714 [Epoch: 240 Step: 00031800] Batch Recognition Loss:   0.000240 => Gls Tokens per Sec:     1205 || Batch Translation Loss:   0.022870 => Txt Tokens per Sec:     3217 || Lr: 0.000050
2024-02-05 19:48:22,520 [Epoch: 240 Step: 00031900] Batch Recognition Loss:   0.002667 => Gls Tokens per Sec:     1375 || Batch Translation Loss:   0.227411 => Txt Tokens per Sec:     3812 || Lr: 0.000050
2024-02-05 19:48:23,529 Epoch 240: Total Training Recognition Loss 0.22  Total Training Translation Loss 9.52 
2024-02-05 19:48:23,529 EPOCH 241
2024-02-05 19:48:27,830 [Epoch: 241 Step: 00032000] Batch Recognition Loss:   0.006518 => Gls Tokens per Sec:     1489 || Batch Translation Loss:   0.016053 => Txt Tokens per Sec:     4043 || Lr: 0.000050
2024-02-05 19:48:38,665 Validation result at epoch 241, step    32000: duration: 10.8350s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 6.60586	Translation Loss: 94054.10156	PPL: 12017.70703
	Eval Metric: BLEU
	WER 4.24	(DEL: 0.00,	INS: 0.00,	SUB: 4.24)
	BLEU-4 0.52	(BLEU-1: 11.41,	BLEU-2: 3.52,	BLEU-3: 1.20,	BLEU-4: 0.52)
	CHRF 17.06	ROUGE 9.41
2024-02-05 19:48:38,666 Logging Recognition and Translation Outputs
2024-02-05 19:48:38,666 ========================================================================================================================
2024-02-05 19:48:38,666 Logging Sequence: 162_133.00
2024-02-05 19:48:38,667 	Gloss Reference :	A B+C+D+E
2024-02-05 19:48:38,667 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:48:38,667 	Gloss Alignment :	         
2024-02-05 19:48:38,667 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:48:38,668 	Text Reference  :	they also sent rape threats to       his    9-month old daughter
2024-02-05 19:48:38,668 	Text Hypothesis :	**** pm   modi and  pm      albanese handed over    the team    
2024-02-05 19:48:38,668 	Text Alignment  :	D    S    S    S    S       S        S      S       S   S       
2024-02-05 19:48:38,668 ========================================================================================================================
2024-02-05 19:48:38,668 Logging Sequence: 134_236.00
2024-02-05 19:48:38,668 	Gloss Reference :	A B+C+D+E
2024-02-05 19:48:38,668 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:48:38,669 	Gloss Alignment :	         
2024-02-05 19:48:38,669 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:48:38,670 	Text Reference  :	** **** after the     interaction modi tweeted the   images  and   captioned it saying
2024-02-05 19:48:38,670 	Text Hypothesis :	pm modi even  chatted with        5    time    medal winning women will      be hanged
2024-02-05 19:48:38,670 	Text Alignment  :	I  I    S     S       S           S    S       S     S       S     S         S  S     
2024-02-05 19:48:38,670 ========================================================================================================================
2024-02-05 19:48:38,670 Logging Sequence: 145_52.00
2024-02-05 19:48:38,670 	Gloss Reference :	A B+C+D+E
2024-02-05 19:48:38,671 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:48:38,671 	Gloss Alignment :	         
2024-02-05 19:48:38,671 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:48:38,672 	Text Reference  :	her       name was  dropped despite having qualified as  she   was the only female athlete  
2024-02-05 19:48:38,672 	Text Hypothesis :	yesterday on   23rd may     2023    in     the       ipl there was a   huge fan    following
2024-02-05 19:48:38,672 	Text Alignment  :	S         S    S    S       S       S      S         S   S         S   S    S      S        
2024-02-05 19:48:38,672 ========================================================================================================================
2024-02-05 19:48:38,672 Logging Sequence: 175_40.00
2024-02-05 19:48:38,673 	Gloss Reference :	A B+C+D+E
2024-02-05 19:48:38,673 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:48:38,673 	Gloss Alignment :	         
2024-02-05 19:48:38,673 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:48:38,674 	Text Reference  :	***** *** *** **** * soumyadeep and   shreya bagged three   medals each    including a  silver medal each 
2024-02-05 19:48:38,674 	Text Hypothesis :	india did not have a t20        world cup    match  between 1st    october 2023      in the    same  place
2024-02-05 19:48:38,674 	Text Alignment  :	I     I   I   I    I S          S     S      S      S       S      S       S         S  S      S     S    
2024-02-05 19:48:38,675 ========================================================================================================================
2024-02-05 19:48:38,675 Logging Sequence: 156_51.00
2024-02-05 19:48:38,675 	Gloss Reference :	A B+C+D+E
2024-02-05 19:48:38,675 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:48:38,675 	Gloss Alignment :	         
2024-02-05 19:48:38,675 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:48:38,676 	Text Reference  :	the selection of  the players was similar to    that of   ipl 
2024-02-05 19:48:38,676 	Text Hypothesis :	*** in        ipl we  have    kkr and     video was  very soon
2024-02-05 19:48:38,676 	Text Alignment  :	D   S         S   S   S       S   S       S     S    S    S   
2024-02-05 19:48:38,676 ========================================================================================================================
2024-02-05 19:48:41,688 Epoch 241: Total Training Recognition Loss 0.29  Total Training Translation Loss 8.00 
2024-02-05 19:48:41,688 EPOCH 242
2024-02-05 19:48:44,242 [Epoch: 242 Step: 00032100] Batch Recognition Loss:   0.000211 => Gls Tokens per Sec:     1473 || Batch Translation Loss:   0.020874 => Txt Tokens per Sec:     4203 || Lr: 0.000050
2024-02-05 19:48:49,017 Epoch 242: Total Training Recognition Loss 0.22  Total Training Translation Loss 7.04 
2024-02-05 19:48:49,017 EPOCH 243
2024-02-05 19:48:49,718 [Epoch: 243 Step: 00032200] Batch Recognition Loss:   0.000309 => Gls Tokens per Sec:     1600 || Batch Translation Loss:   0.015928 => Txt Tokens per Sec:     4451 || Lr: 0.000050
2024-02-05 19:48:55,165 [Epoch: 243 Step: 00032300] Batch Recognition Loss:   0.000446 => Gls Tokens per Sec:     1469 || Batch Translation Loss:   0.045436 => Txt Tokens per Sec:     4098 || Lr: 0.000050
2024-02-05 19:48:56,226 Epoch 243: Total Training Recognition Loss 0.24  Total Training Translation Loss 7.44 
2024-02-05 19:48:56,226 EPOCH 244
2024-02-05 19:49:00,625 [Epoch: 244 Step: 00032400] Batch Recognition Loss:   0.000284 => Gls Tokens per Sec:     1473 || Batch Translation Loss:   0.037663 => Txt Tokens per Sec:     4011 || Lr: 0.000050
2024-02-05 19:49:03,651 Epoch 244: Total Training Recognition Loss 0.18  Total Training Translation Loss 7.54 
2024-02-05 19:49:03,652 EPOCH 245
2024-02-05 19:49:06,442 [Epoch: 245 Step: 00032500] Batch Recognition Loss:   0.000248 => Gls Tokens per Sec:     1377 || Batch Translation Loss:   0.027276 => Txt Tokens per Sec:     3746 || Lr: 0.000050
2024-02-05 19:49:11,050 Epoch 245: Total Training Recognition Loss 0.15  Total Training Translation Loss 6.02 
2024-02-05 19:49:11,050 EPOCH 246
2024-02-05 19:49:11,815 [Epoch: 246 Step: 00032600] Batch Recognition Loss:   0.002604 => Gls Tokens per Sec:     1573 || Batch Translation Loss:   0.021141 => Txt Tokens per Sec:     4277 || Lr: 0.000050
2024-02-05 19:49:16,978 [Epoch: 246 Step: 00032700] Batch Recognition Loss:   0.000224 => Gls Tokens per Sec:     1549 || Batch Translation Loss:   0.024256 => Txt Tokens per Sec:     4308 || Lr: 0.000050
2024-02-05 19:49:18,042 Epoch 246: Total Training Recognition Loss 0.19  Total Training Translation Loss 5.66 
2024-02-05 19:49:18,042 EPOCH 247
2024-02-05 19:49:22,900 [Epoch: 247 Step: 00032800] Batch Recognition Loss:   0.000287 => Gls Tokens per Sec:     1350 || Batch Translation Loss:   0.009770 => Txt Tokens per Sec:     3772 || Lr: 0.000050
2024-02-05 19:49:25,560 Epoch 247: Total Training Recognition Loss 0.29  Total Training Translation Loss 6.37 
2024-02-05 19:49:25,560 EPOCH 248
2024-02-05 19:49:27,998 [Epoch: 248 Step: 00032900] Batch Recognition Loss:   0.002946 => Gls Tokens per Sec:     1609 || Batch Translation Loss:   0.037818 => Txt Tokens per Sec:     4474 || Lr: 0.000050
2024-02-05 19:49:32,776 Epoch 248: Total Training Recognition Loss 0.15  Total Training Translation Loss 6.92 
2024-02-05 19:49:32,777 EPOCH 249
2024-02-05 19:49:33,571 [Epoch: 249 Step: 00033000] Batch Recognition Loss:   0.016952 => Gls Tokens per Sec:     1615 || Batch Translation Loss:   0.009319 => Txt Tokens per Sec:     4164 || Lr: 0.000050
2024-02-05 19:49:38,891 [Epoch: 249 Step: 00033100] Batch Recognition Loss:   0.000146 => Gls Tokens per Sec:     1501 || Batch Translation Loss:   0.020259 => Txt Tokens per Sec:     4201 || Lr: 0.000050
2024-02-05 19:49:39,900 Epoch 249: Total Training Recognition Loss 0.16  Total Training Translation Loss 7.38 
2024-02-05 19:49:39,901 EPOCH 250
2024-02-05 19:49:44,600 [Epoch: 250 Step: 00033200] Batch Recognition Loss:   0.000242 => Gls Tokens per Sec:     1414 || Batch Translation Loss:   0.035488 => Txt Tokens per Sec:     3958 || Lr: 0.000050
2024-02-05 19:49:47,616 Epoch 250: Total Training Recognition Loss 0.21  Total Training Translation Loss 7.92 
2024-02-05 19:49:47,617 EPOCH 251
2024-02-05 19:49:50,362 [Epoch: 251 Step: 00033300] Batch Recognition Loss:   0.000276 => Gls Tokens per Sec:     1458 || Batch Translation Loss:   0.023317 => Txt Tokens per Sec:     3847 || Lr: 0.000050
2024-02-05 19:49:54,884 Epoch 251: Total Training Recognition Loss 0.24  Total Training Translation Loss 6.14 
2024-02-05 19:49:54,884 EPOCH 252
2024-02-05 19:49:55,736 [Epoch: 252 Step: 00033400] Batch Recognition Loss:   0.000486 => Gls Tokens per Sec:     1596 || Batch Translation Loss:   0.026519 => Txt Tokens per Sec:     4237 || Lr: 0.000050
2024-02-05 19:50:01,548 [Epoch: 252 Step: 00033500] Batch Recognition Loss:   0.000522 => Gls Tokens per Sec:     1373 || Batch Translation Loss:   0.023900 => Txt Tokens per Sec:     3821 || Lr: 0.000050
2024-02-05 19:50:02,449 Epoch 252: Total Training Recognition Loss 0.09  Total Training Translation Loss 5.56 
2024-02-05 19:50:02,449 EPOCH 253
2024-02-05 19:50:07,353 [Epoch: 253 Step: 00033600] Batch Recognition Loss:   0.003410 => Gls Tokens per Sec:     1370 || Batch Translation Loss:   0.026217 => Txt Tokens per Sec:     3762 || Lr: 0.000050
2024-02-05 19:50:10,158 Epoch 253: Total Training Recognition Loss 0.12  Total Training Translation Loss 6.10 
2024-02-05 19:50:10,158 EPOCH 254
2024-02-05 19:50:12,855 [Epoch: 254 Step: 00033700] Batch Recognition Loss:   0.000380 => Gls Tokens per Sec:     1513 || Batch Translation Loss:   0.014126 => Txt Tokens per Sec:     4251 || Lr: 0.000050
2024-02-05 19:50:17,358 Epoch 254: Total Training Recognition Loss 0.26  Total Training Translation Loss 4.63 
2024-02-05 19:50:17,359 EPOCH 255
2024-02-05 19:50:18,463 [Epoch: 255 Step: 00033800] Batch Recognition Loss:   0.000403 => Gls Tokens per Sec:     1306 || Batch Translation Loss:   0.010380 => Txt Tokens per Sec:     3664 || Lr: 0.000050
2024-02-05 19:50:23,948 [Epoch: 255 Step: 00033900] Batch Recognition Loss:   0.000319 => Gls Tokens per Sec:     1455 || Batch Translation Loss:   0.012989 => Txt Tokens per Sec:     4038 || Lr: 0.000050
2024-02-05 19:50:24,856 Epoch 255: Total Training Recognition Loss 0.23  Total Training Translation Loss 4.70 
2024-02-05 19:50:24,856 EPOCH 256
2024-02-05 19:50:29,062 [Epoch: 256 Step: 00034000] Batch Recognition Loss:   0.001660 => Gls Tokens per Sec:     1617 || Batch Translation Loss:   0.013072 => Txt Tokens per Sec:     4399 || Lr: 0.000050
2024-02-05 19:50:40,102 Validation result at epoch 256, step    34000: duration: 11.0394s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 6.76804	Translation Loss: 92661.48438	PPL: 10457.15918
	Eval Metric: BLEU
	WER 4.10	(DEL: 0.00,	INS: 0.00,	SUB: 4.10)
	BLEU-4 0.00	(BLEU-1: 10.38,	BLEU-2: 3.09,	BLEU-3: 1.03,	BLEU-4: 0.00)
	CHRF 16.80	ROUGE 8.80
2024-02-05 19:50:40,103 Logging Recognition and Translation Outputs
2024-02-05 19:50:40,104 ========================================================================================================================
2024-02-05 19:50:40,104 Logging Sequence: 171_158.00
2024-02-05 19:50:40,104 	Gloss Reference :	A B+C+D+E
2024-02-05 19:50:40,104 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:50:40,104 	Gloss Alignment :	         
2024-02-05 19:50:40,104 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:50:40,105 	Text Reference  :	with speculations of dhoni being banned are spreading many say     that it    is  unlikely to    happen   
2024-02-05 19:50:40,105 	Text Hypothesis :	**** ************ ** ***** ***** ****** *** ********* **** however the  venue saw heavy    rains yesterday
2024-02-05 19:50:40,105 	Text Alignment  :	D    D            D  D     D     D      D   D         D    S       S    S     S   S        S     S        
2024-02-05 19:50:40,105 ========================================================================================================================
2024-02-05 19:50:40,106 Logging Sequence: 108_235.00
2024-02-05 19:50:40,106 	Gloss Reference :	A B+C+D+E
2024-02-05 19:50:40,106 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:50:40,106 	Gloss Alignment :	         
2024-02-05 19:50:40,106 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:50:40,108 	Text Reference  :	****** he   was  taken to   the ****** ********* **** hospital and it was reported that he is not   in any     danger
2024-02-05 19:50:40,108 	Text Hypothesis :	people were very happy that the player practices very hard     and it was ******** **** ** ** proud of india's win   
2024-02-05 19:50:40,108 	Text Alignment  :	I      S    S    S     S        I      I         I    S                   D        D    D  D  S     S  S       S     
2024-02-05 19:50:40,108 ========================================================================================================================
2024-02-05 19:50:40,108 Logging Sequence: 153_206.00
2024-02-05 19:50:40,109 	Gloss Reference :	A B+C+D+E
2024-02-05 19:50:40,109 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:50:40,109 	Gloss Alignment :	         
2024-02-05 19:50:40,109 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:50:40,110 	Text Reference  :	*** **** ***** *** **** now on  13th november  everyone is    hoping pakistan rewrites history
2024-02-05 19:50:40,110 	Text Hypothesis :	the 2022 final was held in  the same melbourne stadium  where the    1992     world    cup    
2024-02-05 19:50:40,110 	Text Alignment  :	I   I    I     I   I    S   S   S    S         S        S     S      S        S        S      
2024-02-05 19:50:40,110 ========================================================================================================================
2024-02-05 19:50:40,110 Logging Sequence: 87_202.00
2024-02-05 19:50:40,111 	Gloss Reference :	A B+C+D+E
2024-02-05 19:50:40,111 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:50:40,111 	Gloss Alignment :	         
2024-02-05 19:50:40,111 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:50:40,112 	Text Reference  :	*** **** ******** ** *** ** *** ** i  love  our          players and i         love my     country
2024-02-05 19:50:40,112 	Text Hypothesis :	the vast majority of the of the 56 of india participated in      the followers on   social media  
2024-02-05 19:50:40,112 	Text Alignment  :	I   I    I        I  I   I  I   I  S  S     S            S       S   S         S    S      S      
2024-02-05 19:50:40,112 ========================================================================================================================
2024-02-05 19:50:40,112 Logging Sequence: 84_2.00
2024-02-05 19:50:40,113 	Gloss Reference :	A B+C+D+E
2024-02-05 19:50:40,113 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:50:40,113 	Gloss Alignment :	         
2024-02-05 19:50:40,113 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:50:40,115 	Text Reference  :	the 2022 fifa football world  cup     is going on   in   qatar from 20th november 2022 to **** 18th december 2022   
2024-02-05 19:50:40,115 	Text Hypothesis :	*** **** bcci chief    sourav ganguly on 28th  june said that  she  does not      want to wait for  the      auction
2024-02-05 19:50:40,115 	Text Alignment  :	D   D    S    S        S      S       S  S     S    S    S     S    S    S        S       I    S    S        S      
2024-02-05 19:50:40,115 ========================================================================================================================
2024-02-05 19:50:42,934 Epoch 256: Total Training Recognition Loss 0.23  Total Training Translation Loss 4.29 
2024-02-05 19:50:42,935 EPOCH 257
2024-02-05 19:50:45,600 [Epoch: 257 Step: 00034100] Batch Recognition Loss:   0.000677 => Gls Tokens per Sec:     1562 || Batch Translation Loss:   0.018601 => Txt Tokens per Sec:     4223 || Lr: 0.000050
2024-02-05 19:50:50,268 Epoch 257: Total Training Recognition Loss 0.12  Total Training Translation Loss 4.08 
2024-02-05 19:50:50,269 EPOCH 258
2024-02-05 19:50:51,181 [Epoch: 258 Step: 00034200] Batch Recognition Loss:   0.000081 => Gls Tokens per Sec:     1668 || Batch Translation Loss:   0.010611 => Txt Tokens per Sec:     4356 || Lr: 0.000050
2024-02-05 19:50:56,511 [Epoch: 258 Step: 00034300] Batch Recognition Loss:   0.000218 => Gls Tokens per Sec:     1497 || Batch Translation Loss:   0.021191 => Txt Tokens per Sec:     4180 || Lr: 0.000050
2024-02-05 19:50:57,228 Epoch 258: Total Training Recognition Loss 0.10  Total Training Translation Loss 5.12 
2024-02-05 19:50:57,228 EPOCH 259
2024-02-05 19:51:02,054 [Epoch: 259 Step: 00034400] Batch Recognition Loss:   0.000257 => Gls Tokens per Sec:     1426 || Batch Translation Loss:   0.032958 => Txt Tokens per Sec:     3976 || Lr: 0.000050
2024-02-05 19:51:04,786 Epoch 259: Total Training Recognition Loss 0.17  Total Training Translation Loss 6.68 
2024-02-05 19:51:04,787 EPOCH 260
2024-02-05 19:51:07,787 [Epoch: 260 Step: 00034500] Batch Recognition Loss:   0.000483 => Gls Tokens per Sec:     1414 || Batch Translation Loss:   0.047460 => Txt Tokens per Sec:     3958 || Lr: 0.000050
2024-02-05 19:51:12,181 Epoch 260: Total Training Recognition Loss 0.22  Total Training Translation Loss 13.39 
2024-02-05 19:51:12,181 EPOCH 261
2024-02-05 19:51:13,112 [Epoch: 261 Step: 00034600] Batch Recognition Loss:   0.000547 => Gls Tokens per Sec:     1720 || Batch Translation Loss:   0.032071 => Txt Tokens per Sec:     4510 || Lr: 0.000050
2024-02-05 19:51:17,985 [Epoch: 261 Step: 00034700] Batch Recognition Loss:   0.000177 => Gls Tokens per Sec:     1642 || Batch Translation Loss:   0.015986 => Txt Tokens per Sec:     4566 || Lr: 0.000050
2024-02-05 19:51:18,702 Epoch 261: Total Training Recognition Loss 0.22  Total Training Translation Loss 7.37 
2024-02-05 19:51:18,702 EPOCH 262
2024-02-05 19:51:23,388 [Epoch: 262 Step: 00034800] Batch Recognition Loss:   0.001167 => Gls Tokens per Sec:     1486 || Batch Translation Loss:   0.012936 => Txt Tokens per Sec:     4097 || Lr: 0.000050
2024-02-05 19:51:26,101 Epoch 262: Total Training Recognition Loss 0.18  Total Training Translation Loss 7.68 
2024-02-05 19:51:26,101 EPOCH 263
2024-02-05 19:51:29,125 [Epoch: 263 Step: 00034900] Batch Recognition Loss:   0.000531 => Gls Tokens per Sec:     1429 || Batch Translation Loss:   0.035052 => Txt Tokens per Sec:     4089 || Lr: 0.000050
2024-02-05 19:51:33,194 Epoch 263: Total Training Recognition Loss 0.32  Total Training Translation Loss 5.77 
2024-02-05 19:51:33,195 EPOCH 264
2024-02-05 19:51:34,369 [Epoch: 264 Step: 00035000] Batch Recognition Loss:   0.000545 => Gls Tokens per Sec:     1432 || Batch Translation Loss:   0.017745 => Txt Tokens per Sec:     3891 || Lr: 0.000050
2024-02-05 19:51:39,679 [Epoch: 264 Step: 00035100] Batch Recognition Loss:   0.014337 => Gls Tokens per Sec:     1503 || Batch Translation Loss:   0.019212 => Txt Tokens per Sec:     4170 || Lr: 0.000050
2024-02-05 19:51:40,374 Epoch 264: Total Training Recognition Loss 0.52  Total Training Translation Loss 5.53 
2024-02-05 19:51:40,374 EPOCH 265
2024-02-05 19:51:45,460 [Epoch: 265 Step: 00035200] Batch Recognition Loss:   0.000442 => Gls Tokens per Sec:     1385 || Batch Translation Loss:   0.020567 => Txt Tokens per Sec:     3854 || Lr: 0.000050
2024-02-05 19:51:47,951 Epoch 265: Total Training Recognition Loss 0.46  Total Training Translation Loss 5.27 
2024-02-05 19:51:47,951 EPOCH 266
2024-02-05 19:51:50,813 [Epoch: 266 Step: 00035300] Batch Recognition Loss:   0.000248 => Gls Tokens per Sec:     1538 || Batch Translation Loss:   0.013766 => Txt Tokens per Sec:     4312 || Lr: 0.000050
2024-02-05 19:51:55,102 Epoch 266: Total Training Recognition Loss 0.19  Total Training Translation Loss 6.17 
2024-02-05 19:51:55,102 EPOCH 267
2024-02-05 19:51:56,305 [Epoch: 267 Step: 00035400] Batch Recognition Loss:   0.000410 => Gls Tokens per Sec:     1464 || Batch Translation Loss:   0.029024 => Txt Tokens per Sec:     4356 || Lr: 0.000050
2024-02-05 19:52:01,421 [Epoch: 267 Step: 00035500] Batch Recognition Loss:   0.002102 => Gls Tokens per Sec:     1560 || Batch Translation Loss:   0.838122 => Txt Tokens per Sec:     4202 || Lr: 0.000050
2024-02-05 19:52:02,068 Epoch 267: Total Training Recognition Loss 0.20  Total Training Translation Loss 7.10 
2024-02-05 19:52:02,069 EPOCH 268
2024-02-05 19:52:07,050 [Epoch: 268 Step: 00035600] Batch Recognition Loss:   0.000216 => Gls Tokens per Sec:     1430 || Batch Translation Loss:   0.023400 => Txt Tokens per Sec:     3999 || Lr: 0.000050
2024-02-05 19:52:09,567 Epoch 268: Total Training Recognition Loss 0.18  Total Training Translation Loss 5.12 
2024-02-05 19:52:09,567 EPOCH 269
2024-02-05 19:52:12,655 [Epoch: 269 Step: 00035700] Batch Recognition Loss:   0.000189 => Gls Tokens per Sec:     1452 || Batch Translation Loss:   0.017386 => Txt Tokens per Sec:     3996 || Lr: 0.000050
2024-02-05 19:52:17,033 Epoch 269: Total Training Recognition Loss 0.15  Total Training Translation Loss 3.75 
2024-02-05 19:52:17,034 EPOCH 270
2024-02-05 19:52:18,267 [Epoch: 270 Step: 00035800] Batch Recognition Loss:   0.000285 => Gls Tokens per Sec:     1494 || Batch Translation Loss:   0.021357 => Txt Tokens per Sec:     4305 || Lr: 0.000050
2024-02-05 19:52:24,035 [Epoch: 270 Step: 00035900] Batch Recognition Loss:   0.001125 => Gls Tokens per Sec:     1383 || Batch Translation Loss:   0.041138 => Txt Tokens per Sec:     3807 || Lr: 0.000050
2024-02-05 19:52:24,534 Epoch 270: Total Training Recognition Loss 0.13  Total Training Translation Loss 5.26 
2024-02-05 19:52:24,534 EPOCH 271
2024-02-05 19:52:29,847 [Epoch: 271 Step: 00036000] Batch Recognition Loss:   0.000252 => Gls Tokens per Sec:     1355 || Batch Translation Loss:   0.015534 => Txt Tokens per Sec:     3775 || Lr: 0.000050
2024-02-05 19:52:42,242 Validation result at epoch 271, step    36000: duration: 12.3949s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 6.21384	Translation Loss: 92594.63281	PPL: 10387.57324
	Eval Metric: BLEU
	WER 3.95	(DEL: 0.00,	INS: 0.00,	SUB: 3.95)
	BLEU-4 0.36	(BLEU-1: 10.56,	BLEU-2: 3.04,	BLEU-3: 1.06,	BLEU-4: 0.36)
	CHRF 16.62	ROUGE 9.11
2024-02-05 19:52:42,244 Logging Recognition and Translation Outputs
2024-02-05 19:52:42,244 ========================================================================================================================
2024-02-05 19:52:42,244 Logging Sequence: 153_36.00
2024-02-05 19:52:42,244 	Gloss Reference :	A B+C+D+E
2024-02-05 19:52:42,244 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:52:42,244 	Gloss Alignment :	         
2024-02-05 19:52:42,244 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:52:42,245 	Text Reference  :	india made a    good  score   of    1686 in       20    overs 
2024-02-05 19:52:42,245 	Text Hypothesis :	in    the  same match between india and  pakistan south africa
2024-02-05 19:52:42,245 	Text Alignment  :	S     S    S    S     S       S     S    S        S     S     
2024-02-05 19:52:42,245 ========================================================================================================================
2024-02-05 19:52:42,246 Logging Sequence: 163_30.00
2024-02-05 19:52:42,246 	Gloss Reference :	A B+C+D+E
2024-02-05 19:52:42,246 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:52:42,246 	Gloss Alignment :	         
2024-02-05 19:52:42,246 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:52:42,247 	Text Reference  :	*** *** ********** ******* ***** ***** they never permitted    anyone to reveal her  face   
2024-02-05 19:52:42,247 	Text Hypothesis :	for the post-match meeting virat kohli made an    announcement and    it was    very private
2024-02-05 19:52:42,247 	Text Alignment  :	I   I   I          I       I     I     S    S     S            S      S  S      S    S      
2024-02-05 19:52:42,248 ========================================================================================================================
2024-02-05 19:52:42,248 Logging Sequence: 167_60.00
2024-02-05 19:52:42,248 	Gloss Reference :	A B+C+D+E
2024-02-05 19:52:42,248 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:52:42,248 	Gloss Alignment :	         
2024-02-05 19:52:42,248 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:52:42,249 	Text Reference  :	camel  flu spreads   rapidly when one comes in  close contact with the  infected  
2024-02-05 19:52:42,250 	Text Hypothesis :	people are desparate to      see  the child but the   couple  are  very protective
2024-02-05 19:52:42,250 	Text Alignment  :	S      S   S         S       S    S   S     S   S     S       S    S    S         
2024-02-05 19:52:42,250 ========================================================================================================================
2024-02-05 19:52:42,250 Logging Sequence: 84_35.00
2024-02-05 19:52:42,250 	Gloss Reference :	A B+C+D+E
2024-02-05 19:52:42,250 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:52:42,250 	Gloss Alignment :	         
2024-02-05 19:52:42,251 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:52:42,251 	Text Reference  :	here is the reason why they covered their mouth
2024-02-05 19:52:42,251 	Text Hypothesis :	**** ** *** ****** *** let  me      tell  you  
2024-02-05 19:52:42,251 	Text Alignment  :	D    D  D   D      D   S    S       S     S    
2024-02-05 19:52:42,251 ========================================================================================================================
2024-02-05 19:52:42,251 Logging Sequence: 96_2.00
2024-02-05 19:52:42,252 	Gloss Reference :	A B+C+D+E    
2024-02-05 19:52:42,252 	Gloss Hypothesis:	A B+C+D+E+D+E
2024-02-05 19:52:42,252 	Gloss Alignment :	  S          
2024-02-05 19:52:42,252 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:52:42,253 	Text Reference  :	the world is preparing for the      t20     world cup scheduled to  start from 16th october **** this year     
2024-02-05 19:52:42,254 	Text Hypothesis :	the ***** ** ********* icc under-19 cricket world cup ********* was held  on   23rd october 2022 in   bengaluru
2024-02-05 19:52:42,254 	Text Alignment  :	    D     D  D         S   S        S                 D         S   S     S    S            I    S    S        
2024-02-05 19:52:42,254 ========================================================================================================================
2024-02-05 19:52:44,703 Epoch 271: Total Training Recognition Loss 0.15  Total Training Translation Loss 8.37 
2024-02-05 19:52:44,703 EPOCH 272
2024-02-05 19:52:47,540 [Epoch: 272 Step: 00036100] Batch Recognition Loss:   0.000612 => Gls Tokens per Sec:     1608 || Batch Translation Loss:   0.032493 => Txt Tokens per Sec:     4335 || Lr: 0.000050
2024-02-05 19:52:51,996 Epoch 272: Total Training Recognition Loss 0.21  Total Training Translation Loss 11.05 
2024-02-05 19:52:51,997 EPOCH 273
2024-02-05 19:52:53,176 [Epoch: 273 Step: 00036200] Batch Recognition Loss:   0.000832 => Gls Tokens per Sec:     1629 || Batch Translation Loss:   0.022883 => Txt Tokens per Sec:     4209 || Lr: 0.000050
2024-02-05 19:52:58,645 [Epoch: 273 Step: 00036300] Batch Recognition Loss:   0.000813 => Gls Tokens per Sec:     1459 || Batch Translation Loss:   0.030343 => Txt Tokens per Sec:     4078 || Lr: 0.000050
2024-02-05 19:52:59,193 Epoch 273: Total Training Recognition Loss 0.19  Total Training Translation Loss 9.35 
2024-02-05 19:52:59,193 EPOCH 274
2024-02-05 19:53:04,353 [Epoch: 274 Step: 00036400] Batch Recognition Loss:   0.000243 => Gls Tokens per Sec:     1411 || Batch Translation Loss:   0.018400 => Txt Tokens per Sec:     3923 || Lr: 0.000050
2024-02-05 19:53:06,695 Epoch 274: Total Training Recognition Loss 0.16  Total Training Translation Loss 7.69 
2024-02-05 19:53:06,695 EPOCH 275
2024-02-05 19:53:10,244 [Epoch: 275 Step: 00036500] Batch Recognition Loss:   0.000376 => Gls Tokens per Sec:     1307 || Batch Translation Loss:   0.019086 => Txt Tokens per Sec:     3678 || Lr: 0.000050
2024-02-05 19:53:14,032 Epoch 275: Total Training Recognition Loss 0.16  Total Training Translation Loss 4.35 
2024-02-05 19:53:14,032 EPOCH 276
2024-02-05 19:53:15,232 [Epoch: 276 Step: 00036600] Batch Recognition Loss:   0.000155 => Gls Tokens per Sec:     1668 || Batch Translation Loss:   0.024533 => Txt Tokens per Sec:     4549 || Lr: 0.000050
2024-02-05 19:53:20,997 [Epoch: 276 Step: 00036700] Batch Recognition Loss:   0.002789 => Gls Tokens per Sec:     1384 || Batch Translation Loss:   0.037618 => Txt Tokens per Sec:     3855 || Lr: 0.000050
2024-02-05 19:53:21,479 Epoch 276: Total Training Recognition Loss 0.14  Total Training Translation Loss 6.64 
2024-02-05 19:53:21,479 EPOCH 277
2024-02-05 19:53:26,486 [Epoch: 277 Step: 00036800] Batch Recognition Loss:   0.000224 => Gls Tokens per Sec:     1470 || Batch Translation Loss:   0.024231 => Txt Tokens per Sec:     4016 || Lr: 0.000050
2024-02-05 19:53:28,907 Epoch 277: Total Training Recognition Loss 0.09  Total Training Translation Loss 5.06 
2024-02-05 19:53:28,908 EPOCH 278
2024-02-05 19:53:32,365 [Epoch: 278 Step: 00036900] Batch Recognition Loss:   0.000139 => Gls Tokens per Sec:     1366 || Batch Translation Loss:   0.020039 => Txt Tokens per Sec:     3626 || Lr: 0.000050
2024-02-05 19:53:36,302 Epoch 278: Total Training Recognition Loss 0.11  Total Training Translation Loss 7.96 
2024-02-05 19:53:36,302 EPOCH 279
2024-02-05 19:53:37,828 [Epoch: 279 Step: 00037000] Batch Recognition Loss:   0.000169 => Gls Tokens per Sec:     1365 || Batch Translation Loss:   0.021918 => Txt Tokens per Sec:     3959 || Lr: 0.000050
2024-02-05 19:53:43,299 [Epoch: 279 Step: 00037100] Batch Recognition Loss:   0.000301 => Gls Tokens per Sec:     1459 || Batch Translation Loss:   0.125831 => Txt Tokens per Sec:     4009 || Lr: 0.000050
2024-02-05 19:53:43,647 Epoch 279: Total Training Recognition Loss 0.16  Total Training Translation Loss 8.43 
2024-02-05 19:53:43,648 EPOCH 280
2024-02-05 19:53:48,805 [Epoch: 280 Step: 00037200] Batch Recognition Loss:   0.000213 => Gls Tokens per Sec:     1443 || Batch Translation Loss:   0.045715 => Txt Tokens per Sec:     4004 || Lr: 0.000050
2024-02-05 19:53:51,058 Epoch 280: Total Training Recognition Loss 0.09  Total Training Translation Loss 7.92 
2024-02-05 19:53:51,058 EPOCH 281
2024-02-05 19:53:54,086 [Epoch: 281 Step: 00037300] Batch Recognition Loss:   0.000339 => Gls Tokens per Sec:     1586 || Batch Translation Loss:   0.036379 => Txt Tokens per Sec:     4438 || Lr: 0.000050
2024-02-05 19:53:58,348 Epoch 281: Total Training Recognition Loss 0.15  Total Training Translation Loss 7.33 
2024-02-05 19:53:58,348 EPOCH 282
2024-02-05 19:54:00,010 [Epoch: 282 Step: 00037400] Batch Recognition Loss:   0.001325 => Gls Tokens per Sec:     1301 || Batch Translation Loss:   0.042303 => Txt Tokens per Sec:     3674 || Lr: 0.000050
2024-02-05 19:54:05,174 [Epoch: 282 Step: 00037500] Batch Recognition Loss:   0.000421 => Gls Tokens per Sec:     1545 || Batch Translation Loss:   0.017654 => Txt Tokens per Sec:     4265 || Lr: 0.000050
2024-02-05 19:54:05,477 Epoch 282: Total Training Recognition Loss 0.12  Total Training Translation Loss 5.69 
2024-02-05 19:54:05,477 EPOCH 283
2024-02-05 19:54:10,771 [Epoch: 283 Step: 00037600] Batch Recognition Loss:   0.002789 => Gls Tokens per Sec:     1421 || Batch Translation Loss:   0.022093 => Txt Tokens per Sec:     3904 || Lr: 0.000050
2024-02-05 19:54:13,108 Epoch 283: Total Training Recognition Loss 0.12  Total Training Translation Loss 4.50 
2024-02-05 19:54:13,109 EPOCH 284
2024-02-05 19:54:16,677 [Epoch: 284 Step: 00037700] Batch Recognition Loss:   0.000605 => Gls Tokens per Sec:     1369 || Batch Translation Loss:   0.024773 => Txt Tokens per Sec:     3881 || Lr: 0.000050
2024-02-05 19:54:20,589 Epoch 284: Total Training Recognition Loss 0.12  Total Training Translation Loss 4.35 
2024-02-05 19:54:20,589 EPOCH 285
2024-02-05 19:54:21,929 [Epoch: 285 Step: 00037800] Batch Recognition Loss:   0.000330 => Gls Tokens per Sec:     1673 || Batch Translation Loss:   0.044868 => Txt Tokens per Sec:     4469 || Lr: 0.000050
2024-02-05 19:54:26,998 [Epoch: 285 Step: 00037900] Batch Recognition Loss:   0.000221 => Gls Tokens per Sec:     1578 || Batch Translation Loss:   0.039209 => Txt Tokens per Sec:     4399 || Lr: 0.000050
2024-02-05 19:54:27,388 Epoch 285: Total Training Recognition Loss 0.09  Total Training Translation Loss 6.76 
2024-02-05 19:54:27,389 EPOCH 286
2024-02-05 19:54:32,711 [Epoch: 286 Step: 00038000] Batch Recognition Loss:   0.000289 => Gls Tokens per Sec:     1428 || Batch Translation Loss:   0.102262 => Txt Tokens per Sec:     3995 || Lr: 0.000050
2024-02-05 19:54:45,447 Validation result at epoch 286, step    38000: duration: 12.7360s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 5.93627	Translation Loss: 91640.64844	PPL: 9443.49121
	Eval Metric: BLEU
	WER 3.74	(DEL: 0.00,	INS: 0.00,	SUB: 3.74)
	BLEU-4 0.71	(BLEU-1: 10.68,	BLEU-2: 3.44,	BLEU-3: 1.37,	BLEU-4: 0.71)
	CHRF 16.98	ROUGE 9.14
2024-02-05 19:54:45,448 Logging Recognition and Translation Outputs
2024-02-05 19:54:45,449 ========================================================================================================================
2024-02-05 19:54:45,449 Logging Sequence: 59_152.00
2024-02-05 19:54:45,449 	Gloss Reference :	A B+C+D+E
2024-02-05 19:54:45,449 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:54:45,449 	Gloss Alignment :	         
2024-02-05 19:54:45,449 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:54:45,450 	Text Reference  :	the organisers encouraged athletes to   use the condoms      in     their home countries  
2024-02-05 19:54:45,451 	Text Hypothesis :	*** well       let        me       tell you the broadcasting rights for   the  coronavirus
2024-02-05 19:54:45,451 	Text Alignment  :	D   S          S          S        S    S       S            S      S     S    S          
2024-02-05 19:54:45,451 ========================================================================================================================
2024-02-05 19:54:45,451 Logging Sequence: 155_78.00
2024-02-05 19:54:45,451 	Gloss Reference :	A B+C+D+E
2024-02-05 19:54:45,451 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:54:45,451 	Gloss Alignment :	         
2024-02-05 19:54:45,452 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:54:45,453 	Text Reference  :	it was difficult for icc to disqualify the afghan team at the last    minute  so       they    included them        as  per     the schedule
2024-02-05 19:54:45,453 	Text Hypothesis :	** *** ********* *** *** ** ********** *** ****** **** ** the taliban swiftly regained control of       afghanistan was through the loss    
2024-02-05 19:54:45,453 	Text Alignment  :	D  D   D         D   D   D  D          D   D      D    D      S       S       S        S       S        S           S   S           S       
2024-02-05 19:54:45,454 ========================================================================================================================
2024-02-05 19:54:45,454 Logging Sequence: 102_147.00
2024-02-05 19:54:45,454 	Gloss Reference :	A B+C+D+E
2024-02-05 19:54:45,454 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:54:45,454 	Gloss Alignment :	         
2024-02-05 19:54:45,454 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:54:45,457 	Text Reference  :	despite the muscle cramps this  young boy lifted  such a  huge weight and made the ******* country proud by    securing a      gold medal
2024-02-05 19:54:45,457 	Text Hypothesis :	******* *** ****** a      total of    22  matches will be held in     uae with the penalty shoot   out   while italy    scored 1    lakh 
2024-02-05 19:54:45,457 	Text Alignment  :	D       D   D      S      S     S     S   S       S    S  S    S      S   S        I       S       S     S     S        S      S    S    
2024-02-05 19:54:45,457 ========================================================================================================================
2024-02-05 19:54:45,457 Logging Sequence: 105_2.00
2024-02-05 19:54:45,458 	Gloss Reference :	A B+C+D+E
2024-02-05 19:54:45,458 	Gloss Hypothesis:	A B+C+D  
2024-02-05 19:54:45,458 	Gloss Alignment :	  S      
2024-02-05 19:54:45,458 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:54:45,459 	Text Reference  :	*** ***** ** the ***** **** airthings    masters tournament is      an    online chess tournament
2024-02-05 19:54:45,459 	Text Hypothesis :	the final of the world test championship was     held       between india and    new   zealand   
2024-02-05 19:54:45,459 	Text Alignment  :	I   I     I      I     I    S            S       S          S       S     S      S     S         
2024-02-05 19:54:45,459 ========================================================================================================================
2024-02-05 19:54:45,459 Logging Sequence: 96_31.00
2024-02-05 19:54:45,460 	Gloss Reference :	A B+C+D+E
2024-02-05 19:54:45,460 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:54:45,460 	Gloss Alignment :	         
2024-02-05 19:54:45,460 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:54:45,461 	Text Reference  :	and then 2 teams   will go   on    to  play    the final 
2024-02-05 19:54:45,461 	Text Hypothesis :	*** **** * however a    very happy and secured a   wicket
2024-02-05 19:54:45,461 	Text Alignment  :	D   D    D S       S    S    S     S   S       S   S     
2024-02-05 19:54:45,461 ========================================================================================================================
2024-02-05 19:54:47,653 Epoch 286: Total Training Recognition Loss 0.13  Total Training Translation Loss 7.14 
2024-02-05 19:54:47,653 EPOCH 287
2024-02-05 19:54:51,191 [Epoch: 287 Step: 00038100] Batch Recognition Loss:   0.000395 => Gls Tokens per Sec:     1403 || Batch Translation Loss:   0.120262 => Txt Tokens per Sec:     3902 || Lr: 0.000050
2024-02-05 19:54:55,194 Epoch 287: Total Training Recognition Loss 0.18  Total Training Translation Loss 9.35 
2024-02-05 19:54:55,195 EPOCH 288
2024-02-05 19:54:56,780 [Epoch: 288 Step: 00038200] Batch Recognition Loss:   0.000817 => Gls Tokens per Sec:     1465 || Batch Translation Loss:   0.042680 => Txt Tokens per Sec:     4192 || Lr: 0.000050
2024-02-05 19:55:02,412 [Epoch: 288 Step: 00038300] Batch Recognition Loss:   0.001583 => Gls Tokens per Sec:     1417 || Batch Translation Loss:   0.048662 => Txt Tokens per Sec:     3907 || Lr: 0.000050
2024-02-05 19:55:02,670 Epoch 288: Total Training Recognition Loss 0.14  Total Training Translation Loss 9.09 
2024-02-05 19:55:02,670 EPOCH 289
2024-02-05 19:55:07,918 [Epoch: 289 Step: 00038400] Batch Recognition Loss:   0.001076 => Gls Tokens per Sec:     1464 || Batch Translation Loss:   0.033565 => Txt Tokens per Sec:     4047 || Lr: 0.000050
2024-02-05 19:55:09,992 Epoch 289: Total Training Recognition Loss 0.14  Total Training Translation Loss 6.88 
2024-02-05 19:55:09,992 EPOCH 290
2024-02-05 19:55:13,366 [Epoch: 290 Step: 00038500] Batch Recognition Loss:   0.000191 => Gls Tokens per Sec:     1494 || Batch Translation Loss:   0.022457 => Txt Tokens per Sec:     4111 || Lr: 0.000050
2024-02-05 19:55:17,440 Epoch 290: Total Training Recognition Loss 0.15  Total Training Translation Loss 8.10 
2024-02-05 19:55:17,440 EPOCH 291
2024-02-05 19:55:19,125 [Epoch: 291 Step: 00038600] Batch Recognition Loss:   0.000271 => Gls Tokens per Sec:     1425 || Batch Translation Loss:   0.041922 => Txt Tokens per Sec:     3899 || Lr: 0.000050
2024-02-05 19:55:24,504 [Epoch: 291 Step: 00038700] Batch Recognition Loss:   0.000318 => Gls Tokens per Sec:     1484 || Batch Translation Loss:   0.029322 => Txt Tokens per Sec:     4097 || Lr: 0.000050
2024-02-05 19:55:24,703 Epoch 291: Total Training Recognition Loss 0.17  Total Training Translation Loss 8.78 
2024-02-05 19:55:24,703 EPOCH 292
2024-02-05 19:55:30,287 [Epoch: 292 Step: 00038800] Batch Recognition Loss:   0.001204 => Gls Tokens per Sec:     1390 || Batch Translation Loss:   0.036898 => Txt Tokens per Sec:     3832 || Lr: 0.000050
2024-02-05 19:55:32,372 Epoch 292: Total Training Recognition Loss 0.14  Total Training Translation Loss 7.35 
2024-02-05 19:55:32,372 EPOCH 293
2024-02-05 19:55:35,610 [Epoch: 293 Step: 00038900] Batch Recognition Loss:   0.001455 => Gls Tokens per Sec:     1582 || Batch Translation Loss:   0.034995 => Txt Tokens per Sec:     4415 || Lr: 0.000050
2024-02-05 19:55:39,580 Epoch 293: Total Training Recognition Loss 0.16  Total Training Translation Loss 8.80 
2024-02-05 19:55:39,580 EPOCH 294
2024-02-05 19:55:41,490 [Epoch: 294 Step: 00039000] Batch Recognition Loss:   0.000216 => Gls Tokens per Sec:     1299 || Batch Translation Loss:   0.027989 => Txt Tokens per Sec:     3689 || Lr: 0.000050
2024-02-05 19:55:47,065 [Epoch: 294 Step: 00039100] Batch Recognition Loss:   0.001746 => Gls Tokens per Sec:     1432 || Batch Translation Loss:   0.010611 => Txt Tokens per Sec:     3933 || Lr: 0.000050
2024-02-05 19:55:47,159 Epoch 294: Total Training Recognition Loss 0.12  Total Training Translation Loss 7.03 
2024-02-05 19:55:47,160 EPOCH 295
2024-02-05 19:55:52,622 [Epoch: 295 Step: 00039200] Batch Recognition Loss:   0.000736 => Gls Tokens per Sec:     1435 || Batch Translation Loss:   0.019047 => Txt Tokens per Sec:     3974 || Lr: 0.000050
2024-02-05 19:55:54,644 Epoch 295: Total Training Recognition Loss 0.12  Total Training Translation Loss 7.16 
2024-02-05 19:55:54,644 EPOCH 296
2024-02-05 19:55:58,266 [Epoch: 296 Step: 00039300] Batch Recognition Loss:   0.000236 => Gls Tokens per Sec:     1437 || Batch Translation Loss:   0.014082 => Txt Tokens per Sec:     3959 || Lr: 0.000050
2024-02-05 19:56:01,947 Epoch 296: Total Training Recognition Loss 0.21  Total Training Translation Loss 5.81 
2024-02-05 19:56:01,947 EPOCH 297
2024-02-05 19:56:03,637 [Epoch: 297 Step: 00039400] Batch Recognition Loss:   0.000675 => Gls Tokens per Sec:     1516 || Batch Translation Loss:   0.030826 => Txt Tokens per Sec:     4033 || Lr: 0.000050
2024-02-05 19:56:09,524 [Epoch: 297 Step: 00039500] Batch Recognition Loss:   0.000344 => Gls Tokens per Sec:     1356 || Batch Translation Loss:   0.140118 => Txt Tokens per Sec:     3802 || Lr: 0.000050
2024-02-05 19:56:09,582 Epoch 297: Total Training Recognition Loss 0.21  Total Training Translation Loss 6.93 
2024-02-05 19:56:09,582 EPOCH 298
2024-02-05 19:56:14,813 [Epoch: 298 Step: 00039600] Batch Recognition Loss:   0.001023 => Gls Tokens per Sec:     1514 || Batch Translation Loss:   0.032671 => Txt Tokens per Sec:     4159 || Lr: 0.000050
2024-02-05 19:56:16,907 Epoch 298: Total Training Recognition Loss 0.22  Total Training Translation Loss 3.44 
2024-02-05 19:56:16,907 EPOCH 299
2024-02-05 19:56:20,920 [Epoch: 299 Step: 00039700] Batch Recognition Loss:   0.000908 => Gls Tokens per Sec:     1317 || Batch Translation Loss:   0.033853 => Txt Tokens per Sec:     3724 || Lr: 0.000050
2024-02-05 19:56:24,527 Epoch 299: Total Training Recognition Loss 0.09  Total Training Translation Loss 8.42 
2024-02-05 19:56:24,527 EPOCH 300
2024-02-05 19:56:26,559 [Epoch: 300 Step: 00039800] Batch Recognition Loss:   0.000437 => Gls Tokens per Sec:     1299 || Batch Translation Loss:   0.030789 => Txt Tokens per Sec:     3693 || Lr: 0.000050
2024-02-05 19:56:31,805 [Epoch: 300 Step: 00039900] Batch Recognition Loss:   0.000464 => Gls Tokens per Sec:     1522 || Batch Translation Loss:   0.040345 => Txt Tokens per Sec:     4171 || Lr: 0.000050
2024-02-05 19:56:31,805 Epoch 300: Total Training Recognition Loss 0.10  Total Training Translation Loss 6.64 
2024-02-05 19:56:31,806 EPOCH 301
2024-02-05 19:56:37,568 [Epoch: 301 Step: 00040000] Batch Recognition Loss:   0.000448 => Gls Tokens per Sec:     1388 || Batch Translation Loss:   0.116771 => Txt Tokens per Sec:     3853 || Lr: 0.000050
2024-02-05 19:56:47,877 Validation result at epoch 301, step    40000: duration: 10.3075s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 6.31522	Translation Loss: 92226.41406	PPL: 10012.47363
	Eval Metric: BLEU
	WER 3.81	(DEL: 0.00,	INS: 0.00,	SUB: 3.81)
	BLEU-4 0.49	(BLEU-1: 11.08,	BLEU-2: 3.52,	BLEU-3: 1.28,	BLEU-4: 0.49)
	CHRF 16.90	ROUGE 9.80
2024-02-05 19:56:47,878 Logging Recognition and Translation Outputs
2024-02-05 19:56:47,878 ========================================================================================================================
2024-02-05 19:56:47,878 Logging Sequence: 86_84.00
2024-02-05 19:56:47,879 	Gloss Reference :	A B+C+D+E
2024-02-05 19:56:47,879 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:56:47,879 	Gloss Alignment :	         
2024-02-05 19:56:47,879 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:56:47,880 	Text Reference  :	amassing 8933 runs which included 21  centuries with    a   highest score of      201 not out 
2024-02-05 19:56:47,880 	Text Hypothesis :	******** **** **** he    played   194 odi       matches and took    293   wickets due to  1983
2024-02-05 19:56:47,880 	Text Alignment  :	D        D    D    S     S        S   S         S       S   S       S     S       S   S   S   
2024-02-05 19:56:47,880 ========================================================================================================================
2024-02-05 19:56:47,881 Logging Sequence: 179_110.00
2024-02-05 19:56:47,881 	Gloss Reference :	A B+C+D+E    
2024-02-05 19:56:47,881 	Gloss Hypothesis:	A B+C+D+E+D+E
2024-02-05 19:56:47,881 	Gloss Alignment :	  S          
2024-02-05 19:56:47,881 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:56:47,882 	Text Reference  :	*** *** ** phogat refused to  stay      in  the same room with other indian female wrestlers
2024-02-05 19:56:47,883 	Text Hypothesis :	the ban is a      holiday wfi collected for the wfi  and  that she   is     the    stumps   
2024-02-05 19:56:47,883 	Text Alignment  :	I   I   I  S      S       S   S         S       S    S    S    S     S      S      S        
2024-02-05 19:56:47,883 ========================================================================================================================
2024-02-05 19:56:47,883 Logging Sequence: 102_2.00
2024-02-05 19:56:47,883 	Gloss Reference :	A B+C+D+E
2024-02-05 19:56:47,883 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:56:47,883 	Gloss Alignment :	         
2024-02-05 19:56:47,883 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:56:47,884 	Text Reference  :	commonwealth games are among the world's most recognised gaming championships after   the     olympics
2024-02-05 19:56:47,884 	Text Hypothesis :	************ ***** *** ***** in  3       days india      and    bangladesh    women's cricket team    
2024-02-05 19:56:47,884 	Text Alignment  :	D            D     D   D     S   S       S    S          S      S             S       S       S       
2024-02-05 19:56:47,885 ========================================================================================================================
2024-02-05 19:56:47,885 Logging Sequence: 60_195.00
2024-02-05 19:56:47,885 	Gloss Reference :	A B+C+D+E
2024-02-05 19:56:47,885 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:56:47,885 	Gloss Alignment :	         
2024-02-05 19:56:47,885 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:56:47,886 	Text Reference  :	** people loved to      watch his   aggressive expressions and     his bowling
2024-02-05 19:56:47,886 	Text Hypothesis :	in his    142   matches for   india chhetri    has         slammed 92  goals  
2024-02-05 19:56:47,886 	Text Alignment  :	I  S      S     S       S     S     S          S           S       S   S      
2024-02-05 19:56:47,886 ========================================================================================================================
2024-02-05 19:56:47,886 Logging Sequence: 70_200.00
2024-02-05 19:56:47,887 	Gloss Reference :	A B+C+D+E
2024-02-05 19:56:47,887 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:56:47,887 	Gloss Alignment :	         
2024-02-05 19:56:47,887 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:56:47,887 	Text Reference  :	showing ronaldo whole-heartedly endorsing the ***** brand **********
2024-02-05 19:56:47,887 	Text Hypothesis :	******* ******* *************** in        the state brand ambassador
2024-02-05 19:56:47,888 	Text Alignment  :	D       D       D               S             I           I         
2024-02-05 19:56:47,888 ========================================================================================================================
2024-02-05 19:56:49,867 Epoch 301: Total Training Recognition Loss 0.13  Total Training Translation Loss 9.16 
2024-02-05 19:56:49,868 EPOCH 302
2024-02-05 19:56:53,626 [Epoch: 302 Step: 00040100] Batch Recognition Loss:   0.000171 => Gls Tokens per Sec:     1427 || Batch Translation Loss:   0.025797 => Txt Tokens per Sec:     3912 || Lr: 0.000050
2024-02-05 19:56:57,281 Epoch 302: Total Training Recognition Loss 0.08  Total Training Translation Loss 5.66 
2024-02-05 19:56:57,281 EPOCH 303
2024-02-05 19:56:58,806 [Epoch: 303 Step: 00040200] Batch Recognition Loss:   0.000317 => Gls Tokens per Sec:     1785 || Batch Translation Loss:   0.045549 => Txt Tokens per Sec:     5004 || Lr: 0.000050
2024-02-05 19:57:04,326 Epoch 303: Total Training Recognition Loss 0.13  Total Training Translation Loss 7.17 
2024-02-05 19:57:04,327 EPOCH 304
2024-02-05 19:57:04,386 [Epoch: 304 Step: 00040300] Batch Recognition Loss:   0.000293 => Gls Tokens per Sec:     1379 || Batch Translation Loss:   0.026895 => Txt Tokens per Sec:     4276 || Lr: 0.000050
2024-02-05 19:57:10,311 [Epoch: 304 Step: 00040400] Batch Recognition Loss:   0.000254 => Gls Tokens per Sec:     1350 || Batch Translation Loss:   0.148767 => Txt Tokens per Sec:     3726 || Lr: 0.000050
2024-02-05 19:57:12,057 Epoch 304: Total Training Recognition Loss 0.16  Total Training Translation Loss 8.18 
2024-02-05 19:57:12,058 EPOCH 305
2024-02-05 19:57:15,651 [Epoch: 305 Step: 00040500] Batch Recognition Loss:   0.000341 => Gls Tokens per Sec:     1514 || Batch Translation Loss:   0.018493 => Txt Tokens per Sec:     4255 || Lr: 0.000050
2024-02-05 19:57:19,331 Epoch 305: Total Training Recognition Loss 0.21  Total Training Translation Loss 4.32 
2024-02-05 19:57:19,332 EPOCH 306
2024-02-05 19:57:21,107 [Epoch: 306 Step: 00040600] Batch Recognition Loss:   0.000294 => Gls Tokens per Sec:     1578 || Batch Translation Loss:   0.023659 => Txt Tokens per Sec:     4154 || Lr: 0.000050
2024-02-05 19:57:26,368 Epoch 306: Total Training Recognition Loss 0.11  Total Training Translation Loss 7.10 
2024-02-05 19:57:26,369 EPOCH 307
2024-02-05 19:57:26,488 [Epoch: 307 Step: 00040700] Batch Recognition Loss:   0.000808 => Gls Tokens per Sec:     1345 || Batch Translation Loss:   0.016051 => Txt Tokens per Sec:     3815 || Lr: 0.000050
2024-02-05 19:57:32,139 [Epoch: 307 Step: 00040800] Batch Recognition Loss:   0.000323 => Gls Tokens per Sec:     1412 || Batch Translation Loss:   0.027140 => Txt Tokens per Sec:     3898 || Lr: 0.000050
2024-02-05 19:57:33,837 Epoch 307: Total Training Recognition Loss 0.11  Total Training Translation Loss 5.67 
2024-02-05 19:57:33,837 EPOCH 308
2024-02-05 19:57:37,354 [Epoch: 308 Step: 00040900] Batch Recognition Loss:   0.000180 => Gls Tokens per Sec:     1570 || Batch Translation Loss:   0.061621 => Txt Tokens per Sec:     4344 || Lr: 0.000050
2024-02-05 19:57:41,077 Epoch 308: Total Training Recognition Loss 0.10  Total Training Translation Loss 4.53 
2024-02-05 19:57:41,077 EPOCH 309
2024-02-05 19:57:42,966 [Epoch: 309 Step: 00041000] Batch Recognition Loss:   0.002155 => Gls Tokens per Sec:     1525 || Batch Translation Loss:   0.032035 => Txt Tokens per Sec:     4260 || Lr: 0.000050
2024-02-05 19:57:48,232 Epoch 309: Total Training Recognition Loss 0.22  Total Training Translation Loss 3.64 
2024-02-05 19:57:48,232 EPOCH 310
2024-02-05 19:57:48,416 [Epoch: 310 Step: 00041100] Batch Recognition Loss:   0.000286 => Gls Tokens per Sec:     1309 || Batch Translation Loss:   0.019496 => Txt Tokens per Sec:     3824 || Lr: 0.000050
2024-02-05 19:57:53,945 [Epoch: 310 Step: 00041200] Batch Recognition Loss:   0.000156 => Gls Tokens per Sec:     1443 || Batch Translation Loss:   0.022948 => Txt Tokens per Sec:     3989 || Lr: 0.000050
2024-02-05 19:57:55,550 Epoch 310: Total Training Recognition Loss 0.11  Total Training Translation Loss 5.12 
2024-02-05 19:57:55,550 EPOCH 311
2024-02-05 19:57:59,503 [Epoch: 311 Step: 00041300] Batch Recognition Loss:   0.007613 => Gls Tokens per Sec:     1417 || Batch Translation Loss:   0.081303 => Txt Tokens per Sec:     3861 || Lr: 0.000050
2024-02-05 19:58:03,202 Epoch 311: Total Training Recognition Loss 0.20  Total Training Translation Loss 11.56 
2024-02-05 19:58:03,202 EPOCH 312
2024-02-05 19:58:05,244 [Epoch: 312 Step: 00041400] Batch Recognition Loss:   0.001063 => Gls Tokens per Sec:     1450 || Batch Translation Loss:   0.109667 => Txt Tokens per Sec:     3996 || Lr: 0.000050
2024-02-05 19:58:10,531 Epoch 312: Total Training Recognition Loss 0.15  Total Training Translation Loss 9.26 
2024-02-05 19:58:10,531 EPOCH 313
2024-02-05 19:58:10,748 [Epoch: 313 Step: 00041500] Batch Recognition Loss:   0.000423 => Gls Tokens per Sec:     1481 || Batch Translation Loss:   0.193406 => Txt Tokens per Sec:     4426 || Lr: 0.000050
2024-02-05 19:58:16,615 [Epoch: 313 Step: 00041600] Batch Recognition Loss:   0.000209 => Gls Tokens per Sec:     1364 || Batch Translation Loss:   0.023609 => Txt Tokens per Sec:     3748 || Lr: 0.000050
2024-02-05 19:58:18,282 Epoch 313: Total Training Recognition Loss 0.11  Total Training Translation Loss 7.56 
2024-02-05 19:58:18,282 EPOCH 314
2024-02-05 19:58:22,157 [Epoch: 314 Step: 00041700] Batch Recognition Loss:   0.000266 => Gls Tokens per Sec:     1466 || Batch Translation Loss:   0.024633 => Txt Tokens per Sec:     4042 || Lr: 0.000050
2024-02-05 19:58:25,552 Epoch 314: Total Training Recognition Loss 0.19  Total Training Translation Loss 6.98 
2024-02-05 19:58:25,553 EPOCH 315
2024-02-05 19:58:27,803 [Epoch: 315 Step: 00041800] Batch Recognition Loss:   0.000242 => Gls Tokens per Sec:     1352 || Batch Translation Loss:   0.105030 => Txt Tokens per Sec:     3887 || Lr: 0.000050
2024-02-05 19:58:32,813 Epoch 315: Total Training Recognition Loss 0.12  Total Training Translation Loss 5.23 
2024-02-05 19:58:32,813 EPOCH 316
2024-02-05 19:58:33,056 [Epoch: 316 Step: 00041900] Batch Recognition Loss:   0.000204 => Gls Tokens per Sec:     1653 || Batch Translation Loss:   0.017212 => Txt Tokens per Sec:     4360 || Lr: 0.000050
2024-02-05 19:58:38,803 [Epoch: 316 Step: 00042000] Batch Recognition Loss:   0.000787 => Gls Tokens per Sec:     1389 || Batch Translation Loss:   0.026768 => Txt Tokens per Sec:     3842 || Lr: 0.000050
2024-02-05 19:58:49,665 Validation result at epoch 316, step    42000: duration: 10.8616s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 6.06617	Translation Loss: 91442.53125	PPL: 9258.46094
	Eval Metric: BLEU
	WER 3.18	(DEL: 0.00,	INS: 0.00,	SUB: 3.18)
	BLEU-4 0.61	(BLEU-1: 10.99,	BLEU-2: 3.65,	BLEU-3: 1.34,	BLEU-4: 0.61)
	CHRF 16.97	ROUGE 9.53
2024-02-05 19:58:49,666 Logging Recognition and Translation Outputs
2024-02-05 19:58:49,666 ========================================================================================================================
2024-02-05 19:58:49,667 Logging Sequence: 154_94.00
2024-02-05 19:58:49,667 	Gloss Reference :	A B+C+D+E
2024-02-05 19:58:49,667 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:58:49,667 	Gloss Alignment :	         
2024-02-05 19:58:49,667 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:58:49,668 	Text Reference  :	the ipl  will also be held in uae from september 19          to   october 15  
2024-02-05 19:58:49,668 	Text Hypothesis :	but they will now  be **** ** *** **** ********* quarantined till 13th    june
2024-02-05 19:58:49,668 	Text Alignment  :	S   S         S       D    D  D   D    D         S           S    S       S   
2024-02-05 19:58:49,668 ========================================================================================================================
2024-02-05 19:58:49,669 Logging Sequence: 118_2.00
2024-02-05 19:58:49,669 	Gloss Reference :	A B+C+D+E  
2024-02-05 19:58:49,669 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-05 19:58:49,669 	Gloss Alignment :	  S        
2024-02-05 19:58:49,669 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:58:49,670 	Text Reference  :	yesterday was a very exciting day      people  across the   world were watching
2024-02-05 19:58:49,670 	Text Hypothesis :	********* *** * **** ******** everyone thought that   india and   deaf love    
2024-02-05 19:58:49,670 	Text Alignment  :	D         D   D D    D        S        S       S      S     S     S    S       
2024-02-05 19:58:49,670 ========================================================================================================================
2024-02-05 19:58:49,670 Logging Sequence: 165_453.00
2024-02-05 19:58:49,670 	Gloss Reference :	A B+C+D+E
2024-02-05 19:58:49,671 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:58:49,671 	Gloss Alignment :	         
2024-02-05 19:58:49,671 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:58:49,671 	Text Reference  :	** icc did not  agree to sehwag' decision of wearing a   numberless jersey
2024-02-05 19:58:49,672 	Text Hypothesis :	he has a   very sad   to ******* ******** ** ******* see his        wife  
2024-02-05 19:58:49,672 	Text Alignment  :	I  S   S   S    S        D       D        D  D       S   S          S     
2024-02-05 19:58:49,672 ========================================================================================================================
2024-02-05 19:58:49,672 Logging Sequence: 126_163.00
2024-02-05 19:58:49,672 	Gloss Reference :	A B+C+D+E
2024-02-05 19:58:49,672 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 19:58:49,672 	Gloss Alignment :	         
2024-02-05 19:58:49,672 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:58:49,673 	Text Reference  :	your  hard   work  has helped secure a    medal at     the tokyo olympics
2024-02-05 19:58:49,674 	Text Hypothesis :	since stokes broke his family and    very calm  during the ***** olympics
2024-02-05 19:58:49,674 	Text Alignment  :	S     S      S     S   S      S      S    S     S          D             
2024-02-05 19:58:49,674 ========================================================================================================================
2024-02-05 19:58:49,674 Logging Sequence: 84_2.00
2024-02-05 19:58:49,674 	Gloss Reference :	A B+C+D+E  
2024-02-05 19:58:49,674 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-05 19:58:49,674 	Gloss Alignment :	  S        
2024-02-05 19:58:49,675 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 19:58:49,676 	Text Reference  :	the 2022 fifa      football world cup   is   going   on in qatar from 20th november 2022 to 18th december 2022 
2024-02-05 19:58:49,676 	Text Hypothesis :	*** **** cricketer and      hazel keech were married on ** ***** **** **** ******** **** ** **** social   media
2024-02-05 19:58:49,676 	Text Alignment  :	D   D    S         S        S     S     S    S          D  D     D    D    D        D    D  D    S        S    
2024-02-05 19:58:49,676 ========================================================================================================================
2024-02-05 19:58:51,211 Epoch 316: Total Training Recognition Loss 0.19  Total Training Translation Loss 5.40 
2024-02-05 19:58:51,211 EPOCH 317
2024-02-05 19:58:55,365 [Epoch: 317 Step: 00042100] Batch Recognition Loss:   0.000220 => Gls Tokens per Sec:     1387 || Batch Translation Loss:   0.021706 => Txt Tokens per Sec:     3895 || Lr: 0.000050
2024-02-05 19:58:58,937 Epoch 317: Total Training Recognition Loss 0.08  Total Training Translation Loss 4.41 
2024-02-05 19:58:58,938 EPOCH 318
2024-02-05 19:59:00,963 [Epoch: 318 Step: 00042200] Batch Recognition Loss:   0.000182 => Gls Tokens per Sec:     1541 || Batch Translation Loss:   0.073869 => Txt Tokens per Sec:     4209 || Lr: 0.000050
2024-02-05 19:59:06,044 Epoch 318: Total Training Recognition Loss 0.15  Total Training Translation Loss 5.13 
2024-02-05 19:59:06,044 EPOCH 319
2024-02-05 19:59:06,398 [Epoch: 319 Step: 00042300] Batch Recognition Loss:   0.001154 => Gls Tokens per Sec:     1360 || Batch Translation Loss:   0.021564 => Txt Tokens per Sec:     4079 || Lr: 0.000050
2024-02-05 19:59:11,992 [Epoch: 319 Step: 00042400] Batch Recognition Loss:   0.000204 => Gls Tokens per Sec:     1430 || Batch Translation Loss:   0.020538 => Txt Tokens per Sec:     3918 || Lr: 0.000050
2024-02-05 19:59:13,568 Epoch 319: Total Training Recognition Loss 0.19  Total Training Translation Loss 7.08 
2024-02-05 19:59:13,568 EPOCH 320
2024-02-05 19:59:17,567 [Epoch: 320 Step: 00042500] Batch Recognition Loss:   0.000229 => Gls Tokens per Sec:     1461 || Batch Translation Loss:   0.030349 => Txt Tokens per Sec:     3995 || Lr: 0.000050
2024-02-05 19:59:20,915 Epoch 320: Total Training Recognition Loss 0.19  Total Training Translation Loss 5.62 
2024-02-05 19:59:20,916 EPOCH 321
2024-02-05 19:59:23,222 [Epoch: 321 Step: 00042600] Batch Recognition Loss:   0.011007 => Gls Tokens per Sec:     1388 || Batch Translation Loss:   0.040613 => Txt Tokens per Sec:     3948 || Lr: 0.000050
2024-02-05 19:59:28,392 Epoch 321: Total Training Recognition Loss 0.14  Total Training Translation Loss 5.53 
2024-02-05 19:59:28,393 EPOCH 322
2024-02-05 19:59:28,742 [Epoch: 322 Step: 00042700] Batch Recognition Loss:   0.000096 => Gls Tokens per Sec:     1609 || Batch Translation Loss:   0.013649 => Txt Tokens per Sec:     4569 || Lr: 0.000050
2024-02-05 19:59:34,100 [Epoch: 322 Step: 00042800] Batch Recognition Loss:   0.000216 => Gls Tokens per Sec:     1493 || Batch Translation Loss:   0.035773 => Txt Tokens per Sec:     4128 || Lr: 0.000050
2024-02-05 19:59:35,538 Epoch 322: Total Training Recognition Loss 0.23  Total Training Translation Loss 6.10 
2024-02-05 19:59:35,539 EPOCH 323
2024-02-05 19:59:39,797 [Epoch: 323 Step: 00042900] Batch Recognition Loss:   0.001869 => Gls Tokens per Sec:     1391 || Batch Translation Loss:   0.040001 => Txt Tokens per Sec:     3902 || Lr: 0.000050
2024-02-05 19:59:43,100 Epoch 323: Total Training Recognition Loss 0.08  Total Training Translation Loss 12.84 
2024-02-05 19:59:43,100 EPOCH 324
2024-02-05 19:59:45,349 [Epoch: 324 Step: 00043000] Batch Recognition Loss:   0.000186 => Gls Tokens per Sec:     1459 || Batch Translation Loss:   0.023780 => Txt Tokens per Sec:     3972 || Lr: 0.000050
2024-02-05 19:59:50,354 Epoch 324: Total Training Recognition Loss 0.16  Total Training Translation Loss 13.26 
2024-02-05 19:59:50,355 EPOCH 325
2024-02-05 19:59:50,828 [Epoch: 325 Step: 00043100] Batch Recognition Loss:   0.001223 => Gls Tokens per Sec:     1353 || Batch Translation Loss:   0.121079 => Txt Tokens per Sec:     4023 || Lr: 0.000050
2024-02-05 19:59:56,560 [Epoch: 325 Step: 00043200] Batch Recognition Loss:   0.000405 => Gls Tokens per Sec:     1392 || Batch Translation Loss:   0.027212 => Txt Tokens per Sec:     3865 || Lr: 0.000050
2024-02-05 19:59:57,859 Epoch 325: Total Training Recognition Loss 0.13  Total Training Translation Loss 5.22 
2024-02-05 19:59:57,859 EPOCH 326
2024-02-05 20:00:01,762 [Epoch: 326 Step: 00043300] Batch Recognition Loss:   0.000494 => Gls Tokens per Sec:     1538 || Batch Translation Loss:   0.013571 => Txt Tokens per Sec:     4281 || Lr: 0.000050
2024-02-05 20:00:04,939 Epoch 326: Total Training Recognition Loss 0.14  Total Training Translation Loss 6.24 
2024-02-05 20:00:04,939 EPOCH 327
2024-02-05 20:00:07,218 [Epoch: 327 Step: 00043400] Batch Recognition Loss:   0.000258 => Gls Tokens per Sec:     1476 || Batch Translation Loss:   0.022794 => Txt Tokens per Sec:     3850 || Lr: 0.000050
2024-02-05 20:00:12,349 Epoch 327: Total Training Recognition Loss 0.15  Total Training Translation Loss 5.84 
2024-02-05 20:00:12,349 EPOCH 328
2024-02-05 20:00:12,819 [Epoch: 328 Step: 00043500] Batch Recognition Loss:   0.000243 => Gls Tokens per Sec:     1535 || Batch Translation Loss:   0.047424 => Txt Tokens per Sec:     4386 || Lr: 0.000050
2024-02-05 20:00:18,474 [Epoch: 328 Step: 00043600] Batch Recognition Loss:   0.000222 => Gls Tokens per Sec:     1415 || Batch Translation Loss:   0.104008 => Txt Tokens per Sec:     3898 || Lr: 0.000050
2024-02-05 20:00:20,038 Epoch 328: Total Training Recognition Loss 0.08  Total Training Translation Loss 7.76 
2024-02-05 20:00:20,039 EPOCH 329
2024-02-05 20:00:24,433 [Epoch: 329 Step: 00043700] Batch Recognition Loss:   0.000240 => Gls Tokens per Sec:     1384 || Batch Translation Loss:   0.027792 => Txt Tokens per Sec:     3839 || Lr: 0.000050
2024-02-05 20:00:27,660 Epoch 329: Total Training Recognition Loss 0.17  Total Training Translation Loss 6.12 
2024-02-05 20:00:27,660 EPOCH 330
2024-02-05 20:00:29,770 [Epoch: 330 Step: 00043800] Batch Recognition Loss:   0.000708 => Gls Tokens per Sec:     1631 || Batch Translation Loss:   0.078613 => Txt Tokens per Sec:     4458 || Lr: 0.000050
2024-02-05 20:00:35,116 Epoch 330: Total Training Recognition Loss 0.19  Total Training Translation Loss 5.85 
2024-02-05 20:00:35,116 EPOCH 331
2024-02-05 20:00:35,554 [Epoch: 331 Step: 00043900] Batch Recognition Loss:   0.000117 => Gls Tokens per Sec:     1831 || Batch Translation Loss:   0.008912 => Txt Tokens per Sec:     4531 || Lr: 0.000050
2024-02-05 20:00:40,830 [Epoch: 331 Step: 00044000] Batch Recognition Loss:   0.000261 => Gls Tokens per Sec:     1516 || Batch Translation Loss:   0.019133 => Txt Tokens per Sec:     4245 || Lr: 0.000050
2024-02-05 20:00:51,897 Validation result at epoch 331, step    44000: duration: 11.0650s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 6.05813	Translation Loss: 90728.05469	PPL: 8620.77832
	Eval Metric: BLEU
	WER 2.97	(DEL: 0.00,	INS: 0.00,	SUB: 2.97)
	BLEU-4 0.50	(BLEU-1: 10.23,	BLEU-2: 3.11,	BLEU-3: 1.14,	BLEU-4: 0.50)
	CHRF 17.01	ROUGE 8.83
2024-02-05 20:00:51,898 Logging Recognition and Translation Outputs
2024-02-05 20:00:51,899 ========================================================================================================================
2024-02-05 20:00:51,899 Logging Sequence: 57_104.00
2024-02-05 20:00:51,899 	Gloss Reference :	A B+C+D+E
2024-02-05 20:00:51,899 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 20:00:51,899 	Gloss Alignment :	         
2024-02-05 20:00:51,899 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:00:51,901 	Text Reference  :	the next day kohli and kl rahul continued from where they  had left and displayed amazing batting performance without losing their wickets  
2024-02-05 20:00:51,901 	Text Hypothesis :	*** **** *** ***** *** ** soon  after     that the   score of  odi  and pakistan  were    also    faced       a       huge   fan   following
2024-02-05 20:00:51,902 	Text Alignment  :	D   D    D   D     D   D  S     S         S    S     S     S   S        S         S       S       S           S       S      S     S        
2024-02-05 20:00:51,902 ========================================================================================================================
2024-02-05 20:00:51,902 Logging Sequence: 136_64.00
2024-02-05 20:00:51,902 	Gloss Reference :	A B+C+D+E
2024-02-05 20:00:51,902 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 20:00:51,902 	Gloss Alignment :	         
2024-02-05 20:00:51,903 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:00:51,903 	Text Reference  :	**** in all     she         has won     2   medals
2024-02-05 20:00:51,903 	Text Hypothesis :	this is because allegations are ruining his first 
2024-02-05 20:00:51,903 	Text Alignment  :	I    S  S       S           S   S       S   S     
2024-02-05 20:00:51,904 ========================================================================================================================
2024-02-05 20:00:51,904 Logging Sequence: 54_123.00
2024-02-05 20:00:51,904 	Gloss Reference :	A B+C+D+E
2024-02-05 20:00:51,904 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 20:00:51,904 	Gloss Alignment :	         
2024-02-05 20:00:51,904 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:00:51,905 	Text Reference  :	*** * **** vips   sponsors international cricket groups have already booked their hotel rooms
2024-02-05 20:00:51,906 	Text Hypothesis :	but i know people love     to            see     this   is   because of     the   same  room 
2024-02-05 20:00:51,906 	Text Alignment  :	I   I I    S      S        S             S       S      S    S       S      S     S     S    
2024-02-05 20:00:51,906 ========================================================================================================================
2024-02-05 20:00:51,906 Logging Sequence: 168_115.00
2024-02-05 20:00:51,906 	Gloss Reference :	A B+C+D+E
2024-02-05 20:00:51,906 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 20:00:51,906 	Gloss Alignment :	         
2024-02-05 20:00:51,907 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:00:51,907 	Text Reference  :	* ***** ****** **** ***** ** this  has sparked   a  major discussion on social media
2024-02-05 20:00:51,908 	Text Hypothesis :	4 rahul dravid were glued to their fan following in the   field      oh 5      time 
2024-02-05 20:00:51,908 	Text Alignment  :	I I     I      I    I     I  S     S   S         S  S     S          S  S      S    
2024-02-05 20:00:51,908 ========================================================================================================================
2024-02-05 20:00:51,908 Logging Sequence: 121_132.00
2024-02-05 20:00:51,908 	Gloss Reference :	A B+C+D+E
2024-02-05 20:00:51,908 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 20:00:51,908 	Gloss Alignment :	         
2024-02-05 20:00:51,909 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:00:51,909 	Text Reference  :	which is why they     will be retesting her   to check if she consumed any stamina enhancing drugs  
2024-02-05 20:00:51,910 	Text Hypothesis :	***** ** the olympics will be ********* given to ***** ** *** ******** *** ******* ********* mirabai
2024-02-05 20:00:51,910 	Text Alignment  :	D     D  S   S                D         S        D     D  D   D        D   D       D         S      
2024-02-05 20:00:51,910 ========================================================================================================================
2024-02-05 20:00:53,375 Epoch 331: Total Training Recognition Loss 0.14  Total Training Translation Loss 4.84 
2024-02-05 20:00:53,376 EPOCH 332
2024-02-05 20:00:57,859 [Epoch: 332 Step: 00044100] Batch Recognition Loss:   0.000310 => Gls Tokens per Sec:     1374 || Batch Translation Loss:   0.009447 => Txt Tokens per Sec:     3806 || Lr: 0.000050
2024-02-05 20:01:00,877 Epoch 332: Total Training Recognition Loss 0.12  Total Training Translation Loss 4.23 
2024-02-05 20:01:00,877 EPOCH 333
2024-02-05 20:01:03,250 [Epoch: 333 Step: 00044200] Batch Recognition Loss:   0.000524 => Gls Tokens per Sec:     1484 || Batch Translation Loss:   0.014380 => Txt Tokens per Sec:     4130 || Lr: 0.000050
2024-02-05 20:01:08,290 Epoch 333: Total Training Recognition Loss 0.11  Total Training Translation Loss 3.97 
2024-02-05 20:01:08,290 EPOCH 334
2024-02-05 20:01:08,840 [Epoch: 334 Step: 00044300] Batch Recognition Loss:   0.001085 => Gls Tokens per Sec:     1606 || Batch Translation Loss:   0.005262 => Txt Tokens per Sec:     4265 || Lr: 0.000050
2024-02-05 20:01:14,201 [Epoch: 334 Step: 00044400] Batch Recognition Loss:   0.000206 => Gls Tokens per Sec:     1492 || Batch Translation Loss:   0.035103 => Txt Tokens per Sec:     4123 || Lr: 0.000050
2024-02-05 20:01:15,505 Epoch 334: Total Training Recognition Loss 0.11  Total Training Translation Loss 4.95 
2024-02-05 20:01:15,506 EPOCH 335
2024-02-05 20:01:19,764 [Epoch: 335 Step: 00044500] Batch Recognition Loss:   0.000204 => Gls Tokens per Sec:     1466 || Batch Translation Loss:   0.007037 => Txt Tokens per Sec:     4061 || Lr: 0.000050
2024-02-05 20:01:22,888 Epoch 335: Total Training Recognition Loss 0.09  Total Training Translation Loss 7.59 
2024-02-05 20:01:22,888 EPOCH 336
2024-02-05 20:01:25,533 [Epoch: 336 Step: 00044600] Batch Recognition Loss:   0.000211 => Gls Tokens per Sec:     1362 || Batch Translation Loss:   0.019626 => Txt Tokens per Sec:     3914 || Lr: 0.000050
2024-02-05 20:01:30,222 Epoch 336: Total Training Recognition Loss 0.18  Total Training Translation Loss 8.47 
2024-02-05 20:01:30,222 EPOCH 337
2024-02-05 20:01:30,780 [Epoch: 337 Step: 00044700] Batch Recognition Loss:   0.000257 => Gls Tokens per Sec:     1724 || Batch Translation Loss:   0.060796 => Txt Tokens per Sec:     4794 || Lr: 0.000050
2024-02-05 20:01:36,299 [Epoch: 337 Step: 00044800] Batch Recognition Loss:   0.000253 => Gls Tokens per Sec:     1450 || Batch Translation Loss:   0.032897 => Txt Tokens per Sec:     3949 || Lr: 0.000050
2024-02-05 20:01:37,504 Epoch 337: Total Training Recognition Loss 0.17  Total Training Translation Loss 11.02 
2024-02-05 20:01:37,504 EPOCH 338
2024-02-05 20:01:41,783 [Epoch: 338 Step: 00044900] Batch Recognition Loss:   0.000539 => Gls Tokens per Sec:     1478 || Batch Translation Loss:   0.018012 => Txt Tokens per Sec:     4144 || Lr: 0.000050
2024-02-05 20:01:44,786 Epoch 338: Total Training Recognition Loss 0.11  Total Training Translation Loss 11.23 
2024-02-05 20:01:44,787 EPOCH 339
2024-02-05 20:01:47,457 [Epoch: 339 Step: 00045000] Batch Recognition Loss:   0.004571 => Gls Tokens per Sec:     1379 || Batch Translation Loss:   0.014918 => Txt Tokens per Sec:     3883 || Lr: 0.000050
2024-02-05 20:01:52,113 Epoch 339: Total Training Recognition Loss 0.19  Total Training Translation Loss 6.20 
2024-02-05 20:01:52,114 EPOCH 340
2024-02-05 20:01:52,814 [Epoch: 340 Step: 00045100] Batch Recognition Loss:   0.000237 => Gls Tokens per Sec:     1486 || Batch Translation Loss:   0.014862 => Txt Tokens per Sec:     3966 || Lr: 0.000050
2024-02-05 20:01:58,328 [Epoch: 340 Step: 00045200] Batch Recognition Loss:   0.000323 => Gls Tokens per Sec:     1447 || Batch Translation Loss:   0.020735 => Txt Tokens per Sec:     3972 || Lr: 0.000050
2024-02-05 20:01:59,484 Epoch 340: Total Training Recognition Loss 0.09  Total Training Translation Loss 6.22 
2024-02-05 20:01:59,484 EPOCH 341
2024-02-05 20:02:04,042 [Epoch: 341 Step: 00045300] Batch Recognition Loss:   0.000543 => Gls Tokens per Sec:     1404 || Batch Translation Loss:   0.033081 => Txt Tokens per Sec:     3927 || Lr: 0.000050
2024-02-05 20:02:06,872 Epoch 341: Total Training Recognition Loss 0.10  Total Training Translation Loss 8.94 
2024-02-05 20:02:06,872 EPOCH 342
2024-02-05 20:02:09,288 [Epoch: 342 Step: 00045400] Batch Recognition Loss:   0.000241 => Gls Tokens per Sec:     1558 || Batch Translation Loss:   0.023097 => Txt Tokens per Sec:     4307 || Lr: 0.000050
2024-02-05 20:02:14,159 Epoch 342: Total Training Recognition Loss 0.12  Total Training Translation Loss 10.86 
2024-02-05 20:02:14,160 EPOCH 343
2024-02-05 20:02:14,829 [Epoch: 343 Step: 00045500] Batch Recognition Loss:   0.000249 => Gls Tokens per Sec:     1677 || Batch Translation Loss:   0.028609 => Txt Tokens per Sec:     4704 || Lr: 0.000050
2024-02-05 20:02:20,101 [Epoch: 343 Step: 00045600] Batch Recognition Loss:   0.000262 => Gls Tokens per Sec:     1517 || Batch Translation Loss:   0.033596 => Txt Tokens per Sec:     4191 || Lr: 0.000050
2024-02-05 20:02:21,207 Epoch 343: Total Training Recognition Loss 0.22  Total Training Translation Loss 7.09 
2024-02-05 20:02:21,207 EPOCH 344
2024-02-05 20:02:25,746 [Epoch: 344 Step: 00045700] Batch Recognition Loss:   0.000238 => Gls Tokens per Sec:     1428 || Batch Translation Loss:   0.019897 => Txt Tokens per Sec:     3866 || Lr: 0.000050
2024-02-05 20:02:28,610 Epoch 344: Total Training Recognition Loss 0.15  Total Training Translation Loss 4.18 
2024-02-05 20:02:28,610 EPOCH 345
2024-02-05 20:02:30,948 [Epoch: 345 Step: 00045800] Batch Recognition Loss:   0.000211 => Gls Tokens per Sec:     1643 || Batch Translation Loss:   0.175811 => Txt Tokens per Sec:     4549 || Lr: 0.000050
2024-02-05 20:02:35,430 Epoch 345: Total Training Recognition Loss 0.07  Total Training Translation Loss 6.76 
2024-02-05 20:02:35,431 EPOCH 346
2024-02-05 20:02:36,188 [Epoch: 346 Step: 00045900] Batch Recognition Loss:   0.002885 => Gls Tokens per Sec:     1587 || Batch Translation Loss:   0.182336 => Txt Tokens per Sec:     4312 || Lr: 0.000050
2024-02-05 20:02:41,894 [Epoch: 346 Step: 00046000] Batch Recognition Loss:   0.000790 => Gls Tokens per Sec:     1399 || Batch Translation Loss:   0.018163 => Txt Tokens per Sec:     3887 || Lr: 0.000050
2024-02-05 20:02:52,942 Validation result at epoch 346, step    46000: duration: 11.0483s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 6.44919	Translation Loss: 90168.47656	PPL: 8152.17920
	Eval Metric: BLEU
	WER 3.53	(DEL: 0.00,	INS: 0.00,	SUB: 3.53)
	BLEU-4 0.51	(BLEU-1: 11.28,	BLEU-2: 3.64,	BLEU-3: 1.33,	BLEU-4: 0.51)
	CHRF 17.39	ROUGE 9.46
2024-02-05 20:02:52,944 Logging Recognition and Translation Outputs
2024-02-05 20:02:52,944 ========================================================================================================================
2024-02-05 20:02:52,944 Logging Sequence: 87_207.00
2024-02-05 20:02:52,945 	Gloss Reference :	A B+C+D+E
2024-02-05 20:02:52,945 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 20:02:52,945 	Gloss Alignment :	         
2024-02-05 20:02:52,945 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:02:52,946 	Text Reference  :	there were 2-3  pakistanis who were speaking anti-india things and things on    kashmir
2024-02-05 20:02:52,946 	Text Hypothesis :	***** what will bcci       do  with your     medals     since  his games  candy crush  
2024-02-05 20:02:52,946 	Text Alignment  :	D     S    S    S          S   S    S        S          S      S   S      S     S      
2024-02-05 20:02:52,946 ========================================================================================================================
2024-02-05 20:02:52,947 Logging Sequence: 67_73.00
2024-02-05 20:02:52,947 	Gloss Reference :	A B+C+D+E
2024-02-05 20:02:52,947 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 20:02:52,947 	Gloss Alignment :	         
2024-02-05 20:02:52,947 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:02:52,948 	Text Reference  :	in his tweet    he      also  said
2024-02-05 20:02:52,948 	Text Hypothesis :	** icc under-19 cricket world cup 
2024-02-05 20:02:52,948 	Text Alignment  :	D  S   S        S       S     S   
2024-02-05 20:02:52,948 ========================================================================================================================
2024-02-05 20:02:52,948 Logging Sequence: 172_267.00
2024-02-05 20:02:52,948 	Gloss Reference :	A B+C+D+E
2024-02-05 20:02:52,948 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 20:02:52,949 	Gloss Alignment :	         
2024-02-05 20:02:52,949 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:02:52,949 	Text Reference  :	**** ** *** such provisions have been made     
2024-02-05 20:02:52,949 	Text Hypothesis :	this is why the  bcci       had  17   statement
2024-02-05 20:02:52,949 	Text Alignment  :	I    I  I   S    S          S    S    S        
2024-02-05 20:02:52,949 ========================================================================================================================
2024-02-05 20:02:52,950 Logging Sequence: 144_23.00
2024-02-05 20:02:52,950 	Gloss Reference :	A B+C+D+E
2024-02-05 20:02:52,950 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 20:02:52,950 	Gloss Alignment :	         
2024-02-05 20:02:52,950 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:02:52,951 	Text Reference  :	***** the girl is 14-year-old mumal mehar and she is         from kanasar village   of   barmer in        rajasthan
2024-02-05 20:02:52,952 	Text Hypothesis :	since the **** ** *********** ***** start of  the tournament on   23rd    september till 29th   september 2023     
2024-02-05 20:02:52,952 	Text Alignment  :	I         D    D  D           D     S     S   S   S          S    S       S         S    S      S         S        
2024-02-05 20:02:52,952 ========================================================================================================================
2024-02-05 20:02:52,952 Logging Sequence: 133_202.00
2024-02-05 20:02:52,952 	Gloss Reference :	A B+C+D+E
2024-02-05 20:02:52,952 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 20:02:52,952 	Gloss Alignment :	         
2024-02-05 20:02:52,953 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:02:52,953 	Text Reference  :	australia has already qualified for       the   final      if  india wins    it will face australia
2024-02-05 20:02:52,954 	Text Hypothesis :	********* *** ******* ********* similarly smith introduced his team  members to the  pm   amazing  
2024-02-05 20:02:52,954 	Text Alignment  :	D         D   D       D         S         S     S          S   S     S       S  S    S    S        
2024-02-05 20:02:52,954 ========================================================================================================================
2024-02-05 20:02:54,002 Epoch 346: Total Training Recognition Loss 0.10  Total Training Translation Loss 6.71 
2024-02-05 20:02:54,002 EPOCH 347
2024-02-05 20:02:58,394 [Epoch: 347 Step: 00046100] Batch Recognition Loss:   0.000188 => Gls Tokens per Sec:     1494 || Batch Translation Loss:   0.080152 => Txt Tokens per Sec:     4144 || Lr: 0.000050
2024-02-05 20:03:01,352 Epoch 347: Total Training Recognition Loss 0.09  Total Training Translation Loss 7.12 
2024-02-05 20:03:01,352 EPOCH 348
2024-02-05 20:03:04,194 [Epoch: 348 Step: 00046200] Batch Recognition Loss:   0.000399 => Gls Tokens per Sec:     1380 || Batch Translation Loss:   0.031050 => Txt Tokens per Sec:     3727 || Lr: 0.000050
2024-02-05 20:03:08,864 Epoch 348: Total Training Recognition Loss 0.12  Total Training Translation Loss 8.43 
2024-02-05 20:03:08,864 EPOCH 349
2024-02-05 20:03:09,764 [Epoch: 349 Step: 00046300] Batch Recognition Loss:   0.000508 => Gls Tokens per Sec:     1424 || Batch Translation Loss:   0.034060 => Txt Tokens per Sec:     4087 || Lr: 0.000050
2024-02-05 20:03:15,303 [Epoch: 349 Step: 00046400] Batch Recognition Loss:   0.000237 => Gls Tokens per Sec:     1445 || Batch Translation Loss:   0.017809 => Txt Tokens per Sec:     3951 || Lr: 0.000050
2024-02-05 20:03:16,270 Epoch 349: Total Training Recognition Loss 0.09  Total Training Translation Loss 9.31 
2024-02-05 20:03:16,270 EPOCH 350
2024-02-05 20:03:20,660 [Epoch: 350 Step: 00046500] Batch Recognition Loss:   0.000442 => Gls Tokens per Sec:     1513 || Batch Translation Loss:   0.058460 => Txt Tokens per Sec:     4157 || Lr: 0.000050
2024-02-05 20:03:23,696 Epoch 350: Total Training Recognition Loss 0.08  Total Training Translation Loss 6.08 
2024-02-05 20:03:23,696 EPOCH 351
2024-02-05 20:03:26,241 [Epoch: 351 Step: 00046600] Batch Recognition Loss:   0.000413 => Gls Tokens per Sec:     1572 || Batch Translation Loss:   0.037713 => Txt Tokens per Sec:     4086 || Lr: 0.000050
2024-02-05 20:03:30,936 Epoch 351: Total Training Recognition Loss 0.12  Total Training Translation Loss 4.61 
2024-02-05 20:03:30,936 EPOCH 352
2024-02-05 20:03:31,908 [Epoch: 352 Step: 00046700] Batch Recognition Loss:   0.000201 => Gls Tokens per Sec:     1400 || Batch Translation Loss:   0.009378 => Txt Tokens per Sec:     3812 || Lr: 0.000050
2024-02-05 20:03:37,489 [Epoch: 352 Step: 00046800] Batch Recognition Loss:   0.000335 => Gls Tokens per Sec:     1430 || Batch Translation Loss:   0.020630 => Txt Tokens per Sec:     3973 || Lr: 0.000050
2024-02-05 20:03:38,464 Epoch 352: Total Training Recognition Loss 0.12  Total Training Translation Loss 3.42 
2024-02-05 20:03:38,464 EPOCH 353
2024-02-05 20:03:43,255 [Epoch: 353 Step: 00046900] Batch Recognition Loss:   0.000156 => Gls Tokens per Sec:     1403 || Batch Translation Loss:   0.008800 => Txt Tokens per Sec:     3919 || Lr: 0.000050
2024-02-05 20:03:45,890 Epoch 353: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.97 
2024-02-05 20:03:45,890 EPOCH 354
2024-02-05 20:03:48,609 [Epoch: 354 Step: 00047000] Batch Recognition Loss:   0.000454 => Gls Tokens per Sec:     1502 || Batch Translation Loss:   0.046252 => Txt Tokens per Sec:     4134 || Lr: 0.000050
2024-02-05 20:03:53,293 Epoch 354: Total Training Recognition Loss 0.13  Total Training Translation Loss 5.83 
2024-02-05 20:03:53,293 EPOCH 355
2024-02-05 20:03:54,213 [Epoch: 355 Step: 00047100] Batch Recognition Loss:   0.000874 => Gls Tokens per Sec:     1569 || Batch Translation Loss:   0.056730 => Txt Tokens per Sec:     4077 || Lr: 0.000050
2024-02-05 20:03:59,811 [Epoch: 355 Step: 00047200] Batch Recognition Loss:   0.000510 => Gls Tokens per Sec:     1426 || Batch Translation Loss:   0.043143 => Txt Tokens per Sec:     4036 || Lr: 0.000050
2024-02-05 20:04:00,533 Epoch 355: Total Training Recognition Loss 0.11  Total Training Translation Loss 16.27 
2024-02-05 20:04:00,533 EPOCH 356
2024-02-05 20:04:05,395 [Epoch: 356 Step: 00047300] Batch Recognition Loss:   0.000197 => Gls Tokens per Sec:     1399 || Batch Translation Loss:   0.024469 => Txt Tokens per Sec:     3900 || Lr: 0.000050
2024-02-05 20:04:08,020 Epoch 356: Total Training Recognition Loss 0.10  Total Training Translation Loss 7.89 
2024-02-05 20:04:08,020 EPOCH 357
2024-02-05 20:04:10,581 [Epoch: 357 Step: 00047400] Batch Recognition Loss:   0.006284 => Gls Tokens per Sec:     1625 || Batch Translation Loss:   0.030163 => Txt Tokens per Sec:     4406 || Lr: 0.000050
2024-02-05 20:04:15,145 Epoch 357: Total Training Recognition Loss 0.07  Total Training Translation Loss 6.88 
2024-02-05 20:04:15,145 EPOCH 358
2024-02-05 20:04:16,132 [Epoch: 358 Step: 00047500] Batch Recognition Loss:   0.005738 => Gls Tokens per Sec:     1542 || Batch Translation Loss:   0.005315 => Txt Tokens per Sec:     3912 || Lr: 0.000050
2024-02-05 20:04:21,952 [Epoch: 358 Step: 00047600] Batch Recognition Loss:   0.000351 => Gls Tokens per Sec:     1371 || Batch Translation Loss:   0.022980 => Txt Tokens per Sec:     3893 || Lr: 0.000050
2024-02-05 20:04:22,728 Epoch 358: Total Training Recognition Loss 0.08  Total Training Translation Loss 4.88 
2024-02-05 20:04:22,728 EPOCH 359
2024-02-05 20:04:27,369 [Epoch: 359 Step: 00047700] Batch Recognition Loss:   0.000286 => Gls Tokens per Sec:     1483 || Batch Translation Loss:   0.030416 => Txt Tokens per Sec:     4082 || Lr: 0.000050
2024-02-05 20:04:29,960 Epoch 359: Total Training Recognition Loss 0.14  Total Training Translation Loss 5.38 
2024-02-05 20:04:29,961 EPOCH 360
2024-02-05 20:04:33,065 [Epoch: 360 Step: 00047800] Batch Recognition Loss:   0.000122 => Gls Tokens per Sec:     1366 || Batch Translation Loss:   0.023480 => Txt Tokens per Sec:     3784 || Lr: 0.000050
2024-02-05 20:04:37,505 Epoch 360: Total Training Recognition Loss 0.21  Total Training Translation Loss 4.79 
2024-02-05 20:04:37,506 EPOCH 361
2024-02-05 20:04:38,388 [Epoch: 361 Step: 00047900] Batch Recognition Loss:   0.000444 => Gls Tokens per Sec:     1816 || Batch Translation Loss:   0.011391 => Txt Tokens per Sec:     4698 || Lr: 0.000050
2024-02-05 20:04:43,964 [Epoch: 361 Step: 00048000] Batch Recognition Loss:   0.000345 => Gls Tokens per Sec:     1431 || Batch Translation Loss:   0.016960 => Txt Tokens per Sec:     4044 || Lr: 0.000050
2024-02-05 20:04:55,534 Validation result at epoch 361, step    48000: duration: 11.5703s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 6.05626	Translation Loss: 91368.46094	PPL: 9190.21387
	Eval Metric: BLEU
	WER 3.53	(DEL: 0.00,	INS: 0.00,	SUB: 3.53)
	BLEU-4 0.85	(BLEU-1: 11.02,	BLEU-2: 3.59,	BLEU-3: 1.60,	BLEU-4: 0.85)
	CHRF 16.98	ROUGE 9.72
2024-02-05 20:04:55,535 Logging Recognition and Translation Outputs
2024-02-05 20:04:55,535 ========================================================================================================================
2024-02-05 20:04:55,535 Logging Sequence: 96_93.00
2024-02-05 20:04:55,536 	Gloss Reference :	A B+C+D+E
2024-02-05 20:04:55,536 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 20:04:55,536 	Gloss Alignment :	         
2024-02-05 20:04:55,536 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:04:55,537 	Text Reference  :	bhuvneshwar kumar took 4   wickets and hardik  pandya took 3 wickets wonderful
2024-02-05 20:04:55,537 	Text Hypothesis :	*********** ***** **** now the     own cricket team   took * a       wicket   
2024-02-05 20:04:55,537 	Text Alignment  :	D           D     D    S   S       S   S       S           D S       S        
2024-02-05 20:04:55,537 ========================================================================================================================
2024-02-05 20:04:55,537 Logging Sequence: 144_2.00
2024-02-05 20:04:55,537 	Gloss Reference :	A B+C+D+E    
2024-02-05 20:04:55,537 	Gloss Hypothesis:	A B+C+D+E+D+E
2024-02-05 20:04:55,537 	Gloss Alignment :	  S          
2024-02-05 20:04:55,538 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:04:55,539 	Text Reference  :	a girl      posted a  video  of herself playing cricket on a village farm on social media the     video      has        gone      viral  
2024-02-05 20:04:55,539 	Text Hypothesis :	* moroccans living in cities of ******* ******* ******* ** * ******* **** ** ****** ***** belgium netherland celebrated morocco's victory
2024-02-05 20:04:55,539 	Text Alignment  :	D S         S      S  S         D       D       D       D  D D       D    D  D      D     S       S          S          S         S      
2024-02-05 20:04:55,539 ========================================================================================================================
2024-02-05 20:04:55,539 Logging Sequence: 178_83.00
2024-02-05 20:04:55,540 	Gloss Reference :	A B+C+D+E
2024-02-05 20:04:55,540 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 20:04:55,540 	Gloss Alignment :	         
2024-02-05 20:04:55,540 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:04:55,540 	Text Reference  :	and the police still haven't apprehended the   wrestler
2024-02-05 20:04:55,541 	Text Hypothesis :	and the couple were  very    particular  about it      
2024-02-05 20:04:55,541 	Text Alignment  :	        S      S     S       S           S     S       
2024-02-05 20:04:55,541 ========================================================================================================================
2024-02-05 20:04:55,541 Logging Sequence: 169_214.00
2024-02-05 20:04:55,541 	Gloss Reference :	A B+C+D+E
2024-02-05 20:04:55,541 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 20:04:55,541 	Gloss Alignment :	         
2024-02-05 20:04:55,541 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:04:55,543 	Text Reference  :	virat kohli said that though arshdeep dropped the catch he   is still a ******** ************ strong  part of  the     indian team 
2024-02-05 20:04:55,543 	Text Hypothesis :	***** ***** **** the  car    was      told    to  pay   this is ***** a talented wicketkeeper captain old  and medical be     added
2024-02-05 20:04:55,544 	Text Alignment  :	D     D     D    S    S      S        S       S   S     S       D       I        I            S       S    S   S       S      S    
2024-02-05 20:04:55,544 ========================================================================================================================
2024-02-05 20:04:55,544 Logging Sequence: 147_202.00
2024-02-05 20:04:55,544 	Gloss Reference :	A B+C+D+E
2024-02-05 20:04:55,544 	Gloss Hypothesis:	A B+C+D  
2024-02-05 20:04:55,544 	Gloss Alignment :	  S      
2024-02-05 20:04:55,545 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:04:55,546 	Text Reference  :	were impressed that she   took      the difficult decision to      withdraw from  the   olympics and focus on her mental   health
2024-02-05 20:04:55,546 	Text Hypothesis :	**** earlier   the  board organised the test      series   against each     other celeb parents  and ***** ** *** venomous abuses
2024-02-05 20:04:55,546 	Text Alignment  :	D    S         S    S     S             S         S        S       S        S     S     S            D     D  D   S        S     
2024-02-05 20:04:55,547 ========================================================================================================================
2024-02-05 20:04:56,223 Epoch 361: Total Training Recognition Loss 0.09  Total Training Translation Loss 3.60 
2024-02-05 20:04:56,223 EPOCH 362
2024-02-05 20:05:01,175 [Epoch: 362 Step: 00048100] Batch Recognition Loss:   0.000255 => Gls Tokens per Sec:     1406 || Batch Translation Loss:   0.025866 => Txt Tokens per Sec:     3887 || Lr: 0.000050
2024-02-05 20:05:03,791 Epoch 362: Total Training Recognition Loss 0.12  Total Training Translation Loss 6.00 
2024-02-05 20:05:03,792 EPOCH 363
2024-02-05 20:05:06,666 [Epoch: 363 Step: 00048200] Batch Recognition Loss:   0.000336 => Gls Tokens per Sec:     1504 || Batch Translation Loss:   0.019994 => Txt Tokens per Sec:     4221 || Lr: 0.000050
2024-02-05 20:05:10,978 Epoch 363: Total Training Recognition Loss 0.12  Total Training Translation Loss 9.12 
2024-02-05 20:05:10,979 EPOCH 364
2024-02-05 20:05:12,123 [Epoch: 364 Step: 00048300] Batch Recognition Loss:   0.000174 => Gls Tokens per Sec:     1470 || Batch Translation Loss:   0.015572 => Txt Tokens per Sec:     4163 || Lr: 0.000050
2024-02-05 20:05:17,735 [Epoch: 364 Step: 00048400] Batch Recognition Loss:   0.000239 => Gls Tokens per Sec:     1422 || Batch Translation Loss:   0.039097 => Txt Tokens per Sec:     3889 || Lr: 0.000050
2024-02-05 20:05:18,377 Epoch 364: Total Training Recognition Loss 0.09  Total Training Translation Loss 10.04 
2024-02-05 20:05:18,377 EPOCH 365
2024-02-05 20:05:23,372 [Epoch: 365 Step: 00048500] Batch Recognition Loss:   0.000146 => Gls Tokens per Sec:     1409 || Batch Translation Loss:   1.304635 => Txt Tokens per Sec:     3946 || Lr: 0.000050
2024-02-05 20:05:25,732 Epoch 365: Total Training Recognition Loss 0.18  Total Training Translation Loss 10.35 
2024-02-05 20:05:25,733 EPOCH 366
2024-02-05 20:05:28,356 [Epoch: 366 Step: 00048600] Batch Recognition Loss:   0.000256 => Gls Tokens per Sec:     1678 || Batch Translation Loss:   0.019390 => Txt Tokens per Sec:     4607 || Lr: 0.000050
2024-02-05 20:05:33,044 Epoch 366: Total Training Recognition Loss 0.12  Total Training Translation Loss 9.01 
2024-02-05 20:05:33,045 EPOCH 367
2024-02-05 20:05:34,143 [Epoch: 367 Step: 00048700] Batch Recognition Loss:   0.000231 => Gls Tokens per Sec:     1603 || Batch Translation Loss:   0.041038 => Txt Tokens per Sec:     4348 || Lr: 0.000050
2024-02-05 20:05:39,636 [Epoch: 367 Step: 00048800] Batch Recognition Loss:   0.000448 => Gls Tokens per Sec:     1453 || Batch Translation Loss:   0.027587 => Txt Tokens per Sec:     4044 || Lr: 0.000050
2024-02-05 20:05:40,267 Epoch 367: Total Training Recognition Loss 0.11  Total Training Translation Loss 6.88 
2024-02-05 20:05:40,267 EPOCH 368
2024-02-05 20:05:45,183 [Epoch: 368 Step: 00048900] Batch Recognition Loss:   0.000172 => Gls Tokens per Sec:     1449 || Batch Translation Loss:   0.014533 => Txt Tokens per Sec:     3997 || Lr: 0.000050
2024-02-05 20:05:47,826 Epoch 368: Total Training Recognition Loss 0.09  Total Training Translation Loss 4.93 
2024-02-05 20:05:47,827 EPOCH 369
2024-02-05 20:05:51,156 [Epoch: 369 Step: 00049000] Batch Recognition Loss:   0.000303 => Gls Tokens per Sec:     1346 || Batch Translation Loss:   0.023190 => Txt Tokens per Sec:     3743 || Lr: 0.000050
2024-02-05 20:05:55,178 Epoch 369: Total Training Recognition Loss 0.11  Total Training Translation Loss 3.89 
2024-02-05 20:05:55,178 EPOCH 370
2024-02-05 20:05:56,506 [Epoch: 370 Step: 00049100] Batch Recognition Loss:   0.000432 => Gls Tokens per Sec:     1388 || Batch Translation Loss:   0.020999 => Txt Tokens per Sec:     4006 || Lr: 0.000050
2024-02-05 20:06:01,923 [Epoch: 370 Step: 00049200] Batch Recognition Loss:   0.000221 => Gls Tokens per Sec:     1477 || Batch Translation Loss:   0.024265 => Txt Tokens per Sec:     4071 || Lr: 0.000050
2024-02-05 20:06:02,516 Epoch 370: Total Training Recognition Loss 0.10  Total Training Translation Loss 3.97 
2024-02-05 20:06:02,517 EPOCH 371
2024-02-05 20:06:07,452 [Epoch: 371 Step: 00049300] Batch Recognition Loss:   0.000304 => Gls Tokens per Sec:     1459 || Batch Translation Loss:   0.053105 => Txt Tokens per Sec:     4048 || Lr: 0.000050
2024-02-05 20:06:09,909 Epoch 371: Total Training Recognition Loss 0.23  Total Training Translation Loss 12.12 
2024-02-05 20:06:09,909 EPOCH 372
2024-02-05 20:06:13,093 [Epoch: 372 Step: 00049400] Batch Recognition Loss:   0.000466 => Gls Tokens per Sec:     1432 || Batch Translation Loss:   0.599464 => Txt Tokens per Sec:     3944 || Lr: 0.000050
2024-02-05 20:06:17,296 Epoch 372: Total Training Recognition Loss 0.12  Total Training Translation Loss 11.06 
2024-02-05 20:06:17,296 EPOCH 373
2024-02-05 20:06:18,486 [Epoch: 373 Step: 00049500] Batch Recognition Loss:   0.001640 => Gls Tokens per Sec:     1614 || Batch Translation Loss:   0.044032 => Txt Tokens per Sec:     4442 || Lr: 0.000050
2024-02-05 20:06:24,091 [Epoch: 373 Step: 00049600] Batch Recognition Loss:   0.000699 => Gls Tokens per Sec:     1424 || Batch Translation Loss:   0.061461 => Txt Tokens per Sec:     3948 || Lr: 0.000050
2024-02-05 20:06:24,627 Epoch 373: Total Training Recognition Loss 0.14  Total Training Translation Loss 7.43 
2024-02-05 20:06:24,627 EPOCH 374
2024-02-05 20:06:29,789 [Epoch: 374 Step: 00049700] Batch Recognition Loss:   0.001781 => Gls Tokens per Sec:     1410 || Batch Translation Loss:   0.022241 => Txt Tokens per Sec:     3923 || Lr: 0.000050
2024-02-05 20:06:32,228 Epoch 374: Total Training Recognition Loss 0.14  Total Training Translation Loss 7.00 
2024-02-05 20:06:32,228 EPOCH 375
2024-02-05 20:06:35,493 [Epoch: 375 Step: 00049800] Batch Recognition Loss:   0.000333 => Gls Tokens per Sec:     1422 || Batch Translation Loss:   0.010216 => Txt Tokens per Sec:     3974 || Lr: 0.000050
2024-02-05 20:06:39,676 Epoch 375: Total Training Recognition Loss 0.11  Total Training Translation Loss 4.39 
2024-02-05 20:06:39,676 EPOCH 376
2024-02-05 20:06:41,112 [Epoch: 376 Step: 00049900] Batch Recognition Loss:   0.000193 => Gls Tokens per Sec:     1393 || Batch Translation Loss:   0.010830 => Txt Tokens per Sec:     3645 || Lr: 0.000050
2024-02-05 20:06:46,767 [Epoch: 376 Step: 00050000] Batch Recognition Loss:   0.000465 => Gls Tokens per Sec:     1412 || Batch Translation Loss:   0.026805 => Txt Tokens per Sec:     3963 || Lr: 0.000050
2024-02-05 20:06:58,190 Validation result at epoch 376, step    50000: duration: 11.4219s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 6.27305	Translation Loss: 89925.65625	PPL: 7956.83643
	Eval Metric: BLEU
	WER 4.03	(DEL: 0.00,	INS: 0.00,	SUB: 4.03)
	BLEU-4 0.77	(BLEU-1: 11.20,	BLEU-2: 3.79,	BLEU-3: 1.59,	BLEU-4: 0.77)
	CHRF 17.06	ROUGE 9.45
2024-02-05 20:06:58,192 Logging Recognition and Translation Outputs
2024-02-05 20:06:58,192 ========================================================================================================================
2024-02-05 20:06:58,192 Logging Sequence: 178_157.00
2024-02-05 20:06:58,193 	Gloss Reference :	A B+C+D+E
2024-02-05 20:06:58,193 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 20:06:58,193 	Gloss Alignment :	         
2024-02-05 20:06:58,193 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:06:58,194 	Text Reference  :	this is why sushil kumar will have to    be arrested
2024-02-05 20:06:58,194 	Text Hypothesis :	**** ** *** and    that  was  very proud of him     
2024-02-05 20:06:58,195 	Text Alignment  :	D    D  D   S      S     S    S    S     S  S       
2024-02-05 20:06:58,195 ========================================================================================================================
2024-02-05 20:06:58,195 Logging Sequence: 118_111.00
2024-02-05 20:06:58,195 	Gloss Reference :	A B+C+D+E
2024-02-05 20:06:58,195 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 20:06:58,196 	Gloss Alignment :	         
2024-02-05 20:06:58,196 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:06:58,197 	Text Reference  :	and  people encourage him   have hope  for the next world cup  
2024-02-05 20:06:58,197 	Text Hypothesis :	here is     the       first time india won the **** ***** match
2024-02-05 20:06:58,197 	Text Alignment  :	S    S      S         S     S    S     S       D    D     S    
2024-02-05 20:06:58,198 ========================================================================================================================
2024-02-05 20:06:58,198 Logging Sequence: 148_2.00
2024-02-05 20:06:58,198 	Gloss Reference :	A B+C+D+E  
2024-02-05 20:06:58,198 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-05 20:06:58,199 	Gloss Alignment :	  S        
2024-02-05 20:06:58,199 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:06:58,201 	Text Reference  :	the final of the asia cup   2023 cricket tournament was played between india and sri lanka on   17th september 2023
2024-02-05 20:06:58,201 	Text Hypothesis :	*** ***** ** the **** first time hardik  pandya     was ****** ******* ***** *** *** ***** held in   t20i      runs
2024-02-05 20:06:58,202 	Text Alignment  :	D   D     D      D    S     S    S       S              D      D       D     D   D   D     S    S    S         S   
2024-02-05 20:06:58,202 ========================================================================================================================
2024-02-05 20:06:58,202 Logging Sequence: 83_129.00
2024-02-05 20:06:58,202 	Gloss Reference :	A B+C+D+E
2024-02-05 20:06:58,203 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 20:06:58,203 	Gloss Alignment :	         
2024-02-05 20:06:58,203 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:06:58,204 	Text Reference  :	** ** later the  denmark football association tweeted
2024-02-05 20:06:58,204 	Text Hypothesis :	he is a     very proud   of       euro        2020   
2024-02-05 20:06:58,204 	Text Alignment  :	I  I  S     S    S       S        S           S      
2024-02-05 20:06:58,204 ========================================================================================================================
2024-02-05 20:06:58,204 Logging Sequence: 99_158.00
2024-02-05 20:06:58,205 	Gloss Reference :	A B+C+D+E
2024-02-05 20:06:58,205 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 20:06:58,205 	Gloss Alignment :	         
2024-02-05 20:06:58,205 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:06:58,207 	Text Reference  :	** *** **** ***** ** the  incident occured  in dubai and    it     was extremely shameful
2024-02-05 20:06:58,207 	Text Hypothesis :	he was then moved to play against  pakistan in ***** sports rather you call      it      
2024-02-05 20:06:58,207 	Text Alignment  :	I  I   I    I     I  S    S        S           D     S      S      S   S         S       
2024-02-05 20:06:58,207 ========================================================================================================================
2024-02-05 20:06:58,894 Epoch 376: Total Training Recognition Loss 0.17  Total Training Translation Loss 4.02 
2024-02-05 20:06:58,895 EPOCH 377
2024-02-05 20:07:05,915 [Epoch: 377 Step: 00050100] Batch Recognition Loss:   0.000561 => Gls Tokens per Sec:     1049 || Batch Translation Loss:   0.032990 => Txt Tokens per Sec:     2897 || Lr: 0.000050
2024-02-05 20:07:08,165 Epoch 377: Total Training Recognition Loss 0.11  Total Training Translation Loss 5.63 
2024-02-05 20:07:08,166 EPOCH 378
2024-02-05 20:07:11,319 [Epoch: 378 Step: 00050200] Batch Recognition Loss:   0.000222 => Gls Tokens per Sec:     1498 || Batch Translation Loss:   0.023392 => Txt Tokens per Sec:     4081 || Lr: 0.000050
2024-02-05 20:07:15,517 Epoch 378: Total Training Recognition Loss 0.10  Total Training Translation Loss 4.91 
2024-02-05 20:07:15,517 EPOCH 379
2024-02-05 20:07:16,901 [Epoch: 379 Step: 00050300] Batch Recognition Loss:   0.000344 => Gls Tokens per Sec:     1504 || Batch Translation Loss:   0.021692 => Txt Tokens per Sec:     4402 || Lr: 0.000050
2024-02-05 20:07:22,214 [Epoch: 379 Step: 00050400] Batch Recognition Loss:   0.000786 => Gls Tokens per Sec:     1502 || Batch Translation Loss:   0.058153 => Txt Tokens per Sec:     4100 || Lr: 0.000050
2024-02-05 20:07:22,638 Epoch 379: Total Training Recognition Loss 0.08  Total Training Translation Loss 7.79 
2024-02-05 20:07:22,638 EPOCH 380
2024-02-05 20:07:28,063 [Epoch: 380 Step: 00050500] Batch Recognition Loss:   0.000505 => Gls Tokens per Sec:     1372 || Batch Translation Loss:   0.085183 => Txt Tokens per Sec:     3779 || Lr: 0.000050
2024-02-05 20:07:30,381 Epoch 380: Total Training Recognition Loss 0.17  Total Training Translation Loss 9.38 
2024-02-05 20:07:30,382 EPOCH 381
2024-02-05 20:07:33,752 [Epoch: 381 Step: 00050600] Batch Recognition Loss:   0.000252 => Gls Tokens per Sec:     1425 || Batch Translation Loss:   0.046499 => Txt Tokens per Sec:     3994 || Lr: 0.000050
2024-02-05 20:07:37,762 Epoch 381: Total Training Recognition Loss 0.13  Total Training Translation Loss 7.52 
2024-02-05 20:07:37,762 EPOCH 382
2024-02-05 20:07:39,050 [Epoch: 382 Step: 00050700] Batch Recognition Loss:   0.002175 => Gls Tokens per Sec:     1680 || Batch Translation Loss:   0.033888 => Txt Tokens per Sec:     4697 || Lr: 0.000050
2024-02-05 20:07:44,605 [Epoch: 382 Step: 00050800] Batch Recognition Loss:   0.000856 => Gls Tokens per Sec:     1440 || Batch Translation Loss:   0.068081 => Txt Tokens per Sec:     3971 || Lr: 0.000050
2024-02-05 20:07:45,001 Epoch 382: Total Training Recognition Loss 0.13  Total Training Translation Loss 7.70 
2024-02-05 20:07:45,001 EPOCH 383
2024-02-05 20:07:50,350 [Epoch: 383 Step: 00050900] Batch Recognition Loss:   0.000467 => Gls Tokens per Sec:     1406 || Batch Translation Loss:   0.037449 => Txt Tokens per Sec:     3934 || Lr: 0.000050
2024-02-05 20:07:52,536 Epoch 383: Total Training Recognition Loss 0.11  Total Training Translation Loss 10.64 
2024-02-05 20:07:52,536 EPOCH 384
2024-02-05 20:07:55,939 [Epoch: 384 Step: 00051000] Batch Recognition Loss:   0.000239 => Gls Tokens per Sec:     1434 || Batch Translation Loss:   0.019376 => Txt Tokens per Sec:     3961 || Lr: 0.000050
2024-02-05 20:07:59,970 Epoch 384: Total Training Recognition Loss 0.12  Total Training Translation Loss 5.14 
2024-02-05 20:07:59,970 EPOCH 385
2024-02-05 20:08:01,479 [Epoch: 385 Step: 00051100] Batch Recognition Loss:   0.000221 => Gls Tokens per Sec:     1486 || Batch Translation Loss:   0.038949 => Txt Tokens per Sec:     3955 || Lr: 0.000050
2024-02-05 20:08:07,407 [Epoch: 385 Step: 00051200] Batch Recognition Loss:   0.000567 => Gls Tokens per Sec:     1346 || Batch Translation Loss:   0.038128 => Txt Tokens per Sec:     3756 || Lr: 0.000050
2024-02-05 20:08:07,716 Epoch 385: Total Training Recognition Loss 0.09  Total Training Translation Loss 5.17 
2024-02-05 20:08:07,716 EPOCH 386
2024-02-05 20:08:12,947 [Epoch: 386 Step: 00051300] Batch Recognition Loss:   0.000413 => Gls Tokens per Sec:     1453 || Batch Translation Loss:   0.020444 => Txt Tokens per Sec:     4038 || Lr: 0.000050
2024-02-05 20:08:15,086 Epoch 386: Total Training Recognition Loss 0.12  Total Training Translation Loss 4.79 
2024-02-05 20:08:15,086 EPOCH 387
2024-02-05 20:08:18,657 [Epoch: 387 Step: 00051400] Batch Recognition Loss:   0.001108 => Gls Tokens per Sec:     1390 || Batch Translation Loss:   0.034891 => Txt Tokens per Sec:     3793 || Lr: 0.000050
2024-02-05 20:08:22,583 Epoch 387: Total Training Recognition Loss 0.14  Total Training Translation Loss 3.30 
2024-02-05 20:08:22,583 EPOCH 388
2024-02-05 20:08:24,030 [Epoch: 388 Step: 00051500] Batch Recognition Loss:   0.000100 => Gls Tokens per Sec:     1604 || Batch Translation Loss:   0.010196 => Txt Tokens per Sec:     4442 || Lr: 0.000050
2024-02-05 20:08:29,830 [Epoch: 388 Step: 00051600] Batch Recognition Loss:   0.000240 => Gls Tokens per Sec:     1376 || Batch Translation Loss:   0.029994 => Txt Tokens per Sec:     3798 || Lr: 0.000050
2024-02-05 20:08:30,074 Epoch 388: Total Training Recognition Loss 0.12  Total Training Translation Loss 4.24 
2024-02-05 20:08:30,074 EPOCH 389
2024-02-05 20:08:35,745 [Epoch: 389 Step: 00051700] Batch Recognition Loss:   0.000190 => Gls Tokens per Sec:     1355 || Batch Translation Loss:   0.039002 => Txt Tokens per Sec:     3704 || Lr: 0.000050
2024-02-05 20:08:37,950 Epoch 389: Total Training Recognition Loss 0.15  Total Training Translation Loss 9.90 
2024-02-05 20:08:37,950 EPOCH 390
2024-02-05 20:08:41,334 [Epoch: 390 Step: 00051800] Batch Recognition Loss:   0.000227 => Gls Tokens per Sec:     1490 || Batch Translation Loss:   0.049594 => Txt Tokens per Sec:     3976 || Lr: 0.000050
2024-02-05 20:08:45,428 Epoch 390: Total Training Recognition Loss 0.08  Total Training Translation Loss 8.89 
2024-02-05 20:08:45,428 EPOCH 391
2024-02-05 20:08:46,958 [Epoch: 391 Step: 00051900] Batch Recognition Loss:   0.000664 => Gls Tokens per Sec:     1571 || Batch Translation Loss:   0.018134 => Txt Tokens per Sec:     4217 || Lr: 0.000050
2024-02-05 20:08:52,849 [Epoch: 391 Step: 00052000] Batch Recognition Loss:   0.000461 => Gls Tokens per Sec:     1355 || Batch Translation Loss:   0.109415 => Txt Tokens per Sec:     3801 || Lr: 0.000050
2024-02-05 20:09:03,323 Validation result at epoch 391, step    52000: duration: 10.4722s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 5.67766	Translation Loss: 90945.58594	PPL: 8810.13672
	Eval Metric: BLEU
	WER 4.17	(DEL: 0.00,	INS: 0.00,	SUB: 4.17)
	BLEU-4 0.48	(BLEU-1: 10.37,	BLEU-2: 3.15,	BLEU-3: 1.08,	BLEU-4: 0.48)
	CHRF 17.09	ROUGE 8.95
2024-02-05 20:09:03,324 Logging Recognition and Translation Outputs
2024-02-05 20:09:03,324 ========================================================================================================================
2024-02-05 20:09:03,324 Logging Sequence: 59_101.00
2024-02-05 20:09:03,325 	Gloss Reference :	A B+C+D+E
2024-02-05 20:09:03,325 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 20:09:03,325 	Gloss Alignment :	         
2024-02-05 20:09:03,325 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:09:03,327 	Text Reference  :	did you see  the video fox said she     won       her     medals because of  a condom and is  very happy
2024-02-05 20:09:03,327 	Text Hypothesis :	*** in  june the ***** *** most strange statement against sri    lanka   for a look   at  the 4th  over 
2024-02-05 20:09:03,327 	Text Alignment  :	D   S   S        D     D   S    S       S         S       S      S       S     S      S   S   S    S    
2024-02-05 20:09:03,327 ========================================================================================================================
2024-02-05 20:09:03,327 Logging Sequence: 103_112.00
2024-02-05 20:09:03,328 	Gloss Reference :	A B+C+D+E
2024-02-05 20:09:03,328 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 20:09:03,328 	Gloss Alignment :	         
2024-02-05 20:09:03,328 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:09:03,329 	Text Reference  :	you are aware that earlier the britishers had colonized a     lot  of     countries in      the    world
2024-02-05 20:09:03,329 	Text Hypothesis :	*** *** ***** **** ******* the ********** *** ********* games were called their     british empire games
2024-02-05 20:09:03,329 	Text Alignment  :	D   D   D     D    D           D          D   D         S     S    S      S         S       S      S    
2024-02-05 20:09:03,329 ========================================================================================================================
2024-02-05 20:09:03,329 Logging Sequence: 143_11.00
2024-02-05 20:09:03,329 	Gloss Reference :	A B+C+D+E
2024-02-05 20:09:03,330 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 20:09:03,330 	Gloss Alignment :	         
2024-02-05 20:09:03,330 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:09:03,331 	Text Reference  :	ronaldo has also become the first person to have  500      million followers on  instagram  he  is    the       most loved      footballer
2024-02-05 20:09:03,332 	Text Hypothesis :	******* *** **** ****** the ***** ****** ** other football team    about     the tournament and found ronaldo's name definitely pops      
2024-02-05 20:09:03,332 	Text Alignment  :	D       D   D    D          D     D      D  S     S        S       S         S   S          S   S     S         S    S          S         
2024-02-05 20:09:03,332 ========================================================================================================================
2024-02-05 20:09:03,332 Logging Sequence: 183_23.00
2024-02-05 20:09:03,332 	Gloss Reference :	A B+C+D+E
2024-02-05 20:09:03,332 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 20:09:03,332 	Gloss Alignment :	         
2024-02-05 20:09:03,333 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:09:03,334 	Text Reference  :	however everybody has been   waiting for  them to    announce the **** *** name   of      the *** child
2024-02-05 20:09:03,334 	Text Hypothesis :	and     scored    a   report said    that the  hotel with     the mind was played between the and it   
2024-02-05 20:09:03,334 	Text Alignment  :	S       S         S   S      S       S    S    S     S            I    I   S      S           I   S    
2024-02-05 20:09:03,334 ========================================================================================================================
2024-02-05 20:09:03,334 Logging Sequence: 169_165.00
2024-02-05 20:09:03,335 	Gloss Reference :	A B+C+D+E
2024-02-05 20:09:03,335 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 20:09:03,335 	Gloss Alignment :	         
2024-02-05 20:09:03,335 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:09:03,336 	Text Reference  :	the ***** indian  government was ********** ** outraged by   the  incident and      these changes were   undone by  wikipedia
2024-02-05 20:09:03,337 	Text Hypothesis :	the men's cricket team       was devastated as they     were sent to       complete the   british empire and    the ball     
2024-02-05 20:09:03,337 	Text Alignment  :	    I     S       S              I          I  S        S    S    S        S        S     S       S      S      S   S        
2024-02-05 20:09:03,337 ========================================================================================================================
2024-02-05 20:09:03,475 Epoch 391: Total Training Recognition Loss 0.07  Total Training Translation Loss 6.37 
2024-02-05 20:09:03,475 EPOCH 392
2024-02-05 20:09:09,424 [Epoch: 392 Step: 00052100] Batch Recognition Loss:   0.000607 => Gls Tokens per Sec:     1305 || Batch Translation Loss:   0.024300 => Txt Tokens per Sec:     3654 || Lr: 0.000050
2024-02-05 20:09:11,435 Epoch 392: Total Training Recognition Loss 0.12  Total Training Translation Loss 7.66 
2024-02-05 20:09:11,435 EPOCH 393
2024-02-05 20:09:14,825 [Epoch: 393 Step: 00052200] Batch Recognition Loss:   0.000233 => Gls Tokens per Sec:     1511 || Batch Translation Loss:   0.016235 => Txt Tokens per Sec:     4236 || Lr: 0.000050
2024-02-05 20:09:18,938 Epoch 393: Total Training Recognition Loss 0.11  Total Training Translation Loss 7.68 
2024-02-05 20:09:18,939 EPOCH 394
2024-02-05 20:09:20,741 [Epoch: 394 Step: 00052300] Batch Recognition Loss:   0.000215 => Gls Tokens per Sec:     1377 || Batch Translation Loss:   0.030829 => Txt Tokens per Sec:     3768 || Lr: 0.000050
2024-02-05 20:09:26,181 [Epoch: 394 Step: 00052400] Batch Recognition Loss:   0.000704 => Gls Tokens per Sec:     1467 || Batch Translation Loss:   0.026053 => Txt Tokens per Sec:     4053 || Lr: 0.000050
2024-02-05 20:09:26,296 Epoch 394: Total Training Recognition Loss 0.13  Total Training Translation Loss 10.15 
2024-02-05 20:09:26,296 EPOCH 395
2024-02-05 20:09:31,756 [Epoch: 395 Step: 00052500] Batch Recognition Loss:   0.000998 => Gls Tokens per Sec:     1436 || Batch Translation Loss:   0.049264 => Txt Tokens per Sec:     3953 || Lr: 0.000050
2024-02-05 20:09:33,841 Epoch 395: Total Training Recognition Loss 0.16  Total Training Translation Loss 12.39 
2024-02-05 20:09:33,841 EPOCH 396
2024-02-05 20:09:37,030 [Epoch: 396 Step: 00052600] Batch Recognition Loss:   0.003955 => Gls Tokens per Sec:     1631 || Batch Translation Loss:   0.023038 => Txt Tokens per Sec:     4513 || Lr: 0.000050
2024-02-05 20:09:40,876 Epoch 396: Total Training Recognition Loss 0.14  Total Training Translation Loss 8.30 
2024-02-05 20:09:40,877 EPOCH 397
2024-02-05 20:09:42,550 [Epoch: 397 Step: 00052700] Batch Recognition Loss:   0.000641 => Gls Tokens per Sec:     1531 || Batch Translation Loss:   0.046157 => Txt Tokens per Sec:     4267 || Lr: 0.000050
2024-02-05 20:09:48,256 [Epoch: 397 Step: 00052800] Batch Recognition Loss:   0.000421 => Gls Tokens per Sec:     1402 || Batch Translation Loss:   0.030542 => Txt Tokens per Sec:     3864 || Lr: 0.000050
2024-02-05 20:09:48,358 Epoch 397: Total Training Recognition Loss 0.20  Total Training Translation Loss 4.78 
2024-02-05 20:09:48,358 EPOCH 398
2024-02-05 20:09:53,679 [Epoch: 398 Step: 00052900] Batch Recognition Loss:   0.002183 => Gls Tokens per Sec:     1489 || Batch Translation Loss:   0.016841 => Txt Tokens per Sec:     4133 || Lr: 0.000050
2024-02-05 20:09:55,729 Epoch 398: Total Training Recognition Loss 0.09  Total Training Translation Loss 4.06 
2024-02-05 20:09:55,730 EPOCH 399
2024-02-05 20:09:59,397 [Epoch: 399 Step: 00053000] Batch Recognition Loss:   0.000175 => Gls Tokens per Sec:     1440 || Batch Translation Loss:   0.011109 => Txt Tokens per Sec:     3909 || Lr: 0.000050
2024-02-05 20:10:03,019 Epoch 399: Total Training Recognition Loss 0.07  Total Training Translation Loss 4.83 
2024-02-05 20:10:03,019 EPOCH 400
2024-02-05 20:10:04,772 [Epoch: 400 Step: 00053100] Batch Recognition Loss:   0.000496 => Gls Tokens per Sec:     1508 || Batch Translation Loss:   0.016613 => Txt Tokens per Sec:     4310 || Lr: 0.000050
2024-02-05 20:10:10,470 [Epoch: 400 Step: 00053200] Batch Recognition Loss:   0.000580 => Gls Tokens per Sec:     1401 || Batch Translation Loss:   0.025783 => Txt Tokens per Sec:     3832 || Lr: 0.000050
2024-02-05 20:10:10,470 Epoch 400: Total Training Recognition Loss 0.10  Total Training Translation Loss 4.92 
2024-02-05 20:10:10,471 EPOCH 401
2024-02-05 20:10:16,152 [Epoch: 401 Step: 00053300] Batch Recognition Loss:   0.000266 => Gls Tokens per Sec:     1408 || Batch Translation Loss:   0.138513 => Txt Tokens per Sec:     3902 || Lr: 0.000050
2024-02-05 20:10:17,992 Epoch 401: Total Training Recognition Loss 0.09  Total Training Translation Loss 6.35 
2024-02-05 20:10:17,992 EPOCH 402
2024-02-05 20:10:21,401 [Epoch: 402 Step: 00053400] Batch Recognition Loss:   0.002260 => Gls Tokens per Sec:     1573 || Batch Translation Loss:   0.254009 => Txt Tokens per Sec:     4371 || Lr: 0.000050
2024-02-05 20:10:25,374 Epoch 402: Total Training Recognition Loss 0.16  Total Training Translation Loss 6.18 
2024-02-05 20:10:25,375 EPOCH 403
2024-02-05 20:10:27,415 [Epoch: 403 Step: 00053500] Batch Recognition Loss:   0.000268 => Gls Tokens per Sec:     1334 || Batch Translation Loss:   0.054298 => Txt Tokens per Sec:     3668 || Lr: 0.000050
2024-02-05 20:10:32,784 Epoch 403: Total Training Recognition Loss 0.11  Total Training Translation Loss 6.05 
2024-02-05 20:10:32,784 EPOCH 404
2024-02-05 20:10:32,875 [Epoch: 404 Step: 00053600] Batch Recognition Loss:   0.000737 => Gls Tokens per Sec:      889 || Batch Translation Loss:   0.148861 => Txt Tokens per Sec:     3233 || Lr: 0.000050
2024-02-05 20:10:38,581 [Epoch: 404 Step: 00053700] Batch Recognition Loss:   0.001014 => Gls Tokens per Sec:     1402 || Batch Translation Loss:   0.031019 => Txt Tokens per Sec:     3868 || Lr: 0.000050
2024-02-05 20:10:40,283 Epoch 404: Total Training Recognition Loss 0.11  Total Training Translation Loss 6.09 
2024-02-05 20:10:40,283 EPOCH 405
2024-02-05 20:10:43,558 [Epoch: 405 Step: 00053800] Batch Recognition Loss:   0.000516 => Gls Tokens per Sec:     1662 || Batch Translation Loss:   0.016390 => Txt Tokens per Sec:     4632 || Lr: 0.000050
2024-02-05 20:10:47,151 Epoch 405: Total Training Recognition Loss 0.07  Total Training Translation Loss 4.25 
2024-02-05 20:10:47,152 EPOCH 406
2024-02-05 20:10:48,979 [Epoch: 406 Step: 00053900] Batch Recognition Loss:   0.002718 => Gls Tokens per Sec:     1533 || Batch Translation Loss:   0.038715 => Txt Tokens per Sec:     4108 || Lr: 0.000050
2024-02-05 20:10:54,737 Epoch 406: Total Training Recognition Loss 0.15  Total Training Translation Loss 5.40 
2024-02-05 20:10:54,738 EPOCH 407
2024-02-05 20:10:54,841 [Epoch: 407 Step: 00054000] Batch Recognition Loss:   0.000223 => Gls Tokens per Sec:     1569 || Batch Translation Loss:   0.014483 => Txt Tokens per Sec:     4039 || Lr: 0.000050
2024-02-05 20:11:08,043 Validation result at epoch 407, step    54000: duration: 13.2027s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 5.79307	Translation Loss: 89250.80469	PPL: 7438.19092
	Eval Metric: BLEU
	WER 3.39	(DEL: 0.00,	INS: 0.00,	SUB: 3.39)
	BLEU-4 0.65	(BLEU-1: 11.56,	BLEU-2: 3.63,	BLEU-3: 1.36,	BLEU-4: 0.65)
	CHRF 17.32	ROUGE 9.68
2024-02-05 20:11:08,045 Logging Recognition and Translation Outputs
2024-02-05 20:11:08,045 ========================================================================================================================
2024-02-05 20:11:08,045 Logging Sequence: 166_243.00
2024-02-05 20:11:08,045 	Gloss Reference :	A B+C+D+E
2024-02-05 20:11:08,045 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 20:11:08,045 	Gloss Alignment :	         
2024-02-05 20:11:08,045 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:11:08,047 	Text Reference  :	*** ********* *********** ********* *** ***** ** icc     worked with members boards like bcci pcb   cricket australia etc 
2024-02-05 20:11:08,047 	Text Hypothesis :	the broadcast advertisers ticketing etc would be decided by     the  board   of     the  2    teams playing the       test
2024-02-05 20:11:08,047 	Text Alignment  :	I   I         I           I         I   I     I  S       S      S    S       S      S    S    S     S       S         S   
2024-02-05 20:11:08,047 ========================================================================================================================
2024-02-05 20:11:08,047 Logging Sequence: 179_409.00
2024-02-05 20:11:08,048 	Gloss Reference :	A B+C+D+E
2024-02-05 20:11:08,048 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 20:11:08,048 	Gloss Alignment :	         
2024-02-05 20:11:08,048 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:11:08,048 	Text Reference  :	the passport  was  at    the       wfi      office in  delhi
2024-02-05 20:11:08,049 	Text Hypothesis :	*** meanwhile some funny incidents happened during the match
2024-02-05 20:11:08,049 	Text Alignment  :	D   S         S    S     S         S        S      S   S    
2024-02-05 20:11:08,049 ========================================================================================================================
2024-02-05 20:11:08,049 Logging Sequence: 81_407.00
2024-02-05 20:11:08,049 	Gloss Reference :	A B+C+D+E
2024-02-05 20:11:08,049 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 20:11:08,049 	Gloss Alignment :	         
2024-02-05 20:11:08,049 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:11:08,051 	Text Reference  :	the government company -   national buildings construction corporation and  they will complete them     in a   time-bound manner  
2024-02-05 20:11:08,051 	Text Hypothesis :	the ********** company was not      able      to           harbhajan   said it   will be       focussed by the delhi      capitals
2024-02-05 20:11:08,051 	Text Alignment  :	    D                  S   S        S         S            S           S    S         S        S        S  S   S          S       
2024-02-05 20:11:08,051 ========================================================================================================================
2024-02-05 20:11:08,051 Logging Sequence: 96_31.00
2024-02-05 20:11:08,052 	Gloss Reference :	A B+C+D+E
2024-02-05 20:11:08,052 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 20:11:08,052 	Gloss Alignment :	         
2024-02-05 20:11:08,052 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:11:08,053 	Text Reference  :	and then 2     teams will go  on    to  play the     final
2024-02-05 20:11:08,053 	Text Hypothesis :	*** **** india had   won  the match for 15th october 2023 
2024-02-05 20:11:08,053 	Text Alignment  :	D   D    S     S     S    S   S     S   S    S       S    
2024-02-05 20:11:08,053 ========================================================================================================================
2024-02-05 20:11:08,053 Logging Sequence: 160_87.00
2024-02-05 20:11:08,053 	Gloss Reference :	A B+C+D+E
2024-02-05 20:11:08,054 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 20:11:08,054 	Gloss Alignment :	         
2024-02-05 20:11:08,054 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:11:08,054 	Text Reference  :	********* ******** ** ********** kohli held  a   press conference and said
2024-02-05 20:11:08,054 	Text Hypothesis :	currently everyone to understand that  india was done  in         the usa 
2024-02-05 20:11:08,055 	Text Alignment  :	I         I        I  I          S     S     S   S     S          S   S   
2024-02-05 20:11:08,055 ========================================================================================================================
2024-02-05 20:11:14,069 [Epoch: 407 Step: 00054100] Batch Recognition Loss:   0.000223 => Gls Tokens per Sec:     1328 || Batch Translation Loss:   0.063237 => Txt Tokens per Sec:     3696 || Lr: 0.000050
2024-02-05 20:11:15,885 Epoch 407: Total Training Recognition Loss 0.14  Total Training Translation Loss 6.73 
2024-02-05 20:11:15,885 EPOCH 408
2024-02-05 20:11:19,483 [Epoch: 408 Step: 00054200] Batch Recognition Loss:   0.000142 => Gls Tokens per Sec:     1535 || Batch Translation Loss:   0.014872 => Txt Tokens per Sec:     4250 || Lr: 0.000050
2024-02-05 20:11:23,072 Epoch 408: Total Training Recognition Loss 0.12  Total Training Translation Loss 9.43 
2024-02-05 20:11:23,073 EPOCH 409
2024-02-05 20:11:25,050 [Epoch: 409 Step: 00054300] Batch Recognition Loss:   0.000213 => Gls Tokens per Sec:     1458 || Batch Translation Loss:   0.048023 => Txt Tokens per Sec:     3957 || Lr: 0.000050
2024-02-05 20:11:30,447 Epoch 409: Total Training Recognition Loss 0.11  Total Training Translation Loss 7.92 
2024-02-05 20:11:30,448 EPOCH 410
2024-02-05 20:11:30,671 [Epoch: 410 Step: 00054400] Batch Recognition Loss:   0.000251 => Gls Tokens per Sec:     1086 || Batch Translation Loss:   0.037719 => Txt Tokens per Sec:     3502 || Lr: 0.000050
2024-02-05 20:11:36,306 [Epoch: 410 Step: 00054500] Batch Recognition Loss:   0.000315 => Gls Tokens per Sec:     1420 || Batch Translation Loss:   0.088560 => Txt Tokens per Sec:     3887 || Lr: 0.000050
2024-02-05 20:11:38,077 Epoch 410: Total Training Recognition Loss 0.08  Total Training Translation Loss 7.07 
2024-02-05 20:11:38,077 EPOCH 411
2024-02-05 20:11:41,827 [Epoch: 411 Step: 00054600] Batch Recognition Loss:   0.001879 => Gls Tokens per Sec:     1494 || Batch Translation Loss:   0.653306 => Txt Tokens per Sec:     4077 || Lr: 0.000050
2024-02-05 20:11:45,283 Epoch 411: Total Training Recognition Loss 0.10  Total Training Translation Loss 7.25 
2024-02-05 20:11:45,283 EPOCH 412
2024-02-05 20:11:47,317 [Epoch: 412 Step: 00054700] Batch Recognition Loss:   0.000173 => Gls Tokens per Sec:     1456 || Batch Translation Loss:   0.020483 => Txt Tokens per Sec:     4009 || Lr: 0.000050
2024-02-05 20:11:52,997 Epoch 412: Total Training Recognition Loss 0.09  Total Training Translation Loss 8.47 
2024-02-05 20:11:52,998 EPOCH 413
2024-02-05 20:11:53,207 [Epoch: 413 Step: 00054800] Batch Recognition Loss:   0.000257 => Gls Tokens per Sec:     1539 || Batch Translation Loss:   0.132181 => Txt Tokens per Sec:     4005 || Lr: 0.000050
2024-02-05 20:11:58,880 [Epoch: 413 Step: 00054900] Batch Recognition Loss:   0.000357 => Gls Tokens per Sec:     1411 || Batch Translation Loss:   0.032951 => Txt Tokens per Sec:     3896 || Lr: 0.000050
2024-02-05 20:12:00,492 Epoch 413: Total Training Recognition Loss 0.07  Total Training Translation Loss 8.21 
2024-02-05 20:12:00,492 EPOCH 414
2024-02-05 20:12:04,459 [Epoch: 414 Step: 00055000] Batch Recognition Loss:   0.000557 => Gls Tokens per Sec:     1432 || Batch Translation Loss:   0.034054 => Txt Tokens per Sec:     4016 || Lr: 0.000050
2024-02-05 20:12:07,946 Epoch 414: Total Training Recognition Loss 0.07  Total Training Translation Loss 7.21 
2024-02-05 20:12:07,947 EPOCH 415
2024-02-05 20:12:09,851 [Epoch: 415 Step: 00055100] Batch Recognition Loss:   0.000154 => Gls Tokens per Sec:     1597 || Batch Translation Loss:   0.116314 => Txt Tokens per Sec:     4277 || Lr: 0.000050
2024-02-05 20:12:15,074 Epoch 415: Total Training Recognition Loss 0.10  Total Training Translation Loss 7.48 
2024-02-05 20:12:15,075 EPOCH 416
2024-02-05 20:12:15,323 [Epoch: 416 Step: 00055200] Batch Recognition Loss:   0.000998 => Gls Tokens per Sec:     1613 || Batch Translation Loss:   0.026478 => Txt Tokens per Sec:     4319 || Lr: 0.000050
2024-02-05 20:12:20,139 [Epoch: 416 Step: 00055300] Batch Recognition Loss:   0.000211 => Gls Tokens per Sec:     1657 || Batch Translation Loss:   0.073097 => Txt Tokens per Sec:     4597 || Lr: 0.000050
2024-02-05 20:12:21,571 Epoch 416: Total Training Recognition Loss 0.17  Total Training Translation Loss 8.43 
2024-02-05 20:12:21,572 EPOCH 417
2024-02-05 20:12:25,526 [Epoch: 417 Step: 00055400] Batch Recognition Loss:   0.000367 => Gls Tokens per Sec:     1457 || Batch Translation Loss:   0.020822 => Txt Tokens per Sec:     4062 || Lr: 0.000050
2024-02-05 20:12:28,816 Epoch 417: Total Training Recognition Loss 0.17  Total Training Translation Loss 5.16 
2024-02-05 20:12:28,817 EPOCH 418
2024-02-05 20:12:30,943 [Epoch: 418 Step: 00055500] Batch Recognition Loss:   0.000436 => Gls Tokens per Sec:     1468 || Batch Translation Loss:   0.091524 => Txt Tokens per Sec:     4093 || Lr: 0.000050
2024-02-05 20:12:36,043 Epoch 418: Total Training Recognition Loss 0.08  Total Training Translation Loss 4.49 
2024-02-05 20:12:36,044 EPOCH 419
2024-02-05 20:12:36,352 [Epoch: 419 Step: 00055600] Batch Recognition Loss:   0.001535 => Gls Tokens per Sec:     1561 || Batch Translation Loss:   0.027716 => Txt Tokens per Sec:     4426 || Lr: 0.000050
2024-02-05 20:12:41,968 [Epoch: 419 Step: 00055700] Batch Recognition Loss:   0.000158 => Gls Tokens per Sec:     1424 || Batch Translation Loss:   0.016623 => Txt Tokens per Sec:     3941 || Lr: 0.000050
2024-02-05 20:12:43,526 Epoch 419: Total Training Recognition Loss 0.13  Total Training Translation Loss 6.77 
2024-02-05 20:12:43,526 EPOCH 420
2024-02-05 20:12:47,546 [Epoch: 420 Step: 00055800] Batch Recognition Loss:   0.000260 => Gls Tokens per Sec:     1453 || Batch Translation Loss:   0.197564 => Txt Tokens per Sec:     4008 || Lr: 0.000050
2024-02-05 20:12:50,948 Epoch 420: Total Training Recognition Loss 0.25  Total Training Translation Loss 9.52 
2024-02-05 20:12:50,948 EPOCH 421
2024-02-05 20:12:53,207 [Epoch: 421 Step: 00055900] Batch Recognition Loss:   0.000456 => Gls Tokens per Sec:     1417 || Batch Translation Loss:   0.020926 => Txt Tokens per Sec:     3842 || Lr: 0.000050
2024-02-05 20:12:58,267 Epoch 421: Total Training Recognition Loss 0.16  Total Training Translation Loss 5.09 
2024-02-05 20:12:58,267 EPOCH 422
2024-02-05 20:12:58,586 [Epoch: 422 Step: 00056000] Batch Recognition Loss:   0.000264 => Gls Tokens per Sec:     1761 || Batch Translation Loss:   0.024911 => Txt Tokens per Sec:     5233 || Lr: 0.000050
2024-02-05 20:13:09,880 Validation result at epoch 422, step    56000: duration: 11.2938s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 6.34038	Translation Loss: 89711.23438	PPL: 7788.24609
	Eval Metric: BLEU
	WER 4.24	(DEL: 0.00,	INS: 0.00,	SUB: 4.24)
	BLEU-4 0.68	(BLEU-1: 10.77,	BLEU-2: 3.47,	BLEU-3: 1.35,	BLEU-4: 0.68)
	CHRF 16.97	ROUGE 9.35
2024-02-05 20:13:09,881 Logging Recognition and Translation Outputs
2024-02-05 20:13:09,881 ========================================================================================================================
2024-02-05 20:13:09,881 Logging Sequence: 177_167.00
2024-02-05 20:13:09,881 	Gloss Reference :	A B+C+D+E
2024-02-05 20:13:09,881 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 20:13:09,882 	Gloss Alignment :	         
2024-02-05 20:13:09,882 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:13:09,883 	Text Reference  :	*** ********** this    is  because sushil   wanted  to  establish his  fear   to   ensure no one       would oppose him    
2024-02-05 20:13:09,883 	Text Hypothesis :	who graciously accepts one and     politely returns the wfi       kept saying that will   be available for   the    matches
2024-02-05 20:13:09,884 	Text Alignment  :	I   I          S       S   S       S        S       S   S         S    S      S    S      S  S         S     S      S      
2024-02-05 20:13:09,884 ========================================================================================================================
2024-02-05 20:13:09,884 Logging Sequence: 127_140.00
2024-02-05 20:13:09,884 	Gloss Reference :	A B+C+D+E
2024-02-05 20:13:09,884 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 20:13:09,884 	Gloss Alignment :	         
2024-02-05 20:13:09,884 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:13:09,886 	Text Reference  :	this  is  india' 3rd  medal in the world athletics championships he    is very talented and his performance is   highly  impressive
2024-02-05 20:13:09,886 	Text Hypothesis :	india now hotel  room rates in *** ***** ********* ************* india is **** ******** *** *** *********** 15th october 2023      
2024-02-05 20:13:09,886 	Text Alignment  :	S     S   S      S    S        D   D     D         D             S        D    D        D   D   D           S    S       S         
2024-02-05 20:13:09,886 ========================================================================================================================
2024-02-05 20:13:09,886 Logging Sequence: 126_200.00
2024-02-05 20:13:09,887 	Gloss Reference :	A B+C+D+E
2024-02-05 20:13:09,887 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 20:13:09,887 	Gloss Alignment :	         
2024-02-05 20:13:09,887 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:13:09,887 	Text Reference  :	let me tell you about them
2024-02-05 20:13:09,887 	Text Hypothesis :	let me tell you about it  
2024-02-05 20:13:09,887 	Text Alignment  :	                      S   
2024-02-05 20:13:09,888 ========================================================================================================================
2024-02-05 20:13:09,888 Logging Sequence: 104_119.00
2024-02-05 20:13:09,888 	Gloss Reference :	A B+C+D+E
2024-02-05 20:13:09,888 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 20:13:09,888 	Gloss Alignment :	         
2024-02-05 20:13:09,888 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:13:09,890 	Text Reference  :	famous chess players like viswanathan anand and     praggnanandhaa's coach r   b   ramesh congratulated him   for    his     impressive performance
2024-02-05 20:13:09,890 	Text Hypothesis :	****** ***** ******* **** *********** the   women's team             too   had won a      young         young player without the        world      
2024-02-05 20:13:09,890 	Text Alignment  :	D      D     D       D    D           S     S       S                S     S   S   S      S             S     S      S       S          S          
2024-02-05 20:13:09,890 ========================================================================================================================
2024-02-05 20:13:09,890 Logging Sequence: 172_267.00
2024-02-05 20:13:09,890 	Gloss Reference :	A B+C+D+E
2024-02-05 20:13:09,890 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 20:13:09,891 	Gloss Alignment :	         
2024-02-05 20:13:09,891 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:13:09,891 	Text Reference  :	**** ** *** *** **** such provisions have      been made 
2024-02-05 20:13:09,891 	Text Hypothesis :	this is why the bcci had  a          statement on   there
2024-02-05 20:13:09,891 	Text Alignment  :	I    I  I   I   I    S    S          S         S    S    
2024-02-05 20:13:09,892 ========================================================================================================================
2024-02-05 20:13:09,895 Training ended since there were no improvements inthe last learning rate step: 0.000050
2024-02-05 20:13:09,896 Best validation result at step     4000:   0.90 eval_metric.
2024-02-05 20:13:34,827 ------------------------------------------------------------
2024-02-05 20:13:34,827 [DEV] partition [RECOGNITION] experiment [BW]: 1
2024-02-05 20:13:46,167 finished in 11.3404s 
2024-02-05 20:13:46,168 ************************************************************
2024-02-05 20:13:46,168 [DEV] partition [RECOGNITION] results:
	New Best CTC Decode Beam Size: 1
	WER 6.21	(DEL: 0.00,	INS: 0.00,	SUB: 6.21)
2024-02-05 20:13:46,168 ************************************************************
2024-02-05 20:13:46,168 ------------------------------------------------------------
2024-02-05 20:13:46,168 [DEV] partition [RECOGNITION] experiment [BW]: 2
2024-02-05 20:13:56,716 finished in 10.5472s 
2024-02-05 20:13:56,716 ************************************************************
2024-02-05 20:13:56,716 [DEV] partition [RECOGNITION] results:
	New Best CTC Decode Beam Size: 2
	WER 6.00	(DEL: 0.00,	INS: 0.00,	SUB: 6.00)
2024-02-05 20:13:56,716 ************************************************************
2024-02-05 20:13:56,717 ------------------------------------------------------------
2024-02-05 20:13:56,717 [DEV] partition [RECOGNITION] experiment [BW]: 3
2024-02-05 20:14:08,859 finished in 12.1421s 
2024-02-05 20:14:08,859 ------------------------------------------------------------
2024-02-05 20:14:08,860 [DEV] partition [RECOGNITION] experiment [BW]: 4
2024-02-05 20:14:19,514 finished in 10.6540s 
2024-02-05 20:14:19,515 ------------------------------------------------------------
2024-02-05 20:14:19,515 [DEV] partition [RECOGNITION] experiment [BW]: 5
2024-02-05 20:14:30,373 finished in 10.8577s 
2024-02-05 20:14:30,374 ------------------------------------------------------------
2024-02-05 20:14:30,374 [DEV] partition [RECOGNITION] experiment [BW]: 6
2024-02-05 20:14:41,399 finished in 11.0250s 
2024-02-05 20:14:41,399 ------------------------------------------------------------
2024-02-05 20:14:41,399 [DEV] partition [RECOGNITION] experiment [BW]: 7
2024-02-05 20:14:52,112 finished in 10.7120s 
2024-02-05 20:14:52,112 ------------------------------------------------------------
2024-02-05 20:14:52,113 [DEV] partition [RECOGNITION] experiment [BW]: 8
2024-02-05 20:15:03,609 finished in 11.4964s 
2024-02-05 20:15:03,610 ------------------------------------------------------------
2024-02-05 20:15:03,610 [DEV] partition [RECOGNITION] experiment [BW]: 9
2024-02-05 20:15:14,661 finished in 11.0515s 
2024-02-05 20:15:14,662 ------------------------------------------------------------
2024-02-05 20:15:14,662 [DEV] partition [RECOGNITION] experiment [BW]: 10
2024-02-05 20:15:25,782 finished in 11.1197s 
2024-02-05 20:15:25,783 ============================================================
2024-02-05 20:15:36,531 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 1 and Alpha: -1
	BLEU-4 0.90	(BLEU-1: 13.04,	BLEU-2: 4.63,	BLEU-3: 1.79,	BLEU-4: 0.90)
	CHRF 16.97	ROUGE 10.51
2024-02-05 20:15:36,532 ------------------------------------------------------------
2024-02-05 20:16:53,745 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 2 and Alpha: -1
	BLEU-4 0.93	(BLEU-1: 11.44,	BLEU-2: 4.12,	BLEU-3: 1.78,	BLEU-4: 0.93)
	CHRF 16.22	ROUGE 9.63
2024-02-05 20:16:53,746 ------------------------------------------------------------
2024-02-05 20:17:18,580 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 2 and Alpha: 1
	BLEU-4 0.93	(BLEU-1: 11.64,	BLEU-2: 4.22,	BLEU-3: 1.81,	BLEU-4: 0.93)
	CHRF 16.26	ROUGE 9.61
2024-02-05 20:17:18,581 ------------------------------------------------------------
2024-02-05 20:17:31,024 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 2 and Alpha: 2
	BLEU-4 0.96	(BLEU-1: 11.92,	BLEU-2: 4.27,	BLEU-3: 1.82,	BLEU-4: 0.96)
	CHRF 16.30	ROUGE 9.54
2024-02-05 20:17:31,025 ------------------------------------------------------------
2024-02-05 20:17:43,618 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 2 and Alpha: 3
	BLEU-4 0.96	(BLEU-1: 11.98,	BLEU-2: 4.31,	BLEU-3: 1.83,	BLEU-4: 0.96)
	CHRF 16.31	ROUGE 9.52
2024-02-05 20:17:43,618 ------------------------------------------------------------
2024-02-05 20:18:58,460 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 3 and Alpha: 2
	BLEU-4 1.05	(BLEU-1: 11.65,	BLEU-2: 4.38,	BLEU-3: 1.98,	BLEU-4: 1.05)
	CHRF 16.14	ROUGE 9.57
2024-02-05 20:18:58,460 ------------------------------------------------------------
2024-02-05 20:19:22,842 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 3 and Alpha: 4
	BLEU-4 1.05	(BLEU-1: 11.81,	BLEU-2: 4.42,	BLEU-3: 1.99,	BLEU-4: 1.05)
	CHRF 16.18	ROUGE 9.55
2024-02-05 20:19:22,843 ------------------------------------------------------------
2024-02-05 20:32:11,725 ************************************************************
2024-02-05 20:32:11,725 [DEV] partition [Recognition & Translation] results:
	Best CTC Decode Beam Size: 2
	Best Translation Beam Size: 3 and Alpha: 4
	WER 6.00	(DEL: 0.00,	INS: 0.00,	SUB: 6.00)
	BLEU-4 1.05	(BLEU-1: 11.81,	BLEU-2: 4.42,	BLEU-3: 1.99,	BLEU-4: 1.05)
	CHRF 16.18	ROUGE 9.55
2024-02-05 20:32:11,726 ************************************************************
2024-02-05 20:32:24,305 [TEST] partition [Recognition & Translation] results:
	Best CTC Decode Beam Size: 2
	Best Translation Beam Size: 3 and Alpha: 4
	WER 4.17	(DEL: 0.00,	INS: 0.00,	SUB: 4.17)
	BLEU-4 0.81	(BLEU-1: 11.35,	BLEU-2: 3.91,	BLEU-3: 1.65,	BLEU-4: 0.81)
	CHRF 16.08	ROUGE 9.35
2024-02-05 20:32:24,305 ************************************************************
