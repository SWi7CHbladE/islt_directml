2024-02-02 20:38:13,406 Hello! This is Joey-NMT.
2024-02-02 20:38:13,421 Total params: 25639944
2024-02-02 20:38:13,422 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'gloss_output_layer.bias', 'gloss_output_layer.weight', 'sgn_embed.ln.bias', 'sgn_embed.ln.weight', 'sgn_embed.norm.norm.bias', 'sgn_embed.norm.norm.weight', 'txt_embed.norm.norm.bias', 'txt_embed.norm.norm.weight']
2024-02-02 20:38:14,538 cfg.name                           : sign_experiment
2024-02-02 20:38:14,538 cfg.data.data_path                 : ./data/Sports_dataset/0/
2024-02-02 20:38:14,538 cfg.data.version                   : phoenix_2014_trans
2024-02-02 20:38:14,538 cfg.data.sgn                       : sign
2024-02-02 20:38:14,538 cfg.data.txt                       : text
2024-02-02 20:38:14,538 cfg.data.gls                       : gloss
2024-02-02 20:38:14,538 cfg.data.train                     : excel_data.train
2024-02-02 20:38:14,539 cfg.data.dev                       : excel_data.dev
2024-02-02 20:38:14,539 cfg.data.test                      : excel_data.test
2024-02-02 20:38:14,539 cfg.data.feature_size              : 2560
2024-02-02 20:38:14,539 cfg.data.level                     : word
2024-02-02 20:38:14,539 cfg.data.txt_lowercase             : True
2024-02-02 20:38:14,539 cfg.data.max_sent_length           : 500
2024-02-02 20:38:14,539 cfg.data.random_train_subset       : -1
2024-02-02 20:38:14,539 cfg.data.random_dev_subset         : -1
2024-02-02 20:38:14,539 cfg.testing.recognition_beam_sizes : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
2024-02-02 20:38:14,540 cfg.testing.translation_beam_sizes : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
2024-02-02 20:38:14,540 cfg.testing.translation_beam_alphas : [-1, 0, 1, 2, 3, 4, 5]
2024-02-02 20:38:14,540 cfg.training.reset_best_ckpt       : False
2024-02-02 20:38:14,540 cfg.training.reset_scheduler       : False
2024-02-02 20:38:14,540 cfg.training.reset_optimizer       : False
2024-02-02 20:38:14,540 cfg.training.random_seed           : 42
2024-02-02 20:38:14,540 cfg.training.model_dir             : ./sign_sample_model/fold0/32head/32batch
2024-02-02 20:38:14,540 cfg.training.recognition_loss_weight : 1.0
2024-02-02 20:38:14,541 cfg.training.translation_loss_weight : 1.0
2024-02-02 20:38:14,541 cfg.training.eval_metric           : bleu
2024-02-02 20:38:14,541 cfg.training.optimizer             : adam
2024-02-02 20:38:14,541 cfg.training.learning_rate         : 0.0001
2024-02-02 20:38:14,541 cfg.training.batch_size            : 32
2024-02-02 20:38:14,541 cfg.training.num_valid_log         : 5
2024-02-02 20:38:14,541 cfg.training.epochs                : 50000
2024-02-02 20:38:14,541 cfg.training.early_stopping_metric : eval_metric
2024-02-02 20:38:14,541 cfg.training.batch_type            : sentence
2024-02-02 20:38:14,542 cfg.training.translation_normalization : batch
2024-02-02 20:38:14,542 cfg.training.eval_recognition_beam_size : 1
2024-02-02 20:38:14,542 cfg.training.eval_translation_beam_size : 1
2024-02-02 20:38:14,542 cfg.training.eval_translation_beam_alpha : -1
2024-02-02 20:38:14,542 cfg.training.overwrite             : True
2024-02-02 20:38:14,542 cfg.training.shuffle               : True
2024-02-02 20:38:14,542 cfg.training.use_cuda              : True
2024-02-02 20:38:14,543 cfg.training.translation_max_output_length : 40
2024-02-02 20:38:14,543 cfg.training.keep_last_ckpts       : 1
2024-02-02 20:38:14,543 cfg.training.batch_multiplier      : 1
2024-02-02 20:38:14,543 cfg.training.logging_freq          : 100
2024-02-02 20:38:14,543 cfg.training.validation_freq       : 2000
2024-02-02 20:38:14,543 cfg.training.betas                 : [0.9, 0.998]
2024-02-02 20:38:14,543 cfg.training.scheduling            : plateau
2024-02-02 20:38:14,543 cfg.training.learning_rate_min     : 1e-08
2024-02-02 20:38:14,544 cfg.training.weight_decay          : 0.0001
2024-02-02 20:38:14,544 cfg.training.patience              : 12
2024-02-02 20:38:14,544 cfg.training.decrease_factor       : 0.5
2024-02-02 20:38:14,544 cfg.training.label_smoothing       : 0.0
2024-02-02 20:38:14,544 cfg.model.initializer              : xavier
2024-02-02 20:38:14,544 cfg.model.bias_initializer         : zeros
2024-02-02 20:38:14,544 cfg.model.init_gain                : 1.0
2024-02-02 20:38:14,544 cfg.model.embed_initializer        : xavier
2024-02-02 20:38:14,544 cfg.model.embed_init_gain          : 1.0
2024-02-02 20:38:14,545 cfg.model.tied_softmax             : True
2024-02-02 20:38:14,545 cfg.model.encoder.type             : transformer
2024-02-02 20:38:14,545 cfg.model.encoder.num_layers       : 3
2024-02-02 20:38:14,545 cfg.model.encoder.num_heads        : 32
2024-02-02 20:38:14,545 cfg.model.encoder.embeddings.embedding_dim : 512
2024-02-02 20:38:14,545 cfg.model.encoder.embeddings.scale : False
2024-02-02 20:38:14,545 cfg.model.encoder.embeddings.dropout : 0.1
2024-02-02 20:38:14,545 cfg.model.encoder.embeddings.norm_type : batch
2024-02-02 20:38:14,545 cfg.model.encoder.embeddings.activation_type : softsign
2024-02-02 20:38:14,546 cfg.model.encoder.hidden_size      : 512
2024-02-02 20:38:14,546 cfg.model.encoder.ff_size          : 2048
2024-02-02 20:38:14,546 cfg.model.encoder.dropout          : 0.1
2024-02-02 20:38:14,546 cfg.model.decoder.type             : transformer
2024-02-02 20:38:14,546 cfg.model.decoder.num_layers       : 3
2024-02-02 20:38:14,546 cfg.model.decoder.num_heads        : 32
2024-02-02 20:38:14,546 cfg.model.decoder.embeddings.embedding_dim : 512
2024-02-02 20:38:14,546 cfg.model.decoder.embeddings.scale : False
2024-02-02 20:38:14,546 cfg.model.decoder.embeddings.dropout : 0.1
2024-02-02 20:38:14,547 cfg.model.decoder.embeddings.norm_type : batch
2024-02-02 20:38:14,547 cfg.model.decoder.embeddings.activation_type : softsign
2024-02-02 20:38:14,547 cfg.model.decoder.hidden_size      : 512
2024-02-02 20:38:14,547 cfg.model.decoder.ff_size          : 2048
2024-02-02 20:38:14,547 cfg.model.decoder.dropout          : 0.1
2024-02-02 20:38:14,547 Data set sizes: 
	train 2126,
	valid 708,
	test 706
2024-02-02 20:38:14,547 First training example:
	[GLS] A B C D E
	[TXT] although new zealand was disappointed to faltered at the finals against australia they did well throughout the tournament
2024-02-02 20:38:14,547 First 10 words (gls): (0) <si> (1) <unk> (2) <pad> (3) A (4) B (5) C (6) D (7) E
2024-02-02 20:38:14,548 First 10 words (txt): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) the (5) and (6) to (7) a (8) in (9) of
2024-02-02 20:38:14,548 Number of unique glosses (types): 8
2024-02-02 20:38:14,548 Number of unique words (types): 4397
2024-02-02 20:38:14,548 SignModel(
	encoder=TransformerEncoder(num_layers=3, num_heads=32),
	decoder=TransformerDecoder(num_layers=3, num_heads=32),
	sgn_embed=SpatialEmbeddings(embedding_dim=512, input_size=2560),
	txt_embed=Embeddings(embedding_dim=512, vocab_size=4397))
2024-02-02 20:38:14,551 EPOCH 1
2024-02-02 20:38:20,465 Epoch   1: Total Training Recognition Loss 156.13  Total Training Translation Loss 6519.90 
2024-02-02 20:38:20,466 EPOCH 2
2024-02-02 20:38:23,170 [Epoch: 002 Step: 00000100] Batch Recognition Loss:   0.068378 => Gls Tokens per Sec:     1953 || Batch Translation Loss:  84.538765 => Txt Tokens per Sec:     5565 || Lr: 0.000100
2024-02-02 20:38:25,733 Epoch   2: Total Training Recognition Loss 6.80  Total Training Translation Loss 5945.03 
2024-02-02 20:38:25,734 EPOCH 3
2024-02-02 20:38:30,356 [Epoch: 003 Step: 00000200] Batch Recognition Loss:   0.007355 => Gls Tokens per Sec:     2266 || Batch Translation Loss:  85.197311 => Txt Tokens per Sec:     6298 || Lr: 0.000100
2024-02-02 20:38:30,400 Epoch   3: Total Training Recognition Loss 1.24  Total Training Translation Loss 5836.25 
2024-02-02 20:38:30,401 EPOCH 4
2024-02-02 20:38:35,966 Epoch   4: Total Training Recognition Loss 0.69  Total Training Translation Loss 5650.69 
2024-02-02 20:38:35,967 EPOCH 5
2024-02-02 20:38:38,154 [Epoch: 005 Step: 00000300] Batch Recognition Loss:   0.011016 => Gls Tokens per Sec:     2342 || Batch Translation Loss:  89.680824 => Txt Tokens per Sec:     6207 || Lr: 0.000100
2024-02-02 20:38:41,144 Epoch   5: Total Training Recognition Loss 0.56  Total Training Translation Loss 5413.92 
2024-02-02 20:38:41,145 EPOCH 6
2024-02-02 20:38:46,109 [Epoch: 006 Step: 00000400] Batch Recognition Loss:   0.005602 => Gls Tokens per Sec:     2078 || Batch Translation Loss:  73.186089 => Txt Tokens per Sec:     5755 || Lr: 0.000100
2024-02-02 20:38:46,288 Epoch   6: Total Training Recognition Loss 0.45  Total Training Translation Loss 5193.87 
2024-02-02 20:38:46,288 EPOCH 7
2024-02-02 20:38:51,483 Epoch   7: Total Training Recognition Loss 0.46  Total Training Translation Loss 4981.99 
2024-02-02 20:38:51,484 EPOCH 8
2024-02-02 20:38:53,848 [Epoch: 008 Step: 00000500] Batch Recognition Loss:   0.004510 => Gls Tokens per Sec:     2100 || Batch Translation Loss:  56.967659 => Txt Tokens per Sec:     5930 || Lr: 0.000100
2024-02-02 20:38:56,568 Epoch   8: Total Training Recognition Loss 0.49  Total Training Translation Loss 4773.05 
2024-02-02 20:38:56,568 EPOCH 9
2024-02-02 20:39:01,178 [Epoch: 009 Step: 00000600] Batch Recognition Loss:   0.004418 => Gls Tokens per Sec:     2202 || Batch Translation Loss:  85.164215 => Txt Tokens per Sec:     6094 || Lr: 0.000100
2024-02-02 20:39:01,417 Epoch   9: Total Training Recognition Loss 0.48  Total Training Translation Loss 4589.40 
2024-02-02 20:39:01,417 EPOCH 10
2024-02-02 20:39:06,901 Epoch  10: Total Training Recognition Loss 0.59  Total Training Translation Loss 4392.86 
2024-02-02 20:39:06,901 EPOCH 11
2024-02-02 20:39:08,925 [Epoch: 011 Step: 00000700] Batch Recognition Loss:   0.017090 => Gls Tokens per Sec:     2373 || Batch Translation Loss:  35.557781 => Txt Tokens per Sec:     6475 || Lr: 0.000100
2024-02-02 20:39:12,021 Epoch  11: Total Training Recognition Loss 0.48  Total Training Translation Loss 4205.92 
2024-02-02 20:39:12,022 EPOCH 12
2024-02-02 20:39:17,281 [Epoch: 012 Step: 00000800] Batch Recognition Loss:   0.010456 => Gls Tokens per Sec:     1900 || Batch Translation Loss:  77.286919 => Txt Tokens per Sec:     5276 || Lr: 0.000100
2024-02-02 20:39:17,618 Epoch  12: Total Training Recognition Loss 0.61  Total Training Translation Loss 4039.99 
2024-02-02 20:39:17,619 EPOCH 13
2024-02-02 20:39:22,924 Epoch  13: Total Training Recognition Loss 0.61  Total Training Translation Loss 3858.21 
2024-02-02 20:39:22,924 EPOCH 14
2024-02-02 20:39:25,564 [Epoch: 014 Step: 00000900] Batch Recognition Loss:   0.014015 => Gls Tokens per Sec:     1758 || Batch Translation Loss:  62.888702 => Txt Tokens per Sec:     5171 || Lr: 0.000100
2024-02-02 20:39:28,422 Epoch  14: Total Training Recognition Loss 0.73  Total Training Translation Loss 3704.99 
2024-02-02 20:39:28,423 EPOCH 15
2024-02-02 20:39:33,652 [Epoch: 015 Step: 00001000] Batch Recognition Loss:   0.010471 => Gls Tokens per Sec:     1881 || Batch Translation Loss:  57.831085 => Txt Tokens per Sec:     5208 || Lr: 0.000100
2024-02-02 20:39:34,062 Epoch  15: Total Training Recognition Loss 0.79  Total Training Translation Loss 3542.06 
2024-02-02 20:39:34,062 EPOCH 16
2024-02-02 20:39:39,021 Epoch  16: Total Training Recognition Loss 0.86  Total Training Translation Loss 3384.41 
2024-02-02 20:39:39,021 EPOCH 17
2024-02-02 20:39:41,259 [Epoch: 017 Step: 00001100] Batch Recognition Loss:   0.014944 => Gls Tokens per Sec:     1962 || Batch Translation Loss:  27.346943 => Txt Tokens per Sec:     5483 || Lr: 0.000100
2024-02-02 20:39:44,382 Epoch  17: Total Training Recognition Loss 0.95  Total Training Translation Loss 3236.05 
2024-02-02 20:39:44,383 EPOCH 18
2024-02-02 20:39:48,987 [Epoch: 018 Step: 00001200] Batch Recognition Loss:   0.015148 => Gls Tokens per Sec:     2101 || Batch Translation Loss:  49.439247 => Txt Tokens per Sec:     5767 || Lr: 0.000100
2024-02-02 20:39:49,605 Epoch  18: Total Training Recognition Loss 0.92  Total Training Translation Loss 3075.21 
2024-02-02 20:39:49,606 EPOCH 19
2024-02-02 20:39:54,980 Epoch  19: Total Training Recognition Loss 1.01  Total Training Translation Loss 2940.61 
2024-02-02 20:39:54,980 EPOCH 20
2024-02-02 20:39:56,872 [Epoch: 020 Step: 00001300] Batch Recognition Loss:   0.006821 => Gls Tokens per Sec:     2284 || Batch Translation Loss:  37.866283 => Txt Tokens per Sec:     6259 || Lr: 0.000100
2024-02-02 20:39:59,655 Epoch  20: Total Training Recognition Loss 0.98  Total Training Translation Loss 2768.82 
2024-02-02 20:39:59,655 EPOCH 21
2024-02-02 20:40:04,539 [Epoch: 021 Step: 00001400] Batch Recognition Loss:   0.010371 => Gls Tokens per Sec:     1948 || Batch Translation Loss:  36.026287 => Txt Tokens per Sec:     5348 || Lr: 0.000100
2024-02-02 20:40:05,118 Epoch  21: Total Training Recognition Loss 1.10  Total Training Translation Loss 2654.94 
2024-02-02 20:40:05,118 EPOCH 22
2024-02-02 20:40:10,485 Epoch  22: Total Training Recognition Loss 1.17  Total Training Translation Loss 2537.75 
2024-02-02 20:40:10,486 EPOCH 23
2024-02-02 20:40:12,525 [Epoch: 023 Step: 00001500] Batch Recognition Loss:   0.011304 => Gls Tokens per Sec:     2041 || Batch Translation Loss:  30.842436 => Txt Tokens per Sec:     5525 || Lr: 0.000100
2024-02-02 20:40:15,664 Epoch  23: Total Training Recognition Loss 1.07  Total Training Translation Loss 2380.60 
2024-02-02 20:40:15,664 EPOCH 24
2024-02-02 20:40:20,072 [Epoch: 024 Step: 00001600] Batch Recognition Loss:   0.016121 => Gls Tokens per Sec:     2121 || Batch Translation Loss:  43.913242 => Txt Tokens per Sec:     5900 || Lr: 0.000100
2024-02-02 20:40:20,841 Epoch  24: Total Training Recognition Loss 1.07  Total Training Translation Loss 2232.37 
2024-02-02 20:40:20,841 EPOCH 25
2024-02-02 20:40:25,983 Epoch  25: Total Training Recognition Loss 1.13  Total Training Translation Loss 2100.75 
2024-02-02 20:40:25,983 EPOCH 26
2024-02-02 20:40:27,981 [Epoch: 026 Step: 00001700] Batch Recognition Loss:   0.017810 => Gls Tokens per Sec:     2003 || Batch Translation Loss:  42.242226 => Txt Tokens per Sec:     5603 || Lr: 0.000100
2024-02-02 20:40:31,387 Epoch  26: Total Training Recognition Loss 1.06  Total Training Translation Loss 1969.47 
2024-02-02 20:40:31,388 EPOCH 27
2024-02-02 20:40:35,611 [Epoch: 027 Step: 00001800] Batch Recognition Loss:   0.018736 => Gls Tokens per Sec:     2176 || Batch Translation Loss:  35.857605 => Txt Tokens per Sec:     6051 || Lr: 0.000100
2024-02-02 20:40:36,352 Epoch  27: Total Training Recognition Loss 1.10  Total Training Translation Loss 1853.85 
2024-02-02 20:40:36,352 EPOCH 28
2024-02-02 20:40:41,813 Epoch  28: Total Training Recognition Loss 1.11  Total Training Translation Loss 1733.50 
2024-02-02 20:40:41,813 EPOCH 29
2024-02-02 20:40:43,776 [Epoch: 029 Step: 00001900] Batch Recognition Loss:   0.013843 => Gls Tokens per Sec:     1958 || Batch Translation Loss:  12.938041 => Txt Tokens per Sec:     5549 || Lr: 0.000100
2024-02-02 20:40:47,360 Epoch  29: Total Training Recognition Loss 1.06  Total Training Translation Loss 1616.86 
2024-02-02 20:40:47,361 EPOCH 30
2024-02-02 20:40:51,675 [Epoch: 030 Step: 00002000] Batch Recognition Loss:   0.015733 => Gls Tokens per Sec:     2094 || Batch Translation Loss:  16.055706 => Txt Tokens per Sec:     5742 || Lr: 0.000100
2024-02-02 20:41:00,680 Hooray! New best validation result [eval_metric]!
2024-02-02 20:41:00,681 Saving new checkpoint.
2024-02-02 20:41:00,931 Validation result at epoch  30, step     2000: duration: 9.2558s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.10733	Translation Loss: 63108.16406	PPL: 552.93311
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.82	(BLEU-1: 13.04,	BLEU-2: 4.31,	BLEU-3: 1.76,	BLEU-4: 0.82)
	CHRF 16.63	ROUGE 11.39
2024-02-02 20:41:00,932 Logging Recognition and Translation Outputs
2024-02-02 20:41:00,932 ========================================================================================================================
2024-02-02 20:41:00,933 Logging Sequence: 182_115.00
2024-02-02 20:41:00,933 	Gloss Reference :	A B+C+D+E
2024-02-02 20:41:00,933 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 20:41:00,933 	Gloss Alignment :	         
2024-02-02 20:41:00,933 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 20:41:00,935 	Text Reference  :	fans are unclear whether yuvraj will be    returning to    play    test match odi  or   in   t20 leagues from february 2022
2024-02-02 20:41:00,935 	Text Hypothesis :	**** *** ******* kohli   was    very proud by        these matches as   they  were very well and now     on   the      game
2024-02-02 20:41:00,936 	Text Alignment  :	D    D   D       S       S      S    S     S         S     S       S    S     S    S    S    S   S       S    S        S   
2024-02-02 20:41:00,936 ========================================================================================================================
2024-02-02 20:41:00,936 Logging Sequence: 140_120.00
2024-02-02 20:41:00,936 	Gloss Reference :	A B+C+D+E
2024-02-02 20:41:00,936 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 20:41:00,936 	Gloss Alignment :	         
2024-02-02 20:41:00,936 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 20:41:00,938 	Text Reference  :	but why so it is because pant is     a talented player and     it      will help  encouraging the youth of uttarakhand toward sports
2024-02-02 20:41:00,939 	Text Hypothesis :	*** *** ** he is because he   played a test     series against england and  india won         the ***** ** *********** world  cup   
2024-02-02 20:41:00,939 	Text Alignment  :	D   D   D  S             S    S        S        S      S       S       S    S     S               D     D  D           S      S     
2024-02-02 20:41:00,939 ========================================================================================================================
2024-02-02 20:41:00,939 Logging Sequence: 85_36.00
2024-02-02 20:41:00,939 	Gloss Reference :	A B+C+D+E
2024-02-02 20:41:00,939 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 20:41:00,939 	Gloss Alignment :	         
2024-02-02 20:41:00,939 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 20:41:00,940 	Text Reference  :	**** symonds has scored 2 centuries in 26 tests that   he played  for his    country
2024-02-02 20:41:00,940 	Text Hypothesis :	when he      has ****** * ********* ** ** ***** played 8  wickets and scored 3175   
2024-02-02 20:41:00,941 	Text Alignment  :	I    S           D      D D         D  D  D     S      S  S       S   S      S      
2024-02-02 20:41:00,941 ========================================================================================================================
2024-02-02 20:41:00,941 Logging Sequence: 164_100.00
2024-02-02 20:41:00,941 	Gloss Reference :	A B+C+D+E
2024-02-02 20:41:00,941 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 20:41:00,941 	Gloss Alignment :	         
2024-02-02 20:41:00,941 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 20:41:00,943 	Text Reference  :	the  tv rights  for broadcasting ipl matches in  india for the next 5 years went to star   india for          rs  23575 crore
2024-02-02 20:41:00,943 	Text Hypothesis :	this is because of  the          ipl ******* and now   won the **** * ***** **** ** rights of    broadcasting the world cup  
2024-02-02 20:41:00,944 	Text Alignment  :	S    S  S       S   S                D       S   S     S       D    D D     D    D  S      S     S            S   S     S    
2024-02-02 20:41:00,944 ========================================================================================================================
2024-02-02 20:41:00,944 Logging Sequence: 76_79.00
2024-02-02 20:41:00,944 	Gloss Reference :	A B+C+D+E
2024-02-02 20:41:00,944 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 20:41:00,944 	Gloss Alignment :	         
2024-02-02 20:41:00,944 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 20:41:00,945 	Text Reference  :	*** *** **** ** ****** ******* ****** *** ****** ******* ******* speaking to      ani     csk     ceo  kasi viswanathan said 
2024-02-02 20:41:00,946 	Text Hypothesis :	the ipl will be played between mumbai and mumbai indians indians indians  indians indians indians were held in          dubai
2024-02-02 20:41:00,946 	Text Alignment  :	I   I   I    I  I      I       I      I   I      I       I       S        S       S       S       S    S    S           S    
2024-02-02 20:41:00,946 ========================================================================================================================
2024-02-02 20:41:01,884 Epoch  30: Total Training Recognition Loss 1.06  Total Training Translation Loss 1521.35 
2024-02-02 20:41:01,884 EPOCH 31
2024-02-02 20:41:07,517 Epoch  31: Total Training Recognition Loss 1.08  Total Training Translation Loss 1416.65 
2024-02-02 20:41:07,518 EPOCH 32
2024-02-02 20:41:09,322 [Epoch: 032 Step: 00002100] Batch Recognition Loss:   0.012512 => Gls Tokens per Sec:     2041 || Batch Translation Loss:  12.424413 => Txt Tokens per Sec:     5684 || Lr: 0.000100
2024-02-02 20:41:12,437 Epoch  32: Total Training Recognition Loss 0.99  Total Training Translation Loss 1292.76 
2024-02-02 20:41:12,437 EPOCH 33
2024-02-02 20:41:17,025 [Epoch: 033 Step: 00002200] Batch Recognition Loss:   0.013944 => Gls Tokens per Sec:     1954 || Batch Translation Loss:  16.506128 => Txt Tokens per Sec:     5512 || Lr: 0.000100
2024-02-02 20:41:17,881 Epoch  33: Total Training Recognition Loss 0.97  Total Training Translation Loss 1209.46 
2024-02-02 20:41:17,881 EPOCH 34
2024-02-02 20:41:23,168 Epoch  34: Total Training Recognition Loss 0.97  Total Training Translation Loss 1114.27 
2024-02-02 20:41:23,169 EPOCH 35
2024-02-02 20:41:25,069 [Epoch: 035 Step: 00002300] Batch Recognition Loss:   0.017814 => Gls Tokens per Sec:     1805 || Batch Translation Loss:  19.528605 => Txt Tokens per Sec:     4977 || Lr: 0.000100
2024-02-02 20:41:28,519 Epoch  35: Total Training Recognition Loss 0.95  Total Training Translation Loss 1023.61 
2024-02-02 20:41:28,519 EPOCH 36
2024-02-02 20:41:32,562 [Epoch: 036 Step: 00002400] Batch Recognition Loss:   0.010742 => Gls Tokens per Sec:     2155 || Batch Translation Loss:   5.497292 => Txt Tokens per Sec:     5912 || Lr: 0.000100
2024-02-02 20:41:33,760 Epoch  36: Total Training Recognition Loss 0.85  Total Training Translation Loss 944.11 
2024-02-02 20:41:33,760 EPOCH 37
2024-02-02 20:41:38,811 Epoch  37: Total Training Recognition Loss 0.87  Total Training Translation Loss 852.16 
2024-02-02 20:41:38,812 EPOCH 38
2024-02-02 20:41:40,586 [Epoch: 038 Step: 00002500] Batch Recognition Loss:   0.006773 => Gls Tokens per Sec:     1894 || Batch Translation Loss:  10.575414 => Txt Tokens per Sec:     5581 || Lr: 0.000100
2024-02-02 20:41:44,211 Epoch  38: Total Training Recognition Loss 0.78  Total Training Translation Loss 774.47 
2024-02-02 20:41:44,212 EPOCH 39
2024-02-02 20:41:48,523 [Epoch: 039 Step: 00002600] Batch Recognition Loss:   0.009040 => Gls Tokens per Sec:     1984 || Batch Translation Loss:   8.785995 => Txt Tokens per Sec:     5654 || Lr: 0.000100
2024-02-02 20:41:49,559 Epoch  39: Total Training Recognition Loss 0.79  Total Training Translation Loss 707.48 
2024-02-02 20:41:49,560 EPOCH 40
2024-02-02 20:41:54,936 Epoch  40: Total Training Recognition Loss 0.70  Total Training Translation Loss 652.62 
2024-02-02 20:41:54,937 EPOCH 41
2024-02-02 20:41:56,130 [Epoch: 041 Step: 00002700] Batch Recognition Loss:   0.008793 => Gls Tokens per Sec:     2685 || Batch Translation Loss:  10.972318 => Txt Tokens per Sec:     7154 || Lr: 0.000100
2024-02-02 20:42:00,016 Epoch  41: Total Training Recognition Loss 0.70  Total Training Translation Loss 600.80 
2024-02-02 20:42:00,017 EPOCH 42
2024-02-02 20:42:04,256 [Epoch: 042 Step: 00002800] Batch Recognition Loss:   0.007025 => Gls Tokens per Sec:     2001 || Batch Translation Loss:   8.818874 => Txt Tokens per Sec:     5603 || Lr: 0.000100
2024-02-02 20:42:05,343 Epoch  42: Total Training Recognition Loss 0.66  Total Training Translation Loss 540.48 
2024-02-02 20:42:05,343 EPOCH 43
2024-02-02 20:42:10,753 Epoch  43: Total Training Recognition Loss 0.60  Total Training Translation Loss 485.23 
2024-02-02 20:42:10,753 EPOCH 44
2024-02-02 20:42:12,186 [Epoch: 044 Step: 00002900] Batch Recognition Loss:   0.005390 => Gls Tokens per Sec:     2061 || Batch Translation Loss:   3.065128 => Txt Tokens per Sec:     5704 || Lr: 0.000100
2024-02-02 20:42:16,299 Epoch  44: Total Training Recognition Loss 0.61  Total Training Translation Loss 436.62 
2024-02-02 20:42:16,299 EPOCH 45
2024-02-02 20:42:20,045 [Epoch: 045 Step: 00003000] Batch Recognition Loss:   0.006274 => Gls Tokens per Sec:     2199 || Batch Translation Loss:   7.073267 => Txt Tokens per Sec:     5949 || Lr: 0.000100
2024-02-02 20:42:21,318 Epoch  45: Total Training Recognition Loss 0.52  Total Training Translation Loss 397.28 
2024-02-02 20:42:21,318 EPOCH 46
2024-02-02 20:42:26,670 Epoch  46: Total Training Recognition Loss 0.50  Total Training Translation Loss 367.79 
2024-02-02 20:42:26,671 EPOCH 47
2024-02-02 20:42:27,739 [Epoch: 047 Step: 00003100] Batch Recognition Loss:   0.006830 => Gls Tokens per Sec:     2696 || Batch Translation Loss:   4.357892 => Txt Tokens per Sec:     6592 || Lr: 0.000100
2024-02-02 20:42:31,901 Epoch  47: Total Training Recognition Loss 0.51  Total Training Translation Loss 342.59 
2024-02-02 20:42:31,902 EPOCH 48
2024-02-02 20:42:35,989 [Epoch: 048 Step: 00003200] Batch Recognition Loss:   0.006620 => Gls Tokens per Sec:     1997 || Batch Translation Loss:   4.145802 => Txt Tokens per Sec:     5484 || Lr: 0.000100
2024-02-02 20:42:37,285 Epoch  48: Total Training Recognition Loss 0.43  Total Training Translation Loss 307.46 
2024-02-02 20:42:37,286 EPOCH 49
2024-02-02 20:42:42,488 Epoch  49: Total Training Recognition Loss 0.41  Total Training Translation Loss 276.34 
2024-02-02 20:42:42,489 EPOCH 50
2024-02-02 20:42:43,703 [Epoch: 050 Step: 00003300] Batch Recognition Loss:   0.004649 => Gls Tokens per Sec:     2242 || Batch Translation Loss:   4.814589 => Txt Tokens per Sec:     6229 || Lr: 0.000100
2024-02-02 20:42:47,512 Epoch  50: Total Training Recognition Loss 0.39  Total Training Translation Loss 257.12 
2024-02-02 20:42:47,512 EPOCH 51
2024-02-02 20:42:51,535 [Epoch: 051 Step: 00003400] Batch Recognition Loss:   0.005110 => Gls Tokens per Sec:     1967 || Batch Translation Loss:   4.113646 => Txt Tokens per Sec:     5562 || Lr: 0.000100
2024-02-02 20:42:52,896 Epoch  51: Total Training Recognition Loss 0.38  Total Training Translation Loss 236.06 
2024-02-02 20:42:52,896 EPOCH 52
2024-02-02 20:42:57,947 Epoch  52: Total Training Recognition Loss 0.36  Total Training Translation Loss 215.15 
2024-02-02 20:42:57,947 EPOCH 53
2024-02-02 20:42:59,408 [Epoch: 053 Step: 00003500] Batch Recognition Loss:   0.004698 => Gls Tokens per Sec:     1755 || Batch Translation Loss:   2.442250 => Txt Tokens per Sec:     5023 || Lr: 0.000100
2024-02-02 20:43:03,420 Epoch  53: Total Training Recognition Loss 0.31  Total Training Translation Loss 199.20 
2024-02-02 20:43:03,420 EPOCH 54
2024-02-02 20:43:06,911 [Epoch: 054 Step: 00003600] Batch Recognition Loss:   0.005418 => Gls Tokens per Sec:     2247 || Batch Translation Loss:   2.762872 => Txt Tokens per Sec:     6157 || Lr: 0.000100
2024-02-02 20:43:08,508 Epoch  54: Total Training Recognition Loss 0.31  Total Training Translation Loss 181.46 
2024-02-02 20:43:08,509 EPOCH 55
2024-02-02 20:43:13,815 Epoch  55: Total Training Recognition Loss 0.30  Total Training Translation Loss 169.90 
2024-02-02 20:43:13,815 EPOCH 56
2024-02-02 20:43:15,079 [Epoch: 056 Step: 00003700] Batch Recognition Loss:   0.002555 => Gls Tokens per Sec:     1828 || Batch Translation Loss:   1.862972 => Txt Tokens per Sec:     5439 || Lr: 0.000100
2024-02-02 20:43:19,121 Epoch  56: Total Training Recognition Loss 0.28  Total Training Translation Loss 160.03 
2024-02-02 20:43:19,121 EPOCH 57
2024-02-02 20:43:22,894 [Epoch: 057 Step: 00003800] Batch Recognition Loss:   0.003980 => Gls Tokens per Sec:     2036 || Batch Translation Loss:   3.076225 => Txt Tokens per Sec:     5655 || Lr: 0.000100
2024-02-02 20:43:24,293 Epoch  57: Total Training Recognition Loss 0.26  Total Training Translation Loss 150.00 
2024-02-02 20:43:24,293 EPOCH 58
2024-02-02 20:43:29,645 Epoch  58: Total Training Recognition Loss 0.24  Total Training Translation Loss 138.02 
2024-02-02 20:43:29,645 EPOCH 59
2024-02-02 20:43:30,608 [Epoch: 059 Step: 00003900] Batch Recognition Loss:   0.004688 => Gls Tokens per Sec:     2328 || Batch Translation Loss:   2.654354 => Txt Tokens per Sec:     6386 || Lr: 0.000100
2024-02-02 20:43:34,654 Epoch  59: Total Training Recognition Loss 0.23  Total Training Translation Loss 132.18 
2024-02-02 20:43:34,654 EPOCH 60
2024-02-02 20:43:38,454 [Epoch: 060 Step: 00004000] Batch Recognition Loss:   0.002119 => Gls Tokens per Sec:     1956 || Batch Translation Loss:   1.915775 => Txt Tokens per Sec:     5420 || Lr: 0.000100
2024-02-02 20:43:46,422 Validation result at epoch  60, step     4000: duration: 7.9665s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.01232	Translation Loss: 73582.21094	PPL: 1577.15149
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.58	(BLEU-1: 12.03,	BLEU-2: 3.79,	BLEU-3: 1.37,	BLEU-4: 0.58)
	CHRF 16.80	ROUGE 10.37
2024-02-02 20:43:46,423 Logging Recognition and Translation Outputs
2024-02-02 20:43:46,423 ========================================================================================================================
2024-02-02 20:43:46,423 Logging Sequence: 133_173.00
2024-02-02 20:43:46,423 	Gloss Reference :	A B+C+D+E
2024-02-02 20:43:46,423 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 20:43:46,424 	Gloss Alignment :	         
2024-02-02 20:43:46,424 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 20:43:46,424 	Text Reference  :	according to sources the leaders of the two countries are set to join the commentary panel   as  well   
2024-02-02 20:43:46,425 	Text Hypothesis :	********* ** ******* *** ******* ** *** *** ********* *** *** ** **** pm  modi       tweeted his arrival
2024-02-02 20:43:46,425 	Text Alignment  :	D         D  D       D   D       D  D   D   D         D   D   D  D    S   S          S       S   S      
2024-02-02 20:43:46,425 ========================================================================================================================
2024-02-02 20:43:46,425 Logging Sequence: 83_33.00
2024-02-02 20:43:46,425 	Gloss Reference :	A B+C+D+E
2024-02-02 20:43:46,425 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 20:43:46,425 	Gloss Alignment :	         
2024-02-02 20:43:46,425 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 20:43:46,427 	Text Reference  :	*** ****** ******* a   football **** ** ***** match lasts for two   equal halves of  45      minutes
2024-02-02 20:43:46,427 	Text Hypothesis :	the camera records the football team is awake and   he    is  awake and   at     the denmark team   
2024-02-02 20:43:46,427 	Text Alignment  :	I   I      I       S            I    I  I     S     S     S   S     S     S      S   S       S      
2024-02-02 20:43:46,427 ========================================================================================================================
2024-02-02 20:43:46,427 Logging Sequence: 68_147.00
2024-02-02 20:43:46,427 	Gloss Reference :	A B+C+D+E
2024-02-02 20:43:46,427 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 20:43:46,428 	Gloss Alignment :	         
2024-02-02 20:43:46,428 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 20:43:46,429 	Text Reference  :	remember the    2007 t20 world cup  amid   a     lot  of     sledging by   english players
2024-02-02 20:43:46,429 	Text Hypothesis :	bumrah   scored runs in  an    over stuart broad away stuart broad    gave away    over   
2024-02-02 20:43:46,429 	Text Alignment  :	S        S      S    S   S     S    S      S     S    S      S        S    S       S      
2024-02-02 20:43:46,429 ========================================================================================================================
2024-02-02 20:43:46,429 Logging Sequence: 165_8.00
2024-02-02 20:43:46,430 	Gloss Reference :	A B+C+D+E
2024-02-02 20:43:46,430 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 20:43:46,430 	Gloss Alignment :	         
2024-02-02 20:43:46,430 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 20:43:46,430 	Text Reference  :	however many don't believe in it  it   varies among   people
2024-02-02 20:43:46,431 	Text Hypothesis :	******* **** ***** ******* he has been an     amazing player
2024-02-02 20:43:46,431 	Text Alignment  :	D       D    D     D       S  S   S    S      S       S     
2024-02-02 20:43:46,431 ========================================================================================================================
2024-02-02 20:43:46,431 Logging Sequence: 119_71.00
2024-02-02 20:43:46,431 	Gloss Reference :	A B+C+D+E
2024-02-02 20:43:46,431 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 20:43:46,431 	Gloss Alignment :	         
2024-02-02 20:43:46,431 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 20:43:46,433 	Text Reference  :	the special gold devices have each player' names and    jersey numbers next to   the camera
2024-02-02 20:43:46,433 	Text Hypothesis :	*** he      is   also    a    gift that    the   team's phones to      be   held in  qartar
2024-02-02 20:43:46,433 	Text Alignment  :	D   S       S    S       S    S    S       S     S      S      S       S    S    S   S     
2024-02-02 20:43:46,433 ========================================================================================================================
2024-02-02 20:43:48,192 Epoch  60: Total Training Recognition Loss 0.23  Total Training Translation Loss 127.12 
2024-02-02 20:43:48,192 EPOCH 61
2024-02-02 20:43:53,461 Epoch  61: Total Training Recognition Loss 0.24  Total Training Translation Loss 116.66 
2024-02-02 20:43:53,461 EPOCH 62
2024-02-02 20:43:54,674 [Epoch: 062 Step: 00004100] Batch Recognition Loss:   0.002819 => Gls Tokens per Sec:     1717 || Batch Translation Loss:   2.198868 => Txt Tokens per Sec:     4693 || Lr: 0.000100
2024-02-02 20:43:58,963 Epoch  62: Total Training Recognition Loss 0.22  Total Training Translation Loss 111.44 
2024-02-02 20:43:58,963 EPOCH 63
2024-02-02 20:44:01,936 [Epoch: 063 Step: 00004200] Batch Recognition Loss:   0.002143 => Gls Tokens per Sec:     2477 || Batch Translation Loss:   1.204916 => Txt Tokens per Sec:     6766 || Lr: 0.000100
2024-02-02 20:44:03,964 Epoch  63: Total Training Recognition Loss 0.20  Total Training Translation Loss 103.00 
2024-02-02 20:44:03,965 EPOCH 64
2024-02-02 20:44:09,374 Epoch  64: Total Training Recognition Loss 0.19  Total Training Translation Loss 96.59 
2024-02-02 20:44:09,375 EPOCH 65
2024-02-02 20:44:10,034 [Epoch: 065 Step: 00004300] Batch Recognition Loss:   0.002497 => Gls Tokens per Sec:     2912 || Batch Translation Loss:   1.050015 => Txt Tokens per Sec:     7234 || Lr: 0.000100
2024-02-02 20:44:14,615 Epoch  65: Total Training Recognition Loss 0.19  Total Training Translation Loss 99.90 
2024-02-02 20:44:14,616 EPOCH 66
2024-02-02 20:44:18,264 [Epoch: 066 Step: 00004400] Batch Recognition Loss:   0.002459 => Gls Tokens per Sec:     1950 || Batch Translation Loss:   1.550283 => Txt Tokens per Sec:     5550 || Lr: 0.000100
2024-02-02 20:44:19,696 Epoch  66: Total Training Recognition Loss 0.19  Total Training Translation Loss 94.98 
2024-02-02 20:44:19,696 EPOCH 67
2024-02-02 20:44:25,035 Epoch  67: Total Training Recognition Loss 0.17  Total Training Translation Loss 88.63 
2024-02-02 20:44:25,035 EPOCH 68
2024-02-02 20:44:25,830 [Epoch: 068 Step: 00004500] Batch Recognition Loss:   0.003682 => Gls Tokens per Sec:     2103 || Batch Translation Loss:   1.079048 => Txt Tokens per Sec:     5746 || Lr: 0.000100
2024-02-02 20:44:29,932 Epoch  68: Total Training Recognition Loss 0.19  Total Training Translation Loss 90.96 
2024-02-02 20:44:29,932 EPOCH 69
2024-02-02 20:44:33,533 [Epoch: 069 Step: 00004600] Batch Recognition Loss:   0.001711 => Gls Tokens per Sec:     1931 || Batch Translation Loss:   1.015203 => Txt Tokens per Sec:     5308 || Lr: 0.000100
2024-02-02 20:44:35,531 Epoch  69: Total Training Recognition Loss 0.17  Total Training Translation Loss 81.50 
2024-02-02 20:44:35,531 EPOCH 70
2024-02-02 20:44:40,609 Epoch  70: Total Training Recognition Loss 0.16  Total Training Translation Loss 79.16 
2024-02-02 20:44:40,610 EPOCH 71
2024-02-02 20:44:41,411 [Epoch: 071 Step: 00004700] Batch Recognition Loss:   0.002125 => Gls Tokens per Sec:     1999 || Batch Translation Loss:   1.198389 => Txt Tokens per Sec:     5046 || Lr: 0.000100
2024-02-02 20:44:45,908 Epoch  71: Total Training Recognition Loss 0.17  Total Training Translation Loss 73.42 
2024-02-02 20:44:45,908 EPOCH 72
2024-02-02 20:44:49,297 [Epoch: 072 Step: 00004800] Batch Recognition Loss:   0.003334 => Gls Tokens per Sec:     2004 || Batch Translation Loss:   1.623815 => Txt Tokens per Sec:     5700 || Lr: 0.000100
2024-02-02 20:44:51,042 Epoch  72: Total Training Recognition Loss 0.15  Total Training Translation Loss 70.74 
2024-02-02 20:44:51,043 EPOCH 73
2024-02-02 20:44:56,305 Epoch  73: Total Training Recognition Loss 0.15  Total Training Translation Loss 62.45 
2024-02-02 20:44:56,306 EPOCH 74
2024-02-02 20:44:57,093 [Epoch: 074 Step: 00004900] Batch Recognition Loss:   0.003875 => Gls Tokens per Sec:     1832 || Batch Translation Loss:   1.419467 => Txt Tokens per Sec:     5385 || Lr: 0.000100
2024-02-02 20:45:01,802 Epoch  74: Total Training Recognition Loss 0.13  Total Training Translation Loss 59.04 
2024-02-02 20:45:01,802 EPOCH 75
2024-02-02 20:45:04,738 [Epoch: 075 Step: 00005000] Batch Recognition Loss:   0.001504 => Gls Tokens per Sec:     2290 || Batch Translation Loss:   0.820179 => Txt Tokens per Sec:     6208 || Lr: 0.000100
2024-02-02 20:45:06,678 Epoch  75: Total Training Recognition Loss 0.12  Total Training Translation Loss 59.29 
2024-02-02 20:45:06,678 EPOCH 76
2024-02-02 20:45:11,649 Epoch  76: Total Training Recognition Loss 0.12  Total Training Translation Loss 57.58 
2024-02-02 20:45:11,649 EPOCH 77
2024-02-02 20:45:12,484 [Epoch: 077 Step: 00005100] Batch Recognition Loss:   0.001947 => Gls Tokens per Sec:     1427 || Batch Translation Loss:   1.346205 => Txt Tokens per Sec:     4093 || Lr: 0.000100
2024-02-02 20:45:17,056 Epoch  77: Total Training Recognition Loss 0.13  Total Training Translation Loss 62.44 
2024-02-02 20:45:17,056 EPOCH 78
2024-02-02 20:45:19,974 [Epoch: 078 Step: 00005200] Batch Recognition Loss:   0.000690 => Gls Tokens per Sec:     2218 || Batch Translation Loss:   0.897808 => Txt Tokens per Sec:     5945 || Lr: 0.000100
2024-02-02 20:45:22,284 Epoch  78: Total Training Recognition Loss 0.10  Total Training Translation Loss 60.61 
2024-02-02 20:45:22,284 EPOCH 79
2024-02-02 20:45:27,777 Epoch  79: Total Training Recognition Loss 0.12  Total Training Translation Loss 59.37 
2024-02-02 20:45:27,778 EPOCH 80
2024-02-02 20:45:28,364 [Epoch: 080 Step: 00005300] Batch Recognition Loss:   0.001717 => Gls Tokens per Sec:     1911 || Batch Translation Loss:   1.323673 => Txt Tokens per Sec:     5189 || Lr: 0.000100
2024-02-02 20:45:33,023 Epoch  80: Total Training Recognition Loss 0.12  Total Training Translation Loss 57.59 
2024-02-02 20:45:33,023 EPOCH 81
2024-02-02 20:45:36,037 [Epoch: 081 Step: 00005400] Batch Recognition Loss:   0.001578 => Gls Tokens per Sec:     2124 || Batch Translation Loss:   0.775564 => Txt Tokens per Sec:     5898 || Lr: 0.000100
2024-02-02 20:45:38,440 Epoch  81: Total Training Recognition Loss 0.11  Total Training Translation Loss 52.75 
2024-02-02 20:45:38,441 EPOCH 82
2024-02-02 20:45:43,876 Epoch  82: Total Training Recognition Loss 0.11  Total Training Translation Loss 56.07 
2024-02-02 20:45:43,877 EPOCH 83
2024-02-02 20:45:44,494 [Epoch: 083 Step: 00005500] Batch Recognition Loss:   0.001965 => Gls Tokens per Sec:     1556 || Batch Translation Loss:   0.897299 => Txt Tokens per Sec:     4535 || Lr: 0.000100
2024-02-02 20:45:49,400 Epoch  83: Total Training Recognition Loss 0.10  Total Training Translation Loss 52.57 
2024-02-02 20:45:49,400 EPOCH 84
2024-02-02 20:45:52,262 [Epoch: 084 Step: 00005600] Batch Recognition Loss:   0.001008 => Gls Tokens per Sec:     2150 || Batch Translation Loss:   0.524964 => Txt Tokens per Sec:     6252 || Lr: 0.000100
2024-02-02 20:45:54,445 Epoch  84: Total Training Recognition Loss 0.10  Total Training Translation Loss 49.95 
2024-02-02 20:45:54,446 EPOCH 85
2024-02-02 20:45:59,550 Epoch  85: Total Training Recognition Loss 0.12  Total Training Translation Loss 51.18 
2024-02-02 20:45:59,550 EPOCH 86
2024-02-02 20:45:59,842 [Epoch: 086 Step: 00005700] Batch Recognition Loss:   0.001303 => Gls Tokens per Sec:     2759 || Batch Translation Loss:   0.877711 => Txt Tokens per Sec:     7462 || Lr: 0.000100
2024-02-02 20:46:04,966 Epoch  86: Total Training Recognition Loss 0.09  Total Training Translation Loss 48.35 
2024-02-02 20:46:04,966 EPOCH 87
2024-02-02 20:46:07,761 [Epoch: 087 Step: 00005800] Batch Recognition Loss:   0.002063 => Gls Tokens per Sec:     2144 || Batch Translation Loss:   0.497431 => Txt Tokens per Sec:     5934 || Lr: 0.000100
2024-02-02 20:46:09,948 Epoch  87: Total Training Recognition Loss 0.10  Total Training Translation Loss 41.43 
2024-02-02 20:46:09,949 EPOCH 88
2024-02-02 20:46:15,487 Epoch  88: Total Training Recognition Loss 0.09  Total Training Translation Loss 42.75 
2024-02-02 20:46:15,487 EPOCH 89
2024-02-02 20:46:15,766 [Epoch: 089 Step: 00005900] Batch Recognition Loss:   0.002038 => Gls Tokens per Sec:     2294 || Batch Translation Loss:   0.370352 => Txt Tokens per Sec:     6358 || Lr: 0.000100
2024-02-02 20:46:20,993 Epoch  89: Total Training Recognition Loss 0.09  Total Training Translation Loss 40.96 
2024-02-02 20:46:20,994 EPOCH 90
2024-02-02 20:46:23,752 [Epoch: 090 Step: 00006000] Batch Recognition Loss:   0.001244 => Gls Tokens per Sec:     2115 || Batch Translation Loss:   0.504546 => Txt Tokens per Sec:     5777 || Lr: 0.000100
2024-02-02 20:46:31,992 Validation result at epoch  90, step     6000: duration: 8.2401s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00354	Translation Loss: 81867.34375	PPL: 3613.63525
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.58	(BLEU-1: 10.41,	BLEU-2: 3.42,	BLEU-3: 1.25,	BLEU-4: 0.58)
	CHRF 16.42	ROUGE 9.41
2024-02-02 20:46:31,993 Logging Recognition and Translation Outputs
2024-02-02 20:46:31,993 ========================================================================================================================
2024-02-02 20:46:31,993 Logging Sequence: 89_111.00
2024-02-02 20:46:31,993 	Gloss Reference :	A B+C+D+E
2024-02-02 20:46:31,993 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 20:46:31,994 	Gloss Alignment :	         
2024-02-02 20:46:31,994 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 20:46:31,994 	Text Reference  :	*** ******* ******** * however  selectors never    selected me   for the    team 
2024-02-02 20:46:31,995 	Text Hypothesis :	the spinner received a plethora of        messages and      went on  social media
2024-02-02 20:46:31,995 	Text Alignment  :	I   I       I        I S        S         S        S        S    S   S      S    
2024-02-02 20:46:31,995 ========================================================================================================================
2024-02-02 20:46:31,995 Logging Sequence: 137_23.00
2024-02-02 20:46:31,995 	Gloss Reference :	A B+C+D+E
2024-02-02 20:46:31,995 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 20:46:31,995 	Gloss Alignment :	         
2024-02-02 20:46:31,995 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 20:46:31,997 	Text Reference  :	fan from around  the ***** world are in      qatar for the **** **** fifa    world cup 
2024-02-02 20:46:31,997 	Text Hypothesis :	*** **** however the match could not allowed to    all the fans were shocked by    this
2024-02-02 20:46:31,997 	Text Alignment  :	D   D    S           I     S     S   S       S     S       I    I    S       S     S   
2024-02-02 20:46:31,997 ========================================================================================================================
2024-02-02 20:46:31,997 Logging Sequence: 128_145.00
2024-02-02 20:46:31,997 	Gloss Reference :	A B+C+D+E
2024-02-02 20:46:31,997 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 20:46:31,997 	Gloss Alignment :	         
2024-02-02 20:46:31,998 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 20:46:31,998 	Text Reference  :	**** ** *** ** icc also uploaded a   video of   the same
2024-02-02 20:46:31,998 	Text Hypothesis :	this is why he did not  get      her and   then hit him 
2024-02-02 20:46:31,999 	Text Alignment  :	I    I  I   I  S   S    S        S   S     S    S   S   
2024-02-02 20:46:31,999 ========================================================================================================================
2024-02-02 20:46:31,999 Logging Sequence: 165_192.00
2024-02-02 20:46:31,999 	Gloss Reference :	A B+C+D+E
2024-02-02 20:46:31,999 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 20:46:31,999 	Gloss Alignment :	         
2024-02-02 20:46:31,999 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 20:46:32,000 	Text Reference  :	******* 3  ravichandran ashwin believes  that his bag ** *** is     lucky   
2024-02-02 20:46:32,000 	Text Hypothesis :	however he had          been   postponed and  his bag to the indian athletes
2024-02-02 20:46:32,000 	Text Alignment  :	I       S  S            S      S         S            I  I   S      S       
2024-02-02 20:46:32,000 ========================================================================================================================
2024-02-02 20:46:32,001 Logging Sequence: 180_494.00
2024-02-02 20:46:32,001 	Gloss Reference :	A B+C+D+E
2024-02-02 20:46:32,001 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 20:46:32,001 	Gloss Alignment :	         
2024-02-02 20:46:32,001 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 20:46:32,002 	Text Reference  :	the women wrestlers spoke angrily against the police and     the  controversy in  front  of       the    media 
2024-02-02 20:46:32,002 	Text Hypothesis :	*** ***** ********* ***** ******* however a   police officer said that        the sports minister anurag thakur
2024-02-02 20:46:32,002 	Text Alignment  :	D   D     D         D     D       S       S          S       S    S           S   S      S        S      S     
2024-02-02 20:46:32,003 ========================================================================================================================
2024-02-02 20:46:34,570 Epoch  90: Total Training Recognition Loss 0.07  Total Training Translation Loss 38.22 
2024-02-02 20:46:34,571 EPOCH 91
2024-02-02 20:46:40,062 Epoch  91: Total Training Recognition Loss 0.08  Total Training Translation Loss 39.62 
2024-02-02 20:46:40,063 EPOCH 92
2024-02-02 20:46:40,293 [Epoch: 092 Step: 00006100] Batch Recognition Loss:   0.001672 => Gls Tokens per Sec:     2096 || Batch Translation Loss:   0.499406 => Txt Tokens per Sec:     6179 || Lr: 0.000100
2024-02-02 20:46:45,195 Epoch  92: Total Training Recognition Loss 0.09  Total Training Translation Loss 39.94 
2024-02-02 20:46:45,196 EPOCH 93
2024-02-02 20:46:47,988 [Epoch: 093 Step: 00006200] Batch Recognition Loss:   0.000956 => Gls Tokens per Sec:     2063 || Batch Translation Loss:   0.529095 => Txt Tokens per Sec:     5817 || Lr: 0.000100
2024-02-02 20:46:50,485 Epoch  93: Total Training Recognition Loss 0.08  Total Training Translation Loss 38.56 
2024-02-02 20:46:50,486 EPOCH 94
2024-02-02 20:46:55,513 Epoch  94: Total Training Recognition Loss 0.08  Total Training Translation Loss 34.29 
2024-02-02 20:46:55,513 EPOCH 95
2024-02-02 20:46:55,624 [Epoch: 095 Step: 00006300] Batch Recognition Loss:   0.000863 => Gls Tokens per Sec:     2909 || Batch Translation Loss:   0.306947 => Txt Tokens per Sec:     6264 || Lr: 0.000100
2024-02-02 20:47:01,070 Epoch  95: Total Training Recognition Loss 0.09  Total Training Translation Loss 32.40 
2024-02-02 20:47:01,070 EPOCH 96
2024-02-02 20:47:03,731 [Epoch: 096 Step: 00006400] Batch Recognition Loss:   0.000858 => Gls Tokens per Sec:     2072 || Batch Translation Loss:   0.403916 => Txt Tokens per Sec:     5863 || Lr: 0.000100
2024-02-02 20:47:05,980 Epoch  96: Total Training Recognition Loss 0.07  Total Training Translation Loss 33.56 
2024-02-02 20:47:05,981 EPOCH 97
2024-02-02 20:47:11,145 Epoch  97: Total Training Recognition Loss 0.07  Total Training Translation Loss 33.78 
2024-02-02 20:47:11,145 EPOCH 98
2024-02-02 20:47:11,206 [Epoch: 098 Step: 00006500] Batch Recognition Loss:   0.000800 => Gls Tokens per Sec:     2667 || Batch Translation Loss:   0.324441 => Txt Tokens per Sec:     5884 || Lr: 0.000100
2024-02-02 20:47:16,449 Epoch  98: Total Training Recognition Loss 0.07  Total Training Translation Loss 36.45 
2024-02-02 20:47:16,450 EPOCH 99
2024-02-02 20:47:19,214 [Epoch: 099 Step: 00006600] Batch Recognition Loss:   0.000654 => Gls Tokens per Sec:     1968 || Batch Translation Loss:   1.096990 => Txt Tokens per Sec:     5383 || Lr: 0.000100
2024-02-02 20:47:21,672 Epoch  99: Total Training Recognition Loss 0.07  Total Training Translation Loss 35.87 
2024-02-02 20:47:21,672 EPOCH 100
2024-02-02 20:47:27,083 [Epoch: 100 Step: 00006700] Batch Recognition Loss:   0.000787 => Gls Tokens per Sec:     1965 || Batch Translation Loss:   1.180939 => Txt Tokens per Sec:     5455 || Lr: 0.000100
2024-02-02 20:47:27,084 Epoch 100: Total Training Recognition Loss 0.08  Total Training Translation Loss 39.18 
2024-02-02 20:47:27,084 EPOCH 101
2024-02-02 20:47:32,195 Epoch 101: Total Training Recognition Loss 0.07  Total Training Translation Loss 38.09 
2024-02-02 20:47:32,196 EPOCH 102
2024-02-02 20:47:34,815 [Epoch: 102 Step: 00006800] Batch Recognition Loss:   0.000707 => Gls Tokens per Sec:     2018 || Batch Translation Loss:   0.590640 => Txt Tokens per Sec:     5596 || Lr: 0.000100
2024-02-02 20:47:37,554 Epoch 102: Total Training Recognition Loss 0.10  Total Training Translation Loss 33.65 
2024-02-02 20:47:37,554 EPOCH 103
2024-02-02 20:47:42,144 [Epoch: 103 Step: 00006900] Batch Recognition Loss:   0.001334 => Gls Tokens per Sec:     2282 || Batch Translation Loss:   0.377379 => Txt Tokens per Sec:     6343 || Lr: 0.000100
2024-02-02 20:47:42,198 Epoch 103: Total Training Recognition Loss 0.06  Total Training Translation Loss 29.61 
2024-02-02 20:47:42,198 EPOCH 104
2024-02-02 20:47:47,864 Epoch 104: Total Training Recognition Loss 0.06  Total Training Translation Loss 30.27 
2024-02-02 20:47:47,864 EPOCH 105
2024-02-02 20:47:50,592 [Epoch: 105 Step: 00007000] Batch Recognition Loss:   0.000755 => Gls Tokens per Sec:     1845 || Batch Translation Loss:   0.497617 => Txt Tokens per Sec:     5254 || Lr: 0.000100
2024-02-02 20:47:53,334 Epoch 105: Total Training Recognition Loss 0.06  Total Training Translation Loss 29.51 
2024-02-02 20:47:53,335 EPOCH 106
2024-02-02 20:47:58,178 [Epoch: 106 Step: 00007100] Batch Recognition Loss:   0.000598 => Gls Tokens per Sec:     2129 || Batch Translation Loss:   0.256547 => Txt Tokens per Sec:     5899 || Lr: 0.000100
2024-02-02 20:47:58,387 Epoch 106: Total Training Recognition Loss 0.07  Total Training Translation Loss 26.92 
2024-02-02 20:47:58,387 EPOCH 107
2024-02-02 20:48:03,869 Epoch 107: Total Training Recognition Loss 0.05  Total Training Translation Loss 25.87 
2024-02-02 20:48:03,870 EPOCH 108
2024-02-02 20:48:05,894 [Epoch: 108 Step: 00007200] Batch Recognition Loss:   0.000730 => Gls Tokens per Sec:     2451 || Batch Translation Loss:   0.416116 => Txt Tokens per Sec:     6498 || Lr: 0.000100
2024-02-02 20:48:09,089 Epoch 108: Total Training Recognition Loss 0.06  Total Training Translation Loss 26.87 
2024-02-02 20:48:09,089 EPOCH 109
2024-02-02 20:48:14,296 [Epoch: 109 Step: 00007300] Batch Recognition Loss:   0.001418 => Gls Tokens per Sec:     1950 || Batch Translation Loss:   0.215946 => Txt Tokens per Sec:     5439 || Lr: 0.000100
2024-02-02 20:48:14,494 Epoch 109: Total Training Recognition Loss 0.06  Total Training Translation Loss 26.74 
2024-02-02 20:48:14,494 EPOCH 110
2024-02-02 20:48:19,095 Epoch 110: Total Training Recognition Loss 0.06  Total Training Translation Loss 30.31 
2024-02-02 20:48:19,096 EPOCH 111
2024-02-02 20:48:21,330 [Epoch: 111 Step: 00007400] Batch Recognition Loss:   0.000406 => Gls Tokens per Sec:     2109 || Batch Translation Loss:   0.278599 => Txt Tokens per Sec:     6123 || Lr: 0.000100
2024-02-02 20:48:23,843 Epoch 111: Total Training Recognition Loss 0.06  Total Training Translation Loss 28.25 
2024-02-02 20:48:23,843 EPOCH 112
2024-02-02 20:48:28,329 [Epoch: 112 Step: 00007500] Batch Recognition Loss:   0.000823 => Gls Tokens per Sec:     2227 || Batch Translation Loss:   0.874455 => Txt Tokens per Sec:     6268 || Lr: 0.000100
2024-02-02 20:48:28,530 Epoch 112: Total Training Recognition Loss 0.06  Total Training Translation Loss 27.22 
2024-02-02 20:48:28,530 EPOCH 113
2024-02-02 20:48:34,144 Epoch 113: Total Training Recognition Loss 0.05  Total Training Translation Loss 26.02 
2024-02-02 20:48:34,145 EPOCH 114
2024-02-02 20:48:36,535 [Epoch: 114 Step: 00007600] Batch Recognition Loss:   0.000614 => Gls Tokens per Sec:     1942 || Batch Translation Loss:   0.653700 => Txt Tokens per Sec:     5282 || Lr: 0.000100
2024-02-02 20:48:39,667 Epoch 114: Total Training Recognition Loss 0.05  Total Training Translation Loss 29.35 
2024-02-02 20:48:39,667 EPOCH 115
2024-02-02 20:48:44,074 [Epoch: 115 Step: 00007700] Batch Recognition Loss:   0.000518 => Gls Tokens per Sec:     2232 || Batch Translation Loss:   0.523230 => Txt Tokens per Sec:     6146 || Lr: 0.000100
2024-02-02 20:48:44,457 Epoch 115: Total Training Recognition Loss 0.05  Total Training Translation Loss 28.70 
2024-02-02 20:48:44,458 EPOCH 116
2024-02-02 20:48:49,729 Epoch 116: Total Training Recognition Loss 0.06  Total Training Translation Loss 26.34 
2024-02-02 20:48:49,730 EPOCH 117
2024-02-02 20:48:52,081 [Epoch: 117 Step: 00007800] Batch Recognition Loss:   0.000733 => Gls Tokens per Sec:     1868 || Batch Translation Loss:   0.306692 => Txt Tokens per Sec:     5229 || Lr: 0.000100
2024-02-02 20:48:55,214 Epoch 117: Total Training Recognition Loss 0.05  Total Training Translation Loss 21.69 
2024-02-02 20:48:55,215 EPOCH 118
2024-02-02 20:49:00,358 [Epoch: 118 Step: 00007900] Batch Recognition Loss:   0.000277 => Gls Tokens per Sec:     1881 || Batch Translation Loss:   0.245410 => Txt Tokens per Sec:     5224 || Lr: 0.000100
2024-02-02 20:49:00,840 Epoch 118: Total Training Recognition Loss 0.04  Total Training Translation Loss 19.48 
2024-02-02 20:49:00,840 EPOCH 119
2024-02-02 20:49:06,069 Epoch 119: Total Training Recognition Loss 0.05  Total Training Translation Loss 20.73 
2024-02-02 20:49:06,069 EPOCH 120
2024-02-02 20:49:07,904 [Epoch: 120 Step: 00008000] Batch Recognition Loss:   0.000355 => Gls Tokens per Sec:     2308 || Batch Translation Loss:   0.282195 => Txt Tokens per Sec:     5953 || Lr: 0.000100
2024-02-02 20:49:16,079 Validation result at epoch 120, step     8000: duration: 8.1750s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00203	Translation Loss: 88007.92969	PPL: 6680.56738
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.36	(BLEU-1: 11.28,	BLEU-2: 3.12,	BLEU-3: 1.04,	BLEU-4: 0.36)
	CHRF 16.80	ROUGE 9.61
2024-02-02 20:49:16,080 Logging Recognition and Translation Outputs
2024-02-02 20:49:16,080 ========================================================================================================================
2024-02-02 20:49:16,081 Logging Sequence: 88_57.00
2024-02-02 20:49:16,081 	Gloss Reference :	A B+C+D+E
2024-02-02 20:49:16,081 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 20:49:16,081 	Gloss Alignment :	         
2024-02-02 20:49:16,081 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 20:49:16,083 	Text Reference  :	which stated messi  we're waiting    for you    to      come  here   you      will      be     finished    when you   come   
2024-02-02 20:49:16,083 	Text Hypothesis :	***** people failed to    understand the reason however mayor javkin attacked argentina police authorities over their failure
2024-02-02 20:49:16,083 	Text Alignment  :	D     S      S      S     S          S   S      S       S     S      S        S         S      S           S    S     S      
2024-02-02 20:49:16,083 ========================================================================================================================
2024-02-02 20:49:16,083 Logging Sequence: 171_142.00
2024-02-02 20:49:16,083 	Gloss Reference :	A B+C+D+E
2024-02-02 20:49:16,084 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 20:49:16,084 	Gloss Alignment :	         
2024-02-02 20:49:16,084 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 20:49:16,085 	Text Reference  :	******* this decision  on      dhoni    made a ***** **** significant impact as   pathirana claimed  two  tough wickets
2024-02-02 20:49:16,085 	Text Hypothesis :	however the  wrestlers started speaking with a great time that        he     will be        refunded with 5     years  
2024-02-02 20:49:16,085 	Text Alignment  :	I       S    S         S       S        S      I     I    S           S      S    S         S        S    S     S      
2024-02-02 20:49:16,086 ========================================================================================================================
2024-02-02 20:49:16,086 Logging Sequence: 125_207.00
2024-02-02 20:49:16,086 	Gloss Reference :	A B+C+D+E
2024-02-02 20:49:16,086 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 20:49:16,086 	Gloss Alignment :	         
2024-02-02 20:49:16,086 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 20:49:16,087 	Text Reference  :	he had not practised since       he   returned and      he      had also      fallen sick 
2024-02-02 20:49:16,087 	Text Hypothesis :	he was not ********* comfortable with the      comments section is  currently in     india
2024-02-02 20:49:16,088 	Text Alignment  :	   S       D         S           S    S        S        S       S   S         S      S    
2024-02-02 20:49:16,088 ========================================================================================================================
2024-02-02 20:49:16,088 Logging Sequence: 68_230.00
2024-02-02 20:49:16,088 	Gloss Reference :	A B+C+D+E
2024-02-02 20:49:16,088 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 20:49:16,088 	Gloss Alignment :	         
2024-02-02 20:49:16,088 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 20:49:16,089 	Text Reference  :	let us know what you  think  in   the ****** comments below 
2024-02-02 20:49:16,089 	Text Hypothesis :	*** ** **** but  some walked with the bowler deepak   chahar
2024-02-02 20:49:16,089 	Text Alignment  :	D   D  D    S    S    S      S        I      S        S     
2024-02-02 20:49:16,089 ========================================================================================================================
2024-02-02 20:49:16,089 Logging Sequence: 126_82.00
2024-02-02 20:49:16,090 	Gloss Reference :	A B+C+D+E
2024-02-02 20:49:16,090 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 20:49:16,090 	Gloss Alignment :	         
2024-02-02 20:49:16,090 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 20:49:16,092 	Text Reference  :	** ******* neeraj also    dedicated his gold medal to  former indian olympians who came close to  winning medals
2024-02-02 20:49:16,092 	Text Hypothesis :	he stepped away   because he        did not  stay  and then   the    players   who **** is    the second  team  
2024-02-02 20:49:16,092 	Text Alignment  :	I  I       S      S       S         S   S    S     S   S      S      S             D    S     S   S       S     
2024-02-02 20:49:16,092 ========================================================================================================================
2024-02-02 20:49:19,516 Epoch 120: Total Training Recognition Loss 0.04  Total Training Translation Loss 21.96 
2024-02-02 20:49:19,516 EPOCH 121
2024-02-02 20:49:24,550 [Epoch: 121 Step: 00008100] Batch Recognition Loss:   0.000481 => Gls Tokens per Sec:     1890 || Batch Translation Loss:   0.474048 => Txt Tokens per Sec:     5145 || Lr: 0.000100
2024-02-02 20:49:25,242 Epoch 121: Total Training Recognition Loss 0.06  Total Training Translation Loss 22.82 
2024-02-02 20:49:25,242 EPOCH 122
2024-02-02 20:49:30,069 Epoch 122: Total Training Recognition Loss 0.07  Total Training Translation Loss 24.01 
2024-02-02 20:49:30,070 EPOCH 123
2024-02-02 20:49:31,934 [Epoch: 123 Step: 00008200] Batch Recognition Loss:   0.000413 => Gls Tokens per Sec:     2231 || Batch Translation Loss:   0.221369 => Txt Tokens per Sec:     6337 || Lr: 0.000100
2024-02-02 20:49:35,241 Epoch 123: Total Training Recognition Loss 0.06  Total Training Translation Loss 25.13 
2024-02-02 20:49:35,242 EPOCH 124
2024-02-02 20:49:40,083 [Epoch: 124 Step: 00008300] Batch Recognition Loss:   0.001161 => Gls Tokens per Sec:     1932 || Batch Translation Loss:   0.203920 => Txt Tokens per Sec:     5457 || Lr: 0.000100
2024-02-02 20:49:40,629 Epoch 124: Total Training Recognition Loss 0.07  Total Training Translation Loss 23.50 
2024-02-02 20:49:40,629 EPOCH 125
2024-02-02 20:49:45,872 Epoch 125: Total Training Recognition Loss 0.05  Total Training Translation Loss 20.00 
2024-02-02 20:49:45,872 EPOCH 126
2024-02-02 20:49:48,000 [Epoch: 126 Step: 00008400] Batch Recognition Loss:   0.000844 => Gls Tokens per Sec:     1881 || Batch Translation Loss:   0.202149 => Txt Tokens per Sec:     5491 || Lr: 0.000100
2024-02-02 20:49:51,386 Epoch 126: Total Training Recognition Loss 0.05  Total Training Translation Loss 18.01 
2024-02-02 20:49:51,387 EPOCH 127
2024-02-02 20:49:55,764 [Epoch: 127 Step: 00008500] Batch Recognition Loss:   0.000646 => Gls Tokens per Sec:     2100 || Batch Translation Loss:   0.259363 => Txt Tokens per Sec:     5874 || Lr: 0.000100
2024-02-02 20:49:56,501 Epoch 127: Total Training Recognition Loss 0.05  Total Training Translation Loss 19.35 
2024-02-02 20:49:56,502 EPOCH 128
2024-02-02 20:50:01,977 Epoch 128: Total Training Recognition Loss 0.05  Total Training Translation Loss 20.43 
2024-02-02 20:50:01,978 EPOCH 129
2024-02-02 20:50:03,589 [Epoch: 129 Step: 00008600] Batch Recognition Loss:   0.000638 => Gls Tokens per Sec:     2386 || Batch Translation Loss:   0.297057 => Txt Tokens per Sec:     6673 || Lr: 0.000100
2024-02-02 20:50:07,140 Epoch 129: Total Training Recognition Loss 0.06  Total Training Translation Loss 21.91 
2024-02-02 20:50:07,141 EPOCH 130
2024-02-02 20:50:11,565 [Epoch: 130 Step: 00008700] Batch Recognition Loss:   0.000250 => Gls Tokens per Sec:     2042 || Batch Translation Loss:   0.245142 => Txt Tokens per Sec:     5684 || Lr: 0.000100
2024-02-02 20:50:12,328 Epoch 130: Total Training Recognition Loss 0.05  Total Training Translation Loss 21.15 
2024-02-02 20:50:12,328 EPOCH 131
2024-02-02 20:50:17,689 Epoch 131: Total Training Recognition Loss 0.05  Total Training Translation Loss 19.87 
2024-02-02 20:50:17,690 EPOCH 132
2024-02-02 20:50:19,577 [Epoch: 132 Step: 00008800] Batch Recognition Loss:   0.000691 => Gls Tokens per Sec:     1903 || Batch Translation Loss:   0.512159 => Txt Tokens per Sec:     5317 || Lr: 0.000100
2024-02-02 20:50:22,616 Epoch 132: Total Training Recognition Loss 0.06  Total Training Translation Loss 20.32 
2024-02-02 20:50:22,616 EPOCH 133
2024-02-02 20:50:27,232 [Epoch: 133 Step: 00008900] Batch Recognition Loss:   0.000502 => Gls Tokens per Sec:     1922 || Batch Translation Loss:   0.546167 => Txt Tokens per Sec:     5358 || Lr: 0.000100
2024-02-02 20:50:28,040 Epoch 133: Total Training Recognition Loss 0.04  Total Training Translation Loss 20.74 
2024-02-02 20:50:28,040 EPOCH 134
2024-02-02 20:50:33,064 Epoch 134: Total Training Recognition Loss 0.05  Total Training Translation Loss 22.42 
2024-02-02 20:50:33,065 EPOCH 135
2024-02-02 20:50:35,157 [Epoch: 135 Step: 00009000] Batch Recognition Loss:   0.000549 => Gls Tokens per Sec:     1640 || Batch Translation Loss:   0.293315 => Txt Tokens per Sec:     4625 || Lr: 0.000100
2024-02-02 20:50:38,608 Epoch 135: Total Training Recognition Loss 0.04  Total Training Translation Loss 20.56 
2024-02-02 20:50:38,608 EPOCH 136
2024-02-02 20:50:42,357 [Epoch: 136 Step: 00009100] Batch Recognition Loss:   0.000499 => Gls Tokens per Sec:     2324 || Batch Translation Loss:   0.350573 => Txt Tokens per Sec:     6327 || Lr: 0.000100
2024-02-02 20:50:43,310 Epoch 136: Total Training Recognition Loss 0.06  Total Training Translation Loss 24.33 
2024-02-02 20:50:43,310 EPOCH 137
2024-02-02 20:50:47,805 Epoch 137: Total Training Recognition Loss 0.04  Total Training Translation Loss 21.15 
2024-02-02 20:50:47,806 EPOCH 138
2024-02-02 20:50:49,590 [Epoch: 138 Step: 00009200] Batch Recognition Loss:   0.000596 => Gls Tokens per Sec:     1883 || Batch Translation Loss:   0.588839 => Txt Tokens per Sec:     5043 || Lr: 0.000100
2024-02-02 20:50:53,296 Epoch 138: Total Training Recognition Loss 0.06  Total Training Translation Loss 18.00 
2024-02-02 20:50:53,296 EPOCH 139
2024-02-02 20:50:57,539 [Epoch: 139 Step: 00009300] Batch Recognition Loss:   0.000281 => Gls Tokens per Sec:     2016 || Batch Translation Loss:   0.257559 => Txt Tokens per Sec:     5598 || Lr: 0.000100
2024-02-02 20:50:58,683 Epoch 139: Total Training Recognition Loss 0.04  Total Training Translation Loss 17.19 
2024-02-02 20:50:58,684 EPOCH 140
2024-02-02 20:51:04,048 Epoch 140: Total Training Recognition Loss 0.04  Total Training Translation Loss 18.48 
2024-02-02 20:51:04,048 EPOCH 141
2024-02-02 20:51:05,580 [Epoch: 141 Step: 00009400] Batch Recognition Loss:   0.000529 => Gls Tokens per Sec:     2090 || Batch Translation Loss:   0.240838 => Txt Tokens per Sec:     6166 || Lr: 0.000100
2024-02-02 20:51:09,339 Epoch 141: Total Training Recognition Loss 0.04  Total Training Translation Loss 17.53 
2024-02-02 20:51:09,340 EPOCH 142
2024-02-02 20:51:13,321 [Epoch: 142 Step: 00009500] Batch Recognition Loss:   0.000324 => Gls Tokens per Sec:     2108 || Batch Translation Loss:   0.281397 => Txt Tokens per Sec:     5908 || Lr: 0.000100
2024-02-02 20:51:14,339 Epoch 142: Total Training Recognition Loss 0.04  Total Training Translation Loss 18.91 
2024-02-02 20:51:14,339 EPOCH 143
2024-02-02 20:51:19,372 Epoch 143: Total Training Recognition Loss 0.04  Total Training Translation Loss 18.40 
2024-02-02 20:51:19,373 EPOCH 144
2024-02-02 20:51:21,194 [Epoch: 144 Step: 00009600] Batch Recognition Loss:   0.000534 => Gls Tokens per Sec:     1621 || Batch Translation Loss:   0.152016 => Txt Tokens per Sec:     4805 || Lr: 0.000100
2024-02-02 20:51:24,811 Epoch 144: Total Training Recognition Loss 0.04  Total Training Translation Loss 19.95 
2024-02-02 20:51:24,811 EPOCH 145
2024-02-02 20:51:29,246 [Epoch: 145 Step: 00009700] Batch Recognition Loss:   0.000450 => Gls Tokens per Sec:     1856 || Batch Translation Loss:   0.205833 => Txt Tokens per Sec:     5168 || Lr: 0.000100
2024-02-02 20:51:30,270 Epoch 145: Total Training Recognition Loss 0.05  Total Training Translation Loss 18.79 
2024-02-02 20:51:30,271 EPOCH 146
2024-02-02 20:51:35,505 Epoch 146: Total Training Recognition Loss 0.04  Total Training Translation Loss 16.34 
2024-02-02 20:51:35,506 EPOCH 147
2024-02-02 20:51:36,841 [Epoch: 147 Step: 00009800] Batch Recognition Loss:   0.000499 => Gls Tokens per Sec:     2159 || Batch Translation Loss:   0.278385 => Txt Tokens per Sec:     5788 || Lr: 0.000100
2024-02-02 20:51:40,709 Epoch 147: Total Training Recognition Loss 0.03  Total Training Translation Loss 15.48 
2024-02-02 20:51:40,709 EPOCH 148
2024-02-02 20:51:44,593 [Epoch: 148 Step: 00009900] Batch Recognition Loss:   0.000462 => Gls Tokens per Sec:     2102 || Batch Translation Loss:   0.227690 => Txt Tokens per Sec:     5745 || Lr: 0.000100
2024-02-02 20:51:46,153 Epoch 148: Total Training Recognition Loss 0.04  Total Training Translation Loss 15.21 
2024-02-02 20:51:46,153 EPOCH 149
2024-02-02 20:51:51,083 Epoch 149: Total Training Recognition Loss 0.04  Total Training Translation Loss 13.78 
2024-02-02 20:51:51,083 EPOCH 150
2024-02-02 20:51:52,643 [Epoch: 150 Step: 00010000] Batch Recognition Loss:   0.000185 => Gls Tokens per Sec:     1745 || Batch Translation Loss:   0.064468 => Txt Tokens per Sec:     4821 || Lr: 0.000100
2024-02-02 20:52:00,935 Hooray! New best validation result [eval_metric]!
2024-02-02 20:52:00,936 Saving new checkpoint.
2024-02-02 20:52:01,229 Validation result at epoch 150, step    10000: duration: 8.5855s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00184	Translation Loss: 89947.78906	PPL: 8111.83643
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 1.02	(BLEU-1: 11.89,	BLEU-2: 4.00,	BLEU-3: 1.85,	BLEU-4: 1.02)
	CHRF 17.47	ROUGE 10.05
2024-02-02 20:52:01,231 Logging Recognition and Translation Outputs
2024-02-02 20:52:01,231 ========================================================================================================================
2024-02-02 20:52:01,231 Logging Sequence: 159_139.00
2024-02-02 20:52:01,232 	Gloss Reference :	A B+C+D+E
2024-02-02 20:52:01,232 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 20:52:01,232 	Gloss Alignment :	         
2024-02-02 20:52:01,232 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 20:52:01,233 	Text Reference  :	he took time    and  finally was ready for the asia cup where he       scored the century
2024-02-02 20:52:01,233 	Text Hypothesis :	** **** earlier when he      was ***** *** *** **** *** ***** stepping down   as  well   
2024-02-02 20:52:01,233 	Text Alignment  :	D  D    S       S    S           D     D   D   D    D   D     S        S      S   S      
2024-02-02 20:52:01,234 ========================================================================================================================
2024-02-02 20:52:01,234 Logging Sequence: 159_159.00
2024-02-02 20:52:01,234 	Gloss Reference :	A B+C+D+E
2024-02-02 20:52:01,234 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 20:52:01,234 	Gloss Alignment :	         
2024-02-02 20:52:01,234 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 20:52:01,237 	Text Reference  :	he   said it     wasn't   easy    the  mind has   to  be    focussed and he is glad that he is back in  form  with the asia    cup          century
2024-02-02 20:52:01,237 	Text Hypothesis :	well my   crowds everyone thought that both match the first time     and ** ** **** **** ** ** **** the score was  not playing professional cricket
2024-02-02 20:52:01,237 	Text Alignment  :	S    S    S      S        S       S    S    S     S   S     S            D  D  D    D    D  D  D    S   S     S    S   S       S            S      
2024-02-02 20:52:01,237 ========================================================================================================================
2024-02-02 20:52:01,238 Logging Sequence: 103_8.00
2024-02-02 20:52:01,238 	Gloss Reference :	A B+C+D+E
2024-02-02 20:52:01,238 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 20:52:01,238 	Gloss Alignment :	         
2024-02-02 20:52:01,238 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 20:52:01,239 	Text Reference  :	were going on           in    birmingham england      from 28th july to  8th     august 2022 
2024-02-02 20:52:01,239 	Text Hypothesis :	**** the   commonwealth games encourage  independence from **** **** the british empire games
2024-02-02 20:52:01,239 	Text Alignment  :	D    S     S            S     S          S                 D    D    S   S       S      S    
2024-02-02 20:52:01,239 ========================================================================================================================
2024-02-02 20:52:01,240 Logging Sequence: 164_546.00
2024-02-02 20:52:01,240 	Gloss Reference :	A B+C+D+E
2024-02-02 20:52:01,240 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 20:52:01,240 	Gloss Alignment :	         
2024-02-02 20:52:01,240 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 20:52:01,241 	Text Reference  :	reliance has turned out   to    be  the    strongest company
2024-02-02 20:52:01,241 	Text Hypothesis :	after    a   tough  match india has beaten them      19     
2024-02-02 20:52:01,241 	Text Alignment  :	S        S   S      S     S     S   S      S         S      
2024-02-02 20:52:01,241 ========================================================================================================================
2024-02-02 20:52:01,242 Logging Sequence: 132_173.00
2024-02-02 20:52:01,242 	Gloss Reference :	A B+C+D+E
2024-02-02 20:52:01,242 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 20:52:01,242 	Gloss Alignment :	         
2024-02-02 20:52:01,242 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 20:52:01,243 	Text Reference  :	**** *** ****** ******** *** ******** ** ** ******** **** ** ****** **** * usman is   australia' first muslim player
2024-02-02 20:52:01,243 	Text Hypothesis :	bcci had tested positive for covid-19 on 26 december 2021 he always used a break from playing    at    1      year  
2024-02-02 20:52:01,243 	Text Alignment  :	I    I   I      I        I   I        I  I  I        I    I  I      I    I S     S    S          S     S      S     
2024-02-02 20:52:01,243 ========================================================================================================================
2024-02-02 20:52:05,208 Epoch 150: Total Training Recognition Loss 0.03  Total Training Translation Loss 11.62 
2024-02-02 20:52:05,208 EPOCH 151
2024-02-02 20:52:08,981 [Epoch: 151 Step: 00010100] Batch Recognition Loss:   0.000470 => Gls Tokens per Sec:     2097 || Batch Translation Loss:   0.202198 => Txt Tokens per Sec:     5847 || Lr: 0.000100
2024-02-02 20:52:10,292 Epoch 151: Total Training Recognition Loss 0.03  Total Training Translation Loss 16.40 
2024-02-02 20:52:10,292 EPOCH 152
2024-02-02 20:52:15,873 Epoch 152: Total Training Recognition Loss 0.04  Total Training Translation Loss 13.15 
2024-02-02 20:52:15,873 EPOCH 153
2024-02-02 20:52:16,900 [Epoch: 153 Step: 00010200] Batch Recognition Loss:   0.000361 => Gls Tokens per Sec:     2495 || Batch Translation Loss:   0.162987 => Txt Tokens per Sec:     6752 || Lr: 0.000100
2024-02-02 20:52:21,118 Epoch 153: Total Training Recognition Loss 0.04  Total Training Translation Loss 15.89 
2024-02-02 20:52:21,119 EPOCH 154
2024-02-02 20:52:24,965 [Epoch: 154 Step: 00010300] Batch Recognition Loss:   0.000479 => Gls Tokens per Sec:     2016 || Batch Translation Loss:   0.127608 => Txt Tokens per Sec:     5677 || Lr: 0.000100
2024-02-02 20:52:26,270 Epoch 154: Total Training Recognition Loss 0.05  Total Training Translation Loss 16.59 
2024-02-02 20:52:26,271 EPOCH 155
2024-02-02 20:52:31,435 Epoch 155: Total Training Recognition Loss 0.04  Total Training Translation Loss 17.67 
2024-02-02 20:52:31,436 EPOCH 156
2024-02-02 20:52:32,715 [Epoch: 156 Step: 00010400] Batch Recognition Loss:   0.000917 => Gls Tokens per Sec:     1879 || Batch Translation Loss:   0.456122 => Txt Tokens per Sec:     5576 || Lr: 0.000100
2024-02-02 20:52:36,588 Epoch 156: Total Training Recognition Loss 0.05  Total Training Translation Loss 24.15 
2024-02-02 20:52:36,588 EPOCH 157
2024-02-02 20:52:40,671 [Epoch: 157 Step: 00010500] Batch Recognition Loss:   0.000773 => Gls Tokens per Sec:     1859 || Batch Translation Loss:   0.623731 => Txt Tokens per Sec:     5147 || Lr: 0.000100
2024-02-02 20:52:42,158 Epoch 157: Total Training Recognition Loss 0.05  Total Training Translation Loss 19.03 
2024-02-02 20:52:42,158 EPOCH 158
2024-02-02 20:52:47,100 Epoch 158: Total Training Recognition Loss 0.06  Total Training Translation Loss 16.07 
2024-02-02 20:52:47,101 EPOCH 159
2024-02-02 20:52:48,219 [Epoch: 159 Step: 00010600] Batch Recognition Loss:   0.000657 => Gls Tokens per Sec:     2005 || Batch Translation Loss:   0.175865 => Txt Tokens per Sec:     5440 || Lr: 0.000100
2024-02-02 20:52:52,491 Epoch 159: Total Training Recognition Loss 0.05  Total Training Translation Loss 19.28 
2024-02-02 20:52:52,491 EPOCH 160
2024-02-02 20:52:55,538 [Epoch: 160 Step: 00010700] Batch Recognition Loss:   0.000477 => Gls Tokens per Sec:     2469 || Batch Translation Loss:   0.320791 => Txt Tokens per Sec:     6782 || Lr: 0.000100
2024-02-02 20:52:57,528 Epoch 160: Total Training Recognition Loss 0.04  Total Training Translation Loss 18.49 
2024-02-02 20:52:57,529 EPOCH 161
2024-02-02 20:53:02,939 Epoch 161: Total Training Recognition Loss 0.05  Total Training Translation Loss 18.32 
2024-02-02 20:53:02,939 EPOCH 162
2024-02-02 20:53:04,054 [Epoch: 162 Step: 00010800] Batch Recognition Loss:   0.000419 => Gls Tokens per Sec:     1786 || Batch Translation Loss:   0.190623 => Txt Tokens per Sec:     5162 || Lr: 0.000100
2024-02-02 20:53:08,207 Epoch 162: Total Training Recognition Loss 0.05  Total Training Translation Loss 21.00 
2024-02-02 20:53:08,208 EPOCH 163
2024-02-02 20:53:11,541 [Epoch: 163 Step: 00010900] Batch Recognition Loss:   0.000543 => Gls Tokens per Sec:     2182 || Batch Translation Loss:   0.227671 => Txt Tokens per Sec:     6056 || Lr: 0.000100
2024-02-02 20:53:13,243 Epoch 163: Total Training Recognition Loss 0.04  Total Training Translation Loss 17.60 
2024-02-02 20:53:13,244 EPOCH 164
2024-02-02 20:53:18,858 Epoch 164: Total Training Recognition Loss 0.04  Total Training Translation Loss 14.46 
2024-02-02 20:53:18,858 EPOCH 165
2024-02-02 20:53:19,797 [Epoch: 165 Step: 00011000] Batch Recognition Loss:   0.000412 => Gls Tokens per Sec:     2047 || Batch Translation Loss:   0.316343 => Txt Tokens per Sec:     5678 || Lr: 0.000100
2024-02-02 20:53:24,300 Epoch 165: Total Training Recognition Loss 0.03  Total Training Translation Loss 13.08 
2024-02-02 20:53:24,301 EPOCH 166
2024-02-02 20:53:27,952 [Epoch: 166 Step: 00011100] Batch Recognition Loss:   0.000331 => Gls Tokens per Sec:     1973 || Batch Translation Loss:   0.228150 => Txt Tokens per Sec:     5402 || Lr: 0.000100
2024-02-02 20:53:29,983 Epoch 166: Total Training Recognition Loss 0.04  Total Training Translation Loss 13.97 
2024-02-02 20:53:29,983 EPOCH 167
2024-02-02 20:53:35,155 Epoch 167: Total Training Recognition Loss 0.03  Total Training Translation Loss 13.38 
2024-02-02 20:53:35,155 EPOCH 168
2024-02-02 20:53:35,845 [Epoch: 168 Step: 00011200] Batch Recognition Loss:   0.000261 => Gls Tokens per Sec:     2558 || Batch Translation Loss:   0.142082 => Txt Tokens per Sec:     6484 || Lr: 0.000100
2024-02-02 20:53:40,332 Epoch 168: Total Training Recognition Loss 0.03  Total Training Translation Loss 12.02 
2024-02-02 20:53:40,333 EPOCH 169
2024-02-02 20:53:43,909 [Epoch: 169 Step: 00011300] Batch Recognition Loss:   0.000351 => Gls Tokens per Sec:     1944 || Batch Translation Loss:   0.075558 => Txt Tokens per Sec:     5341 || Lr: 0.000100
2024-02-02 20:53:45,717 Epoch 169: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.47 
2024-02-02 20:53:45,718 EPOCH 170
2024-02-02 20:53:50,819 Epoch 170: Total Training Recognition Loss 0.03  Total Training Translation Loss 13.88 
2024-02-02 20:53:50,819 EPOCH 171
2024-02-02 20:53:51,721 [Epoch: 171 Step: 00011400] Batch Recognition Loss:   0.001871 => Gls Tokens per Sec:     1778 || Batch Translation Loss:   0.142272 => Txt Tokens per Sec:     4816 || Lr: 0.000100
2024-02-02 20:53:56,376 Epoch 171: Total Training Recognition Loss 0.03  Total Training Translation Loss 14.49 
2024-02-02 20:53:56,377 EPOCH 172
2024-02-02 20:53:59,468 [Epoch: 172 Step: 00011500] Batch Recognition Loss:   0.000263 => Gls Tokens per Sec:     2197 || Batch Translation Loss:   0.192948 => Txt Tokens per Sec:     6058 || Lr: 0.000100
2024-02-02 20:54:01,329 Epoch 172: Total Training Recognition Loss 0.03  Total Training Translation Loss 14.03 
2024-02-02 20:54:01,330 EPOCH 173
2024-02-02 20:54:06,791 Epoch 173: Total Training Recognition Loss 0.04  Total Training Translation Loss 14.79 
2024-02-02 20:54:06,791 EPOCH 174
2024-02-02 20:54:07,471 [Epoch: 174 Step: 00011600] Batch Recognition Loss:   0.000574 => Gls Tokens per Sec:     2121 || Batch Translation Loss:   0.100335 => Txt Tokens per Sec:     6144 || Lr: 0.000100
2024-02-02 20:54:11,625 Epoch 174: Total Training Recognition Loss 0.03  Total Training Translation Loss 15.43 
2024-02-02 20:54:11,626 EPOCH 175
2024-02-02 20:54:15,248 [Epoch: 175 Step: 00011700] Batch Recognition Loss:   0.000418 => Gls Tokens per Sec:     1831 || Batch Translation Loss:   0.135173 => Txt Tokens per Sec:     5173 || Lr: 0.000100
2024-02-02 20:54:17,184 Epoch 175: Total Training Recognition Loss 0.04  Total Training Translation Loss 13.87 
2024-02-02 20:54:17,184 EPOCH 176
2024-02-02 20:54:22,495 Epoch 176: Total Training Recognition Loss 0.04  Total Training Translation Loss 11.82 
2024-02-02 20:54:22,495 EPOCH 177
2024-02-02 20:54:23,263 [Epoch: 177 Step: 00011800] Batch Recognition Loss:   0.000334 => Gls Tokens per Sec:     1669 || Batch Translation Loss:   0.159641 => Txt Tokens per Sec:     5036 || Lr: 0.000100
2024-02-02 20:54:27,426 Epoch 177: Total Training Recognition Loss 0.03  Total Training Translation Loss 11.36 
2024-02-02 20:54:27,426 EPOCH 178
2024-02-02 20:54:30,988 [Epoch: 178 Step: 00011900] Batch Recognition Loss:   0.000524 => Gls Tokens per Sec:     1817 || Batch Translation Loss:   1.126117 => Txt Tokens per Sec:     5046 || Lr: 0.000100
2024-02-02 20:54:33,048 Epoch 178: Total Training Recognition Loss 0.04  Total Training Translation Loss 13.70 
2024-02-02 20:54:33,048 EPOCH 179
2024-02-02 20:54:38,406 Epoch 179: Total Training Recognition Loss 0.03  Total Training Translation Loss 14.86 
2024-02-02 20:54:38,407 EPOCH 180
2024-02-02 20:54:39,003 [Epoch: 180 Step: 00012000] Batch Recognition Loss:   0.000432 => Gls Tokens per Sec:     1882 || Batch Translation Loss:   0.122855 => Txt Tokens per Sec:     5534 || Lr: 0.000100
2024-02-02 20:54:47,571 Validation result at epoch 180, step    12000: duration: 8.5687s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00296	Translation Loss: 90178.62500	PPL: 8301.40039
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.86	(BLEU-1: 11.69,	BLEU-2: 3.73,	BLEU-3: 1.61,	BLEU-4: 0.86)
	CHRF 17.99	ROUGE 9.72
2024-02-02 20:54:47,572 Logging Recognition and Translation Outputs
2024-02-02 20:54:47,572 ========================================================================================================================
2024-02-02 20:54:47,572 Logging Sequence: 177_50.00
2024-02-02 20:54:47,573 	Gloss Reference :	A B+C+D+E
2024-02-02 20:54:47,573 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 20:54:47,573 	Gloss Alignment :	         
2024-02-02 20:54:47,573 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 20:54:47,575 	Text Reference  :	a     similar reward    of rs 50000 was  announced for information ******* ** against his    associate ajay   kumar
2024-02-02 20:54:47,575 	Text Hypothesis :	delhi police  announced a  rs 1     lakh reward    for information leading to the     arrest of        sushil kumar
2024-02-02 20:54:47,575 	Text Alignment  :	S     S       S         S     S     S    S                         I       I  S       S      S         S           
2024-02-02 20:54:47,575 ========================================================================================================================
2024-02-02 20:54:47,575 Logging Sequence: 122_86.00
2024-02-02 20:54:47,575 	Gloss Reference :	A B+C+D+E
2024-02-02 20:54:47,575 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 20:54:47,575 	Gloss Alignment :	         
2024-02-02 20:54:47,576 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 20:54:47,576 	Text Reference  :	after winning chanu spoke    to    the media and   said      
2024-02-02 20:54:47,576 	Text Hypothesis :	as    per     the   olympics rules if  a     press conference
2024-02-02 20:54:47,576 	Text Alignment  :	S     S       S     S        S     S   S     S     S         
2024-02-02 20:54:47,577 ========================================================================================================================
2024-02-02 20:54:47,577 Logging Sequence: 165_27.00
2024-02-02 20:54:47,577 	Gloss Reference :	A B+C+D+E
2024-02-02 20:54:47,577 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 20:54:47,577 	Gloss Alignment :	         
2024-02-02 20:54:47,577 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 20:54:47,579 	Text Reference  :	so  then ****** **** *** they      change their    routes some people believe in  this      while some don't
2024-02-02 20:54:47,579 	Text Hypothesis :	and then handed over the captaincy to     ravindra jadeja who  is     watch   the captaincy to    give it   
2024-02-02 20:54:47,579 	Text Alignment  :	S        I      I    I   S         S      S        S      S    S      S       S   S         S     S    S    
2024-02-02 20:54:47,579 ========================================================================================================================
2024-02-02 20:54:47,579 Logging Sequence: 70_65.00
2024-02-02 20:54:47,579 	Gloss Reference :	A B+C+D+E
2024-02-02 20:54:47,579 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 20:54:47,580 	Gloss Alignment :	         
2024-02-02 20:54:47,580 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 20:54:47,581 	Text Reference  :	during the press conference a        table was placed   in front of         the media
2024-02-02 20:54:47,581 	Text Hypothesis :	****** the ***** ********** olympics will  be  creating a  press conference for 170  
2024-02-02 20:54:47,581 	Text Alignment  :	D          D     D          S        S     S   S        S  S     S          S   S    
2024-02-02 20:54:47,581 ========================================================================================================================
2024-02-02 20:54:47,581 Logging Sequence: 149_65.00
2024-02-02 20:54:47,581 	Gloss Reference :	A B+C+D+E
2024-02-02 20:54:47,581 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 20:54:47,581 	Gloss Alignment :	         
2024-02-02 20:54:47,582 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 20:54:47,584 	Text Reference  :	*** at  6am   on      6th  november 2022 the  police reached sri lankan team's  hotel in   sydney australia's central business district cbd 
2024-02-02 20:54:47,584 	Text Hypothesis :	now the woman alleged that if       i    will be     eager   to  host   matches are   more than   talking     with    2nd      november 2022
2024-02-02 20:54:47,584 	Text Alignment  :	I   S   S     S       S    S        S    S    S      S       S   S      S       S     S    S      S           S       S        S        S   
2024-02-02 20:54:47,584 ========================================================================================================================
2024-02-02 20:54:52,376 Epoch 180: Total Training Recognition Loss 0.04  Total Training Translation Loss 17.04 
2024-02-02 20:54:52,376 EPOCH 181
2024-02-02 20:54:55,114 [Epoch: 181 Step: 00012100] Batch Recognition Loss:   0.000387 => Gls Tokens per Sec:     2339 || Batch Translation Loss:   0.075937 => Txt Tokens per Sec:     6473 || Lr: 0.000100
2024-02-02 20:54:57,106 Epoch 181: Total Training Recognition Loss 0.04  Total Training Translation Loss 16.52 
2024-02-02 20:54:57,106 EPOCH 182
2024-02-02 20:55:02,500 Epoch 182: Total Training Recognition Loss 0.05  Total Training Translation Loss 15.08 
2024-02-02 20:55:02,501 EPOCH 183
2024-02-02 20:55:03,095 [Epoch: 183 Step: 00012200] Batch Recognition Loss:   0.000525 => Gls Tokens per Sec:     1466 || Batch Translation Loss:   2.123478 => Txt Tokens per Sec:     4381 || Lr: 0.000100
2024-02-02 20:55:07,748 Epoch 183: Total Training Recognition Loss 0.04  Total Training Translation Loss 22.68 
2024-02-02 20:55:07,749 EPOCH 184
2024-02-02 20:55:11,124 [Epoch: 184 Step: 00012300] Batch Recognition Loss:   0.001613 => Gls Tokens per Sec:     1822 || Batch Translation Loss:   0.156872 => Txt Tokens per Sec:     5202 || Lr: 0.000100
2024-02-02 20:55:13,036 Epoch 184: Total Training Recognition Loss 0.07  Total Training Translation Loss 17.70 
2024-02-02 20:55:13,036 EPOCH 185
2024-02-02 20:55:18,350 Epoch 185: Total Training Recognition Loss 0.04  Total Training Translation Loss 12.62 
2024-02-02 20:55:18,350 EPOCH 186
2024-02-02 20:55:18,703 [Epoch: 186 Step: 00012400] Batch Recognition Loss:   0.000298 => Gls Tokens per Sec:     2279 || Batch Translation Loss:   0.254736 => Txt Tokens per Sec:     6641 || Lr: 0.000100
2024-02-02 20:55:23,422 Epoch 186: Total Training Recognition Loss 0.04  Total Training Translation Loss 12.37 
2024-02-02 20:55:23,422 EPOCH 187
2024-02-02 20:55:26,707 [Epoch: 187 Step: 00012500] Batch Recognition Loss:   0.000421 => Gls Tokens per Sec:     1825 || Batch Translation Loss:   0.109592 => Txt Tokens per Sec:     4933 || Lr: 0.000100
2024-02-02 20:55:29,007 Epoch 187: Total Training Recognition Loss 0.05  Total Training Translation Loss 12.02 
2024-02-02 20:55:29,007 EPOCH 188
2024-02-02 20:55:34,401 Epoch 188: Total Training Recognition Loss 0.04  Total Training Translation Loss 9.36 
2024-02-02 20:55:34,402 EPOCH 189
2024-02-02 20:55:34,721 [Epoch: 189 Step: 00012600] Batch Recognition Loss:   0.000159 => Gls Tokens per Sec:     2019 || Batch Translation Loss:   0.108974 => Txt Tokens per Sec:     5675 || Lr: 0.000100
2024-02-02 20:55:39,632 Epoch 189: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.35 
2024-02-02 20:55:39,632 EPOCH 190
2024-02-02 20:55:42,040 [Epoch: 190 Step: 00012700] Batch Recognition Loss:   0.000424 => Gls Tokens per Sec:     2459 || Batch Translation Loss:   0.082082 => Txt Tokens per Sec:     6772 || Lr: 0.000100
2024-02-02 20:55:44,730 Epoch 190: Total Training Recognition Loss 0.04  Total Training Translation Loss 9.88 
2024-02-02 20:55:44,730 EPOCH 191
2024-02-02 20:55:49,928 Epoch 191: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.67 
2024-02-02 20:55:49,929 EPOCH 192
2024-02-02 20:55:50,245 [Epoch: 192 Step: 00012800] Batch Recognition Loss:   0.000397 => Gls Tokens per Sec:     1519 || Batch Translation Loss:   0.139151 => Txt Tokens per Sec:     5054 || Lr: 0.000100
2024-02-02 20:55:54,720 Epoch 192: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.80 
2024-02-02 20:55:54,720 EPOCH 193
2024-02-02 20:55:57,064 [Epoch: 193 Step: 00012900] Batch Recognition Loss:   0.000396 => Gls Tokens per Sec:     2457 || Batch Translation Loss:   0.174885 => Txt Tokens per Sec:     6570 || Lr: 0.000100
2024-02-02 20:55:59,369 Epoch 193: Total Training Recognition Loss 0.03  Total Training Translation Loss 10.39 
2024-02-02 20:55:59,369 EPOCH 194
2024-02-02 20:56:04,817 Epoch 194: Total Training Recognition Loss 0.04  Total Training Translation Loss 12.82 
2024-02-02 20:56:04,818 EPOCH 195
2024-02-02 20:56:05,029 [Epoch: 195 Step: 00013000] Batch Recognition Loss:   0.000357 => Gls Tokens per Sec:     1524 || Batch Translation Loss:   0.132561 => Txt Tokens per Sec:     4819 || Lr: 0.000100
2024-02-02 20:56:10,075 Epoch 195: Total Training Recognition Loss 0.03  Total Training Translation Loss 15.32 
2024-02-02 20:56:10,076 EPOCH 196
2024-02-02 20:56:12,908 [Epoch: 196 Step: 00013100] Batch Recognition Loss:   0.000285 => Gls Tokens per Sec:     1946 || Batch Translation Loss:   0.236120 => Txt Tokens per Sec:     5335 || Lr: 0.000100
2024-02-02 20:56:15,462 Epoch 196: Total Training Recognition Loss 0.03  Total Training Translation Loss 13.77 
2024-02-02 20:56:15,462 EPOCH 197
2024-02-02 20:56:20,762 Epoch 197: Total Training Recognition Loss 0.03  Total Training Translation Loss 12.92 
2024-02-02 20:56:20,763 EPOCH 198
2024-02-02 20:56:20,822 [Epoch: 198 Step: 00013200] Batch Recognition Loss:   0.000253 => Gls Tokens per Sec:     2759 || Batch Translation Loss:   0.145470 => Txt Tokens per Sec:     6172 || Lr: 0.000100
2024-02-02 20:56:25,790 Epoch 198: Total Training Recognition Loss 0.04  Total Training Translation Loss 17.21 
2024-02-02 20:56:25,791 EPOCH 199
2024-02-02 20:56:28,737 [Epoch: 199 Step: 00013300] Batch Recognition Loss:   0.000529 => Gls Tokens per Sec:     1847 || Batch Translation Loss:   0.301362 => Txt Tokens per Sec:     5088 || Lr: 0.000100
2024-02-02 20:56:31,320 Epoch 199: Total Training Recognition Loss 0.04  Total Training Translation Loss 19.23 
2024-02-02 20:56:31,320 EPOCH 200
2024-02-02 20:56:36,403 [Epoch: 200 Step: 00013400] Batch Recognition Loss:   0.000795 => Gls Tokens per Sec:     2092 || Batch Translation Loss:   0.152964 => Txt Tokens per Sec:     5808 || Lr: 0.000100
2024-02-02 20:56:36,404 Epoch 200: Total Training Recognition Loss 0.05  Total Training Translation Loss 25.59 
2024-02-02 20:56:36,404 EPOCH 201
2024-02-02 20:56:41,887 Epoch 201: Total Training Recognition Loss 0.05  Total Training Translation Loss 16.78 
2024-02-02 20:56:41,887 EPOCH 202
2024-02-02 20:56:44,239 [Epoch: 202 Step: 00013500] Batch Recognition Loss:   0.000314 => Gls Tokens per Sec:     2208 || Batch Translation Loss:   0.153140 => Txt Tokens per Sec:     6238 || Lr: 0.000100
2024-02-02 20:56:46,993 Epoch 202: Total Training Recognition Loss 0.04  Total Training Translation Loss 15.06 
2024-02-02 20:56:46,993 EPOCH 203
2024-02-02 20:56:51,931 [Epoch: 203 Step: 00013600] Batch Recognition Loss:   0.000272 => Gls Tokens per Sec:     2121 || Batch Translation Loss:   0.233083 => Txt Tokens per Sec:     5863 || Lr: 0.000100
2024-02-02 20:56:52,060 Epoch 203: Total Training Recognition Loss 0.03  Total Training Translation Loss 12.57 
2024-02-02 20:56:52,060 EPOCH 204
2024-02-02 20:56:56,957 Epoch 204: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.49 
2024-02-02 20:56:56,957 EPOCH 205
2024-02-02 20:56:59,488 [Epoch: 205 Step: 00013700] Batch Recognition Loss:   0.000475 => Gls Tokens per Sec:     2023 || Batch Translation Loss:   0.116873 => Txt Tokens per Sec:     5541 || Lr: 0.000100
2024-02-02 20:57:02,412 Epoch 205: Total Training Recognition Loss 0.03  Total Training Translation Loss 10.27 
2024-02-02 20:57:02,412 EPOCH 206
2024-02-02 20:57:07,650 [Epoch: 206 Step: 00013800] Batch Recognition Loss:   0.000515 => Gls Tokens per Sec:     1968 || Batch Translation Loss:   0.072234 => Txt Tokens per Sec:     5479 || Lr: 0.000100
2024-02-02 20:57:07,772 Epoch 206: Total Training Recognition Loss 0.03  Total Training Translation Loss 13.22 
2024-02-02 20:57:07,772 EPOCH 207
2024-02-02 20:57:12,725 Epoch 207: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.37 
2024-02-02 20:57:12,725 EPOCH 208
2024-02-02 20:57:15,378 [Epoch: 208 Step: 00013900] Batch Recognition Loss:   0.000389 => Gls Tokens per Sec:     1836 || Batch Translation Loss:   0.225587 => Txt Tokens per Sec:     5210 || Lr: 0.000100
2024-02-02 20:57:18,197 Epoch 208: Total Training Recognition Loss 0.04  Total Training Translation Loss 8.64 
2024-02-02 20:57:18,197 EPOCH 209
2024-02-02 20:57:23,271 [Epoch: 209 Step: 00014000] Batch Recognition Loss:   0.000499 => Gls Tokens per Sec:     2001 || Batch Translation Loss:   0.130231 => Txt Tokens per Sec:     5579 || Lr: 0.000100
2024-02-02 20:57:31,593 Validation result at epoch 209, step    14000: duration: 8.3217s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00125	Translation Loss: 92781.15625	PPL: 10771.02051
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.93	(BLEU-1: 9.98,	BLEU-2: 3.38,	BLEU-3: 1.62,	BLEU-4: 0.93)
	CHRF 16.52	ROUGE 8.86
2024-02-02 20:57:31,594 Logging Recognition and Translation Outputs
2024-02-02 20:57:31,594 ========================================================================================================================
2024-02-02 20:57:31,594 Logging Sequence: 141_40.00
2024-02-02 20:57:31,594 	Gloss Reference :	A B+C+D+E
2024-02-02 20:57:31,594 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 20:57:31,595 	Gloss Alignment :	         
2024-02-02 20:57:31,595 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 20:57:31,595 	Text Reference  :	got infected with covid-19 he was quarantined and could not take part  in   the   warmup match 
2024-02-02 20:57:31,596 	Text Hypothesis :	*** ******** **** ******** ** *** *********** *** ***** you are  aware that india south  africa
2024-02-02 20:57:31,596 	Text Alignment  :	D   D        D    D        D  D   D           D   D     S   S    S     S    S     S      S     
2024-02-02 20:57:31,596 ========================================================================================================================
2024-02-02 20:57:31,596 Logging Sequence: 117_37.00
2024-02-02 20:57:31,596 	Gloss Reference :	A B+C+D+E
2024-02-02 20:57:31,596 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 20:57:31,596 	Gloss Alignment :	         
2024-02-02 20:57:31,597 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 20:57:31,597 	Text Reference  :	* ** **** shikhar dhawan put  up   a    wonderful performance scoring 98  runs
2024-02-02 20:57:31,598 	Text Hypothesis :	i am sure you     all    must play very well      and         scored  159 runs
2024-02-02 20:57:31,598 	Text Alignment  :	I I  I    S       S      S    S    S    S         S           S       S       
2024-02-02 20:57:31,598 ========================================================================================================================
2024-02-02 20:57:31,598 Logging Sequence: 64_13.00
2024-02-02 20:57:31,598 	Gloss Reference :	A B+C+D+E
2024-02-02 20:57:31,598 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 20:57:31,598 	Gloss Alignment :	         
2024-02-02 20:57:31,598 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 20:57:31,599 	Text Reference  :	*** arrangements were made     to move all the ipl matches to      the  wankhede stadium in  mumbai
2024-02-02 20:57:31,600 	Text Hypothesis :	ipl that         was  supposed to **** *** *** *** ******* promote safe sex      and     all out   
2024-02-02 20:57:31,600 	Text Alignment  :	I   S            S    S           D    D   D   D   D       S       S    S        S       S   S     
2024-02-02 20:57:31,600 ========================================================================================================================
2024-02-02 20:57:31,600 Logging Sequence: 98_121.00
2024-02-02 20:57:31,600 	Gloss Reference :	A B+C+D+E
2024-02-02 20:57:31,600 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 20:57:31,600 	Gloss Alignment :	         
2024-02-02 20:57:31,601 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 20:57:31,601 	Text Reference  :	so then england legends and bangladesh legends were added to the  tournament
2024-02-02 20:57:31,601 	Text Hypothesis :	** **** ******* ******* *** she        did     not  want  to risk this      
2024-02-02 20:57:31,601 	Text Alignment  :	D  D    D       D       D   S          S       S    S        S    S         
2024-02-02 20:57:31,602 ========================================================================================================================
2024-02-02 20:57:31,602 Logging Sequence: 179_414.00
2024-02-02 20:57:31,602 	Gloss Reference :	A B+C+D+E
2024-02-02 20:57:31,602 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 20:57:31,602 	Gloss Alignment :	         
2024-02-02 20:57:31,602 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 20:57:31,604 	Text Reference  :	we could  not  travel to   delhi as  there was  a lockdown in our home town haryana 
2024-02-02 20:57:31,604 	Text Hypothesis :	a  source said vinesh told you   can see   what a ******** ** *** **** sai  decision
2024-02-02 20:57:31,604 	Text Alignment  :	S  S      S    S      S    S     S   S     S      D        D  D   D    S    S       
2024-02-02 20:57:31,604 ========================================================================================================================
2024-02-02 20:57:31,801 Epoch 209: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.96 
2024-02-02 20:57:31,801 EPOCH 210
2024-02-02 20:57:37,152 Epoch 210: Total Training Recognition Loss 0.03  Total Training Translation Loss 11.78 
2024-02-02 20:57:37,152 EPOCH 211
2024-02-02 20:57:39,535 [Epoch: 211 Step: 00014100] Batch Recognition Loss:   0.000365 => Gls Tokens per Sec:     1976 || Batch Translation Loss:   0.156504 => Txt Tokens per Sec:     5617 || Lr: 0.000100
2024-02-02 20:57:42,435 Epoch 211: Total Training Recognition Loss 0.04  Total Training Translation Loss 13.06 
2024-02-02 20:57:42,435 EPOCH 212
2024-02-02 20:57:47,049 [Epoch: 212 Step: 00014200] Batch Recognition Loss:   0.000295 => Gls Tokens per Sec:     2166 || Batch Translation Loss:   0.174237 => Txt Tokens per Sec:     6016 || Lr: 0.000100
2024-02-02 20:57:47,305 Epoch 212: Total Training Recognition Loss 0.03  Total Training Translation Loss 15.57 
2024-02-02 20:57:47,305 EPOCH 213
2024-02-02 20:57:52,877 Epoch 213: Total Training Recognition Loss 0.04  Total Training Translation Loss 12.67 
2024-02-02 20:57:52,878 EPOCH 214
2024-02-02 20:57:54,820 [Epoch: 214 Step: 00014300] Batch Recognition Loss:   0.000325 => Gls Tokens per Sec:     2391 || Batch Translation Loss:   0.142808 => Txt Tokens per Sec:     6480 || Lr: 0.000100
2024-02-02 20:57:58,051 Epoch 214: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.57 
2024-02-02 20:57:58,052 EPOCH 215
2024-02-02 20:58:02,988 [Epoch: 215 Step: 00014400] Batch Recognition Loss:   0.000821 => Gls Tokens per Sec:     1992 || Batch Translation Loss:   0.029238 => Txt Tokens per Sec:     5554 || Lr: 0.000100
2024-02-02 20:58:03,361 Epoch 215: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.44 
2024-02-02 20:58:03,361 EPOCH 216
2024-02-02 20:58:08,700 Epoch 216: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.08 
2024-02-02 20:58:08,701 EPOCH 217
2024-02-02 20:58:10,716 [Epoch: 217 Step: 00014500] Batch Recognition Loss:   0.000676 => Gls Tokens per Sec:     2180 || Batch Translation Loss:   0.050404 => Txt Tokens per Sec:     5885 || Lr: 0.000100
2024-02-02 20:58:13,727 Epoch 217: Total Training Recognition Loss 0.04  Total Training Translation Loss 11.09 
2024-02-02 20:58:13,727 EPOCH 218
2024-02-02 20:58:18,523 [Epoch: 218 Step: 00014600] Batch Recognition Loss:   0.000281 => Gls Tokens per Sec:     2017 || Batch Translation Loss:   0.109797 => Txt Tokens per Sec:     5617 || Lr: 0.000100
2024-02-02 20:58:18,949 Epoch 218: Total Training Recognition Loss 0.04  Total Training Translation Loss 9.40 
2024-02-02 20:58:18,949 EPOCH 219
2024-02-02 20:58:24,267 Epoch 219: Total Training Recognition Loss 0.02  Total Training Translation Loss 8.69 
2024-02-02 20:58:24,267 EPOCH 220
2024-02-02 20:58:26,349 [Epoch: 220 Step: 00014700] Batch Recognition Loss:   0.000237 => Gls Tokens per Sec:     2034 || Batch Translation Loss:   0.050007 => Txt Tokens per Sec:     5479 || Lr: 0.000100
2024-02-02 20:58:29,842 Epoch 220: Total Training Recognition Loss 0.02  Total Training Translation Loss 9.60 
2024-02-02 20:58:29,843 EPOCH 221
2024-02-02 20:58:34,750 [Epoch: 221 Step: 00014800] Batch Recognition Loss:   0.000371 => Gls Tokens per Sec:     1938 || Batch Translation Loss:   0.059218 => Txt Tokens per Sec:     5387 || Lr: 0.000100
2024-02-02 20:58:35,289 Epoch 221: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.59 
2024-02-02 20:58:35,289 EPOCH 222
2024-02-02 20:58:40,550 Epoch 222: Total Training Recognition Loss 0.03  Total Training Translation Loss 12.78 
2024-02-02 20:58:40,550 EPOCH 223
2024-02-02 20:58:42,447 [Epoch: 223 Step: 00014900] Batch Recognition Loss:   0.000488 => Gls Tokens per Sec:     2146 || Batch Translation Loss:   0.413533 => Txt Tokens per Sec:     5455 || Lr: 0.000100
2024-02-02 20:58:46,042 Epoch 223: Total Training Recognition Loss 0.04  Total Training Translation Loss 12.95 
2024-02-02 20:58:46,042 EPOCH 224
2024-02-02 20:58:50,256 [Epoch: 224 Step: 00015000] Batch Recognition Loss:   0.000534 => Gls Tokens per Sec:     2219 || Batch Translation Loss:   0.220927 => Txt Tokens per Sec:     6196 || Lr: 0.000100
2024-02-02 20:58:51,015 Epoch 224: Total Training Recognition Loss 0.02  Total Training Translation Loss 13.33 
2024-02-02 20:58:51,016 EPOCH 225
2024-02-02 20:58:56,373 Epoch 225: Total Training Recognition Loss 0.04  Total Training Translation Loss 12.19 
2024-02-02 20:58:56,374 EPOCH 226
2024-02-02 20:58:58,035 [Epoch: 226 Step: 00015100] Batch Recognition Loss:   0.000295 => Gls Tokens per Sec:     2410 || Batch Translation Loss:   0.072437 => Txt Tokens per Sec:     6504 || Lr: 0.000100
2024-02-02 20:59:01,493 Epoch 226: Total Training Recognition Loss 0.03  Total Training Translation Loss 13.31 
2024-02-02 20:59:01,493 EPOCH 227
2024-02-02 20:59:06,036 [Epoch: 227 Step: 00015200] Batch Recognition Loss:   0.000572 => Gls Tokens per Sec:     2023 || Batch Translation Loss:   0.117165 => Txt Tokens per Sec:     5592 || Lr: 0.000100
2024-02-02 20:59:06,782 Epoch 227: Total Training Recognition Loss 0.03  Total Training Translation Loss 13.10 
2024-02-02 20:59:06,782 EPOCH 228
2024-02-02 20:59:12,210 Epoch 228: Total Training Recognition Loss 0.04  Total Training Translation Loss 14.81 
2024-02-02 20:59:12,211 EPOCH 229
2024-02-02 20:59:13,891 [Epoch: 229 Step: 00015300] Batch Recognition Loss:   0.000457 => Gls Tokens per Sec:     2288 || Batch Translation Loss:   0.127695 => Txt Tokens per Sec:     6262 || Lr: 0.000100
2024-02-02 20:59:17,423 Epoch 229: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.86 
2024-02-02 20:59:17,424 EPOCH 230
2024-02-02 20:59:22,141 [Epoch: 230 Step: 00015400] Batch Recognition Loss:   0.000526 => Gls Tokens per Sec:     1915 || Batch Translation Loss:   0.068728 => Txt Tokens per Sec:     5368 || Lr: 0.000100
2024-02-02 20:59:22,890 Epoch 230: Total Training Recognition Loss 0.04  Total Training Translation Loss 10.42 
2024-02-02 20:59:22,890 EPOCH 231
2024-02-02 20:59:28,441 Epoch 231: Total Training Recognition Loss 0.04  Total Training Translation Loss 9.91 
2024-02-02 20:59:28,442 EPOCH 232
2024-02-02 20:59:30,207 [Epoch: 232 Step: 00015500] Batch Recognition Loss:   0.000461 => Gls Tokens per Sec:     2086 || Batch Translation Loss:   0.084399 => Txt Tokens per Sec:     5633 || Lr: 0.000100
2024-02-02 20:59:33,696 Epoch 232: Total Training Recognition Loss 0.03  Total Training Translation Loss 11.28 
2024-02-02 20:59:33,696 EPOCH 233
2024-02-02 20:59:37,913 [Epoch: 233 Step: 00015600] Batch Recognition Loss:   0.000350 => Gls Tokens per Sec:     2104 || Batch Translation Loss:   0.124577 => Txt Tokens per Sec:     5851 || Lr: 0.000100
2024-02-02 20:59:38,777 Epoch 233: Total Training Recognition Loss 0.02  Total Training Translation Loss 10.44 
2024-02-02 20:59:38,777 EPOCH 234
2024-02-02 20:59:43,874 Epoch 234: Total Training Recognition Loss 0.03  Total Training Translation Loss 12.13 
2024-02-02 20:59:43,875 EPOCH 235
2024-02-02 20:59:45,364 [Epoch: 235 Step: 00015700] Batch Recognition Loss:   0.000356 => Gls Tokens per Sec:     2364 || Batch Translation Loss:   0.236447 => Txt Tokens per Sec:     6403 || Lr: 0.000100
2024-02-02 20:59:49,308 Epoch 235: Total Training Recognition Loss 0.03  Total Training Translation Loss 12.43 
2024-02-02 20:59:49,309 EPOCH 236
2024-02-02 20:59:53,401 [Epoch: 236 Step: 00015800] Batch Recognition Loss:   0.000399 => Gls Tokens per Sec:     2129 || Batch Translation Loss:   0.330331 => Txt Tokens per Sec:     5831 || Lr: 0.000100
2024-02-02 20:59:54,417 Epoch 236: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.03 
2024-02-02 20:59:54,417 EPOCH 237
2024-02-02 21:00:00,066 Epoch 237: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.24 
2024-02-02 21:00:00,067 EPOCH 238
2024-02-02 21:00:01,738 [Epoch: 238 Step: 00015900] Batch Recognition Loss:   0.000301 => Gls Tokens per Sec:     1958 || Batch Translation Loss:   0.073930 => Txt Tokens per Sec:     5646 || Lr: 0.000100
2024-02-02 21:00:05,497 Epoch 238: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.57 
2024-02-02 21:00:05,498 EPOCH 239
2024-02-02 21:00:09,914 [Epoch: 239 Step: 00016000] Batch Recognition Loss:   0.000520 => Gls Tokens per Sec:     1957 || Batch Translation Loss:   0.043159 => Txt Tokens per Sec:     5455 || Lr: 0.000100
2024-02-02 21:00:18,157 Validation result at epoch 239, step    16000: duration: 8.2430s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00202	Translation Loss: 94566.32812	PPL: 12877.75781
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.56	(BLEU-1: 11.45,	BLEU-2: 3.55,	BLEU-3: 1.21,	BLEU-4: 0.56)
	CHRF 17.28	ROUGE 9.72
2024-02-02 21:00:18,158 Logging Recognition and Translation Outputs
2024-02-02 21:00:18,158 ========================================================================================================================
2024-02-02 21:00:18,158 Logging Sequence: 147_132.00
2024-02-02 21:00:18,158 	Gloss Reference :	A B+C+D+E
2024-02-02 21:00:18,159 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:00:18,159 	Gloss Alignment :	         
2024-02-02 21:00:18,159 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:00:18,160 	Text Reference  :	*** **** i      can not     earlier    i   used to  have fun in  gymnastics
2024-02-02 21:00:18,160 	Text Hypothesis :	the next wanted to  promote positivity and they had sent her win this      
2024-02-02 21:00:18,160 	Text Alignment  :	I   I    S      S   S       S          S   S    S   S    S   S   S         
2024-02-02 21:00:18,160 ========================================================================================================================
2024-02-02 21:00:18,160 Logging Sequence: 116_162.00
2024-02-02 21:00:18,161 	Gloss Reference :	A B+C+D+E
2024-02-02 21:00:18,161 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:00:18,161 	Gloss Alignment :	         
2024-02-02 21:00:18,161 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:00:18,162 	Text Reference  :	***** ******* **** turned       out the **** video was  shared on social media by a    staff    at   the  hotel   
2024-02-02 21:00:18,163 	Text Hypothesis :	seven players also participated in  the same room  with going  on ****** ***** ** 12th november 2022 this happened
2024-02-02 21:00:18,163 	Text Alignment  :	I     I       I    S            S       I    S     S    S         D      D     D  S    S        S    S    S       
2024-02-02 21:00:18,163 ========================================================================================================================
2024-02-02 21:00:18,163 Logging Sequence: 73_79.00
2024-02-02 21:00:18,163 	Gloss Reference :	A B+C+D+E
2024-02-02 21:00:18,163 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:00:18,163 	Gloss Alignment :	         
2024-02-02 21:00:18,163 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:00:18,165 	Text Reference  :	raina resturant has food from the rich spices of north india to  the aromatic curries of     south  india      
2024-02-02 21:00:18,165 	Text Hypothesis :	***** ********* *** **** **** the **** ****** ** match has   now 8   things   as      'raina indian restaurant'
2024-02-02 21:00:18,165 	Text Alignment  :	D     D         D   D    D        D    D      D  S     S     S   S   S        S       S      S      S          
2024-02-02 21:00:18,165 ========================================================================================================================
2024-02-02 21:00:18,165 Logging Sequence: 165_523.00
2024-02-02 21:00:18,165 	Gloss Reference :	A B+C+D+E
2024-02-02 21:00:18,165 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:00:18,166 	Gloss Alignment :	         
2024-02-02 21:00:18,166 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:00:18,167 	Text Reference  :	as he believed that his team might lose if   he      takes off      his batting pads 
2024-02-02 21:00:18,167 	Text Hypothesis :	** ** ******** **** *** but  we    have been sharing their emotions on  social  media
2024-02-02 21:00:18,167 	Text Alignment  :	D  D  D        D    D   S    S     S    S    S       S     S        S   S       S    
2024-02-02 21:00:18,167 ========================================================================================================================
2024-02-02 21:00:18,167 Logging Sequence: 125_72.00
2024-02-02 21:00:18,167 	Gloss Reference :	A B+C+D+E
2024-02-02 21:00:18,168 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:00:18,168 	Gloss Alignment :	         
2024-02-02 21:00:18,168 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:00:18,169 	Text Reference  :	some said the pakistani javelineer had  milicious intentions of   tampering with the javelin out of jealousy   
2024-02-02 21:00:18,169 	Text Hypothesis :	**** **** *** new       zealand    lost by        372        runs and       euro the ******* *** ** semi-finals
2024-02-02 21:00:18,169 	Text Alignment  :	D    D    D   S         S          S    S         S          S    S         S        D       D   D  S          
2024-02-02 21:00:18,169 ========================================================================================================================
2024-02-02 21:00:19,215 Epoch 239: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.23 
2024-02-02 21:00:19,216 EPOCH 240
2024-02-02 21:00:24,817 Epoch 240: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.21 
2024-02-02 21:00:24,818 EPOCH 241
2024-02-02 21:00:26,260 [Epoch: 241 Step: 00016100] Batch Recognition Loss:   0.000440 => Gls Tokens per Sec:     2223 || Batch Translation Loss:   0.116253 => Txt Tokens per Sec:     5835 || Lr: 0.000100
2024-02-02 21:00:29,855 Epoch 241: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.78 
2024-02-02 21:00:29,855 EPOCH 242
2024-02-02 21:00:34,362 [Epoch: 242 Step: 00016200] Batch Recognition Loss:   0.000509 => Gls Tokens per Sec:     1862 || Batch Translation Loss:   0.093814 => Txt Tokens per Sec:     5265 || Lr: 0.000100
2024-02-02 21:00:35,366 Epoch 242: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.28 
2024-02-02 21:00:35,367 EPOCH 243
2024-02-02 21:00:40,552 Epoch 243: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.98 
2024-02-02 21:00:40,553 EPOCH 244
2024-02-02 21:00:42,109 [Epoch: 244 Step: 00016300] Batch Recognition Loss:   0.000343 => Gls Tokens per Sec:     1956 || Batch Translation Loss:   0.089489 => Txt Tokens per Sec:     5358 || Lr: 0.000100
2024-02-02 21:00:45,859 Epoch 244: Total Training Recognition Loss 0.02  Total Training Translation Loss 10.73 
2024-02-02 21:00:45,860 EPOCH 245
2024-02-02 21:00:49,959 [Epoch: 245 Step: 00016400] Batch Recognition Loss:   0.000409 => Gls Tokens per Sec:     2008 || Batch Translation Loss:   0.405465 => Txt Tokens per Sec:     5638 || Lr: 0.000100
2024-02-02 21:00:51,061 Epoch 245: Total Training Recognition Loss 0.03  Total Training Translation Loss 11.39 
2024-02-02 21:00:51,061 EPOCH 246
2024-02-02 21:00:56,171 Epoch 246: Total Training Recognition Loss 0.04  Total Training Translation Loss 12.30 
2024-02-02 21:00:56,172 EPOCH 247
2024-02-02 21:00:57,271 [Epoch: 247 Step: 00016500] Batch Recognition Loss:   0.000412 => Gls Tokens per Sec:     2620 || Batch Translation Loss:   0.173547 => Txt Tokens per Sec:     6967 || Lr: 0.000100
2024-02-02 21:01:01,435 Epoch 247: Total Training Recognition Loss 0.03  Total Training Translation Loss 12.44 
2024-02-02 21:01:01,436 EPOCH 248
2024-02-02 21:01:05,232 [Epoch: 248 Step: 00016600] Batch Recognition Loss:   0.000557 => Gls Tokens per Sec:     2127 || Batch Translation Loss:   0.412658 => Txt Tokens per Sec:     5715 || Lr: 0.000100
2024-02-02 21:01:06,921 Epoch 248: Total Training Recognition Loss 0.03  Total Training Translation Loss 11.36 
2024-02-02 21:01:06,922 EPOCH 249
2024-02-02 21:01:12,343 Epoch 249: Total Training Recognition Loss 0.02  Total Training Translation Loss 11.89 
2024-02-02 21:01:12,344 EPOCH 250
2024-02-02 21:01:13,681 [Epoch: 250 Step: 00016700] Batch Recognition Loss:   0.000570 => Gls Tokens per Sec:     1967 || Batch Translation Loss:   0.132740 => Txt Tokens per Sec:     5378 || Lr: 0.000100
2024-02-02 21:01:17,867 Epoch 250: Total Training Recognition Loss 0.03  Total Training Translation Loss 10.96 
2024-02-02 21:01:17,868 EPOCH 251
2024-02-02 21:01:21,742 [Epoch: 251 Step: 00016800] Batch Recognition Loss:   0.000317 => Gls Tokens per Sec:     2042 || Batch Translation Loss:   0.131842 => Txt Tokens per Sec:     5706 || Lr: 0.000100
2024-02-02 21:01:23,084 Epoch 251: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.49 
2024-02-02 21:01:23,084 EPOCH 252
2024-02-02 21:01:28,614 Epoch 252: Total Training Recognition Loss 0.03  Total Training Translation Loss 11.29 
2024-02-02 21:01:28,614 EPOCH 253
2024-02-02 21:01:29,734 [Epoch: 253 Step: 00016900] Batch Recognition Loss:   0.000329 => Gls Tokens per Sec:     2288 || Batch Translation Loss:   0.256906 => Txt Tokens per Sec:     6162 || Lr: 0.000100
2024-02-02 21:01:33,866 Epoch 253: Total Training Recognition Loss 0.03  Total Training Translation Loss 12.03 
2024-02-02 21:01:33,867 EPOCH 254
2024-02-02 21:01:37,523 [Epoch: 254 Step: 00017000] Batch Recognition Loss:   0.000604 => Gls Tokens per Sec:     2121 || Batch Translation Loss:   0.154867 => Txt Tokens per Sec:     5621 || Lr: 0.000100
2024-02-02 21:01:39,290 Epoch 254: Total Training Recognition Loss 0.04  Total Training Translation Loss 14.47 
2024-02-02 21:01:39,290 EPOCH 255
2024-02-02 21:01:44,674 Epoch 255: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.13 
2024-02-02 21:01:44,674 EPOCH 256
2024-02-02 21:01:45,962 [Epoch: 256 Step: 00017100] Batch Recognition Loss:   0.000188 => Gls Tokens per Sec:     1865 || Batch Translation Loss:   0.093770 => Txt Tokens per Sec:     5256 || Lr: 0.000100
2024-02-02 21:01:49,904 Epoch 256: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.68 
2024-02-02 21:01:49,904 EPOCH 257
2024-02-02 21:01:54,021 [Epoch: 257 Step: 00017200] Batch Recognition Loss:   0.000189 => Gls Tokens per Sec:     1866 || Batch Translation Loss:   0.076349 => Txt Tokens per Sec:     5154 || Lr: 0.000100
2024-02-02 21:01:55,572 Epoch 257: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.87 
2024-02-02 21:01:55,573 EPOCH 258
2024-02-02 21:02:00,985 Epoch 258: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.08 
2024-02-02 21:02:00,986 EPOCH 259
2024-02-02 21:02:02,090 [Epoch: 259 Step: 00017300] Batch Recognition Loss:   0.000220 => Gls Tokens per Sec:     2031 || Batch Translation Loss:   0.164139 => Txt Tokens per Sec:     5717 || Lr: 0.000100
2024-02-02 21:02:06,394 Epoch 259: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.29 
2024-02-02 21:02:06,395 EPOCH 260
2024-02-02 21:02:09,809 [Epoch: 260 Step: 00017400] Batch Recognition Loss:   0.000166 => Gls Tokens per Sec:     2177 || Batch Translation Loss:   0.239865 => Txt Tokens per Sec:     6096 || Lr: 0.000100
2024-02-02 21:02:11,330 Epoch 260: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.05 
2024-02-02 21:02:11,330 EPOCH 261
2024-02-02 21:02:16,764 Epoch 261: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.34 
2024-02-02 21:02:16,765 EPOCH 262
2024-02-02 21:02:17,716 [Epoch: 262 Step: 00017500] Batch Recognition Loss:   0.000965 => Gls Tokens per Sec:     2192 || Batch Translation Loss:   0.037492 => Txt Tokens per Sec:     6552 || Lr: 0.000100
2024-02-02 21:02:22,052 Epoch 262: Total Training Recognition Loss 0.03  Total Training Translation Loss 10.39 
2024-02-02 21:02:22,053 EPOCH 263
2024-02-02 21:02:25,716 [Epoch: 263 Step: 00017600] Batch Recognition Loss:   0.000287 => Gls Tokens per Sec:     2009 || Batch Translation Loss:   0.254851 => Txt Tokens per Sec:     5791 || Lr: 0.000100
2024-02-02 21:02:27,064 Epoch 263: Total Training Recognition Loss 0.03  Total Training Translation Loss 13.15 
2024-02-02 21:02:27,064 EPOCH 264
2024-02-02 21:02:32,584 Epoch 264: Total Training Recognition Loss 0.03  Total Training Translation Loss 10.13 
2024-02-02 21:02:32,585 EPOCH 265
2024-02-02 21:02:33,429 [Epoch: 265 Step: 00017700] Batch Recognition Loss:   0.000779 => Gls Tokens per Sec:     2277 || Batch Translation Loss:   0.209223 => Txt Tokens per Sec:     6113 || Lr: 0.000100
2024-02-02 21:02:37,674 Epoch 265: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.82 
2024-02-02 21:02:37,675 EPOCH 266
2024-02-02 21:02:41,362 [Epoch: 266 Step: 00017800] Batch Recognition Loss:   0.000224 => Gls Tokens per Sec:     1929 || Batch Translation Loss:   0.060005 => Txt Tokens per Sec:     5337 || Lr: 0.000100
2024-02-02 21:02:43,248 Epoch 266: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.77 
2024-02-02 21:02:43,248 EPOCH 267
2024-02-02 21:02:48,809 Epoch 267: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.33 
2024-02-02 21:02:48,809 EPOCH 268
2024-02-02 21:02:49,726 [Epoch: 268 Step: 00017900] Batch Recognition Loss:   0.000509 => Gls Tokens per Sec:     1921 || Batch Translation Loss:   0.075562 => Txt Tokens per Sec:     5390 || Lr: 0.000100
2024-02-02 21:02:54,248 Epoch 268: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.46 
2024-02-02 21:02:54,249 EPOCH 269
2024-02-02 21:02:57,779 [Epoch: 269 Step: 00018000] Batch Recognition Loss:   0.000476 => Gls Tokens per Sec:     1970 || Batch Translation Loss:   0.152828 => Txt Tokens per Sec:     5463 || Lr: 0.000100
2024-02-02 21:03:05,972 Validation result at epoch 269, step    18000: duration: 8.1913s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00130	Translation Loss: 94472.32031	PPL: 12757.18457
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.75	(BLEU-1: 10.70,	BLEU-2: 3.39,	BLEU-3: 1.46,	BLEU-4: 0.75)
	CHRF 16.78	ROUGE 9.48
2024-02-02 21:03:05,973 Logging Recognition and Translation Outputs
2024-02-02 21:03:05,973 ========================================================================================================================
2024-02-02 21:03:05,973 Logging Sequence: 155_119.00
2024-02-02 21:03:05,973 	Gloss Reference :	A B+C+D+E
2024-02-02 21:03:05,973 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:03:05,974 	Gloss Alignment :	         
2024-02-02 21:03:05,974 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:03:05,975 	Text Reference  :	a report said that the  taliban wanted icc   to replace the afghan flag with its own       
2024-02-02 21:03:05,975 	Text Hypothesis :	* ****** and  was  born on      social media to ******* *** win    they lost the tournament
2024-02-02 21:03:05,975 	Text Alignment  :	D D      S    S    S    S       S      S        D       D   S      S    S    S   S         
2024-02-02 21:03:05,976 ========================================================================================================================
2024-02-02 21:03:05,976 Logging Sequence: 153_43.00
2024-02-02 21:03:05,976 	Gloss Reference :	A B+C+D+E
2024-02-02 21:03:05,976 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:03:05,976 	Gloss Alignment :	         
2024-02-02 21:03:05,977 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:03:05,978 	Text Reference  :	***** ** ******* ***** *** these runs were all  because of hardik pandya   and virat kohli  
2024-02-02 21:03:05,978 	Text Hypothesis :	india is because dhoni was not   in   the  same ritual  of the    security of  the   british
2024-02-02 21:03:05,978 	Text Alignment  :	I     I  I       I     I   S     S    S    S    S          S      S        S   S     S      
2024-02-02 21:03:05,978 ========================================================================================================================
2024-02-02 21:03:05,978 Logging Sequence: 150_35.00
2024-02-02 21:03:05,978 	Gloss Reference :	A B+C+D+E
2024-02-02 21:03:05,978 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:03:05,979 	Gloss Alignment :	         
2024-02-02 21:03:05,979 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:03:05,979 	Text Reference  :	*** wow  india football team    is     really     strong   
2024-02-02 21:03:05,979 	Text Hypothesis :	one must not   venture  outside unless absolutely necessary
2024-02-02 21:03:05,980 	Text Alignment  :	I   S    S     S        S       S      S          S        
2024-02-02 21:03:05,980 ========================================================================================================================
2024-02-02 21:03:05,980 Logging Sequence: 146_154.00
2024-02-02 21:03:05,980 	Gloss Reference :	A B+C+D+E
2024-02-02 21:03:05,980 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:03:05,980 	Gloss Alignment :	         
2024-02-02 21:03:05,981 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:03:05,982 	Text Reference  :	*** bwf    said that testing protocols have   been implemented to      ensure the  health and safety of all participants
2024-02-02 21:03:05,982 	Text Hypothesis :	the second time that ******* ********* taylor came in          england and    held every  4   years  of the world       
2024-02-02 21:03:05,982 	Text Alignment  :	I   S      S         D       D         S      S    S           S       S      S    S      S   S         S   S           
2024-02-02 21:03:05,983 ========================================================================================================================
2024-02-02 21:03:05,983 Logging Sequence: 76_79.00
2024-02-02 21:03:05,983 	Gloss Reference :	A B+C+D+E
2024-02-02 21:03:05,983 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:03:05,983 	Gloss Alignment :	         
2024-02-02 21:03:05,983 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:03:05,984 	Text Reference  :	** speaking to        ani csk       ceo kasi   viswanathan said 
2024-02-02 21:03:05,984 	Text Hypothesis :	on 23rd     september the president of  mumbai the         match
2024-02-02 21:03:05,984 	Text Alignment  :	I  S        S         S   S         S   S      S           S    
2024-02-02 21:03:05,984 ========================================================================================================================
2024-02-02 21:03:07,732 Epoch 269: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.00 
2024-02-02 21:03:07,732 EPOCH 270
2024-02-02 21:03:13,290 Epoch 270: Total Training Recognition Loss 0.02  Total Training Translation Loss 13.63 
2024-02-02 21:03:13,291 EPOCH 271
2024-02-02 21:03:13,983 [Epoch: 271 Step: 00018100] Batch Recognition Loss:   0.001237 => Gls Tokens per Sec:     2185 || Batch Translation Loss:   0.148006 => Txt Tokens per Sec:     5401 || Lr: 0.000100
2024-02-02 21:03:18,193 Epoch 271: Total Training Recognition Loss 0.04  Total Training Translation Loss 11.28 
2024-02-02 21:03:18,193 EPOCH 272
2024-02-02 21:03:21,745 [Epoch: 272 Step: 00018200] Batch Recognition Loss:   0.000424 => Gls Tokens per Sec:     1912 || Batch Translation Loss:   0.095735 => Txt Tokens per Sec:     5156 || Lr: 0.000100
2024-02-02 21:03:23,742 Epoch 272: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.93 
2024-02-02 21:03:23,742 EPOCH 273
2024-02-02 21:03:28,732 Epoch 273: Total Training Recognition Loss 0.02  Total Training Translation Loss 9.31 
2024-02-02 21:03:28,733 EPOCH 274
2024-02-02 21:03:29,492 [Epoch: 274 Step: 00018300] Batch Recognition Loss:   0.000543 => Gls Tokens per Sec:     1901 || Batch Translation Loss:   0.093934 => Txt Tokens per Sec:     5159 || Lr: 0.000100
2024-02-02 21:03:34,245 Epoch 274: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.09 
2024-02-02 21:03:34,246 EPOCH 275
2024-02-02 21:03:37,209 [Epoch: 275 Step: 00018400] Batch Recognition Loss:   0.000186 => Gls Tokens per Sec:     2238 || Batch Translation Loss:   0.057357 => Txt Tokens per Sec:     6122 || Lr: 0.000100
2024-02-02 21:03:39,451 Epoch 275: Total Training Recognition Loss 0.02  Total Training Translation Loss 8.15 
2024-02-02 21:03:39,451 EPOCH 276
2024-02-02 21:03:44,793 Epoch 276: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.93 
2024-02-02 21:03:44,793 EPOCH 277
2024-02-02 21:03:45,470 [Epoch: 277 Step: 00018500] Batch Recognition Loss:   0.000429 => Gls Tokens per Sec:     1895 || Batch Translation Loss:   0.048512 => Txt Tokens per Sec:     5495 || Lr: 0.000100
2024-02-02 21:03:50,337 Epoch 277: Total Training Recognition Loss 0.02  Total Training Translation Loss 8.03 
2024-02-02 21:03:50,338 EPOCH 278
2024-02-02 21:03:53,562 [Epoch: 278 Step: 00018600] Batch Recognition Loss:   0.000383 => Gls Tokens per Sec:     2007 || Batch Translation Loss:   0.188316 => Txt Tokens per Sec:     5402 || Lr: 0.000100
2024-02-02 21:03:55,961 Epoch 278: Total Training Recognition Loss 0.06  Total Training Translation Loss 9.64 
2024-02-02 21:03:55,961 EPOCH 279
2024-02-02 21:04:01,355 Epoch 279: Total Training Recognition Loss 0.02  Total Training Translation Loss 8.02 
2024-02-02 21:04:01,355 EPOCH 280
2024-02-02 21:04:02,062 [Epoch: 280 Step: 00018700] Batch Recognition Loss:   0.000446 => Gls Tokens per Sec:     1586 || Batch Translation Loss:   0.420716 => Txt Tokens per Sec:     4969 || Lr: 0.000100
2024-02-02 21:04:06,930 Epoch 280: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.43 
2024-02-02 21:04:06,930 EPOCH 281
2024-02-02 21:04:09,886 [Epoch: 281 Step: 00018800] Batch Recognition Loss:   0.000347 => Gls Tokens per Sec:     2166 || Batch Translation Loss:   0.087337 => Txt Tokens per Sec:     6058 || Lr: 0.000100
2024-02-02 21:04:12,090 Epoch 281: Total Training Recognition Loss 0.02  Total Training Translation Loss 10.55 
2024-02-02 21:04:12,091 EPOCH 282
2024-02-02 21:04:17,412 Epoch 282: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.71 
2024-02-02 21:04:17,413 EPOCH 283
2024-02-02 21:04:17,925 [Epoch: 283 Step: 00018900] Batch Recognition Loss:   0.000325 => Gls Tokens per Sec:     1882 || Batch Translation Loss:   0.124575 => Txt Tokens per Sec:     5578 || Lr: 0.000100
2024-02-02 21:04:22,294 Epoch 283: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.75 
2024-02-02 21:04:22,294 EPOCH 284
2024-02-02 21:04:25,458 [Epoch: 284 Step: 00019000] Batch Recognition Loss:   0.000348 => Gls Tokens per Sec:     1944 || Batch Translation Loss:   0.300879 => Txt Tokens per Sec:     5193 || Lr: 0.000100
2024-02-02 21:04:27,809 Epoch 284: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.91 
2024-02-02 21:04:27,809 EPOCH 285
2024-02-02 21:04:32,668 Epoch 285: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.59 
2024-02-02 21:04:32,669 EPOCH 286
2024-02-02 21:04:33,035 [Epoch: 286 Step: 00019100] Batch Recognition Loss:   0.000182 => Gls Tokens per Sec:     2192 || Batch Translation Loss:   0.276311 => Txt Tokens per Sec:     5830 || Lr: 0.000100
2024-02-02 21:04:38,121 Epoch 286: Total Training Recognition Loss 0.03  Total Training Translation Loss 12.66 
2024-02-02 21:04:38,121 EPOCH 287
2024-02-02 21:04:40,915 [Epoch: 287 Step: 00019200] Batch Recognition Loss:   0.000251 => Gls Tokens per Sec:     2145 || Batch Translation Loss:   0.032913 => Txt Tokens per Sec:     6171 || Lr: 0.000100
2024-02-02 21:04:42,702 Epoch 287: Total Training Recognition Loss 0.04  Total Training Translation Loss 11.95 
2024-02-02 21:04:42,702 EPOCH 288
2024-02-02 21:04:48,057 Epoch 288: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.73 
2024-02-02 21:04:48,058 EPOCH 289
2024-02-02 21:04:48,319 [Epoch: 289 Step: 00019300] Batch Recognition Loss:   0.000187 => Gls Tokens per Sec:     2462 || Batch Translation Loss:   0.107137 => Txt Tokens per Sec:     6400 || Lr: 0.000100
2024-02-02 21:04:53,443 Epoch 289: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.66 
2024-02-02 21:04:53,444 EPOCH 290
2024-02-02 21:04:56,154 [Epoch: 290 Step: 00019400] Batch Recognition Loss:   0.000297 => Gls Tokens per Sec:     2151 || Batch Translation Loss:   0.210426 => Txt Tokens per Sec:     5665 || Lr: 0.000100
2024-02-02 21:04:58,694 Epoch 290: Total Training Recognition Loss 0.02  Total Training Translation Loss 8.41 
2024-02-02 21:04:58,695 EPOCH 291
2024-02-02 21:05:03,940 Epoch 291: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.21 
2024-02-02 21:05:03,940 EPOCH 292
2024-02-02 21:05:04,183 [Epoch: 292 Step: 00019500] Batch Recognition Loss:   0.000291 => Gls Tokens per Sec:     1984 || Batch Translation Loss:   0.045423 => Txt Tokens per Sec:     5595 || Lr: 0.000100
2024-02-02 21:05:09,205 Epoch 292: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.37 
2024-02-02 21:05:09,205 EPOCH 293
2024-02-02 21:05:11,991 [Epoch: 293 Step: 00019600] Batch Recognition Loss:   0.000347 => Gls Tokens per Sec:     2068 || Batch Translation Loss:   0.346467 => Txt Tokens per Sec:     5652 || Lr: 0.000100
2024-02-02 21:05:14,532 Epoch 293: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.29 
2024-02-02 21:05:14,532 EPOCH 294
2024-02-02 21:05:19,569 Epoch 294: Total Training Recognition Loss 0.02  Total Training Translation Loss 10.34 
2024-02-02 21:05:19,569 EPOCH 295
2024-02-02 21:05:19,718 [Epoch: 295 Step: 00019700] Batch Recognition Loss:   0.000223 => Gls Tokens per Sec:     2162 || Batch Translation Loss:   0.307749 => Txt Tokens per Sec:     6905 || Lr: 0.000100
2024-02-02 21:05:24,954 Epoch 295: Total Training Recognition Loss 0.03  Total Training Translation Loss 11.02 
2024-02-02 21:05:24,955 EPOCH 296
2024-02-02 21:05:27,716 [Epoch: 296 Step: 00019800] Batch Recognition Loss:   0.000300 => Gls Tokens per Sec:     2030 || Batch Translation Loss:   0.166933 => Txt Tokens per Sec:     5607 || Lr: 0.000100
2024-02-02 21:05:30,473 Epoch 296: Total Training Recognition Loss 0.03  Total Training Translation Loss 10.55 
2024-02-02 21:05:30,474 EPOCH 297
2024-02-02 21:05:35,817 Epoch 297: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.56 
2024-02-02 21:05:35,818 EPOCH 298
2024-02-02 21:05:35,879 [Epoch: 298 Step: 00019900] Batch Recognition Loss:   0.000352 => Gls Tokens per Sec:     2623 || Batch Translation Loss:   0.093851 => Txt Tokens per Sec:     5836 || Lr: 0.000100
2024-02-02 21:05:40,984 Epoch 298: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.16 
2024-02-02 21:05:40,984 EPOCH 299
2024-02-02 21:05:43,715 [Epoch: 299 Step: 00020000] Batch Recognition Loss:   0.000593 => Gls Tokens per Sec:     1992 || Batch Translation Loss:   0.037426 => Txt Tokens per Sec:     5582 || Lr: 0.000100
2024-02-02 21:05:51,777 Validation result at epoch 299, step    20000: duration: 8.0608s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00196	Translation Loss: 95074.60938	PPL: 13549.71875
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.66	(BLEU-1: 10.55,	BLEU-2: 3.62,	BLEU-3: 1.47,	BLEU-4: 0.66)
	CHRF 17.13	ROUGE 9.27
2024-02-02 21:05:51,779 Logging Recognition and Translation Outputs
2024-02-02 21:05:51,779 ========================================================================================================================
2024-02-02 21:05:51,779 Logging Sequence: 174_121.00
2024-02-02 21:05:51,779 	Gloss Reference :	A B+C+D+E
2024-02-02 21:05:51,779 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:05:51,779 	Gloss Alignment :	         
2024-02-02 21:05:51,779 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:05:51,781 	Text Reference  :	*** ****** ********* there was   a   strong  competition     and  a     difficult auction for the 5  franchise owners
2024-02-02 21:05:51,781 	Text Hypothesis :	the sports authority of    india sai tweeted congratulations team india was       given   a   lot of 2         crore 
2024-02-02 21:05:51,781 	Text Alignment  :	I   I      I         S     S     S   S       S               S    S     S         S       S   S   S  S         S     
2024-02-02 21:05:51,781 ========================================================================================================================
2024-02-02 21:05:51,781 Logging Sequence: 170_24.00
2024-02-02 21:05:51,781 	Gloss Reference :	A B+C+D+E
2024-02-02 21:05:51,782 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:05:51,782 	Gloss Alignment :	         
2024-02-02 21:05:51,782 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:05:51,782 	Text Reference  :	******* **** ****** ****** let  me    tell you   about it     
2024-02-02 21:05:51,782 	Text Hypothesis :	earlier when wooden stumps were glued to   their tv    screens
2024-02-02 21:05:51,783 	Text Alignment  :	I       I    I      I      S    S     S    S     S     S      
2024-02-02 21:05:51,783 ========================================================================================================================
2024-02-02 21:05:51,783 Logging Sequence: 73_79.00
2024-02-02 21:05:51,783 	Gloss Reference :	A B+C+D+E
2024-02-02 21:05:51,783 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:05:51,783 	Gloss Alignment :	         
2024-02-02 21:05:51,783 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:05:51,785 	Text Reference  :	raina resturant has food from the  rich  spices of north india to  the aromatic        curries of south india 
2024-02-02 21:05:51,785 	Text Hypothesis :	***** ********* pm  modi took this match as     he was   taken for the ahmedabad-based dish    on the   umpire
2024-02-02 21:05:51,785 	Text Alignment  :	D     D         S   S    S    S    S     S      S  S     S     S       S               S       S  S     S     
2024-02-02 21:05:51,785 ========================================================================================================================
2024-02-02 21:05:51,785 Logging Sequence: 140_2.00
2024-02-02 21:05:51,786 	Gloss Reference :	A B+C+D+E
2024-02-02 21:05:51,786 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:05:51,786 	Gloss Alignment :	         
2024-02-02 21:05:51,786 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:05:51,787 	Text Reference  :	**** indian batsman-wicket keeper rishabh pant has    outstanding skills in cricket
2024-02-02 21:05:51,787 	Text Hypothesis :	felt sachin was            lucky  so      he   always gave        him    to india  
2024-02-02 21:05:51,787 	Text Alignment  :	I    S      S              S      S       S    S      S           S      S  S      
2024-02-02 21:05:51,787 ========================================================================================================================
2024-02-02 21:05:51,787 Logging Sequence: 81_470.00
2024-02-02 21:05:51,787 	Gloss Reference :	A B+C+D+E
2024-02-02 21:05:51,788 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:05:51,788 	Gloss Alignment :	         
2024-02-02 21:05:51,788 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:05:51,789 	Text Reference  :	or you don't know if you do  let us  know      in the comments
2024-02-02 21:05:51,789 	Text Hypothesis :	** *** ***** **** ** the two men has completed as 6   balls   
2024-02-02 21:05:51,789 	Text Alignment  :	D  D   D     D    D  S   S   S   S   S         S  S   S       
2024-02-02 21:05:51,789 ========================================================================================================================
2024-02-02 21:05:54,422 Epoch 299: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.97 
2024-02-02 21:05:54,422 EPOCH 300
2024-02-02 21:05:59,827 [Epoch: 300 Step: 00020100] Batch Recognition Loss:   0.000299 => Gls Tokens per Sec:     1967 || Batch Translation Loss:   0.065317 => Txt Tokens per Sec:     5460 || Lr: 0.000100
2024-02-02 21:05:59,828 Epoch 300: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.87 
2024-02-02 21:05:59,828 EPOCH 301
2024-02-02 21:06:05,411 Epoch 301: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.82 
2024-02-02 21:06:05,412 EPOCH 302
2024-02-02 21:06:08,487 [Epoch: 302 Step: 00020200] Batch Recognition Loss:   0.000746 => Gls Tokens per Sec:     1688 || Batch Translation Loss:   0.608963 => Txt Tokens per Sec:     4982 || Lr: 0.000100
2024-02-02 21:06:10,890 Epoch 302: Total Training Recognition Loss 0.03  Total Training Translation Loss 10.13 
2024-02-02 21:06:10,891 EPOCH 303
2024-02-02 21:06:15,823 [Epoch: 303 Step: 00020300] Batch Recognition Loss:   0.000432 => Gls Tokens per Sec:     2123 || Batch Translation Loss:   0.200373 => Txt Tokens per Sec:     5893 || Lr: 0.000100
2024-02-02 21:06:15,876 Epoch 303: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.65 
2024-02-02 21:06:15,876 EPOCH 304
2024-02-02 21:06:21,441 Epoch 304: Total Training Recognition Loss 0.03  Total Training Translation Loss 15.42 
2024-02-02 21:06:21,442 EPOCH 305
2024-02-02 21:06:23,883 [Epoch: 305 Step: 00020400] Batch Recognition Loss:   0.000446 => Gls Tokens per Sec:     2098 || Batch Translation Loss:   0.252773 => Txt Tokens per Sec:     5763 || Lr: 0.000100
2024-02-02 21:06:26,934 Epoch 305: Total Training Recognition Loss 0.03  Total Training Translation Loss 15.46 
2024-02-02 21:06:26,935 EPOCH 306
2024-02-02 21:06:32,302 [Epoch: 306 Step: 00020500] Batch Recognition Loss:   0.000484 => Gls Tokens per Sec:     1922 || Batch Translation Loss:   0.316393 => Txt Tokens per Sec:     5319 || Lr: 0.000100
2024-02-02 21:06:32,441 Epoch 306: Total Training Recognition Loss 0.03  Total Training Translation Loss 12.30 
2024-02-02 21:06:32,441 EPOCH 307
2024-02-02 21:06:37,815 Epoch 307: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.43 
2024-02-02 21:06:37,816 EPOCH 308
2024-02-02 21:06:40,217 [Epoch: 308 Step: 00020600] Batch Recognition Loss:   0.000203 => Gls Tokens per Sec:     2067 || Batch Translation Loss:   0.067129 => Txt Tokens per Sec:     5878 || Lr: 0.000100
2024-02-02 21:06:43,242 Epoch 308: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.53 
2024-02-02 21:06:43,243 EPOCH 309
2024-02-02 21:06:48,366 [Epoch: 309 Step: 00020700] Batch Recognition Loss:   0.000298 => Gls Tokens per Sec:     1982 || Batch Translation Loss:   0.041018 => Txt Tokens per Sec:     5512 || Lr: 0.000100
2024-02-02 21:06:48,585 Epoch 309: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.32 
2024-02-02 21:06:48,585 EPOCH 310
2024-02-02 21:06:54,310 Epoch 310: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.27 
2024-02-02 21:06:54,311 EPOCH 311
2024-02-02 21:06:56,804 [Epoch: 311 Step: 00020800] Batch Recognition Loss:   0.000289 => Gls Tokens per Sec:     1925 || Batch Translation Loss:   0.071636 => Txt Tokens per Sec:     5418 || Lr: 0.000100
2024-02-02 21:06:59,751 Epoch 311: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.98 
2024-02-02 21:06:59,752 EPOCH 312
2024-02-02 21:07:04,935 [Epoch: 312 Step: 00020900] Batch Recognition Loss:   0.000226 => Gls Tokens per Sec:     1928 || Batch Translation Loss:   0.040756 => Txt Tokens per Sec:     5373 || Lr: 0.000100
2024-02-02 21:07:05,181 Epoch 312: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.86 
2024-02-02 21:07:05,182 EPOCH 313
2024-02-02 21:07:10,224 Epoch 313: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.01 
2024-02-02 21:07:10,224 EPOCH 314
2024-02-02 21:07:12,886 [Epoch: 314 Step: 00021000] Batch Recognition Loss:   0.000229 => Gls Tokens per Sec:     1745 || Batch Translation Loss:   0.044661 => Txt Tokens per Sec:     4795 || Lr: 0.000100
2024-02-02 21:07:15,859 Epoch 314: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.48 
2024-02-02 21:07:15,860 EPOCH 315
2024-02-02 21:07:20,683 [Epoch: 315 Step: 00021100] Batch Recognition Loss:   0.000728 => Gls Tokens per Sec:     2039 || Batch Translation Loss:   0.101296 => Txt Tokens per Sec:     5706 || Lr: 0.000100
2024-02-02 21:07:21,010 Epoch 315: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.82 
2024-02-02 21:07:21,010 EPOCH 316
2024-02-02 21:07:26,238 Epoch 316: Total Training Recognition Loss 0.02  Total Training Translation Loss 10.32 
2024-02-02 21:07:26,238 EPOCH 317
2024-02-02 21:07:28,198 [Epoch: 317 Step: 00021200] Batch Recognition Loss:   0.000432 => Gls Tokens per Sec:     2241 || Batch Translation Loss:   0.209921 => Txt Tokens per Sec:     6383 || Lr: 0.000100
2024-02-02 21:07:31,338 Epoch 317: Total Training Recognition Loss 0.02  Total Training Translation Loss 9.73 
2024-02-02 21:07:31,339 EPOCH 318
2024-02-02 21:07:36,229 [Epoch: 318 Step: 00021300] Batch Recognition Loss:   0.001223 => Gls Tokens per Sec:     1978 || Batch Translation Loss:   0.037213 => Txt Tokens per Sec:     5565 || Lr: 0.000100
2024-02-02 21:07:36,578 Epoch 318: Total Training Recognition Loss 0.03  Total Training Translation Loss 11.07 
2024-02-02 21:07:36,579 EPOCH 319
2024-02-02 21:07:42,036 Epoch 319: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.37 
2024-02-02 21:07:42,037 EPOCH 320
2024-02-02 21:07:44,054 [Epoch: 320 Step: 00021400] Batch Recognition Loss:   0.000315 => Gls Tokens per Sec:     2097 || Batch Translation Loss:   0.083148 => Txt Tokens per Sec:     5811 || Lr: 0.000100
2024-02-02 21:07:46,967 Epoch 320: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.23 
2024-02-02 21:07:46,967 EPOCH 321
2024-02-02 21:07:51,924 [Epoch: 321 Step: 00021500] Batch Recognition Loss:   0.000298 => Gls Tokens per Sec:     1919 || Batch Translation Loss:   0.057418 => Txt Tokens per Sec:     5360 || Lr: 0.000100
2024-02-02 21:07:52,412 Epoch 321: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.14 
2024-02-02 21:07:52,412 EPOCH 322
2024-02-02 21:07:57,518 Epoch 322: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.93 
2024-02-02 21:07:57,518 EPOCH 323
2024-02-02 21:07:59,636 [Epoch: 323 Step: 00021600] Batch Recognition Loss:   0.000280 => Gls Tokens per Sec:     1965 || Batch Translation Loss:   0.032377 => Txt Tokens per Sec:     5508 || Lr: 0.000100
2024-02-02 21:08:02,973 Epoch 323: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.49 
2024-02-02 21:08:02,974 EPOCH 324
2024-02-02 21:08:07,634 [Epoch: 324 Step: 00021700] Batch Recognition Loss:   0.000263 => Gls Tokens per Sec:     2007 || Batch Translation Loss:   0.039420 => Txt Tokens per Sec:     5598 || Lr: 0.000100
2024-02-02 21:08:08,196 Epoch 324: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.37 
2024-02-02 21:08:08,196 EPOCH 325
2024-02-02 21:08:13,328 Epoch 325: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.18 
2024-02-02 21:08:13,328 EPOCH 326
2024-02-02 21:08:15,335 [Epoch: 326 Step: 00021800] Batch Recognition Loss:   0.000153 => Gls Tokens per Sec:     1994 || Batch Translation Loss:   0.041521 => Txt Tokens per Sec:     5631 || Lr: 0.000100
2024-02-02 21:08:18,711 Epoch 326: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.67 
2024-02-02 21:08:18,712 EPOCH 327
2024-02-02 21:08:23,161 [Epoch: 327 Step: 00021900] Batch Recognition Loss:   0.000291 => Gls Tokens per Sec:     2066 || Batch Translation Loss:   0.060100 => Txt Tokens per Sec:     5760 || Lr: 0.000100
2024-02-02 21:08:23,763 Epoch 327: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.11 
2024-02-02 21:08:23,764 EPOCH 328
2024-02-02 21:08:29,361 Epoch 328: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.76 
2024-02-02 21:08:29,362 EPOCH 329
2024-02-02 21:08:31,090 [Epoch: 329 Step: 00022000] Batch Recognition Loss:   0.000306 => Gls Tokens per Sec:     2171 || Batch Translation Loss:   0.034656 => Txt Tokens per Sec:     5944 || Lr: 0.000100
2024-02-02 21:08:39,838 Validation result at epoch 329, step    22000: duration: 8.7470s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00146	Translation Loss: 93459.41406	PPL: 11527.47461
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.68	(BLEU-1: 10.44,	BLEU-2: 3.46,	BLEU-3: 1.45,	BLEU-4: 0.68)
	CHRF 17.06	ROUGE 8.98
2024-02-02 21:08:39,839 Logging Recognition and Translation Outputs
2024-02-02 21:08:39,839 ========================================================================================================================
2024-02-02 21:08:39,839 Logging Sequence: 146_56.00
2024-02-02 21:08:39,839 	Gloss Reference :	A B+C+D+E
2024-02-02 21:08:39,840 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:08:39,840 	Gloss Alignment :	         
2024-02-02 21:08:39,840 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:08:39,842 	Text Reference  :	when the players go back to the hotel  as   per rules     all of   them have     to      undergo rtpcr test  for covid-19 everyday
2024-02-02 21:08:39,842 	Text Hypothesis :	**** *** ******* ** **** ** the indian team is  extremely fit when a    champion players for     the   death of  fans     everyday
2024-02-02 21:08:39,842 	Text Alignment  :	D    D   D       D  D    D      S      S    S   S         S   S    S    S        S       S       S     S     S   S                
2024-02-02 21:08:39,842 ========================================================================================================================
2024-02-02 21:08:39,842 Logging Sequence: 118_338.00
2024-02-02 21:08:39,842 	Gloss Reference :	A B+C+D+E
2024-02-02 21:08:39,843 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:08:39,843 	Gloss Alignment :	         
2024-02-02 21:08:39,843 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:08:39,843 	Text Reference  :	** *** ****** this   is   why   even        messi wore it    
2024-02-02 21:08:39,843 	Text Hypothesis :	as the entire nation were still speculating on    the  matter
2024-02-02 21:08:39,844 	Text Alignment  :	I  I   I      S      S    S     S           S     S    S     
2024-02-02 21:08:39,844 ========================================================================================================================
2024-02-02 21:08:39,844 Logging Sequence: 66_61.00
2024-02-02 21:08:39,844 	Gloss Reference :	A B+C+D+E
2024-02-02 21:08:39,844 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:08:39,844 	Gloss Alignment :	         
2024-02-02 21:08:39,844 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:08:39,845 	Text Reference  :	instead   of     returning back to  his        homeland because of his injury
2024-02-02 21:08:39,845 	Text Hypothesis :	rajasthan royals ben       as   the government did      not     us to  him   
2024-02-02 21:08:39,845 	Text Alignment  :	S         S      S         S    S   S          S        S       S  S   S     
2024-02-02 21:08:39,845 ========================================================================================================================
2024-02-02 21:08:39,846 Logging Sequence: 81_278.00
2024-02-02 21:08:39,846 	Gloss Reference :	A B+C+D+E
2024-02-02 21:08:39,846 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:08:39,846 	Gloss Alignment :	         
2024-02-02 21:08:39,846 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:08:39,848 	Text Reference  :	*** ******* ***** of this amrapali group paid rs 3570 crore the remaining rs 652 crore was paid by amrapali sapphire developers a ******** subsidiary of amrapali group   
2024-02-02 21:08:39,848 	Text Hypothesis :	the supreme court of **** amrapali group **** ** **** ***** *** ********* ** *** ***** *** paid ** ******** ******** ********** a forensic audit      of amrapali builders
2024-02-02 21:08:39,848 	Text Alignment  :	I   I       I        D                   D    D  D    D     D   D         D  D   D     D        D  D        D        D            I        S                      S       
2024-02-02 21:08:39,848 ========================================================================================================================
2024-02-02 21:08:39,848 Logging Sequence: 162_125.00
2024-02-02 21:08:39,849 	Gloss Reference :	A B+C+D+E
2024-02-02 21:08:39,849 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:08:39,849 	Gloss Alignment :	         
2024-02-02 21:08:39,849 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:08:39,850 	Text Reference  :	******* *** **** ****** *** ******* in        response to  this       kohli received many    hate     comments on social media
2024-02-02 21:08:39,850 	Text Hypothesis :	ronaldo has also become the penalty shoot-out a        sex trafficker so    he       praised neeraj's throw    at the    car  
2024-02-02 21:08:39,851 	Text Alignment  :	I       I   I    I      I   I       S         S        S   S          S     S        S       S        S        S  S      S    
2024-02-02 21:08:39,851 ========================================================================================================================
2024-02-02 21:08:43,427 Epoch 329: Total Training Recognition Loss 0.02  Total Training Translation Loss 8.81 
2024-02-02 21:08:43,428 EPOCH 330
2024-02-02 21:08:48,302 [Epoch: 330 Step: 00022100] Batch Recognition Loss:   0.000324 => Gls Tokens per Sec:     1853 || Batch Translation Loss:   0.097623 => Txt Tokens per Sec:     5149 || Lr: 0.000100
2024-02-02 21:08:49,013 Epoch 330: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.57 
2024-02-02 21:08:49,013 EPOCH 331
2024-02-02 21:08:54,289 Epoch 331: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.24 
2024-02-02 21:08:54,290 EPOCH 332
2024-02-02 21:08:55,947 [Epoch: 332 Step: 00022200] Batch Recognition Loss:   0.000505 => Gls Tokens per Sec:     2221 || Batch Translation Loss:   0.079901 => Txt Tokens per Sec:     5997 || Lr: 0.000100
2024-02-02 21:08:59,751 Epoch 332: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.54 
2024-02-02 21:08:59,752 EPOCH 333
2024-02-02 21:09:04,531 [Epoch: 333 Step: 00022300] Batch Recognition Loss:   0.000319 => Gls Tokens per Sec:     1856 || Batch Translation Loss:   0.090670 => Txt Tokens per Sec:     5274 || Lr: 0.000100
2024-02-02 21:09:05,231 Epoch 333: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.49 
2024-02-02 21:09:05,231 EPOCH 334
2024-02-02 21:09:09,903 Epoch 334: Total Training Recognition Loss 0.03  Total Training Translation Loss 12.80 
2024-02-02 21:09:09,903 EPOCH 335
2024-02-02 21:09:11,702 [Epoch: 335 Step: 00022400] Batch Recognition Loss:   0.000217 => Gls Tokens per Sec:     1909 || Batch Translation Loss:   0.075777 => Txt Tokens per Sec:     5086 || Lr: 0.000100
2024-02-02 21:09:15,432 Epoch 335: Total Training Recognition Loss 0.02  Total Training Translation Loss 8.25 
2024-02-02 21:09:15,432 EPOCH 336
2024-02-02 21:09:19,909 [Epoch: 336 Step: 00022500] Batch Recognition Loss:   0.000529 => Gls Tokens per Sec:     1946 || Batch Translation Loss:   0.039282 => Txt Tokens per Sec:     5423 || Lr: 0.000100
2024-02-02 21:09:20,761 Epoch 336: Total Training Recognition Loss 0.04  Total Training Translation Loss 5.15 
2024-02-02 21:09:20,761 EPOCH 337
2024-02-02 21:09:26,220 Epoch 337: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.61 
2024-02-02 21:09:26,221 EPOCH 338
2024-02-02 21:09:27,843 [Epoch: 338 Step: 00022600] Batch Recognition Loss:   0.000279 => Gls Tokens per Sec:     2072 || Batch Translation Loss:   0.051734 => Txt Tokens per Sec:     6135 || Lr: 0.000100
2024-02-02 21:09:31,329 Epoch 338: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.60 
2024-02-02 21:09:31,330 EPOCH 339
2024-02-02 21:09:35,764 [Epoch: 339 Step: 00022700] Batch Recognition Loss:   0.000140 => Gls Tokens per Sec:     1949 || Batch Translation Loss:   0.042010 => Txt Tokens per Sec:     5451 || Lr: 0.000100
2024-02-02 21:09:36,817 Epoch 339: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.00 
2024-02-02 21:09:36,817 EPOCH 340
2024-02-02 21:09:42,082 Epoch 340: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.77 
2024-02-02 21:09:42,083 EPOCH 341
2024-02-02 21:09:43,977 [Epoch: 341 Step: 00022800] Batch Recognition Loss:   0.000428 => Gls Tokens per Sec:     1690 || Batch Translation Loss:   0.063805 => Txt Tokens per Sec:     5089 || Lr: 0.000100
2024-02-02 21:09:47,712 Epoch 341: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.90 
2024-02-02 21:09:47,713 EPOCH 342
2024-02-02 21:09:51,830 [Epoch: 342 Step: 00022900] Batch Recognition Loss:   0.000225 => Gls Tokens per Sec:     2038 || Batch Translation Loss:   0.040362 => Txt Tokens per Sec:     5635 || Lr: 0.000100
2024-02-02 21:09:52,890 Epoch 342: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.54 
2024-02-02 21:09:52,890 EPOCH 343
2024-02-02 21:09:58,031 Epoch 343: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.57 
2024-02-02 21:09:58,032 EPOCH 344
2024-02-02 21:09:59,501 [Epoch: 344 Step: 00023000] Batch Recognition Loss:   0.000366 => Gls Tokens per Sec:     2007 || Batch Translation Loss:   0.186269 => Txt Tokens per Sec:     5383 || Lr: 0.000100
2024-02-02 21:10:03,261 Epoch 344: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.29 
2024-02-02 21:10:03,261 EPOCH 345
2024-02-02 21:10:07,705 [Epoch: 345 Step: 00023100] Batch Recognition Loss:   0.000923 => Gls Tokens per Sec:     1853 || Batch Translation Loss:   0.081743 => Txt Tokens per Sec:     5090 || Lr: 0.000100
2024-02-02 21:10:08,981 Epoch 345: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.00 
2024-02-02 21:10:08,981 EPOCH 346
2024-02-02 21:10:13,947 Epoch 346: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.52 
2024-02-02 21:10:13,948 EPOCH 347
2024-02-02 21:10:15,475 [Epoch: 347 Step: 00023200] Batch Recognition Loss:   0.000523 => Gls Tokens per Sec:     1888 || Batch Translation Loss:   0.608555 => Txt Tokens per Sec:     5333 || Lr: 0.000100
2024-02-02 21:10:19,439 Epoch 347: Total Training Recognition Loss 0.02  Total Training Translation Loss 9.05 
2024-02-02 21:10:19,440 EPOCH 348
2024-02-02 21:10:23,857 [Epoch: 348 Step: 00023300] Batch Recognition Loss:   0.000394 => Gls Tokens per Sec:     1827 || Batch Translation Loss:   0.109943 => Txt Tokens per Sec:     5122 || Lr: 0.000100
2024-02-02 21:10:24,988 Epoch 348: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.97 
2024-02-02 21:10:24,988 EPOCH 349
2024-02-02 21:10:30,040 Epoch 349: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.63 
2024-02-02 21:10:30,040 EPOCH 350
2024-02-02 21:10:31,299 [Epoch: 350 Step: 00023400] Batch Recognition Loss:   0.000490 => Gls Tokens per Sec:     2163 || Batch Translation Loss:   0.023001 => Txt Tokens per Sec:     5744 || Lr: 0.000100
2024-02-02 21:10:35,467 Epoch 350: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.51 
2024-02-02 21:10:35,468 EPOCH 351
2024-02-02 21:10:38,850 [Epoch: 351 Step: 00023500] Batch Recognition Loss:   0.000339 => Gls Tokens per Sec:     2366 || Batch Translation Loss:   0.152642 => Txt Tokens per Sec:     6500 || Lr: 0.000100
2024-02-02 21:10:40,250 Epoch 351: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.14 
2024-02-02 21:10:40,250 EPOCH 352
2024-02-02 21:10:45,835 Epoch 352: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.07 
2024-02-02 21:10:45,836 EPOCH 353
2024-02-02 21:10:46,926 [Epoch: 353 Step: 00023600] Batch Recognition Loss:   0.000266 => Gls Tokens per Sec:     2350 || Batch Translation Loss:   0.073136 => Txt Tokens per Sec:     6548 || Lr: 0.000100
2024-02-02 21:10:50,925 Epoch 353: Total Training Recognition Loss 0.02  Total Training Translation Loss 9.27 
2024-02-02 21:10:50,926 EPOCH 354
2024-02-02 21:10:54,980 [Epoch: 354 Step: 00023700] Batch Recognition Loss:   0.000159 => Gls Tokens per Sec:     1935 || Batch Translation Loss:   0.076229 => Txt Tokens per Sec:     5374 || Lr: 0.000100
2024-02-02 21:10:56,351 Epoch 354: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.90 
2024-02-02 21:10:56,351 EPOCH 355
2024-02-02 21:11:01,541 Epoch 355: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.25 
2024-02-02 21:11:01,542 EPOCH 356
2024-02-02 21:11:02,676 [Epoch: 356 Step: 00023800] Batch Recognition Loss:   0.000369 => Gls Tokens per Sec:     2116 || Batch Translation Loss:   0.092213 => Txt Tokens per Sec:     6105 || Lr: 0.000100
2024-02-02 21:11:06,576 Epoch 356: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.60 
2024-02-02 21:11:06,576 EPOCH 357
2024-02-02 21:11:10,357 [Epoch: 357 Step: 00023900] Batch Recognition Loss:   0.000264 => Gls Tokens per Sec:     2008 || Batch Translation Loss:   0.041411 => Txt Tokens per Sec:     5555 || Lr: 0.000100
2024-02-02 21:11:11,934 Epoch 357: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.56 
2024-02-02 21:11:11,934 EPOCH 358
2024-02-02 21:11:17,113 Epoch 358: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.38 
2024-02-02 21:11:17,113 EPOCH 359
2024-02-02 21:11:18,221 [Epoch: 359 Step: 00024000] Batch Recognition Loss:   0.000185 => Gls Tokens per Sec:     2023 || Batch Translation Loss:   0.062867 => Txt Tokens per Sec:     5367 || Lr: 0.000100
2024-02-02 21:11:26,164 Validation result at epoch 359, step    24000: duration: 7.9421s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00164	Translation Loss: 94482.25000	PPL: 12769.86719
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.91	(BLEU-1: 9.74,	BLEU-2: 3.38,	BLEU-3: 1.64,	BLEU-4: 0.91)
	CHRF 16.61	ROUGE 8.79
2024-02-02 21:11:26,166 Logging Recognition and Translation Outputs
2024-02-02 21:11:26,166 ========================================================================================================================
2024-02-02 21:11:26,166 Logging Sequence: 169_165.00
2024-02-02 21:11:26,166 	Gloss Reference :	A B+C+D+E
2024-02-02 21:11:26,166 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:11:26,167 	Gloss Alignment :	         
2024-02-02 21:11:26,167 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:11:26,168 	Text Reference  :	the indian government was outraged by      the incident and these  changes were undone  by  wikipedia
2024-02-02 21:11:26,168 	Text Hypothesis :	*** ****** he         was badly    trolled and mocked   on  social media   for  missing the catch    
2024-02-02 21:11:26,169 	Text Alignment  :	D   D      S              S        S       S   S        S   S      S       S    S       S   S        
2024-02-02 21:11:26,169 ========================================================================================================================
2024-02-02 21:11:26,169 Logging Sequence: 175_60.00
2024-02-02 21:11:26,169 	Gloss Reference :	A B+C+D+E
2024-02-02 21:11:26,169 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:11:26,169 	Gloss Alignment :	         
2024-02-02 21:11:26,170 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:11:26,171 	Text Reference  :	******* **** **** ******* ** *** *** that is how   india bagged 9  medals in  the  youth tournament
2024-02-02 21:11:26,171 	Text Hypothesis :	indians were very excited to see him win  a  world cup   match  as he     has been a     viral     
2024-02-02 21:11:26,171 	Text Alignment  :	I       I    I    I       I  I   I   S    S  S     S     S      S  S      S   S    S     S         
2024-02-02 21:11:26,172 ========================================================================================================================
2024-02-02 21:11:26,172 Logging Sequence: 61_255.00
2024-02-02 21:11:26,172 	Gloss Reference :	A B+C+D+E
2024-02-02 21:11:26,172 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:11:26,172 	Gloss Alignment :	         
2024-02-02 21:11:26,172 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:11:26,173 	Text Reference  :	** ** *** in    2011 we   decided to   marry and  informed our families
2024-02-02 21:11:26,174 	Text Hypothesis :	it is not known how  much longer  have a     part of       the country 
2024-02-02 21:11:26,174 	Text Alignment  :	I  I  I   S     S    S    S       S    S     S    S        S   S       
2024-02-02 21:11:26,174 ========================================================================================================================
2024-02-02 21:11:26,174 Logging Sequence: 173_39.00
2024-02-02 21:11:26,174 	Gloss Reference :	A B+C+D+E
2024-02-02 21:11:26,174 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:11:26,174 	Gloss Alignment :	         
2024-02-02 21:11:26,175 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:11:26,175 	Text Reference  :	*** **** **** kohli will    step down as  india' captain  
2024-02-02 21:11:26,175 	Text Hypothesis :	csk were just two   tickets to   play the other  celebrity
2024-02-02 21:11:26,176 	Text Alignment  :	I   I    I    S     S       S    S    S   S      S        
2024-02-02 21:11:26,176 ========================================================================================================================
2024-02-02 21:11:26,176 Logging Sequence: 172_82.00
2024-02-02 21:11:26,176 	Gloss Reference :	A B+C+D+E
2024-02-02 21:11:26,176 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:11:26,176 	Gloss Alignment :	         
2024-02-02 21:11:26,176 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:11:26,179 	Text Reference  :	you all know that    the toss was about to *** start   at  700 pm    but        it started raining     at       around 630    pm  
2024-02-02 21:11:26,179 	Text Hypothesis :	*** *** **** however icc did  not agree to the demands the two sides eventually on their   differences allowing the    afghan ball
2024-02-02 21:11:26,180 	Text Alignment  :	D   D   D    S       S   S    S   S        I   S       S   S   S     S          S  S       S           S        S      S      S   
2024-02-02 21:11:26,180 ========================================================================================================================
2024-02-02 21:11:30,683 Epoch 359: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.23 
2024-02-02 21:11:30,684 EPOCH 360
2024-02-02 21:11:34,012 [Epoch: 360 Step: 00024100] Batch Recognition Loss:   0.000530 => Gls Tokens per Sec:     2233 || Batch Translation Loss:   0.106605 => Txt Tokens per Sec:     6057 || Lr: 0.000100
2024-02-02 21:11:35,685 Epoch 360: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.50 
2024-02-02 21:11:35,685 EPOCH 361
2024-02-02 21:11:41,115 Epoch 361: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.61 
2024-02-02 21:11:41,116 EPOCH 362
2024-02-02 21:11:41,931 [Epoch: 362 Step: 00024200] Batch Recognition Loss:   0.000213 => Gls Tokens per Sec:     2551 || Batch Translation Loss:   0.155021 => Txt Tokens per Sec:     6623 || Lr: 0.000100
2024-02-02 21:11:46,329 Epoch 362: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.99 
2024-02-02 21:11:46,330 EPOCH 363
2024-02-02 21:11:50,288 [Epoch: 363 Step: 00024300] Batch Recognition Loss:   0.000234 => Gls Tokens per Sec:     1837 || Batch Translation Loss:   0.199070 => Txt Tokens per Sec:     5222 || Lr: 0.000100
2024-02-02 21:11:51,681 Epoch 363: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.90 
2024-02-02 21:11:51,681 EPOCH 364
2024-02-02 21:11:56,812 Epoch 364: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.80 
2024-02-02 21:11:56,813 EPOCH 365
2024-02-02 21:11:57,731 [Epoch: 365 Step: 00024400] Batch Recognition Loss:   0.000319 => Gls Tokens per Sec:     2096 || Batch Translation Loss:   0.116985 => Txt Tokens per Sec:     6096 || Lr: 0.000100
2024-02-02 21:12:01,877 Epoch 365: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.05 
2024-02-02 21:12:01,877 EPOCH 366
2024-02-02 21:12:05,645 [Epoch: 366 Step: 00024500] Batch Recognition Loss:   0.000427 => Gls Tokens per Sec:     1888 || Batch Translation Loss:   0.283204 => Txt Tokens per Sec:     5341 || Lr: 0.000100
2024-02-02 21:12:07,227 Epoch 366: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.53 
2024-02-02 21:12:07,227 EPOCH 367
2024-02-02 21:12:12,331 Epoch 367: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.12 
2024-02-02 21:12:12,332 EPOCH 368
2024-02-02 21:12:13,380 [Epoch: 368 Step: 00024600] Batch Recognition Loss:   0.000797 => Gls Tokens per Sec:     1597 || Batch Translation Loss:   0.095769 => Txt Tokens per Sec:     4363 || Lr: 0.000100
2024-02-02 21:12:17,874 Epoch 368: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.53 
2024-02-02 21:12:17,875 EPOCH 369
2024-02-02 21:12:21,355 [Epoch: 369 Step: 00024700] Batch Recognition Loss:   0.000211 => Gls Tokens per Sec:     1997 || Batch Translation Loss:   0.052615 => Txt Tokens per Sec:     5599 || Lr: 0.000100
2024-02-02 21:12:23,369 Epoch 369: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.85 
2024-02-02 21:12:23,369 EPOCH 370
2024-02-02 21:12:28,498 Epoch 370: Total Training Recognition Loss 0.03  Total Training Translation Loss 14.48 
2024-02-02 21:12:28,498 EPOCH 371
2024-02-02 21:12:29,275 [Epoch: 371 Step: 00024800] Batch Recognition Loss:   0.000429 => Gls Tokens per Sec:     2062 || Batch Translation Loss:   0.232463 => Txt Tokens per Sec:     6189 || Lr: 0.000100
2024-02-02 21:12:33,770 Epoch 371: Total Training Recognition Loss 0.04  Total Training Translation Loss 15.30 
2024-02-02 21:12:33,770 EPOCH 372
2024-02-02 21:12:37,182 [Epoch: 372 Step: 00024900] Batch Recognition Loss:   0.000255 => Gls Tokens per Sec:     1991 || Batch Translation Loss:   0.050179 => Txt Tokens per Sec:     5621 || Lr: 0.000100
2024-02-02 21:12:38,929 Epoch 372: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.65 
2024-02-02 21:12:38,930 EPOCH 373
2024-02-02 21:12:44,224 Epoch 373: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.16 
2024-02-02 21:12:44,225 EPOCH 374
2024-02-02 21:12:44,884 [Epoch: 374 Step: 00025000] Batch Recognition Loss:   0.000191 => Gls Tokens per Sec:     2191 || Batch Translation Loss:   0.130840 => Txt Tokens per Sec:     6189 || Lr: 0.000100
2024-02-02 21:12:49,020 Epoch 374: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.57 
2024-02-02 21:12:49,020 EPOCH 375
2024-02-02 21:12:52,629 [Epoch: 375 Step: 00025100] Batch Recognition Loss:   0.000169 => Gls Tokens per Sec:     1838 || Batch Translation Loss:   0.065394 => Txt Tokens per Sec:     5153 || Lr: 0.000100
2024-02-02 21:12:54,656 Epoch 375: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.92 
2024-02-02 21:12:54,656 EPOCH 376
2024-02-02 21:12:59,738 Epoch 376: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.13 
2024-02-02 21:12:59,739 EPOCH 377
2024-02-02 21:13:00,356 [Epoch: 377 Step: 00025200] Batch Recognition Loss:   0.000214 => Gls Tokens per Sec:     2076 || Batch Translation Loss:   0.041469 => Txt Tokens per Sec:     5395 || Lr: 0.000100
2024-02-02 21:13:05,092 Epoch 377: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.26 
2024-02-02 21:13:05,092 EPOCH 378
2024-02-02 21:13:07,954 [Epoch: 378 Step: 00025300] Batch Recognition Loss:   0.000227 => Gls Tokens per Sec:     2294 || Batch Translation Loss:   0.047839 => Txt Tokens per Sec:     6273 || Lr: 0.000100
2024-02-02 21:13:09,781 Epoch 378: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.09 
2024-02-02 21:13:09,781 EPOCH 379
2024-02-02 21:13:15,362 Epoch 379: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.22 
2024-02-02 21:13:15,363 EPOCH 380
2024-02-02 21:13:15,870 [Epoch: 380 Step: 00025400] Batch Recognition Loss:   0.000297 => Gls Tokens per Sec:     2213 || Batch Translation Loss:   0.031708 => Txt Tokens per Sec:     6532 || Lr: 0.000100
2024-02-02 21:13:20,810 Epoch 380: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.95 
2024-02-02 21:13:20,810 EPOCH 381
2024-02-02 21:13:23,870 [Epoch: 381 Step: 00025500] Batch Recognition Loss:   0.000360 => Gls Tokens per Sec:     2062 || Batch Translation Loss:   0.099691 => Txt Tokens per Sec:     5736 || Lr: 0.000100
2024-02-02 21:13:25,704 Epoch 381: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.79 
2024-02-02 21:13:25,704 EPOCH 382
2024-02-02 21:13:31,171 Epoch 382: Total Training Recognition Loss 0.02  Total Training Translation Loss 8.24 
2024-02-02 21:13:31,171 EPOCH 383
2024-02-02 21:13:31,555 [Epoch: 383 Step: 00025600] Batch Recognition Loss:   0.000266 => Gls Tokens per Sec:     2513 || Batch Translation Loss:   0.699245 => Txt Tokens per Sec:     6463 || Lr: 0.000100
2024-02-02 21:13:36,578 Epoch 383: Total Training Recognition Loss 0.03  Total Training Translation Loss 12.78 
2024-02-02 21:13:36,578 EPOCH 384
2024-02-02 21:13:39,696 [Epoch: 384 Step: 00025700] Batch Recognition Loss:   0.000512 => Gls Tokens per Sec:     2002 || Batch Translation Loss:   0.411978 => Txt Tokens per Sec:     5650 || Lr: 0.000100
2024-02-02 21:13:41,914 Epoch 384: Total Training Recognition Loss 0.02  Total Training Translation Loss 8.10 
2024-02-02 21:13:41,915 EPOCH 385
2024-02-02 21:13:47,574 Epoch 385: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.39 
2024-02-02 21:13:47,575 EPOCH 386
2024-02-02 21:13:47,929 [Epoch: 386 Step: 00025800] Batch Recognition Loss:   0.000498 => Gls Tokens per Sec:     2266 || Batch Translation Loss:   0.074739 => Txt Tokens per Sec:     5972 || Lr: 0.000100
2024-02-02 21:13:52,444 Epoch 386: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.24 
2024-02-02 21:13:52,445 EPOCH 387
2024-02-02 21:13:55,736 [Epoch: 387 Step: 00025900] Batch Recognition Loss:   0.000762 => Gls Tokens per Sec:     1820 || Batch Translation Loss:   0.065077 => Txt Tokens per Sec:     5218 || Lr: 0.000100
2024-02-02 21:13:58,099 Epoch 387: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.96 
2024-02-02 21:13:58,100 EPOCH 388
2024-02-02 21:14:03,495 Epoch 388: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.55 
2024-02-02 21:14:03,496 EPOCH 389
2024-02-02 21:14:04,035 [Epoch: 389 Step: 00026000] Batch Recognition Loss:   0.000847 => Gls Tokens per Sec:     1193 || Batch Translation Loss:   0.020026 => Txt Tokens per Sec:     3493 || Lr: 0.000100
2024-02-02 21:14:12,212 Validation result at epoch 389, step    26000: duration: 8.1774s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00145	Translation Loss: 95236.20312	PPL: 13770.60449
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.83	(BLEU-1: 10.03,	BLEU-2: 3.44,	BLEU-3: 1.60,	BLEU-4: 0.83)
	CHRF 16.72	ROUGE 8.70
2024-02-02 21:14:12,213 Logging Recognition and Translation Outputs
2024-02-02 21:14:12,213 ========================================================================================================================
2024-02-02 21:14:12,214 Logging Sequence: 130_139.00
2024-02-02 21:14:12,214 	Gloss Reference :	A B+C+D+E
2024-02-02 21:14:12,214 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:14:12,214 	Gloss Alignment :	         
2024-02-02 21:14:12,214 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:14:12,217 	Text Reference  :	he shared a picture of  a little pouch he knit for his olympic gold   medal with uk flag on  one  side   and **** japanese flag on      the     other
2024-02-02 21:14:12,217 	Text Hypothesis :	he ****** * ******* won a ****** ***** ** **** *** *** ******* bronze medal **** ** at   the 2012 london and 2016 rio      de   janeiro olympic games
2024-02-02 21:14:12,217 	Text Alignment  :	   D      D D       S     D      D     D  D    D   D   D       S            D    D  S    S   S    S          I    S        S    S       S       S    
2024-02-02 21:14:12,217 ========================================================================================================================
2024-02-02 21:14:12,217 Logging Sequence: 72_194.00
2024-02-02 21:14:12,217 	Gloss Reference :	A B+C+D+E
2024-02-02 21:14:12,217 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:14:12,218 	Gloss Alignment :	         
2024-02-02 21:14:12,218 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:14:12,219 	Text Reference  :	shah told her to do what she wants and filed a  police complaint against her  
2024-02-02 21:14:12,219 	Text Hypothesis :	**** **** *** ** ** **** one of    the video of people are       going   viral
2024-02-02 21:14:12,219 	Text Alignment  :	D    D    D   D  D  D    S   S     S   S     S  S      S         S       S    
2024-02-02 21:14:12,219 ========================================================================================================================
2024-02-02 21:14:12,219 Logging Sequence: 69_177.00
2024-02-02 21:14:12,219 	Gloss Reference :	A B+C+D+E
2024-02-02 21:14:12,219 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:14:12,220 	Gloss Alignment :	         
2024-02-02 21:14:12,220 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:14:12,221 	Text Reference  :	he   said 'i   will    continue playing i   know it's about time i retire i      also have   a    knee condition
2024-02-02 21:14:12,222 	Text Hypothesis :	when csk  were waiting to       see     him in   the  first time * ****** people were elated with the  win      
2024-02-02 21:14:12,222 	Text Alignment  :	S    S    S    S       S        S       S   S    S    S          D D      S      S    S      S    S    S        
2024-02-02 21:14:12,222 ========================================================================================================================
2024-02-02 21:14:12,222 Logging Sequence: 95_118.00
2024-02-02 21:14:12,222 	Gloss Reference :	A B+C+D+E
2024-02-02 21:14:12,222 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:14:12,222 	Gloss Alignment :	         
2024-02-02 21:14:12,223 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:14:12,224 	Text Reference  :	**** **** **** ***** *** ****** the game  was stopped strangely due to ** ******* excessive sunlight
2024-02-02 21:14:12,224 	Text Hypothesis :	they were tire marks all around the pitch and the     play      had to be stopped how       crazy   
2024-02-02 21:14:12,224 	Text Alignment  :	I    I    I    I     I   I          S     S   S       S         S      I  I       S         S       
2024-02-02 21:14:12,224 ========================================================================================================================
2024-02-02 21:14:12,224 Logging Sequence: 112_8.00
2024-02-02 21:14:12,224 	Gloss Reference :	A B+C+D+E
2024-02-02 21:14:12,224 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:14:12,224 	Gloss Alignment :	         
2024-02-02 21:14:12,225 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:14:12,227 	Text Reference  :	before there were 8 teams such as    mumbai indians delhi capitals punjab kings etc     and now   there will be  10 teams in 2022
2024-02-02 21:14:12,227 	Text Hypothesis :	****** ***** **** * you   can  check out    of      1     month    yes    ipl   matches has never been  made the 6  teams ** ****
2024-02-02 21:14:12,227 	Text Alignment  :	D      D     D    D S     S    S     S      S       S     S        S      S     S       S   S     S     S    S   S        D  D   
2024-02-02 21:14:12,227 ========================================================================================================================
2024-02-02 21:14:17,275 Epoch 389: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.26 
2024-02-02 21:14:17,276 EPOCH 390
2024-02-02 21:14:20,247 [Epoch: 390 Step: 00026100] Batch Recognition Loss:   0.000212 => Gls Tokens per Sec:     1993 || Batch Translation Loss:   0.173120 => Txt Tokens per Sec:     5716 || Lr: 0.000100
2024-02-02 21:14:22,639 Epoch 390: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.51 
2024-02-02 21:14:22,639 EPOCH 391
2024-02-02 21:14:27,869 Epoch 391: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.33 
2024-02-02 21:14:27,869 EPOCH 392
2024-02-02 21:14:28,019 [Epoch: 392 Step: 00026200] Batch Recognition Loss:   0.000187 => Gls Tokens per Sec:     3221 || Batch Translation Loss:   0.020227 => Txt Tokens per Sec:     8034 || Lr: 0.000100
2024-02-02 21:14:32,410 Epoch 392: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.94 
2024-02-02 21:14:32,410 EPOCH 393
2024-02-02 21:14:34,992 [Epoch: 393 Step: 00026300] Batch Recognition Loss:   0.000286 => Gls Tokens per Sec:     2233 || Batch Translation Loss:   0.051734 => Txt Tokens per Sec:     6258 || Lr: 0.000100
2024-02-02 21:14:37,703 Epoch 393: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.86 
2024-02-02 21:14:37,704 EPOCH 394
2024-02-02 21:14:43,090 Epoch 394: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.18 
2024-02-02 21:14:43,090 EPOCH 395
2024-02-02 21:14:43,291 [Epoch: 395 Step: 00026400] Batch Recognition Loss:   0.000155 => Gls Tokens per Sec:     1613 || Batch Translation Loss:   0.030941 => Txt Tokens per Sec:     5116 || Lr: 0.000100
2024-02-02 21:14:48,560 Epoch 395: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.11 
2024-02-02 21:14:48,560 EPOCH 396
2024-02-02 21:14:51,407 [Epoch: 396 Step: 00026500] Batch Recognition Loss:   0.000306 => Gls Tokens per Sec:     1968 || Batch Translation Loss:   0.158217 => Txt Tokens per Sec:     5532 || Lr: 0.000100
2024-02-02 21:14:53,905 Epoch 396: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.13 
2024-02-02 21:14:53,905 EPOCH 397
2024-02-02 21:14:58,895 Epoch 397: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.98 
2024-02-02 21:14:58,895 EPOCH 398
2024-02-02 21:14:58,978 [Epoch: 398 Step: 00026600] Batch Recognition Loss:   0.000198 => Gls Tokens per Sec:     2000 || Batch Translation Loss:   0.121972 => Txt Tokens per Sec:     5538 || Lr: 0.000100
2024-02-02 21:15:04,196 Epoch 398: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.47 
2024-02-02 21:15:04,196 EPOCH 399
2024-02-02 21:15:06,731 [Epoch: 399 Step: 00026700] Batch Recognition Loss:   0.000286 => Gls Tokens per Sec:     2147 || Batch Translation Loss:   0.045800 => Txt Tokens per Sec:     5883 || Lr: 0.000100
2024-02-02 21:15:09,545 Epoch 399: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.02 
2024-02-02 21:15:09,545 EPOCH 400
2024-02-02 21:15:14,407 [Epoch: 400 Step: 00026800] Batch Recognition Loss:   0.000206 => Gls Tokens per Sec:     2187 || Batch Translation Loss:   0.054804 => Txt Tokens per Sec:     6071 || Lr: 0.000100
2024-02-02 21:15:14,407 Epoch 400: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.60 
2024-02-02 21:15:14,407 EPOCH 401
2024-02-02 21:15:19,909 Epoch 401: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.01 
2024-02-02 21:15:19,910 EPOCH 402
2024-02-02 21:15:22,409 [Epoch: 402 Step: 00026900] Batch Recognition Loss:   0.000359 => Gls Tokens per Sec:     2078 || Batch Translation Loss:   0.056309 => Txt Tokens per Sec:     5650 || Lr: 0.000100
2024-02-02 21:15:25,083 Epoch 402: Total Training Recognition Loss 0.02  Total Training Translation Loss 9.54 
2024-02-02 21:15:25,083 EPOCH 403
2024-02-02 21:15:30,389 [Epoch: 403 Step: 00027000] Batch Recognition Loss:   0.000305 => Gls Tokens per Sec:     1974 || Batch Translation Loss:   0.050984 => Txt Tokens per Sec:     5511 || Lr: 0.000100
2024-02-02 21:15:30,437 Epoch 403: Total Training Recognition Loss 0.03  Total Training Translation Loss 11.09 
2024-02-02 21:15:30,438 EPOCH 404
2024-02-02 21:15:35,648 Epoch 404: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.50 
2024-02-02 21:15:35,649 EPOCH 405
2024-02-02 21:15:38,119 [Epoch: 405 Step: 00027100] Batch Recognition Loss:   0.000535 => Gls Tokens per Sec:     2037 || Batch Translation Loss:   0.015723 => Txt Tokens per Sec:     5476 || Lr: 0.000100
2024-02-02 21:15:40,970 Epoch 405: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.73 
2024-02-02 21:15:40,971 EPOCH 406
2024-02-02 21:15:45,510 [Epoch: 406 Step: 00027200] Batch Recognition Loss:   0.000391 => Gls Tokens per Sec:     2291 || Batch Translation Loss:   0.040312 => Txt Tokens per Sec:     6403 || Lr: 0.000100
2024-02-02 21:15:45,684 Epoch 406: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.72 
2024-02-02 21:15:45,685 EPOCH 407
2024-02-02 21:15:51,192 Epoch 407: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.36 
2024-02-02 21:15:51,192 EPOCH 408
2024-02-02 21:15:53,540 [Epoch: 408 Step: 00027300] Batch Recognition Loss:   0.000293 => Gls Tokens per Sec:     2075 || Batch Translation Loss:   0.032096 => Txt Tokens per Sec:     5660 || Lr: 0.000100
2024-02-02 21:15:56,531 Epoch 408: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.45 
2024-02-02 21:15:56,531 EPOCH 409
2024-02-02 21:16:01,376 [Epoch: 409 Step: 00027400] Batch Recognition Loss:   0.000333 => Gls Tokens per Sec:     2095 || Batch Translation Loss:   0.051978 => Txt Tokens per Sec:     5822 || Lr: 0.000100
2024-02-02 21:16:01,682 Epoch 409: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.86 
2024-02-02 21:16:01,682 EPOCH 410
2024-02-02 21:16:07,109 Epoch 410: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.69 
2024-02-02 21:16:07,109 EPOCH 411
2024-02-02 21:16:09,309 [Epoch: 411 Step: 00027500] Batch Recognition Loss:   0.000333 => Gls Tokens per Sec:     2141 || Batch Translation Loss:   0.054618 => Txt Tokens per Sec:     5976 || Lr: 0.000100
2024-02-02 21:16:12,471 Epoch 411: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.39 
2024-02-02 21:16:12,471 EPOCH 412
2024-02-02 21:16:17,151 [Epoch: 412 Step: 00027600] Batch Recognition Loss:   0.000188 => Gls Tokens per Sec:     2155 || Batch Translation Loss:   0.030779 => Txt Tokens per Sec:     5926 || Lr: 0.000100
2024-02-02 21:16:17,600 Epoch 412: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.29 
2024-02-02 21:16:17,600 EPOCH 413
2024-02-02 21:16:22,966 Epoch 413: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.39 
2024-02-02 21:16:22,967 EPOCH 414
2024-02-02 21:16:24,892 [Epoch: 414 Step: 00027700] Batch Recognition Loss:   0.000235 => Gls Tokens per Sec:     2412 || Batch Translation Loss:   0.158446 => Txt Tokens per Sec:     6484 || Lr: 0.000100
2024-02-02 21:16:28,110 Epoch 414: Total Training Recognition Loss 0.03  Total Training Translation Loss 10.07 
2024-02-02 21:16:28,110 EPOCH 415
2024-02-02 21:16:33,180 [Epoch: 415 Step: 00027800] Batch Recognition Loss:   0.000400 => Gls Tokens per Sec:     1939 || Batch Translation Loss:   0.087588 => Txt Tokens per Sec:     5397 || Lr: 0.000100
2024-02-02 21:16:33,641 Epoch 415: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.63 
2024-02-02 21:16:33,642 EPOCH 416
2024-02-02 21:16:38,675 Epoch 416: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.71 
2024-02-02 21:16:38,675 EPOCH 417
2024-02-02 21:16:41,087 [Epoch: 417 Step: 00027900] Batch Recognition Loss:   0.000287 => Gls Tokens per Sec:     1859 || Batch Translation Loss:   0.121889 => Txt Tokens per Sec:     5101 || Lr: 0.000100
2024-02-02 21:16:44,225 Epoch 417: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.41 
2024-02-02 21:16:44,226 EPOCH 418
2024-02-02 21:16:48,973 [Epoch: 418 Step: 00028000] Batch Recognition Loss:   0.000523 => Gls Tokens per Sec:     2037 || Batch Translation Loss:   0.074568 => Txt Tokens per Sec:     5642 || Lr: 0.000100
2024-02-02 21:16:57,976 Validation result at epoch 418, step    28000: duration: 9.0015s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00173	Translation Loss: 94105.71875	PPL: 12297.65527
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.41	(BLEU-1: 10.80,	BLEU-2: 3.10,	BLEU-3: 1.00,	BLEU-4: 0.41)
	CHRF 16.95	ROUGE 9.54
2024-02-02 21:16:57,977 Logging Recognition and Translation Outputs
2024-02-02 21:16:57,977 ========================================================================================================================
2024-02-02 21:16:57,978 Logging Sequence: 67_98.00
2024-02-02 21:16:57,978 	Gloss Reference :	A B+C+D+E
2024-02-02 21:16:57,978 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:16:57,978 	Gloss Alignment :	         
2024-02-02 21:16:57,978 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:16:57,980 	Text Reference  :	***** *** *** it   saddens me      to **** ***** *** see people suffering and      dying due  to    lack of oxygen
2024-02-02 21:16:57,980 	Text Hypothesis :	india won the toss and     decided to ball first but in  the    tokyo     olympics that  ever since 2012 to win   
2024-02-02 21:16:57,980 	Text Alignment  :	I     I   I   S    S       S          I    I     I   S   S      S         S        S     S    S     S    S  S     
2024-02-02 21:16:57,981 ========================================================================================================================
2024-02-02 21:16:57,981 Logging Sequence: 157_83.00
2024-02-02 21:16:57,981 	Gloss Reference :	A B+C+D+E
2024-02-02 21:16:57,981 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:16:57,981 	Gloss Alignment :	         
2024-02-02 21:16:57,981 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:16:57,983 	Text Reference  :	also when    you     eat     sandwich at a streetside hawker  or  stall the sandwich maker      will first apply butter with  a   knife
2024-02-02 21:16:57,984 	Text Hypothesis :	**** cricket council noticed that     at * the        stadium was sent  to  her      spectators to   had   come  to     watch the match
2024-02-02 21:16:57,984 	Text Alignment  :	D    S       S       S       S           D S          S       S   S     S   S        S          S    S     S     S      S     S   S    
2024-02-02 21:16:57,984 ========================================================================================================================
2024-02-02 21:16:57,984 Logging Sequence: 76_35.00
2024-02-02 21:16:57,984 	Gloss Reference :	A B+C+D+E
2024-02-02 21:16:57,984 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:16:57,984 	Gloss Alignment :	         
2024-02-02 21:16:57,984 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:16:57,985 	Text Reference  :	bcci president sourav ganguly along with board secretary jay  shah        
2024-02-02 21:16:57,985 	Text Hypothesis :	**** in        the    match   kkr   had  won   the       saff championship
2024-02-02 21:16:57,985 	Text Alignment  :	D    S         S      S       S     S    S     S         S    S           
2024-02-02 21:16:57,985 ========================================================================================================================
2024-02-02 21:16:57,986 Logging Sequence: 139_180.00
2024-02-02 21:16:57,986 	Gloss Reference :	A B+C+D+E
2024-02-02 21:16:57,986 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:16:57,986 	Gloss Alignment :	         
2024-02-02 21:16:57,986 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:16:57,986 	Text Reference  :	netherlands also  faced similar riots
2024-02-02 21:16:57,987 	Text Hypothesis :	the         video has   not     viral
2024-02-02 21:16:57,987 	Text Alignment  :	S           S     S     S       S    
2024-02-02 21:16:57,987 ========================================================================================================================
2024-02-02 21:16:57,987 Logging Sequence: 98_87.00
2024-02-02 21:16:57,987 	Gloss Reference :	A B+C+D+E
2024-02-02 21:16:57,987 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:16:57,987 	Gloss Alignment :	         
2024-02-02 21:16:57,987 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:16:57,989 	Text Reference  :	instead of starting afresh     in ********* ***** 2021  the organizers opted   to resume with the previous edition
2024-02-02 21:16:57,989 	Text Hypothesis :	he      is least    interested in answering phone calls and was        allowed to focus  on   the ******** field  
2024-02-02 21:16:57,989 	Text Alignment  :	S       S  S        S             I         I     S     S   S          S          S      S        D        S      
2024-02-02 21:16:57,989 ========================================================================================================================
2024-02-02 21:16:58,464 Epoch 418: Total Training Recognition Loss 0.02  Total Training Translation Loss 8.53 
2024-02-02 21:16:58,464 EPOCH 419
2024-02-02 21:17:04,096 Epoch 419: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.72 
2024-02-02 21:17:04,096 EPOCH 420
2024-02-02 21:17:06,122 [Epoch: 420 Step: 00028100] Batch Recognition Loss:   0.000166 => Gls Tokens per Sec:     2088 || Batch Translation Loss:   0.123225 => Txt Tokens per Sec:     5948 || Lr: 0.000100
2024-02-02 21:17:09,348 Epoch 420: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.69 
2024-02-02 21:17:09,349 EPOCH 421
2024-02-02 21:17:14,118 [Epoch: 421 Step: 00028200] Batch Recognition Loss:   0.000298 => Gls Tokens per Sec:     1995 || Batch Translation Loss:   0.033062 => Txt Tokens per Sec:     5525 || Lr: 0.000100
2024-02-02 21:17:14,626 Epoch 421: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.38 
2024-02-02 21:17:14,627 EPOCH 422
2024-02-02 21:17:19,932 Epoch 422: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.61 
2024-02-02 21:17:19,932 EPOCH 423
2024-02-02 21:17:22,104 [Epoch: 423 Step: 00028300] Batch Recognition Loss:   0.000354 => Gls Tokens per Sec:     1876 || Batch Translation Loss:   0.209746 => Txt Tokens per Sec:     5147 || Lr: 0.000100
2024-02-02 21:17:25,573 Epoch 423: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.16 
2024-02-02 21:17:25,574 EPOCH 424
2024-02-02 21:17:30,690 [Epoch: 424 Step: 00028400] Batch Recognition Loss:   0.000450 => Gls Tokens per Sec:     1828 || Batch Translation Loss:   0.017073 => Txt Tokens per Sec:     5121 || Lr: 0.000100
2024-02-02 21:17:31,246 Epoch 424: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.65 
2024-02-02 21:17:31,246 EPOCH 425
2024-02-02 21:17:36,415 Epoch 425: Total Training Recognition Loss 0.02  Total Training Translation Loss 8.01 
2024-02-02 21:17:36,416 EPOCH 426
2024-02-02 21:17:38,351 [Epoch: 426 Step: 00028500] Batch Recognition Loss:   0.000186 => Gls Tokens per Sec:     2069 || Batch Translation Loss:   0.034457 => Txt Tokens per Sec:     5700 || Lr: 0.000100
2024-02-02 21:17:41,930 Epoch 426: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.14 
2024-02-02 21:17:41,930 EPOCH 427
2024-02-02 21:17:46,779 [Epoch: 427 Step: 00028600] Batch Recognition Loss:   0.000217 => Gls Tokens per Sec:     1914 || Batch Translation Loss:   0.055601 => Txt Tokens per Sec:     5403 || Lr: 0.000100
2024-02-02 21:17:47,430 Epoch 427: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.76 
2024-02-02 21:17:47,431 EPOCH 428
2024-02-02 21:17:52,992 Epoch 428: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.37 
2024-02-02 21:17:52,993 EPOCH 429
2024-02-02 21:17:55,004 [Epoch: 429 Step: 00028700] Batch Recognition Loss:   0.000255 => Gls Tokens per Sec:     1866 || Batch Translation Loss:   0.063486 => Txt Tokens per Sec:     5196 || Lr: 0.000100
2024-02-02 21:17:58,576 Epoch 429: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.03 
2024-02-02 21:17:58,576 EPOCH 430
2024-02-02 21:18:02,861 [Epoch: 430 Step: 00028800] Batch Recognition Loss:   0.000258 => Gls Tokens per Sec:     2108 || Batch Translation Loss:   0.103556 => Txt Tokens per Sec:     5789 || Lr: 0.000100
2024-02-02 21:18:03,677 Epoch 430: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.36 
2024-02-02 21:18:03,677 EPOCH 431
2024-02-02 21:18:09,234 Epoch 431: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.11 
2024-02-02 21:18:09,235 EPOCH 432
2024-02-02 21:18:10,672 [Epoch: 432 Step: 00028900] Batch Recognition Loss:   0.000321 => Gls Tokens per Sec:     2564 || Batch Translation Loss:   0.010762 => Txt Tokens per Sec:     6955 || Lr: 0.000100
2024-02-02 21:18:14,454 Epoch 432: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.84 
2024-02-02 21:18:14,454 EPOCH 433
2024-02-02 21:18:19,183 [Epoch: 433 Step: 00029000] Batch Recognition Loss:   0.000177 => Gls Tokens per Sec:     1895 || Batch Translation Loss:   0.025008 => Txt Tokens per Sec:     5311 || Lr: 0.000100
2024-02-02 21:18:20,034 Epoch 433: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.95 
2024-02-02 21:18:20,035 EPOCH 434
2024-02-02 21:18:25,498 Epoch 434: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.95 
2024-02-02 21:18:25,499 EPOCH 435
2024-02-02 21:18:27,159 [Epoch: 435 Step: 00029100] Batch Recognition Loss:   0.000418 => Gls Tokens per Sec:     2120 || Batch Translation Loss:   0.026377 => Txt Tokens per Sec:     5716 || Lr: 0.000100
2024-02-02 21:18:30,799 Epoch 435: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.07 
2024-02-02 21:18:30,800 EPOCH 436
2024-02-02 21:18:35,399 [Epoch: 436 Step: 00029200] Batch Recognition Loss:   0.000390 => Gls Tokens per Sec:     1895 || Batch Translation Loss:   0.185744 => Txt Tokens per Sec:     5229 || Lr: 0.000100
2024-02-02 21:18:36,384 Epoch 436: Total Training Recognition Loss 0.02  Total Training Translation Loss 10.96 
2024-02-02 21:18:36,385 EPOCH 437
2024-02-02 21:18:41,708 Epoch 437: Total Training Recognition Loss 0.04  Total Training Translation Loss 12.12 
2024-02-02 21:18:41,709 EPOCH 438
2024-02-02 21:18:43,754 [Epoch: 438 Step: 00029300] Batch Recognition Loss:   0.000521 => Gls Tokens per Sec:     1601 || Batch Translation Loss:   0.101167 => Txt Tokens per Sec:     4719 || Lr: 0.000100
2024-02-02 21:18:47,174 Epoch 438: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.15 
2024-02-02 21:18:47,174 EPOCH 439
2024-02-02 21:18:51,020 [Epoch: 439 Step: 00029400] Batch Recognition Loss:   0.000547 => Gls Tokens per Sec:     2224 || Batch Translation Loss:   0.473954 => Txt Tokens per Sec:     6182 || Lr: 0.000100
2024-02-02 21:18:51,880 Epoch 439: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.35 
2024-02-02 21:18:51,881 EPOCH 440
2024-02-02 21:18:57,408 Epoch 440: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.95 
2024-02-02 21:18:57,409 EPOCH 441
2024-02-02 21:18:59,275 [Epoch: 441 Step: 00029500] Batch Recognition Loss:   0.000371 => Gls Tokens per Sec:     1668 || Batch Translation Loss:   0.066990 => Txt Tokens per Sec:     4895 || Lr: 0.000100
2024-02-02 21:19:03,049 Epoch 441: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.69 
2024-02-02 21:19:03,050 EPOCH 442
2024-02-02 21:19:07,196 [Epoch: 442 Step: 00029600] Batch Recognition Loss:   0.000293 => Gls Tokens per Sec:     2024 || Batch Translation Loss:   0.029014 => Txt Tokens per Sec:     5639 || Lr: 0.000100
2024-02-02 21:19:08,391 Epoch 442: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.32 
2024-02-02 21:19:08,391 EPOCH 443
2024-02-02 21:19:13,882 Epoch 443: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.46 
2024-02-02 21:19:13,883 EPOCH 444
2024-02-02 21:19:15,326 [Epoch: 444 Step: 00029700] Batch Recognition Loss:   0.000269 => Gls Tokens per Sec:     2108 || Batch Translation Loss:   0.034565 => Txt Tokens per Sec:     6199 || Lr: 0.000100
2024-02-02 21:19:19,000 Epoch 444: Total Training Recognition Loss 0.02  Total Training Translation Loss 8.18 
2024-02-02 21:19:19,001 EPOCH 445
2024-02-02 21:19:23,455 [Epoch: 445 Step: 00029800] Batch Recognition Loss:   0.000389 => Gls Tokens per Sec:     1848 || Batch Translation Loss:   0.079493 => Txt Tokens per Sec:     5170 || Lr: 0.000100
2024-02-02 21:19:24,670 Epoch 445: Total Training Recognition Loss 0.03  Total Training Translation Loss 10.73 
2024-02-02 21:19:24,671 EPOCH 446
2024-02-02 21:19:30,074 Epoch 446: Total Training Recognition Loss 0.03  Total Training Translation Loss 11.09 
2024-02-02 21:19:30,075 EPOCH 447
2024-02-02 21:19:31,323 [Epoch: 447 Step: 00029900] Batch Recognition Loss:   0.000205 => Gls Tokens per Sec:     2312 || Batch Translation Loss:   0.049516 => Txt Tokens per Sec:     6219 || Lr: 0.000100
2024-02-02 21:19:35,326 Epoch 447: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.48 
2024-02-02 21:19:35,327 EPOCH 448
2024-02-02 21:19:39,681 [Epoch: 448 Step: 00030000] Batch Recognition Loss:   0.000325 => Gls Tokens per Sec:     1854 || Batch Translation Loss:   0.058251 => Txt Tokens per Sec:     5274 || Lr: 0.000100
2024-02-02 21:19:48,222 Validation result at epoch 448, step    30000: duration: 8.5411s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00156	Translation Loss: 94343.07031	PPL: 12593.24805
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.30	(BLEU-1: 10.10,	BLEU-2: 2.75,	BLEU-3: 0.84,	BLEU-4: 0.30)
	CHRF 16.19	ROUGE 8.72
2024-02-02 21:19:48,223 Logging Recognition and Translation Outputs
2024-02-02 21:19:48,224 ========================================================================================================================
2024-02-02 21:19:48,224 Logging Sequence: 165_502.00
2024-02-02 21:19:48,224 	Gloss Reference :	A B+C+D+E
2024-02-02 21:19:48,224 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:19:48,224 	Gloss Alignment :	         
2024-02-02 21:19:48,225 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:19:48,226 	Text Reference  :	tendulkar would sit   in       the **** pavilion wearing both      his     batting pads even after   he   got   out
2024-02-02 21:19:48,226 	Text Hypothesis :	********* this  group includes the many teams    like    australia england india   must play against each other out
2024-02-02 21:19:48,226 	Text Alignment  :	D         S     S     S            I    S        S       S         S       S       S    S    S       S    S        
2024-02-02 21:19:48,227 ========================================================================================================================
2024-02-02 21:19:48,227 Logging Sequence: 127_57.00
2024-02-02 21:19:48,227 	Gloss Reference :	A B+C+D+E
2024-02-02 21:19:48,227 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:19:48,227 	Gloss Alignment :	         
2024-02-02 21:19:48,227 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:19:48,229 	Text Reference  :	till date india had won only 2 medals at the championships which  like the olympics is   the             highest level championship
2024-02-02 21:19:48,229 	Text Hypothesis :	**** **** ***** *** *** **** * ****** ** the bidding       amount for  the same     time congratulations to      west  indies      
2024-02-02 21:19:48,229 	Text Alignment  :	D    D    D     D   D   D    D D      D      S             S      S        S        S    S               S       S     S           
2024-02-02 21:19:48,229 ========================================================================================================================
2024-02-02 21:19:48,229 Logging Sequence: 169_10.00
2024-02-02 21:19:48,229 	Gloss Reference :	A B+C+D+E
2024-02-02 21:19:48,229 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:19:48,229 	Gloss Alignment :	         
2024-02-02 21:19:48,230 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:19:48,231 	Text Reference  :	the 18th over was bowled by     ravi bishnoi with khushdil shah  and     asif ali     on  the crease 
2024-02-02 21:19:48,231 	Text Hypothesis :	*** **** **** a   first  person in   the     asia cup      final against each captain for a   victory
2024-02-02 21:19:48,231 	Text Alignment  :	D   D    D    S   S      S      S    S       S    S        S     S       S    S       S   S   S      
2024-02-02 21:19:48,231 ========================================================================================================================
2024-02-02 21:19:48,232 Logging Sequence: 64_89.00
2024-02-02 21:19:48,232 	Gloss Reference :	A B+C+D+E
2024-02-02 21:19:48,232 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:19:48,232 	Gloss Alignment :	         
2024-02-02 21:19:48,232 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:19:48,233 	Text Reference  :	but this can  not go on amidst the rising cases human lives    need to   be safeguarded
2024-02-02 21:19:48,233 	Text Hypothesis :	*** ipl  will not ** ** ****** *** ****** ***** be    possible in   june as well       
2024-02-02 21:19:48,233 	Text Alignment  :	D   S    S        D  D  D      D   D      D     S     S        S    S    S  S          
2024-02-02 21:19:48,233 ========================================================================================================================
2024-02-02 21:19:48,234 Logging Sequence: 166_261.00
2024-02-02 21:19:48,234 	Gloss Reference :	A B+C+D+E
2024-02-02 21:19:48,234 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:19:48,234 	Gloss Alignment :	         
2024-02-02 21:19:48,234 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:19:48,235 	Text Reference  :	***** for all organizational matters and the schedule   
2024-02-02 21:19:48,235 	Text Hypothesis :	babar and the ball           leaving for the coronavirus
2024-02-02 21:19:48,235 	Text Alignment  :	I     S   S   S              S       S       S          
2024-02-02 21:19:48,235 ========================================================================================================================
2024-02-02 21:19:49,411 Epoch 448: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.95 
2024-02-02 21:19:49,411 EPOCH 449
2024-02-02 21:19:54,811 Epoch 449: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.68 
2024-02-02 21:19:54,811 EPOCH 450
2024-02-02 21:19:56,083 [Epoch: 450 Step: 00030100] Batch Recognition Loss:   0.000574 => Gls Tokens per Sec:     2069 || Batch Translation Loss:   0.077038 => Txt Tokens per Sec:     5506 || Lr: 0.000100
2024-02-02 21:20:00,384 Epoch 450: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.14 
2024-02-02 21:20:00,385 EPOCH 451
2024-02-02 21:20:04,070 [Epoch: 451 Step: 00030200] Batch Recognition Loss:   0.000408 => Gls Tokens per Sec:     2148 || Batch Translation Loss:   0.102783 => Txt Tokens per Sec:     5863 || Lr: 0.000100
2024-02-02 21:20:05,571 Epoch 451: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.02 
2024-02-02 21:20:05,571 EPOCH 452
2024-02-02 21:20:11,110 Epoch 452: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.52 
2024-02-02 21:20:11,111 EPOCH 453
2024-02-02 21:20:12,343 [Epoch: 453 Step: 00030300] Batch Recognition Loss:   0.000651 => Gls Tokens per Sec:     2079 || Batch Translation Loss:   0.035915 => Txt Tokens per Sec:     5945 || Lr: 0.000100
2024-02-02 21:20:16,664 Epoch 453: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.37 
2024-02-02 21:20:16,664 EPOCH 454
2024-02-02 21:20:20,652 [Epoch: 454 Step: 00030400] Batch Recognition Loss:   0.000322 => Gls Tokens per Sec:     1944 || Batch Translation Loss:   0.035644 => Txt Tokens per Sec:     5458 || Lr: 0.000100
2024-02-02 21:20:21,915 Epoch 454: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.09 
2024-02-02 21:20:21,915 EPOCH 455
2024-02-02 21:20:27,193 Epoch 455: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.97 
2024-02-02 21:20:27,194 EPOCH 456
2024-02-02 21:20:28,259 [Epoch: 456 Step: 00030500] Batch Recognition Loss:   0.000138 => Gls Tokens per Sec:     2257 || Batch Translation Loss:   0.019941 => Txt Tokens per Sec:     6117 || Lr: 0.000100
2024-02-02 21:20:32,168 Epoch 456: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.88 
2024-02-02 21:20:32,168 EPOCH 457
2024-02-02 21:20:36,106 [Epoch: 457 Step: 00030600] Batch Recognition Loss:   0.000364 => Gls Tokens per Sec:     1928 || Batch Translation Loss:   0.047418 => Txt Tokens per Sec:     5391 || Lr: 0.000100
2024-02-02 21:20:37,562 Epoch 457: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.79 
2024-02-02 21:20:37,562 EPOCH 458
2024-02-02 21:20:42,931 Epoch 458: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.35 
2024-02-02 21:20:42,932 EPOCH 459
2024-02-02 21:20:44,175 [Epoch: 459 Step: 00030700] Batch Recognition Loss:   0.000286 => Gls Tokens per Sec:     1806 || Batch Translation Loss:   0.045051 => Txt Tokens per Sec:     5215 || Lr: 0.000100
2024-02-02 21:20:48,499 Epoch 459: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.89 
2024-02-02 21:20:48,500 EPOCH 460
2024-02-02 21:20:52,137 [Epoch: 460 Step: 00030800] Batch Recognition Loss:   0.000202 => Gls Tokens per Sec:     2044 || Batch Translation Loss:   0.040729 => Txt Tokens per Sec:     5555 || Lr: 0.000100
2024-02-02 21:20:53,948 Epoch 460: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.01 
2024-02-02 21:20:53,949 EPOCH 461
2024-02-02 21:20:58,990 Epoch 461: Total Training Recognition Loss 0.02  Total Training Translation Loss 9.97 
2024-02-02 21:20:58,990 EPOCH 462
2024-02-02 21:20:59,815 [Epoch: 462 Step: 00030900] Batch Recognition Loss:   0.000395 => Gls Tokens per Sec:     2523 || Batch Translation Loss:   0.091936 => Txt Tokens per Sec:     6760 || Lr: 0.000100
2024-02-02 21:21:04,339 Epoch 462: Total Training Recognition Loss 0.04  Total Training Translation Loss 11.32 
2024-02-02 21:21:04,340 EPOCH 463
2024-02-02 21:21:08,108 [Epoch: 463 Step: 00031000] Batch Recognition Loss:   0.000556 => Gls Tokens per Sec:     1930 || Batch Translation Loss:   0.155593 => Txt Tokens per Sec:     5400 || Lr: 0.000100
2024-02-02 21:21:09,450 Epoch 463: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.25 
2024-02-02 21:21:09,450 EPOCH 464
2024-02-02 21:21:14,912 Epoch 464: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.25 
2024-02-02 21:21:14,913 EPOCH 465
2024-02-02 21:21:15,674 [Epoch: 465 Step: 00031100] Batch Recognition Loss:   0.000218 => Gls Tokens per Sec:     2523 || Batch Translation Loss:   0.065139 => Txt Tokens per Sec:     6439 || Lr: 0.000100
2024-02-02 21:21:19,817 Epoch 465: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.46 
2024-02-02 21:21:19,817 EPOCH 466
2024-02-02 21:21:23,730 [Epoch: 466 Step: 00031200] Batch Recognition Loss:   0.000358 => Gls Tokens per Sec:     1818 || Batch Translation Loss:   0.092953 => Txt Tokens per Sec:     5196 || Lr: 0.000100
2024-02-02 21:21:25,319 Epoch 466: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.18 
2024-02-02 21:21:25,319 EPOCH 467
2024-02-02 21:21:30,494 Epoch 467: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.18 
2024-02-02 21:21:30,495 EPOCH 468
2024-02-02 21:21:31,386 [Epoch: 468 Step: 00031300] Batch Recognition Loss:   0.000326 => Gls Tokens per Sec:     1979 || Batch Translation Loss:   0.034070 => Txt Tokens per Sec:     5639 || Lr: 0.000100
2024-02-02 21:21:35,823 Epoch 468: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.52 
2024-02-02 21:21:35,823 EPOCH 469
2024-02-02 21:21:38,939 [Epoch: 469 Step: 00031400] Batch Recognition Loss:   0.000215 => Gls Tokens per Sec:     2231 || Batch Translation Loss:   0.081730 => Txt Tokens per Sec:     6036 || Lr: 0.000100
2024-02-02 21:21:40,914 Epoch 469: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.24 
2024-02-02 21:21:40,915 EPOCH 470
2024-02-02 21:21:46,126 Epoch 470: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.04 
2024-02-02 21:21:46,126 EPOCH 471
2024-02-02 21:21:46,783 [Epoch: 471 Step: 00031500] Batch Recognition Loss:   0.000290 => Gls Tokens per Sec:     2439 || Batch Translation Loss:   0.070090 => Txt Tokens per Sec:     7088 || Lr: 0.000100
2024-02-02 21:21:51,404 Epoch 471: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.96 
2024-02-02 21:21:51,405 EPOCH 472
2024-02-02 21:21:54,683 [Epoch: 472 Step: 00031600] Batch Recognition Loss:   0.000229 => Gls Tokens per Sec:     2072 || Batch Translation Loss:   0.036931 => Txt Tokens per Sec:     5615 || Lr: 0.000100
2024-02-02 21:21:56,840 Epoch 472: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.78 
2024-02-02 21:21:56,840 EPOCH 473
2024-02-02 21:22:02,213 Epoch 473: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.04 
2024-02-02 21:22:02,213 EPOCH 474
2024-02-02 21:22:02,856 [Epoch: 474 Step: 00031700] Batch Recognition Loss:   0.000149 => Gls Tokens per Sec:     2246 || Batch Translation Loss:   0.137432 => Txt Tokens per Sec:     6861 || Lr: 0.000100
2024-02-02 21:22:07,564 Epoch 474: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.40 
2024-02-02 21:22:07,565 EPOCH 475
2024-02-02 21:22:10,770 [Epoch: 475 Step: 00031800] Batch Recognition Loss:   0.000292 => Gls Tokens per Sec:     2097 || Batch Translation Loss:   0.046150 => Txt Tokens per Sec:     5872 || Lr: 0.000100
2024-02-02 21:22:12,843 Epoch 475: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.25 
2024-02-02 21:22:12,843 EPOCH 476
2024-02-02 21:22:17,948 Epoch 476: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.66 
2024-02-02 21:22:17,949 EPOCH 477
2024-02-02 21:22:18,623 [Epoch: 477 Step: 00031900] Batch Recognition Loss:   0.000469 => Gls Tokens per Sec:     1906 || Batch Translation Loss:   0.266912 => Txt Tokens per Sec:     5539 || Lr: 0.000100
2024-02-02 21:22:23,189 Epoch 477: Total Training Recognition Loss 0.02  Total Training Translation Loss 8.45 
2024-02-02 21:22:23,189 EPOCH 478
2024-02-02 21:22:26,213 [Epoch: 478 Step: 00032000] Batch Recognition Loss:   0.000234 => Gls Tokens per Sec:     2140 || Batch Translation Loss:   0.140852 => Txt Tokens per Sec:     5861 || Lr: 0.000100
2024-02-02 21:22:34,765 Validation result at epoch 478, step    32000: duration: 8.5510s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00142	Translation Loss: 93260.83594	PPL: 11300.66406
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.69	(BLEU-1: 10.93,	BLEU-2: 3.45,	BLEU-3: 1.47,	BLEU-4: 0.69)
	CHRF 17.16	ROUGE 9.09
2024-02-02 21:22:34,766 Logging Recognition and Translation Outputs
2024-02-02 21:22:34,766 ========================================================================================================================
2024-02-02 21:22:34,767 Logging Sequence: 86_11.00
2024-02-02 21:22:34,767 	Gloss Reference :	A B+C+D+E
2024-02-02 21:22:34,767 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:22:34,767 	Gloss Alignment :	         
2024-02-02 21:22:34,767 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:22:34,768 	Text Reference  :	he was **** ******* ** 66  years old    
2024-02-02 21:22:34,768 	Text Hypothesis :	he was very excited to see her   victory
2024-02-02 21:22:34,768 	Text Alignment  :	       I    I       I  S   S     S      
2024-02-02 21:22:34,768 ========================================================================================================================
2024-02-02 21:22:34,768 Logging Sequence: 67_16.00
2024-02-02 21:22:34,768 	Gloss Reference :	A B+C+D+E
2024-02-02 21:22:34,768 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:22:34,769 	Gloss Alignment :	         
2024-02-02 21:22:34,769 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:22:34,769 	Text Reference  :	* **** ********* **** to    help india's fight against the    covid-19 pandemic
2024-02-02 21:22:34,769 	Text Hypothesis :	i have dedicated this match as   a       month has     landed in       love    
2024-02-02 21:22:34,770 	Text Alignment  :	I I    I         I    S     S    S       S     S       S      S        S       
2024-02-02 21:22:34,770 ========================================================================================================================
2024-02-02 21:22:34,770 Logging Sequence: 69_177.00
2024-02-02 21:22:34,770 	Gloss Reference :	A B+C+D+E
2024-02-02 21:22:34,770 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:22:34,770 	Gloss Alignment :	         
2024-02-02 21:22:34,770 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:22:34,772 	Text Reference  :	he said 'i will continue playing i know it's about time  i   retire    i   also have a   knee    condition
2024-02-02 21:22:34,772 	Text Hypothesis :	** **** ** **** ******** ******* * when both the   score was presented the end  of   the lucknow season   
2024-02-02 21:22:34,772 	Text Alignment  :	D  D    D  D    D        D       D S    S    S     S     S   S         S   S    S    S   S       S        
2024-02-02 21:22:34,772 ========================================================================================================================
2024-02-02 21:22:34,772 Logging Sequence: 165_615.00
2024-02-02 21:22:34,772 	Gloss Reference :	A B+C+D+E
2024-02-02 21:22:34,773 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:22:34,773 	Gloss Alignment :	         
2024-02-02 21:22:34,773 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:22:34,773 	Text Reference  :	**** ** *** ** *** **** ********* *** we defeated pakistan too     
2024-02-02 21:22:34,773 	Text Hypothesis :	this is why it has been postponed due to the      covid    pandemic
2024-02-02 21:22:34,773 	Text Alignment  :	I    I  I   I  I   I    I         I   S  S        S        S       
2024-02-02 21:22:34,774 ========================================================================================================================
2024-02-02 21:22:34,774 Logging Sequence: 61_5.00
2024-02-02 21:22:34,774 	Gloss Reference :	A B+C+D+E
2024-02-02 21:22:34,774 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:22:34,774 	Gloss Alignment :	         
2024-02-02 21:22:34,774 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:22:34,775 	Text Reference  :	they rivalry is    seen   the  most during india pakistan cricket matches
2024-02-02 21:22:34,775 	Text Hypothesis :	**** ******* babar azam's chat and  told   him   call     became  viral  
2024-02-02 21:22:34,775 	Text Alignment  :	D    D       S     S      S    S    S      S     S        S       S      
2024-02-02 21:22:34,775 ========================================================================================================================
2024-02-02 21:22:36,876 Epoch 478: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.02 
2024-02-02 21:22:36,876 EPOCH 479
2024-02-02 21:22:42,019 Epoch 479: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.81 
2024-02-02 21:22:42,019 EPOCH 480
2024-02-02 21:22:42,395 [Epoch: 480 Step: 00032100] Batch Recognition Loss:   0.000170 => Gls Tokens per Sec:     2987 || Batch Translation Loss:   0.014244 => Txt Tokens per Sec:     6731 || Lr: 0.000100
2024-02-02 21:22:47,555 Epoch 480: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.74 
2024-02-02 21:22:47,556 EPOCH 481
2024-02-02 21:22:50,601 [Epoch: 481 Step: 00032200] Batch Recognition Loss:   0.000268 => Gls Tokens per Sec:     2102 || Batch Translation Loss:   0.095234 => Txt Tokens per Sec:     5810 || Lr: 0.000100
2024-02-02 21:22:52,956 Epoch 481: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.10 
2024-02-02 21:22:52,957 EPOCH 482
2024-02-02 21:22:58,249 Epoch 482: Total Training Recognition Loss 0.02  Total Training Translation Loss 10.02 
2024-02-02 21:22:58,249 EPOCH 483
2024-02-02 21:22:58,701 [Epoch: 483 Step: 00032300] Batch Recognition Loss:   0.000263 => Gls Tokens per Sec:     2133 || Batch Translation Loss:   0.254509 => Txt Tokens per Sec:     6029 || Lr: 0.000100
2024-02-02 21:23:03,733 Epoch 483: Total Training Recognition Loss 0.03  Total Training Translation Loss 12.41 
2024-02-02 21:23:03,734 EPOCH 484
2024-02-02 21:23:06,702 [Epoch: 484 Step: 00032400] Batch Recognition Loss:   0.000424 => Gls Tokens per Sec:     2103 || Batch Translation Loss:   0.052021 => Txt Tokens per Sec:     5843 || Lr: 0.000100
2024-02-02 21:23:09,057 Epoch 484: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.18 
2024-02-02 21:23:09,058 EPOCH 485
2024-02-02 21:23:14,587 Epoch 485: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.50 
2024-02-02 21:23:14,587 EPOCH 486
2024-02-02 21:23:14,965 [Epoch: 486 Step: 00032500] Batch Recognition Loss:   0.000298 => Gls Tokens per Sec:     2122 || Batch Translation Loss:   0.046117 => Txt Tokens per Sec:     5533 || Lr: 0.000100
2024-02-02 21:23:19,879 Epoch 486: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.45 
2024-02-02 21:23:19,879 EPOCH 487
2024-02-02 21:23:22,766 [Epoch: 487 Step: 00032600] Batch Recognition Loss:   0.000415 => Gls Tokens per Sec:     2107 || Batch Translation Loss:   0.031905 => Txt Tokens per Sec:     5650 || Lr: 0.000100
2024-02-02 21:23:25,285 Epoch 487: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.66 
2024-02-02 21:23:25,285 EPOCH 488
2024-02-02 21:23:30,714 Epoch 488: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.01 
2024-02-02 21:23:30,715 EPOCH 489
2024-02-02 21:23:30,998 [Epoch: 489 Step: 00032700] Batch Recognition Loss:   0.000262 => Gls Tokens per Sec:     2270 || Batch Translation Loss:   0.021046 => Txt Tokens per Sec:     6205 || Lr: 0.000100
2024-02-02 21:23:36,255 Epoch 489: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.69 
2024-02-02 21:23:36,256 EPOCH 490
2024-02-02 21:23:39,378 [Epoch: 490 Step: 00032800] Batch Recognition Loss:   0.000203 => Gls Tokens per Sec:     1868 || Batch Translation Loss:   0.074152 => Txt Tokens per Sec:     5200 || Lr: 0.000100
2024-02-02 21:23:41,758 Epoch 490: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.34 
2024-02-02 21:23:41,758 EPOCH 491
2024-02-02 21:23:47,314 Epoch 491: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.34 
2024-02-02 21:23:47,315 EPOCH 492
2024-02-02 21:23:47,530 [Epoch: 492 Step: 00032900] Batch Recognition Loss:   0.000438 => Gls Tokens per Sec:     2243 || Batch Translation Loss:   0.032832 => Txt Tokens per Sec:     5416 || Lr: 0.000100
2024-02-02 21:23:52,416 Epoch 492: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.08 
2024-02-02 21:23:52,417 EPOCH 493
2024-02-02 21:23:55,515 [Epoch: 493 Step: 00033000] Batch Recognition Loss:   0.000248 => Gls Tokens per Sec:     1831 || Batch Translation Loss:   0.090874 => Txt Tokens per Sec:     5092 || Lr: 0.000100
2024-02-02 21:23:58,029 Epoch 493: Total Training Recognition Loss 0.02  Total Training Translation Loss 8.09 
2024-02-02 21:23:58,029 EPOCH 494
2024-02-02 21:24:03,349 Epoch 494: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.69 
2024-02-02 21:24:03,350 EPOCH 495
2024-02-02 21:24:03,469 [Epoch: 495 Step: 00033100] Batch Recognition Loss:   0.000270 => Gls Tokens per Sec:     2712 || Batch Translation Loss:   0.034659 => Txt Tokens per Sec:     6042 || Lr: 0.000100
2024-02-02 21:24:08,367 Epoch 495: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.06 
2024-02-02 21:24:08,367 EPOCH 496
2024-02-02 21:24:11,180 [Epoch: 496 Step: 00033200] Batch Recognition Loss:   0.000528 => Gls Tokens per Sec:     1960 || Batch Translation Loss:   0.080137 => Txt Tokens per Sec:     5494 || Lr: 0.000100
2024-02-02 21:24:13,691 Epoch 496: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.25 
2024-02-02 21:24:13,692 EPOCH 497
2024-02-02 21:24:18,682 Epoch 497: Total Training Recognition Loss 0.04  Total Training Translation Loss 6.99 
2024-02-02 21:24:18,682 EPOCH 498
2024-02-02 21:24:18,732 [Epoch: 498 Step: 00033300] Batch Recognition Loss:   0.000200 => Gls Tokens per Sec:     3265 || Batch Translation Loss:   0.066659 => Txt Tokens per Sec:     7306 || Lr: 0.000100
2024-02-02 21:24:23,782 Epoch 498: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.00 
2024-02-02 21:24:23,782 EPOCH 499
2024-02-02 21:24:26,522 [Epoch: 499 Step: 00033400] Batch Recognition Loss:   0.000240 => Gls Tokens per Sec:     1986 || Batch Translation Loss:   0.076463 => Txt Tokens per Sec:     5480 || Lr: 0.000100
2024-02-02 21:24:29,205 Epoch 499: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.68 
2024-02-02 21:24:29,205 EPOCH 500
2024-02-02 21:24:34,603 [Epoch: 500 Step: 00033500] Batch Recognition Loss:   0.000436 => Gls Tokens per Sec:     1970 || Batch Translation Loss:   0.265320 => Txt Tokens per Sec:     5468 || Lr: 0.000100
2024-02-02 21:24:34,604 Epoch 500: Total Training Recognition Loss 0.04  Total Training Translation Loss 7.50 
2024-02-02 21:24:34,604 EPOCH 501
2024-02-02 21:24:40,055 Epoch 501: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.15 
2024-02-02 21:24:40,056 EPOCH 502
2024-02-02 21:24:42,949 [Epoch: 502 Step: 00033600] Batch Recognition Loss:   0.000212 => Gls Tokens per Sec:     1795 || Batch Translation Loss:   0.102087 => Txt Tokens per Sec:     5098 || Lr: 0.000100
2024-02-02 21:24:45,644 Epoch 502: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.15 
2024-02-02 21:24:45,645 EPOCH 503
2024-02-02 21:24:50,915 [Epoch: 503 Step: 00033700] Batch Recognition Loss:   0.000174 => Gls Tokens per Sec:     1987 || Batch Translation Loss:   0.039650 => Txt Tokens per Sec:     5517 || Lr: 0.000100
2024-02-02 21:24:50,984 Epoch 503: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.34 
2024-02-02 21:24:50,985 EPOCH 504
2024-02-02 21:24:56,220 Epoch 504: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.37 
2024-02-02 21:24:56,221 EPOCH 505
2024-02-02 21:24:58,603 [Epoch: 505 Step: 00033800] Batch Recognition Loss:   0.000210 => Gls Tokens per Sec:     2149 || Batch Translation Loss:   0.048196 => Txt Tokens per Sec:     6015 || Lr: 0.000100
2024-02-02 21:25:01,260 Epoch 505: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.76 
2024-02-02 21:25:01,260 EPOCH 506
2024-02-02 21:25:06,264 [Epoch: 506 Step: 00033900] Batch Recognition Loss:   0.000480 => Gls Tokens per Sec:     2061 || Batch Translation Loss:   0.306768 => Txt Tokens per Sec:     5696 || Lr: 0.000100
2024-02-02 21:25:06,445 Epoch 506: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.62 
2024-02-02 21:25:06,445 EPOCH 507
2024-02-02 21:25:12,050 Epoch 507: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.44 
2024-02-02 21:25:12,050 EPOCH 508
2024-02-02 21:25:14,473 [Epoch: 508 Step: 00034000] Batch Recognition Loss:   0.000239 => Gls Tokens per Sec:     2049 || Batch Translation Loss:   0.036463 => Txt Tokens per Sec:     5674 || Lr: 0.000100
2024-02-02 21:25:23,096 Validation result at epoch 508, step    34000: duration: 8.6222s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00176	Translation Loss: 92418.50000	PPL: 10387.14648
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.38	(BLEU-1: 10.11,	BLEU-2: 3.13,	BLEU-3: 1.15,	BLEU-4: 0.38)
	CHRF 17.11	ROUGE 8.39
2024-02-02 21:25:23,097 Logging Recognition and Translation Outputs
2024-02-02 21:25:23,097 ========================================================================================================================
2024-02-02 21:25:23,097 Logging Sequence: 92_199.00
2024-02-02 21:25:23,097 	Gloss Reference :	A B+C+D+E
2024-02-02 21:25:23,097 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:25:23,097 	Gloss Alignment :	         
2024-02-02 21:25:23,098 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:25:23,098 	Text Reference  :	*** people on     social media said that   
2024-02-02 21:25:23,098 	Text Hypothesis :	now he     became one    over  his  brother
2024-02-02 21:25:23,098 	Text Alignment  :	I   S      S      S      S     S    S      
2024-02-02 21:25:23,098 ========================================================================================================================
2024-02-02 21:25:23,098 Logging Sequence: 109_64.00
2024-02-02 21:25:23,099 	Gloss Reference :	A B+C+D+E
2024-02-02 21:25:23,099 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:25:23,099 	Gloss Alignment :	         
2024-02-02 21:25:23,099 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:25:23,100 	Text Reference  :	the     2 players as   well as    the ******** entire kkr team have     been  quarantined
2024-02-02 21:25:23,100 	Text Hypothesis :	however a medical team will check the contacts of     the two  positive cases everyday   
2024-02-02 21:25:23,100 	Text Alignment  :	S       S S       S    S    S         I        S      S   S    S        S     S          
2024-02-02 21:25:23,101 ========================================================================================================================
2024-02-02 21:25:23,101 Logging Sequence: 84_108.00
2024-02-02 21:25:23,101 	Gloss Reference :	A B+C+D+E
2024-02-02 21:25:23,101 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:25:23,101 	Gloss Alignment :	         
2024-02-02 21:25:23,101 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:25:23,103 	Text Reference  :	so in order to show their protest they     covered their mouth in    the   photos which then went   viral
2024-02-02 21:25:23,103 	Text Hypothesis :	** ** ***** ** **** isn't that    shocking 2022    you   are   still going to     focus on   social media
2024-02-02 21:25:23,103 	Text Alignment  :	D  D  D     D  D    S     S       S        S       S     S     S     S     S      S     S    S      S    
2024-02-02 21:25:23,103 ========================================================================================================================
2024-02-02 21:25:23,103 Logging Sequence: 115_24.00
2024-02-02 21:25:23,103 	Gloss Reference :	A B+C+D+E
2024-02-02 21:25:23,103 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:25:23,103 	Gloss Alignment :	         
2024-02-02 21:25:23,104 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:25:23,104 	Text Reference  :	bumrah also did not   participate in the 5        match t20 series  
2024-02-02 21:25:23,104 	Text Hypothesis :	****** **** the image was         of his marriage to    be  together
2024-02-02 21:25:23,105 	Text Alignment  :	D      D    S   S     S           S  S   S        S     S   S       
2024-02-02 21:25:23,105 ========================================================================================================================
2024-02-02 21:25:23,105 Logging Sequence: 96_129.00
2024-02-02 21:25:23,105 	Gloss Reference :	A B+C+D+E
2024-02-02 21:25:23,105 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:25:23,105 	Gloss Alignment :	         
2024-02-02 21:25:23,105 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:25:23,106 	Text Reference  :	***** *** ****** *** **** **** *** **** viewers were  very stressed
2024-02-02 21:25:23,106 	Text Hypothesis :	while the couple met with this win will be      happy to   win     
2024-02-02 21:25:23,106 	Text Alignment  :	I     I   I      I   I    I    I   I    S       S     S    S       
2024-02-02 21:25:23,106 ========================================================================================================================
2024-02-02 21:25:26,101 Epoch 508: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.13 
2024-02-02 21:25:26,101 EPOCH 509
2024-02-02 21:25:31,121 [Epoch: 509 Step: 00034100] Batch Recognition Loss:   0.000296 => Gls Tokens per Sec:     2023 || Batch Translation Loss:   0.061504 => Txt Tokens per Sec:     5625 || Lr: 0.000100
2024-02-02 21:25:31,320 Epoch 509: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.40 
2024-02-02 21:25:31,320 EPOCH 510
2024-02-02 21:25:36,870 Epoch 510: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.95 
2024-02-02 21:25:36,870 EPOCH 511
2024-02-02 21:25:39,100 [Epoch: 511 Step: 00034200] Batch Recognition Loss:   0.000290 => Gls Tokens per Sec:     2114 || Batch Translation Loss:   0.030098 => Txt Tokens per Sec:     5735 || Lr: 0.000100
2024-02-02 21:25:41,944 Epoch 511: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.77 
2024-02-02 21:25:41,945 EPOCH 512
2024-02-02 21:25:47,236 [Epoch: 512 Step: 00034300] Batch Recognition Loss:   0.000177 => Gls Tokens per Sec:     1889 || Batch Translation Loss:   0.064989 => Txt Tokens per Sec:     5291 || Lr: 0.000100
2024-02-02 21:25:47,503 Epoch 512: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.15 
2024-02-02 21:25:47,504 EPOCH 513
2024-02-02 21:25:52,750 Epoch 513: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.19 
2024-02-02 21:25:52,751 EPOCH 514
2024-02-02 21:25:55,114 [Epoch: 514 Step: 00034400] Batch Recognition Loss:   0.000242 => Gls Tokens per Sec:     1926 || Batch Translation Loss:   0.106364 => Txt Tokens per Sec:     5535 || Lr: 0.000100
2024-02-02 21:25:57,993 Epoch 514: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.26 
2024-02-02 21:25:57,994 EPOCH 515
2024-02-02 21:26:03,186 [Epoch: 515 Step: 00034500] Batch Recognition Loss:   0.000600 => Gls Tokens per Sec:     1894 || Batch Translation Loss:   0.044934 => Txt Tokens per Sec:     5309 || Lr: 0.000100
2024-02-02 21:26:03,556 Epoch 515: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.33 
2024-02-02 21:26:03,556 EPOCH 516
2024-02-02 21:26:08,412 Epoch 516: Total Training Recognition Loss 0.02  Total Training Translation Loss 9.62 
2024-02-02 21:26:08,413 EPOCH 517
2024-02-02 21:26:10,352 [Epoch: 517 Step: 00034600] Batch Recognition Loss:   0.000345 => Gls Tokens per Sec:     2310 || Batch Translation Loss:   0.090264 => Txt Tokens per Sec:     6437 || Lr: 0.000100
2024-02-02 21:26:13,355 Epoch 517: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.59 
2024-02-02 21:26:13,355 EPOCH 518
2024-02-02 21:26:18,511 [Epoch: 518 Step: 00034700] Batch Recognition Loss:   0.000234 => Gls Tokens per Sec:     1876 || Batch Translation Loss:   0.054383 => Txt Tokens per Sec:     5208 || Lr: 0.000100
2024-02-02 21:26:18,937 Epoch 518: Total Training Recognition Loss 0.02  Total Training Translation Loss 9.01 
2024-02-02 21:26:18,937 EPOCH 519
2024-02-02 21:26:23,806 Epoch 519: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.99 
2024-02-02 21:26:23,807 EPOCH 520
2024-02-02 21:26:25,697 [Epoch: 520 Step: 00034800] Batch Recognition Loss:   0.000358 => Gls Tokens per Sec:     2286 || Batch Translation Loss:   0.051643 => Txt Tokens per Sec:     6300 || Lr: 0.000100
2024-02-02 21:26:28,635 Epoch 520: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.19 
2024-02-02 21:26:28,635 EPOCH 521
2024-02-02 21:26:33,419 [Epoch: 521 Step: 00034900] Batch Recognition Loss:   0.000171 => Gls Tokens per Sec:     1988 || Batch Translation Loss:   0.029456 => Txt Tokens per Sec:     5499 || Lr: 0.000100
2024-02-02 21:26:34,023 Epoch 521: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.52 
2024-02-02 21:26:34,023 EPOCH 522
2024-02-02 21:26:39,302 Epoch 522: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.45 
2024-02-02 21:26:39,302 EPOCH 523
2024-02-02 21:26:41,390 [Epoch: 523 Step: 00035000] Batch Recognition Loss:   0.000462 => Gls Tokens per Sec:     1993 || Batch Translation Loss:   0.038568 => Txt Tokens per Sec:     5664 || Lr: 0.000100
2024-02-02 21:26:44,470 Epoch 523: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.19 
2024-02-02 21:26:44,470 EPOCH 524
2024-02-02 21:26:49,205 [Epoch: 524 Step: 00035100] Batch Recognition Loss:   0.000281 => Gls Tokens per Sec:     1975 || Batch Translation Loss:   0.033534 => Txt Tokens per Sec:     5429 || Lr: 0.000100
2024-02-02 21:26:49,982 Epoch 524: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.30 
2024-02-02 21:26:49,982 EPOCH 525
2024-02-02 21:26:55,094 Epoch 525: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.01 
2024-02-02 21:26:55,094 EPOCH 526
2024-02-02 21:26:57,040 [Epoch: 526 Step: 00035200] Batch Recognition Loss:   0.000227 => Gls Tokens per Sec:     2057 || Batch Translation Loss:   0.015195 => Txt Tokens per Sec:     5636 || Lr: 0.000100
2024-02-02 21:27:00,275 Epoch 526: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.18 
2024-02-02 21:27:00,275 EPOCH 527
2024-02-02 21:27:04,639 [Epoch: 527 Step: 00035300] Batch Recognition Loss:   0.000338 => Gls Tokens per Sec:     2107 || Batch Translation Loss:   0.041663 => Txt Tokens per Sec:     5738 || Lr: 0.000100
2024-02-02 21:27:05,605 Epoch 527: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.64 
2024-02-02 21:27:05,605 EPOCH 528
2024-02-02 21:27:11,222 Epoch 528: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.10 
2024-02-02 21:27:11,222 EPOCH 529
2024-02-02 21:27:13,182 [Epoch: 529 Step: 00035400] Batch Recognition Loss:   0.000244 => Gls Tokens per Sec:     1960 || Batch Translation Loss:   0.022954 => Txt Tokens per Sec:     5382 || Lr: 0.000100
2024-02-02 21:27:16,821 Epoch 529: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.00 
2024-02-02 21:27:16,821 EPOCH 530
2024-02-02 21:27:21,188 [Epoch: 530 Step: 00035500] Batch Recognition Loss:   0.000288 => Gls Tokens per Sec:     2068 || Batch Translation Loss:   0.198069 => Txt Tokens per Sec:     5726 || Lr: 0.000100
2024-02-02 21:27:22,163 Epoch 530: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.64 
2024-02-02 21:27:22,164 EPOCH 531
2024-02-02 21:27:27,404 Epoch 531: Total Training Recognition Loss 0.02  Total Training Translation Loss 8.54 
2024-02-02 21:27:27,404 EPOCH 532
2024-02-02 21:27:29,013 [Epoch: 532 Step: 00035600] Batch Recognition Loss:   0.000464 => Gls Tokens per Sec:     2291 || Batch Translation Loss:   0.088669 => Txt Tokens per Sec:     6021 || Lr: 0.000100
2024-02-02 21:27:33,012 Epoch 532: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.36 
2024-02-02 21:27:33,012 EPOCH 533
2024-02-02 21:27:37,610 [Epoch: 533 Step: 00035700] Batch Recognition Loss:   0.000286 => Gls Tokens per Sec:     1930 || Batch Translation Loss:   0.038243 => Txt Tokens per Sec:     5376 || Lr: 0.000100
2024-02-02 21:27:38,539 Epoch 533: Total Training Recognition Loss 0.02  Total Training Translation Loss 8.42 
2024-02-02 21:27:38,540 EPOCH 534
2024-02-02 21:27:43,925 Epoch 534: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.11 
2024-02-02 21:27:43,926 EPOCH 535
2024-02-02 21:27:45,388 [Epoch: 535 Step: 00035800] Batch Recognition Loss:   0.000246 => Gls Tokens per Sec:     2409 || Batch Translation Loss:   0.027608 => Txt Tokens per Sec:     6413 || Lr: 0.000100
2024-02-02 21:27:48,838 Epoch 535: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.88 
2024-02-02 21:27:48,839 EPOCH 536
2024-02-02 21:27:53,424 [Epoch: 536 Step: 00035900] Batch Recognition Loss:   0.000258 => Gls Tokens per Sec:     1900 || Batch Translation Loss:   0.143341 => Txt Tokens per Sec:     5325 || Lr: 0.000100
2024-02-02 21:27:54,317 Epoch 536: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.95 
2024-02-02 21:27:54,317 EPOCH 537
2024-02-02 21:27:59,999 Epoch 537: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.60 
2024-02-02 21:28:00,000 EPOCH 538
2024-02-02 21:28:01,683 [Epoch: 538 Step: 00036000] Batch Recognition Loss:   0.000476 => Gls Tokens per Sec:     1944 || Batch Translation Loss:   0.015080 => Txt Tokens per Sec:     5398 || Lr: 0.000100
2024-02-02 21:28:10,019 Validation result at epoch 538, step    36000: duration: 8.3351s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00120	Translation Loss: 92463.72656	PPL: 10434.25684
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.81	(BLEU-1: 10.30,	BLEU-2: 3.50,	BLEU-3: 1.56,	BLEU-4: 0.81)
	CHRF 17.12	ROUGE 8.85
2024-02-02 21:28:10,020 Logging Recognition and Translation Outputs
2024-02-02 21:28:10,020 ========================================================================================================================
2024-02-02 21:28:10,020 Logging Sequence: 78_198.00
2024-02-02 21:28:10,020 	Gloss Reference :	A B+C+D+E
2024-02-02 21:28:10,021 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:28:10,021 	Gloss Alignment :	         
2024-02-02 21:28:10,021 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:28:10,022 	Text Reference  :	******* **** ** ***** ** *** *** ****** they have been  flooded with congratulations comments  
2024-02-02 21:28:10,022 	Text Hypothesis :	england lost to italy in the ipl rights for  each other teams   of   the             semi-final
2024-02-02 21:28:10,022 	Text Alignment  :	I       I    I  I     I  I   I   I      S    S    S     S       S    S               S         
2024-02-02 21:28:10,022 ========================================================================================================================
2024-02-02 21:28:10,022 Logging Sequence: 145_216.00
2024-02-02 21:28:10,022 	Gloss Reference :	A B+C+D+E
2024-02-02 21:28:10,022 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:28:10,022 	Gloss Alignment :	         
2024-02-02 21:28:10,023 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:28:10,024 	Text Reference  :	asking him to include sameeha in  the  world championship as      she was a       talented athlete  
2024-02-02 21:28:10,024 	Text Hypothesis :	****** *** ** ******* he      has also taken it's         revenge by  his batting and      captaincy
2024-02-02 21:28:10,024 	Text Alignment  :	D      D   D  D       S       S   S    S     S            S       S   S   S       S        S        
2024-02-02 21:28:10,024 ========================================================================================================================
2024-02-02 21:28:10,024 Logging Sequence: 70_137.00
2024-02-02 21:28:10,024 	Gloss Reference :	A B+C+D+E
2024-02-02 21:28:10,024 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:28:10,024 	Gloss Alignment :	         
2024-02-02 21:28:10,025 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:28:10,026 	Text Reference  :	the ******** small gesture appeared    to  encourage people to  drink water instead    of aerated drinks  
2024-02-02 21:28:10,026 	Text Hypothesis :	the olympics still haven't apprehended the olympic   games  and she   took  spectators to various olympics
2024-02-02 21:28:10,026 	Text Alignment  :	    I        S     S       S           S   S         S      S   S     S     S          S  S       S       
2024-02-02 21:28:10,026 ========================================================================================================================
2024-02-02 21:28:10,026 Logging Sequence: 119_20.00
2024-02-02 21:28:10,027 	Gloss Reference :	A B+C+D+E
2024-02-02 21:28:10,027 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:28:10,027 	Gloss Alignment :	         
2024-02-02 21:28:10,027 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:28:10,028 	Text Reference  :	messi intended to  gift  something to      all the  players and the staff to special to     celebrate the     moment
2024-02-02 21:28:10,029 	Text Hypothesis :	as    per      the mayor of        rosario is  also termed  as  the ***** ** ******* reason for       idesign gold  
2024-02-02 21:28:10,029 	Text Alignment  :	S     S        S   S     S         S       S   S    S       S       D     D  D       S      S         S       S     
2024-02-02 21:28:10,029 ========================================================================================================================
2024-02-02 21:28:10,029 Logging Sequence: 106_15.00
2024-02-02 21:28:10,029 	Gloss Reference :	A B+C+D+E
2024-02-02 21:28:10,029 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:28:10,029 	Gloss Alignment :	         
2024-02-02 21:28:10,029 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:28:10,030 	Text Reference  :	but what about   women's cricket   earlier we      never spoke about it ** **** **
2024-02-02 21:28:10,031 	Text Hypothesis :	so  what amazing women   wrestlers are     stunned to    know  that  it is also up
2024-02-02 21:28:10,031 	Text Alignment  :	S        S       S       S         S       S       S     S     S        I  I    I 
2024-02-02 21:28:10,031 ========================================================================================================================
2024-02-02 21:28:13,826 Epoch 538: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.19 
2024-02-02 21:28:13,827 EPOCH 539
2024-02-02 21:28:17,586 [Epoch: 539 Step: 00036100] Batch Recognition Loss:   0.000201 => Gls Tokens per Sec:     2274 || Batch Translation Loss:   0.033515 => Txt Tokens per Sec:     6302 || Lr: 0.000050
2024-02-02 21:28:18,608 Epoch 539: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.70 
2024-02-02 21:28:18,608 EPOCH 540
2024-02-02 21:28:24,307 Epoch 540: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.53 
2024-02-02 21:28:24,308 EPOCH 541
2024-02-02 21:28:25,972 [Epoch: 541 Step: 00036200] Batch Recognition Loss:   0.000170 => Gls Tokens per Sec:     1871 || Batch Translation Loss:   0.041080 => Txt Tokens per Sec:     5060 || Lr: 0.000050
2024-02-02 21:28:29,742 Epoch 541: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.44 
2024-02-02 21:28:29,742 EPOCH 542
2024-02-02 21:28:33,773 [Epoch: 542 Step: 00036300] Batch Recognition Loss:   0.000271 => Gls Tokens per Sec:     2105 || Batch Translation Loss:   0.014268 => Txt Tokens per Sec:     5846 || Lr: 0.000050
2024-02-02 21:28:34,962 Epoch 542: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.33 
2024-02-02 21:28:34,962 EPOCH 543
2024-02-02 21:28:40,470 Epoch 543: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.29 
2024-02-02 21:28:40,471 EPOCH 544
2024-02-02 21:28:41,699 [Epoch: 544 Step: 00036400] Batch Recognition Loss:   0.000156 => Gls Tokens per Sec:     2475 || Batch Translation Loss:   0.016994 => Txt Tokens per Sec:     6821 || Lr: 0.000050
2024-02-02 21:28:45,830 Epoch 544: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.97 
2024-02-02 21:28:45,830 EPOCH 545
2024-02-02 21:28:49,737 [Epoch: 545 Step: 00036500] Batch Recognition Loss:   0.000141 => Gls Tokens per Sec:     2131 || Batch Translation Loss:   0.014634 => Txt Tokens per Sec:     5880 || Lr: 0.000050
2024-02-02 21:28:50,921 Epoch 545: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-02 21:28:50,921 EPOCH 546
2024-02-02 21:28:56,495 Epoch 546: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.91 
2024-02-02 21:28:56,495 EPOCH 547
2024-02-02 21:28:57,870 [Epoch: 547 Step: 00036600] Batch Recognition Loss:   0.000126 => Gls Tokens per Sec:     2098 || Batch Translation Loss:   0.016801 => Txt Tokens per Sec:     5747 || Lr: 0.000050
2024-02-02 21:29:02,063 Epoch 547: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.88 
2024-02-02 21:29:02,063 EPOCH 548
2024-02-02 21:29:05,830 [Epoch: 548 Step: 00036700] Batch Recognition Loss:   0.000189 => Gls Tokens per Sec:     2167 || Batch Translation Loss:   0.012359 => Txt Tokens per Sec:     5948 || Lr: 0.000050
2024-02-02 21:29:07,274 Epoch 548: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.85 
2024-02-02 21:29:07,275 EPOCH 549
2024-02-02 21:29:12,842 Epoch 549: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.01 
2024-02-02 21:29:12,843 EPOCH 550
2024-02-02 21:29:14,157 [Epoch: 550 Step: 00036800] Batch Recognition Loss:   0.000176 => Gls Tokens per Sec:     2073 || Batch Translation Loss:   0.014054 => Txt Tokens per Sec:     6037 || Lr: 0.000050
2024-02-02 21:29:18,171 Epoch 550: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.48 
2024-02-02 21:29:18,171 EPOCH 551
2024-02-02 21:29:22,327 [Epoch: 551 Step: 00036900] Batch Recognition Loss:   0.000107 => Gls Tokens per Sec:     1925 || Batch Translation Loss:   0.004854 => Txt Tokens per Sec:     5322 || Lr: 0.000050
2024-02-02 21:29:23,646 Epoch 551: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.00 
2024-02-02 21:29:23,646 EPOCH 552
2024-02-02 21:29:29,065 Epoch 552: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.57 
2024-02-02 21:29:29,066 EPOCH 553
2024-02-02 21:29:30,316 [Epoch: 553 Step: 00037000] Batch Recognition Loss:   0.000200 => Gls Tokens per Sec:     2050 || Batch Translation Loss:   0.010437 => Txt Tokens per Sec:     5661 || Lr: 0.000050
2024-02-02 21:29:34,273 Epoch 553: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.79 
2024-02-02 21:29:34,274 EPOCH 554
2024-02-02 21:29:38,554 [Epoch: 554 Step: 00037100] Batch Recognition Loss:   0.000173 => Gls Tokens per Sec:     1811 || Batch Translation Loss:   0.034328 => Txt Tokens per Sec:     5094 || Lr: 0.000050
2024-02-02 21:29:39,868 Epoch 554: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.24 
2024-02-02 21:29:39,868 EPOCH 555
2024-02-02 21:29:45,066 Epoch 555: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.30 
2024-02-02 21:29:45,066 EPOCH 556
2024-02-02 21:29:46,253 [Epoch: 556 Step: 00037200] Batch Recognition Loss:   0.000161 => Gls Tokens per Sec:     1948 || Batch Translation Loss:   0.015342 => Txt Tokens per Sec:     5268 || Lr: 0.000050
2024-02-02 21:29:50,322 Epoch 556: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.57 
2024-02-02 21:29:50,323 EPOCH 557
2024-02-02 21:29:54,454 [Epoch: 557 Step: 00037300] Batch Recognition Loss:   0.000191 => Gls Tokens per Sec:     1838 || Batch Translation Loss:   0.013197 => Txt Tokens per Sec:     5208 || Lr: 0.000050
2024-02-02 21:29:55,907 Epoch 557: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.29 
2024-02-02 21:29:55,908 EPOCH 558
2024-02-02 21:30:01,274 Epoch 558: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.15 
2024-02-02 21:30:01,274 EPOCH 559
2024-02-02 21:30:02,318 [Epoch: 559 Step: 00037400] Batch Recognition Loss:   0.000200 => Gls Tokens per Sec:     2148 || Batch Translation Loss:   0.012407 => Txt Tokens per Sec:     6197 || Lr: 0.000050
2024-02-02 21:30:06,282 Epoch 559: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.42 
2024-02-02 21:30:06,282 EPOCH 560
2024-02-02 21:30:10,056 [Epoch: 560 Step: 00037500] Batch Recognition Loss:   0.000196 => Gls Tokens per Sec:     1969 || Batch Translation Loss:   0.022326 => Txt Tokens per Sec:     5443 || Lr: 0.000050
2024-02-02 21:30:11,517 Epoch 560: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.03 
2024-02-02 21:30:11,518 EPOCH 561
2024-02-02 21:30:16,540 Epoch 561: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.79 
2024-02-02 21:30:16,540 EPOCH 562
2024-02-02 21:30:17,606 [Epoch: 562 Step: 00037600] Batch Recognition Loss:   0.000246 => Gls Tokens per Sec:     1955 || Batch Translation Loss:   0.021869 => Txt Tokens per Sec:     5578 || Lr: 0.000050
2024-02-02 21:30:21,709 Epoch 562: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.26 
2024-02-02 21:30:21,709 EPOCH 563
2024-02-02 21:30:25,323 [Epoch: 563 Step: 00037700] Batch Recognition Loss:   0.000307 => Gls Tokens per Sec:     2012 || Batch Translation Loss:   0.007301 => Txt Tokens per Sec:     5571 || Lr: 0.000050
2024-02-02 21:30:26,973 Epoch 563: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.15 
2024-02-02 21:30:26,973 EPOCH 564
2024-02-02 21:30:32,060 Epoch 564: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.18 
2024-02-02 21:30:32,060 EPOCH 565
2024-02-02 21:30:33,292 [Epoch: 565 Step: 00037800] Batch Recognition Loss:   0.000229 => Gls Tokens per Sec:     1487 || Batch Translation Loss:   0.028847 => Txt Tokens per Sec:     4383 || Lr: 0.000050
2024-02-02 21:30:37,466 Epoch 565: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.83 
2024-02-02 21:30:37,466 EPOCH 566
2024-02-02 21:30:40,523 [Epoch: 566 Step: 00037900] Batch Recognition Loss:   0.000248 => Gls Tokens per Sec:     2326 || Batch Translation Loss:   0.033525 => Txt Tokens per Sec:     6480 || Lr: 0.000050
2024-02-02 21:30:42,337 Epoch 566: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.68 
2024-02-02 21:30:42,337 EPOCH 567
2024-02-02 21:30:47,869 Epoch 567: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.08 
2024-02-02 21:30:47,869 EPOCH 568
2024-02-02 21:30:48,548 [Epoch: 568 Step: 00038000] Batch Recognition Loss:   0.000165 => Gls Tokens per Sec:     2603 || Batch Translation Loss:   0.009297 => Txt Tokens per Sec:     7148 || Lr: 0.000050
2024-02-02 21:30:56,949 Validation result at epoch 568, step    38000: duration: 8.4009s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00120	Translation Loss: 93236.91406	PPL: 11273.64648
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.74	(BLEU-1: 10.16,	BLEU-2: 3.29,	BLEU-3: 1.45,	BLEU-4: 0.74)
	CHRF 16.99	ROUGE 8.85
2024-02-02 21:30:56,950 Logging Recognition and Translation Outputs
2024-02-02 21:30:56,950 ========================================================================================================================
2024-02-02 21:30:56,950 Logging Sequence: 72_194.00
2024-02-02 21:30:56,950 	Gloss Reference :	A B+C+D+E
2024-02-02 21:30:56,950 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:30:56,950 	Gloss Alignment :	         
2024-02-02 21:30:56,951 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:30:56,951 	Text Reference  :	shah told her to do what she   wants and  filed  a    police complaint against her    
2024-02-02 21:30:56,952 	Text Hypothesis :	**** **** *** ** ** **** babar kept  such people were glued  to        their   victory
2024-02-02 21:30:56,952 	Text Alignment  :	D    D    D   D  D  D    S     S     S    S      S    S      S         S       S      
2024-02-02 21:30:56,952 ========================================================================================================================
2024-02-02 21:30:56,952 Logging Sequence: 108_59.00
2024-02-02 21:30:56,952 	Gloss Reference :	A B+C+D+E
2024-02-02 21:30:56,952 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:30:56,952 	Gloss Alignment :	         
2024-02-02 21:30:56,953 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:30:56,954 	Text Reference  :	ishan kishan remained the biggest buy of ipl   as mumbai indians paid     a         whopping rs     1525 crore to keep him   
2024-02-02 21:30:56,954 	Text Hypothesis :	***** ****** ******** *** ******* he  is known as the    most    followed cricketer with     people are  when  2  of   vamika
2024-02-02 21:30:56,954 	Text Alignment  :	D     D      D        D   D       S   S  S        S      S       S        S         S        S      S    S     S  S    S     
2024-02-02 21:30:56,955 ========================================================================================================================
2024-02-02 21:30:56,955 Logging Sequence: 109_10.00
2024-02-02 21:30:56,955 	Gloss Reference :	A B+C+D+E
2024-02-02 21:30:56,955 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:30:56,955 	Gloss Alignment :	         
2024-02-02 21:30:56,955 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:30:56,956 	Text Reference  :	was scheduled to    be  played at      the   narendra modi  stadium in  ahmedabad
2024-02-02 21:30:56,956 	Text Hypothesis :	*** ********* there are many   batsmen about 4        times if      any players  
2024-02-02 21:30:56,956 	Text Alignment  :	D   D         S     S   S      S       S     S        S     S       S   S        
2024-02-02 21:30:56,956 ========================================================================================================================
2024-02-02 21:30:56,957 Logging Sequence: 103_202.00
2024-02-02 21:30:56,957 	Gloss Reference :	A B+C+D+E
2024-02-02 21:30:56,957 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:30:56,957 	Gloss Alignment :	         
2024-02-02 21:30:56,957 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:30:56,958 	Text Reference  :	india in total has won 61           medals including 22           gold medals 16      silver    medals 23     bronze      medals
2024-02-02 21:30:56,958 	Text Hypothesis :	***** ** ***** *** *** commonwealth games  encourage independence from the    british democracy human  rights development etc   
2024-02-02 21:30:56,959 	Text Alignment  :	D     D  D     D   D   S            S      S         S            S    S      S       S         S      S      S           S     
2024-02-02 21:30:56,959 ========================================================================================================================
2024-02-02 21:30:56,959 Logging Sequence: 149_77.00
2024-02-02 21:30:56,959 	Gloss Reference :	A B+C+D+E
2024-02-02 21:30:56,959 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:30:56,959 	Gloss Alignment :	         
2024-02-02 21:30:56,959 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:30:56,961 	Text Reference  :	and arrested danushka for  alleged sexual assault  of a 29     year       old  woman whose name has not been disclosed
2024-02-02 21:30:56,961 	Text Hypothesis :	*** woman    from     rose bay     a      suburban of * sydney complained that she   was   not  get rs  1    crore    
2024-02-02 21:30:56,961 	Text Alignment  :	D   S        S        S    S       S      S           D S      S          S    S     S     S    S   S   S    S        
2024-02-02 21:30:56,962 ========================================================================================================================
2024-02-02 21:31:01,723 Epoch 568: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.05 
2024-02-02 21:31:01,723 EPOCH 569
2024-02-02 21:31:05,135 [Epoch: 569 Step: 00038100] Batch Recognition Loss:   0.000159 => Gls Tokens per Sec:     2064 || Batch Translation Loss:   0.020623 => Txt Tokens per Sec:     5696 || Lr: 0.000050
2024-02-02 21:31:07,139 Epoch 569: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.73 
2024-02-02 21:31:07,140 EPOCH 570
2024-02-02 21:31:12,137 Epoch 570: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.38 
2024-02-02 21:31:12,138 EPOCH 571
2024-02-02 21:31:12,986 [Epoch: 571 Step: 00038200] Batch Recognition Loss:   0.000143 => Gls Tokens per Sec:     1888 || Batch Translation Loss:   0.010057 => Txt Tokens per Sec:     5085 || Lr: 0.000050
2024-02-02 21:31:17,349 Epoch 571: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.91 
2024-02-02 21:31:17,350 EPOCH 572
2024-02-02 21:31:20,467 [Epoch: 572 Step: 00038300] Batch Recognition Loss:   0.000159 => Gls Tokens per Sec:     2209 || Batch Translation Loss:   0.006645 => Txt Tokens per Sec:     5987 || Lr: 0.000050
2024-02-02 21:31:22,652 Epoch 572: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.21 
2024-02-02 21:31:22,652 EPOCH 573
2024-02-02 21:31:27,894 Epoch 573: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.28 
2024-02-02 21:31:27,895 EPOCH 574
2024-02-02 21:31:28,513 [Epoch: 574 Step: 00038400] Batch Recognition Loss:   0.000148 => Gls Tokens per Sec:     2333 || Batch Translation Loss:   0.011850 => Txt Tokens per Sec:     6241 || Lr: 0.000050
2024-02-02 21:31:33,317 Epoch 574: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.11 
2024-02-02 21:31:33,317 EPOCH 575
2024-02-02 21:31:36,571 [Epoch: 575 Step: 00038500] Batch Recognition Loss:   0.000179 => Gls Tokens per Sec:     2038 || Batch Translation Loss:   0.015645 => Txt Tokens per Sec:     5649 || Lr: 0.000050
2024-02-02 21:31:38,639 Epoch 575: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.32 
2024-02-02 21:31:38,639 EPOCH 576
2024-02-02 21:31:43,926 Epoch 576: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.35 
2024-02-02 21:31:43,926 EPOCH 577
2024-02-02 21:31:44,568 [Epoch: 577 Step: 00038600] Batch Recognition Loss:   0.000157 => Gls Tokens per Sec:     1996 || Batch Translation Loss:   0.010787 => Txt Tokens per Sec:     5694 || Lr: 0.000050
2024-02-02 21:31:48,933 Epoch 577: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.41 
2024-02-02 21:31:48,934 EPOCH 578
2024-02-02 21:31:51,982 [Epoch: 578 Step: 00038700] Batch Recognition Loss:   0.000186 => Gls Tokens per Sec:     2154 || Batch Translation Loss:   0.015798 => Txt Tokens per Sec:     5991 || Lr: 0.000050
2024-02-02 21:31:54,133 Epoch 578: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.99 
2024-02-02 21:31:54,134 EPOCH 579
2024-02-02 21:31:59,250 Epoch 579: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.16 
2024-02-02 21:31:59,251 EPOCH 580
2024-02-02 21:31:59,813 [Epoch: 580 Step: 00038800] Batch Recognition Loss:   0.000166 => Gls Tokens per Sec:     1996 || Batch Translation Loss:   0.016106 => Txt Tokens per Sec:     5487 || Lr: 0.000050
2024-02-02 21:32:04,368 Epoch 580: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.22 
2024-02-02 21:32:04,368 EPOCH 581
2024-02-02 21:32:07,501 [Epoch: 581 Step: 00038900] Batch Recognition Loss:   0.000131 => Gls Tokens per Sec:     2044 || Batch Translation Loss:   0.015628 => Txt Tokens per Sec:     5814 || Lr: 0.000050
2024-02-02 21:32:09,748 Epoch 581: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.32 
2024-02-02 21:32:09,748 EPOCH 582
2024-02-02 21:32:14,641 Epoch 582: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.54 
2024-02-02 21:32:14,641 EPOCH 583
2024-02-02 21:32:15,001 [Epoch: 583 Step: 00039000] Batch Recognition Loss:   0.000217 => Gls Tokens per Sec:     2674 || Batch Translation Loss:   0.029178 => Txt Tokens per Sec:     6964 || Lr: 0.000050
2024-02-02 21:32:20,118 Epoch 583: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.22 
2024-02-02 21:32:20,119 EPOCH 584
2024-02-02 21:32:23,242 [Epoch: 584 Step: 00039100] Batch Recognition Loss:   0.000145 => Gls Tokens per Sec:     1970 || Batch Translation Loss:   0.044346 => Txt Tokens per Sec:     5681 || Lr: 0.000050
2024-02-02 21:32:25,610 Epoch 584: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.38 
2024-02-02 21:32:25,610 EPOCH 585
2024-02-02 21:32:30,901 Epoch 585: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.80 
2024-02-02 21:32:30,902 EPOCH 586
2024-02-02 21:32:31,421 [Epoch: 586 Step: 00039200] Batch Recognition Loss:   0.000342 => Gls Tokens per Sec:     1544 || Batch Translation Loss:   0.015027 => Txt Tokens per Sec:     4844 || Lr: 0.000050
2024-02-02 21:32:36,500 Epoch 586: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.33 
2024-02-02 21:32:36,501 EPOCH 587
2024-02-02 21:32:39,294 [Epoch: 587 Step: 00039300] Batch Recognition Loss:   0.000204 => Gls Tokens per Sec:     2144 || Batch Translation Loss:   0.116520 => Txt Tokens per Sec:     5863 || Lr: 0.000050
2024-02-02 21:32:41,676 Epoch 587: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.44 
2024-02-02 21:32:41,676 EPOCH 588
2024-02-02 21:32:47,066 Epoch 588: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.80 
2024-02-02 21:32:47,067 EPOCH 589
2024-02-02 21:32:47,369 [Epoch: 589 Step: 00039400] Batch Recognition Loss:   0.000169 => Gls Tokens per Sec:     2126 || Batch Translation Loss:   0.038942 => Txt Tokens per Sec:     6003 || Lr: 0.000050
2024-02-02 21:32:52,473 Epoch 589: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.23 
2024-02-02 21:32:52,474 EPOCH 590
2024-02-02 21:32:55,597 [Epoch: 590 Step: 00039500] Batch Recognition Loss:   0.000136 => Gls Tokens per Sec:     1867 || Batch Translation Loss:   0.024913 => Txt Tokens per Sec:     5285 || Lr: 0.000050
2024-02-02 21:32:57,862 Epoch 590: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-02 21:32:57,862 EPOCH 591
2024-02-02 21:33:03,382 Epoch 591: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.05 
2024-02-02 21:33:03,383 EPOCH 592
2024-02-02 21:33:03,618 [Epoch: 592 Step: 00039600] Batch Recognition Loss:   0.000158 => Gls Tokens per Sec:     2060 || Batch Translation Loss:   0.011586 => Txt Tokens per Sec:     5807 || Lr: 0.000050
2024-02-02 21:33:08,923 Epoch 592: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.93 
2024-02-02 21:33:08,924 EPOCH 593
2024-02-02 21:33:11,910 [Epoch: 593 Step: 00039700] Batch Recognition Loss:   0.000124 => Gls Tokens per Sec:     1930 || Batch Translation Loss:   0.012858 => Txt Tokens per Sec:     5514 || Lr: 0.000050
2024-02-02 21:33:14,402 Epoch 593: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.91 
2024-02-02 21:33:14,403 EPOCH 594
2024-02-02 21:33:19,703 Epoch 594: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.91 
2024-02-02 21:33:19,704 EPOCH 595
2024-02-02 21:33:19,821 [Epoch: 595 Step: 00039800] Batch Recognition Loss:   0.000156 => Gls Tokens per Sec:     2759 || Batch Translation Loss:   0.007782 => Txt Tokens per Sec:     6354 || Lr: 0.000050
2024-02-02 21:33:25,245 Epoch 595: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.13 
2024-02-02 21:33:25,246 EPOCH 596
2024-02-02 21:33:28,179 [Epoch: 596 Step: 00039900] Batch Recognition Loss:   0.000184 => Gls Tokens per Sec:     1879 || Batch Translation Loss:   0.011974 => Txt Tokens per Sec:     5463 || Lr: 0.000050
2024-02-02 21:33:30,621 Epoch 596: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.25 
2024-02-02 21:33:30,622 EPOCH 597
2024-02-02 21:33:35,896 Epoch 597: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.88 
2024-02-02 21:33:35,896 EPOCH 598
2024-02-02 21:33:36,018 [Epoch: 598 Step: 00040000] Batch Recognition Loss:   0.000173 => Gls Tokens per Sec:     1330 || Batch Translation Loss:   0.014964 => Txt Tokens per Sec:     4389 || Lr: 0.000050
2024-02-02 21:33:44,450 Validation result at epoch 598, step    40000: duration: 8.4323s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00089	Translation Loss: 91782.34375	PPL: 9746.50293
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.75	(BLEU-1: 10.52,	BLEU-2: 3.40,	BLEU-3: 1.48,	BLEU-4: 0.75)
	CHRF 17.21	ROUGE 9.11
2024-02-02 21:33:44,451 Logging Recognition and Translation Outputs
2024-02-02 21:33:44,451 ========================================================================================================================
2024-02-02 21:33:44,451 Logging Sequence: 123_104.00
2024-02-02 21:33:44,451 	Gloss Reference :	A B+C+D+E
2024-02-02 21:33:44,451 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:33:44,451 	Gloss Alignment :	         
2024-02-02 21:33:44,452 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:33:44,453 	Text Reference  :	***** *** *** the car  was presented to the former india cricketer from an unknown person  
2024-02-02 21:33:44,453 	Text Hypothesis :	india had won the toss and decided   to *** ****** ***** field     for  a  gold    medalist
2024-02-02 21:33:44,453 	Text Alignment  :	I     I   I       S    S   S            D   D      D     S         S    S  S       S       
2024-02-02 21:33:44,453 ========================================================================================================================
2024-02-02 21:33:44,453 Logging Sequence: 107_23.00
2024-02-02 21:33:44,453 	Gloss Reference :	A B+C+D+E
2024-02-02 21:33:44,453 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:33:44,454 	Gloss Alignment :	         
2024-02-02 21:33:44,454 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:33:44,454 	Text Reference  :	and viktor lilov  who is  also  from   the  usa 
2024-02-02 21:33:44,454 	Text Hypothesis :	*** ****** phogat won the delhi police made well
2024-02-02 21:33:44,454 	Text Alignment  :	D   D      S      S   S   S     S      S    S   
2024-02-02 21:33:44,455 ========================================================================================================================
2024-02-02 21:33:44,455 Logging Sequence: 134_212.00
2024-02-02 21:33:44,455 	Gloss Reference :	A B+C+D+E
2024-02-02 21:33:44,455 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:33:44,455 	Gloss Alignment :	         
2024-02-02 21:33:44,455 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:33:44,456 	Text Reference  :	** **** ******* ****** *** ******* dhanush said that         he practises little yoga 
2024-02-02 21:33:44,456 	Text Hypothesis :	so many hearing people are stunned at      your achievements i  am        so     proud
2024-02-02 21:33:44,456 	Text Alignment  :	I  I    I       I      I   I       S       S    S            S  S         S      S    
2024-02-02 21:33:44,456 ========================================================================================================================
2024-02-02 21:33:44,457 Logging Sequence: 165_577.00
2024-02-02 21:33:44,457 	Gloss Reference :	A B+C+D+E
2024-02-02 21:33:44,457 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:33:44,457 	Gloss Alignment :	         
2024-02-02 21:33:44,457 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:33:44,458 	Text Reference  :	then after 28 years india    won  the world cup again in   2011
2024-02-02 21:33:44,458 	Text Hypothesis :	**** it    is quite shocking that if  any   has been  many days
2024-02-02 21:33:44,458 	Text Alignment  :	D    S     S  S     S        S    S   S     S   S     S    S   
2024-02-02 21:33:44,458 ========================================================================================================================
2024-02-02 21:33:44,458 Logging Sequence: 88_142.00
2024-02-02 21:33:44,459 	Gloss Reference :	A B+C+D+E
2024-02-02 21:33:44,459 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:33:44,459 	Gloss Alignment :	         
2024-02-02 21:33:44,459 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:33:44,460 	Text Reference  :	*** this  is    because the police ***** does ******** ** ***** not     do  anything
2024-02-02 21:33:44,460 	Text Hypothesis :	the mayor added that    the police never does anything to catch however the ceremony
2024-02-02 21:33:44,460 	Text Alignment  :	I   S     S     S                  I          I        I  I     S       S   S       
2024-02-02 21:33:44,460 ========================================================================================================================
2024-02-02 21:33:50,121 Epoch 598: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.97 
2024-02-02 21:33:50,122 EPOCH 599
2024-02-02 21:33:52,853 [Epoch: 599 Step: 00040100] Batch Recognition Loss:   0.000269 => Gls Tokens per Sec:     1960 || Batch Translation Loss:   0.026501 => Txt Tokens per Sec:     5553 || Lr: 0.000050
2024-02-02 21:33:55,365 Epoch 599: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.58 
2024-02-02 21:33:55,365 EPOCH 600
2024-02-02 21:34:00,838 [Epoch: 600 Step: 00040200] Batch Recognition Loss:   0.000269 => Gls Tokens per Sec:     1943 || Batch Translation Loss:   0.023577 => Txt Tokens per Sec:     5392 || Lr: 0.000050
2024-02-02 21:34:00,839 Epoch 600: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.77 
2024-02-02 21:34:00,839 EPOCH 601
2024-02-02 21:34:05,959 Epoch 601: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.18 
2024-02-02 21:34:05,959 EPOCH 602
2024-02-02 21:34:08,736 [Epoch: 602 Step: 00040300] Batch Recognition Loss:   0.000365 => Gls Tokens per Sec:     1901 || Batch Translation Loss:   0.059833 => Txt Tokens per Sec:     5505 || Lr: 0.000050
2024-02-02 21:34:11,433 Epoch 602: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.84 
2024-02-02 21:34:11,433 EPOCH 603
2024-02-02 21:34:16,269 [Epoch: 603 Step: 00040400] Batch Recognition Loss:   0.000211 => Gls Tokens per Sec:     2166 || Batch Translation Loss:   0.069609 => Txt Tokens per Sec:     5984 || Lr: 0.000050
2024-02-02 21:34:16,417 Epoch 603: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.97 
2024-02-02 21:34:16,417 EPOCH 604
2024-02-02 21:34:21,823 Epoch 604: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.52 
2024-02-02 21:34:21,824 EPOCH 605
2024-02-02 21:34:24,198 [Epoch: 605 Step: 00040500] Batch Recognition Loss:   0.000346 => Gls Tokens per Sec:     2120 || Batch Translation Loss:   0.026658 => Txt Tokens per Sec:     6037 || Lr: 0.000050
2024-02-02 21:34:26,885 Epoch 605: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.66 
2024-02-02 21:34:26,886 EPOCH 606
2024-02-02 21:34:32,318 [Epoch: 606 Step: 00040600] Batch Recognition Loss:   0.000237 => Gls Tokens per Sec:     1899 || Batch Translation Loss:   0.010718 => Txt Tokens per Sec:     5241 || Lr: 0.000050
2024-02-02 21:34:32,526 Epoch 606: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.73 
2024-02-02 21:34:32,526 EPOCH 607
2024-02-02 21:34:38,115 Epoch 607: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.63 
2024-02-02 21:34:38,116 EPOCH 608
2024-02-02 21:34:40,498 [Epoch: 608 Step: 00040700] Batch Recognition Loss:   0.000260 => Gls Tokens per Sec:     2083 || Batch Translation Loss:   0.019576 => Txt Tokens per Sec:     5988 || Lr: 0.000050
2024-02-02 21:34:43,437 Epoch 608: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.17 
2024-02-02 21:34:43,437 EPOCH 609
2024-02-02 21:34:48,576 [Epoch: 609 Step: 00040800] Batch Recognition Loss:   0.000176 => Gls Tokens per Sec:     1976 || Batch Translation Loss:   0.011189 => Txt Tokens per Sec:     5471 || Lr: 0.000050
2024-02-02 21:34:48,819 Epoch 609: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.14 
2024-02-02 21:34:48,819 EPOCH 610
2024-02-02 21:34:54,302 Epoch 610: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.67 
2024-02-02 21:34:54,303 EPOCH 611
2024-02-02 21:34:56,701 [Epoch: 611 Step: 00040900] Batch Recognition Loss:   0.000214 => Gls Tokens per Sec:     1965 || Batch Translation Loss:   0.013916 => Txt Tokens per Sec:     5559 || Lr: 0.000050
2024-02-02 21:34:59,516 Epoch 611: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.62 
2024-02-02 21:34:59,517 EPOCH 612
2024-02-02 21:35:04,692 [Epoch: 612 Step: 00041000] Batch Recognition Loss:   0.000146 => Gls Tokens per Sec:     1931 || Batch Translation Loss:   0.011210 => Txt Tokens per Sec:     5297 || Lr: 0.000050
2024-02-02 21:35:05,118 Epoch 612: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.10 
2024-02-02 21:35:05,118 EPOCH 613
2024-02-02 21:35:09,855 Epoch 613: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.57 
2024-02-02 21:35:09,855 EPOCH 614
2024-02-02 21:35:12,133 [Epoch: 614 Step: 00041100] Batch Recognition Loss:   0.000178 => Gls Tokens per Sec:     2038 || Batch Translation Loss:   0.012134 => Txt Tokens per Sec:     5676 || Lr: 0.000050
2024-02-02 21:35:15,144 Epoch 614: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.12 
2024-02-02 21:35:15,145 EPOCH 615
2024-02-02 21:35:19,776 [Epoch: 615 Step: 00041200] Batch Recognition Loss:   0.000188 => Gls Tokens per Sec:     2122 || Batch Translation Loss:   0.019958 => Txt Tokens per Sec:     5928 || Lr: 0.000050
2024-02-02 21:35:20,077 Epoch 615: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.72 
2024-02-02 21:35:20,077 EPOCH 616
2024-02-02 21:35:25,583 Epoch 616: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.61 
2024-02-02 21:35:25,583 EPOCH 617
2024-02-02 21:35:27,569 [Epoch: 617 Step: 00041300] Batch Recognition Loss:   0.000161 => Gls Tokens per Sec:     2212 || Batch Translation Loss:   0.019526 => Txt Tokens per Sec:     5922 || Lr: 0.000050
2024-02-02 21:35:30,598 Epoch 617: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.10 
2024-02-02 21:35:30,599 EPOCH 618
2024-02-02 21:35:35,364 [Epoch: 618 Step: 00041400] Batch Recognition Loss:   0.000179 => Gls Tokens per Sec:     2030 || Batch Translation Loss:   0.026863 => Txt Tokens per Sec:     5590 || Lr: 0.000050
2024-02-02 21:35:35,937 Epoch 618: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.22 
2024-02-02 21:35:35,938 EPOCH 619
2024-02-02 21:35:42,106 Epoch 619: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.74 
2024-02-02 21:35:42,106 EPOCH 620
2024-02-02 21:35:44,446 [Epoch: 620 Step: 00041500] Batch Recognition Loss:   0.000198 => Gls Tokens per Sec:     1848 || Batch Translation Loss:   0.014130 => Txt Tokens per Sec:     5261 || Lr: 0.000050
2024-02-02 21:35:47,590 Epoch 620: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.02 
2024-02-02 21:35:47,590 EPOCH 621
2024-02-02 21:35:52,833 [Epoch: 621 Step: 00041600] Batch Recognition Loss:   0.000169 => Gls Tokens per Sec:     1814 || Batch Translation Loss:   0.056532 => Txt Tokens per Sec:     5081 || Lr: 0.000050
2024-02-02 21:35:53,318 Epoch 621: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.23 
2024-02-02 21:35:53,318 EPOCH 622
2024-02-02 21:35:58,244 Epoch 622: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.58 
2024-02-02 21:35:58,245 EPOCH 623
2024-02-02 21:36:00,394 [Epoch: 623 Step: 00041700] Batch Recognition Loss:   0.000227 => Gls Tokens per Sec:     1935 || Batch Translation Loss:   0.009310 => Txt Tokens per Sec:     5393 || Lr: 0.000050
2024-02-02 21:36:03,774 Epoch 623: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.17 
2024-02-02 21:36:03,775 EPOCH 624
2024-02-02 21:36:08,038 [Epoch: 624 Step: 00041800] Batch Recognition Loss:   0.000311 => Gls Tokens per Sec:     2214 || Batch Translation Loss:   0.022363 => Txt Tokens per Sec:     6146 || Lr: 0.000050
2024-02-02 21:36:08,646 Epoch 624: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.72 
2024-02-02 21:36:08,646 EPOCH 625
2024-02-02 21:36:13,439 Epoch 625: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.88 
2024-02-02 21:36:13,439 EPOCH 626
2024-02-02 21:36:15,042 [Epoch: 626 Step: 00041900] Batch Recognition Loss:   0.000216 => Gls Tokens per Sec:     2496 || Batch Translation Loss:   0.021814 => Txt Tokens per Sec:     6639 || Lr: 0.000050
2024-02-02 21:36:18,435 Epoch 626: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.95 
2024-02-02 21:36:18,436 EPOCH 627
2024-02-02 21:36:23,386 [Epoch: 627 Step: 00042000] Batch Recognition Loss:   0.000316 => Gls Tokens per Sec:     1857 || Batch Translation Loss:   0.012811 => Txt Tokens per Sec:     5221 || Lr: 0.000050
2024-02-02 21:36:31,848 Validation result at epoch 627, step    42000: duration: 8.4619s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00223	Translation Loss: 90963.48438	PPL: 8979.68652
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.51	(BLEU-1: 9.88,	BLEU-2: 2.78,	BLEU-3: 1.17,	BLEU-4: 0.51)
	CHRF 16.49	ROUGE 8.53
2024-02-02 21:36:31,849 Logging Recognition and Translation Outputs
2024-02-02 21:36:31,849 ========================================================================================================================
2024-02-02 21:36:31,849 Logging Sequence: 81_8.00
2024-02-02 21:36:31,850 	Gloss Reference :	A B+C+D+E
2024-02-02 21:36:31,850 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:36:31,850 	Gloss Alignment :	         
2024-02-02 21:36:31,850 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:36:31,852 	Text Reference  :	have   been    involved in  a huge controversy in connection to real estate developer amrapali group  since    last 7        years
2024-02-02 21:36:31,852 	Text Hypothesis :	yuvraj decided to       get a **** bar         in ********** ** the  uae    oman      the      bcci's decision of   amrapali mahi 
2024-02-02 21:36:31,852 	Text Alignment  :	S      S       S        S     D    S              D          D  S    S      S         S        S      S        S    S        S    
2024-02-02 21:36:31,852 ========================================================================================================================
2024-02-02 21:36:31,852 Logging Sequence: 148_239.00
2024-02-02 21:36:31,852 	Gloss Reference :	A B+C+D+E
2024-02-02 21:36:31,853 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:36:31,853 	Gloss Alignment :	         
2024-02-02 21:36:31,853 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:36:31,854 	Text Reference  :	the ground staff were    very happy and     thanked the bowler for his kind    gesture
2024-02-02 21:36:31,854 	Text Hypothesis :	*** yuvraj and   gambhir have been  flooded with    the ****** *** *** current season 
2024-02-02 21:36:31,854 	Text Alignment  :	D   S      S     S       S    S     S       S           D      D   D   S       S      
2024-02-02 21:36:31,854 ========================================================================================================================
2024-02-02 21:36:31,854 Logging Sequence: 165_8.00
2024-02-02 21:36:31,854 	Gloss Reference :	A B+C+D+E
2024-02-02 21:36:31,855 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:36:31,855 	Gloss Alignment :	         
2024-02-02 21:36:31,855 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:36:31,855 	Text Reference  :	however many don't believe in       it   it   varies  among people
2024-02-02 21:36:31,855 	Text Hypothesis :	******* **** ***** he      believed that this brought him   luck  
2024-02-02 21:36:31,856 	Text Alignment  :	D       D    D     S       S        S    S    S       S     S     
2024-02-02 21:36:31,856 ========================================================================================================================
2024-02-02 21:36:31,856 Logging Sequence: 93_93.00
2024-02-02 21:36:31,856 	Gloss Reference :	A B+C+D+E
2024-02-02 21:36:31,856 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:36:31,856 	Gloss Alignment :	         
2024-02-02 21:36:31,856 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:36:31,857 	Text Reference  :	** ****** ** ******* rooney was at     the club as  well 
2024-02-02 21:36:31,857 	Text Hypothesis :	he seemed to sloshed and    had passed out on   the chair
2024-02-02 21:36:31,857 	Text Alignment  :	I  I      I  I       S      S   S      S   S    S   S    
2024-02-02 21:36:31,857 ========================================================================================================================
2024-02-02 21:36:31,857 Logging Sequence: 96_129.00
2024-02-02 21:36:31,858 	Gloss Reference :	A B+C+D+E
2024-02-02 21:36:31,858 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:36:31,858 	Gloss Alignment :	         
2024-02-02 21:36:31,858 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:36:31,858 	Text Reference  :	***** *** viewers were very stressed
2024-02-02 21:36:31,858 	Text Hypothesis :	while the couple  were **** lost    
2024-02-02 21:36:31,858 	Text Alignment  :	I     I   S            D    S       
2024-02-02 21:36:31,859 ========================================================================================================================
2024-02-02 21:36:32,471 Epoch 627: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.74 
2024-02-02 21:36:32,471 EPOCH 628
2024-02-02 21:36:38,055 Epoch 628: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.09 
2024-02-02 21:36:38,056 EPOCH 629
2024-02-02 21:36:39,672 [Epoch: 629 Step: 00042100] Batch Recognition Loss:   0.000298 => Gls Tokens per Sec:     2378 || Batch Translation Loss:   0.017285 => Txt Tokens per Sec:     6225 || Lr: 0.000050
2024-02-02 21:36:43,573 Epoch 629: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.85 
2024-02-02 21:36:43,574 EPOCH 630
2024-02-02 21:36:47,956 [Epoch: 630 Step: 00042200] Batch Recognition Loss:   0.000210 => Gls Tokens per Sec:     2082 || Batch Translation Loss:   0.019599 => Txt Tokens per Sec:     5824 || Lr: 0.000050
2024-02-02 21:36:48,780 Epoch 630: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.34 
2024-02-02 21:36:48,780 EPOCH 631
2024-02-02 21:36:53,822 Epoch 631: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.41 
2024-02-02 21:36:53,823 EPOCH 632
2024-02-02 21:36:55,525 [Epoch: 632 Step: 00042300] Batch Recognition Loss:   0.000158 => Gls Tokens per Sec:     2111 || Batch Translation Loss:   0.018840 => Txt Tokens per Sec:     5556 || Lr: 0.000050
2024-02-02 21:36:59,176 Epoch 632: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.19 
2024-02-02 21:36:59,176 EPOCH 633
2024-02-02 21:37:03,060 [Epoch: 633 Step: 00042400] Batch Recognition Loss:   0.000197 => Gls Tokens per Sec:     2285 || Batch Translation Loss:   0.017202 => Txt Tokens per Sec:     6222 || Lr: 0.000050
2024-02-02 21:37:03,905 Epoch 633: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-02 21:37:03,905 EPOCH 634
2024-02-02 21:37:09,049 Epoch 634: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.21 
2024-02-02 21:37:09,050 EPOCH 635
2024-02-02 21:37:10,900 [Epoch: 635 Step: 00042500] Batch Recognition Loss:   0.000272 => Gls Tokens per Sec:     1856 || Batch Translation Loss:   0.019391 => Txt Tokens per Sec:     5172 || Lr: 0.000050
2024-02-02 21:37:14,368 Epoch 635: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.20 
2024-02-02 21:37:14,368 EPOCH 636
2024-02-02 21:37:18,781 [Epoch: 636 Step: 00042600] Batch Recognition Loss:   0.000161 => Gls Tokens per Sec:     1975 || Batch Translation Loss:   0.010361 => Txt Tokens per Sec:     5500 || Lr: 0.000050
2024-02-02 21:37:19,668 Epoch 636: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.00 
2024-02-02 21:37:19,668 EPOCH 637
2024-02-02 21:37:24,745 Epoch 637: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.99 
2024-02-02 21:37:24,745 EPOCH 638
2024-02-02 21:37:26,266 [Epoch: 638 Step: 00042700] Batch Recognition Loss:   0.000230 => Gls Tokens per Sec:     2211 || Batch Translation Loss:   0.015751 => Txt Tokens per Sec:     6049 || Lr: 0.000050
2024-02-02 21:37:30,203 Epoch 638: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.02 
2024-02-02 21:37:30,204 EPOCH 639
2024-02-02 21:37:34,098 [Epoch: 639 Step: 00042800] Batch Recognition Loss:   0.000151 => Gls Tokens per Sec:     2196 || Batch Translation Loss:   0.017148 => Txt Tokens per Sec:     6034 || Lr: 0.000050
2024-02-02 21:37:35,182 Epoch 639: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.91 
2024-02-02 21:37:35,182 EPOCH 640
2024-02-02 21:37:40,665 Epoch 640: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.98 
2024-02-02 21:37:40,666 EPOCH 641
2024-02-02 21:37:42,079 [Epoch: 641 Step: 00042900] Batch Recognition Loss:   0.000117 => Gls Tokens per Sec:     2204 || Batch Translation Loss:   0.093629 => Txt Tokens per Sec:     6059 || Lr: 0.000050
2024-02-02 21:37:45,845 Epoch 641: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.95 
2024-02-02 21:37:45,846 EPOCH 642
2024-02-02 21:37:50,154 [Epoch: 642 Step: 00043000] Batch Recognition Loss:   0.000421 => Gls Tokens per Sec:     1969 || Batch Translation Loss:   0.035397 => Txt Tokens per Sec:     5461 || Lr: 0.000050
2024-02-02 21:37:51,288 Epoch 642: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.91 
2024-02-02 21:37:51,288 EPOCH 643
2024-02-02 21:37:56,440 Epoch 643: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.06 
2024-02-02 21:37:56,441 EPOCH 644
2024-02-02 21:37:57,856 [Epoch: 644 Step: 00043100] Batch Recognition Loss:   0.000145 => Gls Tokens per Sec:     2150 || Batch Translation Loss:   0.015575 => Txt Tokens per Sec:     5864 || Lr: 0.000050
2024-02-02 21:38:01,573 Epoch 644: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.94 
2024-02-02 21:38:01,573 EPOCH 645
2024-02-02 21:38:05,026 [Epoch: 645 Step: 00043200] Batch Recognition Loss:   0.000213 => Gls Tokens per Sec:     2385 || Batch Translation Loss:   0.030048 => Txt Tokens per Sec:     6461 || Lr: 0.000050
2024-02-02 21:38:06,492 Epoch 645: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.72 
2024-02-02 21:38:06,493 EPOCH 646
2024-02-02 21:38:12,122 Epoch 646: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.33 
2024-02-02 21:38:12,123 EPOCH 647
2024-02-02 21:38:13,470 [Epoch: 647 Step: 00043300] Batch Recognition Loss:   0.000260 => Gls Tokens per Sec:     2073 || Batch Translation Loss:   0.028711 => Txt Tokens per Sec:     5374 || Lr: 0.000050
2024-02-02 21:38:17,672 Epoch 647: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.64 
2024-02-02 21:38:17,672 EPOCH 648
2024-02-02 21:38:21,794 [Epoch: 648 Step: 00043400] Batch Recognition Loss:   0.000167 => Gls Tokens per Sec:     1958 || Batch Translation Loss:   0.023327 => Txt Tokens per Sec:     5456 || Lr: 0.000050
2024-02-02 21:38:23,080 Epoch 648: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.56 
2024-02-02 21:38:23,080 EPOCH 649
2024-02-02 21:38:28,618 Epoch 649: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.95 
2024-02-02 21:38:28,619 EPOCH 650
2024-02-02 21:38:30,056 [Epoch: 650 Step: 00043500] Batch Recognition Loss:   0.000146 => Gls Tokens per Sec:     1831 || Batch Translation Loss:   0.012351 => Txt Tokens per Sec:     4977 || Lr: 0.000050
2024-02-02 21:38:34,039 Epoch 650: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.54 
2024-02-02 21:38:34,039 EPOCH 651
2024-02-02 21:38:38,389 [Epoch: 651 Step: 00043600] Batch Recognition Loss:   0.000198 => Gls Tokens per Sec:     1819 || Batch Translation Loss:   0.013563 => Txt Tokens per Sec:     5100 || Lr: 0.000050
2024-02-02 21:38:39,672 Epoch 651: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.36 
2024-02-02 21:38:39,672 EPOCH 652
2024-02-02 21:38:44,733 Epoch 652: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.26 
2024-02-02 21:38:44,733 EPOCH 653
2024-02-02 21:38:45,831 [Epoch: 653 Step: 00043700] Batch Recognition Loss:   0.000165 => Gls Tokens per Sec:     2334 || Batch Translation Loss:   0.021406 => Txt Tokens per Sec:     6519 || Lr: 0.000050
2024-02-02 21:38:49,895 Epoch 653: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.26 
2024-02-02 21:38:49,896 EPOCH 654
2024-02-02 21:38:53,776 [Epoch: 654 Step: 00043800] Batch Recognition Loss:   0.000217 => Gls Tokens per Sec:     2021 || Batch Translation Loss:   0.016408 => Txt Tokens per Sec:     5653 || Lr: 0.000050
2024-02-02 21:38:55,157 Epoch 654: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.04 
2024-02-02 21:38:55,157 EPOCH 655
2024-02-02 21:39:00,621 Epoch 655: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.22 
2024-02-02 21:39:00,621 EPOCH 656
2024-02-02 21:39:01,694 [Epoch: 656 Step: 00043900] Batch Recognition Loss:   0.000133 => Gls Tokens per Sec:     2153 || Batch Translation Loss:   0.045893 => Txt Tokens per Sec:     5661 || Lr: 0.000050
2024-02-02 21:39:05,624 Epoch 656: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.27 
2024-02-02 21:39:05,624 EPOCH 657
2024-02-02 21:39:09,348 [Epoch: 657 Step: 00044000] Batch Recognition Loss:   0.000347 => Gls Tokens per Sec:     2039 || Batch Translation Loss:   0.093709 => Txt Tokens per Sec:     5712 || Lr: 0.000050
2024-02-02 21:39:17,899 Validation result at epoch 657, step    44000: duration: 8.5494s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00209	Translation Loss: 90328.07031	PPL: 8426.48438
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.73	(BLEU-1: 11.41,	BLEU-2: 3.67,	BLEU-3: 1.58,	BLEU-4: 0.73)
	CHRF 16.91	ROUGE 9.73
2024-02-02 21:39:17,901 Logging Recognition and Translation Outputs
2024-02-02 21:39:17,901 ========================================================================================================================
2024-02-02 21:39:17,901 Logging Sequence: 117_29.00
2024-02-02 21:39:17,901 	Gloss Reference :	A B+C+D+E
2024-02-02 21:39:17,901 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:39:17,902 	Gloss Alignment :	         
2024-02-02 21:39:17,902 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:39:17,903 	Text Reference  :	however england was  unable to reach the target  they  were  all  out  lost   by      66    runs  
2024-02-02 21:39:17,903 	Text Hypothesis :	the     match   went on     to score -   chennai super kings runs were played between south africa
2024-02-02 21:39:17,904 	Text Alignment  :	S       S       S    S         S     S   S       S     S     S    S    S      S       S     S     
2024-02-02 21:39:17,904 ========================================================================================================================
2024-02-02 21:39:17,904 Logging Sequence: 84_176.00
2024-02-02 21:39:17,904 	Gloss Reference :	A B+C+D+E
2024-02-02 21:39:17,904 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:39:17,904 	Gloss Alignment :	         
2024-02-02 21:39:17,904 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:39:17,906 	Text Reference  :	*** **** *** **** ** germany's nancy faeser  who attended  the *** game in doha   against japan said   
2024-02-02 21:39:17,906 	Text Hypothesis :	and told him that he wanted    to    support the community the ban will be strong to      be    matches
2024-02-02 21:39:17,906 	Text Alignment  :	I   I    I   I    I  S         S     S       S   S             I   S    S  S      S       S     S      
2024-02-02 21:39:17,906 ========================================================================================================================
2024-02-02 21:39:17,906 Logging Sequence: 172_98.00
2024-02-02 21:39:17,906 	Gloss Reference :	A B+C+D+E
2024-02-02 21:39:17,906 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:39:17,907 	Gloss Alignment :	         
2024-02-02 21:39:17,907 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:39:17,908 	Text Reference  :	*** ***** ** *** since      700 pm     it      kept raining the intensity plunged around 915    pm   
2024-02-02 21:39:17,908 	Text Hypothesis :	the final of the tournament was played between csk  and     sri lanka     legends on     social media
2024-02-02 21:39:17,908 	Text Alignment  :	I   I     I  I   S          S   S      S       S    S       S   S         S       S      S      S    
2024-02-02 21:39:17,908 ========================================================================================================================
2024-02-02 21:39:17,908 Logging Sequence: 135_92.00
2024-02-02 21:39:17,909 	Gloss Reference :	A B+C+D+E
2024-02-02 21:39:17,909 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:39:17,909 	Gloss Alignment :	         
2024-02-02 21:39:17,909 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:39:17,910 	Text Reference  :	she wrote that **** ***** half    had      already been raised by  the ****** ***** ** ******* ***** ** ***** family's online fundraiser
2024-02-02 21:39:17,911 	Text Hypothesis :	*** after that they would restart training in      2019 and    won the silver medal in javelin throw in tokyo oylmpics in     2021      
2024-02-02 21:39:17,911 	Text Alignment  :	D   S          I    I     S       S        S       S    S      S       I      I     I  I       I     I  I     S        S      S         
2024-02-02 21:39:17,911 ========================================================================================================================
2024-02-02 21:39:17,911 Logging Sequence: 180_332.00
2024-02-02 21:39:17,911 	Gloss Reference :	A B+C+D+E
2024-02-02 21:39:17,911 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:39:17,911 	Gloss Alignment :	         
2024-02-02 21:39:17,912 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:39:17,913 	Text Reference  :	did i     eat     roti   made of   shilajit that **** **** ** i      got energy   to         assault so        many      girls 
2024-02-02 21:39:17,913 	Text Hypothesis :	*** after talking drinks and  told him      that they will be medals for sexually assaulting female  wrestlers including minors
2024-02-02 21:39:17,913 	Text Alignment  :	D   S     S       S      S    S    S             I    I    I  S      S   S        S          S       S         S         S     
2024-02-02 21:39:17,914 ========================================================================================================================
2024-02-02 21:39:19,299 Epoch 657: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.12 
2024-02-02 21:39:19,299 EPOCH 658
2024-02-02 21:39:24,775 Epoch 658: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.42 
2024-02-02 21:39:24,776 EPOCH 659
2024-02-02 21:39:26,062 [Epoch: 659 Step: 00044100] Batch Recognition Loss:   0.000368 => Gls Tokens per Sec:     1744 || Batch Translation Loss:   0.055412 => Txt Tokens per Sec:     4897 || Lr: 0.000050
2024-02-02 21:39:30,325 Epoch 659: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.07 
2024-02-02 21:39:30,326 EPOCH 660
2024-02-02 21:39:33,945 [Epoch: 660 Step: 00044200] Batch Recognition Loss:   0.000190 => Gls Tokens per Sec:     2078 || Batch Translation Loss:   0.028024 => Txt Tokens per Sec:     5729 || Lr: 0.000050
2024-02-02 21:39:35,568 Epoch 660: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.80 
2024-02-02 21:39:35,568 EPOCH 661
2024-02-02 21:39:40,678 Epoch 661: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.87 
2024-02-02 21:39:40,678 EPOCH 662
2024-02-02 21:39:41,533 [Epoch: 662 Step: 00044300] Batch Recognition Loss:   0.000162 => Gls Tokens per Sec:     2436 || Batch Translation Loss:   0.015088 => Txt Tokens per Sec:     6512 || Lr: 0.000050
2024-02-02 21:39:45,459 Epoch 662: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.38 
2024-02-02 21:39:45,459 EPOCH 663
2024-02-02 21:39:49,535 [Epoch: 663 Step: 00044400] Batch Recognition Loss:   0.000215 => Gls Tokens per Sec:     1784 || Batch Translation Loss:   0.091051 => Txt Tokens per Sec:     4992 || Lr: 0.000050
2024-02-02 21:39:51,185 Epoch 663: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.31 
2024-02-02 21:39:51,185 EPOCH 664
2024-02-02 21:39:56,771 Epoch 664: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.88 
2024-02-02 21:39:56,772 EPOCH 665
2024-02-02 21:39:57,795 [Epoch: 665 Step: 00044500] Batch Recognition Loss:   0.000236 => Gls Tokens per Sec:     1789 || Batch Translation Loss:   0.031787 => Txt Tokens per Sec:     5457 || Lr: 0.000050
2024-02-02 21:40:01,392 Epoch 665: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.16 
2024-02-02 21:40:01,392 EPOCH 666
2024-02-02 21:40:04,471 [Epoch: 666 Step: 00044600] Batch Recognition Loss:   0.000111 => Gls Tokens per Sec:     2310 || Batch Translation Loss:   0.012737 => Txt Tokens per Sec:     6395 || Lr: 0.000050
2024-02-02 21:40:06,115 Epoch 666: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.33 
2024-02-02 21:40:06,115 EPOCH 667
2024-02-02 21:40:11,441 Epoch 667: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.06 
2024-02-02 21:40:11,442 EPOCH 668
2024-02-02 21:40:12,230 [Epoch: 668 Step: 00044700] Batch Recognition Loss:   0.000272 => Gls Tokens per Sec:     2241 || Batch Translation Loss:   0.022132 => Txt Tokens per Sec:     5815 || Lr: 0.000050
2024-02-02 21:40:16,912 Epoch 668: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.68 
2024-02-02 21:40:16,913 EPOCH 669
2024-02-02 21:40:20,555 [Epoch: 669 Step: 00044800] Batch Recognition Loss:   0.000214 => Gls Tokens per Sec:     1933 || Batch Translation Loss:   0.269825 => Txt Tokens per Sec:     5487 || Lr: 0.000050
2024-02-02 21:40:22,415 Epoch 669: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.67 
2024-02-02 21:40:22,416 EPOCH 670
2024-02-02 21:40:27,834 Epoch 670: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.43 
2024-02-02 21:40:27,834 EPOCH 671
2024-02-02 21:40:28,497 [Epoch: 671 Step: 00044900] Batch Recognition Loss:   0.000246 => Gls Tokens per Sec:     2416 || Batch Translation Loss:   0.019844 => Txt Tokens per Sec:     6504 || Lr: 0.000050
2024-02-02 21:40:33,063 Epoch 671: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.28 
2024-02-02 21:40:33,064 EPOCH 672
2024-02-02 21:40:36,844 [Epoch: 672 Step: 00045000] Batch Recognition Loss:   0.000172 => Gls Tokens per Sec:     1797 || Batch Translation Loss:   0.015928 => Txt Tokens per Sec:     5045 || Lr: 0.000050
2024-02-02 21:40:38,619 Epoch 672: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.24 
2024-02-02 21:40:38,619 EPOCH 673
2024-02-02 21:40:43,392 Epoch 673: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.68 
2024-02-02 21:40:43,392 EPOCH 674
2024-02-02 21:40:44,088 [Epoch: 674 Step: 00045100] Batch Recognition Loss:   0.000201 => Gls Tokens per Sec:     1945 || Batch Translation Loss:   0.017869 => Txt Tokens per Sec:     5609 || Lr: 0.000050
2024-02-02 21:40:48,549 Epoch 674: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.12 
2024-02-02 21:40:48,550 EPOCH 675
2024-02-02 21:40:51,603 [Epoch: 675 Step: 00045200] Batch Recognition Loss:   0.000194 => Gls Tokens per Sec:     2201 || Batch Translation Loss:   0.014765 => Txt Tokens per Sec:     6019 || Lr: 0.000050
2024-02-02 21:40:53,829 Epoch 675: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.24 
2024-02-02 21:40:53,829 EPOCH 676
2024-02-02 21:40:59,427 Epoch 676: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.17 
2024-02-02 21:40:59,428 EPOCH 677
2024-02-02 21:41:00,066 [Epoch: 677 Step: 00045300] Batch Recognition Loss:   0.000148 => Gls Tokens per Sec:     2007 || Batch Translation Loss:   0.025687 => Txt Tokens per Sec:     5523 || Lr: 0.000050
2024-02-02 21:41:05,041 Epoch 677: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.29 
2024-02-02 21:41:05,042 EPOCH 678
2024-02-02 21:41:08,360 [Epoch: 678 Step: 00045400] Batch Recognition Loss:   0.000262 => Gls Tokens per Sec:     1951 || Batch Translation Loss:   0.807081 => Txt Tokens per Sec:     5545 || Lr: 0.000050
2024-02-02 21:41:10,139 Epoch 678: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.80 
2024-02-02 21:41:10,139 EPOCH 679
2024-02-02 21:41:15,497 Epoch 679: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.53 
2024-02-02 21:41:15,498 EPOCH 680
2024-02-02 21:41:16,003 [Epoch: 680 Step: 00045500] Batch Recognition Loss:   0.000164 => Gls Tokens per Sec:     2226 || Batch Translation Loss:   0.150144 => Txt Tokens per Sec:     6279 || Lr: 0.000050
2024-02-02 21:41:20,547 Epoch 680: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.82 
2024-02-02 21:41:20,547 EPOCH 681
2024-02-02 21:41:23,812 [Epoch: 681 Step: 00045600] Batch Recognition Loss:   0.000340 => Gls Tokens per Sec:     1934 || Batch Translation Loss:   0.046765 => Txt Tokens per Sec:     5260 || Lr: 0.000050
2024-02-02 21:41:26,139 Epoch 681: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.95 
2024-02-02 21:41:26,139 EPOCH 682
2024-02-02 21:41:31,278 Epoch 682: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.63 
2024-02-02 21:41:31,279 EPOCH 683
2024-02-02 21:41:31,832 [Epoch: 683 Step: 00045700] Batch Recognition Loss:   0.000253 => Gls Tokens per Sec:     1740 || Batch Translation Loss:   0.013724 => Txt Tokens per Sec:     4586 || Lr: 0.000050
2024-02-02 21:41:36,765 Epoch 683: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.51 
2024-02-02 21:41:36,766 EPOCH 684
2024-02-02 21:41:39,591 [Epoch: 684 Step: 00045800] Batch Recognition Loss:   0.000225 => Gls Tokens per Sec:     2210 || Batch Translation Loss:   0.022890 => Txt Tokens per Sec:     6206 || Lr: 0.000050
2024-02-02 21:41:41,538 Epoch 684: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.85 
2024-02-02 21:41:41,538 EPOCH 685
2024-02-02 21:41:47,032 Epoch 685: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.30 
2024-02-02 21:41:47,032 EPOCH 686
2024-02-02 21:41:47,410 [Epoch: 686 Step: 00045900] Batch Recognition Loss:   0.000234 => Gls Tokens per Sec:     2118 || Batch Translation Loss:   0.025158 => Txt Tokens per Sec:     5925 || Lr: 0.000050
2024-02-02 21:41:52,687 Epoch 686: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.65 
2024-02-02 21:41:52,688 EPOCH 687
2024-02-02 21:41:55,807 [Epoch: 687 Step: 00046000] Batch Recognition Loss:   0.000156 => Gls Tokens per Sec:     1921 || Batch Translation Loss:   0.018197 => Txt Tokens per Sec:     5310 || Lr: 0.000050
2024-02-02 21:42:04,324 Validation result at epoch 687, step    46000: duration: 8.5151s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00114	Translation Loss: 90451.80469	PPL: 8531.46680
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.60	(BLEU-1: 10.55,	BLEU-2: 3.02,	BLEU-3: 1.23,	BLEU-4: 0.60)
	CHRF 16.75	ROUGE 9.03
2024-02-02 21:42:04,325 Logging Recognition and Translation Outputs
2024-02-02 21:42:04,325 ========================================================================================================================
2024-02-02 21:42:04,325 Logging Sequence: 126_121.00
2024-02-02 21:42:04,325 	Gloss Reference :	A B+C+D+E
2024-02-02 21:42:04,325 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:42:04,325 	Gloss Alignment :	         
2024-02-02 21:42:04,325 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:42:04,326 	Text Reference  :	* *** everyone was       very happy by  his victory
2024-02-02 21:42:04,326 	Text Hypothesis :	1 day is       currently at   the   age of  india  
2024-02-02 21:42:04,326 	Text Alignment  :	I I   S        S         S    S     S   S   S      
2024-02-02 21:42:04,326 ========================================================================================================================
2024-02-02 21:42:04,327 Logging Sequence: 73_79.00
2024-02-02 21:42:04,327 	Gloss Reference :	A B+C+D+E
2024-02-02 21:42:04,327 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:42:04,327 	Gloss Alignment :	         
2024-02-02 21:42:04,327 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:42:04,329 	Text Reference  :	raina resturant has food from the ******* rich  spices of   north india  to the   aromatic curries of   south  india
2024-02-02 21:42:04,329 	Text Hypothesis :	***** ********* *** **** **** the company filed a      case on    behalf od dhoni as       they    also manage dhoni
2024-02-02 21:42:04,329 	Text Alignment  :	D     D         D   D    D        I       S     S      S    S     S      S  S     S        S       S    S      S    
2024-02-02 21:42:04,329 ========================================================================================================================
2024-02-02 21:42:04,329 Logging Sequence: 95_152.00
2024-02-02 21:42:04,329 	Gloss Reference :	A B+C+D+E
2024-02-02 21:42:04,330 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:42:04,330 	Gloss Alignment :	         
2024-02-02 21:42:04,330 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:42:04,330 	Text Reference  :	** ** * **** ****** how strange 
2024-02-02 21:42:04,330 	Text Hypothesis :	he is a very strong and pakistan
2024-02-02 21:42:04,330 	Text Alignment  :	I  I  I I    I      S   S       
2024-02-02 21:42:04,330 ========================================================================================================================
2024-02-02 21:42:04,330 Logging Sequence: 135_39.00
2024-02-02 21:42:04,331 	Gloss Reference :	A B+C+D+E
2024-02-02 21:42:04,331 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:42:04,331 	Gloss Alignment :	         
2024-02-02 21:42:04,331 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:42:04,332 	Text Reference  :	who needs to *** travel from     poland to stanford university in ******* california
2024-02-02 21:42:04,332 	Text Hypothesis :	*** flew  to the tokyo  olympics won    a  silver   medal      in javelin throw     
2024-02-02 21:42:04,332 	Text Alignment  :	D   S        I   S      S        S      S  S        S             I       S         
2024-02-02 21:42:04,332 ========================================================================================================================
2024-02-02 21:42:04,332 Logging Sequence: 87_2.00
2024-02-02 21:42:04,332 	Gloss Reference :	A B+C+D+E
2024-02-02 21:42:04,332 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:42:04,332 	Gloss Alignment :	         
2024-02-02 21:42:04,332 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:42:04,335 	Text Reference  :	cricketer gautam gambhir's jealousy against ms    dhoni and virat kohli has        been increasing day by day    
2024-02-02 21:42:04,335 	Text Hypothesis :	what      an     intense   bidding  for     india who   are now   any   spectators to   carry      on  in support
2024-02-02 21:42:04,335 	Text Alignment  :	S         S      S         S        S       S     S     S   S     S     S          S    S          S   S  S      
2024-02-02 21:42:04,335 ========================================================================================================================
2024-02-02 21:42:06,674 Epoch 687: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.27 
2024-02-02 21:42:06,674 EPOCH 688
2024-02-02 21:42:12,266 Epoch 688: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.51 
2024-02-02 21:42:12,266 EPOCH 689
2024-02-02 21:42:12,487 [Epoch: 689 Step: 00046100] Batch Recognition Loss:   0.000130 => Gls Tokens per Sec:     2911 || Batch Translation Loss:   0.013430 => Txt Tokens per Sec:     7110 || Lr: 0.000050
2024-02-02 21:42:17,526 Epoch 689: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.84 
2024-02-02 21:42:17,527 EPOCH 690
2024-02-02 21:42:20,707 [Epoch: 690 Step: 00046200] Batch Recognition Loss:   0.000152 => Gls Tokens per Sec:     1833 || Batch Translation Loss:   0.011797 => Txt Tokens per Sec:     5196 || Lr: 0.000050
2024-02-02 21:42:23,035 Epoch 690: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.78 
2024-02-02 21:42:23,035 EPOCH 691
2024-02-02 21:42:28,336 Epoch 691: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.37 
2024-02-02 21:42:28,337 EPOCH 692
2024-02-02 21:42:28,552 [Epoch: 692 Step: 00046300] Batch Recognition Loss:   0.000195 => Gls Tokens per Sec:     2248 || Batch Translation Loss:   0.021158 => Txt Tokens per Sec:     6042 || Lr: 0.000050
2024-02-02 21:42:33,331 Epoch 692: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.62 
2024-02-02 21:42:33,332 EPOCH 693
2024-02-02 21:42:36,343 [Epoch: 693 Step: 00046400] Batch Recognition Loss:   0.000171 => Gls Tokens per Sec:     1883 || Batch Translation Loss:   0.021983 => Txt Tokens per Sec:     5248 || Lr: 0.000050
2024-02-02 21:42:38,879 Epoch 693: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.94 
2024-02-02 21:42:38,879 EPOCH 694
2024-02-02 21:42:43,909 Epoch 694: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.68 
2024-02-02 21:42:43,909 EPOCH 695
2024-02-02 21:42:44,076 [Epoch: 695 Step: 00046500] Batch Recognition Loss:   0.000120 => Gls Tokens per Sec:     1932 || Batch Translation Loss:   0.013937 => Txt Tokens per Sec:     4353 || Lr: 0.000050
2024-02-02 21:42:49,732 Epoch 695: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.40 
2024-02-02 21:42:49,733 EPOCH 696
2024-02-02 21:42:52,499 [Epoch: 696 Step: 00046600] Batch Recognition Loss:   0.000134 => Gls Tokens per Sec:     1993 || Batch Translation Loss:   0.025012 => Txt Tokens per Sec:     5354 || Lr: 0.000050
2024-02-02 21:42:55,199 Epoch 696: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.54 
2024-02-02 21:42:55,200 EPOCH 697
2024-02-02 21:43:00,460 Epoch 697: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.72 
2024-02-02 21:43:00,460 EPOCH 698
2024-02-02 21:43:00,526 [Epoch: 698 Step: 00046700] Batch Recognition Loss:   0.000237 => Gls Tokens per Sec:     2455 || Batch Translation Loss:   0.029169 => Txt Tokens per Sec:     7197 || Lr: 0.000050
2024-02-02 21:43:05,878 Epoch 698: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.76 
2024-02-02 21:43:05,879 EPOCH 699
2024-02-02 21:43:08,468 [Epoch: 699 Step: 00046800] Batch Recognition Loss:   0.000185 => Gls Tokens per Sec:     2067 || Batch Translation Loss:   0.016991 => Txt Tokens per Sec:     5653 || Lr: 0.000050
2024-02-02 21:43:11,126 Epoch 699: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.61 
2024-02-02 21:43:11,126 EPOCH 700
2024-02-02 21:43:16,637 [Epoch: 700 Step: 00046900] Batch Recognition Loss:   0.000215 => Gls Tokens per Sec:     1930 || Batch Translation Loss:   0.016749 => Txt Tokens per Sec:     5357 || Lr: 0.000050
2024-02-02 21:43:16,637 Epoch 700: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.95 
2024-02-02 21:43:16,638 EPOCH 701
2024-02-02 21:43:21,699 Epoch 701: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.80 
2024-02-02 21:43:21,699 EPOCH 702
2024-02-02 21:43:24,294 [Epoch: 702 Step: 00047000] Batch Recognition Loss:   0.000285 => Gls Tokens per Sec:     2001 || Batch Translation Loss:   0.021183 => Txt Tokens per Sec:     5517 || Lr: 0.000050
2024-02-02 21:43:27,096 Epoch 702: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.81 
2024-02-02 21:43:27,096 EPOCH 703
2024-02-02 21:43:31,899 [Epoch: 703 Step: 00047100] Batch Recognition Loss:   0.000253 => Gls Tokens per Sec:     2180 || Batch Translation Loss:   0.036673 => Txt Tokens per Sec:     6046 || Lr: 0.000050
2024-02-02 21:43:31,964 Epoch 703: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.38 
2024-02-02 21:43:31,964 EPOCH 704
2024-02-02 21:43:36,803 Epoch 704: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.35 
2024-02-02 21:43:36,804 EPOCH 705
2024-02-02 21:43:39,253 [Epoch: 705 Step: 00047200] Batch Recognition Loss:   0.000125 => Gls Tokens per Sec:     2054 || Batch Translation Loss:   0.026121 => Txt Tokens per Sec:     5473 || Lr: 0.000050
2024-02-02 21:43:42,375 Epoch 705: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.36 
2024-02-02 21:43:42,375 EPOCH 706
2024-02-02 21:43:47,706 [Epoch: 706 Step: 00047300] Batch Recognition Loss:   0.000180 => Gls Tokens per Sec:     1951 || Batch Translation Loss:   0.017887 => Txt Tokens per Sec:     5397 || Lr: 0.000050
2024-02-02 21:43:47,946 Epoch 706: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.56 
2024-02-02 21:43:47,946 EPOCH 707
2024-02-02 21:43:53,261 Epoch 707: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.78 
2024-02-02 21:43:53,261 EPOCH 708
2024-02-02 21:43:55,907 [Epoch: 708 Step: 00047400] Batch Recognition Loss:   0.000236 => Gls Tokens per Sec:     1841 || Batch Translation Loss:   0.024842 => Txt Tokens per Sec:     5048 || Lr: 0.000050
2024-02-02 21:43:58,763 Epoch 708: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.79 
2024-02-02 21:43:58,763 EPOCH 709
2024-02-02 21:44:03,180 [Epoch: 709 Step: 00047500] Batch Recognition Loss:   0.000142 => Gls Tokens per Sec:     2298 || Batch Translation Loss:   0.021576 => Txt Tokens per Sec:     6323 || Lr: 0.000050
2024-02-02 21:44:03,492 Epoch 709: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.61 
2024-02-02 21:44:03,493 EPOCH 710
2024-02-02 21:44:09,087 Epoch 710: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.47 
2024-02-02 21:44:09,087 EPOCH 711
2024-02-02 21:44:11,128 [Epoch: 711 Step: 00047600] Batch Recognition Loss:   0.000140 => Gls Tokens per Sec:     2355 || Batch Translation Loss:   0.020552 => Txt Tokens per Sec:     6499 || Lr: 0.000050
2024-02-02 21:44:14,316 Epoch 711: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.45 
2024-02-02 21:44:14,317 EPOCH 712
2024-02-02 21:44:19,429 [Epoch: 712 Step: 00047700] Batch Recognition Loss:   0.000198 => Gls Tokens per Sec:     1954 || Batch Translation Loss:   0.021452 => Txt Tokens per Sec:     5399 || Lr: 0.000050
2024-02-02 21:44:19,734 Epoch 712: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.64 
2024-02-02 21:44:19,734 EPOCH 713
2024-02-02 21:44:25,058 Epoch 713: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.33 
2024-02-02 21:44:25,059 EPOCH 714
2024-02-02 21:44:27,140 [Epoch: 714 Step: 00047800] Batch Recognition Loss:   0.000140 => Gls Tokens per Sec:     2187 || Batch Translation Loss:   0.023960 => Txt Tokens per Sec:     5883 || Lr: 0.000050
2024-02-02 21:44:30,051 Epoch 714: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.25 
2024-02-02 21:44:30,051 EPOCH 715
2024-02-02 21:44:35,056 [Epoch: 715 Step: 00047900] Batch Recognition Loss:   0.000330 => Gls Tokens per Sec:     1965 || Batch Translation Loss:   0.117539 => Txt Tokens per Sec:     5425 || Lr: 0.000050
2024-02-02 21:44:35,430 Epoch 715: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.38 
2024-02-02 21:44:35,430 EPOCH 716
2024-02-02 21:44:40,627 Epoch 716: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.59 
2024-02-02 21:44:40,627 EPOCH 717
2024-02-02 21:44:43,121 [Epoch: 717 Step: 00048000] Batch Recognition Loss:   0.000173 => Gls Tokens per Sec:     1762 || Batch Translation Loss:   0.019919 => Txt Tokens per Sec:     4940 || Lr: 0.000050
2024-02-02 21:44:51,417 Validation result at epoch 717, step    48000: duration: 8.2952s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00117	Translation Loss: 91252.39062	PPL: 9243.08398
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.63	(BLEU-1: 10.55,	BLEU-2: 3.18,	BLEU-3: 1.22,	BLEU-4: 0.63)
	CHRF 16.76	ROUGE 8.99
2024-02-02 21:44:51,418 Logging Recognition and Translation Outputs
2024-02-02 21:44:51,418 ========================================================================================================================
2024-02-02 21:44:51,418 Logging Sequence: 88_159.00
2024-02-02 21:44:51,418 	Gloss Reference :	A B+C+D+E
2024-02-02 21:44:51,418 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:44:51,419 	Gloss Alignment :	         
2024-02-02 21:44:51,419 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:44:51,420 	Text Reference  :	however he  often comes to   the ****** ***** town to ** meet his relatives
2024-02-02 21:44:51,420 	Text Hypothesis :	******* the mayor added that the police never does to be held in  tokyo    
2024-02-02 21:44:51,420 	Text Alignment  :	D       S   S     S     S        I      I     S       I  S    S   S        
2024-02-02 21:44:51,420 ========================================================================================================================
2024-02-02 21:44:51,420 Logging Sequence: 180_53.00
2024-02-02 21:44:51,421 	Gloss Reference :	A B+C+D+E
2024-02-02 21:44:51,421 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:44:51,421 	Gloss Alignment :	         
2024-02-02 21:44:51,421 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:44:51,422 	Text Reference  :	*** the ****** ******* ** *** protest is       against singh     again
2024-02-02 21:44:51,422 	Text Hypothesis :	and the police decided to not sports  minister have    intensely that 
2024-02-02 21:44:51,422 	Text Alignment  :	I       I      I       I  I   S       S        S       S         S    
2024-02-02 21:44:51,422 ========================================================================================================================
2024-02-02 21:44:51,422 Logging Sequence: 163_30.00
2024-02-02 21:44:51,422 	Gloss Reference :	A B+C+D+E
2024-02-02 21:44:51,422 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:44:51,423 	Gloss Alignment :	         
2024-02-02 21:44:51,423 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:44:51,423 	Text Reference  :	** ** they never permitted anyone to      reveal her  face   
2024-02-02 21:44:51,423 	Text Hypothesis :	it is not  known if        the    meeting his    wife deepika
2024-02-02 21:44:51,423 	Text Alignment  :	I  I  S    S     S         S      S       S      S    S      
2024-02-02 21:44:51,424 ========================================================================================================================
2024-02-02 21:44:51,424 Logging Sequence: 51_110.00
2024-02-02 21:44:51,424 	Gloss Reference :	A B+C+D+E
2024-02-02 21:44:51,424 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:44:51,424 	Gloss Alignment :	         
2024-02-02 21:44:51,424 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:44:51,425 	Text Reference  :	the   aussies were very happy  with their victory
2024-02-02 21:44:51,425 	Text Hypothesis :	these are     the  same ritual of   their matches
2024-02-02 21:44:51,425 	Text Alignment  :	S     S       S    S    S      S          S      
2024-02-02 21:44:51,425 ========================================================================================================================
2024-02-02 21:44:51,425 Logging Sequence: 70_249.00
2024-02-02 21:44:51,425 	Gloss Reference :	A B+C+D+E
2024-02-02 21:44:51,426 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:44:51,426 	Gloss Alignment :	         
2024-02-02 21:44:51,426 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:44:51,427 	Text Reference  :	**** **** ********* **** **** **** ** **** have a   look at this video   
2024-02-02 21:44:51,427 	Text Hypothesis :	suga then announced that from 5610 to 5522 in   the end  of the  olympics
2024-02-02 21:44:51,427 	Text Alignment  :	I    I    I         I    I    I    I  I    S    S   S    S  S    S       
2024-02-02 21:44:51,427 ========================================================================================================================
2024-02-02 21:44:54,545 Epoch 717: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.96 
2024-02-02 21:44:54,546 EPOCH 718
2024-02-02 21:44:59,120 [Epoch: 718 Step: 00048100] Batch Recognition Loss:   0.000205 => Gls Tokens per Sec:     2115 || Batch Translation Loss:   0.036920 => Txt Tokens per Sec:     5927 || Lr: 0.000050
2024-02-02 21:44:59,661 Epoch 718: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.26 
2024-02-02 21:44:59,662 EPOCH 719
2024-02-02 21:45:05,197 Epoch 719: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.61 
2024-02-02 21:45:05,198 EPOCH 720
2024-02-02 21:45:07,363 [Epoch: 720 Step: 00048200] Batch Recognition Loss:   0.000267 => Gls Tokens per Sec:     1954 || Batch Translation Loss:   0.032263 => Txt Tokens per Sec:     5778 || Lr: 0.000050
2024-02-02 21:45:10,459 Epoch 720: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.78 
2024-02-02 21:45:10,460 EPOCH 721
2024-02-02 21:45:15,296 [Epoch: 721 Step: 00048300] Batch Recognition Loss:   0.000153 => Gls Tokens per Sec:     1967 || Batch Translation Loss:   0.015253 => Txt Tokens per Sec:     5473 || Lr: 0.000050
2024-02-02 21:45:15,760 Epoch 721: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.36 
2024-02-02 21:45:15,760 EPOCH 722
2024-02-02 21:45:20,896 Epoch 722: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.29 
2024-02-02 21:45:20,896 EPOCH 723
2024-02-02 21:45:22,918 [Epoch: 723 Step: 00048400] Batch Recognition Loss:   0.000217 => Gls Tokens per Sec:     2058 || Batch Translation Loss:   0.019713 => Txt Tokens per Sec:     5898 || Lr: 0.000050
2024-02-02 21:45:26,074 Epoch 723: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.92 
2024-02-02 21:45:26,075 EPOCH 724
2024-02-02 21:45:30,986 [Epoch: 724 Step: 00048500] Batch Recognition Loss:   0.000240 => Gls Tokens per Sec:     1904 || Batch Translation Loss:   0.023961 => Txt Tokens per Sec:     5358 || Lr: 0.000050
2024-02-02 21:45:31,545 Epoch 724: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.58 
2024-02-02 21:45:31,545 EPOCH 725
2024-02-02 21:45:36,650 Epoch 725: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.28 
2024-02-02 21:45:36,650 EPOCH 726
2024-02-02 21:45:39,134 [Epoch: 726 Step: 00048600] Batch Recognition Loss:   0.000208 => Gls Tokens per Sec:     1611 || Batch Translation Loss:   0.020695 => Txt Tokens per Sec:     4603 || Lr: 0.000050
2024-02-02 21:45:42,207 Epoch 726: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.67 
2024-02-02 21:45:42,207 EPOCH 727
2024-02-02 21:45:46,560 [Epoch: 727 Step: 00048700] Batch Recognition Loss:   0.000219 => Gls Tokens per Sec:     2112 || Batch Translation Loss:   0.022510 => Txt Tokens per Sec:     5920 || Lr: 0.000050
2024-02-02 21:45:47,239 Epoch 727: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.95 
2024-02-02 21:45:47,240 EPOCH 728
2024-02-02 21:45:52,876 Epoch 728: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.73 
2024-02-02 21:45:52,877 EPOCH 729
2024-02-02 21:45:54,627 [Epoch: 729 Step: 00048800] Batch Recognition Loss:   0.000257 => Gls Tokens per Sec:     2195 || Batch Translation Loss:   0.029407 => Txt Tokens per Sec:     5982 || Lr: 0.000050
2024-02-02 21:45:58,232 Epoch 729: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.75 
2024-02-02 21:45:58,232 EPOCH 730
2024-02-02 21:46:02,785 [Epoch: 730 Step: 00048900] Batch Recognition Loss:   0.000249 => Gls Tokens per Sec:     1984 || Batch Translation Loss:   0.024850 => Txt Tokens per Sec:     5478 || Lr: 0.000050
2024-02-02 21:46:03,529 Epoch 730: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.47 
2024-02-02 21:46:03,530 EPOCH 731
2024-02-02 21:46:08,994 Epoch 731: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.50 
2024-02-02 21:46:08,995 EPOCH 732
2024-02-02 21:46:10,607 [Epoch: 732 Step: 00049000] Batch Recognition Loss:   0.000173 => Gls Tokens per Sec:     2284 || Batch Translation Loss:   0.016556 => Txt Tokens per Sec:     6288 || Lr: 0.000050
2024-02-02 21:46:14,129 Epoch 732: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.42 
2024-02-02 21:46:14,130 EPOCH 733
2024-02-02 21:46:18,785 [Epoch: 733 Step: 00049100] Batch Recognition Loss:   0.000197 => Gls Tokens per Sec:     1906 || Batch Translation Loss:   0.012754 => Txt Tokens per Sec:     5313 || Lr: 0.000050
2024-02-02 21:46:19,602 Epoch 733: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.13 
2024-02-02 21:46:19,602 EPOCH 734
2024-02-02 21:46:24,948 Epoch 734: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.27 
2024-02-02 21:46:24,948 EPOCH 735
2024-02-02 21:46:26,787 [Epoch: 735 Step: 00049200] Batch Recognition Loss:   0.000223 => Gls Tokens per Sec:     1867 || Batch Translation Loss:   0.085230 => Txt Tokens per Sec:     5287 || Lr: 0.000050
2024-02-02 21:46:30,003 Epoch 735: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.41 
2024-02-02 21:46:30,003 EPOCH 736
2024-02-02 21:46:34,223 [Epoch: 736 Step: 00049300] Batch Recognition Loss:   0.000241 => Gls Tokens per Sec:     2065 || Batch Translation Loss:   0.027188 => Txt Tokens per Sec:     5640 || Lr: 0.000050
2024-02-02 21:46:35,293 Epoch 736: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.69 
2024-02-02 21:46:35,293 EPOCH 737
2024-02-02 21:46:40,533 Epoch 737: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.89 
2024-02-02 21:46:40,534 EPOCH 738
2024-02-02 21:46:42,402 [Epoch: 738 Step: 00049400] Batch Recognition Loss:   0.000395 => Gls Tokens per Sec:     1752 || Batch Translation Loss:   0.076129 => Txt Tokens per Sec:     4719 || Lr: 0.000050
2024-02-02 21:46:46,094 Epoch 738: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.53 
2024-02-02 21:46:46,094 EPOCH 739
2024-02-02 21:46:49,898 [Epoch: 739 Step: 00049500] Batch Recognition Loss:   0.000179 => Gls Tokens per Sec:     2248 || Batch Translation Loss:   0.022265 => Txt Tokens per Sec:     6154 || Lr: 0.000050
2024-02-02 21:46:51,125 Epoch 739: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.00 
2024-02-02 21:46:51,126 EPOCH 740
2024-02-02 21:46:56,531 Epoch 740: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.62 
2024-02-02 21:46:56,531 EPOCH 741
2024-02-02 21:46:58,065 [Epoch: 741 Step: 00049600] Batch Recognition Loss:   0.000267 => Gls Tokens per Sec:     2087 || Batch Translation Loss:   0.019577 => Txt Tokens per Sec:     6028 || Lr: 0.000050
2024-02-02 21:47:01,661 Epoch 741: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.71 
2024-02-02 21:47:01,661 EPOCH 742
2024-02-02 21:47:05,747 [Epoch: 742 Step: 00049700] Batch Recognition Loss:   0.000150 => Gls Tokens per Sec:     2076 || Batch Translation Loss:   0.013067 => Txt Tokens per Sec:     5796 || Lr: 0.000050
2024-02-02 21:47:06,857 Epoch 742: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.78 
2024-02-02 21:47:06,857 EPOCH 743
2024-02-02 21:47:12,275 Epoch 743: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.73 
2024-02-02 21:47:12,275 EPOCH 744
2024-02-02 21:47:13,900 [Epoch: 744 Step: 00049800] Batch Recognition Loss:   0.000281 => Gls Tokens per Sec:     1818 || Batch Translation Loss:   0.029983 => Txt Tokens per Sec:     5399 || Lr: 0.000050
2024-02-02 21:47:17,238 Epoch 744: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.15 
2024-02-02 21:47:17,238 EPOCH 745
2024-02-02 21:47:21,721 [Epoch: 745 Step: 00049900] Batch Recognition Loss:   0.000101 => Gls Tokens per Sec:     1836 || Batch Translation Loss:   0.009740 => Txt Tokens per Sec:     5179 || Lr: 0.000050
2024-02-02 21:47:22,786 Epoch 745: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.55 
2024-02-02 21:47:22,787 EPOCH 746
2024-02-02 21:47:27,905 Epoch 746: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.51 
2024-02-02 21:47:27,906 EPOCH 747
2024-02-02 21:47:29,382 [Epoch: 747 Step: 00050000] Batch Recognition Loss:   0.000200 => Gls Tokens per Sec:     1954 || Batch Translation Loss:   0.020791 => Txt Tokens per Sec:     5367 || Lr: 0.000050
2024-02-02 21:47:38,031 Validation result at epoch 747, step    50000: duration: 8.6471s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00135	Translation Loss: 90096.63281	PPL: 8233.57031
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.68	(BLEU-1: 10.30,	BLEU-2: 3.10,	BLEU-3: 1.30,	BLEU-4: 0.68)
	CHRF 16.77	ROUGE 8.66
2024-02-02 21:47:38,032 Logging Recognition and Translation Outputs
2024-02-02 21:47:38,032 ========================================================================================================================
2024-02-02 21:47:38,032 Logging Sequence: 59_58.00
2024-02-02 21:47:38,033 	Gloss Reference :	A B+C+D+E
2024-02-02 21:47:38,033 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:47:38,033 	Gloss Alignment :	         
2024-02-02 21:47:38,033 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:47:38,034 	Text Reference  :	*** to      fix   the damage they did not have a       lot     of time     
2024-02-02 21:47:38,034 	Text Hypothesis :	and talking about the ****** **** *** *** **** harmful effects of coca-cola
2024-02-02 21:47:38,034 	Text Alignment  :	I   S       S         D      D    D   D   D    S       S          S        
2024-02-02 21:47:38,034 ========================================================================================================================
2024-02-02 21:47:38,034 Logging Sequence: 165_2.00
2024-02-02 21:47:38,035 	Gloss Reference :	A B+C+D+E
2024-02-02 21:47:38,035 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:47:38,035 	Gloss Alignment :	         
2024-02-02 21:47:38,035 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:47:38,036 	Text Reference  :	many people believe in superstitions and think it       brings good  luck and bad   luck
2024-02-02 21:47:38,036 	Text Hypothesis :	**** ****** ******* ** ************* *** he    believed that   india won  the world cup 
2024-02-02 21:47:38,036 	Text Alignment  :	D    D      D       D  D             D   S     S        S      S     S    S   S     S   
2024-02-02 21:47:38,036 ========================================================================================================================
2024-02-02 21:47:38,036 Logging Sequence: 58_147.00
2024-02-02 21:47:38,036 	Gloss Reference :	A B+C+D+E
2024-02-02 21:47:38,037 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:47:38,037 	Gloss Alignment :	         
2024-02-02 21:47:38,037 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:47:38,038 	Text Reference  :	the women's cricket team grabbed gold by beating sri lanka in     the finals  what       a           historic win
2024-02-02 21:47:38,038 	Text Hypothesis :	*** ******* ******* **** ******* **** ** ******* *** many  former and current cricketers politicians fans     etc
2024-02-02 21:47:38,038 	Text Alignment  :	D   D       D       D    D       D    D  D       D   S     S      S   S       S          S           S        S  
2024-02-02 21:47:38,038 ========================================================================================================================
2024-02-02 21:47:38,038 Logging Sequence: 81_139.00
2024-02-02 21:47:38,038 	Gloss Reference :	A B+C+D+E
2024-02-02 21:47:38,038 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:47:38,039 	Gloss Alignment :	         
2024-02-02 21:47:38,039 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:47:38,040 	Text Reference  :	in 2017 the case was filed first in delhi high court by        rhiti sports     management on  behalf of ************* dhoni  
2024-02-02 21:47:38,040 	Text Hypothesis :	** **** *** **** *** ***** ***** in 2020  ms   dhoni announced his   retirement from       all forms  of international cricket
2024-02-02 21:47:38,040 	Text Alignment  :	D  D    D   D    D   D     D        S     S    S     S         S     S          S          S   S         I             S      
2024-02-02 21:47:38,041 ========================================================================================================================
2024-02-02 21:47:38,041 Logging Sequence: 125_72.00
2024-02-02 21:47:38,041 	Gloss Reference :	A B+C+D+E
2024-02-02 21:47:38,041 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:47:38,041 	Gloss Alignment :	         
2024-02-02 21:47:38,041 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:47:38,042 	Text Reference  :	some said the     pakistani javelineer had       milicious intentions of   tampering with  the javelin out of jealousy
2024-02-02 21:47:38,043 	Text Hypothesis :	**** **** shekhar filed     a          complaint against   the        team were      happy by  javelin *** ** throw   
2024-02-02 21:47:38,043 	Text Alignment  :	D    D    S       S         S          S         S         S          S    S         S     S           D   D  S       
2024-02-02 21:47:38,043 ========================================================================================================================
2024-02-02 21:47:42,160 Epoch 747: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.65 
2024-02-02 21:47:42,160 EPOCH 748
2024-02-02 21:47:45,945 [Epoch: 748 Step: 00050100] Batch Recognition Loss:   0.000148 => Gls Tokens per Sec:     2133 || Batch Translation Loss:   0.020617 => Txt Tokens per Sec:     5851 || Lr: 0.000050
2024-02-02 21:47:47,314 Epoch 748: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.92 
2024-02-02 21:47:47,315 EPOCH 749
2024-02-02 21:47:52,620 Epoch 749: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.67 
2024-02-02 21:47:52,621 EPOCH 750
2024-02-02 21:47:53,768 [Epoch: 750 Step: 00050200] Batch Recognition Loss:   0.000233 => Gls Tokens per Sec:     2371 || Batch Translation Loss:   0.071500 => Txt Tokens per Sec:     6383 || Lr: 0.000050
2024-02-02 21:47:57,447 Epoch 750: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.77 
2024-02-02 21:47:57,447 EPOCH 751
2024-02-02 21:48:00,923 [Epoch: 751 Step: 00050300] Batch Recognition Loss:   0.000253 => Gls Tokens per Sec:     2302 || Batch Translation Loss:   0.031658 => Txt Tokens per Sec:     6365 || Lr: 0.000050
2024-02-02 21:48:02,390 Epoch 751: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.22 
2024-02-02 21:48:02,390 EPOCH 752
2024-02-02 21:48:07,844 Epoch 752: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.33 
2024-02-02 21:48:07,845 EPOCH 753
2024-02-02 21:48:08,849 [Epoch: 753 Step: 00050400] Batch Recognition Loss:   0.000164 => Gls Tokens per Sec:     2552 || Batch Translation Loss:   0.028655 => Txt Tokens per Sec:     6883 || Lr: 0.000050
2024-02-02 21:48:13,096 Epoch 753: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.44 
2024-02-02 21:48:13,097 EPOCH 754
2024-02-02 21:48:17,101 [Epoch: 754 Step: 00050500] Batch Recognition Loss:   0.000344 => Gls Tokens per Sec:     1958 || Batch Translation Loss:   0.021961 => Txt Tokens per Sec:     5577 || Lr: 0.000050
2024-02-02 21:48:18,429 Epoch 754: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.99 
2024-02-02 21:48:18,430 EPOCH 755
2024-02-02 21:48:23,965 Epoch 755: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.75 
2024-02-02 21:48:23,966 EPOCH 756
2024-02-02 21:48:25,034 [Epoch: 756 Step: 00050600] Batch Recognition Loss:   0.000217 => Gls Tokens per Sec:     2247 || Batch Translation Loss:   0.021257 => Txt Tokens per Sec:     6246 || Lr: 0.000050
2024-02-02 21:48:28,787 Epoch 756: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.26 
2024-02-02 21:48:28,787 EPOCH 757
2024-02-02 21:48:32,137 [Epoch: 757 Step: 00050700] Batch Recognition Loss:   0.000143 => Gls Tokens per Sec:     2293 || Batch Translation Loss:   0.032815 => Txt Tokens per Sec:     6357 || Lr: 0.000050
2024-02-02 21:48:33,484 Epoch 757: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.91 
2024-02-02 21:48:33,484 EPOCH 758
2024-02-02 21:48:38,703 Epoch 758: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.89 
2024-02-02 21:48:38,704 EPOCH 759
2024-02-02 21:48:39,717 [Epoch: 759 Step: 00050800] Batch Recognition Loss:   0.000236 => Gls Tokens per Sec:     2215 || Batch Translation Loss:   0.019798 => Txt Tokens per Sec:     6173 || Lr: 0.000050
2024-02-02 21:48:43,851 Epoch 759: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.44 
2024-02-02 21:48:43,852 EPOCH 760
2024-02-02 21:48:47,874 [Epoch: 760 Step: 00050900] Batch Recognition Loss:   0.000167 => Gls Tokens per Sec:     1848 || Batch Translation Loss:   0.018000 => Txt Tokens per Sec:     5008 || Lr: 0.000050
2024-02-02 21:48:49,516 Epoch 760: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.41 
2024-02-02 21:48:49,516 EPOCH 761
2024-02-02 21:48:54,740 Epoch 761: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.13 
2024-02-02 21:48:54,740 EPOCH 762
2024-02-02 21:48:56,003 [Epoch: 762 Step: 00051000] Batch Recognition Loss:   0.000275 => Gls Tokens per Sec:     1649 || Batch Translation Loss:   0.019669 => Txt Tokens per Sec:     4842 || Lr: 0.000050
2024-02-02 21:49:00,050 Epoch 762: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.42 
2024-02-02 21:49:00,051 EPOCH 763
2024-02-02 21:49:03,266 [Epoch: 763 Step: 00051100] Batch Recognition Loss:   0.000200 => Gls Tokens per Sec:     2262 || Batch Translation Loss:   0.025332 => Txt Tokens per Sec:     6101 || Lr: 0.000050
2024-02-02 21:49:05,161 Epoch 763: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.73 
2024-02-02 21:49:05,161 EPOCH 764
2024-02-02 21:49:10,621 Epoch 764: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.72 
2024-02-02 21:49:10,621 EPOCH 765
2024-02-02 21:49:11,774 [Epoch: 765 Step: 00051200] Batch Recognition Loss:   0.000193 => Gls Tokens per Sec:     1589 || Batch Translation Loss:   0.021222 => Txt Tokens per Sec:     4822 || Lr: 0.000050
2024-02-02 21:49:16,201 Epoch 765: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.28 
2024-02-02 21:49:16,201 EPOCH 766
2024-02-02 21:49:19,158 [Epoch: 766 Step: 00051300] Batch Recognition Loss:   0.000131 => Gls Tokens per Sec:     2436 || Batch Translation Loss:   0.024998 => Txt Tokens per Sec:     6711 || Lr: 0.000050
2024-02-02 21:49:21,064 Epoch 766: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.97 
2024-02-02 21:49:21,065 EPOCH 767
2024-02-02 21:49:26,565 Epoch 767: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.91 
2024-02-02 21:49:26,565 EPOCH 768
2024-02-02 21:49:27,311 [Epoch: 768 Step: 00051400] Batch Recognition Loss:   0.000204 => Gls Tokens per Sec:     2363 || Batch Translation Loss:   0.019057 => Txt Tokens per Sec:     6271 || Lr: 0.000050
2024-02-02 21:49:31,893 Epoch 768: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.92 
2024-02-02 21:49:31,893 EPOCH 769
2024-02-02 21:49:35,276 [Epoch: 769 Step: 00051500] Batch Recognition Loss:   0.000165 => Gls Tokens per Sec:     2082 || Batch Translation Loss:   0.016838 => Txt Tokens per Sec:     5691 || Lr: 0.000050
2024-02-02 21:49:37,204 Epoch 769: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.29 
2024-02-02 21:49:37,204 EPOCH 770
2024-02-02 21:49:42,591 Epoch 770: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.56 
2024-02-02 21:49:42,592 EPOCH 771
2024-02-02 21:49:43,351 [Epoch: 771 Step: 00051600] Batch Recognition Loss:   0.000331 => Gls Tokens per Sec:     2109 || Batch Translation Loss:   0.145291 => Txt Tokens per Sec:     5832 || Lr: 0.000050
2024-02-02 21:49:47,507 Epoch 771: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.53 
2024-02-02 21:49:47,507 EPOCH 772
2024-02-02 21:49:50,754 [Epoch: 772 Step: 00051700] Batch Recognition Loss:   0.000141 => Gls Tokens per Sec:     2092 || Batch Translation Loss:   0.029454 => Txt Tokens per Sec:     5901 || Lr: 0.000050
2024-02-02 21:49:52,724 Epoch 772: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.01 
2024-02-02 21:49:52,725 EPOCH 773
2024-02-02 21:49:58,242 Epoch 773: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.65 
2024-02-02 21:49:58,243 EPOCH 774
2024-02-02 21:49:59,002 [Epoch: 774 Step: 00051800] Batch Recognition Loss:   0.000222 => Gls Tokens per Sec:     1902 || Batch Translation Loss:   0.020563 => Txt Tokens per Sec:     5133 || Lr: 0.000050
2024-02-02 21:50:03,678 Epoch 774: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.33 
2024-02-02 21:50:03,680 EPOCH 775
2024-02-02 21:50:06,783 [Epoch: 775 Step: 00051900] Batch Recognition Loss:   0.000237 => Gls Tokens per Sec:     2137 || Batch Translation Loss:   0.023662 => Txt Tokens per Sec:     6027 || Lr: 0.000050
2024-02-02 21:50:08,448 Epoch 775: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.11 
2024-02-02 21:50:08,449 EPOCH 776
2024-02-02 21:50:13,914 Epoch 776: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.64 
2024-02-02 21:50:13,914 EPOCH 777
2024-02-02 21:50:14,536 [Epoch: 777 Step: 00052000] Batch Recognition Loss:   0.000184 => Gls Tokens per Sec:     2059 || Batch Translation Loss:   0.036091 => Txt Tokens per Sec:     5842 || Lr: 0.000050
2024-02-02 21:50:23,030 Validation result at epoch 777, step    52000: duration: 8.4936s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00109	Translation Loss: 90238.28125	PPL: 8351.10742
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.68	(BLEU-1: 10.16,	BLEU-2: 3.12,	BLEU-3: 1.36,	BLEU-4: 0.68)
	CHRF 16.88	ROUGE 8.68
2024-02-02 21:50:23,031 Logging Recognition and Translation Outputs
2024-02-02 21:50:23,031 ========================================================================================================================
2024-02-02 21:50:23,031 Logging Sequence: 87_229.00
2024-02-02 21:50:23,032 	Gloss Reference :	A B+C+D+E
2024-02-02 21:50:23,032 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:50:23,032 	Gloss Alignment :	         
2024-02-02 21:50:23,032 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:50:23,033 	Text Reference  :	it   was not  against dhoni or   kohli
2024-02-02 21:50:23,033 	Text Hypothesis :	what a   good news    had   been fined
2024-02-02 21:50:23,033 	Text Alignment  :	S    S   S    S       S     S    S    
2024-02-02 21:50:23,033 ========================================================================================================================
2024-02-02 21:50:23,033 Logging Sequence: 134_153.00
2024-02-02 21:50:23,034 	Gloss Reference :	A B+C+D+E
2024-02-02 21:50:23,034 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:50:23,034 	Gloss Alignment :	         
2024-02-02 21:50:23,034 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:50:23,037 	Text Reference  :	pm    modi          in his interaction said that deaf    athletes must   fight for   their ***** goals      and *** **** never give up despite the losses     
2024-02-02 21:50:23,037 	Text Hypothesis :	after participating in his *********** **** joy  cricket athletes asking them  about their life' challenges and was just 83    over to win     the interaction
2024-02-02 21:50:23,037 	Text Alignment  :	S     S                    D           D    S    S                S      S     S           I     S              I   I    S     S    S  S           S          
2024-02-02 21:50:23,037 ========================================================================================================================
2024-02-02 21:50:23,037 Logging Sequence: 137_155.00
2024-02-02 21:50:23,037 	Gloss Reference :	A B+C+D+E
2024-02-02 21:50:23,037 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:50:23,038 	Gloss Alignment :	         
2024-02-02 21:50:23,038 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:50:23,039 	Text Reference  :	**** * an extremely high tax named  as          sin tax will be     applied
2024-02-02 21:50:23,039 	Text Hypothesis :	when i am overjoyed with the team's performance due to  his  sudden demise 
2024-02-02 21:50:23,039 	Text Alignment  :	I    I S  S         S    S   S      S           S   S   S    S      S      
2024-02-02 21:50:23,039 ========================================================================================================================
2024-02-02 21:50:23,039 Logging Sequence: 59_18.00
2024-02-02 21:50:23,039 	Gloss Reference :	A B+C+D+E
2024-02-02 21:50:23,039 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:50:23,040 	Gloss Alignment :	         
2024-02-02 21:50:23,040 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:50:23,041 	Text Reference  :	** 27-year-old jessica   fox  from australia won a  bronze  a   gold medal    in canoeing
2024-02-02 21:50:23,041 	Text Hypothesis :	he then        announced that he   does      not to qualify for the  olympics as it      
2024-02-02 21:50:23,041 	Text Alignment  :	I  S           S         S    S    S         S   S  S       S   S    S        S  S       
2024-02-02 21:50:23,041 ========================================================================================================================
2024-02-02 21:50:23,041 Logging Sequence: 173_103.00
2024-02-02 21:50:23,042 	Gloss Reference :	A B+C+D+E
2024-02-02 21:50:23,042 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:50:23,042 	Gloss Alignment :	         
2024-02-02 21:50:23,042 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:50:23,043 	Text Reference  :	***** *** ******** ****** ****** * ********* these rumours are      absolutely rubbish
2024-02-02 21:50:23,043 	Text Hypothesis :	after the incident yuvraj posted a statement on    his     official twitter    handle 
2024-02-02 21:50:23,043 	Text Alignment  :	I     I   I        I      I      I I         S     S       S        S          S      
2024-02-02 21:50:23,043 ========================================================================================================================
2024-02-02 21:50:28,179 Epoch 777: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.91 
2024-02-02 21:50:28,180 EPOCH 778
2024-02-02 21:50:31,164 [Epoch: 778 Step: 00052100] Batch Recognition Loss:   0.000245 => Gls Tokens per Sec:     2199 || Batch Translation Loss:   0.038978 => Txt Tokens per Sec:     6109 || Lr: 0.000050
2024-02-02 21:50:33,558 Epoch 778: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.74 
2024-02-02 21:50:33,559 EPOCH 779
2024-02-02 21:50:39,057 Epoch 779: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.78 
2024-02-02 21:50:39,058 EPOCH 780
2024-02-02 21:50:39,537 [Epoch: 780 Step: 00052200] Batch Recognition Loss:   0.000532 => Gls Tokens per Sec:     2344 || Batch Translation Loss:   0.116384 => Txt Tokens per Sec:     6704 || Lr: 0.000050
2024-02-02 21:50:44,152 Epoch 780: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.19 
2024-02-02 21:50:44,152 EPOCH 781
2024-02-02 21:50:47,241 [Epoch: 781 Step: 00052300] Batch Recognition Loss:   0.000186 => Gls Tokens per Sec:     2073 || Batch Translation Loss:   0.024475 => Txt Tokens per Sec:     5571 || Lr: 0.000050
2024-02-02 21:50:49,526 Epoch 781: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.51 
2024-02-02 21:50:49,526 EPOCH 782
2024-02-02 21:50:54,982 Epoch 782: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.91 
2024-02-02 21:50:54,982 EPOCH 783
2024-02-02 21:50:55,380 [Epoch: 783 Step: 00052400] Batch Recognition Loss:   0.000164 => Gls Tokens per Sec:     2425 || Batch Translation Loss:   0.018983 => Txt Tokens per Sec:     6599 || Lr: 0.000050
2024-02-02 21:50:59,989 Epoch 783: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.08 
2024-02-02 21:50:59,989 EPOCH 784
2024-02-02 21:51:03,155 [Epoch: 784 Step: 00052500] Batch Recognition Loss:   0.000187 => Gls Tokens per Sec:     1971 || Batch Translation Loss:   0.021423 => Txt Tokens per Sec:     5652 || Lr: 0.000050
2024-02-02 21:51:05,503 Epoch 784: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.68 
2024-02-02 21:51:05,503 EPOCH 785
2024-02-02 21:51:11,088 Epoch 785: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.61 
2024-02-02 21:51:11,089 EPOCH 786
2024-02-02 21:51:11,435 [Epoch: 786 Step: 00052600] Batch Recognition Loss:   0.000126 => Gls Tokens per Sec:     2320 || Batch Translation Loss:   0.015330 => Txt Tokens per Sec:     6053 || Lr: 0.000050
2024-02-02 21:51:16,617 Epoch 786: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.57 
2024-02-02 21:51:16,618 EPOCH 787
2024-02-02 21:51:19,624 [Epoch: 787 Step: 00052700] Batch Recognition Loss:   0.000188 => Gls Tokens per Sec:     2023 || Batch Translation Loss:   0.087103 => Txt Tokens per Sec:     5773 || Lr: 0.000050
2024-02-02 21:51:21,937 Epoch 787: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.85 
2024-02-02 21:51:21,937 EPOCH 788
2024-02-02 21:51:26,992 Epoch 788: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.14 
2024-02-02 21:51:26,993 EPOCH 789
2024-02-02 21:51:27,266 [Epoch: 789 Step: 00052800] Batch Recognition Loss:   0.000133 => Gls Tokens per Sec:     2022 || Batch Translation Loss:   0.050426 => Txt Tokens per Sec:     4574 || Lr: 0.000050
2024-02-02 21:51:32,429 Epoch 789: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.62 
2024-02-02 21:51:32,430 EPOCH 790
2024-02-02 21:51:35,170 [Epoch: 790 Step: 00052900] Batch Recognition Loss:   0.000266 => Gls Tokens per Sec:     2129 || Batch Translation Loss:   0.525852 => Txt Tokens per Sec:     5840 || Lr: 0.000050
2024-02-02 21:51:37,577 Epoch 790: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.82 
2024-02-02 21:51:37,577 EPOCH 791
2024-02-02 21:51:43,105 Epoch 791: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.58 
2024-02-02 21:51:43,106 EPOCH 792
2024-02-02 21:51:43,471 [Epoch: 792 Step: 00053000] Batch Recognition Loss:   0.000143 => Gls Tokens per Sec:     1322 || Batch Translation Loss:   0.014889 => Txt Tokens per Sec:     4213 || Lr: 0.000050
2024-02-02 21:51:48,419 Epoch 792: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.83 
2024-02-02 21:51:48,420 EPOCH 793
2024-02-02 21:51:51,451 [Epoch: 793 Step: 00053100] Batch Recognition Loss:   0.000110 => Gls Tokens per Sec:     1901 || Batch Translation Loss:   0.016357 => Txt Tokens per Sec:     5285 || Lr: 0.000050
2024-02-02 21:51:54,005 Epoch 793: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.62 
2024-02-02 21:51:54,006 EPOCH 794
2024-02-02 21:51:59,285 Epoch 794: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.45 
2024-02-02 21:51:59,286 EPOCH 795
2024-02-02 21:51:59,472 [Epoch: 795 Step: 00053200] Batch Recognition Loss:   0.000404 => Gls Tokens per Sec:     1726 || Batch Translation Loss:   0.036159 => Txt Tokens per Sec:     5081 || Lr: 0.000050
2024-02-02 21:52:04,632 Epoch 795: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.42 
2024-02-02 21:52:04,632 EPOCH 796
2024-02-02 21:52:07,598 [Epoch: 796 Step: 00053300] Batch Recognition Loss:   0.000190 => Gls Tokens per Sec:     1859 || Batch Translation Loss:   0.033947 => Txt Tokens per Sec:     5293 || Lr: 0.000050
2024-02-02 21:52:10,316 Epoch 796: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.22 
2024-02-02 21:52:10,317 EPOCH 797
2024-02-02 21:52:15,823 Epoch 797: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.85 
2024-02-02 21:52:15,823 EPOCH 798
2024-02-02 21:52:15,983 [Epoch: 798 Step: 00053400] Batch Recognition Loss:   0.000235 => Gls Tokens per Sec:     1011 || Batch Translation Loss:   0.029184 => Txt Tokens per Sec:     3589 || Lr: 0.000050
2024-02-02 21:52:21,393 Epoch 798: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.69 
2024-02-02 21:52:21,394 EPOCH 799
2024-02-02 21:52:24,212 [Epoch: 799 Step: 00053500] Batch Recognition Loss:   0.000132 => Gls Tokens per Sec:     1931 || Batch Translation Loss:   0.013456 => Txt Tokens per Sec:     5311 || Lr: 0.000050
2024-02-02 21:52:27,019 Epoch 799: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.88 
2024-02-02 21:52:27,020 EPOCH 800
2024-02-02 21:52:32,019 [Epoch: 800 Step: 00053600] Batch Recognition Loss:   0.000137 => Gls Tokens per Sec:     2126 || Batch Translation Loss:   0.014174 => Txt Tokens per Sec:     5903 || Lr: 0.000050
2024-02-02 21:52:32,019 Epoch 800: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.51 
2024-02-02 21:52:32,019 EPOCH 801
2024-02-02 21:52:37,622 Epoch 801: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.78 
2024-02-02 21:52:37,622 EPOCH 802
2024-02-02 21:52:40,072 [Epoch: 802 Step: 00053700] Batch Recognition Loss:   0.000192 => Gls Tokens per Sec:     2119 || Batch Translation Loss:   0.045847 => Txt Tokens per Sec:     5946 || Lr: 0.000050
2024-02-02 21:52:42,739 Epoch 802: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.77 
2024-02-02 21:52:42,741 EPOCH 803
2024-02-02 21:52:48,184 [Epoch: 803 Step: 00053800] Batch Recognition Loss:   0.000124 => Gls Tokens per Sec:     1924 || Batch Translation Loss:   0.020064 => Txt Tokens per Sec:     5346 || Lr: 0.000050
2024-02-02 21:52:48,255 Epoch 803: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.78 
2024-02-02 21:52:48,255 EPOCH 804
2024-02-02 21:52:53,494 Epoch 804: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.62 
2024-02-02 21:52:53,494 EPOCH 805
2024-02-02 21:52:56,263 [Epoch: 805 Step: 00053900] Batch Recognition Loss:   0.000198 => Gls Tokens per Sec:     1817 || Batch Translation Loss:   0.026434 => Txt Tokens per Sec:     5203 || Lr: 0.000050
2024-02-02 21:52:58,607 Epoch 805: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.22 
2024-02-02 21:52:58,608 EPOCH 806
2024-02-02 21:53:03,897 [Epoch: 806 Step: 00054000] Batch Recognition Loss:   0.000124 => Gls Tokens per Sec:     1950 || Batch Translation Loss:   0.016492 => Txt Tokens per Sec:     5434 || Lr: 0.000050
2024-02-02 21:53:12,726 Validation result at epoch 806, step    54000: duration: 8.8290s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00098	Translation Loss: 91795.75000	PPL: 9759.58984
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.76	(BLEU-1: 10.59,	BLEU-2: 3.22,	BLEU-3: 1.39,	BLEU-4: 0.76)
	CHRF 16.81	ROUGE 9.24
2024-02-02 21:53:12,727 Logging Recognition and Translation Outputs
2024-02-02 21:53:12,727 ========================================================================================================================
2024-02-02 21:53:12,727 Logging Sequence: 130_139.00
2024-02-02 21:53:12,727 	Gloss Reference :	A B+C+D+E
2024-02-02 21:53:12,727 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:53:12,728 	Gloss Alignment :	         
2024-02-02 21:53:12,728 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:53:12,730 	Text Reference  :	he shared a picture of  a little pouch he knit for his olympic gold   medal with uk flag on  one  side   and **** japanese flag on      the     other
2024-02-02 21:53:12,730 	Text Hypothesis :	he ****** * ******* won a ****** ***** ** **** *** *** ******* bronze medal **** ** at   the 2012 london and 2016 rio      de   janeiro olympic games
2024-02-02 21:53:12,730 	Text Alignment  :	   D      D D       S     D      D     D  D    D   D   D       S            D    D  S    S   S    S          I    S        S    S       S       S    
2024-02-02 21:53:12,730 ========================================================================================================================
2024-02-02 21:53:12,731 Logging Sequence: 148_155.00
2024-02-02 21:53:12,731 	Gloss Reference :	A B+C+D+E
2024-02-02 21:53:12,731 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:53:12,731 	Gloss Alignment :	         
2024-02-02 21:53:12,731 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:53:12,732 	Text Reference  :	india won the match with 263   balls remaining and  without losing any wicket
2024-02-02 21:53:12,732 	Text Hypothesis :	india won the ***** **** world cup   2023      will be      held   in  qatar 
2024-02-02 21:53:12,732 	Text Alignment  :	              D     D    S     S     S         S    S       S      S   S     
2024-02-02 21:53:12,732 ========================================================================================================================
2024-02-02 21:53:12,733 Logging Sequence: 126_99.00
2024-02-02 21:53:12,733 	Gloss Reference :	A B+C+D+E
2024-02-02 21:53:12,733 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:53:12,733 	Gloss Alignment :	         
2024-02-02 21:53:12,733 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:53:12,734 	Text Reference  :	he dedicated the     medal to *** ** sprinter   milkha singh
2024-02-02 21:53:12,734 	Text Hypothesis :	he got       another year  to win an individual gold   medal
2024-02-02 21:53:12,734 	Text Alignment  :	   S         S       S        I   I  S          S      S    
2024-02-02 21:53:12,734 ========================================================================================================================
2024-02-02 21:53:12,734 Logging Sequence: 149_77.00
2024-02-02 21:53:12,734 	Gloss Reference :	A B+C+D+E
2024-02-02 21:53:12,734 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:53:12,735 	Gloss Alignment :	         
2024-02-02 21:53:12,735 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:53:12,736 	Text Reference  :	and arrested danushka for alleged sexual assault of  a 29       year old    woman      whose name has not *** been    disclosed
2024-02-02 21:53:12,737 	Text Hypothesis :	*** ******** ******** *** woman   from   rose    bay a suburban of   sydney complained that  she  was not get married 2022     
2024-02-02 21:53:12,737 	Text Alignment  :	D   D        D        D   S       S      S       S     S        S    S      S          S     S    S       I   S       S        
2024-02-02 21:53:12,737 ========================================================================================================================
2024-02-02 21:53:12,737 Logging Sequence: 168_15.00
2024-02-02 21:53:12,737 	Gloss Reference :	A B+C+D+E
2024-02-02 21:53:12,737 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:53:12,737 	Gloss Alignment :	         
2024-02-02 21:53:12,737 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:53:12,738 	Text Reference  :	when in       public the couple are always approached for photographys and   autographs
2024-02-02 21:53:12,738 	Text Hypothesis :	and  covering up     the ****** *** little one        to  a            world cup       
2024-02-02 21:53:12,739 	Text Alignment  :	S    S        S          D      D   S      S          S   S            S     S         
2024-02-02 21:53:12,739 ========================================================================================================================
2024-02-02 21:53:12,870 Epoch 806: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.34 
2024-02-02 21:53:12,870 EPOCH 807
2024-02-02 21:53:18,494 Epoch 807: Total Training Recognition Loss 0.02  Total Training Translation Loss 8.20 
2024-02-02 21:53:18,495 EPOCH 808
2024-02-02 21:53:21,224 [Epoch: 808 Step: 00054100] Batch Recognition Loss:   0.000529 => Gls Tokens per Sec:     1818 || Batch Translation Loss:   0.123615 => Txt Tokens per Sec:     5144 || Lr: 0.000050
2024-02-02 21:53:24,128 Epoch 808: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.40 
2024-02-02 21:53:24,128 EPOCH 809
2024-02-02 21:53:29,373 [Epoch: 809 Step: 00054200] Batch Recognition Loss:   0.000377 => Gls Tokens per Sec:     1936 || Batch Translation Loss:   0.029416 => Txt Tokens per Sec:     5378 || Lr: 0.000050
2024-02-02 21:53:29,584 Epoch 809: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.90 
2024-02-02 21:53:29,584 EPOCH 810
2024-02-02 21:53:34,660 Epoch 810: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.67 
2024-02-02 21:53:34,660 EPOCH 811
2024-02-02 21:53:37,115 [Epoch: 811 Step: 00054300] Batch Recognition Loss:   0.000203 => Gls Tokens per Sec:     1920 || Batch Translation Loss:   0.021960 => Txt Tokens per Sec:     5350 || Lr: 0.000050
2024-02-02 21:53:40,153 Epoch 811: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.98 
2024-02-02 21:53:40,153 EPOCH 812
2024-02-02 21:53:44,613 [Epoch: 812 Step: 00054400] Batch Recognition Loss:   0.000188 => Gls Tokens per Sec:     2241 || Batch Translation Loss:   0.020764 => Txt Tokens per Sec:     6229 || Lr: 0.000050
2024-02-02 21:53:44,887 Epoch 812: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.59 
2024-02-02 21:53:44,887 EPOCH 813
2024-02-02 21:53:50,391 Epoch 813: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.89 
2024-02-02 21:53:50,391 EPOCH 814
2024-02-02 21:53:52,629 [Epoch: 814 Step: 00054500] Batch Recognition Loss:   0.000185 => Gls Tokens per Sec:     2074 || Batch Translation Loss:   0.018934 => Txt Tokens per Sec:     5931 || Lr: 0.000050
2024-02-02 21:53:55,661 Epoch 814: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.90 
2024-02-02 21:53:55,661 EPOCH 815
2024-02-02 21:54:00,510 [Epoch: 815 Step: 00054600] Batch Recognition Loss:   0.000294 => Gls Tokens per Sec:     2028 || Batch Translation Loss:   0.042524 => Txt Tokens per Sec:     5597 || Lr: 0.000050
2024-02-02 21:54:00,954 Epoch 815: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.09 
2024-02-02 21:54:00,954 EPOCH 816
2024-02-02 21:54:06,389 Epoch 816: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.64 
2024-02-02 21:54:06,389 EPOCH 817
2024-02-02 21:54:08,569 [Epoch: 817 Step: 00054700] Batch Recognition Loss:   0.000381 => Gls Tokens per Sec:     2056 || Batch Translation Loss:   0.024214 => Txt Tokens per Sec:     5769 || Lr: 0.000050
2024-02-02 21:54:11,791 Epoch 817: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.24 
2024-02-02 21:54:11,792 EPOCH 818
2024-02-02 21:54:16,835 [Epoch: 818 Step: 00054800] Batch Recognition Loss:   0.000162 => Gls Tokens per Sec:     1918 || Batch Translation Loss:   0.023727 => Txt Tokens per Sec:     5377 || Lr: 0.000050
2024-02-02 21:54:17,272 Epoch 818: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.69 
2024-02-02 21:54:17,272 EPOCH 819
2024-02-02 21:54:21,980 Epoch 819: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.46 
2024-02-02 21:54:21,981 EPOCH 820
2024-02-02 21:54:23,852 [Epoch: 820 Step: 00054900] Batch Recognition Loss:   0.000188 => Gls Tokens per Sec:     2263 || Batch Translation Loss:   0.025384 => Txt Tokens per Sec:     6069 || Lr: 0.000050
2024-02-02 21:54:26,773 Epoch 820: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.24 
2024-02-02 21:54:26,774 EPOCH 821
2024-02-02 21:54:31,148 [Epoch: 821 Step: 00055000] Batch Recognition Loss:   0.000251 => Gls Tokens per Sec:     2174 || Batch Translation Loss:   0.019319 => Txt Tokens per Sec:     6026 || Lr: 0.000050
2024-02-02 21:54:31,630 Epoch 821: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.35 
2024-02-02 21:54:31,630 EPOCH 822
2024-02-02 21:54:37,086 Epoch 822: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.44 
2024-02-02 21:54:37,087 EPOCH 823
2024-02-02 21:54:38,972 [Epoch: 823 Step: 00055100] Batch Recognition Loss:   0.000173 => Gls Tokens per Sec:     2207 || Batch Translation Loss:   0.020795 => Txt Tokens per Sec:     6318 || Lr: 0.000050
2024-02-02 21:54:42,443 Epoch 823: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.10 
2024-02-02 21:54:42,444 EPOCH 824
2024-02-02 21:54:47,373 [Epoch: 824 Step: 00055200] Batch Recognition Loss:   0.000316 => Gls Tokens per Sec:     1897 || Batch Translation Loss:   0.468593 => Txt Tokens per Sec:     5264 || Lr: 0.000050
2024-02-02 21:54:48,018 Epoch 824: Total Training Recognition Loss 0.02  Total Training Translation Loss 9.73 
2024-02-02 21:54:48,018 EPOCH 825
2024-02-02 21:54:53,470 Epoch 825: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.35 
2024-02-02 21:54:53,470 EPOCH 826
2024-02-02 21:54:55,289 [Epoch: 826 Step: 00055300] Batch Recognition Loss:   0.000220 => Gls Tokens per Sec:     2201 || Batch Translation Loss:   0.022477 => Txt Tokens per Sec:     6054 || Lr: 0.000050
2024-02-02 21:54:58,857 Epoch 826: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.90 
2024-02-02 21:54:58,858 EPOCH 827
2024-02-02 21:55:03,126 [Epoch: 827 Step: 00055400] Batch Recognition Loss:   0.000191 => Gls Tokens per Sec:     2154 || Batch Translation Loss:   0.024654 => Txt Tokens per Sec:     5937 || Lr: 0.000050
2024-02-02 21:55:03,799 Epoch 827: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.60 
2024-02-02 21:55:03,799 EPOCH 828
2024-02-02 21:55:09,327 Epoch 828: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.62 
2024-02-02 21:55:09,327 EPOCH 829
2024-02-02 21:55:11,240 [Epoch: 829 Step: 00055500] Batch Recognition Loss:   0.000337 => Gls Tokens per Sec:     2009 || Batch Translation Loss:   0.029104 => Txt Tokens per Sec:     5697 || Lr: 0.000050
2024-02-02 21:55:14,687 Epoch 829: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.61 
2024-02-02 21:55:14,687 EPOCH 830
2024-02-02 21:55:19,744 [Epoch: 830 Step: 00055600] Batch Recognition Loss:   0.000260 => Gls Tokens per Sec:     1786 || Batch Translation Loss:   0.026612 => Txt Tokens per Sec:     4983 || Lr: 0.000050
2024-02-02 21:55:20,484 Epoch 830: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.69 
2024-02-02 21:55:20,484 EPOCH 831
2024-02-02 21:55:25,656 Epoch 831: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.52 
2024-02-02 21:55:25,657 EPOCH 832
2024-02-02 21:55:27,242 [Epoch: 832 Step: 00055700] Batch Recognition Loss:   0.000116 => Gls Tokens per Sec:     2323 || Batch Translation Loss:   0.010137 => Txt Tokens per Sec:     6350 || Lr: 0.000050
2024-02-02 21:55:30,796 Epoch 832: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.01 
2024-02-02 21:55:30,797 EPOCH 833
2024-02-02 21:55:35,415 [Epoch: 833 Step: 00055800] Batch Recognition Loss:   0.000133 => Gls Tokens per Sec:     1921 || Batch Translation Loss:   0.018633 => Txt Tokens per Sec:     5291 || Lr: 0.000050
2024-02-02 21:55:36,328 Epoch 833: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.64 
2024-02-02 21:55:36,328 EPOCH 834
2024-02-02 21:55:41,574 Epoch 834: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.35 
2024-02-02 21:55:41,575 EPOCH 835
2024-02-02 21:55:43,473 [Epoch: 835 Step: 00055900] Batch Recognition Loss:   0.000254 => Gls Tokens per Sec:     1856 || Batch Translation Loss:   0.025379 => Txt Tokens per Sec:     5182 || Lr: 0.000050
2024-02-02 21:55:47,140 Epoch 835: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.17 
2024-02-02 21:55:47,140 EPOCH 836
2024-02-02 21:55:51,191 [Epoch: 836 Step: 00056000] Batch Recognition Loss:   0.000198 => Gls Tokens per Sec:     2151 || Batch Translation Loss:   0.017494 => Txt Tokens per Sec:     6042 || Lr: 0.000050
2024-02-02 21:55:59,523 Validation result at epoch 836, step    56000: duration: 8.3320s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00123	Translation Loss: 91977.75000	PPL: 9938.96289
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.67	(BLEU-1: 9.77,	BLEU-2: 3.06,	BLEU-3: 1.32,	BLEU-4: 0.67)
	CHRF 16.40	ROUGE 8.95
2024-02-02 21:55:59,524 Logging Recognition and Translation Outputs
2024-02-02 21:55:59,525 ========================================================================================================================
2024-02-02 21:55:59,525 Logging Sequence: 122_110.00
2024-02-02 21:55:59,525 	Gloss Reference :	A B+C+D+E
2024-02-02 21:55:59,525 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:55:59,525 	Gloss Alignment :	         
2024-02-02 21:55:59,525 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:55:59,526 	Text Reference  :	** now that i    achieved my dream   and   secured a    silver medal 
2024-02-02 21:55:59,526 	Text Hypothesis :	if her goal then used     to olympic games so      many 9      medals
2024-02-02 21:55:59,526 	Text Alignment  :	I  S   S    S    S        S  S       S     S       S    S      S     
2024-02-02 21:55:59,526 ========================================================================================================================
2024-02-02 21:55:59,527 Logging Sequence: 161_111.00
2024-02-02 21:55:59,527 	Gloss Reference :	A B+C+D+E
2024-02-02 21:55:59,527 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:55:59,527 	Gloss Alignment :	         
2024-02-02 21:55:59,527 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:55:59,529 	Text Reference  :	*** ********* ****** ***** his   last game as  captain  was  the     cape      town    test in   south africa in   jan     2022
2024-02-02 21:55:59,529 	Text Hypothesis :	the rajasthan royals owner whose name is   not revealed that shocked teammates january 2022 that is    on     12th january 2022
2024-02-02 21:55:59,529 	Text Alignment  :	I   I         I      I     S     S    S    S   S        S    S       S         S       S    S    S     S      S    S           
2024-02-02 21:55:59,529 ========================================================================================================================
2024-02-02 21:55:59,529 Logging Sequence: 136_79.00
2024-02-02 21:55:59,530 	Gloss Reference :	A B+C+D+E
2024-02-02 21:55:59,530 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:55:59,530 	Gloss Alignment :	         
2024-02-02 21:55:59,530 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:55:59,531 	Text Reference  :	with this win   sindhu became the first indian woman to  win two         individual olympic medals
2024-02-02 21:55:59,531 	Text Hypothesis :	**** **** sadly sindhu lost   the ***** match  and   she was heartbroken over       the     loss  
2024-02-02 21:55:59,531 	Text Alignment  :	D    D    S            S          D     S      S     S   S   S           S          S       S     
2024-02-02 21:55:59,531 ========================================================================================================================
2024-02-02 21:55:59,532 Logging Sequence: 166_335.00
2024-02-02 21:55:59,532 	Gloss Reference :	A B+C+D+E
2024-02-02 21:55:59,532 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:55:59,532 	Gloss Alignment :	         
2024-02-02 21:55:59,532 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:55:59,534 	Text Reference  :	******* ** ********** the ******** **** ** second   world test       championship is scheduled from june 2021  to 30 april 2023 
2024-02-02 21:55:59,534 	Text Hypothesis :	instead of respecting the couple's wish to maintain their daughter's privacy      as he        is   very close to ** go    waste
2024-02-02 21:55:59,534 	Text Alignment  :	I       I  I              I        I    I  S        S     S          S            S  S         S    S    S        D  S     S    
2024-02-02 21:55:59,534 ========================================================================================================================
2024-02-02 21:55:59,534 Logging Sequence: 95_152.00
2024-02-02 21:55:59,534 	Gloss Reference :	A B+C+D+E
2024-02-02 21:55:59,534 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:55:59,535 	Gloss Alignment :	         
2024-02-02 21:55:59,535 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:55:59,535 	Text Reference  :	**** *** ****** how    strange
2024-02-02 21:55:59,535 	Text Hypothesis :	they are caught indian cricket
2024-02-02 21:55:59,535 	Text Alignment  :	I    I   I      S      S      
2024-02-02 21:55:59,535 ========================================================================================================================
2024-02-02 21:56:00,429 Epoch 836: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.10 
2024-02-02 21:56:00,429 EPOCH 837
2024-02-02 21:56:06,089 Epoch 837: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.28 
2024-02-02 21:56:06,089 EPOCH 838
2024-02-02 21:56:07,729 [Epoch: 838 Step: 00056100] Batch Recognition Loss:   0.000180 => Gls Tokens per Sec:     2050 || Batch Translation Loss:   0.026820 => Txt Tokens per Sec:     5558 || Lr: 0.000050
2024-02-02 21:56:11,067 Epoch 838: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.92 
2024-02-02 21:56:11,068 EPOCH 839
2024-02-02 21:56:15,347 [Epoch: 839 Step: 00056200] Batch Recognition Loss:   0.000144 => Gls Tokens per Sec:     1999 || Batch Translation Loss:   0.024462 => Txt Tokens per Sec:     5566 || Lr: 0.000050
2024-02-02 21:56:16,475 Epoch 839: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.88 
2024-02-02 21:56:16,476 EPOCH 840
2024-02-02 21:56:22,090 Epoch 840: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.59 
2024-02-02 21:56:22,091 EPOCH 841
2024-02-02 21:56:23,708 [Epoch: 841 Step: 00056300] Batch Recognition Loss:   0.000284 => Gls Tokens per Sec:     1923 || Batch Translation Loss:   0.023857 => Txt Tokens per Sec:     5430 || Lr: 0.000050
2024-02-02 21:56:27,256 Epoch 841: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.29 
2024-02-02 21:56:27,256 EPOCH 842
2024-02-02 21:56:31,663 [Epoch: 842 Step: 00056400] Batch Recognition Loss:   0.000135 => Gls Tokens per Sec:     1904 || Batch Translation Loss:   0.011697 => Txt Tokens per Sec:     5313 || Lr: 0.000050
2024-02-02 21:56:32,795 Epoch 842: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.29 
2024-02-02 21:56:32,796 EPOCH 843
2024-02-02 21:56:37,874 Epoch 843: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.54 
2024-02-02 21:56:37,874 EPOCH 844
2024-02-02 21:56:39,200 [Epoch: 844 Step: 00056500] Batch Recognition Loss:   0.000127 => Gls Tokens per Sec:     2296 || Batch Translation Loss:   0.033219 => Txt Tokens per Sec:     6415 || Lr: 0.000050
2024-02-02 21:56:42,418 Epoch 844: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.93 
2024-02-02 21:56:42,418 EPOCH 845
2024-02-02 21:56:46,216 [Epoch: 845 Step: 00056600] Batch Recognition Loss:   0.000303 => Gls Tokens per Sec:     2167 || Batch Translation Loss:   0.064085 => Txt Tokens per Sec:     6172 || Lr: 0.000050
2024-02-02 21:56:47,089 Epoch 845: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.34 
2024-02-02 21:56:47,089 EPOCH 846
2024-02-02 21:56:51,993 Epoch 846: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.01 
2024-02-02 21:56:51,994 EPOCH 847
2024-02-02 21:56:53,379 [Epoch: 847 Step: 00056700] Batch Recognition Loss:   0.000190 => Gls Tokens per Sec:     2015 || Batch Translation Loss:   0.031157 => Txt Tokens per Sec:     5754 || Lr: 0.000050
2024-02-02 21:56:57,564 Epoch 847: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.47 
2024-02-02 21:56:57,564 EPOCH 848
2024-02-02 21:57:01,618 [Epoch: 848 Step: 00056800] Batch Recognition Loss:   0.000158 => Gls Tokens per Sec:     1992 || Batch Translation Loss:   0.017154 => Txt Tokens per Sec:     5453 || Lr: 0.000050
2024-02-02 21:57:02,923 Epoch 848: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.97 
2024-02-02 21:57:02,923 EPOCH 849
2024-02-02 21:57:07,907 Epoch 849: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.99 
2024-02-02 21:57:07,908 EPOCH 850
2024-02-02 21:57:09,298 [Epoch: 850 Step: 00056900] Batch Recognition Loss:   0.000593 => Gls Tokens per Sec:     1896 || Batch Translation Loss:   0.047511 => Txt Tokens per Sec:     5109 || Lr: 0.000050
2024-02-02 21:57:13,621 Epoch 850: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.44 
2024-02-02 21:57:13,622 EPOCH 851
2024-02-02 21:57:17,867 [Epoch: 851 Step: 00057000] Batch Recognition Loss:   0.000181 => Gls Tokens per Sec:     1864 || Batch Translation Loss:   0.017342 => Txt Tokens per Sec:     5275 || Lr: 0.000050
2024-02-02 21:57:19,070 Epoch 851: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.46 
2024-02-02 21:57:19,070 EPOCH 852
2024-02-02 21:57:24,254 Epoch 852: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.63 
2024-02-02 21:57:24,254 EPOCH 853
2024-02-02 21:57:25,598 [Epoch: 853 Step: 00057100] Batch Recognition Loss:   0.000246 => Gls Tokens per Sec:     1906 || Batch Translation Loss:   0.029003 => Txt Tokens per Sec:     5812 || Lr: 0.000050
2024-02-02 21:57:28,959 Epoch 853: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.71 
2024-02-02 21:57:28,959 EPOCH 854
2024-02-02 21:57:33,022 [Epoch: 854 Step: 00057200] Batch Recognition Loss:   0.000195 => Gls Tokens per Sec:     1908 || Batch Translation Loss:   0.021844 => Txt Tokens per Sec:     5392 || Lr: 0.000050
2024-02-02 21:57:34,479 Epoch 854: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.70 
2024-02-02 21:57:34,479 EPOCH 855
2024-02-02 21:57:39,899 Epoch 855: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.39 
2024-02-02 21:57:39,900 EPOCH 856
2024-02-02 21:57:40,949 [Epoch: 856 Step: 00057300] Batch Recognition Loss:   0.000155 => Gls Tokens per Sec:     2288 || Batch Translation Loss:   0.017245 => Txt Tokens per Sec:     6249 || Lr: 0.000050
2024-02-02 21:57:45,336 Epoch 856: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.52 
2024-02-02 21:57:45,336 EPOCH 857
2024-02-02 21:57:48,710 [Epoch: 857 Step: 00057400] Batch Recognition Loss:   0.000264 => Gls Tokens per Sec:     2276 || Batch Translation Loss:   0.012027 => Txt Tokens per Sec:     6263 || Lr: 0.000050
2024-02-02 21:57:50,203 Epoch 857: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.46 
2024-02-02 21:57:50,204 EPOCH 858
2024-02-02 21:57:55,819 Epoch 858: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.56 
2024-02-02 21:57:55,820 EPOCH 859
2024-02-02 21:57:56,847 [Epoch: 859 Step: 00057500] Batch Recognition Loss:   0.000145 => Gls Tokens per Sec:     2183 || Batch Translation Loss:   0.014261 => Txt Tokens per Sec:     5844 || Lr: 0.000050
2024-02-02 21:58:00,996 Epoch 859: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.86 
2024-02-02 21:58:00,996 EPOCH 860
2024-02-02 21:58:04,715 [Epoch: 860 Step: 00057600] Batch Recognition Loss:   0.000186 => Gls Tokens per Sec:     1998 || Batch Translation Loss:   0.027567 => Txt Tokens per Sec:     5454 || Lr: 0.000050
2024-02-02 21:58:06,486 Epoch 860: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.85 
2024-02-02 21:58:06,486 EPOCH 861
2024-02-02 21:58:11,653 Epoch 861: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.89 
2024-02-02 21:58:11,654 EPOCH 862
2024-02-02 21:58:12,859 [Epoch: 862 Step: 00057700] Batch Recognition Loss:   0.000238 => Gls Tokens per Sec:     1654 || Batch Translation Loss:   0.019860 => Txt Tokens per Sec:     4939 || Lr: 0.000050
2024-02-02 21:58:16,759 Epoch 862: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.74 
2024-02-02 21:58:16,759 EPOCH 863
2024-02-02 21:58:20,457 [Epoch: 863 Step: 00057800] Batch Recognition Loss:   0.000144 => Gls Tokens per Sec:     1991 || Batch Translation Loss:   0.045527 => Txt Tokens per Sec:     5496 || Lr: 0.000050
2024-02-02 21:58:22,259 Epoch 863: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.56 
2024-02-02 21:58:22,259 EPOCH 864
2024-02-02 21:58:27,292 Epoch 864: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.70 
2024-02-02 21:58:27,292 EPOCH 865
2024-02-02 21:58:28,136 [Epoch: 865 Step: 00057900] Batch Recognition Loss:   0.000205 => Gls Tokens per Sec:     2278 || Batch Translation Loss:   0.018723 => Txt Tokens per Sec:     6314 || Lr: 0.000050
2024-02-02 21:58:32,520 Epoch 865: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.62 
2024-02-02 21:58:32,520 EPOCH 866
2024-02-02 21:58:36,019 [Epoch: 866 Step: 00058000] Batch Recognition Loss:   0.000150 => Gls Tokens per Sec:     2032 || Batch Translation Loss:   0.013017 => Txt Tokens per Sec:     5497 || Lr: 0.000050
2024-02-02 21:58:44,570 Validation result at epoch 866, step    58000: duration: 8.5510s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00143	Translation Loss: 91837.87500	PPL: 9800.81445
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.81	(BLEU-1: 10.65,	BLEU-2: 3.44,	BLEU-3: 1.56,	BLEU-4: 0.81)
	CHRF 16.69	ROUGE 9.11
2024-02-02 21:58:44,571 Logging Recognition and Translation Outputs
2024-02-02 21:58:44,571 ========================================================================================================================
2024-02-02 21:58:44,572 Logging Sequence: 180_138.00
2024-02-02 21:58:44,572 	Gloss Reference :	A B+C+D+E
2024-02-02 21:58:44,572 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:58:44,572 	Gloss Alignment :	         
2024-02-02 21:58:44,572 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:58:44,574 	Text Reference  :	ioa president p t usha constituted a    seven-member panel which included world    champions from     various sports to    inquire into the     allegations
2024-02-02 21:58:44,574 	Text Hypothesis :	*** ********* * * **** not         only this         match they  blamed   mohammed shami     fielding with    a      towel singh   for  india's loss       
2024-02-02 21:58:44,574 	Text Alignment  :	D   D         D D D    S           S    S            S     S     S        S        S         S        S       S      S     S       S    S       S          
2024-02-02 21:58:44,575 ========================================================================================================================
2024-02-02 21:58:44,575 Logging Sequence: 128_189.00
2024-02-02 21:58:44,575 	Gloss Reference :	A B+C+D+E
2024-02-02 21:58:44,575 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:58:44,575 	Gloss Alignment :	         
2024-02-02 21:58:44,575 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:58:44,576 	Text Reference  :	meanwhile some funny    incidents happened during the    match
2024-02-02 21:58:44,576 	Text Hypothesis :	for       the  olympics are       aware    that   sushil kumar
2024-02-02 21:58:44,576 	Text Alignment  :	S         S    S        S         S        S      S      S    
2024-02-02 21:58:44,576 ========================================================================================================================
2024-02-02 21:58:44,576 Logging Sequence: 165_523.00
2024-02-02 21:58:44,576 	Gloss Reference :	A B+C+D+E
2024-02-02 21:58:44,577 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:58:44,577 	Gloss Alignment :	         
2024-02-02 21:58:44,577 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:58:44,578 	Text Reference  :	as he believed that his team might lose if  he    takes  off     his batting pads
2024-02-02 21:58:44,578 	Text Hypothesis :	** ** ******** **** *** when india lost the match ticket booking the indian  team
2024-02-02 21:58:44,578 	Text Alignment  :	D  D  D        D    D   S    S     S    S   S     S      S       S   S       S   
2024-02-02 21:58:44,578 ========================================================================================================================
2024-02-02 21:58:44,578 Logging Sequence: 145_168.00
2024-02-02 21:58:44,578 	Gloss Reference :	A B+C+D+E
2024-02-02 21:58:44,579 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:58:44,579 	Gloss Alignment :	         
2024-02-02 21:58:44,579 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:58:44,579 	Text Reference  :	******* ** the decision    has     devastated sameeha and  her     parents
2024-02-02 21:58:44,580 	Text Hypothesis :	sameeha is an  interesting history about      indian  deaf women's team   
2024-02-02 21:58:44,580 	Text Alignment  :	I       I  S   S           S       S          S       S    S       S      
2024-02-02 21:58:44,580 ========================================================================================================================
2024-02-02 21:58:44,580 Logging Sequence: 92_123.00
2024-02-02 21:58:44,580 	Gloss Reference :	A B+C+D+E
2024-02-02 21:58:44,580 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 21:58:44,580 	Gloss Alignment :	         
2024-02-02 21:58:44,580 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 21:58:44,581 	Text Reference  :	a heated argument also took place between members of the    family    and     the two men   
2024-02-02 21:58:44,581 	Text Hypothesis :	* ****** ******** **** **** ***** shekhar filed   a  police complaint against the *** indian
2024-02-02 21:58:44,581 	Text Alignment  :	D D      D        D    D    D     S       S       S  S      S         S           D   S     
2024-02-02 21:58:44,582 ========================================================================================================================
2024-02-02 21:58:46,480 Epoch 866: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.55 
2024-02-02 21:58:46,480 EPOCH 867
2024-02-02 21:58:51,928 Epoch 867: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.45 
2024-02-02 21:58:51,929 EPOCH 868
2024-02-02 21:58:52,811 [Epoch: 868 Step: 00058100] Batch Recognition Loss:   0.000184 => Gls Tokens per Sec:     2001 || Batch Translation Loss:   0.016750 => Txt Tokens per Sec:     5579 || Lr: 0.000050
2024-02-02 21:58:57,651 Epoch 868: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.41 
2024-02-02 21:58:57,652 EPOCH 869
2024-02-02 21:59:00,984 [Epoch: 869 Step: 00058200] Batch Recognition Loss:   0.000154 => Gls Tokens per Sec:     2114 || Batch Translation Loss:   0.053372 => Txt Tokens per Sec:     5907 || Lr: 0.000050
2024-02-02 21:59:02,864 Epoch 869: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.69 
2024-02-02 21:59:02,865 EPOCH 870
2024-02-02 21:59:08,074 Epoch 870: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.28 
2024-02-02 21:59:08,075 EPOCH 871
2024-02-02 21:59:08,829 [Epoch: 871 Step: 00058300] Batch Recognition Loss:   0.000525 => Gls Tokens per Sec:     2124 || Batch Translation Loss:   0.198248 => Txt Tokens per Sec:     5819 || Lr: 0.000050
2024-02-02 21:59:13,403 Epoch 871: Total Training Recognition Loss 0.03  Total Training Translation Loss 10.48 
2024-02-02 21:59:13,404 EPOCH 872
2024-02-02 21:59:16,960 [Epoch: 872 Step: 00058400] Batch Recognition Loss:   0.000359 => Gls Tokens per Sec:     1911 || Batch Translation Loss:   0.029487 => Txt Tokens per Sec:     5439 || Lr: 0.000050
2024-02-02 21:59:18,898 Epoch 872: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.89 
2024-02-02 21:59:18,899 EPOCH 873
2024-02-02 21:59:24,119 Epoch 873: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.41 
2024-02-02 21:59:24,120 EPOCH 874
2024-02-02 21:59:25,116 [Epoch: 874 Step: 00058500] Batch Recognition Loss:   0.000270 => Gls Tokens per Sec:     1445 || Batch Translation Loss:   0.030974 => Txt Tokens per Sec:     4274 || Lr: 0.000050
2024-02-02 21:59:29,554 Epoch 874: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.77 
2024-02-02 21:59:29,554 EPOCH 875
2024-02-02 21:59:32,751 [Epoch: 875 Step: 00058600] Batch Recognition Loss:   0.000240 => Gls Tokens per Sec:     2074 || Batch Translation Loss:   0.025738 => Txt Tokens per Sec:     5893 || Lr: 0.000050
2024-02-02 21:59:34,634 Epoch 875: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.65 
2024-02-02 21:59:34,635 EPOCH 876
2024-02-02 21:59:39,856 Epoch 876: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.32 
2024-02-02 21:59:39,856 EPOCH 877
2024-02-02 21:59:40,335 [Epoch: 877 Step: 00058700] Batch Recognition Loss:   0.000155 => Gls Tokens per Sec:     2678 || Batch Translation Loss:   0.010809 => Txt Tokens per Sec:     7374 || Lr: 0.000050
2024-02-02 21:59:45,011 Epoch 877: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.39 
2024-02-02 21:59:45,011 EPOCH 878
2024-02-02 21:59:48,047 [Epoch: 878 Step: 00058800] Batch Recognition Loss:   0.000149 => Gls Tokens per Sec:     2161 || Batch Translation Loss:   0.022524 => Txt Tokens per Sec:     5756 || Lr: 0.000050
2024-02-02 21:59:50,301 Epoch 878: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.87 
2024-02-02 21:59:50,302 EPOCH 879
2024-02-02 21:59:55,520 Epoch 879: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.10 
2024-02-02 21:59:55,520 EPOCH 880
2024-02-02 21:59:56,023 [Epoch: 880 Step: 00058900] Batch Recognition Loss:   0.000184 => Gls Tokens per Sec:     2230 || Batch Translation Loss:   0.062688 => Txt Tokens per Sec:     6255 || Lr: 0.000050
2024-02-02 22:00:00,640 Epoch 880: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.09 
2024-02-02 22:00:00,640 EPOCH 881
2024-02-02 22:00:04,165 [Epoch: 881 Step: 00059000] Batch Recognition Loss:   0.000112 => Gls Tokens per Sec:     1791 || Batch Translation Loss:   0.030483 => Txt Tokens per Sec:     4977 || Lr: 0.000050
2024-02-02 22:00:06,226 Epoch 881: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.82 
2024-02-02 22:00:06,226 EPOCH 882
2024-02-02 22:00:11,330 Epoch 882: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.01 
2024-02-02 22:00:11,331 EPOCH 883
2024-02-02 22:00:11,856 [Epoch: 883 Step: 00059100] Batch Recognition Loss:   0.000219 => Gls Tokens per Sec:     1836 || Batch Translation Loss:   0.011028 => Txt Tokens per Sec:     4321 || Lr: 0.000050
2024-02-02 22:00:16,914 Epoch 883: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.16 
2024-02-02 22:00:16,915 EPOCH 884
2024-02-02 22:00:19,334 [Epoch: 884 Step: 00059200] Batch Recognition Loss:   0.000196 => Gls Tokens per Sec:     2542 || Batch Translation Loss:   0.015748 => Txt Tokens per Sec:     6964 || Lr: 0.000050
2024-02-02 22:00:21,853 Epoch 884: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.60 
2024-02-02 22:00:21,854 EPOCH 885
2024-02-02 22:00:27,262 Epoch 885: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.17 
2024-02-02 22:00:27,262 EPOCH 886
2024-02-02 22:00:27,552 [Epoch: 886 Step: 00059300] Batch Recognition Loss:   0.000174 => Gls Tokens per Sec:     2768 || Batch Translation Loss:   0.006753 => Txt Tokens per Sec:     6834 || Lr: 0.000050
2024-02-02 22:00:32,251 Epoch 886: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.55 
2024-02-02 22:00:32,252 EPOCH 887
2024-02-02 22:00:35,356 [Epoch: 887 Step: 00059400] Batch Recognition Loss:   0.000137 => Gls Tokens per Sec:     1959 || Batch Translation Loss:   0.018391 => Txt Tokens per Sec:     5399 || Lr: 0.000050
2024-02-02 22:00:37,665 Epoch 887: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.49 
2024-02-02 22:00:37,665 EPOCH 888
2024-02-02 22:00:43,074 Epoch 888: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.84 
2024-02-02 22:00:43,075 EPOCH 889
2024-02-02 22:00:43,348 [Epoch: 889 Step: 00059500] Batch Recognition Loss:   0.000135 => Gls Tokens per Sec:     2353 || Batch Translation Loss:   0.024899 => Txt Tokens per Sec:     6868 || Lr: 0.000050
2024-02-02 22:00:47,904 Epoch 889: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.16 
2024-02-02 22:00:47,904 EPOCH 890
2024-02-02 22:00:50,634 [Epoch: 890 Step: 00059600] Batch Recognition Loss:   0.000218 => Gls Tokens per Sec:     2169 || Batch Translation Loss:   0.021684 => Txt Tokens per Sec:     6201 || Lr: 0.000050
2024-02-02 22:00:52,734 Epoch 890: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.82 
2024-02-02 22:00:52,735 EPOCH 891
2024-02-02 22:00:58,317 Epoch 891: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.97 
2024-02-02 22:00:58,318 EPOCH 892
2024-02-02 22:00:58,501 [Epoch: 892 Step: 00059700] Batch Recognition Loss:   0.000202 => Gls Tokens per Sec:     2637 || Batch Translation Loss:   0.057619 => Txt Tokens per Sec:     6528 || Lr: 0.000050
2024-02-02 22:01:03,413 Epoch 892: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.21 
2024-02-02 22:01:03,413 EPOCH 893
2024-02-02 22:01:06,392 [Epoch: 893 Step: 00059800] Batch Recognition Loss:   0.000262 => Gls Tokens per Sec:     1934 || Batch Translation Loss:   0.036813 => Txt Tokens per Sec:     5519 || Lr: 0.000050
2024-02-02 22:01:08,868 Epoch 893: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.74 
2024-02-02 22:01:08,869 EPOCH 894
2024-02-02 22:01:13,948 Epoch 894: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.63 
2024-02-02 22:01:13,948 EPOCH 895
2024-02-02 22:01:14,084 [Epoch: 895 Step: 00059900] Batch Recognition Loss:   0.000288 => Gls Tokens per Sec:     2370 || Batch Translation Loss:   0.017583 => Txt Tokens per Sec:     6392 || Lr: 0.000050
2024-02-02 22:01:19,334 Epoch 895: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.30 
2024-02-02 22:01:19,334 EPOCH 896
2024-02-02 22:01:22,312 [Epoch: 896 Step: 00060000] Batch Recognition Loss:   0.000269 => Gls Tokens per Sec:     1851 || Batch Translation Loss:   0.027502 => Txt Tokens per Sec:     5255 || Lr: 0.000050
2024-02-02 22:01:30,560 Validation result at epoch 896, step    60000: duration: 8.2471s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00124	Translation Loss: 91836.27344	PPL: 9799.24512
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.87	(BLEU-1: 10.79,	BLEU-2: 3.52,	BLEU-3: 1.60,	BLEU-4: 0.87)
	CHRF 17.06	ROUGE 9.28
2024-02-02 22:01:30,562 Logging Recognition and Translation Outputs
2024-02-02 22:01:30,562 ========================================================================================================================
2024-02-02 22:01:30,562 Logging Sequence: 179_269.00
2024-02-02 22:01:30,562 	Gloss Reference :	A B+C+D+E
2024-02-02 22:01:30,562 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:01:30,563 	Gloss Alignment :	         
2024-02-02 22:01:30,563 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:01:30,564 	Text Reference  :	the ban would mean she can't compete in any   national or other      domestic events  
2024-02-02 22:01:30,564 	Text Hypothesis :	*** *** ***** **** *** ***** ******* ** after her      no disrespect was      intended
2024-02-02 22:01:30,564 	Text Alignment  :	D   D   D     D    D   D     D       D  S     S        S  S          S        S       
2024-02-02 22:01:30,564 ========================================================================================================================
2024-02-02 22:01:30,564 Logging Sequence: 94_253.00
2024-02-02 22:01:30,565 	Gloss Reference :	A B+C+D+E
2024-02-02 22:01:30,565 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:01:30,565 	Gloss Alignment :	         
2024-02-02 22:01:30,565 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:01:30,568 	Text Reference  :	however      some  tickets will be       kept aside    for     physical sale       at the stadiums a    few    days prior to   the  match  
2024-02-02 22:01:30,568 	Text Hypothesis :	surprisingly there were    many negative and  pakistan through their    daughter's by the ******** same people can  have  been many wickets
2024-02-02 22:01:30,568 	Text Alignment  :	S            S     S       S    S        S    S        S       S        S          S      D        S    S      S    S     S    S    S      
2024-02-02 22:01:30,568 ========================================================================================================================
2024-02-02 22:01:30,569 Logging Sequence: 114_201.00
2024-02-02 22:01:30,569 	Gloss Reference :	A B+C+D+E
2024-02-02 22:01:30,569 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:01:30,569 	Gloss Alignment :	         
2024-02-02 22:01:30,569 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:01:30,570 	Text Reference  :	*** ****** ****** this is      his first time winning the copa 
2024-02-02 22:01:30,570 	Text Hypothesis :	the couple posted a    picture of  pant  and  made    the final
2024-02-02 22:01:30,570 	Text Alignment  :	I   I      I      S    S       S   S     S    S           S    
2024-02-02 22:01:30,570 ========================================================================================================================
2024-02-02 22:01:30,571 Logging Sequence: 118_104.00
2024-02-02 22:01:30,571 	Gloss Reference :	A B+C+D+E
2024-02-02 22:01:30,571 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:01:30,571 	Gloss Alignment :	         
2024-02-02 22:01:30,571 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:01:30,572 	Text Reference  :	** *** ***** kylian mbappã strong performance in the **** match was greatly appreciated
2024-02-02 22:01:30,573 	Text Hypothesis :	in the extra time   also   said   that        at the goal from  the penalty shootout   
2024-02-02 22:01:30,573 	Text Alignment  :	I  I   I     S      S      S      S           S      I    S     S   S       S          
2024-02-02 22:01:30,573 ========================================================================================================================
2024-02-02 22:01:30,573 Logging Sequence: 144_74.00
2024-02-02 22:01:30,573 	Gloss Reference :	A B+C+D+E
2024-02-02 22:01:30,573 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:01:30,574 	Gloss Alignment :	         
2024-02-02 22:01:30,574 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:01:30,574 	Text Reference  :	** *** isn't that  amazing
2024-02-02 22:01:30,574 	Text Hypothesis :	it was a     tough auction
2024-02-02 22:01:30,574 	Text Alignment  :	I  I   S     S     S      
2024-02-02 22:01:30,575 ========================================================================================================================
2024-02-02 22:01:33,112 Epoch 896: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.18 
2024-02-02 22:01:33,112 EPOCH 897
2024-02-02 22:01:38,276 Epoch 897: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.43 
2024-02-02 22:01:38,276 EPOCH 898
2024-02-02 22:01:38,357 [Epoch: 898 Step: 00060100] Batch Recognition Loss:   0.000194 => Gls Tokens per Sec:     2000 || Batch Translation Loss:   0.022388 => Txt Tokens per Sec:     6737 || Lr: 0.000050
2024-02-02 22:01:43,584 Epoch 898: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.77 
2024-02-02 22:01:43,585 EPOCH 899
2024-02-02 22:01:45,947 [Epoch: 899 Step: 00060200] Batch Recognition Loss:   0.000186 => Gls Tokens per Sec:     2305 || Batch Translation Loss:   0.012286 => Txt Tokens per Sec:     5965 || Lr: 0.000050
2024-02-02 22:01:48,743 Epoch 899: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.28 
2024-02-02 22:01:48,743 EPOCH 900
2024-02-02 22:01:54,422 [Epoch: 900 Step: 00060300] Batch Recognition Loss:   0.000153 => Gls Tokens per Sec:     1872 || Batch Translation Loss:   0.028661 => Txt Tokens per Sec:     5197 || Lr: 0.000050
2024-02-02 22:01:54,423 Epoch 900: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.35 
2024-02-02 22:01:54,423 EPOCH 901
2024-02-02 22:01:59,471 Epoch 901: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.68 
2024-02-02 22:01:59,472 EPOCH 902
2024-02-02 22:02:02,068 [Epoch: 902 Step: 00060400] Batch Recognition Loss:   0.000254 => Gls Tokens per Sec:     2001 || Batch Translation Loss:   0.011169 => Txt Tokens per Sec:     5285 || Lr: 0.000050
2024-02-02 22:02:04,933 Epoch 902: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.31 
2024-02-02 22:02:04,933 EPOCH 903
2024-02-02 22:02:10,207 [Epoch: 903 Step: 00060500] Batch Recognition Loss:   0.000149 => Gls Tokens per Sec:     1986 || Batch Translation Loss:   0.016708 => Txt Tokens per Sec:     5515 || Lr: 0.000050
2024-02-02 22:02:10,285 Epoch 903: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.07 
2024-02-02 22:02:10,285 EPOCH 904
2024-02-02 22:02:15,856 Epoch 904: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.01 
2024-02-02 22:02:15,857 EPOCH 905
2024-02-02 22:02:18,408 [Epoch: 905 Step: 00060600] Batch Recognition Loss:   0.000156 => Gls Tokens per Sec:     2009 || Batch Translation Loss:   0.016220 => Txt Tokens per Sec:     5394 || Lr: 0.000050
2024-02-02 22:02:21,375 Epoch 905: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.13 
2024-02-02 22:02:21,375 EPOCH 906
2024-02-02 22:02:26,079 [Epoch: 906 Step: 00060700] Batch Recognition Loss:   0.000257 => Gls Tokens per Sec:     2193 || Batch Translation Loss:   0.044531 => Txt Tokens per Sec:     6152 || Lr: 0.000050
2024-02-02 22:02:26,178 Epoch 906: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.27 
2024-02-02 22:02:26,178 EPOCH 907
2024-02-02 22:02:31,144 Epoch 907: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.41 
2024-02-02 22:02:31,144 EPOCH 908
2024-02-02 22:02:33,627 [Epoch: 908 Step: 00060800] Batch Recognition Loss:   0.000229 => Gls Tokens per Sec:     1998 || Batch Translation Loss:   0.024303 => Txt Tokens per Sec:     5330 || Lr: 0.000050
2024-02-02 22:02:36,821 Epoch 908: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.00 
2024-02-02 22:02:36,821 EPOCH 909
2024-02-02 22:02:41,836 [Epoch: 909 Step: 00060900] Batch Recognition Loss:   0.000156 => Gls Tokens per Sec:     2024 || Batch Translation Loss:   0.131689 => Txt Tokens per Sec:     5607 || Lr: 0.000050
2024-02-02 22:02:42,056 Epoch 909: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.60 
2024-02-02 22:02:42,057 EPOCH 910
2024-02-02 22:02:47,594 Epoch 910: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.94 
2024-02-02 22:02:47,595 EPOCH 911
2024-02-02 22:02:50,195 [Epoch: 911 Step: 00061000] Batch Recognition Loss:   0.000128 => Gls Tokens per Sec:     1847 || Batch Translation Loss:   0.012816 => Txt Tokens per Sec:     5247 || Lr: 0.000050
2024-02-02 22:02:53,130 Epoch 911: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.29 
2024-02-02 22:02:53,130 EPOCH 912
2024-02-02 22:02:58,171 [Epoch: 912 Step: 00061100] Batch Recognition Loss:   0.000196 => Gls Tokens per Sec:     1982 || Batch Translation Loss:   0.029463 => Txt Tokens per Sec:     5479 || Lr: 0.000050
2024-02-02 22:02:58,581 Epoch 912: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.71 
2024-02-02 22:02:58,582 EPOCH 913
2024-02-02 22:03:04,220 Epoch 913: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.74 
2024-02-02 22:03:04,220 EPOCH 914
2024-02-02 22:03:06,463 [Epoch: 914 Step: 00061200] Batch Recognition Loss:   0.000323 => Gls Tokens per Sec:     2030 || Batch Translation Loss:   0.027026 => Txt Tokens per Sec:     5731 || Lr: 0.000050
2024-02-02 22:03:09,409 Epoch 914: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.26 
2024-02-02 22:03:09,410 EPOCH 915
2024-02-02 22:03:14,338 [Epoch: 915 Step: 00061300] Batch Recognition Loss:   0.000481 => Gls Tokens per Sec:     1995 || Batch Translation Loss:   0.047769 => Txt Tokens per Sec:     5571 || Lr: 0.000050
2024-02-02 22:03:14,690 Epoch 915: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.12 
2024-02-02 22:03:14,691 EPOCH 916
2024-02-02 22:03:20,022 Epoch 916: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.93 
2024-02-02 22:03:20,023 EPOCH 917
2024-02-02 22:03:22,113 [Epoch: 917 Step: 00061400] Batch Recognition Loss:   0.000184 => Gls Tokens per Sec:     2101 || Batch Translation Loss:   0.104926 => Txt Tokens per Sec:     5761 || Lr: 0.000050
2024-02-02 22:03:25,178 Epoch 917: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.13 
2024-02-02 22:03:25,179 EPOCH 918
2024-02-02 22:03:30,288 [Epoch: 918 Step: 00061500] Batch Recognition Loss:   0.000192 => Gls Tokens per Sec:     1893 || Batch Translation Loss:   0.013682 => Txt Tokens per Sec:     5306 || Lr: 0.000050
2024-02-02 22:03:30,626 Epoch 918: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.27 
2024-02-02 22:03:30,626 EPOCH 919
2024-02-02 22:03:36,000 Epoch 919: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.14 
2024-02-02 22:03:36,000 EPOCH 920
2024-02-02 22:03:37,985 [Epoch: 920 Step: 00061600] Batch Recognition Loss:   0.000305 => Gls Tokens per Sec:     2133 || Batch Translation Loss:   0.050692 => Txt Tokens per Sec:     5704 || Lr: 0.000050
2024-02-02 22:03:41,052 Epoch 920: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.35 
2024-02-02 22:03:41,053 EPOCH 921
2024-02-02 22:03:45,902 [Epoch: 921 Step: 00061700] Batch Recognition Loss:   0.000156 => Gls Tokens per Sec:     1962 || Batch Translation Loss:   0.015629 => Txt Tokens per Sec:     5407 || Lr: 0.000050
2024-02-02 22:03:46,548 Epoch 921: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.02 
2024-02-02 22:03:46,549 EPOCH 922
2024-02-02 22:03:51,843 Epoch 922: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.70 
2024-02-02 22:03:51,843 EPOCH 923
2024-02-02 22:03:53,927 [Epoch: 923 Step: 00061800] Batch Recognition Loss:   0.000174 => Gls Tokens per Sec:     1954 || Batch Translation Loss:   0.036124 => Txt Tokens per Sec:     5375 || Lr: 0.000050
2024-02-02 22:03:57,084 Epoch 923: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.32 
2024-02-02 22:03:57,084 EPOCH 924
2024-02-02 22:04:01,502 [Epoch: 924 Step: 00061900] Batch Recognition Loss:   0.000115 => Gls Tokens per Sec:     2117 || Batch Translation Loss:   0.007960 => Txt Tokens per Sec:     5860 || Lr: 0.000050
2024-02-02 22:04:02,142 Epoch 924: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.00 
2024-02-02 22:04:02,142 EPOCH 925
2024-02-02 22:04:07,689 Epoch 925: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.48 
2024-02-02 22:04:07,690 EPOCH 926
2024-02-02 22:04:09,783 [Epoch: 926 Step: 00062000] Batch Recognition Loss:   0.000136 => Gls Tokens per Sec:     1913 || Batch Translation Loss:   0.021666 => Txt Tokens per Sec:     5174 || Lr: 0.000050
2024-02-02 22:04:18,117 Validation result at epoch 926, step    62000: duration: 8.3338s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00124	Translation Loss: 92450.71875	PPL: 10420.68262
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.66	(BLEU-1: 10.33,	BLEU-2: 3.07,	BLEU-3: 1.30,	BLEU-4: 0.66)
	CHRF 16.36	ROUGE 8.99
2024-02-02 22:04:18,119 Logging Recognition and Translation Outputs
2024-02-02 22:04:18,119 ========================================================================================================================
2024-02-02 22:04:18,119 Logging Sequence: 87_52.00
2024-02-02 22:04:18,120 	Gloss Reference :	A B+C+D+E
2024-02-02 22:04:18,120 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:04:18,120 	Gloss Alignment :	         
2024-02-02 22:04:18,120 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:04:18,122 	Text Reference  :	that is when gambhir walked into bat and rescued india with his   brilliant 97 runs 
2024-02-02 22:04:18,122 	Text Hypothesis :	**** ** even though  india  has  won the from    the   4th  world cup       in qatar
2024-02-02 22:04:18,122 	Text Alignment  :	D    D  S    S       S      S    S   S   S       S     S    S     S         S  S    
2024-02-02 22:04:18,122 ========================================================================================================================
2024-02-02 22:04:18,122 Logging Sequence: 85_2.00
2024-02-02 22:04:18,123 	Gloss Reference :	A B+C+D+E
2024-02-02 22:04:18,123 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:04:18,123 	Gloss Alignment :	         
2024-02-02 22:04:18,123 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:04:18,124 	Text Reference  :	andrew symonds is one of the finest all rounders in   the    history of  australian cricket       
2024-02-02 22:04:18,124 	Text Hypothesis :	****** ******* ** *** ** *** ****** he  has      also played 198     one day        internationals
2024-02-02 22:04:18,125 	Text Alignment  :	D      D       D  D   D  D   D      S   S        S    S      S       S   S          S             
2024-02-02 22:04:18,125 ========================================================================================================================
2024-02-02 22:04:18,125 Logging Sequence: 51_110.00
2024-02-02 22:04:18,125 	Gloss Reference :	A B+C+D+E
2024-02-02 22:04:18,125 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:04:18,126 	Gloss Alignment :	         
2024-02-02 22:04:18,126 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:04:18,126 	Text Reference  :	the aussies were very happy with their victory
2024-02-02 22:04:18,126 	Text Hypothesis :	*** ******* now  they have  won  the   toss   
2024-02-02 22:04:18,127 	Text Alignment  :	D   D       S    S    S     S    S     S      
2024-02-02 22:04:18,127 ========================================================================================================================
2024-02-02 22:04:18,127 Logging Sequence: 72_59.00
2024-02-02 22:04:18,127 	Gloss Reference :	A B+C+D+E
2024-02-02 22:04:18,127 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:04:18,127 	Gloss Alignment :	         
2024-02-02 22:04:18,128 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:04:18,129 	Text Reference  :	**** *** after that sapna and    shobit started arguing and ******* misbehaving with the       cricketer  
2024-02-02 22:04:18,129 	Text Hypothesis :	this you can   see  these reason for    my      victory and gambhir was         a    brilliant performance
2024-02-02 22:04:18,129 	Text Alignment  :	I    I   S     S    S     S      S      S       S           I       S           S    S         S          
2024-02-02 22:04:18,130 ========================================================================================================================
2024-02-02 22:04:18,130 Logging Sequence: 122_184.00
2024-02-02 22:04:18,130 	Gloss Reference :	A B+C+D+E
2024-02-02 22:04:18,130 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:04:18,130 	Gloss Alignment :	         
2024-02-02 22:04:18,130 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:04:18,132 	Text Reference  :	are playing exceptionally well    and keeping hopes of     further olympic medals alive
2024-02-02 22:04:18,132 	Text Hypothesis :	*** this    is            because gt  topped  the   league stage   with    the    mlc  
2024-02-02 22:04:18,132 	Text Alignment  :	D   S       S             S       S   S       S     S      S       S       S      S    
2024-02-02 22:04:18,132 ========================================================================================================================
2024-02-02 22:04:18,137 Training ended since there were no improvements inthe last learning rate step: 0.000050
2024-02-02 22:04:18,137 Best validation result at step    10000:   1.02 eval_metric.
2024-02-02 22:04:43,587 ------------------------------------------------------------
2024-02-02 22:04:43,587 [DEV] partition [RECOGNITION] experiment [BW]: 1
2024-02-02 22:04:52,423 finished in 8.8363s 
2024-02-02 22:04:52,423 ************************************************************
2024-02-02 22:04:52,423 [DEV] partition [RECOGNITION] results:
	New Best CTC Decode Beam Size: 1
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
2024-02-02 22:04:52,424 ************************************************************
2024-02-02 22:04:52,424 ------------------------------------------------------------
2024-02-02 22:04:52,424 [DEV] partition [RECOGNITION] experiment [BW]: 2
2024-02-02 22:05:00,563 finished in 8.1386s 
2024-02-02 22:05:00,563 ------------------------------------------------------------
2024-02-02 22:05:00,564 [DEV] partition [RECOGNITION] experiment [BW]: 3
2024-02-02 22:05:09,026 finished in 8.4627s 
2024-02-02 22:05:09,026 ------------------------------------------------------------
2024-02-02 22:05:09,027 [DEV] partition [RECOGNITION] experiment [BW]: 4
2024-02-02 22:05:17,414 finished in 8.3868s 
2024-02-02 22:05:17,415 ------------------------------------------------------------
2024-02-02 22:05:17,415 [DEV] partition [RECOGNITION] experiment [BW]: 5
2024-02-02 22:05:25,424 finished in 8.0093s 
2024-02-02 22:05:25,424 ------------------------------------------------------------
2024-02-02 22:05:25,425 [DEV] partition [RECOGNITION] experiment [BW]: 6
2024-02-02 22:05:33,731 finished in 8.3057s 
2024-02-02 22:05:33,732 ------------------------------------------------------------
2024-02-02 22:05:33,732 [DEV] partition [RECOGNITION] experiment [BW]: 7
2024-02-02 22:05:42,058 finished in 8.3260s 
2024-02-02 22:05:42,058 ------------------------------------------------------------
2024-02-02 22:05:42,058 [DEV] partition [RECOGNITION] experiment [BW]: 8
2024-02-02 22:05:50,507 finished in 8.4486s 
2024-02-02 22:05:50,507 ------------------------------------------------------------
2024-02-02 22:05:50,508 [DEV] partition [RECOGNITION] experiment [BW]: 9
2024-02-02 22:05:58,713 finished in 8.2049s 
2024-02-02 22:05:58,714 ------------------------------------------------------------
2024-02-02 22:05:58,714 [DEV] partition [RECOGNITION] experiment [BW]: 10
2024-02-02 22:06:06,990 finished in 8.2757s 
2024-02-02 22:06:06,990 ============================================================
2024-02-02 22:06:15,350 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 1 and Alpha: -1
	BLEU-4 1.02	(BLEU-1: 11.89,	BLEU-2: 4.00,	BLEU-3: 1.85,	BLEU-4: 1.02)
	CHRF 17.47	ROUGE 10.05
2024-02-02 22:06:15,350 ------------------------------------------------------------
2024-02-02 22:08:26,345 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 3 and Alpha: -1
	BLEU-4 1.02	(BLEU-1: 10.49,	BLEU-2: 3.73,	BLEU-3: 1.79,	BLEU-4: 1.02)
	CHRF 16.71	ROUGE 9.48
2024-02-02 22:08:26,346 ------------------------------------------------------------
2024-02-02 22:08:48,331 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 3 and Alpha: 1
	BLEU-4 1.14	(BLEU-1: 11.20,	BLEU-2: 3.95,	BLEU-3: 1.95,	BLEU-4: 1.14)
	CHRF 17.02	ROUGE 9.80
2024-02-02 22:08:48,332 ------------------------------------------------------------
2024-02-02 22:08:59,264 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 3 and Alpha: 2
	BLEU-4 1.15	(BLEU-1: 11.55,	BLEU-2: 4.06,	BLEU-3: 1.98,	BLEU-4: 1.15)
	CHRF 17.19	ROUGE 9.89
2024-02-02 22:08:59,265 ------------------------------------------------------------
2024-02-02 22:09:21,687 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 3 and Alpha: 4
	BLEU-4 1.15	(BLEU-1: 11.65,	BLEU-2: 4.07,	BLEU-3: 1.99,	BLEU-4: 1.15)
	CHRF 17.21	ROUGE 9.93
2024-02-02 22:09:21,687 ------------------------------------------------------------
2024-02-02 22:10:08,982 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 4 and Alpha: 1
	BLEU-4 1.16	(BLEU-1: 10.83,	BLEU-2: 3.96,	BLEU-3: 1.99,	BLEU-4: 1.16)
	CHRF 16.89	ROUGE 9.60
2024-02-02 22:10:08,983 ------------------------------------------------------------
2024-02-02 22:10:21,090 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 4 and Alpha: 2
	BLEU-4 1.17	(BLEU-1: 11.31,	BLEU-2: 4.07,	BLEU-3: 2.02,	BLEU-4: 1.17)
	CHRF 17.19	ROUGE 9.73
2024-02-02 22:10:21,091 ------------------------------------------------------------
2024-02-02 22:10:33,387 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 4 and Alpha: 3
	BLEU-4 1.17	(BLEU-1: 11.38,	BLEU-2: 4.09,	BLEU-3: 2.03,	BLEU-4: 1.17)
	CHRF 17.20	ROUGE 9.73
2024-02-02 22:10:33,388 ------------------------------------------------------------
2024-02-02 22:11:50,060 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 5 and Alpha: 2
	BLEU-4 1.18	(BLEU-1: 11.11,	BLEU-2: 4.09,	BLEU-3: 2.05,	BLEU-4: 1.18)
	CHRF 17.11	ROUGE 9.66
2024-02-02 22:11:50,060 ------------------------------------------------------------
2024-02-02 22:13:12,775 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 6 and Alpha: 1
	BLEU-4 1.19	(BLEU-1: 10.52,	BLEU-2: 3.97,	BLEU-3: 2.01,	BLEU-4: 1.19)
	CHRF 16.79	ROUGE 9.41
2024-02-02 22:13:12,775 ------------------------------------------------------------
2024-02-02 22:13:27,352 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 6 and Alpha: 2
	BLEU-4 1.22	(BLEU-1: 10.99,	BLEU-2: 4.13,	BLEU-3: 2.09,	BLEU-4: 1.22)
	CHRF 17.03	ROUGE 9.60
2024-02-02 22:13:27,353 ------------------------------------------------------------
2024-02-02 22:13:41,526 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 6 and Alpha: 3
	BLEU-4 1.23	(BLEU-1: 11.09,	BLEU-2: 4.20,	BLEU-3: 2.11,	BLEU-4: 1.23)
	CHRF 17.07	ROUGE 9.61
2024-02-02 22:13:41,526 ------------------------------------------------------------
2024-02-02 22:19:07,630 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 9 and Alpha: 2
	BLEU-4 1.23	(BLEU-1: 10.86,	BLEU-2: 4.06,	BLEU-3: 2.08,	BLEU-4: 1.23)
	CHRF 16.99	ROUGE 9.38
2024-02-02 22:19:07,630 ------------------------------------------------------------
2024-02-02 22:21:16,976 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 10 and Alpha: 2
	BLEU-4 1.32	(BLEU-1: 10.89,	BLEU-2: 4.17,	BLEU-3: 2.18,	BLEU-4: 1.32)
	CHRF 17.04	ROUGE 9.39
2024-02-02 22:21:16,977 ------------------------------------------------------------
2024-02-02 22:22:13,738 ************************************************************
2024-02-02 22:22:13,739 [DEV] partition [Recognition & Translation] results:
	Best CTC Decode Beam Size: 1
	Best Translation Beam Size: 10 and Alpha: 2
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 1.32	(BLEU-1: 10.89,	BLEU-2: 4.17,	BLEU-3: 2.18,	BLEU-4: 1.32)
	CHRF 17.04	ROUGE 9.39
2024-02-02 22:22:13,739 ************************************************************
2024-02-02 22:22:32,525 [TEST] partition [Recognition & Translation] results:
	Best CTC Decode Beam Size: 1
	Best Translation Beam Size: 10 and Alpha: 2
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.93	(BLEU-1: 10.25,	BLEU-2: 3.51,	BLEU-3: 1.67,	BLEU-4: 0.93)
	CHRF 16.63	ROUGE 8.74
2024-02-02 22:22:32,525 ************************************************************
