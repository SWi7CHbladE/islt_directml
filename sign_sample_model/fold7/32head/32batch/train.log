2024-02-08 14:41:24,207 Hello! This is Joey-NMT.
2024-02-08 14:41:24,217 Total params: 25642504
2024-02-08 14:41:24,218 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'gloss_output_layer.bias', 'gloss_output_layer.weight', 'sgn_embed.ln.bias', 'sgn_embed.ln.weight', 'sgn_embed.norm.norm.bias', 'sgn_embed.norm.norm.weight', 'txt_embed.norm.norm.bias', 'txt_embed.norm.norm.weight']
2024-02-08 14:41:25,401 cfg.name                           : sign_experiment
2024-02-08 14:41:25,402 cfg.data.data_path                 : ./data/Sports_dataset/7/
2024-02-08 14:41:25,402 cfg.data.version                   : phoenix_2014_trans
2024-02-08 14:41:25,402 cfg.data.sgn                       : sign
2024-02-08 14:41:25,402 cfg.data.txt                       : text
2024-02-08 14:41:25,402 cfg.data.gls                       : gloss
2024-02-08 14:41:25,402 cfg.data.train                     : excel_data.train
2024-02-08 14:41:25,403 cfg.data.dev                       : excel_data.dev
2024-02-08 14:41:25,403 cfg.data.test                      : excel_data.test
2024-02-08 14:41:25,403 cfg.data.feature_size              : 2560
2024-02-08 14:41:25,403 cfg.data.level                     : word
2024-02-08 14:41:25,403 cfg.data.txt_lowercase             : True
2024-02-08 14:41:25,403 cfg.data.max_sent_length           : 500
2024-02-08 14:41:25,403 cfg.data.random_train_subset       : -1
2024-02-08 14:41:25,404 cfg.data.random_dev_subset         : -1
2024-02-08 14:41:25,404 cfg.testing.recognition_beam_sizes : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
2024-02-08 14:41:25,404 cfg.testing.translation_beam_sizes : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
2024-02-08 14:41:25,405 cfg.testing.translation_beam_alphas : [-1, 0, 1, 2, 3, 4, 5]
2024-02-08 14:41:25,405 cfg.training.reset_best_ckpt       : False
2024-02-08 14:41:25,405 cfg.training.reset_scheduler       : False
2024-02-08 14:41:25,405 cfg.training.reset_optimizer       : False
2024-02-08 14:41:25,405 cfg.training.random_seed           : 42
2024-02-08 14:41:25,405 cfg.training.model_dir             : ./sign_sample_model/fold7/32head/32batch
2024-02-08 14:41:25,405 cfg.training.recognition_loss_weight : 1.0
2024-02-08 14:41:25,405 cfg.training.translation_loss_weight : 1.0
2024-02-08 14:41:25,406 cfg.training.eval_metric           : bleu
2024-02-08 14:41:25,406 cfg.training.optimizer             : adam
2024-02-08 14:41:25,406 cfg.training.learning_rate         : 0.0001
2024-02-08 14:41:25,406 cfg.training.batch_size            : 32
2024-02-08 14:41:25,406 cfg.training.num_valid_log         : 5
2024-02-08 14:41:25,406 cfg.training.epochs                : 50000
2024-02-08 14:41:25,406 cfg.training.early_stopping_metric : eval_metric
2024-02-08 14:41:25,406 cfg.training.batch_type            : sentence
2024-02-08 14:41:25,407 cfg.training.translation_normalization : batch
2024-02-08 14:41:25,407 cfg.training.eval_recognition_beam_size : 1
2024-02-08 14:41:25,407 cfg.training.eval_translation_beam_size : 1
2024-02-08 14:41:25,407 cfg.training.eval_translation_beam_alpha : -1
2024-02-08 14:41:25,407 cfg.training.overwrite             : True
2024-02-08 14:41:25,407 cfg.training.shuffle               : True
2024-02-08 14:41:25,407 cfg.training.use_cuda              : True
2024-02-08 14:41:25,408 cfg.training.translation_max_output_length : 40
2024-02-08 14:41:25,408 cfg.training.keep_last_ckpts       : 1
2024-02-08 14:41:25,408 cfg.training.batch_multiplier      : 1
2024-02-08 14:41:25,408 cfg.training.logging_freq          : 100
2024-02-08 14:41:25,408 cfg.training.validation_freq       : 2000
2024-02-08 14:41:25,408 cfg.training.betas                 : [0.9, 0.998]
2024-02-08 14:41:25,408 cfg.training.scheduling            : plateau
2024-02-08 14:41:25,409 cfg.training.learning_rate_min     : 1e-08
2024-02-08 14:41:25,409 cfg.training.weight_decay          : 0.0001
2024-02-08 14:41:25,409 cfg.training.patience              : 12
2024-02-08 14:41:25,409 cfg.training.decrease_factor       : 0.5
2024-02-08 14:41:25,409 cfg.training.label_smoothing       : 0.0
2024-02-08 14:41:25,409 cfg.model.initializer              : xavier
2024-02-08 14:41:25,409 cfg.model.bias_initializer         : zeros
2024-02-08 14:41:25,409 cfg.model.init_gain                : 1.0
2024-02-08 14:41:25,410 cfg.model.embed_initializer        : xavier
2024-02-08 14:41:25,410 cfg.model.embed_init_gain          : 1.0
2024-02-08 14:41:25,410 cfg.model.tied_softmax             : True
2024-02-08 14:41:25,410 cfg.model.encoder.type             : transformer
2024-02-08 14:41:25,410 cfg.model.encoder.num_layers       : 3
2024-02-08 14:41:25,410 cfg.model.encoder.num_heads        : 32
2024-02-08 14:41:25,410 cfg.model.encoder.embeddings.embedding_dim : 512
2024-02-08 14:41:25,410 cfg.model.encoder.embeddings.scale : False
2024-02-08 14:41:25,411 cfg.model.encoder.embeddings.dropout : 0.1
2024-02-08 14:41:25,411 cfg.model.encoder.embeddings.norm_type : batch
2024-02-08 14:41:25,411 cfg.model.encoder.embeddings.activation_type : softsign
2024-02-08 14:41:25,411 cfg.model.encoder.hidden_size      : 512
2024-02-08 14:41:25,411 cfg.model.encoder.ff_size          : 2048
2024-02-08 14:41:25,411 cfg.model.encoder.dropout          : 0.1
2024-02-08 14:41:25,411 cfg.model.decoder.type             : transformer
2024-02-08 14:41:25,411 cfg.model.decoder.num_layers       : 3
2024-02-08 14:41:25,412 cfg.model.decoder.num_heads        : 32
2024-02-08 14:41:25,412 cfg.model.decoder.embeddings.embedding_dim : 512
2024-02-08 14:41:25,412 cfg.model.decoder.embeddings.scale : False
2024-02-08 14:41:25,412 cfg.model.decoder.embeddings.dropout : 0.1
2024-02-08 14:41:25,412 cfg.model.decoder.embeddings.norm_type : batch
2024-02-08 14:41:25,412 cfg.model.decoder.embeddings.activation_type : softsign
2024-02-08 14:41:25,412 cfg.model.decoder.hidden_size      : 512
2024-02-08 14:41:25,412 cfg.model.decoder.ff_size          : 2048
2024-02-08 14:41:25,413 cfg.model.decoder.dropout          : 0.1
2024-02-08 14:41:25,413 Data set sizes: 
	train 2124,
	valid 708,
	test 708
2024-02-08 14:41:25,413 First training example:
	[GLS] A B C D E
	[TXT] how did she become a champion
2024-02-08 14:41:25,413 First 10 words (gls): (0) <si> (1) <unk> (2) <pad> (3) A (4) B (5) C (6) D (7) E
2024-02-08 14:41:25,413 First 10 words (txt): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) the (5) and (6) to (7) in (8) a (9) of
2024-02-08 14:41:25,413 Number of unique glosses (types): 8
2024-02-08 14:41:25,413 Number of unique words (types): 4402
2024-02-08 14:41:25,414 SignModel(
	encoder=TransformerEncoder(num_layers=3, num_heads=32),
	decoder=TransformerDecoder(num_layers=3, num_heads=32),
	sgn_embed=SpatialEmbeddings(embedding_dim=512, input_size=2560),
	txt_embed=Embeddings(embedding_dim=512, vocab_size=4402))
2024-02-08 14:41:25,417 EPOCH 1
2024-02-08 14:41:32,657 Epoch   1: Total Training Recognition Loss 279.21  Total Training Translation Loss 6501.15 
2024-02-08 14:41:32,657 EPOCH 2
2024-02-08 14:41:36,078 [Epoch: 002 Step: 00000100] Batch Recognition Loss:   1.118769 => Gls Tokens per Sec:     1545 || Batch Translation Loss:  85.757042 => Txt Tokens per Sec:     4344 || Lr: 0.000100
2024-02-08 14:41:39,134 Epoch   2: Total Training Recognition Loss 74.75  Total Training Translation Loss 5941.06 
2024-02-08 14:41:39,134 EPOCH 3
2024-02-08 14:41:45,513 [Epoch: 003 Step: 00000200] Batch Recognition Loss:   0.930459 => Gls Tokens per Sec:     1640 || Batch Translation Loss:  65.551003 => Txt Tokens per Sec:     4513 || Lr: 0.000100
2024-02-08 14:41:45,805 Epoch   3: Total Training Recognition Loss 52.89  Total Training Translation Loss 5839.00 
2024-02-08 14:41:45,805 EPOCH 4
2024-02-08 14:41:51,858 Epoch   4: Total Training Recognition Loss 64.26  Total Training Translation Loss 5675.44 
2024-02-08 14:41:51,859 EPOCH 5
2024-02-08 14:41:54,888 [Epoch: 005 Step: 00000300] Batch Recognition Loss:   1.614083 => Gls Tokens per Sec:     1691 || Batch Translation Loss: 100.207764 => Txt Tokens per Sec:     4469 || Lr: 0.000100
2024-02-08 14:41:58,499 Epoch   5: Total Training Recognition Loss 65.48  Total Training Translation Loss 5456.45 
2024-02-08 14:41:58,499 EPOCH 6
2024-02-08 14:42:04,545 [Epoch: 006 Step: 00000400] Batch Recognition Loss:   0.844744 => Gls Tokens per Sec:     1704 || Batch Translation Loss:  97.861549 => Txt Tokens per Sec:     4704 || Lr: 0.000100
2024-02-08 14:42:04,804 Epoch   6: Total Training Recognition Loss 34.28  Total Training Translation Loss 5236.36 
2024-02-08 14:42:04,804 EPOCH 7
2024-02-08 14:42:11,467 Epoch   7: Total Training Recognition Loss 25.09  Total Training Translation Loss 5031.40 
2024-02-08 14:42:11,468 EPOCH 8
2024-02-08 14:42:14,543 [Epoch: 008 Step: 00000500] Batch Recognition Loss:   0.210097 => Gls Tokens per Sec:     1582 || Batch Translation Loss:  50.569569 => Txt Tokens per Sec:     4358 || Lr: 0.000100
2024-02-08 14:42:18,074 Epoch   8: Total Training Recognition Loss 20.08  Total Training Translation Loss 4825.82 
2024-02-08 14:42:18,075 EPOCH 9
2024-02-08 14:42:24,714 [Epoch: 009 Step: 00000600] Batch Recognition Loss:   0.230150 => Gls Tokens per Sec:     1527 || Batch Translation Loss:  86.013443 => Txt Tokens per Sec:     4222 || Lr: 0.000100
2024-02-08 14:42:24,943 Epoch   9: Total Training Recognition Loss 18.52  Total Training Translation Loss 4639.22 
2024-02-08 14:42:24,943 EPOCH 10
2024-02-08 14:42:31,311 Epoch  10: Total Training Recognition Loss 13.48  Total Training Translation Loss 4454.24 
2024-02-08 14:42:31,311 EPOCH 11
2024-02-08 14:42:33,785 [Epoch: 011 Step: 00000700] Batch Recognition Loss:   0.176469 => Gls Tokens per Sec:     1941 || Batch Translation Loss:  41.837822 => Txt Tokens per Sec:     5231 || Lr: 0.000100
2024-02-08 14:42:37,510 Epoch  11: Total Training Recognition Loss 11.46  Total Training Translation Loss 4281.37 
2024-02-08 14:42:37,511 EPOCH 12
2024-02-08 14:42:43,483 [Epoch: 012 Step: 00000800] Batch Recognition Loss:   0.148978 => Gls Tokens per Sec:     1672 || Batch Translation Loss:  69.092438 => Txt Tokens per Sec:     4631 || Lr: 0.000100
2024-02-08 14:42:43,837 Epoch  12: Total Training Recognition Loss 9.95  Total Training Translation Loss 4116.50 
2024-02-08 14:42:43,837 EPOCH 13
2024-02-08 14:42:50,429 Epoch  13: Total Training Recognition Loss 8.89  Total Training Translation Loss 3945.49 
2024-02-08 14:42:50,430 EPOCH 14
2024-02-08 14:42:53,460 [Epoch: 014 Step: 00000900] Batch Recognition Loss:   0.109302 => Gls Tokens per Sec:     1532 || Batch Translation Loss:  61.687340 => Txt Tokens per Sec:     4445 || Lr: 0.000100
2024-02-08 14:42:56,771 Epoch  14: Total Training Recognition Loss 7.96  Total Training Translation Loss 3782.52 
2024-02-08 14:42:56,772 EPOCH 15
2024-02-08 14:43:03,254 [Epoch: 015 Step: 00001000] Batch Recognition Loss:   0.016024 => Gls Tokens per Sec:     1515 || Batch Translation Loss:  54.141323 => Txt Tokens per Sec:     4182 || Lr: 0.000100
2024-02-08 14:43:03,771 Epoch  15: Total Training Recognition Loss 7.49  Total Training Translation Loss 3641.68 
2024-02-08 14:43:03,771 EPOCH 16
2024-02-08 14:43:10,071 Epoch  16: Total Training Recognition Loss 7.14  Total Training Translation Loss 3508.32 
2024-02-08 14:43:10,071 EPOCH 17
2024-02-08 14:43:12,773 [Epoch: 017 Step: 00001100] Batch Recognition Loss:   0.108937 => Gls Tokens per Sec:     1622 || Batch Translation Loss:  40.103542 => Txt Tokens per Sec:     4323 || Lr: 0.000100
2024-02-08 14:43:16,749 Epoch  17: Total Training Recognition Loss 6.81  Total Training Translation Loss 3353.96 
2024-02-08 14:43:16,749 EPOCH 18
2024-02-08 14:43:22,418 [Epoch: 018 Step: 00001200] Batch Recognition Loss:   0.124655 => Gls Tokens per Sec:     1704 || Batch Translation Loss:  45.735218 => Txt Tokens per Sec:     4681 || Lr: 0.000100
2024-02-08 14:43:23,242 Epoch  18: Total Training Recognition Loss 6.11  Total Training Translation Loss 3195.85 
2024-02-08 14:43:23,242 EPOCH 19
2024-02-08 14:43:29,773 Epoch  19: Total Training Recognition Loss 5.75  Total Training Translation Loss 3035.82 
2024-02-08 14:43:29,774 EPOCH 20
2024-02-08 14:43:32,437 [Epoch: 020 Step: 00001300] Batch Recognition Loss:   0.036419 => Gls Tokens per Sec:     1623 || Batch Translation Loss:  40.125336 => Txt Tokens per Sec:     4360 || Lr: 0.000100
2024-02-08 14:43:36,650 Epoch  20: Total Training Recognition Loss 5.62  Total Training Translation Loss 2908.95 
2024-02-08 14:43:36,651 EPOCH 21
2024-02-08 14:43:42,709 [Epoch: 021 Step: 00001400] Batch Recognition Loss:   0.030549 => Gls Tokens per Sec:     1568 || Batch Translation Loss:  38.219257 => Txt Tokens per Sec:     4277 || Lr: 0.000100
2024-02-08 14:43:43,576 Epoch  21: Total Training Recognition Loss 5.48  Total Training Translation Loss 2775.29 
2024-02-08 14:43:43,576 EPOCH 22
2024-02-08 14:43:49,919 Epoch  22: Total Training Recognition Loss 5.21  Total Training Translation Loss 2634.03 
2024-02-08 14:43:49,920 EPOCH 23
2024-02-08 14:43:52,459 [Epoch: 023 Step: 00001500] Batch Recognition Loss:   0.020480 => Gls Tokens per Sec:     1639 || Batch Translation Loss:  37.200581 => Txt Tokens per Sec:     4433 || Lr: 0.000100
2024-02-08 14:43:56,686 Epoch  23: Total Training Recognition Loss 5.69  Total Training Translation Loss 2511.61 
2024-02-08 14:43:56,687 EPOCH 24
2024-02-08 14:44:02,298 [Epoch: 024 Step: 00001600] Batch Recognition Loss:   0.111751 => Gls Tokens per Sec:     1665 || Batch Translation Loss:  44.312504 => Txt Tokens per Sec:     4599 || Lr: 0.000100
2024-02-08 14:44:03,323 Epoch  24: Total Training Recognition Loss 5.29  Total Training Translation Loss 2372.49 
2024-02-08 14:44:03,324 EPOCH 25
2024-02-08 14:44:09,641 Epoch  25: Total Training Recognition Loss 5.01  Total Training Translation Loss 2231.07 
2024-02-08 14:44:09,641 EPOCH 26
2024-02-08 14:44:11,423 [Epoch: 026 Step: 00001700] Batch Recognition Loss:   0.064942 => Gls Tokens per Sec:     2247 || Batch Translation Loss:  43.196701 => Txt Tokens per Sec:     6075 || Lr: 0.000100
2024-02-08 14:44:14,744 Epoch  26: Total Training Recognition Loss 5.11  Total Training Translation Loss 2102.56 
2024-02-08 14:44:14,745 EPOCH 27
2024-02-08 14:44:19,534 [Epoch: 027 Step: 00001800] Batch Recognition Loss:   0.098922 => Gls Tokens per Sec:     1938 || Batch Translation Loss:  10.862187 => Txt Tokens per Sec:     5419 || Lr: 0.000100
2024-02-08 14:44:20,244 Epoch  27: Total Training Recognition Loss 4.81  Total Training Translation Loss 1987.62 
2024-02-08 14:44:20,244 EPOCH 28
2024-02-08 14:44:25,416 Epoch  28: Total Training Recognition Loss 4.76  Total Training Translation Loss 1874.09 
2024-02-08 14:44:25,418 EPOCH 29
2024-02-08 14:44:27,371 [Epoch: 029 Step: 00001900] Batch Recognition Loss:   0.054694 => Gls Tokens per Sec:     1915 || Batch Translation Loss:  23.445967 => Txt Tokens per Sec:     5481 || Lr: 0.000100
2024-02-08 14:44:30,662 Epoch  29: Total Training Recognition Loss 4.71  Total Training Translation Loss 1742.92 
2024-02-08 14:44:30,662 EPOCH 30
2024-02-08 14:44:35,040 [Epoch: 030 Step: 00002000] Batch Recognition Loss:   0.044610 => Gls Tokens per Sec:     2061 || Batch Translation Loss:  18.157986 => Txt Tokens per Sec:     5671 || Lr: 0.000100
2024-02-08 14:44:44,094 Hooray! New best validation result [eval_metric]!
2024-02-08 14:44:44,094 Saving new checkpoint.
2024-02-08 14:44:44,341 Validation result at epoch  30, step     2000: duration: 9.3008s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 3.12702	Translation Loss: 61835.24219	PPL: 481.11807
	Eval Metric: BLEU
	WER 6.57	(DEL: 0.00,	INS: 0.00,	SUB: 6.57)
	BLEU-4 0.87	(BLEU-1: 12.89,	BLEU-2: 4.57,	BLEU-3: 1.82,	BLEU-4: 0.87)
	CHRF 16.74	ROUGE 10.80
2024-02-08 14:44:44,341 Logging Recognition and Translation Outputs
2024-02-08 14:44:44,342 ========================================================================================================================
2024-02-08 14:44:44,342 Logging Sequence: 165_414.00
2024-02-08 14:44:44,342 	Gloss Reference :	A B+C+D+E
2024-02-08 14:44:44,342 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 14:44:44,342 	Gloss Alignment :	         
2024-02-08 14:44:44,342 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 14:44:44,344 	Text Reference  :	he felt sachin was  lucky so   he always gave his  sweater to give   it to  the umpire       
2024-02-08 14:44:44,344 	Text Hypothesis :	** the  ipl    will be    held in ipl    but  they have    a  person on his own superstitions
2024-02-08 14:44:44,344 	Text Alignment  :	D  S    S      S    S     S    S  S      S    S    S       S  S      S  S   S   S            
2024-02-08 14:44:44,344 ========================================================================================================================
2024-02-08 14:44:44,344 Logging Sequence: 169_268.00
2024-02-08 14:44:44,345 	Gloss Reference :	A B+C+D+E
2024-02-08 14:44:44,345 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 14:44:44,345 	Gloss Alignment :	         
2024-02-08 14:44:44,345 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 14:44:44,346 	Text Reference  :	**** ** *** **** **** shami supports arshdeep and many fans supported him  as  well     
2024-02-08 14:44:44,346 	Text Hypothesis :	this is why they were seen  seen     smashing the ball for  a         huge fan following
2024-02-08 14:44:44,346 	Text Alignment  :	I    I  I   I    I    S     S        S        S   S    S    S         S    S   S        
2024-02-08 14:44:44,346 ========================================================================================================================
2024-02-08 14:44:44,346 Logging Sequence: 172_15.00
2024-02-08 14:44:44,347 	Gloss Reference :	A B+C+D+E
2024-02-08 14:44:44,347 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 14:44:44,347 	Gloss Alignment :	         
2024-02-08 14:44:44,347 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 14:44:44,349 	Text Reference  :	now in the final match   on   28   may  2023   the two    teams  were up against each other at         the ********** same venue
2024-02-08 14:44:44,349 	Text Hypothesis :	*** ** *** ***** however they have been giving the golden golden boot as well    with the   tournament the tournament and  cafes
2024-02-08 14:44:44,349 	Text Alignment  :	D   D  D   D     S       S    S    S    S          S      S      S    S  S       S    S     S              I          S    S    
2024-02-08 14:44:44,350 ========================================================================================================================
2024-02-08 14:44:44,350 Logging Sequence: 96_158.00
2024-02-08 14:44:44,350 	Gloss Reference :	A B+C+D+E
2024-02-08 14:44:44,350 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 14:44:44,350 	Gloss Alignment :	         
2024-02-08 14:44:44,350 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 14:44:44,351 	Text Reference  :	*** ****** **** **** after   this pandya fell on  his knees in   disappointment
2024-02-08 14:44:44,351 	Text Hypothesis :	the couple were very worried and  want   to   see the next  next wicket        
2024-02-08 14:44:44,351 	Text Alignment  :	I   I      I    I    S       S    S      S    S   S   S     S    S             
2024-02-08 14:44:44,351 ========================================================================================================================
2024-02-08 14:44:44,352 Logging Sequence: 152_73.00
2024-02-08 14:44:44,352 	Gloss Reference :	A B+C+D+E
2024-02-08 14:44:44,352 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 14:44:44,352 	Gloss Alignment :	         
2024-02-08 14:44:44,352 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 14:44:44,353 	Text Reference  :	******* **** *** ***** *** **** eventually he  too  got out      by       shaheen  afridi
2024-02-08 14:44:44,353 	Text Hypothesis :	however when the match was held in         uae with his casteist casteist casteist slurts
2024-02-08 14:44:44,353 	Text Alignment  :	I       I    I   I     I   I    S          S   S    S   S        S        S        S     
2024-02-08 14:44:44,353 ========================================================================================================================
2024-02-08 14:44:45,189 Epoch  30: Total Training Recognition Loss 4.71  Total Training Translation Loss 1631.70 
2024-02-08 14:44:45,190 EPOCH 31
2024-02-08 14:44:51,027 Epoch  31: Total Training Recognition Loss 4.61  Total Training Translation Loss 1516.09 
2024-02-08 14:44:51,028 EPOCH 32
2024-02-08 14:44:52,909 [Epoch: 032 Step: 00002100] Batch Recognition Loss:   0.078977 => Gls Tokens per Sec:     1904 || Batch Translation Loss:  28.626492 => Txt Tokens per Sec:     5232 || Lr: 0.000100
2024-02-08 14:44:56,086 Epoch  32: Total Training Recognition Loss 4.47  Total Training Translation Loss 1411.37 
2024-02-08 14:44:56,087 EPOCH 33
2024-02-08 14:45:00,624 [Epoch: 033 Step: 00002200] Batch Recognition Loss:   0.082587 => Gls Tokens per Sec:     1953 || Batch Translation Loss:  19.673037 => Txt Tokens per Sec:     5275 || Lr: 0.000100
2024-02-08 14:45:01,636 Epoch  33: Total Training Recognition Loss 4.63  Total Training Translation Loss 1318.21 
2024-02-08 14:45:01,636 EPOCH 34
2024-02-08 14:45:06,578 Epoch  34: Total Training Recognition Loss 4.50  Total Training Translation Loss 1235.58 
2024-02-08 14:45:06,578 EPOCH 35
2024-02-08 14:45:08,625 [Epoch: 035 Step: 00002300] Batch Recognition Loss:   0.040313 => Gls Tokens per Sec:     1672 || Batch Translation Loss:  20.227282 => Txt Tokens per Sec:     4740 || Lr: 0.000100
2024-02-08 14:45:12,220 Epoch  35: Total Training Recognition Loss 4.33  Total Training Translation Loss 1118.70 
2024-02-08 14:45:12,221 EPOCH 36
2024-02-08 14:45:16,110 [Epoch: 036 Step: 00002400] Batch Recognition Loss:   0.104923 => Gls Tokens per Sec:     2238 || Batch Translation Loss:   6.259461 => Txt Tokens per Sec:     6115 || Lr: 0.000100
2024-02-08 14:45:17,396 Epoch  36: Total Training Recognition Loss 3.95  Total Training Translation Loss 1023.07 
2024-02-08 14:45:17,396 EPOCH 37
2024-02-08 14:45:22,930 Epoch  37: Total Training Recognition Loss 3.86  Total Training Translation Loss 948.39 
2024-02-08 14:45:22,931 EPOCH 38
2024-02-08 14:45:24,271 [Epoch: 038 Step: 00002500] Batch Recognition Loss:   0.074262 => Gls Tokens per Sec:     2510 || Batch Translation Loss:  14.245550 => Txt Tokens per Sec:     6892 || Lr: 0.000100
2024-02-08 14:45:27,916 Epoch  38: Total Training Recognition Loss 3.70  Total Training Translation Loss 881.19 
2024-02-08 14:45:27,917 EPOCH 39
2024-02-08 14:45:32,371 [Epoch: 039 Step: 00002600] Batch Recognition Loss:   0.029235 => Gls Tokens per Sec:     1918 || Batch Translation Loss:   9.800572 => Txt Tokens per Sec:     5328 || Lr: 0.000100
2024-02-08 14:45:33,527 Epoch  39: Total Training Recognition Loss 3.85  Total Training Translation Loss 803.40 
2024-02-08 14:45:33,527 EPOCH 40
2024-02-08 14:45:38,934 Epoch  40: Total Training Recognition Loss 3.75  Total Training Translation Loss 729.16 
2024-02-08 14:45:38,935 EPOCH 41
2024-02-08 14:45:40,596 [Epoch: 041 Step: 00002700] Batch Recognition Loss:   0.049761 => Gls Tokens per Sec:     1928 || Batch Translation Loss:  11.736375 => Txt Tokens per Sec:     5388 || Lr: 0.000100
2024-02-08 14:45:44,195 Epoch  41: Total Training Recognition Loss 3.77  Total Training Translation Loss 663.04 
2024-02-08 14:45:44,195 EPOCH 42
2024-02-08 14:45:47,989 [Epoch: 042 Step: 00002800] Batch Recognition Loss:   0.032361 => Gls Tokens per Sec:     2210 || Batch Translation Loss:   8.131509 => Txt Tokens per Sec:     5912 || Lr: 0.000100
2024-02-08 14:45:49,544 Epoch  42: Total Training Recognition Loss 3.31  Total Training Translation Loss 594.55 
2024-02-08 14:45:49,545 EPOCH 43
2024-02-08 14:45:54,864 Epoch  43: Total Training Recognition Loss 3.29  Total Training Translation Loss 544.43 
2024-02-08 14:45:54,864 EPOCH 44
2024-02-08 14:45:56,186 [Epoch: 044 Step: 00002900] Batch Recognition Loss:   0.017377 => Gls Tokens per Sec:     2302 || Batch Translation Loss:   6.639381 => Txt Tokens per Sec:     6293 || Lr: 0.000100
2024-02-08 14:46:00,280 Epoch  44: Total Training Recognition Loss 3.14  Total Training Translation Loss 487.53 
2024-02-08 14:46:00,281 EPOCH 45
2024-02-08 14:46:04,224 [Epoch: 045 Step: 00003000] Batch Recognition Loss:   0.034035 => Gls Tokens per Sec:     2085 || Batch Translation Loss:   9.108966 => Txt Tokens per Sec:     5690 || Lr: 0.000100
2024-02-08 14:46:05,562 Epoch  45: Total Training Recognition Loss 3.04  Total Training Translation Loss 448.26 
2024-02-08 14:46:05,563 EPOCH 46
2024-02-08 14:46:10,962 Epoch  46: Total Training Recognition Loss 2.87  Total Training Translation Loss 406.84 
2024-02-08 14:46:10,963 EPOCH 47
2024-02-08 14:46:12,422 [Epoch: 047 Step: 00003100] Batch Recognition Loss:   0.043153 => Gls Tokens per Sec:     1975 || Batch Translation Loss:   6.039704 => Txt Tokens per Sec:     5445 || Lr: 0.000100
2024-02-08 14:46:16,541 Epoch  47: Total Training Recognition Loss 3.01  Total Training Translation Loss 369.10 
2024-02-08 14:46:16,542 EPOCH 48
2024-02-08 14:46:21,025 [Epoch: 048 Step: 00003200] Batch Recognition Loss:   0.022557 => Gls Tokens per Sec:     1799 || Batch Translation Loss:   5.704175 => Txt Tokens per Sec:     5063 || Lr: 0.000100
2024-02-08 14:46:22,218 Epoch  48: Total Training Recognition Loss 2.68  Total Training Translation Loss 328.45 
2024-02-08 14:46:22,218 EPOCH 49
2024-02-08 14:46:27,430 Epoch  49: Total Training Recognition Loss 2.54  Total Training Translation Loss 296.78 
2024-02-08 14:46:27,431 EPOCH 50
2024-02-08 14:46:28,784 [Epoch: 050 Step: 00003300] Batch Recognition Loss:   0.014322 => Gls Tokens per Sec:     2014 || Batch Translation Loss:   3.648035 => Txt Tokens per Sec:     5546 || Lr: 0.000100
2024-02-08 14:46:33,061 Epoch  50: Total Training Recognition Loss 2.60  Total Training Translation Loss 275.83 
2024-02-08 14:46:33,061 EPOCH 51
2024-02-08 14:46:37,201 [Epoch: 051 Step: 00003400] Batch Recognition Loss:   0.022410 => Gls Tokens per Sec:     1909 || Batch Translation Loss:   4.052798 => Txt Tokens per Sec:     5293 || Lr: 0.000100
2024-02-08 14:46:38,632 Epoch  51: Total Training Recognition Loss 2.28  Total Training Translation Loss 251.91 
2024-02-08 14:46:38,633 EPOCH 52
2024-02-08 14:46:44,537 Epoch  52: Total Training Recognition Loss 2.27  Total Training Translation Loss 234.78 
2024-02-08 14:46:44,537 EPOCH 53
2024-02-08 14:46:45,971 [Epoch: 053 Step: 00003500] Batch Recognition Loss:   0.025839 => Gls Tokens per Sec:     1787 || Batch Translation Loss:   2.962451 => Txt Tokens per Sec:     5142 || Lr: 0.000100
2024-02-08 14:46:50,360 Epoch  53: Total Training Recognition Loss 2.30  Total Training Translation Loss 220.79 
2024-02-08 14:46:50,361 EPOCH 54
2024-02-08 14:46:54,311 [Epoch: 054 Step: 00003600] Batch Recognition Loss:   0.024871 => Gls Tokens per Sec:     1960 || Batch Translation Loss:   2.324133 => Txt Tokens per Sec:     5443 || Lr: 0.000100
2024-02-08 14:46:55,758 Epoch  54: Total Training Recognition Loss 2.20  Total Training Translation Loss 204.39 
2024-02-08 14:46:55,760 EPOCH 55
2024-02-08 14:47:00,743 Epoch  55: Total Training Recognition Loss 2.18  Total Training Translation Loss 187.17 
2024-02-08 14:47:00,743 EPOCH 56
2024-02-08 14:47:02,213 [Epoch: 056 Step: 00003700] Batch Recognition Loss:   0.018125 => Gls Tokens per Sec:     1566 || Batch Translation Loss:   2.909931 => Txt Tokens per Sec:     4501 || Lr: 0.000100
2024-02-08 14:47:06,497 Epoch  56: Total Training Recognition Loss 2.13  Total Training Translation Loss 171.61 
2024-02-08 14:47:06,497 EPOCH 57
2024-02-08 14:47:10,201 [Epoch: 057 Step: 00003800] Batch Recognition Loss:   0.058165 => Gls Tokens per Sec:     2047 || Batch Translation Loss:   1.912773 => Txt Tokens per Sec:     5650 || Lr: 0.000100
2024-02-08 14:47:11,882 Epoch  57: Total Training Recognition Loss 2.22  Total Training Translation Loss 155.22 
2024-02-08 14:47:11,883 EPOCH 58
2024-02-08 14:47:17,299 Epoch  58: Total Training Recognition Loss 1.70  Total Training Translation Loss 147.67 
2024-02-08 14:47:17,299 EPOCH 59
2024-02-08 14:47:18,229 [Epoch: 059 Step: 00003900] Batch Recognition Loss:   0.024436 => Gls Tokens per Sec:     2411 || Batch Translation Loss:   1.907344 => Txt Tokens per Sec:     6248 || Lr: 0.000100
2024-02-08 14:47:22,161 Epoch  59: Total Training Recognition Loss 1.76  Total Training Translation Loss 137.65 
2024-02-08 14:47:22,161 EPOCH 60
2024-02-08 14:47:26,224 [Epoch: 060 Step: 00004000] Batch Recognition Loss:   0.026191 => Gls Tokens per Sec:     1827 || Batch Translation Loss:   1.877780 => Txt Tokens per Sec:     5053 || Lr: 0.000100
2024-02-08 14:47:34,571 Validation result at epoch  60, step     4000: duration: 8.3458s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 3.23280	Translation Loss: 74213.03125	PPL: 1656.41003
	Eval Metric: BLEU
	WER 6.64	(DEL: 0.00,	INS: 0.00,	SUB: 6.64)
	BLEU-4 0.68	(BLEU-1: 10.84,	BLEU-2: 3.45,	BLEU-3: 1.35,	BLEU-4: 0.68)
	CHRF 17.09	ROUGE 9.11
2024-02-08 14:47:34,572 Logging Recognition and Translation Outputs
2024-02-08 14:47:34,572 ========================================================================================================================
2024-02-08 14:47:34,572 Logging Sequence: 112_165.00
2024-02-08 14:47:34,573 	Gloss Reference :	A B+C+D+E
2024-02-08 14:47:34,573 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 14:47:34,573 	Gloss Alignment :	         
2024-02-08 14:47:34,573 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 14:47:34,574 	Text Reference  :	the narendra modi stadium will be **** the home   for the   ahmedabad-based franchise
2024-02-08 14:47:34,574 	Text Hypothesis :	*** ******** **** they    will be sent to  prison for their own             years    
2024-02-08 14:47:34,574 	Text Alignment  :	D   D        D    S               I    S   S          S     S               S        
2024-02-08 14:47:34,574 ========================================================================================================================
2024-02-08 14:47:34,574 Logging Sequence: 176_154.00
2024-02-08 14:47:34,575 	Gloss Reference :	A B+C+D+E
2024-02-08 14:47:34,575 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 14:47:34,575 	Gloss Alignment :	         
2024-02-08 14:47:34,575 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 14:47:34,576 	Text Reference  :	******* **** *** dahiya could potentially bring        home india's second gold    medal
2024-02-08 14:47:34,576 	Text Hypothesis :	however they had to     play  against     kazakhstan's runs in      the    3-match match
2024-02-08 14:47:34,576 	Text Alignment  :	I       I    I   S      S     S           S            S    S       S      S       S    
2024-02-08 14:47:34,576 ========================================================================================================================
2024-02-08 14:47:34,577 Logging Sequence: 94_2.00
2024-02-08 14:47:34,577 	Gloss Reference :	A B+C+D+E
2024-02-08 14:47:34,577 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 14:47:34,577 	Gloss Alignment :	         
2024-02-08 14:47:34,577 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 14:47:34,578 	Text Reference  :	the icc odi   men' world cup  2023 will  be      hosted by  india on   5th october 2023   
2024-02-08 14:47:34,579 	Text Hypothesis :	*** *** india did  not   know this match between india  and open  with two tough   wickets
2024-02-08 14:47:34,579 	Text Alignment  :	D   D   S     S    S     S    S    S     S       S      S   S     S    S   S       S      
2024-02-08 14:47:34,579 ========================================================================================================================
2024-02-08 14:47:34,579 Logging Sequence: 165_453.00
2024-02-08 14:47:34,579 	Gloss Reference :	A B+C+D+E
2024-02-08 14:47:34,579 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 14:47:34,579 	Gloss Alignment :	         
2024-02-08 14:47:34,580 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 14:47:34,580 	Text Reference  :	****** icc did  not      agree   to sehwag' decision of wearing a numberless jersey
2024-02-08 14:47:34,580 	Text Hypothesis :	people had been shocking players to ******* ******** ** ******* * see        him   
2024-02-08 14:47:34,581 	Text Alignment  :	I      S   S    S        S          D       D        D  D       D S          S     
2024-02-08 14:47:34,581 ========================================================================================================================
2024-02-08 14:47:34,581 Logging Sequence: 139_46.00
2024-02-08 14:47:34,581 	Gloss Reference :	A B+C+D+E
2024-02-08 14:47:34,581 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 14:47:34,581 	Gloss Alignment :	         
2024-02-08 14:47:34,582 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 14:47:34,583 	Text Reference  :	everyone thought it would be a one sided match   because   morocco is        an       amateur team   and   belgium ranks 2nd  in the world
2024-02-08 14:47:34,583 	Text Hypothesis :	******** ******* ** ***** ** * *** 4     another argentine enzo    fernandez recieved the     golden glove for     being held in *** qatar
2024-02-08 14:47:34,584 	Text Alignment  :	D        D       D  D     D  D D   S     S       S         S       S         S        S       S      S     S       S     S       D   S    
2024-02-08 14:47:34,584 ========================================================================================================================
2024-02-08 14:47:36,250 Epoch  60: Total Training Recognition Loss 1.76  Total Training Translation Loss 130.35 
2024-02-08 14:47:36,251 EPOCH 61
2024-02-08 14:47:41,837 Epoch  61: Total Training Recognition Loss 1.72  Total Training Translation Loss 122.72 
2024-02-08 14:47:41,837 EPOCH 62
2024-02-08 14:47:42,817 [Epoch: 062 Step: 00004100] Batch Recognition Loss:   0.027202 => Gls Tokens per Sec:     2123 || Batch Translation Loss:   2.129956 => Txt Tokens per Sec:     5960 || Lr: 0.000100
2024-02-08 14:47:47,316 Epoch  62: Total Training Recognition Loss 1.59  Total Training Translation Loss 112.42 
2024-02-08 14:47:47,317 EPOCH 63
2024-02-08 14:47:50,756 [Epoch: 063 Step: 00004200] Batch Recognition Loss:   0.010925 => Gls Tokens per Sec:     2141 || Batch Translation Loss:   1.279594 => Txt Tokens per Sec:     5982 || Lr: 0.000100
2024-02-08 14:47:52,349 Epoch  63: Total Training Recognition Loss 1.58  Total Training Translation Loss 110.88 
2024-02-08 14:47:52,349 EPOCH 64
2024-02-08 14:47:57,931 Epoch  64: Total Training Recognition Loss 1.47  Total Training Translation Loss 99.89 
2024-02-08 14:47:57,932 EPOCH 65
2024-02-08 14:47:58,742 [Epoch: 065 Step: 00004300] Batch Recognition Loss:   0.035691 => Gls Tokens per Sec:     2374 || Batch Translation Loss:   0.920423 => Txt Tokens per Sec:     6214 || Lr: 0.000100
2024-02-08 14:48:03,335 Epoch  65: Total Training Recognition Loss 1.39  Total Training Translation Loss 98.64 
2024-02-08 14:48:03,336 EPOCH 66
2024-02-08 14:48:07,165 [Epoch: 066 Step: 00004400] Batch Recognition Loss:   0.019340 => Gls Tokens per Sec:     1855 || Batch Translation Loss:   1.749513 => Txt Tokens per Sec:     5187 || Lr: 0.000100
2024-02-08 14:48:08,889 Epoch  66: Total Training Recognition Loss 1.25  Total Training Translation Loss 92.22 
2024-02-08 14:48:08,889 EPOCH 67
2024-02-08 14:48:14,206 Epoch  67: Total Training Recognition Loss 1.31  Total Training Translation Loss 103.36 
2024-02-08 14:48:14,207 EPOCH 68
2024-02-08 14:48:15,074 [Epoch: 068 Step: 00004500] Batch Recognition Loss:   0.013698 => Gls Tokens per Sec:     2029 || Batch Translation Loss:   1.369483 => Txt Tokens per Sec:     5753 || Lr: 0.000100
2024-02-08 14:48:19,721 Epoch  68: Total Training Recognition Loss 1.54  Total Training Translation Loss 97.23 
2024-02-08 14:48:19,722 EPOCH 69
2024-02-08 14:48:22,908 [Epoch: 069 Step: 00004600] Batch Recognition Loss:   0.023761 => Gls Tokens per Sec:     2178 || Batch Translation Loss:   1.547156 => Txt Tokens per Sec:     6151 || Lr: 0.000100
2024-02-08 14:48:24,519 Epoch  69: Total Training Recognition Loss 1.45  Total Training Translation Loss 79.46 
2024-02-08 14:48:24,519 EPOCH 70
2024-02-08 14:48:30,219 Epoch  70: Total Training Recognition Loss 1.23  Total Training Translation Loss 76.35 
2024-02-08 14:48:30,220 EPOCH 71
2024-02-08 14:48:31,171 [Epoch: 071 Step: 00004700] Batch Recognition Loss:   0.025133 => Gls Tokens per Sec:     1685 || Batch Translation Loss:   1.043756 => Txt Tokens per Sec:     5124 || Lr: 0.000100
2024-02-08 14:48:35,665 Epoch  71: Total Training Recognition Loss 1.22  Total Training Translation Loss 72.74 
2024-02-08 14:48:35,666 EPOCH 72
2024-02-08 14:48:39,092 [Epoch: 072 Step: 00004800] Batch Recognition Loss:   0.002205 => Gls Tokens per Sec:     1980 || Batch Translation Loss:   1.063172 => Txt Tokens per Sec:     5430 || Lr: 0.000100
2024-02-08 14:48:41,243 Epoch  72: Total Training Recognition Loss 1.18  Total Training Translation Loss 69.89 
2024-02-08 14:48:41,243 EPOCH 73
2024-02-08 14:48:46,753 Epoch  73: Total Training Recognition Loss 1.23  Total Training Translation Loss 71.12 
2024-02-08 14:48:46,754 EPOCH 74
2024-02-08 14:48:47,675 [Epoch: 074 Step: 00004900] Batch Recognition Loss:   0.045442 => Gls Tokens per Sec:     1564 || Batch Translation Loss:   1.526542 => Txt Tokens per Sec:     4749 || Lr: 0.000100
2024-02-08 14:48:52,300 Epoch  74: Total Training Recognition Loss 1.24  Total Training Translation Loss 69.27 
2024-02-08 14:48:52,301 EPOCH 75
2024-02-08 14:48:55,651 [Epoch: 075 Step: 00005000] Batch Recognition Loss:   0.026963 => Gls Tokens per Sec:     2006 || Batch Translation Loss:   1.038107 => Txt Tokens per Sec:     5635 || Lr: 0.000100
2024-02-08 14:48:57,748 Epoch  75: Total Training Recognition Loss 1.27  Total Training Translation Loss 63.71 
2024-02-08 14:48:57,748 EPOCH 76
2024-02-08 14:49:03,327 Epoch  76: Total Training Recognition Loss 1.01  Total Training Translation Loss 58.26 
2024-02-08 14:49:03,328 EPOCH 77
2024-02-08 14:49:03,937 [Epoch: 077 Step: 00005100] Batch Recognition Loss:   0.010204 => Gls Tokens per Sec:     2106 || Batch Translation Loss:   0.686708 => Txt Tokens per Sec:     5832 || Lr: 0.000100
2024-02-08 14:49:08,905 Epoch  77: Total Training Recognition Loss 1.15  Total Training Translation Loss 58.92 
2024-02-08 14:49:08,905 EPOCH 78
2024-02-08 14:49:12,403 [Epoch: 078 Step: 00005200] Batch Recognition Loss:   0.010473 => Gls Tokens per Sec:     1876 || Batch Translation Loss:   0.923447 => Txt Tokens per Sec:     5115 || Lr: 0.000100
2024-02-08 14:49:14,511 Epoch  78: Total Training Recognition Loss 1.08  Total Training Translation Loss 60.03 
2024-02-08 14:49:14,511 EPOCH 79
2024-02-08 14:49:19,917 Epoch  79: Total Training Recognition Loss 1.05  Total Training Translation Loss 53.72 
2024-02-08 14:49:19,918 EPOCH 80
2024-02-08 14:49:20,489 [Epoch: 080 Step: 00005300] Batch Recognition Loss:   0.004713 => Gls Tokens per Sec:     1962 || Batch Translation Loss:   0.903820 => Txt Tokens per Sec:     5891 || Lr: 0.000100
2024-02-08 14:49:25,487 Epoch  80: Total Training Recognition Loss 1.46  Total Training Translation Loss 53.76 
2024-02-08 14:49:25,488 EPOCH 81
2024-02-08 14:49:28,552 [Epoch: 081 Step: 00005400] Batch Recognition Loss:   0.016127 => Gls Tokens per Sec:     2057 || Batch Translation Loss:   1.109797 => Txt Tokens per Sec:     5793 || Lr: 0.000100
2024-02-08 14:49:30,751 Epoch  81: Total Training Recognition Loss 1.07  Total Training Translation Loss 48.61 
2024-02-08 14:49:30,752 EPOCH 82
2024-02-08 14:49:36,349 Epoch  82: Total Training Recognition Loss 0.87  Total Training Translation Loss 50.45 
2024-02-08 14:49:36,349 EPOCH 83
2024-02-08 14:49:37,075 [Epoch: 083 Step: 00005500] Batch Recognition Loss:   0.041438 => Gls Tokens per Sec:     1326 || Batch Translation Loss:   0.898304 => Txt Tokens per Sec:     3796 || Lr: 0.000100
2024-02-08 14:49:45,940 Epoch  83: Total Training Recognition Loss 1.03  Total Training Translation Loss 46.70 
2024-02-08 14:49:45,941 EPOCH 84
2024-02-08 14:49:51,598 [Epoch: 084 Step: 00005600] Batch Recognition Loss:   0.020671 => Gls Tokens per Sec:     1103 || Batch Translation Loss:   1.014787 => Txt Tokens per Sec:     3012 || Lr: 0.000100
2024-02-08 14:49:56,045 Epoch  84: Total Training Recognition Loss 0.90  Total Training Translation Loss 50.81 
2024-02-08 14:49:56,045 EPOCH 85
2024-02-08 14:50:06,088 Epoch  85: Total Training Recognition Loss 1.02  Total Training Translation Loss 47.99 
2024-02-08 14:50:06,089 EPOCH 86
2024-02-08 14:50:06,723 [Epoch: 086 Step: 00005700] Batch Recognition Loss:   0.022024 => Gls Tokens per Sec:     1265 || Batch Translation Loss:   0.220692 => Txt Tokens per Sec:     3549 || Lr: 0.000100
2024-02-08 14:50:16,255 Epoch  86: Total Training Recognition Loss 1.07  Total Training Translation Loss 43.31 
2024-02-08 14:50:16,256 EPOCH 87
2024-02-08 14:50:21,906 [Epoch: 087 Step: 00005800] Batch Recognition Loss:   0.003018 => Gls Tokens per Sec:     1059 || Batch Translation Loss:   0.600937 => Txt Tokens per Sec:     2869 || Lr: 0.000100
2024-02-08 14:50:26,528 Epoch  87: Total Training Recognition Loss 0.85  Total Training Translation Loss 39.46 
2024-02-08 14:50:26,529 EPOCH 88
2024-02-08 14:50:36,733 Epoch  88: Total Training Recognition Loss 0.80  Total Training Translation Loss 41.82 
2024-02-08 14:50:36,734 EPOCH 89
2024-02-08 14:50:37,213 [Epoch: 089 Step: 00005900] Batch Recognition Loss:   0.009952 => Gls Tokens per Sec:     1344 || Batch Translation Loss:   0.282025 => Txt Tokens per Sec:     3176 || Lr: 0.000100
2024-02-08 14:50:46,892 Epoch  89: Total Training Recognition Loss 0.92  Total Training Translation Loss 40.71 
2024-02-08 14:50:46,893 EPOCH 90
2024-02-08 14:50:52,755 [Epoch: 090 Step: 00006000] Batch Recognition Loss:   0.073101 => Gls Tokens per Sec:      993 || Batch Translation Loss:   0.467420 => Txt Tokens per Sec:     2773 || Lr: 0.000100
2024-02-08 14:51:07,126 Validation result at epoch  90, step     6000: duration: 14.3692s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 3.43156	Translation Loss: 81836.67969	PPL: 3547.00439
	Eval Metric: BLEU
	WER 6.07	(DEL: 0.00,	INS: 0.00,	SUB: 6.07)
	BLEU-4 0.84	(BLEU-1: 11.31,	BLEU-2: 3.81,	BLEU-3: 1.63,	BLEU-4: 0.84)
	CHRF 16.76	ROUGE 9.80
2024-02-08 14:51:07,127 Logging Recognition and Translation Outputs
2024-02-08 14:51:07,127 ========================================================================================================================
2024-02-08 14:51:07,128 Logging Sequence: 160_153.00
2024-02-08 14:51:07,128 	Gloss Reference :	A B+C+D+E
2024-02-08 14:51:07,128 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 14:51:07,128 	Gloss Alignment :	         
2024-02-08 14:51:07,128 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 14:51:07,130 	Text Reference  :	i have no hard feelings towards rohit sharma and       he will         always have     my  full support  as he   is  my   teammate
2024-02-08 14:51:07,130 	Text Hypothesis :	* **** ** **** isn't    that    such  a      statement of broadcasting the    decision for his  decision to play the next day     
2024-02-08 14:51:07,130 	Text Alignment  :	D D    D  D    S        S       S     S      S         S  S            S      S        S   S    S        S  S    S   S    S       
2024-02-08 14:51:07,131 ========================================================================================================================
2024-02-08 14:51:07,131 Logging Sequence: 103_253.00
2024-02-08 14:51:07,131 	Gloss Reference :	A B+C+D+E
2024-02-08 14:51:07,131 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 14:51:07,131 	Gloss Alignment :	         
2024-02-08 14:51:07,131 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 14:51:07,132 	Text Reference  :	***** canada is   3rd with    92     medals
2024-02-08 14:51:07,132 	Text Hypothesis :	these games  were the british empire games 
2024-02-08 14:51:07,132 	Text Alignment  :	I     S      S    S   S       S      S     
2024-02-08 14:51:07,132 ========================================================================================================================
2024-02-08 14:51:07,132 Logging Sequence: 155_25.00
2024-02-08 14:51:07,132 	Gloss Reference :	A B+C+D+E
2024-02-08 14:51:07,132 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 14:51:07,133 	Gloss Alignment :	         
2024-02-08 14:51:07,133 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 14:51:07,134 	Text Reference  :	this is because taliban      overthrew the afghan government and **** **** took over  the country
2024-02-08 14:51:07,134 	Text Hypothesis :	i    am very    disappointed by        the ****** news       and they will be   given a   crime  
2024-02-08 14:51:07,134 	Text Alignment  :	S    S  S       S            S             D      S              I    I    S    S     S   S      
2024-02-08 14:51:07,134 ========================================================================================================================
2024-02-08 14:51:07,134 Logging Sequence: 81_105.00
2024-02-08 14:51:07,135 	Gloss Reference :	A B+C+D+E
2024-02-08 14:51:07,135 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 14:51:07,135 	Gloss Alignment :	         
2024-02-08 14:51:07,135 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 14:51:07,135 	Text Reference  :	dhoni was tagged in multiple such posts as he   was  the   brand        ambassador
2024-02-08 14:51:07,135 	Text Hypothesis :	***** *** ****** ** ******** **** ***** ** many find their sponsorships william   
2024-02-08 14:51:07,136 	Text Alignment  :	D     D   D      D  D        D    D     D  S    S    S     S            S         
2024-02-08 14:51:07,136 ========================================================================================================================
2024-02-08 14:51:07,136 Logging Sequence: 105_136.00
2024-02-08 14:51:07,136 	Gloss Reference :	A B+C+D+E
2024-02-08 14:51:07,136 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 14:51:07,136 	Gloss Alignment :	         
2024-02-08 14:51:07,136 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 14:51:07,137 	Text Reference  :	** **** *** beating him  once is  my   biggest dream
2024-02-08 14:51:07,137 	Text Hypothesis :	as they are not     meet out  for them as      well 
2024-02-08 14:51:07,137 	Text Alignment  :	I  I    I   S       S    S    S   S    S       S    
2024-02-08 14:51:07,138 ========================================================================================================================
2024-02-08 14:51:11,388 Epoch  90: Total Training Recognition Loss 0.83  Total Training Translation Loss 42.31 
2024-02-08 14:51:11,389 EPOCH 91
2024-02-08 14:51:21,506 Epoch  91: Total Training Recognition Loss 0.84  Total Training Translation Loss 46.80 
2024-02-08 14:51:21,507 EPOCH 92
2024-02-08 14:51:21,989 [Epoch: 092 Step: 00006100] Batch Recognition Loss:   0.008704 => Gls Tokens per Sec:      999 || Batch Translation Loss:   0.588624 => Txt Tokens per Sec:     3246 || Lr: 0.000100
2024-02-08 14:51:31,679 Epoch  92: Total Training Recognition Loss 0.90  Total Training Translation Loss 43.99 
2024-02-08 14:51:31,680 EPOCH 93
2024-02-08 14:51:36,849 [Epoch: 093 Step: 00006200] Batch Recognition Loss:   0.008029 => Gls Tokens per Sec:     1095 || Batch Translation Loss:   0.758328 => Txt Tokens per Sec:     2978 || Lr: 0.000100
2024-02-08 14:51:41,900 Epoch  93: Total Training Recognition Loss 0.86  Total Training Translation Loss 46.97 
2024-02-08 14:51:41,900 EPOCH 94
2024-02-08 14:51:50,885 Epoch  94: Total Training Recognition Loss 0.67  Total Training Translation Loss 42.00 
2024-02-08 14:51:50,885 EPOCH 95
2024-02-08 14:51:51,026 [Epoch: 095 Step: 00006300] Batch Recognition Loss:   0.007166 => Gls Tokens per Sec:     2283 || Batch Translation Loss:   0.356259 => Txt Tokens per Sec:     6230 || Lr: 0.000100
2024-02-08 14:51:56,489 Epoch  95: Total Training Recognition Loss 0.84  Total Training Translation Loss 35.18 
2024-02-08 14:51:56,490 EPOCH 96
2024-02-08 14:51:59,209 [Epoch: 096 Step: 00006400] Batch Recognition Loss:   0.018104 => Gls Tokens per Sec:     2024 || Batch Translation Loss:   0.865618 => Txt Tokens per Sec:     5600 || Lr: 0.000100
2024-02-08 14:52:01,816 Epoch  96: Total Training Recognition Loss 0.85  Total Training Translation Loss 41.91 
2024-02-08 14:52:01,816 EPOCH 97
2024-02-08 14:52:07,780 Epoch  97: Total Training Recognition Loss 0.71  Total Training Translation Loss 37.81 
2024-02-08 14:52:07,781 EPOCH 98
2024-02-08 14:52:07,844 [Epoch: 098 Step: 00006500] Batch Recognition Loss:   0.010690 => Gls Tokens per Sec:     2537 || Batch Translation Loss:   0.558576 => Txt Tokens per Sec:     7293 || Lr: 0.000100
2024-02-08 14:52:13,083 Epoch  98: Total Training Recognition Loss 0.73  Total Training Translation Loss 35.28 
2024-02-08 14:52:13,084 EPOCH 99
2024-02-08 14:52:15,585 [Epoch: 099 Step: 00006600] Batch Recognition Loss:   0.039433 => Gls Tokens per Sec:     2135 || Batch Translation Loss:   0.518577 => Txt Tokens per Sec:     5690 || Lr: 0.000100
2024-02-08 14:52:18,458 Epoch  99: Total Training Recognition Loss 0.80  Total Training Translation Loss 31.42 
2024-02-08 14:52:18,459 EPOCH 100
2024-02-08 14:52:23,763 [Epoch: 100 Step: 00006700] Batch Recognition Loss:   0.010445 => Gls Tokens per Sec:     2002 || Batch Translation Loss:   0.364635 => Txt Tokens per Sec:     5540 || Lr: 0.000100
2024-02-08 14:52:23,763 Epoch 100: Total Training Recognition Loss 0.65  Total Training Translation Loss 29.09 
2024-02-08 14:52:23,763 EPOCH 101
2024-02-08 14:52:29,293 Epoch 101: Total Training Recognition Loss 0.72  Total Training Translation Loss 25.98 
2024-02-08 14:52:29,294 EPOCH 102
2024-02-08 14:52:32,050 [Epoch: 102 Step: 00006800] Batch Recognition Loss:   0.001132 => Gls Tokens per Sec:     1880 || Batch Translation Loss:   0.564529 => Txt Tokens per Sec:     5350 || Lr: 0.000100
2024-02-08 14:52:34,713 Epoch 102: Total Training Recognition Loss 0.60  Total Training Translation Loss 28.72 
2024-02-08 14:52:34,713 EPOCH 103
2024-02-08 14:52:40,351 [Epoch: 103 Step: 00006900] Batch Recognition Loss:   0.001929 => Gls Tokens per Sec:     1856 || Batch Translation Loss:   0.378288 => Txt Tokens per Sec:     5114 || Lr: 0.000100
2024-02-08 14:52:40,526 Epoch 103: Total Training Recognition Loss 0.75  Total Training Translation Loss 31.18 
2024-02-08 14:52:40,526 EPOCH 104
2024-02-08 14:52:45,610 Epoch 104: Total Training Recognition Loss 0.83  Total Training Translation Loss 27.82 
2024-02-08 14:52:45,610 EPOCH 105
2024-02-08 14:52:48,494 [Epoch: 105 Step: 00007000] Batch Recognition Loss:   0.001816 => Gls Tokens per Sec:     1776 || Batch Translation Loss:   0.351372 => Txt Tokens per Sec:     4846 || Lr: 0.000100
2024-02-08 14:52:51,513 Epoch 105: Total Training Recognition Loss 0.96  Total Training Translation Loss 25.52 
2024-02-08 14:52:51,513 EPOCH 106
2024-02-08 14:52:56,555 [Epoch: 106 Step: 00007100] Batch Recognition Loss:   0.002041 => Gls Tokens per Sec:     2043 || Batch Translation Loss:   0.872590 => Txt Tokens per Sec:     5619 || Lr: 0.000100
2024-02-08 14:52:56,791 Epoch 106: Total Training Recognition Loss 3.59  Total Training Translation Loss 31.35 
2024-02-08 14:52:56,791 EPOCH 107
2024-02-08 14:53:02,650 Epoch 107: Total Training Recognition Loss 1.19  Total Training Translation Loss 33.47 
2024-02-08 14:53:02,651 EPOCH 108
2024-02-08 14:53:04,909 [Epoch: 108 Step: 00007200] Batch Recognition Loss:   0.001181 => Gls Tokens per Sec:     2197 || Batch Translation Loss:   0.331060 => Txt Tokens per Sec:     5785 || Lr: 0.000100
2024-02-08 14:53:09,566 Epoch 108: Total Training Recognition Loss 0.80  Total Training Translation Loss 31.29 
2024-02-08 14:53:09,567 EPOCH 109
2024-02-08 14:53:18,901 [Epoch: 109 Step: 00007300] Batch Recognition Loss:   0.005350 => Gls Tokens per Sec:     1086 || Batch Translation Loss:   0.310973 => Txt Tokens per Sec:     3011 || Lr: 0.000100
2024-02-08 14:53:19,214 Epoch 109: Total Training Recognition Loss 0.58  Total Training Translation Loss 29.34 
2024-02-08 14:53:19,214 EPOCH 110
2024-02-08 14:53:28,671 Epoch 110: Total Training Recognition Loss 0.58  Total Training Translation Loss 29.21 
2024-02-08 14:53:28,671 EPOCH 111
2024-02-08 14:53:33,300 [Epoch: 111 Step: 00007400] Batch Recognition Loss:   0.001753 => Gls Tokens per Sec:     1037 || Batch Translation Loss:   0.211361 => Txt Tokens per Sec:     2997 || Lr: 0.000100
2024-02-08 14:53:38,336 Epoch 111: Total Training Recognition Loss 0.61  Total Training Translation Loss 29.24 
2024-02-08 14:53:38,337 EPOCH 112
2024-02-08 14:53:47,346 [Epoch: 112 Step: 00007500] Batch Recognition Loss:   0.010247 => Gls Tokens per Sec:     1108 || Batch Translation Loss:   0.225765 => Txt Tokens per Sec:     3033 || Lr: 0.000100
2024-02-08 14:53:48,021 Epoch 112: Total Training Recognition Loss 0.47  Total Training Translation Loss 24.15 
2024-02-08 14:53:48,021 EPOCH 113
2024-02-08 14:53:57,889 Epoch 113: Total Training Recognition Loss 0.43  Total Training Translation Loss 24.50 
2024-02-08 14:53:57,889 EPOCH 114
2024-02-08 14:54:01,833 [Epoch: 114 Step: 00007600] Batch Recognition Loss:   0.004430 => Gls Tokens per Sec:     1177 || Batch Translation Loss:   0.513385 => Txt Tokens per Sec:     3219 || Lr: 0.000100
2024-02-08 14:54:07,698 Epoch 114: Total Training Recognition Loss 0.43  Total Training Translation Loss 22.08 
2024-02-08 14:54:07,700 EPOCH 115
2024-02-08 14:54:17,027 [Epoch: 115 Step: 00007700] Batch Recognition Loss:   0.029214 => Gls Tokens per Sec:     1053 || Batch Translation Loss:   0.424294 => Txt Tokens per Sec:     2920 || Lr: 0.000100
2024-02-08 14:54:17,711 Epoch 115: Total Training Recognition Loss 0.46  Total Training Translation Loss 23.58 
2024-02-08 14:54:17,711 EPOCH 116
2024-02-08 14:54:27,808 Epoch 116: Total Training Recognition Loss 0.58  Total Training Translation Loss 26.33 
2024-02-08 14:54:27,809 EPOCH 117
2024-02-08 14:54:32,031 [Epoch: 117 Step: 00007800] Batch Recognition Loss:   0.004505 => Gls Tokens per Sec:     1061 || Batch Translation Loss:   0.349572 => Txt Tokens per Sec:     2950 || Lr: 0.000100
2024-02-08 14:54:37,814 Epoch 117: Total Training Recognition Loss 0.67  Total Training Translation Loss 34.99 
2024-02-08 14:54:37,814 EPOCH 118
2024-02-08 14:54:47,095 [Epoch: 118 Step: 00007900] Batch Recognition Loss:   0.008506 => Gls Tokens per Sec:     1041 || Batch Translation Loss:   0.326621 => Txt Tokens per Sec:     2919 || Lr: 0.000100
2024-02-08 14:54:47,862 Epoch 118: Total Training Recognition Loss 0.56  Total Training Translation Loss 25.80 
2024-02-08 14:54:47,863 EPOCH 119
2024-02-08 14:54:57,930 Epoch 119: Total Training Recognition Loss 0.58  Total Training Translation Loss 20.60 
2024-02-08 14:54:57,931 EPOCH 120
2024-02-08 14:55:01,782 [Epoch: 120 Step: 00008000] Batch Recognition Loss:   0.011464 => Gls Tokens per Sec:     1096 || Batch Translation Loss:   0.345965 => Txt Tokens per Sec:     2949 || Lr: 0.000100
2024-02-08 14:55:15,623 Validation result at epoch 120, step     8000: duration: 13.8398s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 3.32115	Translation Loss: 86544.20312	PPL: 5676.25391
	Eval Metric: BLEU
	WER 5.93	(DEL: 0.00,	INS: 0.00,	SUB: 5.93)
	BLEU-4 0.75	(BLEU-1: 11.23,	BLEU-2: 3.75,	BLEU-3: 1.65,	BLEU-4: 0.75)
	CHRF 17.34	ROUGE 9.65
2024-02-08 14:55:15,624 Logging Recognition and Translation Outputs
2024-02-08 14:55:15,624 ========================================================================================================================
2024-02-08 14:55:15,625 Logging Sequence: 180_236.00
2024-02-08 14:55:15,625 	Gloss Reference :	A B+C+D+E
2024-02-08 14:55:15,625 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 14:55:15,625 	Gloss Alignment :	         
2024-02-08 14:55:15,625 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 14:55:15,626 	Text Reference  :	however the wrestlers returned to the protest  site    at   jantar mantar  with thier    demands   
2024-02-08 14:55:15,626 	Text Hypothesis :	******* *** wrestlers wanted   to *** practise however they are    ruining his  family's reputation
2024-02-08 14:55:15,627 	Text Alignment  :	D       D             S           D   S        S       S    S      S       S    S        S         
2024-02-08 14:55:15,627 ========================================================================================================================
2024-02-08 14:55:15,627 Logging Sequence: 111_154.00
2024-02-08 14:55:15,627 	Gloss Reference :	A B+C+D+E  
2024-02-08 14:55:15,627 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-08 14:55:15,627 	Gloss Alignment :	  S        
2024-02-08 14:55:15,628 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 14:55:15,629 	Text Reference  :	***** ****** due to   csk's slow    over rate dhoni was     fined    rs   12  lakh   
2024-02-08 14:55:15,629 	Text Hypothesis :	rohit sharma was seen the   captain of   his  16    players infected with the captain
2024-02-08 14:55:15,629 	Text Alignment  :	I     I      S   S    S     S       S    S    S     S       S        S    S   S      
2024-02-08 14:55:15,629 ========================================================================================================================
2024-02-08 14:55:15,629 Logging Sequence: 118_314.00
2024-02-08 14:55:15,630 	Gloss Reference :	A B+C+D+E
2024-02-08 14:55:15,630 	Gloss Hypothesis:	A B+C+D  
2024-02-08 14:55:15,630 	Gloss Alignment :	  S      
2024-02-08 14:55:15,630 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 14:55:15,631 	Text Reference  :	**** wow even the president had come  to  watch  
2024-02-08 14:55:15,631 	Text Hypothesis :	what is  why  the ********* *** match are delayed
2024-02-08 14:55:15,631 	Text Alignment  :	I    S   S        D         D   S     S   S      
2024-02-08 14:55:15,631 ========================================================================================================================
2024-02-08 14:55:15,631 Logging Sequence: 156_197.00
2024-02-08 14:55:15,631 	Gloss Reference :	A B+C+D+E
2024-02-08 14:55:15,631 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 14:55:15,632 	Gloss Alignment :	         
2024-02-08 14:55:15,632 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 14:55:15,632 	Text Reference  :	seattle orcas sor is owned by many investors including satya nadella microsoft ceo    
2024-02-08 14:55:15,632 	Text Hypothesis :	******* ***** *** ** ***** ** we   will      have      to    wait    for       updates
2024-02-08 14:55:15,633 	Text Alignment  :	D       D     D   D  D     D  S    S         S         S     S       S         S      
2024-02-08 14:55:15,633 ========================================================================================================================
2024-02-08 14:55:15,633 Logging Sequence: 183_159.00
2024-02-08 14:55:15,633 	Gloss Reference :	A B+C+D+E  
2024-02-08 14:55:15,633 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-08 14:55:15,633 	Gloss Alignment :	  S        
2024-02-08 14:55:15,633 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 14:55:15,636 	Text Reference  :	however an      exception to  this is   virat kohli     and his wife anushka sharma who refuse to share images of    their daughter
2024-02-08 14:55:15,636 	Text Hypothesis :	new     zealand players   get a    huge fan   following and his wife ******* ****** *** ****** ** ***** ****** hazel keech singh   
2024-02-08 14:55:15,636 	Text Alignment  :	S       S       S         S   S    S    S     S                      D       D      D   D      D  D     D      S     S     S       
2024-02-08 14:55:15,636 ========================================================================================================================
2024-02-08 14:55:21,820 Epoch 120: Total Training Recognition Loss 0.40  Total Training Translation Loss 19.30 
2024-02-08 14:55:21,820 EPOCH 121
2024-02-08 14:55:30,786 [Epoch: 121 Step: 00008100] Batch Recognition Loss:   0.006570 => Gls Tokens per Sec:     1060 || Batch Translation Loss:   0.337262 => Txt Tokens per Sec:     2947 || Lr: 0.000100
2024-02-08 14:55:31,748 Epoch 121: Total Training Recognition Loss 0.42  Total Training Translation Loss 17.45 
2024-02-08 14:55:31,748 EPOCH 122
2024-02-08 14:55:41,855 Epoch 122: Total Training Recognition Loss 0.40  Total Training Translation Loss 18.88 
2024-02-08 14:55:41,856 EPOCH 123
2024-02-08 14:55:45,676 [Epoch: 123 Step: 00008200] Batch Recognition Loss:   0.003449 => Gls Tokens per Sec:     1063 || Batch Translation Loss:   0.171234 => Txt Tokens per Sec:     2932 || Lr: 0.000100
2024-02-08 14:55:51,707 Epoch 123: Total Training Recognition Loss 0.40  Total Training Translation Loss 17.73 
2024-02-08 14:55:51,707 EPOCH 124
2024-02-08 14:55:59,456 [Epoch: 124 Step: 00008300] Batch Recognition Loss:   0.001640 => Gls Tokens per Sec:     1205 || Batch Translation Loss:   0.175076 => Txt Tokens per Sec:     3276 || Lr: 0.000100
2024-02-08 14:56:01,011 Epoch 124: Total Training Recognition Loss 0.45  Total Training Translation Loss 18.82 
2024-02-08 14:56:01,012 EPOCH 125
2024-02-08 14:56:11,039 Epoch 125: Total Training Recognition Loss 0.49  Total Training Translation Loss 20.15 
2024-02-08 14:56:11,040 EPOCH 126
2024-02-08 14:56:15,204 [Epoch: 126 Step: 00008400] Batch Recognition Loss:   0.021333 => Gls Tokens per Sec:      960 || Batch Translation Loss:   0.367626 => Txt Tokens per Sec:     2815 || Lr: 0.000100
2024-02-08 14:56:20,910 Epoch 126: Total Training Recognition Loss 0.57  Total Training Translation Loss 19.85 
2024-02-08 14:56:20,911 EPOCH 127
2024-02-08 14:56:29,945 [Epoch: 127 Step: 00008500] Batch Recognition Loss:   0.005927 => Gls Tokens per Sec:     1016 || Batch Translation Loss:   0.274163 => Txt Tokens per Sec:     2863 || Lr: 0.000100
2024-02-08 14:56:31,115 Epoch 127: Total Training Recognition Loss 0.53  Total Training Translation Loss 24.29 
2024-02-08 14:56:31,115 EPOCH 128
2024-02-08 14:56:41,167 Epoch 128: Total Training Recognition Loss 0.51  Total Training Translation Loss 19.34 
2024-02-08 14:56:41,168 EPOCH 129
2024-02-08 14:56:44,733 [Epoch: 129 Step: 00008600] Batch Recognition Loss:   0.001752 => Gls Tokens per Sec:     1078 || Batch Translation Loss:   0.292465 => Txt Tokens per Sec:     2926 || Lr: 0.000100
2024-02-08 14:56:51,316 Epoch 129: Total Training Recognition Loss 0.31  Total Training Translation Loss 20.11 
2024-02-08 14:56:51,317 EPOCH 130
2024-02-08 14:56:59,928 [Epoch: 130 Step: 00008700] Batch Recognition Loss:   0.027466 => Gls Tokens per Sec:     1059 || Batch Translation Loss:   0.096530 => Txt Tokens per Sec:     2936 || Lr: 0.000100
2024-02-08 14:57:01,516 Epoch 130: Total Training Recognition Loss 0.44  Total Training Translation Loss 19.53 
2024-02-08 14:57:01,516 EPOCH 131
2024-02-08 14:57:11,694 Epoch 131: Total Training Recognition Loss 0.44  Total Training Translation Loss 22.67 
2024-02-08 14:57:11,695 EPOCH 132
2024-02-08 14:57:15,077 [Epoch: 132 Step: 00008800] Batch Recognition Loss:   0.009749 => Gls Tokens per Sec:     1059 || Batch Translation Loss:   0.550833 => Txt Tokens per Sec:     2927 || Lr: 0.000100
2024-02-08 14:57:21,800 Epoch 132: Total Training Recognition Loss 0.44  Total Training Translation Loss 26.05 
2024-02-08 14:57:21,800 EPOCH 133
2024-02-08 14:57:30,361 [Epoch: 133 Step: 00008900] Batch Recognition Loss:   0.001490 => Gls Tokens per Sec:     1035 || Batch Translation Loss:   0.679808 => Txt Tokens per Sec:     2843 || Lr: 0.000100
2024-02-08 14:57:32,045 Epoch 133: Total Training Recognition Loss 0.55  Total Training Translation Loss 26.14 
2024-02-08 14:57:32,046 EPOCH 134
2024-02-08 14:57:42,085 Epoch 134: Total Training Recognition Loss 0.44  Total Training Translation Loss 20.95 
2024-02-08 14:57:42,086 EPOCH 135
2024-02-08 14:57:45,462 [Epoch: 135 Step: 00009000] Batch Recognition Loss:   0.002261 => Gls Tokens per Sec:     1043 || Batch Translation Loss:   0.281222 => Txt Tokens per Sec:     2741 || Lr: 0.000100
2024-02-08 14:57:52,044 Epoch 135: Total Training Recognition Loss 0.46  Total Training Translation Loss 17.80 
2024-02-08 14:57:52,045 EPOCH 136
2024-02-08 14:57:59,809 [Epoch: 136 Step: 00009100] Batch Recognition Loss:   0.014792 => Gls Tokens per Sec:     1121 || Batch Translation Loss:   0.109298 => Txt Tokens per Sec:     3032 || Lr: 0.000100
2024-02-08 14:58:02,102 Epoch 136: Total Training Recognition Loss 0.39  Total Training Translation Loss 16.79 
2024-02-08 14:58:02,102 EPOCH 137
2024-02-08 14:58:12,148 Epoch 137: Total Training Recognition Loss 0.40  Total Training Translation Loss 16.67 
2024-02-08 14:58:12,149 EPOCH 138
2024-02-08 14:58:15,327 [Epoch: 138 Step: 00009200] Batch Recognition Loss:   0.004909 => Gls Tokens per Sec:     1057 || Batch Translation Loss:   0.558713 => Txt Tokens per Sec:     2968 || Lr: 0.000100
2024-02-08 14:58:22,048 Epoch 138: Total Training Recognition Loss 0.33  Total Training Translation Loss 20.23 
2024-02-08 14:58:22,049 EPOCH 139
2024-02-08 14:58:30,187 [Epoch: 139 Step: 00009300] Batch Recognition Loss:   0.003749 => Gls Tokens per Sec:     1050 || Batch Translation Loss:   0.575873 => Txt Tokens per Sec:     2905 || Lr: 0.000100
2024-02-08 14:58:31,899 Epoch 139: Total Training Recognition Loss 0.40  Total Training Translation Loss 19.65 
2024-02-08 14:58:31,899 EPOCH 140
2024-02-08 14:58:41,217 Epoch 140: Total Training Recognition Loss 0.49  Total Training Translation Loss 24.44 
2024-02-08 14:58:41,217 EPOCH 141
2024-02-08 14:58:43,915 [Epoch: 141 Step: 00009400] Batch Recognition Loss:   0.003430 => Gls Tokens per Sec:     1187 || Batch Translation Loss:   0.208106 => Txt Tokens per Sec:     3120 || Lr: 0.000100
2024-02-08 14:58:51,381 Epoch 141: Total Training Recognition Loss 0.40  Total Training Translation Loss 17.53 
2024-02-08 14:58:51,381 EPOCH 142
2024-02-08 14:58:58,938 [Epoch: 142 Step: 00009500] Batch Recognition Loss:   0.003937 => Gls Tokens per Sec:     1109 || Batch Translation Loss:   0.168007 => Txt Tokens per Sec:     2990 || Lr: 0.000100
2024-02-08 14:59:01,416 Epoch 142: Total Training Recognition Loss 0.51  Total Training Translation Loss 16.15 
2024-02-08 14:59:01,416 EPOCH 143
2024-02-08 14:59:11,519 Epoch 143: Total Training Recognition Loss 0.38  Total Training Translation Loss 17.80 
2024-02-08 14:59:11,520 EPOCH 144
2024-02-08 14:59:14,666 [Epoch: 144 Step: 00009600] Batch Recognition Loss:   0.008651 => Gls Tokens per Sec:      967 || Batch Translation Loss:   0.181214 => Txt Tokens per Sec:     2782 || Lr: 0.000100
2024-02-08 14:59:21,424 Epoch 144: Total Training Recognition Loss 0.42  Total Training Translation Loss 15.13 
2024-02-08 14:59:21,425 EPOCH 145
2024-02-08 14:59:29,162 [Epoch: 145 Step: 00009700] Batch Recognition Loss:   0.010215 => Gls Tokens per Sec:     1076 || Batch Translation Loss:   0.102153 => Txt Tokens per Sec:     2959 || Lr: 0.000100
2024-02-08 14:59:31,546 Epoch 145: Total Training Recognition Loss 0.37  Total Training Translation Loss 13.99 
2024-02-08 14:59:31,546 EPOCH 146
2024-02-08 14:59:41,475 Epoch 146: Total Training Recognition Loss 0.46  Total Training Translation Loss 16.99 
2024-02-08 14:59:41,476 EPOCH 147
2024-02-08 14:59:44,018 [Epoch: 147 Step: 00009800] Batch Recognition Loss:   0.000984 => Gls Tokens per Sec:     1134 || Batch Translation Loss:   0.189676 => Txt Tokens per Sec:     3181 || Lr: 0.000100
2024-02-08 14:59:51,435 Epoch 147: Total Training Recognition Loss 0.30  Total Training Translation Loss 16.67 
2024-02-08 14:59:51,436 EPOCH 148
2024-02-08 14:59:58,838 [Epoch: 148 Step: 00009900] Batch Recognition Loss:   0.001238 => Gls Tokens per Sec:     1089 || Batch Translation Loss:   0.172897 => Txt Tokens per Sec:     3029 || Lr: 0.000100
2024-02-08 15:00:01,250 Epoch 148: Total Training Recognition Loss 0.38  Total Training Translation Loss 17.96 
2024-02-08 15:00:01,251 EPOCH 149
2024-02-08 15:00:11,221 Epoch 149: Total Training Recognition Loss 0.32  Total Training Translation Loss 23.35 
2024-02-08 15:00:11,222 EPOCH 150
2024-02-08 15:00:13,623 [Epoch: 150 Step: 00010000] Batch Recognition Loss:   0.002080 => Gls Tokens per Sec:     1092 || Batch Translation Loss:   0.158512 => Txt Tokens per Sec:     2996 || Lr: 0.000100
2024-02-08 15:00:27,339 Validation result at epoch 150, step    10000: duration: 13.7168s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 3.30891	Translation Loss: 89999.78906	PPL: 8015.97754
	Eval Metric: BLEU
	WER 5.44	(DEL: 0.00,	INS: 0.00,	SUB: 5.44)
	BLEU-4 0.49	(BLEU-1: 10.89,	BLEU-2: 3.03,	BLEU-3: 1.12,	BLEU-4: 0.49)
	CHRF 16.99	ROUGE 9.44
2024-02-08 15:00:27,340 Logging Recognition and Translation Outputs
2024-02-08 15:00:27,340 ========================================================================================================================
2024-02-08 15:00:27,341 Logging Sequence: 123_147.00
2024-02-08 15:00:27,341 	Gloss Reference :	A B+C+D+E
2024-02-08 15:00:27,341 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:00:27,341 	Gloss Alignment :	         
2024-02-08 15:00:27,341 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:00:27,342 	Text Reference  :	the former captain also owns the pontiac firebird trans am    car worth rs 68     lakh      
2024-02-08 15:00:27,342 	Text Hypothesis :	*** ****** ******* **** **** *** ******* ******** ***** dhoni has been  an ardent bike-lover
2024-02-08 15:00:27,342 	Text Alignment  :	D   D      D       D    D    D   D       D        D     S     S   S     S  S      S         
2024-02-08 15:00:27,342 ========================================================================================================================
2024-02-08 15:00:27,342 Logging Sequence: 58_196.00
2024-02-08 15:00:27,343 	Gloss Reference :	A B+C+D+E
2024-02-08 15:00:27,343 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:00:27,343 	Gloss Alignment :	         
2024-02-08 15:00:27,343 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:00:27,344 	Text Reference  :	the talents and     skills of  our athletes knows no bounds
2024-02-08 15:00:27,344 	Text Hypothesis :	*** two     cricket team   was in  england  for   a  games 
2024-02-08 15:00:27,344 	Text Alignment  :	D   S       S       S      S   S   S        S     S  S     
2024-02-08 15:00:27,344 ========================================================================================================================
2024-02-08 15:00:27,344 Logging Sequence: 168_184.00
2024-02-08 15:00:27,344 	Gloss Reference :	A B+C+D+E    
2024-02-08 15:00:27,344 	Gloss Hypothesis:	A B+C+D+E+D+E
2024-02-08 15:00:27,344 	Gloss Alignment :	  S          
2024-02-08 15:00:27,345 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:00:27,346 	Text Reference  :	people ** say that     we  may        get a    true glimpse of         vamika in february 2022 when she turns 1 year old
2024-02-08 15:00:27,346 	Text Hypothesis :	people do you remember the tournament so  many find their   respective teams  in ******** **** **** *** ***** * **** ipl
2024-02-08 15:00:27,347 	Text Alignment  :	       I  S   S        S   S          S   S    S    S       S          S         D        D    D    D   D     D D    S  
2024-02-08 15:00:27,347 ========================================================================================================================
2024-02-08 15:00:27,347 Logging Sequence: 87_123.00
2024-02-08 15:00:27,347 	Gloss Reference :	A B+C+D+E
2024-02-08 15:00:27,347 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:00:27,347 	Gloss Alignment :	         
2024-02-08 15:00:27,347 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:00:27,349 	Text Reference  :	**** *** he said that he    hoped kl     rahul would       be     fit for  the upcoming world cup    
2024-02-08 15:00:27,349 	Text Hypothesis :	well let me tell you  about the   middle east  respiratory system or  mers you can      be    working
2024-02-08 15:00:27,349 	Text Alignment  :	I    I   S  S    S    S     S     S      S     S           S      S   S    S   S        S     S      
2024-02-08 15:00:27,349 ========================================================================================================================
2024-02-08 15:00:27,349 Logging Sequence: 144_154.00
2024-02-08 15:00:27,349 	Gloss Reference :	A B+C+D+E
2024-02-08 15:00:27,350 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:00:27,350 	Gloss Alignment :	         
2024-02-08 15:00:27,350 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:00:27,351 	Text Reference  :	she also participated in the rural   olympic games    organised in  rajasthan a       few   months
2024-02-08 15:00:27,351 	Text Hypothesis :	*** **** ************ on her outfits were    designed by        the luxury    company louis runs  
2024-02-08 15:00:27,351 	Text Alignment  :	D   D    D            S  S   S       S       S        S         S   S         S       S     S     
2024-02-08 15:00:27,351 ========================================================================================================================
2024-02-08 15:00:34,975 Epoch 150: Total Training Recognition Loss 0.44  Total Training Translation Loss 20.60 
2024-02-08 15:00:34,976 EPOCH 151
2024-02-08 15:00:42,250 [Epoch: 151 Step: 00010100] Batch Recognition Loss:   0.000967 => Gls Tokens per Sec:     1086 || Batch Translation Loss:   0.166962 => Txt Tokens per Sec:     2918 || Lr: 0.000100
2024-02-08 15:00:44,828 Epoch 151: Total Training Recognition Loss 0.37  Total Training Translation Loss 16.33 
2024-02-08 15:00:44,828 EPOCH 152
2024-02-08 15:00:54,684 Epoch 152: Total Training Recognition Loss 0.36  Total Training Translation Loss 13.27 
2024-02-08 15:00:54,685 EPOCH 153
2024-02-08 15:00:57,308 [Epoch: 153 Step: 00010200] Batch Recognition Loss:   0.001345 => Gls Tokens per Sec:      938 || Batch Translation Loss:   0.482170 => Txt Tokens per Sec:     2609 || Lr: 0.000100
2024-02-08 15:01:04,591 Epoch 153: Total Training Recognition Loss 0.42  Total Training Translation Loss 12.75 
2024-02-08 15:01:04,592 EPOCH 154
2024-02-08 15:01:11,667 [Epoch: 154 Step: 00010300] Batch Recognition Loss:   0.009248 => Gls Tokens per Sec:     1094 || Batch Translation Loss:   0.145564 => Txt Tokens per Sec:     2967 || Lr: 0.000100
2024-02-08 15:01:14,487 Epoch 154: Total Training Recognition Loss 0.39  Total Training Translation Loss 14.79 
2024-02-08 15:01:14,489 EPOCH 155
2024-02-08 15:01:24,738 Epoch 155: Total Training Recognition Loss 0.41  Total Training Translation Loss 19.33 
2024-02-08 15:01:24,739 EPOCH 156
2024-02-08 15:01:27,085 [Epoch: 156 Step: 00010400] Batch Recognition Loss:   0.002789 => Gls Tokens per Sec:     1023 || Batch Translation Loss:   0.132176 => Txt Tokens per Sec:     2780 || Lr: 0.000100
2024-02-08 15:01:35,093 Epoch 156: Total Training Recognition Loss 0.38  Total Training Translation Loss 21.39 
2024-02-08 15:01:35,093 EPOCH 157
2024-02-08 15:01:42,087 [Epoch: 157 Step: 00010500] Batch Recognition Loss:   0.002954 => Gls Tokens per Sec:     1098 || Batch Translation Loss:   0.091674 => Txt Tokens per Sec:     3071 || Lr: 0.000100
2024-02-08 15:01:45,013 Epoch 157: Total Training Recognition Loss 0.40  Total Training Translation Loss 18.91 
2024-02-08 15:01:45,014 EPOCH 158
2024-02-08 15:01:54,910 Epoch 158: Total Training Recognition Loss 0.34  Total Training Translation Loss 13.48 
2024-02-08 15:01:54,911 EPOCH 159
2024-02-08 15:01:56,952 [Epoch: 159 Step: 00010600] Batch Recognition Loss:   0.001586 => Gls Tokens per Sec:     1098 || Batch Translation Loss:   0.116037 => Txt Tokens per Sec:     3047 || Lr: 0.000100
2024-02-08 15:02:04,954 Epoch 159: Total Training Recognition Loss 0.39  Total Training Translation Loss 10.89 
2024-02-08 15:02:04,954 EPOCH 160
2024-02-08 15:02:12,140 [Epoch: 160 Step: 00010700] Batch Recognition Loss:   0.002604 => Gls Tokens per Sec:     1033 || Batch Translation Loss:   0.159308 => Txt Tokens per Sec:     2926 || Lr: 0.000100
2024-02-08 15:02:14,927 Epoch 160: Total Training Recognition Loss 0.39  Total Training Translation Loss 11.28 
2024-02-08 15:02:14,928 EPOCH 161
2024-02-08 15:02:24,963 Epoch 161: Total Training Recognition Loss 0.31  Total Training Translation Loss 13.60 
2024-02-08 15:02:24,964 EPOCH 162
2024-02-08 15:02:27,017 [Epoch: 162 Step: 00010800] Batch Recognition Loss:   0.002237 => Gls Tokens per Sec:     1014 || Batch Translation Loss:   0.275618 => Txt Tokens per Sec:     2833 || Lr: 0.000100
2024-02-08 15:02:34,844 Epoch 162: Total Training Recognition Loss 0.29  Total Training Translation Loss 18.69 
2024-02-08 15:02:34,844 EPOCH 163
2024-02-08 15:02:41,366 [Epoch: 163 Step: 00010900] Batch Recognition Loss:   0.032902 => Gls Tokens per Sec:     1129 || Batch Translation Loss:   0.323099 => Txt Tokens per Sec:     3093 || Lr: 0.000100
2024-02-08 15:02:44,991 Epoch 163: Total Training Recognition Loss 0.40  Total Training Translation Loss 22.48 
2024-02-08 15:02:44,992 EPOCH 164
2024-02-08 15:02:54,987 Epoch 164: Total Training Recognition Loss 0.34  Total Training Translation Loss 23.81 
2024-02-08 15:02:54,988 EPOCH 165
2024-02-08 15:02:56,545 [Epoch: 165 Step: 00011000] Batch Recognition Loss:   0.002378 => Gls Tokens per Sec:     1233 || Batch Translation Loss:   0.192426 => Txt Tokens per Sec:     3446 || Lr: 0.000100
2024-02-08 15:03:05,000 Epoch 165: Total Training Recognition Loss 0.42  Total Training Translation Loss 15.65 
2024-02-08 15:03:05,000 EPOCH 166
2024-02-08 15:03:11,616 [Epoch: 166 Step: 00011100] Batch Recognition Loss:   0.001715 => Gls Tokens per Sec:     1074 || Batch Translation Loss:   0.230032 => Txt Tokens per Sec:     2984 || Lr: 0.000100
2024-02-08 15:03:14,679 Epoch 166: Total Training Recognition Loss 0.29  Total Training Translation Loss 13.41 
2024-02-08 15:03:14,679 EPOCH 167
2024-02-08 15:03:24,597 Epoch 167: Total Training Recognition Loss 0.32  Total Training Translation Loss 12.01 
2024-02-08 15:03:24,598 EPOCH 168
2024-02-08 15:03:25,844 [Epoch: 168 Step: 00011200] Batch Recognition Loss:   0.001316 => Gls Tokens per Sec:     1413 || Batch Translation Loss:   0.123763 => Txt Tokens per Sec:     3397 || Lr: 0.000100
2024-02-08 15:03:34,755 Epoch 168: Total Training Recognition Loss 0.29  Total Training Translation Loss 12.80 
2024-02-08 15:03:34,755 EPOCH 169
2024-02-08 15:03:41,329 [Epoch: 169 Step: 00011300] Batch Recognition Loss:   0.000876 => Gls Tokens per Sec:     1056 || Batch Translation Loss:   0.227008 => Txt Tokens per Sec:     2925 || Lr: 0.000100
2024-02-08 15:03:44,902 Epoch 169: Total Training Recognition Loss 0.39  Total Training Translation Loss 10.54 
2024-02-08 15:03:44,902 EPOCH 170
2024-02-08 15:03:55,057 Epoch 170: Total Training Recognition Loss 0.37  Total Training Translation Loss 10.92 
2024-02-08 15:03:55,058 EPOCH 171
2024-02-08 15:03:55,991 [Epoch: 171 Step: 00011400] Batch Recognition Loss:   0.000864 => Gls Tokens per Sec:     1720 || Batch Translation Loss:   0.153179 => Txt Tokens per Sec:     4913 || Lr: 0.000100
2024-02-08 15:04:02,217 Epoch 171: Total Training Recognition Loss 0.34  Total Training Translation Loss 11.06 
2024-02-08 15:04:02,218 EPOCH 172
2024-02-08 15:04:07,684 [Epoch: 172 Step: 00011500] Batch Recognition Loss:   0.003129 => Gls Tokens per Sec:     1241 || Batch Translation Loss:   0.404103 => Txt Tokens per Sec:     3492 || Lr: 0.000100
2024-02-08 15:04:10,639 Epoch 172: Total Training Recognition Loss 0.32  Total Training Translation Loss 11.22 
2024-02-08 15:04:10,640 EPOCH 173
2024-02-08 15:04:19,286 Epoch 173: Total Training Recognition Loss 0.33  Total Training Translation Loss 11.59 
2024-02-08 15:04:19,286 EPOCH 174
2024-02-08 15:04:20,305 [Epoch: 174 Step: 00011600] Batch Recognition Loss:   0.001050 => Gls Tokens per Sec:     1316 || Batch Translation Loss:   0.035110 => Txt Tokens per Sec:     3591 || Lr: 0.000100
2024-02-08 15:04:27,957 Epoch 174: Total Training Recognition Loss 0.33  Total Training Translation Loss 10.72 
2024-02-08 15:04:27,958 EPOCH 175
2024-02-08 15:04:33,322 [Epoch: 175 Step: 00011700] Batch Recognition Loss:   0.002670 => Gls Tokens per Sec:     1234 || Batch Translation Loss:   0.115803 => Txt Tokens per Sec:     3342 || Lr: 0.000100
2024-02-08 15:04:36,565 Epoch 175: Total Training Recognition Loss 0.26  Total Training Translation Loss 9.16 
2024-02-08 15:04:36,565 EPOCH 176
2024-02-08 15:04:45,074 Epoch 176: Total Training Recognition Loss 0.31  Total Training Translation Loss 11.08 
2024-02-08 15:04:45,074 EPOCH 177
2024-02-08 15:04:46,173 [Epoch: 177 Step: 00011800] Batch Recognition Loss:   0.000812 => Gls Tokens per Sec:     1077 || Batch Translation Loss:   0.250238 => Txt Tokens per Sec:     3241 || Lr: 0.000100
2024-02-08 15:04:53,791 Epoch 177: Total Training Recognition Loss 0.41  Total Training Translation Loss 14.80 
2024-02-08 15:04:53,792 EPOCH 178
2024-02-08 15:04:58,940 [Epoch: 178 Step: 00011900] Batch Recognition Loss:   0.000759 => Gls Tokens per Sec:     1255 || Batch Translation Loss:   0.262197 => Txt Tokens per Sec:     3447 || Lr: 0.000100
2024-02-08 15:05:02,260 Epoch 178: Total Training Recognition Loss 0.25  Total Training Translation Loss 16.02 
2024-02-08 15:05:02,261 EPOCH 179
2024-02-08 15:05:10,916 Epoch 179: Total Training Recognition Loss 0.35  Total Training Translation Loss 17.03 
2024-02-08 15:05:10,916 EPOCH 180
2024-02-08 15:05:11,527 [Epoch: 180 Step: 00012000] Batch Recognition Loss:   0.002102 => Gls Tokens per Sec:     1836 || Batch Translation Loss:   0.148023 => Txt Tokens per Sec:     4243 || Lr: 0.000100
2024-02-08 15:05:25,117 Validation result at epoch 180, step    12000: duration: 13.5888s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 3.02282	Translation Loss: 90975.95312	PPL: 8836.89551
	Eval Metric: BLEU
	WER 5.01	(DEL: 0.00,	INS: 0.00,	SUB: 5.01)
	BLEU-4 0.37	(BLEU-1: 11.52,	BLEU-2: 3.59,	BLEU-3: 1.10,	BLEU-4: 0.37)
	CHRF 17.14	ROUGE 9.66
2024-02-08 15:05:25,118 Logging Recognition and Translation Outputs
2024-02-08 15:05:25,119 ========================================================================================================================
2024-02-08 15:05:25,119 Logging Sequence: 168_56.00
2024-02-08 15:05:25,119 	Gloss Reference :	A B+C+D+E
2024-02-08 15:05:25,120 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:05:25,120 	Gloss Alignment :	         
2024-02-08 15:05:25,120 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:05:25,121 	Text Reference  :	fans    have been waiting to   see vamika for     a    long time  
2024-02-08 15:05:25,122 	Text Hypothesis :	however some said that    they are no     clarity over the  reason
2024-02-08 15:05:25,122 	Text Alignment  :	S       S    S    S       S    S   S      S       S    S    S     
2024-02-08 15:05:25,122 ========================================================================================================================
2024-02-08 15:05:25,122 Logging Sequence: 161_74.00
2024-02-08 15:05:25,123 	Gloss Reference :	A B+C+D+E
2024-02-08 15:05:25,123 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:05:25,123 	Gloss Alignment :	         
2024-02-08 15:05:25,123 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:05:25,124 	Text Reference  :	i am   proud of  the **** indian team's   achievements
2024-02-08 15:05:25,124 	Text Hypothesis :	* this is    why the most news   agencies spotted     
2024-02-08 15:05:25,124 	Text Alignment  :	D S    S     S       I    S      S        S           
2024-02-08 15:05:25,125 ========================================================================================================================
2024-02-08 15:05:25,125 Logging Sequence: 111_83.00
2024-02-08 15:05:25,125 	Gloss Reference :	A B+C+D+E
2024-02-08 15:05:25,125 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:05:25,126 	Gloss Alignment :	         
2024-02-08 15:05:25,126 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:05:25,128 	Text Reference  :	and     the other 10    team members are     fined 25         of   the     match fee or rs  6       lakh     
2024-02-08 15:05:25,129 	Text Hypothesis :	however the ***** first time the     winning women cricketers were allowed to    pay in the winning over-rate
2024-02-08 15:05:25,129 	Text Alignment  :	S           D     S     S    S       S       S     S          S    S       S     S   S  S   S       S        
2024-02-08 15:05:25,129 ========================================================================================================================
2024-02-08 15:05:25,129 Logging Sequence: 61_218.00
2024-02-08 15:05:25,129 	Gloss Reference :	A B+C+D+E
2024-02-08 15:05:25,130 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:05:25,130 	Gloss Alignment :	         
2024-02-08 15:05:25,130 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:05:25,131 	Text Reference  :	******* ******** in    2020   a ***** ***** **** woman had   said  at the press conference
2024-02-08 15:05:25,132 	Text Hypothesis :	however recently sania shared a video along with his   insta story in the world cup       
2024-02-08 15:05:25,132 	Text Alignment  :	I       I        S     S        I     I     I    S     S     S     S      S     S         
2024-02-08 15:05:25,132 ========================================================================================================================
2024-02-08 15:05:25,132 Logging Sequence: 94_123.00
2024-02-08 15:05:25,132 	Gloss Reference :	A B+C+D+E
2024-02-08 15:05:25,133 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:05:25,133 	Gloss Alignment :	         
2024-02-08 15:05:25,133 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:05:25,136 	Text Reference  :	the venue narendra modi stadium for the  india-pakistan match has been    kept    the same people can book flights   etc 
2024-02-08 15:05:25,136 	Text Hypothesis :	*** only  1        day  earlier ie  14th october        was   not majorly disrupt the **** ****** *** most prominent over
2024-02-08 15:05:25,136 	Text Alignment  :	D   S     S        S    S       S   S    S              S     S   S       S           D    D      D   S    S         S   
2024-02-08 15:05:25,136 ========================================================================================================================
2024-02-08 15:05:33,481 Epoch 180: Total Training Recognition Loss 0.34  Total Training Translation Loss 18.38 
2024-02-08 15:05:33,481 EPOCH 181
2024-02-08 15:05:39,195 [Epoch: 181 Step: 00012100] Batch Recognition Loss:   0.008398 => Gls Tokens per Sec:     1103 || Batch Translation Loss:   0.332137 => Txt Tokens per Sec:     3144 || Lr: 0.000100
2024-02-08 15:05:42,230 Epoch 181: Total Training Recognition Loss 0.23  Total Training Translation Loss 17.96 
2024-02-08 15:05:42,230 EPOCH 182
2024-02-08 15:05:50,786 Epoch 182: Total Training Recognition Loss 0.34  Total Training Translation Loss 16.89 
2024-02-08 15:05:50,787 EPOCH 183
2024-02-08 15:05:51,494 [Epoch: 183 Step: 00012200] Batch Recognition Loss:   0.002218 => Gls Tokens per Sec:     1361 || Batch Translation Loss:   0.121495 => Txt Tokens per Sec:     3618 || Lr: 0.000100
2024-02-08 15:05:59,346 Epoch 183: Total Training Recognition Loss 0.44  Total Training Translation Loss 22.37 
2024-02-08 15:05:59,347 EPOCH 184
2024-02-08 15:06:04,724 [Epoch: 184 Step: 00012300] Batch Recognition Loss:   0.001774 => Gls Tokens per Sec:     1161 || Batch Translation Loss:   0.305939 => Txt Tokens per Sec:     3241 || Lr: 0.000100
2024-02-08 15:06:08,037 Epoch 184: Total Training Recognition Loss 0.46  Total Training Translation Loss 22.88 
2024-02-08 15:06:08,038 EPOCH 185
2024-02-08 15:06:16,646 Epoch 185: Total Training Recognition Loss 0.45  Total Training Translation Loss 18.50 
2024-02-08 15:06:16,646 EPOCH 186
2024-02-08 15:06:17,241 [Epoch: 186 Step: 00012400] Batch Recognition Loss:   0.005488 => Gls Tokens per Sec:     1348 || Batch Translation Loss:   0.133498 => Txt Tokens per Sec:     3813 || Lr: 0.000100
2024-02-08 15:06:25,364 Epoch 186: Total Training Recognition Loss 0.41  Total Training Translation Loss 13.85 
2024-02-08 15:06:25,364 EPOCH 187
2024-02-08 15:06:30,165 [Epoch: 187 Step: 00012500] Batch Recognition Loss:   0.002170 => Gls Tokens per Sec:     1267 || Batch Translation Loss:   0.403323 => Txt Tokens per Sec:     3479 || Lr: 0.000100
2024-02-08 15:06:34,061 Epoch 187: Total Training Recognition Loss 0.32  Total Training Translation Loss 9.53 
2024-02-08 15:06:34,061 EPOCH 188
2024-02-08 15:06:42,361 Epoch 188: Total Training Recognition Loss 0.33  Total Training Translation Loss 7.88 
2024-02-08 15:06:42,362 EPOCH 189
2024-02-08 15:06:42,738 [Epoch: 189 Step: 00012600] Batch Recognition Loss:   0.001797 => Gls Tokens per Sec:     1709 || Batch Translation Loss:   0.019209 => Txt Tokens per Sec:     3641 || Lr: 0.000100
2024-02-08 15:06:51,131 Epoch 189: Total Training Recognition Loss 0.29  Total Training Translation Loss 7.48 
2024-02-08 15:06:51,132 EPOCH 190
2024-02-08 15:06:55,965 [Epoch: 190 Step: 00012700] Batch Recognition Loss:   0.002972 => Gls Tokens per Sec:     1225 || Batch Translation Loss:   0.032139 => Txt Tokens per Sec:     3298 || Lr: 0.000100
2024-02-08 15:06:59,902 Epoch 190: Total Training Recognition Loss 0.26  Total Training Translation Loss 6.31 
2024-02-08 15:06:59,903 EPOCH 191
2024-02-08 15:07:08,708 Epoch 191: Total Training Recognition Loss 0.30  Total Training Translation Loss 9.42 
2024-02-08 15:07:08,708 EPOCH 192
2024-02-08 15:07:09,007 [Epoch: 192 Step: 00012800] Batch Recognition Loss:   0.000387 => Gls Tokens per Sec:     1610 || Batch Translation Loss:   0.069234 => Txt Tokens per Sec:     4081 || Lr: 0.000100
2024-02-08 15:07:17,382 Epoch 192: Total Training Recognition Loss 0.26  Total Training Translation Loss 11.50 
2024-02-08 15:07:17,382 EPOCH 193
2024-02-08 15:07:22,118 [Epoch: 193 Step: 00012900] Batch Recognition Loss:   0.017258 => Gls Tokens per Sec:     1216 || Batch Translation Loss:   0.103566 => Txt Tokens per Sec:     3322 || Lr: 0.000100
2024-02-08 15:07:26,195 Epoch 193: Total Training Recognition Loss 0.18  Total Training Translation Loss 12.94 
2024-02-08 15:07:26,196 EPOCH 194
2024-02-08 15:07:34,804 Epoch 194: Total Training Recognition Loss 0.17  Total Training Translation Loss 12.40 
2024-02-08 15:07:34,805 EPOCH 195
2024-02-08 15:07:35,298 [Epoch: 195 Step: 00013000] Batch Recognition Loss:   0.004943 => Gls Tokens per Sec:      652 || Batch Translation Loss:   0.084799 => Txt Tokens per Sec:     2061 || Lr: 0.000100
2024-02-08 15:07:43,378 Epoch 195: Total Training Recognition Loss 0.25  Total Training Translation Loss 12.76 
2024-02-08 15:07:43,379 EPOCH 196
2024-02-08 15:07:48,129 [Epoch: 196 Step: 00013100] Batch Recognition Loss:   0.003381 => Gls Tokens per Sec:     1158 || Batch Translation Loss:   0.154635 => Txt Tokens per Sec:     3171 || Lr: 0.000100
2024-02-08 15:07:52,054 Epoch 196: Total Training Recognition Loss 0.51  Total Training Translation Loss 14.81 
2024-02-08 15:07:52,054 EPOCH 197
2024-02-08 15:08:00,659 Epoch 197: Total Training Recognition Loss 1.25  Total Training Translation Loss 11.52 
2024-02-08 15:08:00,659 EPOCH 198
2024-02-08 15:08:00,758 [Epoch: 198 Step: 00013200] Batch Recognition Loss:   0.002120 => Gls Tokens per Sec:     1660 || Batch Translation Loss:   0.096349 => Txt Tokens per Sec:     4535 || Lr: 0.000100
2024-02-08 15:08:09,044 Epoch 198: Total Training Recognition Loss 1.02  Total Training Translation Loss 12.99 
2024-02-08 15:08:09,044 EPOCH 199
2024-02-08 15:08:13,393 [Epoch: 199 Step: 00013300] Batch Recognition Loss:   0.001178 => Gls Tokens per Sec:     1228 || Batch Translation Loss:   0.216685 => Txt Tokens per Sec:     3465 || Lr: 0.000100
2024-02-08 15:08:17,552 Epoch 199: Total Training Recognition Loss 0.91  Total Training Translation Loss 15.79 
2024-02-08 15:08:17,553 EPOCH 200
2024-02-08 15:08:26,058 [Epoch: 200 Step: 00013400] Batch Recognition Loss:   0.002551 => Gls Tokens per Sec:     1249 || Batch Translation Loss:   0.180565 => Txt Tokens per Sec:     3455 || Lr: 0.000100
2024-02-08 15:08:26,059 Epoch 200: Total Training Recognition Loss 0.58  Total Training Translation Loss 14.33 
2024-02-08 15:08:26,059 EPOCH 201
2024-02-08 15:08:34,802 Epoch 201: Total Training Recognition Loss 0.32  Total Training Translation Loss 10.49 
2024-02-08 15:08:34,802 EPOCH 202
2024-02-08 15:08:39,029 [Epoch: 202 Step: 00013500] Batch Recognition Loss:   0.000552 => Gls Tokens per Sec:     1250 || Batch Translation Loss:   0.259098 => Txt Tokens per Sec:     3345 || Lr: 0.000100
2024-02-08 15:08:43,711 Epoch 202: Total Training Recognition Loss 0.31  Total Training Translation Loss 8.91 
2024-02-08 15:08:43,712 EPOCH 203
2024-02-08 15:08:52,204 [Epoch: 203 Step: 00013600] Batch Recognition Loss:   0.003502 => Gls Tokens per Sec:     1232 || Batch Translation Loss:   0.060978 => Txt Tokens per Sec:     3421 || Lr: 0.000100
2024-02-08 15:08:52,279 Epoch 203: Total Training Recognition Loss 0.26  Total Training Translation Loss 10.28 
2024-02-08 15:08:52,279 EPOCH 204
2024-02-08 15:09:00,943 Epoch 204: Total Training Recognition Loss 0.25  Total Training Translation Loss 13.93 
2024-02-08 15:09:00,943 EPOCH 205
2024-02-08 15:09:05,109 [Epoch: 205 Step: 00013700] Batch Recognition Loss:   0.000354 => Gls Tokens per Sec:     1229 || Batch Translation Loss:   0.472718 => Txt Tokens per Sec:     3439 || Lr: 0.000100
2024-02-08 15:09:09,309 Epoch 205: Total Training Recognition Loss 0.21  Total Training Translation Loss 14.58 
2024-02-08 15:09:09,309 EPOCH 206
2024-02-08 15:09:18,002 [Epoch: 206 Step: 00013800] Batch Recognition Loss:   0.000575 => Gls Tokens per Sec:     1185 || Batch Translation Loss:   0.147653 => Txt Tokens per Sec:     3263 || Lr: 0.000100
2024-02-08 15:09:18,306 Epoch 206: Total Training Recognition Loss 0.24  Total Training Translation Loss 14.31 
2024-02-08 15:09:18,306 EPOCH 207
2024-02-08 15:09:26,936 Epoch 207: Total Training Recognition Loss 0.25  Total Training Translation Loss 12.07 
2024-02-08 15:09:26,937 EPOCH 208
2024-02-08 15:09:30,676 [Epoch: 208 Step: 00013900] Batch Recognition Loss:   0.000592 => Gls Tokens per Sec:     1300 || Batch Translation Loss:   0.221235 => Txt Tokens per Sec:     3615 || Lr: 0.000100
2024-02-08 15:09:33,934 Epoch 208: Total Training Recognition Loss 0.30  Total Training Translation Loss 13.07 
2024-02-08 15:09:33,935 EPOCH 209
2024-02-08 15:09:40,421 [Epoch: 209 Step: 00014000] Batch Recognition Loss:   0.000773 => Gls Tokens per Sec:     1563 || Batch Translation Loss:   0.133479 => Txt Tokens per Sec:     4358 || Lr: 0.000100
2024-02-08 15:09:50,706 Validation result at epoch 209, step    14000: duration: 10.2845s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.86102	Translation Loss: 92108.77344	PPL: 9895.52246
	Eval Metric: BLEU
	WER 4.73	(DEL: 0.00,	INS: 0.00,	SUB: 4.73)
	BLEU-4 0.56	(BLEU-1: 11.07,	BLEU-2: 3.37,	BLEU-3: 1.33,	BLEU-4: 0.56)
	CHRF 16.72	ROUGE 9.34
2024-02-08 15:09:50,708 Logging Recognition and Translation Outputs
2024-02-08 15:09:50,708 ========================================================================================================================
2024-02-08 15:09:50,708 Logging Sequence: 177_50.00
2024-02-08 15:09:50,708 	Gloss Reference :	A B+C+D+E    
2024-02-08 15:09:50,708 	Gloss Hypothesis:	A B+C+D+E+D+E
2024-02-08 15:09:50,709 	Gloss Alignment :	  S          
2024-02-08 15:09:50,709 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:09:50,710 	Text Reference  :	***** a   similar  reward of    rs     50000 was announced    for    information against his associate ajay   kumar
2024-02-08 15:09:50,710 	Text Hypothesis :	after the olympics the    delhi police filed a   non-bailable arrest warrant     against *** ********* sushil kumar
2024-02-08 15:09:50,710 	Text Alignment  :	I     S   S        S      S     S      S     S   S            S      S                   D   D         S           
2024-02-08 15:09:50,710 ========================================================================================================================
2024-02-08 15:09:50,711 Logging Sequence: 136_175.00
2024-02-08 15:09:50,711 	Gloss Reference :	A B+C+D+E  
2024-02-08 15:09:50,711 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-08 15:09:50,711 	Gloss Alignment :	  S        
2024-02-08 15:09:50,711 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:09:50,712 	Text Reference  :	after 49 years india' hockey team beat   britain and qualified for  the    semi-finals
2024-02-08 15:09:50,712 	Text Hypothesis :	***** ** she   has    won    a    bronze medal   at  the       2012 london olympics   
2024-02-08 15:09:50,712 	Text Alignment  :	D     D  S     S      S      S    S      S       S   S         S    S      S          
2024-02-08 15:09:50,713 ========================================================================================================================
2024-02-08 15:09:50,713 Logging Sequence: 126_159.00
2024-02-08 15:09:50,713 	Gloss Reference :	A B+C+D+E
2024-02-08 15:09:50,713 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:09:50,713 	Gloss Alignment :	         
2024-02-08 15:09:50,713 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:09:50,714 	Text Reference  :	despite multiple challenges and injuries you   did not give  up 
2024-02-08 15:09:50,714 	Text Hypothesis :	******* ******** he         won a        medal in  the medal wow
2024-02-08 15:09:50,714 	Text Alignment  :	D       D        S          S   S        S     S   S   S     S  
2024-02-08 15:09:50,715 ========================================================================================================================
2024-02-08 15:09:50,715 Logging Sequence: 70_88.00
2024-02-08 15:09:50,715 	Gloss Reference :	A B+C+D+E
2024-02-08 15:09:50,715 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:09:50,715 	Gloss Alignment :	         
2024-02-08 15:09:50,715 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:09:50,716 	Text Reference  :	******** *** two coca-cola bottles    were placed on   the table next   to the mic  
2024-02-08 15:09:50,716 	Text Hypothesis :	actually won the press     conference in   euro   2020 is  also  placed in her press
2024-02-08 15:09:50,716 	Text Alignment  :	I        I   S   S         S          S    S      S    S   S     S      S  S   S    
2024-02-08 15:09:50,716 ========================================================================================================================
2024-02-08 15:09:50,716 Logging Sequence: 54_201.00
2024-02-08 15:09:50,717 	Gloss Reference :	A B+C+D+E    
2024-02-08 15:09:50,717 	Gloss Hypothesis:	A B+C+D+E+D+E
2024-02-08 15:09:50,717 	Gloss Alignment :	  S          
2024-02-08 15:09:50,717 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:09:50,719 	Text Reference  :	there is a huge demand mostly from non-resident indians nris who are excited to see the match and they have booked the hotel rooms
2024-02-08 15:09:50,719 	Text Hypothesis :	***** ** * **** ****** ****** **** we           have    won  icc has married to *** *** ***** *** **** know in     the 2016  games
2024-02-08 15:09:50,719 	Text Alignment  :	D     D  D D    D      D      D    S            S       S    S   S   S          D   D   D     D   D    S    S          S     S    
2024-02-08 15:09:50,719 ========================================================================================================================
2024-02-08 15:09:50,928 Epoch 209: Total Training Recognition Loss 0.26  Total Training Translation Loss 10.83 
2024-02-08 15:09:50,928 EPOCH 210
2024-02-08 15:09:57,827 Epoch 210: Total Training Recognition Loss 0.27  Total Training Translation Loss 9.01 
2024-02-08 15:09:57,828 EPOCH 211
2024-02-08 15:10:00,808 [Epoch: 211 Step: 00014100] Batch Recognition Loss:   0.001471 => Gls Tokens per Sec:     1577 || Batch Translation Loss:   0.106146 => Txt Tokens per Sec:     4525 || Lr: 0.000100
2024-02-08 15:10:04,394 Epoch 211: Total Training Recognition Loss 0.22  Total Training Translation Loss 7.06 
2024-02-08 15:10:04,395 EPOCH 212
2024-02-08 15:10:10,863 [Epoch: 212 Step: 00014200] Batch Recognition Loss:   0.004384 => Gls Tokens per Sec:     1543 || Batch Translation Loss:   0.088227 => Txt Tokens per Sec:     4293 || Lr: 0.000100
2024-02-08 15:10:11,171 Epoch 212: Total Training Recognition Loss 0.18  Total Training Translation Loss 8.38 
2024-02-08 15:10:11,172 EPOCH 213
2024-02-08 15:10:17,221 Epoch 213: Total Training Recognition Loss 0.18  Total Training Translation Loss 11.73 
2024-02-08 15:10:17,221 EPOCH 214
2024-02-08 15:10:20,082 [Epoch: 214 Step: 00014300] Batch Recognition Loss:   0.000795 => Gls Tokens per Sec:     1623 || Batch Translation Loss:   0.057086 => Txt Tokens per Sec:     4384 || Lr: 0.000100
2024-02-08 15:10:23,844 Epoch 214: Total Training Recognition Loss 0.26  Total Training Translation Loss 11.01 
2024-02-08 15:10:23,844 EPOCH 215
2024-02-08 15:10:29,771 [Epoch: 215 Step: 00014400] Batch Recognition Loss:   0.002544 => Gls Tokens per Sec:     1657 || Batch Translation Loss:   0.106924 => Txt Tokens per Sec:     4598 || Lr: 0.000100
2024-02-08 15:10:30,212 Epoch 215: Total Training Recognition Loss 0.24  Total Training Translation Loss 13.68 
2024-02-08 15:10:30,212 EPOCH 216
2024-02-08 15:10:36,645 Epoch 216: Total Training Recognition Loss 0.28  Total Training Translation Loss 11.89 
2024-02-08 15:10:36,645 EPOCH 217
2024-02-08 15:10:39,474 [Epoch: 217 Step: 00014500] Batch Recognition Loss:   0.002497 => Gls Tokens per Sec:     1549 || Batch Translation Loss:   0.148609 => Txt Tokens per Sec:     4238 || Lr: 0.000100
2024-02-08 15:10:43,325 Epoch 217: Total Training Recognition Loss 0.22  Total Training Translation Loss 11.88 
2024-02-08 15:10:43,326 EPOCH 218
2024-02-08 15:10:49,089 [Epoch: 218 Step: 00014600] Batch Recognition Loss:   0.001872 => Gls Tokens per Sec:     1676 || Batch Translation Loss:   0.150149 => Txt Tokens per Sec:     4649 || Lr: 0.000100
2024-02-08 15:10:49,687 Epoch 218: Total Training Recognition Loss 0.26  Total Training Translation Loss 12.85 
2024-02-08 15:10:49,687 EPOCH 219
2024-02-08 15:10:56,274 Epoch 219: Total Training Recognition Loss 0.24  Total Training Translation Loss 10.29 
2024-02-08 15:10:56,275 EPOCH 220
2024-02-08 15:10:58,526 [Epoch: 220 Step: 00014700] Batch Recognition Loss:   0.006855 => Gls Tokens per Sec:     1921 || Batch Translation Loss:   0.352102 => Txt Tokens per Sec:     5334 || Lr: 0.000100
2024-02-08 15:11:02,558 Epoch 220: Total Training Recognition Loss 0.23  Total Training Translation Loss 12.02 
2024-02-08 15:11:02,559 EPOCH 221
2024-02-08 15:11:08,625 [Epoch: 221 Step: 00014800] Batch Recognition Loss:   0.000606 => Gls Tokens per Sec:     1583 || Batch Translation Loss:   0.231499 => Txt Tokens per Sec:     4427 || Lr: 0.000100
2024-02-08 15:11:09,261 Epoch 221: Total Training Recognition Loss 0.25  Total Training Translation Loss 11.27 
2024-02-08 15:11:09,262 EPOCH 222
2024-02-08 15:11:15,904 Epoch 222: Total Training Recognition Loss 0.15  Total Training Translation Loss 10.58 
2024-02-08 15:11:15,905 EPOCH 223
2024-02-08 15:11:18,398 [Epoch: 223 Step: 00014900] Batch Recognition Loss:   0.000682 => Gls Tokens per Sec:     1669 || Batch Translation Loss:   0.177111 => Txt Tokens per Sec:     4791 || Lr: 0.000100
2024-02-08 15:11:22,064 Epoch 223: Total Training Recognition Loss 0.22  Total Training Translation Loss 11.51 
2024-02-08 15:11:22,065 EPOCH 224
2024-02-08 15:11:27,864 [Epoch: 224 Step: 00015000] Batch Recognition Loss:   0.000481 => Gls Tokens per Sec:     1611 || Batch Translation Loss:   0.169994 => Txt Tokens per Sec:     4460 || Lr: 0.000100
2024-02-08 15:11:28,733 Epoch 224: Total Training Recognition Loss 0.28  Total Training Translation Loss 10.60 
2024-02-08 15:11:28,733 EPOCH 225
2024-02-08 15:11:34,943 Epoch 225: Total Training Recognition Loss 0.21  Total Training Translation Loss 8.50 
2024-02-08 15:11:34,944 EPOCH 226
2024-02-08 15:11:37,546 [Epoch: 226 Step: 00015100] Batch Recognition Loss:   0.000798 => Gls Tokens per Sec:     1538 || Batch Translation Loss:   0.086209 => Txt Tokens per Sec:     4312 || Lr: 0.000100
2024-02-08 15:11:41,643 Epoch 226: Total Training Recognition Loss 0.19  Total Training Translation Loss 10.40 
2024-02-08 15:11:41,643 EPOCH 227
2024-02-08 15:11:47,214 [Epoch: 227 Step: 00015200] Batch Recognition Loss:   0.006057 => Gls Tokens per Sec:     1648 || Batch Translation Loss:   0.165737 => Txt Tokens per Sec:     4504 || Lr: 0.000100
2024-02-08 15:11:48,160 Epoch 227: Total Training Recognition Loss 0.28  Total Training Translation Loss 12.09 
2024-02-08 15:11:48,160 EPOCH 228
2024-02-08 15:11:54,458 Epoch 228: Total Training Recognition Loss 0.28  Total Training Translation Loss 12.66 
2024-02-08 15:11:54,459 EPOCH 229
2024-02-08 15:11:56,978 [Epoch: 229 Step: 00015300] Batch Recognition Loss:   0.001259 => Gls Tokens per Sec:     1485 || Batch Translation Loss:   0.119413 => Txt Tokens per Sec:     4028 || Lr: 0.000100
2024-02-08 15:12:01,333 Epoch 229: Total Training Recognition Loss 0.28  Total Training Translation Loss 10.34 
2024-02-08 15:12:01,334 EPOCH 230
2024-02-08 15:12:06,866 [Epoch: 230 Step: 00015400] Batch Recognition Loss:   0.001275 => Gls Tokens per Sec:     1631 || Batch Translation Loss:   0.308605 => Txt Tokens per Sec:     4562 || Lr: 0.000100
2024-02-08 15:12:07,760 Epoch 230: Total Training Recognition Loss 0.25  Total Training Translation Loss 12.43 
2024-02-08 15:12:07,760 EPOCH 231
2024-02-08 15:12:14,124 Epoch 231: Total Training Recognition Loss 0.25  Total Training Translation Loss 11.31 
2024-02-08 15:12:14,124 EPOCH 232
2024-02-08 15:12:16,785 [Epoch: 232 Step: 00015500] Batch Recognition Loss:   0.001803 => Gls Tokens per Sec:     1347 || Batch Translation Loss:   0.303200 => Txt Tokens per Sec:     3908 || Lr: 0.000100
2024-02-08 15:12:20,756 Epoch 232: Total Training Recognition Loss 0.18  Total Training Translation Loss 11.25 
2024-02-08 15:12:20,757 EPOCH 233
2024-02-08 15:12:26,354 [Epoch: 233 Step: 00015600] Batch Recognition Loss:   0.001517 => Gls Tokens per Sec:     1601 || Batch Translation Loss:   0.063191 => Txt Tokens per Sec:     4389 || Lr: 0.000100
2024-02-08 15:12:27,559 Epoch 233: Total Training Recognition Loss 0.19  Total Training Translation Loss 10.96 
2024-02-08 15:12:27,559 EPOCH 234
2024-02-08 15:12:34,105 Epoch 234: Total Training Recognition Loss 0.18  Total Training Translation Loss 11.29 
2024-02-08 15:12:34,106 EPOCH 235
2024-02-08 15:12:36,372 [Epoch: 235 Step: 00015700] Batch Recognition Loss:   0.000281 => Gls Tokens per Sec:     1554 || Batch Translation Loss:   0.127060 => Txt Tokens per Sec:     4350 || Lr: 0.000100
2024-02-08 15:12:40,856 Epoch 235: Total Training Recognition Loss 0.18  Total Training Translation Loss 8.80 
2024-02-08 15:12:40,856 EPOCH 236
2024-02-08 15:12:46,189 [Epoch: 236 Step: 00015800] Batch Recognition Loss:   0.001675 => Gls Tokens per Sec:     1632 || Batch Translation Loss:   0.090472 => Txt Tokens per Sec:     4479 || Lr: 0.000100
2024-02-08 15:12:47,332 Epoch 236: Total Training Recognition Loss 0.24  Total Training Translation Loss 9.48 
2024-02-08 15:12:47,332 EPOCH 237
2024-02-08 15:12:53,851 Epoch 237: Total Training Recognition Loss 0.23  Total Training Translation Loss 9.73 
2024-02-08 15:12:53,852 EPOCH 238
2024-02-08 15:12:55,815 [Epoch: 238 Step: 00015900] Batch Recognition Loss:   0.000649 => Gls Tokens per Sec:     1713 || Batch Translation Loss:   0.360661 => Txt Tokens per Sec:     4622 || Lr: 0.000100
2024-02-08 15:13:00,622 Epoch 238: Total Training Recognition Loss 0.19  Total Training Translation Loss 8.27 
2024-02-08 15:13:00,623 EPOCH 239
2024-02-08 15:13:05,553 [Epoch: 239 Step: 00016000] Batch Recognition Loss:   0.002935 => Gls Tokens per Sec:     1732 || Batch Translation Loss:   0.089444 => Txt Tokens per Sec:     4841 || Lr: 0.000100
2024-02-08 15:13:15,691 Validation result at epoch 239, step    16000: duration: 10.1370s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 3.45317	Translation Loss: 93788.28125	PPL: 11702.84375
	Eval Metric: BLEU
	WER 4.94	(DEL: 0.00,	INS: 0.00,	SUB: 4.94)
	BLEU-4 0.51	(BLEU-1: 11.27,	BLEU-2: 3.28,	BLEU-3: 1.18,	BLEU-4: 0.51)
	CHRF 16.79	ROUGE 9.53
2024-02-08 15:13:15,692 Logging Recognition and Translation Outputs
2024-02-08 15:13:15,692 ========================================================================================================================
2024-02-08 15:13:15,692 Logging Sequence: 163_116.00
2024-02-08 15:13:15,692 	Gloss Reference :	A B+C+D+E
2024-02-08 15:13:15,693 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:13:15,693 	Gloss Alignment :	         
2024-02-08 15:13:15,693 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:13:15,694 	Text Reference  :	******** ****** ***** ***** ******** people said  that she looked similar to **** *** virat  
2024-02-08 15:13:15,694 	Text Hypothesis :	whenever anyone talks about pictures of     their son  and some   staff   to wait and updates
2024-02-08 15:13:15,694 	Text Alignment  :	I        I      I     I     I        S      S     S    S   S      S          I    I   S      
2024-02-08 15:13:15,694 ========================================================================================================================
2024-02-08 15:13:15,694 Logging Sequence: 53_161.00
2024-02-08 15:13:15,694 	Gloss Reference :	A B+C+D+E
2024-02-08 15:13:15,695 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:13:15,695 	Gloss Alignment :	         
2024-02-08 15:13:15,695 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:13:15,696 	Text Reference  :	rashid has also been urging people    to donate to his   rashid khan     foundation and afghanistan cricket association
2024-02-08 15:13:15,696 	Text Hypothesis :	****** *** **** **** now    according to ****** a  habit of     shooting in         the 2008        olympic games      
2024-02-08 15:13:15,696 	Text Alignment  :	D      D   D    D    S      S            D      S  S     S      S        S          S   S           S       S          
2024-02-08 15:13:15,696 ========================================================================================================================
2024-02-08 15:13:15,696 Logging Sequence: 67_73.00
2024-02-08 15:13:15,696 	Gloss Reference :	A B+C+D+E
2024-02-08 15:13:15,697 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:13:15,697 	Gloss Alignment :	         
2024-02-08 15:13:15,697 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:13:15,698 	Text Reference  :	*** **** **** *** ******* in     his tweet he    also said
2024-02-08 15:13:15,698 	Text Hypothesis :	she said that the taliban wanted to  find  their own  etc 
2024-02-08 15:13:15,698 	Text Alignment  :	I   I    I    I   I       S      S   S     S     S    S   
2024-02-08 15:13:15,698 ========================================================================================================================
2024-02-08 15:13:15,698 Logging Sequence: 137_44.00
2024-02-08 15:13:15,698 	Gloss Reference :	A B+C+D+E
2024-02-08 15:13:15,699 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:13:15,699 	Gloss Alignment :	         
2024-02-08 15:13:15,699 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:13:15,700 	Text Reference  :	let me tell you the rules that qatar has announced for  the fans travelling for the world cup 
2024-02-08 15:13:15,700 	Text Hypothesis :	*** ** **** *** *** ***** **** ***** he  then      went to  be   played     at  the state team
2024-02-08 15:13:15,700 	Text Alignment  :	D   D  D    D   D   D     D    D     S   S         S    S   S    S          S       S     S   
2024-02-08 15:13:15,700 ========================================================================================================================
2024-02-08 15:13:15,700 Logging Sequence: 99_158.00
2024-02-08 15:13:15,700 	Gloss Reference :	A B+C+D+E
2024-02-08 15:13:15,700 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:13:15,702 	Gloss Alignment :	         
2024-02-08 15:13:15,702 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:13:15,703 	Text Reference  :	*** **** ** the incident occured in dubai and  it    was extremely shameful
2024-02-08 15:13:15,703 	Text Hypothesis :	his fans to see him      however he has   been going to  these     match   
2024-02-08 15:13:15,703 	Text Alignment  :	I   I    I  S   S        S       S  S     S    S     S   S         S       
2024-02-08 15:13:15,703 ========================================================================================================================
2024-02-08 15:13:16,973 Epoch 239: Total Training Recognition Loss 0.15  Total Training Translation Loss 8.40 
2024-02-08 15:13:16,973 EPOCH 240
2024-02-08 15:13:23,746 Epoch 240: Total Training Recognition Loss 0.16  Total Training Translation Loss 8.98 
2024-02-08 15:13:23,747 EPOCH 241
2024-02-08 15:13:25,422 [Epoch: 241 Step: 00016100] Batch Recognition Loss:   0.000486 => Gls Tokens per Sec:     1913 || Batch Translation Loss:   0.034568 => Txt Tokens per Sec:     5417 || Lr: 0.000100
2024-02-08 15:13:30,215 Epoch 241: Total Training Recognition Loss 0.27  Total Training Translation Loss 7.11 
2024-02-08 15:13:30,215 EPOCH 242
2024-02-08 15:13:35,179 [Epoch: 242 Step: 00016200] Batch Recognition Loss:   0.001131 => Gls Tokens per Sec:     1689 || Batch Translation Loss:   0.135403 => Txt Tokens per Sec:     4718 || Lr: 0.000100
2024-02-08 15:13:36,458 Epoch 242: Total Training Recognition Loss 0.15  Total Training Translation Loss 6.11 
2024-02-08 15:13:36,458 EPOCH 243
2024-02-08 15:13:43,401 Epoch 243: Total Training Recognition Loss 0.22  Total Training Translation Loss 9.91 
2024-02-08 15:13:43,401 EPOCH 244
2024-02-08 15:13:45,303 [Epoch: 244 Step: 00016300] Batch Recognition Loss:   0.000849 => Gls Tokens per Sec:     1600 || Batch Translation Loss:   0.077171 => Txt Tokens per Sec:     4355 || Lr: 0.000100
2024-02-08 15:13:49,920 Epoch 244: Total Training Recognition Loss 0.17  Total Training Translation Loss 11.28 
2024-02-08 15:13:49,921 EPOCH 245
2024-02-08 15:13:55,212 [Epoch: 245 Step: 00016400] Batch Recognition Loss:   0.003711 => Gls Tokens per Sec:     1554 || Batch Translation Loss:   0.284034 => Txt Tokens per Sec:     4317 || Lr: 0.000100
2024-02-08 15:13:56,716 Epoch 245: Total Training Recognition Loss 0.41  Total Training Translation Loss 13.41 
2024-02-08 15:13:56,716 EPOCH 246
2024-02-08 15:14:03,313 Epoch 246: Total Training Recognition Loss 0.26  Total Training Translation Loss 11.59 
2024-02-08 15:14:03,314 EPOCH 247
2024-02-08 15:14:05,015 [Epoch: 247 Step: 00016500] Batch Recognition Loss:   0.000775 => Gls Tokens per Sec:     1695 || Batch Translation Loss:   0.178640 => Txt Tokens per Sec:     4520 || Lr: 0.000100
2024-02-08 15:14:09,974 Epoch 247: Total Training Recognition Loss 0.32  Total Training Translation Loss 12.24 
2024-02-08 15:14:09,975 EPOCH 248
2024-02-08 15:14:15,219 [Epoch: 248 Step: 00016600] Batch Recognition Loss:   0.000368 => Gls Tokens per Sec:     1537 || Batch Translation Loss:   0.061993 => Txt Tokens per Sec:     4280 || Lr: 0.000100
2024-02-08 15:14:16,628 Epoch 248: Total Training Recognition Loss 0.27  Total Training Translation Loss 8.47 
2024-02-08 15:14:16,628 EPOCH 249
2024-02-08 15:14:22,689 Epoch 249: Total Training Recognition Loss 0.31  Total Training Translation Loss 8.60 
2024-02-08 15:14:22,689 EPOCH 250
2024-02-08 15:14:24,018 [Epoch: 250 Step: 00016700] Batch Recognition Loss:   0.000225 => Gls Tokens per Sec:     2048 || Batch Translation Loss:   0.149486 => Txt Tokens per Sec:     5955 || Lr: 0.000100
2024-02-08 15:14:27,769 Epoch 250: Total Training Recognition Loss 0.32  Total Training Translation Loss 9.91 
2024-02-08 15:14:27,769 EPOCH 251
2024-02-08 15:14:31,896 [Epoch: 251 Step: 00016800] Batch Recognition Loss:   0.000322 => Gls Tokens per Sec:     1939 || Batch Translation Loss:   0.306806 => Txt Tokens per Sec:     5308 || Lr: 0.000100
2024-02-08 15:14:33,483 Epoch 251: Total Training Recognition Loss 0.18  Total Training Translation Loss 9.77 
2024-02-08 15:14:33,483 EPOCH 252
2024-02-08 15:14:39,151 Epoch 252: Total Training Recognition Loss 0.17  Total Training Translation Loss 6.95 
2024-02-08 15:14:39,152 EPOCH 253
2024-02-08 15:14:40,474 [Epoch: 253 Step: 00016900] Batch Recognition Loss:   0.000512 => Gls Tokens per Sec:     1862 || Batch Translation Loss:   0.053641 => Txt Tokens per Sec:     5088 || Lr: 0.000100
2024-02-08 15:14:44,653 Epoch 253: Total Training Recognition Loss 0.13  Total Training Translation Loss 6.99 
2024-02-08 15:14:44,654 EPOCH 254
2024-02-08 15:14:48,916 [Epoch: 254 Step: 00017000] Batch Recognition Loss:   0.000630 => Gls Tokens per Sec:     1816 || Batch Translation Loss:   0.073219 => Txt Tokens per Sec:     5079 || Lr: 0.000100
2024-02-08 15:14:50,283 Epoch 254: Total Training Recognition Loss 0.12  Total Training Translation Loss 7.09 
2024-02-08 15:14:50,284 EPOCH 255
2024-02-08 15:14:55,335 Epoch 255: Total Training Recognition Loss 0.12  Total Training Translation Loss 7.79 
2024-02-08 15:14:55,336 EPOCH 256
2024-02-08 15:14:56,280 [Epoch: 256 Step: 00017100] Batch Recognition Loss:   0.000288 => Gls Tokens per Sec:     2550 || Batch Translation Loss:   0.058739 => Txt Tokens per Sec:     6724 || Lr: 0.000100
2024-02-08 15:15:00,636 Epoch 256: Total Training Recognition Loss 0.13  Total Training Translation Loss 5.16 
2024-02-08 15:15:00,637 EPOCH 257
2024-02-08 15:15:04,260 [Epoch: 257 Step: 00017200] Batch Recognition Loss:   0.004213 => Gls Tokens per Sec:     2120 || Batch Translation Loss:   0.053858 => Txt Tokens per Sec:     5781 || Lr: 0.000100
2024-02-08 15:15:05,985 Epoch 257: Total Training Recognition Loss 0.22  Total Training Translation Loss 7.82 
2024-02-08 15:15:05,985 EPOCH 258
2024-02-08 15:15:10,954 Epoch 258: Total Training Recognition Loss 0.23  Total Training Translation Loss 9.65 
2024-02-08 15:15:10,955 EPOCH 259
2024-02-08 15:15:12,290 [Epoch: 259 Step: 00017300] Batch Recognition Loss:   0.014359 => Gls Tokens per Sec:     1605 || Batch Translation Loss:   0.134162 => Txt Tokens per Sec:     4619 || Lr: 0.000100
2024-02-08 15:15:16,472 Epoch 259: Total Training Recognition Loss 0.29  Total Training Translation Loss 8.30 
2024-02-08 15:15:16,472 EPOCH 260
2024-02-08 15:15:19,901 [Epoch: 260 Step: 00017400] Batch Recognition Loss:   0.002182 => Gls Tokens per Sec:     2164 || Batch Translation Loss:   0.052889 => Txt Tokens per Sec:     6125 || Lr: 0.000100
2024-02-08 15:15:21,319 Epoch 260: Total Training Recognition Loss 0.18  Total Training Translation Loss 7.77 
2024-02-08 15:15:21,320 EPOCH 261
2024-02-08 15:15:26,923 Epoch 261: Total Training Recognition Loss 0.18  Total Training Translation Loss 7.59 
2024-02-08 15:15:26,923 EPOCH 262
2024-02-08 15:15:27,824 [Epoch: 262 Step: 00017500] Batch Recognition Loss:   0.001359 => Gls Tokens per Sec:     2310 || Batch Translation Loss:   0.103694 => Txt Tokens per Sec:     6449 || Lr: 0.000100
2024-02-08 15:15:32,274 Epoch 262: Total Training Recognition Loss 0.26  Total Training Translation Loss 11.58 
2024-02-08 15:15:32,275 EPOCH 263
2024-02-08 15:15:35,979 [Epoch: 263 Step: 00017600] Batch Recognition Loss:   0.000848 => Gls Tokens per Sec:     1960 || Batch Translation Loss:   0.087794 => Txt Tokens per Sec:     5471 || Lr: 0.000100
2024-02-08 15:15:37,764 Epoch 263: Total Training Recognition Loss 0.27  Total Training Translation Loss 14.76 
2024-02-08 15:15:37,764 EPOCH 264
2024-02-08 15:15:42,959 Epoch 264: Total Training Recognition Loss 0.22  Total Training Translation Loss 16.44 
2024-02-08 15:15:42,960 EPOCH 265
2024-02-08 15:15:43,857 [Epoch: 265 Step: 00017700] Batch Recognition Loss:   0.001153 => Gls Tokens per Sec:     2144 || Batch Translation Loss:   0.116295 => Txt Tokens per Sec:     5985 || Lr: 0.000100
2024-02-08 15:15:48,462 Epoch 265: Total Training Recognition Loss 0.25  Total Training Translation Loss 13.76 
2024-02-08 15:15:48,462 EPOCH 266
2024-02-08 15:15:52,157 [Epoch: 266 Step: 00017800] Batch Recognition Loss:   0.000606 => Gls Tokens per Sec:     1922 || Batch Translation Loss:   0.039006 => Txt Tokens per Sec:     5300 || Lr: 0.000100
2024-02-08 15:15:53,905 Epoch 266: Total Training Recognition Loss 0.23  Total Training Translation Loss 7.68 
2024-02-08 15:15:53,905 EPOCH 267
2024-02-08 15:15:59,411 Epoch 267: Total Training Recognition Loss 0.19  Total Training Translation Loss 7.74 
2024-02-08 15:15:59,412 EPOCH 268
2024-02-08 15:16:00,137 [Epoch: 268 Step: 00017900] Batch Recognition Loss:   0.001109 => Gls Tokens per Sec:     2434 || Batch Translation Loss:   0.256992 => Txt Tokens per Sec:     6741 || Lr: 0.000100
2024-02-08 15:16:04,894 Epoch 268: Total Training Recognition Loss 0.21  Total Training Translation Loss 8.80 
2024-02-08 15:16:04,895 EPOCH 269
2024-02-08 15:16:08,479 [Epoch: 269 Step: 00018000] Batch Recognition Loss:   0.001071 => Gls Tokens per Sec:     1937 || Batch Translation Loss:   0.090831 => Txt Tokens per Sec:     5420 || Lr: 0.000100
2024-02-08 15:16:17,160 Validation result at epoch 269, step    18000: duration: 8.6806s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 3.03277	Translation Loss: 93732.18750	PPL: 11637.45703
	Eval Metric: BLEU
	WER 4.59	(DEL: 0.00,	INS: 0.00,	SUB: 4.59)
	BLEU-4 0.44	(BLEU-1: 10.25,	BLEU-2: 3.03,	BLEU-3: 1.09,	BLEU-4: 0.44)
	CHRF 17.14	ROUGE 8.57
2024-02-08 15:16:17,161 Logging Recognition and Translation Outputs
2024-02-08 15:16:17,161 ========================================================================================================================
2024-02-08 15:16:17,161 Logging Sequence: 179_309.00
2024-02-08 15:16:17,162 	Gloss Reference :	A B+C+D+E
2024-02-08 15:16:17,162 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:16:17,162 	Gloss Alignment :	         
2024-02-08 15:16:17,162 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:16:17,163 	Text Reference  :	before the ioa could send the    notice wfi   has asked phogat to explain her  indiscipline
2024-02-08 15:16:17,163 	Text Hypothesis :	****** *** we  could not  travel to     delhi as  there was    no one     more controversy 
2024-02-08 15:16:17,163 	Text Alignment  :	D      D   S         S    S      S      S     S   S     S      S  S       S    S           
2024-02-08 15:16:17,164 ========================================================================================================================
2024-02-08 15:16:17,164 Logging Sequence: 156_35.00
2024-02-08 15:16:17,164 	Gloss Reference :	A B+C+D+E
2024-02-08 15:16:17,164 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:16:17,164 	Gloss Alignment :	         
2024-02-08 15:16:17,164 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:16:17,167 	Text Reference  :	the     first  season of mlc   began on        13th  july 2023 and  ended   on   30th   july 2023 with   six teams
2024-02-08 15:16:17,167 	Text Hypothesis :	kolkata knight riders is owned by    bollywood actor shah rukh khan actress juhi chawla and  her  spouse jay mehta
2024-02-08 15:16:17,167 	Text Alignment  :	S       S      S      S  S     S     S         S     S    S    S    S       S    S      S    S    S      S   S    
2024-02-08 15:16:17,167 ========================================================================================================================
2024-02-08 15:16:17,167 Logging Sequence: 129_45.00
2024-02-08 15:16:17,167 	Gloss Reference :	A B+C+D+E
2024-02-08 15:16:17,168 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:16:17,168 	Gloss Alignment :	         
2024-02-08 15:16:17,168 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:16:17,169 	Text Reference  :	suga then announced that from    5  july onwards japan    will   be  in **** a    state of      emergency
2024-02-08 15:16:17,169 	Text Hypothesis :	**** **** ********* **** amazing to the  covid   pandemic people are in july 2021 to    provide him      
2024-02-08 15:16:17,169 	Text Alignment  :	D    D    D         D    S       S  S    S       S        S      S      I    S    S     S       S        
2024-02-08 15:16:17,170 ========================================================================================================================
2024-02-08 15:16:17,170 Logging Sequence: 56_17.00
2024-02-08 15:16:17,170 	Gloss Reference :	A B+C+D+E    
2024-02-08 15:16:17,170 	Gloss Hypothesis:	A B+C+D+E+D+E
2024-02-08 15:16:17,170 	Gloss Alignment :	  S          
2024-02-08 15:16:17,170 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:16:17,171 	Text Reference  :	it     was  held at    mumbai's wankhede stadium
2024-02-08 15:16:17,171 	Text Hypothesis :	people were very happy that     the      stump  
2024-02-08 15:16:17,171 	Text Alignment  :	S      S    S    S     S        S        S      
2024-02-08 15:16:17,171 ========================================================================================================================
2024-02-08 15:16:17,171 Logging Sequence: 152_73.00
2024-02-08 15:16:17,171 	Gloss Reference :	A B+C+D+E
2024-02-08 15:16:17,171 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:16:17,172 	Gloss Alignment :	         
2024-02-08 15:16:17,172 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:16:17,173 	Text Reference  :	********** ******** ** *** ***** **** ** *** *** eventually he too  got out  by shaheen afridi
2024-02-08 15:16:17,173 	Text Hypothesis :	pakistan's decision to bat first time to win any national   or will be  held in 18      overs 
2024-02-08 15:16:17,173 	Text Alignment  :	I          I        I  I   I     I    I  I   I   S          S  S    S   S    S  S       S     
2024-02-08 15:16:17,173 ========================================================================================================================
2024-02-08 15:16:19,197 Epoch 269: Total Training Recognition Loss 0.17  Total Training Translation Loss 7.99 
2024-02-08 15:16:19,199 EPOCH 270
2024-02-08 15:16:24,833 Epoch 270: Total Training Recognition Loss 0.14  Total Training Translation Loss 10.35 
2024-02-08 15:16:24,833 EPOCH 271
2024-02-08 15:16:25,468 [Epoch: 271 Step: 00018100] Batch Recognition Loss:   0.001009 => Gls Tokens per Sec:     2364 || Batch Translation Loss:   0.124861 => Txt Tokens per Sec:     5790 || Lr: 0.000100
2024-02-08 15:16:30,158 Epoch 271: Total Training Recognition Loss 0.18  Total Training Translation Loss 16.13 
2024-02-08 15:16:30,159 EPOCH 272
2024-02-08 15:16:33,640 [Epoch: 272 Step: 00018200] Batch Recognition Loss:   0.001685 => Gls Tokens per Sec:     1977 || Batch Translation Loss:   0.065893 => Txt Tokens per Sec:     5472 || Lr: 0.000100
2024-02-08 15:16:35,657 Epoch 272: Total Training Recognition Loss 0.26  Total Training Translation Loss 10.99 
2024-02-08 15:16:35,657 EPOCH 273
2024-02-08 15:16:41,000 Epoch 273: Total Training Recognition Loss 0.18  Total Training Translation Loss 7.67 
2024-02-08 15:16:41,001 EPOCH 274
2024-02-08 15:16:41,682 [Epoch: 274 Step: 00018300] Batch Recognition Loss:   0.000701 => Gls Tokens per Sec:     2118 || Batch Translation Loss:   0.056636 => Txt Tokens per Sec:     5246 || Lr: 0.000100
2024-02-08 15:16:46,647 Epoch 274: Total Training Recognition Loss 0.19  Total Training Translation Loss 5.90 
2024-02-08 15:16:46,647 EPOCH 275
2024-02-08 15:16:49,998 [Epoch: 275 Step: 00018400] Batch Recognition Loss:   0.000850 => Gls Tokens per Sec:     1976 || Batch Translation Loss:   0.183784 => Txt Tokens per Sec:     5481 || Lr: 0.000100
2024-02-08 15:16:52,196 Epoch 275: Total Training Recognition Loss 0.14  Total Training Translation Loss 5.60 
2024-02-08 15:16:52,197 EPOCH 276
2024-02-08 15:16:57,577 Epoch 276: Total Training Recognition Loss 0.16  Total Training Translation Loss 5.52 
2024-02-08 15:16:57,578 EPOCH 277
2024-02-08 15:16:58,138 [Epoch: 277 Step: 00018500] Batch Recognition Loss:   0.000820 => Gls Tokens per Sec:     2295 || Batch Translation Loss:   0.060839 => Txt Tokens per Sec:     5747 || Lr: 0.000100
2024-02-08 15:17:03,298 Epoch 277: Total Training Recognition Loss 0.19  Total Training Translation Loss 6.14 
2024-02-08 15:17:03,299 EPOCH 278
2024-02-08 15:17:06,259 [Epoch: 278 Step: 00018600] Batch Recognition Loss:   0.000552 => Gls Tokens per Sec:     2183 || Batch Translation Loss:   0.031265 => Txt Tokens per Sec:     5912 || Lr: 0.000100
2024-02-08 15:17:08,502 Epoch 278: Total Training Recognition Loss 0.15  Total Training Translation Loss 5.45 
2024-02-08 15:17:08,502 EPOCH 279
2024-02-08 15:17:13,911 Epoch 279: Total Training Recognition Loss 0.27  Total Training Translation Loss 5.27 
2024-02-08 15:17:13,912 EPOCH 280
2024-02-08 15:17:14,435 [Epoch: 280 Step: 00018700] Batch Recognition Loss:   0.012835 => Gls Tokens per Sec:     2146 || Batch Translation Loss:   0.253499 => Txt Tokens per Sec:     5826 || Lr: 0.000100
2024-02-08 15:17:19,275 Epoch 280: Total Training Recognition Loss 0.26  Total Training Translation Loss 5.70 
2024-02-08 15:17:19,275 EPOCH 281
2024-02-08 15:17:22,708 [Epoch: 281 Step: 00018800] Batch Recognition Loss:   0.001455 => Gls Tokens per Sec:     1865 || Batch Translation Loss:   0.042832 => Txt Tokens per Sec:     5253 || Lr: 0.000100
2024-02-08 15:17:24,934 Epoch 281: Total Training Recognition Loss 0.40  Total Training Translation Loss 9.94 
2024-02-08 15:17:24,934 EPOCH 282
2024-02-08 15:17:30,283 Epoch 282: Total Training Recognition Loss 0.30  Total Training Translation Loss 13.88 
2024-02-08 15:17:30,284 EPOCH 283
2024-02-08 15:17:30,676 [Epoch: 283 Step: 00018900] Batch Recognition Loss:   0.024449 => Gls Tokens per Sec:     2459 || Batch Translation Loss:   0.278313 => Txt Tokens per Sec:     6235 || Lr: 0.000100
2024-02-08 15:17:35,876 Epoch 283: Total Training Recognition Loss 0.82  Total Training Translation Loss 17.91 
2024-02-08 15:17:35,877 EPOCH 284
2024-02-08 15:17:38,816 [Epoch: 284 Step: 00019000] Batch Recognition Loss:   0.007561 => Gls Tokens per Sec:     2123 || Batch Translation Loss:   0.216000 => Txt Tokens per Sec:     5791 || Lr: 0.000100
2024-02-08 15:17:41,268 Epoch 284: Total Training Recognition Loss 2.36  Total Training Translation Loss 17.03 
2024-02-08 15:17:41,269 EPOCH 285
2024-02-08 15:17:46,746 Epoch 285: Total Training Recognition Loss 1.32  Total Training Translation Loss 13.92 
2024-02-08 15:17:46,747 EPOCH 286
2024-02-08 15:17:47,216 [Epoch: 286 Step: 00019100] Batch Recognition Loss:   0.000629 => Gls Tokens per Sec:     1707 || Batch Translation Loss:   0.324582 => Txt Tokens per Sec:     5462 || Lr: 0.000100
2024-02-08 15:17:52,141 Epoch 286: Total Training Recognition Loss 0.36  Total Training Translation Loss 16.71 
2024-02-08 15:17:52,141 EPOCH 287
2024-02-08 15:17:55,519 [Epoch: 287 Step: 00019200] Batch Recognition Loss:   0.000348 => Gls Tokens per Sec:     1771 || Batch Translation Loss:   0.112859 => Txt Tokens per Sec:     4904 || Lr: 0.000100
2024-02-08 15:17:57,712 Epoch 287: Total Training Recognition Loss 0.29  Total Training Translation Loss 16.21 
2024-02-08 15:17:57,712 EPOCH 288
2024-02-08 15:18:03,140 Epoch 288: Total Training Recognition Loss 0.11  Total Training Translation Loss 8.67 
2024-02-08 15:18:03,141 EPOCH 289
2024-02-08 15:18:03,578 [Epoch: 289 Step: 00019300] Batch Recognition Loss:   0.002065 => Gls Tokens per Sec:     1470 || Batch Translation Loss:   0.143066 => Txt Tokens per Sec:     4667 || Lr: 0.000100
2024-02-08 15:18:08,703 Epoch 289: Total Training Recognition Loss 0.16  Total Training Translation Loss 4.92 
2024-02-08 15:18:08,703 EPOCH 290
2024-02-08 15:18:11,940 [Epoch: 290 Step: 00019400] Batch Recognition Loss:   0.000326 => Gls Tokens per Sec:     1830 || Batch Translation Loss:   0.069097 => Txt Tokens per Sec:     5152 || Lr: 0.000100
2024-02-08 15:18:14,284 Epoch 290: Total Training Recognition Loss 0.16  Total Training Translation Loss 5.24 
2024-02-08 15:18:14,284 EPOCH 291
2024-02-08 15:18:19,612 Epoch 291: Total Training Recognition Loss 0.15  Total Training Translation Loss 4.82 
2024-02-08 15:18:19,612 EPOCH 292
2024-02-08 15:18:19,858 [Epoch: 292 Step: 00019500] Batch Recognition Loss:   0.008350 => Gls Tokens per Sec:     1962 || Batch Translation Loss:   0.181343 => Txt Tokens per Sec:     5002 || Lr: 0.000100
2024-02-08 15:18:25,130 Epoch 292: Total Training Recognition Loss 0.14  Total Training Translation Loss 5.15 
2024-02-08 15:18:25,131 EPOCH 293
2024-02-08 15:18:28,115 [Epoch: 293 Step: 00019600] Batch Recognition Loss:   0.000399 => Gls Tokens per Sec:     1898 || Batch Translation Loss:   0.027698 => Txt Tokens per Sec:     5451 || Lr: 0.000100
2024-02-08 15:18:30,575 Epoch 293: Total Training Recognition Loss 0.16  Total Training Translation Loss 4.36 
2024-02-08 15:18:30,575 EPOCH 294
2024-02-08 15:18:36,026 Epoch 294: Total Training Recognition Loss 0.18  Total Training Translation Loss 3.69 
2024-02-08 15:18:36,027 EPOCH 295
2024-02-08 15:18:36,137 [Epoch: 295 Step: 00019700] Batch Recognition Loss:   0.000500 => Gls Tokens per Sec:     2944 || Batch Translation Loss:   0.020007 => Txt Tokens per Sec:     6844 || Lr: 0.000100
2024-02-08 15:18:41,330 Epoch 295: Total Training Recognition Loss 0.14  Total Training Translation Loss 5.38 
2024-02-08 15:18:41,331 EPOCH 296
2024-02-08 15:18:44,197 [Epoch: 296 Step: 00019800] Batch Recognition Loss:   0.000353 => Gls Tokens per Sec:     1954 || Batch Translation Loss:   0.069775 => Txt Tokens per Sec:     5318 || Lr: 0.000100
2024-02-08 15:18:46,842 Epoch 296: Total Training Recognition Loss 0.09  Total Training Translation Loss 5.85 
2024-02-08 15:18:46,843 EPOCH 297
2024-02-08 15:18:52,268 Epoch 297: Total Training Recognition Loss 0.12  Total Training Translation Loss 7.11 
2024-02-08 15:18:52,269 EPOCH 298
2024-02-08 15:18:52,367 [Epoch: 298 Step: 00019900] Batch Recognition Loss:   0.001748 => Gls Tokens per Sec:     1654 || Batch Translation Loss:   0.092579 => Txt Tokens per Sec:     4736 || Lr: 0.000100
2024-02-08 15:18:57,772 Epoch 298: Total Training Recognition Loss 0.13  Total Training Translation Loss 9.95 
2024-02-08 15:18:57,773 EPOCH 299
2024-02-08 15:19:00,598 [Epoch: 299 Step: 00020000] Batch Recognition Loss:   0.010714 => Gls Tokens per Sec:     1892 || Batch Translation Loss:   0.020830 => Txt Tokens per Sec:     5223 || Lr: 0.000100
2024-02-08 15:19:09,219 Validation result at epoch 299, step    20000: duration: 8.6218s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 3.02108	Translation Loss: 94263.98438	PPL: 12272.30176
	Eval Metric: BLEU
	WER 4.59	(DEL: 0.00,	INS: 0.00,	SUB: 4.59)
	BLEU-4 0.64	(BLEU-1: 11.10,	BLEU-2: 3.36,	BLEU-3: 1.34,	BLEU-4: 0.64)
	CHRF 17.06	ROUGE 9.28
2024-02-08 15:19:09,220 Logging Recognition and Translation Outputs
2024-02-08 15:19:09,221 ========================================================================================================================
2024-02-08 15:19:09,221 Logging Sequence: 120_7.00
2024-02-08 15:19:09,221 	Gloss Reference :	A B+C+D+E
2024-02-08 15:19:09,221 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:19:09,221 	Gloss Alignment :	         
2024-02-08 15:19:09,222 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:19:09,222 	Text Reference  :	** he          had    tested positive for    covid-19 on     may    19  
2024-02-08 15:19:09,222 	Text Hypothesis :	on questioning people about  the      murder the      police learnt that
2024-02-08 15:19:09,223 	Text Alignment  :	I  S           S      S      S        S      S        S      S      S   
2024-02-08 15:19:09,223 ========================================================================================================================
2024-02-08 15:19:09,223 Logging Sequence: 148_186.00
2024-02-08 15:19:09,223 	Gloss Reference :	A B+C+D+E
2024-02-08 15:19:09,223 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:19:09,223 	Gloss Alignment :	         
2024-02-08 15:19:09,223 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:19:09,226 	Text Reference  :	*** siraj  also    took     four wickets in 1   over   thus      becoming the    record-holder for most   wickets in        an over in         odis    
2024-02-08 15:19:09,226 	Text Hypothesis :	sri lankan batsmen remained the  fall    of the eighth encounter between  hardik pandya        and shobit were    dismissed at a    commercial partners
2024-02-08 15:19:09,226 	Text Alignment  :	I   S      S       S        S    S       S  S   S      S         S        S      S             S   S      S       S         S  S    S          S       
2024-02-08 15:19:09,226 ========================================================================================================================
2024-02-08 15:19:09,226 Logging Sequence: 67_73.00
2024-02-08 15:19:09,227 	Gloss Reference :	A B+C+D+E
2024-02-08 15:19:09,227 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:19:09,227 	Gloss Alignment :	         
2024-02-08 15:19:09,227 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:19:09,228 	Text Reference  :	***** in   his       tweet he      also said
2024-02-08 15:19:09,228 	Text Hypothesis :	there were jubiliant for   various news here
2024-02-08 15:19:09,228 	Text Alignment  :	I     S    S         S     S       S    S   
2024-02-08 15:19:09,228 ========================================================================================================================
2024-02-08 15:19:09,229 Logging Sequence: 164_526.00
2024-02-08 15:19:09,229 	Gloss Reference :	A B+C+D+E
2024-02-08 15:19:09,229 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:19:09,229 	Gloss Alignment :	         
2024-02-08 15:19:09,229 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:19:09,230 	Text Reference  :	*** *** ***** ********* you  are      aware that viacom18 bought the  broadcast rights of     ipl     
2024-02-08 15:19:09,230 	Text Hypothesis :	the two prime ministers were welcomed with  loud cheers   as     they took      a      sudden decision
2024-02-08 15:19:09,230 	Text Alignment  :	I   I   I     I         S    S        S     S    S        S      S    S         S      S      S       
2024-02-08 15:19:09,231 ========================================================================================================================
2024-02-08 15:19:09,231 Logging Sequence: 108_28.00
2024-02-08 15:19:09,231 	Gloss Reference :	A B+C+D+E
2024-02-08 15:19:09,231 	Gloss Hypothesis:	A B+C+D  
2024-02-08 15:19:09,231 	Gloss Alignment :	  S      
2024-02-08 15:19:09,231 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:19:09,233 	Text Reference  :	the 10 teams bought 204    players including 67  foreign players after spending a  total of       rs     55170 crore
2024-02-08 15:19:09,233 	Text Hypothesis :	*** ** ***** ****** handed over    the       ipl matches were    sold  out      to be    produced before any   crore
2024-02-08 15:19:09,233 	Text Alignment  :	D   D  D     D      S      S       S         S   S       S       S     S        S  S     S        S      S          
2024-02-08 15:19:09,233 ========================================================================================================================
2024-02-08 15:19:11,917 Epoch 299: Total Training Recognition Loss 0.18  Total Training Translation Loss 9.88 
2024-02-08 15:19:11,917 EPOCH 300
2024-02-08 15:19:17,567 [Epoch: 300 Step: 00020100] Batch Recognition Loss:   0.001490 => Gls Tokens per Sec:     1880 || Batch Translation Loss:   0.072072 => Txt Tokens per Sec:     5201 || Lr: 0.000100
2024-02-08 15:19:17,568 Epoch 300: Total Training Recognition Loss 0.14  Total Training Translation Loss 7.30 
2024-02-08 15:19:17,568 EPOCH 301
2024-02-08 15:19:23,069 Epoch 301: Total Training Recognition Loss 0.11  Total Training Translation Loss 8.08 
2024-02-08 15:19:23,069 EPOCH 302
2024-02-08 15:19:25,674 [Epoch: 302 Step: 00020200] Batch Recognition Loss:   0.001073 => Gls Tokens per Sec:     1990 || Batch Translation Loss:   0.152252 => Txt Tokens per Sec:     5378 || Lr: 0.000100
2024-02-08 15:19:28,653 Epoch 302: Total Training Recognition Loss 0.11  Total Training Translation Loss 8.74 
2024-02-08 15:19:28,653 EPOCH 303
2024-02-08 15:19:33,219 [Epoch: 303 Step: 00020300] Batch Recognition Loss:   0.000524 => Gls Tokens per Sec:     2292 || Batch Translation Loss:   0.118939 => Txt Tokens per Sec:     6326 || Lr: 0.000100
2024-02-08 15:19:33,324 Epoch 303: Total Training Recognition Loss 0.12  Total Training Translation Loss 8.88 
2024-02-08 15:19:33,324 EPOCH 304
2024-02-08 15:19:38,977 Epoch 304: Total Training Recognition Loss 0.08  Total Training Translation Loss 6.63 
2024-02-08 15:19:38,977 EPOCH 305
2024-02-08 15:19:41,507 [Epoch: 305 Step: 00020400] Batch Recognition Loss:   0.000409 => Gls Tokens per Sec:     1986 || Batch Translation Loss:   0.041756 => Txt Tokens per Sec:     5768 || Lr: 0.000100
2024-02-08 15:19:44,094 Epoch 305: Total Training Recognition Loss 0.18  Total Training Translation Loss 5.68 
2024-02-08 15:19:44,095 EPOCH 306
2024-02-08 15:19:49,460 [Epoch: 306 Step: 00020500] Batch Recognition Loss:   0.005588 => Gls Tokens per Sec:     1920 || Batch Translation Loss:   0.090465 => Txt Tokens per Sec:     5301 || Lr: 0.000100
2024-02-08 15:19:49,616 Epoch 306: Total Training Recognition Loss 0.17  Total Training Translation Loss 8.93 
2024-02-08 15:19:49,616 EPOCH 307
2024-02-08 15:19:54,926 Epoch 307: Total Training Recognition Loss 0.14  Total Training Translation Loss 8.56 
2024-02-08 15:19:54,927 EPOCH 308
2024-02-08 15:19:57,674 [Epoch: 308 Step: 00020600] Batch Recognition Loss:   0.000352 => Gls Tokens per Sec:     1770 || Batch Translation Loss:   0.177087 => Txt Tokens per Sec:     4878 || Lr: 0.000100
2024-02-08 15:20:00,580 Epoch 308: Total Training Recognition Loss 0.14  Total Training Translation Loss 9.53 
2024-02-08 15:20:00,580 EPOCH 309
2024-02-08 15:20:05,476 [Epoch: 309 Step: 00020700] Batch Recognition Loss:   0.000365 => Gls Tokens per Sec:     2072 || Batch Translation Loss:   0.051037 => Txt Tokens per Sec:     5742 || Lr: 0.000100
2024-02-08 15:20:05,672 Epoch 309: Total Training Recognition Loss 0.10  Total Training Translation Loss 8.75 
2024-02-08 15:20:05,672 EPOCH 310
2024-02-08 15:20:11,330 Epoch 310: Total Training Recognition Loss 0.12  Total Training Translation Loss 9.55 
2024-02-08 15:20:11,331 EPOCH 311
2024-02-08 15:20:13,512 [Epoch: 311 Step: 00020800] Batch Recognition Loss:   0.000737 => Gls Tokens per Sec:     2202 || Batch Translation Loss:   0.123341 => Txt Tokens per Sec:     5919 || Lr: 0.000100
2024-02-08 15:20:16,393 Epoch 311: Total Training Recognition Loss 0.11  Total Training Translation Loss 8.77 
2024-02-08 15:20:16,394 EPOCH 312
2024-02-08 15:20:20,876 [Epoch: 312 Step: 00020900] Batch Recognition Loss:   0.003948 => Gls Tokens per Sec:     2227 || Batch Translation Loss:   0.140154 => Txt Tokens per Sec:     6134 || Lr: 0.000100
2024-02-08 15:20:21,133 Epoch 312: Total Training Recognition Loss 0.16  Total Training Translation Loss 9.42 
2024-02-08 15:20:21,133 EPOCH 313
2024-02-08 15:20:26,619 Epoch 313: Total Training Recognition Loss 0.07  Total Training Translation Loss 8.81 
2024-02-08 15:20:26,619 EPOCH 314
2024-02-08 15:20:29,000 [Epoch: 314 Step: 00021000] Batch Recognition Loss:   0.000623 => Gls Tokens per Sec:     1907 || Batch Translation Loss:   0.011260 => Txt Tokens per Sec:     5175 || Lr: 0.000100
2024-02-08 15:20:32,070 Epoch 314: Total Training Recognition Loss 0.12  Total Training Translation Loss 7.24 
2024-02-08 15:20:32,071 EPOCH 315
2024-02-08 15:20:37,190 [Epoch: 315 Step: 00021100] Batch Recognition Loss:   0.004604 => Gls Tokens per Sec:     1918 || Batch Translation Loss:   0.084013 => Txt Tokens per Sec:     5333 || Lr: 0.000100
2024-02-08 15:20:37,558 Epoch 315: Total Training Recognition Loss 0.25  Total Training Translation Loss 9.48 
2024-02-08 15:20:37,558 EPOCH 316
2024-02-08 15:20:42,618 Epoch 316: Total Training Recognition Loss 0.14  Total Training Translation Loss 11.03 
2024-02-08 15:20:42,619 EPOCH 317
2024-02-08 15:20:44,793 [Epoch: 317 Step: 00021200] Batch Recognition Loss:   0.000426 => Gls Tokens per Sec:     2014 || Batch Translation Loss:   0.081124 => Txt Tokens per Sec:     5800 || Lr: 0.000100
2024-02-08 15:20:48,068 Epoch 317: Total Training Recognition Loss 0.14  Total Training Translation Loss 9.71 
2024-02-08 15:20:48,068 EPOCH 318
2024-02-08 15:20:52,828 [Epoch: 318 Step: 00021300] Batch Recognition Loss:   0.005200 => Gls Tokens per Sec:     2030 || Batch Translation Loss:   0.368053 => Txt Tokens per Sec:     5649 || Lr: 0.000100
2024-02-08 15:20:53,203 Epoch 318: Total Training Recognition Loss 0.19  Total Training Translation Loss 9.91 
2024-02-08 15:20:53,203 EPOCH 319
2024-02-08 15:20:58,732 Epoch 319: Total Training Recognition Loss 0.17  Total Training Translation Loss 7.76 
2024-02-08 15:20:58,733 EPOCH 320
2024-02-08 15:21:00,681 [Epoch: 320 Step: 00021400] Batch Recognition Loss:   0.002125 => Gls Tokens per Sec:     2218 || Batch Translation Loss:   0.110163 => Txt Tokens per Sec:     5998 || Lr: 0.000100
2024-02-08 15:21:03,740 Epoch 320: Total Training Recognition Loss 0.17  Total Training Translation Loss 7.43 
2024-02-08 15:21:03,740 EPOCH 321
2024-02-08 15:21:08,566 [Epoch: 321 Step: 00021500] Batch Recognition Loss:   0.000669 => Gls Tokens per Sec:     1969 || Batch Translation Loss:   0.031870 => Txt Tokens per Sec:     5445 || Lr: 0.000100
2024-02-08 15:21:09,292 Epoch 321: Total Training Recognition Loss 0.19  Total Training Translation Loss 4.66 
2024-02-08 15:21:09,292 EPOCH 322
2024-02-08 15:21:14,368 Epoch 322: Total Training Recognition Loss 0.19  Total Training Translation Loss 5.18 
2024-02-08 15:21:14,368 EPOCH 323
2024-02-08 15:21:16,627 [Epoch: 323 Step: 00021600] Batch Recognition Loss:   0.000736 => Gls Tokens per Sec:     1843 || Batch Translation Loss:   0.489639 => Txt Tokens per Sec:     5325 || Lr: 0.000100
2024-02-08 15:21:20,021 Epoch 323: Total Training Recognition Loss 0.17  Total Training Translation Loss 7.78 
2024-02-08 15:21:20,022 EPOCH 324
2024-02-08 15:21:24,829 [Epoch: 324 Step: 00021700] Batch Recognition Loss:   0.010457 => Gls Tokens per Sec:     1944 || Batch Translation Loss:   0.140988 => Txt Tokens per Sec:     5321 || Lr: 0.000100
2024-02-08 15:21:25,677 Epoch 324: Total Training Recognition Loss 0.18  Total Training Translation Loss 8.00 
2024-02-08 15:21:25,678 EPOCH 325
2024-02-08 15:21:31,087 Epoch 325: Total Training Recognition Loss 0.10  Total Training Translation Loss 9.71 
2024-02-08 15:21:31,087 EPOCH 326
2024-02-08 15:21:32,966 [Epoch: 326 Step: 00021800] Batch Recognition Loss:   0.001783 => Gls Tokens per Sec:     2076 || Batch Translation Loss:   0.029783 => Txt Tokens per Sec:     5628 || Lr: 0.000100
2024-02-08 15:21:36,154 Epoch 326: Total Training Recognition Loss 0.11  Total Training Translation Loss 8.74 
2024-02-08 15:21:36,155 EPOCH 327
2024-02-08 15:21:41,361 [Epoch: 327 Step: 00021900] Batch Recognition Loss:   0.002117 => Gls Tokens per Sec:     1764 || Batch Translation Loss:   0.022579 => Txt Tokens per Sec:     4898 || Lr: 0.000100
2024-02-08 15:21:42,016 Epoch 327: Total Training Recognition Loss 0.12  Total Training Translation Loss 6.23 
2024-02-08 15:21:42,017 EPOCH 328
2024-02-08 15:21:47,018 Epoch 328: Total Training Recognition Loss 0.10  Total Training Translation Loss 5.22 
2024-02-08 15:21:47,019 EPOCH 329
2024-02-08 15:21:48,750 [Epoch: 329 Step: 00022000] Batch Recognition Loss:   0.005974 => Gls Tokens per Sec:     2220 || Batch Translation Loss:   0.056805 => Txt Tokens per Sec:     6187 || Lr: 0.000100
2024-02-08 15:21:57,480 Validation result at epoch 329, step    22000: duration: 8.7305s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 3.14691	Translation Loss: 94200.10938	PPL: 12194.25293
	Eval Metric: BLEU
	WER 4.73	(DEL: 0.00,	INS: 0.00,	SUB: 4.73)
	BLEU-4 0.53	(BLEU-1: 10.96,	BLEU-2: 3.45,	BLEU-3: 1.22,	BLEU-4: 0.53)
	CHRF 16.59	ROUGE 9.36
2024-02-08 15:21:57,481 Logging Recognition and Translation Outputs
2024-02-08 15:21:57,482 ========================================================================================================================
2024-02-08 15:21:57,482 Logging Sequence: 179_2.00
2024-02-08 15:21:57,482 	Gloss Reference :	A B+C+D+E
2024-02-08 15:21:57,482 	Gloss Hypothesis:	A B+C+D  
2024-02-08 15:21:57,482 	Gloss Alignment :	  S      
2024-02-08 15:21:57,482 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:21:57,483 	Text Reference  :	*** *** **** vinesh phogat is           a  well known   wrestler
2024-02-08 15:21:57,483 	Text Hypothesis :	the csk team was    very   disappointed by the  winning it      
2024-02-08 15:21:57,483 	Text Alignment  :	I   I   I    S      S      S            S  S    S       S       
2024-02-08 15:21:57,483 ========================================================================================================================
2024-02-08 15:21:57,483 Logging Sequence: 55_124.00
2024-02-08 15:21:57,484 	Gloss Reference :	A B+C+D+E
2024-02-08 15:21:57,484 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:21:57,484 	Gloss Alignment :	         
2024-02-08 15:21:57,484 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:21:57,485 	Text Reference  :	*** next   to   him  with    the    patel    jersey was ajaz patel 
2024-02-08 15:21:57,485 	Text Hypothesis :	the mumbai team vice captain smriti mandhana is     in  the  reason
2024-02-08 15:21:57,485 	Text Alignment  :	I   S      S    S    S       S      S        S      S   S    S     
2024-02-08 15:21:57,485 ========================================================================================================================
2024-02-08 15:21:57,485 Logging Sequence: 148_105.00
2024-02-08 15:21:57,485 	Gloss Reference :	A B+C+D+E
2024-02-08 15:21:57,486 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:21:57,486 	Gloss Alignment :	         
2024-02-08 15:21:57,486 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:21:57,487 	Text Reference  :	later with amazing bowling by hardik pandya and kuldeep yadav sri  lanka were    all out in     just 50   runs     
2024-02-08 15:21:57,487 	Text Hypothesis :	***** **** ******* ******* ** ****** pandya *** led     his   team to    victory in  the indian team were postponed
2024-02-08 15:21:57,487 	Text Alignment  :	D     D    D       D       D  D             D   S       S     S    S     S       S   S   S      S    S    S        
2024-02-08 15:21:57,488 ========================================================================================================================
2024-02-08 15:21:57,488 Logging Sequence: 125_165.00
2024-02-08 15:21:57,488 	Gloss Reference :	A B+C+D+E
2024-02-08 15:21:57,488 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:21:57,488 	Gloss Alignment :	         
2024-02-08 15:21:57,488 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:21:57,489 	Text Reference  :	please do   not  target   nadeem we  speak to each other and share       a  good bond
2024-02-08 15:21:57,489 	Text Hypothesis :	****** many such comments does   not want  to **** ***** *** participate in the  2020
2024-02-08 15:21:57,489 	Text Alignment  :	D      S    S    S        S      S   S        D    D     D   S           S  S    S   
2024-02-08 15:21:57,490 ========================================================================================================================
2024-02-08 15:21:57,490 Logging Sequence: 77_52.00
2024-02-08 15:21:57,490 	Gloss Reference :	A B+C+D+E
2024-02-08 15:21:57,490 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:21:57,490 	Gloss Alignment :	         
2024-02-08 15:21:57,490 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:21:57,492 	Text Reference  :	kane    williamson held down the fort for hyderabad by      scoring 66   runs  and     ended the match in a     tie    
2024-02-08 15:21:57,492 	Text Hypothesis :	however some       said that the **** *** ********* taliban won     this while playing for   the ***** ** trent rockets
2024-02-08 15:21:57,492 	Text Alignment  :	S       S          S    S        D    D   D         S       S       S    S     S       S         D     D  S     S      
2024-02-08 15:21:57,492 ========================================================================================================================
2024-02-08 15:22:01,060 Epoch 329: Total Training Recognition Loss 0.11  Total Training Translation Loss 5.17 
2024-02-08 15:22:01,061 EPOCH 330
2024-02-08 15:22:05,384 [Epoch: 330 Step: 00022100] Batch Recognition Loss:   0.002632 => Gls Tokens per Sec:     2087 || Batch Translation Loss:   0.152742 => Txt Tokens per Sec:     5665 || Lr: 0.000100
2024-02-08 15:22:06,556 Epoch 330: Total Training Recognition Loss 0.13  Total Training Translation Loss 4.27 
2024-02-08 15:22:06,557 EPOCH 331
2024-02-08 15:22:12,235 Epoch 331: Total Training Recognition Loss 0.13  Total Training Translation Loss 3.90 
2024-02-08 15:22:12,236 EPOCH 332
2024-02-08 15:22:13,954 [Epoch: 332 Step: 00022200] Batch Recognition Loss:   0.000189 => Gls Tokens per Sec:     2141 || Batch Translation Loss:   0.031410 => Txt Tokens per Sec:     5824 || Lr: 0.000100
2024-02-08 15:22:17,660 Epoch 332: Total Training Recognition Loss 0.17  Total Training Translation Loss 4.66 
2024-02-08 15:22:17,661 EPOCH 333
2024-02-08 15:22:22,257 [Epoch: 333 Step: 00022300] Batch Recognition Loss:   0.000347 => Gls Tokens per Sec:     1928 || Batch Translation Loss:   0.081963 => Txt Tokens per Sec:     5328 || Lr: 0.000100
2024-02-08 15:22:23,227 Epoch 333: Total Training Recognition Loss 0.07  Total Training Translation Loss 7.07 
2024-02-08 15:22:23,227 EPOCH 334
2024-02-08 15:22:27,947 Epoch 334: Total Training Recognition Loss 0.15  Total Training Translation Loss 9.23 
2024-02-08 15:22:27,948 EPOCH 335
2024-02-08 15:22:29,756 [Epoch: 335 Step: 00022400] Batch Recognition Loss:   0.000424 => Gls Tokens per Sec:     1949 || Batch Translation Loss:   0.194276 => Txt Tokens per Sec:     5392 || Lr: 0.000100
2024-02-08 15:22:33,599 Epoch 335: Total Training Recognition Loss 0.17  Total Training Translation Loss 11.34 
2024-02-08 15:22:33,599 EPOCH 336
2024-02-08 15:22:37,936 [Epoch: 336 Step: 00022500] Batch Recognition Loss:   0.000913 => Gls Tokens per Sec:     2007 || Batch Translation Loss:   0.038680 => Txt Tokens per Sec:     5639 || Lr: 0.000100
2024-02-08 15:22:38,977 Epoch 336: Total Training Recognition Loss 0.17  Total Training Translation Loss 11.72 
2024-02-08 15:22:38,977 EPOCH 337
2024-02-08 15:22:44,709 Epoch 337: Total Training Recognition Loss 0.12  Total Training Translation Loss 9.27 
2024-02-08 15:22:44,709 EPOCH 338
2024-02-08 15:22:46,335 [Epoch: 338 Step: 00022600] Batch Recognition Loss:   0.028149 => Gls Tokens per Sec:     2006 || Batch Translation Loss:   0.109825 => Txt Tokens per Sec:     5385 || Lr: 0.000100
2024-02-08 15:22:50,279 Epoch 338: Total Training Recognition Loss 0.16  Total Training Translation Loss 7.40 
2024-02-08 15:22:50,279 EPOCH 339
2024-02-08 15:22:54,147 [Epoch: 339 Step: 00022700] Batch Recognition Loss:   0.002658 => Gls Tokens per Sec:     2208 || Batch Translation Loss:   0.060168 => Txt Tokens per Sec:     6142 || Lr: 0.000100
2024-02-08 15:22:55,154 Epoch 339: Total Training Recognition Loss 0.12  Total Training Translation Loss 7.86 
2024-02-08 15:22:55,155 EPOCH 340
2024-02-08 15:23:00,712 Epoch 340: Total Training Recognition Loss 0.13  Total Training Translation Loss 7.76 
2024-02-08 15:23:00,712 EPOCH 341
2024-02-08 15:23:02,240 [Epoch: 341 Step: 00022800] Batch Recognition Loss:   0.008883 => Gls Tokens per Sec:     2030 || Batch Translation Loss:   0.048700 => Txt Tokens per Sec:     5572 || Lr: 0.000100
2024-02-08 15:23:05,697 Epoch 341: Total Training Recognition Loss 0.20  Total Training Translation Loss 6.49 
2024-02-08 15:23:05,698 EPOCH 342
2024-02-08 15:23:09,979 [Epoch: 342 Step: 00022900] Batch Recognition Loss:   0.000585 => Gls Tokens per Sec:     1957 || Batch Translation Loss:   0.180908 => Txt Tokens per Sec:     5428 || Lr: 0.000100
2024-02-08 15:23:11,117 Epoch 342: Total Training Recognition Loss 0.07  Total Training Translation Loss 6.23 
2024-02-08 15:23:11,117 EPOCH 343
2024-02-08 15:23:16,060 Epoch 343: Total Training Recognition Loss 0.13  Total Training Translation Loss 6.92 
2024-02-08 15:23:16,060 EPOCH 344
2024-02-08 15:23:17,635 [Epoch: 344 Step: 00023000] Batch Recognition Loss:   0.001012 => Gls Tokens per Sec:     1933 || Batch Translation Loss:   0.078789 => Txt Tokens per Sec:     5128 || Lr: 0.000100
2024-02-08 15:23:21,648 Epoch 344: Total Training Recognition Loss 0.17  Total Training Translation Loss 6.11 
2024-02-08 15:23:21,648 EPOCH 345
2024-02-08 15:23:25,260 [Epoch: 345 Step: 00023100] Batch Recognition Loss:   0.000744 => Gls Tokens per Sec:     2276 || Batch Translation Loss:   0.036157 => Txt Tokens per Sec:     6307 || Lr: 0.000100
2024-02-08 15:23:26,531 Epoch 345: Total Training Recognition Loss 0.19  Total Training Translation Loss 6.71 
2024-02-08 15:23:26,531 EPOCH 346
2024-02-08 15:23:32,040 Epoch 346: Total Training Recognition Loss 0.10  Total Training Translation Loss 7.39 
2024-02-08 15:23:32,040 EPOCH 347
2024-02-08 15:23:33,251 [Epoch: 347 Step: 00023200] Batch Recognition Loss:   0.000437 => Gls Tokens per Sec:     2380 || Batch Translation Loss:   0.054993 => Txt Tokens per Sec:     6372 || Lr: 0.000100
2024-02-08 15:23:36,896 Epoch 347: Total Training Recognition Loss 0.25  Total Training Translation Loss 11.34 
2024-02-08 15:23:36,897 EPOCH 348
2024-02-08 15:23:41,338 [Epoch: 348 Step: 00023300] Batch Recognition Loss:   0.002277 => Gls Tokens per Sec:     1816 || Batch Translation Loss:   0.129186 => Txt Tokens per Sec:     5008 || Lr: 0.000100
2024-02-08 15:23:42,526 Epoch 348: Total Training Recognition Loss 0.25  Total Training Translation Loss 12.66 
2024-02-08 15:23:42,527 EPOCH 349
2024-02-08 15:23:47,595 Epoch 349: Total Training Recognition Loss 0.22  Total Training Translation Loss 8.38 
2024-02-08 15:23:47,596 EPOCH 350
2024-02-08 15:23:48,932 [Epoch: 350 Step: 00023400] Batch Recognition Loss:   0.000493 => Gls Tokens per Sec:     2036 || Batch Translation Loss:   0.042886 => Txt Tokens per Sec:     5430 || Lr: 0.000100
2024-02-08 15:23:52,930 Epoch 350: Total Training Recognition Loss 0.14  Total Training Translation Loss 4.40 
2024-02-08 15:23:52,930 EPOCH 351
2024-02-08 15:23:56,644 [Epoch: 351 Step: 00023500] Batch Recognition Loss:   0.015318 => Gls Tokens per Sec:     2128 || Batch Translation Loss:   0.150966 => Txt Tokens per Sec:     5919 || Lr: 0.000100
2024-02-08 15:23:57,998 Epoch 351: Total Training Recognition Loss 0.18  Total Training Translation Loss 7.24 
2024-02-08 15:23:57,999 EPOCH 352
2024-02-08 15:24:03,397 Epoch 352: Total Training Recognition Loss 0.14  Total Training Translation Loss 7.10 
2024-02-08 15:24:03,398 EPOCH 353
2024-02-08 15:24:04,454 [Epoch: 353 Step: 00023600] Batch Recognition Loss:   0.000230 => Gls Tokens per Sec:     2427 || Batch Translation Loss:   0.082422 => Txt Tokens per Sec:     6531 || Lr: 0.000100
2024-02-08 15:24:08,611 Epoch 353: Total Training Recognition Loss 0.12  Total Training Translation Loss 4.49 
2024-02-08 15:24:08,611 EPOCH 354
2024-02-08 15:24:12,488 [Epoch: 354 Step: 00023700] Batch Recognition Loss:   0.006798 => Gls Tokens per Sec:     1997 || Batch Translation Loss:   0.027030 => Txt Tokens per Sec:     5513 || Lr: 0.000100
2024-02-08 15:24:13,998 Epoch 354: Total Training Recognition Loss 0.12  Total Training Translation Loss 4.70 
2024-02-08 15:24:13,999 EPOCH 355
2024-02-08 15:24:19,399 Epoch 355: Total Training Recognition Loss 0.11  Total Training Translation Loss 4.01 
2024-02-08 15:24:19,400 EPOCH 356
2024-02-08 15:24:20,459 [Epoch: 356 Step: 00023800] Batch Recognition Loss:   0.001191 => Gls Tokens per Sec:     2271 || Batch Translation Loss:   0.017029 => Txt Tokens per Sec:     6050 || Lr: 0.000100
2024-02-08 15:24:24,982 Epoch 356: Total Training Recognition Loss 0.10  Total Training Translation Loss 5.15 
2024-02-08 15:24:24,982 EPOCH 357
2024-02-08 15:24:29,040 [Epoch: 357 Step: 00023900] Batch Recognition Loss:   0.000405 => Gls Tokens per Sec:     1869 || Batch Translation Loss:   0.049269 => Txt Tokens per Sec:     5244 || Lr: 0.000100
2024-02-08 15:24:30,518 Epoch 357: Total Training Recognition Loss 0.05  Total Training Translation Loss 4.72 
2024-02-08 15:24:30,518 EPOCH 358
2024-02-08 15:24:35,404 Epoch 358: Total Training Recognition Loss 0.08  Total Training Translation Loss 4.32 
2024-02-08 15:24:35,405 EPOCH 359
2024-02-08 15:24:36,817 [Epoch: 359 Step: 00024000] Batch Recognition Loss:   0.000564 => Gls Tokens per Sec:     1588 || Batch Translation Loss:   0.780096 => Txt Tokens per Sec:     4498 || Lr: 0.000100
2024-02-08 15:24:45,373 Validation result at epoch 359, step    24000: duration: 8.5550s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 3.25261	Translation Loss: 93765.86719	PPL: 11676.66699
	Eval Metric: BLEU
	WER 4.66	(DEL: 0.00,	INS: 0.00,	SUB: 4.66)
	BLEU-4 0.51	(BLEU-1: 10.21,	BLEU-2: 3.11,	BLEU-3: 1.17,	BLEU-4: 0.51)
	CHRF 17.00	ROUGE 8.68
2024-02-08 15:24:45,374 Logging Recognition and Translation Outputs
2024-02-08 15:24:45,374 ========================================================================================================================
2024-02-08 15:24:45,374 Logging Sequence: 171_2.00
2024-02-08 15:24:45,374 	Gloss Reference :	A B+C+D+E  
2024-02-08 15:24:45,374 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-08 15:24:45,374 	Gloss Alignment :	  S        
2024-02-08 15:24:45,375 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:24:45,376 	Text Reference  :	as you might all know that the ipl is      about to    end     the finals are    on       28th   may  
2024-02-08 15:24:45,376 	Text Hypothesis :	** *** ***** *** **** **** on  16  october in    hansi haryana the ****** police summoned yuvraj singh
2024-02-08 15:24:45,376 	Text Alignment  :	D  D   D     D   D    D    S   S   S       S     S     S           D      S      S        S      S    
2024-02-08 15:24:45,376 ========================================================================================================================
2024-02-08 15:24:45,376 Logging Sequence: 119_33.00
2024-02-08 15:24:45,376 	Gloss Reference :	A B+C+D+E
2024-02-08 15:24:45,377 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:24:45,377 	Gloss Alignment :	         
2024-02-08 15:24:45,377 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:24:45,378 	Text Reference  :	he **** ******* * **** *** *** ******* ********** ******* wanted to *** gift  35 people    wow wonderful
2024-02-08 15:24:45,378 	Text Hypothesis :	he then noticed a gift how was popular footballer numbers next   to the staff to celebrate the gift     
2024-02-08 15:24:45,378 	Text Alignment  :	   I    I       I I    I   I   I       I          I       S         I   S     S  S         S   S        
2024-02-08 15:24:45,378 ========================================================================================================================
2024-02-08 15:24:45,378 Logging Sequence: 158_131.00
2024-02-08 15:24:45,378 	Gloss Reference :	A B+C+D+E
2024-02-08 15:24:45,379 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:24:45,379 	Gloss Alignment :	         
2024-02-08 15:24:45,379 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:24:45,380 	Text Reference  :	on 10th april 2023 there was a match between rcb     and   lsg  in    bengaluru
2024-02-08 15:24:45,380 	Text Hypothesis :	do you  know  that daley is  a ***** deaf    cricket along with their choice   
2024-02-08 15:24:45,380 	Text Alignment  :	S  S    S     S    S     S     D     S       S       S     S    S     S        
2024-02-08 15:24:45,380 ========================================================================================================================
2024-02-08 15:24:45,380 Logging Sequence: 164_412.00
2024-02-08 15:24:45,381 	Gloss Reference :	A B+C+D+E
2024-02-08 15:24:45,381 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:24:45,381 	Gloss Alignment :	         
2024-02-08 15:24:45,381 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:24:45,382 	Text Reference  :	if you divide these two figures you will be shocked to know     that       each ball's worth  is rs  50         lakhs  
2024-02-08 15:24:45,382 	Text Hypothesis :	** *** ****** ***** *** ******* *** **** ** ******* ** reliance industries owns 51     shares of the viacom18's company
2024-02-08 15:24:45,382 	Text Alignment  :	D  D   D      D     D   D       D   D    D  D       D  S        S          S    S      S      S  S   S          S      
2024-02-08 15:24:45,382 ========================================================================================================================
2024-02-08 15:24:45,383 Logging Sequence: 159_112.00
2024-02-08 15:24:45,383 	Gloss Reference :	A B+C+D+E    
2024-02-08 15:24:45,383 	Gloss Hypothesis:	A B+C+D+E+D+E
2024-02-08 15:24:45,383 	Gloss Alignment :	  S          
2024-02-08 15:24:45,383 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:24:45,385 	Text Reference  :	******** kohli had revealed that **** before the tournament he    did not  touch his bat   for  a  month yes 1   month
2024-02-08 15:24:45,385 	Text Hypothesis :	mohammed shami has said     that both nabi   and t20        world cup many teams are being sent to focus on  his odi  
2024-02-08 15:24:45,386 	Text Alignment  :	I        S     S   S             I    S      S   S          S     S   S    S     S   S     S    S  S     S   S   S    
2024-02-08 15:24:45,386 ========================================================================================================================
2024-02-08 15:24:49,705 Epoch 359: Total Training Recognition Loss 0.08  Total Training Translation Loss 7.73 
2024-02-08 15:24:49,706 EPOCH 360
2024-02-08 15:24:53,550 [Epoch: 360 Step: 00024100] Batch Recognition Loss:   0.004915 => Gls Tokens per Sec:     1931 || Batch Translation Loss:   0.248798 => Txt Tokens per Sec:     5266 || Lr: 0.000100
2024-02-08 15:24:55,180 Epoch 360: Total Training Recognition Loss 0.12  Total Training Translation Loss 16.48 
2024-02-08 15:24:55,180 EPOCH 361
2024-02-08 15:25:00,214 Epoch 361: Total Training Recognition Loss 0.27  Total Training Translation Loss 12.30 
2024-02-08 15:25:00,215 EPOCH 362
2024-02-08 15:25:01,130 [Epoch: 362 Step: 00024200] Batch Recognition Loss:   0.000455 => Gls Tokens per Sec:     2276 || Batch Translation Loss:   0.117068 => Txt Tokens per Sec:     6029 || Lr: 0.000100
2024-02-08 15:25:05,630 Epoch 362: Total Training Recognition Loss 0.19  Total Training Translation Loss 9.47 
2024-02-08 15:25:05,631 EPOCH 363
2024-02-08 15:25:08,726 [Epoch: 363 Step: 00024300] Batch Recognition Loss:   0.003834 => Gls Tokens per Sec:     2346 || Batch Translation Loss:   0.701861 => Txt Tokens per Sec:     6360 || Lr: 0.000100
2024-02-08 15:25:10,771 Epoch 363: Total Training Recognition Loss 0.16  Total Training Translation Loss 12.41 
2024-02-08 15:25:10,772 EPOCH 364
2024-02-08 15:25:16,060 Epoch 364: Total Training Recognition Loss 0.14  Total Training Translation Loss 8.07 
2024-02-08 15:25:16,061 EPOCH 365
2024-02-08 15:25:16,915 [Epoch: 365 Step: 00024400] Batch Recognition Loss:   0.006094 => Gls Tokens per Sec:     2248 || Batch Translation Loss:   0.031985 => Txt Tokens per Sec:     6304 || Lr: 0.000100
2024-02-08 15:25:21,383 Epoch 365: Total Training Recognition Loss 0.10  Total Training Translation Loss 5.40 
2024-02-08 15:25:21,384 EPOCH 366
2024-02-08 15:25:25,193 [Epoch: 366 Step: 00024500] Batch Recognition Loss:   0.000604 => Gls Tokens per Sec:     1891 || Batch Translation Loss:   0.051763 => Txt Tokens per Sec:     5256 || Lr: 0.000100
2024-02-08 15:25:26,855 Epoch 366: Total Training Recognition Loss 0.12  Total Training Translation Loss 3.72 
2024-02-08 15:25:26,855 EPOCH 367
2024-02-08 15:25:31,947 Epoch 367: Total Training Recognition Loss 0.07  Total Training Translation Loss 4.23 
2024-02-08 15:25:31,948 EPOCH 368
2024-02-08 15:25:32,983 [Epoch: 368 Step: 00024600] Batch Recognition Loss:   0.000135 => Gls Tokens per Sec:     1702 || Batch Translation Loss:   0.029903 => Txt Tokens per Sec:     4937 || Lr: 0.000100
2024-02-08 15:25:37,366 Epoch 368: Total Training Recognition Loss 0.10  Total Training Translation Loss 3.79 
2024-02-08 15:25:37,367 EPOCH 369
2024-02-08 15:25:40,593 [Epoch: 369 Step: 00024700] Batch Recognition Loss:   0.001061 => Gls Tokens per Sec:     2183 || Batch Translation Loss:   0.029479 => Txt Tokens per Sec:     6046 || Lr: 0.000100
2024-02-08 15:25:42,593 Epoch 369: Total Training Recognition Loss 0.11  Total Training Translation Loss 5.44 
2024-02-08 15:25:42,594 EPOCH 370
2024-02-08 15:25:47,825 Epoch 370: Total Training Recognition Loss 0.15  Total Training Translation Loss 6.62 
2024-02-08 15:25:47,825 EPOCH 371
2024-02-08 15:25:48,513 [Epoch: 371 Step: 00024800] Batch Recognition Loss:   0.003997 => Gls Tokens per Sec:     2329 || Batch Translation Loss:   0.091610 => Txt Tokens per Sec:     6313 || Lr: 0.000100
2024-02-08 15:25:52,982 Epoch 371: Total Training Recognition Loss 0.10  Total Training Translation Loss 7.72 
2024-02-08 15:25:52,982 EPOCH 372
2024-02-08 15:25:56,078 [Epoch: 372 Step: 00024900] Batch Recognition Loss:   0.000481 => Gls Tokens per Sec:     2191 || Batch Translation Loss:   0.128935 => Txt Tokens per Sec:     5928 || Lr: 0.000100
2024-02-08 15:25:58,323 Epoch 372: Total Training Recognition Loss 0.11  Total Training Translation Loss 7.29 
2024-02-08 15:25:58,324 EPOCH 373
2024-02-08 15:26:03,783 Epoch 373: Total Training Recognition Loss 0.13  Total Training Translation Loss 9.42 
2024-02-08 15:26:03,783 EPOCH 374
2024-02-08 15:26:04,391 [Epoch: 374 Step: 00025000] Batch Recognition Loss:   0.011158 => Gls Tokens per Sec:     2372 || Batch Translation Loss:   0.091409 => Txt Tokens per Sec:     6570 || Lr: 0.000100
2024-02-08 15:26:08,937 Epoch 374: Total Training Recognition Loss 0.13  Total Training Translation Loss 9.40 
2024-02-08 15:26:08,937 EPOCH 375
2024-02-08 15:26:12,452 [Epoch: 375 Step: 00025100] Batch Recognition Loss:   0.004096 => Gls Tokens per Sec:     1913 || Batch Translation Loss:   0.064613 => Txt Tokens per Sec:     5333 || Lr: 0.000100
2024-02-08 15:26:14,551 Epoch 375: Total Training Recognition Loss 0.15  Total Training Translation Loss 7.73 
2024-02-08 15:26:14,552 EPOCH 376
2024-02-08 15:26:19,730 Epoch 376: Total Training Recognition Loss 0.08  Total Training Translation Loss 7.01 
2024-02-08 15:26:19,731 EPOCH 377
2024-02-08 15:26:20,343 [Epoch: 377 Step: 00025200] Batch Recognition Loss:   0.001546 => Gls Tokens per Sec:     2095 || Batch Translation Loss:   0.262683 => Txt Tokens per Sec:     5884 || Lr: 0.000100
2024-02-08 15:26:25,252 Epoch 377: Total Training Recognition Loss 0.12  Total Training Translation Loss 5.33 
2024-02-08 15:26:25,252 EPOCH 378
2024-02-08 15:26:28,364 [Epoch: 378 Step: 00025300] Batch Recognition Loss:   0.000679 => Gls Tokens per Sec:     2077 || Batch Translation Loss:   0.049530 => Txt Tokens per Sec:     5734 || Lr: 0.000100
2024-02-08 15:26:30,354 Epoch 378: Total Training Recognition Loss 0.11  Total Training Translation Loss 4.18 
2024-02-08 15:26:30,355 EPOCH 379
2024-02-08 15:26:35,304 Epoch 379: Total Training Recognition Loss 0.13  Total Training Translation Loss 4.39 
2024-02-08 15:26:35,305 EPOCH 380
2024-02-08 15:26:35,958 [Epoch: 380 Step: 00025400] Batch Recognition Loss:   0.003421 => Gls Tokens per Sec:     1720 || Batch Translation Loss:   0.036283 => Txt Tokens per Sec:     5043 || Lr: 0.000100
2024-02-08 15:26:40,873 Epoch 380: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.74 
2024-02-08 15:26:40,874 EPOCH 381
2024-02-08 15:26:43,791 [Epoch: 381 Step: 00025500] Batch Recognition Loss:   0.000802 => Gls Tokens per Sec:     2195 || Batch Translation Loss:   0.042791 => Txt Tokens per Sec:     5954 || Lr: 0.000100
2024-02-08 15:26:46,080 Epoch 381: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.20 
2024-02-08 15:26:46,080 EPOCH 382
2024-02-08 15:26:51,586 Epoch 382: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.55 
2024-02-08 15:26:51,586 EPOCH 383
2024-02-08 15:26:51,920 [Epoch: 383 Step: 00025600] Batch Recognition Loss:   0.000190 => Gls Tokens per Sec:     2883 || Batch Translation Loss:   0.015241 => Txt Tokens per Sec:     6027 || Lr: 0.000100
2024-02-08 15:26:57,233 Epoch 383: Total Training Recognition Loss 0.10  Total Training Translation Loss 6.27 
2024-02-08 15:26:57,233 EPOCH 384
2024-02-08 15:27:00,350 [Epoch: 384 Step: 00025700] Batch Recognition Loss:   0.000604 => Gls Tokens per Sec:     2003 || Batch Translation Loss:   0.279039 => Txt Tokens per Sec:     5397 || Lr: 0.000100
2024-02-08 15:27:02,609 Epoch 384: Total Training Recognition Loss 0.09  Total Training Translation Loss 8.48 
2024-02-08 15:27:02,610 EPOCH 385
2024-02-08 15:27:08,193 Epoch 385: Total Training Recognition Loss 0.15  Total Training Translation Loss 8.42 
2024-02-08 15:27:08,194 EPOCH 386
2024-02-08 15:27:08,641 [Epoch: 386 Step: 00025800] Batch Recognition Loss:   0.000238 => Gls Tokens per Sec:     1794 || Batch Translation Loss:   0.048424 => Txt Tokens per Sec:     5029 || Lr: 0.000100
2024-02-08 15:27:13,377 Epoch 386: Total Training Recognition Loss 0.20  Total Training Translation Loss 10.57 
2024-02-08 15:27:13,378 EPOCH 387
2024-02-08 15:27:16,755 [Epoch: 387 Step: 00025900] Batch Recognition Loss:   0.008423 => Gls Tokens per Sec:     1801 || Batch Translation Loss:   0.108955 => Txt Tokens per Sec:     5144 || Lr: 0.000100
2024-02-08 15:27:19,084 Epoch 387: Total Training Recognition Loss 0.17  Total Training Translation Loss 7.42 
2024-02-08 15:27:19,084 EPOCH 388
2024-02-08 15:27:24,361 Epoch 388: Total Training Recognition Loss 0.10  Total Training Translation Loss 8.50 
2024-02-08 15:27:24,362 EPOCH 389
2024-02-08 15:27:24,660 [Epoch: 389 Step: 00026000] Batch Recognition Loss:   0.000823 => Gls Tokens per Sec:     2148 || Batch Translation Loss:   0.029018 => Txt Tokens per Sec:     6037 || Lr: 0.000100
2024-02-08 15:27:33,211 Validation result at epoch 389, step    26000: duration: 8.5510s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 3.43288	Translation Loss: 92656.03125	PPL: 10451.46582
	Eval Metric: BLEU
	WER 4.24	(DEL: 0.00,	INS: 0.00,	SUB: 4.24)
	BLEU-4 0.52	(BLEU-1: 10.27,	BLEU-2: 2.88,	BLEU-3: 1.08,	BLEU-4: 0.52)
	CHRF 16.75	ROUGE 8.84
2024-02-08 15:27:33,212 Logging Recognition and Translation Outputs
2024-02-08 15:27:33,212 ========================================================================================================================
2024-02-08 15:27:33,212 Logging Sequence: 166_243.00
2024-02-08 15:27:33,213 	Gloss Reference :	A B+C+D+E
2024-02-08 15:27:33,213 	Gloss Hypothesis:	A B+C+D  
2024-02-08 15:27:33,213 	Gloss Alignment :	  S      
2024-02-08 15:27:33,213 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:27:33,214 	Text Reference  :	*** ********* *********** ********* *** ***** ** icc     worked with members boards like bcci pcb   cricket australia etc 
2024-02-08 15:27:33,214 	Text Hypothesis :	the broadcast advertisers ticketing etc would be decided by     the  board   of     the  2    teams playing the       test
2024-02-08 15:27:33,214 	Text Alignment  :	I   I         I           I         I   I     I  S       S      S    S       S      S    S    S     S       S         S   
2024-02-08 15:27:33,214 ========================================================================================================================
2024-02-08 15:27:33,215 Logging Sequence: 59_152.00
2024-02-08 15:27:33,215 	Gloss Reference :	A B+C+D+E
2024-02-08 15:27:33,215 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:27:33,215 	Gloss Alignment :	         
2024-02-08 15:27:33,215 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:27:33,216 	Text Reference  :	*** the organisers encouraged athletes to    use the    condoms in        their home countries
2024-02-08 15:27:33,216 	Text Hypothesis :	for the olympics   are        2        years and people are     scheduled on    23rd may      
2024-02-08 15:27:33,216 	Text Alignment  :	I       S          S          S        S     S   S      S       S         S     S    S        
2024-02-08 15:27:33,217 ========================================================================================================================
2024-02-08 15:27:33,217 Logging Sequence: 145_52.00
2024-02-08 15:27:33,217 	Gloss Reference :	A B+C+D+E
2024-02-08 15:27:33,217 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:27:33,217 	Gloss Alignment :	         
2024-02-08 15:27:33,217 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:27:33,218 	Text Reference  :	her name was dropped despite having qualified as she was   the only female               athlete
2024-02-08 15:27:33,218 	Text Hypothesis :	*** **** *** ******* ******* ****** ********* ** *** dhoni is  a    once-in-a-generation player 
2024-02-08 15:27:33,218 	Text Alignment  :	D   D    D   D       D       D      D         D  D   S     S   S    S                    S      
2024-02-08 15:27:33,218 ========================================================================================================================
2024-02-08 15:27:33,218 Logging Sequence: 172_163.00
2024-02-08 15:27:33,219 	Gloss Reference :	A B+C+D+E
2024-02-08 15:27:33,219 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:27:33,219 	Gloss Alignment :	         
2024-02-08 15:27:33,219 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:27:33,221 	Text Reference  :	** if  the match starts      anywhere between     730       pm  to   935  pm  a    full 20-over match can    be  played
2024-02-08 15:27:33,221 	Text Hypothesis :	as you are many  dignitaries indian   celebrities filmstars etc have been not stop her  probe   and   submit the report
2024-02-08 15:27:33,221 	Text Alignment  :	I  S   S   S     S           S        S           S         S   S    S    S   S    S    S       S     S      S   S     
2024-02-08 15:27:33,221 ========================================================================================================================
2024-02-08 15:27:33,221 Logging Sequence: 150_20.00
2024-02-08 15:27:33,222 	Gloss Reference :	A B+C+D+E
2024-02-08 15:27:33,222 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:27:33,222 	Gloss Alignment :	         
2024-02-08 15:27:33,222 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:27:33,223 	Text Reference  :	after *** ******** ** *** a      tough   match    india  won the saff  championship 2023 title
2024-02-08 15:27:33,223 	Text Hypothesis :	after his accident he was played between pakistan people are 4   years and          3    days 
2024-02-08 15:27:33,223 	Text Alignment  :	      I   I        I  I   S      S       S        S      S   S   S     S            S    S    
2024-02-08 15:27:33,223 ========================================================================================================================
2024-02-08 15:27:38,410 Epoch 389: Total Training Recognition Loss 0.11  Total Training Translation Loss 11.52 
2024-02-08 15:27:38,411 EPOCH 390
2024-02-08 15:27:41,263 [Epoch: 390 Step: 00026100] Batch Recognition Loss:   0.000528 => Gls Tokens per Sec:     2041 || Batch Translation Loss:   0.177018 => Txt Tokens per Sec:     5388 || Lr: 0.000100
2024-02-08 15:27:43,829 Epoch 390: Total Training Recognition Loss 0.18  Total Training Translation Loss 8.66 
2024-02-08 15:27:43,829 EPOCH 391
2024-02-08 15:27:48,926 Epoch 391: Total Training Recognition Loss 0.13  Total Training Translation Loss 9.45 
2024-02-08 15:27:48,926 EPOCH 392
2024-02-08 15:27:49,131 [Epoch: 392 Step: 00026200] Batch Recognition Loss:   0.001154 => Gls Tokens per Sec:     2365 || Batch Translation Loss:   0.104471 => Txt Tokens per Sec:     7030 || Lr: 0.000100
2024-02-08 15:27:53,742 Epoch 392: Total Training Recognition Loss 0.19  Total Training Translation Loss 6.56 
2024-02-08 15:27:53,742 EPOCH 393
2024-02-08 15:27:56,776 [Epoch: 393 Step: 00026300] Batch Recognition Loss:   0.001919 => Gls Tokens per Sec:     1866 || Batch Translation Loss:   0.052363 => Txt Tokens per Sec:     5222 || Lr: 0.000100
2024-02-08 15:27:59,166 Epoch 393: Total Training Recognition Loss 0.11  Total Training Translation Loss 4.37 
2024-02-08 15:27:59,166 EPOCH 394
2024-02-08 15:28:04,316 Epoch 394: Total Training Recognition Loss 0.11  Total Training Translation Loss 4.11 
2024-02-08 15:28:04,316 EPOCH 395
2024-02-08 15:28:04,496 [Epoch: 395 Step: 00026400] Batch Recognition Loss:   0.000277 => Gls Tokens per Sec:     1798 || Batch Translation Loss:   0.021139 => Txt Tokens per Sec:     5011 || Lr: 0.000100
2024-02-08 15:28:09,917 Epoch 395: Total Training Recognition Loss 0.16  Total Training Translation Loss 4.38 
2024-02-08 15:28:09,917 EPOCH 396
2024-02-08 15:28:12,778 [Epoch: 396 Step: 00026500] Batch Recognition Loss:   0.001252 => Gls Tokens per Sec:     1923 || Batch Translation Loss:   0.034439 => Txt Tokens per Sec:     5372 || Lr: 0.000100
2024-02-08 15:28:15,317 Epoch 396: Total Training Recognition Loss 0.10  Total Training Translation Loss 2.93 
2024-02-08 15:28:15,317 EPOCH 397
2024-02-08 15:28:20,809 Epoch 397: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.98 
2024-02-08 15:28:20,809 EPOCH 398
2024-02-08 15:28:20,888 [Epoch: 398 Step: 00026600] Batch Recognition Loss:   0.000459 => Gls Tokens per Sec:     2072 || Batch Translation Loss:   0.019588 => Txt Tokens per Sec:     5593 || Lr: 0.000100
2024-02-08 15:28:25,989 Epoch 398: Total Training Recognition Loss 0.08  Total Training Translation Loss 6.49 
2024-02-08 15:28:25,990 EPOCH 399
2024-02-08 15:28:28,615 [Epoch: 399 Step: 00026700] Batch Recognition Loss:   0.000248 => Gls Tokens per Sec:     2036 || Batch Translation Loss:   0.054350 => Txt Tokens per Sec:     5445 || Lr: 0.000100
2024-02-08 15:28:31,481 Epoch 399: Total Training Recognition Loss 0.12  Total Training Translation Loss 6.22 
2024-02-08 15:28:31,481 EPOCH 400
2024-02-08 15:28:36,434 [Epoch: 400 Step: 00026800] Batch Recognition Loss:   0.003312 => Gls Tokens per Sec:     2145 || Batch Translation Loss:   0.104618 => Txt Tokens per Sec:     5933 || Lr: 0.000100
2024-02-08 15:28:36,434 Epoch 400: Total Training Recognition Loss 0.12  Total Training Translation Loss 8.25 
2024-02-08 15:28:36,434 EPOCH 401
2024-02-08 15:28:41,932 Epoch 401: Total Training Recognition Loss 0.21  Total Training Translation Loss 12.95 
2024-02-08 15:28:41,932 EPOCH 402
2024-02-08 15:28:44,272 [Epoch: 402 Step: 00026900] Batch Recognition Loss:   0.000704 => Gls Tokens per Sec:     2257 || Batch Translation Loss:   0.040534 => Txt Tokens per Sec:     5977 || Lr: 0.000100
2024-02-08 15:28:46,842 Epoch 402: Total Training Recognition Loss 0.25  Total Training Translation Loss 10.06 
2024-02-08 15:28:46,842 EPOCH 403
2024-02-08 15:28:52,384 [Epoch: 403 Step: 00027000] Batch Recognition Loss:   0.001526 => Gls Tokens per Sec:     1888 || Batch Translation Loss:   0.069666 => Txt Tokens per Sec:     5231 || Lr: 0.000100
2024-02-08 15:28:52,441 Epoch 403: Total Training Recognition Loss 0.17  Total Training Translation Loss 6.31 
2024-02-08 15:28:52,441 EPOCH 404
2024-02-08 15:28:57,962 Epoch 404: Total Training Recognition Loss 0.15  Total Training Translation Loss 3.71 
2024-02-08 15:28:57,962 EPOCH 405
2024-02-08 15:29:00,404 [Epoch: 405 Step: 00027100] Batch Recognition Loss:   0.000316 => Gls Tokens per Sec:     2098 || Batch Translation Loss:   0.025373 => Txt Tokens per Sec:     5590 || Lr: 0.000100
2024-02-08 15:29:03,457 Epoch 405: Total Training Recognition Loss 0.08  Total Training Translation Loss 4.07 
2024-02-08 15:29:03,458 EPOCH 406
2024-02-08 15:29:08,298 [Epoch: 406 Step: 00027200] Batch Recognition Loss:   0.001490 => Gls Tokens per Sec:     2129 || Batch Translation Loss:   0.128342 => Txt Tokens per Sec:     5932 || Lr: 0.000100
2024-02-08 15:29:08,388 Epoch 406: Total Training Recognition Loss 0.13  Total Training Translation Loss 5.90 
2024-02-08 15:29:08,389 EPOCH 407
2024-02-08 15:29:13,752 Epoch 407: Total Training Recognition Loss 0.08  Total Training Translation Loss 5.83 
2024-02-08 15:29:13,753 EPOCH 408
2024-02-08 15:29:15,915 [Epoch: 408 Step: 00027300] Batch Recognition Loss:   0.000690 => Gls Tokens per Sec:     2295 || Batch Translation Loss:   0.060111 => Txt Tokens per Sec:     6125 || Lr: 0.000100
2024-02-08 15:29:18,747 Epoch 408: Total Training Recognition Loss 0.13  Total Training Translation Loss 4.23 
2024-02-08 15:29:18,747 EPOCH 409
2024-02-08 15:29:23,462 [Epoch: 409 Step: 00027400] Batch Recognition Loss:   0.000325 => Gls Tokens per Sec:     2151 || Batch Translation Loss:   0.025594 => Txt Tokens per Sec:     5942 || Lr: 0.000100
2024-02-08 15:29:23,679 Epoch 409: Total Training Recognition Loss 0.09  Total Training Translation Loss 3.92 
2024-02-08 15:29:23,679 EPOCH 410
2024-02-08 15:29:28,756 Epoch 410: Total Training Recognition Loss 0.10  Total Training Translation Loss 4.26 
2024-02-08 15:29:28,757 EPOCH 411
2024-02-08 15:29:30,971 [Epoch: 411 Step: 00027500] Batch Recognition Loss:   0.001035 => Gls Tokens per Sec:     2169 || Batch Translation Loss:   0.071814 => Txt Tokens per Sec:     5712 || Lr: 0.000100
2024-02-08 15:29:34,440 Epoch 411: Total Training Recognition Loss 0.07  Total Training Translation Loss 6.18 
2024-02-08 15:29:34,440 EPOCH 412
2024-02-08 15:29:39,599 [Epoch: 412 Step: 00027600] Batch Recognition Loss:   0.000258 => Gls Tokens per Sec:     1935 || Batch Translation Loss:   0.060737 => Txt Tokens per Sec:     5395 || Lr: 0.000100
2024-02-08 15:29:39,838 Epoch 412: Total Training Recognition Loss 0.05  Total Training Translation Loss 5.85 
2024-02-08 15:29:39,838 EPOCH 413
2024-02-08 15:29:45,276 Epoch 413: Total Training Recognition Loss 0.10  Total Training Translation Loss 7.01 
2024-02-08 15:29:45,276 EPOCH 414
2024-02-08 15:29:47,472 [Epoch: 414 Step: 00027700] Batch Recognition Loss:   0.000327 => Gls Tokens per Sec:     2114 || Batch Translation Loss:   0.042286 => Txt Tokens per Sec:     5788 || Lr: 0.000100
2024-02-08 15:29:50,663 Epoch 414: Total Training Recognition Loss 0.18  Total Training Translation Loss 9.01 
2024-02-08 15:29:50,663 EPOCH 415
2024-02-08 15:29:55,667 [Epoch: 415 Step: 00027800] Batch Recognition Loss:   0.018486 => Gls Tokens per Sec:     1963 || Batch Translation Loss:   0.074700 => Txt Tokens per Sec:     5405 || Lr: 0.000100
2024-02-08 15:29:56,104 Epoch 415: Total Training Recognition Loss 0.17  Total Training Translation Loss 9.11 
2024-02-08 15:29:56,105 EPOCH 416
2024-02-08 15:30:01,333 Epoch 416: Total Training Recognition Loss 0.21  Total Training Translation Loss 7.89 
2024-02-08 15:30:01,333 EPOCH 417
2024-02-08 15:30:03,436 [Epoch: 417 Step: 00027900] Batch Recognition Loss:   0.000304 => Gls Tokens per Sec:     2131 || Batch Translation Loss:   0.070051 => Txt Tokens per Sec:     5848 || Lr: 0.000100
2024-02-08 15:30:06,975 Epoch 417: Total Training Recognition Loss 0.17  Total Training Translation Loss 7.18 
2024-02-08 15:30:06,976 EPOCH 418
2024-02-08 15:30:11,648 [Epoch: 418 Step: 00028000] Batch Recognition Loss:   0.000419 => Gls Tokens per Sec:     2089 || Batch Translation Loss:   0.034236 => Txt Tokens per Sec:     5778 || Lr: 0.000100
2024-02-08 15:30:20,838 Validation result at epoch 418, step    28000: duration: 9.1883s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 3.61357	Translation Loss: 91873.75000	PPL: 9665.93359
	Eval Metric: BLEU
	WER 3.88	(DEL: 0.00,	INS: 0.00,	SUB: 3.88)
	BLEU-4 0.54	(BLEU-1: 10.92,	BLEU-2: 3.35,	BLEU-3: 1.26,	BLEU-4: 0.54)
	CHRF 17.36	ROUGE 9.17
2024-02-08 15:30:20,839 Logging Recognition and Translation Outputs
2024-02-08 15:30:20,839 ========================================================================================================================
2024-02-08 15:30:20,839 Logging Sequence: 156_288.00
2024-02-08 15:30:20,839 	Gloss Reference :	A B+C+D+E
2024-02-08 15:30:20,839 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:30:20,840 	Gloss Alignment :	         
2024-02-08 15:30:20,840 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:30:20,841 	Text Reference  :	pooran led the team to victory miny   became winners of          the **** 1st    season  
2024-02-08 15:30:20,841 	Text Hypothesis :	****** *** *** **** ** then    people have   seen    celebrating the most famous wrestler
2024-02-08 15:30:20,841 	Text Alignment  :	D      D   D   D    D  S       S      S      S       S               I    S      S       
2024-02-08 15:30:20,841 ========================================================================================================================
2024-02-08 15:30:20,841 Logging Sequence: 98_135.00
2024-02-08 15:30:20,841 	Gloss Reference :	A B+C+D+E
2024-02-08 15:30:20,841 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:30:20,842 	Gloss Alignment :	         
2024-02-08 15:30:20,842 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:30:20,842 	Text Reference  :	*** however due      to *** the rise in coronavirus cases the tournament was shifted
2024-02-08 15:30:20,843 	Text Hypothesis :	and is      expected to tie the **** ** number      of    the tournament *** *******
2024-02-08 15:30:20,843 	Text Alignment  :	I   S       S           I       D    D  S           S                    D   D      
2024-02-08 15:30:20,843 ========================================================================================================================
2024-02-08 15:30:20,843 Logging Sequence: 161_47.00
2024-02-08 15:30:20,843 	Gloss Reference :	A B+C+D+E
2024-02-08 15:30:20,843 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:30:20,843 	Gloss Alignment :	         
2024-02-08 15:30:20,844 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:30:20,844 	Text Reference  :	he requested confidentiality as he was planning to  make an  official announcement
2024-02-08 15:30:20,844 	Text Hypothesis :	** ********* *************** ** ** we  were     all out  for the      series      
2024-02-08 15:30:20,845 	Text Alignment  :	D  D         D               D  D  S   S        S   S    S   S        S           
2024-02-08 15:30:20,845 ========================================================================================================================
2024-02-08 15:30:20,845 Logging Sequence: 131_159.00
2024-02-08 15:30:20,845 	Gloss Reference :	A B+C+D+E  
2024-02-08 15:30:20,845 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-08 15:30:20,845 	Gloss Alignment :	  S        
2024-02-08 15:30:20,845 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:30:20,846 	Text Reference  :	****** chanu    also   met    biren singh   following the meeting singh described chanu as our nation' pride      
2024-02-08 15:30:20,846 	Text Hypothesis :	sports minister anurag thakur also  present at        the ******* ***** ********* ***** ** *** ******* interaction
2024-02-08 15:30:20,847 	Text Alignment  :	I      S        S      S      S     S       S             D       D     D         D     D  D   D       S          
2024-02-08 15:30:20,847 ========================================================================================================================
2024-02-08 15:30:20,847 Logging Sequence: 137_167.00
2024-02-08 15:30:20,847 	Gloss Reference :	A B+C+D+E
2024-02-08 15:30:20,847 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:30:20,847 	Gloss Alignment :	         
2024-02-08 15:30:20,847 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:30:20,849 	Text Reference  :	however after 630 pm there will be     certain fan  zones        where beer will    be        available     and *** nowhere else 
2024-02-08 15:30:20,849 	Text Hypothesis :	******* ***** *** ** ***** the  mumbai indians team participated in    the  winning athletics championships and way to      catch
2024-02-08 15:30:20,849 	Text Alignment  :	D       D     D   D  D     S    S      S       S    S            S     S    S       S         S                 I   S       S    
2024-02-08 15:30:20,849 ========================================================================================================================
2024-02-08 15:30:21,301 Epoch 418: Total Training Recognition Loss 0.48  Total Training Translation Loss 5.09 
2024-02-08 15:30:21,301 EPOCH 419
2024-02-08 15:30:26,874 Epoch 419: Total Training Recognition Loss 0.78  Total Training Translation Loss 3.72 
2024-02-08 15:30:26,875 EPOCH 420
2024-02-08 15:30:29,068 [Epoch: 420 Step: 00028100] Batch Recognition Loss:   0.000673 => Gls Tokens per Sec:     1970 || Batch Translation Loss:   0.029171 => Txt Tokens per Sec:     5479 || Lr: 0.000050
2024-02-08 15:30:32,270 Epoch 420: Total Training Recognition Loss 0.33  Total Training Translation Loss 1.77 
2024-02-08 15:30:32,271 EPOCH 421
2024-02-08 15:30:36,756 [Epoch: 421 Step: 00028200] Batch Recognition Loss:   0.000615 => Gls Tokens per Sec:     2118 || Batch Translation Loss:   0.015274 => Txt Tokens per Sec:     5820 || Lr: 0.000050
2024-02-08 15:30:37,434 Epoch 421: Total Training Recognition Loss 0.19  Total Training Translation Loss 1.42 
2024-02-08 15:30:37,434 EPOCH 422
2024-02-08 15:30:42,995 Epoch 422: Total Training Recognition Loss 0.12  Total Training Translation Loss 1.33 
2024-02-08 15:30:42,996 EPOCH 423
2024-02-08 15:30:44,953 [Epoch: 423 Step: 00028300] Batch Recognition Loss:   0.000600 => Gls Tokens per Sec:     2077 || Batch Translation Loss:   0.016084 => Txt Tokens per Sec:     5573 || Lr: 0.000050
2024-02-08 15:30:47,942 Epoch 423: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.22 
2024-02-08 15:30:47,942 EPOCH 424
2024-02-08 15:30:52,944 [Epoch: 424 Step: 00028400] Batch Recognition Loss:   0.000245 => Gls Tokens per Sec:     1868 || Batch Translation Loss:   0.012539 => Txt Tokens per Sec:     5193 || Lr: 0.000050
2024-02-08 15:30:53,522 Epoch 424: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.89 
2024-02-08 15:30:53,523 EPOCH 425
2024-02-08 15:30:58,242 Epoch 425: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.97 
2024-02-08 15:30:58,242 EPOCH 426
2024-02-08 15:31:00,527 [Epoch: 426 Step: 00028500] Batch Recognition Loss:   0.002586 => Gls Tokens per Sec:     1708 || Batch Translation Loss:   0.017666 => Txt Tokens per Sec:     4898 || Lr: 0.000050
2024-02-08 15:31:03,830 Epoch 426: Total Training Recognition Loss 0.12  Total Training Translation Loss 2.71 
2024-02-08 15:31:03,830 EPOCH 427
2024-02-08 15:31:08,134 [Epoch: 427 Step: 00028600] Batch Recognition Loss:   0.000771 => Gls Tokens per Sec:     2134 || Batch Translation Loss:   0.014641 => Txt Tokens per Sec:     5910 || Lr: 0.000050
2024-02-08 15:31:08,749 Epoch 427: Total Training Recognition Loss 0.10  Total Training Translation Loss 1.30 
2024-02-08 15:31:08,749 EPOCH 428
2024-02-08 15:31:14,421 Epoch 428: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.21 
2024-02-08 15:31:14,421 EPOCH 429
2024-02-08 15:31:16,077 [Epoch: 429 Step: 00028700] Batch Recognition Loss:   0.000301 => Gls Tokens per Sec:     2320 || Batch Translation Loss:   0.070642 => Txt Tokens per Sec:     6385 || Lr: 0.000050
2024-02-08 15:31:19,386 Epoch 429: Total Training Recognition Loss 0.09  Total Training Translation Loss 1.29 
2024-02-08 15:31:19,386 EPOCH 430
2024-02-08 15:31:24,249 [Epoch: 430 Step: 00028800] Batch Recognition Loss:   0.000122 => Gls Tokens per Sec:     1855 || Batch Translation Loss:   0.014043 => Txt Tokens per Sec:     5056 || Lr: 0.000050
2024-02-08 15:31:25,107 Epoch 430: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.11 
2024-02-08 15:31:25,107 EPOCH 431
2024-02-08 15:31:30,573 Epoch 431: Total Training Recognition Loss 0.06  Total Training Translation Loss 0.94 
2024-02-08 15:31:30,573 EPOCH 432
2024-02-08 15:31:32,440 [Epoch: 432 Step: 00028900] Batch Recognition Loss:   0.000237 => Gls Tokens per Sec:     1919 || Batch Translation Loss:   0.013721 => Txt Tokens per Sec:     5118 || Lr: 0.000050
2024-02-08 15:31:35,955 Epoch 432: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.02 
2024-02-08 15:31:35,955 EPOCH 433
2024-02-08 15:31:39,943 [Epoch: 433 Step: 00029000] Batch Recognition Loss:   0.000469 => Gls Tokens per Sec:     2247 || Batch Translation Loss:   0.005285 => Txt Tokens per Sec:     6200 || Lr: 0.000050
2024-02-08 15:31:40,787 Epoch 433: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.87 
2024-02-08 15:31:40,788 EPOCH 434
2024-02-08 15:31:46,237 Epoch 434: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.03 
2024-02-08 15:31:46,238 EPOCH 435
2024-02-08 15:31:47,827 [Epoch: 435 Step: 00029100] Batch Recognition Loss:   0.000199 => Gls Tokens per Sec:     2217 || Batch Translation Loss:   0.013221 => Txt Tokens per Sec:     6115 || Lr: 0.000050
2024-02-08 15:31:51,212 Epoch 435: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.92 
2024-02-08 15:31:51,213 EPOCH 436
2024-02-08 15:31:55,692 [Epoch: 436 Step: 00029200] Batch Recognition Loss:   0.000146 => Gls Tokens per Sec:     1943 || Batch Translation Loss:   0.015980 => Txt Tokens per Sec:     5252 || Lr: 0.000050
2024-02-08 15:31:56,852 Epoch 436: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.01 
2024-02-08 15:31:56,852 EPOCH 437
2024-02-08 15:32:02,117 Epoch 437: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.73 
2024-02-08 15:32:02,118 EPOCH 438
2024-02-08 15:32:03,764 [Epoch: 438 Step: 00029300] Batch Recognition Loss:   0.000200 => Gls Tokens per Sec:     2041 || Batch Translation Loss:   0.014384 => Txt Tokens per Sec:     5553 || Lr: 0.000050
2024-02-08 15:32:07,602 Epoch 438: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.64 
2024-02-08 15:32:07,602 EPOCH 439
2024-02-08 15:32:11,549 [Epoch: 439 Step: 00029400] Batch Recognition Loss:   0.000208 => Gls Tokens per Sec:     2190 || Batch Translation Loss:   0.014265 => Txt Tokens per Sec:     6014 || Lr: 0.000050
2024-02-08 15:32:12,444 Epoch 439: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.47 
2024-02-08 15:32:12,444 EPOCH 440
2024-02-08 15:32:18,040 Epoch 440: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.85 
2024-02-08 15:32:18,041 EPOCH 441
2024-02-08 15:32:19,369 [Epoch: 441 Step: 00029500] Batch Recognition Loss:   0.000142 => Gls Tokens per Sec:     2411 || Batch Translation Loss:   0.012447 => Txt Tokens per Sec:     6503 || Lr: 0.000050
2024-02-08 15:32:23,041 Epoch 441: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.91 
2024-02-08 15:32:23,041 EPOCH 442
2024-02-08 15:32:27,737 [Epoch: 442 Step: 00029600] Batch Recognition Loss:   0.000205 => Gls Tokens per Sec:     1806 || Batch Translation Loss:   0.015987 => Txt Tokens per Sec:     5055 || Lr: 0.000050
2024-02-08 15:32:28,772 Epoch 442: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.33 
2024-02-08 15:32:28,772 EPOCH 443
2024-02-08 15:32:33,921 Epoch 443: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.73 
2024-02-08 15:32:33,921 EPOCH 444
2024-02-08 15:32:35,552 [Epoch: 444 Step: 00029700] Batch Recognition Loss:   0.000294 => Gls Tokens per Sec:     1865 || Batch Translation Loss:   0.014218 => Txt Tokens per Sec:     4960 || Lr: 0.000050
2024-02-08 15:32:39,608 Epoch 444: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.84 
2024-02-08 15:32:39,609 EPOCH 445
2024-02-08 15:32:43,809 [Epoch: 445 Step: 00029800] Batch Recognition Loss:   0.000170 => Gls Tokens per Sec:     1958 || Batch Translation Loss:   0.022777 => Txt Tokens per Sec:     5435 || Lr: 0.000050
2024-02-08 15:32:44,953 Epoch 445: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.29 
2024-02-08 15:32:44,953 EPOCH 446
2024-02-08 15:32:50,482 Epoch 446: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.18 
2024-02-08 15:32:50,483 EPOCH 447
2024-02-08 15:32:52,045 [Epoch: 447 Step: 00029900] Batch Recognition Loss:   0.000268 => Gls Tokens per Sec:     1780 || Batch Translation Loss:   0.010116 => Txt Tokens per Sec:     5287 || Lr: 0.000050
2024-02-08 15:32:55,338 Epoch 447: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.30 
2024-02-08 15:32:55,338 EPOCH 448
2024-02-08 15:32:59,665 [Epoch: 448 Step: 00030000] Batch Recognition Loss:   0.000251 => Gls Tokens per Sec:     1863 || Batch Translation Loss:   0.114374 => Txt Tokens per Sec:     5214 || Lr: 0.000050
2024-02-08 15:33:08,093 Validation result at epoch 448, step    30000: duration: 8.4272s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 3.44275	Translation Loss: 92996.65625	PPL: 10813.15625
	Eval Metric: BLEU
	WER 4.52	(DEL: 0.00,	INS: 0.00,	SUB: 4.52)
	BLEU-4 0.81	(BLEU-1: 10.36,	BLEU-2: 3.55,	BLEU-3: 1.55,	BLEU-4: 0.81)
	CHRF 16.68	ROUGE 9.20
2024-02-08 15:33:08,095 Logging Recognition and Translation Outputs
2024-02-08 15:33:08,095 ========================================================================================================================
2024-02-08 15:33:08,095 Logging Sequence: 146_102.00
2024-02-08 15:33:08,096 	Gloss Reference :	A B+C+D+E
2024-02-08 15:33:08,096 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:33:08,096 	Gloss Alignment :	         
2024-02-08 15:33:08,096 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:33:08,097 	Text Reference  :	famous indian champion players like kidambi srikanth and ashwini ponappa have tested     positive for    coronavirus
2024-02-08 15:33:08,097 	Text Hypothesis :	****** ****** ******** ******* **** ******* ******** *** ******* usman   is   australia' first    muslim player     
2024-02-08 15:33:08,097 	Text Alignment  :	D      D      D        D       D    D       D        D   D       S       S    S          S        S      S          
2024-02-08 15:33:08,097 ========================================================================================================================
2024-02-08 15:33:08,097 Logging Sequence: 53_178.00
2024-02-08 15:33:08,098 	Gloss Reference :	A B+C+D+E
2024-02-08 15:33:08,098 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:33:08,098 	Gloss Alignment :	         
2024-02-08 15:33:08,098 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:33:08,099 	Text Reference  :	the money   would help  all   those affected by the humanitarian crisis in afghanistan
2024-02-08 15:33:08,099 	Text Hypothesis :	we  request the   other squad for   privacy  at the entire       family in afghanistan
2024-02-08 15:33:08,099 	Text Alignment  :	S   S       S     S     S     S     S        S      S            S                    
2024-02-08 15:33:08,100 ========================================================================================================================
2024-02-08 15:33:08,100 Logging Sequence: 129_200.00
2024-02-08 15:33:08,100 	Gloss Reference :	A B+C+D+E  
2024-02-08 15:33:08,100 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-08 15:33:08,100 	Gloss Alignment :	  S        
2024-02-08 15:33:08,100 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:33:08,102 	Text Reference  :	**** the ioc would lose  about 4   billion if  the olympics were to     be  cancelled
2024-02-08 15:33:08,102 	Text Hypothesis :	this is  how fox   could use   the condoms and her unique   idea helped her win      
2024-02-08 15:33:08,102 	Text Alignment  :	I    S   S   S     S     S     S   S       S   S   S        S    S      S   S        
2024-02-08 15:33:08,102 ========================================================================================================================
2024-02-08 15:33:08,102 Logging Sequence: 77_2.00
2024-02-08 15:33:08,102 	Gloss Reference :	A B+C+D+E  
2024-02-08 15:33:08,103 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-08 15:33:08,103 	Gloss Alignment :	  S        
2024-02-08 15:33:08,103 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:33:08,104 	Text Reference  :	on 25th april the    ipl match between sunrisers hyderabad and delhi capitals ended in a tie    
2024-02-08 15:33:08,104 	Text Hypothesis :	** **** he    played 3   out   of      the       event     and ***** ******** ***** ** * england
2024-02-08 15:33:08,104 	Text Alignment  :	D  D    S     S      S   S     S       S         S             D     D        D     D  D S      
2024-02-08 15:33:08,104 ========================================================================================================================
2024-02-08 15:33:08,104 Logging Sequence: 119_170.00
2024-02-08 15:33:08,105 	Gloss Reference :	A B+C+D+E
2024-02-08 15:33:08,105 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:33:08,105 	Gloss Alignment :	         
2024-02-08 15:33:08,105 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:33:08,106 	Text Reference  :	they said it was a        proud moment messi is a   big hearted  man 
2024-02-08 15:33:08,106 	Text Hypothesis :	**** **** ** *** whenever she   gave   birth to win an  official here
2024-02-08 15:33:08,106 	Text Alignment  :	D    D    D  D   S        S     S      S     S  S   S   S        S   
2024-02-08 15:33:08,106 ========================================================================================================================
2024-02-08 15:33:09,481 Epoch 448: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.60 
2024-02-08 15:33:09,482 EPOCH 449
2024-02-08 15:33:14,806 Epoch 449: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.03 
2024-02-08 15:33:14,806 EPOCH 450
2024-02-08 15:33:15,982 [Epoch: 450 Step: 00030100] Batch Recognition Loss:   0.000659 => Gls Tokens per Sec:     2315 || Batch Translation Loss:   0.045908 => Txt Tokens per Sec:     6546 || Lr: 0.000050
2024-02-08 15:33:19,980 Epoch 450: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.44 
2024-02-08 15:33:19,980 EPOCH 451
2024-02-08 15:33:24,101 [Epoch: 451 Step: 00030200] Batch Recognition Loss:   0.000316 => Gls Tokens per Sec:     1917 || Batch Translation Loss:   0.025356 => Txt Tokens per Sec:     5447 || Lr: 0.000050
2024-02-08 15:33:25,244 Epoch 451: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.65 
2024-02-08 15:33:25,245 EPOCH 452
2024-02-08 15:33:30,686 Epoch 452: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.04 
2024-02-08 15:33:30,687 EPOCH 453
2024-02-08 15:33:32,095 [Epoch: 453 Step: 00030300] Batch Recognition Loss:   0.000740 => Gls Tokens per Sec:     1747 || Batch Translation Loss:   0.013439 => Txt Tokens per Sec:     4803 || Lr: 0.000050
2024-02-08 15:33:36,226 Epoch 453: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.66 
2024-02-08 15:33:36,226 EPOCH 454
2024-02-08 15:33:40,030 [Epoch: 454 Step: 00030400] Batch Recognition Loss:   0.000217 => Gls Tokens per Sec:     2035 || Batch Translation Loss:   0.031283 => Txt Tokens per Sec:     5457 || Lr: 0.000050
2024-02-08 15:33:41,749 Epoch 454: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.87 
2024-02-08 15:33:41,749 EPOCH 455
2024-02-08 15:33:47,283 Epoch 455: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.92 
2024-02-08 15:33:47,283 EPOCH 456
2024-02-08 15:33:48,591 [Epoch: 456 Step: 00030500] Batch Recognition Loss:   0.000203 => Gls Tokens per Sec:     1760 || Batch Translation Loss:   0.020969 => Txt Tokens per Sec:     4909 || Lr: 0.000050
2024-02-08 15:33:52,850 Epoch 456: Total Training Recognition Loss 0.11  Total Training Translation Loss 2.21 
2024-02-08 15:33:52,850 EPOCH 457
2024-02-08 15:33:56,847 [Epoch: 457 Step: 00030600] Batch Recognition Loss:   0.000331 => Gls Tokens per Sec:     1897 || Batch Translation Loss:   0.007354 => Txt Tokens per Sec:     5314 || Lr: 0.000050
2024-02-08 15:33:58,384 Epoch 457: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.09 
2024-02-08 15:33:58,385 EPOCH 458
2024-02-08 15:34:03,914 Epoch 458: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.43 
2024-02-08 15:34:03,914 EPOCH 459
2024-02-08 15:34:04,916 [Epoch: 459 Step: 00030700] Batch Recognition Loss:   0.000146 => Gls Tokens per Sec:     2238 || Batch Translation Loss:   0.020949 => Txt Tokens per Sec:     6409 || Lr: 0.000050
2024-02-08 15:34:08,478 Epoch 459: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.12 
2024-02-08 15:34:08,478 EPOCH 460
2024-02-08 15:34:12,439 [Epoch: 460 Step: 00030800] Batch Recognition Loss:   0.000176 => Gls Tokens per Sec:     1899 || Batch Translation Loss:   0.016518 => Txt Tokens per Sec:     5168 || Lr: 0.000050
2024-02-08 15:34:14,126 Epoch 460: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.65 
2024-02-08 15:34:14,126 EPOCH 461
2024-02-08 15:34:19,992 Epoch 461: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.35 
2024-02-08 15:34:19,992 EPOCH 462
2024-02-08 15:34:20,922 [Epoch: 462 Step: 00030900] Batch Recognition Loss:   0.000156 => Gls Tokens per Sec:     2245 || Batch Translation Loss:   0.022993 => Txt Tokens per Sec:     5947 || Lr: 0.000050
2024-02-08 15:34:25,568 Epoch 462: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.80 
2024-02-08 15:34:25,568 EPOCH 463
2024-02-08 15:34:29,448 [Epoch: 463 Step: 00031000] Batch Recognition Loss:   0.000179 => Gls Tokens per Sec:     1872 || Batch Translation Loss:   0.020704 => Txt Tokens per Sec:     5224 || Lr: 0.000050
2024-02-08 15:34:30,972 Epoch 463: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.36 
2024-02-08 15:34:30,972 EPOCH 464
2024-02-08 15:34:36,430 Epoch 464: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.51 
2024-02-08 15:34:36,430 EPOCH 465
2024-02-08 15:34:37,257 [Epoch: 465 Step: 00031100] Batch Recognition Loss:   0.000237 => Gls Tokens per Sec:     2324 || Batch Translation Loss:   0.012607 => Txt Tokens per Sec:     5754 || Lr: 0.000050
2024-02-08 15:34:41,840 Epoch 465: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.03 
2024-02-08 15:34:41,840 EPOCH 466
2024-02-08 15:34:45,693 [Epoch: 466 Step: 00031200] Batch Recognition Loss:   0.000191 => Gls Tokens per Sec:     1843 || Batch Translation Loss:   0.011928 => Txt Tokens per Sec:     5344 || Lr: 0.000050
2024-02-08 15:34:47,453 Epoch 466: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.76 
2024-02-08 15:34:47,454 EPOCH 467
2024-02-08 15:34:52,857 Epoch 467: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.29 
2024-02-08 15:34:52,857 EPOCH 468
2024-02-08 15:34:53,773 [Epoch: 468 Step: 00031300] Batch Recognition Loss:   0.000658 => Gls Tokens per Sec:     1926 || Batch Translation Loss:   0.040351 => Txt Tokens per Sec:     5328 || Lr: 0.000050
2024-02-08 15:34:58,254 Epoch 468: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.68 
2024-02-08 15:34:58,255 EPOCH 469
2024-02-08 15:35:01,542 [Epoch: 469 Step: 00031400] Batch Recognition Loss:   0.000182 => Gls Tokens per Sec:     2112 || Batch Translation Loss:   0.144116 => Txt Tokens per Sec:     5837 || Lr: 0.000050
2024-02-08 15:35:03,657 Epoch 469: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.79 
2024-02-08 15:35:03,657 EPOCH 470
2024-02-08 15:35:09,204 Epoch 470: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.56 
2024-02-08 15:35:09,204 EPOCH 471
2024-02-08 15:35:09,883 [Epoch: 471 Step: 00031500] Batch Recognition Loss:   0.000221 => Gls Tokens per Sec:     2360 || Batch Translation Loss:   0.070301 => Txt Tokens per Sec:     6761 || Lr: 0.000050
2024-02-08 15:35:14,457 Epoch 471: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.22 
2024-02-08 15:35:14,458 EPOCH 472
2024-02-08 15:35:17,898 [Epoch: 472 Step: 00031600] Batch Recognition Loss:   0.000408 => Gls Tokens per Sec:     2001 || Batch Translation Loss:   0.011119 => Txt Tokens per Sec:     5377 || Lr: 0.000050
2024-02-08 15:35:20,139 Epoch 472: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.07 
2024-02-08 15:35:20,140 EPOCH 473
2024-02-08 15:35:25,928 Epoch 473: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.05 
2024-02-08 15:35:25,928 EPOCH 474
2024-02-08 15:35:26,671 [Epoch: 474 Step: 00031700] Batch Recognition Loss:   0.000291 => Gls Tokens per Sec:     1946 || Batch Translation Loss:   0.017703 => Txt Tokens per Sec:     5424 || Lr: 0.000050
2024-02-08 15:35:31,508 Epoch 474: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.36 
2024-02-08 15:35:31,509 EPOCH 475
2024-02-08 15:35:34,627 [Epoch: 475 Step: 00031800] Batch Recognition Loss:   0.000181 => Gls Tokens per Sec:     2124 || Batch Translation Loss:   0.009807 => Txt Tokens per Sec:     5899 || Lr: 0.000050
2024-02-08 15:35:36,708 Epoch 475: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.01 
2024-02-08 15:35:36,708 EPOCH 476
2024-02-08 15:35:42,187 Epoch 476: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.58 
2024-02-08 15:35:42,187 EPOCH 477
2024-02-08 15:35:42,682 [Epoch: 477 Step: 00031900] Batch Recognition Loss:   0.000266 => Gls Tokens per Sec:     2591 || Batch Translation Loss:   0.009334 => Txt Tokens per Sec:     6907 || Lr: 0.000050
2024-02-08 15:35:47,415 Epoch 477: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.56 
2024-02-08 15:35:47,416 EPOCH 478
2024-02-08 15:35:50,991 [Epoch: 478 Step: 00032000] Batch Recognition Loss:   0.000432 => Gls Tokens per Sec:     1808 || Batch Translation Loss:   0.016696 => Txt Tokens per Sec:     5044 || Lr: 0.000050
2024-02-08 15:35:59,943 Validation result at epoch 478, step    32000: duration: 8.9520s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 3.52171	Translation Loss: 91619.54688	PPL: 9423.60840
	Eval Metric: BLEU
	WER 4.10	(DEL: 0.00,	INS: 0.00,	SUB: 4.10)
	BLEU-4 0.73	(BLEU-1: 11.84,	BLEU-2: 3.73,	BLEU-3: 1.50,	BLEU-4: 0.73)
	CHRF 17.69	ROUGE 9.69
2024-02-08 15:35:59,944 Logging Recognition and Translation Outputs
2024-02-08 15:35:59,944 ========================================================================================================================
2024-02-08 15:35:59,944 Logging Sequence: 162_133.00
2024-02-08 15:35:59,944 	Gloss Reference :	A B+C+D+E
2024-02-08 15:35:59,944 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:35:59,944 	Gloss Alignment :	         
2024-02-08 15:35:59,945 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:35:59,946 	Text Reference  :	******* ** ****** *** they also sent  rape    threats to his 9-month old       daughter
2024-02-08 15:35:59,946 	Text Hypothesis :	'œusman is muslim and does not  drink alcohol because of his ******* religious beliefs 
2024-02-08 15:35:59,946 	Text Alignment  :	I       I  I      I   S    S    S     S       S       S      D       S         S       
2024-02-08 15:35:59,946 ========================================================================================================================
2024-02-08 15:35:59,946 Logging Sequence: 134_236.00
2024-02-08 15:35:59,946 	Gloss Reference :	A B+C+D+E
2024-02-08 15:35:59,946 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:35:59,947 	Gloss Alignment :	         
2024-02-08 15:35:59,947 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:35:59,948 	Text Reference  :	after the ***** **** ****** *** ******* *** *********** **** interaction modi       tweeted the images and         captioned it  saying   
2024-02-08 15:35:59,948 	Text Hypothesis :	***** the first time hardik met natasha was interacting with india's     contingent at      the ****** deaflympics at        the residence
2024-02-08 15:35:59,948 	Text Alignment  :	D         I     I    I      I   I       I   I           I    S           S          S           D      S           S         S   S        
2024-02-08 15:35:59,948 ========================================================================================================================
2024-02-08 15:35:59,948 Logging Sequence: 145_52.00
2024-02-08 15:35:59,949 	Gloss Reference :	A B+C+D+E
2024-02-08 15:35:59,949 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:35:59,949 	Gloss Alignment :	         
2024-02-08 15:35:59,949 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:35:59,950 	Text Reference  :	her name was dropped despite having qualified as  she was   the only female athlete
2024-02-08 15:35:59,950 	Text Hypothesis :	*** **** *** ******* ******* india  has       won the world cup for  12     lakh   
2024-02-08 15:35:59,950 	Text Alignment  :	D   D    D   D       D       S      S         S   S   S     S   S    S      S      
2024-02-08 15:35:59,950 ========================================================================================================================
2024-02-08 15:35:59,950 Logging Sequence: 175_40.00
2024-02-08 15:35:59,951 	Gloss Reference :	A B+C+D+E
2024-02-08 15:35:59,951 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:35:59,951 	Gloss Alignment :	         
2024-02-08 15:35:59,951 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:35:59,952 	Text Reference  :	** *** *** ***** *** soumyadeep and shreya bagged   three medals each including a     silver medal each      
2024-02-08 15:35:59,952 	Text Hypothesis :	in the icc world cup india      and ****** pakistan have  faced  each ********* other seven  times previously
2024-02-08 15:35:59,952 	Text Alignment  :	I  I   I   I     I   S              D      S        S     S           D         S     S      S     S         
2024-02-08 15:35:59,952 ========================================================================================================================
2024-02-08 15:35:59,952 Logging Sequence: 156_51.00
2024-02-08 15:35:59,953 	Gloss Reference :	A B+C+D+E
2024-02-08 15:35:59,953 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:35:59,953 	Gloss Alignment :	         
2024-02-08 15:35:59,953 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:35:59,954 	Text Reference  :	******** ***************** the    selection of the players was similar to that  of   ipl   
2024-02-08 15:35:59,954 	Text Hypothesis :	multiple ticket-collection points would     be set up      at  venues  to avoid long queues
2024-02-08 15:35:59,954 	Text Alignment  :	I        I                 S      S         S  S   S       S   S          S     S    S     
2024-02-08 15:35:59,954 ========================================================================================================================
2024-02-08 15:36:02,012 Epoch 478: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.69 
2024-02-08 15:36:02,012 EPOCH 479
2024-02-08 15:36:07,412 Epoch 479: Total Training Recognition Loss 0.11  Total Training Translation Loss 4.99 
2024-02-08 15:36:07,413 EPOCH 480
2024-02-08 15:36:07,870 [Epoch: 480 Step: 00032100] Batch Recognition Loss:   0.010252 => Gls Tokens per Sec:     2456 || Batch Translation Loss:   0.038302 => Txt Tokens per Sec:     6603 || Lr: 0.000050
2024-02-08 15:36:12,877 Epoch 480: Total Training Recognition Loss 0.14  Total Training Translation Loss 6.99 
2024-02-08 15:36:12,877 EPOCH 481
2024-02-08 15:36:15,984 [Epoch: 481 Step: 00032200] Batch Recognition Loss:   0.000280 => Gls Tokens per Sec:     2029 || Batch Translation Loss:   0.070901 => Txt Tokens per Sec:     5632 || Lr: 0.000050
2024-02-08 15:36:18,145 Epoch 481: Total Training Recognition Loss 0.07  Total Training Translation Loss 5.05 
2024-02-08 15:36:18,145 EPOCH 482
2024-02-08 15:36:23,553 Epoch 482: Total Training Recognition Loss 0.11  Total Training Translation Loss 3.74 
2024-02-08 15:36:23,554 EPOCH 483
2024-02-08 15:36:24,002 [Epoch: 483 Step: 00032300] Batch Recognition Loss:   0.002452 => Gls Tokens per Sec:     2143 || Batch Translation Loss:   0.017745 => Txt Tokens per Sec:     5772 || Lr: 0.000050
2024-02-08 15:36:29,024 Epoch 483: Total Training Recognition Loss 0.08  Total Training Translation Loss 3.69 
2024-02-08 15:36:29,024 EPOCH 484
2024-02-08 15:36:32,215 [Epoch: 484 Step: 00032400] Batch Recognition Loss:   0.002735 => Gls Tokens per Sec:     1925 || Batch Translation Loss:   0.107261 => Txt Tokens per Sec:     5203 || Lr: 0.000050
2024-02-08 15:36:34,491 Epoch 484: Total Training Recognition Loss 0.06  Total Training Translation Loss 4.92 
2024-02-08 15:36:34,491 EPOCH 485
2024-02-08 15:36:39,716 Epoch 485: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.53 
2024-02-08 15:36:39,716 EPOCH 486
2024-02-08 15:36:40,095 [Epoch: 486 Step: 00032500] Batch Recognition Loss:   0.000469 => Gls Tokens per Sec:     2122 || Batch Translation Loss:   0.123185 => Txt Tokens per Sec:     4645 || Lr: 0.000050
2024-02-08 15:36:45,289 Epoch 486: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.01 
2024-02-08 15:36:45,290 EPOCH 487
2024-02-08 15:36:48,533 [Epoch: 487 Step: 00032600] Batch Recognition Loss:   0.000361 => Gls Tokens per Sec:     1845 || Batch Translation Loss:   0.028840 => Txt Tokens per Sec:     5132 || Lr: 0.000050
2024-02-08 15:36:50,803 Epoch 487: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.84 
2024-02-08 15:36:50,803 EPOCH 488
2024-02-08 15:36:56,141 Epoch 488: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.06 
2024-02-08 15:36:56,142 EPOCH 489
2024-02-08 15:36:56,403 [Epoch: 489 Step: 00032700] Batch Recognition Loss:   0.000251 => Gls Tokens per Sec:     2462 || Batch Translation Loss:   0.016727 => Txt Tokens per Sec:     6412 || Lr: 0.000050
2024-02-08 15:37:01,499 Epoch 489: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.38 
2024-02-08 15:37:01,500 EPOCH 490
2024-02-08 15:37:04,600 [Epoch: 490 Step: 00032800] Batch Recognition Loss:   0.000208 => Gls Tokens per Sec:     1879 || Batch Translation Loss:   0.021285 => Txt Tokens per Sec:     5365 || Lr: 0.000050
2024-02-08 15:37:06,833 Epoch 490: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.33 
2024-02-08 15:37:06,833 EPOCH 491
2024-02-08 15:37:12,430 Epoch 491: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.71 
2024-02-08 15:37:12,431 EPOCH 492
2024-02-08 15:37:12,639 [Epoch: 492 Step: 00032900] Batch Recognition Loss:   0.000134 => Gls Tokens per Sec:     2330 || Batch Translation Loss:   0.012354 => Txt Tokens per Sec:     6150 || Lr: 0.000050
2024-02-08 15:37:18,057 Epoch 492: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.18 
2024-02-08 15:37:18,058 EPOCH 493
2024-02-08 15:37:20,812 [Epoch: 493 Step: 00033000] Batch Recognition Loss:   0.000355 => Gls Tokens per Sec:     2092 || Batch Translation Loss:   0.019994 => Txt Tokens per Sec:     5740 || Lr: 0.000050
2024-02-08 15:37:23,320 Epoch 493: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.30 
2024-02-08 15:37:23,321 EPOCH 494
2024-02-08 15:37:28,748 Epoch 494: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.05 
2024-02-08 15:37:28,748 EPOCH 495
2024-02-08 15:37:28,869 [Epoch: 495 Step: 00033100] Batch Recognition Loss:   0.000123 => Gls Tokens per Sec:     2667 || Batch Translation Loss:   0.011750 => Txt Tokens per Sec:     6583 || Lr: 0.000050
2024-02-08 15:37:33,925 Epoch 495: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.72 
2024-02-08 15:37:33,925 EPOCH 496
2024-02-08 15:37:36,502 [Epoch: 496 Step: 00033200] Batch Recognition Loss:   0.000155 => Gls Tokens per Sec:     2175 || Batch Translation Loss:   0.016732 => Txt Tokens per Sec:     6015 || Lr: 0.000050
2024-02-08 15:37:39,246 Epoch 496: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.92 
2024-02-08 15:37:39,247 EPOCH 497
2024-02-08 15:37:44,496 Epoch 497: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.07 
2024-02-08 15:37:44,496 EPOCH 498
2024-02-08 15:37:44,599 [Epoch: 498 Step: 00033300] Batch Recognition Loss:   0.000415 => Gls Tokens per Sec:     1569 || Batch Translation Loss:   0.050361 => Txt Tokens per Sec:     5471 || Lr: 0.000050
2024-02-08 15:37:49,742 Epoch 498: Total Training Recognition Loss 0.05  Total Training Translation Loss 5.01 
2024-02-08 15:37:49,742 EPOCH 499
2024-02-08 15:37:52,572 [Epoch: 499 Step: 00033400] Batch Recognition Loss:   0.000726 => Gls Tokens per Sec:     1888 || Batch Translation Loss:   0.040794 => Txt Tokens per Sec:     5132 || Lr: 0.000050
2024-02-08 15:37:55,242 Epoch 499: Total Training Recognition Loss 0.07  Total Training Translation Loss 5.64 
2024-02-08 15:37:55,243 EPOCH 500
2024-02-08 15:38:00,225 [Epoch: 500 Step: 00033500] Batch Recognition Loss:   0.000326 => Gls Tokens per Sec:     2131 || Batch Translation Loss:   0.022742 => Txt Tokens per Sec:     5897 || Lr: 0.000050
2024-02-08 15:38:00,225 Epoch 500: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.51 
2024-02-08 15:38:00,225 EPOCH 501
2024-02-08 15:38:05,573 Epoch 501: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.62 
2024-02-08 15:38:05,574 EPOCH 502
2024-02-08 15:38:08,078 [Epoch: 502 Step: 00033600] Batch Recognition Loss:   0.001134 => Gls Tokens per Sec:     2070 || Batch Translation Loss:   0.098712 => Txt Tokens per Sec:     6002 || Lr: 0.000050
2024-02-08 15:38:10,667 Epoch 502: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.46 
2024-02-08 15:38:10,668 EPOCH 503
2024-02-08 15:38:15,639 [Epoch: 503 Step: 00033700] Batch Recognition Loss:   0.000557 => Gls Tokens per Sec:     2105 || Batch Translation Loss:   0.018413 => Txt Tokens per Sec:     5830 || Lr: 0.000050
2024-02-08 15:38:15,708 Epoch 503: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.53 
2024-02-08 15:38:15,709 EPOCH 504
2024-02-08 15:38:21,168 Epoch 504: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.95 
2024-02-08 15:38:21,168 EPOCH 505
2024-02-08 15:38:23,453 [Epoch: 505 Step: 00033800] Batch Recognition Loss:   0.001773 => Gls Tokens per Sec:     2198 || Batch Translation Loss:   0.017041 => Txt Tokens per Sec:     5926 || Lr: 0.000050
2024-02-08 15:38:26,419 Epoch 505: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.98 
2024-02-08 15:38:26,420 EPOCH 506
2024-02-08 15:38:31,663 [Epoch: 506 Step: 00033900] Batch Recognition Loss:   0.000332 => Gls Tokens per Sec:     1965 || Batch Translation Loss:   0.036211 => Txt Tokens per Sec:     5412 || Lr: 0.000050
2024-02-08 15:38:31,867 Epoch 506: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.08 
2024-02-08 15:38:31,867 EPOCH 507
2024-02-08 15:38:36,621 Epoch 507: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.38 
2024-02-08 15:38:36,622 EPOCH 508
2024-02-08 15:38:39,005 [Epoch: 508 Step: 00034000] Batch Recognition Loss:   0.000165 => Gls Tokens per Sec:     2082 || Batch Translation Loss:   0.014852 => Txt Tokens per Sec:     6058 || Lr: 0.000050
2024-02-08 15:38:47,312 Validation result at epoch 508, step    34000: duration: 8.3056s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 3.44541	Translation Loss: 90992.14062	PPL: 8851.19238
	Eval Metric: BLEU
	WER 4.31	(DEL: 0.00,	INS: 0.00,	SUB: 4.31)
	BLEU-4 0.72	(BLEU-1: 11.42,	BLEU-2: 3.59,	BLEU-3: 1.46,	BLEU-4: 0.72)
	CHRF 17.30	ROUGE 9.55
2024-02-08 15:38:47,312 Logging Recognition and Translation Outputs
2024-02-08 15:38:47,312 ========================================================================================================================
2024-02-08 15:38:47,313 Logging Sequence: 171_158.00
2024-02-08 15:38:47,313 	Gloss Reference :	A B+C+D+E
2024-02-08 15:38:47,313 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:38:47,313 	Gloss Alignment :	         
2024-02-08 15:38:47,313 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:38:47,314 	Text Reference  :	with    speculations of dhoni being banned are spreading many say   that it is        unlikely to  happen
2024-02-08 15:38:47,315 	Text Hypothesis :	however this         is why   the   finals are ********* **** going to   be completed with     the ipl   
2024-02-08 15:38:47,315 	Text Alignment  :	S       S            S  S     S     S          D         D    S     S    S  S         S        S   S     
2024-02-08 15:38:47,315 ========================================================================================================================
2024-02-08 15:38:47,315 Logging Sequence: 108_235.00
2024-02-08 15:38:47,315 	Gloss Reference :	A B+C+D+E
2024-02-08 15:38:47,315 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:38:47,315 	Gloss Alignment :	         
2024-02-08 15:38:47,316 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:38:47,317 	Text Reference  :	he was taken to   the           hospital and it   was  reported that he ****** is   not       in  any    danger   
2024-02-08 15:38:47,317 	Text Hypothesis :	** *** ***** many congratulated him      and said that they     hope he scores more centuries and double centuries
2024-02-08 15:38:47,317 	Text Alignment  :	D  D   D     S    S             S            S    S    S        S       I      S    S         S   S      S        
2024-02-08 15:38:47,317 ========================================================================================================================
2024-02-08 15:38:47,317 Logging Sequence: 153_206.00
2024-02-08 15:38:47,318 	Gloss Reference :	A B+C+D+E
2024-02-08 15:38:47,318 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:38:47,318 	Gloss Alignment :	         
2024-02-08 15:38:47,318 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:38:47,319 	Text Reference  :	*** ***** **** *** **** ***** now on   13th november everyone is   hoping pakistan rewrites history
2024-02-08 15:38:47,319 	Text Hypothesis :	the first time the 2022 final was held in   uae      with     2022 to     become   the      years  
2024-02-08 15:38:47,319 	Text Alignment  :	I   I     I    I   I    I     S   S    S    S        S        S    S      S        S        S      
2024-02-08 15:38:47,319 ========================================================================================================================
2024-02-08 15:38:47,319 Logging Sequence: 87_202.00
2024-02-08 15:38:47,320 	Gloss Reference :	A B+C+D+E
2024-02-08 15:38:47,320 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:38:47,320 	Gloss Alignment :	         
2024-02-08 15:38:47,320 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:38:47,321 	Text Reference  :	*** i      love    our players and  i   love  my country
2024-02-08 15:38:47,321 	Text Hypothesis :	she really created by  her     name was known as well   
2024-02-08 15:38:47,321 	Text Alignment  :	I   S      S       S   S       S    S   S     S  S      
2024-02-08 15:38:47,321 ========================================================================================================================
2024-02-08 15:38:47,321 Logging Sequence: 84_2.00
2024-02-08 15:38:47,321 	Gloss Reference :	A B+C+D+E
2024-02-08 15:38:47,321 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:38:47,322 	Gloss Alignment :	         
2024-02-08 15:38:47,322 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:38:47,324 	Text Reference  :	******* the 2022 fifa football world cup is    going on      in ******** qatar from 20th november 2022    to 18th  december 2022  
2024-02-08 15:38:47,324 	Text Hypothesis :	however the **** **** ******** ***** *** first time  sources in congress said  'i   am   very     excited to watch the      finals
2024-02-08 15:38:47,324 	Text Alignment  :	I           D    D    D        D     D   S     S     S          I        S     S    S    S        S          S     S        S     
2024-02-08 15:38:47,324 ========================================================================================================================
2024-02-08 15:38:49,547 Epoch 508: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.36 
2024-02-08 15:38:49,547 EPOCH 509
2024-02-08 15:38:54,660 [Epoch: 509 Step: 00034100] Batch Recognition Loss:   0.000391 => Gls Tokens per Sec:     1984 || Batch Translation Loss:   0.030351 => Txt Tokens per Sec:     5495 || Lr: 0.000050
2024-02-08 15:38:54,998 Epoch 509: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.22 
2024-02-08 15:38:54,998 EPOCH 510
2024-02-08 15:39:00,166 Epoch 510: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.32 
2024-02-08 15:39:00,166 EPOCH 511
2024-02-08 15:39:02,516 [Epoch: 511 Step: 00034200] Batch Recognition Loss:   0.000360 => Gls Tokens per Sec:     2001 || Batch Translation Loss:   0.066665 => Txt Tokens per Sec:     5261 || Lr: 0.000050
2024-02-08 15:39:05,621 Epoch 511: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.76 
2024-02-08 15:39:05,621 EPOCH 512
2024-02-08 15:39:10,276 [Epoch: 512 Step: 00034300] Batch Recognition Loss:   0.000510 => Gls Tokens per Sec:     2144 || Batch Translation Loss:   0.019705 => Txt Tokens per Sec:     5953 || Lr: 0.000050
2024-02-08 15:39:10,497 Epoch 512: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.81 
2024-02-08 15:39:10,497 EPOCH 513
2024-02-08 15:39:15,856 Epoch 513: Total Training Recognition Loss 0.11  Total Training Translation Loss 1.45 
2024-02-08 15:39:15,856 EPOCH 514
2024-02-08 15:39:17,880 [Epoch: 514 Step: 00034400] Batch Recognition Loss:   0.000115 => Gls Tokens per Sec:     2294 || Batch Translation Loss:   0.013614 => Txt Tokens per Sec:     6036 || Lr: 0.000050
2024-02-08 15:39:21,021 Epoch 514: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.65 
2024-02-08 15:39:21,022 EPOCH 515
2024-02-08 15:39:26,072 [Epoch: 515 Step: 00034500] Batch Recognition Loss:   0.000353 => Gls Tokens per Sec:     1945 || Batch Translation Loss:   0.029475 => Txt Tokens per Sec:     5407 || Lr: 0.000050
2024-02-08 15:39:26,408 Epoch 515: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.51 
2024-02-08 15:39:26,409 EPOCH 516
2024-02-08 15:39:31,556 Epoch 516: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.42 
2024-02-08 15:39:31,557 EPOCH 517
2024-02-08 15:39:33,919 [Epoch: 517 Step: 00034600] Batch Recognition Loss:   0.000407 => Gls Tokens per Sec:     1898 || Batch Translation Loss:   0.013808 => Txt Tokens per Sec:     5164 || Lr: 0.000050
2024-02-08 15:39:37,314 Epoch 517: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.63 
2024-02-08 15:39:37,315 EPOCH 518
2024-02-08 15:39:42,524 [Epoch: 518 Step: 00034700] Batch Recognition Loss:   0.000156 => Gls Tokens per Sec:     1855 || Batch Translation Loss:   0.229934 => Txt Tokens per Sec:     5197 || Lr: 0.000050
2024-02-08 15:39:42,970 Epoch 518: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.17 
2024-02-08 15:39:42,970 EPOCH 519
2024-02-08 15:39:48,363 Epoch 519: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.79 
2024-02-08 15:39:48,363 EPOCH 520
2024-02-08 15:39:50,205 [Epoch: 520 Step: 00034800] Batch Recognition Loss:   0.000305 => Gls Tokens per Sec:     2347 || Batch Translation Loss:   0.034595 => Txt Tokens per Sec:     6452 || Lr: 0.000050
2024-02-08 15:39:53,051 Epoch 520: Total Training Recognition Loss 0.05  Total Training Translation Loss 6.29 
2024-02-08 15:39:53,052 EPOCH 521
2024-02-08 15:39:57,852 [Epoch: 521 Step: 00034900] Batch Recognition Loss:   0.000269 => Gls Tokens per Sec:     2000 || Batch Translation Loss:   0.181251 => Txt Tokens per Sec:     5586 || Lr: 0.000050
2024-02-08 15:39:58,417 Epoch 521: Total Training Recognition Loss 0.09  Total Training Translation Loss 8.24 
2024-02-08 15:39:58,418 EPOCH 522
2024-02-08 15:40:03,482 Epoch 522: Total Training Recognition Loss 0.07  Total Training Translation Loss 7.59 
2024-02-08 15:40:03,482 EPOCH 523
2024-02-08 15:40:05,238 [Epoch: 523 Step: 00035000] Batch Recognition Loss:   0.007647 => Gls Tokens per Sec:     2370 || Batch Translation Loss:   0.027967 => Txt Tokens per Sec:     6431 || Lr: 0.000050
2024-02-08 15:40:08,449 Epoch 523: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.46 
2024-02-08 15:40:08,449 EPOCH 524
2024-02-08 15:40:13,215 [Epoch: 524 Step: 00035100] Batch Recognition Loss:   0.000215 => Gls Tokens per Sec:     1982 || Batch Translation Loss:   0.033991 => Txt Tokens per Sec:     5464 || Lr: 0.000050
2024-02-08 15:40:13,881 Epoch 524: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.74 
2024-02-08 15:40:13,881 EPOCH 525
2024-02-08 15:40:18,847 Epoch 525: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.87 
2024-02-08 15:40:18,848 EPOCH 526
2024-02-08 15:40:20,715 [Epoch: 526 Step: 00035200] Batch Recognition Loss:   0.002579 => Gls Tokens per Sec:     2144 || Batch Translation Loss:   0.023055 => Txt Tokens per Sec:     5485 || Lr: 0.000050
2024-02-08 15:40:24,372 Epoch 526: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.85 
2024-02-08 15:40:24,373 EPOCH 527
2024-02-08 15:40:28,597 [Epoch: 527 Step: 00035300] Batch Recognition Loss:   0.000474 => Gls Tokens per Sec:     2173 || Batch Translation Loss:   0.018912 => Txt Tokens per Sec:     5969 || Lr: 0.000050
2024-02-08 15:40:29,497 Epoch 527: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.58 
2024-02-08 15:40:29,497 EPOCH 528
2024-02-08 15:40:34,792 Epoch 528: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.25 
2024-02-08 15:40:34,793 EPOCH 529
2024-02-08 15:40:36,358 [Epoch: 529 Step: 00035400] Batch Recognition Loss:   0.000148 => Gls Tokens per Sec:     2390 || Batch Translation Loss:   0.016491 => Txt Tokens per Sec:     6721 || Lr: 0.000050
2024-02-08 15:40:39,660 Epoch 529: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.78 
2024-02-08 15:40:39,661 EPOCH 530
2024-02-08 15:40:44,440 [Epoch: 530 Step: 00035500] Batch Recognition Loss:   0.000151 => Gls Tokens per Sec:     1888 || Batch Translation Loss:   0.019067 => Txt Tokens per Sec:     5229 || Lr: 0.000050
2024-02-08 15:40:45,210 Epoch 530: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.84 
2024-02-08 15:40:45,210 EPOCH 531
2024-02-08 15:40:50,359 Epoch 531: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.52 
2024-02-08 15:40:50,360 EPOCH 532
2024-02-08 15:40:52,140 [Epoch: 532 Step: 00035600] Batch Recognition Loss:   0.000232 => Gls Tokens per Sec:     2013 || Batch Translation Loss:   0.016674 => Txt Tokens per Sec:     5535 || Lr: 0.000050
2024-02-08 15:40:55,673 Epoch 532: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.19 
2024-02-08 15:40:55,673 EPOCH 533
2024-02-08 15:40:59,819 [Epoch: 533 Step: 00035700] Batch Recognition Loss:   0.000144 => Gls Tokens per Sec:     2138 || Batch Translation Loss:   0.005707 => Txt Tokens per Sec:     5800 || Lr: 0.000050
2024-02-08 15:41:00,843 Epoch 533: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.28 
2024-02-08 15:41:00,844 EPOCH 534
2024-02-08 15:41:06,047 Epoch 534: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.15 
2024-02-08 15:41:06,047 EPOCH 535
2024-02-08 15:41:07,704 [Epoch: 535 Step: 00035800] Batch Recognition Loss:   0.000195 => Gls Tokens per Sec:     2126 || Batch Translation Loss:   0.018722 => Txt Tokens per Sec:     5834 || Lr: 0.000050
2024-02-08 15:41:11,338 Epoch 535: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.25 
2024-02-08 15:41:11,339 EPOCH 536
2024-02-08 15:41:15,851 [Epoch: 536 Step: 00035900] Batch Recognition Loss:   0.000932 => Gls Tokens per Sec:     1929 || Batch Translation Loss:   0.018275 => Txt Tokens per Sec:     5380 || Lr: 0.000050
2024-02-08 15:41:16,669 Epoch 536: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.20 
2024-02-08 15:41:16,669 EPOCH 537
2024-02-08 15:41:22,238 Epoch 537: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.36 
2024-02-08 15:41:22,238 EPOCH 538
2024-02-08 15:41:23,923 [Epoch: 538 Step: 00036000] Batch Recognition Loss:   0.000170 => Gls Tokens per Sec:     1995 || Batch Translation Loss:   0.021792 => Txt Tokens per Sec:     5349 || Lr: 0.000050
2024-02-08 15:41:32,229 Validation result at epoch 538, step    36000: duration: 8.3060s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 3.47123	Translation Loss: 93734.89844	PPL: 11640.61035
	Eval Metric: BLEU
	WER 4.38	(DEL: 0.00,	INS: 0.00,	SUB: 4.38)
	BLEU-4 0.50	(BLEU-1: 10.23,	BLEU-2: 3.57,	BLEU-3: 1.29,	BLEU-4: 0.50)
	CHRF 16.07	ROUGE 9.40
2024-02-08 15:41:32,230 Logging Recognition and Translation Outputs
2024-02-08 15:41:32,231 ========================================================================================================================
2024-02-08 15:41:32,231 Logging Sequence: 153_36.00
2024-02-08 15:41:32,231 	Gloss Reference :	A B+C+D+E
2024-02-08 15:41:32,231 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:41:32,231 	Gloss Alignment :	         
2024-02-08 15:41:32,231 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:41:32,232 	Text Reference  :	india made a good score of  1686  in 20 overs ******* ****** *** ****** ***
2024-02-08 15:41:32,232 	Text Hypothesis :	india **** * had  won   the match in 61 overs without losing any wicket wow
2024-02-08 15:41:32,233 	Text Alignment  :	      D    D S    S     S   S        S        I       I      I   I      I  
2024-02-08 15:41:32,233 ========================================================================================================================
2024-02-08 15:41:32,233 Logging Sequence: 163_30.00
2024-02-08 15:41:32,233 	Gloss Reference :	A B+C+D+E
2024-02-08 15:41:32,233 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:41:32,233 	Gloss Alignment :	         
2024-02-08 15:41:32,234 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:41:32,234 	Text Reference  :	**** ** they never permitted anyone to reveal her   face      
2024-02-08 15:41:32,234 	Text Hypothesis :	both of the  world cup       will   be in     quick succession
2024-02-08 15:41:32,234 	Text Alignment  :	I    I  S    S     S         S      S  S      S     S         
2024-02-08 15:41:32,235 ========================================================================================================================
2024-02-08 15:41:32,235 Logging Sequence: 167_60.00
2024-02-08 15:41:32,235 	Gloss Reference :	A B+C+D+E
2024-02-08 15:41:32,235 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:41:32,235 	Gloss Alignment :	         
2024-02-08 15:41:32,235 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:41:32,236 	Text Reference  :	camel flu spreads rapidly when one comes in close contact with the infected
2024-02-08 15:41:32,236 	Text Hypothesis :	***** *** ******* ******* **** *** ***** ** ***** ******* for  the stadium 
2024-02-08 15:41:32,236 	Text Alignment  :	D     D   D       D       D    D   D     D  D     D       S        S       
2024-02-08 15:41:32,236 ========================================================================================================================
2024-02-08 15:41:32,236 Logging Sequence: 84_35.00
2024-02-08 15:41:32,237 	Gloss Reference :	A B+C+D+E
2024-02-08 15:41:32,237 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:41:32,237 	Gloss Alignment :	         
2024-02-08 15:41:32,237 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:41:32,237 	Text Reference  :	here is the reason why they covered their mouth
2024-02-08 15:41:32,238 	Text Hypothesis :	he   is *** ****** *** we   decided to    win  
2024-02-08 15:41:32,238 	Text Alignment  :	S       D   D      D   S    S       S     S    
2024-02-08 15:41:32,238 ========================================================================================================================
2024-02-08 15:41:32,238 Logging Sequence: 96_2.00
2024-02-08 15:41:32,238 	Gloss Reference :	A B+C+D+E
2024-02-08 15:41:32,238 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:41:32,238 	Gloss Alignment :	         
2024-02-08 15:41:32,239 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:41:32,240 	Text Reference  :	the world is preparing for the      t20     world cup scheduled to start from 16th  october this year
2024-02-08 15:41:32,240 	Text Hypothesis :	the ***** ** ********* icc under-19 cricket world cup ********* ** ***** was  first played  in   uae 
2024-02-08 15:41:32,240 	Text Alignment  :	    D     D  D         S   S        S                 D         D  D     S    S     S       S    S   
2024-02-08 15:41:32,240 ========================================================================================================================
2024-02-08 15:41:35,981 Epoch 538: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.72 
2024-02-08 15:41:35,981 EPOCH 539
2024-02-08 15:41:40,331 [Epoch: 539 Step: 00036100] Batch Recognition Loss:   0.000223 => Gls Tokens per Sec:     1963 || Batch Translation Loss:   0.040794 => Txt Tokens per Sec:     5463 || Lr: 0.000050
2024-02-08 15:41:41,336 Epoch 539: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.77 
2024-02-08 15:41:41,337 EPOCH 540
2024-02-08 15:41:47,136 Epoch 540: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.88 
2024-02-08 15:41:47,136 EPOCH 541
2024-02-08 15:41:48,563 [Epoch: 541 Step: 00036200] Batch Recognition Loss:   0.000137 => Gls Tokens per Sec:     2245 || Batch Translation Loss:   0.019036 => Txt Tokens per Sec:     5745 || Lr: 0.000050
2024-02-08 15:41:52,373 Epoch 541: Total Training Recognition Loss 0.05  Total Training Translation Loss 5.51 
2024-02-08 15:41:52,374 EPOCH 542
2024-02-08 15:41:56,280 [Epoch: 542 Step: 00036300] Batch Recognition Loss:   0.001073 => Gls Tokens per Sec:     2146 || Batch Translation Loss:   0.033019 => Txt Tokens per Sec:     5935 || Lr: 0.000050
2024-02-08 15:41:57,544 Epoch 542: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.70 
2024-02-08 15:41:57,545 EPOCH 543
2024-02-08 15:42:02,861 Epoch 543: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.15 
2024-02-08 15:42:02,862 EPOCH 544
2024-02-08 15:42:04,091 [Epoch: 544 Step: 00036400] Batch Recognition Loss:   0.000266 => Gls Tokens per Sec:     2474 || Batch Translation Loss:   0.021606 => Txt Tokens per Sec:     6712 || Lr: 0.000050
2024-02-08 15:42:08,014 Epoch 544: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.75 
2024-02-08 15:42:08,014 EPOCH 545
2024-02-08 15:42:12,076 [Epoch: 545 Step: 00036500] Batch Recognition Loss:   0.000163 => Gls Tokens per Sec:     2025 || Batch Translation Loss:   0.016378 => Txt Tokens per Sec:     5564 || Lr: 0.000050
2024-02-08 15:42:13,316 Epoch 545: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.88 
2024-02-08 15:42:13,316 EPOCH 546
2024-02-08 15:42:18,417 Epoch 546: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.03 
2024-02-08 15:42:18,417 EPOCH 547
2024-02-08 15:42:20,036 [Epoch: 547 Step: 00036600] Batch Recognition Loss:   0.000124 => Gls Tokens per Sec:     1780 || Batch Translation Loss:   0.013547 => Txt Tokens per Sec:     5111 || Lr: 0.000050
2024-02-08 15:42:23,636 Epoch 547: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.67 
2024-02-08 15:42:23,637 EPOCH 548
2024-02-08 15:42:27,512 [Epoch: 548 Step: 00036700] Batch Recognition Loss:   0.000972 => Gls Tokens per Sec:     2106 || Batch Translation Loss:   0.006441 => Txt Tokens per Sec:     5838 || Lr: 0.000050
2024-02-08 15:42:28,767 Epoch 548: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.44 
2024-02-08 15:42:28,767 EPOCH 549
2024-02-08 15:42:34,021 Epoch 549: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.66 
2024-02-08 15:42:34,022 EPOCH 550
2024-02-08 15:42:35,392 [Epoch: 550 Step: 00036800] Batch Recognition Loss:   0.000172 => Gls Tokens per Sec:     1987 || Batch Translation Loss:   0.038581 => Txt Tokens per Sec:     5882 || Lr: 0.000050
2024-02-08 15:42:39,124 Epoch 550: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.51 
2024-02-08 15:42:39,125 EPOCH 551
2024-02-08 15:42:43,092 [Epoch: 551 Step: 00036900] Batch Recognition Loss:   0.000236 => Gls Tokens per Sec:     1992 || Batch Translation Loss:   0.052503 => Txt Tokens per Sec:     5594 || Lr: 0.000050
2024-02-08 15:42:44,341 Epoch 551: Total Training Recognition Loss 0.08  Total Training Translation Loss 3.83 
2024-02-08 15:42:44,341 EPOCH 552
2024-02-08 15:42:49,369 Epoch 552: Total Training Recognition Loss 0.06  Total Training Translation Loss 4.14 
2024-02-08 15:42:49,370 EPOCH 553
2024-02-08 15:42:50,733 [Epoch: 553 Step: 00037000] Batch Recognition Loss:   0.007002 => Gls Tokens per Sec:     1880 || Batch Translation Loss:   0.035390 => Txt Tokens per Sec:     5427 || Lr: 0.000050
2024-02-08 15:42:54,832 Epoch 553: Total Training Recognition Loss 0.05  Total Training Translation Loss 4.50 
2024-02-08 15:42:54,832 EPOCH 554
2024-02-08 15:42:58,470 [Epoch: 554 Step: 00037100] Batch Recognition Loss:   0.000229 => Gls Tokens per Sec:     2129 || Batch Translation Loss:   0.036136 => Txt Tokens per Sec:     6024 || Lr: 0.000050
2024-02-08 15:42:59,912 Epoch 554: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.75 
2024-02-08 15:42:59,913 EPOCH 555
2024-02-08 15:43:05,349 Epoch 555: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.96 
2024-02-08 15:43:05,350 EPOCH 556
2024-02-08 15:43:06,724 [Epoch: 556 Step: 00037200] Batch Recognition Loss:   0.000183 => Gls Tokens per Sec:     1748 || Batch Translation Loss:   0.021598 => Txt Tokens per Sec:     5132 || Lr: 0.000050
2024-02-08 15:43:10,731 Epoch 556: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.34 
2024-02-08 15:43:10,732 EPOCH 557
2024-02-08 15:43:14,732 [Epoch: 557 Step: 00037300] Batch Recognition Loss:   0.000234 => Gls Tokens per Sec:     1895 || Batch Translation Loss:   0.018077 => Txt Tokens per Sec:     5425 || Lr: 0.000050
2024-02-08 15:43:15,955 Epoch 557: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.88 
2024-02-08 15:43:15,955 EPOCH 558
2024-02-08 15:43:21,054 Epoch 558: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.11 
2024-02-08 15:43:21,055 EPOCH 559
2024-02-08 15:43:22,178 [Epoch: 559 Step: 00037400] Batch Recognition Loss:   0.000205 => Gls Tokens per Sec:     1996 || Batch Translation Loss:   0.024566 => Txt Tokens per Sec:     5753 || Lr: 0.000050
2024-02-08 15:43:26,443 Epoch 559: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.82 
2024-02-08 15:43:26,443 EPOCH 560
2024-02-08 15:43:30,232 [Epoch: 560 Step: 00037500] Batch Recognition Loss:   0.000218 => Gls Tokens per Sec:     1959 || Batch Translation Loss:   0.022156 => Txt Tokens per Sec:     5342 || Lr: 0.000050
2024-02-08 15:43:31,920 Epoch 560: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.84 
2024-02-08 15:43:31,920 EPOCH 561
2024-02-08 15:43:37,111 Epoch 561: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.58 
2024-02-08 15:43:37,112 EPOCH 562
2024-02-08 15:43:38,010 [Epoch: 562 Step: 00037600] Batch Recognition Loss:   0.000142 => Gls Tokens per Sec:     2316 || Batch Translation Loss:   0.015542 => Txt Tokens per Sec:     6461 || Lr: 0.000050
2024-02-08 15:43:42,173 Epoch 562: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.29 
2024-02-08 15:43:42,173 EPOCH 563
2024-02-08 15:43:46,039 [Epoch: 563 Step: 00037700] Batch Recognition Loss:   0.000424 => Gls Tokens per Sec:     1879 || Batch Translation Loss:   0.016717 => Txt Tokens per Sec:     5195 || Lr: 0.000050
2024-02-08 15:43:47,781 Epoch 563: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.62 
2024-02-08 15:43:47,782 EPOCH 564
2024-02-08 15:43:53,122 Epoch 564: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.41 
2024-02-08 15:43:53,122 EPOCH 565
2024-02-08 15:43:53,965 [Epoch: 565 Step: 00037800] Batch Recognition Loss:   0.000252 => Gls Tokens per Sec:     2164 || Batch Translation Loss:   0.012814 => Txt Tokens per Sec:     5912 || Lr: 0.000050
2024-02-08 15:43:58,417 Epoch 565: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.89 
2024-02-08 15:43:58,418 EPOCH 566
2024-02-08 15:44:01,996 [Epoch: 566 Step: 00037900] Batch Recognition Loss:   0.000322 => Gls Tokens per Sec:     1985 || Batch Translation Loss:   0.146508 => Txt Tokens per Sec:     5430 || Lr: 0.000050
2024-02-08 15:44:03,923 Epoch 566: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.14 
2024-02-08 15:44:03,924 EPOCH 567
2024-02-08 15:44:09,427 Epoch 567: Total Training Recognition Loss 0.04  Total Training Translation Loss 7.81 
2024-02-08 15:44:09,428 EPOCH 568
2024-02-08 15:44:10,245 [Epoch: 568 Step: 00038000] Batch Recognition Loss:   0.000974 => Gls Tokens per Sec:     2157 || Batch Translation Loss:   0.043222 => Txt Tokens per Sec:     5430 || Lr: 0.000050
2024-02-08 15:44:18,874 Validation result at epoch 568, step    38000: duration: 8.6280s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.94868	Translation Loss: 89063.64844	PPL: 7300.44092
	Eval Metric: BLEU
	WER 4.10	(DEL: 0.00,	INS: 0.00,	SUB: 4.10)
	BLEU-4 0.55	(BLEU-1: 10.26,	BLEU-2: 2.98,	BLEU-3: 1.16,	BLEU-4: 0.55)
	CHRF 16.34	ROUGE 8.63
2024-02-08 15:44:18,875 Logging Recognition and Translation Outputs
2024-02-08 15:44:18,875 ========================================================================================================================
2024-02-08 15:44:18,875 Logging Sequence: 59_152.00
2024-02-08 15:44:18,875 	Gloss Reference :	A B+C+D+E
2024-02-08 15:44:18,876 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:44:18,876 	Gloss Alignment :	         
2024-02-08 15:44:18,876 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:44:18,877 	Text Reference  :	**** ****** the       organisers encouraged athletes to use  the condoms in their home    countries
2024-02-08 15:44:18,877 	Text Hypothesis :	even though pathirana can        be         held     in 2020 but there   is no    clarity over     
2024-02-08 15:44:18,877 	Text Alignment  :	I    I      S         S          S          S        S  S    S   S       S  S     S       S        
2024-02-08 15:44:18,878 ========================================================================================================================
2024-02-08 15:44:18,878 Logging Sequence: 155_78.00
2024-02-08 15:44:18,878 	Gloss Reference :	A B+C+D+E
2024-02-08 15:44:18,878 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:44:18,878 	Gloss Alignment :	         
2024-02-08 15:44:18,878 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:44:18,881 	Text Reference  :	** **** it was difficult for icc to disqualify the afghan team at   the   last    minute     so  they *** included them as   per the schedule
2024-02-08 15:44:18,881 	Text Hypothesis :	in 2019 it was ********* *** set to ********** *** ****** **** take place between manchester and they had to       take part in  the field   
2024-02-08 15:44:18,881 	Text Alignment  :	I  I           D         D   S      D          D   D      D    S    S     S       S          S        I   S        S    S    S       S       
2024-02-08 15:44:18,881 ========================================================================================================================
2024-02-08 15:44:18,881 Logging Sequence: 102_147.00
2024-02-08 15:44:18,881 	Gloss Reference :	A B+C+D+E
2024-02-08 15:44:18,881 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:44:18,881 	Gloss Alignment :	         
2024-02-08 15:44:18,882 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:44:18,883 	Text Reference  :	despite the muscle cramps this young   boy     lifted such a huge weight and made the country proud by          securing a   gold     medal 
2024-02-08 15:44:18,883 	Text Hypothesis :	******* the ****** couple are  unclear whether she    is   a **** ****** *** **** *** ******* ***** grandfather for      his casteist slurts
2024-02-08 15:44:18,884 	Text Alignment  :	D           D      S      S    S       S       S      S      D    D      D   D    D   D       D     S           S        S   S        S     
2024-02-08 15:44:18,884 ========================================================================================================================
2024-02-08 15:44:18,884 Logging Sequence: 105_2.00
2024-02-08 15:44:18,884 	Gloss Reference :	A B+C+D+E
2024-02-08 15:44:18,884 	Gloss Hypothesis:	A B+C+D  
2024-02-08 15:44:18,884 	Gloss Alignment :	  S      
2024-02-08 15:44:18,884 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:44:18,885 	Text Reference  :	the airthings masters tournament is an online chess tournament
2024-02-08 15:44:18,885 	Text Hypothesis :	the ********* second  time       is in the    asian games     
2024-02-08 15:44:18,885 	Text Alignment  :	    D         S       S             S  S      S     S         
2024-02-08 15:44:18,885 ========================================================================================================================
2024-02-08 15:44:18,885 Logging Sequence: 96_31.00
2024-02-08 15:44:18,886 	Gloss Reference :	A B+C+D+E
2024-02-08 15:44:18,886 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:44:18,886 	Gloss Alignment :	         
2024-02-08 15:44:18,886 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:44:18,887 	Text Reference  :	and then 2 teams will  go  on  to   play the  final 
2024-02-08 15:44:18,887 	Text Hypothesis :	*** **** * ***** india had won with a    huge margin
2024-02-08 15:44:18,887 	Text Alignment  :	D   D    D D     S     S   S   S    S    S    S     
2024-02-08 15:44:18,887 ========================================================================================================================
2024-02-08 15:44:23,634 Epoch 568: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.23 
2024-02-08 15:44:23,634 EPOCH 569
2024-02-08 15:44:27,006 [Epoch: 569 Step: 00038100] Batch Recognition Loss:   0.000482 => Gls Tokens per Sec:     2088 || Batch Translation Loss:   0.025873 => Txt Tokens per Sec:     5842 || Lr: 0.000050
2024-02-08 15:44:28,919 Epoch 569: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.99 
2024-02-08 15:44:28,919 EPOCH 570
2024-02-08 15:44:34,357 Epoch 570: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.50 
2024-02-08 15:44:34,358 EPOCH 571
2024-02-08 15:44:35,119 [Epoch: 571 Step: 00038200] Batch Recognition Loss:   0.000639 => Gls Tokens per Sec:     2105 || Batch Translation Loss:   0.023837 => Txt Tokens per Sec:     5837 || Lr: 0.000050
2024-02-08 15:44:39,727 Epoch 571: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.38 
2024-02-08 15:44:39,728 EPOCH 572
2024-02-08 15:44:43,358 [Epoch: 572 Step: 00038300] Batch Recognition Loss:   0.000456 => Gls Tokens per Sec:     1869 || Batch Translation Loss:   0.020162 => Txt Tokens per Sec:     5200 || Lr: 0.000050
2024-02-08 15:44:45,167 Epoch 572: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.74 
2024-02-08 15:44:45,167 EPOCH 573
2024-02-08 15:44:50,359 Epoch 573: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.70 
2024-02-08 15:44:50,360 EPOCH 574
2024-02-08 15:44:51,093 [Epoch: 574 Step: 00038400] Batch Recognition Loss:   0.000271 => Gls Tokens per Sec:     1967 || Batch Translation Loss:   0.015672 => Txt Tokens per Sec:     5486 || Lr: 0.000050
2024-02-08 15:44:55,879 Epoch 574: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.31 
2024-02-08 15:44:55,880 EPOCH 575
2024-02-08 15:44:58,862 [Epoch: 575 Step: 00038500] Batch Recognition Loss:   0.000162 => Gls Tokens per Sec:     2221 || Batch Translation Loss:   0.023135 => Txt Tokens per Sec:     6157 || Lr: 0.000050
2024-02-08 15:45:00,838 Epoch 575: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.18 
2024-02-08 15:45:00,838 EPOCH 576
2024-02-08 15:45:06,337 Epoch 576: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.80 
2024-02-08 15:45:06,338 EPOCH 577
2024-02-08 15:45:06,969 [Epoch: 577 Step: 00038600] Batch Recognition Loss:   0.000173 => Gls Tokens per Sec:     2032 || Batch Translation Loss:   0.022783 => Txt Tokens per Sec:     5257 || Lr: 0.000050
2024-02-08 15:45:11,171 Epoch 577: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.83 
2024-02-08 15:45:11,172 EPOCH 578
2024-02-08 15:45:14,110 [Epoch: 578 Step: 00038700] Batch Recognition Loss:   0.000206 => Gls Tokens per Sec:     2200 || Batch Translation Loss:   0.028346 => Txt Tokens per Sec:     6063 || Lr: 0.000050
2024-02-08 15:45:16,460 Epoch 578: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.85 
2024-02-08 15:45:16,461 EPOCH 579
2024-02-08 15:45:22,015 Epoch 579: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.89 
2024-02-08 15:45:22,015 EPOCH 580
2024-02-08 15:45:22,435 [Epoch: 580 Step: 00038800] Batch Recognition Loss:   0.000172 => Gls Tokens per Sec:     2673 || Batch Translation Loss:   0.035427 => Txt Tokens per Sec:     6993 || Lr: 0.000050
2024-02-08 15:45:26,713 Epoch 580: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.59 
2024-02-08 15:45:26,714 EPOCH 581
2024-02-08 15:45:29,529 [Epoch: 581 Step: 00038900] Batch Recognition Loss:   0.000158 => Gls Tokens per Sec:     2274 || Batch Translation Loss:   0.016408 => Txt Tokens per Sec:     6384 || Lr: 0.000050
2024-02-08 15:45:31,673 Epoch 581: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.91 
2024-02-08 15:45:31,673 EPOCH 582
2024-02-08 15:45:37,191 Epoch 582: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.00 
2024-02-08 15:45:37,191 EPOCH 583
2024-02-08 15:45:37,684 [Epoch: 583 Step: 00039000] Batch Recognition Loss:   0.000280 => Gls Tokens per Sec:     1951 || Batch Translation Loss:   0.022623 => Txt Tokens per Sec:     5797 || Lr: 0.000050
2024-02-08 15:45:42,536 Epoch 583: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.26 
2024-02-08 15:45:42,536 EPOCH 584
2024-02-08 15:45:45,680 [Epoch: 584 Step: 00039100] Batch Recognition Loss:   0.000985 => Gls Tokens per Sec:     1985 || Batch Translation Loss:   0.016116 => Txt Tokens per Sec:     5519 || Lr: 0.000050
2024-02-08 15:45:47,972 Epoch 584: Total Training Recognition Loss 0.14  Total Training Translation Loss 1.62 
2024-02-08 15:45:47,972 EPOCH 585
2024-02-08 15:45:53,189 Epoch 585: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.37 
2024-02-08 15:45:53,190 EPOCH 586
2024-02-08 15:45:53,498 [Epoch: 586 Step: 00039200] Batch Recognition Loss:   0.000321 => Gls Tokens per Sec:     2614 || Batch Translation Loss:   0.022776 => Txt Tokens per Sec:     6346 || Lr: 0.000050
2024-02-08 15:45:58,736 Epoch 586: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.30 
2024-02-08 15:45:58,736 EPOCH 587
2024-02-08 15:46:01,517 [Epoch: 587 Step: 00039300] Batch Recognition Loss:   0.000218 => Gls Tokens per Sec:     2152 || Batch Translation Loss:   0.022320 => Txt Tokens per Sec:     6109 || Lr: 0.000050
2024-02-08 15:46:03,903 Epoch 587: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.52 
2024-02-08 15:46:03,903 EPOCH 588
2024-02-08 15:46:09,451 Epoch 588: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.20 
2024-02-08 15:46:09,452 EPOCH 589
2024-02-08 15:46:09,840 [Epoch: 589 Step: 00039400] Batch Recognition Loss:   0.000508 => Gls Tokens per Sec:     1654 || Batch Translation Loss:   0.035825 => Txt Tokens per Sec:     4698 || Lr: 0.000050
2024-02-08 15:46:14,729 Epoch 589: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.81 
2024-02-08 15:46:14,729 EPOCH 590
2024-02-08 15:46:17,369 [Epoch: 590 Step: 00039500] Batch Recognition Loss:   0.000143 => Gls Tokens per Sec:     2244 || Batch Translation Loss:   0.037451 => Txt Tokens per Sec:     5998 || Lr: 0.000050
2024-02-08 15:46:20,062 Epoch 590: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.33 
2024-02-08 15:46:20,062 EPOCH 591
2024-02-08 15:46:24,928 Epoch 591: Total Training Recognition Loss 0.27  Total Training Translation Loss 3.52 
2024-02-08 15:46:24,928 EPOCH 592
2024-02-08 15:46:25,072 [Epoch: 592 Step: 00039600] Batch Recognition Loss:   0.000336 => Gls Tokens per Sec:     3357 || Batch Translation Loss:   0.067349 => Txt Tokens per Sec:     7322 || Lr: 0.000050
2024-02-08 15:46:29,681 Epoch 592: Total Training Recognition Loss 0.29  Total Training Translation Loss 3.46 
2024-02-08 15:46:29,681 EPOCH 593
2024-02-08 15:46:32,243 [Epoch: 593 Step: 00039700] Batch Recognition Loss:   0.224687 => Gls Tokens per Sec:     2210 || Batch Translation Loss:   0.012511 => Txt Tokens per Sec:     5976 || Lr: 0.000050
2024-02-08 15:46:34,277 Epoch 593: Total Training Recognition Loss 0.38  Total Training Translation Loss 2.59 
2024-02-08 15:46:34,277 EPOCH 594
2024-02-08 15:46:38,923 Epoch 594: Total Training Recognition Loss 0.29  Total Training Translation Loss 2.56 
2024-02-08 15:46:38,923 EPOCH 595
2024-02-08 15:46:39,038 [Epoch: 595 Step: 00039800] Batch Recognition Loss:   0.000168 => Gls Tokens per Sec:     2807 || Batch Translation Loss:   0.015421 => Txt Tokens per Sec:     6544 || Lr: 0.000050
2024-02-08 15:46:43,923 Epoch 595: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.73 
2024-02-08 15:46:43,923 EPOCH 596
2024-02-08 15:46:46,736 [Epoch: 596 Step: 00039900] Batch Recognition Loss:   0.000176 => Gls Tokens per Sec:     1991 || Batch Translation Loss:   0.020135 => Txt Tokens per Sec:     5389 || Lr: 0.000050
2024-02-08 15:46:49,491 Epoch 596: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.88 
2024-02-08 15:46:49,491 EPOCH 597
2024-02-08 15:46:55,108 Epoch 597: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.22 
2024-02-08 15:46:55,109 EPOCH 598
2024-02-08 15:46:55,164 [Epoch: 598 Step: 00040000] Batch Recognition Loss:   0.000258 => Gls Tokens per Sec:     2963 || Batch Translation Loss:   0.007858 => Txt Tokens per Sec:     5241 || Lr: 0.000050
2024-02-08 15:47:03,529 Validation result at epoch 598, step    40000: duration: 8.3640s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 3.19013	Translation Loss: 90265.48438	PPL: 8231.54492
	Eval Metric: BLEU
	WER 4.03	(DEL: 0.00,	INS: 0.00,	SUB: 4.03)
	BLEU-4 0.70	(BLEU-1: 11.04,	BLEU-2: 3.50,	BLEU-3: 1.41,	BLEU-4: 0.70)
	CHRF 17.15	ROUGE 9.57
2024-02-08 15:47:03,530 Logging Recognition and Translation Outputs
2024-02-08 15:47:03,530 ========================================================================================================================
2024-02-08 15:47:03,530 Logging Sequence: 86_84.00
2024-02-08 15:47:03,530 	Gloss Reference :	A B+C+D+E
2024-02-08 15:47:03,530 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:47:03,530 	Gloss Alignment :	         
2024-02-08 15:47:03,531 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:47:03,532 	Text Reference  :	amassing 8933 runs which included 21   centuries with a     highest score of    201     not   out   
2024-02-08 15:47:03,532 	Text Hypothesis :	******** **** **** ***** he       also served    as   coach of      the   uttar pradesh ranji trophy
2024-02-08 15:47:03,532 	Text Alignment  :	D        D    D    D     S        S    S         S    S     S       S     S     S       S     S     
2024-02-08 15:47:03,532 ========================================================================================================================
2024-02-08 15:47:03,532 Logging Sequence: 179_110.00
2024-02-08 15:47:03,532 	Gloss Reference :	A B+C+D+E
2024-02-08 15:47:03,532 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:47:03,533 	Gloss Alignment :	         
2024-02-08 15:47:03,533 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:47:03,534 	Text Reference  :	*** *** *** *********** phogat    refused to     stay in   the    same  room with   other indian female wrestlers
2024-02-08 15:47:03,534 	Text Hypothesis :	the wfi has temporarily suspended vinesh  phogat on   10th august after her  return from  the    tokyo  olympics 
2024-02-08 15:47:03,535 	Text Alignment  :	I   I   I   I           S         S       S      S    S    S      S     S    S      S     S      S      S        
2024-02-08 15:47:03,535 ========================================================================================================================
2024-02-08 15:47:03,535 Logging Sequence: 102_2.00
2024-02-08 15:47:03,535 	Gloss Reference :	A B+C+D+E    
2024-02-08 15:47:03,535 	Gloss Hypothesis:	A B+C+D+E+D+E
2024-02-08 15:47:03,535 	Gloss Alignment :	  S          
2024-02-08 15:47:03,535 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:47:03,536 	Text Reference  :	commonwealth games are among the  world's most recognised gaming championships after the ***** olympics
2024-02-08 15:47:03,537 	Text Hypothesis :	for          the   4   days  from 8th     june 2023       on     instagram     with  the tokyo olympics
2024-02-08 15:47:03,537 	Text Alignment  :	S            S     S   S     S    S       S    S          S      S             S         I             
2024-02-08 15:47:03,537 ========================================================================================================================
2024-02-08 15:47:03,537 Logging Sequence: 60_195.00
2024-02-08 15:47:03,537 	Gloss Reference :	A B+C+D+E
2024-02-08 15:47:03,537 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:47:03,537 	Gloss Alignment :	         
2024-02-08 15:47:03,538 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:47:03,538 	Text Reference  :	** people loved to     watch    his  aggressive expressions and his  bowling 
2024-02-08 15:47:03,538 	Text Hypothesis :	if you    are   having problems with support    old         he  then returned
2024-02-08 15:47:03,539 	Text Alignment  :	I  S      S     S      S        S    S          S           S   S    S       
2024-02-08 15:47:03,539 ========================================================================================================================
2024-02-08 15:47:03,539 Logging Sequence: 70_200.00
2024-02-08 15:47:03,539 	Gloss Reference :	A B+C+D+E
2024-02-08 15:47:03,539 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:47:03,539 	Gloss Alignment :	         
2024-02-08 15:47:03,539 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:47:03,540 	Text Reference  :	** * *** showing ronaldo whole-heartedly endorsing the brand
2024-02-08 15:47:03,540 	Text Hypothesis :	in a few days    later   on              24        may 2023 
2024-02-08 15:47:03,540 	Text Alignment  :	I  I I   S       S       S               S         S   S    
2024-02-08 15:47:03,540 ========================================================================================================================
2024-02-08 15:47:09,050 Epoch 598: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.65 
2024-02-08 15:47:09,051 EPOCH 599
2024-02-08 15:47:11,818 [Epoch: 599 Step: 00040100] Batch Recognition Loss:   0.000201 => Gls Tokens per Sec:     1967 || Batch Translation Loss:   0.022822 => Txt Tokens per Sec:     5409 || Lr: 0.000050
2024-02-08 15:47:14,527 Epoch 599: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.58 
2024-02-08 15:47:14,528 EPOCH 600
2024-02-08 15:47:20,004 [Epoch: 600 Step: 00040200] Batch Recognition Loss:   0.000186 => Gls Tokens per Sec:     1940 || Batch Translation Loss:   0.019349 => Txt Tokens per Sec:     5366 || Lr: 0.000050
2024-02-08 15:47:20,004 Epoch 600: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.90 
2024-02-08 15:47:20,005 EPOCH 601
2024-02-08 15:47:25,593 Epoch 601: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.06 
2024-02-08 15:47:25,593 EPOCH 602
2024-02-08 15:47:28,506 [Epoch: 602 Step: 00040300] Batch Recognition Loss:   0.000128 => Gls Tokens per Sec:     1814 || Batch Translation Loss:   0.028906 => Txt Tokens per Sec:     5140 || Lr: 0.000050
2024-02-08 15:47:31,147 Epoch 602: Total Training Recognition Loss 0.08  Total Training Translation Loss 3.51 
2024-02-08 15:47:31,147 EPOCH 603
2024-02-08 15:47:36,499 [Epoch: 603 Step: 00040400] Batch Recognition Loss:   0.000122 => Gls Tokens per Sec:     1955 || Batch Translation Loss:   0.027195 => Txt Tokens per Sec:     5380 || Lr: 0.000050
2024-02-08 15:47:36,651 Epoch 603: Total Training Recognition Loss 0.17  Total Training Translation Loss 3.62 
2024-02-08 15:47:36,651 EPOCH 604
2024-02-08 15:47:42,091 Epoch 604: Total Training Recognition Loss 0.85  Total Training Translation Loss 1.99 
2024-02-08 15:47:42,092 EPOCH 605
2024-02-08 15:47:44,693 [Epoch: 605 Step: 00040500] Batch Recognition Loss:   0.000842 => Gls Tokens per Sec:     1970 || Batch Translation Loss:   0.034564 => Txt Tokens per Sec:     5483 || Lr: 0.000050
2024-02-08 15:47:47,705 Epoch 605: Total Training Recognition Loss 0.33  Total Training Translation Loss 2.26 
2024-02-08 15:47:47,705 EPOCH 606
2024-02-08 15:47:53,017 [Epoch: 606 Step: 00040600] Batch Recognition Loss:   0.001304 => Gls Tokens per Sec:     1939 || Batch Translation Loss:   0.036440 => Txt Tokens per Sec:     5361 || Lr: 0.000050
2024-02-08 15:47:53,183 Epoch 606: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.90 
2024-02-08 15:47:53,183 EPOCH 607
2024-02-08 15:47:57,922 Epoch 607: Total Training Recognition Loss 0.09  Total Training Translation Loss 2.35 
2024-02-08 15:47:57,922 EPOCH 608
2024-02-08 15:48:00,423 [Epoch: 608 Step: 00040700] Batch Recognition Loss:   0.000784 => Gls Tokens per Sec:     1986 || Batch Translation Loss:   0.015172 => Txt Tokens per Sec:     5406 || Lr: 0.000050
2024-02-08 15:48:03,387 Epoch 608: Total Training Recognition Loss 0.10  Total Training Translation Loss 2.06 
2024-02-08 15:48:03,387 EPOCH 609
2024-02-08 15:48:07,965 [Epoch: 609 Step: 00040800] Batch Recognition Loss:   0.000218 => Gls Tokens per Sec:     2216 || Batch Translation Loss:   0.023934 => Txt Tokens per Sec:     6125 || Lr: 0.000050
2024-02-08 15:48:08,318 Epoch 609: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.80 
2024-02-08 15:48:08,319 EPOCH 610
2024-02-08 15:48:13,853 Epoch 610: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.07 
2024-02-08 15:48:13,853 EPOCH 611
2024-02-08 15:48:16,050 [Epoch: 611 Step: 00040900] Batch Recognition Loss:   0.000281 => Gls Tokens per Sec:     2140 || Batch Translation Loss:   0.020577 => Txt Tokens per Sec:     6015 || Lr: 0.000050
2024-02-08 15:48:18,918 Epoch 611: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.58 
2024-02-08 15:48:18,919 EPOCH 612
2024-02-08 15:48:23,941 [Epoch: 612 Step: 00041000] Batch Recognition Loss:   0.000133 => Gls Tokens per Sec:     1988 || Batch Translation Loss:   0.015772 => Txt Tokens per Sec:     5483 || Lr: 0.000050
2024-02-08 15:48:24,236 Epoch 612: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.07 
2024-02-08 15:48:24,236 EPOCH 613
2024-02-08 15:48:29,267 Epoch 613: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.52 
2024-02-08 15:48:29,268 EPOCH 614
2024-02-08 15:48:31,469 [Epoch: 614 Step: 00041100] Batch Recognition Loss:   0.000142 => Gls Tokens per Sec:     2109 || Batch Translation Loss:   0.031809 => Txt Tokens per Sec:     5549 || Lr: 0.000050
2024-02-08 15:48:34,789 Epoch 614: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.39 
2024-02-08 15:48:34,789 EPOCH 615
2024-02-08 15:48:39,498 [Epoch: 615 Step: 00041200] Batch Recognition Loss:   0.000206 => Gls Tokens per Sec:     2086 || Batch Translation Loss:   0.049142 => Txt Tokens per Sec:     5751 || Lr: 0.000050
2024-02-08 15:48:39,921 Epoch 615: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.11 
2024-02-08 15:48:39,921 EPOCH 616
2024-02-08 15:48:45,160 Epoch 616: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.24 
2024-02-08 15:48:45,160 EPOCH 617
2024-02-08 15:48:47,147 [Epoch: 617 Step: 00041300] Batch Recognition Loss:   0.000758 => Gls Tokens per Sec:     2256 || Batch Translation Loss:   0.066139 => Txt Tokens per Sec:     6234 || Lr: 0.000050
2024-02-08 15:48:50,185 Epoch 617: Total Training Recognition Loss 0.04  Total Training Translation Loss 5.20 
2024-02-08 15:48:50,186 EPOCH 618
2024-02-08 15:48:55,264 [Epoch: 618 Step: 00041400] Batch Recognition Loss:   0.000353 => Gls Tokens per Sec:     1903 || Batch Translation Loss:   0.055004 => Txt Tokens per Sec:     5273 || Lr: 0.000050
2024-02-08 15:48:55,701 Epoch 618: Total Training Recognition Loss 0.07  Total Training Translation Loss 4.78 
2024-02-08 15:48:55,701 EPOCH 619
2024-02-08 15:49:00,565 Epoch 619: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.67 
2024-02-08 15:49:00,565 EPOCH 620
2024-02-08 15:49:02,732 [Epoch: 620 Step: 00041500] Batch Recognition Loss:   0.000331 => Gls Tokens per Sec:     1948 || Batch Translation Loss:   0.019080 => Txt Tokens per Sec:     5433 || Lr: 0.000050
2024-02-08 15:49:05,946 Epoch 620: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.39 
2024-02-08 15:49:05,947 EPOCH 621
2024-02-08 15:49:10,364 [Epoch: 621 Step: 00041600] Batch Recognition Loss:   0.000211 => Gls Tokens per Sec:     2151 || Batch Translation Loss:   0.026038 => Txt Tokens per Sec:     5927 || Lr: 0.000050
2024-02-08 15:49:10,903 Epoch 621: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.72 
2024-02-08 15:49:10,903 EPOCH 622
2024-02-08 15:49:16,272 Epoch 622: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.52 
2024-02-08 15:49:16,273 EPOCH 623
2024-02-08 15:49:18,271 [Epoch: 623 Step: 00041700] Batch Recognition Loss:   0.000254 => Gls Tokens per Sec:     2083 || Batch Translation Loss:   0.036897 => Txt Tokens per Sec:     5957 || Lr: 0.000050
2024-02-08 15:49:21,361 Epoch 623: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.75 
2024-02-08 15:49:21,361 EPOCH 624
2024-02-08 15:49:25,464 [Epoch: 624 Step: 00041800] Batch Recognition Loss:   0.000356 => Gls Tokens per Sec:     2278 || Batch Translation Loss:   0.021284 => Txt Tokens per Sec:     6222 || Lr: 0.000050
2024-02-08 15:49:26,119 Epoch 624: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.43 
2024-02-08 15:49:26,119 EPOCH 625
2024-02-08 15:49:31,013 Epoch 625: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.37 
2024-02-08 15:49:31,014 EPOCH 626
2024-02-08 15:49:33,136 [Epoch: 626 Step: 00041900] Batch Recognition Loss:   0.000201 => Gls Tokens per Sec:     1886 || Batch Translation Loss:   0.021240 => Txt Tokens per Sec:     5274 || Lr: 0.000050
2024-02-08 15:49:36,557 Epoch 626: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.15 
2024-02-08 15:49:36,557 EPOCH 627
2024-02-08 15:49:41,096 [Epoch: 627 Step: 00042000] Batch Recognition Loss:   0.000137 => Gls Tokens per Sec:     2023 || Batch Translation Loss:   0.017154 => Txt Tokens per Sec:     5652 || Lr: 0.000050
2024-02-08 15:49:49,254 Hooray! New best validation result [eval_metric]!
2024-02-08 15:49:49,255 Saving new checkpoint.
2024-02-08 15:49:49,517 Validation result at epoch 627, step    42000: duration: 8.4205s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.86793	Translation Loss: 90841.21875	PPL: 8718.77148
	Eval Metric: BLEU
	WER 3.88	(DEL: 0.00,	INS: 0.00,	SUB: 3.88)
	BLEU-4 0.89	(BLEU-1: 10.95,	BLEU-2: 3.73,	BLEU-3: 1.65,	BLEU-4: 0.89)
	CHRF 17.08	ROUGE 9.39
2024-02-08 15:49:49,517 Logging Recognition and Translation Outputs
2024-02-08 15:49:49,518 ========================================================================================================================
2024-02-08 15:49:49,518 Logging Sequence: 154_94.00
2024-02-08 15:49:49,518 	Gloss Reference :	A B+C+D+E
2024-02-08 15:49:49,518 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:49:49,518 	Gloss Alignment :	         
2024-02-08 15:49:49,518 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:49:49,519 	Text Reference  :	**** the ipl will also  be  held  in uae from september 19 to october 15       
2024-02-08 15:49:49,519 	Text Hypothesis :	have won a   t20  world cup match in *** **** ********* ** ** ******* ahmedabad
2024-02-08 15:49:49,519 	Text Alignment  :	I    S   S   S    S     S   S        D   D    D         D  D  D       S        
2024-02-08 15:49:49,519 ========================================================================================================================
2024-02-08 15:49:49,520 Logging Sequence: 118_2.00
2024-02-08 15:49:49,520 	Gloss Reference :	A B+C+D+E
2024-02-08 15:49:49,520 	Gloss Hypothesis:	A B+C+D  
2024-02-08 15:49:49,520 	Gloss Alignment :	  S      
2024-02-08 15:49:49,520 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:49:49,521 	Text Reference  :	yesterday was a   very exciting day people across the   world were   watching
2024-02-08 15:49:49,521 	Text Hypothesis :	********* you all know that     the match  was    going on    social media   
2024-02-08 15:49:49,522 	Text Alignment  :	D         S   S   S    S        S   S      S      S     S     S      S       
2024-02-08 15:49:49,522 ========================================================================================================================
2024-02-08 15:49:49,522 Logging Sequence: 165_453.00
2024-02-08 15:49:49,522 	Gloss Reference :	A B+C+D+E
2024-02-08 15:49:49,522 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:49:49,522 	Gloss Alignment :	         
2024-02-08 15:49:49,522 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:49:49,523 	Text Reference  :	** icc    did  not agree to sehwag' decision of wearing a   numberless jersey
2024-02-08 15:49:49,523 	Text Hypothesis :	he handed over the cup   to ******* ******** ** ******* his csk        team  
2024-02-08 15:49:49,523 	Text Alignment  :	I  S      S    S   S        D       D        D  D       S   S          S     
2024-02-08 15:49:49,523 ========================================================================================================================
2024-02-08 15:49:49,524 Logging Sequence: 126_163.00
2024-02-08 15:49:49,524 	Gloss Reference :	A B+C+D+E
2024-02-08 15:49:49,524 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:49:49,524 	Gloss Alignment :	         
2024-02-08 15:49:49,524 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:49:49,525 	Text Reference  :	your hard work has   helped secure a    medal    at     the tokyo olympics
2024-02-08 15:49:49,525 	Text Hypothesis :	**** **** **** those who    is     that everyone wanted to  do    anything
2024-02-08 15:49:49,525 	Text Alignment  :	D    D    D    S     S      S      S    S        S      S   S     S       
2024-02-08 15:49:49,525 ========================================================================================================================
2024-02-08 15:49:49,525 Logging Sequence: 84_2.00
2024-02-08 15:49:49,526 	Gloss Reference :	A B+C+D+E  
2024-02-08 15:49:49,526 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-08 15:49:49,526 	Gloss Alignment :	  S        
2024-02-08 15:49:49,526 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:49:49,527 	Text Reference  :	the 2022 fifa football world     cup is   going on  in        qatar from      20th november 2022 to    18th december 2022
2024-02-08 15:49:49,528 	Text Hypothesis :	*** **** **** ******** according to  shah this  was incorrect be    respected and  posted   a    crime of   51       runs
2024-02-08 15:49:49,528 	Text Alignment  :	D   D    D    D        S         S   S    S     S   S         S     S         S    S        S    S     S    S        S   
2024-02-08 15:49:49,528 ========================================================================================================================
2024-02-08 15:49:50,233 Epoch 627: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.08 
2024-02-08 15:49:50,234 EPOCH 628
2024-02-08 15:49:55,918 Epoch 628: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.10 
2024-02-08 15:49:55,919 EPOCH 629
2024-02-08 15:49:57,577 [Epoch: 629 Step: 00042100] Batch Recognition Loss:   0.000425 => Gls Tokens per Sec:     2316 || Batch Translation Loss:   0.019207 => Txt Tokens per Sec:     6310 || Lr: 0.000050
2024-02-08 15:50:00,666 Epoch 629: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.22 
2024-02-08 15:50:00,667 EPOCH 630
2024-02-08 15:50:05,461 [Epoch: 630 Step: 00042200] Batch Recognition Loss:   0.000337 => Gls Tokens per Sec:     1903 || Batch Translation Loss:   0.132495 => Txt Tokens per Sec:     5289 || Lr: 0.000050
2024-02-08 15:50:06,222 Epoch 630: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.68 
2024-02-08 15:50:06,222 EPOCH 631
2024-02-08 15:50:11,299 Epoch 631: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.93 
2024-02-08 15:50:11,300 EPOCH 632
2024-02-08 15:50:13,378 [Epoch: 632 Step: 00042300] Batch Recognition Loss:   0.002508 => Gls Tokens per Sec:     1724 || Batch Translation Loss:   0.036426 => Txt Tokens per Sec:     4696 || Lr: 0.000050
2024-02-08 15:50:16,876 Epoch 632: Total Training Recognition Loss 0.08  Total Training Translation Loss 4.11 
2024-02-08 15:50:16,876 EPOCH 633
2024-02-08 15:50:20,728 [Epoch: 633 Step: 00042400] Batch Recognition Loss:   0.000987 => Gls Tokens per Sec:     2301 || Batch Translation Loss:   0.065745 => Txt Tokens per Sec:     6349 || Lr: 0.000050
2024-02-08 15:50:21,696 Epoch 633: Total Training Recognition Loss 0.07  Total Training Translation Loss 4.82 
2024-02-08 15:50:21,697 EPOCH 634
2024-02-08 15:50:27,177 Epoch 634: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.61 
2024-02-08 15:50:27,178 EPOCH 635
2024-02-08 15:50:28,785 [Epoch: 635 Step: 00042500] Batch Recognition Loss:   0.000189 => Gls Tokens per Sec:     2192 || Batch Translation Loss:   0.016599 => Txt Tokens per Sec:     6342 || Lr: 0.000050
2024-02-08 15:50:32,083 Epoch 635: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.02 
2024-02-08 15:50:32,084 EPOCH 636
2024-02-08 15:50:36,720 [Epoch: 636 Step: 00042600] Batch Recognition Loss:   0.000201 => Gls Tokens per Sec:     1877 || Batch Translation Loss:   0.013406 => Txt Tokens per Sec:     5198 || Lr: 0.000050
2024-02-08 15:50:37,682 Epoch 636: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.66 
2024-02-08 15:50:37,682 EPOCH 637
2024-02-08 15:50:42,554 Epoch 637: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.54 
2024-02-08 15:50:42,554 EPOCH 638
2024-02-08 15:50:44,196 [Epoch: 638 Step: 00042700] Batch Recognition Loss:   0.000335 => Gls Tokens per Sec:     2050 || Batch Translation Loss:   0.022370 => Txt Tokens per Sec:     5504 || Lr: 0.000050
2024-02-08 15:50:47,985 Epoch 638: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.45 
2024-02-08 15:50:47,985 EPOCH 639
2024-02-08 15:50:52,011 [Epoch: 639 Step: 00042800] Batch Recognition Loss:   0.001882 => Gls Tokens per Sec:     2122 || Batch Translation Loss:   0.029106 => Txt Tokens per Sec:     5950 || Lr: 0.000050
2024-02-08 15:50:53,087 Epoch 639: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.64 
2024-02-08 15:50:53,088 EPOCH 640
2024-02-08 15:50:58,548 Epoch 640: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.00 
2024-02-08 15:50:58,548 EPOCH 641
2024-02-08 15:50:59,875 [Epoch: 641 Step: 00042900] Batch Recognition Loss:   0.000800 => Gls Tokens per Sec:     2338 || Batch Translation Loss:   0.026042 => Txt Tokens per Sec:     6337 || Lr: 0.000050
2024-02-08 15:51:03,675 Epoch 641: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.42 
2024-02-08 15:51:03,676 EPOCH 642
2024-02-08 15:51:07,852 [Epoch: 642 Step: 00043000] Batch Recognition Loss:   0.000357 => Gls Tokens per Sec:     2007 || Batch Translation Loss:   0.032736 => Txt Tokens per Sec:     5502 || Lr: 0.000050
2024-02-08 15:51:08,956 Epoch 642: Total Training Recognition Loss 0.05  Total Training Translation Loss 4.42 
2024-02-08 15:51:08,957 EPOCH 643
2024-02-08 15:51:13,926 Epoch 643: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.74 
2024-02-08 15:51:13,927 EPOCH 644
2024-02-08 15:51:15,397 [Epoch: 644 Step: 00043100] Batch Recognition Loss:   0.000233 => Gls Tokens per Sec:     2069 || Batch Translation Loss:   0.018743 => Txt Tokens per Sec:     5637 || Lr: 0.000050
2024-02-08 15:51:19,221 Epoch 644: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.85 
2024-02-08 15:51:19,221 EPOCH 645
2024-02-08 15:51:22,982 [Epoch: 645 Step: 00043200] Batch Recognition Loss:   0.000203 => Gls Tokens per Sec:     2186 || Batch Translation Loss:   0.011008 => Txt Tokens per Sec:     5991 || Lr: 0.000050
2024-02-08 15:51:24,277 Epoch 645: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.38 
2024-02-08 15:51:24,278 EPOCH 646
2024-02-08 15:51:29,664 Epoch 646: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.47 
2024-02-08 15:51:29,664 EPOCH 647
2024-02-08 15:51:30,972 [Epoch: 647 Step: 00043300] Batch Recognition Loss:   0.000171 => Gls Tokens per Sec:     2204 || Batch Translation Loss:   0.114849 => Txt Tokens per Sec:     6147 || Lr: 0.000050
2024-02-08 15:51:34,703 Epoch 647: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.43 
2024-02-08 15:51:34,703 EPOCH 648
2024-02-08 15:51:38,747 [Epoch: 648 Step: 00043400] Batch Recognition Loss:   0.000398 => Gls Tokens per Sec:     2019 || Batch Translation Loss:   0.012162 => Txt Tokens per Sec:     5634 || Lr: 0.000050
2024-02-08 15:51:40,040 Epoch 648: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.11 
2024-02-08 15:51:40,040 EPOCH 649
2024-02-08 15:51:45,124 Epoch 649: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.78 
2024-02-08 15:51:45,125 EPOCH 650
2024-02-08 15:51:46,490 [Epoch: 650 Step: 00043500] Batch Recognition Loss:   0.000875 => Gls Tokens per Sec:     1994 || Batch Translation Loss:   0.022807 => Txt Tokens per Sec:     5367 || Lr: 0.000050
2024-02-08 15:51:50,391 Epoch 650: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.48 
2024-02-08 15:51:50,391 EPOCH 651
2024-02-08 15:51:53,959 [Epoch: 651 Step: 00043600] Batch Recognition Loss:   0.000206 => Gls Tokens per Sec:     2215 || Batch Translation Loss:   0.018524 => Txt Tokens per Sec:     6107 || Lr: 0.000050
2024-02-08 15:51:55,321 Epoch 651: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.50 
2024-02-08 15:51:55,321 EPOCH 652
2024-02-08 15:52:00,811 Epoch 652: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.65 
2024-02-08 15:52:00,811 EPOCH 653
2024-02-08 15:52:01,905 [Epoch: 653 Step: 00043700] Batch Recognition Loss:   0.000214 => Gls Tokens per Sec:     2342 || Batch Translation Loss:   0.015783 => Txt Tokens per Sec:     6309 || Lr: 0.000050
2024-02-08 15:52:05,854 Epoch 653: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.96 
2024-02-08 15:52:05,854 EPOCH 654
2024-02-08 15:52:09,991 [Epoch: 654 Step: 00043800] Batch Recognition Loss:   0.000482 => Gls Tokens per Sec:     1871 || Batch Translation Loss:   0.031304 => Txt Tokens per Sec:     5192 || Lr: 0.000050
2024-02-08 15:52:11,455 Epoch 654: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.97 
2024-02-08 15:52:11,456 EPOCH 655
2024-02-08 15:52:16,305 Epoch 655: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.66 
2024-02-08 15:52:16,306 EPOCH 656
2024-02-08 15:52:17,547 [Epoch: 656 Step: 00043900] Batch Recognition Loss:   0.000262 => Gls Tokens per Sec:     1855 || Batch Translation Loss:   0.009812 => Txt Tokens per Sec:     4877 || Lr: 0.000050
2024-02-08 15:52:21,724 Epoch 656: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.91 
2024-02-08 15:52:21,724 EPOCH 657
2024-02-08 15:52:25,115 [Epoch: 657 Step: 00044000] Batch Recognition Loss:   0.000319 => Gls Tokens per Sec:     2236 || Batch Translation Loss:   0.012211 => Txt Tokens per Sec:     6208 || Lr: 0.000050
2024-02-08 15:52:33,526 Validation result at epoch 657, step    44000: duration: 8.4105s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 3.05679	Translation Loss: 90265.29688	PPL: 8231.39551
	Eval Metric: BLEU
	WER 3.81	(DEL: 0.00,	INS: 0.00,	SUB: 3.81)
	BLEU-4 0.52	(BLEU-1: 10.10,	BLEU-2: 2.87,	BLEU-3: 1.19,	BLEU-4: 0.52)
	CHRF 16.98	ROUGE 8.33
2024-02-08 15:52:33,527 Logging Recognition and Translation Outputs
2024-02-08 15:52:33,527 ========================================================================================================================
2024-02-08 15:52:33,527 Logging Sequence: 57_104.00
2024-02-08 15:52:33,527 	Gloss Reference :	A B+C+D+E
2024-02-08 15:52:33,527 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:52:33,528 	Gloss Alignment :	         
2024-02-08 15:52:33,528 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:52:33,530 	Text Reference  :	the next day kohli and kl   rahul continued from  where  they had   left     and displayed amazing batting performance without losing their wickets
2024-02-08 15:52:33,530 	Text Hypothesis :	*** **** *** ***** *** they have  hiked     their travel to   delhi capitals and ********* ******* ******* india       also    asked  for   them   
2024-02-08 15:52:33,530 	Text Alignment  :	D   D    D   D     D   S    S     S         S     S      S    S     S            D         D       D       S           S       S      S     S      
2024-02-08 15:52:33,530 ========================================================================================================================
2024-02-08 15:52:33,530 Logging Sequence: 136_64.00
2024-02-08 15:52:33,530 	Gloss Reference :	A B+C+D+E
2024-02-08 15:52:33,530 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:52:33,531 	Gloss Alignment :	         
2024-02-08 15:52:33,531 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:52:33,531 	Text Reference  :	in all she  has   won 2   medals
2024-02-08 15:52:33,531 	Text Hypothesis :	i  am  very sorry it  was done  
2024-02-08 15:52:33,531 	Text Alignment  :	S  S   S    S     S   S   S     
2024-02-08 15:52:33,532 ========================================================================================================================
2024-02-08 15:52:33,532 Logging Sequence: 54_123.00
2024-02-08 15:52:33,532 	Gloss Reference :	A B+C+D+E
2024-02-08 15:52:33,532 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:52:33,532 	Gloss Alignment :	         
2024-02-08 15:52:33,532 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:52:33,533 	Text Reference  :	**** ** vips    sponsors international cricket  groups have already booked their hotel rooms 
2024-02-08 15:52:33,533 	Text Hypothesis :	this is because of       the           covid-19 will   have to      see    their ***** choice
2024-02-08 15:52:33,533 	Text Alignment  :	I    I  S       S        S             S        S           S       S            D     S     
2024-02-08 15:52:33,533 ========================================================================================================================
2024-02-08 15:52:33,534 Logging Sequence: 168_115.00
2024-02-08 15:52:33,534 	Gloss Reference :	A B+C+D+E
2024-02-08 15:52:33,534 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:52:33,534 	Gloss Alignment :	         
2024-02-08 15:52:33,534 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:52:33,535 	Text Reference  :	****** **** this   has sparked a     major discussion on   social media
2024-02-08 15:52:33,535 	Text Hypothesis :	people were amazed by  his     child but   are        only 56     runs 
2024-02-08 15:52:33,535 	Text Alignment  :	I      I    S      S   S       S     S     S          S    S      S    
2024-02-08 15:52:33,535 ========================================================================================================================
2024-02-08 15:52:33,535 Logging Sequence: 121_132.00
2024-02-08 15:52:33,536 	Gloss Reference :	A B+C+D+E
2024-02-08 15:52:33,536 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:52:33,536 	Gloss Alignment :	         
2024-02-08 15:52:33,536 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:52:33,538 	Text Reference  :	which is why they will be       retesting her to  check if   she   consumed any stamina enhancing drugs     
2024-02-08 15:52:33,538 	Text Hypothesis :	***** ** as  per  the  olympics rules     if  hou has   been found to       be  using   such      substances
2024-02-08 15:52:33,538 	Text Alignment  :	D     D  S   S    S    S        S         S   S   S     S    S     S        S   S       S         S         
2024-02-08 15:52:33,538 ========================================================================================================================
2024-02-08 15:52:35,170 Epoch 657: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.92 
2024-02-08 15:52:35,170 EPOCH 658
2024-02-08 15:52:40,743 Epoch 658: Total Training Recognition Loss 0.04  Total Training Translation Loss 6.66 
2024-02-08 15:52:40,743 EPOCH 659
2024-02-08 15:52:41,819 [Epoch: 659 Step: 00044100] Batch Recognition Loss:   0.000389 => Gls Tokens per Sec:     2086 || Batch Translation Loss:   0.116945 => Txt Tokens per Sec:     5736 || Lr: 0.000050
2024-02-08 15:52:46,036 Epoch 659: Total Training Recognition Loss 0.07  Total Training Translation Loss 6.36 
2024-02-08 15:52:46,037 EPOCH 660
2024-02-08 15:52:49,826 [Epoch: 660 Step: 00044200] Batch Recognition Loss:   0.000661 => Gls Tokens per Sec:     1985 || Batch Translation Loss:   0.025018 => Txt Tokens per Sec:     5435 || Lr: 0.000050
2024-02-08 15:52:51,569 Epoch 660: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.34 
2024-02-08 15:52:51,569 EPOCH 661
2024-02-08 15:52:56,894 Epoch 661: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.04 
2024-02-08 15:52:56,894 EPOCH 662
2024-02-08 15:52:57,921 [Epoch: 662 Step: 00044300] Batch Recognition Loss:   0.000258 => Gls Tokens per Sec:     2027 || Batch Translation Loss:   0.015121 => Txt Tokens per Sec:     5819 || Lr: 0.000050
2024-02-08 15:53:02,359 Epoch 662: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.75 
2024-02-08 15:53:02,360 EPOCH 663
2024-02-08 15:53:05,841 [Epoch: 663 Step: 00044400] Batch Recognition Loss:   0.000215 => Gls Tokens per Sec:     2116 || Batch Translation Loss:   0.012805 => Txt Tokens per Sec:     5826 || Lr: 0.000050
2024-02-08 15:53:07,701 Epoch 663: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.99 
2024-02-08 15:53:07,701 EPOCH 664
2024-02-08 15:53:13,083 Epoch 664: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.36 
2024-02-08 15:53:13,083 EPOCH 665
2024-02-08 15:53:13,923 [Epoch: 665 Step: 00044500] Batch Recognition Loss:   0.000681 => Gls Tokens per Sec:     2173 || Batch Translation Loss:   0.018899 => Txt Tokens per Sec:     5809 || Lr: 0.000050
2024-02-08 15:53:17,894 Epoch 665: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.28 
2024-02-08 15:53:17,895 EPOCH 666
2024-02-08 15:53:21,670 [Epoch: 666 Step: 00044600] Batch Recognition Loss:   0.000667 => Gls Tokens per Sec:     1881 || Batch Translation Loss:   0.047669 => Txt Tokens per Sec:     5221 || Lr: 0.000050
2024-02-08 15:53:23,344 Epoch 666: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.83 
2024-02-08 15:53:23,345 EPOCH 667
2024-02-08 15:53:28,380 Epoch 667: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.71 
2024-02-08 15:53:28,381 EPOCH 668
2024-02-08 15:53:29,115 [Epoch: 668 Step: 00044700] Batch Recognition Loss:   0.000217 => Gls Tokens per Sec:     2265 || Batch Translation Loss:   0.016273 => Txt Tokens per Sec:     6157 || Lr: 0.000050
2024-02-08 15:53:33,881 Epoch 668: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.67 
2024-02-08 15:53:33,882 EPOCH 669
2024-02-08 15:53:37,472 [Epoch: 669 Step: 00044800] Batch Recognition Loss:   0.000114 => Gls Tokens per Sec:     1961 || Batch Translation Loss:   0.014168 => Txt Tokens per Sec:     5504 || Lr: 0.000050
2024-02-08 15:53:39,232 Epoch 669: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.86 
2024-02-08 15:53:39,232 EPOCH 670
2024-02-08 15:53:44,732 Epoch 670: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.76 
2024-02-08 15:53:44,733 EPOCH 671
2024-02-08 15:53:45,610 [Epoch: 671 Step: 00044900] Batch Recognition Loss:   0.000256 => Gls Tokens per Sec:     1829 || Batch Translation Loss:   0.019799 => Txt Tokens per Sec:     5613 || Lr: 0.000050
2024-02-08 15:53:50,338 Epoch 671: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.45 
2024-02-08 15:53:50,339 EPOCH 672
2024-02-08 15:53:53,903 [Epoch: 672 Step: 00045000] Batch Recognition Loss:   0.020050 => Gls Tokens per Sec:     1931 || Batch Translation Loss:   0.048306 => Txt Tokens per Sec:     5433 || Lr: 0.000050
2024-02-08 15:53:55,650 Epoch 672: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.70 
2024-02-08 15:53:55,651 EPOCH 673
2024-02-08 15:54:00,479 Epoch 673: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.43 
2024-02-08 15:54:00,479 EPOCH 674
2024-02-08 15:54:01,456 [Epoch: 674 Step: 00045100] Batch Recognition Loss:   0.000283 => Gls Tokens per Sec:     1475 || Batch Translation Loss:   0.021377 => Txt Tokens per Sec:     4514 || Lr: 0.000050
2024-02-08 15:54:05,969 Epoch 674: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.02 
2024-02-08 15:54:05,969 EPOCH 675
2024-02-08 15:54:08,949 [Epoch: 675 Step: 00045200] Batch Recognition Loss:   0.000137 => Gls Tokens per Sec:     2256 || Batch Translation Loss:   0.031051 => Txt Tokens per Sec:     6330 || Lr: 0.000050
2024-02-08 15:54:10,968 Epoch 675: Total Training Recognition Loss 0.06  Total Training Translation Loss 4.77 
2024-02-08 15:54:10,968 EPOCH 676
2024-02-08 15:54:16,566 Epoch 676: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.63 
2024-02-08 15:54:16,567 EPOCH 677
2024-02-08 15:54:17,055 [Epoch: 677 Step: 00045300] Batch Recognition Loss:   0.000358 => Gls Tokens per Sec:     2623 || Batch Translation Loss:   0.046724 => Txt Tokens per Sec:     7090 || Lr: 0.000050
2024-02-08 15:54:21,388 Epoch 677: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.23 
2024-02-08 15:54:21,388 EPOCH 678
2024-02-08 15:54:24,995 [Epoch: 678 Step: 00045400] Batch Recognition Loss:   0.000357 => Gls Tokens per Sec:     1819 || Batch Translation Loss:   0.033501 => Txt Tokens per Sec:     5161 || Lr: 0.000050
2024-02-08 15:54:26,857 Epoch 678: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.71 
2024-02-08 15:54:26,857 EPOCH 679
2024-02-08 15:54:31,757 Epoch 679: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.46 
2024-02-08 15:54:31,758 EPOCH 680
2024-02-08 15:54:32,464 [Epoch: 680 Step: 00045500] Batch Recognition Loss:   0.000317 => Gls Tokens per Sec:     1589 || Batch Translation Loss:   0.023621 => Txt Tokens per Sec:     4772 || Lr: 0.000050
2024-02-08 15:54:37,344 Epoch 680: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.28 
2024-02-08 15:54:37,344 EPOCH 681
2024-02-08 15:54:40,303 [Epoch: 681 Step: 00045600] Batch Recognition Loss:   0.000255 => Gls Tokens per Sec:     2130 || Batch Translation Loss:   0.021326 => Txt Tokens per Sec:     6046 || Lr: 0.000050
2024-02-08 15:54:42,300 Epoch 681: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.70 
2024-02-08 15:54:42,301 EPOCH 682
2024-02-08 15:54:47,663 Epoch 682: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.30 
2024-02-08 15:54:47,663 EPOCH 683
2024-02-08 15:54:48,086 [Epoch: 683 Step: 00045700] Batch Recognition Loss:   0.000148 => Gls Tokens per Sec:     2038 || Batch Translation Loss:   0.029249 => Txt Tokens per Sec:     5175 || Lr: 0.000050
2024-02-08 15:54:52,784 Epoch 683: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.82 
2024-02-08 15:54:52,784 EPOCH 684
2024-02-08 15:54:55,990 [Epoch: 684 Step: 00045800] Batch Recognition Loss:   0.000160 => Gls Tokens per Sec:     1916 || Batch Translation Loss:   0.030743 => Txt Tokens per Sec:     5099 || Lr: 0.000050
2024-02-08 15:54:58,166 Epoch 684: Total Training Recognition Loss 0.04  Total Training Translation Loss 4.44 
2024-02-08 15:54:58,167 EPOCH 685
2024-02-08 15:55:03,255 Epoch 685: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.31 
2024-02-08 15:55:03,256 EPOCH 686
2024-02-08 15:55:03,752 [Epoch: 686 Step: 00045900] Batch Recognition Loss:   0.000249 => Gls Tokens per Sec:     1616 || Batch Translation Loss:   0.028773 => Txt Tokens per Sec:     4905 || Lr: 0.000050
2024-02-08 15:55:08,548 Epoch 686: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.09 
2024-02-08 15:55:08,548 EPOCH 687
2024-02-08 15:55:11,247 [Epoch: 687 Step: 00046000] Batch Recognition Loss:   0.002675 => Gls Tokens per Sec:     2216 || Batch Translation Loss:   0.022184 => Txt Tokens per Sec:     6119 || Lr: 0.000050
2024-02-08 15:55:20,450 Validation result at epoch 687, step    46000: duration: 9.2030s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 3.14381	Translation Loss: 90116.02344	PPL: 8109.57764
	Eval Metric: BLEU
	WER 3.81	(DEL: 0.00,	INS: 0.00,	SUB: 3.81)
	BLEU-4 0.66	(BLEU-1: 10.78,	BLEU-2: 3.37,	BLEU-3: 1.30,	BLEU-4: 0.66)
	CHRF 16.83	ROUGE 9.01
2024-02-08 15:55:20,451 Logging Recognition and Translation Outputs
2024-02-08 15:55:20,451 ========================================================================================================================
2024-02-08 15:55:20,451 Logging Sequence: 87_207.00
2024-02-08 15:55:20,451 	Gloss Reference :	A B+C+D+E
2024-02-08 15:55:20,451 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:55:20,451 	Gloss Alignment :	         
2024-02-08 15:55:20,452 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:55:20,453 	Text Reference  :	there were 2-3  pakistanis who were speaking anti-india things and things  on     kashmir
2024-02-08 15:55:20,453 	Text Hypothesis :	***** **** this is         why he   was      tired      by     the british empire games  
2024-02-08 15:55:20,453 	Text Alignment  :	D     D    S    S          S   S    S        S          S      S   S       S      S      
2024-02-08 15:55:20,453 ========================================================================================================================
2024-02-08 15:55:20,453 Logging Sequence: 67_73.00
2024-02-08 15:55:20,453 	Gloss Reference :	A B+C+D+E
2024-02-08 15:55:20,453 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:55:20,454 	Gloss Alignment :	         
2024-02-08 15:55:20,454 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:55:20,454 	Text Reference  :	in  his   tweet he   also  said 
2024-02-08 15:55:20,454 	Text Hypothesis :	the girls had   made india proud
2024-02-08 15:55:20,454 	Text Alignment  :	S   S     S     S    S     S    
2024-02-08 15:55:20,454 ========================================================================================================================
2024-02-08 15:55:20,454 Logging Sequence: 172_267.00
2024-02-08 15:55:20,455 	Gloss Reference :	A B+C+D+E
2024-02-08 15:55:20,455 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:55:20,455 	Gloss Alignment :	         
2024-02-08 15:55:20,455 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:55:20,455 	Text Reference  :	**** ** **** such    provisions have been made
2024-02-08 15:55:20,456 	Text Hypothesis :	this is just rubbish this       is   all  fake
2024-02-08 15:55:20,456 	Text Alignment  :	I    I  I    S       S          S    S    S   
2024-02-08 15:55:20,456 ========================================================================================================================
2024-02-08 15:55:20,456 Logging Sequence: 144_23.00
2024-02-08 15:55:20,456 	Gloss Reference :	A B+C+D+E
2024-02-08 15:55:20,456 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:55:20,456 	Gloss Alignment :	         
2024-02-08 15:55:20,456 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:55:20,459 	Text Reference  :	the     girl is   14-year-old mumal mehar and     she   is **** **** *** ****** **** from kanasar village of ****** barmer in    rajasthan
2024-02-08 15:55:20,459 	Text Hypothesis :	however they were elated      by    your  victory there is back home but didn't have a    huge    number  of famous by     being time     
2024-02-08 15:55:20,459 	Text Alignment  :	S       S    S    S           S     S     S       S        I    I    I   I      I    S    S       S          I      S      S     S        
2024-02-08 15:55:20,459 ========================================================================================================================
2024-02-08 15:55:20,459 Logging Sequence: 133_202.00
2024-02-08 15:55:20,459 	Gloss Reference :	A B+C+D+E
2024-02-08 15:55:20,459 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:55:20,459 	Gloss Alignment :	         
2024-02-08 15:55:20,460 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:55:20,461 	Text Reference  :	australia has already qualified for   the final if  india wins it     will       face australia
2024-02-08 15:55:20,461 	Text Hypothesis :	********* as  per     the       rules of  ipl   but they  are  always approached for  covid-19 
2024-02-08 15:55:20,461 	Text Alignment  :	D         S   S       S         S     S   S     S   S     S    S      S          S    S        
2024-02-08 15:55:20,461 ========================================================================================================================
2024-02-08 15:55:22,748 Epoch 687: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.53 
2024-02-08 15:55:22,749 EPOCH 688
2024-02-08 15:55:28,282 Epoch 688: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.56 
2024-02-08 15:55:28,283 EPOCH 689
2024-02-08 15:55:28,731 [Epoch: 689 Step: 00046100] Batch Recognition Loss:   0.002081 => Gls Tokens per Sec:     1432 || Batch Translation Loss:   0.017199 => Txt Tokens per Sec:     4405 || Lr: 0.000050
2024-02-08 15:55:33,925 Epoch 689: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.91 
2024-02-08 15:55:33,926 EPOCH 690
2024-02-08 15:55:36,593 [Epoch: 690 Step: 00046200] Batch Recognition Loss:   0.000226 => Gls Tokens per Sec:     2182 || Batch Translation Loss:   0.016368 => Txt Tokens per Sec:     6140 || Lr: 0.000050
2024-02-08 15:55:38,995 Epoch 690: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.18 
2024-02-08 15:55:38,996 EPOCH 691
2024-02-08 15:55:44,388 Epoch 691: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.42 
2024-02-08 15:55:44,389 EPOCH 692
2024-02-08 15:55:44,661 [Epoch: 692 Step: 00046300] Batch Recognition Loss:   0.000222 => Gls Tokens per Sec:     1765 || Batch Translation Loss:   0.029047 => Txt Tokens per Sec:     5699 || Lr: 0.000050
2024-02-08 15:55:49,488 Epoch 692: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.23 
2024-02-08 15:55:49,489 EPOCH 693
2024-02-08 15:55:52,437 [Epoch: 693 Step: 00046400] Batch Recognition Loss:   0.000317 => Gls Tokens per Sec:     1921 || Batch Translation Loss:   0.017855 => Txt Tokens per Sec:     5512 || Lr: 0.000050
2024-02-08 15:55:55,102 Epoch 693: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.39 
2024-02-08 15:55:55,103 EPOCH 694
2024-02-08 15:56:00,524 Epoch 694: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.54 
2024-02-08 15:56:00,525 EPOCH 695
2024-02-08 15:56:00,617 [Epoch: 695 Step: 00046500] Batch Recognition Loss:   0.000108 => Gls Tokens per Sec:     3517 || Batch Translation Loss:   0.016978 => Txt Tokens per Sec:     7110 || Lr: 0.000050
2024-02-08 15:56:05,869 Epoch 695: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.46 
2024-02-08 15:56:05,870 EPOCH 696
2024-02-08 15:56:08,302 [Epoch: 696 Step: 00046600] Batch Recognition Loss:   0.000256 => Gls Tokens per Sec:     2303 || Batch Translation Loss:   0.005791 => Txt Tokens per Sec:     6026 || Lr: 0.000050
2024-02-08 15:56:11,414 Epoch 696: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.83 
2024-02-08 15:56:11,414 EPOCH 697
2024-02-08 15:56:16,676 Epoch 697: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.94 
2024-02-08 15:56:16,677 EPOCH 698
2024-02-08 15:56:16,746 [Epoch: 698 Step: 00046700] Batch Recognition Loss:   0.000312 => Gls Tokens per Sec:     2319 || Batch Translation Loss:   0.025683 => Txt Tokens per Sec:     5971 || Lr: 0.000050
2024-02-08 15:56:21,971 Epoch 698: Total Training Recognition Loss 0.07  Total Training Translation Loss 5.54 
2024-02-08 15:56:21,971 EPOCH 699
2024-02-08 15:56:24,618 [Epoch: 699 Step: 00046800] Batch Recognition Loss:   0.000497 => Gls Tokens per Sec:     2057 || Batch Translation Loss:   0.028816 => Txt Tokens per Sec:     5853 || Lr: 0.000050
2024-02-08 15:56:27,003 Epoch 699: Total Training Recognition Loss 0.05  Total Training Translation Loss 5.41 
2024-02-08 15:56:27,003 EPOCH 700
2024-02-08 15:56:32,221 [Epoch: 700 Step: 00046900] Batch Recognition Loss:   0.000458 => Gls Tokens per Sec:     2036 || Batch Translation Loss:   0.116674 => Txt Tokens per Sec:     5632 || Lr: 0.000050
2024-02-08 15:56:32,222 Epoch 700: Total Training Recognition Loss 0.09  Total Training Translation Loss 5.22 
2024-02-08 15:56:32,222 EPOCH 701
2024-02-08 15:56:37,895 Epoch 701: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.51 
2024-02-08 15:56:37,896 EPOCH 702
2024-02-08 15:56:40,325 [Epoch: 702 Step: 00047000] Batch Recognition Loss:   0.000336 => Gls Tokens per Sec:     2133 || Batch Translation Loss:   0.045696 => Txt Tokens per Sec:     5681 || Lr: 0.000050
2024-02-08 15:56:43,082 Epoch 702: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.34 
2024-02-08 15:56:43,082 EPOCH 703
2024-02-08 15:56:48,409 [Epoch: 703 Step: 00047100] Batch Recognition Loss:   0.000162 => Gls Tokens per Sec:     1964 || Batch Translation Loss:   0.024427 => Txt Tokens per Sec:     5429 || Lr: 0.000050
2024-02-08 15:56:48,480 Epoch 703: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.88 
2024-02-08 15:56:48,480 EPOCH 704
2024-02-08 15:56:53,074 Epoch 704: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.46 
2024-02-08 15:56:53,074 EPOCH 705
2024-02-08 15:56:55,376 [Epoch: 705 Step: 00047200] Batch Recognition Loss:   0.000383 => Gls Tokens per Sec:     2226 || Batch Translation Loss:   0.027071 => Txt Tokens per Sec:     6118 || Lr: 0.000050
2024-02-08 15:56:58,370 Epoch 705: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.15 
2024-02-08 15:56:58,371 EPOCH 706
2024-02-08 15:57:03,596 [Epoch: 706 Step: 00047300] Batch Recognition Loss:   0.000252 => Gls Tokens per Sec:     1972 || Batch Translation Loss:   0.017146 => Txt Tokens per Sec:     5459 || Lr: 0.000050
2024-02-08 15:57:03,735 Epoch 706: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.41 
2024-02-08 15:57:03,736 EPOCH 707
2024-02-08 15:57:08,969 Epoch 707: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.20 
2024-02-08 15:57:08,970 EPOCH 708
2024-02-08 15:57:11,660 [Epoch: 708 Step: 00047400] Batch Recognition Loss:   0.000141 => Gls Tokens per Sec:     1807 || Batch Translation Loss:   0.016423 => Txt Tokens per Sec:     5151 || Lr: 0.000050
2024-02-08 15:57:14,128 Epoch 708: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.54 
2024-02-08 15:57:14,128 EPOCH 709
2024-02-08 15:57:18,762 [Epoch: 709 Step: 00047500] Batch Recognition Loss:   0.001361 => Gls Tokens per Sec:     2188 || Batch Translation Loss:   0.008760 => Txt Tokens per Sec:     6018 || Lr: 0.000050
2024-02-08 15:57:19,070 Epoch 709: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.81 
2024-02-08 15:57:19,070 EPOCH 710
2024-02-08 15:57:24,544 Epoch 710: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.07 
2024-02-08 15:57:24,545 EPOCH 711
2024-02-08 15:57:26,786 [Epoch: 711 Step: 00047600] Batch Recognition Loss:   0.000277 => Gls Tokens per Sec:     2098 || Batch Translation Loss:   0.019419 => Txt Tokens per Sec:     6028 || Lr: 0.000050
2024-02-08 15:57:29,170 Epoch 711: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.80 
2024-02-08 15:57:29,170 EPOCH 712
2024-02-08 15:57:34,123 [Epoch: 712 Step: 00047700] Batch Recognition Loss:   0.000224 => Gls Tokens per Sec:     2016 || Batch Translation Loss:   0.019742 => Txt Tokens per Sec:     5592 || Lr: 0.000050
2024-02-08 15:57:34,396 Epoch 712: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.47 
2024-02-08 15:57:34,396 EPOCH 713
2024-02-08 15:57:40,034 Epoch 713: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.72 
2024-02-08 15:57:40,035 EPOCH 714
2024-02-08 15:57:42,613 [Epoch: 714 Step: 00047800] Batch Recognition Loss:   0.000184 => Gls Tokens per Sec:     1801 || Batch Translation Loss:   0.013749 => Txt Tokens per Sec:     5212 || Lr: 0.000050
2024-02-08 15:57:45,518 Epoch 714: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.71 
2024-02-08 15:57:45,518 EPOCH 715
2024-02-08 15:57:50,918 [Epoch: 715 Step: 00047900] Batch Recognition Loss:   0.000866 => Gls Tokens per Sec:     1819 || Batch Translation Loss:   0.044075 => Txt Tokens per Sec:     5046 || Lr: 0.000050
2024-02-08 15:57:51,294 Epoch 715: Total Training Recognition Loss 0.04  Total Training Translation Loss 4.00 
2024-02-08 15:57:51,295 EPOCH 716
2024-02-08 15:57:56,564 Epoch 716: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.60 
2024-02-08 15:57:56,565 EPOCH 717
2024-02-08 15:57:58,797 [Epoch: 717 Step: 00048000] Batch Recognition Loss:   0.000991 => Gls Tokens per Sec:     1964 || Batch Translation Loss:   0.047418 => Txt Tokens per Sec:     5629 || Lr: 0.000050
2024-02-08 15:58:07,346 Validation result at epoch 717, step    48000: duration: 8.5476s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.99906	Translation Loss: 90787.52344	PPL: 8672.13379
	Eval Metric: BLEU
	WER 3.74	(DEL: 0.00,	INS: 0.00,	SUB: 3.74)
	BLEU-4 0.70	(BLEU-1: 11.08,	BLEU-2: 3.52,	BLEU-3: 1.42,	BLEU-4: 0.70)
	CHRF 16.85	ROUGE 9.21
2024-02-08 15:58:07,347 Logging Recognition and Translation Outputs
2024-02-08 15:58:07,347 ========================================================================================================================
2024-02-08 15:58:07,347 Logging Sequence: 96_93.00
2024-02-08 15:58:07,347 	Gloss Reference :	A B+C+D+E
2024-02-08 15:58:07,347 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:58:07,348 	Gloss Alignment :	         
2024-02-08 15:58:07,348 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:58:07,349 	Text Reference  :	bhuvneshwar kumar took 4   wickets and     hardik pandya took 3      wickets wonderful
2024-02-08 15:58:07,349 	Text Hypothesis :	*********** ***** this was india'  innings when   he     was  bowled in      shot     
2024-02-08 15:58:07,349 	Text Alignment  :	D           D     S    S   S       S       S      S      S    S      S       S        
2024-02-08 15:58:07,349 ========================================================================================================================
2024-02-08 15:58:07,349 Logging Sequence: 144_2.00
2024-02-08 15:58:07,349 	Gloss Reference :	A B+C+D+E      
2024-02-08 15:58:07,349 	Gloss Hypothesis:	A B+C+D+E+D+E+D
2024-02-08 15:58:07,349 	Gloss Alignment :	  S            
2024-02-08 15:58:07,350 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:58:07,351 	Text Reference  :	a girl      posted a  video  of herself playing cricket on a village farm on social media the     video      has        gone      viral  
2024-02-08 15:58:07,351 	Text Hypothesis :	* moroccans living in cities of ******* ******* ******* ** * ******* **** ** ****** ***** belgium netherland celebrated morocco's victory
2024-02-08 15:58:07,351 	Text Alignment  :	D S         S      S  S         D       D       D       D  D D       D    D  D      D     S       S          S          S         S      
2024-02-08 15:58:07,352 ========================================================================================================================
2024-02-08 15:58:07,352 Logging Sequence: 178_83.00
2024-02-08 15:58:07,352 	Gloss Reference :	A B+C+D+E
2024-02-08 15:58:07,352 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:58:07,352 	Gloss Alignment :	         
2024-02-08 15:58:07,352 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:58:07,353 	Text Reference  :	and ***** *** the     police still haven't apprehended the wrestler
2024-02-08 15:58:07,353 	Text Hypothesis :	and weeks his country was    very  strong  but         he  died    
2024-02-08 15:58:07,353 	Text Alignment  :	    I     I   S       S      S     S       S           S   S       
2024-02-08 15:58:07,353 ========================================================================================================================
2024-02-08 15:58:07,354 Logging Sequence: 169_214.00
2024-02-08 15:58:07,354 	Gloss Reference :	A B+C+D+E
2024-02-08 15:58:07,354 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:58:07,354 	Gloss Alignment :	         
2024-02-08 15:58:07,354 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:58:07,356 	Text Reference  :	virat kohli said that though arshdeep dropped   the      catch       he is     still a strong part of the indian team    
2024-02-08 15:58:07,356 	Text Hypothesis :	***** you   know that ****** ******** wikipedia provides information on celebs like  a height age  of *** ****** vehicles
2024-02-08 15:58:07,356 	Text Alignment  :	D     S     S         D      D        S         S        S           S  S      S       S      S       D   D      S       
2024-02-08 15:58:07,356 ========================================================================================================================
2024-02-08 15:58:07,356 Logging Sequence: 147_202.00
2024-02-08 15:58:07,356 	Gloss Reference :	A B+C+D+E
2024-02-08 15:58:07,357 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 15:58:07,357 	Gloss Alignment :	         
2024-02-08 15:58:07,357 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 15:58:07,359 	Text Reference  :	**** were      impressed that  she    took  the difficult decision to withdraw from the   olympics   and focus on her     mental health 
2024-02-08 15:58:07,359 	Text Hypothesis :	when someone's team      loses people found the ********* ******** ** ******** cup  while performing and ***** ** secured a      victory
2024-02-08 15:58:07,359 	Text Alignment  :	I    S         S         S     S      S         D         D        D  D        S    S     S              D     D  S       S      S      
2024-02-08 15:58:07,359 ========================================================================================================================
2024-02-08 15:58:10,617 Epoch 717: Total Training Recognition Loss 0.11  Total Training Translation Loss 3.92 
2024-02-08 15:58:10,617 EPOCH 718
2024-02-08 15:58:15,296 [Epoch: 718 Step: 00048100] Batch Recognition Loss:   0.000169 => Gls Tokens per Sec:     2065 || Batch Translation Loss:   0.033778 => Txt Tokens per Sec:     5751 || Lr: 0.000050
2024-02-08 15:58:15,648 Epoch 718: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.84 
2024-02-08 15:58:15,648 EPOCH 719
2024-02-08 15:58:21,192 Epoch 719: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.51 
2024-02-08 15:58:21,193 EPOCH 720
2024-02-08 15:58:23,367 [Epoch: 720 Step: 00048200] Batch Recognition Loss:   0.006531 => Gls Tokens per Sec:     1988 || Batch Translation Loss:   0.021649 => Txt Tokens per Sec:     5662 || Lr: 0.000050
2024-02-08 15:58:26,174 Epoch 720: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.93 
2024-02-08 15:58:26,175 EPOCH 721
2024-02-08 15:58:31,205 [Epoch: 721 Step: 00048300] Batch Recognition Loss:   0.000184 => Gls Tokens per Sec:     1889 || Batch Translation Loss:   0.022004 => Txt Tokens per Sec:     5264 || Lr: 0.000050
2024-02-08 15:58:31,642 Epoch 721: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.62 
2024-02-08 15:58:31,642 EPOCH 722
2024-02-08 15:58:36,442 Epoch 722: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.19 
2024-02-08 15:58:36,442 EPOCH 723
2024-02-08 15:58:38,255 [Epoch: 723 Step: 00048400] Batch Recognition Loss:   0.000359 => Gls Tokens per Sec:     2241 || Batch Translation Loss:   0.055627 => Txt Tokens per Sec:     5942 || Lr: 0.000050
2024-02-08 15:58:41,173 Epoch 723: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.32 
2024-02-08 15:58:41,173 EPOCH 724
2024-02-08 15:58:45,391 [Epoch: 724 Step: 00048500] Batch Recognition Loss:   0.001132 => Gls Tokens per Sec:     2215 || Batch Translation Loss:   0.122275 => Txt Tokens per Sec:     6124 || Lr: 0.000050
2024-02-08 15:58:45,877 Epoch 724: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.74 
2024-02-08 15:58:45,877 EPOCH 725
2024-02-08 15:58:50,883 Epoch 725: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.70 
2024-02-08 15:58:50,884 EPOCH 726
2024-02-08 15:58:52,840 [Epoch: 726 Step: 00048600] Batch Recognition Loss:   0.000130 => Gls Tokens per Sec:     1995 || Batch Translation Loss:   0.023835 => Txt Tokens per Sec:     5498 || Lr: 0.000050
2024-02-08 15:58:56,178 Epoch 726: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.63 
2024-02-08 15:58:56,178 EPOCH 727
2024-02-08 15:59:00,266 [Epoch: 727 Step: 00048700] Batch Recognition Loss:   0.000307 => Gls Tokens per Sec:     2246 || Batch Translation Loss:   0.023823 => Txt Tokens per Sec:     6130 || Lr: 0.000050
2024-02-08 15:59:01,087 Epoch 727: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.34 
2024-02-08 15:59:01,088 EPOCH 728
2024-02-08 15:59:06,628 Epoch 728: Total Training Recognition Loss 0.06  Total Training Translation Loss 4.54 
2024-02-08 15:59:06,628 EPOCH 729
2024-02-08 15:59:08,775 [Epoch: 729 Step: 00048800] Batch Recognition Loss:   0.000405 => Gls Tokens per Sec:     1789 || Batch Translation Loss:   0.074792 => Txt Tokens per Sec:     5136 || Lr: 0.000050
2024-02-08 15:59:12,167 Epoch 729: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.48 
2024-02-08 15:59:12,168 EPOCH 730
2024-02-08 15:59:16,813 [Epoch: 730 Step: 00048900] Batch Recognition Loss:   0.000315 => Gls Tokens per Sec:     1943 || Batch Translation Loss:   0.036381 => Txt Tokens per Sec:     5460 || Lr: 0.000050
2024-02-08 15:59:17,613 Epoch 730: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.05 
2024-02-08 15:59:17,613 EPOCH 731
2024-02-08 15:59:23,134 Epoch 731: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.35 
2024-02-08 15:59:23,135 EPOCH 732
2024-02-08 15:59:24,827 [Epoch: 732 Step: 00049000] Batch Recognition Loss:   0.000182 => Gls Tokens per Sec:     2178 || Batch Translation Loss:   0.016079 => Txt Tokens per Sec:     6084 || Lr: 0.000050
2024-02-08 15:59:28,641 Epoch 732: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.04 
2024-02-08 15:59:28,642 EPOCH 733
2024-02-08 15:59:33,352 [Epoch: 733 Step: 00049100] Batch Recognition Loss:   0.000237 => Gls Tokens per Sec:     1882 || Batch Translation Loss:   0.021878 => Txt Tokens per Sec:     5322 || Lr: 0.000050
2024-02-08 15:59:34,126 Epoch 733: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.72 
2024-02-08 15:59:34,126 EPOCH 734
2024-02-08 15:59:39,157 Epoch 734: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.53 
2024-02-08 15:59:39,157 EPOCH 735
2024-02-08 15:59:40,791 [Epoch: 735 Step: 00049200] Batch Recognition Loss:   0.000513 => Gls Tokens per Sec:     2156 || Batch Translation Loss:   0.014401 => Txt Tokens per Sec:     5948 || Lr: 0.000050
2024-02-08 15:59:44,493 Epoch 735: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.08 
2024-02-08 15:59:44,494 EPOCH 736
2024-02-08 15:59:48,989 [Epoch: 736 Step: 00049300] Batch Recognition Loss:   0.000320 => Gls Tokens per Sec:     1936 || Batch Translation Loss:   0.017884 => Txt Tokens per Sec:     5446 || Lr: 0.000050
2024-02-08 15:59:49,721 Epoch 736: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.08 
2024-02-08 15:59:49,721 EPOCH 737
2024-02-08 15:59:55,411 Epoch 737: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.86 
2024-02-08 15:59:55,412 EPOCH 738
2024-02-08 15:59:56,970 [Epoch: 738 Step: 00049400] Batch Recognition Loss:   0.000148 => Gls Tokens per Sec:     2158 || Batch Translation Loss:   0.019448 => Txt Tokens per Sec:     5720 || Lr: 0.000050
2024-02-08 16:00:00,615 Epoch 738: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.98 
2024-02-08 16:00:00,615 EPOCH 739
2024-02-08 16:00:05,295 [Epoch: 739 Step: 00049500] Batch Recognition Loss:   0.000570 => Gls Tokens per Sec:     1826 || Batch Translation Loss:   0.011895 => Txt Tokens per Sec:     5042 || Lr: 0.000050
2024-02-08 16:00:06,296 Epoch 739: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.47 
2024-02-08 16:00:06,296 EPOCH 740
2024-02-08 16:00:11,664 Epoch 740: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.97 
2024-02-08 16:00:11,664 EPOCH 741
2024-02-08 16:00:13,148 [Epoch: 741 Step: 00049600] Batch Recognition Loss:   0.000216 => Gls Tokens per Sec:     2158 || Batch Translation Loss:   0.191573 => Txt Tokens per Sec:     6024 || Lr: 0.000050
2024-02-08 16:00:16,770 Epoch 741: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.82 
2024-02-08 16:00:16,770 EPOCH 742
2024-02-08 16:00:20,966 [Epoch: 742 Step: 00049700] Batch Recognition Loss:   0.000614 => Gls Tokens per Sec:     1998 || Batch Translation Loss:   0.020707 => Txt Tokens per Sec:     5509 || Lr: 0.000050
2024-02-08 16:00:22,241 Epoch 742: Total Training Recognition Loss 0.07  Total Training Translation Loss 4.74 
2024-02-08 16:00:22,241 EPOCH 743
2024-02-08 16:00:27,705 Epoch 743: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.76 
2024-02-08 16:00:27,706 EPOCH 744
2024-02-08 16:00:29,176 [Epoch: 744 Step: 00049800] Batch Recognition Loss:   0.000175 => Gls Tokens per Sec:     2001 || Batch Translation Loss:   0.042433 => Txt Tokens per Sec:     5482 || Lr: 0.000050
2024-02-08 16:00:33,142 Epoch 744: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.40 
2024-02-08 16:00:33,143 EPOCH 745
2024-02-08 16:00:37,175 [Epoch: 745 Step: 00049900] Batch Recognition Loss:   0.000497 => Gls Tokens per Sec:     2065 || Batch Translation Loss:   0.022174 => Txt Tokens per Sec:     5597 || Lr: 0.000050
2024-02-08 16:00:38,604 Epoch 745: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.12 
2024-02-08 16:00:38,604 EPOCH 746
2024-02-08 16:00:43,652 Epoch 746: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.75 
2024-02-08 16:00:43,652 EPOCH 747
2024-02-08 16:00:45,060 [Epoch: 747 Step: 00050000] Batch Recognition Loss:   0.006045 => Gls Tokens per Sec:     1976 || Batch Translation Loss:   0.016200 => Txt Tokens per Sec:     5741 || Lr: 0.000050
2024-02-08 16:00:53,595 Validation result at epoch 747, step    50000: duration: 8.5353s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.85102	Translation Loss: 90332.52344	PPL: 8286.84570
	Eval Metric: BLEU
	WER 3.88	(DEL: 0.00,	INS: 0.00,	SUB: 3.88)
	BLEU-4 0.36	(BLEU-1: 10.05,	BLEU-2: 2.97,	BLEU-3: 1.07,	BLEU-4: 0.36)
	CHRF 16.85	ROUGE 8.53
2024-02-08 16:00:53,596 Logging Recognition and Translation Outputs
2024-02-08 16:00:53,596 ========================================================================================================================
2024-02-08 16:00:53,597 Logging Sequence: 178_157.00
2024-02-08 16:00:53,597 	Gloss Reference :	A B+C+D+E
2024-02-08 16:00:53,597 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:00:53,597 	Gloss Alignment :	         
2024-02-08 16:00:53,597 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:00:53,598 	Text Reference  :	this is why sushil  kumar will have      to *** ** ** **** be  arrested
2024-02-08 16:00:53,598 	Text Hypothesis :	that is *** because this  was  postponed to win so us lost the match   
2024-02-08 16:00:53,599 	Text Alignment  :	S       D   S       S     S    S            I   I  I  I    S   S       
2024-02-08 16:00:53,599 ========================================================================================================================
2024-02-08 16:00:53,599 Logging Sequence: 118_111.00
2024-02-08 16:00:53,599 	Gloss Reference :	A B+C+D+E
2024-02-08 16:00:53,599 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:00:53,599 	Gloss Alignment :	         
2024-02-08 16:00:53,599 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:00:53,600 	Text Reference  :	and people encourage him have hope for    the ****** ***** ***** ***** **** next world cup        
2024-02-08 16:00:53,601 	Text Hypothesis :	*** this   is        not go   on   amidst the rising cases human lives need to   be    safeguarded
2024-02-08 16:00:53,601 	Text Alignment  :	D   S      S         S   S    S    S          I      I     I     I     I    S    S     S          
2024-02-08 16:00:53,601 ========================================================================================================================
2024-02-08 16:00:53,601 Logging Sequence: 148_2.00
2024-02-08 16:00:53,601 	Gloss Reference :	A B+C+D+E
2024-02-08 16:00:53,601 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:00:53,601 	Gloss Alignment :	         
2024-02-08 16:00:53,602 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:00:53,603 	Text Reference  :	the final of      the   asia cup  2023  cricket tournament was  played between india and     sri lanka on 17th september 2023
2024-02-08 16:00:53,603 	Text Hypothesis :	the ***** winners claim that both teams had     scored     only 2      wickets in    colombo sri lanka ** **** ********* ****
2024-02-08 16:00:53,604 	Text Alignment  :	    D     S       S     S    S    S     S       S          S    S      S       S     S                 D  D    D         D   
2024-02-08 16:00:53,604 ========================================================================================================================
2024-02-08 16:00:53,604 Logging Sequence: 83_129.00
2024-02-08 16:00:53,604 	Gloss Reference :	A B+C+D+E
2024-02-08 16:00:53,604 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:00:53,604 	Gloss Alignment :	         
2024-02-08 16:00:53,604 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:00:53,605 	Text Reference  :	later the   denmark football association tweeted
2024-02-08 16:00:53,605 	Text Hypothesis :	2     hours after   this     was         done   
2024-02-08 16:00:53,605 	Text Alignment  :	S     S     S       S        S           S      
2024-02-08 16:00:53,605 ========================================================================================================================
2024-02-08 16:00:53,605 Logging Sequence: 99_158.00
2024-02-08 16:00:53,606 	Gloss Reference :	A B+C+D+E
2024-02-08 16:00:53,606 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:00:53,606 	Gloss Alignment :	         
2024-02-08 16:00:53,606 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:00:53,607 	Text Reference  :	**** ** *** *** the incident occured in  dubai   and   it was  extremely shameful
2024-02-08 16:00:53,607 	Text Hypothesis :	many of you may be  believe  that    his support would be show in        love    
2024-02-08 16:00:53,607 	Text Alignment  :	I    I  I   I   S   S        S       S   S       S     S  S    S         S       
2024-02-08 16:00:53,607 ========================================================================================================================
2024-02-08 16:00:57,497 Epoch 747: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.30 
2024-02-08 16:00:57,497 EPOCH 748
2024-02-08 16:01:01,991 [Epoch: 748 Step: 00050100] Batch Recognition Loss:   0.000346 => Gls Tokens per Sec:     1794 || Batch Translation Loss:   0.059407 => Txt Tokens per Sec:     5074 || Lr: 0.000050
2024-02-08 16:01:03,285 Epoch 748: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.45 
2024-02-08 16:01:03,286 EPOCH 749
2024-02-08 16:01:08,872 Epoch 749: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.18 
2024-02-08 16:01:08,872 EPOCH 750
2024-02-08 16:01:10,004 [Epoch: 750 Step: 00050200] Batch Recognition Loss:   0.000232 => Gls Tokens per Sec:     2406 || Batch Translation Loss:   0.019704 => Txt Tokens per Sec:     6244 || Lr: 0.000050
2024-02-08 16:01:13,697 Epoch 750: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.94 
2024-02-08 16:01:13,697 EPOCH 751
2024-02-08 16:01:17,946 [Epoch: 751 Step: 00050300] Batch Recognition Loss:   0.001315 => Gls Tokens per Sec:     1860 || Batch Translation Loss:   0.026221 => Txt Tokens per Sec:     5107 || Lr: 0.000050
2024-02-08 16:01:19,368 Epoch 751: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.58 
2024-02-08 16:01:19,368 EPOCH 752
2024-02-08 16:01:24,232 Epoch 752: Total Training Recognition Loss 0.07  Total Training Translation Loss 5.00 
2024-02-08 16:01:24,232 EPOCH 753
2024-02-08 16:01:25,433 [Epoch: 753 Step: 00050400] Batch Recognition Loss:   0.000260 => Gls Tokens per Sec:     2133 || Batch Translation Loss:   0.009620 => Txt Tokens per Sec:     5357 || Lr: 0.000050
2024-02-08 16:01:29,760 Epoch 753: Total Training Recognition Loss 0.05  Total Training Translation Loss 7.54 
2024-02-08 16:01:29,760 EPOCH 754
2024-02-08 16:01:33,784 [Epoch: 754 Step: 00050500] Batch Recognition Loss:   0.000235 => Gls Tokens per Sec:     1924 || Batch Translation Loss:   0.030813 => Txt Tokens per Sec:     5280 || Lr: 0.000050
2024-02-08 16:01:35,419 Epoch 754: Total Training Recognition Loss 0.09  Total Training Translation Loss 4.55 
2024-02-08 16:01:35,420 EPOCH 755
2024-02-08 16:01:40,615 Epoch 755: Total Training Recognition Loss 0.10  Total Training Translation Loss 2.80 
2024-02-08 16:01:40,615 EPOCH 756
2024-02-08 16:01:41,912 [Epoch: 756 Step: 00050600] Batch Recognition Loss:   0.000217 => Gls Tokens per Sec:     1852 || Batch Translation Loss:   0.011881 => Txt Tokens per Sec:     5329 || Lr: 0.000050
2024-02-08 16:01:46,230 Epoch 756: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.54 
2024-02-08 16:01:46,231 EPOCH 757
2024-02-08 16:01:50,038 [Epoch: 757 Step: 00050700] Batch Recognition Loss:   0.000687 => Gls Tokens per Sec:     2018 || Batch Translation Loss:   0.024235 => Txt Tokens per Sec:     5517 || Lr: 0.000050
2024-02-08 16:01:51,644 Epoch 757: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.07 
2024-02-08 16:01:51,645 EPOCH 758
2024-02-08 16:01:57,144 Epoch 758: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.75 
2024-02-08 16:01:57,145 EPOCH 759
2024-02-08 16:01:58,104 [Epoch: 759 Step: 00050800] Batch Recognition Loss:   0.000216 => Gls Tokens per Sec:     2338 || Batch Translation Loss:   0.014956 => Txt Tokens per Sec:     6049 || Lr: 0.000050
2024-02-08 16:02:02,339 Epoch 759: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.93 
2024-02-08 16:02:02,339 EPOCH 760
2024-02-08 16:02:05,825 [Epoch: 760 Step: 00050900] Batch Recognition Loss:   0.000174 => Gls Tokens per Sec:     2129 || Batch Translation Loss:   0.008565 => Txt Tokens per Sec:     6030 || Lr: 0.000050
2024-02-08 16:02:07,297 Epoch 760: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.26 
2024-02-08 16:02:07,297 EPOCH 761
2024-02-08 16:02:12,890 Epoch 761: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.87 
2024-02-08 16:02:12,890 EPOCH 762
2024-02-08 16:02:13,905 [Epoch: 762 Step: 00051000] Batch Recognition Loss:   0.000174 => Gls Tokens per Sec:     2053 || Batch Translation Loss:   0.011448 => Txt Tokens per Sec:     5709 || Lr: 0.000050
2024-02-08 16:02:18,544 Epoch 762: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.51 
2024-02-08 16:02:18,545 EPOCH 763
2024-02-08 16:02:21,995 [Epoch: 763 Step: 00051100] Batch Recognition Loss:   0.000181 => Gls Tokens per Sec:     2135 || Batch Translation Loss:   0.007694 => Txt Tokens per Sec:     5760 || Lr: 0.000050
2024-02-08 16:02:23,779 Epoch 763: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.77 
2024-02-08 16:02:23,779 EPOCH 764
2024-02-08 16:02:29,364 Epoch 764: Total Training Recognition Loss 0.18  Total Training Translation Loss 1.52 
2024-02-08 16:02:29,365 EPOCH 765
2024-02-08 16:02:30,342 [Epoch: 765 Step: 00051200] Batch Recognition Loss:   0.000435 => Gls Tokens per Sec:     1966 || Batch Translation Loss:   0.010475 => Txt Tokens per Sec:     5314 || Lr: 0.000050
2024-02-08 16:02:34,648 Epoch 765: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.23 
2024-02-08 16:02:34,649 EPOCH 766
2024-02-08 16:02:38,152 [Epoch: 766 Step: 00051300] Batch Recognition Loss:   0.000129 => Gls Tokens per Sec:     2027 || Batch Translation Loss:   0.023770 => Txt Tokens per Sec:     5381 || Lr: 0.000050
2024-02-08 16:02:40,270 Epoch 766: Total Training Recognition Loss 0.11  Total Training Translation Loss 1.47 
2024-02-08 16:02:40,270 EPOCH 767
2024-02-08 16:02:45,395 Epoch 767: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.61 
2024-02-08 16:02:45,395 EPOCH 768
2024-02-08 16:02:46,100 [Epoch: 768 Step: 00051400] Batch Recognition Loss:   0.000345 => Gls Tokens per Sec:     2500 || Batch Translation Loss:   0.031064 => Txt Tokens per Sec:     6981 || Lr: 0.000050
2024-02-08 16:02:50,617 Epoch 768: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.84 
2024-02-08 16:02:50,618 EPOCH 769
2024-02-08 16:02:54,282 [Epoch: 769 Step: 00051500] Batch Recognition Loss:   0.000413 => Gls Tokens per Sec:     1895 || Batch Translation Loss:   0.040659 => Txt Tokens per Sec:     5277 || Lr: 0.000050
2024-02-08 16:02:55,886 Epoch 769: Total Training Recognition Loss 0.04  Total Training Translation Loss 4.27 
2024-02-08 16:02:55,886 EPOCH 770
2024-02-08 16:03:00,544 Epoch 770: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.63 
2024-02-08 16:03:00,544 EPOCH 771
2024-02-08 16:03:01,258 [Epoch: 771 Step: 00051600] Batch Recognition Loss:   0.000426 => Gls Tokens per Sec:     2244 || Batch Translation Loss:   0.017231 => Txt Tokens per Sec:     6324 || Lr: 0.000050
2024-02-08 16:03:05,996 Epoch 771: Total Training Recognition Loss 0.10  Total Training Translation Loss 1.77 
2024-02-08 16:03:05,997 EPOCH 772
2024-02-08 16:03:09,358 [Epoch: 772 Step: 00051700] Batch Recognition Loss:   0.000254 => Gls Tokens per Sec:     2048 || Batch Translation Loss:   0.039930 => Txt Tokens per Sec:     5782 || Lr: 0.000050
2024-02-08 16:03:11,228 Epoch 772: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.32 
2024-02-08 16:03:11,229 EPOCH 773
2024-02-08 16:03:16,752 Epoch 773: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.38 
2024-02-08 16:03:16,753 EPOCH 774
2024-02-08 16:03:17,427 [Epoch: 774 Step: 00051800] Batch Recognition Loss:   0.000478 => Gls Tokens per Sec:     2143 || Batch Translation Loss:   0.019277 => Txt Tokens per Sec:     6051 || Lr: 0.000050
2024-02-08 16:03:21,984 Epoch 774: Total Training Recognition Loss 0.04  Total Training Translation Loss 5.66 
2024-02-08 16:03:21,985 EPOCH 775
2024-02-08 16:03:25,299 [Epoch: 775 Step: 00051900] Batch Recognition Loss:   0.000933 => Gls Tokens per Sec:     2028 || Batch Translation Loss:   0.068661 => Txt Tokens per Sec:     5479 || Lr: 0.000050
2024-02-08 16:03:27,610 Epoch 775: Total Training Recognition Loss 0.10  Total Training Translation Loss 7.62 
2024-02-08 16:03:27,610 EPOCH 776
2024-02-08 16:03:32,646 Epoch 776: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.31 
2024-02-08 16:03:32,646 EPOCH 777
2024-02-08 16:03:33,259 [Epoch: 777 Step: 00052000] Batch Recognition Loss:   0.000674 => Gls Tokens per Sec:     2088 || Batch Translation Loss:   0.028671 => Txt Tokens per Sec:     5369 || Lr: 0.000050
2024-02-08 16:03:41,766 Hooray! New best validation result [eval_metric]!
2024-02-08 16:03:41,767 Saving new checkpoint.
2024-02-08 16:03:42,029 Validation result at epoch 777, step    52000: duration: 8.7700s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.72897	Translation Loss: 90569.69531	PPL: 8485.49902
	Eval Metric: BLEU
	WER 3.46	(DEL: 0.00,	INS: 0.00,	SUB: 3.46)
	BLEU-4 0.96	(BLEU-1: 12.10,	BLEU-2: 4.17,	BLEU-3: 1.82,	BLEU-4: 0.96)
	CHRF 17.34	ROUGE 10.18
2024-02-08 16:03:42,030 Logging Recognition and Translation Outputs
2024-02-08 16:03:42,030 ========================================================================================================================
2024-02-08 16:03:42,030 Logging Sequence: 59_101.00
2024-02-08 16:03:42,031 	Gloss Reference :	A B+C+D+E
2024-02-08 16:03:42,031 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:03:42,031 	Gloss Alignment :	         
2024-02-08 16:03:42,031 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:03:42,032 	Text Reference  :	** did you see  the video fox said she won her medals because of a condom and    is       very   happy
2024-02-08 16:03:42,033 	Text Hypothesis :	on 4th may 2023 the ***** *** **** *** *** *** ****** ******* ** * ****** indian everyone walked over 
2024-02-08 16:03:42,033 	Text Alignment  :	I  S   S   S        D     D   D    D   D   D   D      D       D  D D      S      S        S      S    
2024-02-08 16:03:42,033 ========================================================================================================================
2024-02-08 16:03:42,033 Logging Sequence: 103_112.00
2024-02-08 16:03:42,033 	Gloss Reference :	A B+C+D+E
2024-02-08 16:03:42,033 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:03:42,033 	Gloss Alignment :	         
2024-02-08 16:03:42,033 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:03:42,035 	Text Reference  :	you are aware that earlier the britishers had    colonized a   lot of   countries in   the ****** world
2024-02-08 16:03:42,035 	Text Hypothesis :	*** *** ***** the  rights  of  4          groups is        now the even richer    than the second time 
2024-02-08 16:03:42,035 	Text Alignment  :	D   D   D     S    S       S   S          S      S         S   S   S    S         S        I      S    
2024-02-08 16:03:42,035 ========================================================================================================================
2024-02-08 16:03:42,035 Logging Sequence: 143_11.00
2024-02-08 16:03:42,035 	Gloss Reference :	A B+C+D+E
2024-02-08 16:03:42,036 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:03:42,036 	Gloss Alignment :	         
2024-02-08 16:03:42,036 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:03:42,037 	Text Reference  :	ronaldo has also become the first person   to          have 500 million followers  on     instagram he      is  the most loved footballer
2024-02-08 16:03:42,037 	Text Hypothesis :	******* *** **** ****** the ***** football association fa   and the     merseyside police banned    ronaldo for the **** ***** tournament
2024-02-08 16:03:42,038 	Text Alignment  :	D       D   D    D          D     S        S           S    S   S       S          S      S         S       S       D    D     S         
2024-02-08 16:03:42,038 ========================================================================================================================
2024-02-08 16:03:42,038 Logging Sequence: 183_23.00
2024-02-08 16:03:42,038 	Gloss Reference :	A B+C+D+E
2024-02-08 16:03:42,038 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:03:42,038 	Gloss Alignment :	         
2024-02-08 16:03:42,038 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:03:42,040 	Text Reference  :	however everybody has been waiting for *** them  to     announce the      name of        the       child  
2024-02-08 16:03:42,040 	Text Hypothesis :	******* and       he  also played  for the great leader who      respects his  teammate' religious beliefs
2024-02-08 16:03:42,040 	Text Alignment  :	D       S         S   S    S           I   S     S      S        S        S    S         S         S      
2024-02-08 16:03:42,040 ========================================================================================================================
2024-02-08 16:03:42,040 Logging Sequence: 169_165.00
2024-02-08 16:03:42,040 	Gloss Reference :	A B+C+D+E
2024-02-08 16:03:42,040 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:03:42,040 	Gloss Alignment :	         
2024-02-08 16:03:42,041 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:03:42,042 	Text Reference  :	the ***** **** ** indian government was   outraged by  the incident and *** these changes were undone by    wikipedia
2024-02-08 16:03:42,042 	Text Hypothesis :	the teams need to be     edited     false info     can be  removed  and the right info    can  visit  icc's website  
2024-02-08 16:03:42,042 	Text Alignment  :	    I     I    I  S      S          S     S        S   S   S            I   S     S       S    S      S     S        
2024-02-08 16:03:42,043 ========================================================================================================================
2024-02-08 16:03:46,880 Epoch 777: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.69 
2024-02-08 16:03:46,880 EPOCH 778
2024-02-08 16:03:50,151 [Epoch: 778 Step: 00052100] Batch Recognition Loss:   0.000376 => Gls Tokens per Sec:     1976 || Batch Translation Loss:   0.017143 => Txt Tokens per Sec:     5315 || Lr: 0.000050
2024-02-08 16:03:52,393 Epoch 778: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.84 
2024-02-08 16:03:52,393 EPOCH 779
2024-02-08 16:03:57,847 Epoch 779: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.60 
2024-02-08 16:03:57,848 EPOCH 780
2024-02-08 16:03:58,467 [Epoch: 780 Step: 00052200] Batch Recognition Loss:   0.000401 => Gls Tokens per Sec:     1812 || Batch Translation Loss:   0.022781 => Txt Tokens per Sec:     4958 || Lr: 0.000050
2024-02-08 16:04:03,388 Epoch 780: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.73 
2024-02-08 16:04:03,388 EPOCH 781
2024-02-08 16:04:06,650 [Epoch: 781 Step: 00052300] Batch Recognition Loss:   0.000182 => Gls Tokens per Sec:     1932 || Batch Translation Loss:   0.010324 => Txt Tokens per Sec:     5310 || Lr: 0.000050
2024-02-08 16:04:09,027 Epoch 781: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.81 
2024-02-08 16:04:09,027 EPOCH 782
2024-02-08 16:04:14,417 Epoch 782: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.73 
2024-02-08 16:04:14,417 EPOCH 783
2024-02-08 16:04:15,094 [Epoch: 783 Step: 00052400] Batch Recognition Loss:   0.000326 => Gls Tokens per Sec:     1272 || Batch Translation Loss:   0.032434 => Txt Tokens per Sec:     3698 || Lr: 0.000050
2024-02-08 16:04:19,933 Epoch 783: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.03 
2024-02-08 16:04:19,934 EPOCH 784
2024-02-08 16:04:23,074 [Epoch: 784 Step: 00052500] Batch Recognition Loss:   0.000179 => Gls Tokens per Sec:     1988 || Batch Translation Loss:   0.021036 => Txt Tokens per Sec:     5603 || Lr: 0.000050
2024-02-08 16:04:25,320 Epoch 784: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.08 
2024-02-08 16:04:25,320 EPOCH 785
2024-02-08 16:04:30,850 Epoch 785: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.74 
2024-02-08 16:04:30,851 EPOCH 786
2024-02-08 16:04:31,269 [Epoch: 786 Step: 00052600] Batch Recognition Loss:   0.000229 => Gls Tokens per Sec:     1918 || Batch Translation Loss:   0.019286 => Txt Tokens per Sec:     6113 || Lr: 0.000050
2024-02-08 16:04:36,003 Epoch 786: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.63 
2024-02-08 16:04:36,003 EPOCH 787
2024-02-08 16:04:39,058 [Epoch: 787 Step: 00052700] Batch Recognition Loss:   0.000505 => Gls Tokens per Sec:     1991 || Batch Translation Loss:   0.031250 => Txt Tokens per Sec:     5528 || Lr: 0.000050
2024-02-08 16:04:41,477 Epoch 787: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.05 
2024-02-08 16:04:41,477 EPOCH 788
2024-02-08 16:04:46,667 Epoch 788: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.26 
2024-02-08 16:04:46,667 EPOCH 789
2024-02-08 16:04:47,026 [Epoch: 789 Step: 00052800] Batch Recognition Loss:   0.000590 => Gls Tokens per Sec:     1788 || Batch Translation Loss:   0.063233 => Txt Tokens per Sec:     5539 || Lr: 0.000050
2024-02-08 16:04:51,883 Epoch 789: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.85 
2024-02-08 16:04:51,884 EPOCH 790
2024-02-08 16:04:54,844 [Epoch: 790 Step: 00052900] Batch Recognition Loss:   0.001276 => Gls Tokens per Sec:     1967 || Batch Translation Loss:   0.043235 => Txt Tokens per Sec:     5512 || Lr: 0.000050
2024-02-08 16:04:57,054 Epoch 790: Total Training Recognition Loss 0.05  Total Training Translation Loss 4.00 
2024-02-08 16:04:57,054 EPOCH 791
2024-02-08 16:05:02,240 Epoch 791: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.79 
2024-02-08 16:05:02,240 EPOCH 792
2024-02-08 16:05:02,440 [Epoch: 792 Step: 00053000] Batch Recognition Loss:   0.000781 => Gls Tokens per Sec:     2412 || Batch Translation Loss:   0.038308 => Txt Tokens per Sec:     6543 || Lr: 0.000050
2024-02-08 16:05:07,573 Epoch 792: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.24 
2024-02-08 16:05:07,573 EPOCH 793
2024-02-08 16:05:10,336 [Epoch: 793 Step: 00053100] Batch Recognition Loss:   0.001486 => Gls Tokens per Sec:     2085 || Batch Translation Loss:   0.026921 => Txt Tokens per Sec:     5627 || Lr: 0.000050
2024-02-08 16:05:13,066 Epoch 793: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.59 
2024-02-08 16:05:13,067 EPOCH 794
2024-02-08 16:05:18,646 Epoch 794: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.41 
2024-02-08 16:05:18,646 EPOCH 795
2024-02-08 16:05:18,770 [Epoch: 795 Step: 00053200] Batch Recognition Loss:   0.000248 => Gls Tokens per Sec:     2602 || Batch Translation Loss:   0.019813 => Txt Tokens per Sec:     6545 || Lr: 0.000050
2024-02-08 16:05:23,521 Epoch 795: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.33 
2024-02-08 16:05:23,522 EPOCH 796
2024-02-08 16:05:26,417 [Epoch: 796 Step: 00053300] Batch Recognition Loss:   0.000328 => Gls Tokens per Sec:     1900 || Batch Translation Loss:   0.131060 => Txt Tokens per Sec:     5132 || Lr: 0.000050
2024-02-08 16:05:29,083 Epoch 796: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.14 
2024-02-08 16:05:29,083 EPOCH 797
2024-02-08 16:05:34,927 Epoch 797: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.19 
2024-02-08 16:05:34,928 EPOCH 798
2024-02-08 16:05:34,983 [Epoch: 798 Step: 00053400] Batch Recognition Loss:   0.004691 => Gls Tokens per Sec:     2963 || Batch Translation Loss:   0.004990 => Txt Tokens per Sec:     3796 || Lr: 0.000050
2024-02-08 16:05:40,439 Epoch 798: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.28 
2024-02-08 16:05:40,439 EPOCH 799
2024-02-08 16:05:43,021 [Epoch: 799 Step: 00053500] Batch Recognition Loss:   0.002892 => Gls Tokens per Sec:     2069 || Batch Translation Loss:   0.025407 => Txt Tokens per Sec:     5937 || Lr: 0.000050
2024-02-08 16:05:45,871 Epoch 799: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.19 
2024-02-08 16:05:45,872 EPOCH 800
2024-02-08 16:05:51,265 [Epoch: 800 Step: 00053600] Batch Recognition Loss:   0.000240 => Gls Tokens per Sec:     1970 || Batch Translation Loss:   0.023348 => Txt Tokens per Sec:     5449 || Lr: 0.000050
2024-02-08 16:05:51,266 Epoch 800: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.80 
2024-02-08 16:05:51,266 EPOCH 801
2024-02-08 16:05:56,898 Epoch 801: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.36 
2024-02-08 16:05:56,898 EPOCH 802
2024-02-08 16:05:59,365 [Epoch: 802 Step: 00053700] Batch Recognition Loss:   0.000164 => Gls Tokens per Sec:     2141 || Batch Translation Loss:   0.015580 => Txt Tokens per Sec:     5867 || Lr: 0.000050
2024-02-08 16:06:02,269 Epoch 802: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.13 
2024-02-08 16:06:02,270 EPOCH 803
2024-02-08 16:06:07,529 [Epoch: 803 Step: 00053800] Batch Recognition Loss:   0.000216 => Gls Tokens per Sec:     1989 || Batch Translation Loss:   0.013977 => Txt Tokens per Sec:     5491 || Lr: 0.000050
2024-02-08 16:06:07,606 Epoch 803: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.20 
2024-02-08 16:06:07,606 EPOCH 804
2024-02-08 16:06:12,874 Epoch 804: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.48 
2024-02-08 16:06:12,875 EPOCH 805
2024-02-08 16:06:15,547 [Epoch: 805 Step: 00053900] Batch Recognition Loss:   0.000921 => Gls Tokens per Sec:     1918 || Batch Translation Loss:   0.020428 => Txt Tokens per Sec:     5339 || Lr: 0.000050
2024-02-08 16:06:18,330 Epoch 805: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.51 
2024-02-08 16:06:18,330 EPOCH 806
2024-02-08 16:06:23,931 [Epoch: 806 Step: 00054000] Batch Recognition Loss:   0.002435 => Gls Tokens per Sec:     1839 || Batch Translation Loss:   0.038909 => Txt Tokens per Sec:     5087 || Lr: 0.000050
2024-02-08 16:06:32,316 Validation result at epoch 806, step    54000: duration: 8.3840s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.78763	Translation Loss: 90431.21094	PPL: 8368.93457
	Eval Metric: BLEU
	WER 3.53	(DEL: 0.00,	INS: 0.00,	SUB: 3.53)
	BLEU-4 0.48	(BLEU-1: 10.88,	BLEU-2: 3.41,	BLEU-3: 1.24,	BLEU-4: 0.48)
	CHRF 16.94	ROUGE 8.96
2024-02-08 16:06:32,317 Logging Recognition and Translation Outputs
2024-02-08 16:06:32,317 ========================================================================================================================
2024-02-08 16:06:32,317 Logging Sequence: 166_243.00
2024-02-08 16:06:32,318 	Gloss Reference :	A B+C+D+E  
2024-02-08 16:06:32,318 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-08 16:06:32,318 	Gloss Alignment :	  S        
2024-02-08 16:06:32,318 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:06:32,319 	Text Reference  :	*** ********* *********** ********* *** ***** ** icc     worked with members boards like bcci pcb   cricket australia etc 
2024-02-08 16:06:32,319 	Text Hypothesis :	the broadcast advertisers ticketing etc would be decided by     the  board   of     the  2    teams playing the       test
2024-02-08 16:06:32,319 	Text Alignment  :	I   I         I           I         I   I     I  S       S      S    S       S      S    S    S     S       S         S   
2024-02-08 16:06:32,319 ========================================================================================================================
2024-02-08 16:06:32,320 Logging Sequence: 179_409.00
2024-02-08 16:06:32,320 	Gloss Reference :	A B+C+D+E
2024-02-08 16:06:32,320 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:06:32,320 	Gloss Alignment :	         
2024-02-08 16:06:32,320 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:06:32,321 	Text Reference  :	************* *** *** the passport was      at   the ******** **** wfi    office in  delhi   
2024-02-08 16:06:32,321 	Text Hypothesis :	unfortunately she had a   heated   exchange with the olympics then handed over   the olympics
2024-02-08 16:06:32,321 	Text Alignment  :	I             I   I   S   S        S        S        I        I    S      S      S   S       
2024-02-08 16:06:32,321 ========================================================================================================================
2024-02-08 16:06:32,321 Logging Sequence: 81_407.00
2024-02-08 16:06:32,322 	Gloss Reference :	A B+C+D+E
2024-02-08 16:06:32,322 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:06:32,322 	Gloss Alignment :	         
2024-02-08 16:06:32,322 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:06:32,323 	Text Reference  :	the government company - national buildings construction corporation and they  will complete them in    a  time-bound manner
2024-02-08 16:06:32,323 	Text Hypothesis :	the ********** ******* * ******** amrapali  group        paid        a   total of   rs       4222 crore to rhiti      sports
2024-02-08 16:06:32,324 	Text Alignment  :	    D          D       D D        S         S            S           S   S     S    S        S    S     S  S          S     
2024-02-08 16:06:32,324 ========================================================================================================================
2024-02-08 16:06:32,324 Logging Sequence: 96_31.00
2024-02-08 16:06:32,324 	Gloss Reference :	A B+C+D+E
2024-02-08 16:06:32,324 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:06:32,324 	Gloss Alignment :	         
2024-02-08 16:06:32,324 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:06:32,325 	Text Reference  :	and then 2 teams will go   on      to  play   the  final  
2024-02-08 16:06:32,325 	Text Hypothesis :	*** **** * ***** **** with india's win people felt relaxed
2024-02-08 16:06:32,325 	Text Alignment  :	D   D    D D     D    S    S       S   S      S    S      
2024-02-08 16:06:32,325 ========================================================================================================================
2024-02-08 16:06:32,325 Logging Sequence: 160_87.00
2024-02-08 16:06:32,326 	Gloss Reference :	A B+C+D+E
2024-02-08 16:06:32,326 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:06:32,326 	Gloss Alignment :	         
2024-02-08 16:06:32,326 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:06:32,327 	Text Reference  :	******* ** ******* *** ***** ** ***** ******* kohli held a  press   conference and said  
2024-02-08 16:06:32,327 	Text Hypothesis :	however my country was glued to their screens as    this is excited to         the finals
2024-02-08 16:06:32,327 	Text Alignment  :	I       I  I       I   I     I  I     I       S     S    S  S       S          S   S     
2024-02-08 16:06:32,327 ========================================================================================================================
2024-02-08 16:06:32,491 Epoch 806: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.88 
2024-02-08 16:06:32,491 EPOCH 807
2024-02-08 16:06:38,328 Epoch 807: Total Training Recognition Loss 0.04  Total Training Translation Loss 4.32 
2024-02-08 16:06:38,329 EPOCH 808
2024-02-08 16:06:40,792 [Epoch: 808 Step: 00054100] Batch Recognition Loss:   0.000391 => Gls Tokens per Sec:     1974 || Batch Translation Loss:   0.035246 => Txt Tokens per Sec:     5330 || Lr: 0.000050
2024-02-08 16:06:43,530 Epoch 808: Total Training Recognition Loss 0.05  Total Training Translation Loss 5.18 
2024-02-08 16:06:43,530 EPOCH 809
2024-02-08 16:06:48,555 [Epoch: 809 Step: 00054200] Batch Recognition Loss:   0.000374 => Gls Tokens per Sec:     2018 || Batch Translation Loss:   0.198695 => Txt Tokens per Sec:     5600 || Lr: 0.000050
2024-02-08 16:06:48,789 Epoch 809: Total Training Recognition Loss 0.06  Total Training Translation Loss 4.22 
2024-02-08 16:06:48,790 EPOCH 810
2024-02-08 16:06:53,976 Epoch 810: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.31 
2024-02-08 16:06:53,976 EPOCH 811
2024-02-08 16:06:56,128 [Epoch: 811 Step: 00054300] Batch Recognition Loss:   0.000276 => Gls Tokens per Sec:     2185 || Batch Translation Loss:   0.039095 => Txt Tokens per Sec:     6026 || Lr: 0.000050
2024-02-08 16:06:59,229 Epoch 811: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.81 
2024-02-08 16:06:59,230 EPOCH 812
2024-02-08 16:07:03,970 [Epoch: 812 Step: 00054400] Batch Recognition Loss:   0.000286 => Gls Tokens per Sec:     2106 || Batch Translation Loss:   0.043122 => Txt Tokens per Sec:     5851 || Lr: 0.000050
2024-02-08 16:07:04,278 Epoch 812: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.48 
2024-02-08 16:07:04,278 EPOCH 813
2024-02-08 16:07:09,439 Epoch 813: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.90 
2024-02-08 16:07:09,439 EPOCH 814
2024-02-08 16:07:11,723 [Epoch: 814 Step: 00054500] Batch Recognition Loss:   0.000201 => Gls Tokens per Sec:     2032 || Batch Translation Loss:   0.025296 => Txt Tokens per Sec:     5621 || Lr: 0.000050
2024-02-08 16:07:14,687 Epoch 814: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.26 
2024-02-08 16:07:14,687 EPOCH 815
2024-02-08 16:07:19,119 [Epoch: 815 Step: 00054600] Batch Recognition Loss:   0.000211 => Gls Tokens per Sec:     2216 || Batch Translation Loss:   0.017593 => Txt Tokens per Sec:     6186 || Lr: 0.000050
2024-02-08 16:07:19,380 Epoch 815: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.59 
2024-02-08 16:07:19,381 EPOCH 816
2024-02-08 16:07:24,939 Epoch 816: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.87 
2024-02-08 16:07:24,940 EPOCH 817
2024-02-08 16:07:27,112 [Epoch: 817 Step: 00054700] Batch Recognition Loss:   0.000704 => Gls Tokens per Sec:     2064 || Batch Translation Loss:   0.028360 => Txt Tokens per Sec:     5842 || Lr: 0.000050
2024-02-08 16:07:29,739 Epoch 817: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.95 
2024-02-08 16:07:29,739 EPOCH 818
2024-02-08 16:07:34,737 [Epoch: 818 Step: 00054800] Batch Recognition Loss:   0.000279 => Gls Tokens per Sec:     1953 || Batch Translation Loss:   0.019391 => Txt Tokens per Sec:     5425 || Lr: 0.000050
2024-02-08 16:07:35,233 Epoch 818: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.36 
2024-02-08 16:07:35,233 EPOCH 819
2024-02-08 16:07:40,195 Epoch 819: Total Training Recognition Loss 0.09  Total Training Translation Loss 4.53 
2024-02-08 16:07:40,195 EPOCH 820
2024-02-08 16:07:42,533 [Epoch: 820 Step: 00054900] Batch Recognition Loss:   0.000862 => Gls Tokens per Sec:     1849 || Batch Translation Loss:   0.036191 => Txt Tokens per Sec:     5284 || Lr: 0.000050
2024-02-08 16:07:45,912 Epoch 820: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.70 
2024-02-08 16:07:45,913 EPOCH 821
2024-02-08 16:07:50,721 [Epoch: 821 Step: 00055000] Batch Recognition Loss:   0.000683 => Gls Tokens per Sec:     1976 || Batch Translation Loss:   0.016929 => Txt Tokens per Sec:     5397 || Lr: 0.000050
2024-02-08 16:07:51,314 Epoch 821: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.09 
2024-02-08 16:07:51,314 EPOCH 822
2024-02-08 16:07:56,143 Epoch 822: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.55 
2024-02-08 16:07:56,143 EPOCH 823
2024-02-08 16:07:58,293 [Epoch: 823 Step: 00055100] Batch Recognition Loss:   0.000459 => Gls Tokens per Sec:     1936 || Batch Translation Loss:   0.022218 => Txt Tokens per Sec:     5355 || Lr: 0.000050
2024-02-08 16:08:01,644 Epoch 823: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.53 
2024-02-08 16:08:01,644 EPOCH 824
2024-02-08 16:08:06,050 [Epoch: 824 Step: 00055200] Batch Recognition Loss:   0.000133 => Gls Tokens per Sec:     2121 || Batch Translation Loss:   0.016825 => Txt Tokens per Sec:     5967 || Lr: 0.000050
2024-02-08 16:08:06,737 Epoch 824: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.80 
2024-02-08 16:08:06,737 EPOCH 825
2024-02-08 16:08:12,034 Epoch 825: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.19 
2024-02-08 16:08:12,034 EPOCH 826
2024-02-08 16:08:13,873 [Epoch: 826 Step: 00055300] Batch Recognition Loss:   0.000197 => Gls Tokens per Sec:     2177 || Batch Translation Loss:   0.018499 => Txt Tokens per Sec:     6221 || Lr: 0.000050
2024-02-08 16:08:17,121 Epoch 826: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.29 
2024-02-08 16:08:17,121 EPOCH 827
2024-02-08 16:08:21,774 [Epoch: 827 Step: 00055400] Batch Recognition Loss:   0.000251 => Gls Tokens per Sec:     1973 || Batch Translation Loss:   0.012513 => Txt Tokens per Sec:     5459 || Lr: 0.000050
2024-02-08 16:08:22,486 Epoch 827: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.87 
2024-02-08 16:08:22,486 EPOCH 828
2024-02-08 16:08:27,771 Epoch 828: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.66 
2024-02-08 16:08:27,772 EPOCH 829
2024-02-08 16:08:30,044 [Epoch: 829 Step: 00055500] Batch Recognition Loss:   0.000362 => Gls Tokens per Sec:     1691 || Batch Translation Loss:   0.026982 => Txt Tokens per Sec:     4922 || Lr: 0.000050
2024-02-08 16:08:33,377 Epoch 829: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.42 
2024-02-08 16:08:33,377 EPOCH 830
2024-02-08 16:08:37,327 [Epoch: 830 Step: 00055600] Batch Recognition Loss:   0.000140 => Gls Tokens per Sec:     2285 || Batch Translation Loss:   0.017807 => Txt Tokens per Sec:     6290 || Lr: 0.000050
2024-02-08 16:08:38,216 Epoch 830: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.81 
2024-02-08 16:08:38,216 EPOCH 831
2024-02-08 16:08:43,702 Epoch 831: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.37 
2024-02-08 16:08:43,703 EPOCH 832
2024-02-08 16:08:45,200 [Epoch: 832 Step: 00055700] Batch Recognition Loss:   0.000221 => Gls Tokens per Sec:     2460 || Batch Translation Loss:   0.090182 => Txt Tokens per Sec:     6328 || Lr: 0.000050
2024-02-08 16:08:48,596 Epoch 832: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.45 
2024-02-08 16:08:48,596 EPOCH 833
2024-02-08 16:08:53,049 [Epoch: 833 Step: 00055800] Batch Recognition Loss:   0.000169 => Gls Tokens per Sec:     1990 || Batch Translation Loss:   0.068022 => Txt Tokens per Sec:     5431 || Lr: 0.000050
2024-02-08 16:08:54,145 Epoch 833: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.90 
2024-02-08 16:08:54,145 EPOCH 834
2024-02-08 16:08:59,066 Epoch 834: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.50 
2024-02-08 16:08:59,066 EPOCH 835
2024-02-08 16:09:01,044 [Epoch: 835 Step: 00055900] Batch Recognition Loss:   0.000227 => Gls Tokens per Sec:     1731 || Batch Translation Loss:   0.023737 => Txt Tokens per Sec:     4780 || Lr: 0.000050
2024-02-08 16:09:04,501 Epoch 835: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.42 
2024-02-08 16:09:04,501 EPOCH 836
2024-02-08 16:09:08,599 [Epoch: 836 Step: 00056000] Batch Recognition Loss:   0.000222 => Gls Tokens per Sec:     2124 || Batch Translation Loss:   0.025053 => Txt Tokens per Sec:     5932 || Lr: 0.000050
2024-02-08 16:09:17,190 Validation result at epoch 836, step    56000: duration: 8.5910s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 3.02736	Translation Loss: 91036.88281	PPL: 8890.83594
	Eval Metric: BLEU
	WER 3.46	(DEL: 0.00,	INS: 0.00,	SUB: 3.46)
	BLEU-4 0.64	(BLEU-1: 10.92,	BLEU-2: 3.66,	BLEU-3: 1.43,	BLEU-4: 0.64)
	CHRF 17.06	ROUGE 9.40
2024-02-08 16:09:17,191 Logging Recognition and Translation Outputs
2024-02-08 16:09:17,192 ========================================================================================================================
2024-02-08 16:09:17,192 Logging Sequence: 177_167.00
2024-02-08 16:09:17,192 	Gloss Reference :	A B+C+D+E
2024-02-08 16:09:17,192 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:09:17,192 	Gloss Alignment :	         
2024-02-08 16:09:17,192 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:09:17,194 	Text Reference  :	*** ****** this is because    sushil wanted to **** establish his fear   to     ensure no    one  would  oppose him 
2024-02-08 16:09:17,194 	Text Hypothesis :	the police want to interogate sushil ajay   to find out       the motive behind the    brawl that killed sagar  rana
2024-02-08 16:09:17,194 	Text Alignment  :	I   I      S    S  S                 S         I    S         S   S      S      S      S     S    S      S      S   
2024-02-08 16:09:17,195 ========================================================================================================================
2024-02-08 16:09:17,195 Logging Sequence: 127_140.00
2024-02-08 16:09:17,195 	Gloss Reference :	A B+C+D+E
2024-02-08 16:09:17,195 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:09:17,195 	Gloss Alignment :	         
2024-02-08 16:09:17,195 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:09:17,197 	Text Reference  :	*** this is  india' 3rd  medal in the world athletics championships he is very talented and his performance is      highly impressive
2024-02-08 16:09:17,197 	Text Hypothesis :	who had  won 4      gold medal at the world ********* ************* ** ** **** ******** *** *** cup         triumph in     2022      
2024-02-08 16:09:17,197 	Text Alignment  :	I   S    S   S      S          S            D         D             D  D  D    D        D   D   S           S       S      S         
2024-02-08 16:09:17,197 ========================================================================================================================
2024-02-08 16:09:17,197 Logging Sequence: 126_200.00
2024-02-08 16:09:17,197 	Gloss Reference :	A B+C+D+E
2024-02-08 16:09:17,198 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:09:17,198 	Gloss Alignment :	         
2024-02-08 16:09:17,198 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:09:17,198 	Text Reference  :	let me  tell    you about them
2024-02-08 16:09:17,198 	Text Hypothesis :	he  was granted by  the   bcci
2024-02-08 16:09:17,199 	Text Alignment  :	S   S   S       S   S     S   
2024-02-08 16:09:17,199 ========================================================================================================================
2024-02-08 16:09:17,199 Logging Sequence: 104_119.00
2024-02-08 16:09:17,199 	Gloss Reference :	A B+C+D+E    
2024-02-08 16:09:17,199 	Gloss Hypothesis:	A B+C+D+E+D+E
2024-02-08 16:09:17,199 	Gloss Alignment :	  S          
2024-02-08 16:09:17,199 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:09:17,201 	Text Reference  :	famous chess players like  viswanathan anand and   praggnanandhaa's coach r     b  ramesh congratulated him  for  his impressive performance
2024-02-08 16:09:17,201 	Text Hypothesis :	****** ***** the     young boy         was   fined rs               10    crore to decide the           will also be  returning  soon       
2024-02-08 16:09:17,201 	Text Alignment  :	D      D     S       S     S           S     S     S                S     S     S  S      S             S    S    S   S          S          
2024-02-08 16:09:17,201 ========================================================================================================================
2024-02-08 16:09:17,201 Logging Sequence: 172_267.00
2024-02-08 16:09:17,202 	Gloss Reference :	A B+C+D+E
2024-02-08 16:09:17,202 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:09:17,202 	Gloss Alignment :	         
2024-02-08 16:09:17,202 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:09:17,202 	Text Reference  :	such provisions have been made   
2024-02-08 16:09:17,202 	Text Hypothesis :	**** this       is   just rubbish
2024-02-08 16:09:17,203 	Text Alignment  :	D    S          S    S    S      
2024-02-08 16:09:17,203 ========================================================================================================================
2024-02-08 16:09:18,189 Epoch 836: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.06 
2024-02-08 16:09:18,189 EPOCH 837
2024-02-08 16:09:23,854 Epoch 837: Total Training Recognition Loss 0.10  Total Training Translation Loss 2.17 
2024-02-08 16:09:23,855 EPOCH 838
2024-02-08 16:09:25,532 [Epoch: 838 Step: 00056100] Batch Recognition Loss:   0.000215 => Gls Tokens per Sec:     1946 || Batch Translation Loss:   0.175566 => Txt Tokens per Sec:     5287 || Lr: 0.000050
2024-02-08 16:09:28,972 Epoch 838: Total Training Recognition Loss 0.09  Total Training Translation Loss 3.85 
2024-02-08 16:09:28,972 EPOCH 839
2024-02-08 16:09:33,246 [Epoch: 839 Step: 00056200] Batch Recognition Loss:   0.000525 => Gls Tokens per Sec:     1999 || Batch Translation Loss:   0.054140 => Txt Tokens per Sec:     5485 || Lr: 0.000050
2024-02-08 16:09:34,313 Epoch 839: Total Training Recognition Loss 0.08  Total Training Translation Loss 12.25 
2024-02-08 16:09:34,313 EPOCH 840
2024-02-08 16:09:39,481 Epoch 840: Total Training Recognition Loss 0.14  Total Training Translation Loss 10.20 
2024-02-08 16:09:39,481 EPOCH 841
2024-02-08 16:09:40,799 [Epoch: 841 Step: 00056300] Batch Recognition Loss:   0.001836 => Gls Tokens per Sec:     2430 || Batch Translation Loss:   0.034529 => Txt Tokens per Sec:     6611 || Lr: 0.000050
2024-02-08 16:09:45,001 Epoch 841: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.59 
2024-02-08 16:09:45,002 EPOCH 842
2024-02-08 16:09:49,105 [Epoch: 842 Step: 00056400] Batch Recognition Loss:   0.000278 => Gls Tokens per Sec:     2042 || Batch Translation Loss:   0.027826 => Txt Tokens per Sec:     5633 || Lr: 0.000050
2024-02-08 16:09:50,220 Epoch 842: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.06 
2024-02-08 16:09:50,220 EPOCH 843
2024-02-08 16:09:55,797 Epoch 843: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.58 
2024-02-08 16:09:55,798 EPOCH 844
2024-02-08 16:09:57,198 [Epoch: 844 Step: 00056500] Batch Recognition Loss:   0.000236 => Gls Tokens per Sec:     2173 || Batch Translation Loss:   0.029436 => Txt Tokens per Sec:     5719 || Lr: 0.000050
2024-02-08 16:10:01,034 Epoch 844: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.55 
2024-02-08 16:10:01,035 EPOCH 845
2024-02-08 16:10:05,555 [Epoch: 845 Step: 00056600] Batch Recognition Loss:   0.000192 => Gls Tokens per Sec:     1819 || Batch Translation Loss:   0.015511 => Txt Tokens per Sec:     5137 || Lr: 0.000050
2024-02-08 16:10:06,615 Epoch 845: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.74 
2024-02-08 16:10:06,615 EPOCH 846
2024-02-08 16:10:11,837 Epoch 846: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.65 
2024-02-08 16:10:11,838 EPOCH 847
2024-02-08 16:10:13,242 [Epoch: 847 Step: 00056700] Batch Recognition Loss:   0.000194 => Gls Tokens per Sec:     2054 || Batch Translation Loss:   0.017070 => Txt Tokens per Sec:     5709 || Lr: 0.000050
2024-02-08 16:10:17,208 Epoch 847: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.49 
2024-02-08 16:10:17,208 EPOCH 848
2024-02-08 16:10:20,839 [Epoch: 848 Step: 00056800] Batch Recognition Loss:   0.000148 => Gls Tokens per Sec:     2248 || Batch Translation Loss:   0.020649 => Txt Tokens per Sec:     6286 || Lr: 0.000050
2024-02-08 16:10:22,006 Epoch 848: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.61 
2024-02-08 16:10:22,006 EPOCH 849
2024-02-08 16:10:27,293 Epoch 849: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.51 
2024-02-08 16:10:27,293 EPOCH 850
2024-02-08 16:10:28,592 [Epoch: 850 Step: 00056900] Batch Recognition Loss:   0.000126 => Gls Tokens per Sec:     2096 || Batch Translation Loss:   0.015879 => Txt Tokens per Sec:     5885 || Lr: 0.000050
2024-02-08 16:10:32,379 Epoch 850: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.20 
2024-02-08 16:10:32,380 EPOCH 851
2024-02-08 16:10:35,772 [Epoch: 851 Step: 00057000] Batch Recognition Loss:   0.000285 => Gls Tokens per Sec:     2358 || Batch Translation Loss:   0.016303 => Txt Tokens per Sec:     6490 || Lr: 0.000050
2024-02-08 16:10:37,245 Epoch 851: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.11 
2024-02-08 16:10:37,246 EPOCH 852
2024-02-08 16:10:42,620 Epoch 852: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.11 
2024-02-08 16:10:42,621 EPOCH 853
2024-02-08 16:10:43,860 [Epoch: 853 Step: 00057100] Batch Recognition Loss:   0.000268 => Gls Tokens per Sec:     2068 || Batch Translation Loss:   0.017338 => Txt Tokens per Sec:     5949 || Lr: 0.000050
2024-02-08 16:10:47,702 Epoch 853: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.00 
2024-02-08 16:10:47,703 EPOCH 854
2024-02-08 16:10:51,580 [Epoch: 854 Step: 00057200] Batch Recognition Loss:   0.000261 => Gls Tokens per Sec:     2023 || Batch Translation Loss:   0.021020 => Txt Tokens per Sec:     5527 || Lr: 0.000050
2024-02-08 16:10:53,212 Epoch 854: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.42 
2024-02-08 16:10:53,213 EPOCH 855
2024-02-08 16:10:58,795 Epoch 855: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.66 
2024-02-08 16:10:58,795 EPOCH 856
2024-02-08 16:10:59,872 [Epoch: 856 Step: 00057300] Batch Recognition Loss:   0.000227 => Gls Tokens per Sec:     2230 || Batch Translation Loss:   0.048204 => Txt Tokens per Sec:     5760 || Lr: 0.000050
2024-02-08 16:11:04,033 Epoch 856: Total Training Recognition Loss 0.05  Total Training Translation Loss 5.53 
2024-02-08 16:11:04,033 EPOCH 857
2024-02-08 16:11:07,827 [Epoch: 857 Step: 00057400] Batch Recognition Loss:   0.000782 => Gls Tokens per Sec:     1999 || Batch Translation Loss:   0.081164 => Txt Tokens per Sec:     5503 || Lr: 0.000050
2024-02-08 16:11:09,563 Epoch 857: Total Training Recognition Loss 0.06  Total Training Translation Loss 6.43 
2024-02-08 16:11:09,564 EPOCH 858
2024-02-08 16:11:14,825 Epoch 858: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.92 
2024-02-08 16:11:14,825 EPOCH 859
2024-02-08 16:11:15,775 [Epoch: 859 Step: 00057500] Batch Recognition Loss:   0.000331 => Gls Tokens per Sec:     2360 || Batch Translation Loss:   0.030379 => Txt Tokens per Sec:     6566 || Lr: 0.000050
2024-02-08 16:11:19,767 Epoch 859: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.75 
2024-02-08 16:11:19,768 EPOCH 860
2024-02-08 16:11:23,716 [Epoch: 860 Step: 00057600] Batch Recognition Loss:   0.000373 => Gls Tokens per Sec:     1880 || Batch Translation Loss:   0.130717 => Txt Tokens per Sec:     5266 || Lr: 0.000050
2024-02-08 16:11:25,163 Epoch 860: Total Training Recognition Loss 0.05  Total Training Translation Loss 5.28 
2024-02-08 16:11:25,163 EPOCH 861
2024-02-08 16:11:29,868 Epoch 861: Total Training Recognition Loss 0.07  Total Training Translation Loss 4.49 
2024-02-08 16:11:29,869 EPOCH 862
2024-02-08 16:11:31,008 [Epoch: 862 Step: 00057700] Batch Recognition Loss:   0.000311 => Gls Tokens per Sec:     1741 || Batch Translation Loss:   0.019618 => Txt Tokens per Sec:     4704 || Lr: 0.000050
2024-02-08 16:11:35,455 Epoch 862: Total Training Recognition Loss 0.12  Total Training Translation Loss 2.41 
2024-02-08 16:11:35,455 EPOCH 863
2024-02-08 16:11:38,903 [Epoch: 863 Step: 00057800] Batch Recognition Loss:   0.000329 => Gls Tokens per Sec:     2106 || Batch Translation Loss:   0.009028 => Txt Tokens per Sec:     5926 || Lr: 0.000050
2024-02-08 16:11:40,306 Epoch 863: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.77 
2024-02-08 16:11:40,307 EPOCH 864
2024-02-08 16:11:45,323 Epoch 864: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.62 
2024-02-08 16:11:45,323 EPOCH 865
2024-02-08 16:11:46,389 [Epoch: 865 Step: 00057900] Batch Recognition Loss:   0.000198 => Gls Tokens per Sec:     1709 || Batch Translation Loss:   0.014122 => Txt Tokens per Sec:     4637 || Lr: 0.000050
2024-02-08 16:11:50,614 Epoch 865: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.85 
2024-02-08 16:11:50,614 EPOCH 866
2024-02-08 16:11:53,853 [Epoch: 866 Step: 00058000] Batch Recognition Loss:   0.000098 => Gls Tokens per Sec:     2224 || Batch Translation Loss:   0.014427 => Txt Tokens per Sec:     5962 || Lr: 0.000050
2024-02-08 16:12:02,330 Validation result at epoch 866, step    58000: duration: 8.4763s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.87459	Translation Loss: 90925.00781	PPL: 8792.04004
	Eval Metric: BLEU
	WER 3.81	(DEL: 0.00,	INS: 0.00,	SUB: 3.81)
	BLEU-4 0.51	(BLEU-1: 11.06,	BLEU-2: 3.29,	BLEU-3: 1.18,	BLEU-4: 0.51)
	CHRF 17.07	ROUGE 9.50
2024-02-08 16:12:02,331 Logging Recognition and Translation Outputs
2024-02-08 16:12:02,331 ========================================================================================================================
2024-02-08 16:12:02,332 Logging Sequence: 60_264.00
2024-02-08 16:12:02,332 	Gloss Reference :	A B+C+D+E
2024-02-08 16:12:02,332 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:12:02,332 	Gloss Alignment :	         
2024-02-08 16:12:02,332 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:12:02,333 	Text Reference  :	plus do you know that a sex tape    of his    with two  women     had gone     viral  
2024-02-08 16:12:02,334 	Text Hypothesis :	**** ** *** **** **** * *** however my family has  been embroiled in  multiple wickets
2024-02-08 16:12:02,334 	Text Alignment  :	D    D  D   D    D    D D   S       S  S      S    S    S         S   S        S      
2024-02-08 16:12:02,334 ========================================================================================================================
2024-02-08 16:12:02,334 Logging Sequence: 100_50.00
2024-02-08 16:12:02,334 	Gloss Reference :	A B+C+D+E
2024-02-08 16:12:02,334 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:12:02,334 	Gloss Alignment :	         
2024-02-08 16:12:02,335 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:12:02,335 	Text Reference  :	with virat kohli as the captain
2024-02-08 16:12:02,335 	Text Hypothesis :	**** ***** ***** ** how strange
2024-02-08 16:12:02,335 	Text Alignment  :	D    D     D     D  S   S      
2024-02-08 16:12:02,335 ========================================================================================================================
2024-02-08 16:12:02,335 Logging Sequence: 137_44.00
2024-02-08 16:12:02,335 	Gloss Reference :	A B+C+D+E
2024-02-08 16:12:02,336 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:12:02,336 	Gloss Alignment :	         
2024-02-08 16:12:02,336 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:12:02,338 	Text Reference  :	let me tell    you the  rules that       qatar          has  announced for     the     fans travelling for   the world cup 
2024-02-08 16:12:02,338 	Text Hypothesis :	*** ** india's 16  year old   rameshbabu praggnanandhaa from both      cricket council acc  whose      chief is  no    head
2024-02-08 16:12:02,338 	Text Alignment  :	D   D  S       S   S    S     S          S              S    S         S       S       S    S          S     S   S     S   
2024-02-08 16:12:02,338 ========================================================================================================================
2024-02-08 16:12:02,338 Logging Sequence: 58_27.00
2024-02-08 16:12:02,338 	Gloss Reference :	A B+C+D+E
2024-02-08 16:12:02,338 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:12:02,339 	Gloss Alignment :	         
2024-02-08 16:12:02,339 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:12:02,340 	Text Reference  :	*** *** ** ***** the     19th     asian games 2022 were  to be      held       in     hangzhou china     
2024-02-08 16:12:02,340 	Text Hypothesis :	ipl has 10 teams against pakistan in    the   same place of india's contingent during the      tournament
2024-02-08 16:12:02,340 	Text Alignment  :	I   I   I  I     S       S        S     S     S    S     S  S       S          S      S        S         
2024-02-08 16:12:02,340 ========================================================================================================================
2024-02-08 16:12:02,340 Logging Sequence: 75_255.00
2024-02-08 16:12:02,341 	Gloss Reference :	A B+C+D+E  
2024-02-08 16:12:02,341 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-08 16:12:02,341 	Gloss Alignment :	  S        
2024-02-08 16:12:02,341 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:12:02,343 	Text Reference  :	we miss our baby boy  with      this  ronaldo' total baby count       has reached 5     with     2  boys 3       girls
2024-02-08 16:12:02,343 	Text Hypothesis :	** **** *** a    more intensive audit revealed that  if   afghanistan had covered their decision on 25th october 2022 
2024-02-08 16:12:02,343 	Text Alignment  :	D  D    D   S    S    S         S     S        S     S    S           S   S       S     S        S  S    S       S    
2024-02-08 16:12:02,343 ========================================================================================================================
2024-02-08 16:12:04,510 Epoch 866: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.88 
2024-02-08 16:12:04,510 EPOCH 867
2024-02-08 16:12:10,072 Epoch 867: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.34 
2024-02-08 16:12:10,073 EPOCH 868
2024-02-08 16:12:10,957 [Epoch: 868 Step: 00058100] Batch Recognition Loss:   0.001512 => Gls Tokens per Sec:     1995 || Batch Translation Loss:   0.021447 => Txt Tokens per Sec:     5960 || Lr: 0.000050
2024-02-08 16:12:15,400 Epoch 868: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.26 
2024-02-08 16:12:15,401 EPOCH 869
2024-02-08 16:12:18,703 [Epoch: 869 Step: 00058200] Batch Recognition Loss:   0.000149 => Gls Tokens per Sec:     2103 || Batch Translation Loss:   0.027519 => Txt Tokens per Sec:     5687 || Lr: 0.000050
2024-02-08 16:12:20,643 Epoch 869: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.21 
2024-02-08 16:12:20,643 EPOCH 870
2024-02-08 16:12:26,031 Epoch 870: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.80 
2024-02-08 16:12:26,032 EPOCH 871
2024-02-08 16:12:26,830 [Epoch: 871 Step: 00058300] Batch Recognition Loss:   0.000315 => Gls Tokens per Sec:     2008 || Batch Translation Loss:   0.020832 => Txt Tokens per Sec:     5644 || Lr: 0.000050
2024-02-08 16:12:31,583 Epoch 871: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.73 
2024-02-08 16:12:31,583 EPOCH 872
2024-02-08 16:12:35,180 [Epoch: 872 Step: 00058400] Batch Recognition Loss:   0.000211 => Gls Tokens per Sec:     1886 || Batch Translation Loss:   0.023661 => Txt Tokens per Sec:     5322 || Lr: 0.000050
2024-02-08 16:12:37,118 Epoch 872: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.47 
2024-02-08 16:12:37,118 EPOCH 873
2024-02-08 16:12:42,641 Epoch 873: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.51 
2024-02-08 16:12:42,641 EPOCH 874
2024-02-08 16:12:43,270 [Epoch: 874 Step: 00058500] Batch Recognition Loss:   0.000212 => Gls Tokens per Sec:     2293 || Batch Translation Loss:   0.015161 => Txt Tokens per Sec:     5755 || Lr: 0.000050
2024-02-08 16:12:48,057 Epoch 874: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.49 
2024-02-08 16:12:48,057 EPOCH 875
2024-02-08 16:12:51,402 [Epoch: 875 Step: 00058600] Batch Recognition Loss:   0.000465 => Gls Tokens per Sec:     1980 || Batch Translation Loss:   0.012718 => Txt Tokens per Sec:     5280 || Lr: 0.000050
2024-02-08 16:12:53,540 Epoch 875: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.55 
2024-02-08 16:12:53,540 EPOCH 876
2024-02-08 16:12:58,866 Epoch 876: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.99 
2024-02-08 16:12:58,866 EPOCH 877
2024-02-08 16:12:59,359 [Epoch: 877 Step: 00058700] Batch Recognition Loss:   0.000231 => Gls Tokens per Sec:     2602 || Batch Translation Loss:   0.027427 => Txt Tokens per Sec:     6500 || Lr: 0.000050
2024-02-08 16:13:04,496 Epoch 877: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.36 
2024-02-08 16:13:04,496 EPOCH 878
2024-02-08 16:13:07,811 [Epoch: 878 Step: 00058800] Batch Recognition Loss:   0.000531 => Gls Tokens per Sec:     1949 || Batch Translation Loss:   0.055441 => Txt Tokens per Sec:     5398 || Lr: 0.000050
2024-02-08 16:13:09,806 Epoch 878: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.77 
2024-02-08 16:13:09,806 EPOCH 879
2024-02-08 16:13:15,358 Epoch 879: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.83 
2024-02-08 16:13:15,358 EPOCH 880
2024-02-08 16:13:15,829 [Epoch: 880 Step: 00058900] Batch Recognition Loss:   0.000573 => Gls Tokens per Sec:     2383 || Batch Translation Loss:   0.055776 => Txt Tokens per Sec:     5989 || Lr: 0.000050
2024-02-08 16:13:20,968 Epoch 880: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.28 
2024-02-08 16:13:20,968 EPOCH 881
2024-02-08 16:13:24,192 [Epoch: 881 Step: 00059000] Batch Recognition Loss:   0.000460 => Gls Tokens per Sec:     1955 || Batch Translation Loss:   0.060109 => Txt Tokens per Sec:     5520 || Lr: 0.000050
2024-02-08 16:13:26,304 Epoch 881: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.91 
2024-02-08 16:13:26,304 EPOCH 882
2024-02-08 16:13:31,850 Epoch 882: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.25 
2024-02-08 16:13:31,850 EPOCH 883
2024-02-08 16:13:32,230 [Epoch: 883 Step: 00059100] Batch Recognition Loss:   0.000498 => Gls Tokens per Sec:     2533 || Batch Translation Loss:   0.036326 => Txt Tokens per Sec:     6551 || Lr: 0.000050
2024-02-08 16:13:37,187 Epoch 883: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.78 
2024-02-08 16:13:37,187 EPOCH 884
2024-02-08 16:13:40,264 [Epoch: 884 Step: 00059200] Batch Recognition Loss:   0.000257 => Gls Tokens per Sec:     2030 || Batch Translation Loss:   0.020010 => Txt Tokens per Sec:     5569 || Lr: 0.000050
2024-02-08 16:13:42,613 Epoch 884: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.65 
2024-02-08 16:13:42,613 EPOCH 885
2024-02-08 16:13:48,240 Epoch 885: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.82 
2024-02-08 16:13:48,241 EPOCH 886
2024-02-08 16:13:48,621 [Epoch: 886 Step: 00059300] Batch Recognition Loss:   0.000223 => Gls Tokens per Sec:     2111 || Batch Translation Loss:   0.028246 => Txt Tokens per Sec:     6092 || Lr: 0.000050
2024-02-08 16:13:53,656 Epoch 886: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.67 
2024-02-08 16:13:53,657 EPOCH 887
2024-02-08 16:13:56,725 [Epoch: 887 Step: 00059400] Batch Recognition Loss:   0.000370 => Gls Tokens per Sec:     1982 || Batch Translation Loss:   0.030813 => Txt Tokens per Sec:     5460 || Lr: 0.000050
2024-02-08 16:13:58,939 Epoch 887: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.28 
2024-02-08 16:13:58,940 EPOCH 888
2024-02-08 16:14:03,981 Epoch 888: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.00 
2024-02-08 16:14:03,982 EPOCH 889
2024-02-08 16:14:04,231 [Epoch: 889 Step: 00059500] Batch Recognition Loss:   0.000314 => Gls Tokens per Sec:     2581 || Batch Translation Loss:   0.037926 => Txt Tokens per Sec:     6351 || Lr: 0.000050
2024-02-08 16:14:09,543 Epoch 889: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.65 
2024-02-08 16:14:09,543 EPOCH 890
2024-02-08 16:14:12,457 [Epoch: 890 Step: 00059600] Batch Recognition Loss:   0.000297 => Gls Tokens per Sec:     1998 || Batch Translation Loss:   0.022566 => Txt Tokens per Sec:     5359 || Lr: 0.000050
2024-02-08 16:14:14,887 Epoch 890: Total Training Recognition Loss 0.09  Total Training Translation Loss 2.05 
2024-02-08 16:14:14,887 EPOCH 891
2024-02-08 16:14:20,473 Epoch 891: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.93 
2024-02-08 16:14:20,473 EPOCH 892
2024-02-08 16:14:20,750 [Epoch: 892 Step: 00059700] Batch Recognition Loss:   0.000528 => Gls Tokens per Sec:     1739 || Batch Translation Loss:   0.022719 => Txt Tokens per Sec:     5547 || Lr: 0.000050
2024-02-08 16:14:25,916 Epoch 892: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.96 
2024-02-08 16:14:25,917 EPOCH 893
2024-02-08 16:14:28,793 [Epoch: 893 Step: 00059800] Batch Recognition Loss:   0.000575 => Gls Tokens per Sec:     2003 || Batch Translation Loss:   0.016259 => Txt Tokens per Sec:     5678 || Lr: 0.000050
2024-02-08 16:14:31,224 Epoch 893: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.45 
2024-02-08 16:14:31,225 EPOCH 894
2024-02-08 16:14:36,842 Epoch 894: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.52 
2024-02-08 16:14:36,842 EPOCH 895
2024-02-08 16:14:36,963 [Epoch: 895 Step: 00059900] Batch Recognition Loss:   0.000358 => Gls Tokens per Sec:     2667 || Batch Translation Loss:   0.016499 => Txt Tokens per Sec:     7575 || Lr: 0.000050
2024-02-08 16:14:42,080 Epoch 895: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.55 
2024-02-08 16:14:42,081 EPOCH 896
2024-02-08 16:14:44,977 [Epoch: 896 Step: 00060000] Batch Recognition Loss:   0.000190 => Gls Tokens per Sec:     1900 || Batch Translation Loss:   0.023824 => Txt Tokens per Sec:     5203 || Lr: 0.000050
2024-02-08 16:14:53,542 Validation result at epoch 896, step    60000: duration: 8.5640s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.50044	Translation Loss: 92219.97656	PPL: 10006.04004
	Eval Metric: BLEU
	WER 3.67	(DEL: 0.00,	INS: 0.00,	SUB: 3.67)
	BLEU-4 0.75	(BLEU-1: 11.23,	BLEU-2: 3.53,	BLEU-3: 1.47,	BLEU-4: 0.75)
	CHRF 16.97	ROUGE 9.41
2024-02-08 16:14:53,543 Logging Recognition and Translation Outputs
2024-02-08 16:14:53,543 ========================================================================================================================
2024-02-08 16:14:53,544 Logging Sequence: 75_58.00
2024-02-08 16:14:53,544 	Gloss Reference :	A B+C+D+E
2024-02-08 16:14:53,544 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:14:53,544 	Gloss Alignment :	         
2024-02-08 16:14:53,544 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:14:53,546 	Text Reference  :	it seems like  he like to      date   women and does not want to get married people seem to respect his  choices
2024-02-08 16:14:53,546 	Text Hypothesis :	it is    known as the  british empire games and **** *** **** ** *** ******* ****** **** ** are     very well   
2024-02-08 16:14:53,546 	Text Alignment  :	   S     S     S  S    S       S      S         D    D   D    D  D   D       D      D    D  S       S    S      
2024-02-08 16:14:53,546 ========================================================================================================================
2024-02-08 16:14:53,546 Logging Sequence: 152_113.00
2024-02-08 16:14:53,546 	Gloss Reference :	A B+C+D+E
2024-02-08 16:14:53,547 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:14:53,547 	Gloss Alignment :	         
2024-02-08 16:14:53,547 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:14:53,548 	Text Reference  :	**** **** *** indians hoping for a     victory were  distraught at    the   defeat
2024-02-08 16:14:53,548 	Text Hypothesis :	what were his happy   by     the event was     glued to         their first time  
2024-02-08 16:14:53,548 	Text Alignment  :	I    I    I   S       S      S   S     S       S     S          S     S     S     
2024-02-08 16:14:53,548 ========================================================================================================================
2024-02-08 16:14:53,548 Logging Sequence: 176_41.00
2024-02-08 16:14:53,548 	Gloss Reference :	A B+C+D+E
2024-02-08 16:14:53,549 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:14:53,549 	Gloss Alignment :	         
2024-02-08 16:14:53,549 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:14:53,550 	Text Reference  :	dahiya did not loose  hope and put     up  a  strong fight
2024-02-08 16:14:53,550 	Text Hypothesis :	it     was a   strong bout and sanayev was in the    lead 
2024-02-08 16:14:53,550 	Text Alignment  :	S      S   S   S      S        S       S   S  S      S    
2024-02-08 16:14:53,550 ========================================================================================================================
2024-02-08 16:14:53,550 Logging Sequence: 77_190.00
2024-02-08 16:14:53,550 	Gloss Reference :	A B+C+D+E
2024-02-08 16:14:53,551 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:14:53,551 	Gloss Alignment :	         
2024-02-08 16:14:53,551 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:14:53,552 	Text Reference  :	there are many  batsmen who   have   scrored 36    runs   in     6           balls
2024-02-08 16:14:53,552 	Text Hypothesis :	on    the first 3       balls bowled by      patel jadeja scored consecutive sixes
2024-02-08 16:14:53,552 	Text Alignment  :	S     S   S     S       S     S      S       S     S      S      S           S    
2024-02-08 16:14:53,552 ========================================================================================================================
2024-02-08 16:14:53,552 Logging Sequence: 155_170.00
2024-02-08 16:14:53,552 	Gloss Reference :	A B+C+D+E
2024-02-08 16:14:53,552 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:14:53,553 	Gloss Alignment :	         
2024-02-08 16:14:53,553 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:14:53,553 	Text Reference  :	india lost the matches and could not secure a place in  the semi final  
2024-02-08 16:14:53,554 	Text Hypothesis :	we    hope the ******* *** ***** *** ****** * bcci  has won many wickets
2024-02-08 16:14:53,554 	Text Alignment  :	S     S        D       D   D     D   D      D S     S   S   S    S      
2024-02-08 16:14:53,554 ========================================================================================================================
2024-02-08 16:14:56,203 Epoch 896: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.00 
2024-02-08 16:14:56,203 EPOCH 897
2024-02-08 16:15:01,575 Epoch 897: Total Training Recognition Loss 0.06  Total Training Translation Loss 5.01 
2024-02-08 16:15:01,576 EPOCH 898
2024-02-08 16:15:01,630 [Epoch: 898 Step: 00060100] Batch Recognition Loss:   0.000205 => Gls Tokens per Sec:     3019 || Batch Translation Loss:   0.040746 => Txt Tokens per Sec:     6000 || Lr: 0.000050
2024-02-08 16:15:06,967 Epoch 898: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.63 
2024-02-08 16:15:06,967 EPOCH 899
2024-02-08 16:15:09,337 [Epoch: 899 Step: 00060200] Batch Recognition Loss:   0.000295 => Gls Tokens per Sec:     2296 || Batch Translation Loss:   0.024346 => Txt Tokens per Sec:     6372 || Lr: 0.000050
2024-02-08 16:15:12,147 Epoch 899: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.88 
2024-02-08 16:15:12,147 EPOCH 900
2024-02-08 16:15:17,568 [Epoch: 900 Step: 00060300] Batch Recognition Loss:   0.000249 => Gls Tokens per Sec:     1959 || Batch Translation Loss:   0.036691 => Txt Tokens per Sec:     5421 || Lr: 0.000050
2024-02-08 16:15:17,569 Epoch 900: Total Training Recognition Loss 0.09  Total Training Translation Loss 3.35 
2024-02-08 16:15:17,569 EPOCH 901
2024-02-08 16:15:22,921 Epoch 901: Total Training Recognition Loss 0.09  Total Training Translation Loss 2.36 
2024-02-08 16:15:22,922 EPOCH 902
2024-02-08 16:15:25,752 [Epoch: 902 Step: 00060400] Batch Recognition Loss:   0.000223 => Gls Tokens per Sec:     1867 || Batch Translation Loss:   0.028344 => Txt Tokens per Sec:     5181 || Lr: 0.000050
2024-02-08 16:15:28,394 Epoch 902: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.53 
2024-02-08 16:15:28,395 EPOCH 903
2024-02-08 16:15:33,586 [Epoch: 903 Step: 00060500] Batch Recognition Loss:   0.000122 => Gls Tokens per Sec:     2016 || Batch Translation Loss:   0.020511 => Txt Tokens per Sec:     5561 || Lr: 0.000050
2024-02-08 16:15:33,660 Epoch 903: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.32 
2024-02-08 16:15:33,660 EPOCH 904
2024-02-08 16:15:39,044 Epoch 904: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.92 
2024-02-08 16:15:39,044 EPOCH 905
2024-02-08 16:15:41,372 [Epoch: 905 Step: 00060600] Batch Recognition Loss:   0.000186 => Gls Tokens per Sec:     2157 || Batch Translation Loss:   0.014158 => Txt Tokens per Sec:     5887 || Lr: 0.000050
2024-02-08 16:15:44,376 Epoch 905: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.30 
2024-02-08 16:15:44,376 EPOCH 906
2024-02-08 16:15:49,745 [Epoch: 906 Step: 00060700] Batch Recognition Loss:   0.000148 => Gls Tokens per Sec:     1937 || Batch Translation Loss:   0.016838 => Txt Tokens per Sec:     5374 || Lr: 0.000050
2024-02-08 16:15:49,916 Epoch 906: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.79 
2024-02-08 16:15:49,916 EPOCH 907
2024-02-08 16:15:54,670 Epoch 907: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.36 
2024-02-08 16:15:54,670 EPOCH 908
2024-02-08 16:15:57,231 [Epoch: 908 Step: 00060800] Batch Recognition Loss:   0.000412 => Gls Tokens per Sec:     1899 || Batch Translation Loss:   0.086412 => Txt Tokens per Sec:     5257 || Lr: 0.000050
2024-02-08 16:16:00,153 Epoch 908: Total Training Recognition Loss 0.10  Total Training Translation Loss 2.75 
2024-02-08 16:16:00,154 EPOCH 909
2024-02-08 16:16:05,512 [Epoch: 909 Step: 00060900] Batch Recognition Loss:   0.000255 => Gls Tokens per Sec:     1893 || Batch Translation Loss:   0.023161 => Txt Tokens per Sec:     5250 || Lr: 0.000050
2024-02-08 16:16:05,688 Epoch 909: Total Training Recognition Loss 0.06  Total Training Translation Loss 5.06 
2024-02-08 16:16:05,688 EPOCH 910
2024-02-08 16:16:11,274 Epoch 910: Total Training Recognition Loss 0.07  Total Training Translation Loss 5.63 
2024-02-08 16:16:11,274 EPOCH 911
2024-02-08 16:16:13,407 [Epoch: 911 Step: 00061000] Batch Recognition Loss:   0.000705 => Gls Tokens per Sec:     2252 || Batch Translation Loss:   0.030518 => Txt Tokens per Sec:     6189 || Lr: 0.000050
2024-02-08 16:16:16,445 Epoch 911: Total Training Recognition Loss 0.10  Total Training Translation Loss 5.57 
2024-02-08 16:16:16,446 EPOCH 912
2024-02-08 16:16:21,630 [Epoch: 912 Step: 00061100] Batch Recognition Loss:   0.000389 => Gls Tokens per Sec:     1926 || Batch Translation Loss:   0.035729 => Txt Tokens per Sec:     5306 || Lr: 0.000050
2024-02-08 16:16:21,988 Epoch 912: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.54 
2024-02-08 16:16:21,988 EPOCH 913
2024-02-08 16:16:27,557 Epoch 913: Total Training Recognition Loss 0.09  Total Training Translation Loss 2.14 
2024-02-08 16:16:27,558 EPOCH 914
2024-02-08 16:16:29,806 [Epoch: 914 Step: 00061200] Batch Recognition Loss:   0.000323 => Gls Tokens per Sec:     2021 || Batch Translation Loss:   0.030322 => Txt Tokens per Sec:     5853 || Lr: 0.000050
2024-02-08 16:16:32,703 Epoch 914: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.30 
2024-02-08 16:16:32,703 EPOCH 915
2024-02-08 16:16:37,821 [Epoch: 915 Step: 00061300] Batch Recognition Loss:   0.002015 => Gls Tokens per Sec:     1919 || Batch Translation Loss:   0.026703 => Txt Tokens per Sec:     5351 || Lr: 0.000050
2024-02-08 16:16:38,216 Epoch 915: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.64 
2024-02-08 16:16:38,216 EPOCH 916
2024-02-08 16:16:43,182 Epoch 916: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.36 
2024-02-08 16:16:43,182 EPOCH 917
2024-02-08 16:16:45,028 [Epoch: 917 Step: 00061400] Batch Recognition Loss:   0.000162 => Gls Tokens per Sec:     2375 || Batch Translation Loss:   0.011530 => Txt Tokens per Sec:     6540 || Lr: 0.000050
2024-02-08 16:16:48,357 Epoch 917: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.02 
2024-02-08 16:16:48,357 EPOCH 918
2024-02-08 16:16:53,072 [Epoch: 918 Step: 00061500] Batch Recognition Loss:   0.001244 => Gls Tokens per Sec:     2049 || Batch Translation Loss:   0.010910 => Txt Tokens per Sec:     5660 || Lr: 0.000050
2024-02-08 16:16:53,506 Epoch 918: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.37 
2024-02-08 16:16:53,507 EPOCH 919
2024-02-08 16:16:59,060 Epoch 919: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.77 
2024-02-08 16:16:59,060 EPOCH 920
2024-02-08 16:17:01,254 [Epoch: 920 Step: 00061600] Batch Recognition Loss:   0.001200 => Gls Tokens per Sec:     1924 || Batch Translation Loss:   0.008406 => Txt Tokens per Sec:     5144 || Lr: 0.000050
2024-02-08 16:17:04,647 Epoch 920: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.26 
2024-02-08 16:17:04,648 EPOCH 921
2024-02-08 16:17:09,596 [Epoch: 921 Step: 00061700] Batch Recognition Loss:   0.001422 => Gls Tokens per Sec:     1920 || Batch Translation Loss:   0.012158 => Txt Tokens per Sec:     5324 || Lr: 0.000050
2024-02-08 16:17:10,099 Epoch 921: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.15 
2024-02-08 16:17:10,099 EPOCH 922
2024-02-08 16:17:15,215 Epoch 922: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.46 
2024-02-08 16:17:15,215 EPOCH 923
2024-02-08 16:17:17,321 [Epoch: 923 Step: 00061800] Batch Recognition Loss:   0.000228 => Gls Tokens per Sec:     1976 || Batch Translation Loss:   0.020204 => Txt Tokens per Sec:     5623 || Lr: 0.000050
2024-02-08 16:17:20,514 Epoch 923: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.36 
2024-02-08 16:17:20,515 EPOCH 924
2024-02-08 16:17:25,592 [Epoch: 924 Step: 00061900] Batch Recognition Loss:   0.000471 => Gls Tokens per Sec:     1840 || Batch Translation Loss:   0.026307 => Txt Tokens per Sec:     5098 || Lr: 0.000050
2024-02-08 16:17:26,209 Epoch 924: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.18 
2024-02-08 16:17:26,209 EPOCH 925
2024-02-08 16:17:31,788 Epoch 925: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.56 
2024-02-08 16:17:31,789 EPOCH 926
2024-02-08 16:17:33,431 [Epoch: 926 Step: 00062000] Batch Recognition Loss:   0.000233 => Gls Tokens per Sec:     2438 || Batch Translation Loss:   0.033400 => Txt Tokens per Sec:     6601 || Lr: 0.000050
2024-02-08 16:17:42,257 Validation result at epoch 926, step    62000: duration: 8.8246s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.71431	Translation Loss: 91838.18750	PPL: 9631.66602
	Eval Metric: BLEU
	WER 2.97	(DEL: 0.00,	INS: 0.00,	SUB: 2.97)
	BLEU-4 0.42	(BLEU-1: 9.89,	BLEU-2: 3.04,	BLEU-3: 1.02,	BLEU-4: 0.42)
	CHRF 16.81	ROUGE 8.14
2024-02-08 16:17:42,258 Logging Recognition and Translation Outputs
2024-02-08 16:17:42,258 ========================================================================================================================
2024-02-08 16:17:42,258 Logging Sequence: 165_523.00
2024-02-08 16:17:42,258 	Gloss Reference :	A B+C+D+E
2024-02-08 16:17:42,258 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:17:42,259 	Gloss Alignment :	         
2024-02-08 16:17:42,259 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:17:42,260 	Text Reference  :	as he believed that his team might lose  if he takes off   his    batting pads
2024-02-08 16:17:42,260 	Text Hypothesis :	** he came     out  as  a    huge  shock as he was   never remove the     pads
2024-02-08 16:17:42,260 	Text Alignment  :	D     S        S    S   S    S     S     S     S     S     S      S           
2024-02-08 16:17:42,260 ========================================================================================================================
2024-02-08 16:17:42,261 Logging Sequence: 165_233.00
2024-02-08 16:17:42,261 	Gloss Reference :	A B+C+D+E
2024-02-08 16:17:42,261 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:17:42,261 	Gloss Alignment :	         
2024-02-08 16:17:42,261 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:17:42,262 	Text Reference  :	irrespective of whether he was playing the match or not he always sat with his bag he   was happy when the  team    won
2024-02-08 16:17:42,262 	Text Hypothesis :	************ ** ******* ** *** ******* *** ***** ** *** ** ****** *** **** his *** team was ***** so   what happens ipl
2024-02-08 16:17:42,263 	Text Alignment  :	D            D  D       D  D   D       D   D     D  D   D  D      D   D        D   S        D     S    S    S       S  
2024-02-08 16:17:42,263 ========================================================================================================================
2024-02-08 16:17:42,263 Logging Sequence: 169_214.00
2024-02-08 16:17:42,263 	Gloss Reference :	A B+C+D+E
2024-02-08 16:17:42,263 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:17:42,263 	Gloss Alignment :	         
2024-02-08 16:17:42,263 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:17:42,265 	Text Reference  :	virat kohli said that though arshdeep dropped   the      catch       he is     still a strong part   of  the    indian     team
2024-02-08 16:17:42,265 	Text Hypothesis :	***** you   know that ****** ******** wikipedia provides information on celebs like  a ****** height age family background etc 
2024-02-08 16:17:42,265 	Text Alignment  :	D     S     S         D      D        S         S        S           S  S      S       D      S      S   S      S          S   
2024-02-08 16:17:42,266 ========================================================================================================================
2024-02-08 16:17:42,266 Logging Sequence: 88_67.00
2024-02-08 16:17:42,266 	Gloss Reference :	A B+C+D+E
2024-02-08 16:17:42,266 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:17:42,266 	Gloss Alignment :	         
2024-02-08 16:17:42,266 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:17:42,268 	Text Reference  :	* pablo    javkin the    mayor of      rosario is     also a  drug trafficker so    he     won't  take  care of     you      
2024-02-08 16:17:42,268 	Text Hypothesis :	1 france's kylian mbappe was   awarded the     golden boot as with 8          goals really police filed a    police complaint
2024-02-08 16:17:42,268 	Text Alignment  :	I S        S      S      S     S       S       S      S    S  S    S          S     S      S      S     S    S      S        
2024-02-08 16:17:42,269 ========================================================================================================================
2024-02-08 16:17:42,269 Logging Sequence: 69_95.00
2024-02-08 16:17:42,269 	Gloss Reference :	A B+C+D+E
2024-02-08 16:17:42,269 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:17:42,269 	Gloss Alignment :	         
2024-02-08 16:17:42,269 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:17:42,271 	Text Reference  :	**** **** **** a  six and   a  four sealed   csk's victory and **** ** ****** the team won   the match
2024-02-08 16:17:42,271 	Text Hypothesis :	fans said that he was tired by the  exacting game  and     and when he wanted was to   watch the match
2024-02-08 16:17:42,271 	Text Alignment  :	I    I    I    S  S   S     S  S    S        S     S           I    I  I      S   S    S              
2024-02-08 16:17:42,271 ========================================================================================================================
2024-02-08 16:17:46,033 Epoch 926: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.76 
2024-02-08 16:17:46,034 EPOCH 927
2024-02-08 16:17:50,629 [Epoch: 927 Step: 00062100] Batch Recognition Loss:   0.000140 => Gls Tokens per Sec:     1998 || Batch Translation Loss:   0.028949 => Txt Tokens per Sec:     5470 || Lr: 0.000050
2024-02-08 16:17:51,417 Epoch 927: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.30 
2024-02-08 16:17:51,417 EPOCH 928
2024-02-08 16:17:56,607 Epoch 928: Total Training Recognition Loss 0.07  Total Training Translation Loss 6.54 
2024-02-08 16:17:56,608 EPOCH 929
2024-02-08 16:17:58,662 [Epoch: 929 Step: 00062200] Batch Recognition Loss:   0.000511 => Gls Tokens per Sec:     1870 || Batch Translation Loss:   0.129724 => Txt Tokens per Sec:     5205 || Lr: 0.000050
2024-02-08 16:18:02,285 Epoch 929: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.96 
2024-02-08 16:18:02,285 EPOCH 930
2024-02-08 16:18:07,003 [Epoch: 930 Step: 00062300] Batch Recognition Loss:   0.000192 => Gls Tokens per Sec:     1913 || Batch Translation Loss:   0.016229 => Txt Tokens per Sec:     5282 || Lr: 0.000050
2024-02-08 16:18:07,861 Epoch 930: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.19 
2024-02-08 16:18:07,861 EPOCH 931
2024-02-08 16:18:13,173 Epoch 931: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.69 
2024-02-08 16:18:13,174 EPOCH 932
2024-02-08 16:18:14,939 [Epoch: 932 Step: 00062400] Batch Recognition Loss:   0.000311 => Gls Tokens per Sec:     2029 || Batch Translation Loss:   0.022258 => Txt Tokens per Sec:     5259 || Lr: 0.000050
2024-02-08 16:18:18,747 Epoch 932: Total Training Recognition Loss 0.09  Total Training Translation Loss 2.24 
2024-02-08 16:18:18,748 EPOCH 933
2024-02-08 16:18:23,243 [Epoch: 933 Step: 00062500] Batch Recognition Loss:   0.001201 => Gls Tokens per Sec:     1971 || Batch Translation Loss:   0.028067 => Txt Tokens per Sec:     5550 || Lr: 0.000050
2024-02-08 16:18:24,022 Epoch 933: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.18 
2024-02-08 16:18:24,022 EPOCH 934
2024-02-08 16:18:29,497 Epoch 934: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.73 
2024-02-08 16:18:29,498 EPOCH 935
2024-02-08 16:18:31,182 [Epoch: 935 Step: 00062600] Batch Recognition Loss:   0.000174 => Gls Tokens per Sec:     2032 || Batch Translation Loss:   0.012377 => Txt Tokens per Sec:     5438 || Lr: 0.000050
2024-02-08 16:18:34,428 Epoch 935: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.81 
2024-02-08 16:18:34,428 EPOCH 936
2024-02-08 16:18:38,817 [Epoch: 936 Step: 00062700] Batch Recognition Loss:   0.000245 => Gls Tokens per Sec:     1983 || Batch Translation Loss:   0.024145 => Txt Tokens per Sec:     5429 || Lr: 0.000050
2024-02-08 16:18:39,861 Epoch 936: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.85 
2024-02-08 16:18:39,861 EPOCH 937
2024-02-08 16:18:45,428 Epoch 937: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.62 
2024-02-08 16:18:45,429 EPOCH 938
2024-02-08 16:18:46,877 [Epoch: 938 Step: 00062800] Batch Recognition Loss:   0.000250 => Gls Tokens per Sec:     2322 || Batch Translation Loss:   0.096546 => Txt Tokens per Sec:     6475 || Lr: 0.000050
2024-02-08 16:18:50,092 Epoch 938: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.38 
2024-02-08 16:18:50,092 EPOCH 939
2024-02-08 16:18:54,700 [Epoch: 939 Step: 00062900] Batch Recognition Loss:   0.000686 => Gls Tokens per Sec:     1854 || Batch Translation Loss:   0.005865 => Txt Tokens per Sec:     5154 || Lr: 0.000050
2024-02-08 16:18:55,704 Epoch 939: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.57 
2024-02-08 16:18:55,704 EPOCH 940
2024-02-08 16:19:00,461 Epoch 940: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.27 
2024-02-08 16:19:00,461 EPOCH 941
2024-02-08 16:19:02,281 [Epoch: 941 Step: 00063000] Batch Recognition Loss:   0.001228 => Gls Tokens per Sec:     1704 || Batch Translation Loss:   0.007507 => Txt Tokens per Sec:     4700 || Lr: 0.000050
2024-02-08 16:19:06,075 Epoch 941: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.67 
2024-02-08 16:19:06,075 EPOCH 942
2024-02-08 16:19:09,969 [Epoch: 942 Step: 00063100] Batch Recognition Loss:   0.000141 => Gls Tokens per Sec:     2153 || Batch Translation Loss:   0.013491 => Txt Tokens per Sec:     6040 || Lr: 0.000050
2024-02-08 16:19:10,874 Epoch 942: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.80 
2024-02-08 16:19:10,874 EPOCH 943
2024-02-08 16:19:16,384 Epoch 943: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.69 
2024-02-08 16:19:16,384 EPOCH 944
2024-02-08 16:19:17,802 [Epoch: 944 Step: 00063200] Batch Recognition Loss:   0.000255 => Gls Tokens per Sec:     2145 || Batch Translation Loss:   0.718703 => Txt Tokens per Sec:     6232 || Lr: 0.000050
2024-02-08 16:19:21,299 Epoch 944: Total Training Recognition Loss 0.05  Total Training Translation Loss 4.29 
2024-02-08 16:19:21,299 EPOCH 945
2024-02-08 16:19:25,055 [Epoch: 945 Step: 00063300] Batch Recognition Loss:   0.000203 => Gls Tokens per Sec:     2189 || Batch Translation Loss:   0.017885 => Txt Tokens per Sec:     6024 || Lr: 0.000050
2024-02-08 16:19:26,086 Epoch 945: Total Training Recognition Loss 0.11  Total Training Translation Loss 4.83 
2024-02-08 16:19:26,086 EPOCH 946
2024-02-08 16:19:31,470 Epoch 946: Total Training Recognition Loss 0.11  Total Training Translation Loss 3.18 
2024-02-08 16:19:31,471 EPOCH 947
2024-02-08 16:19:32,877 [Epoch: 947 Step: 00063400] Batch Recognition Loss:   0.000268 => Gls Tokens per Sec:     1979 || Batch Translation Loss:   0.024581 => Txt Tokens per Sec:     5409 || Lr: 0.000050
2024-02-08 16:19:36,616 Epoch 947: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.84 
2024-02-08 16:19:36,616 EPOCH 948
2024-02-08 16:19:40,541 [Epoch: 948 Step: 00063500] Batch Recognition Loss:   0.000532 => Gls Tokens per Sec:     2080 || Batch Translation Loss:   0.031539 => Txt Tokens per Sec:     5747 || Lr: 0.000050
2024-02-08 16:19:41,931 Epoch 948: Total Training Recognition Loss 0.09  Total Training Translation Loss 3.12 
2024-02-08 16:19:41,931 EPOCH 949
2024-02-08 16:19:47,130 Epoch 949: Total Training Recognition Loss 0.06  Total Training Translation Loss 4.11 
2024-02-08 16:19:47,130 EPOCH 950
2024-02-08 16:19:48,422 [Epoch: 950 Step: 00063600] Batch Recognition Loss:   0.001018 => Gls Tokens per Sec:     2029 || Batch Translation Loss:   0.032489 => Txt Tokens per Sec:     5863 || Lr: 0.000050
2024-02-08 16:19:52,625 Epoch 950: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.93 
2024-02-08 16:19:52,625 EPOCH 951
2024-02-08 16:19:56,491 [Epoch: 951 Step: 00063700] Batch Recognition Loss:   0.007184 => Gls Tokens per Sec:     2044 || Batch Translation Loss:   0.034923 => Txt Tokens per Sec:     5719 || Lr: 0.000050
2024-02-08 16:19:57,713 Epoch 951: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.29 
2024-02-08 16:19:57,713 EPOCH 952
2024-02-08 16:20:03,137 Epoch 952: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.84 
2024-02-08 16:20:03,137 EPOCH 953
2024-02-08 16:20:04,207 [Epoch: 953 Step: 00063800] Batch Recognition Loss:   0.000346 => Gls Tokens per Sec:     2395 || Batch Translation Loss:   0.030073 => Txt Tokens per Sec:     6521 || Lr: 0.000050
2024-02-08 16:20:07,984 Epoch 953: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.41 
2024-02-08 16:20:07,984 EPOCH 954
2024-02-08 16:20:12,166 [Epoch: 954 Step: 00063900] Batch Recognition Loss:   0.000168 => Gls Tokens per Sec:     1852 || Batch Translation Loss:   0.020506 => Txt Tokens per Sec:     5203 || Lr: 0.000050
2024-02-08 16:20:13,566 Epoch 954: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.33 
2024-02-08 16:20:13,567 EPOCH 955
2024-02-08 16:20:18,546 Epoch 955: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.88 
2024-02-08 16:20:18,546 EPOCH 956
2024-02-08 16:20:19,719 [Epoch: 956 Step: 00064000] Batch Recognition Loss:   0.000211 => Gls Tokens per Sec:     2050 || Batch Translation Loss:   0.018582 => Txt Tokens per Sec:     5767 || Lr: 0.000050
2024-02-08 16:20:28,232 Validation result at epoch 956, step    64000: duration: 8.5125s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.58785	Translation Loss: 92505.66406	PPL: 10295.67383
	Eval Metric: BLEU
	WER 3.32	(DEL: 0.00,	INS: 0.00,	SUB: 3.32)
	BLEU-4 0.39	(BLEU-1: 10.04,	BLEU-2: 2.76,	BLEU-3: 0.92,	BLEU-4: 0.39)
	CHRF 16.55	ROUGE 8.46
2024-02-08 16:20:28,233 Logging Recognition and Translation Outputs
2024-02-08 16:20:28,233 ========================================================================================================================
2024-02-08 16:20:28,233 Logging Sequence: 122_86.00
2024-02-08 16:20:28,233 	Gloss Reference :	A B+C+D+E
2024-02-08 16:20:28,233 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:20:28,234 	Gloss Alignment :	         
2024-02-08 16:20:28,234 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:20:28,235 	Text Reference  :	after   winning chanu spoke   to     the ***** *** **** media and said   
2024-02-08 16:20:28,235 	Text Hypothesis :	however a       few   minutes before the score was held in    the stadium
2024-02-08 16:20:28,235 	Text Alignment  :	S       S       S     S       S          I     I   I    S     S   S      
2024-02-08 16:20:28,235 ========================================================================================================================
2024-02-08 16:20:28,235 Logging Sequence: 82_81.00
2024-02-08 16:20:28,235 	Gloss Reference :	A B+C+D+E
2024-02-08 16:20:28,235 	Gloss Hypothesis:	A B+C+D  
2024-02-08 16:20:28,235 	Gloss Alignment :	  S      
2024-02-08 16:20:28,236 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:20:28,237 	Text Reference  :	since the couple were residents of **** mumbai ******* **** the     mumbai police  cyber cell     began investigating the   matter 
2024-02-08 16:20:28,237 	Text Hypothesis :	***** *** ****** **** then      of this mumbai indians team doctors nurses physios and   everyone else  for           their support
2024-02-08 16:20:28,237 	Text Alignment  :	D     D   D      D    S            I           I       I    S       S      S       S     S        S     S             S     S      
2024-02-08 16:20:28,237 ========================================================================================================================
2024-02-08 16:20:28,237 Logging Sequence: 61_65.00
2024-02-08 16:20:28,238 	Gloss Reference :	A B+C+D+E
2024-02-08 16:20:28,238 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:20:28,238 	Gloss Alignment :	         
2024-02-08 16:20:28,238 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:20:28,239 	Text Reference  :	the name seems indian but  whether it  has been made by   an      indian
2024-02-08 16:20:28,239 	Text Hypothesis :	*** **** ***** ****** that they    did not want to   play against chahal
2024-02-08 16:20:28,239 	Text Alignment  :	D   D    D     D      S    S       S   S   S    S    S    S       S     
2024-02-08 16:20:28,239 ========================================================================================================================
2024-02-08 16:20:28,239 Logging Sequence: 179_126.00
2024-02-08 16:20:28,239 	Gloss Reference :	A B+C+D+E
2024-02-08 16:20:28,240 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:20:28,240 	Gloss Alignment :	         
2024-02-08 16:20:28,240 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:20:28,242 	Text Reference  :	*** ********** ** vinesh argued that she might contract coronavirus since these wrestlers travelled from    india where there are many infections
2024-02-08 16:20:28,242 	Text Hypothesis :	the federation or sai    people want to  pick  their    office      at    par   with      the       airport on    the   day   of  her  win       
2024-02-08 16:20:28,242 	Text Alignment  :	I   I          I  S      S      S    S   S     S        S           S     S     S         S         S       S     S     S     S   S    S         
2024-02-08 16:20:28,242 ========================================================================================================================
2024-02-08 16:20:28,243 Logging Sequence: 62_24.00
2024-02-08 16:20:28,243 	Gloss Reference :	A B+C+D+E
2024-02-08 16:20:28,243 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:20:28,243 	Gloss Alignment :	         
2024-02-08 16:20:28,243 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:20:28,245 	Text Reference  :	now the women's cricket team too   is giving splendid performances which are  at       par with    the men's team   
2024-02-08 16:20:28,245 	Text Hypothesis :	*** do  you     know    that daley is ****** ******** ************ a     huge argument on  england in  uttar pradesh
2024-02-08 16:20:28,245 	Text Alignment  :	D   S   S       S       S    S        D      D        D            S     S    S        S   S       S   S     S      
2024-02-08 16:20:28,245 ========================================================================================================================
2024-02-08 16:20:32,547 Epoch 956: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.38 
2024-02-08 16:20:32,547 EPOCH 957
2024-02-08 16:20:36,264 [Epoch: 957 Step: 00064100] Batch Recognition Loss:   0.001583 => Gls Tokens per Sec:     2040 || Batch Translation Loss:   0.053513 => Txt Tokens per Sec:     5755 || Lr: 0.000050
2024-02-08 16:20:37,783 Epoch 957: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.30 
2024-02-08 16:20:37,784 EPOCH 958
2024-02-08 16:20:43,344 Epoch 958: Total Training Recognition Loss 0.06  Total Training Translation Loss 4.75 
2024-02-08 16:20:43,344 EPOCH 959
2024-02-08 16:20:44,464 [Epoch: 959 Step: 00064200] Batch Recognition Loss:   0.000331 => Gls Tokens per Sec:     2002 || Batch Translation Loss:   0.041815 => Txt Tokens per Sec:     5752 || Lr: 0.000050
2024-02-08 16:20:48,808 Epoch 959: Total Training Recognition Loss 0.08  Total Training Translation Loss 5.55 
2024-02-08 16:20:48,809 EPOCH 960
2024-02-08 16:20:52,861 [Epoch: 960 Step: 00064300] Batch Recognition Loss:   0.000373 => Gls Tokens per Sec:     1832 || Batch Translation Loss:   0.037253 => Txt Tokens per Sec:     5237 || Lr: 0.000050
2024-02-08 16:20:54,193 Epoch 960: Total Training Recognition Loss 0.08  Total Training Translation Loss 3.18 
2024-02-08 16:20:54,193 EPOCH 961
2024-02-08 16:20:59,377 Epoch 961: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.06 
2024-02-08 16:20:59,378 EPOCH 962
2024-02-08 16:21:00,235 [Epoch: 962 Step: 00064400] Batch Recognition Loss:   0.000354 => Gls Tokens per Sec:     2430 || Batch Translation Loss:   0.019414 => Txt Tokens per Sec:     6298 || Lr: 0.000050
2024-02-08 16:21:04,794 Epoch 962: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.78 
2024-02-08 16:21:04,795 EPOCH 963
2024-02-08 16:21:08,031 [Epoch: 963 Step: 00064500] Batch Recognition Loss:   0.000803 => Gls Tokens per Sec:     2275 || Batch Translation Loss:   0.006027 => Txt Tokens per Sec:     6234 || Lr: 0.000050
2024-02-08 16:21:09,584 Epoch 963: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.60 
2024-02-08 16:21:09,585 EPOCH 964
2024-02-08 16:21:14,572 Epoch 964: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.36 
2024-02-08 16:21:14,572 EPOCH 965
2024-02-08 16:21:15,365 [Epoch: 965 Step: 00064600] Batch Recognition Loss:   0.000200 => Gls Tokens per Sec:     2424 || Batch Translation Loss:   0.015187 => Txt Tokens per Sec:     6112 || Lr: 0.000050
2024-02-08 16:21:20,055 Epoch 965: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.66 
2024-02-08 16:21:20,055 EPOCH 966
2024-02-08 16:21:23,327 [Epoch: 966 Step: 00064700] Batch Recognition Loss:   0.000594 => Gls Tokens per Sec:     2171 || Batch Translation Loss:   0.011511 => Txt Tokens per Sec:     5747 || Lr: 0.000050
2024-02-08 16:21:25,354 Epoch 966: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.24 
2024-02-08 16:21:25,354 EPOCH 967
2024-02-08 16:21:30,854 Epoch 967: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.19 
2024-02-08 16:21:30,855 EPOCH 968
2024-02-08 16:21:31,608 [Epoch: 968 Step: 00064800] Batch Recognition Loss:   0.000424 => Gls Tokens per Sec:     2207 || Batch Translation Loss:   0.027373 => Txt Tokens per Sec:     5879 || Lr: 0.000050
2024-02-08 16:21:36,138 Epoch 968: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.48 
2024-02-08 16:21:36,139 EPOCH 969
2024-02-08 16:21:39,942 [Epoch: 969 Step: 00064900] Batch Recognition Loss:   0.000160 => Gls Tokens per Sec:     1825 || Batch Translation Loss:   0.014935 => Txt Tokens per Sec:     5096 || Lr: 0.000050
2024-02-08 16:21:41,592 Epoch 969: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.59 
2024-02-08 16:21:41,593 EPOCH 970
2024-02-08 16:21:46,756 Epoch 970: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.19 
2024-02-08 16:21:46,756 EPOCH 971
2024-02-08 16:21:47,442 [Epoch: 971 Step: 00065000] Batch Recognition Loss:   0.000296 => Gls Tokens per Sec:     2336 || Batch Translation Loss:   0.024323 => Txt Tokens per Sec:     6000 || Lr: 0.000050
2024-02-08 16:21:52,282 Epoch 971: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.21 
2024-02-08 16:21:52,282 EPOCH 972
2024-02-08 16:21:55,661 [Epoch: 972 Step: 00065100] Batch Recognition Loss:   0.000144 => Gls Tokens per Sec:     2037 || Batch Translation Loss:   0.036497 => Txt Tokens per Sec:     5693 || Lr: 0.000050
2024-02-08 16:21:57,619 Epoch 972: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.07 
2024-02-08 16:21:57,620 EPOCH 973
2024-02-08 16:22:03,159 Epoch 973: Total Training Recognition Loss 0.04  Total Training Translation Loss 4.24 
2024-02-08 16:22:03,160 EPOCH 974
2024-02-08 16:22:03,750 [Epoch: 974 Step: 00065200] Batch Recognition Loss:   0.000212 => Gls Tokens per Sec:     2445 || Batch Translation Loss:   0.070345 => Txt Tokens per Sec:     6503 || Lr: 0.000050
2024-02-08 16:22:08,585 Epoch 974: Total Training Recognition Loss 0.04  Total Training Translation Loss 4.33 
2024-02-08 16:22:08,586 EPOCH 975
2024-02-08 16:22:11,957 [Epoch: 975 Step: 00065300] Batch Recognition Loss:   0.000204 => Gls Tokens per Sec:     1994 || Batch Translation Loss:   0.054273 => Txt Tokens per Sec:     5613 || Lr: 0.000050
2024-02-08 16:22:13,877 Epoch 975: Total Training Recognition Loss 0.06  Total Training Translation Loss 4.50 
2024-02-08 16:22:13,877 EPOCH 976
2024-02-08 16:22:19,491 Epoch 976: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.16 
2024-02-08 16:22:19,492 EPOCH 977
2024-02-08 16:22:20,108 [Epoch: 977 Step: 00065400] Batch Recognition Loss:   0.000888 => Gls Tokens per Sec:     2081 || Batch Translation Loss:   0.020778 => Txt Tokens per Sec:     6111 || Lr: 0.000050
2024-02-08 16:22:24,769 Epoch 977: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.14 
2024-02-08 16:22:24,770 EPOCH 978
2024-02-08 16:22:27,895 [Epoch: 978 Step: 00065500] Batch Recognition Loss:   0.000287 => Gls Tokens per Sec:     2068 || Batch Translation Loss:   0.016573 => Txt Tokens per Sec:     5559 || Lr: 0.000050
2024-02-08 16:22:30,405 Epoch 978: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.36 
2024-02-08 16:22:30,405 EPOCH 979
2024-02-08 16:22:35,653 Epoch 979: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.98 
2024-02-08 16:22:35,653 EPOCH 980
2024-02-08 16:22:36,233 [Epoch: 980 Step: 00065600] Batch Recognition Loss:   0.001204 => Gls Tokens per Sec:     1934 || Batch Translation Loss:   0.030426 => Txt Tokens per Sec:     5537 || Lr: 0.000050
2024-02-08 16:22:41,224 Epoch 980: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.52 
2024-02-08 16:22:41,224 EPOCH 981
2024-02-08 16:22:44,487 [Epoch: 981 Step: 00065700] Batch Recognition Loss:   0.000538 => Gls Tokens per Sec:     1932 || Batch Translation Loss:   0.184508 => Txt Tokens per Sec:     5291 || Lr: 0.000050
2024-02-08 16:22:46,801 Epoch 981: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.67 
2024-02-08 16:22:46,801 EPOCH 982
2024-02-08 16:22:52,019 Epoch 982: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.95 
2024-02-08 16:22:52,020 EPOCH 983
2024-02-08 16:22:52,399 [Epoch: 983 Step: 00065800] Batch Recognition Loss:   0.000212 => Gls Tokens per Sec:     2540 || Batch Translation Loss:   0.008408 => Txt Tokens per Sec:     6725 || Lr: 0.000050
2024-02-08 16:22:57,373 Epoch 983: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.39 
2024-02-08 16:22:57,374 EPOCH 984
2024-02-08 16:23:00,745 [Epoch: 984 Step: 00065900] Batch Recognition Loss:   0.000344 => Gls Tokens per Sec:     1852 || Batch Translation Loss:   0.020048 => Txt Tokens per Sec:     5241 || Lr: 0.000050
2024-02-08 16:23:02,903 Epoch 984: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.57 
2024-02-08 16:23:02,903 EPOCH 985
2024-02-08 16:23:08,444 Epoch 985: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.54 
2024-02-08 16:23:08,444 EPOCH 986
2024-02-08 16:23:08,839 [Epoch: 986 Step: 00066000] Batch Recognition Loss:   0.000370 => Gls Tokens per Sec:     2030 || Batch Translation Loss:   0.028686 => Txt Tokens per Sec:     6000 || Lr: 0.000050
2024-02-08 16:23:17,144 Validation result at epoch 986, step    66000: duration: 8.3037s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.54453	Translation Loss: 92882.28125	PPL: 10690.32910
	Eval Metric: BLEU
	WER 3.60	(DEL: 0.00,	INS: 0.00,	SUB: 3.60)
	BLEU-4 0.46	(BLEU-1: 9.76,	BLEU-2: 3.00,	BLEU-3: 1.01,	BLEU-4: 0.46)
	CHRF 16.31	ROUGE 8.64
2024-02-08 16:23:17,145 Logging Recognition and Translation Outputs
2024-02-08 16:23:17,146 ========================================================================================================================
2024-02-08 16:23:17,146 Logging Sequence: 85_97.00
2024-02-08 16:23:17,146 	Gloss Reference :	A B+C+D+E
2024-02-08 16:23:17,146 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:23:17,146 	Gloss Alignment :	         
2024-02-08 16:23:17,146 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:23:17,147 	Text Reference  :	** *** ****** like    india's ******* bcci        australia has cricket australia
2024-02-08 16:23:17,147 	Text Hypothesis :	in his people perform india's amazing performance in        the indian  team     
2024-02-08 16:23:17,147 	Text Alignment  :	I  I   I      S               I       S           S         S   S       S        
2024-02-08 16:23:17,147 ========================================================================================================================
2024-02-08 16:23:17,147 Logging Sequence: 53_161.00
2024-02-08 16:23:17,148 	Gloss Reference :	A B+C+D+E
2024-02-08 16:23:17,148 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:23:17,148 	Gloss Alignment :	         
2024-02-08 16:23:17,148 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:23:17,149 	Text Reference  :	** rashid has also been urging people to donate      to his rashid khan   foundation and afghanistan cricket association
2024-02-08 16:23:17,150 	Text Hypothesis :	he too    has **** **** ****** ****** ** afghanistan to win the    silver medal      for rs          32      crore      
2024-02-08 16:23:17,150 	Text Alignment  :	I  S          D    D    D      D      D  S              S   S      S      S          S   S           S       S          
2024-02-08 16:23:17,150 ========================================================================================================================
2024-02-08 16:23:17,150 Logging Sequence: 101_97.00
2024-02-08 16:23:17,150 	Gloss Reference :	A B+C+D+E
2024-02-08 16:23:17,150 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:23:17,151 	Gloss Alignment :	         
2024-02-08 16:23:17,151 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:23:17,153 	Text Reference  :	2 of the  best   indian bowlers who   bowled  very well were raj bawa who took 5 wickets and ravi kumar who     took 4   wickets
2024-02-08 16:23:17,153 	Text Hypothesis :	* ** when sharma was    batting india playing very well **** *** **** *** **** * ******* and **** ***** england lost the team   
2024-02-08 16:23:17,153 	Text Alignment  :	D D  S    S      S      S       S     S                 D    D   D    D   D    D D           D    D     S       S    S   S      
2024-02-08 16:23:17,153 ========================================================================================================================
2024-02-08 16:23:17,153 Logging Sequence: 118_130.00
2024-02-08 16:23:17,153 	Gloss Reference :	A B+C+D+E
2024-02-08 16:23:17,153 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:23:17,154 	Gloss Alignment :	         
2024-02-08 16:23:17,154 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:23:17,154 	Text Reference  :	messi's fans the entire team were in tears everyone was overwhelmed by       the    victory
2024-02-08 16:23:17,154 	Text Hypothesis :	******* **** *** ****** **** **** ** ***** this     was krunal      pandya's maiden odi    
2024-02-08 16:23:17,155 	Text Alignment  :	D       D    D   D      D    D    D  D     S            S           S        S      S      
2024-02-08 16:23:17,155 ========================================================================================================================
2024-02-08 16:23:17,155 Logging Sequence: 170_195.00
2024-02-08 16:23:17,155 	Gloss Reference :	A B+C+D+E
2024-02-08 16:23:17,155 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:23:17,155 	Gloss Alignment :	         
2024-02-08 16:23:17,155 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:23:17,156 	Text Reference  :	i moved to      rajasthan royals team  as    they  paid me  8      crores
2024-02-08 16:23:17,156 	Text Hypothesis :	* as    neither of        the    teams could score in   the league ipl   
2024-02-08 16:23:17,156 	Text Alignment  :	D S     S       S         S      S     S     S     S    S   S      S     
2024-02-08 16:23:17,157 ========================================================================================================================
2024-02-08 16:23:22,218 Epoch 986: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.57 
2024-02-08 16:23:22,218 EPOCH 987
2024-02-08 16:23:25,241 [Epoch: 987 Step: 00066100] Batch Recognition Loss:   0.002493 => Gls Tokens per Sec:     2013 || Batch Translation Loss:   0.011225 => Txt Tokens per Sec:     5555 || Lr: 0.000050
2024-02-08 16:23:27,764 Epoch 987: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.15 
2024-02-08 16:23:27,764 EPOCH 988
2024-02-08 16:23:33,163 Epoch 988: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.17 
2024-02-08 16:23:33,164 EPOCH 989
2024-02-08 16:23:33,477 [Epoch: 989 Step: 00066200] Batch Recognition Loss:   0.000458 => Gls Tokens per Sec:     2051 || Batch Translation Loss:   0.038384 => Txt Tokens per Sec:     5933 || Lr: 0.000050
2024-02-08 16:23:38,551 Epoch 989: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.86 
2024-02-08 16:23:38,552 EPOCH 990
2024-02-08 16:23:41,491 [Epoch: 990 Step: 00066300] Batch Recognition Loss:   0.000209 => Gls Tokens per Sec:     2016 || Batch Translation Loss:   0.011388 => Txt Tokens per Sec:     5442 || Lr: 0.000050
2024-02-08 16:23:44,089 Epoch 990: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.37 
2024-02-08 16:23:44,090 EPOCH 991
2024-02-08 16:23:49,442 Epoch 991: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.42 
2024-02-08 16:23:49,442 EPOCH 992
2024-02-08 16:23:49,689 [Epoch: 992 Step: 00066400] Batch Recognition Loss:   0.001623 => Gls Tokens per Sec:     1951 || Batch Translation Loss:   0.021528 => Txt Tokens per Sec:     5923 || Lr: 0.000050
2024-02-08 16:23:54,795 Epoch 992: Total Training Recognition Loss 0.06  Total Training Translation Loss 4.46 
2024-02-08 16:23:54,795 EPOCH 993
2024-02-08 16:23:57,717 [Epoch: 993 Step: 00066500] Batch Recognition Loss:   0.000513 => Gls Tokens per Sec:     1972 || Batch Translation Loss:   0.025528 => Txt Tokens per Sec:     5434 || Lr: 0.000050
2024-02-08 16:24:00,361 Epoch 993: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.51 
2024-02-08 16:24:00,361 EPOCH 994
2024-02-08 16:24:05,170 Epoch 994: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.20 
2024-02-08 16:24:05,170 EPOCH 995
2024-02-08 16:24:05,326 [Epoch: 995 Step: 00066600] Batch Recognition Loss:   0.000142 => Gls Tokens per Sec:     2078 || Batch Translation Loss:   0.018852 => Txt Tokens per Sec:     5526 || Lr: 0.000050
2024-02-08 16:24:10,649 Epoch 995: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.17 
2024-02-08 16:24:10,650 EPOCH 996
2024-02-08 16:24:13,009 [Epoch: 996 Step: 00066700] Batch Recognition Loss:   0.000348 => Gls Tokens per Sec:     2375 || Batch Translation Loss:   0.028568 => Txt Tokens per Sec:     6352 || Lr: 0.000050
2024-02-08 16:24:15,475 Epoch 996: Total Training Recognition Loss 0.09  Total Training Translation Loss 2.19 
2024-02-08 16:24:15,476 EPOCH 997
2024-02-08 16:24:20,958 Epoch 997: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.62 
2024-02-08 16:24:20,958 EPOCH 998
2024-02-08 16:24:21,045 [Epoch: 998 Step: 00066800] Batch Recognition Loss:   0.000364 => Gls Tokens per Sec:     1882 || Batch Translation Loss:   0.022310 => Txt Tokens per Sec:     5376 || Lr: 0.000050
2024-02-08 16:24:26,051 Epoch 998: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.33 
2024-02-08 16:24:26,051 EPOCH 999
2024-02-08 16:24:29,049 [Epoch: 999 Step: 00066900] Batch Recognition Loss:   0.000289 => Gls Tokens per Sec:     1815 || Batch Translation Loss:   0.015431 => Txt Tokens per Sec:     5212 || Lr: 0.000050
2024-02-08 16:24:31,600 Epoch 999: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.59 
2024-02-08 16:24:31,600 EPOCH 1000
2024-02-08 16:24:36,513 [Epoch: 1000 Step: 00067000] Batch Recognition Loss:   0.000180 => Gls Tokens per Sec:     2162 || Batch Translation Loss:   0.019704 => Txt Tokens per Sec:     5981 || Lr: 0.000050
2024-02-08 16:24:36,513 Epoch 1000: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.33 
2024-02-08 16:24:36,513 EPOCH 1001
2024-02-08 16:24:41,761 Epoch 1001: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.87 
2024-02-08 16:24:41,762 EPOCH 1002
2024-02-08 16:24:44,300 [Epoch: 1002 Step: 00067100] Batch Recognition Loss:   0.000183 => Gls Tokens per Sec:     2081 || Batch Translation Loss:   0.044806 => Txt Tokens per Sec:     5784 || Lr: 0.000050
2024-02-08 16:24:47,063 Epoch 1002: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.30 
2024-02-08 16:24:47,064 EPOCH 1003
2024-02-08 16:24:52,589 [Epoch: 1003 Step: 00067200] Batch Recognition Loss:   0.000341 => Gls Tokens per Sec:     1894 || Batch Translation Loss:   0.018359 => Txt Tokens per Sec:     5247 || Lr: 0.000050
2024-02-08 16:24:52,651 Epoch 1003: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.76 
2024-02-08 16:24:52,651 EPOCH 1004
2024-02-08 16:24:57,961 Epoch 1004: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.75 
2024-02-08 16:24:57,962 EPOCH 1005
2024-02-08 16:25:00,675 [Epoch: 1005 Step: 00067300] Batch Recognition Loss:   0.000280 => Gls Tokens per Sec:     1852 || Batch Translation Loss:   0.015917 => Txt Tokens per Sec:     5236 || Lr: 0.000050
2024-02-08 16:25:03,386 Epoch 1005: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.37 
2024-02-08 16:25:03,387 EPOCH 1006
2024-02-08 16:25:08,345 [Epoch: 1006 Step: 00067400] Batch Recognition Loss:   0.000156 => Gls Tokens per Sec:     2077 || Batch Translation Loss:   0.020915 => Txt Tokens per Sec:     5781 || Lr: 0.000050
2024-02-08 16:25:08,491 Epoch 1006: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.02 
2024-02-08 16:25:08,491 EPOCH 1007
2024-02-08 16:25:14,310 Epoch 1007: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.49 
2024-02-08 16:25:14,311 EPOCH 1008
2024-02-08 16:25:16,604 [Epoch: 1008 Step: 00067500] Batch Recognition Loss:   0.000242 => Gls Tokens per Sec:     2120 || Batch Translation Loss:   0.021293 => Txt Tokens per Sec:     5819 || Lr: 0.000050
2024-02-08 16:25:19,291 Epoch 1008: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.14 
2024-02-08 16:25:19,292 EPOCH 1009
2024-02-08 16:25:24,297 [Epoch: 1009 Step: 00067600] Batch Recognition Loss:   0.000180 => Gls Tokens per Sec:     2026 || Batch Translation Loss:   0.019878 => Txt Tokens per Sec:     5656 || Lr: 0.000050
2024-02-08 16:25:24,478 Epoch 1009: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.69 
2024-02-08 16:25:24,478 EPOCH 1010
2024-02-08 16:25:29,848 Epoch 1010: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.33 
2024-02-08 16:25:29,849 EPOCH 1011
2024-02-08 16:25:31,964 [Epoch: 1011 Step: 00067700] Batch Recognition Loss:   0.000264 => Gls Tokens per Sec:     2223 || Batch Translation Loss:   0.014458 => Txt Tokens per Sec:     6072 || Lr: 0.000050
2024-02-08 16:25:35,231 Epoch 1011: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.14 
2024-02-08 16:25:35,231 EPOCH 1012
2024-02-08 16:25:40,273 [Epoch: 1012 Step: 00067800] Batch Recognition Loss:   0.000259 => Gls Tokens per Sec:     1980 || Batch Translation Loss:   0.007351 => Txt Tokens per Sec:     5545 || Lr: 0.000050
2024-02-08 16:25:40,476 Epoch 1012: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.36 
2024-02-08 16:25:40,476 EPOCH 1013
2024-02-08 16:25:45,732 Epoch 1013: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.03 
2024-02-08 16:25:45,732 EPOCH 1014
2024-02-08 16:25:47,771 [Epoch: 1014 Step: 00067900] Batch Recognition Loss:   0.000192 => Gls Tokens per Sec:     2228 || Batch Translation Loss:   0.020972 => Txt Tokens per Sec:     6056 || Lr: 0.000050
2024-02-08 16:25:50,882 Epoch 1014: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.98 
2024-02-08 16:25:50,883 EPOCH 1015
2024-02-08 16:25:55,826 [Epoch: 1015 Step: 00068000] Batch Recognition Loss:   0.000127 => Gls Tokens per Sec:     2007 || Batch Translation Loss:   0.012581 => Txt Tokens per Sec:     5514 || Lr: 0.000050
2024-02-08 16:26:04,211 Validation result at epoch 1015, step    68000: duration: 8.3850s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.47786	Translation Loss: 92505.24219	PPL: 10295.23242
	Eval Metric: BLEU
	WER 3.18	(DEL: 0.00,	INS: 0.00,	SUB: 3.18)
	BLEU-4 0.69	(BLEU-1: 11.20,	BLEU-2: 3.65,	BLEU-3: 1.39,	BLEU-4: 0.69)
	CHRF 17.20	ROUGE 9.37
2024-02-08 16:26:04,212 Logging Recognition and Translation Outputs
2024-02-08 16:26:04,212 ========================================================================================================================
2024-02-08 16:26:04,212 Logging Sequence: 148_186.00
2024-02-08 16:26:04,212 	Gloss Reference :	A B+C+D+E
2024-02-08 16:26:04,212 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:26:04,212 	Gloss Alignment :	         
2024-02-08 16:26:04,212 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:26:04,215 	Text Reference  :	*** siraj   also  took four  wickets in    1      over thus becoming the record-holder for most wickets in        an            over in odis
2024-02-08 16:26:04,215 	Text Hypothesis :	the winners claim that india beat    south africa and  put  etc      was named         as  the  world   athletics championships wac  in 2019
2024-02-08 16:26:04,215 	Text Alignment  :	I   S       S     S    S     S       S     S      S    S    S        S   S             S   S    S       S         S             S       S   
2024-02-08 16:26:04,215 ========================================================================================================================
2024-02-08 16:26:04,215 Logging Sequence: 61_181.00
2024-02-08 16:26:04,215 	Gloss Reference :	A B+C+D+E
2024-02-08 16:26:04,216 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:26:04,216 	Gloss Alignment :	         
2024-02-08 16:26:04,216 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:26:04,218 	Text Reference  :	* one  other fan  said it     is   babar's  personal chat   we    should    focuc    on this cricketing career and  not his personal life   
2024-02-08 16:26:04,218 	Text Hypothesis :	i will be    tell you  before your hardwork and      friend about extremely saddened by itc  hotels     in     2017 for his ******** parents
2024-02-08 16:26:04,218 	Text Alignment  :	I S    S     S    S    S      S    S        S        S      S     S         S        S  S    S          S      S    S       D        S      
2024-02-08 16:26:04,219 ========================================================================================================================
2024-02-08 16:26:04,219 Logging Sequence: 123_24.00
2024-02-08 16:26:04,219 	Gloss Reference :	A B+C+D+E
2024-02-08 16:26:04,219 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:26:04,219 	Gloss Alignment :	         
2024-02-08 16:26:04,219 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:26:04,220 	Text Reference  :	did you know  that other than cricket  dhoni has      another passion
2024-02-08 16:26:04,220 	Text Hypothesis :	*** *** dhoni was  a     huge surprise for   everyone as      well   
2024-02-08 16:26:04,220 	Text Alignment  :	D   D   S     S    S     S    S        S     S        S       S      
2024-02-08 16:26:04,220 ========================================================================================================================
2024-02-08 16:26:04,220 Logging Sequence: 84_76.00
2024-02-08 16:26:04,221 	Gloss Reference :	A B+C+D+E
2024-02-08 16:26:04,221 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:26:04,221 	Gloss Alignment :	         
2024-02-08 16:26:04,221 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:26:04,222 	Text Reference  :	** * ********* ** ** the teams wanted  to support but were  refused
2024-02-08 16:26:04,222 	Text Hypothesis :	if a situation is so bad then  covered to win     the photo session
2024-02-08 16:26:04,222 	Text Alignment  :	I  I I         I  I  S   S     S          S       S   S     S      
2024-02-08 16:26:04,223 ========================================================================================================================
2024-02-08 16:26:04,223 Logging Sequence: 126_188.00
2024-02-08 16:26:04,223 	Gloss Reference :	A B+C+D+E
2024-02-08 16:26:04,223 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:26:04,223 	Gloss Alignment :	         
2024-02-08 16:26:04,223 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:26:04,224 	Text Reference  :	now he  has  become a    gold       medalist at  the 2020 tokyo olympics
2024-02-08 16:26:04,224 	Text Hypothesis :	*** and they were   very particular as       now the **** ***** fans    
2024-02-08 16:26:04,224 	Text Alignment  :	D   S   S    S      S    S          S        S       D    D     S       
2024-02-08 16:26:04,224 ========================================================================================================================
2024-02-08 16:26:04,777 Epoch 1015: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.96 
2024-02-08 16:26:04,777 EPOCH 1016
2024-02-08 16:26:10,474 Epoch 1016: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.24 
2024-02-08 16:26:10,475 EPOCH 1017
2024-02-08 16:26:12,508 [Epoch: 1017 Step: 00068100] Batch Recognition Loss:   0.000203 => Gls Tokens per Sec:     2156 || Batch Translation Loss:   0.020471 => Txt Tokens per Sec:     5952 || Lr: 0.000050
2024-02-08 16:26:15,379 Epoch 1017: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.01 
2024-02-08 16:26:15,379 EPOCH 1018
2024-02-08 16:26:20,317 [Epoch: 1018 Step: 00068200] Batch Recognition Loss:   0.001203 => Gls Tokens per Sec:     1957 || Batch Translation Loss:   0.074566 => Txt Tokens per Sec:     5413 || Lr: 0.000050
2024-02-08 16:26:20,720 Epoch 1018: Total Training Recognition Loss 0.23  Total Training Translation Loss 3.68 
2024-02-08 16:26:20,720 EPOCH 1019
2024-02-08 16:26:25,713 Epoch 1019: Total Training Recognition Loss 0.48  Total Training Translation Loss 5.77 
2024-02-08 16:26:25,713 EPOCH 1020
2024-02-08 16:26:27,972 [Epoch: 1020 Step: 00068300] Batch Recognition Loss:   0.000170 => Gls Tokens per Sec:     1913 || Batch Translation Loss:   0.022816 => Txt Tokens per Sec:     5268 || Lr: 0.000050
2024-02-08 16:26:31,257 Epoch 1020: Total Training Recognition Loss 0.27  Total Training Translation Loss 5.33 
2024-02-08 16:26:31,257 EPOCH 1021
2024-02-08 16:26:35,628 [Epoch: 1021 Step: 00068400] Batch Recognition Loss:   0.000348 => Gls Tokens per Sec:     2174 || Batch Translation Loss:   0.107273 => Txt Tokens per Sec:     5988 || Lr: 0.000050
2024-02-08 16:26:36,184 Epoch 1021: Total Training Recognition Loss 0.20  Total Training Translation Loss 5.41 
2024-02-08 16:26:36,184 EPOCH 1022
2024-02-08 16:26:41,663 Epoch 1022: Total Training Recognition Loss 0.11  Total Training Translation Loss 7.93 
2024-02-08 16:26:41,663 EPOCH 1023
2024-02-08 16:26:43,577 [Epoch: 1023 Step: 00068500] Batch Recognition Loss:   0.000350 => Gls Tokens per Sec:     2176 || Batch Translation Loss:   0.036846 => Txt Tokens per Sec:     6027 || Lr: 0.000050
2024-02-08 16:26:47,005 Epoch 1023: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.73 
2024-02-08 16:26:47,006 EPOCH 1024
2024-02-08 16:26:51,864 [Epoch: 1024 Step: 00068600] Batch Recognition Loss:   0.000329 => Gls Tokens per Sec:     1923 || Batch Translation Loss:   0.072815 => Txt Tokens per Sec:     5276 || Lr: 0.000050
2024-02-08 16:26:52,622 Epoch 1024: Total Training Recognition Loss 0.09  Total Training Translation Loss 2.15 
2024-02-08 16:26:52,622 EPOCH 1025
2024-02-08 16:26:58,035 Epoch 1025: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.71 
2024-02-08 16:26:58,036 EPOCH 1026
2024-02-08 16:26:59,920 [Epoch: 1026 Step: 00068700] Batch Recognition Loss:   0.000252 => Gls Tokens per Sec:     2070 || Batch Translation Loss:   0.038250 => Txt Tokens per Sec:     5613 || Lr: 0.000050
2024-02-08 16:27:03,360 Epoch 1026: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.09 
2024-02-08 16:27:03,360 EPOCH 1027
2024-02-08 16:27:08,177 [Epoch: 1027 Step: 00068800] Batch Recognition Loss:   0.000210 => Gls Tokens per Sec:     1927 || Batch Translation Loss:   0.016391 => Txt Tokens per Sec:     5407 || Lr: 0.000050
2024-02-08 16:27:08,906 Epoch 1027: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.02 
2024-02-08 16:27:08,907 EPOCH 1028
2024-02-08 16:27:14,460 Epoch 1028: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.64 
2024-02-08 16:27:14,460 EPOCH 1029
2024-02-08 16:27:16,470 [Epoch: 1029 Step: 00068900] Batch Recognition Loss:   0.001463 => Gls Tokens per Sec:     1863 || Batch Translation Loss:   0.046479 => Txt Tokens per Sec:     5078 || Lr: 0.000050
2024-02-08 16:27:20,171 Epoch 1029: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.78 
2024-02-08 16:27:20,172 EPOCH 1030
2024-02-08 16:27:24,624 [Epoch: 1030 Step: 00069000] Batch Recognition Loss:   0.000161 => Gls Tokens per Sec:     2027 || Batch Translation Loss:   0.020560 => Txt Tokens per Sec:     5651 || Lr: 0.000050
2024-02-08 16:27:25,249 Epoch 1030: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.62 
2024-02-08 16:27:25,249 EPOCH 1031
2024-02-08 16:27:29,927 Epoch 1031: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.25 
2024-02-08 16:27:29,927 EPOCH 1032
2024-02-08 16:27:31,594 [Epoch: 1032 Step: 00069100] Batch Recognition Loss:   0.001149 => Gls Tokens per Sec:     2149 || Batch Translation Loss:   0.014407 => Txt Tokens per Sec:     6015 || Lr: 0.000050
2024-02-08 16:27:35,154 Epoch 1032: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.30 
2024-02-08 16:27:35,155 EPOCH 1033
2024-02-08 16:27:39,341 [Epoch: 1033 Step: 00069200] Batch Recognition Loss:   0.000227 => Gls Tokens per Sec:     2118 || Batch Translation Loss:   0.016243 => Txt Tokens per Sec:     5727 || Lr: 0.000050
2024-02-08 16:27:40,482 Epoch 1033: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.11 
2024-02-08 16:27:40,482 EPOCH 1034
2024-02-08 16:27:45,844 Epoch 1034: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.02 
2024-02-08 16:27:45,845 EPOCH 1035
2024-02-08 16:27:47,353 [Epoch: 1035 Step: 00069300] Batch Recognition Loss:   0.000200 => Gls Tokens per Sec:     2336 || Batch Translation Loss:   0.014293 => Txt Tokens per Sec:     6084 || Lr: 0.000050
2024-02-08 16:27:50,971 Epoch 1035: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.34 
2024-02-08 16:27:50,971 EPOCH 1036
2024-02-08 16:27:55,154 [Epoch: 1036 Step: 00069400] Batch Recognition Loss:   0.001779 => Gls Tokens per Sec:     2081 || Batch Translation Loss:   0.020797 => Txt Tokens per Sec:     5805 || Lr: 0.000050
2024-02-08 16:27:56,106 Epoch 1036: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.91 
2024-02-08 16:27:56,106 EPOCH 1037
2024-02-08 16:28:01,680 Epoch 1037: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.50 
2024-02-08 16:28:01,680 EPOCH 1038
2024-02-08 16:28:03,199 [Epoch: 1038 Step: 00069500] Batch Recognition Loss:   0.000235 => Gls Tokens per Sec:     2148 || Batch Translation Loss:   0.008760 => Txt Tokens per Sec:     5625 || Lr: 0.000050
2024-02-08 16:28:07,172 Epoch 1038: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.79 
2024-02-08 16:28:07,172 EPOCH 1039
2024-02-08 16:28:11,331 [Epoch: 1039 Step: 00069600] Batch Recognition Loss:   0.000195 => Gls Tokens per Sec:     2054 || Batch Translation Loss:   0.025237 => Txt Tokens per Sec:     5765 || Lr: 0.000050
2024-02-08 16:28:12,138 Epoch 1039: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.62 
2024-02-08 16:28:12,138 EPOCH 1040
2024-02-08 16:28:17,361 Epoch 1040: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.29 
2024-02-08 16:28:17,362 EPOCH 1041
2024-02-08 16:28:18,840 [Epoch: 1041 Step: 00069700] Batch Recognition Loss:   0.000259 => Gls Tokens per Sec:     2167 || Batch Translation Loss:   0.038042 => Txt Tokens per Sec:     6031 || Lr: 0.000050
2024-02-08 16:28:22,569 Epoch 1041: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.75 
2024-02-08 16:28:22,569 EPOCH 1042
2024-02-08 16:28:26,833 [Epoch: 1042 Step: 00069800] Batch Recognition Loss:   0.000238 => Gls Tokens per Sec:     1966 || Batch Translation Loss:   0.026428 => Txt Tokens per Sec:     5307 || Lr: 0.000050
2024-02-08 16:28:28,045 Epoch 1042: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.03 
2024-02-08 16:28:28,045 EPOCH 1043
2024-02-08 16:28:33,053 Epoch 1043: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.66 
2024-02-08 16:28:33,053 EPOCH 1044
2024-02-08 16:28:34,255 [Epoch: 1044 Step: 00069900] Batch Recognition Loss:   0.000999 => Gls Tokens per Sec:     2531 || Batch Translation Loss:   0.037062 => Txt Tokens per Sec:     6858 || Lr: 0.000050
2024-02-08 16:28:38,194 Epoch 1044: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.43 
2024-02-08 16:28:38,194 EPOCH 1045
2024-02-08 16:28:42,424 [Epoch: 1045 Step: 00070000] Batch Recognition Loss:   0.000841 => Gls Tokens per Sec:     1944 || Batch Translation Loss:   0.015208 => Txt Tokens per Sec:     5424 || Lr: 0.000050
2024-02-08 16:28:50,363 Validation result at epoch 1045, step    70000: duration: 7.9380s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.68700	Translation Loss: 93176.62500	PPL: 11009.29004
	Eval Metric: BLEU
	WER 3.25	(DEL: 0.00,	INS: 0.00,	SUB: 3.25)
	BLEU-4 0.44	(BLEU-1: 9.72,	BLEU-2: 2.99,	BLEU-3: 1.08,	BLEU-4: 0.44)
	CHRF 16.02	ROUGE 8.42
2024-02-08 16:28:50,364 Logging Recognition and Translation Outputs
2024-02-08 16:28:50,364 ========================================================================================================================
2024-02-08 16:28:50,364 Logging Sequence: 129_90.00
2024-02-08 16:28:50,364 	Gloss Reference :	A B+C+D+E
2024-02-08 16:28:50,365 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:28:50,365 	Gloss Alignment :	         
2024-02-08 16:28:50,365 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:28:50,366 	Text Reference  :	however because of the      emergency games will now be    held   without any spectators
2024-02-08 16:28:50,366 	Text Hypothesis :	******* ******* a  football match     lasts for  two equal halves of      45  minutes   
2024-02-08 16:28:50,366 	Text Alignment  :	D       D       S  S        S         S     S    S   S     S      S       S   S         
2024-02-08 16:28:50,366 ========================================================================================================================
2024-02-08 16:28:50,366 Logging Sequence: 179_378.00
2024-02-08 16:28:50,366 	Gloss Reference :	A B+C+D+E
2024-02-08 16:28:50,366 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:28:50,367 	Gloss Alignment :	         
2024-02-08 16:28:50,367 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:28:50,368 	Text Reference  :	these kids think that they are  going  to   the   olympics so they've become some    kind of  stars
2024-02-08 16:28:50,368 	Text Hypothesis :	***** **** ***** **** on   16th august 2022 there was      a  strong  match  between me   and japan
2024-02-08 16:28:50,368 	Text Alignment  :	D     D    D     D    S    S    S      S    S     S        S  S       S      S       S    S   S    
2024-02-08 16:28:50,368 ========================================================================================================================
2024-02-08 16:28:50,369 Logging Sequence: 162_20.00
2024-02-08 16:28:50,369 	Gloss Reference :	A B+C+D+E
2024-02-08 16:28:50,369 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:28:50,369 	Gloss Alignment :	         
2024-02-08 16:28:50,369 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:28:50,371 	Text Reference  :	***** not  only this but      they  blamed mohammed shami's religion    as     the reason for india's loss   
2024-02-08 16:28:50,371 	Text Hypothesis :	kohli said that he   supports shami 200    and      their   brotherhood cannot be  broken by  these   attacks
2024-02-08 16:28:50,371 	Text Alignment  :	I     S    S    S    S        S     S      S        S       S           S      S   S      S   S       S      
2024-02-08 16:28:50,371 ========================================================================================================================
2024-02-08 16:28:50,371 Logging Sequence: 106_169.00
2024-02-08 16:28:50,371 	Gloss Reference :	A B+C+D+E
2024-02-08 16:28:50,371 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:28:50,371 	Gloss Alignment :	         
2024-02-08 16:28:50,372 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:28:50,372 	Text Reference  :	prime minister narendra modi also expressed his happiness while congratulating the **** team on    twitter
2024-02-08 16:28:50,372 	Text Hypothesis :	***** ******** ******** **** **** ********* *** ********* he    reached        the toss and  women said   
2024-02-08 16:28:50,373 	Text Alignment  :	D     D        D        D    D    D         D   D         S     S                  I    S    S     S      
2024-02-08 16:28:50,373 ========================================================================================================================
2024-02-08 16:28:50,373 Logging Sequence: 65_77.00
2024-02-08 16:28:50,373 	Gloss Reference :	A B+C+D+E
2024-02-08 16:28:50,373 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:28:50,373 	Gloss Alignment :	         
2024-02-08 16:28:50,374 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:28:50,374 	Text Reference  :	** *** **** **** *** indian ******* ******* team ** travelling included 16    players
2024-02-08 16:28:50,374 	Text Hypothesis :	do you know that the indian women's cricket team is currently  in       south africa 
2024-02-08 16:28:50,375 	Text Alignment  :	I  I   I    I    I          I       I            I  S          S        S     S      
2024-02-08 16:28:50,375 ========================================================================================================================
2024-02-08 16:28:51,471 Epoch 1045: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.62 
2024-02-08 16:28:51,472 EPOCH 1046
2024-02-08 16:28:56,848 Epoch 1046: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.82 
2024-02-08 16:28:56,848 EPOCH 1047
2024-02-08 16:28:58,549 [Epoch: 1047 Step: 00070100] Batch Recognition Loss:   0.000182 => Gls Tokens per Sec:     1635 || Batch Translation Loss:   0.015584 => Txt Tokens per Sec:     4823 || Lr: 0.000050
2024-02-08 16:29:02,098 Epoch 1047: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.88 
2024-02-08 16:29:02,098 EPOCH 1048
2024-02-08 16:29:05,585 [Epoch: 1048 Step: 00070200] Batch Recognition Loss:   0.000219 => Gls Tokens per Sec:     2312 || Batch Translation Loss:   0.015071 => Txt Tokens per Sec:     6406 || Lr: 0.000050
2024-02-08 16:29:06,649 Epoch 1048: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.22 
2024-02-08 16:29:06,649 EPOCH 1049
2024-02-08 16:29:12,258 Epoch 1049: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.45 
2024-02-08 16:29:12,259 EPOCH 1050
2024-02-08 16:29:13,776 [Epoch: 1050 Step: 00070300] Batch Recognition Loss:   0.005982 => Gls Tokens per Sec:     1728 || Batch Translation Loss:   0.019496 => Txt Tokens per Sec:     4954 || Lr: 0.000050
2024-02-08 16:29:17,874 Epoch 1050: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.40 
2024-02-08 16:29:17,874 EPOCH 1051
2024-02-08 16:29:21,505 [Epoch: 1051 Step: 00070400] Batch Recognition Loss:   0.003362 => Gls Tokens per Sec:     2176 || Batch Translation Loss:   0.014830 => Txt Tokens per Sec:     6037 || Lr: 0.000050
2024-02-08 16:29:22,942 Epoch 1051: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.80 
2024-02-08 16:29:22,942 EPOCH 1052
2024-02-08 16:29:28,423 Epoch 1052: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.83 
2024-02-08 16:29:28,424 EPOCH 1053
2024-02-08 16:29:29,787 [Epoch: 1053 Step: 00070500] Batch Recognition Loss:   0.000474 => Gls Tokens per Sec:     1807 || Batch Translation Loss:   0.045259 => Txt Tokens per Sec:     5381 || Lr: 0.000050
2024-02-08 16:29:33,136 Epoch 1053: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.49 
2024-02-08 16:29:33,137 EPOCH 1054
2024-02-08 16:29:37,072 [Epoch: 1054 Step: 00070600] Batch Recognition Loss:   0.000141 => Gls Tokens per Sec:     1993 || Batch Translation Loss:   0.005524 => Txt Tokens per Sec:     5369 || Lr: 0.000050
2024-02-08 16:29:38,785 Epoch 1054: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.30 
2024-02-08 16:29:38,785 EPOCH 1055
2024-02-08 16:29:43,760 Epoch 1055: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.13 
2024-02-08 16:29:43,761 EPOCH 1056
2024-02-08 16:29:45,172 [Epoch: 1056 Step: 00070700] Batch Recognition Loss:   0.001734 => Gls Tokens per Sec:     1631 || Batch Translation Loss:   0.020637 => Txt Tokens per Sec:     4670 || Lr: 0.000050
2024-02-08 16:29:49,473 Epoch 1056: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.19 
2024-02-08 16:29:49,473 EPOCH 1057
2024-02-08 16:29:53,085 [Epoch: 1057 Step: 00070800] Batch Recognition Loss:   0.000234 => Gls Tokens per Sec:     2100 || Batch Translation Loss:   0.021843 => Txt Tokens per Sec:     5777 || Lr: 0.000050
2024-02-08 16:29:54,523 Epoch 1057: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.17 
2024-02-08 16:29:54,523 EPOCH 1058
2024-02-08 16:29:59,839 Epoch 1058: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.61 
2024-02-08 16:29:59,840 EPOCH 1059
2024-02-08 16:30:00,903 [Epoch: 1059 Step: 00070900] Batch Recognition Loss:   0.001478 => Gls Tokens per Sec:     2109 || Batch Translation Loss:   0.043848 => Txt Tokens per Sec:     5890 || Lr: 0.000050
2024-02-08 16:30:05,417 Epoch 1059: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.34 
2024-02-08 16:30:05,418 EPOCH 1060
2024-02-08 16:30:09,282 [Epoch: 1060 Step: 00071000] Batch Recognition Loss:   0.001757 => Gls Tokens per Sec:     1921 || Batch Translation Loss:   0.063957 => Txt Tokens per Sec:     5288 || Lr: 0.000050
2024-02-08 16:30:10,892 Epoch 1060: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.31 
2024-02-08 16:30:10,892 EPOCH 1061
2024-02-08 16:30:16,130 Epoch 1061: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.91 
2024-02-08 16:30:16,131 EPOCH 1062
2024-02-08 16:30:17,157 [Epoch: 1062 Step: 00071100] Batch Recognition Loss:   0.000346 => Gls Tokens per Sec:     2029 || Batch Translation Loss:   0.021693 => Txt Tokens per Sec:     5436 || Lr: 0.000050
2024-02-08 16:30:21,555 Epoch 1062: Total Training Recognition Loss 0.34  Total Training Translation Loss 2.83 
2024-02-08 16:30:21,555 EPOCH 1063
2024-02-08 16:30:25,215 [Epoch: 1063 Step: 00071200] Batch Recognition Loss:   0.000301 => Gls Tokens per Sec:     2011 || Batch Translation Loss:   0.020080 => Txt Tokens per Sec:     5692 || Lr: 0.000050
2024-02-08 16:30:26,929 Epoch 1063: Total Training Recognition Loss 2.17  Total Training Translation Loss 3.31 
2024-02-08 16:30:26,929 EPOCH 1064
2024-02-08 16:30:32,626 Epoch 1064: Total Training Recognition Loss 6.19  Total Training Translation Loss 3.35 
2024-02-08 16:30:32,627 EPOCH 1065
2024-02-08 16:30:33,534 [Epoch: 1065 Step: 00071300] Batch Recognition Loss:   0.000438 => Gls Tokens per Sec:     2119 || Batch Translation Loss:   0.030007 => Txt Tokens per Sec:     6134 || Lr: 0.000050
2024-02-08 16:30:38,083 Epoch 1065: Total Training Recognition Loss 0.35  Total Training Translation Loss 2.70 
2024-02-08 16:30:38,084 EPOCH 1066
2024-02-08 16:30:41,536 [Epoch: 1066 Step: 00071400] Batch Recognition Loss:   0.000740 => Gls Tokens per Sec:     2057 || Batch Translation Loss:   0.034459 => Txt Tokens per Sec:     5461 || Lr: 0.000050
2024-02-08 16:30:43,461 Epoch 1066: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.00 
2024-02-08 16:30:43,461 EPOCH 1067
2024-02-08 16:30:48,398 Epoch 1067: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.00 
2024-02-08 16:30:48,399 EPOCH 1068
2024-02-08 16:30:49,281 [Epoch: 1068 Step: 00071500] Batch Recognition Loss:   0.000117 => Gls Tokens per Sec:     1998 || Batch Translation Loss:   0.014804 => Txt Tokens per Sec:     5692 || Lr: 0.000050
2024-02-08 16:30:54,068 Epoch 1068: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.14 
2024-02-08 16:30:54,069 EPOCH 1069
2024-02-08 16:30:57,361 [Epoch: 1069 Step: 00071600] Batch Recognition Loss:   0.000242 => Gls Tokens per Sec:     2108 || Batch Translation Loss:   0.021595 => Txt Tokens per Sec:     5888 || Lr: 0.000050
2024-02-08 16:30:59,011 Epoch 1069: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.24 
2024-02-08 16:30:59,011 EPOCH 1070
2024-02-08 16:31:04,419 Epoch 1070: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.75 
2024-02-08 16:31:04,420 EPOCH 1071
2024-02-08 16:31:05,273 [Epoch: 1071 Step: 00071700] Batch Recognition Loss:   0.004271 => Gls Tokens per Sec:     1761 || Batch Translation Loss:   0.060892 => Txt Tokens per Sec:     5161 || Lr: 0.000050
2024-02-08 16:31:09,470 Epoch 1071: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.16 
2024-02-08 16:31:09,470 EPOCH 1072
2024-02-08 16:31:12,902 [Epoch: 1072 Step: 00071800] Batch Recognition Loss:   0.000118 => Gls Tokens per Sec:     1976 || Batch Translation Loss:   0.018025 => Txt Tokens per Sec:     5289 || Lr: 0.000050
2024-02-08 16:31:15,128 Epoch 1072: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.20 
2024-02-08 16:31:15,129 EPOCH 1073
2024-02-08 16:31:20,823 Epoch 1073: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.98 
2024-02-08 16:31:20,824 EPOCH 1074
2024-02-08 16:31:21,407 [Epoch: 1074 Step: 00071900] Batch Recognition Loss:   0.000205 => Gls Tokens per Sec:     2478 || Batch Translation Loss:   0.021259 => Txt Tokens per Sec:     6458 || Lr: 0.000050
2024-02-08 16:31:25,986 Epoch 1074: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.80 
2024-02-08 16:31:25,986 EPOCH 1075
2024-02-08 16:31:28,768 [Epoch: 1075 Step: 00072000] Batch Recognition Loss:   0.003580 => Gls Tokens per Sec:     2417 || Batch Translation Loss:   0.014180 => Txt Tokens per Sec:     6710 || Lr: 0.000050
2024-02-08 16:31:37,450 Validation result at epoch 1075, step    72000: duration: 8.6823s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.39236	Translation Loss: 93141.41406	PPL: 10970.63574
	Eval Metric: BLEU
	WER 2.61	(DEL: 0.00,	INS: 0.00,	SUB: 2.61)
	BLEU-4 0.53	(BLEU-1: 9.98,	BLEU-2: 3.11,	BLEU-3: 1.23,	BLEU-4: 0.53)
	CHRF 16.74	ROUGE 8.37
2024-02-08 16:31:37,451 Logging Recognition and Translation Outputs
2024-02-08 16:31:37,451 ========================================================================================================================
2024-02-08 16:31:37,451 Logging Sequence: 101_92.00
2024-02-08 16:31:37,452 	Gloss Reference :	A B+C+D+E
2024-02-08 16:31:37,452 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:31:37,452 	Gloss Alignment :	         
2024-02-08 16:31:37,452 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:31:37,453 	Text Reference  :	******** india had     to   score 190  runs to  win 
2024-02-08 16:31:37,453 	Text Hypothesis :	pakistan team  members know who   have left the loss
2024-02-08 16:31:37,453 	Text Alignment  :	I        S     S       S    S     S    S    S   S   
2024-02-08 16:31:37,453 ========================================================================================================================
2024-02-08 16:31:37,453 Logging Sequence: 164_412.00
2024-02-08 16:31:37,453 	Gloss Reference :	A B+C+D+E
2024-02-08 16:31:37,453 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:31:37,453 	Gloss Alignment :	         
2024-02-08 16:31:37,454 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:31:37,455 	Text Reference  :	if  you divide these two  figures you will be shocked to know that each ball's worth is   rs  50      lakhs  
2024-02-08 16:31:37,456 	Text Hypothesis :	but it  was    to    bcci do      you **** ** ******* ** know who  will have   to    wait for further updates
2024-02-08 16:31:37,456 	Text Alignment  :	S   S   S      S     S    S           D    D  D       D       S    S    S      S     S    S   S       S      
2024-02-08 16:31:37,456 ========================================================================================================================
2024-02-08 16:31:37,456 Logging Sequence: 177_160.00
2024-02-08 16:31:37,456 	Gloss Reference :	A B+C+D+E
2024-02-08 16:31:37,456 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:31:37,456 	Gloss Alignment :	         
2024-02-08 16:31:37,456 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:31:37,458 	Text Reference  :	the police also said    that sushil had     asked his friend to ** record a   video   
2024-02-08 16:31:37,458 	Text Hypothesis :	*** ****** **** however this was    because of    her strict to do with   the olympics
2024-02-08 16:31:37,458 	Text Alignment  :	D   D      D    S       S    S      S       S     S   S         I  S      S   S       
2024-02-08 16:31:37,458 ========================================================================================================================
2024-02-08 16:31:37,458 Logging Sequence: 124_62.00
2024-02-08 16:31:37,458 	Gloss Reference :	A B+C+D+E
2024-02-08 16:31:37,458 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:31:37,459 	Gloss Alignment :	         
2024-02-08 16:31:37,459 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:31:37,460 	Text Reference  :	however dhoni has said that he will continue to  play      for the team
2024-02-08 16:31:37,460 	Text Hypothesis :	******* dhoni has **** **** ** a    huge     fan following on  his well
2024-02-08 16:31:37,460 	Text Alignment  :	D                 D    D    D  S    S        S   S         S   S   S   
2024-02-08 16:31:37,460 ========================================================================================================================
2024-02-08 16:31:37,460 Logging Sequence: 71_149.00
2024-02-08 16:31:37,460 	Gloss Reference :	A B+C+D+E
2024-02-08 16:31:37,460 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:31:37,461 	Gloss Alignment :	         
2024-02-08 16:31:37,461 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:31:37,462 	Text Reference  :	**** *** ** his coach sanjay had  suggested his name for the  madhya pradesh  ranji trophy team 
2024-02-08 16:31:37,462 	Text Hypothesis :	just and he was not   by     your hardwork  and he   had also an     outbreak which went   viral
2024-02-08 16:31:37,462 	Text Alignment  :	I    I   I  S   S     S      S    S         S   S    S   S    S      S        S     S      S    
2024-02-08 16:31:37,463 ========================================================================================================================
2024-02-08 16:31:39,519 Epoch 1075: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.38 
2024-02-08 16:31:39,519 EPOCH 1076
2024-02-08 16:31:45,142 Epoch 1076: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.96 
2024-02-08 16:31:45,142 EPOCH 1077
2024-02-08 16:31:45,728 [Epoch: 1077 Step: 00072100] Batch Recognition Loss:   0.000242 => Gls Tokens per Sec:     2188 || Batch Translation Loss:   0.016875 => Txt Tokens per Sec:     6111 || Lr: 0.000050
2024-02-08 16:31:50,329 Epoch 1077: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.03 
2024-02-08 16:31:50,329 EPOCH 1078
2024-02-08 16:31:53,471 [Epoch: 1078 Step: 00072200] Batch Recognition Loss:   0.000134 => Gls Tokens per Sec:     2089 || Batch Translation Loss:   0.023120 => Txt Tokens per Sec:     5709 || Lr: 0.000050
2024-02-08 16:31:55,747 Epoch 1078: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.40 
2024-02-08 16:31:55,748 EPOCH 1079
2024-02-08 16:32:00,825 Epoch 1079: Total Training Recognition Loss 0.05  Total Training Translation Loss 4.56 
2024-02-08 16:32:00,826 EPOCH 1080
2024-02-08 16:32:01,306 [Epoch: 1080 Step: 00072300] Batch Recognition Loss:   0.000468 => Gls Tokens per Sec:     2333 || Batch Translation Loss:   0.034574 => Txt Tokens per Sec:     6698 || Lr: 0.000050
2024-02-08 16:32:06,188 Epoch 1080: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.56 
2024-02-08 16:32:06,188 EPOCH 1081
2024-02-08 16:32:09,120 [Epoch: 1081 Step: 00072400] Batch Recognition Loss:   0.001295 => Gls Tokens per Sec:     2184 || Batch Translation Loss:   0.032631 => Txt Tokens per Sec:     5881 || Lr: 0.000050
2024-02-08 16:32:11,350 Epoch 1081: Total Training Recognition Loss 0.05  Total Training Translation Loss 5.73 
2024-02-08 16:32:11,350 EPOCH 1082
2024-02-08 16:32:16,735 Epoch 1082: Total Training Recognition Loss 0.04  Total Training Translation Loss 4.05 
2024-02-08 16:32:16,735 EPOCH 1083
2024-02-08 16:32:17,185 [Epoch: 1083 Step: 00072500] Batch Recognition Loss:   0.000297 => Gls Tokens per Sec:     2143 || Batch Translation Loss:   0.018456 => Txt Tokens per Sec:     5804 || Lr: 0.000050
2024-02-08 16:32:21,755 Epoch 1083: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.80 
2024-02-08 16:32:21,756 EPOCH 1084
2024-02-08 16:32:24,927 [Epoch: 1084 Step: 00072600] Batch Recognition Loss:   0.000407 => Gls Tokens per Sec:     1937 || Batch Translation Loss:   0.011149 => Txt Tokens per Sec:     5416 || Lr: 0.000050
2024-02-08 16:32:27,182 Epoch 1084: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.61 
2024-02-08 16:32:27,183 EPOCH 1085
2024-02-08 16:32:32,227 Epoch 1085: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.41 
2024-02-08 16:32:32,227 EPOCH 1086
2024-02-08 16:32:32,569 [Epoch: 1086 Step: 00072700] Batch Recognition Loss:   0.000246 => Gls Tokens per Sec:     2346 || Batch Translation Loss:   0.022943 => Txt Tokens per Sec:     6305 || Lr: 0.000050
2024-02-08 16:32:37,866 Epoch 1086: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.25 
2024-02-08 16:32:37,867 EPOCH 1087
2024-02-08 16:32:40,597 [Epoch: 1087 Step: 00072800] Batch Recognition Loss:   0.000133 => Gls Tokens per Sec:     2228 || Batch Translation Loss:   0.014471 => Txt Tokens per Sec:     6068 || Lr: 0.000050
2024-02-08 16:32:42,672 Epoch 1087: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.21 
2024-02-08 16:32:42,673 EPOCH 1088
2024-02-08 16:32:48,123 Epoch 1088: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.59 
2024-02-08 16:32:48,124 EPOCH 1089
2024-02-08 16:32:48,381 [Epoch: 1089 Step: 00072900] Batch Recognition Loss:   0.000245 => Gls Tokens per Sec:     2500 || Batch Translation Loss:   0.005221 => Txt Tokens per Sec:     5668 || Lr: 0.000050
2024-02-08 16:32:53,241 Epoch 1089: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.70 
2024-02-08 16:32:53,241 EPOCH 1090
2024-02-08 16:32:56,299 [Epoch: 1090 Step: 00073000] Batch Recognition Loss:   0.000220 => Gls Tokens per Sec:     1937 || Batch Translation Loss:   0.069846 => Txt Tokens per Sec:     5478 || Lr: 0.000050
2024-02-08 16:32:58,734 Epoch 1090: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.71 
2024-02-08 16:32:58,734 EPOCH 1091
2024-02-08 16:33:04,144 Epoch 1091: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.42 
2024-02-08 16:33:04,144 EPOCH 1092
2024-02-08 16:33:04,435 [Epoch: 1092 Step: 00073100] Batch Recognition Loss:   0.000227 => Gls Tokens per Sec:     1655 || Batch Translation Loss:   0.018254 => Txt Tokens per Sec:     4248 || Lr: 0.000050
2024-02-08 16:33:09,522 Epoch 1092: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.49 
2024-02-08 16:33:09,522 EPOCH 1093
2024-02-08 16:33:12,531 [Epoch: 1093 Step: 00073200] Batch Recognition Loss:   0.000156 => Gls Tokens per Sec:     1915 || Batch Translation Loss:   0.017772 => Txt Tokens per Sec:     5477 || Lr: 0.000050
2024-02-08 16:33:15,118 Epoch 1093: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.47 
2024-02-08 16:33:15,118 EPOCH 1094
2024-02-08 16:33:20,580 Epoch 1094: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.30 
2024-02-08 16:33:20,580 EPOCH 1095
2024-02-08 16:33:20,777 [Epoch: 1095 Step: 00073300] Batch Recognition Loss:   0.000256 => Gls Tokens per Sec:     1633 || Batch Translation Loss:   0.017108 => Txt Tokens per Sec:     5393 || Lr: 0.000050
2024-02-08 16:33:26,313 Epoch 1095: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.06 
2024-02-08 16:33:26,313 EPOCH 1096
2024-02-08 16:33:29,032 [Epoch: 1096 Step: 00073400] Batch Recognition Loss:   0.001608 => Gls Tokens per Sec:     2062 || Batch Translation Loss:   0.013492 => Txt Tokens per Sec:     5551 || Lr: 0.000050
2024-02-08 16:33:31,699 Epoch 1096: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.05 
2024-02-08 16:33:31,700 EPOCH 1097
2024-02-08 16:33:37,231 Epoch 1097: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.27 
2024-02-08 16:33:37,232 EPOCH 1098
2024-02-08 16:33:37,287 [Epoch: 1098 Step: 00073500] Batch Recognition Loss:   0.000151 => Gls Tokens per Sec:     2963 || Batch Translation Loss:   0.017270 => Txt Tokens per Sec:     7259 || Lr: 0.000050
2024-02-08 16:33:42,692 Epoch 1098: Total Training Recognition Loss 0.04  Total Training Translation Loss 6.24 
2024-02-08 16:33:42,693 EPOCH 1099
2024-02-08 16:33:45,433 [Epoch: 1099 Step: 00073600] Batch Recognition Loss:   0.000240 => Gls Tokens per Sec:     1950 || Batch Translation Loss:   0.045119 => Txt Tokens per Sec:     5455 || Lr: 0.000050
2024-02-08 16:33:48,159 Epoch 1099: Total Training Recognition Loss 0.04  Total Training Translation Loss 7.33 
2024-02-08 16:33:48,159 EPOCH 1100
2024-02-08 16:33:53,500 [Epoch: 1100 Step: 00073700] Batch Recognition Loss:   0.000297 => Gls Tokens per Sec:     1989 || Batch Translation Loss:   0.033155 => Txt Tokens per Sec:     5502 || Lr: 0.000050
2024-02-08 16:33:53,501 Epoch 1100: Total Training Recognition Loss 0.04  Total Training Translation Loss 5.22 
2024-02-08 16:33:53,501 EPOCH 1101
2024-02-08 16:33:58,914 Epoch 1101: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.80 
2024-02-08 16:33:58,914 EPOCH 1102
2024-02-08 16:34:01,572 [Epoch: 1102 Step: 00073800] Batch Recognition Loss:   0.000256 => Gls Tokens per Sec:     1950 || Batch Translation Loss:   0.059794 => Txt Tokens per Sec:     5499 || Lr: 0.000050
2024-02-08 16:34:04,379 Epoch 1102: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.53 
2024-02-08 16:34:04,380 EPOCH 1103
2024-02-08 16:34:09,794 [Epoch: 1103 Step: 00073900] Batch Recognition Loss:   0.000241 => Gls Tokens per Sec:     1932 || Batch Translation Loss:   0.027343 => Txt Tokens per Sec:     5329 || Lr: 0.000050
2024-02-08 16:34:09,920 Epoch 1103: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.85 
2024-02-08 16:34:09,921 EPOCH 1104
2024-02-08 16:34:15,281 Epoch 1104: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.91 
2024-02-08 16:34:15,282 EPOCH 1105
2024-02-08 16:34:17,982 [Epoch: 1105 Step: 00074000] Batch Recognition Loss:   0.001910 => Gls Tokens per Sec:     1897 || Batch Translation Loss:   0.008591 => Txt Tokens per Sec:     5312 || Lr: 0.000050
2024-02-08 16:34:26,337 Validation result at epoch 1105, step    74000: duration: 8.3540s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.41774	Translation Loss: 92614.47656	PPL: 10408.17773
	Eval Metric: BLEU
	WER 2.82	(DEL: 0.00,	INS: 0.00,	SUB: 2.82)
	BLEU-4 0.52	(BLEU-1: 9.91,	BLEU-2: 2.96,	BLEU-3: 1.10,	BLEU-4: 0.52)
	CHRF 16.89	ROUGE 8.42
2024-02-08 16:34:26,338 Logging Recognition and Translation Outputs
2024-02-08 16:34:26,338 ========================================================================================================================
2024-02-08 16:34:26,338 Logging Sequence: 77_172.00
2024-02-08 16:34:26,339 	Gloss Reference :	A B+C+D+E
2024-02-08 16:34:26,339 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:34:26,339 	Gloss Alignment :	         
2024-02-08 16:34:26,339 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:34:26,341 	Text Reference  :	he scored 2 runs on      the fourth ball on     the       5th    6th   jadeja scored a          6      and a           boundary respectively
2024-02-08 16:34:26,341 	Text Hypothesis :	** ****** * **** towards the ****** sri  lankan stadium's ground staff who    toiled diligently during the rain-soaked asia     cup         
2024-02-08 16:34:26,341 	Text Alignment  :	D  D      D D    S           D      S    S      S         S      S     S      S      S          S      S   S           S        S           
2024-02-08 16:34:26,341 ========================================================================================================================
2024-02-08 16:34:26,341 Logging Sequence: 173_39.00
2024-02-08 16:34:26,342 	Gloss Reference :	A B+C+D+E
2024-02-08 16:34:26,342 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:34:26,342 	Gloss Alignment :	         
2024-02-08 16:34:26,342 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:34:26,343 	Text Reference  :	kohli will step down      as      india' captain
2024-02-08 16:34:26,343 	Text Hypothesis :	***** i    am   extremely sadened by     this   
2024-02-08 16:34:26,343 	Text Alignment  :	D     S    S    S         S       S      S      
2024-02-08 16:34:26,343 ========================================================================================================================
2024-02-08 16:34:26,343 Logging Sequence: 138_224.00
2024-02-08 16:34:26,343 	Gloss Reference :	A B+C+D+E
2024-02-08 16:34:26,343 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:34:26,344 	Gloss Alignment :	         
2024-02-08 16:34:26,344 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:34:26,345 	Text Reference  :	then people wrote positive messages and stuck them on  the plastic sheets  
2024-02-08 16:34:26,345 	Text Hypothesis :	**** ****** ***** they     will     be  sent  to   see the covid   pandemic
2024-02-08 16:34:26,345 	Text Alignment  :	D    D      D     S        S        S   S     S    S       S       S       
2024-02-08 16:34:26,345 ========================================================================================================================
2024-02-08 16:34:26,345 Logging Sequence: 128_98.00
2024-02-08 16:34:26,345 	Gloss Reference :	A B+C+D+E
2024-02-08 16:34:26,345 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:34:26,346 	Gloss Alignment :	         
2024-02-08 16:34:26,346 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:34:26,347 	Text Reference  :	with 8 wickets and 43 balls remaining they won the   match   in   such        a   short         time  
2024-02-08 16:34:26,347 	Text Hypothesis :	**** * ******* *** ** ***** ********* **** but their captain kane williamson' key contributions helped
2024-02-08 16:34:26,347 	Text Alignment  :	D    D D       D   D  D     D         D    S   S     S       S    S           S   S             S     
2024-02-08 16:34:26,347 ========================================================================================================================
2024-02-08 16:34:26,347 Logging Sequence: 126_159.00
2024-02-08 16:34:26,347 	Gloss Reference :	A B+C+D+E
2024-02-08 16:34:26,348 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:34:26,348 	Gloss Alignment :	         
2024-02-08 16:34:26,348 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:34:26,349 	Text Reference  :	despite multiple challenges and injuries you   did   not give up   
2024-02-08 16:34:26,349 	Text Hypothesis :	******* this     is         why the      medal stood in  the  medal
2024-02-08 16:34:26,349 	Text Alignment  :	D       S        S          S   S        S     S     S   S    S    
2024-02-08 16:34:26,349 ========================================================================================================================
2024-02-08 16:34:29,369 Epoch 1105: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.84 
2024-02-08 16:34:29,369 EPOCH 1106
2024-02-08 16:34:34,428 [Epoch: 1106 Step: 00074100] Batch Recognition Loss:   0.000254 => Gls Tokens per Sec:     2036 || Batch Translation Loss:   0.034749 => Txt Tokens per Sec:     5629 || Lr: 0.000050
2024-02-08 16:34:34,613 Epoch 1106: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.48 
2024-02-08 16:34:34,613 EPOCH 1107
2024-02-08 16:34:40,065 Epoch 1107: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.54 
2024-02-08 16:34:40,065 EPOCH 1108
2024-02-08 16:34:42,323 [Epoch: 1108 Step: 00074200] Batch Recognition Loss:   0.000150 => Gls Tokens per Sec:     2198 || Batch Translation Loss:   0.019574 => Txt Tokens per Sec:     5768 || Lr: 0.000050
2024-02-08 16:34:45,227 Epoch 1108: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.05 
2024-02-08 16:34:45,227 EPOCH 1109
2024-02-08 16:34:50,030 [Epoch: 1109 Step: 00074300] Batch Recognition Loss:   0.000261 => Gls Tokens per Sec:     2112 || Batch Translation Loss:   0.019906 => Txt Tokens per Sec:     5827 || Lr: 0.000050
2024-02-08 16:34:50,295 Epoch 1109: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.46 
2024-02-08 16:34:50,295 EPOCH 1110
2024-02-08 16:34:55,677 Epoch 1110: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.44 
2024-02-08 16:34:55,677 EPOCH 1111
2024-02-08 16:34:57,865 [Epoch: 1111 Step: 00074400] Batch Recognition Loss:   0.000187 => Gls Tokens per Sec:     2195 || Batch Translation Loss:   0.014793 => Txt Tokens per Sec:     6182 || Lr: 0.000050
2024-02-08 16:35:00,882 Epoch 1111: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.60 
2024-02-08 16:35:00,882 EPOCH 1112
2024-02-08 16:35:06,039 [Epoch: 1112 Step: 00074500] Batch Recognition Loss:   0.000212 => Gls Tokens per Sec:     1936 || Batch Translation Loss:   0.036948 => Txt Tokens per Sec:     5364 || Lr: 0.000050
2024-02-08 16:35:06,330 Epoch 1112: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.58 
2024-02-08 16:35:06,330 EPOCH 1113
2024-02-08 16:35:11,485 Epoch 1113: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.81 
2024-02-08 16:35:11,486 EPOCH 1114
2024-02-08 16:35:13,517 [Epoch: 1114 Step: 00074600] Batch Recognition Loss:   0.032269 => Gls Tokens per Sec:     2286 || Batch Translation Loss:   0.093578 => Txt Tokens per Sec:     6054 || Lr: 0.000050
2024-02-08 16:35:16,725 Epoch 1114: Total Training Recognition Loss 0.09  Total Training Translation Loss 8.81 
2024-02-08 16:35:16,725 EPOCH 1115
2024-02-08 16:35:21,500 [Epoch: 1115 Step: 00074700] Batch Recognition Loss:   0.000835 => Gls Tokens per Sec:     2057 || Batch Translation Loss:   0.041064 => Txt Tokens per Sec:     5646 || Lr: 0.000050
2024-02-08 16:35:22,004 Epoch 1115: Total Training Recognition Loss 0.11  Total Training Translation Loss 6.15 
2024-02-08 16:35:22,004 EPOCH 1116
2024-02-08 16:35:27,226 Epoch 1116: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.48 
2024-02-08 16:35:27,226 EPOCH 1117
2024-02-08 16:35:29,226 [Epoch: 1117 Step: 00074800] Batch Recognition Loss:   0.000621 => Gls Tokens per Sec:     2241 || Batch Translation Loss:   0.044487 => Txt Tokens per Sec:     6138 || Lr: 0.000050
2024-02-08 16:35:32,526 Epoch 1117: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.62 
2024-02-08 16:35:32,526 EPOCH 1118
2024-02-08 16:35:37,096 [Epoch: 1118 Step: 00074900] Batch Recognition Loss:   0.000261 => Gls Tokens per Sec:     2114 || Batch Translation Loss:   0.019869 => Txt Tokens per Sec:     5797 || Lr: 0.000050
2024-02-08 16:35:37,630 Epoch 1118: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.57 
2024-02-08 16:35:37,631 EPOCH 1119
2024-02-08 16:35:42,550 Epoch 1119: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.32 
2024-02-08 16:35:42,551 EPOCH 1120
2024-02-08 16:35:44,994 [Epoch: 1120 Step: 00075000] Batch Recognition Loss:   0.000134 => Gls Tokens per Sec:     1769 || Batch Translation Loss:   0.012905 => Txt Tokens per Sec:     5073 || Lr: 0.000050
2024-02-08 16:35:48,112 Epoch 1120: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.46 
2024-02-08 16:35:48,113 EPOCH 1121
2024-02-08 16:35:52,896 [Epoch: 1121 Step: 00075100] Batch Recognition Loss:   0.001100 => Gls Tokens per Sec:     1987 || Batch Translation Loss:   0.006686 => Txt Tokens per Sec:     5587 || Lr: 0.000050
2024-02-08 16:35:53,356 Epoch 1121: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.19 
2024-02-08 16:35:53,356 EPOCH 1122
2024-02-08 16:35:58,885 Epoch 1122: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.10 
2024-02-08 16:35:58,885 EPOCH 1123
2024-02-08 16:36:00,872 [Epoch: 1123 Step: 00075200] Batch Recognition Loss:   0.000130 => Gls Tokens per Sec:     2044 || Batch Translation Loss:   0.010394 => Txt Tokens per Sec:     5403 || Lr: 0.000050
2024-02-08 16:36:04,335 Epoch 1123: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.99 
2024-02-08 16:36:04,336 EPOCH 1124
2024-02-08 16:36:08,900 [Epoch: 1124 Step: 00075300] Batch Recognition Loss:   0.000140 => Gls Tokens per Sec:     2047 || Batch Translation Loss:   0.031433 => Txt Tokens per Sec:     5635 || Lr: 0.000050
2024-02-08 16:36:09,547 Epoch 1124: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.04 
2024-02-08 16:36:09,547 EPOCH 1125
2024-02-08 16:36:14,920 Epoch 1125: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.96 
2024-02-08 16:36:14,920 EPOCH 1126
2024-02-08 16:36:16,682 [Epoch: 1126 Step: 00075400] Batch Recognition Loss:   0.000797 => Gls Tokens per Sec:     2215 || Batch Translation Loss:   0.014413 => Txt Tokens per Sec:     5873 || Lr: 0.000050
2024-02-08 16:36:20,023 Epoch 1126: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.99 
2024-02-08 16:36:20,024 EPOCH 1127
2024-02-08 16:36:24,609 [Epoch: 1127 Step: 00075500] Batch Recognition Loss:   0.000934 => Gls Tokens per Sec:     2003 || Batch Translation Loss:   0.010899 => Txt Tokens per Sec:     5539 || Lr: 0.000050
2024-02-08 16:36:25,324 Epoch 1127: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.98 
2024-02-08 16:36:25,325 EPOCH 1128
2024-02-08 16:36:30,561 Epoch 1128: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.93 
2024-02-08 16:36:30,562 EPOCH 1129
2024-02-08 16:36:32,353 [Epoch: 1129 Step: 00075600] Batch Recognition Loss:   0.000195 => Gls Tokens per Sec:     2089 || Batch Translation Loss:   0.018252 => Txt Tokens per Sec:     5930 || Lr: 0.000050
2024-02-08 16:36:35,551 Epoch 1129: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.95 
2024-02-08 16:36:35,552 EPOCH 1130
2024-02-08 16:36:40,433 [Epoch: 1130 Step: 00075700] Batch Recognition Loss:   0.000136 => Gls Tokens per Sec:     1848 || Batch Translation Loss:   0.014287 => Txt Tokens per Sec:     5134 || Lr: 0.000050
2024-02-08 16:36:41,082 Epoch 1130: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.10 
2024-02-08 16:36:41,082 EPOCH 1131
2024-02-08 16:36:45,888 Epoch 1131: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.38 
2024-02-08 16:36:45,888 EPOCH 1132
2024-02-08 16:36:47,702 [Epoch: 1132 Step: 00075800] Batch Recognition Loss:   0.000145 => Gls Tokens per Sec:     2030 || Batch Translation Loss:   0.021077 => Txt Tokens per Sec:     5349 || Lr: 0.000050
2024-02-08 16:36:51,439 Epoch 1132: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.79 
2024-02-08 16:36:51,439 EPOCH 1133
2024-02-08 16:36:55,473 [Epoch: 1133 Step: 00075900] Batch Recognition Loss:   0.000349 => Gls Tokens per Sec:     2197 || Batch Translation Loss:   0.087385 => Txt Tokens per Sec:     6052 || Lr: 0.000050
2024-02-08 16:36:56,428 Epoch 1133: Total Training Recognition Loss 0.10  Total Training Translation Loss 8.92 
2024-02-08 16:36:56,429 EPOCH 1134
2024-02-08 16:37:01,971 Epoch 1134: Total Training Recognition Loss 0.06  Total Training Translation Loss 6.62 
2024-02-08 16:37:01,971 EPOCH 1135
2024-02-08 16:37:03,550 [Epoch: 1135 Step: 00076000] Batch Recognition Loss:   0.000828 => Gls Tokens per Sec:     2167 || Batch Translation Loss:   0.016387 => Txt Tokens per Sec:     5854 || Lr: 0.000050
2024-02-08 16:37:12,261 Validation result at epoch 1135, step    76000: duration: 8.7110s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.42274	Translation Loss: 93758.59375	PPL: 11668.18555
	Eval Metric: BLEU
	WER 2.90	(DEL: 0.00,	INS: 0.00,	SUB: 2.90)
	BLEU-4 0.40	(BLEU-1: 9.68,	BLEU-2: 2.79,	BLEU-3: 0.97,	BLEU-4: 0.40)
	CHRF 16.16	ROUGE 8.42
2024-02-08 16:37:12,262 Logging Recognition and Translation Outputs
2024-02-08 16:37:12,262 ========================================================================================================================
2024-02-08 16:37:12,263 Logging Sequence: 96_25.00
2024-02-08 16:37:12,263 	Gloss Reference :	A B+C+D+E
2024-02-08 16:37:12,263 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:37:12,263 	Gloss Alignment :	         
2024-02-08 16:37:12,263 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:37:12,265 	Text Reference  :	after the initial matches top 2 teams of each group will go on to play super 4   where they  will play      each other
2024-02-08 16:37:12,265 	Text Hypothesis :	***** *** ******* ******* *** * ***** ** **** ***** **** ** ** ** this 2022  win was   given a    pakistani sri  lanka
2024-02-08 16:37:12,265 	Text Alignment  :	D     D   D       D       D   D D     D  D    D     D    D  D  D  S    S     S   S     S     S    S         S    S    
2024-02-08 16:37:12,265 ========================================================================================================================
2024-02-08 16:37:12,265 Logging Sequence: 83_57.00
2024-02-08 16:37:12,265 	Gloss Reference :	A B+C+D+E
2024-02-08 16:37:12,265 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:37:12,266 	Gloss Alignment :	         
2024-02-08 16:37:12,266 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:37:12,266 	Text Reference  :	collapsed face      first on       the field he  was completely unconscious
2024-02-08 16:37:12,267 	Text Hypothesis :	the       wrestlers were  supposed to  have  fun in  the        usa        
2024-02-08 16:37:12,267 	Text Alignment  :	S         S         S     S        S   S     S   S   S          S          
2024-02-08 16:37:12,267 ========================================================================================================================
2024-02-08 16:37:12,267 Logging Sequence: 86_80.00
2024-02-08 16:37:12,267 	Gloss Reference :	A B+C+D+E
2024-02-08 16:37:12,267 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:37:12,268 	Gloss Alignment :	         
2024-02-08 16:37:12,268 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:37:12,268 	Text Reference  :	* ** **** yashpal    played 160 ranji   matches
2024-02-08 16:37:12,268 	Text Hypothesis :	i am very particular about  his morning walks  
2024-02-08 16:37:12,268 	Text Alignment  :	I I  I    S          S      S   S       S      
2024-02-08 16:37:12,269 ========================================================================================================================
2024-02-08 16:37:12,269 Logging Sequence: 121_200.00
2024-02-08 16:37:12,269 	Gloss Reference :	A B+C+D+E
2024-02-08 16:37:12,269 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:37:12,269 	Gloss Alignment :	         
2024-02-08 16:37:12,269 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:37:12,271 	Text Reference  :	****** **** ** ** ***** indian   players suffered massive losses table tennis  champ manika batra tennis player sumit nagal 
2024-02-08 16:37:12,271 	Text Hypothesis :	mumbai will be in tokyo olympics in      one      place   where  any   thrower can   use    it    this   was    the   injury
2024-02-08 16:37:12,271 	Text Alignment  :	I      I    I  I  I     S        S       S        S       S      S     S       S     S      S     S      S      S     S     
2024-02-08 16:37:12,271 ========================================================================================================================
2024-02-08 16:37:12,272 Logging Sequence: 155_154.00
2024-02-08 16:37:12,272 	Gloss Reference :	A B+C+D+E
2024-02-08 16:37:12,272 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:37:12,272 	Gloss Alignment :	         
2024-02-08 16:37:12,272 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:37:12,274 	Text Reference  :	however icc did not agree to the   demands the two sides eventually resolved their differences allowing  the     afghan team to participate
2024-02-08 16:37:12,274 	Text Hypothesis :	******* *** *** *** ***** we would auction the *** ***** ********** ******** ***** same        melbourne stadium it     was  20 overs      
2024-02-08 16:37:12,274 	Text Alignment  :	D       D   D   D   D     S  S     S           D   D     D          D        D     S           S         S       S      S    S  S          
2024-02-08 16:37:12,274 ========================================================================================================================
2024-02-08 16:37:15,992 Epoch 1135: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.02 
2024-02-08 16:37:15,993 EPOCH 1136
2024-02-08 16:37:20,353 [Epoch: 1136 Step: 00076100] Batch Recognition Loss:   0.000509 => Gls Tokens per Sec:     2019 || Batch Translation Loss:   0.024144 => Txt Tokens per Sec:     5665 || Lr: 0.000050
2024-02-08 16:37:21,249 Epoch 1136: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.91 
2024-02-08 16:37:21,249 EPOCH 1137
2024-02-08 16:37:26,934 Epoch 1137: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.47 
2024-02-08 16:37:26,935 EPOCH 1138
2024-02-08 16:37:28,625 [Epoch: 1138 Step: 00076200] Batch Recognition Loss:   0.000189 => Gls Tokens per Sec:     1989 || Batch Translation Loss:   0.012767 => Txt Tokens per Sec:     5708 || Lr: 0.000050
2024-02-08 16:37:32,439 Epoch 1138: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.99 
2024-02-08 16:37:32,440 EPOCH 1139
2024-02-08 16:37:36,891 [Epoch: 1139 Step: 00076300] Batch Recognition Loss:   0.000881 => Gls Tokens per Sec:     1920 || Batch Translation Loss:   0.020469 => Txt Tokens per Sec:     5278 || Lr: 0.000050
2024-02-08 16:37:37,988 Epoch 1139: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.09 
2024-02-08 16:37:37,988 EPOCH 1140
2024-02-08 16:37:43,507 Epoch 1140: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.98 
2024-02-08 16:37:43,508 EPOCH 1141
2024-02-08 16:37:45,021 [Epoch: 1141 Step: 00076400] Batch Recognition Loss:   0.000258 => Gls Tokens per Sec:     2050 || Batch Translation Loss:   0.014150 => Txt Tokens per Sec:     5876 || Lr: 0.000050
2024-02-08 16:37:48,421 Epoch 1141: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.39 
2024-02-08 16:37:48,421 EPOCH 1142
2024-02-08 16:37:52,704 [Epoch: 1142 Step: 00076500] Batch Recognition Loss:   0.000187 => Gls Tokens per Sec:     1980 || Batch Translation Loss:   0.015972 => Txt Tokens per Sec:     5506 || Lr: 0.000050
2024-02-08 16:37:53,871 Epoch 1142: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.35 
2024-02-08 16:37:53,871 EPOCH 1143
2024-02-08 16:37:58,861 Epoch 1143: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.43 
2024-02-08 16:37:58,861 EPOCH 1144
2024-02-08 16:38:00,345 [Epoch: 1144 Step: 00076600] Batch Recognition Loss:   0.000157 => Gls Tokens per Sec:     2050 || Batch Translation Loss:   0.033637 => Txt Tokens per Sec:     5731 || Lr: 0.000050
2024-02-08 16:38:04,135 Epoch 1144: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.48 
2024-02-08 16:38:04,135 EPOCH 1145
2024-02-08 16:38:08,039 [Epoch: 1145 Step: 00076700] Batch Recognition Loss:   0.000109 => Gls Tokens per Sec:     2107 || Batch Translation Loss:   0.014808 => Txt Tokens per Sec:     5790 || Lr: 0.000050
2024-02-08 16:38:09,179 Epoch 1145: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.74 
2024-02-08 16:38:09,179 EPOCH 1146
2024-02-08 16:38:14,665 Epoch 1146: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.67 
2024-02-08 16:38:14,666 EPOCH 1147
2024-02-08 16:38:16,245 [Epoch: 1147 Step: 00076800] Batch Recognition Loss:   0.000186 => Gls Tokens per Sec:     1762 || Batch Translation Loss:   0.042601 => Txt Tokens per Sec:     4735 || Lr: 0.000050
2024-02-08 16:38:19,893 Epoch 1147: Total Training Recognition Loss 0.12  Total Training Translation Loss 3.85 
2024-02-08 16:38:19,893 EPOCH 1148
2024-02-08 16:38:23,808 [Epoch: 1148 Step: 00076900] Batch Recognition Loss:   0.000299 => Gls Tokens per Sec:     2059 || Batch Translation Loss:   0.056301 => Txt Tokens per Sec:     5662 || Lr: 0.000050
2024-02-08 16:38:25,224 Epoch 1148: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.36 
2024-02-08 16:38:25,224 EPOCH 1149
2024-02-08 16:38:30,217 Epoch 1149: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.31 
2024-02-08 16:38:30,217 EPOCH 1150
2024-02-08 16:38:31,442 [Epoch: 1150 Step: 00077000] Batch Recognition Loss:   0.000233 => Gls Tokens per Sec:     2224 || Batch Translation Loss:   0.031328 => Txt Tokens per Sec:     6168 || Lr: 0.000050
2024-02-08 16:38:35,715 Epoch 1150: Total Training Recognition Loss 0.05  Total Training Translation Loss 4.94 
2024-02-08 16:38:35,716 EPOCH 1151
2024-02-08 16:38:39,666 [Epoch: 1151 Step: 00077100] Batch Recognition Loss:   0.000218 => Gls Tokens per Sec:     2001 || Batch Translation Loss:   0.022779 => Txt Tokens per Sec:     5674 || Lr: 0.000050
2024-02-08 16:38:40,812 Epoch 1151: Total Training Recognition Loss 0.07  Total Training Translation Loss 4.19 
2024-02-08 16:38:40,813 EPOCH 1152
2024-02-08 16:38:46,294 Epoch 1152: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.67 
2024-02-08 16:38:46,295 EPOCH 1153
2024-02-08 16:38:47,355 [Epoch: 1153 Step: 00077200] Batch Recognition Loss:   0.000335 => Gls Tokens per Sec:     2417 || Batch Translation Loss:   0.202130 => Txt Tokens per Sec:     6543 || Lr: 0.000050
2024-02-08 16:38:51,131 Epoch 1153: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.37 
2024-02-08 16:38:51,132 EPOCH 1154
2024-02-08 16:38:55,321 [Epoch: 1154 Step: 00077300] Batch Recognition Loss:   0.000281 => Gls Tokens per Sec:     1848 || Batch Translation Loss:   0.017037 => Txt Tokens per Sec:     5119 || Lr: 0.000050
2024-02-08 16:38:56,825 Epoch 1154: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.88 
2024-02-08 16:38:56,825 EPOCH 1155
2024-02-08 16:39:02,174 Epoch 1155: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.71 
2024-02-08 16:39:02,174 EPOCH 1156
2024-02-08 16:39:03,259 [Epoch: 1156 Step: 00077400] Batch Recognition Loss:   0.000798 => Gls Tokens per Sec:     2214 || Batch Translation Loss:   0.007585 => Txt Tokens per Sec:     6155 || Lr: 0.000050
2024-02-08 16:39:07,335 Epoch 1156: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.14 
2024-02-08 16:39:07,336 EPOCH 1157
2024-02-08 16:39:11,287 [Epoch: 1157 Step: 00077500] Batch Recognition Loss:   0.000241 => Gls Tokens per Sec:     1919 || Batch Translation Loss:   0.023843 => Txt Tokens per Sec:     5363 || Lr: 0.000050
2024-02-08 16:39:12,614 Epoch 1157: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.99 
2024-02-08 16:39:12,614 EPOCH 1158
2024-02-08 16:39:17,296 Epoch 1158: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.66 
2024-02-08 16:39:17,297 EPOCH 1159
2024-02-08 16:39:18,842 [Epoch: 1159 Step: 00077600] Batch Recognition Loss:   0.000147 => Gls Tokens per Sec:     1451 || Batch Translation Loss:   0.021436 => Txt Tokens per Sec:     4412 || Lr: 0.000050
2024-02-08 16:39:22,904 Epoch 1159: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.73 
2024-02-08 16:39:22,904 EPOCH 1160
2024-02-08 16:39:26,479 [Epoch: 1160 Step: 00077700] Batch Recognition Loss:   0.000785 => Gls Tokens per Sec:     2104 || Batch Translation Loss:   0.025496 => Txt Tokens per Sec:     5870 || Lr: 0.000050
2024-02-08 16:39:28,000 Epoch 1160: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.97 
2024-02-08 16:39:28,001 EPOCH 1161
2024-02-08 16:39:33,467 Epoch 1161: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.76 
2024-02-08 16:39:33,468 EPOCH 1162
2024-02-08 16:39:34,382 [Epoch: 1162 Step: 00077800] Batch Recognition Loss:   0.000277 => Gls Tokens per Sec:     2278 || Batch Translation Loss:   0.021237 => Txt Tokens per Sec:     6005 || Lr: 0.000050
2024-02-08 16:39:38,258 Epoch 1162: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.29 
2024-02-08 16:39:38,258 EPOCH 1163
2024-02-08 16:39:42,391 [Epoch: 1163 Step: 00077900] Batch Recognition Loss:   0.000188 => Gls Tokens per Sec:     1757 || Batch Translation Loss:   0.023351 => Txt Tokens per Sec:     5094 || Lr: 0.000050
2024-02-08 16:39:43,817 Epoch 1163: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.79 
2024-02-08 16:39:43,817 EPOCH 1164
2024-02-08 16:39:49,485 Epoch 1164: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.75 
2024-02-08 16:39:49,485 EPOCH 1165
2024-02-08 16:39:50,368 [Epoch: 1165 Step: 00078000] Batch Recognition Loss:   0.000220 => Gls Tokens per Sec:     2179 || Batch Translation Loss:   0.222564 => Txt Tokens per Sec:     5947 || Lr: 0.000050
2024-02-08 16:39:58,967 Validation result at epoch 1165, step    78000: duration: 8.5990s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.06859	Translation Loss: 92792.72656	PPL: 10595.13672
	Eval Metric: BLEU
	WER 2.75	(DEL: 0.00,	INS: 0.00,	SUB: 2.75)
	BLEU-4 0.53	(BLEU-1: 10.34,	BLEU-2: 3.18,	BLEU-3: 1.12,	BLEU-4: 0.53)
	CHRF 16.96	ROUGE 8.73
2024-02-08 16:39:58,968 Logging Recognition and Translation Outputs
2024-02-08 16:39:58,968 ========================================================================================================================
2024-02-08 16:39:58,968 Logging Sequence: 153_36.00
2024-02-08 16:39:58,968 	Gloss Reference :	A B+C+D+E
2024-02-08 16:39:58,968 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:39:58,968 	Gloss Alignment :	         
2024-02-08 16:39:58,969 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:39:58,970 	Text Reference  :	**** ** *** ***** *** india **** made a   good  score of  1686     in 20 overs
2024-02-08 16:39:58,970 	Text Hypothesis :	both of the world cup india lost the  t20 world cup   and pakistan in ** qatar
2024-02-08 16:39:58,970 	Text Alignment  :	I    I  I   I     I         I    S    S   S     S     S   S           D  S    
2024-02-08 16:39:58,970 ========================================================================================================================
2024-02-08 16:39:58,970 Logging Sequence: 181_46.00
2024-02-08 16:39:58,970 	Gloss Reference :	A B+C+D+E
2024-02-08 16:39:58,970 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:39:58,970 	Gloss Alignment :	         
2024-02-08 16:39:58,971 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:39:58,972 	Text Reference  :	*** people were overjoyed with the news singh        posted on   social media saying
2024-02-08 16:39:58,972 	Text Hypothesis :	but there  was  a         huge odi and  commonwealth games  like me     tell  you   
2024-02-08 16:39:58,972 	Text Alignment  :	I   S      S    S         S    S   S    S            S      S    S      S     S     
2024-02-08 16:39:58,972 ========================================================================================================================
2024-02-08 16:39:58,972 Logging Sequence: 59_2.00
2024-02-08 16:39:58,972 	Gloss Reference :	A B+C+D+E
2024-02-08 16:39:58,972 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:39:58,972 	Gloss Alignment :	         
2024-02-08 16:39:58,973 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:39:58,973 	Text Reference  :	** ***** **** *** **** at  the  2020 tokyo olympics in  japan
2024-02-08 16:39:58,973 	Text Hypothesis :	we don't know who will win will have to    wait     and watch
2024-02-08 16:39:58,973 	Text Alignment  :	I  I     I    I   I    S   S    S    S     S        S   S    
2024-02-08 16:39:58,974 ========================================================================================================================
2024-02-08 16:39:58,974 Logging Sequence: 123_14.00
2024-02-08 16:39:58,974 	Gloss Reference :	A B+C+D+E
2024-02-08 16:39:58,974 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:39:58,974 	Gloss Alignment :	         
2024-02-08 16:39:58,974 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:39:58,975 	Text Reference  :	he is very down to    earth   and amazing leader as  well    
2024-02-08 16:39:58,975 	Text Hypothesis :	** ** **** this upset gambhir a   lot     of     his decision
2024-02-08 16:39:58,975 	Text Alignment  :	D  D  D    S    S     S       S   S       S      S   S       
2024-02-08 16:39:58,975 ========================================================================================================================
2024-02-08 16:39:58,975 Logging Sequence: 166_120.00
2024-02-08 16:39:58,976 	Gloss Reference :	A B+C+D+E
2024-02-08 16:39:58,976 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:39:58,976 	Gloss Alignment :	         
2024-02-08 16:39:58,976 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:39:58,977 	Text Reference  :	this was the world test    championship let    me tell you more about it         
2024-02-08 16:39:58,977 	Text Hypothesis :	**** *** *** ***** gujarat titans       racked up and  it  was  not   comfortable
2024-02-08 16:39:58,977 	Text Alignment  :	D    D   D   D     S       S            S      S  S    S   S    S     S          
2024-02-08 16:39:58,977 ========================================================================================================================
2024-02-08 16:40:03,611 Epoch 1165: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.65 
2024-02-08 16:40:03,612 EPOCH 1166
2024-02-08 16:40:06,981 [Epoch: 1166 Step: 00078100] Batch Recognition Loss:   0.000325 => Gls Tokens per Sec:     2108 || Batch Translation Loss:   0.021438 => Txt Tokens per Sec:     5928 || Lr: 0.000025
2024-02-08 16:40:08,461 Epoch 1166: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.24 
2024-02-08 16:40:08,461 EPOCH 1167
2024-02-08 16:40:13,876 Epoch 1167: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.25 
2024-02-08 16:40:13,877 EPOCH 1168
2024-02-08 16:40:14,669 [Epoch: 1168 Step: 00078200] Batch Recognition Loss:   0.000155 => Gls Tokens per Sec:     2225 || Batch Translation Loss:   0.010421 => Txt Tokens per Sec:     5832 || Lr: 0.000025
2024-02-08 16:40:19,022 Epoch 1168: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.08 
2024-02-08 16:40:19,022 EPOCH 1169
2024-02-08 16:40:22,581 [Epoch: 1169 Step: 00078300] Batch Recognition Loss:   0.000137 => Gls Tokens per Sec:     1951 || Batch Translation Loss:   0.010472 => Txt Tokens per Sec:     5397 || Lr: 0.000025
2024-02-08 16:40:24,453 Epoch 1169: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.99 
2024-02-08 16:40:24,453 EPOCH 1170
2024-02-08 16:40:29,272 Epoch 1170: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.98 
2024-02-08 16:40:29,272 EPOCH 1171
2024-02-08 16:40:30,018 [Epoch: 1171 Step: 00078400] Batch Recognition Loss:   0.000146 => Gls Tokens per Sec:     2148 || Batch Translation Loss:   0.065392 => Txt Tokens per Sec:     6066 || Lr: 0.000025
2024-02-08 16:40:34,686 Epoch 1171: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.00 
2024-02-08 16:40:34,687 EPOCH 1172
2024-02-08 16:40:38,061 [Epoch: 1172 Step: 00078500] Batch Recognition Loss:   0.000145 => Gls Tokens per Sec:     2010 || Batch Translation Loss:   0.014348 => Txt Tokens per Sec:     5682 || Lr: 0.000025
2024-02-08 16:40:39,773 Epoch 1172: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.96 
2024-02-08 16:40:39,773 EPOCH 1173
2024-02-08 16:40:45,259 Epoch 1173: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.02 
2024-02-08 16:40:45,260 EPOCH 1174
2024-02-08 16:40:45,931 [Epoch: 1174 Step: 00078600] Batch Recognition Loss:   0.000255 => Gls Tokens per Sec:     2149 || Batch Translation Loss:   0.008472 => Txt Tokens per Sec:     5336 || Lr: 0.000025
2024-02-08 16:40:50,252 Epoch 1174: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.90 
2024-02-08 16:40:50,252 EPOCH 1175
2024-02-08 16:40:53,455 [Epoch: 1175 Step: 00078700] Batch Recognition Loss:   0.000705 => Gls Tokens per Sec:     2098 || Batch Translation Loss:   0.015493 => Txt Tokens per Sec:     5739 || Lr: 0.000025
2024-02-08 16:40:55,499 Epoch 1175: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.86 
2024-02-08 16:40:55,500 EPOCH 1176
2024-02-08 16:41:00,734 Epoch 1176: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.84 
2024-02-08 16:41:00,734 EPOCH 1177
2024-02-08 16:41:01,281 [Epoch: 1177 Step: 00078800] Batch Recognition Loss:   0.000155 => Gls Tokens per Sec:     2340 || Batch Translation Loss:   0.010619 => Txt Tokens per Sec:     6596 || Lr: 0.000025
2024-02-08 16:41:05,991 Epoch 1177: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.86 
2024-02-08 16:41:05,992 EPOCH 1178
2024-02-08 16:41:09,264 [Epoch: 1178 Step: 00078900] Batch Recognition Loss:   0.000109 => Gls Tokens per Sec:     2005 || Batch Translation Loss:   0.008528 => Txt Tokens per Sec:     5501 || Lr: 0.000025
2024-02-08 16:41:11,196 Epoch 1178: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.98 
2024-02-08 16:41:11,197 EPOCH 1179
2024-02-08 16:41:16,554 Epoch 1179: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.94 
2024-02-08 16:41:16,554 EPOCH 1180
2024-02-08 16:41:17,086 [Epoch: 1180 Step: 00079000] Batch Recognition Loss:   0.000171 => Gls Tokens per Sec:     2109 || Batch Translation Loss:   0.007812 => Txt Tokens per Sec:     5876 || Lr: 0.000025
2024-02-08 16:41:21,683 Epoch 1180: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.88 
2024-02-08 16:41:21,684 EPOCH 1181
2024-02-08 16:41:24,688 [Epoch: 1181 Step: 00079100] Batch Recognition Loss:   0.000178 => Gls Tokens per Sec:     2130 || Batch Translation Loss:   0.010505 => Txt Tokens per Sec:     6015 || Lr: 0.000025
2024-02-08 16:41:26,953 Epoch 1181: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.91 
2024-02-08 16:41:26,953 EPOCH 1182
2024-02-08 16:41:32,217 Epoch 1182: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.97 
2024-02-08 16:41:32,217 EPOCH 1183
2024-02-08 16:41:32,721 [Epoch: 1183 Step: 00079200] Batch Recognition Loss:   0.000183 => Gls Tokens per Sec:     1905 || Batch Translation Loss:   0.258067 => Txt Tokens per Sec:     5308 || Lr: 0.000025
2024-02-08 16:41:37,363 Epoch 1183: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.39 
2024-02-08 16:41:37,364 EPOCH 1184
2024-02-08 16:41:40,263 [Epoch: 1184 Step: 00079300] Batch Recognition Loss:   0.000120 => Gls Tokens per Sec:     2119 || Batch Translation Loss:   0.019351 => Txt Tokens per Sec:     5801 || Lr: 0.000025
2024-02-08 16:41:42,833 Epoch 1184: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.89 
2024-02-08 16:41:42,834 EPOCH 1185
2024-02-08 16:41:48,304 Epoch 1185: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.22 
2024-02-08 16:41:48,304 EPOCH 1186
2024-02-08 16:41:48,669 [Epoch: 1186 Step: 00079400] Batch Recognition Loss:   0.000127 => Gls Tokens per Sec:     2198 || Batch Translation Loss:   0.012844 => Txt Tokens per Sec:     6415 || Lr: 0.000025
2024-02-08 16:41:53,345 Epoch 1186: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.11 
2024-02-08 16:41:53,346 EPOCH 1187
2024-02-08 16:41:55,916 [Epoch: 1187 Step: 00079500] Batch Recognition Loss:   0.000156 => Gls Tokens per Sec:     2328 || Batch Translation Loss:   0.014246 => Txt Tokens per Sec:     6222 || Lr: 0.000025
2024-02-08 16:41:58,558 Epoch 1187: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.29 
2024-02-08 16:41:58,558 EPOCH 1188
2024-02-08 16:42:03,710 Epoch 1188: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.59 
2024-02-08 16:42:03,710 EPOCH 1189
2024-02-08 16:42:03,982 [Epoch: 1189 Step: 00079600] Batch Recognition Loss:   0.000123 => Gls Tokens per Sec:     2362 || Batch Translation Loss:   0.013699 => Txt Tokens per Sec:     6801 || Lr: 0.000025
2024-02-08 16:42:09,226 Epoch 1189: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.83 
2024-02-08 16:42:09,227 EPOCH 1190
2024-02-08 16:42:12,279 [Epoch: 1190 Step: 00079700] Batch Recognition Loss:   0.000154 => Gls Tokens per Sec:     1908 || Batch Translation Loss:   0.010614 => Txt Tokens per Sec:     5333 || Lr: 0.000025
2024-02-08 16:42:14,573 Epoch 1190: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.50 
2024-02-08 16:42:14,574 EPOCH 1191
2024-02-08 16:42:20,279 Epoch 1191: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.14 
2024-02-08 16:42:20,280 EPOCH 1192
2024-02-08 16:42:20,543 [Epoch: 1192 Step: 00079800] Batch Recognition Loss:   0.000233 => Gls Tokens per Sec:     1839 || Batch Translation Loss:   0.016081 => Txt Tokens per Sec:     5667 || Lr: 0.000025
2024-02-08 16:42:25,674 Epoch 1192: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.14 
2024-02-08 16:42:25,674 EPOCH 1193
2024-02-08 16:42:28,632 [Epoch: 1193 Step: 00079900] Batch Recognition Loss:   0.000252 => Gls Tokens per Sec:     1949 || Batch Translation Loss:   0.011649 => Txt Tokens per Sec:     5350 || Lr: 0.000025
2024-02-08 16:42:31,141 Epoch 1193: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.11 
2024-02-08 16:42:31,142 EPOCH 1194
2024-02-08 16:42:36,504 Epoch 1194: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.34 
2024-02-08 16:42:36,504 EPOCH 1195
2024-02-08 16:42:36,652 [Epoch: 1195 Step: 00080000] Batch Recognition Loss:   0.000182 => Gls Tokens per Sec:     2177 || Batch Translation Loss:   0.011163 => Txt Tokens per Sec:     5435 || Lr: 0.000025
2024-02-08 16:42:45,232 Validation result at epoch 1195, step    80000: duration: 8.5790s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.42593	Translation Loss: 93270.70312	PPL: 11113.22656
	Eval Metric: BLEU
	WER 3.18	(DEL: 0.00,	INS: 0.00,	SUB: 3.18)
	BLEU-4 0.69	(BLEU-1: 10.36,	BLEU-2: 3.30,	BLEU-3: 1.38,	BLEU-4: 0.69)
	CHRF 16.54	ROUGE 8.96
2024-02-08 16:42:45,233 Logging Recognition and Translation Outputs
2024-02-08 16:42:45,233 ========================================================================================================================
2024-02-08 16:42:45,233 Logging Sequence: 118_111.00
2024-02-08 16:42:45,234 	Gloss Reference :	A B+C+D+E
2024-02-08 16:42:45,234 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:42:45,234 	Gloss Alignment :	         
2024-02-08 16:42:45,234 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:42:45,235 	Text Reference  :	and  people encourage him have  hope for  the   next world cup      
2024-02-08 16:42:45,235 	Text Hypothesis :	this is     not       the first time that kohli will be    jubiliant
2024-02-08 16:42:45,235 	Text Alignment  :	S    S      S         S   S     S    S    S     S    S     S        
2024-02-08 16:42:45,236 ========================================================================================================================
2024-02-08 16:42:45,236 Logging Sequence: 95_16.00
2024-02-08 16:42:45,236 	Gloss Reference :	A B+C+D+E
2024-02-08 16:42:45,236 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:42:45,236 	Gloss Alignment :	         
2024-02-08 16:42:45,236 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:42:45,238 	Text Reference  :	guwahati hosts a very   few   international matches and     that   is  why   thousands of people had    thronged the stadium
2024-02-08 16:42:45,238 	Text Hypothesis :	******** ***** a maruti wagon r             car     circled around the pitch during    a  ranji  trophy match    in  delhi  
2024-02-08 16:42:45,238 	Text Alignment  :	D        D       S      S     S             S       S       S      S   S     S         S  S      S      S        S   S      
2024-02-08 16:42:45,238 ========================================================================================================================
2024-02-08 16:42:45,238 Logging Sequence: 114_2.00
2024-02-08 16:42:45,238 	Gloss Reference :	A B+C+D+E
2024-02-08 16:42:45,239 	Gloss Hypothesis:	A B+C+D  
2024-02-08 16:42:45,239 	Gloss Alignment :	  S      
2024-02-08 16:42:45,239 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:42:45,240 	Text Reference  :	******* the         euro    2020         was held  in the year 2021 as it was postponed due     to  the covid pandemic
2024-02-08 16:42:45,240 	Text Hypothesis :	england all-rounder natalie sciver-brunt was roped in *** **** **** ** ** by  indian    captain for rs  150   crore   
2024-02-08 16:42:45,241 	Text Alignment  :	I       S           S       S                S        D   D    D    D  D  S   S         S       S   S   S     S       
2024-02-08 16:42:45,241 ========================================================================================================================
2024-02-08 16:42:45,241 Logging Sequence: 179_126.00
2024-02-08 16:42:45,241 	Gloss Reference :	A B+C+D+E
2024-02-08 16:42:45,241 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:42:45,241 	Gloss Alignment :	         
2024-02-08 16:42:45,241 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:42:45,243 	Text Reference  :	vinesh argued that       she might contract coronavirus since these wrestlers travelled from     india where     there are many    infections
2024-02-08 16:42:45,243 	Text Hypothesis :	****** the    federation or  sai   did      not         want  to    pick      their     passport was   scheduled on    2nd october 2023      
2024-02-08 16:42:45,243 	Text Alignment  :	D      S      S          S   S     S        S           S     S     S         S         S        S     S         S     S   S       S         
2024-02-08 16:42:45,243 ========================================================================================================================
2024-02-08 16:42:45,244 Logging Sequence: 94_2.00
2024-02-08 16:42:45,244 	Gloss Reference :	A B+C+D+E
2024-02-08 16:42:45,244 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:42:45,244 	Gloss Alignment :	         
2024-02-08 16:42:45,244 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:42:45,245 	Text Reference  :	the icc odi men' world cup 2023 will be hosted by    india on  5th      october 2023 
2024-02-08 16:42:45,245 	Text Hypothesis :	*** *** *** **** india is  now  face of the    world deaf  and pakistan heart   emoji
2024-02-08 16:42:45,246 	Text Alignment  :	D   D   D   D    S     S   S    S    S  S      S     S     S   S        S       S    
2024-02-08 16:42:45,246 ========================================================================================================================
2024-02-08 16:42:50,809 Epoch 1195: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.77 
2024-02-08 16:42:50,809 EPOCH 1196
2024-02-08 16:42:53,582 [Epoch: 1196 Step: 00080100] Batch Recognition Loss:   0.000213 => Gls Tokens per Sec:     1984 || Batch Translation Loss:   0.018023 => Txt Tokens per Sec:     5541 || Lr: 0.000025
2024-02-08 16:42:56,020 Epoch 1196: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.33 
2024-02-08 16:42:56,021 EPOCH 1197
2024-02-08 16:43:01,403 Epoch 1197: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.31 
2024-02-08 16:43:01,404 EPOCH 1198
2024-02-08 16:43:01,482 [Epoch: 1198 Step: 00080200] Batch Recognition Loss:   0.000128 => Gls Tokens per Sec:     2078 || Batch Translation Loss:   0.045400 => Txt Tokens per Sec:     6792 || Lr: 0.000025
2024-02-08 16:43:06,736 Epoch 1198: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.38 
2024-02-08 16:43:06,736 EPOCH 1199
2024-02-08 16:43:09,469 [Epoch: 1199 Step: 00080300] Batch Recognition Loss:   0.000155 => Gls Tokens per Sec:     1991 || Batch Translation Loss:   0.010580 => Txt Tokens per Sec:     5432 || Lr: 0.000025
2024-02-08 16:43:12,227 Epoch 1199: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.64 
2024-02-08 16:43:12,228 EPOCH 1200
2024-02-08 16:43:17,541 [Epoch: 1200 Step: 00080400] Batch Recognition Loss:   0.000123 => Gls Tokens per Sec:     2000 || Batch Translation Loss:   0.012542 => Txt Tokens per Sec:     5532 || Lr: 0.000025
2024-02-08 16:43:17,541 Epoch 1200: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.33 
2024-02-08 16:43:17,541 EPOCH 1201
2024-02-08 16:43:22,984 Epoch 1201: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.52 
2024-02-08 16:43:22,985 EPOCH 1202
2024-02-08 16:43:25,241 [Epoch: 1202 Step: 00080500] Batch Recognition Loss:   0.000161 => Gls Tokens per Sec:     2297 || Batch Translation Loss:   0.014173 => Txt Tokens per Sec:     6177 || Lr: 0.000025
2024-02-08 16:43:28,211 Epoch 1202: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.37 
2024-02-08 16:43:28,212 EPOCH 1203
2024-02-08 16:43:33,608 [Epoch: 1203 Step: 00080600] Batch Recognition Loss:   0.000134 => Gls Tokens per Sec:     1939 || Batch Translation Loss:   0.009032 => Txt Tokens per Sec:     5344 || Lr: 0.000025
2024-02-08 16:43:33,774 Epoch 1203: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.25 
2024-02-08 16:43:33,774 EPOCH 1204
2024-02-08 16:43:38,654 Epoch 1204: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.22 
2024-02-08 16:43:38,654 EPOCH 1205
2024-02-08 16:43:40,835 [Epoch: 1205 Step: 00080700] Batch Recognition Loss:   0.000332 => Gls Tokens per Sec:     2303 || Batch Translation Loss:   0.026040 => Txt Tokens per Sec:     6246 || Lr: 0.000025
2024-02-08 16:43:43,907 Epoch 1205: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.08 
2024-02-08 16:43:43,907 EPOCH 1206
2024-02-08 16:43:48,849 [Epoch: 1206 Step: 00080800] Batch Recognition Loss:   0.000126 => Gls Tokens per Sec:     2085 || Batch Translation Loss:   0.013477 => Txt Tokens per Sec:     5761 || Lr: 0.000025
2024-02-08 16:43:49,004 Epoch 1206: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.02 
2024-02-08 16:43:49,004 EPOCH 1207
2024-02-08 16:43:54,325 Epoch 1207: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.03 
2024-02-08 16:43:54,325 EPOCH 1208
2024-02-08 16:43:56,770 [Epoch: 1208 Step: 00080900] Batch Recognition Loss:   0.000126 => Gls Tokens per Sec:     2029 || Batch Translation Loss:   0.042983 => Txt Tokens per Sec:     5546 || Lr: 0.000025
2024-02-08 16:43:59,573 Epoch 1208: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.99 
2024-02-08 16:43:59,574 EPOCH 1209
2024-02-08 16:44:04,631 [Epoch: 1209 Step: 00081000] Batch Recognition Loss:   0.000127 => Gls Tokens per Sec:     2006 || Batch Translation Loss:   0.014014 => Txt Tokens per Sec:     5529 || Lr: 0.000025
2024-02-08 16:44:04,891 Epoch 1209: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.72 
2024-02-08 16:44:04,891 EPOCH 1210
2024-02-08 16:44:09,916 Epoch 1210: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.05 
2024-02-08 16:44:09,916 EPOCH 1211
2024-02-08 16:44:12,278 [Epoch: 1211 Step: 00081100] Batch Recognition Loss:   0.000218 => Gls Tokens per Sec:     1992 || Batch Translation Loss:   0.006502 => Txt Tokens per Sec:     5283 || Lr: 0.000025
2024-02-08 16:44:15,453 Epoch 1211: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.00 
2024-02-08 16:44:15,453 EPOCH 1212
2024-02-08 16:44:20,053 [Epoch: 1212 Step: 00081200] Batch Recognition Loss:   0.000331 => Gls Tokens per Sec:     2170 || Batch Translation Loss:   0.007734 => Txt Tokens per Sec:     5960 || Lr: 0.000025
2024-02-08 16:44:20,492 Epoch 1212: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.13 
2024-02-08 16:44:20,493 EPOCH 1213
2024-02-08 16:44:25,931 Epoch 1213: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.06 
2024-02-08 16:44:25,932 EPOCH 1214
2024-02-08 16:44:28,079 [Epoch: 1214 Step: 00081300] Batch Recognition Loss:   0.000209 => Gls Tokens per Sec:     2162 || Batch Translation Loss:   0.016496 => Txt Tokens per Sec:     6149 || Lr: 0.000025
2024-02-08 16:44:30,809 Epoch 1214: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.94 
2024-02-08 16:44:30,810 EPOCH 1215
2024-02-08 16:44:35,800 [Epoch: 1215 Step: 00081400] Batch Recognition Loss:   0.000169 => Gls Tokens per Sec:     1968 || Batch Translation Loss:   0.042937 => Txt Tokens per Sec:     5436 || Lr: 0.000025
2024-02-08 16:44:36,260 Epoch 1215: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.42 
2024-02-08 16:44:36,260 EPOCH 1216
2024-02-08 16:44:41,699 Epoch 1216: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.30 
2024-02-08 16:44:41,700 EPOCH 1217
2024-02-08 16:44:43,857 [Epoch: 1217 Step: 00081500] Batch Recognition Loss:   0.000205 => Gls Tokens per Sec:     2078 || Batch Translation Loss:   0.029813 => Txt Tokens per Sec:     5607 || Lr: 0.000025
2024-02-08 16:44:47,263 Epoch 1217: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.13 
2024-02-08 16:44:47,263 EPOCH 1218
2024-02-08 16:44:52,187 [Epoch: 1218 Step: 00081600] Batch Recognition Loss:   0.000306 => Gls Tokens per Sec:     1963 || Batch Translation Loss:   0.021429 => Txt Tokens per Sec:     5484 || Lr: 0.000025
2024-02-08 16:44:52,580 Epoch 1218: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.24 
2024-02-08 16:44:52,580 EPOCH 1219
2024-02-08 16:44:58,007 Epoch 1219: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.44 
2024-02-08 16:44:58,008 EPOCH 1220
2024-02-08 16:45:00,078 [Epoch: 1220 Step: 00081700] Batch Recognition Loss:   0.000234 => Gls Tokens per Sec:     2088 || Batch Translation Loss:   0.020635 => Txt Tokens per Sec:     5856 || Lr: 0.000025
2024-02-08 16:45:03,557 Epoch 1220: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.17 
2024-02-08 16:45:03,558 EPOCH 1221
2024-02-08 16:45:08,527 [Epoch: 1221 Step: 00081800] Batch Recognition Loss:   0.000206 => Gls Tokens per Sec:     1912 || Batch Translation Loss:   0.024252 => Txt Tokens per Sec:     5286 || Lr: 0.000025
2024-02-08 16:45:09,050 Epoch 1221: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.32 
2024-02-08 16:45:09,050 EPOCH 1222
2024-02-08 16:45:14,595 Epoch 1222: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.45 
2024-02-08 16:45:14,596 EPOCH 1223
2024-02-08 16:45:16,669 [Epoch: 1223 Step: 00081900] Batch Recognition Loss:   0.000259 => Gls Tokens per Sec:     2008 || Batch Translation Loss:   0.022627 => Txt Tokens per Sec:     5782 || Lr: 0.000025
2024-02-08 16:45:19,801 Epoch 1223: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.02 
2024-02-08 16:45:19,801 EPOCH 1224
2024-02-08 16:45:24,897 [Epoch: 1224 Step: 00082000] Batch Recognition Loss:   0.000168 => Gls Tokens per Sec:     1833 || Batch Translation Loss:   0.017958 => Txt Tokens per Sec:     5095 || Lr: 0.000025
2024-02-08 16:45:33,580 Validation result at epoch 1224, step    82000: duration: 8.6820s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.27538	Translation Loss: 93018.75781	PPL: 10837.05566
	Eval Metric: BLEU
	WER 3.11	(DEL: 0.00,	INS: 0.00,	SUB: 3.11)
	BLEU-4 0.60	(BLEU-1: 10.28,	BLEU-2: 3.17,	BLEU-3: 1.22,	BLEU-4: 0.60)
	CHRF 16.69	ROUGE 8.71
2024-02-08 16:45:33,581 Logging Recognition and Translation Outputs
2024-02-08 16:45:33,581 ========================================================================================================================
2024-02-08 16:45:33,581 Logging Sequence: 96_203.00
2024-02-08 16:45:33,581 	Gloss Reference :	A B+C+D+E
2024-02-08 16:45:33,581 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:45:33,582 	Gloss Alignment :	         
2024-02-08 16:45:33,582 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:45:33,583 	Text Reference  :	hardik pandya who scored 33 runs in just 17  balls  was     given the   man   of   the match
2024-02-08 16:45:33,583 	Text Hypothesis :	hardik ****** *** ****** ** **** ** **** and natasa decided to    renew their vows in  2019 
2024-02-08 16:45:33,583 	Text Alignment  :	       D      D   D      D  D    D  D    S   S      S       S     S     S     S    S   S    
2024-02-08 16:45:33,583 ========================================================================================================================
2024-02-08 16:45:33,584 Logging Sequence: 115_44.00
2024-02-08 16:45:33,584 	Gloss Reference :	A B+C+D+E
2024-02-08 16:45:33,584 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:45:33,584 	Gloss Alignment :	         
2024-02-08 16:45:33,584 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:45:33,585 	Text Reference  :	** the *** ***** image was of   his marriage to          sanjana   ganesan
2024-02-08 16:45:33,585 	Text Hypothesis :	on the 4th match he    was held in  a        traditional gurudwara wedding
2024-02-08 16:45:33,585 	Text Alignment  :	I      I   I     S         S    S   S        S           S         S      
2024-02-08 16:45:33,585 ========================================================================================================================
2024-02-08 16:45:33,585 Logging Sequence: 83_57.00
2024-02-08 16:45:33,586 	Gloss Reference :	A B+C+D+E
2024-02-08 16:45:33,586 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:45:33,586 	Gloss Alignment :	         
2024-02-08 16:45:33,586 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:45:33,587 	Text Reference  :	collapsed face first on  the field  he was     completely unconscious
2024-02-08 16:45:33,587 	Text Hypothesis :	********* **** a     lot of  people is talking about      cricket    
2024-02-08 16:45:33,587 	Text Alignment  :	D         D    S     S   S   S      S  S       S          S          
2024-02-08 16:45:33,587 ========================================================================================================================
2024-02-08 16:45:33,587 Logging Sequence: 65_48.00
2024-02-08 16:45:33,588 	Gloss Reference :	A B+C+D+E
2024-02-08 16:45:33,588 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:45:33,588 	Gloss Alignment :	         
2024-02-08 16:45:33,588 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:45:33,590 	Text Reference  :	after which instead of returning to         india and  then  going to    west indies the team directly flew to west indies    
2024-02-08 16:45:33,590 	Text Hypothesis :	***** the   final   of the       tournament was   held every 4     years but  they   did not  take     part in the  tournament
2024-02-08 16:45:33,590 	Text Alignment  :	D     S     S          S         S          S     S    S     S     S     S    S      S   S    S        S    S  S    S         
2024-02-08 16:45:33,590 ========================================================================================================================
2024-02-08 16:45:33,591 Logging Sequence: 120_41.00
2024-02-08 16:45:33,591 	Gloss Reference :	A B+C+D+E
2024-02-08 16:45:33,591 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:45:33,591 	Gloss Alignment :	         
2024-02-08 16:45:33,591 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:45:33,593 	Text Reference  :	got infected with covid  and was   admitted to        the hospital on  26   may 2       days after her husband
2024-02-08 16:45:33,593 	Text Hypothesis :	*** ******** he   played 18  tests on       instagram and save     its upto a   whereas they will  be  played 
2024-02-08 16:45:33,593 	Text Alignment  :	D   D        S    S      S   S     S        S         S   S        S   S    S   S       S    S     S   S      
2024-02-08 16:45:33,593 ========================================================================================================================
2024-02-08 16:45:34,232 Epoch 1224: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.29 
2024-02-08 16:45:34,232 EPOCH 1225
2024-02-08 16:45:39,883 Epoch 1225: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.17 
2024-02-08 16:45:39,884 EPOCH 1226
2024-02-08 16:45:41,852 [Epoch: 1226 Step: 00082100] Batch Recognition Loss:   0.000345 => Gls Tokens per Sec:     2034 || Batch Translation Loss:   0.098704 => Txt Tokens per Sec:     5417 || Lr: 0.000025
2024-02-08 16:45:45,330 Epoch 1226: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.59 
2024-02-08 16:45:45,330 EPOCH 1227
2024-02-08 16:45:49,947 [Epoch: 1227 Step: 00082200] Batch Recognition Loss:   0.000294 => Gls Tokens per Sec:     1989 || Batch Translation Loss:   0.027236 => Txt Tokens per Sec:     5517 || Lr: 0.000025
2024-02-08 16:45:50,707 Epoch 1227: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.69 
2024-02-08 16:45:50,707 EPOCH 1228
2024-02-08 16:45:56,207 Epoch 1228: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.37 
2024-02-08 16:45:56,208 EPOCH 1229
2024-02-08 16:45:58,048 [Epoch: 1229 Step: 00082300] Batch Recognition Loss:   0.000106 => Gls Tokens per Sec:     2034 || Batch Translation Loss:   0.019638 => Txt Tokens per Sec:     5508 || Lr: 0.000025
2024-02-08 16:46:01,548 Epoch 1229: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.62 
2024-02-08 16:46:01,549 EPOCH 1230
2024-02-08 16:46:06,488 [Epoch: 1230 Step: 00082400] Batch Recognition Loss:   0.000165 => Gls Tokens per Sec:     1827 || Batch Translation Loss:   0.028181 => Txt Tokens per Sec:     5088 || Lr: 0.000025
2024-02-08 16:46:07,219 Epoch 1230: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.52 
2024-02-08 16:46:07,219 EPOCH 1231
2024-02-08 16:46:12,512 Epoch 1231: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.50 
2024-02-08 16:46:12,513 EPOCH 1232
2024-02-08 16:46:14,269 [Epoch: 1232 Step: 00082500] Batch Recognition Loss:   0.000306 => Gls Tokens per Sec:     2097 || Batch Translation Loss:   0.050226 => Txt Tokens per Sec:     5834 || Lr: 0.000025
2024-02-08 16:46:18,098 Epoch 1232: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.25 
2024-02-08 16:46:18,098 EPOCH 1233
2024-02-08 16:46:22,464 [Epoch: 1233 Step: 00082600] Batch Recognition Loss:   0.000147 => Gls Tokens per Sec:     2030 || Batch Translation Loss:   0.017598 => Txt Tokens per Sec:     5625 || Lr: 0.000025
2024-02-08 16:46:23,472 Epoch 1233: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.20 
2024-02-08 16:46:23,473 EPOCH 1234
2024-02-08 16:46:29,047 Epoch 1234: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.26 
2024-02-08 16:46:29,047 EPOCH 1235
2024-02-08 16:46:30,584 [Epoch: 1235 Step: 00082700] Batch Recognition Loss:   0.000149 => Gls Tokens per Sec:     2292 || Batch Translation Loss:   0.015320 => Txt Tokens per Sec:     6118 || Lr: 0.000025
2024-02-08 16:46:34,309 Epoch 1235: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.02 
2024-02-08 16:46:34,310 EPOCH 1236
2024-02-08 16:46:38,805 [Epoch: 1236 Step: 00082800] Batch Recognition Loss:   0.000127 => Gls Tokens per Sec:     1959 || Batch Translation Loss:   0.016665 => Txt Tokens per Sec:     5465 || Lr: 0.000025
2024-02-08 16:46:39,711 Epoch 1236: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.14 
2024-02-08 16:46:39,711 EPOCH 1237
2024-02-08 16:46:45,011 Epoch 1237: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.98 
2024-02-08 16:46:45,012 EPOCH 1238
2024-02-08 16:46:46,844 [Epoch: 1238 Step: 00082900] Batch Recognition Loss:   0.000190 => Gls Tokens per Sec:     1780 || Batch Translation Loss:   0.015957 => Txt Tokens per Sec:     5029 || Lr: 0.000025
2024-02-08 16:46:50,579 Epoch 1238: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.06 
2024-02-08 16:46:50,580 EPOCH 1239
2024-02-08 16:46:54,906 [Epoch: 1239 Step: 00083000] Batch Recognition Loss:   0.000149 => Gls Tokens per Sec:     1975 || Batch Translation Loss:   0.017619 => Txt Tokens per Sec:     5494 || Lr: 0.000025
2024-02-08 16:46:55,929 Epoch 1239: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.58 
2024-02-08 16:46:55,930 EPOCH 1240
2024-02-08 16:47:01,392 Epoch 1240: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.10 
2024-02-08 16:47:01,393 EPOCH 1241
2024-02-08 16:47:02,948 [Epoch: 1241 Step: 00083100] Batch Recognition Loss:   0.001220 => Gls Tokens per Sec:     2059 || Batch Translation Loss:   0.044286 => Txt Tokens per Sec:     5786 || Lr: 0.000025
2024-02-08 16:47:06,693 Epoch 1241: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.22 
2024-02-08 16:47:06,694 EPOCH 1242
2024-02-08 16:47:11,196 [Epoch: 1242 Step: 00083200] Batch Recognition Loss:   0.000133 => Gls Tokens per Sec:     1862 || Batch Translation Loss:   0.012426 => Txt Tokens per Sec:     5170 || Lr: 0.000025
2024-02-08 16:47:12,238 Epoch 1242: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.28 
2024-02-08 16:47:12,238 EPOCH 1243
2024-02-08 16:47:17,634 Epoch 1243: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.31 
2024-02-08 16:47:17,634 EPOCH 1244
2024-02-08 16:47:19,127 [Epoch: 1244 Step: 00083300] Batch Recognition Loss:   0.000150 => Gls Tokens per Sec:     2038 || Batch Translation Loss:   0.019745 => Txt Tokens per Sec:     5503 || Lr: 0.000025
2024-02-08 16:47:23,043 Epoch 1244: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.47 
2024-02-08 16:47:23,043 EPOCH 1245
2024-02-08 16:47:26,926 [Epoch: 1245 Step: 00083400] Batch Recognition Loss:   0.000199 => Gls Tokens per Sec:     2117 || Batch Translation Loss:   0.022083 => Txt Tokens per Sec:     5860 || Lr: 0.000025
2024-02-08 16:47:28,221 Epoch 1245: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.86 
2024-02-08 16:47:28,221 EPOCH 1246
2024-02-08 16:47:33,776 Epoch 1246: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.39 
2024-02-08 16:47:33,776 EPOCH 1247
2024-02-08 16:47:35,421 [Epoch: 1247 Step: 00083500] Batch Recognition Loss:   0.000471 => Gls Tokens per Sec:     1691 || Batch Translation Loss:   0.008450 => Txt Tokens per Sec:     4891 || Lr: 0.000025
2024-02-08 16:47:39,134 Epoch 1247: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.24 
2024-02-08 16:47:39,134 EPOCH 1248
2024-02-08 16:47:43,423 [Epoch: 1248 Step: 00083600] Batch Recognition Loss:   0.000169 => Gls Tokens per Sec:     1880 || Batch Translation Loss:   0.009364 => Txt Tokens per Sec:     5219 || Lr: 0.000025
2024-02-08 16:47:44,563 Epoch 1248: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.14 
2024-02-08 16:47:44,563 EPOCH 1249
2024-02-08 16:47:49,496 Epoch 1249: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.91 
2024-02-08 16:47:49,496 EPOCH 1250
2024-02-08 16:47:50,810 [Epoch: 1250 Step: 00083700] Batch Recognition Loss:   0.000229 => Gls Tokens per Sec:     2072 || Batch Translation Loss:   0.019816 => Txt Tokens per Sec:     5736 || Lr: 0.000025
2024-02-08 16:47:54,942 Epoch 1250: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.38 
2024-02-08 16:47:54,942 EPOCH 1251
2024-02-08 16:47:59,016 [Epoch: 1251 Step: 00083800] Batch Recognition Loss:   0.000168 => Gls Tokens per Sec:     1940 || Batch Translation Loss:   0.013069 => Txt Tokens per Sec:     5444 || Lr: 0.000025
2024-02-08 16:48:00,454 Epoch 1251: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.64 
2024-02-08 16:48:00,454 EPOCH 1252
2024-02-08 16:48:06,101 Epoch 1252: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.72 
2024-02-08 16:48:06,101 EPOCH 1253
2024-02-08 16:48:07,211 [Epoch: 1253 Step: 00083900] Batch Recognition Loss:   0.000207 => Gls Tokens per Sec:     2308 || Batch Translation Loss:   0.020269 => Txt Tokens per Sec:     6195 || Lr: 0.000025
2024-02-08 16:48:11,472 Epoch 1253: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.31 
2024-02-08 16:48:11,472 EPOCH 1254
2024-02-08 16:48:15,484 [Epoch: 1254 Step: 00084000] Batch Recognition Loss:   0.000114 => Gls Tokens per Sec:     1930 || Batch Translation Loss:   0.013286 => Txt Tokens per Sec:     5246 || Lr: 0.000025
2024-02-08 16:48:23,882 Validation result at epoch 1254, step    84000: duration: 8.3970s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.47259	Translation Loss: 93332.81250	PPL: 11182.37305
	Eval Metric: BLEU
	WER 3.18	(DEL: 0.00,	INS: 0.00,	SUB: 3.18)
	BLEU-4 0.69	(BLEU-1: 10.31,	BLEU-2: 3.50,	BLEU-3: 1.39,	BLEU-4: 0.69)
	CHRF 16.66	ROUGE 8.92
2024-02-08 16:48:23,883 Logging Recognition and Translation Outputs
2024-02-08 16:48:23,883 ========================================================================================================================
2024-02-08 16:48:23,883 Logging Sequence: 113_196.00
2024-02-08 16:48:23,883 	Gloss Reference :	A B+C+D+E
2024-02-08 16:48:23,884 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:48:23,884 	Gloss Alignment :	         
2024-02-08 16:48:23,884 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:48:23,885 	Text Reference  :	infidelity being the       reason  for separation is just a       rumour sad   
2024-02-08 16:48:23,885 	Text Hypothesis :	everyone   was   excitedly waiting for ********** ** the  auction in     mumbai
2024-02-08 16:48:23,885 	Text Alignment  :	S          S     S         S           D          D  S    S       S      S     
2024-02-08 16:48:23,885 ========================================================================================================================
2024-02-08 16:48:23,885 Logging Sequence: 138_48.00
2024-02-08 16:48:23,885 	Gloss Reference :	A B+C+D+E
2024-02-08 16:48:23,885 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:48:23,885 	Gloss Alignment :	         
2024-02-08 16:48:23,886 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:48:23,887 	Text Reference  :	******* *** ****** **** ** *** ***** similarly jadon sancho and        bukayo saka's shot    was blocked by the      goalkeeper
2024-02-08 16:48:23,887 	Text Hypothesis :	however the second half of the match went      viral a      five-match test   series against was ******* a  software engineer  
2024-02-08 16:48:23,887 	Text Alignment  :	I       I   I      I    I  I   I     S         S     S      S          S      S      S           D       S  S        S         
2024-02-08 16:48:23,887 ========================================================================================================================
2024-02-08 16:48:23,887 Logging Sequence: 180_114.00
2024-02-08 16:48:23,888 	Gloss Reference :	A B+C+D+E
2024-02-08 16:48:23,888 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:48:23,888 	Gloss Alignment :	         
2024-02-08 16:48:23,888 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:48:23,890 	Text Reference  :	the wrestlers then wrote a         complaint letter to      the indian olympic association ioa president pt    usha
2024-02-08 16:48:23,890 	Text Hypothesis :	he  said      it   was   wonderful by        your   victory and death  for     a           lot of        weeks ago 
2024-02-08 16:48:23,890 	Text Alignment  :	S   S         S    S     S         S         S      S       S   S      S       S           S   S         S     S   
2024-02-08 16:48:23,890 ========================================================================================================================
2024-02-08 16:48:23,890 Logging Sequence: 126_163.00
2024-02-08 16:48:23,890 	Gloss Reference :	A B+C+D+E
2024-02-08 16:48:23,890 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:48:23,891 	Gloss Alignment :	         
2024-02-08 16:48:23,891 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:48:23,891 	Text Reference  :	your hard work has       helped secure a         medal   at   the   tokyo olympics
2024-02-08 16:48:23,891 	Text Hypothesis :	**** **** **** harbhajan is     an     excellent spinner much loved by    all     
2024-02-08 16:48:23,892 	Text Alignment  :	D    D    D    S         S      S      S         S       S    S     S     S       
2024-02-08 16:48:23,892 ========================================================================================================================
2024-02-08 16:48:23,892 Logging Sequence: 169_165.00
2024-02-08 16:48:23,892 	Gloss Reference :	A B+C+D+E
2024-02-08 16:48:23,892 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:48:23,892 	Gloss Alignment :	         
2024-02-08 16:48:23,892 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:48:23,894 	Text Reference  :	* ***** ** ** the     indian government was    outraged by  the  incident   and ** *** these changes were undone    by  wikipedia
2024-02-08 16:48:23,895 	Text Hypothesis :	a total of 22 matches will   be         played at       par with spectators and is why they  did     not  revealing her face     
2024-02-08 16:48:23,895 	Text Alignment  :	I I     I  I  S       S      S          S      S        S   S    S              I  I   S     S       S    S         S   S        
2024-02-08 16:48:23,895 ========================================================================================================================
2024-02-08 16:48:25,368 Epoch 1254: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.32 
2024-02-08 16:48:25,368 EPOCH 1255
2024-02-08 16:48:31,011 Epoch 1255: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.08 
2024-02-08 16:48:31,012 EPOCH 1256
2024-02-08 16:48:32,093 [Epoch: 1256 Step: 00084100] Batch Recognition Loss:   0.000147 => Gls Tokens per Sec:     2222 || Batch Translation Loss:   0.010121 => Txt Tokens per Sec:     5780 || Lr: 0.000025
2024-02-08 16:48:36,536 Epoch 1256: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.03 
2024-02-08 16:48:36,536 EPOCH 1257
2024-02-08 16:48:40,515 [Epoch: 1257 Step: 00084200] Batch Recognition Loss:   0.000284 => Gls Tokens per Sec:     1905 || Batch Translation Loss:   0.023515 => Txt Tokens per Sec:     5246 || Lr: 0.000025
2024-02-08 16:48:41,965 Epoch 1257: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.08 
2024-02-08 16:48:41,965 EPOCH 1258
2024-02-08 16:48:47,407 Epoch 1258: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.25 
2024-02-08 16:48:47,407 EPOCH 1259
2024-02-08 16:48:48,641 [Epoch: 1259 Step: 00084300] Batch Recognition Loss:   0.000224 => Gls Tokens per Sec:     1817 || Batch Translation Loss:   0.016885 => Txt Tokens per Sec:     5303 || Lr: 0.000025
2024-02-08 16:48:52,750 Epoch 1259: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.42 
2024-02-08 16:48:52,751 EPOCH 1260
2024-02-08 16:48:56,800 [Epoch: 1260 Step: 00084400] Batch Recognition Loss:   0.009710 => Gls Tokens per Sec:     1833 || Batch Translation Loss:   0.026561 => Txt Tokens per Sec:     5120 || Lr: 0.000025
2024-02-08 16:48:58,526 Epoch 1260: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.55 
2024-02-08 16:48:58,527 EPOCH 1261
2024-02-08 16:49:03,848 Epoch 1261: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.16 
2024-02-08 16:49:03,848 EPOCH 1262
2024-02-08 16:49:04,919 [Epoch: 1262 Step: 00084500] Batch Recognition Loss:   0.000278 => Gls Tokens per Sec:     1946 || Batch Translation Loss:   0.019003 => Txt Tokens per Sec:     5696 || Lr: 0.000025
2024-02-08 16:49:09,446 Epoch 1262: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.24 
2024-02-08 16:49:09,447 EPOCH 1263
2024-02-08 16:49:13,163 [Epoch: 1263 Step: 00084600] Batch Recognition Loss:   0.000161 => Gls Tokens per Sec:     1954 || Batch Translation Loss:   0.017664 => Txt Tokens per Sec:     5541 || Lr: 0.000025
2024-02-08 16:49:14,739 Epoch 1263: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.64 
2024-02-08 16:49:14,740 EPOCH 1264
2024-02-08 16:49:20,329 Epoch 1264: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.61 
2024-02-08 16:49:20,330 EPOCH 1265
2024-02-08 16:49:21,134 [Epoch: 1265 Step: 00084700] Batch Recognition Loss:   0.000169 => Gls Tokens per Sec:     2391 || Batch Translation Loss:   0.009732 => Txt Tokens per Sec:     5472 || Lr: 0.000025
2024-02-08 16:49:25,787 Epoch 1265: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.57 
2024-02-08 16:49:25,788 EPOCH 1266
2024-02-08 16:49:29,447 [Epoch: 1266 Step: 00084800] Batch Recognition Loss:   0.001549 => Gls Tokens per Sec:     1941 || Batch Translation Loss:   0.033432 => Txt Tokens per Sec:     5371 || Lr: 0.000025
2024-02-08 16:49:31,230 Epoch 1266: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.71 
2024-02-08 16:49:31,230 EPOCH 1267
2024-02-08 16:49:36,860 Epoch 1267: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.55 
2024-02-08 16:49:36,860 EPOCH 1268
2024-02-08 16:49:37,767 [Epoch: 1268 Step: 00084900] Batch Recognition Loss:   0.000167 => Gls Tokens per Sec:     1943 || Batch Translation Loss:   0.010438 => Txt Tokens per Sec:     5406 || Lr: 0.000025
2024-02-08 16:49:42,521 Epoch 1268: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.35 
2024-02-08 16:49:42,521 EPOCH 1269
2024-02-08 16:49:45,772 [Epoch: 1269 Step: 00085000] Batch Recognition Loss:   0.000192 => Gls Tokens per Sec:     2166 || Batch Translation Loss:   0.033511 => Txt Tokens per Sec:     5975 || Lr: 0.000025
2024-02-08 16:49:48,038 Epoch 1269: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.79 
2024-02-08 16:49:48,039 EPOCH 1270
2024-02-08 16:49:53,531 Epoch 1270: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.42 
2024-02-08 16:49:53,532 EPOCH 1271
2024-02-08 16:49:54,301 [Epoch: 1271 Step: 00085100] Batch Recognition Loss:   0.000202 => Gls Tokens per Sec:     1953 || Batch Translation Loss:   0.022294 => Txt Tokens per Sec:     5224 || Lr: 0.000025
2024-02-08 16:49:58,758 Epoch 1271: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.65 
2024-02-08 16:49:58,759 EPOCH 1272
2024-02-08 16:50:02,479 [Epoch: 1272 Step: 00085200] Batch Recognition Loss:   0.000189 => Gls Tokens per Sec:     1823 || Batch Translation Loss:   0.010087 => Txt Tokens per Sec:     5120 || Lr: 0.000025
2024-02-08 16:50:04,361 Epoch 1272: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.23 
2024-02-08 16:50:04,362 EPOCH 1273
2024-02-08 16:50:09,773 Epoch 1273: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.23 
2024-02-08 16:50:09,774 EPOCH 1274
2024-02-08 16:50:10,456 [Epoch: 1274 Step: 00085300] Batch Recognition Loss:   0.000188 => Gls Tokens per Sec:     2115 || Batch Translation Loss:   0.017059 => Txt Tokens per Sec:     5787 || Lr: 0.000025
2024-02-08 16:50:15,283 Epoch 1274: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.17 
2024-02-08 16:50:15,284 EPOCH 1275
2024-02-08 16:50:18,285 [Epoch: 1275 Step: 00085400] Batch Recognition Loss:   0.000316 => Gls Tokens per Sec:     2207 || Batch Translation Loss:   0.056232 => Txt Tokens per Sec:     6119 || Lr: 0.000025
2024-02-08 16:50:20,468 Epoch 1275: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.20 
2024-02-08 16:50:20,469 EPOCH 1276
2024-02-08 16:50:25,915 Epoch 1276: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.22 
2024-02-08 16:50:25,915 EPOCH 1277
2024-02-08 16:50:26,682 [Epoch: 1277 Step: 00085500] Batch Recognition Loss:   0.000167 => Gls Tokens per Sec:     1671 || Batch Translation Loss:   0.012947 => Txt Tokens per Sec:     5149 || Lr: 0.000025
2024-02-08 16:50:31,225 Epoch 1277: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.15 
2024-02-08 16:50:31,225 EPOCH 1278
2024-02-08 16:50:34,629 [Epoch: 1278 Step: 00085600] Batch Recognition Loss:   0.000182 => Gls Tokens per Sec:     1898 || Batch Translation Loss:   0.025811 => Txt Tokens per Sec:     5309 || Lr: 0.000025
2024-02-08 16:50:36,782 Epoch 1278: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.23 
2024-02-08 16:50:36,782 EPOCH 1279
2024-02-08 16:50:41,754 Epoch 1279: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.31 
2024-02-08 16:50:41,755 EPOCH 1280
2024-02-08 16:50:42,355 [Epoch: 1280 Step: 00085700] Batch Recognition Loss:   0.000169 => Gls Tokens per Sec:     1873 || Batch Translation Loss:   0.009501 => Txt Tokens per Sec:     5164 || Lr: 0.000025
2024-02-08 16:50:47,205 Epoch 1280: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.17 
2024-02-08 16:50:47,206 EPOCH 1281
2024-02-08 16:50:50,074 [Epoch: 1281 Step: 00085800] Batch Recognition Loss:   0.000150 => Gls Tokens per Sec:     2232 || Batch Translation Loss:   0.015080 => Txt Tokens per Sec:     5950 || Lr: 0.000025
2024-02-08 16:50:52,614 Epoch 1281: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.05 
2024-02-08 16:50:52,615 EPOCH 1282
2024-02-08 16:50:58,151 Epoch 1282: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.18 
2024-02-08 16:50:58,151 EPOCH 1283
2024-02-08 16:50:58,611 [Epoch: 1283 Step: 00085900] Batch Recognition Loss:   0.000199 => Gls Tokens per Sec:     2096 || Batch Translation Loss:   0.014276 => Txt Tokens per Sec:     5782 || Lr: 0.000025
2024-02-08 16:51:03,372 Epoch 1283: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.61 
2024-02-08 16:51:03,372 EPOCH 1284
2024-02-08 16:51:06,646 [Epoch: 1284 Step: 00086000] Batch Recognition Loss:   0.000188 => Gls Tokens per Sec:     1876 || Batch Translation Loss:   0.024536 => Txt Tokens per Sec:     5119 || Lr: 0.000025
2024-02-08 16:51:15,161 Validation result at epoch 1284, step    86000: duration: 8.5140s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.43700	Translation Loss: 93002.00781	PPL: 10818.94336
	Eval Metric: BLEU
	WER 2.90	(DEL: 0.00,	INS: 0.00,	SUB: 2.90)
	BLEU-4 0.70	(BLEU-1: 11.08,	BLEU-2: 3.48,	BLEU-3: 1.35,	BLEU-4: 0.70)
	CHRF 17.06	ROUGE 9.29
2024-02-08 16:51:15,162 Logging Recognition and Translation Outputs
2024-02-08 16:51:15,162 ========================================================================================================================
2024-02-08 16:51:15,162 Logging Sequence: 161_52.00
2024-02-08 16:51:15,163 	Gloss Reference :	A B+C+D+E
2024-02-08 16:51:15,163 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:51:15,163 	Gloss Alignment :	         
2024-02-08 16:51:15,163 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:51:15,164 	Text Reference  :	the next day  on      15th january 2022 virat kohli      posted a       tweet saying
2024-02-08 16:51:15,164 	Text Hypothesis :	*** **** star batsmen and  bowlers of   the   tournament while  playing the   match 
2024-02-08 16:51:15,164 	Text Alignment  :	D   D    S    S       S    S       S    S     S          S      S       S     S     
2024-02-08 16:51:15,164 ========================================================================================================================
2024-02-08 16:51:15,164 Logging Sequence: 127_140.00
2024-02-08 16:51:15,165 	Gloss Reference :	A B+C+D+E
2024-02-08 16:51:15,165 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:51:15,165 	Gloss Alignment :	         
2024-02-08 16:51:15,165 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:51:15,167 	Text Reference  :	this is india' 3rd   medal in  the world athletics championships he   is  very  talented  and his performance is     highly impressive
2024-02-08 16:51:15,167 	Text Hypothesis :	**** ** ****** india had   won the ***** ********* match         with 263 balls remaining and *** without     losing any    wicket    
2024-02-08 16:51:15,167 	Text Alignment  :	D    D  D      S     S     S       D     D         S             S    S   S     S             D   S           S      S      S         
2024-02-08 16:51:15,167 ========================================================================================================================
2024-02-08 16:51:15,167 Logging Sequence: 104_110.00
2024-02-08 16:51:15,167 	Gloss Reference :	A B+C+D+E
2024-02-08 16:51:15,167 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:51:15,168 	Gloss Alignment :	         
2024-02-08 16:51:15,168 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:51:15,169 	Text Reference  :	however remember though praggnanandhaa stood second he   achieved this at a   young age         of 18  
2024-02-08 16:51:15,169 	Text Hypothesis :	******* ******** ****** ************** the   then   went to       this ** was very  heartbroken to this
2024-02-08 16:51:15,169 	Text Alignment  :	D       D        D      D              S     S      S    S             D  S   S     S           S  S   
2024-02-08 16:51:15,169 ========================================================================================================================
2024-02-08 16:51:15,169 Logging Sequence: 164_412.00
2024-02-08 16:51:15,170 	Gloss Reference :	A B+C+D+E
2024-02-08 16:51:15,170 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:51:15,170 	Gloss Alignment :	         
2024-02-08 16:51:15,170 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:51:15,172 	Text Reference  :	if you divide these two figures you will be shocked to    know that  each ball's worth is  rs      50  lakhs  
2024-02-08 16:51:15,172 	Text Hypothesis :	** *** ****** ***** *** ******* but will be in      front the  media and  media  for   his privacy was invaded
2024-02-08 16:51:15,172 	Text Alignment  :	D  D   D      D     D   D       S           S       S     S    S     S    S      S     S   S       S   S      
2024-02-08 16:51:15,172 ========================================================================================================================
2024-02-08 16:51:15,172 Logging Sequence: 154_2.00
2024-02-08 16:51:15,172 	Gloss Reference :	A B+C+D+E
2024-02-08 16:51:15,172 	Gloss Hypothesis:	A B+C+D  
2024-02-08 16:51:15,173 	Gloss Alignment :	  S      
2024-02-08 16:51:15,173 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:51:15,173 	Text Reference  :	****** ******* **** in      june the ******* icc   had given the        bcci  
2024-02-08 16:51:15,173 	Text Hypothesis :	indian cricket team members of   the wedding going to  their respective medals
2024-02-08 16:51:15,174 	Text Alignment  :	I      I       I    S       S        I       S     S   S     S          S     
2024-02-08 16:51:15,174 ========================================================================================================================
2024-02-08 16:51:17,398 Epoch 1284: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.23 
2024-02-08 16:51:17,399 EPOCH 1285
2024-02-08 16:51:22,732 Epoch 1285: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.68 
2024-02-08 16:51:22,732 EPOCH 1286
2024-02-08 16:51:23,112 [Epoch: 1286 Step: 00086100] Batch Recognition Loss:   0.000138 => Gls Tokens per Sec:     2111 || Batch Translation Loss:   0.010175 => Txt Tokens per Sec:     5868 || Lr: 0.000025
2024-02-08 16:51:28,374 Epoch 1286: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.01 
2024-02-08 16:51:28,375 EPOCH 1287
2024-02-08 16:51:31,352 [Epoch: 1287 Step: 00086200] Batch Recognition Loss:   0.000188 => Gls Tokens per Sec:     2010 || Batch Translation Loss:   0.019273 => Txt Tokens per Sec:     5433 || Lr: 0.000025
2024-02-08 16:51:33,666 Epoch 1287: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.61 
2024-02-08 16:51:33,666 EPOCH 1288
2024-02-08 16:51:39,013 Epoch 1288: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.34 
2024-02-08 16:51:39,014 EPOCH 1289
2024-02-08 16:51:39,571 [Epoch: 1289 Step: 00086300] Batch Recognition Loss:   0.000283 => Gls Tokens per Sec:     1153 || Batch Translation Loss:   0.022624 => Txt Tokens per Sec:     3822 || Lr: 0.000025
2024-02-08 16:51:44,371 Epoch 1289: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.54 
2024-02-08 16:51:44,371 EPOCH 1290
2024-02-08 16:51:47,625 [Epoch: 1290 Step: 00086400] Batch Recognition Loss:   0.000133 => Gls Tokens per Sec:     1789 || Batch Translation Loss:   0.020524 => Txt Tokens per Sec:     5188 || Lr: 0.000025
2024-02-08 16:51:49,940 Epoch 1290: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.35 
2024-02-08 16:51:49,941 EPOCH 1291
2024-02-08 16:51:55,376 Epoch 1291: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.32 
2024-02-08 16:51:55,376 EPOCH 1292
2024-02-08 16:51:55,667 [Epoch: 1292 Step: 00086500] Batch Recognition Loss:   0.000146 => Gls Tokens per Sec:     1661 || Batch Translation Loss:   0.016597 => Txt Tokens per Sec:     4516 || Lr: 0.000025
2024-02-08 16:52:00,903 Epoch 1292: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.76 
2024-02-08 16:52:00,904 EPOCH 1293
2024-02-08 16:52:03,660 [Epoch: 1293 Step: 00086600] Batch Recognition Loss:   0.000245 => Gls Tokens per Sec:     2054 || Batch Translation Loss:   0.019029 => Txt Tokens per Sec:     5741 || Lr: 0.000025
2024-02-08 16:52:06,243 Epoch 1293: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.87 
2024-02-08 16:52:06,244 EPOCH 1294
2024-02-08 16:52:11,757 Epoch 1294: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.22 
2024-02-08 16:52:11,757 EPOCH 1295
2024-02-08 16:52:11,916 [Epoch: 1295 Step: 00086700] Batch Recognition Loss:   0.000133 => Gls Tokens per Sec:     2025 || Batch Translation Loss:   0.011829 => Txt Tokens per Sec:     5608 || Lr: 0.000025
2024-02-08 16:52:17,302 Epoch 1295: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.08 
2024-02-08 16:52:17,303 EPOCH 1296
2024-02-08 16:52:19,931 [Epoch: 1296 Step: 00086800] Batch Recognition Loss:   0.000275 => Gls Tokens per Sec:     2132 || Batch Translation Loss:   0.018428 => Txt Tokens per Sec:     5807 || Lr: 0.000025
2024-02-08 16:52:22,742 Epoch 1296: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.11 
2024-02-08 16:52:22,743 EPOCH 1297
2024-02-08 16:52:27,968 Epoch 1297: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.32 
2024-02-08 16:52:27,968 EPOCH 1298
2024-02-08 16:52:28,138 [Epoch: 1298 Step: 00086900] Batch Recognition Loss:   0.001843 => Gls Tokens per Sec:      952 || Batch Translation Loss:   0.022280 => Txt Tokens per Sec:     3280 || Lr: 0.000025
2024-02-08 16:52:33,480 Epoch 1298: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.60 
2024-02-08 16:52:33,481 EPOCH 1299
2024-02-08 16:52:36,041 [Epoch: 1299 Step: 00087000] Batch Recognition Loss:   0.000290 => Gls Tokens per Sec:     2087 || Batch Translation Loss:   0.033713 => Txt Tokens per Sec:     5622 || Lr: 0.000025
2024-02-08 16:52:38,783 Epoch 1299: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.24 
2024-02-08 16:52:38,784 EPOCH 1300
2024-02-08 16:52:44,235 [Epoch: 1300 Step: 00087100] Batch Recognition Loss:   0.000117 => Gls Tokens per Sec:     1949 || Batch Translation Loss:   0.008697 => Txt Tokens per Sec:     5391 || Lr: 0.000025
2024-02-08 16:52:44,236 Epoch 1300: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.22 
2024-02-08 16:52:44,236 EPOCH 1301
2024-02-08 16:52:49,134 Epoch 1301: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.31 
2024-02-08 16:52:49,135 EPOCH 1302
2024-02-08 16:52:51,208 [Epoch: 1302 Step: 00087200] Batch Recognition Loss:   0.000205 => Gls Tokens per Sec:     2500 || Batch Translation Loss:   0.036736 => Txt Tokens per Sec:     6764 || Lr: 0.000025
2024-02-08 16:52:53,706 Epoch 1302: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.09 
2024-02-08 16:52:53,706 EPOCH 1303
2024-02-08 16:52:59,289 [Epoch: 1303 Step: 00087300] Batch Recognition Loss:   0.000194 => Gls Tokens per Sec:     1874 || Batch Translation Loss:   0.035374 => Txt Tokens per Sec:     5181 || Lr: 0.000025
2024-02-08 16:52:59,374 Epoch 1303: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.12 
2024-02-08 16:52:59,374 EPOCH 1304
2024-02-08 16:53:04,662 Epoch 1304: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.23 
2024-02-08 16:53:04,663 EPOCH 1305
2024-02-08 16:53:07,088 [Epoch: 1305 Step: 00087400] Batch Recognition Loss:   0.000122 => Gls Tokens per Sec:     2113 || Batch Translation Loss:   0.013601 => Txt Tokens per Sec:     5685 || Lr: 0.000025
2024-02-08 16:53:10,288 Epoch 1305: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.25 
2024-02-08 16:53:10,288 EPOCH 1306
2024-02-08 16:53:15,218 [Epoch: 1306 Step: 00087500] Batch Recognition Loss:   0.000222 => Gls Tokens per Sec:     2090 || Batch Translation Loss:   0.029281 => Txt Tokens per Sec:     5765 || Lr: 0.000025
2024-02-08 16:53:15,380 Epoch 1306: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.37 
2024-02-08 16:53:15,380 EPOCH 1307
2024-02-08 16:53:20,465 Epoch 1307: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.58 
2024-02-08 16:53:20,466 EPOCH 1308
2024-02-08 16:53:22,831 [Epoch: 1308 Step: 00087600] Batch Recognition Loss:   0.000260 => Gls Tokens per Sec:     2056 || Batch Translation Loss:   0.021149 => Txt Tokens per Sec:     5398 || Lr: 0.000025
2024-02-08 16:53:25,984 Epoch 1308: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.46 
2024-02-08 16:53:25,984 EPOCH 1309
2024-02-08 16:53:30,769 [Epoch: 1309 Step: 00087700] Batch Recognition Loss:   0.000250 => Gls Tokens per Sec:     2120 || Batch Translation Loss:   0.018973 => Txt Tokens per Sec:     5887 || Lr: 0.000025
2024-02-08 16:53:30,973 Epoch 1309: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.29 
2024-02-08 16:53:30,973 EPOCH 1310
2024-02-08 16:53:36,390 Epoch 1310: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.16 
2024-02-08 16:53:36,390 EPOCH 1311
2024-02-08 16:53:38,515 [Epoch: 1311 Step: 00087800] Batch Recognition Loss:   0.000225 => Gls Tokens per Sec:     2260 || Batch Translation Loss:   0.014174 => Txt Tokens per Sec:     6431 || Lr: 0.000025
2024-02-08 16:53:41,322 Epoch 1311: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.07 
2024-02-08 16:53:41,323 EPOCH 1312
2024-02-08 16:53:46,527 [Epoch: 1312 Step: 00087900] Batch Recognition Loss:   0.000169 => Gls Tokens per Sec:     1918 || Batch Translation Loss:   0.011308 => Txt Tokens per Sec:     5335 || Lr: 0.000025
2024-02-08 16:53:46,826 Epoch 1312: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.07 
2024-02-08 16:53:46,826 EPOCH 1313
2024-02-08 16:53:51,916 Epoch 1313: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.24 
2024-02-08 16:53:51,917 EPOCH 1314
2024-02-08 16:53:54,052 [Epoch: 1314 Step: 00088000] Batch Recognition Loss:   0.000650 => Gls Tokens per Sec:     2175 || Batch Translation Loss:   0.052902 => Txt Tokens per Sec:     5868 || Lr: 0.000025
2024-02-08 16:54:02,574 Validation result at epoch 1314, step    88000: duration: 8.5220s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.27728	Translation Loss: 93364.28906	PPL: 11217.58789
	Eval Metric: BLEU
	WER 2.97	(DEL: 0.00,	INS: 0.00,	SUB: 2.97)
	BLEU-4 0.57	(BLEU-1: 9.89,	BLEU-2: 3.15,	BLEU-3: 1.24,	BLEU-4: 0.57)
	CHRF 16.86	ROUGE 8.45
2024-02-08 16:54:02,575 Logging Recognition and Translation Outputs
2024-02-08 16:54:02,575 ========================================================================================================================
2024-02-08 16:54:02,575 Logging Sequence: 173_2.00
2024-02-08 16:54:02,576 	Gloss Reference :	A B+C+D+E
2024-02-08 16:54:02,576 	Gloss Hypothesis:	A B+C+D  
2024-02-08 16:54:02,576 	Gloss Alignment :	  S      
2024-02-08 16:54:02,576 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:54:02,577 	Text Reference  :	****** ****** *** ***** virat kohli has   captained india        in 45 t20 internationals matches and 95  odis 
2024-02-08 16:54:02,577 	Text Hypothesis :	krunal became the first set   to    score a         half-century in ** 26  deliveries     on      his odi debut
2024-02-08 16:54:02,578 	Text Alignment  :	I      I      I   I     S     S     S     S         S               D  S   S              S       S   S   S    
2024-02-08 16:54:02,578 ========================================================================================================================
2024-02-08 16:54:02,578 Logging Sequence: 93_134.00
2024-02-08 16:54:02,578 	Gloss Reference :	A B+C+D+E
2024-02-08 16:54:02,578 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:54:02,578 	Gloss Alignment :	         
2024-02-08 16:54:02,579 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:54:02,579 	Text Reference  :	he  even      hugged her    and kissed her    
2024-02-08 16:54:02,579 	Text Hypothesis :	the cricketer was    former and ****** england
2024-02-08 16:54:02,579 	Text Alignment  :	S   S         S      S          D      S      
2024-02-08 16:54:02,579 ========================================================================================================================
2024-02-08 16:54:02,579 Logging Sequence: 63_44.00
2024-02-08 16:54:02,580 	Gloss Reference :	A B+C+D+E
2024-02-08 16:54:02,580 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:54:02,580 	Gloss Alignment :	         
2024-02-08 16:54:02,580 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:54:02,581 	Text Reference  :	**** ********* **** ****** **** ******* *** the   amount for the form  is  non-refundable
2024-02-08 16:54:02,581 	Text Hypothesis :	many companies will submit such tenders are their teams  for the final and time          
2024-02-08 16:54:02,581 	Text Alignment  :	I    I         I    I      I    I       I   S     S              S     S   S             
2024-02-08 16:54:02,581 ========================================================================================================================
2024-02-08 16:54:02,581 Logging Sequence: 164_394.00
2024-02-08 16:54:02,581 	Gloss Reference :	A B+C+D+E
2024-02-08 16:54:02,582 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:54:02,582 	Gloss Alignment :	         
2024-02-08 16:54:02,582 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:54:02,583 	Text Reference  :	calculating will bring the       total to   98400 balls  in 5   years
2024-02-08 16:54:02,583 	Text Hypothesis :	*********** the  bcci  secretary jay   shah was   amazed by his bcci 
2024-02-08 16:54:02,583 	Text Alignment  :	D           S    S     S         S     S    S     S      S  S   S    
2024-02-08 16:54:02,583 ========================================================================================================================
2024-02-08 16:54:02,583 Logging Sequence: 65_77.00
2024-02-08 16:54:02,583 	Gloss Reference :	A B+C+D+E
2024-02-08 16:54:02,584 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:54:02,584 	Gloss Alignment :	         
2024-02-08 16:54:02,584 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:54:02,584 	Text Reference  :	*** *** ***** ***** *** **** ** indian team   travelling included 16 players
2024-02-08 16:54:02,585 	Text Hypothesis :	for the third match was held at the    iconic wankhede   stadium  in mumbai 
2024-02-08 16:54:02,585 	Text Alignment  :	I   I   I     I     I   I    I  S      S      S          S        S  S      
2024-02-08 16:54:02,585 ========================================================================================================================
2024-02-08 16:54:05,898 Epoch 1314: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.54 
2024-02-08 16:54:05,898 EPOCH 1315
2024-02-08 16:54:10,406 [Epoch: 1315 Step: 00088100] Batch Recognition Loss:   0.000363 => Gls Tokens per Sec:     2201 || Batch Translation Loss:   0.010027 => Txt Tokens per Sec:     6122 || Lr: 0.000025
2024-02-08 16:54:10,764 Epoch 1315: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.10 
2024-02-08 16:54:10,764 EPOCH 1316
2024-02-08 16:54:16,394 Epoch 1316: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.36 
2024-02-08 16:54:16,395 EPOCH 1317
2024-02-08 16:54:18,595 [Epoch: 1317 Step: 00088200] Batch Recognition Loss:   0.000248 => Gls Tokens per Sec:     2037 || Batch Translation Loss:   0.025367 => Txt Tokens per Sec:     5540 || Lr: 0.000025
2024-02-08 16:54:21,419 Epoch 1317: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.50 
2024-02-08 16:54:21,419 EPOCH 1318
2024-02-08 16:54:26,242 [Epoch: 1318 Step: 00088300] Batch Recognition Loss:   0.000223 => Gls Tokens per Sec:     2003 || Batch Translation Loss:   0.012241 => Txt Tokens per Sec:     5488 || Lr: 0.000025
2024-02-08 16:54:26,895 Epoch 1318: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.57 
2024-02-08 16:54:26,895 EPOCH 1319
2024-02-08 16:54:31,800 Epoch 1319: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.88 
2024-02-08 16:54:31,800 EPOCH 1320
2024-02-08 16:54:34,427 [Epoch: 1320 Step: 00088400] Batch Recognition Loss:   0.000776 => Gls Tokens per Sec:     1646 || Batch Translation Loss:   0.030491 => Txt Tokens per Sec:     4824 || Lr: 0.000025
2024-02-08 16:54:37,430 Epoch 1320: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.91 
2024-02-08 16:54:37,430 EPOCH 1321
2024-02-08 16:54:42,566 [Epoch: 1321 Step: 00088500] Batch Recognition Loss:   0.000466 => Gls Tokens per Sec:     1850 || Batch Translation Loss:   0.024782 => Txt Tokens per Sec:     5126 || Lr: 0.000025
2024-02-08 16:54:43,120 Epoch 1321: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.01 
2024-02-08 16:54:43,120 EPOCH 1322
2024-02-08 16:54:48,486 Epoch 1322: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.45 
2024-02-08 16:54:48,486 EPOCH 1323
2024-02-08 16:54:50,769 [Epoch: 1323 Step: 00088600] Batch Recognition Loss:   0.000393 => Gls Tokens per Sec:     1779 || Batch Translation Loss:   0.020293 => Txt Tokens per Sec:     5096 || Lr: 0.000025
2024-02-08 16:54:53,920 Epoch 1323: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.29 
2024-02-08 16:54:53,921 EPOCH 1324
2024-02-08 16:54:58,803 [Epoch: 1324 Step: 00088700] Batch Recognition Loss:   0.000159 => Gls Tokens per Sec:     1914 || Batch Translation Loss:   0.018160 => Txt Tokens per Sec:     5383 || Lr: 0.000025
2024-02-08 16:54:59,323 Epoch 1324: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.22 
2024-02-08 16:54:59,324 EPOCH 1325
2024-02-08 16:55:05,023 Epoch 1325: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.20 
2024-02-08 16:55:05,024 EPOCH 1326
2024-02-08 16:55:06,943 [Epoch: 1326 Step: 00088800] Batch Recognition Loss:   0.000147 => Gls Tokens per Sec:     2086 || Batch Translation Loss:   0.012411 => Txt Tokens per Sec:     5770 || Lr: 0.000025
2024-02-08 16:55:10,354 Epoch 1326: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.28 
2024-02-08 16:55:10,355 EPOCH 1327
2024-02-08 16:55:15,141 [Epoch: 1327 Step: 00088900] Batch Recognition Loss:   0.001729 => Gls Tokens per Sec:     1918 || Batch Translation Loss:   0.025153 => Txt Tokens per Sec:     5284 || Lr: 0.000025
2024-02-08 16:55:15,822 Epoch 1327: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.61 
2024-02-08 16:55:15,822 EPOCH 1328
2024-02-08 16:55:20,691 Epoch 1328: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.82 
2024-02-08 16:55:20,691 EPOCH 1329
2024-02-08 16:55:22,492 [Epoch: 1329 Step: 00089000] Batch Recognition Loss:   0.000147 => Gls Tokens per Sec:     2078 || Batch Translation Loss:   0.019914 => Txt Tokens per Sec:     5675 || Lr: 0.000025
2024-02-08 16:55:25,498 Epoch 1329: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.51 
2024-02-08 16:55:25,498 EPOCH 1330
2024-02-08 16:55:30,420 [Epoch: 1330 Step: 00089100] Batch Recognition Loss:   0.000955 => Gls Tokens per Sec:     1833 || Batch Translation Loss:   0.029403 => Txt Tokens per Sec:     5059 || Lr: 0.000025
2024-02-08 16:55:31,236 Epoch 1330: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.33 
2024-02-08 16:55:31,237 EPOCH 1331
2024-02-08 16:55:36,456 Epoch 1331: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.52 
2024-02-08 16:55:36,456 EPOCH 1332
2024-02-08 16:55:38,118 [Epoch: 1332 Step: 00089200] Batch Recognition Loss:   0.000729 => Gls Tokens per Sec:     2155 || Batch Translation Loss:   0.016653 => Txt Tokens per Sec:     6176 || Lr: 0.000025
2024-02-08 16:55:41,334 Epoch 1332: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.26 
2024-02-08 16:55:41,334 EPOCH 1333
2024-02-08 16:55:45,855 [Epoch: 1333 Step: 00089300] Batch Recognition Loss:   0.000120 => Gls Tokens per Sec:     1961 || Batch Translation Loss:   0.014809 => Txt Tokens per Sec:     5421 || Lr: 0.000025
2024-02-08 16:55:46,765 Epoch 1333: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.13 
2024-02-08 16:55:46,765 EPOCH 1334
2024-02-08 16:55:51,965 Epoch 1334: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.18 
2024-02-08 16:55:51,966 EPOCH 1335
2024-02-08 16:55:53,707 [Epoch: 1335 Step: 00089400] Batch Recognition Loss:   0.000098 => Gls Tokens per Sec:     2024 || Batch Translation Loss:   0.015063 => Txt Tokens per Sec:     5348 || Lr: 0.000025
2024-02-08 16:55:57,640 Epoch 1335: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.60 
2024-02-08 16:55:57,640 EPOCH 1336
2024-02-08 16:56:02,089 [Epoch: 1336 Step: 00089500] Batch Recognition Loss:   0.000413 => Gls Tokens per Sec:     1978 || Batch Translation Loss:   0.025472 => Txt Tokens per Sec:     5527 || Lr: 0.000025
2024-02-08 16:56:03,249 Epoch 1336: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.08 
2024-02-08 16:56:03,250 EPOCH 1337
2024-02-08 16:56:08,597 Epoch 1337: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.55 
2024-02-08 16:56:08,597 EPOCH 1338
2024-02-08 16:56:10,147 [Epoch: 1338 Step: 00089600] Batch Recognition Loss:   0.000140 => Gls Tokens per Sec:     2171 || Batch Translation Loss:   0.023995 => Txt Tokens per Sec:     6189 || Lr: 0.000025
2024-02-08 16:56:13,823 Epoch 1338: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.29 
2024-02-08 16:56:13,824 EPOCH 1339
2024-02-08 16:56:18,361 [Epoch: 1339 Step: 00089700] Batch Recognition Loss:   0.000293 => Gls Tokens per Sec:     1883 || Batch Translation Loss:   0.022540 => Txt Tokens per Sec:     5231 || Lr: 0.000025
2024-02-08 16:56:19,283 Epoch 1339: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.35 
2024-02-08 16:56:19,283 EPOCH 1340
2024-02-08 16:56:24,503 Epoch 1340: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.14 
2024-02-08 16:56:24,504 EPOCH 1341
2024-02-08 16:56:26,306 [Epoch: 1341 Step: 00089800] Batch Recognition Loss:   0.000156 => Gls Tokens per Sec:     1777 || Batch Translation Loss:   0.017085 => Txt Tokens per Sec:     5310 || Lr: 0.000025
2024-02-08 16:56:29,909 Epoch 1341: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.15 
2024-02-08 16:56:29,910 EPOCH 1342
2024-02-08 16:56:33,806 [Epoch: 1342 Step: 00089900] Batch Recognition Loss:   0.000221 => Gls Tokens per Sec:     2151 || Batch Translation Loss:   0.016776 => Txt Tokens per Sec:     5907 || Lr: 0.000025
2024-02-08 16:56:34,899 Epoch 1342: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.07 
2024-02-08 16:56:34,899 EPOCH 1343
2024-02-08 16:56:40,268 Epoch 1343: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.07 
2024-02-08 16:56:40,268 EPOCH 1344
2024-02-08 16:56:41,525 [Epoch: 1344 Step: 00090000] Batch Recognition Loss:   0.000347 => Gls Tokens per Sec:     2420 || Batch Translation Loss:   0.017890 => Txt Tokens per Sec:     6193 || Lr: 0.000025
2024-02-08 16:56:50,045 Validation result at epoch 1344, step    90000: duration: 8.5199s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.20756	Translation Loss: 94108.99219	PPL: 12083.77734
	Eval Metric: BLEU
	WER 3.11	(DEL: 0.00,	INS: 0.00,	SUB: 3.11)
	BLEU-4 0.66	(BLEU-1: 10.11,	BLEU-2: 3.19,	BLEU-3: 1.30,	BLEU-4: 0.66)
	CHRF 16.63	ROUGE 8.84
2024-02-08 16:56:50,046 Logging Recognition and Translation Outputs
2024-02-08 16:56:50,046 ========================================================================================================================
2024-02-08 16:56:50,047 Logging Sequence: 105_42.00
2024-02-08 16:56:50,047 	Gloss Reference :	A B+C+D+E
2024-02-08 16:56:50,047 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:56:50,047 	Gloss Alignment :	         
2024-02-08 16:56:50,047 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:56:50,049 	Text Reference  :	**** *** ** * was  playing  against ** ******* 31 year old norwegian magnus carlsen who is world number one  player
2024-02-08 16:56:50,049 	Text Hypothesis :	this led to a huge argument against he debuted in his  ipl seasons   and    has     led to its   may    jump kohli 
2024-02-08 16:56:50,049 	Text Alignment  :	I    I   I  I S    S                I  I       S  S    S   S         S      S       S   S  S     S      S    S     
2024-02-08 16:56:50,049 ========================================================================================================================
2024-02-08 16:56:50,050 Logging Sequence: 118_232.00
2024-02-08 16:56:50,050 	Gloss Reference :	A B+C+D+E
2024-02-08 16:56:50,050 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:56:50,050 	Gloss Alignment :	         
2024-02-08 16:56:50,050 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:56:50,051 	Text Reference  :	messi was * ********** *** *** overjoyed upon recieving the award    
2024-02-08 16:56:50,051 	Text Hypothesis :	there was a government did not permit    it   this      is  incorrect
2024-02-08 16:56:50,051 	Text Alignment  :	S         I I          I   I   S         S    S         S   S        
2024-02-08 16:56:50,051 ========================================================================================================================
2024-02-08 16:56:50,051 Logging Sequence: 171_2.00
2024-02-08 16:56:50,052 	Gloss Reference :	A B+C+D+E
2024-02-08 16:56:50,052 	Gloss Hypothesis:	A B+C+D  
2024-02-08 16:56:50,052 	Gloss Alignment :	  S      
2024-02-08 16:56:50,052 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:56:50,053 	Text Reference  :	** as   you  might      all know that the ipl is about to end the finals are on  28th may        
2024-02-08 16:56:50,053 	Text Hypothesis :	in june 2020 cricketers all **** **** the ipl ** ***** ** *** *** ****** and has been quarantined
2024-02-08 16:56:50,053 	Text Alignment  :	I  S    S    S              D    D            D  D     D  D   D   D      S   S   S    S          
2024-02-08 16:56:50,054 ========================================================================================================================
2024-02-08 16:56:50,054 Logging Sequence: 136_107.00
2024-02-08 16:56:50,054 	Gloss Reference :	A B+C+D+E
2024-02-08 16:56:50,054 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:56:50,054 	Gloss Alignment :	         
2024-02-08 16:56:50,054 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:56:50,055 	Text Reference  :	sindhu replied that she had stop    eating icecream    because of      her  training
2024-02-08 16:56:50,055 	Text Hypothesis :	****** ******* **** *** new zealand beat   afghanistan and     secured many medals  
2024-02-08 16:56:50,056 	Text Alignment  :	D      D       D    D   S   S       S      S           S       S       S    S       
2024-02-08 16:56:50,056 ========================================================================================================================
2024-02-08 16:56:50,056 Logging Sequence: 93_93.00
2024-02-08 16:56:50,056 	Gloss Reference :	A B+C+D+E
2024-02-08 16:56:50,056 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:56:50,056 	Gloss Alignment :	         
2024-02-08 16:56:50,057 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:56:50,057 	Text Reference  :	****** ** rooney was at   the club as       well    
2024-02-08 16:56:50,057 	Text Hypothesis :	series on 4th    may 2021 the **** covid-19 pandemic
2024-02-08 16:56:50,057 	Text Alignment  :	I      I  S      S   S        D    S        S       
2024-02-08 16:56:50,057 ========================================================================================================================
2024-02-08 16:56:54,109 Epoch 1344: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.07 
2024-02-08 16:56:54,110 EPOCH 1345
2024-02-08 16:56:57,698 [Epoch: 1345 Step: 00090100] Batch Recognition Loss:   0.000126 => Gls Tokens per Sec:     2319 || Batch Translation Loss:   0.012344 => Txt Tokens per Sec:     6311 || Lr: 0.000025
2024-02-08 16:56:59,186 Epoch 1345: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.04 
2024-02-08 16:56:59,187 EPOCH 1346
2024-02-08 16:57:04,616 Epoch 1346: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.19 
2024-02-08 16:57:04,616 EPOCH 1347
2024-02-08 16:57:05,989 [Epoch: 1347 Step: 00090200] Batch Recognition Loss:   0.000169 => Gls Tokens per Sec:     2099 || Batch Translation Loss:   0.017465 => Txt Tokens per Sec:     6194 || Lr: 0.000025
2024-02-08 16:57:09,874 Epoch 1347: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.08 
2024-02-08 16:57:09,874 EPOCH 1348
2024-02-08 16:57:14,139 [Epoch: 1348 Step: 00090300] Batch Recognition Loss:   0.000299 => Gls Tokens per Sec:     1891 || Batch Translation Loss:   0.025289 => Txt Tokens per Sec:     5271 || Lr: 0.000025
2024-02-08 16:57:15,395 Epoch 1348: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.69 
2024-02-08 16:57:15,395 EPOCH 1349
2024-02-08 16:57:20,895 Epoch 1349: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.39 
2024-02-08 16:57:20,896 EPOCH 1350
2024-02-08 16:57:22,118 [Epoch: 1350 Step: 00090400] Batch Recognition Loss:   0.001019 => Gls Tokens per Sec:     2146 || Batch Translation Loss:   0.007579 => Txt Tokens per Sec:     6101 || Lr: 0.000025
2024-02-08 16:57:26,094 Epoch 1350: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.19 
2024-02-08 16:57:26,095 EPOCH 1351
2024-02-08 16:57:30,435 [Epoch: 1351 Step: 00090500] Batch Recognition Loss:   0.005460 => Gls Tokens per Sec:     1844 || Batch Translation Loss:   0.004665 => Txt Tokens per Sec:     5192 || Lr: 0.000025
2024-02-08 16:57:31,718 Epoch 1351: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.08 
2024-02-08 16:57:31,719 EPOCH 1352
2024-02-08 16:57:36,760 Epoch 1352: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.07 
2024-02-08 16:57:36,761 EPOCH 1353
2024-02-08 16:57:37,806 [Epoch: 1353 Step: 00090600] Batch Recognition Loss:   0.000160 => Gls Tokens per Sec:     2450 || Batch Translation Loss:   0.011598 => Txt Tokens per Sec:     6707 || Lr: 0.000025
2024-02-08 16:57:41,947 Epoch 1353: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.20 
2024-02-08 16:57:41,948 EPOCH 1354
2024-02-08 16:57:45,881 [Epoch: 1354 Step: 00090700] Batch Recognition Loss:   0.000311 => Gls Tokens per Sec:     1969 || Batch Translation Loss:   0.005029 => Txt Tokens per Sec:     5333 || Lr: 0.000025
2024-02-08 16:57:47,380 Epoch 1354: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.36 
2024-02-08 16:57:47,380 EPOCH 1355
2024-02-08 16:57:52,775 Epoch 1355: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.72 
2024-02-08 16:57:52,775 EPOCH 1356
2024-02-08 16:57:53,794 [Epoch: 1356 Step: 00090800] Batch Recognition Loss:   0.000491 => Gls Tokens per Sec:     2358 || Batch Translation Loss:   0.024151 => Txt Tokens per Sec:     6240 || Lr: 0.000025
2024-02-08 16:57:58,053 Epoch 1356: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.59 
2024-02-08 16:57:58,053 EPOCH 1357
2024-02-08 16:58:02,092 [Epoch: 1357 Step: 00090900] Batch Recognition Loss:   0.000198 => Gls Tokens per Sec:     1878 || Batch Translation Loss:   0.023197 => Txt Tokens per Sec:     5189 || Lr: 0.000025
2024-02-08 16:58:03,559 Epoch 1357: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.97 
2024-02-08 16:58:03,559 EPOCH 1358
2024-02-08 16:58:08,806 Epoch 1358: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.40 
2024-02-08 16:58:08,806 EPOCH 1359
2024-02-08 16:58:09,952 [Epoch: 1359 Step: 00091000] Batch Recognition Loss:   0.000484 => Gls Tokens per Sec:     1958 || Batch Translation Loss:   0.035948 => Txt Tokens per Sec:     5584 || Lr: 0.000025
2024-02-08 16:58:14,236 Epoch 1359: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.39 
2024-02-08 16:58:14,237 EPOCH 1360
2024-02-08 16:58:17,890 [Epoch: 1360 Step: 00091100] Batch Recognition Loss:   0.000100 => Gls Tokens per Sec:     2059 || Batch Translation Loss:   0.011206 => Txt Tokens per Sec:     5724 || Lr: 0.000025
2024-02-08 16:58:19,651 Epoch 1360: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.24 
2024-02-08 16:58:19,652 EPOCH 1361
2024-02-08 16:58:25,101 Epoch 1361: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.62 
2024-02-08 16:58:25,102 EPOCH 1362
2024-02-08 16:58:26,219 [Epoch: 1362 Step: 00091200] Batch Recognition Loss:   0.000133 => Gls Tokens per Sec:     1865 || Batch Translation Loss:   0.019933 => Txt Tokens per Sec:     5343 || Lr: 0.000025
2024-02-08 16:58:30,632 Epoch 1362: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.33 
2024-02-08 16:58:30,633 EPOCH 1363
2024-02-08 16:58:34,461 [Epoch: 1363 Step: 00091300] Batch Recognition Loss:   0.000181 => Gls Tokens per Sec:     1897 || Batch Translation Loss:   0.024288 => Txt Tokens per Sec:     5277 || Lr: 0.000025
2024-02-08 16:58:36,013 Epoch 1363: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.05 
2024-02-08 16:58:36,014 EPOCH 1364
2024-02-08 16:58:41,625 Epoch 1364: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.09 
2024-02-08 16:58:41,626 EPOCH 1365
2024-02-08 16:58:42,604 [Epoch: 1365 Step: 00091400] Batch Recognition Loss:   0.000325 => Gls Tokens per Sec:     1965 || Batch Translation Loss:   0.043650 => Txt Tokens per Sec:     5527 || Lr: 0.000025
2024-02-08 16:58:47,181 Epoch 1365: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.59 
2024-02-08 16:58:47,182 EPOCH 1366
2024-02-08 16:58:50,684 [Epoch: 1366 Step: 00091500] Batch Recognition Loss:   0.001312 => Gls Tokens per Sec:     2029 || Batch Translation Loss:   0.014305 => Txt Tokens per Sec:     5504 || Lr: 0.000025
2024-02-08 16:58:52,661 Epoch 1366: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.53 
2024-02-08 16:58:52,661 EPOCH 1367
2024-02-08 16:58:58,196 Epoch 1367: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.64 
2024-02-08 16:58:58,196 EPOCH 1368
2024-02-08 16:58:59,015 [Epoch: 1368 Step: 00091600] Batch Recognition Loss:   0.000207 => Gls Tokens per Sec:     2152 || Batch Translation Loss:   0.018085 => Txt Tokens per Sec:     5900 || Lr: 0.000025
2024-02-08 16:59:03,214 Epoch 1368: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.37 
2024-02-08 16:59:03,214 EPOCH 1369
2024-02-08 16:59:06,416 [Epoch: 1369 Step: 00091700] Batch Recognition Loss:   0.000301 => Gls Tokens per Sec:     2199 || Batch Translation Loss:   0.022520 => Txt Tokens per Sec:     6138 || Lr: 0.000025
2024-02-08 16:59:08,345 Epoch 1369: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.10 
2024-02-08 16:59:08,346 EPOCH 1370
2024-02-08 16:59:13,841 Epoch 1370: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.08 
2024-02-08 16:59:13,842 EPOCH 1371
2024-02-08 16:59:14,706 [Epoch: 1371 Step: 00091800] Batch Recognition Loss:   0.000204 => Gls Tokens per Sec:     1852 || Batch Translation Loss:   0.021048 => Txt Tokens per Sec:     5343 || Lr: 0.000025
2024-02-08 16:59:18,660 Epoch 1371: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.90 
2024-02-08 16:59:18,661 EPOCH 1372
2024-02-08 16:59:22,395 [Epoch: 1372 Step: 00091900] Batch Recognition Loss:   0.000980 => Gls Tokens per Sec:     1844 || Batch Translation Loss:   0.029780 => Txt Tokens per Sec:     5157 || Lr: 0.000025
2024-02-08 16:59:24,234 Epoch 1372: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.93 
2024-02-08 16:59:24,235 EPOCH 1373
2024-02-08 16:59:29,938 Epoch 1373: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.59 
2024-02-08 16:59:29,939 EPOCH 1374
2024-02-08 16:59:30,587 [Epoch: 1374 Step: 00092000] Batch Recognition Loss:   0.000164 => Gls Tokens per Sec:     2226 || Batch Translation Loss:   0.010769 => Txt Tokens per Sec:     5756 || Lr: 0.000025
2024-02-08 16:59:39,227 Validation result at epoch 1374, step    92000: duration: 8.6390s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.28181	Translation Loss: 94672.17188	PPL: 12782.97754
	Eval Metric: BLEU
	WER 3.18	(DEL: 0.00,	INS: 0.00,	SUB: 3.18)
	BLEU-4 0.80	(BLEU-1: 9.94,	BLEU-2: 3.23,	BLEU-3: 1.43,	BLEU-4: 0.80)
	CHRF 16.69	ROUGE 8.10
2024-02-08 16:59:39,228 Logging Recognition and Translation Outputs
2024-02-08 16:59:39,228 ========================================================================================================================
2024-02-08 16:59:39,229 Logging Sequence: 122_208.00
2024-02-08 16:59:39,229 	Gloss Reference :	A B+C+D+E
2024-02-08 16:59:39,229 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:59:39,229 	Gloss Alignment :	         
2024-02-08 16:59:39,229 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:59:39,231 	Text Reference  :	***** ****** **** **** *** **** ** in   the  equestrian category of   the olympics indrajit lamba qualified in     1996     
2024-02-08 16:59:39,231 	Text Hypothesis :	sadly nirmal kaur lost her life on 13th june as         they     must be  a        few      of    the       deadly infection
2024-02-08 16:59:39,231 	Text Alignment  :	I     I      I    I    I   I    I  S    S    S          S        S    S   S        S        S     S         S      S        
2024-02-08 16:59:39,231 ========================================================================================================================
2024-02-08 16:59:39,231 Logging Sequence: 161_37.00
2024-02-08 16:59:39,232 	Gloss Reference :	A B+C+D+E
2024-02-08 16:59:39,232 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:59:39,232 	Gloss Alignment :	         
2024-02-08 16:59:39,232 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:59:39,233 	Text Reference  :	the team was shocked by this and virat requested everyone to     keep it          confidential this   happened on        14th  january 2022   
2024-02-08 16:59:39,234 	Text Hypothesis :	*** **** *** ******* ** **** *** ***** however   they     played for  encouraging healthy      habits creating awareness about 9       minutes
2024-02-08 16:59:39,234 	Text Alignment  :	D   D    D   D       D  D    D   D     S         S        S      S    S           S            S      S        S         S     S       S      
2024-02-08 16:59:39,234 ========================================================================================================================
2024-02-08 16:59:39,234 Logging Sequence: 178_62.00
2024-02-08 16:59:39,234 	Gloss Reference :	A B+C+D+E
2024-02-08 16:59:39,235 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:59:39,235 	Gloss Alignment :	         
2024-02-08 16:59:39,235 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:59:39,236 	Text Reference  :	**** ** *** *** ****** ** ************ delhi court   had     issued a       non-bailable warrant against sushil
2024-02-08 16:59:39,236 	Text Hypothesis :	this is why the finals of broadcasting the   opening players were   allowed to           wear    clothes that  
2024-02-08 16:59:39,236 	Text Alignment  :	I    I  I   I   I      I  I            S     S       S       S      S       S            S       S       S     
2024-02-08 16:59:39,236 ========================================================================================================================
2024-02-08 16:59:39,236 Logging Sequence: 172_15.00
2024-02-08 16:59:39,237 	Gloss Reference :	A B+C+D+E
2024-02-08 16:59:39,237 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:59:39,237 	Gloss Alignment :	         
2024-02-08 16:59:39,237 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:59:39,239 	Text Reference  :	now in the final match     on     28   may   2023  the     two  teams were up  against each other at  the same venue
2024-02-08 16:59:39,239 	Text Hypothesis :	*** ** *** ***** generally people wear their goals however they won   a    new match   as   they  get out for  2-2  
2024-02-08 16:59:39,239 	Text Alignment  :	D   D  D   D     S         S      S    S     S     S       S    S     S    S   S       S    S     S   S   S    S    
2024-02-08 16:59:39,240 ========================================================================================================================
2024-02-08 16:59:39,240 Logging Sequence: 73_88.00
2024-02-08 16:59:39,240 	Gloss Reference :	A B+C+D+E
2024-02-08 16:59:39,240 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 16:59:39,240 	Gloss Alignment :	         
2024-02-08 16:59:39,241 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 16:59:39,243 	Text Reference  :	there   will be      chicken kebab pani puri sev     puri aloo     chaat etc it     will have dishes    from  different parts of the  country
2024-02-08 16:59:39,243 	Text Hypothesis :	however some players may     the   team was  wearing the  exacting game  and inputs off  the  bengaluru crowd asking    them  to join us     
2024-02-08 16:59:39,244 	Text Alignment  :	S       S    S       S       S     S    S    S       S    S        S     S   S      S    S    S         S     S         S     S  S    S      
2024-02-08 16:59:39,244 ========================================================================================================================
2024-02-08 16:59:44,116 Epoch 1374: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.31 
2024-02-08 16:59:44,116 EPOCH 1375
2024-02-08 16:59:47,512 [Epoch: 1375 Step: 00092100] Batch Recognition Loss:   0.000350 => Gls Tokens per Sec:     1950 || Batch Translation Loss:   0.006912 => Txt Tokens per Sec:     5336 || Lr: 0.000025
2024-02-08 16:59:49,553 Epoch 1375: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.16 
2024-02-08 16:59:49,554 EPOCH 1376
2024-02-08 16:59:54,717 Epoch 1376: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.34 
2024-02-08 16:59:54,718 EPOCH 1377
2024-02-08 16:59:55,333 [Epoch: 1377 Step: 00092200] Batch Recognition Loss:   0.000162 => Gls Tokens per Sec:     2085 || Batch Translation Loss:   0.021875 => Txt Tokens per Sec:     5964 || Lr: 0.000025
2024-02-08 17:00:00,288 Epoch 1377: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.55 
2024-02-08 17:00:00,289 EPOCH 1378
2024-02-08 17:00:03,681 [Epoch: 1378 Step: 00092300] Batch Recognition Loss:   0.000181 => Gls Tokens per Sec:     1905 || Batch Translation Loss:   0.024082 => Txt Tokens per Sec:     5302 || Lr: 0.000025
2024-02-08 17:00:05,594 Epoch 1378: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.27 
2024-02-08 17:00:05,594 EPOCH 1379
2024-02-08 17:00:11,133 Epoch 1379: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.62 
2024-02-08 17:00:11,133 EPOCH 1380
2024-02-08 17:00:11,564 [Epoch: 1380 Step: 00092400] Batch Recognition Loss:   0.000151 => Gls Tokens per Sec:     2605 || Batch Translation Loss:   0.053206 => Txt Tokens per Sec:     6700 || Lr: 0.000025
2024-02-08 17:00:16,436 Epoch 1380: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.01 
2024-02-08 17:00:16,437 EPOCH 1381
2024-02-08 17:00:19,697 [Epoch: 1381 Step: 00092500] Batch Recognition Loss:   0.000153 => Gls Tokens per Sec:     1934 || Batch Translation Loss:   0.016851 => Txt Tokens per Sec:     5354 || Lr: 0.000025
2024-02-08 17:00:21,862 Epoch 1381: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.74 
2024-02-08 17:00:21,863 EPOCH 1382
2024-02-08 17:00:27,408 Epoch 1382: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.59 
2024-02-08 17:00:27,409 EPOCH 1383
2024-02-08 17:00:28,102 [Epoch: 1383 Step: 00092600] Batch Recognition Loss:   0.000184 => Gls Tokens per Sec:     1387 || Batch Translation Loss:   0.022825 => Txt Tokens per Sec:     4207 || Lr: 0.000025
2024-02-08 17:00:32,821 Epoch 1383: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.84 
2024-02-08 17:00:32,821 EPOCH 1384
2024-02-08 17:00:36,030 [Epoch: 1384 Step: 00092700] Batch Recognition Loss:   0.000171 => Gls Tokens per Sec:     1945 || Batch Translation Loss:   0.012606 => Txt Tokens per Sec:     5488 || Lr: 0.000025
2024-02-08 17:00:38,297 Epoch 1384: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.39 
2024-02-08 17:00:38,297 EPOCH 1385
2024-02-08 17:00:43,692 Epoch 1385: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.17 
2024-02-08 17:00:43,693 EPOCH 1386
2024-02-08 17:00:44,006 [Epoch: 1386 Step: 00092800] Batch Recognition Loss:   0.000233 => Gls Tokens per Sec:     2564 || Batch Translation Loss:   0.012841 => Txt Tokens per Sec:     6401 || Lr: 0.000025
2024-02-08 17:00:49,383 Epoch 1386: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.16 
2024-02-08 17:00:49,383 EPOCH 1387
2024-02-08 17:00:52,427 [Epoch: 1387 Step: 00092900] Batch Recognition Loss:   0.000222 => Gls Tokens per Sec:     1965 || Batch Translation Loss:   0.087573 => Txt Tokens per Sec:     5377 || Lr: 0.000025
2024-02-08 17:00:54,688 Epoch 1387: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.28 
2024-02-08 17:00:54,689 EPOCH 1388
2024-02-08 17:00:59,239 Epoch 1388: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.66 
2024-02-08 17:00:59,239 EPOCH 1389
2024-02-08 17:00:59,519 [Epoch: 1389 Step: 00093000] Batch Recognition Loss:   0.000172 => Gls Tokens per Sec:     2294 || Batch Translation Loss:   0.024398 => Txt Tokens per Sec:     6455 || Lr: 0.000025
2024-02-08 17:01:03,956 Epoch 1389: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.86 
2024-02-08 17:01:03,956 EPOCH 1390
2024-02-08 17:01:06,462 [Epoch: 1390 Step: 00093100] Batch Recognition Loss:   0.000258 => Gls Tokens per Sec:     2323 || Batch Translation Loss:   0.024348 => Txt Tokens per Sec:     6474 || Lr: 0.000025
2024-02-08 17:01:08,712 Epoch 1390: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.71 
2024-02-08 17:01:08,713 EPOCH 1391
2024-02-08 17:01:13,890 Epoch 1391: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.64 
2024-02-08 17:01:13,891 EPOCH 1392
2024-02-08 17:01:14,099 [Epoch: 1392 Step: 00093200] Batch Recognition Loss:   0.000182 => Gls Tokens per Sec:     2319 || Batch Translation Loss:   0.015858 => Txt Tokens per Sec:     5686 || Lr: 0.000025
2024-02-08 17:01:19,090 Epoch 1392: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.62 
2024-02-08 17:01:19,090 EPOCH 1393
2024-02-08 17:01:22,030 [Epoch: 1393 Step: 00093300] Batch Recognition Loss:   0.000198 => Gls Tokens per Sec:     1926 || Batch Translation Loss:   0.041684 => Txt Tokens per Sec:     5305 || Lr: 0.000025
2024-02-08 17:01:24,564 Epoch 1393: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.38 
2024-02-08 17:01:24,565 EPOCH 1394
2024-02-08 17:01:30,452 Epoch 1394: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.16 
2024-02-08 17:01:30,452 EPOCH 1395
2024-02-08 17:01:30,563 [Epoch: 1395 Step: 00093400] Batch Recognition Loss:   0.000141 => Gls Tokens per Sec:     2948 || Batch Translation Loss:   0.011526 => Txt Tokens per Sec:     6311 || Lr: 0.000025
2024-02-08 17:01:35,645 Epoch 1395: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.19 
2024-02-08 17:01:35,646 EPOCH 1396
2024-02-08 17:01:38,789 [Epoch: 1396 Step: 00093500] Batch Recognition Loss:   0.000275 => Gls Tokens per Sec:     1750 || Batch Translation Loss:   0.019556 => Txt Tokens per Sec:     5092 || Lr: 0.000025
2024-02-08 17:01:41,251 Epoch 1396: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.14 
2024-02-08 17:01:41,251 EPOCH 1397
2024-02-08 17:01:46,661 Epoch 1397: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.15 
2024-02-08 17:01:46,661 EPOCH 1398
2024-02-08 17:01:46,746 [Epoch: 1398 Step: 00093600] Batch Recognition Loss:   0.000181 => Gls Tokens per Sec:     1905 || Batch Translation Loss:   0.016303 => Txt Tokens per Sec:     5238 || Lr: 0.000025
2024-02-08 17:01:51,995 Epoch 1398: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.21 
2024-02-08 17:01:51,995 EPOCH 1399
2024-02-08 17:01:54,519 [Epoch: 1399 Step: 00093700] Batch Recognition Loss:   0.000120 => Gls Tokens per Sec:     2117 || Batch Translation Loss:   0.014204 => Txt Tokens per Sec:     5766 || Lr: 0.000025
2024-02-08 17:01:57,410 Epoch 1399: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.61 
2024-02-08 17:01:57,410 EPOCH 1400
2024-02-08 17:02:02,879 [Epoch: 1400 Step: 00093800] Batch Recognition Loss:   0.000092 => Gls Tokens per Sec:     1942 || Batch Translation Loss:   0.012925 => Txt Tokens per Sec:     5373 || Lr: 0.000025
2024-02-08 17:02:02,881 Epoch 1400: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.09 
2024-02-08 17:02:02,881 EPOCH 1401
2024-02-08 17:02:07,930 Epoch 1401: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.11 
2024-02-08 17:02:07,930 EPOCH 1402
2024-02-08 17:02:10,264 [Epoch: 1402 Step: 00093900] Batch Recognition Loss:   0.000109 => Gls Tokens per Sec:     2219 || Batch Translation Loss:   0.014261 => Txt Tokens per Sec:     6127 || Lr: 0.000025
2024-02-08 17:02:13,270 Epoch 1402: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.36 
2024-02-08 17:02:13,272 EPOCH 1403
2024-02-08 17:02:18,371 [Epoch: 1403 Step: 00094000] Batch Recognition Loss:   0.000368 => Gls Tokens per Sec:     2051 || Batch Translation Loss:   0.047341 => Txt Tokens per Sec:     5676 || Lr: 0.000025
2024-02-08 17:02:26,886 Validation result at epoch 1403, step    94000: duration: 8.5135s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.12951	Translation Loss: 94575.17188	PPL: 12659.72949
	Eval Metric: BLEU
	WER 2.82	(DEL: 0.00,	INS: 0.00,	SUB: 2.82)
	BLEU-4 0.49	(BLEU-1: 10.63,	BLEU-2: 3.27,	BLEU-3: 1.11,	BLEU-4: 0.49)
	CHRF 16.93	ROUGE 8.74
2024-02-08 17:02:26,888 Logging Recognition and Translation Outputs
2024-02-08 17:02:26,888 ========================================================================================================================
2024-02-08 17:02:26,888 Logging Sequence: 107_135.00
2024-02-08 17:02:26,889 	Gloss Reference :	A B+C+D+E
2024-02-08 17:02:26,889 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 17:02:26,889 	Gloss Alignment :	         
2024-02-08 17:02:26,889 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 17:02:26,890 	Text Reference  :	** *** four indian tennis players such as     
2024-02-08 17:02:26,890 	Text Hypothesis :	if she does not    want   to      be   careful
2024-02-08 17:02:26,890 	Text Alignment  :	I  I   S    S      S      S       S    S      
2024-02-08 17:02:26,890 ========================================================================================================================
2024-02-08 17:02:26,890 Logging Sequence: 105_160.00
2024-02-08 17:02:26,890 	Gloss Reference :	A B+C+D+E
2024-02-08 17:02:26,891 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 17:02:26,891 	Gloss Alignment :	         
2024-02-08 17:02:26,891 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 17:02:26,891 	Text Reference  :	** *** ******** **** many people  tweeted congratulatory messages for praggnanandhaa
2024-02-08 17:02:26,892 	Text Hypothesis :	he was actually born and  brought him     in             the      his team          
2024-02-08 17:02:26,892 	Text Alignment  :	I  I   I        I    S    S       S       S              S        S   S             
2024-02-08 17:02:26,892 ========================================================================================================================
2024-02-08 17:02:26,892 Logging Sequence: 134_217.00
2024-02-08 17:02:26,892 	Gloss Reference :	A B+C+D+E
2024-02-08 17:02:26,893 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 17:02:26,893 	Gloss Alignment :	         
2024-02-08 17:02:26,893 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 17:02:26,896 	Text Reference  :	******** **** **** ** pm modi  told him that yoga and meditation would  help him concentrate and ****** ****** will      help him win    more  medals  
2024-02-08 17:02:26,896 	Text Hypothesis :	virendra said that 'i am happy but  i   want to   win more       medals as   my  uncle       and father always encourage me   to  attain newer heights'
2024-02-08 17:02:26,896 	Text Alignment  :	I        I    I    I  S  S     S    S   S    S    S   S          S      S    S   S               I      I      S         S    S   S      S     S       
2024-02-08 17:02:26,896 ========================================================================================================================
2024-02-08 17:02:26,896 Logging Sequence: 164_128.00
2024-02-08 17:02:26,896 	Gloss Reference :	A B+C+D+E
2024-02-08 17:02:26,897 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 17:02:26,897 	Gloss Alignment :	         
2024-02-08 17:02:26,897 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 17:02:26,899 	Text Reference  :	viacom18 grabbed the digital      rights of telecasting ipl matches for the next 5 years **** ** **** ***** for rs 20500 crore
2024-02-08 17:02:26,899 	Text Hypothesis :	one      of      the broadcasting rights of *********** ipl matches in  the next 5 years went to star india for rs 23575 crore
2024-02-08 17:02:26,899 	Text Alignment  :	S        S           S                      D                       S                    I    I  I    I            S          
2024-02-08 17:02:26,899 ========================================================================================================================
2024-02-08 17:02:26,900 Logging Sequence: 87_136.00
2024-02-08 17:02:26,900 	Gloss Reference :	A B+C+D+E
2024-02-08 17:02:26,900 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 17:02:26,900 	Gloss Alignment :	         
2024-02-08 17:02:26,900 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 17:02:26,902 	Text Reference  :	while kaif was in support of kl rahul gambhir was   in support of   ishan kishan due to     his   amazing batting     form     
2024-02-08 17:02:26,902 	Text Hypothesis :	***** **** *** ** ******* ** it is    really  known as your    have faced with   her punjab kings royal   challengers bangalore
2024-02-08 17:02:26,902 	Text Alignment  :	D     D    D   D  D       D  S  S     S       S     S  S       S    S     S      S   S      S     S       S           S        
2024-02-08 17:02:26,902 ========================================================================================================================
2024-02-08 17:02:27,003 Epoch 1403: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.35 
2024-02-08 17:02:27,003 EPOCH 1404
2024-02-08 17:02:32,612 Epoch 1404: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.25 
2024-02-08 17:02:32,612 EPOCH 1405
2024-02-08 17:02:35,118 [Epoch: 1405 Step: 00094100] Batch Recognition Loss:   0.000172 => Gls Tokens per Sec:     2004 || Batch Translation Loss:   0.024741 => Txt Tokens per Sec:     5621 || Lr: 0.000025
2024-02-08 17:02:37,688 Epoch 1405: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.62 
2024-02-08 17:02:37,689 EPOCH 1406
2024-02-08 17:02:42,746 [Epoch: 1406 Step: 00094200] Batch Recognition Loss:   0.005376 => Gls Tokens per Sec:     2037 || Batch Translation Loss:   0.025308 => Txt Tokens per Sec:     5621 || Lr: 0.000025
2024-02-08 17:02:42,993 Epoch 1406: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.07 
2024-02-08 17:02:42,993 EPOCH 1407
2024-02-08 17:02:48,345 Epoch 1407: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.09 
2024-02-08 17:02:48,346 EPOCH 1408
2024-02-08 17:02:50,892 [Epoch: 1408 Step: 00094300] Batch Recognition Loss:   0.000363 => Gls Tokens per Sec:     1948 || Batch Translation Loss:   0.007320 => Txt Tokens per Sec:     5541 || Lr: 0.000025
2024-02-08 17:02:53,620 Epoch 1408: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.71 
2024-02-08 17:02:53,620 EPOCH 1409
2024-02-08 17:02:58,756 [Epoch: 1409 Step: 00094400] Batch Recognition Loss:   0.000225 => Gls Tokens per Sec:     1975 || Batch Translation Loss:   0.048952 => Txt Tokens per Sec:     5481 || Lr: 0.000025
2024-02-08 17:02:58,976 Epoch 1409: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.09 
2024-02-08 17:02:58,977 EPOCH 1410
2024-02-08 17:03:04,279 Epoch 1410: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.53 
2024-02-08 17:03:04,280 EPOCH 1411
2024-02-08 17:03:06,970 [Epoch: 1411 Step: 00094500] Batch Recognition Loss:   0.000219 => Gls Tokens per Sec:     1748 || Batch Translation Loss:   0.020999 => Txt Tokens per Sec:     5114 || Lr: 0.000025
2024-02-08 17:03:09,780 Epoch 1411: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.59 
2024-02-08 17:03:09,781 EPOCH 1412
2024-02-08 17:03:14,909 [Epoch: 1412 Step: 00094600] Batch Recognition Loss:   0.000195 => Gls Tokens per Sec:     1947 || Batch Translation Loss:   0.019183 => Txt Tokens per Sec:     5456 || Lr: 0.000025
2024-02-08 17:03:15,173 Epoch 1412: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.48 
2024-02-08 17:03:15,173 EPOCH 1413
2024-02-08 17:03:20,695 Epoch 1413: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.23 
2024-02-08 17:03:20,696 EPOCH 1414
2024-02-08 17:03:22,938 [Epoch: 1414 Step: 00094700] Batch Recognition Loss:   0.002164 => Gls Tokens per Sec:     2025 || Batch Translation Loss:   0.025357 => Txt Tokens per Sec:     5754 || Lr: 0.000025
2024-02-08 17:03:25,621 Epoch 1414: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.26 
2024-02-08 17:03:25,621 EPOCH 1415
2024-02-08 17:03:30,510 [Epoch: 1415 Step: 00094800] Batch Recognition Loss:   0.000133 => Gls Tokens per Sec:     2009 || Batch Translation Loss:   0.036870 => Txt Tokens per Sec:     5535 || Lr: 0.000025
2024-02-08 17:03:30,909 Epoch 1415: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.39 
2024-02-08 17:03:30,909 EPOCH 1416
2024-02-08 17:03:36,268 Epoch 1416: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.38 
2024-02-08 17:03:36,268 EPOCH 1417
2024-02-08 17:03:38,538 [Epoch: 1417 Step: 00094900] Batch Recognition Loss:   0.000116 => Gls Tokens per Sec:     1974 || Batch Translation Loss:   0.010536 => Txt Tokens per Sec:     5519 || Lr: 0.000025
2024-02-08 17:03:41,741 Epoch 1417: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.33 
2024-02-08 17:03:41,741 EPOCH 1418
2024-02-08 17:03:46,181 [Epoch: 1418 Step: 00095000] Batch Recognition Loss:   0.000107 => Gls Tokens per Sec:     2176 || Batch Translation Loss:   0.017541 => Txt Tokens per Sec:     5958 || Lr: 0.000025
2024-02-08 17:03:46,642 Epoch 1418: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.28 
2024-02-08 17:03:46,642 EPOCH 1419
2024-02-08 17:03:52,050 Epoch 1419: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.13 
2024-02-08 17:03:52,050 EPOCH 1420
2024-02-08 17:03:53,866 [Epoch: 1420 Step: 00095100] Batch Recognition Loss:   0.000143 => Gls Tokens per Sec:     2381 || Batch Translation Loss:   0.006991 => Txt Tokens per Sec:     6238 || Lr: 0.000025
2024-02-08 17:03:57,055 Epoch 1420: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.02 
2024-02-08 17:03:57,055 EPOCH 1421
2024-02-08 17:04:01,944 [Epoch: 1421 Step: 00095200] Batch Recognition Loss:   0.000446 => Gls Tokens per Sec:     1944 || Batch Translation Loss:   0.021334 => Txt Tokens per Sec:     5406 || Lr: 0.000025
2024-02-08 17:04:02,466 Epoch 1421: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.03 
2024-02-08 17:04:02,466 EPOCH 1422
2024-02-08 17:04:07,365 Epoch 1422: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.02 
2024-02-08 17:04:07,366 EPOCH 1423
2024-02-08 17:04:09,112 [Epoch: 1423 Step: 00095300] Batch Recognition Loss:   0.000158 => Gls Tokens per Sec:     2384 || Batch Translation Loss:   0.039250 => Txt Tokens per Sec:     6300 || Lr: 0.000025
2024-02-08 17:04:12,745 Epoch 1423: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.71 
2024-02-08 17:04:12,746 EPOCH 1424
2024-02-08 17:04:17,222 [Epoch: 1424 Step: 00095400] Batch Recognition Loss:   0.000145 => Gls Tokens per Sec:     2087 || Batch Translation Loss:   0.024690 => Txt Tokens per Sec:     5751 || Lr: 0.000025
2024-02-08 17:04:17,862 Epoch 1424: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.27 
2024-02-08 17:04:17,862 EPOCH 1425
2024-02-08 17:04:23,550 Epoch 1425: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.77 
2024-02-08 17:04:23,551 EPOCH 1426
2024-02-08 17:04:25,682 [Epoch: 1426 Step: 00095500] Batch Recognition Loss:   0.000289 => Gls Tokens per Sec:     1878 || Batch Translation Loss:   0.021445 => Txt Tokens per Sec:     5412 || Lr: 0.000025
2024-02-08 17:04:28,927 Epoch 1426: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.72 
2024-02-08 17:04:28,927 EPOCH 1427
2024-02-08 17:04:33,426 [Epoch: 1427 Step: 00095600] Batch Recognition Loss:   0.000145 => Gls Tokens per Sec:     2041 || Batch Translation Loss:   0.013767 => Txt Tokens per Sec:     5600 || Lr: 0.000025
2024-02-08 17:04:34,135 Epoch 1427: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.53 
2024-02-08 17:04:34,135 EPOCH 1428
2024-02-08 17:04:39,545 Epoch 1428: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.33 
2024-02-08 17:04:39,545 EPOCH 1429
2024-02-08 17:04:41,461 [Epoch: 1429 Step: 00095700] Batch Recognition Loss:   0.000156 => Gls Tokens per Sec:     1953 || Batch Translation Loss:   0.012781 => Txt Tokens per Sec:     5478 || Lr: 0.000025
2024-02-08 17:04:45,024 Epoch 1429: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.67 
2024-02-08 17:04:45,025 EPOCH 1430
2024-02-08 17:04:49,391 [Epoch: 1430 Step: 00095800] Batch Recognition Loss:   0.000511 => Gls Tokens per Sec:     2066 || Batch Translation Loss:   0.026285 => Txt Tokens per Sec:     5624 || Lr: 0.000025
2024-02-08 17:04:50,350 Epoch 1430: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.58 
2024-02-08 17:04:50,351 EPOCH 1431
2024-02-08 17:04:55,817 Epoch 1431: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.20 
2024-02-08 17:04:55,818 EPOCH 1432
2024-02-08 17:04:57,548 [Epoch: 1432 Step: 00095900] Batch Recognition Loss:   0.000131 => Gls Tokens per Sec:     2071 || Batch Translation Loss:   0.012770 => Txt Tokens per Sec:     5660 || Lr: 0.000025
2024-02-08 17:05:01,010 Epoch 1432: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.13 
2024-02-08 17:05:01,011 EPOCH 1433
2024-02-08 17:05:05,596 [Epoch: 1433 Step: 00096000] Batch Recognition Loss:   0.000158 => Gls Tokens per Sec:     1955 || Batch Translation Loss:   0.023540 => Txt Tokens per Sec:     5425 || Lr: 0.000025
2024-02-08 17:05:14,192 Validation result at epoch 1433, step    96000: duration: 8.5950s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.15812	Translation Loss: 95439.68750	PPL: 13801.46191
	Eval Metric: BLEU
	WER 2.75	(DEL: 0.00,	INS: 0.00,	SUB: 2.75)
	BLEU-4 0.50	(BLEU-1: 9.63,	BLEU-2: 3.02,	BLEU-3: 1.15,	BLEU-4: 0.50)
	CHRF 16.53	ROUGE 8.19
2024-02-08 17:05:14,193 Logging Recognition and Translation Outputs
2024-02-08 17:05:14,193 ========================================================================================================================
2024-02-08 17:05:14,193 Logging Sequence: 178_77.00
2024-02-08 17:05:14,194 	Gloss Reference :	A B+C+D+E
2024-02-08 17:05:14,194 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 17:05:14,194 	Gloss Alignment :	         
2024-02-08 17:05:14,194 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 17:05:14,195 	Text Reference  :	its been more than    two  weeks since the murder on 4th    may  
2024-02-08 17:05:14,195 	Text Hypothesis :	*** **** was  dropped from 3     balls he  had    2  silver medal
2024-02-08 17:05:14,195 	Text Alignment  :	D   D    S    S       S    S     S     S   S      S  S      S    
2024-02-08 17:05:14,196 ========================================================================================================================
2024-02-08 17:05:14,196 Logging Sequence: 118_314.00
2024-02-08 17:05:14,196 	Gloss Reference :	A B+C+D+E
2024-02-08 17:05:14,196 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 17:05:14,196 	Gloss Alignment :	         
2024-02-08 17:05:14,196 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 17:05:14,197 	Text Reference  :	** wow even the president had come to watch
2024-02-08 17:05:14,197 	Text Hypothesis :	it is  why  the ********* *** **** ** match
2024-02-08 17:05:14,197 	Text Alignment  :	I  S   S        D         D   D    D  S    
2024-02-08 17:05:14,197 ========================================================================================================================
2024-02-08 17:05:14,197 Logging Sequence: 149_210.00
2024-02-08 17:05:14,197 	Gloss Reference :	A B+C+D+E
2024-02-08 17:05:14,197 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 17:05:14,198 	Gloss Alignment :	         
2024-02-08 17:05:14,198 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 17:05:14,199 	Text Reference  :	meanwhile in the t20 world cup pakistan will play new zealand in the first semi final on 9th of        november 2022   
2024-02-08 17:05:14,199 	Text Hypothesis :	********* ** the t20 world cup ******** will **** be  held    in *** ***** **** ***** ** *** australia new      zealand
2024-02-08 17:05:14,199 	Text Alignment  :	D         D                    D             D    S   S          D   D     D    D     D  D   S         S        S      
2024-02-08 17:05:14,199 ========================================================================================================================
2024-02-08 17:05:14,200 Logging Sequence: 155_25.00
2024-02-08 17:05:14,200 	Gloss Reference :	A B+C+D+E
2024-02-08 17:05:14,200 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 17:05:14,200 	Gloss Alignment :	         
2024-02-08 17:05:14,200 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 17:05:14,201 	Text Reference  :	this is because taliban overthrew the afghan government and took over   the  country
2024-02-08 17:05:14,201 	Text Hypothesis :	**** i  am      very    grate     to  my     fans       and the  family went viral  
2024-02-08 17:05:14,201 	Text Alignment  :	D    S  S       S       S         S   S      S              S    S      S    S      
2024-02-08 17:05:14,201 ========================================================================================================================
2024-02-08 17:05:14,202 Logging Sequence: 80_16.00
2024-02-08 17:05:14,202 	Gloss Reference :	A B+C+D+E
2024-02-08 17:05:14,202 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 17:05:14,202 	Gloss Alignment :	         
2024-02-08 17:05:14,202 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 17:05:14,203 	Text Reference  :	whenever yuzvendra travels anywhere in the ***** world  for matches dhanashree always accompanies him    
2024-02-08 17:05:14,203 	Text Hypothesis :	******** ********* in      one      of the india couple are still   going      on     his         arrival
2024-02-08 17:05:14,203 	Text Alignment  :	D        D         S       S        S      I     S      S   S       S          S      S           S      
2024-02-08 17:05:14,203 ========================================================================================================================
2024-02-08 17:05:15,206 Epoch 1433: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.16 
2024-02-08 17:05:15,206 EPOCH 1434
2024-02-08 17:05:20,644 Epoch 1434: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.58 
2024-02-08 17:05:20,644 EPOCH 1435
2024-02-08 17:05:22,316 [Epoch: 1435 Step: 00096100] Batch Recognition Loss:   0.000791 => Gls Tokens per Sec:     2109 || Batch Translation Loss:   0.050538 => Txt Tokens per Sec:     5829 || Lr: 0.000025
2024-02-08 17:05:26,151 Epoch 1435: Total Training Recognition Loss 0.16  Total Training Translation Loss 1.65 
2024-02-08 17:05:26,152 EPOCH 1436
2024-02-08 17:05:30,274 [Epoch: 1436 Step: 00096200] Batch Recognition Loss:   0.000269 => Gls Tokens per Sec:     2136 || Batch Translation Loss:   0.022913 => Txt Tokens per Sec:     5753 || Lr: 0.000025
2024-02-08 17:05:31,455 Epoch 1436: Total Training Recognition Loss 0.41  Total Training Translation Loss 1.71 
2024-02-08 17:05:31,455 EPOCH 1437
2024-02-08 17:05:36,882 Epoch 1437: Total Training Recognition Loss 0.38  Total Training Translation Loss 1.37 
2024-02-08 17:05:36,882 EPOCH 1438
2024-02-08 17:05:38,481 [Epoch: 1438 Step: 00096300] Batch Recognition Loss:   0.000176 => Gls Tokens per Sec:     2040 || Batch Translation Loss:   0.017168 => Txt Tokens per Sec:     5603 || Lr: 0.000025
2024-02-08 17:05:42,215 Epoch 1438: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.34 
2024-02-08 17:05:42,216 EPOCH 1439
2024-02-08 17:05:46,717 [Epoch: 1439 Step: 00096400] Batch Recognition Loss:   0.000120 => Gls Tokens per Sec:     1897 || Batch Translation Loss:   0.012024 => Txt Tokens per Sec:     5283 || Lr: 0.000025
2024-02-08 17:05:47,782 Epoch 1439: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.57 
2024-02-08 17:05:47,783 EPOCH 1440
2024-02-08 17:05:53,445 Epoch 1440: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.55 
2024-02-08 17:05:53,446 EPOCH 1441
2024-02-08 17:05:54,965 [Epoch: 1441 Step: 00096500] Batch Recognition Loss:   0.000113 => Gls Tokens per Sec:     2108 || Batch Translation Loss:   0.013240 => Txt Tokens per Sec:     5861 || Lr: 0.000025
2024-02-08 17:05:58,848 Epoch 1441: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.35 
2024-02-08 17:05:58,849 EPOCH 1442
2024-02-08 17:06:03,416 [Epoch: 1442 Step: 00096600] Batch Recognition Loss:   0.000164 => Gls Tokens per Sec:     1836 || Batch Translation Loss:   0.037819 => Txt Tokens per Sec:     5052 || Lr: 0.000025
2024-02-08 17:06:04,626 Epoch 1442: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.37 
2024-02-08 17:06:04,626 EPOCH 1443
2024-02-08 17:06:10,348 Epoch 1443: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.28 
2024-02-08 17:06:10,349 EPOCH 1444
2024-02-08 17:06:12,057 [Epoch: 1444 Step: 00096700] Batch Recognition Loss:   0.000152 => Gls Tokens per Sec:     1782 || Batch Translation Loss:   0.026618 => Txt Tokens per Sec:     5075 || Lr: 0.000025
2024-02-08 17:06:15,960 Epoch 1444: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.40 
2024-02-08 17:06:15,960 EPOCH 1445
2024-02-08 17:06:19,905 [Epoch: 1445 Step: 00096800] Batch Recognition Loss:   0.000167 => Gls Tokens per Sec:     2110 || Batch Translation Loss:   0.019952 => Txt Tokens per Sec:     5778 || Lr: 0.000025
2024-02-08 17:06:21,356 Epoch 1445: Total Training Recognition Loss 0.10  Total Training Translation Loss 2.16 
2024-02-08 17:06:21,357 EPOCH 1446
2024-02-08 17:06:26,948 Epoch 1446: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.58 
2024-02-08 17:06:26,949 EPOCH 1447
2024-02-08 17:06:28,433 [Epoch: 1447 Step: 00096900] Batch Recognition Loss:   0.000137 => Gls Tokens per Sec:     1943 || Batch Translation Loss:   0.013349 => Txt Tokens per Sec:     5565 || Lr: 0.000025
2024-02-08 17:06:32,406 Epoch 1447: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.84 
2024-02-08 17:06:32,407 EPOCH 1448
2024-02-08 17:06:36,695 [Epoch: 1448 Step: 00097000] Batch Recognition Loss:   0.000441 => Gls Tokens per Sec:     1880 || Batch Translation Loss:   0.026980 => Txt Tokens per Sec:     5213 || Lr: 0.000025
2024-02-08 17:06:37,879 Epoch 1448: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.17 
2024-02-08 17:06:37,879 EPOCH 1449
2024-02-08 17:06:43,213 Epoch 1449: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.03 
2024-02-08 17:06:43,214 EPOCH 1450
2024-02-08 17:06:44,514 [Epoch: 1450 Step: 00097100] Batch Recognition Loss:   0.000132 => Gls Tokens per Sec:     2017 || Batch Translation Loss:   0.009992 => Txt Tokens per Sec:     5410 || Lr: 0.000025
2024-02-08 17:06:48,589 Epoch 1450: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.36 
2024-02-08 17:06:48,590 EPOCH 1451
2024-02-08 17:06:52,689 [Epoch: 1451 Step: 00097200] Batch Recognition Loss:   0.000164 => Gls Tokens per Sec:     1928 || Batch Translation Loss:   0.020970 => Txt Tokens per Sec:     5323 || Lr: 0.000025
2024-02-08 17:06:53,975 Epoch 1451: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.18 
2024-02-08 17:06:53,976 EPOCH 1452
2024-02-08 17:06:59,755 Epoch 1452: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.24 
2024-02-08 17:06:59,756 EPOCH 1453
2024-02-08 17:07:01,003 [Epoch: 1453 Step: 00097300] Batch Recognition Loss:   0.000106 => Gls Tokens per Sec:     2055 || Batch Translation Loss:   0.054342 => Txt Tokens per Sec:     5501 || Lr: 0.000025
2024-02-08 17:07:05,165 Epoch 1453: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.71 
2024-02-08 17:07:05,165 EPOCH 1454
2024-02-08 17:07:09,402 [Epoch: 1454 Step: 00097400] Batch Recognition Loss:   0.000223 => Gls Tokens per Sec:     1828 || Batch Translation Loss:   0.103388 => Txt Tokens per Sec:     5076 || Lr: 0.000025
2024-02-08 17:07:10,823 Epoch 1454: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.54 
2024-02-08 17:07:10,823 EPOCH 1455
2024-02-08 17:07:16,269 Epoch 1455: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.70 
2024-02-08 17:07:16,269 EPOCH 1456
2024-02-08 17:07:17,563 [Epoch: 1456 Step: 00097500] Batch Recognition Loss:   0.000231 => Gls Tokens per Sec:     1780 || Batch Translation Loss:   0.009671 => Txt Tokens per Sec:     4673 || Lr: 0.000025
2024-02-08 17:07:21,994 Epoch 1456: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.68 
2024-02-08 17:07:21,994 EPOCH 1457
2024-02-08 17:07:26,167 [Epoch: 1457 Step: 00097600] Batch Recognition Loss:   0.000213 => Gls Tokens per Sec:     1817 || Batch Translation Loss:   0.021110 => Txt Tokens per Sec:     5234 || Lr: 0.000025
2024-02-08 17:07:27,676 Epoch 1457: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.85 
2024-02-08 17:07:27,677 EPOCH 1458
2024-02-08 17:07:33,203 Epoch 1458: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.55 
2024-02-08 17:07:33,203 EPOCH 1459
2024-02-08 17:07:34,338 [Epoch: 1459 Step: 00097700] Batch Recognition Loss:   0.000244 => Gls Tokens per Sec:     1977 || Batch Translation Loss:   0.023931 => Txt Tokens per Sec:     5342 || Lr: 0.000025
2024-02-08 17:07:38,986 Epoch 1459: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.33 
2024-02-08 17:07:38,986 EPOCH 1460
2024-02-08 17:07:42,480 [Epoch: 1460 Step: 00097800] Batch Recognition Loss:   0.000182 => Gls Tokens per Sec:     2153 || Batch Translation Loss:   0.006940 => Txt Tokens per Sec:     5759 || Lr: 0.000025
2024-02-08 17:07:44,344 Epoch 1460: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.34 
2024-02-08 17:07:44,344 EPOCH 1461
2024-02-08 17:07:49,600 Epoch 1461: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.28 
2024-02-08 17:07:49,600 EPOCH 1462
2024-02-08 17:07:50,669 [Epoch: 1462 Step: 00097900] Batch Recognition Loss:   0.000119 => Gls Tokens per Sec:     1948 || Batch Translation Loss:   0.013221 => Txt Tokens per Sec:     5464 || Lr: 0.000025
2024-02-08 17:07:54,970 Epoch 1462: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.13 
2024-02-08 17:07:54,970 EPOCH 1463
2024-02-08 17:07:58,577 [Epoch: 1463 Step: 00098000] Batch Recognition Loss:   0.000120 => Gls Tokens per Sec:     2013 || Batch Translation Loss:   0.014373 => Txt Tokens per Sec:     5638 || Lr: 0.000025
2024-02-08 17:08:07,093 Validation result at epoch 1463, step    98000: duration: 8.5144s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.30694	Translation Loss: 95593.66406	PPL: 14015.35547
	Eval Metric: BLEU
	WER 2.90	(DEL: 0.00,	INS: 0.00,	SUB: 2.90)
	BLEU-4 0.60	(BLEU-1: 10.23,	BLEU-2: 3.22,	BLEU-3: 1.23,	BLEU-4: 0.60)
	CHRF 16.67	ROUGE 8.58
2024-02-08 17:08:07,094 Logging Recognition and Translation Outputs
2024-02-08 17:08:07,094 ========================================================================================================================
2024-02-08 17:08:07,094 Logging Sequence: 82_81.00
2024-02-08 17:08:07,095 	Gloss Reference :	A B+C+D+E
2024-02-08 17:08:07,095 	Gloss Hypothesis:	A B+C+D  
2024-02-08 17:08:07,095 	Gloss Alignment :	  S      
2024-02-08 17:08:07,095 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 17:08:07,096 	Text Reference  :	since the couple were residents of mumbai the mumbai police cyber cell    began  investigating the matter 
2024-02-08 17:08:07,096 	Text Hypothesis :	***** *** ****** **** ********* ** ****** *** ****** ****** then  gujarat titans only          2   matches
2024-02-08 17:08:07,096 	Text Alignment  :	D     D   D      D    D         D  D      D   D      D      S     S       S      S             S   S      
2024-02-08 17:08:07,096 ========================================================================================================================
2024-02-08 17:08:07,096 Logging Sequence: 155_39.00
2024-02-08 17:08:07,096 	Gloss Reference :	A B+C+D+E
2024-02-08 17:08:07,097 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 17:08:07,097 	Gloss Alignment :	         
2024-02-08 17:08:07,097 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 17:08:07,099 	Text Reference  :	taliban considers itself as  the  government however there is no    actual government hence icc  would decide on  the inclusion of  the   team       
2024-02-08 17:08:07,099 	Text Hypothesis :	******* ********* ****** and they confirmed  they    had   a  great time   spending   the   game that  it     was b   for       its tough competition
2024-02-08 17:08:07,099 	Text Alignment  :	D       D         D      S   S    S          S       S     S  S     S      S          S     S    S     S      S   S   S         S   S     S          
2024-02-08 17:08:07,100 ========================================================================================================================
2024-02-08 17:08:07,100 Logging Sequence: 144_2.00
2024-02-08 17:08:07,100 	Gloss Reference :	A B+C+D+E  
2024-02-08 17:08:07,100 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-08 17:08:07,100 	Gloss Alignment :	  S        
2024-02-08 17:08:07,100 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 17:08:07,102 	Text Reference  :	a girl posted a video of herself playing  cricket on    a     village farm on social media the video     has      gone viral
2024-02-08 17:08:07,102 	Text Hypothesis :	* **** ****** * ***** ** ******* whenever anyone  talks about people  but  i  can't  video of  spreading pictures of   yadav
2024-02-08 17:08:07,102 	Text Alignment  :	D D    D      D D     D  D       S        S       S     S     S       S    S  S      S     S   S         S        S    S    
2024-02-08 17:08:07,102 ========================================================================================================================
2024-02-08 17:08:07,103 Logging Sequence: 105_104.00
2024-02-08 17:08:07,103 	Gloss Reference :	A B+C+D+E
2024-02-08 17:08:07,103 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 17:08:07,103 	Gloss Alignment :	         
2024-02-08 17:08:07,103 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 17:08:07,104 	Text Reference  :	four year  back when praggnanandhaa was ****** ** ***** **** **** ** 12   year old  
2024-02-08 17:08:07,104 	Text Hypothesis :	**** after her  no   one            was bowled by patel said that he lost the  match
2024-02-08 17:08:07,105 	Text Alignment  :	D    S     S    S    S                  I      I  I     I    I    I  S    S    S    
2024-02-08 17:08:07,105 ========================================================================================================================
2024-02-08 17:08:07,105 Logging Sequence: 71_149.00
2024-02-08 17:08:07,105 	Gloss Reference :	A B+C+D+E
2024-02-08 17:08:07,105 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 17:08:07,105 	Gloss Alignment :	         
2024-02-08 17:08:07,106 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 17:08:07,107 	Text Reference  :	his coach sanjay had suggested his  name for   the madhya  pradesh ranji  trophy  team       
2024-02-08 17:08:07,107 	Text Hypothesis :	*** ***** ****** *** just      like to   thank a   picture of      women' cricket association
2024-02-08 17:08:07,107 	Text Alignment  :	D   D     D      D   S         S    S    S     S   S       S       S      S       S          
2024-02-08 17:08:07,107 ========================================================================================================================
2024-02-08 17:08:08,829 Epoch 1463: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.13 
2024-02-08 17:08:08,830 EPOCH 1464
2024-02-08 17:08:14,446 Epoch 1464: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.18 
2024-02-08 17:08:14,447 EPOCH 1465
2024-02-08 17:08:15,277 [Epoch: 1465 Step: 00098100] Batch Recognition Loss:   0.000111 => Gls Tokens per Sec:     2316 || Batch Translation Loss:   0.011766 => Txt Tokens per Sec:     6121 || Lr: 0.000025
2024-02-08 17:08:19,396 Epoch 1465: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.54 
2024-02-08 17:08:19,397 EPOCH 1466
2024-02-08 17:08:23,086 [Epoch: 1466 Step: 00098200] Batch Recognition Loss:   0.000337 => Gls Tokens per Sec:     1925 || Batch Translation Loss:   0.027696 => Txt Tokens per Sec:     5231 || Lr: 0.000025
2024-02-08 17:08:24,896 Epoch 1466: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.57 
2024-02-08 17:08:24,896 EPOCH 1467
2024-02-08 17:08:29,656 Epoch 1467: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.33 
2024-02-08 17:08:29,656 EPOCH 1468
2024-02-08 17:08:30,498 [Epoch: 1468 Step: 00098300] Batch Recognition Loss:   0.000173 => Gls Tokens per Sec:     2095 || Batch Translation Loss:   0.012021 => Txt Tokens per Sec:     5310 || Lr: 0.000025
2024-02-08 17:08:35,365 Epoch 1468: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.76 
2024-02-08 17:08:35,366 EPOCH 1469
2024-02-08 17:08:38,993 [Epoch: 1469 Step: 00098400] Batch Recognition Loss:   0.000136 => Gls Tokens per Sec:     1914 || Batch Translation Loss:   0.021084 => Txt Tokens per Sec:     5378 || Lr: 0.000025
2024-02-08 17:08:41,003 Epoch 1469: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.02 
2024-02-08 17:08:41,004 EPOCH 1470
2024-02-08 17:08:46,680 Epoch 1470: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.22 
2024-02-08 17:08:46,681 EPOCH 1471
2024-02-08 17:08:47,359 [Epoch: 1471 Step: 00098500] Batch Recognition Loss:   0.000827 => Gls Tokens per Sec:     2360 || Batch Translation Loss:   0.012412 => Txt Tokens per Sec:     6394 || Lr: 0.000025
2024-02-08 17:08:51,551 Epoch 1471: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.87 
2024-02-08 17:08:51,551 EPOCH 1472
2024-02-08 17:08:55,025 [Epoch: 1472 Step: 00098600] Batch Recognition Loss:   0.000636 => Gls Tokens per Sec:     1981 || Batch Translation Loss:   0.023123 => Txt Tokens per Sec:     5600 || Lr: 0.000025
2024-02-08 17:08:56,969 Epoch 1472: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.43 
2024-02-08 17:08:56,969 EPOCH 1473
2024-02-08 17:09:02,165 Epoch 1473: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.66 
2024-02-08 17:09:02,166 EPOCH 1474
2024-02-08 17:09:02,819 [Epoch: 1474 Step: 00098700] Batch Recognition Loss:   0.001023 => Gls Tokens per Sec:     2209 || Batch Translation Loss:   0.026187 => Txt Tokens per Sec:     6198 || Lr: 0.000025
2024-02-08 17:09:07,881 Epoch 1474: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.42 
2024-02-08 17:09:07,881 EPOCH 1475
2024-02-08 17:09:11,302 [Epoch: 1475 Step: 00098800] Batch Recognition Loss:   0.000368 => Gls Tokens per Sec:     1965 || Batch Translation Loss:   0.027504 => Txt Tokens per Sec:     5439 || Lr: 0.000025
2024-02-08 17:09:13,305 Epoch 1475: Total Training Recognition Loss 0.19  Total Training Translation Loss 1.33 
2024-02-08 17:09:13,306 EPOCH 1476
2024-02-08 17:09:18,725 Epoch 1476: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.36 
2024-02-08 17:09:18,726 EPOCH 1477
2024-02-08 17:09:19,257 [Epoch: 1477 Step: 00098900] Batch Recognition Loss:   0.000155 => Gls Tokens per Sec:     2415 || Batch Translation Loss:   0.013919 => Txt Tokens per Sec:     6523 || Lr: 0.000025
2024-02-08 17:09:24,073 Epoch 1477: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.51 
2024-02-08 17:09:24,074 EPOCH 1478
2024-02-08 17:09:27,479 [Epoch: 1478 Step: 00099000] Batch Recognition Loss:   0.000241 => Gls Tokens per Sec:     1898 || Batch Translation Loss:   0.018115 => Txt Tokens per Sec:     5294 || Lr: 0.000025
2024-02-08 17:09:29,519 Epoch 1478: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.28 
2024-02-08 17:09:29,519 EPOCH 1479
2024-02-08 17:09:34,627 Epoch 1479: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.27 
2024-02-08 17:09:34,628 EPOCH 1480
2024-02-08 17:09:35,138 [Epoch: 1480 Step: 00099100] Batch Recognition Loss:   0.000168 => Gls Tokens per Sec:     2205 || Batch Translation Loss:   0.012560 => Txt Tokens per Sec:     5835 || Lr: 0.000025
2024-02-08 17:09:39,997 Epoch 1480: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.32 
2024-02-08 17:09:39,997 EPOCH 1481
2024-02-08 17:09:43,144 [Epoch: 1481 Step: 00099200] Batch Recognition Loss:   0.000820 => Gls Tokens per Sec:     2036 || Batch Translation Loss:   0.021516 => Txt Tokens per Sec:     5792 || Lr: 0.000025
2024-02-08 17:09:45,354 Epoch 1481: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.27 
2024-02-08 17:09:45,355 EPOCH 1482
2024-02-08 17:09:50,929 Epoch 1482: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.22 
2024-02-08 17:09:50,929 EPOCH 1483
2024-02-08 17:09:51,389 [Epoch: 1483 Step: 00099300] Batch Recognition Loss:   0.000105 => Gls Tokens per Sec:     1874 || Batch Translation Loss:   0.012029 => Txt Tokens per Sec:     5054 || Lr: 0.000025
2024-02-08 17:09:56,195 Epoch 1483: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.98 
2024-02-08 17:09:56,196 EPOCH 1484
2024-02-08 17:09:59,498 [Epoch: 1484 Step: 00099400] Batch Recognition Loss:   0.000214 => Gls Tokens per Sec:     1890 || Batch Translation Loss:   0.095526 => Txt Tokens per Sec:     5427 || Lr: 0.000025
2024-02-08 17:10:01,620 Epoch 1484: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.07 
2024-02-08 17:10:01,621 EPOCH 1485
2024-02-08 17:10:06,629 Epoch 1485: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.37 
2024-02-08 17:10:06,629 EPOCH 1486
2024-02-08 17:10:06,978 [Epoch: 1486 Step: 00099500] Batch Recognition Loss:   0.000210 => Gls Tokens per Sec:     2299 || Batch Translation Loss:   0.056607 => Txt Tokens per Sec:     6270 || Lr: 0.000025
2024-02-08 17:10:11,987 Epoch 1486: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.59 
2024-02-08 17:10:11,987 EPOCH 1487
2024-02-08 17:10:14,963 [Epoch: 1487 Step: 00099600] Batch Recognition Loss:   0.000427 => Gls Tokens per Sec:     2010 || Batch Translation Loss:   0.015961 => Txt Tokens per Sec:     5538 || Lr: 0.000025
2024-02-08 17:10:17,179 Epoch 1487: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.25 
2024-02-08 17:10:17,179 EPOCH 1488
2024-02-08 17:10:22,518 Epoch 1488: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.19 
2024-02-08 17:10:22,519 EPOCH 1489
2024-02-08 17:10:22,918 [Epoch: 1489 Step: 00099700] Batch Recognition Loss:   0.000174 => Gls Tokens per Sec:     1608 || Batch Translation Loss:   0.046576 => Txt Tokens per Sec:     5173 || Lr: 0.000025
2024-02-08 17:10:27,903 Epoch 1489: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.25 
2024-02-08 17:10:27,904 EPOCH 1490
2024-02-08 17:10:31,098 [Epoch: 1490 Step: 00099800] Batch Recognition Loss:   0.000257 => Gls Tokens per Sec:     1823 || Batch Translation Loss:   0.014389 => Txt Tokens per Sec:     5167 || Lr: 0.000025
2024-02-08 17:10:33,469 Epoch 1490: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.16 
2024-02-08 17:10:33,469 EPOCH 1491
2024-02-08 17:10:38,804 Epoch 1491: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.10 
2024-02-08 17:10:38,805 EPOCH 1492
2024-02-08 17:10:39,054 [Epoch: 1492 Step: 00099900] Batch Recognition Loss:   0.000227 => Gls Tokens per Sec:     1935 || Batch Translation Loss:   0.013526 => Txt Tokens per Sec:     5552 || Lr: 0.000025
2024-02-08 17:10:44,398 Epoch 1492: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.13 
2024-02-08 17:10:44,399 EPOCH 1493
2024-02-08 17:10:46,943 [Epoch: 1493 Step: 00100000] Batch Recognition Loss:   0.000130 => Gls Tokens per Sec:     2226 || Batch Translation Loss:   0.014630 => Txt Tokens per Sec:     6193 || Lr: 0.000025
2024-02-08 17:10:55,649 Validation result at epoch 1493, step   100000: duration: 8.7060s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.19674	Translation Loss: 95708.82031	PPL: 14177.48145
	Eval Metric: BLEU
	WER 3.11	(DEL: 0.00,	INS: 0.00,	SUB: 3.11)
	BLEU-4 0.47	(BLEU-1: 10.33,	BLEU-2: 3.29,	BLEU-3: 1.22,	BLEU-4: 0.47)
	CHRF 16.77	ROUGE 8.73
2024-02-08 17:10:55,650 Logging Recognition and Translation Outputs
2024-02-08 17:10:55,650 ========================================================================================================================
2024-02-08 17:10:55,650 Logging Sequence: 77_60.00
2024-02-08 17:10:55,650 	Gloss Reference :	A B+C+D+E
2024-02-08 17:10:55,651 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 17:10:55,651 	Gloss Alignment :	         
2024-02-08 17:10:55,651 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 17:10:55,651 	Text Reference  :	**** *** ******* *** he  remained not out  
2024-02-08 17:10:55,652 	Text Hypothesis :	this was waiting for the start    of  india
2024-02-08 17:10:55,652 	Text Alignment  :	I    I   I       I   S   S        S   S    
2024-02-08 17:10:55,652 ========================================================================================================================
2024-02-08 17:10:55,652 Logging Sequence: 81_8.00
2024-02-08 17:10:55,652 	Gloss Reference :	A B+C+D+E
2024-02-08 17:10:55,652 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 17:10:55,652 	Gloss Alignment :	         
2024-02-08 17:10:55,652 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 17:10:55,653 	Text Reference  :	have been involved in a huge controversy in   connection to real estate developer amrapali group since        last     7   years
2024-02-08 17:10:55,654 	Text Hypothesis :	**** **** ******** ** * **** he          also decided    to **** ****** ********* ******** ***** indefinitely postpone ipl 2021 
2024-02-08 17:10:55,654 	Text Alignment  :	D    D    D        D  D D    S           S    S             D    D      D         D        D     S            S        S   S    
2024-02-08 17:10:55,654 ========================================================================================================================
2024-02-08 17:10:55,654 Logging Sequence: 87_63.00
2024-02-08 17:10:55,654 	Gloss Reference :	A B+C+D+E
2024-02-08 17:10:55,654 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 17:10:55,654 	Gloss Alignment :	         
2024-02-08 17:10:55,654 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 17:10:55,655 	Text Reference  :	he even hit a massive six  which     helped india    win           the match
2024-02-08 17:10:55,655 	Text Hypothesis :	he **** *** * has     been embroiled in     multiple controversies as  well 
2024-02-08 17:10:55,656 	Text Alignment  :	   D    D   D S       S    S         S      S        S             S   S    
2024-02-08 17:10:55,656 ========================================================================================================================
2024-02-08 17:10:55,656 Logging Sequence: 126_231.00
2024-02-08 17:10:55,656 	Gloss Reference :	A B+C+D+E
2024-02-08 17:10:55,656 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 17:10:55,656 	Gloss Alignment :	         
2024-02-08 17:10:55,656 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 17:10:55,659 	Text Reference  :	and will be   creating a      special jersey   with number 8758 commemorating neeraj's throw distance that won him    the gold medal 
2024-02-08 17:10:55,659 	Text Hypothesis :	*** **** this was      india' best    olympics as   india  won  7             medals   -     1        gold 2   silver and 4    bronze
2024-02-08 17:10:55,659 	Text Alignment  :	D   D    S    S        S      S       S        S    S      S    S             S        S     S        S    S   S      S   S    S     
2024-02-08 17:10:55,659 ========================================================================================================================
2024-02-08 17:10:55,659 Logging Sequence: 121_164.00
2024-02-08 17:10:55,659 	Gloss Reference :	A B+C+D+E
2024-02-08 17:10:55,659 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 17:10:55,659 	Gloss Alignment :	         
2024-02-08 17:10:55,660 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 17:10:55,660 	Text Reference  :	******* *** * mirabai chanu  returned to  india  on     26   july evening
2024-02-08 17:10:55,661 	Text Hypothesis :	however for 3 weeks   sushil kumar    has evaded arrest from the  police 
2024-02-08 17:10:55,661 	Text Alignment  :	I       I   I S       S      S        S   S      S      S    S    S      
2024-02-08 17:10:55,661 ========================================================================================================================
2024-02-08 17:10:58,230 Epoch 1493: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.05 
2024-02-08 17:10:58,230 EPOCH 1494
2024-02-08 17:11:03,785 Epoch 1494: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.04 
2024-02-08 17:11:03,785 EPOCH 1495
2024-02-08 17:11:03,940 [Epoch: 1495 Step: 00100100] Batch Recognition Loss:   0.000141 => Gls Tokens per Sec:     2078 || Batch Translation Loss:   0.012736 => Txt Tokens per Sec:     5461 || Lr: 0.000025
2024-02-08 17:11:09,225 Epoch 1495: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.01 
2024-02-08 17:11:09,226 EPOCH 1496
2024-02-08 17:11:12,097 [Epoch: 1496 Step: 00100200] Batch Recognition Loss:   0.000211 => Gls Tokens per Sec:     1916 || Batch Translation Loss:   0.018412 => Txt Tokens per Sec:     5362 || Lr: 0.000025
2024-02-08 17:11:14,641 Epoch 1496: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.04 
2024-02-08 17:11:14,642 EPOCH 1497
2024-02-08 17:11:20,175 Epoch 1497: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.01 
2024-02-08 17:11:20,176 EPOCH 1498
2024-02-08 17:11:20,223 [Epoch: 1498 Step: 00100300] Batch Recognition Loss:   0.000141 => Gls Tokens per Sec:     3478 || Batch Translation Loss:   0.010277 => Txt Tokens per Sec:     7435 || Lr: 0.000025
2024-02-08 17:11:24,958 Epoch 1498: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.18 
2024-02-08 17:11:24,958 EPOCH 1499
2024-02-08 17:11:28,097 [Epoch: 1499 Step: 00100400] Batch Recognition Loss:   0.000628 => Gls Tokens per Sec:     1734 || Batch Translation Loss:   0.113200 => Txt Tokens per Sec:     4855 || Lr: 0.000025
2024-02-08 17:11:30,567 Epoch 1499: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.12 
2024-02-08 17:11:30,567 EPOCH 1500
2024-02-08 17:11:35,966 [Epoch: 1500 Step: 00100500] Batch Recognition Loss:   0.000216 => Gls Tokens per Sec:     1967 || Batch Translation Loss:   0.011813 => Txt Tokens per Sec:     5443 || Lr: 0.000025
2024-02-08 17:11:35,967 Epoch 1500: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.94 
2024-02-08 17:11:35,967 EPOCH 1501
2024-02-08 17:11:41,415 Epoch 1501: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.10 
2024-02-08 17:11:41,416 EPOCH 1502
2024-02-08 17:11:43,770 [Epoch: 1502 Step: 00100600] Batch Recognition Loss:   0.000201 => Gls Tokens per Sec:     2244 || Batch Translation Loss:   0.011792 => Txt Tokens per Sec:     6261 || Lr: 0.000025
2024-02-08 17:11:46,157 Epoch 1502: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.39 
2024-02-08 17:11:46,157 EPOCH 1503
2024-02-08 17:11:51,730 [Epoch: 1503 Step: 00100700] Batch Recognition Loss:   0.000736 => Gls Tokens per Sec:     1877 || Batch Translation Loss:   0.029009 => Txt Tokens per Sec:     5191 || Lr: 0.000025
2024-02-08 17:11:51,808 Epoch 1503: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.31 
2024-02-08 17:11:51,808 EPOCH 1504
2024-02-08 17:11:57,529 Epoch 1504: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.29 
2024-02-08 17:11:57,530 EPOCH 1505
2024-02-08 17:12:00,031 [Epoch: 1505 Step: 00100800] Batch Recognition Loss:   0.000527 => Gls Tokens per Sec:     2008 || Batch Translation Loss:   0.025286 => Txt Tokens per Sec:     5602 || Lr: 0.000025
2024-02-08 17:12:02,792 Epoch 1505: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.83 
2024-02-08 17:12:02,792 EPOCH 1506
2024-02-08 17:12:07,492 [Epoch: 1506 Step: 00100900] Batch Recognition Loss:   0.002815 => Gls Tokens per Sec:     2192 || Batch Translation Loss:   0.039830 => Txt Tokens per Sec:     6049 || Lr: 0.000025
2024-02-08 17:12:07,662 Epoch 1506: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.32 
2024-02-08 17:12:07,663 EPOCH 1507
2024-02-08 17:12:13,236 Epoch 1507: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.76 
2024-02-08 17:12:13,237 EPOCH 1508
2024-02-08 17:12:15,403 [Epoch: 1508 Step: 00101000] Batch Recognition Loss:   0.000638 => Gls Tokens per Sec:     2291 || Batch Translation Loss:   0.038888 => Txt Tokens per Sec:     6295 || Lr: 0.000025
2024-02-08 17:12:18,207 Epoch 1508: Total Training Recognition Loss 0.07  Total Training Translation Loss 4.82 
2024-02-08 17:12:18,208 EPOCH 1509
2024-02-08 17:12:23,656 [Epoch: 1509 Step: 00101100] Batch Recognition Loss:   0.000255 => Gls Tokens per Sec:     1862 || Batch Translation Loss:   0.023897 => Txt Tokens per Sec:     5099 || Lr: 0.000025
2024-02-08 17:12:23,953 Epoch 1509: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.66 
2024-02-08 17:12:23,953 EPOCH 1510
2024-02-08 17:12:29,368 Epoch 1510: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.42 
2024-02-08 17:12:29,369 EPOCH 1511
2024-02-08 17:12:31,613 [Epoch: 1511 Step: 00101200] Batch Recognition Loss:   0.000175 => Gls Tokens per Sec:     2095 || Batch Translation Loss:   0.013331 => Txt Tokens per Sec:     5815 || Lr: 0.000025
2024-02-08 17:12:34,698 Epoch 1511: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.33 
2024-02-08 17:12:34,698 EPOCH 1512
2024-02-08 17:12:40,088 [Epoch: 1512 Step: 00101300] Batch Recognition Loss:   0.000160 => Gls Tokens per Sec:     1852 || Batch Translation Loss:   0.013787 => Txt Tokens per Sec:     5129 || Lr: 0.000025
2024-02-08 17:12:40,373 Epoch 1512: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.20 
2024-02-08 17:12:40,374 EPOCH 1513
2024-02-08 17:12:45,072 Epoch 1513: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.10 
2024-02-08 17:12:45,073 EPOCH 1514
2024-02-08 17:12:47,512 [Epoch: 1514 Step: 00101400] Batch Recognition Loss:   0.000205 => Gls Tokens per Sec:     1903 || Batch Translation Loss:   0.015335 => Txt Tokens per Sec:     5234 || Lr: 0.000025
2024-02-08 17:12:50,643 Epoch 1514: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.15 
2024-02-08 17:12:50,643 EPOCH 1515
2024-02-08 17:12:55,031 [Epoch: 1515 Step: 00101500] Batch Recognition Loss:   0.000157 => Gls Tokens per Sec:     2238 || Batch Translation Loss:   0.015971 => Txt Tokens per Sec:     6160 || Lr: 0.000025
2024-02-08 17:12:55,394 Epoch 1515: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.25 
2024-02-08 17:12:55,394 EPOCH 1516
2024-02-08 17:13:00,311 Epoch 1516: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.31 
2024-02-08 17:13:00,312 EPOCH 1517
2024-02-08 17:13:02,782 [Epoch: 1517 Step: 00101600] Batch Recognition Loss:   0.000198 => Gls Tokens per Sec:     1815 || Batch Translation Loss:   0.033893 => Txt Tokens per Sec:     5078 || Lr: 0.000025
2024-02-08 17:13:05,943 Epoch 1517: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.19 
2024-02-08 17:13:05,943 EPOCH 1518
2024-02-08 17:13:10,277 [Epoch: 1518 Step: 00101700] Batch Recognition Loss:   0.000324 => Gls Tokens per Sec:     2230 || Batch Translation Loss:   0.028819 => Txt Tokens per Sec:     6175 || Lr: 0.000025
2024-02-08 17:13:10,823 Epoch 1518: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.12 
2024-02-08 17:13:10,824 EPOCH 1519
2024-02-08 17:13:16,346 Epoch 1519: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.16 
2024-02-08 17:13:16,346 EPOCH 1520
2024-02-08 17:13:18,144 [Epoch: 1520 Step: 00101800] Batch Recognition Loss:   0.000191 => Gls Tokens per Sec:     2348 || Batch Translation Loss:   0.018464 => Txt Tokens per Sec:     6410 || Lr: 0.000025
2024-02-08 17:13:20,976 Epoch 1520: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.16 
2024-02-08 17:13:20,976 EPOCH 1521
2024-02-08 17:13:26,078 [Epoch: 1521 Step: 00101900] Batch Recognition Loss:   0.000098 => Gls Tokens per Sec:     1862 || Batch Translation Loss:   0.013674 => Txt Tokens per Sec:     5160 || Lr: 0.000025
2024-02-08 17:13:26,585 Epoch 1521: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.21 
2024-02-08 17:13:26,585 EPOCH 1522
2024-02-08 17:13:31,546 Epoch 1522: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.11 
2024-02-08 17:13:31,546 EPOCH 1523
2024-02-08 17:13:33,323 [Epoch: 1523 Step: 00102000] Batch Recognition Loss:   0.000127 => Gls Tokens per Sec:     2286 || Batch Translation Loss:   0.011199 => Txt Tokens per Sec:     6275 || Lr: 0.000025
2024-02-08 17:13:41,751 Validation result at epoch 1523, step   102000: duration: 8.4270s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.21011	Translation Loss: 96323.50000	PPL: 15075.17773
	Eval Metric: BLEU
	WER 3.11	(DEL: 0.00,	INS: 0.00,	SUB: 3.11)
	BLEU-4 0.59	(BLEU-1: 10.45,	BLEU-2: 3.20,	BLEU-3: 1.20,	BLEU-4: 0.59)
	CHRF 16.62	ROUGE 8.94
2024-02-08 17:13:41,752 Logging Recognition and Translation Outputs
2024-02-08 17:13:41,752 ========================================================================================================================
2024-02-08 17:13:41,752 Logging Sequence: 168_63.00
2024-02-08 17:13:41,753 	Gloss Reference :	A B+C+D+E
2024-02-08 17:13:41,753 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 17:13:41,753 	Gloss Alignment :	         
2024-02-08 17:13:41,753 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 17:13:41,754 	Text Reference  :	kohli has always   been very    protective of vamika and never lets the ********* paps     see   her     
2024-02-08 17:13:41,754 	Text Hypothesis :	***** the document will contain details    of ****** *** ***** **** the company's finances staff salaries
2024-02-08 17:13:41,754 	Text Alignment  :	D     S   S        S    S       S             D      D   D     D        I         S        S     S       
2024-02-08 17:13:41,754 ========================================================================================================================
2024-02-08 17:13:41,754 Logging Sequence: 94_136.00
2024-02-08 17:13:41,755 	Gloss Reference :	A B+C+D+E
2024-02-08 17:13:41,755 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 17:13:41,755 	Gloss Alignment :	         
2024-02-08 17:13:41,755 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 17:13:41,756 	Text Reference  :	***** *** * similarly on    12th november 2023  england-pakistan match   as         scheduled in west bengal
2024-02-08 17:13:41,756 	Text Hypothesis :	india won a bronze    medal at   the      world deaf             cricket tournament held      in **** 2019  
2024-02-08 17:13:41,756 	Text Alignment  :	I     I   I S         S     S    S        S     S                S       S          S            D    S     
2024-02-08 17:13:41,756 ========================================================================================================================
2024-02-08 17:13:41,756 Logging Sequence: 51_152.00
2024-02-08 17:13:41,757 	Gloss Reference :	A B+C+D+E
2024-02-08 17:13:41,757 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 17:13:41,757 	Gloss Alignment :	         
2024-02-08 17:13:41,757 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 17:13:41,758 	Text Reference  :	australian players did  this to    celebrate their victory in     their      cultural way   
2024-02-08 17:13:41,758 	Text Hypothesis :	********** ******* they are  aware that      even  south   africa bangladesh south    africa
2024-02-08 17:13:41,758 	Text Alignment  :	D          D       S    S    S     S         S     S       S      S          S        S     
2024-02-08 17:13:41,758 ========================================================================================================================
2024-02-08 17:13:41,758 Logging Sequence: 112_2.00
2024-02-08 17:13:41,759 	Gloss Reference :	A B+C+D+E  
2024-02-08 17:13:41,759 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-08 17:13:41,759 	Gloss Alignment :	  S        
2024-02-08 17:13:41,759 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 17:13:41,760 	Text Reference  :	earlier the bcci had announced that the ipl in 2022      will have 2 more teams compared to the    8   teams earlier
2024-02-08 17:13:41,760 	Text Hypothesis :	******* *** **** *** ********* **** in  ipl ** currently his  have * **** ***** ******** ** mumbai and 1     death  
2024-02-08 17:13:41,760 	Text Alignment  :	D       D   D    D   D         D    S       D  S         S         D D    D     D        D  S      S   S     S      
2024-02-08 17:13:41,760 ========================================================================================================================
2024-02-08 17:13:41,760 Logging Sequence: 183_99.00
2024-02-08 17:13:41,761 	Gloss Reference :	A B+C+D+E
2024-02-08 17:13:41,761 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 17:13:41,761 	Gloss Alignment :	         
2024-02-08 17:13:41,761 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 17:13:41,762 	Text Reference  :	yuvraj decided to use 'keech' as the middle name as       it's his wife's surname the   post was widely shared  
2024-02-08 17:13:41,762 	Text Hypothesis :	****** ******* ** *** ******* ** *** he     is   survived by   his ****** wife    their son  and a      daughter
2024-02-08 17:13:41,763 	Text Alignment  :	D      D       D  D   D       D  D   S      S    S        S        D      S       S     S    S   S      S       
2024-02-08 17:13:41,763 ========================================================================================================================
2024-02-08 17:13:45,344 Epoch 1523: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.37 
2024-02-08 17:13:45,345 EPOCH 1524
2024-02-08 17:13:50,196 [Epoch: 1524 Step: 00102100] Batch Recognition Loss:   0.000156 => Gls Tokens per Sec:     1926 || Batch Translation Loss:   0.013332 => Txt Tokens per Sec:     5332 || Lr: 0.000025
2024-02-08 17:13:50,839 Epoch 1524: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.59 
2024-02-08 17:13:50,839 EPOCH 1525
2024-02-08 17:13:56,364 Epoch 1525: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.45 
2024-02-08 17:13:56,365 EPOCH 1526
2024-02-08 17:13:58,505 [Epoch: 1526 Step: 00102200] Batch Recognition Loss:   0.000130 => Gls Tokens per Sec:     1871 || Batch Translation Loss:   0.020259 => Txt Tokens per Sec:     5469 || Lr: 0.000025
2024-02-08 17:14:01,617 Epoch 1526: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.52 
2024-02-08 17:14:01,617 EPOCH 1527
2024-02-08 17:14:06,151 [Epoch: 1527 Step: 00102300] Batch Recognition Loss:   0.000249 => Gls Tokens per Sec:     2025 || Batch Translation Loss:   0.015543 => Txt Tokens per Sec:     5559 || Lr: 0.000025
2024-02-08 17:14:06,841 Epoch 1527: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.27 
2024-02-08 17:14:06,841 EPOCH 1528
2024-02-08 17:14:12,175 Epoch 1528: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.20 
2024-02-08 17:14:12,176 EPOCH 1529
2024-02-08 17:14:13,867 [Epoch: 1529 Step: 00102400] Batch Recognition Loss:   0.000615 => Gls Tokens per Sec:     2213 || Batch Translation Loss:   0.010686 => Txt Tokens per Sec:     5856 || Lr: 0.000025
2024-02-08 17:14:17,739 Epoch 1529: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.18 
2024-02-08 17:14:17,740 EPOCH 1530
2024-02-08 17:14:22,361 [Epoch: 1530 Step: 00102500] Batch Recognition Loss:   0.001666 => Gls Tokens per Sec:     1952 || Batch Translation Loss:   0.020171 => Txt Tokens per Sec:     5429 || Lr: 0.000025
2024-02-08 17:14:23,155 Epoch 1530: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.28 
2024-02-08 17:14:23,155 EPOCH 1531
2024-02-08 17:14:28,697 Epoch 1531: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.28 
2024-02-08 17:14:28,698 EPOCH 1532
2024-02-08 17:14:30,409 [Epoch: 1532 Step: 00102600] Batch Recognition Loss:   0.000175 => Gls Tokens per Sec:     2152 || Batch Translation Loss:   0.017561 => Txt Tokens per Sec:     6159 || Lr: 0.000025
2024-02-08 17:14:33,923 Epoch 1532: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.93 
2024-02-08 17:14:33,923 EPOCH 1533
2024-02-08 17:14:38,709 [Epoch: 1533 Step: 00102700] Batch Recognition Loss:   0.000374 => Gls Tokens per Sec:     1852 || Batch Translation Loss:   0.027505 => Txt Tokens per Sec:     5194 || Lr: 0.000025
2024-02-08 17:14:39,583 Epoch 1533: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.86 
2024-02-08 17:14:39,584 EPOCH 1534
2024-02-08 17:14:45,282 Epoch 1534: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.86 
2024-02-08 17:14:45,283 EPOCH 1535
2024-02-08 17:14:47,157 [Epoch: 1535 Step: 00102800] Batch Recognition Loss:   0.000238 => Gls Tokens per Sec:     1879 || Batch Translation Loss:   0.012268 => Txt Tokens per Sec:     4987 || Lr: 0.000025
2024-02-08 17:14:50,990 Epoch 1535: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.38 
2024-02-08 17:14:50,990 EPOCH 1536
2024-02-08 17:14:55,428 [Epoch: 1536 Step: 00102900] Batch Recognition Loss:   0.000126 => Gls Tokens per Sec:     1961 || Batch Translation Loss:   0.011733 => Txt Tokens per Sec:     5400 || Lr: 0.000025
2024-02-08 17:14:56,397 Epoch 1536: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.61 
2024-02-08 17:14:56,398 EPOCH 1537
2024-02-08 17:15:01,918 Epoch 1537: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.39 
2024-02-08 17:15:01,919 EPOCH 1538
2024-02-08 17:15:03,666 [Epoch: 1538 Step: 00103000] Batch Recognition Loss:   0.000236 => Gls Tokens per Sec:     1923 || Batch Translation Loss:   0.013755 => Txt Tokens per Sec:     5329 || Lr: 0.000025
2024-02-08 17:15:07,584 Epoch 1538: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.16 
2024-02-08 17:15:07,585 EPOCH 1539
2024-02-08 17:15:12,085 [Epoch: 1539 Step: 00103100] Batch Recognition Loss:   0.000196 => Gls Tokens per Sec:     1898 || Batch Translation Loss:   0.013055 => Txt Tokens per Sec:     5358 || Lr: 0.000025
2024-02-08 17:15:12,987 Epoch 1539: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.17 
2024-02-08 17:15:12,988 EPOCH 1540
2024-02-08 17:15:18,635 Epoch 1540: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.32 
2024-02-08 17:15:18,636 EPOCH 1541
2024-02-08 17:15:20,337 [Epoch: 1541 Step: 00103200] Batch Recognition Loss:   0.000159 => Gls Tokens per Sec:     1882 || Batch Translation Loss:   0.019684 => Txt Tokens per Sec:     5408 || Lr: 0.000025
2024-02-08 17:15:23,879 Epoch 1541: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.14 
2024-02-08 17:15:23,880 EPOCH 1542
2024-02-08 17:15:28,428 [Epoch: 1542 Step: 00103300] Batch Recognition Loss:   0.000146 => Gls Tokens per Sec:     1843 || Batch Translation Loss:   0.014630 => Txt Tokens per Sec:     5160 || Lr: 0.000025
2024-02-08 17:15:29,466 Epoch 1542: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.07 
2024-02-08 17:15:29,466 EPOCH 1543
2024-02-08 17:15:35,007 Epoch 1543: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.16 
2024-02-08 17:15:35,007 EPOCH 1544
2024-02-08 17:15:36,522 [Epoch: 1544 Step: 00103400] Batch Recognition Loss:   0.000184 => Gls Tokens per Sec:     1942 || Batch Translation Loss:   0.008997 => Txt Tokens per Sec:     5314 || Lr: 0.000025
2024-02-08 17:15:40,458 Epoch 1544: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.23 
2024-02-08 17:15:40,459 EPOCH 1545
2024-02-08 17:15:44,655 [Epoch: 1545 Step: 00103500] Batch Recognition Loss:   0.000147 => Gls Tokens per Sec:     1959 || Batch Translation Loss:   0.013509 => Txt Tokens per Sec:     5446 || Lr: 0.000025
2024-02-08 17:15:45,838 Epoch 1545: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.28 
2024-02-08 17:15:45,838 EPOCH 1546
2024-02-08 17:15:51,256 Epoch 1546: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.96 
2024-02-08 17:15:51,256 EPOCH 1547
2024-02-08 17:15:52,586 [Epoch: 1547 Step: 00103600] Batch Recognition Loss:   0.000178 => Gls Tokens per Sec:     2092 || Batch Translation Loss:   0.019015 => Txt Tokens per Sec:     5604 || Lr: 0.000025
2024-02-08 17:15:56,512 Epoch 1547: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.78 
2024-02-08 17:15:56,512 EPOCH 1548
2024-02-08 17:16:00,576 [Epoch: 1548 Step: 00103700] Batch Recognition Loss:   0.000172 => Gls Tokens per Sec:     1983 || Batch Translation Loss:   0.018120 => Txt Tokens per Sec:     5418 || Lr: 0.000025
2024-02-08 17:16:02,007 Epoch 1548: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.63 
2024-02-08 17:16:02,007 EPOCH 1549
2024-02-08 17:16:07,007 Epoch 1549: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.66 
2024-02-08 17:16:07,007 EPOCH 1550
2024-02-08 17:16:08,188 [Epoch: 1550 Step: 00103800] Batch Recognition Loss:   0.006753 => Gls Tokens per Sec:     2305 || Batch Translation Loss:   0.042967 => Txt Tokens per Sec:     6531 || Lr: 0.000025
2024-02-08 17:16:12,278 Epoch 1550: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.57 
2024-02-08 17:16:12,279 EPOCH 1551
2024-02-08 17:16:16,378 [Epoch: 1551 Step: 00103900] Batch Recognition Loss:   0.000161 => Gls Tokens per Sec:     1928 || Batch Translation Loss:   0.024993 => Txt Tokens per Sec:     5313 || Lr: 0.000025
2024-02-08 17:16:17,757 Epoch 1551: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.80 
2024-02-08 17:16:17,757 EPOCH 1552
2024-02-08 17:16:23,312 Epoch 1552: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.57 
2024-02-08 17:16:23,313 EPOCH 1553
2024-02-08 17:16:24,648 [Epoch: 1553 Step: 00104000] Batch Recognition Loss:   0.000436 => Gls Tokens per Sec:     1844 || Batch Translation Loss:   0.010774 => Txt Tokens per Sec:     4822 || Lr: 0.000025
2024-02-08 17:16:33,483 Validation result at epoch 1553, step   104000: duration: 8.8340s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.16600	Translation Loss: 95590.35156	PPL: 14010.71777
	Eval Metric: BLEU
	WER 3.18	(DEL: 0.00,	INS: 0.00,	SUB: 3.18)
	BLEU-4 0.43	(BLEU-1: 10.31,	BLEU-2: 3.06,	BLEU-3: 1.06,	BLEU-4: 0.43)
	CHRF 16.86	ROUGE 8.53
2024-02-08 17:16:33,484 Logging Recognition and Translation Outputs
2024-02-08 17:16:33,485 ========================================================================================================================
2024-02-08 17:16:33,485 Logging Sequence: 87_164.00
2024-02-08 17:16:33,485 	Gloss Reference :	A B+C+D+E
2024-02-08 17:16:33,485 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 17:16:33,486 	Gloss Alignment :	         
2024-02-08 17:16:33,486 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 17:16:33,486 	Text Reference  :	** *** **** when gambhir was heading back from the field
2024-02-08 17:16:33,486 	Text Hypothesis :	do you know that there   was ******* a    huge fan 2021 
2024-02-08 17:16:33,487 	Text Alignment  :	I  I   I    S    S           D       S    S    S   S    
2024-02-08 17:16:33,487 ========================================================================================================================
2024-02-08 17:16:33,487 Logging Sequence: 67_73.00
2024-02-08 17:16:33,487 	Gloss Reference :	A B+C+D+E
2024-02-08 17:16:33,487 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 17:16:33,487 	Gloss Alignment :	         
2024-02-08 17:16:33,487 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 17:16:33,488 	Text Reference  :	*** **** **** ***** in  his tweet he   also said 
2024-02-08 17:16:33,488 	Text Hypothesis :	the just like there has a   just  like the  other
2024-02-08 17:16:33,488 	Text Alignment  :	I   I    I    I     S   S   S     S    S    S    
2024-02-08 17:16:33,488 ========================================================================================================================
2024-02-08 17:16:33,488 Logging Sequence: 128_98.00
2024-02-08 17:16:33,489 	Gloss Reference :	A B+C+D+E
2024-02-08 17:16:33,489 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 17:16:33,489 	Gloss Alignment :	         
2024-02-08 17:16:33,489 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 17:16:33,490 	Text Reference  :	with 8 wickets and 43 balls remaining they won the   match   in   such        a   short         time  
2024-02-08 17:16:33,490 	Text Hypothesis :	**** * ******* *** ** ***** ********* **** but their captain kane williamson' key contributions helped
2024-02-08 17:16:33,490 	Text Alignment  :	D    D D       D   D  D     D         D    S   S     S       S    S           S   S             S     
2024-02-08 17:16:33,490 ========================================================================================================================
2024-02-08 17:16:33,490 Logging Sequence: 58_112.00
2024-02-08 17:16:33,491 	Gloss Reference :	A B+C+D+E
2024-02-08 17:16:33,491 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 17:16:33,491 	Gloss Alignment :	         
2024-02-08 17:16:33,491 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 17:16:33,492 	Text Reference  :	**** what a   proud moment for  india have a   look at     all   the athletes    and   their accomplisments
2024-02-08 17:16:33,492 	Text Hypothesis :	2022 was  the first time   that india **** won 16   medals since our deaflympics debut in    1965          
2024-02-08 17:16:33,493 	Text Alignment  :	I    S    S   S     S      S          D    S   S    S      S     S   S           S     S     S             
2024-02-08 17:16:33,493 ========================================================================================================================
2024-02-08 17:16:33,493 Logging Sequence: 51_152.00
2024-02-08 17:16:33,493 	Gloss Reference :	A B+C+D+E
2024-02-08 17:16:33,493 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 17:16:33,493 	Gloss Alignment :	         
2024-02-08 17:16:33,493 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 17:16:33,494 	Text Reference  :	australian players did  this     to celebrate their   victory   in their cultural way        
2024-02-08 17:16:33,494 	Text Hypothesis :	********** they    were supposed to play      against australia in ***** the      semi-finals
2024-02-08 17:16:33,495 	Text Alignment  :	D          S       S    S           S         S       S            D     S        S          
2024-02-08 17:16:33,495 ========================================================================================================================
2024-02-08 17:16:33,499 Training ended since there were no improvements inthe last learning rate step: 0.000025
2024-02-08 17:16:33,500 Best validation result at step    52000:   0.96 eval_metric.
2024-02-08 17:16:59,080 ------------------------------------------------------------
2024-02-08 17:16:59,081 [DEV] partition [RECOGNITION] experiment [BW]: 1
2024-02-08 17:17:08,147 finished in 9.0660s 
2024-02-08 17:17:08,148 ************************************************************
2024-02-08 17:17:08,148 [DEV] partition [RECOGNITION] results:
	New Best CTC Decode Beam Size: 1
	WER 3.46	(DEL: 0.00,	INS: 0.00,	SUB: 3.46)
2024-02-08 17:17:08,148 ************************************************************
2024-02-08 17:17:08,148 ------------------------------------------------------------
2024-02-08 17:17:08,148 [DEV] partition [RECOGNITION] experiment [BW]: 2
2024-02-08 17:17:16,473 finished in 8.3253s 
2024-02-08 17:17:16,474 ------------------------------------------------------------
2024-02-08 17:17:16,474 [DEV] partition [RECOGNITION] experiment [BW]: 3
2024-02-08 17:17:24,709 finished in 8.2353s 
2024-02-08 17:17:24,709 ------------------------------------------------------------
2024-02-08 17:17:24,710 [DEV] partition [RECOGNITION] experiment [BW]: 4
2024-02-08 17:17:33,257 finished in 8.5465s 
2024-02-08 17:17:33,258 ------------------------------------------------------------
2024-02-08 17:17:33,258 [DEV] partition [RECOGNITION] experiment [BW]: 5
2024-02-08 17:17:41,658 finished in 8.3990s 
2024-02-08 17:17:41,658 ------------------------------------------------------------
2024-02-08 17:17:41,658 [DEV] partition [RECOGNITION] experiment [BW]: 6
2024-02-08 17:17:49,902 finished in 8.2440s 
2024-02-08 17:17:49,903 ------------------------------------------------------------
2024-02-08 17:17:49,903 [DEV] partition [RECOGNITION] experiment [BW]: 7
2024-02-08 17:17:58,562 finished in 8.6590s 
2024-02-08 17:17:58,563 ------------------------------------------------------------
2024-02-08 17:17:58,563 [DEV] partition [RECOGNITION] experiment [BW]: 8
2024-02-08 17:18:07,024 finished in 8.4618s 
2024-02-08 17:18:07,024 ------------------------------------------------------------
2024-02-08 17:18:07,024 [DEV] partition [RECOGNITION] experiment [BW]: 9
2024-02-08 17:18:16,522 finished in 9.4972s 
2024-02-08 17:18:16,522 ------------------------------------------------------------
2024-02-08 17:18:16,522 [DEV] partition [RECOGNITION] experiment [BW]: 10
2024-02-08 17:18:25,004 finished in 8.4822s 
2024-02-08 17:18:25,005 ============================================================
2024-02-08 17:18:33,197 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 1 and Alpha: -1
	BLEU-4 0.96	(BLEU-1: 12.10,	BLEU-2: 4.17,	BLEU-3: 1.82,	BLEU-4: 0.96)
	CHRF 17.34	ROUGE 10.18
2024-02-08 17:18:33,198 ------------------------------------------------------------
2024-02-08 17:19:32,921 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 2 and Alpha: -1
	BLEU-4 1.08	(BLEU-1: 11.01,	BLEU-2: 4.04,	BLEU-3: 1.94,	BLEU-4: 1.08)
	CHRF 16.93	ROUGE 9.83
2024-02-08 17:19:32,921 ------------------------------------------------------------
2024-02-08 17:19:52,620 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 2 and Alpha: 1
	BLEU-4 1.10	(BLEU-1: 11.37,	BLEU-2: 4.17,	BLEU-3: 1.98,	BLEU-4: 1.10)
	CHRF 17.14	ROUGE 9.97
2024-02-08 17:19:52,620 ------------------------------------------------------------
2024-02-08 17:20:02,494 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 2 and Alpha: 2
	BLEU-4 1.11	(BLEU-1: 11.55,	BLEU-2: 4.25,	BLEU-3: 2.01,	BLEU-4: 1.11)
	CHRF 17.22	ROUGE 10.01
2024-02-08 17:20:02,494 ------------------------------------------------------------
2024-02-08 17:20:13,240 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 2 and Alpha: 3
	BLEU-4 1.12	(BLEU-1: 11.59,	BLEU-2: 4.27,	BLEU-3: 2.03,	BLEU-4: 1.12)
	CHRF 17.21	ROUGE 10.00
2024-02-08 17:20:13,241 ------------------------------------------------------------
2024-02-08 17:20:32,672 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 2 and Alpha: 5
	BLEU-4 1.12	(BLEU-1: 11.63,	BLEU-2: 4.30,	BLEU-3: 2.04,	BLEU-4: 1.12)
	CHRF 17.22	ROUGE 9.97
2024-02-08 17:20:32,673 ------------------------------------------------------------
2024-02-08 17:34:24,509 ************************************************************
2024-02-08 17:34:24,510 [DEV] partition [Recognition & Translation] results:
	Best CTC Decode Beam Size: 1
	Best Translation Beam Size: 2 and Alpha: 5
	WER 3.46	(DEL: 0.00,	INS: 0.00,	SUB: 3.46)
	BLEU-4 1.12	(BLEU-1: 11.63,	BLEU-2: 4.30,	BLEU-3: 2.04,	BLEU-4: 1.12)
	CHRF 17.22	ROUGE 9.97
2024-02-08 17:34:24,510 ************************************************************
2024-02-08 17:34:34,525 [TEST] partition [Recognition & Translation] results:
	Best CTC Decode Beam Size: 1
	Best Translation Beam Size: 2 and Alpha: 5
	WER 2.47	(DEL: 0.00,	INS: 0.00,	SUB: 2.47)
	BLEU-4 0.61	(BLEU-1: 11.08,	BLEU-2: 3.44,	BLEU-3: 1.26,	BLEU-4: 0.61)
	CHRF 16.87	ROUGE 9.07
2024-02-08 17:34:34,525 ************************************************************
