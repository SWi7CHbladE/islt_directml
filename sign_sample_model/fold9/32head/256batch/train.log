2024-02-04 16:12:57,635 Hello! This is Joey-NMT.
2024-02-04 16:12:57,641 Total params: 25639944
2024-02-04 16:12:57,642 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'gloss_output_layer.bias', 'gloss_output_layer.weight', 'sgn_embed.ln.bias', 'sgn_embed.ln.weight', 'sgn_embed.norm.norm.bias', 'sgn_embed.norm.norm.weight', 'txt_embed.norm.norm.bias', 'txt_embed.norm.norm.weight']
2024-02-04 16:12:58,883 cfg.name                           : sign_experiment
2024-02-04 16:12:58,883 cfg.data.data_path                 : ./data/Sports_dataset/9/
2024-02-04 16:12:58,883 cfg.data.version                   : phoenix_2014_trans
2024-02-04 16:12:58,883 cfg.data.sgn                       : sign
2024-02-04 16:12:58,883 cfg.data.txt                       : text
2024-02-04 16:12:58,884 cfg.data.gls                       : gloss
2024-02-04 16:12:58,884 cfg.data.train                     : excel_data.train
2024-02-04 16:12:58,884 cfg.data.dev                       : excel_data.dev
2024-02-04 16:12:58,884 cfg.data.test                      : excel_data.test
2024-02-04 16:12:58,884 cfg.data.feature_size              : 2560
2024-02-04 16:12:58,884 cfg.data.level                     : word
2024-02-04 16:12:58,884 cfg.data.txt_lowercase             : True
2024-02-04 16:12:58,884 cfg.data.max_sent_length           : 500
2024-02-04 16:12:58,885 cfg.data.random_train_subset       : -1
2024-02-04 16:12:58,885 cfg.data.random_dev_subset         : -1
2024-02-04 16:12:58,885 cfg.testing.recognition_beam_sizes : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
2024-02-04 16:12:58,885 cfg.testing.translation_beam_sizes : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
2024-02-04 16:12:58,885 cfg.testing.translation_beam_alphas : [-1, 0, 1, 2, 3, 4, 5]
2024-02-04 16:12:58,885 cfg.training.reset_best_ckpt       : False
2024-02-04 16:12:58,885 cfg.training.reset_scheduler       : False
2024-02-04 16:12:58,885 cfg.training.reset_optimizer       : False
2024-02-04 16:12:58,886 cfg.training.random_seed           : 42
2024-02-04 16:12:58,886 cfg.training.model_dir             : ./sign_sample_model/fold9/32head/256batch
2024-02-04 16:12:58,886 cfg.training.recognition_loss_weight : 1.0
2024-02-04 16:12:58,886 cfg.training.translation_loss_weight : 1.0
2024-02-04 16:12:58,886 cfg.training.eval_metric           : bleu
2024-02-04 16:12:58,886 cfg.training.optimizer             : adam
2024-02-04 16:12:58,886 cfg.training.learning_rate         : 0.0001
2024-02-04 16:12:58,887 cfg.training.batch_size            : 256
2024-02-04 16:12:58,887 cfg.training.num_valid_log         : 5
2024-02-04 16:12:58,887 cfg.training.epochs                : 50000
2024-02-04 16:12:58,887 cfg.training.early_stopping_metric : eval_metric
2024-02-04 16:12:58,887 cfg.training.batch_type            : sentence
2024-02-04 16:12:58,887 cfg.training.translation_normalization : batch
2024-02-04 16:12:58,887 cfg.training.eval_recognition_beam_size : 1
2024-02-04 16:12:58,887 cfg.training.eval_translation_beam_size : 1
2024-02-04 16:12:58,887 cfg.training.eval_translation_beam_alpha : -1
2024-02-04 16:12:58,888 cfg.training.overwrite             : True
2024-02-04 16:12:58,888 cfg.training.shuffle               : True
2024-02-04 16:12:58,888 cfg.training.use_cuda              : True
2024-02-04 16:12:58,888 cfg.training.translation_max_output_length : 40
2024-02-04 16:12:58,888 cfg.training.keep_last_ckpts       : 1
2024-02-04 16:12:58,888 cfg.training.batch_multiplier      : 1
2024-02-04 16:12:58,888 cfg.training.logging_freq          : 100
2024-02-04 16:12:58,888 cfg.training.validation_freq       : 2000
2024-02-04 16:12:58,889 cfg.training.betas                 : [0.9, 0.998]
2024-02-04 16:12:58,889 cfg.training.scheduling            : plateau
2024-02-04 16:12:58,889 cfg.training.learning_rate_min     : 1e-08
2024-02-04 16:12:58,889 cfg.training.weight_decay          : 0.0001
2024-02-04 16:12:58,889 cfg.training.patience              : 12
2024-02-04 16:12:58,889 cfg.training.decrease_factor       : 0.5
2024-02-04 16:12:58,889 cfg.training.label_smoothing       : 0.0
2024-02-04 16:12:58,889 cfg.model.initializer              : xavier
2024-02-04 16:12:58,889 cfg.model.bias_initializer         : zeros
2024-02-04 16:12:58,890 cfg.model.init_gain                : 1.0
2024-02-04 16:12:58,890 cfg.model.embed_initializer        : xavier
2024-02-04 16:12:58,890 cfg.model.embed_init_gain          : 1.0
2024-02-04 16:12:58,890 cfg.model.tied_softmax             : True
2024-02-04 16:12:58,890 cfg.model.encoder.type             : transformer
2024-02-04 16:12:58,890 cfg.model.encoder.num_layers       : 3
2024-02-04 16:12:58,890 cfg.model.encoder.num_heads        : 32
2024-02-04 16:12:58,890 cfg.model.encoder.embeddings.embedding_dim : 512
2024-02-04 16:12:58,890 cfg.model.encoder.embeddings.scale : False
2024-02-04 16:12:58,891 cfg.model.encoder.embeddings.dropout : 0.1
2024-02-04 16:12:58,891 cfg.model.encoder.embeddings.norm_type : batch
2024-02-04 16:12:58,891 cfg.model.encoder.embeddings.activation_type : softsign
2024-02-04 16:12:58,891 cfg.model.encoder.hidden_size      : 512
2024-02-04 16:12:58,891 cfg.model.encoder.ff_size          : 2048
2024-02-04 16:12:58,891 cfg.model.encoder.dropout          : 0.1
2024-02-04 16:12:58,891 cfg.model.decoder.type             : transformer
2024-02-04 16:12:58,891 cfg.model.decoder.num_layers       : 3
2024-02-04 16:12:58,892 cfg.model.decoder.num_heads        : 32
2024-02-04 16:12:58,892 cfg.model.decoder.embeddings.embedding_dim : 512
2024-02-04 16:12:58,892 cfg.model.decoder.embeddings.scale : False
2024-02-04 16:12:58,892 cfg.model.decoder.embeddings.dropout : 0.1
2024-02-04 16:12:58,892 cfg.model.decoder.embeddings.norm_type : batch
2024-02-04 16:12:58,892 cfg.model.decoder.embeddings.activation_type : softsign
2024-02-04 16:12:58,892 cfg.model.decoder.hidden_size      : 512
2024-02-04 16:12:58,892 cfg.model.decoder.ff_size          : 2048
2024-02-04 16:12:58,892 cfg.model.decoder.dropout          : 0.1
2024-02-04 16:12:58,893 Data set sizes: 
	train 2126,
	valid 708,
	test 706
2024-02-04 16:12:58,893 First training example:
	[GLS] A B C D E
	[TXT] although new zealand was disappointed to faltered at the finals against australia they did well throughout the tournament
2024-02-04 16:12:58,893 First 10 words (gls): (0) <si> (1) <unk> (2) <pad> (3) A (4) B (5) C (6) D (7) E
2024-02-04 16:12:58,893 First 10 words (txt): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) the (5) and (6) to (7) a (8) in (9) of
2024-02-04 16:12:58,893 Number of unique glosses (types): 8
2024-02-04 16:12:58,893 Number of unique words (types): 4397
2024-02-04 16:12:58,893 SignModel(
	encoder=TransformerEncoder(num_layers=3, num_heads=32),
	decoder=TransformerDecoder(num_layers=3, num_heads=32),
	sgn_embed=SpatialEmbeddings(embedding_dim=512, input_size=2560),
	txt_embed=Embeddings(embedding_dim=512, vocab_size=4397))
2024-02-04 16:12:58,897 EPOCH 1
2024-02-04 16:13:25,528 Epoch   1: Total Training Recognition Loss 90.68  Total Training Translation Loss 1002.11 
2024-02-04 16:13:25,529 EPOCH 2
2024-02-04 16:13:49,157 Epoch   2: Total Training Recognition Loss 35.95  Total Training Translation Loss 923.08 
2024-02-04 16:13:49,158 EPOCH 3
2024-02-04 16:14:07,166 Epoch   3: Total Training Recognition Loss 22.59  Total Training Translation Loss 886.20 
2024-02-04 16:14:07,167 EPOCH 4
2024-02-04 16:14:25,587 Epoch   4: Total Training Recognition Loss 12.85  Total Training Translation Loss 856.28 
2024-02-04 16:14:25,588 EPOCH 5
2024-02-04 16:14:43,925 Epoch   5: Total Training Recognition Loss 9.36  Total Training Translation Loss 835.29 
2024-02-04 16:14:43,925 EPOCH 6
2024-02-04 16:15:01,959 Epoch   6: Total Training Recognition Loss 6.70  Total Training Translation Loss 823.28 
2024-02-04 16:15:01,960 EPOCH 7
2024-02-04 16:15:20,098 Epoch   7: Total Training Recognition Loss 5.00  Total Training Translation Loss 815.01 
2024-02-04 16:15:20,098 EPOCH 8
2024-02-04 16:15:38,043 Epoch   8: Total Training Recognition Loss 3.63  Total Training Translation Loss 808.94 
2024-02-04 16:15:38,044 EPOCH 9
2024-02-04 16:15:55,826 Epoch   9: Total Training Recognition Loss 2.65  Total Training Translation Loss 805.11 
2024-02-04 16:15:55,827 EPOCH 10
2024-02-04 16:16:13,031 Epoch  10: Total Training Recognition Loss 1.87  Total Training Translation Loss 801.28 
2024-02-04 16:16:13,032 EPOCH 11
2024-02-04 16:16:31,123 Epoch  11: Total Training Recognition Loss 1.27  Total Training Translation Loss 797.88 
2024-02-04 16:16:31,124 EPOCH 12
2024-02-04 16:16:35,132 [Epoch: 012 Step: 00000100] Batch Recognition Loss:   0.105643 => Gls Tokens per Sec:      320 || Batch Translation Loss: 100.761742 => Txt Tokens per Sec:     1014 || Lr: 0.000100
2024-02-04 16:16:49,003 Epoch  12: Total Training Recognition Loss 0.87  Total Training Translation Loss 793.69 
2024-02-04 16:16:49,003 EPOCH 13
2024-02-04 16:17:06,396 Epoch  13: Total Training Recognition Loss 0.62  Total Training Translation Loss 785.92 
2024-02-04 16:17:06,397 EPOCH 14
2024-02-04 16:17:24,262 Epoch  14: Total Training Recognition Loss 0.46  Total Training Translation Loss 778.70 
2024-02-04 16:17:24,263 EPOCH 15
2024-02-04 16:17:42,360 Epoch  15: Total Training Recognition Loss 0.35  Total Training Translation Loss 768.96 
2024-02-04 16:17:42,360 EPOCH 16
2024-02-04 16:18:00,948 Epoch  16: Total Training Recognition Loss 0.28  Total Training Translation Loss 759.32 
2024-02-04 16:18:00,949 EPOCH 17
2024-02-04 16:18:19,013 Epoch  17: Total Training Recognition Loss 0.23  Total Training Translation Loss 747.40 
2024-02-04 16:18:19,014 EPOCH 18
2024-02-04 16:18:37,267 Epoch  18: Total Training Recognition Loss 0.20  Total Training Translation Loss 734.80 
2024-02-04 16:18:37,267 EPOCH 19
2024-02-04 16:18:55,518 Epoch  19: Total Training Recognition Loss 0.17  Total Training Translation Loss 727.84 
2024-02-04 16:18:55,518 EPOCH 20
2024-02-04 16:19:13,577 Epoch  20: Total Training Recognition Loss 0.15  Total Training Translation Loss 716.71 
2024-02-04 16:19:13,578 EPOCH 21
2024-02-04 16:19:31,826 Epoch  21: Total Training Recognition Loss 0.14  Total Training Translation Loss 709.38 
2024-02-04 16:19:31,827 EPOCH 22
2024-02-04 16:19:49,832 Epoch  22: Total Training Recognition Loss 0.13  Total Training Translation Loss 695.90 
2024-02-04 16:19:49,832 EPOCH 23
2024-02-04 16:19:50,385 [Epoch: 023 Step: 00000200] Batch Recognition Loss:   0.015204 => Gls Tokens per Sec:     4638 || Batch Translation Loss:  60.831425 => Txt Tokens per Sec:     9411 || Lr: 0.000100
2024-02-04 16:20:07,845 Epoch  23: Total Training Recognition Loss 0.13  Total Training Translation Loss 687.86 
2024-02-04 16:20:07,846 EPOCH 24
2024-02-04 16:20:26,000 Epoch  24: Total Training Recognition Loss 0.12  Total Training Translation Loss 679.32 
2024-02-04 16:20:26,000 EPOCH 25
2024-02-04 16:20:44,046 Epoch  25: Total Training Recognition Loss 0.10  Total Training Translation Loss 665.11 
2024-02-04 16:20:44,047 EPOCH 26
2024-02-04 16:21:02,139 Epoch  26: Total Training Recognition Loss 0.11  Total Training Translation Loss 653.60 
2024-02-04 16:21:02,140 EPOCH 27
2024-02-04 16:21:20,352 Epoch  27: Total Training Recognition Loss 0.11  Total Training Translation Loss 644.97 
2024-02-04 16:21:20,352 EPOCH 28
2024-02-04 16:21:38,609 Epoch  28: Total Training Recognition Loss 0.10  Total Training Translation Loss 635.27 
2024-02-04 16:21:38,610 EPOCH 29
2024-02-04 16:21:57,084 Epoch  29: Total Training Recognition Loss 0.10  Total Training Translation Loss 625.17 
2024-02-04 16:21:57,085 EPOCH 30
2024-02-04 16:22:17,373 Epoch  30: Total Training Recognition Loss 0.09  Total Training Translation Loss 615.76 
2024-02-04 16:22:17,374 EPOCH 31
2024-02-04 16:22:36,110 Epoch  31: Total Training Recognition Loss 0.09  Total Training Translation Loss 609.15 
2024-02-04 16:22:36,110 EPOCH 32
2024-02-04 16:22:54,405 Epoch  32: Total Training Recognition Loss 0.09  Total Training Translation Loss 604.21 
2024-02-04 16:22:54,405 EPOCH 33
2024-02-04 16:23:12,457 Epoch  33: Total Training Recognition Loss 0.09  Total Training Translation Loss 596.58 
2024-02-04 16:23:12,457 EPOCH 34
2024-02-04 16:23:23,414 [Epoch: 034 Step: 00000300] Batch Recognition Loss:   0.006819 => Gls Tokens per Sec:      351 || Batch Translation Loss:  57.405247 => Txt Tokens per Sec:     1071 || Lr: 0.000100
2024-02-04 16:23:30,708 Epoch  34: Total Training Recognition Loss 0.08  Total Training Translation Loss 591.53 
2024-02-04 16:23:30,708 EPOCH 35
2024-02-04 16:23:49,366 Epoch  35: Total Training Recognition Loss 0.10  Total Training Translation Loss 579.51 
2024-02-04 16:23:49,367 EPOCH 36
2024-02-04 16:24:08,761 Epoch  36: Total Training Recognition Loss 0.08  Total Training Translation Loss 568.19 
2024-02-04 16:24:08,762 EPOCH 37
2024-02-04 16:24:27,518 Epoch  37: Total Training Recognition Loss 0.09  Total Training Translation Loss 560.97 
2024-02-04 16:24:27,519 EPOCH 38
2024-02-04 16:24:45,652 Epoch  38: Total Training Recognition Loss 0.09  Total Training Translation Loss 549.09 
2024-02-04 16:24:45,653 EPOCH 39
2024-02-04 16:25:03,589 Epoch  39: Total Training Recognition Loss 0.08  Total Training Translation Loss 543.53 
2024-02-04 16:25:03,589 EPOCH 40
2024-02-04 16:25:22,194 Epoch  40: Total Training Recognition Loss 0.08  Total Training Translation Loss 536.58 
2024-02-04 16:25:22,195 EPOCH 41
2024-02-04 16:25:42,608 Epoch  41: Total Training Recognition Loss 0.08  Total Training Translation Loss 526.49 
2024-02-04 16:25:42,608 EPOCH 42
2024-02-04 16:26:00,833 Epoch  42: Total Training Recognition Loss 0.09  Total Training Translation Loss 520.11 
2024-02-04 16:26:00,833 EPOCH 43
2024-02-04 16:26:19,093 Epoch  43: Total Training Recognition Loss 0.08  Total Training Translation Loss 517.97 
2024-02-04 16:26:19,094 EPOCH 44
2024-02-04 16:26:37,345 Epoch  44: Total Training Recognition Loss 0.08  Total Training Translation Loss 510.66 
2024-02-04 16:26:37,346 EPOCH 45
2024-02-04 16:26:47,685 [Epoch: 045 Step: 00000400] Batch Recognition Loss:   0.010154 => Gls Tokens per Sec:      409 || Batch Translation Loss:  66.867767 => Txt Tokens per Sec:     1231 || Lr: 0.000100
2024-02-04 16:26:56,060 Epoch  45: Total Training Recognition Loss 0.08  Total Training Translation Loss 501.77 
2024-02-04 16:26:56,061 EPOCH 46
2024-02-04 16:27:14,220 Epoch  46: Total Training Recognition Loss 0.09  Total Training Translation Loss 492.51 
2024-02-04 16:27:14,220 EPOCH 47
2024-02-04 16:27:32,211 Epoch  47: Total Training Recognition Loss 0.09  Total Training Translation Loss 484.85 
2024-02-04 16:27:32,212 EPOCH 48
2024-02-04 16:27:50,384 Epoch  48: Total Training Recognition Loss 0.09  Total Training Translation Loss 487.88 
2024-02-04 16:27:50,385 EPOCH 49
2024-02-04 16:28:08,432 Epoch  49: Total Training Recognition Loss 0.10  Total Training Translation Loss 477.30 
2024-02-04 16:28:08,432 EPOCH 50
2024-02-04 16:28:26,280 Epoch  50: Total Training Recognition Loss 0.10  Total Training Translation Loss 466.39 
2024-02-04 16:28:26,280 EPOCH 51
2024-02-04 16:28:44,144 Epoch  51: Total Training Recognition Loss 0.11  Total Training Translation Loss 458.43 
2024-02-04 16:28:44,145 EPOCH 52
2024-02-04 16:29:01,895 Epoch  52: Total Training Recognition Loss 0.09  Total Training Translation Loss 451.88 
2024-02-04 16:29:01,896 EPOCH 53
2024-02-04 16:29:21,053 Epoch  53: Total Training Recognition Loss 0.10  Total Training Translation Loss 444.82 
2024-02-04 16:29:21,054 EPOCH 54
2024-02-04 16:29:39,897 Epoch  54: Total Training Recognition Loss 0.09  Total Training Translation Loss 439.56 
2024-02-04 16:29:39,898 EPOCH 55
2024-02-04 16:29:58,619 Epoch  55: Total Training Recognition Loss 0.09  Total Training Translation Loss 430.78 
2024-02-04 16:29:58,620 EPOCH 56
2024-02-04 16:30:11,432 [Epoch: 056 Step: 00000500] Batch Recognition Loss:   0.024332 => Gls Tokens per Sec:      430 || Batch Translation Loss:  38.520317 => Txt Tokens per Sec:     1241 || Lr: 0.000100
2024-02-04 16:30:17,031 Epoch  56: Total Training Recognition Loss 0.11  Total Training Translation Loss 426.18 
2024-02-04 16:30:17,031 EPOCH 57
2024-02-04 16:30:35,003 Epoch  57: Total Training Recognition Loss 0.10  Total Training Translation Loss 420.01 
2024-02-04 16:30:35,004 EPOCH 58
2024-02-04 16:30:53,159 Epoch  58: Total Training Recognition Loss 0.11  Total Training Translation Loss 412.06 
2024-02-04 16:30:53,160 EPOCH 59
2024-02-04 16:31:11,355 Epoch  59: Total Training Recognition Loss 0.11  Total Training Translation Loss 404.67 
2024-02-04 16:31:11,355 EPOCH 60
2024-02-04 16:31:29,479 Epoch  60: Total Training Recognition Loss 0.10  Total Training Translation Loss 399.66 
2024-02-04 16:31:29,479 EPOCH 61
2024-02-04 16:31:47,373 Epoch  61: Total Training Recognition Loss 0.10  Total Training Translation Loss 395.13 
2024-02-04 16:31:47,373 EPOCH 62
2024-02-04 16:32:05,513 Epoch  62: Total Training Recognition Loss 0.11  Total Training Translation Loss 388.35 
2024-02-04 16:32:05,513 EPOCH 63
2024-02-04 16:32:23,634 Epoch  63: Total Training Recognition Loss 0.12  Total Training Translation Loss 383.18 
2024-02-04 16:32:23,634 EPOCH 64
2024-02-04 16:32:41,679 Epoch  64: Total Training Recognition Loss 0.11  Total Training Translation Loss 381.05 
2024-02-04 16:32:41,679 EPOCH 65
2024-02-04 16:32:59,692 Epoch  65: Total Training Recognition Loss 0.14  Total Training Translation Loss 376.99 
2024-02-04 16:32:59,692 EPOCH 66
2024-02-04 16:33:17,536 Epoch  66: Total Training Recognition Loss 0.13  Total Training Translation Loss 372.93 
2024-02-04 16:33:17,537 EPOCH 67
2024-02-04 16:33:28,434 [Epoch: 067 Step: 00000600] Batch Recognition Loss:   0.016973 => Gls Tokens per Sec:      623 || Batch Translation Loss:  50.183727 => Txt Tokens per Sec:     1679 || Lr: 0.000100
2024-02-04 16:33:35,589 Epoch  67: Total Training Recognition Loss 0.14  Total Training Translation Loss 370.09 
2024-02-04 16:33:35,590 EPOCH 68
2024-02-04 16:33:53,836 Epoch  68: Total Training Recognition Loss 0.13  Total Training Translation Loss 361.05 
2024-02-04 16:33:53,837 EPOCH 69
2024-02-04 16:34:11,724 Epoch  69: Total Training Recognition Loss 0.14  Total Training Translation Loss 352.45 
2024-02-04 16:34:11,724 EPOCH 70
2024-02-04 16:34:29,860 Epoch  70: Total Training Recognition Loss 0.15  Total Training Translation Loss 344.22 
2024-02-04 16:34:29,860 EPOCH 71
2024-02-04 16:34:48,143 Epoch  71: Total Training Recognition Loss 0.13  Total Training Translation Loss 341.59 
2024-02-04 16:34:48,144 EPOCH 72
2024-02-04 16:35:06,057 Epoch  72: Total Training Recognition Loss 0.14  Total Training Translation Loss 334.68 
2024-02-04 16:35:06,058 EPOCH 73
2024-02-04 16:35:24,082 Epoch  73: Total Training Recognition Loss 0.14  Total Training Translation Loss 327.78 
2024-02-04 16:35:24,082 EPOCH 74
2024-02-04 16:35:42,182 Epoch  74: Total Training Recognition Loss 0.14  Total Training Translation Loss 322.36 
2024-02-04 16:35:42,183 EPOCH 75
2024-02-04 16:35:59,797 Epoch  75: Total Training Recognition Loss 0.15  Total Training Translation Loss 315.87 
2024-02-04 16:35:59,798 EPOCH 76
2024-02-04 16:36:17,782 Epoch  76: Total Training Recognition Loss 0.14  Total Training Translation Loss 311.22 
2024-02-04 16:36:17,783 EPOCH 77
2024-02-04 16:36:35,371 Epoch  77: Total Training Recognition Loss 0.15  Total Training Translation Loss 304.29 
2024-02-04 16:36:35,371 EPOCH 78
2024-02-04 16:36:52,181 [Epoch: 078 Step: 00000700] Batch Recognition Loss:   0.019678 => Gls Tokens per Sec:      480 || Batch Translation Loss:  46.400261 => Txt Tokens per Sec:     1351 || Lr: 0.000100
2024-02-04 16:36:53,454 Epoch  78: Total Training Recognition Loss 0.15  Total Training Translation Loss 300.50 
2024-02-04 16:36:53,454 EPOCH 79
2024-02-04 16:37:11,620 Epoch  79: Total Training Recognition Loss 0.15  Total Training Translation Loss 294.06 
2024-02-04 16:37:11,620 EPOCH 80
2024-02-04 16:37:29,547 Epoch  80: Total Training Recognition Loss 0.15  Total Training Translation Loss 292.00 
2024-02-04 16:37:29,548 EPOCH 81
2024-02-04 16:37:47,419 Epoch  81: Total Training Recognition Loss 0.15  Total Training Translation Loss 290.07 
2024-02-04 16:37:47,420 EPOCH 82
2024-02-04 16:38:05,447 Epoch  82: Total Training Recognition Loss 0.18  Total Training Translation Loss 285.22 
2024-02-04 16:38:05,447 EPOCH 83
2024-02-04 16:38:23,172 Epoch  83: Total Training Recognition Loss 0.14  Total Training Translation Loss 280.02 
2024-02-04 16:38:23,172 EPOCH 84
2024-02-04 16:38:41,129 Epoch  84: Total Training Recognition Loss 0.18  Total Training Translation Loss 275.19 
2024-02-04 16:38:41,129 EPOCH 85
2024-02-04 16:38:59,263 Epoch  85: Total Training Recognition Loss 0.17  Total Training Translation Loss 268.61 
2024-02-04 16:38:59,264 EPOCH 86
2024-02-04 16:39:16,988 Epoch  86: Total Training Recognition Loss 0.17  Total Training Translation Loss 263.93 
2024-02-04 16:39:16,989 EPOCH 87
2024-02-04 16:39:34,835 Epoch  87: Total Training Recognition Loss 0.17  Total Training Translation Loss 258.98 
2024-02-04 16:39:34,835 EPOCH 88
2024-02-04 16:39:52,676 Epoch  88: Total Training Recognition Loss 0.17  Total Training Translation Loss 254.53 
2024-02-04 16:39:52,677 EPOCH 89
2024-02-04 16:40:04,358 [Epoch: 089 Step: 00000800] Batch Recognition Loss:   0.020641 => Gls Tokens per Sec:      801 || Batch Translation Loss:  17.388641 => Txt Tokens per Sec:     2143 || Lr: 0.000100
2024-02-04 16:40:10,498 Epoch  89: Total Training Recognition Loss 0.18  Total Training Translation Loss 248.27 
2024-02-04 16:40:10,499 EPOCH 90
2024-02-04 16:40:28,531 Epoch  90: Total Training Recognition Loss 0.17  Total Training Translation Loss 240.51 
2024-02-04 16:40:28,532 EPOCH 91
2024-02-04 16:40:46,555 Epoch  91: Total Training Recognition Loss 0.18  Total Training Translation Loss 236.74 
2024-02-04 16:40:46,556 EPOCH 92
2024-02-04 16:41:04,407 Epoch  92: Total Training Recognition Loss 0.17  Total Training Translation Loss 233.50 
2024-02-04 16:41:04,408 EPOCH 93
2024-02-04 16:41:22,305 Epoch  93: Total Training Recognition Loss 0.19  Total Training Translation Loss 230.98 
2024-02-04 16:41:22,305 EPOCH 94
2024-02-04 16:41:39,793 Epoch  94: Total Training Recognition Loss 0.17  Total Training Translation Loss 226.10 
2024-02-04 16:41:39,793 EPOCH 95
2024-02-04 16:41:57,933 Epoch  95: Total Training Recognition Loss 0.17  Total Training Translation Loss 220.99 
2024-02-04 16:41:57,933 EPOCH 96
2024-02-04 16:42:15,656 Epoch  96: Total Training Recognition Loss 0.20  Total Training Translation Loss 216.37 
2024-02-04 16:42:15,657 EPOCH 97
2024-02-04 16:42:33,279 Epoch  97: Total Training Recognition Loss 0.19  Total Training Translation Loss 215.06 
2024-02-04 16:42:33,279 EPOCH 98
2024-02-04 16:42:51,133 Epoch  98: Total Training Recognition Loss 0.19  Total Training Translation Loss 208.26 
2024-02-04 16:42:51,133 EPOCH 99
2024-02-04 16:43:08,886 Epoch  99: Total Training Recognition Loss 0.19  Total Training Translation Loss 206.23 
2024-02-04 16:43:08,886 EPOCH 100
2024-02-04 16:43:26,846 [Epoch: 100 Step: 00000900] Batch Recognition Loss:   0.033652 => Gls Tokens per Sec:      592 || Batch Translation Loss:  28.106808 => Txt Tokens per Sec:     1643 || Lr: 0.000100
2024-02-04 16:43:26,847 Epoch 100: Total Training Recognition Loss 0.22  Total Training Translation Loss 202.86 
2024-02-04 16:43:26,847 EPOCH 101
2024-02-04 16:43:44,989 Epoch 101: Total Training Recognition Loss 0.21  Total Training Translation Loss 204.10 
2024-02-04 16:43:44,990 EPOCH 102
2024-02-04 16:44:02,930 Epoch 102: Total Training Recognition Loss 0.19  Total Training Translation Loss 202.21 
2024-02-04 16:44:02,930 EPOCH 103
2024-02-04 16:44:20,887 Epoch 103: Total Training Recognition Loss 0.25  Total Training Translation Loss 200.78 
2024-02-04 16:44:20,887 EPOCH 104
2024-02-04 16:44:38,544 Epoch 104: Total Training Recognition Loss 0.20  Total Training Translation Loss 191.51 
2024-02-04 16:44:38,544 EPOCH 105
2024-02-04 16:44:56,711 Epoch 105: Total Training Recognition Loss 0.21  Total Training Translation Loss 185.25 
2024-02-04 16:44:56,711 EPOCH 106
2024-02-04 16:45:14,640 Epoch 106: Total Training Recognition Loss 0.18  Total Training Translation Loss 178.45 
2024-02-04 16:45:14,640 EPOCH 107
2024-02-04 16:45:32,379 Epoch 107: Total Training Recognition Loss 0.21  Total Training Translation Loss 178.37 
2024-02-04 16:45:32,380 EPOCH 108
2024-02-04 16:45:50,315 Epoch 108: Total Training Recognition Loss 0.22  Total Training Translation Loss 179.38 
2024-02-04 16:45:50,315 EPOCH 109
2024-02-04 16:46:08,043 Epoch 109: Total Training Recognition Loss 0.22  Total Training Translation Loss 174.29 
2024-02-04 16:46:08,044 EPOCH 110
2024-02-04 16:46:26,223 Epoch 110: Total Training Recognition Loss 0.20  Total Training Translation Loss 166.06 
2024-02-04 16:46:26,224 EPOCH 111
2024-02-04 16:46:44,028 Epoch 111: Total Training Recognition Loss 0.20  Total Training Translation Loss 162.33 
2024-02-04 16:46:44,028 EPOCH 112
2024-02-04 16:46:44,523 [Epoch: 112 Step: 00001000] Batch Recognition Loss:   0.013863 => Gls Tokens per Sec:     2593 || Batch Translation Loss:  19.205038 => Txt Tokens per Sec:     7105 || Lr: 0.000100
2024-02-04 16:47:01,609 Epoch 112: Total Training Recognition Loss 0.20  Total Training Translation Loss 158.33 
2024-02-04 16:47:01,609 EPOCH 113
2024-02-04 16:47:20,796 Epoch 113: Total Training Recognition Loss 0.21  Total Training Translation Loss 153.36 
2024-02-04 16:47:20,797 EPOCH 114
2024-02-04 16:47:39,946 Epoch 114: Total Training Recognition Loss 0.19  Total Training Translation Loss 148.33 
2024-02-04 16:47:39,947 EPOCH 115
2024-02-04 16:47:58,550 Epoch 115: Total Training Recognition Loss 0.20  Total Training Translation Loss 144.95 
2024-02-04 16:47:58,551 EPOCH 116
2024-02-04 16:48:16,919 Epoch 116: Total Training Recognition Loss 0.19  Total Training Translation Loss 142.75 
2024-02-04 16:48:16,919 EPOCH 117
2024-02-04 16:48:34,945 Epoch 117: Total Training Recognition Loss 0.20  Total Training Translation Loss 141.11 
2024-02-04 16:48:34,945 EPOCH 118
2024-02-04 16:48:52,860 Epoch 118: Total Training Recognition Loss 0.21  Total Training Translation Loss 135.38 
2024-02-04 16:48:52,861 EPOCH 119
2024-02-04 16:49:10,938 Epoch 119: Total Training Recognition Loss 0.19  Total Training Translation Loss 132.41 
2024-02-04 16:49:10,939 EPOCH 120
2024-02-04 16:49:29,047 Epoch 120: Total Training Recognition Loss 0.19  Total Training Translation Loss 130.18 
2024-02-04 16:49:29,048 EPOCH 121
2024-02-04 16:49:47,212 Epoch 121: Total Training Recognition Loss 0.20  Total Training Translation Loss 127.77 
2024-02-04 16:49:47,213 EPOCH 122
2024-02-04 16:50:05,349 Epoch 122: Total Training Recognition Loss 0.19  Total Training Translation Loss 124.63 
2024-02-04 16:50:05,350 EPOCH 123
2024-02-04 16:50:14,251 [Epoch: 123 Step: 00001100] Batch Recognition Loss:   0.030371 => Gls Tokens per Sec:      188 || Batch Translation Loss:  17.493780 => Txt Tokens per Sec:      618 || Lr: 0.000100
2024-02-04 16:50:23,345 Epoch 123: Total Training Recognition Loss 0.21  Total Training Translation Loss 120.56 
2024-02-04 16:50:23,346 EPOCH 124
2024-02-04 16:50:41,089 Epoch 124: Total Training Recognition Loss 0.19  Total Training Translation Loss 117.52 
2024-02-04 16:50:41,089 EPOCH 125
2024-02-04 16:50:58,732 Epoch 125: Total Training Recognition Loss 0.19  Total Training Translation Loss 114.80 
2024-02-04 16:50:58,733 EPOCH 126
2024-02-04 16:51:16,951 Epoch 126: Total Training Recognition Loss 0.18  Total Training Translation Loss 111.26 
2024-02-04 16:51:16,952 EPOCH 127
2024-02-04 16:51:34,985 Epoch 127: Total Training Recognition Loss 0.19  Total Training Translation Loss 107.47 
2024-02-04 16:51:34,985 EPOCH 128
2024-02-04 16:51:52,806 Epoch 128: Total Training Recognition Loss 0.19  Total Training Translation Loss 105.23 
2024-02-04 16:51:52,806 EPOCH 129
2024-02-04 16:52:10,516 Epoch 129: Total Training Recognition Loss 0.18  Total Training Translation Loss 101.79 
2024-02-04 16:52:10,517 EPOCH 130
2024-02-04 16:52:28,333 Epoch 130: Total Training Recognition Loss 0.17  Total Training Translation Loss 102.42 
2024-02-04 16:52:28,334 EPOCH 131
2024-02-04 16:52:46,387 Epoch 131: Total Training Recognition Loss 0.20  Total Training Translation Loss 99.69 
2024-02-04 16:52:46,387 EPOCH 132
2024-02-04 16:53:04,180 Epoch 132: Total Training Recognition Loss 0.20  Total Training Translation Loss 99.74 
2024-02-04 16:53:04,181 EPOCH 133
2024-02-04 16:53:22,216 Epoch 133: Total Training Recognition Loss 0.19  Total Training Translation Loss 95.41 
2024-02-04 16:53:22,216 EPOCH 134
2024-02-04 16:53:33,572 [Epoch: 134 Step: 00001200] Batch Recognition Loss:   0.027162 => Gls Tokens per Sec:      260 || Batch Translation Loss:  14.562624 => Txt Tokens per Sec:      830 || Lr: 0.000100
2024-02-04 16:53:40,040 Epoch 134: Total Training Recognition Loss 0.18  Total Training Translation Loss 91.34 
2024-02-04 16:53:40,041 EPOCH 135
2024-02-04 16:53:57,860 Epoch 135: Total Training Recognition Loss 0.16  Total Training Translation Loss 88.38 
2024-02-04 16:53:57,860 EPOCH 136
2024-02-04 16:54:15,672 Epoch 136: Total Training Recognition Loss 0.18  Total Training Translation Loss 86.80 
2024-02-04 16:54:15,672 EPOCH 137
2024-02-04 16:54:33,227 Epoch 137: Total Training Recognition Loss 0.18  Total Training Translation Loss 86.03 
2024-02-04 16:54:33,227 EPOCH 138
2024-02-04 16:54:50,952 Epoch 138: Total Training Recognition Loss 0.19  Total Training Translation Loss 84.41 
2024-02-04 16:54:50,952 EPOCH 139
2024-02-04 16:55:08,922 Epoch 139: Total Training Recognition Loss 0.17  Total Training Translation Loss 81.87 
2024-02-04 16:55:08,922 EPOCH 140
2024-02-04 16:55:26,834 Epoch 140: Total Training Recognition Loss 0.18  Total Training Translation Loss 79.81 
2024-02-04 16:55:26,834 EPOCH 141
2024-02-04 16:55:44,808 Epoch 141: Total Training Recognition Loss 0.17  Total Training Translation Loss 78.00 
2024-02-04 16:55:44,809 EPOCH 142
2024-02-04 16:56:02,622 Epoch 142: Total Training Recognition Loss 0.18  Total Training Translation Loss 76.29 
2024-02-04 16:56:02,622 EPOCH 143
2024-02-04 16:56:20,581 Epoch 143: Total Training Recognition Loss 0.20  Total Training Translation Loss 74.70 
2024-02-04 16:56:20,581 EPOCH 144
2024-02-04 16:56:38,405 Epoch 144: Total Training Recognition Loss 0.18  Total Training Translation Loss 73.48 
2024-02-04 16:56:38,406 EPOCH 145
2024-02-04 16:56:44,066 [Epoch: 145 Step: 00001300] Batch Recognition Loss:   0.016879 => Gls Tokens per Sec:      747 || Batch Translation Loss:   6.805230 => Txt Tokens per Sec:     1785 || Lr: 0.000100
2024-02-04 16:56:56,325 Epoch 145: Total Training Recognition Loss 0.17  Total Training Translation Loss 70.24 
2024-02-04 16:56:56,325 EPOCH 146
2024-02-04 16:57:14,195 Epoch 146: Total Training Recognition Loss 0.17  Total Training Translation Loss 68.34 
2024-02-04 16:57:14,196 EPOCH 147
2024-02-04 16:57:32,101 Epoch 147: Total Training Recognition Loss 0.16  Total Training Translation Loss 66.20 
2024-02-04 16:57:32,101 EPOCH 148
2024-02-04 16:57:49,598 Epoch 148: Total Training Recognition Loss 0.16  Total Training Translation Loss 64.52 
2024-02-04 16:57:49,599 EPOCH 149
2024-02-04 16:58:07,712 Epoch 149: Total Training Recognition Loss 0.17  Total Training Translation Loss 63.07 
2024-02-04 16:58:07,713 EPOCH 150
2024-02-04 16:58:25,705 Epoch 150: Total Training Recognition Loss 0.16  Total Training Translation Loss 64.10 
2024-02-04 16:58:25,705 EPOCH 151
2024-02-04 16:58:43,838 Epoch 151: Total Training Recognition Loss 0.17  Total Training Translation Loss 59.73 
2024-02-04 16:58:43,838 EPOCH 152
2024-02-04 16:59:03,331 Epoch 152: Total Training Recognition Loss 0.16  Total Training Translation Loss 57.61 
2024-02-04 16:59:03,331 EPOCH 153
2024-02-04 16:59:21,987 Epoch 153: Total Training Recognition Loss 0.15  Total Training Translation Loss 56.11 
2024-02-04 16:59:21,988 EPOCH 154
2024-02-04 16:59:40,435 Epoch 154: Total Training Recognition Loss 0.16  Total Training Translation Loss 54.76 
2024-02-04 16:59:40,436 EPOCH 155
2024-02-04 16:59:59,051 Epoch 155: Total Training Recognition Loss 0.16  Total Training Translation Loss 52.65 
2024-02-04 16:59:59,051 EPOCH 156
2024-02-04 17:00:09,053 [Epoch: 156 Step: 00001400] Batch Recognition Loss:   0.017657 => Gls Tokens per Sec:      551 || Batch Translation Loss:   1.876001 => Txt Tokens per Sec:     1504 || Lr: 0.000100
2024-02-04 17:00:17,155 Epoch 156: Total Training Recognition Loss 0.15  Total Training Translation Loss 51.24 
2024-02-04 17:00:17,155 EPOCH 157
2024-02-04 17:00:35,342 Epoch 157: Total Training Recognition Loss 0.14  Total Training Translation Loss 49.52 
2024-02-04 17:00:35,343 EPOCH 158
2024-02-04 17:00:53,273 Epoch 158: Total Training Recognition Loss 0.14  Total Training Translation Loss 47.74 
2024-02-04 17:00:53,273 EPOCH 159
2024-02-04 17:01:11,524 Epoch 159: Total Training Recognition Loss 0.13  Total Training Translation Loss 47.17 
2024-02-04 17:01:11,524 EPOCH 160
2024-02-04 17:01:29,492 Epoch 160: Total Training Recognition Loss 0.14  Total Training Translation Loss 45.05 
2024-02-04 17:01:29,493 EPOCH 161
2024-02-04 17:01:47,236 Epoch 161: Total Training Recognition Loss 0.13  Total Training Translation Loss 45.90 
2024-02-04 17:01:47,237 EPOCH 162
2024-02-04 17:02:05,174 Epoch 162: Total Training Recognition Loss 0.14  Total Training Translation Loss 44.56 
2024-02-04 17:02:05,175 EPOCH 163
2024-02-04 17:02:23,231 Epoch 163: Total Training Recognition Loss 0.13  Total Training Translation Loss 44.38 
2024-02-04 17:02:23,232 EPOCH 164
2024-02-04 17:02:41,097 Epoch 164: Total Training Recognition Loss 0.13  Total Training Translation Loss 42.65 
2024-02-04 17:02:41,097 EPOCH 165
2024-02-04 17:02:59,002 Epoch 165: Total Training Recognition Loss 0.13  Total Training Translation Loss 41.67 
2024-02-04 17:02:59,002 EPOCH 166
2024-02-04 17:03:16,999 Epoch 166: Total Training Recognition Loss 0.13  Total Training Translation Loss 39.63 
2024-02-04 17:03:17,000 EPOCH 167
2024-02-04 17:03:29,089 [Epoch: 167 Step: 00001500] Batch Recognition Loss:   0.014580 => Gls Tokens per Sec:      635 || Batch Translation Loss:   5.333277 => Txt Tokens per Sec:     1894 || Lr: 0.000100
2024-02-04 17:03:34,614 Epoch 167: Total Training Recognition Loss 0.13  Total Training Translation Loss 38.94 
2024-02-04 17:03:34,614 EPOCH 168
2024-02-04 17:03:52,604 Epoch 168: Total Training Recognition Loss 0.13  Total Training Translation Loss 39.23 
2024-02-04 17:03:52,604 EPOCH 169
2024-02-04 17:04:10,457 Epoch 169: Total Training Recognition Loss 0.13  Total Training Translation Loss 37.54 
2024-02-04 17:04:10,458 EPOCH 170
2024-02-04 17:04:28,387 Epoch 170: Total Training Recognition Loss 0.13  Total Training Translation Loss 37.86 
2024-02-04 17:04:28,387 EPOCH 171
2024-02-04 17:04:46,259 Epoch 171: Total Training Recognition Loss 0.12  Total Training Translation Loss 36.43 
2024-02-04 17:04:46,260 EPOCH 172
2024-02-04 17:05:04,047 Epoch 172: Total Training Recognition Loss 0.13  Total Training Translation Loss 34.70 
2024-02-04 17:05:04,048 EPOCH 173
2024-02-04 17:05:21,993 Epoch 173: Total Training Recognition Loss 0.12  Total Training Translation Loss 33.13 
2024-02-04 17:05:21,993 EPOCH 174
2024-02-04 17:05:39,795 Epoch 174: Total Training Recognition Loss 0.12  Total Training Translation Loss 33.42 
2024-02-04 17:05:39,796 EPOCH 175
2024-02-04 17:05:57,583 Epoch 175: Total Training Recognition Loss 0.11  Total Training Translation Loss 31.96 
2024-02-04 17:05:57,584 EPOCH 176
2024-02-04 17:06:15,263 Epoch 176: Total Training Recognition Loss 0.11  Total Training Translation Loss 30.66 
2024-02-04 17:06:15,263 EPOCH 177
2024-02-04 17:06:33,271 Epoch 177: Total Training Recognition Loss 0.11  Total Training Translation Loss 31.05 
2024-02-04 17:06:33,271 EPOCH 178
2024-02-04 17:06:44,240 [Epoch: 178 Step: 00001600] Batch Recognition Loss:   0.010348 => Gls Tokens per Sec:      736 || Batch Translation Loss:   2.899161 => Txt Tokens per Sec:     1963 || Lr: 0.000100
2024-02-04 17:06:51,085 Epoch 178: Total Training Recognition Loss 0.11  Total Training Translation Loss 30.37 
2024-02-04 17:06:51,086 EPOCH 179
2024-02-04 17:07:08,943 Epoch 179: Total Training Recognition Loss 0.11  Total Training Translation Loss 28.95 
2024-02-04 17:07:08,944 EPOCH 180
2024-02-04 17:07:26,598 Epoch 180: Total Training Recognition Loss 0.11  Total Training Translation Loss 28.57 
2024-02-04 17:07:26,599 EPOCH 181
2024-02-04 17:07:44,652 Epoch 181: Total Training Recognition Loss 0.10  Total Training Translation Loss 27.84 
2024-02-04 17:07:44,652 EPOCH 182
2024-02-04 17:08:02,484 Epoch 182: Total Training Recognition Loss 0.10  Total Training Translation Loss 27.25 
2024-02-04 17:08:02,485 EPOCH 183
2024-02-04 17:08:20,376 Epoch 183: Total Training Recognition Loss 0.11  Total Training Translation Loss 26.50 
2024-02-04 17:08:20,376 EPOCH 184
2024-02-04 17:08:38,151 Epoch 184: Total Training Recognition Loss 0.11  Total Training Translation Loss 25.29 
2024-02-04 17:08:38,152 EPOCH 185
2024-02-04 17:08:55,927 Epoch 185: Total Training Recognition Loss 0.09  Total Training Translation Loss 25.21 
2024-02-04 17:08:55,928 EPOCH 186
2024-02-04 17:09:13,674 Epoch 186: Total Training Recognition Loss 0.10  Total Training Translation Loss 24.74 
2024-02-04 17:09:13,675 EPOCH 187
2024-02-04 17:09:31,361 Epoch 187: Total Training Recognition Loss 0.09  Total Training Translation Loss 23.89 
2024-02-04 17:09:31,361 EPOCH 188
2024-02-04 17:09:49,535 Epoch 188: Total Training Recognition Loss 0.10  Total Training Translation Loss 23.88 
2024-02-04 17:09:49,536 EPOCH 189
2024-02-04 17:10:00,947 [Epoch: 189 Step: 00001700] Batch Recognition Loss:   0.010205 => Gls Tokens per Sec:      819 || Batch Translation Loss:   2.514980 => Txt Tokens per Sec:     2193 || Lr: 0.000100
2024-02-04 17:10:07,090 Epoch 189: Total Training Recognition Loss 0.10  Total Training Translation Loss 23.49 
2024-02-04 17:10:07,091 EPOCH 190
2024-02-04 17:10:25,233 Epoch 190: Total Training Recognition Loss 0.09  Total Training Translation Loss 22.18 
2024-02-04 17:10:25,234 EPOCH 191
2024-02-04 17:10:42,887 Epoch 191: Total Training Recognition Loss 0.09  Total Training Translation Loss 22.11 
2024-02-04 17:10:42,888 EPOCH 192
2024-02-04 17:11:00,760 Epoch 192: Total Training Recognition Loss 0.10  Total Training Translation Loss 21.24 
2024-02-04 17:11:00,761 EPOCH 193
2024-02-04 17:11:18,676 Epoch 193: Total Training Recognition Loss 0.09  Total Training Translation Loss 21.83 
2024-02-04 17:11:18,677 EPOCH 194
2024-02-04 17:11:36,442 Epoch 194: Total Training Recognition Loss 0.09  Total Training Translation Loss 20.78 
2024-02-04 17:11:36,443 EPOCH 195
2024-02-04 17:11:54,513 Epoch 195: Total Training Recognition Loss 0.09  Total Training Translation Loss 20.77 
2024-02-04 17:11:54,514 EPOCH 196
2024-02-04 17:12:12,516 Epoch 196: Total Training Recognition Loss 0.09  Total Training Translation Loss 19.90 
2024-02-04 17:12:12,516 EPOCH 197
2024-02-04 17:12:30,230 Epoch 197: Total Training Recognition Loss 0.09  Total Training Translation Loss 19.85 
2024-02-04 17:12:30,230 EPOCH 198
2024-02-04 17:12:48,080 Epoch 198: Total Training Recognition Loss 0.09  Total Training Translation Loss 19.29 
2024-02-04 17:12:48,081 EPOCH 199
2024-02-04 17:13:06,059 Epoch 199: Total Training Recognition Loss 0.08  Total Training Translation Loss 19.51 
2024-02-04 17:13:06,059 EPOCH 200
2024-02-04 17:13:23,932 [Epoch: 200 Step: 00001800] Batch Recognition Loss:   0.008060 => Gls Tokens per Sec:      595 || Batch Translation Loss:   2.292879 => Txt Tokens per Sec:     1651 || Lr: 0.000100
2024-02-04 17:13:23,933 Epoch 200: Total Training Recognition Loss 0.08  Total Training Translation Loss 18.51 
2024-02-04 17:13:23,933 EPOCH 201
2024-02-04 17:13:41,834 Epoch 201: Total Training Recognition Loss 0.08  Total Training Translation Loss 18.03 
2024-02-04 17:13:41,835 EPOCH 202
2024-02-04 17:13:59,761 Epoch 202: Total Training Recognition Loss 0.08  Total Training Translation Loss 18.03 
2024-02-04 17:13:59,762 EPOCH 203
2024-02-04 17:14:17,632 Epoch 203: Total Training Recognition Loss 0.08  Total Training Translation Loss 17.22 
2024-02-04 17:14:17,633 EPOCH 204
2024-02-04 17:14:35,411 Epoch 204: Total Training Recognition Loss 0.08  Total Training Translation Loss 16.92 
2024-02-04 17:14:35,412 EPOCH 205
2024-02-04 17:14:53,207 Epoch 205: Total Training Recognition Loss 0.08  Total Training Translation Loss 17.35 
2024-02-04 17:14:53,208 EPOCH 206
2024-02-04 17:15:10,964 Epoch 206: Total Training Recognition Loss 0.08  Total Training Translation Loss 16.62 
2024-02-04 17:15:10,965 EPOCH 207
2024-02-04 17:15:28,941 Epoch 207: Total Training Recognition Loss 0.07  Total Training Translation Loss 16.00 
2024-02-04 17:15:28,942 EPOCH 208
2024-02-04 17:15:46,907 Epoch 208: Total Training Recognition Loss 0.08  Total Training Translation Loss 15.54 
2024-02-04 17:15:46,907 EPOCH 209
2024-02-04 17:16:04,796 Epoch 209: Total Training Recognition Loss 0.07  Total Training Translation Loss 15.31 
2024-02-04 17:16:04,797 EPOCH 210
2024-02-04 17:16:22,705 Epoch 210: Total Training Recognition Loss 0.07  Total Training Translation Loss 15.14 
2024-02-04 17:16:22,705 EPOCH 211
2024-02-04 17:16:40,589 Epoch 211: Total Training Recognition Loss 0.07  Total Training Translation Loss 14.93 
2024-02-04 17:16:40,590 EPOCH 212
2024-02-04 17:16:44,588 [Epoch: 212 Step: 00001900] Batch Recognition Loss:   0.009923 => Gls Tokens per Sec:      320 || Batch Translation Loss:   2.040107 => Txt Tokens per Sec:     1016 || Lr: 0.000100
2024-02-04 17:16:58,293 Epoch 212: Total Training Recognition Loss 0.07  Total Training Translation Loss 14.21 
2024-02-04 17:16:58,294 EPOCH 213
2024-02-04 17:17:16,178 Epoch 213: Total Training Recognition Loss 0.07  Total Training Translation Loss 14.02 
2024-02-04 17:17:16,179 EPOCH 214
2024-02-04 17:17:34,053 Epoch 214: Total Training Recognition Loss 0.07  Total Training Translation Loss 13.61 
2024-02-04 17:17:34,053 EPOCH 215
2024-02-04 17:17:52,041 Epoch 215: Total Training Recognition Loss 0.06  Total Training Translation Loss 13.55 
2024-02-04 17:17:52,042 EPOCH 216
2024-02-04 17:18:09,807 Epoch 216: Total Training Recognition Loss 0.07  Total Training Translation Loss 13.48 
2024-02-04 17:18:09,808 EPOCH 217
2024-02-04 17:18:27,516 Epoch 217: Total Training Recognition Loss 0.06  Total Training Translation Loss 13.20 
2024-02-04 17:18:27,517 EPOCH 218
2024-02-04 17:18:45,040 Epoch 218: Total Training Recognition Loss 0.06  Total Training Translation Loss 13.29 
2024-02-04 17:18:45,041 EPOCH 219
2024-02-04 17:19:02,766 Epoch 219: Total Training Recognition Loss 0.06  Total Training Translation Loss 12.74 
2024-02-04 17:19:02,766 EPOCH 220
2024-02-04 17:19:20,874 Epoch 220: Total Training Recognition Loss 0.06  Total Training Translation Loss 12.68 
2024-02-04 17:19:20,874 EPOCH 221
2024-02-04 17:19:38,729 Epoch 221: Total Training Recognition Loss 0.06  Total Training Translation Loss 12.24 
2024-02-04 17:19:38,730 EPOCH 222
2024-02-04 17:19:56,488 Epoch 222: Total Training Recognition Loss 0.06  Total Training Translation Loss 12.31 
2024-02-04 17:19:56,489 EPOCH 223
2024-02-04 17:20:01,646 [Epoch: 223 Step: 00002000] Batch Recognition Loss:   0.006363 => Gls Tokens per Sec:      324 || Batch Translation Loss:   0.517761 => Txt Tokens per Sec:      952 || Lr: 0.000100
2024-02-04 17:22:12,892 Hooray! New best validation result [eval_metric]!
2024-02-04 17:22:12,895 Saving new checkpoint.
2024-02-04 17:22:13,188 Validation result at epoch 223, step     2000: duration: 131.5408s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00599	Translation Loss: 75709.63281	PPL: 1951.33057
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 1.14	(BLEU-1: 12.71,	BLEU-2: 4.82,	BLEU-3: 2.14,	BLEU-4: 1.14)
	CHRF 17.87	ROUGE 11.04
2024-02-04 17:22:13,190 Logging Recognition and Translation Outputs
2024-02-04 17:22:13,190 ========================================================================================================================
2024-02-04 17:22:13,190 Logging Sequence: 182_115.00
2024-02-04 17:22:13,190 	Gloss Reference :	A B+C+D+E
2024-02-04 17:22:13,190 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 17:22:13,191 	Gloss Alignment :	         
2024-02-04 17:22:13,191 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 17:22:13,192 	Text Reference  :	fans are unclear whether yuvraj will be  returning to play test match odi or in  t20   leagues from february 2022   
2024-02-04 17:22:13,192 	Text Hypothesis :	**** *** ******* ******* she    did  not want      to **** **** ***** *** ** get these rights  for  special  matches
2024-02-04 17:22:13,192 	Text Alignment  :	D    D   D       D       S      S    S   S            D    D    D     D   D  S   S     S       S    S        S      
2024-02-04 17:22:13,192 ========================================================================================================================
2024-02-04 17:22:13,193 Logging Sequence: 140_120.00
2024-02-04 17:22:13,193 	Gloss Reference :	A B+C+D+E
2024-02-04 17:22:13,193 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 17:22:13,193 	Gloss Alignment :	         
2024-02-04 17:22:13,193 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 17:22:13,195 	Text Reference  :	but why so it is because pant is a talented player and it  will help encouraging the youth         of      uttarakhand toward   sports
2024-02-04 17:22:13,195 	Text Hypothesis :	*** *** ** ** ** ******* pant ** * ******** ****** *** has made his  debut       in  international cricket in          february 2017  
2024-02-04 17:22:13,195 	Text Alignment  :	D   D   D  D  D  D            D  D D        D      D   S   S    S    S           S   S             S       S           S        S     
2024-02-04 17:22:13,195 ========================================================================================================================
2024-02-04 17:22:13,195 Logging Sequence: 85_36.00
2024-02-04 17:22:13,195 	Gloss Reference :	A B+C+D+E
2024-02-04 17:22:13,196 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 17:22:13,196 	Gloss Alignment :	         
2024-02-04 17:22:13,196 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 17:22:13,197 	Text Reference  :	**** symonds has scored 2   centuries in  26    tests that he played for his country
2024-02-04 17:22:13,197 	Text Hypothesis :	when symonds *** ****** was driving   the first match when he played for his bat    
2024-02-04 17:22:13,197 	Text Alignment  :	I            D   D      S   S         S   S     S     S                      S      
2024-02-04 17:22:13,197 ========================================================================================================================
2024-02-04 17:22:13,197 Logging Sequence: 164_100.00
2024-02-04 17:22:13,198 	Gloss Reference :	A B+C+D+E
2024-02-04 17:22:13,198 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 17:22:13,198 	Gloss Alignment :	         
2024-02-04 17:22:13,198 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 17:22:13,200 	Text Reference  :	the tv rights for broadcasting ipl matches in   india for  the     next  5  years   went to   star india for rs 23575 crore
2024-02-04 17:22:13,200 	Text Hypothesis :	*** ** group  a   consisted    of  mena    that was   made several final of india's best team on   tv    for ** ***** this 
2024-02-04 17:22:13,201 	Text Alignment  :	D   D  S      S   S            S   S       S    S     S    S       S     S  S       S    S    S    S         D  D     S    
2024-02-04 17:22:13,201 ========================================================================================================================
2024-02-04 17:22:13,201 Logging Sequence: 76_79.00
2024-02-04 17:22:13,201 	Gloss Reference :	A B+C+D+E
2024-02-04 17:22:13,201 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 17:22:13,201 	Gloss Alignment :	         
2024-02-04 17:22:13,201 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 17:22:13,202 	Text Reference  :	** **** ***** **** ***** ***** speaking to ani    csk     ceo  kasi     viswanathan said
2024-02-04 17:22:13,202 	Text Hypothesis :	on 23rd april 2021 virat kohli posted   a  mumbai indians were stressed on          hold
2024-02-04 17:22:13,202 	Text Alignment  :	I  I    I     I    I     I     S        S  S      S       S    S        S           S   
2024-02-04 17:22:13,202 ========================================================================================================================
2024-02-04 17:22:24,690 Epoch 223: Total Training Recognition Loss 0.06  Total Training Translation Loss 12.04 
2024-02-04 17:22:24,691 EPOCH 224
2024-02-04 17:22:37,231 Epoch 224: Total Training Recognition Loss 0.06  Total Training Translation Loss 11.95 
2024-02-04 17:22:37,231 EPOCH 225
2024-02-04 17:22:49,219 Epoch 225: Total Training Recognition Loss 0.06  Total Training Translation Loss 11.57 
2024-02-04 17:22:49,220 EPOCH 226
2024-02-04 17:23:01,327 Epoch 226: Total Training Recognition Loss 0.06  Total Training Translation Loss 11.24 
2024-02-04 17:23:01,327 EPOCH 227
2024-02-04 17:23:13,564 Epoch 227: Total Training Recognition Loss 0.05  Total Training Translation Loss 11.08 
2024-02-04 17:23:13,565 EPOCH 228
2024-02-04 17:23:25,577 Epoch 228: Total Training Recognition Loss 0.06  Total Training Translation Loss 10.98 
2024-02-04 17:23:25,577 EPOCH 229
2024-02-04 17:23:37,480 Epoch 229: Total Training Recognition Loss 0.06  Total Training Translation Loss 10.86 
2024-02-04 17:23:37,481 EPOCH 230
2024-02-04 17:23:49,627 Epoch 230: Total Training Recognition Loss 0.06  Total Training Translation Loss 10.58 
2024-02-04 17:23:49,627 EPOCH 231
2024-02-04 17:24:01,604 Epoch 231: Total Training Recognition Loss 0.05  Total Training Translation Loss 10.59 
2024-02-04 17:24:01,605 EPOCH 232
2024-02-04 17:24:13,604 Epoch 232: Total Training Recognition Loss 0.05  Total Training Translation Loss 10.34 
2024-02-04 17:24:13,604 EPOCH 233
2024-02-04 17:24:25,847 Epoch 233: Total Training Recognition Loss 0.05  Total Training Translation Loss 10.19 
2024-02-04 17:24:25,848 EPOCH 234
2024-02-04 17:24:28,398 [Epoch: 234 Step: 00002100] Batch Recognition Loss:   0.006370 => Gls Tokens per Sec:     1506 || Batch Translation Loss:   0.723210 => Txt Tokens per Sec:     3650 || Lr: 0.000100
2024-02-04 17:24:37,785 Epoch 234: Total Training Recognition Loss 0.05  Total Training Translation Loss 10.15 
2024-02-04 17:24:37,786 EPOCH 235
2024-02-04 17:24:49,800 Epoch 235: Total Training Recognition Loss 0.05  Total Training Translation Loss 9.86 
2024-02-04 17:24:49,800 EPOCH 236
2024-02-04 17:25:01,813 Epoch 236: Total Training Recognition Loss 0.05  Total Training Translation Loss 9.95 
2024-02-04 17:25:01,813 EPOCH 237
2024-02-04 17:25:13,646 Epoch 237: Total Training Recognition Loss 0.05  Total Training Translation Loss 9.93 
2024-02-04 17:25:13,646 EPOCH 238
2024-02-04 17:25:25,521 Epoch 238: Total Training Recognition Loss 0.05  Total Training Translation Loss 9.62 
2024-02-04 17:25:25,522 EPOCH 239
2024-02-04 17:25:37,726 Epoch 239: Total Training Recognition Loss 0.05  Total Training Translation Loss 9.32 
2024-02-04 17:25:37,727 EPOCH 240
2024-02-04 17:25:49,700 Epoch 240: Total Training Recognition Loss 0.05  Total Training Translation Loss 9.33 
2024-02-04 17:25:49,700 EPOCH 241
2024-02-04 17:26:01,931 Epoch 241: Total Training Recognition Loss 0.05  Total Training Translation Loss 9.34 
2024-02-04 17:26:01,931 EPOCH 242
2024-02-04 17:26:13,834 Epoch 242: Total Training Recognition Loss 0.05  Total Training Translation Loss 9.31 
2024-02-04 17:26:13,835 EPOCH 243
2024-02-04 17:26:25,904 Epoch 243: Total Training Recognition Loss 0.05  Total Training Translation Loss 9.30 
2024-02-04 17:26:25,904 EPOCH 244
2024-02-04 17:26:37,936 Epoch 244: Total Training Recognition Loss 0.05  Total Training Translation Loss 9.05 
2024-02-04 17:26:37,937 EPOCH 245
2024-02-04 17:26:44,111 [Epoch: 245 Step: 00002200] Batch Recognition Loss:   0.005543 => Gls Tokens per Sec:      685 || Batch Translation Loss:   0.617119 => Txt Tokens per Sec:     1731 || Lr: 0.000100
2024-02-04 17:26:49,702 Epoch 245: Total Training Recognition Loss 0.05  Total Training Translation Loss 8.98 
2024-02-04 17:26:49,702 EPOCH 246
2024-02-04 17:27:01,743 Epoch 246: Total Training Recognition Loss 0.05  Total Training Translation Loss 8.63 
2024-02-04 17:27:01,744 EPOCH 247
2024-02-04 17:27:13,677 Epoch 247: Total Training Recognition Loss 0.05  Total Training Translation Loss 8.53 
2024-02-04 17:27:13,678 EPOCH 248
2024-02-04 17:27:25,692 Epoch 248: Total Training Recognition Loss 0.04  Total Training Translation Loss 8.62 
2024-02-04 17:27:25,693 EPOCH 249
2024-02-04 17:27:37,566 Epoch 249: Total Training Recognition Loss 0.05  Total Training Translation Loss 8.22 
2024-02-04 17:27:37,567 EPOCH 250
2024-02-04 17:27:49,611 Epoch 250: Total Training Recognition Loss 0.04  Total Training Translation Loss 7.92 
2024-02-04 17:27:49,611 EPOCH 251
2024-02-04 17:28:01,756 Epoch 251: Total Training Recognition Loss 0.05  Total Training Translation Loss 8.02 
2024-02-04 17:28:01,757 EPOCH 252
2024-02-04 17:28:13,914 Epoch 252: Total Training Recognition Loss 0.04  Total Training Translation Loss 7.84 
2024-02-04 17:28:13,915 EPOCH 253
2024-02-04 17:28:25,978 Epoch 253: Total Training Recognition Loss 0.05  Total Training Translation Loss 7.75 
2024-02-04 17:28:25,978 EPOCH 254
2024-02-04 17:28:37,820 Epoch 254: Total Training Recognition Loss 0.04  Total Training Translation Loss 7.74 
2024-02-04 17:28:37,820 EPOCH 255
2024-02-04 17:28:50,223 Epoch 255: Total Training Recognition Loss 0.04  Total Training Translation Loss 7.73 
2024-02-04 17:28:50,223 EPOCH 256
2024-02-04 17:28:52,527 [Epoch: 256 Step: 00002300] Batch Recognition Loss:   0.004538 => Gls Tokens per Sec:     2779 || Batch Translation Loss:   0.748726 => Txt Tokens per Sec:     7499 || Lr: 0.000100
2024-02-04 17:29:02,076 Epoch 256: Total Training Recognition Loss 0.04  Total Training Translation Loss 7.92 
2024-02-04 17:29:02,077 EPOCH 257
2024-02-04 17:29:14,141 Epoch 257: Total Training Recognition Loss 0.04  Total Training Translation Loss 7.88 
2024-02-04 17:29:14,142 EPOCH 258
2024-02-04 17:29:26,232 Epoch 258: Total Training Recognition Loss 0.04  Total Training Translation Loss 7.76 
2024-02-04 17:29:26,232 EPOCH 259
2024-02-04 17:29:38,432 Epoch 259: Total Training Recognition Loss 0.04  Total Training Translation Loss 7.64 
2024-02-04 17:29:38,432 EPOCH 260
2024-02-04 17:29:50,422 Epoch 260: Total Training Recognition Loss 0.05  Total Training Translation Loss 7.54 
2024-02-04 17:29:50,423 EPOCH 261
2024-02-04 17:30:02,229 Epoch 261: Total Training Recognition Loss 0.04  Total Training Translation Loss 7.18 
2024-02-04 17:30:02,229 EPOCH 262
2024-02-04 17:30:14,324 Epoch 262: Total Training Recognition Loss 0.04  Total Training Translation Loss 7.11 
2024-02-04 17:30:14,324 EPOCH 263
2024-02-04 17:30:26,262 Epoch 263: Total Training Recognition Loss 0.04  Total Training Translation Loss 6.88 
2024-02-04 17:30:26,262 EPOCH 264
2024-02-04 17:30:38,391 Epoch 264: Total Training Recognition Loss 0.04  Total Training Translation Loss 6.52 
2024-02-04 17:30:38,392 EPOCH 265
2024-02-04 17:30:49,933 Epoch 265: Total Training Recognition Loss 0.04  Total Training Translation Loss 6.53 
2024-02-04 17:30:49,934 EPOCH 266
2024-02-04 17:31:02,168 Epoch 266: Total Training Recognition Loss 0.04  Total Training Translation Loss 6.53 
2024-02-04 17:31:02,168 EPOCH 267
2024-02-04 17:31:09,836 [Epoch: 267 Step: 00002400] Batch Recognition Loss:   0.004150 => Gls Tokens per Sec:      886 || Batch Translation Loss:   0.818067 => Txt Tokens per Sec:     2551 || Lr: 0.000100
2024-02-04 17:31:13,961 Epoch 267: Total Training Recognition Loss 0.04  Total Training Translation Loss 6.33 
2024-02-04 17:31:13,961 EPOCH 268
2024-02-04 17:31:25,435 Epoch 268: Total Training Recognition Loss 0.04  Total Training Translation Loss 6.34 
2024-02-04 17:31:25,436 EPOCH 269
2024-02-04 17:31:37,541 Epoch 269: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.24 
2024-02-04 17:31:37,541 EPOCH 270
2024-02-04 17:31:49,330 Epoch 270: Total Training Recognition Loss 0.04  Total Training Translation Loss 6.16 
2024-02-04 17:31:49,331 EPOCH 271
2024-02-04 17:32:01,333 Epoch 271: Total Training Recognition Loss 0.04  Total Training Translation Loss 6.02 
2024-02-04 17:32:01,334 EPOCH 272
2024-02-04 17:32:13,128 Epoch 272: Total Training Recognition Loss 0.04  Total Training Translation Loss 6.08 
2024-02-04 17:32:13,128 EPOCH 273
2024-02-04 17:32:25,031 Epoch 273: Total Training Recognition Loss 0.04  Total Training Translation Loss 6.01 
2024-02-04 17:32:25,032 EPOCH 274
2024-02-04 17:32:36,861 Epoch 274: Total Training Recognition Loss 0.04  Total Training Translation Loss 6.06 
2024-02-04 17:32:36,861 EPOCH 275
2024-02-04 17:32:48,780 Epoch 275: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.95 
2024-02-04 17:32:48,780 EPOCH 276
2024-02-04 17:33:00,830 Epoch 276: Total Training Recognition Loss 0.04  Total Training Translation Loss 5.84 
2024-02-04 17:33:00,831 EPOCH 277
2024-02-04 17:33:12,680 Epoch 277: Total Training Recognition Loss 0.04  Total Training Translation Loss 5.69 
2024-02-04 17:33:12,681 EPOCH 278
2024-02-04 17:33:23,311 [Epoch: 278 Step: 00002500] Batch Recognition Loss:   0.003001 => Gls Tokens per Sec:      759 || Batch Translation Loss:   0.255813 => Txt Tokens per Sec:     2059 || Lr: 0.000100
2024-02-04 17:33:24,512 Epoch 278: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.52 
2024-02-04 17:33:24,512 EPOCH 279
2024-02-04 17:33:36,338 Epoch 279: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.62 
2024-02-04 17:33:36,338 EPOCH 280
2024-02-04 17:33:48,398 Epoch 280: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.85 
2024-02-04 17:33:48,398 EPOCH 281
2024-02-04 17:34:00,462 Epoch 281: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.11 
2024-02-04 17:34:00,463 EPOCH 282
2024-02-04 17:34:12,330 Epoch 282: Total Training Recognition Loss 0.04  Total Training Translation Loss 11.16 
2024-02-04 17:34:12,330 EPOCH 283
2024-02-04 17:34:24,309 Epoch 283: Total Training Recognition Loss 0.07  Total Training Translation Loss 47.42 
2024-02-04 17:34:24,310 EPOCH 284
2024-02-04 17:34:36,229 Epoch 284: Total Training Recognition Loss 0.25  Total Training Translation Loss 22.27 
2024-02-04 17:34:36,230 EPOCH 285
2024-02-04 17:34:48,199 Epoch 285: Total Training Recognition Loss 0.10  Total Training Translation Loss 17.08 
2024-02-04 17:34:48,200 EPOCH 286
2024-02-04 17:35:00,421 Epoch 286: Total Training Recognition Loss 0.07  Total Training Translation Loss 12.53 
2024-02-04 17:35:00,421 EPOCH 287
2024-02-04 17:35:12,267 Epoch 287: Total Training Recognition Loss 0.09  Total Training Translation Loss 9.62 
2024-02-04 17:35:12,268 EPOCH 288
2024-02-04 17:35:24,151 Epoch 288: Total Training Recognition Loss 0.06  Total Training Translation Loss 8.07 
2024-02-04 17:35:24,152 EPOCH 289
2024-02-04 17:35:34,351 [Epoch: 289 Step: 00002600] Batch Recognition Loss:   0.006276 => Gls Tokens per Sec:      917 || Batch Translation Loss:   0.928219 => Txt Tokens per Sec:     2489 || Lr: 0.000100
2024-02-04 17:35:36,350 Epoch 289: Total Training Recognition Loss 0.05  Total Training Translation Loss 7.17 
2024-02-04 17:35:36,350 EPOCH 290
2024-02-04 17:35:48,040 Epoch 290: Total Training Recognition Loss 0.05  Total Training Translation Loss 6.43 
2024-02-04 17:35:48,040 EPOCH 291
2024-02-04 17:35:59,986 Epoch 291: Total Training Recognition Loss 0.05  Total Training Translation Loss 6.09 
2024-02-04 17:35:59,987 EPOCH 292
2024-02-04 17:36:11,792 Epoch 292: Total Training Recognition Loss 0.05  Total Training Translation Loss 5.76 
2024-02-04 17:36:11,792 EPOCH 293
2024-02-04 17:36:24,271 Epoch 293: Total Training Recognition Loss 0.04  Total Training Translation Loss 5.42 
2024-02-04 17:36:24,271 EPOCH 294
2024-02-04 17:36:36,113 Epoch 294: Total Training Recognition Loss 0.04  Total Training Translation Loss 5.34 
2024-02-04 17:36:36,113 EPOCH 295
2024-02-04 17:36:47,779 Epoch 295: Total Training Recognition Loss 0.04  Total Training Translation Loss 5.09 
2024-02-04 17:36:47,780 EPOCH 296
2024-02-04 17:36:59,912 Epoch 296: Total Training Recognition Loss 0.04  Total Training Translation Loss 4.88 
2024-02-04 17:36:59,912 EPOCH 297
2024-02-04 17:37:11,757 Epoch 297: Total Training Recognition Loss 0.04  Total Training Translation Loss 4.77 
2024-02-04 17:37:11,758 EPOCH 298
2024-02-04 17:37:23,305 Epoch 298: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.62 
2024-02-04 17:37:23,306 EPOCH 299
2024-02-04 17:37:34,645 Epoch 299: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.52 
2024-02-04 17:37:34,645 EPOCH 300
2024-02-04 17:37:46,628 [Epoch: 300 Step: 00002700] Batch Recognition Loss:   0.003952 => Gls Tokens per Sec:      887 || Batch Translation Loss:   0.649831 => Txt Tokens per Sec:     2463 || Lr: 0.000100
2024-02-04 17:37:46,628 Epoch 300: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.49 
2024-02-04 17:37:46,628 EPOCH 301
2024-02-04 17:37:58,258 Epoch 301: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.31 
2024-02-04 17:37:58,259 EPOCH 302
2024-02-04 17:38:09,896 Epoch 302: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.25 
2024-02-04 17:38:09,897 EPOCH 303
2024-02-04 17:38:21,998 Epoch 303: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.14 
2024-02-04 17:38:21,999 EPOCH 304
2024-02-04 17:38:33,902 Epoch 304: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.17 
2024-02-04 17:38:33,903 EPOCH 305
2024-02-04 17:38:45,702 Epoch 305: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.04 
2024-02-04 17:38:45,702 EPOCH 306
2024-02-04 17:38:57,540 Epoch 306: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.00 
2024-02-04 17:38:57,541 EPOCH 307
2024-02-04 17:39:09,747 Epoch 307: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.03 
2024-02-04 17:39:09,748 EPOCH 308
2024-02-04 17:39:22,105 Epoch 308: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.94 
2024-02-04 17:39:22,105 EPOCH 309
2024-02-04 17:39:34,218 Epoch 309: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.90 
2024-02-04 17:39:34,218 EPOCH 310
2024-02-04 17:39:46,354 Epoch 310: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.81 
2024-02-04 17:39:46,355 EPOCH 311
2024-02-04 17:39:58,067 Epoch 311: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.78 
2024-02-04 17:39:58,068 EPOCH 312
2024-02-04 17:39:58,890 [Epoch: 312 Step: 00002800] Batch Recognition Loss:   0.003412 => Gls Tokens per Sec:     1560 || Batch Translation Loss:   0.520609 => Txt Tokens per Sec:     4752 || Lr: 0.000100
2024-02-04 17:40:10,030 Epoch 312: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.71 
2024-02-04 17:40:10,031 EPOCH 313
2024-02-04 17:40:21,978 Epoch 313: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.71 
2024-02-04 17:40:21,979 EPOCH 314
2024-02-04 17:40:33,928 Epoch 314: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.61 
2024-02-04 17:40:33,929 EPOCH 315
2024-02-04 17:40:45,918 Epoch 315: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.79 
2024-02-04 17:40:45,918 EPOCH 316
2024-02-04 17:40:57,811 Epoch 316: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.56 
2024-02-04 17:40:57,811 EPOCH 317
2024-02-04 17:41:09,829 Epoch 317: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.55 
2024-02-04 17:41:09,830 EPOCH 318
2024-02-04 17:41:21,958 Epoch 318: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.48 
2024-02-04 17:41:21,958 EPOCH 319
2024-02-04 17:41:33,896 Epoch 319: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.41 
2024-02-04 17:41:33,897 EPOCH 320
2024-02-04 17:41:45,629 Epoch 320: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.41 
2024-02-04 17:41:45,629 EPOCH 321
2024-02-04 17:41:57,289 Epoch 321: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.33 
2024-02-04 17:41:57,290 EPOCH 322
2024-02-04 17:42:09,141 Epoch 322: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.23 
2024-02-04 17:42:09,142 EPOCH 323
2024-02-04 17:42:10,176 [Epoch: 323 Step: 00002900] Batch Recognition Loss:   0.002607 => Gls Tokens per Sec:     2477 || Batch Translation Loss:   0.329948 => Txt Tokens per Sec:     6536 || Lr: 0.000100
2024-02-04 17:42:21,096 Epoch 323: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.30 
2024-02-04 17:42:21,097 EPOCH 324
2024-02-04 17:42:33,116 Epoch 324: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.25 
2024-02-04 17:42:33,117 EPOCH 325
2024-02-04 17:42:47,237 Epoch 325: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.26 
2024-02-04 17:42:47,238 EPOCH 326
2024-02-04 17:43:04,116 Epoch 326: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.26 
2024-02-04 17:43:04,117 EPOCH 327
2024-02-04 17:43:18,446 Epoch 327: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.25 
2024-02-04 17:43:18,446 EPOCH 328
2024-02-04 17:43:31,196 Epoch 328: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.16 
2024-02-04 17:43:31,196 EPOCH 329
2024-02-04 17:43:44,141 Epoch 329: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.15 
2024-02-04 17:43:44,142 EPOCH 330
2024-02-04 17:43:56,163 Epoch 330: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.01 
2024-02-04 17:43:56,164 EPOCH 331
2024-02-04 17:44:08,525 Epoch 331: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.05 
2024-02-04 17:44:08,526 EPOCH 332
2024-02-04 17:44:20,915 Epoch 332: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.08 
2024-02-04 17:44:20,915 EPOCH 333
2024-02-04 17:44:33,219 Epoch 333: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.07 
2024-02-04 17:44:33,220 EPOCH 334
2024-02-04 17:44:34,562 [Epoch: 334 Step: 00003000] Batch Recognition Loss:   0.002000 => Gls Tokens per Sec:     2863 || Batch Translation Loss:   0.289192 => Txt Tokens per Sec:     7474 || Lr: 0.000100
2024-02-04 17:44:45,175 Epoch 334: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.99 
2024-02-04 17:44:45,175 EPOCH 335
2024-02-04 17:44:57,475 Epoch 335: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.01 
2024-02-04 17:44:57,475 EPOCH 336
2024-02-04 17:45:09,700 Epoch 336: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.81 
2024-02-04 17:45:09,701 EPOCH 337
2024-02-04 17:45:22,135 Epoch 337: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.91 
2024-02-04 17:45:22,136 EPOCH 338
2024-02-04 17:45:34,128 Epoch 338: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.80 
2024-02-04 17:45:34,129 EPOCH 339
2024-02-04 17:45:46,179 Epoch 339: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.78 
2024-02-04 17:45:46,180 EPOCH 340
2024-02-04 17:45:58,368 Epoch 340: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.82 
2024-02-04 17:45:58,369 EPOCH 341
2024-02-04 17:46:10,451 Epoch 341: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.80 
2024-02-04 17:46:10,452 EPOCH 342
2024-02-04 17:46:22,608 Epoch 342: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.84 
2024-02-04 17:46:22,608 EPOCH 343
2024-02-04 17:46:34,690 Epoch 343: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.81 
2024-02-04 17:46:34,691 EPOCH 344
2024-02-04 17:46:46,909 Epoch 344: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.73 
2024-02-04 17:46:46,910 EPOCH 345
2024-02-04 17:46:53,668 [Epoch: 345 Step: 00003100] Batch Recognition Loss:   0.002245 => Gls Tokens per Sec:      626 || Batch Translation Loss:   0.360045 => Txt Tokens per Sec:     1855 || Lr: 0.000100
2024-02-04 17:46:58,800 Epoch 345: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.79 
2024-02-04 17:46:58,800 EPOCH 346
2024-02-04 17:47:10,590 Epoch 346: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.89 
2024-02-04 17:47:10,590 EPOCH 347
2024-02-04 17:47:22,420 Epoch 347: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.79 
2024-02-04 17:47:22,421 EPOCH 348
2024-02-04 17:47:34,478 Epoch 348: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.80 
2024-02-04 17:47:34,478 EPOCH 349
2024-02-04 17:47:46,815 Epoch 349: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.83 
2024-02-04 17:47:46,816 EPOCH 350
2024-02-04 17:47:58,870 Epoch 350: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.04 
2024-02-04 17:47:58,871 EPOCH 351
2024-02-04 17:48:10,963 Epoch 351: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.92 
2024-02-04 17:48:10,964 EPOCH 352
2024-02-04 17:48:23,138 Epoch 352: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.86 
2024-02-04 17:48:23,139 EPOCH 353
2024-02-04 17:48:35,052 Epoch 353: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.01 
2024-02-04 17:48:35,053 EPOCH 354
2024-02-04 17:48:47,015 Epoch 354: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.95 
2024-02-04 17:48:47,016 EPOCH 355
2024-02-04 17:48:58,991 Epoch 355: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.83 
2024-02-04 17:48:58,991 EPOCH 356
2024-02-04 17:49:05,883 [Epoch: 356 Step: 00003200] Batch Recognition Loss:   0.003274 => Gls Tokens per Sec:      800 || Batch Translation Loss:   0.174292 => Txt Tokens per Sec:     2137 || Lr: 0.000100
2024-02-04 17:49:11,251 Epoch 356: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.64 
2024-02-04 17:49:11,251 EPOCH 357
2024-02-04 17:49:23,376 Epoch 357: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.76 
2024-02-04 17:49:23,376 EPOCH 358
2024-02-04 17:49:35,248 Epoch 358: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.67 
2024-02-04 17:49:35,249 EPOCH 359
2024-02-04 17:49:47,336 Epoch 359: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.49 
2024-02-04 17:49:47,337 EPOCH 360
2024-02-04 17:49:59,564 Epoch 360: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.55 
2024-02-04 17:49:59,564 EPOCH 361
2024-02-04 17:50:11,591 Epoch 361: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.50 
2024-02-04 17:50:11,592 EPOCH 362
2024-02-04 17:50:23,157 Epoch 362: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.45 
2024-02-04 17:50:23,158 EPOCH 363
2024-02-04 17:50:35,147 Epoch 363: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.43 
2024-02-04 17:50:35,147 EPOCH 364
2024-02-04 17:50:47,358 Epoch 364: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.37 
2024-02-04 17:50:47,358 EPOCH 365
2024-02-04 17:50:59,393 Epoch 365: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.35 
2024-02-04 17:50:59,393 EPOCH 366
2024-02-04 17:51:11,241 Epoch 366: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.27 
2024-02-04 17:51:11,241 EPOCH 367
2024-02-04 17:51:17,232 [Epoch: 367 Step: 00003300] Batch Recognition Loss:   0.001504 => Gls Tokens per Sec:     1282 || Batch Translation Loss:   0.193437 => Txt Tokens per Sec:     3393 || Lr: 0.000100
2024-02-04 17:51:23,688 Epoch 367: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.27 
2024-02-04 17:51:23,688 EPOCH 368
2024-02-04 17:51:35,732 Epoch 368: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.26 
2024-02-04 17:51:35,733 EPOCH 369
2024-02-04 17:51:47,207 Epoch 369: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.46 
2024-02-04 17:51:47,207 EPOCH 370
2024-02-04 17:51:59,269 Epoch 370: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.32 
2024-02-04 17:51:59,269 EPOCH 371
2024-02-04 17:52:10,874 Epoch 371: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.13 
2024-02-04 17:52:10,874 EPOCH 372
2024-02-04 17:52:23,148 Epoch 372: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.22 
2024-02-04 17:52:23,148 EPOCH 373
2024-02-04 17:52:35,027 Epoch 373: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.17 
2024-02-04 17:52:35,027 EPOCH 374
2024-02-04 17:52:47,133 Epoch 374: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.06 
2024-02-04 17:52:47,134 EPOCH 375
2024-02-04 17:52:59,148 Epoch 375: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.16 
2024-02-04 17:52:59,148 EPOCH 376
2024-02-04 17:53:10,847 Epoch 376: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.15 
2024-02-04 17:53:10,848 EPOCH 377
2024-02-04 17:53:22,867 Epoch 377: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.17 
2024-02-04 17:53:22,868 EPOCH 378
2024-02-04 17:53:34,048 [Epoch: 378 Step: 00003400] Batch Recognition Loss:   0.001979 => Gls Tokens per Sec:      722 || Batch Translation Loss:   0.173294 => Txt Tokens per Sec:     1991 || Lr: 0.000100
2024-02-04 17:53:35,070 Epoch 378: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.24 
2024-02-04 17:53:35,070 EPOCH 379
2024-02-04 17:53:46,894 Epoch 379: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.07 
2024-02-04 17:53:46,895 EPOCH 380
2024-02-04 17:53:58,839 Epoch 380: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.32 
2024-02-04 17:53:58,839 EPOCH 381
2024-02-04 17:54:10,325 Epoch 381: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.24 
2024-02-04 17:54:10,326 EPOCH 382
2024-02-04 17:54:22,180 Epoch 382: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.33 
2024-02-04 17:54:22,181 EPOCH 383
2024-02-04 17:54:34,024 Epoch 383: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.45 
2024-02-04 17:54:34,025 EPOCH 384
2024-02-04 17:54:46,040 Epoch 384: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.43 
2024-02-04 17:54:46,041 EPOCH 385
2024-02-04 17:54:57,857 Epoch 385: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.64 
2024-02-04 17:54:57,858 EPOCH 386
2024-02-04 17:55:09,434 Epoch 386: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.74 
2024-02-04 17:55:09,435 EPOCH 387
2024-02-04 17:55:21,409 Epoch 387: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.37 
2024-02-04 17:55:21,410 EPOCH 388
2024-02-04 17:55:33,375 Epoch 388: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.43 
2024-02-04 17:55:33,376 EPOCH 389
2024-02-04 17:55:45,081 [Epoch: 389 Step: 00003500] Batch Recognition Loss:   0.001703 => Gls Tokens per Sec:      799 || Batch Translation Loss:   0.197644 => Txt Tokens per Sec:     2328 || Lr: 0.000100
2024-02-04 17:55:45,326 Epoch 389: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.32 
2024-02-04 17:55:45,326 EPOCH 390
2024-02-04 17:55:57,198 Epoch 390: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.20 
2024-02-04 17:55:57,200 EPOCH 391
2024-02-04 17:56:09,105 Epoch 391: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.20 
2024-02-04 17:56:09,106 EPOCH 392
2024-02-04 17:56:21,056 Epoch 392: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.08 
2024-02-04 17:56:21,056 EPOCH 393
2024-02-04 17:56:32,856 Epoch 393: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.96 
2024-02-04 17:56:32,857 EPOCH 394
2024-02-04 17:56:45,093 Epoch 394: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.88 
2024-02-04 17:56:45,094 EPOCH 395
2024-02-04 17:56:57,041 Epoch 395: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.86 
2024-02-04 17:56:57,041 EPOCH 396
2024-02-04 17:57:09,097 Epoch 396: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.93 
2024-02-04 17:57:09,097 EPOCH 397
2024-02-04 17:57:21,175 Epoch 397: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.85 
2024-02-04 17:57:21,175 EPOCH 398
2024-02-04 17:57:33,107 Epoch 398: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.90 
2024-02-04 17:57:33,108 EPOCH 399
2024-02-04 17:57:45,144 Epoch 399: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.83 
2024-02-04 17:57:45,145 EPOCH 400
2024-02-04 17:57:56,914 [Epoch: 400 Step: 00003600] Batch Recognition Loss:   0.001741 => Gls Tokens per Sec:      903 || Batch Translation Loss:   0.248153 => Txt Tokens per Sec:     2508 || Lr: 0.000100
2024-02-04 17:57:56,915 Epoch 400: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.89 
2024-02-04 17:57:56,915 EPOCH 401
2024-02-04 17:58:08,914 Epoch 401: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.09 
2024-02-04 17:58:08,914 EPOCH 402
2024-02-04 17:58:20,786 Epoch 402: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.12 
2024-02-04 17:58:20,786 EPOCH 403
2024-02-04 17:58:32,921 Epoch 403: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.02 
2024-02-04 17:58:32,921 EPOCH 404
2024-02-04 17:58:44,892 Epoch 404: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.05 
2024-02-04 17:58:44,893 EPOCH 405
2024-02-04 17:58:56,922 Epoch 405: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.95 
2024-02-04 17:58:56,923 EPOCH 406
2024-02-04 17:59:08,895 Epoch 406: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.07 
2024-02-04 17:59:08,895 EPOCH 407
2024-02-04 17:59:20,634 Epoch 407: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.03 
2024-02-04 17:59:20,635 EPOCH 408
2024-02-04 17:59:32,792 Epoch 408: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.94 
2024-02-04 17:59:32,793 EPOCH 409
2024-02-04 17:59:44,511 Epoch 409: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.88 
2024-02-04 17:59:44,512 EPOCH 410
2024-02-04 17:59:56,360 Epoch 410: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.96 
2024-02-04 17:59:56,361 EPOCH 411
2024-02-04 18:00:08,076 Epoch 411: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.04 
2024-02-04 18:00:08,077 EPOCH 412
2024-02-04 18:00:11,901 [Epoch: 412 Step: 00003700] Batch Recognition Loss:   0.001285 => Gls Tokens per Sec:      102 || Batch Translation Loss:   0.099856 => Txt Tokens per Sec:      365 || Lr: 0.000100
2024-02-04 18:00:19,922 Epoch 412: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.94 
2024-02-04 18:00:19,923 EPOCH 413
2024-02-04 18:00:31,919 Epoch 413: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.94 
2024-02-04 18:00:31,920 EPOCH 414
2024-02-04 18:00:43,625 Epoch 414: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.95 
2024-02-04 18:00:43,625 EPOCH 415
2024-02-04 18:00:55,459 Epoch 415: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.89 
2024-02-04 18:00:55,460 EPOCH 416
2024-02-04 18:01:07,288 Epoch 416: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.89 
2024-02-04 18:01:07,288 EPOCH 417
2024-02-04 18:01:19,151 Epoch 417: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.09 
2024-02-04 18:01:19,151 EPOCH 418
2024-02-04 18:01:31,250 Epoch 418: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.22 
2024-02-04 18:01:31,251 EPOCH 419
2024-02-04 18:01:42,794 Epoch 419: Total Training Recognition Loss 0.02  Total Training Translation Loss 9.69 
2024-02-04 18:01:42,794 EPOCH 420
2024-02-04 18:01:54,281 Epoch 420: Total Training Recognition Loss 0.06  Total Training Translation Loss 9.18 
2024-02-04 18:01:54,281 EPOCH 421
2024-02-04 18:02:06,164 Epoch 421: Total Training Recognition Loss 0.05  Total Training Translation Loss 8.08 
2024-02-04 18:02:06,165 EPOCH 422
2024-02-04 18:02:18,272 Epoch 422: Total Training Recognition Loss 0.05  Total Training Translation Loss 5.79 
2024-02-04 18:02:18,272 EPOCH 423
2024-02-04 18:02:21,972 [Epoch: 423 Step: 00003800] Batch Recognition Loss:   0.002232 => Gls Tokens per Sec:      692 || Batch Translation Loss:   0.382772 => Txt Tokens per Sec:     2070 || Lr: 0.000100
2024-02-04 18:02:30,002 Epoch 423: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.61 
2024-02-04 18:02:30,003 EPOCH 424
2024-02-04 18:02:41,854 Epoch 424: Total Training Recognition Loss 0.05  Total Training Translation Loss 5.23 
2024-02-04 18:02:41,855 EPOCH 425
2024-02-04 18:02:53,779 Epoch 425: Total Training Recognition Loss 0.04  Total Training Translation Loss 4.07 
2024-02-04 18:02:53,780 EPOCH 426
2024-02-04 18:03:05,344 Epoch 426: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.77 
2024-02-04 18:03:05,345 EPOCH 427
2024-02-04 18:03:16,998 Epoch 427: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.15 
2024-02-04 18:03:16,998 EPOCH 428
2024-02-04 18:03:28,903 Epoch 428: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.63 
2024-02-04 18:03:28,903 EPOCH 429
2024-02-04 18:03:40,890 Epoch 429: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.32 
2024-02-04 18:03:40,890 EPOCH 430
2024-02-04 18:03:52,778 Epoch 430: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.07 
2024-02-04 18:03:52,778 EPOCH 431
2024-02-04 18:04:04,900 Epoch 431: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.83 
2024-02-04 18:04:04,901 EPOCH 432
2024-02-04 18:04:16,635 Epoch 432: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.69 
2024-02-04 18:04:16,635 EPOCH 433
2024-02-04 18:04:28,343 Epoch 433: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.63 
2024-02-04 18:04:28,344 EPOCH 434
2024-02-04 18:04:29,713 [Epoch: 434 Step: 00003900] Batch Recognition Loss:   0.001396 => Gls Tokens per Sec:     2810 || Batch Translation Loss:   0.180068 => Txt Tokens per Sec:     7252 || Lr: 0.000100
2024-02-04 18:04:40,143 Epoch 434: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.65 
2024-02-04 18:04:40,144 EPOCH 435
2024-02-04 18:04:51,890 Epoch 435: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.56 
2024-02-04 18:04:51,891 EPOCH 436
2024-02-04 18:05:03,485 Epoch 436: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.43 
2024-02-04 18:05:03,486 EPOCH 437
2024-02-04 18:05:15,369 Epoch 437: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.42 
2024-02-04 18:05:15,369 EPOCH 438
2024-02-04 18:05:27,295 Epoch 438: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.40 
2024-02-04 18:05:27,295 EPOCH 439
2024-02-04 18:05:38,948 Epoch 439: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.43 
2024-02-04 18:05:38,949 EPOCH 440
2024-02-04 18:05:50,685 Epoch 440: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.44 
2024-02-04 18:05:50,686 EPOCH 441
2024-02-04 18:06:02,619 Epoch 441: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.38 
2024-02-04 18:06:02,619 EPOCH 442
2024-02-04 18:06:14,312 Epoch 442: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.30 
2024-02-04 18:06:14,312 EPOCH 443
2024-02-04 18:06:25,689 Epoch 443: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.34 
2024-02-04 18:06:25,689 EPOCH 444
2024-02-04 18:06:37,555 Epoch 444: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.29 
2024-02-04 18:06:37,556 EPOCH 445
2024-02-04 18:06:42,189 [Epoch: 445 Step: 00004000] Batch Recognition Loss:   0.001484 => Gls Tokens per Sec:     1106 || Batch Translation Loss:   0.202903 => Txt Tokens per Sec:     3052 || Lr: 0.000100
2024-02-04 18:07:04,051 Validation result at epoch 445, step     4000: duration: 21.8627s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00092	Translation Loss: 83831.10156	PPL: 4398.34131
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.99	(BLEU-1: 12.63,	BLEU-2: 4.52,	BLEU-3: 1.96,	BLEU-4: 0.99)
	CHRF 18.09	ROUGE 10.76
2024-02-04 18:07:04,052 Logging Recognition and Translation Outputs
2024-02-04 18:07:04,053 ========================================================================================================================
2024-02-04 18:07:04,053 Logging Sequence: 133_173.00
2024-02-04 18:07:04,053 	Gloss Reference :	A B+C+D+E
2024-02-04 18:07:04,053 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 18:07:04,053 	Gloss Alignment :	         
2024-02-04 18:07:04,054 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 18:07:04,055 	Text Reference  :	according to sources the leaders of  the two    countries are    set to           join the   commentary panel   as  well   
2024-02-04 18:07:04,055 	Text Hypothesis :	********* ** ******* you can     see his jovial behaviour during t20 championship is   virat kohli      through his arrival
2024-02-04 18:07:04,055 	Text Alignment  :	D         D  D       S   S       S   S   S      S         S      S   S            S    S     S          S       S   S      
2024-02-04 18:07:04,056 ========================================================================================================================
2024-02-04 18:07:04,056 Logging Sequence: 83_33.00
2024-02-04 18:07:04,056 	Gloss Reference :	A B+C+D+E
2024-02-04 18:07:04,056 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 18:07:04,056 	Gloss Alignment :	         
2024-02-04 18:07:04,056 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 18:07:04,057 	Text Reference  :	*** ******* ** a       football match lasts for *** ********** two  equal halves of   45   minutes
2024-02-04 18:07:04,058 	Text Hypothesis :	the denmark vs finland football match ***** for the tournament were going on     12th june 2023   
2024-02-04 18:07:04,058 	Text Alignment  :	I   I       I  S                      D         I   I          S    S     S      S    S    S      
2024-02-04 18:07:04,058 ========================================================================================================================
2024-02-04 18:07:04,058 Logging Sequence: 68_147.00
2024-02-04 18:07:04,058 	Gloss Reference :	A B+C+D+E
2024-02-04 18:07:04,058 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 18:07:04,058 	Gloss Alignment :	         
2024-02-04 18:07:04,059 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 18:07:04,060 	Text Reference  :	**** ** ** *********** **** **** **** **** remember the   2007 t20 world cup       amid a   lot      of sledging by english players
2024-02-04 18:07:04,060 	Text Hypothesis :	here is an interesting fact fans said that stuart   broad must be  very  irritated to   see smashing 35 runs     in just    balls  
2024-02-04 18:07:04,061 	Text Alignment  :	I    I  I  I           I    I    I    I    S        S     S    S   S     S         S    S   S        S  S        S  S       S      
2024-02-04 18:07:04,061 ========================================================================================================================
2024-02-04 18:07:04,061 Logging Sequence: 165_8.00
2024-02-04 18:07:04,061 	Gloss Reference :	A B+C+D+E
2024-02-04 18:07:04,061 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 18:07:04,061 	Gloss Alignment :	         
2024-02-04 18:07:04,061 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 18:07:04,062 	Text Reference  :	** ******** however many don't believe in     it      it varies among people
2024-02-04 18:07:04,063 	Text Hypothesis :	he believed that    his  bag   are     moving forward to the    world cup   
2024-02-04 18:07:04,063 	Text Alignment  :	I  I        S       S    S     S       S      S       S  S      S     S     
2024-02-04 18:07:04,063 ========================================================================================================================
2024-02-04 18:07:04,063 Logging Sequence: 119_71.00
2024-02-04 18:07:04,063 	Gloss Reference :	A B+C+D+E
2024-02-04 18:07:04,063 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 18:07:04,063 	Gloss Alignment :	         
2024-02-04 18:07:04,064 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 18:07:04,065 	Text Reference  :	the special gold devices have each player'    names and  jersey numbers next   to the     camera
2024-02-04 18:07:04,065 	Text Hypothesis :	*** idesign gold ******* is   a    contingent are   sold in     the     reason of idesign gold  
2024-02-04 18:07:04,065 	Text Alignment  :	D   S            D       S    S    S          S     S    S      S       S      S  S       S     
2024-02-04 18:07:04,065 ========================================================================================================================
2024-02-04 18:07:11,731 Epoch 445: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.25 
2024-02-04 18:07:11,731 EPOCH 446
2024-02-04 18:07:23,555 Epoch 446: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.22 
2024-02-04 18:07:23,556 EPOCH 447
2024-02-04 18:07:35,640 Epoch 447: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.23 
2024-02-04 18:07:35,640 EPOCH 448
2024-02-04 18:07:47,571 Epoch 448: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.20 
2024-02-04 18:07:47,572 EPOCH 449
2024-02-04 18:07:59,272 Epoch 449: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.14 
2024-02-04 18:07:59,273 EPOCH 450
2024-02-04 18:08:11,005 Epoch 450: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.22 
2024-02-04 18:08:11,005 EPOCH 451
2024-02-04 18:08:23,025 Epoch 451: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.21 
2024-02-04 18:08:23,026 EPOCH 452
2024-02-04 18:08:34,893 Epoch 452: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.20 
2024-02-04 18:08:34,893 EPOCH 453
2024-02-04 18:08:46,791 Epoch 453: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.30 
2024-02-04 18:08:46,791 EPOCH 454
2024-02-04 18:08:58,680 Epoch 454: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.26 
2024-02-04 18:08:58,680 EPOCH 455
2024-02-04 18:09:10,866 Epoch 455: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.23 
2024-02-04 18:09:10,866 EPOCH 456
2024-02-04 18:09:18,167 [Epoch: 456 Step: 00004100] Batch Recognition Loss:   0.001132 => Gls Tokens per Sec:      755 || Batch Translation Loss:   0.159947 => Txt Tokens per Sec:     2277 || Lr: 0.000100
2024-02-04 18:09:22,609 Epoch 456: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.18 
2024-02-04 18:09:22,609 EPOCH 457
2024-02-04 18:09:34,655 Epoch 457: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.12 
2024-02-04 18:09:34,655 EPOCH 458
2024-02-04 18:09:46,709 Epoch 458: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.12 
2024-02-04 18:09:46,710 EPOCH 459
2024-02-04 18:09:58,701 Epoch 459: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.09 
2024-02-04 18:09:58,702 EPOCH 460
2024-02-04 18:10:10,468 Epoch 460: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.07 
2024-02-04 18:10:10,469 EPOCH 461
2024-02-04 18:10:22,264 Epoch 461: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.09 
2024-02-04 18:10:22,264 EPOCH 462
2024-02-04 18:10:34,251 Epoch 462: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.09 
2024-02-04 18:10:34,252 EPOCH 463
2024-02-04 18:10:46,226 Epoch 463: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.00 
2024-02-04 18:10:46,226 EPOCH 464
2024-02-04 18:10:57,980 Epoch 464: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-04 18:10:57,981 EPOCH 465
2024-02-04 18:11:09,780 Epoch 465: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.03 
2024-02-04 18:11:09,781 EPOCH 466
2024-02-04 18:11:22,046 Epoch 466: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.00 
2024-02-04 18:11:22,047 EPOCH 467
2024-02-04 18:11:32,812 [Epoch: 467 Step: 00004200] Batch Recognition Loss:   0.000654 => Gls Tokens per Sec:      631 || Batch Translation Loss:   0.044260 => Txt Tokens per Sec:     1932 || Lr: 0.000100
2024-02-04 18:11:33,934 Epoch 467: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.98 
2024-02-04 18:11:33,934 EPOCH 468
2024-02-04 18:11:45,952 Epoch 468: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.04 
2024-02-04 18:11:45,952 EPOCH 469
2024-02-04 18:11:57,757 Epoch 469: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.03 
2024-02-04 18:11:57,757 EPOCH 470
2024-02-04 18:12:09,923 Epoch 470: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.94 
2024-02-04 18:12:09,923 EPOCH 471
2024-02-04 18:12:21,988 Epoch 471: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.97 
2024-02-04 18:12:21,988 EPOCH 472
2024-02-04 18:12:34,339 Epoch 472: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.98 
2024-02-04 18:12:34,339 EPOCH 473
2024-02-04 18:12:46,693 Epoch 473: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.04 
2024-02-04 18:12:46,694 EPOCH 474
2024-02-04 18:12:58,635 Epoch 474: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.02 
2024-02-04 18:12:58,635 EPOCH 475
2024-02-04 18:13:10,730 Epoch 475: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.01 
2024-02-04 18:13:10,730 EPOCH 476
2024-02-04 18:13:22,763 Epoch 476: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.95 
2024-02-04 18:13:22,763 EPOCH 477
2024-02-04 18:13:35,141 Epoch 477: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.91 
2024-02-04 18:13:35,141 EPOCH 478
2024-02-04 18:13:41,320 [Epoch: 478 Step: 00004300] Batch Recognition Loss:   0.000732 => Gls Tokens per Sec:     1450 || Batch Translation Loss:   0.100765 => Txt Tokens per Sec:     3889 || Lr: 0.000100
2024-02-04 18:13:47,191 Epoch 478: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.94 
2024-02-04 18:13:47,191 EPOCH 479
2024-02-04 18:13:59,168 Epoch 479: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.14 
2024-02-04 18:13:59,169 EPOCH 480
2024-02-04 18:14:11,152 Epoch 480: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.08 
2024-02-04 18:14:11,153 EPOCH 481
2024-02-04 18:14:23,671 Epoch 481: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.47 
2024-02-04 18:14:23,671 EPOCH 482
2024-02-04 18:14:35,375 Epoch 482: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.97 
2024-02-04 18:14:35,375 EPOCH 483
2024-02-04 18:14:47,303 Epoch 483: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.62 
2024-02-04 18:14:47,303 EPOCH 484
2024-02-04 18:14:59,066 Epoch 484: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.68 
2024-02-04 18:14:59,067 EPOCH 485
2024-02-04 18:28:54,447 Epoch 485: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.63 
2024-02-04 18:28:54,448 EPOCH 486
2024-02-04 18:29:07,465 Epoch 486: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.87 
2024-02-04 18:29:07,465 EPOCH 487
2024-02-04 18:29:19,438 Epoch 487: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.02 
2024-02-04 18:29:19,438 EPOCH 488
2024-02-04 18:29:31,127 Epoch 488: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.67 
2024-02-04 18:29:31,127 EPOCH 489
2024-02-04 18:29:42,477 [Epoch: 489 Step: 00004400] Batch Recognition Loss:   0.001563 => Gls Tokens per Sec:      824 || Batch Translation Loss:   0.277684 => Txt Tokens per Sec:     2400 || Lr: 0.000100
2024-02-04 18:29:42,832 Epoch 489: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.73 
2024-02-04 18:29:42,832 EPOCH 490
2024-02-04 18:29:55,399 Epoch 490: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.63 
2024-02-04 18:29:55,400 EPOCH 491
2024-02-04 18:30:07,147 Epoch 491: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.44 
2024-02-04 18:30:07,148 EPOCH 492
2024-02-04 18:30:19,046 Epoch 492: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.32 
2024-02-04 18:30:19,046 EPOCH 493
2024-02-04 18:30:31,698 Epoch 493: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.23 
2024-02-04 18:30:31,698 EPOCH 494
2024-02-04 18:30:43,839 Epoch 494: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.17 
2024-02-04 18:30:43,840 EPOCH 495
2024-02-04 18:30:57,028 Epoch 495: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.07 
2024-02-04 18:30:57,029 EPOCH 496
2024-02-04 18:31:09,296 Epoch 496: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.12 
2024-02-04 18:31:09,297 EPOCH 497
2024-02-04 18:31:21,217 Epoch 497: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.17 
2024-02-04 18:31:21,218 EPOCH 498
2024-02-04 18:31:33,183 Epoch 498: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.58 
2024-02-04 18:31:33,184 EPOCH 499
2024-02-04 18:31:45,100 Epoch 499: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.70 
2024-02-04 18:31:45,101 EPOCH 500
2024-02-04 18:31:56,751 [Epoch: 500 Step: 00004500] Batch Recognition Loss:   0.001030 => Gls Tokens per Sec:      912 || Batch Translation Loss:   0.195190 => Txt Tokens per Sec:     2533 || Lr: 0.000100
2024-02-04 18:31:56,752 Epoch 500: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.57 
2024-02-04 18:31:56,752 EPOCH 501
2024-02-04 18:32:08,704 Epoch 501: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.36 
2024-02-04 18:32:08,705 EPOCH 502
2024-02-04 18:32:21,413 Epoch 502: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.39 
2024-02-04 18:32:21,414 EPOCH 503
2024-02-04 18:32:34,253 Epoch 503: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.32 
2024-02-04 18:32:34,254 EPOCH 504
2024-02-04 18:32:46,276 Epoch 504: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.36 
2024-02-04 18:32:46,277 EPOCH 505
2024-02-04 18:32:58,094 Epoch 505: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.41 
2024-02-04 18:32:58,094 EPOCH 506
2024-02-04 18:33:09,932 Epoch 506: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.31 
2024-02-04 18:33:09,932 EPOCH 507
2024-02-04 18:33:30,000 Epoch 507: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.11 
2024-02-04 18:33:30,001 EPOCH 508
2024-02-04 18:33:42,459 Epoch 508: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.09 
2024-02-04 18:33:42,460 EPOCH 509
2024-02-04 18:33:54,841 Epoch 509: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.05 
2024-02-04 18:33:54,841 EPOCH 510
2024-02-04 18:34:08,279 Epoch 510: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.09 
2024-02-04 18:34:08,279 EPOCH 511
2024-02-04 18:34:21,356 Epoch 511: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.97 
2024-02-04 18:34:21,356 EPOCH 512
2024-02-04 18:34:21,882 [Epoch: 512 Step: 00004600] Batch Recognition Loss:   0.000911 => Gls Tokens per Sec:     2441 || Batch Translation Loss:   0.124135 => Txt Tokens per Sec:     7181 || Lr: 0.000100
2024-02-04 18:34:34,074 Epoch 512: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.97 
2024-02-04 18:34:34,074 EPOCH 513
2024-02-04 18:34:46,769 Epoch 513: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.97 
2024-02-04 18:34:46,769 EPOCH 514
2024-02-04 18:34:59,498 Epoch 514: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.94 
2024-02-04 18:34:59,499 EPOCH 515
2024-02-04 18:35:11,758 Epoch 515: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.94 
2024-02-04 18:35:11,759 EPOCH 516
2024-02-04 18:35:24,055 Epoch 516: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.01 
2024-02-04 18:35:24,056 EPOCH 517
2024-02-04 18:35:35,930 Epoch 517: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.04 
2024-02-04 18:35:35,931 EPOCH 518
2024-02-04 18:35:49,055 Epoch 518: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.05 
2024-02-04 18:35:49,056 EPOCH 519
2024-02-04 18:36:01,673 Epoch 519: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.04 
2024-02-04 18:36:01,673 EPOCH 520
2024-02-04 18:36:14,038 Epoch 520: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.08 
2024-02-04 18:36:14,039 EPOCH 521
2024-02-04 18:36:26,399 Epoch 521: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-04 18:36:26,400 EPOCH 522
2024-02-04 18:36:45,649 Epoch 522: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.01 
2024-02-04 18:36:45,649 EPOCH 523
2024-02-04 18:36:46,730 [Epoch: 523 Step: 00004700] Batch Recognition Loss:   0.000641 => Gls Tokens per Sec:     2371 || Batch Translation Loss:   0.085427 => Txt Tokens per Sec:     6343 || Lr: 0.000100
2024-02-04 18:36:57,885 Epoch 523: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.01 
2024-02-04 18:36:57,886 EPOCH 524
2024-02-04 18:37:10,787 Epoch 524: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.90 
2024-02-04 18:37:10,788 EPOCH 525
2024-02-04 18:37:23,131 Epoch 525: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.02 
2024-02-04 18:37:23,131 EPOCH 526
2024-02-04 18:37:35,681 Epoch 526: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.07 
2024-02-04 18:37:35,682 EPOCH 527
2024-02-04 18:37:47,682 Epoch 527: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.34 
2024-02-04 18:37:47,683 EPOCH 528
2024-02-04 18:38:08,736 Epoch 528: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.19 
2024-02-04 18:38:08,736 EPOCH 529
2024-02-04 18:38:21,329 Epoch 529: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.20 
2024-02-04 18:38:21,330 EPOCH 530
2024-02-04 18:38:33,319 Epoch 530: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.02 
2024-02-04 18:38:33,320 EPOCH 531
2024-02-04 18:38:45,169 Epoch 531: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.96 
2024-02-04 18:38:45,170 EPOCH 532
2024-02-04 18:38:57,487 Epoch 532: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.90 
2024-02-04 18:38:57,488 EPOCH 533
2024-02-04 18:39:09,711 Epoch 533: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.95 
2024-02-04 18:39:09,712 EPOCH 534
2024-02-04 18:39:13,865 [Epoch: 534 Step: 00004800] Batch Recognition Loss:   0.001117 => Gls Tokens per Sec:      925 || Batch Translation Loss:   0.057458 => Txt Tokens per Sec:     2397 || Lr: 0.000100
2024-02-04 18:39:21,588 Epoch 534: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.90 
2024-02-04 18:39:21,588 EPOCH 535
2024-02-04 18:39:35,665 Epoch 535: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.01 
2024-02-04 18:39:35,666 EPOCH 536
2024-02-04 18:39:48,180 Epoch 536: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.13 
2024-02-04 18:39:48,181 EPOCH 537
2024-02-04 18:40:00,562 Epoch 537: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.05 
2024-02-04 18:40:00,563 EPOCH 538
2024-02-04 18:40:12,686 Epoch 538: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.05 
2024-02-04 18:40:12,686 EPOCH 539
2024-02-04 18:40:25,059 Epoch 539: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-04 18:40:25,059 EPOCH 540
2024-02-04 18:40:37,501 Epoch 540: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.01 
2024-02-04 18:40:37,502 EPOCH 541
2024-02-04 18:40:53,123 Epoch 541: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.93 
2024-02-04 18:40:53,124 EPOCH 542
2024-02-04 18:41:06,506 Epoch 542: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.93 
2024-02-04 18:41:06,507 EPOCH 543
2024-02-04 18:41:19,567 Epoch 543: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.91 
2024-02-04 18:41:19,567 EPOCH 544
2024-02-04 18:41:32,180 Epoch 544: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.82 
2024-02-04 18:41:32,181 EPOCH 545
2024-02-04 18:41:39,935 [Epoch: 545 Step: 00004900] Batch Recognition Loss:   0.000899 => Gls Tokens per Sec:      660 || Batch Translation Loss:   0.118137 => Txt Tokens per Sec:     2055 || Lr: 0.000100
2024-02-04 18:41:46,511 Epoch 545: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.19 
2024-02-04 18:41:46,511 EPOCH 546
2024-02-04 18:41:59,888 Epoch 546: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.55 
2024-02-04 18:41:59,889 EPOCH 547
2024-02-04 18:42:14,532 Epoch 547: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.46 
2024-02-04 18:42:14,533 EPOCH 548
2024-02-04 18:42:28,158 Epoch 548: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.37 
2024-02-04 18:42:28,158 EPOCH 549
2024-02-04 18:42:41,454 Epoch 549: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.14 
2024-02-04 18:42:41,455 EPOCH 550
2024-02-04 18:42:55,528 Epoch 550: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.20 
2024-02-04 18:42:55,529 EPOCH 551
2024-02-04 18:43:09,282 Epoch 551: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.14 
2024-02-04 18:43:09,283 EPOCH 552
2024-02-04 18:43:23,001 Epoch 552: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.01 
2024-02-04 18:43:23,001 EPOCH 553
2024-02-04 18:43:38,756 Epoch 553: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.14 
2024-02-04 18:43:38,757 EPOCH 554
2024-02-04 18:43:53,206 Epoch 554: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.26 
2024-02-04 18:43:53,207 EPOCH 555
2024-02-04 18:44:07,237 Epoch 555: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.71 
2024-02-04 18:44:07,237 EPOCH 556
2024-02-04 18:44:14,826 [Epoch: 556 Step: 00005000] Batch Recognition Loss:   0.000849 => Gls Tokens per Sec:      726 || Batch Translation Loss:   0.211420 => Txt Tokens per Sec:     2088 || Lr: 0.000100
2024-02-04 18:44:21,308 Epoch 556: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.72 
2024-02-04 18:44:21,308 EPOCH 557
2024-02-04 18:44:35,482 Epoch 557: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.27 
2024-02-04 18:44:35,482 EPOCH 558
2024-02-04 18:44:49,616 Epoch 558: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.44 
2024-02-04 18:44:49,617 EPOCH 559
2024-02-04 18:45:03,774 Epoch 559: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.13 
2024-02-04 18:45:03,775 EPOCH 560
2024-02-04 18:45:17,928 Epoch 560: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.60 
2024-02-04 18:45:17,929 EPOCH 561
2024-02-04 18:45:32,013 Epoch 561: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.01 
2024-02-04 18:45:32,013 EPOCH 562
2024-02-04 18:45:46,266 Epoch 562: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.81 
2024-02-04 18:45:46,267 EPOCH 563
2024-02-04 18:46:00,222 Epoch 563: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.85 
2024-02-04 18:46:00,223 EPOCH 564
2024-02-04 18:46:14,291 Epoch 564: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.08 
2024-02-04 18:46:14,292 EPOCH 565
2024-02-04 18:46:28,336 Epoch 565: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.77 
2024-02-04 18:46:28,337 EPOCH 566
2024-02-04 18:46:42,271 Epoch 566: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.40 
2024-02-04 18:46:42,272 EPOCH 567
2024-02-04 18:46:55,303 [Epoch: 567 Step: 00005100] Batch Recognition Loss:   0.001099 => Gls Tokens per Sec:      521 || Batch Translation Loss:   0.159977 => Txt Tokens per Sec:     1577 || Lr: 0.000100
2024-02-04 18:46:56,465 Epoch 567: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.32 
2024-02-04 18:46:56,466 EPOCH 568
2024-02-04 18:47:10,544 Epoch 568: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.23 
2024-02-04 18:47:10,544 EPOCH 569
2024-02-04 18:47:24,658 Epoch 569: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.16 
2024-02-04 18:47:24,659 EPOCH 570
2024-02-04 18:47:38,757 Epoch 570: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.00 
2024-02-04 18:47:38,757 EPOCH 571
2024-02-04 18:47:52,595 Epoch 571: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.92 
2024-02-04 18:47:52,596 EPOCH 572
2024-02-04 18:48:06,511 Epoch 572: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.84 
2024-02-04 18:48:06,512 EPOCH 573
2024-02-04 18:48:20,755 Epoch 573: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.76 
2024-02-04 18:48:20,755 EPOCH 574
2024-02-04 18:48:34,873 Epoch 574: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.73 
2024-02-04 18:48:34,873 EPOCH 575
2024-02-04 18:48:48,947 Epoch 575: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-04 18:48:48,948 EPOCH 576
2024-02-04 18:49:03,224 Epoch 576: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-04 18:49:03,225 EPOCH 577
2024-02-04 18:49:16,861 Epoch 577: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-04 18:49:16,862 EPOCH 578
2024-02-04 18:49:29,302 [Epoch: 578 Step: 00005200] Batch Recognition Loss:   0.000837 => Gls Tokens per Sec:      649 || Batch Translation Loss:   0.090200 => Txt Tokens per Sec:     1760 || Lr: 0.000100
2024-02-04 18:49:30,752 Epoch 578: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-04 18:49:30,752 EPOCH 579
2024-02-04 18:49:44,648 Epoch 579: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-04 18:49:44,648 EPOCH 580
2024-02-04 18:49:58,812 Epoch 580: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-04 18:49:58,812 EPOCH 581
2024-02-04 18:50:12,704 Epoch 581: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-04 18:50:12,704 EPOCH 582
2024-02-04 18:50:26,797 Epoch 582: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-04 18:50:26,798 EPOCH 583
2024-02-04 18:50:40,501 Epoch 583: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-04 18:50:40,501 EPOCH 584
2024-02-04 18:50:54,517 Epoch 584: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-04 18:50:54,517 EPOCH 585
2024-02-04 18:51:08,477 Epoch 585: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-04 18:51:08,478 EPOCH 586
2024-02-04 18:51:23,038 Epoch 586: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.60 
2024-02-04 18:51:23,039 EPOCH 587
2024-02-04 18:51:37,324 Epoch 587: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-04 18:51:37,324 EPOCH 588
2024-02-04 18:51:51,530 Epoch 588: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.72 
2024-02-04 18:51:51,531 EPOCH 589
2024-02-04 18:52:05,048 [Epoch: 589 Step: 00005300] Batch Recognition Loss:   0.000607 => Gls Tokens per Sec:      692 || Batch Translation Loss:   0.093012 => Txt Tokens per Sec:     2014 || Lr: 0.000100
2024-02-04 18:52:05,291 Epoch 589: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-04 18:52:05,291 EPOCH 590
2024-02-04 18:52:19,439 Epoch 590: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.72 
2024-02-04 18:52:19,439 EPOCH 591
2024-02-04 18:52:33,557 Epoch 591: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.67 
2024-02-04 18:52:33,557 EPOCH 592
2024-02-04 18:52:47,811 Epoch 592: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-04 18:52:47,812 EPOCH 593
2024-02-04 18:53:01,745 Epoch 593: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-04 18:53:01,745 EPOCH 594
2024-02-04 18:53:16,069 Epoch 594: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.61 
2024-02-04 18:53:16,070 EPOCH 595
2024-02-04 18:53:30,663 Epoch 595: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.55 
2024-02-04 18:53:30,664 EPOCH 596
2024-02-04 18:53:45,083 Epoch 596: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.55 
2024-02-04 18:53:45,083 EPOCH 597
2024-02-04 18:53:59,211 Epoch 597: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-04 18:53:59,212 EPOCH 598
2024-02-04 18:54:13,119 Epoch 598: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-04 18:54:13,120 EPOCH 599
2024-02-04 18:54:27,166 Epoch 599: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-04 18:54:27,167 EPOCH 600
2024-02-04 18:54:40,887 [Epoch: 600 Step: 00005400] Batch Recognition Loss:   0.000377 => Gls Tokens per Sec:      775 || Batch Translation Loss:   0.059902 => Txt Tokens per Sec:     2151 || Lr: 0.000100
2024-02-04 18:54:40,887 Epoch 600: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-04 18:54:40,888 EPOCH 601
2024-02-04 18:54:54,916 Epoch 601: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-04 18:54:54,917 EPOCH 602
2024-02-04 18:55:08,760 Epoch 602: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-04 18:55:08,760 EPOCH 603
2024-02-04 18:55:22,552 Epoch 603: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-04 18:55:22,552 EPOCH 604
2024-02-04 18:55:36,209 Epoch 604: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-04 18:55:36,210 EPOCH 605
2024-02-04 18:55:50,635 Epoch 605: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-04 18:55:50,636 EPOCH 606
2024-02-04 18:56:04,495 Epoch 606: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-04 18:56:04,496 EPOCH 607
2024-02-04 18:56:18,793 Epoch 607: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.55 
2024-02-04 18:56:18,794 EPOCH 608
2024-02-04 18:56:32,717 Epoch 608: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.56 
2024-02-04 18:56:32,718 EPOCH 609
2024-02-04 18:56:46,656 Epoch 609: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.58 
2024-02-04 18:56:46,657 EPOCH 610
2024-02-04 18:57:01,040 Epoch 610: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.55 
2024-02-04 18:57:01,040 EPOCH 611
2024-02-04 18:57:15,032 Epoch 611: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-04 18:57:15,032 EPOCH 612
2024-02-04 18:57:16,894 [Epoch: 612 Step: 00005500] Batch Recognition Loss:   0.000496 => Gls Tokens per Sec:      688 || Batch Translation Loss:   0.072998 => Txt Tokens per Sec:     2188 || Lr: 0.000100
2024-02-04 18:57:29,573 Epoch 612: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.56 
2024-02-04 18:57:29,573 EPOCH 613
2024-02-04 18:57:43,628 Epoch 613: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.61 
2024-02-04 18:57:43,629 EPOCH 614
2024-02-04 18:57:58,176 Epoch 614: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.67 
2024-02-04 18:57:58,176 EPOCH 615
2024-02-04 18:58:12,218 Epoch 615: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.78 
2024-02-04 18:58:12,218 EPOCH 616
2024-02-04 18:58:25,743 Epoch 616: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.79 
2024-02-04 18:58:25,744 EPOCH 617
2024-02-04 18:58:40,019 Epoch 617: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.00 
2024-02-04 18:58:40,019 EPOCH 618
2024-02-04 18:58:54,215 Epoch 618: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.13 
2024-02-04 18:58:54,216 EPOCH 619
2024-02-04 18:59:08,572 Epoch 619: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.00 
2024-02-04 18:59:08,573 EPOCH 620
2024-02-04 18:59:22,909 Epoch 620: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.42 
2024-02-04 18:59:22,909 EPOCH 621
2024-02-04 18:59:37,283 Epoch 621: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.34 
2024-02-04 18:59:37,283 EPOCH 622
2024-02-04 18:59:51,971 Epoch 622: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.23 
2024-02-04 18:59:51,972 EPOCH 623
2024-02-04 18:59:54,488 [Epoch: 623 Step: 00005600] Batch Recognition Loss:   0.000934 => Gls Tokens per Sec:     1018 || Batch Translation Loss:   0.319052 => Txt Tokens per Sec:     3112 || Lr: 0.000100
2024-02-04 19:00:06,172 Epoch 623: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.89 
2024-02-04 19:00:06,173 EPOCH 624
2024-02-04 19:00:20,580 Epoch 624: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.19 
2024-02-04 19:00:20,580 EPOCH 625
2024-02-04 19:00:35,498 Epoch 625: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.45 
2024-02-04 19:00:35,499 EPOCH 626
2024-02-04 19:00:49,684 Epoch 626: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.21 
2024-02-04 19:00:49,684 EPOCH 627
2024-02-04 19:01:04,357 Epoch 627: Total Training Recognition Loss 0.02  Total Training Translation Loss 8.90 
2024-02-04 19:01:04,358 EPOCH 628
2024-02-04 19:01:18,337 Epoch 628: Total Training Recognition Loss 0.06  Total Training Translation Loss 16.08 
2024-02-04 19:01:18,338 EPOCH 629
2024-02-04 19:01:32,606 Epoch 629: Total Training Recognition Loss 0.16  Total Training Translation Loss 14.60 
2024-02-04 19:01:32,606 EPOCH 630
2024-02-04 19:01:46,827 Epoch 630: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.12 
2024-02-04 19:01:46,827 EPOCH 631
2024-02-04 19:02:00,613 Epoch 631: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.85 
2024-02-04 19:02:00,614 EPOCH 632
2024-02-04 19:02:15,070 Epoch 632: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.64 
2024-02-04 19:02:15,071 EPOCH 633
2024-02-04 19:02:29,625 Epoch 633: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.78 
2024-02-04 19:02:29,627 EPOCH 634
2024-02-04 19:02:35,841 [Epoch: 634 Step: 00005700] Batch Recognition Loss:   0.002260 => Gls Tokens per Sec:      618 || Batch Translation Loss:   0.229834 => Txt Tokens per Sec:     1796 || Lr: 0.000100
2024-02-04 19:02:43,835 Epoch 634: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.35 
2024-02-04 19:02:43,835 EPOCH 635
2024-02-04 19:02:58,294 Epoch 635: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.20 
2024-02-04 19:02:58,295 EPOCH 636
2024-02-04 19:03:12,360 Epoch 636: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.99 
2024-02-04 19:03:12,361 EPOCH 637
2024-02-04 19:03:26,349 Epoch 637: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.85 
2024-02-04 19:03:26,350 EPOCH 638
2024-02-04 19:03:40,670 Epoch 638: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.76 
2024-02-04 19:03:40,671 EPOCH 639
2024-02-04 19:03:54,862 Epoch 639: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-04 19:03:54,862 EPOCH 640
2024-02-04 19:04:08,819 Epoch 640: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-04 19:04:08,820 EPOCH 641
2024-02-04 19:04:23,107 Epoch 641: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-04 19:04:23,107 EPOCH 642
2024-02-04 19:04:37,291 Epoch 642: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-04 19:04:37,291 EPOCH 643
2024-02-04 19:04:51,571 Epoch 643: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-04 19:04:51,572 EPOCH 644
2024-02-04 19:05:05,568 Epoch 644: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-04 19:05:05,569 EPOCH 645
2024-02-04 19:05:11,474 [Epoch: 645 Step: 00005800] Batch Recognition Loss:   0.000588 => Gls Tokens per Sec:      716 || Batch Translation Loss:   0.043185 => Txt Tokens per Sec:     1915 || Lr: 0.000100
2024-02-04 19:05:19,952 Epoch 645: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-04 19:05:19,952 EPOCH 646
2024-02-04 19:05:33,953 Epoch 646: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-04 19:05:33,954 EPOCH 647
2024-02-04 19:05:48,680 Epoch 647: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-04 19:05:48,681 EPOCH 648
2024-02-04 19:06:02,917 Epoch 648: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-04 19:06:02,918 EPOCH 649
2024-02-04 19:06:17,108 Epoch 649: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-04 19:06:17,108 EPOCH 650
2024-02-04 19:06:31,028 Epoch 650: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-04 19:06:31,029 EPOCH 651
2024-02-04 19:06:44,850 Epoch 651: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-04 19:06:44,851 EPOCH 652
2024-02-04 19:06:58,612 Epoch 652: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-04 19:06:58,612 EPOCH 653
2024-02-04 19:07:12,574 Epoch 653: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-04 19:07:12,574 EPOCH 654
2024-02-04 19:07:26,592 Epoch 654: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-04 19:07:26,592 EPOCH 655
2024-02-04 19:07:40,582 Epoch 655: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-04 19:07:40,583 EPOCH 656
2024-02-04 19:07:51,226 [Epoch: 656 Step: 00005900] Batch Recognition Loss:   0.000464 => Gls Tokens per Sec:      518 || Batch Translation Loss:   0.037129 => Txt Tokens per Sec:     1404 || Lr: 0.000100
2024-02-04 19:07:54,345 Epoch 656: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-04 19:07:54,345 EPOCH 657
2024-02-04 19:08:08,226 Epoch 657: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-04 19:08:08,227 EPOCH 658
2024-02-04 19:08:22,444 Epoch 658: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-04 19:08:22,444 EPOCH 659
2024-02-04 19:08:36,405 Epoch 659: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-04 19:08:36,406 EPOCH 660
2024-02-04 19:08:50,538 Epoch 660: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-04 19:08:50,538 EPOCH 661
2024-02-04 19:09:04,177 Epoch 661: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-04 19:09:04,177 EPOCH 662
2024-02-04 19:09:18,116 Epoch 662: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-04 19:09:18,117 EPOCH 663
2024-02-04 19:09:31,969 Epoch 663: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-04 19:09:31,970 EPOCH 664
2024-02-04 19:09:45,950 Epoch 664: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-04 19:09:45,951 EPOCH 665
2024-02-04 19:10:00,006 Epoch 665: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-04 19:10:00,006 EPOCH 666
2024-02-04 19:10:14,241 Epoch 666: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-04 19:10:14,242 EPOCH 667
2024-02-04 19:10:25,393 [Epoch: 667 Step: 00006000] Batch Recognition Loss:   0.000423 => Gls Tokens per Sec:      609 || Batch Translation Loss:   0.040052 => Txt Tokens per Sec:     1687 || Lr: 0.000100
2024-02-04 19:10:56,749 Validation result at epoch 667, step     6000: duration: 31.3547s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00025	Translation Loss: 89822.82812	PPL: 8011.03271
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 1.12	(BLEU-1: 12.39,	BLEU-2: 4.49,	BLEU-3: 2.04,	BLEU-4: 1.12)
	CHRF 17.88	ROUGE 10.70
2024-02-04 19:10:56,751 Logging Recognition and Translation Outputs
2024-02-04 19:10:56,752 ========================================================================================================================
2024-02-04 19:10:56,752 Logging Sequence: 89_111.00
2024-02-04 19:10:56,753 	Gloss Reference :	A B+C+D+E
2024-02-04 19:10:56,753 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 19:10:56,753 	Gloss Alignment :	         
2024-02-04 19:10:56,753 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 19:10:56,754 	Text Reference  :	*** ******* ******** * ******** ** ******** however selectors never selected me   for the   team
2024-02-04 19:10:56,754 	Text Hypothesis :	the spinner received a plethora of messages and     posts     from  all      over a   short over
2024-02-04 19:10:56,754 	Text Alignment  :	I   I       I        I I        I  I        S       S         S     S        S    S   S     S   
2024-02-04 19:10:56,755 ========================================================================================================================
2024-02-04 19:10:56,755 Logging Sequence: 137_23.00
2024-02-04 19:10:56,755 	Gloss Reference :	A B+C+D+E
2024-02-04 19:10:56,755 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 19:10:56,755 	Gloss Alignment :	         
2024-02-04 19:10:56,756 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 19:10:56,758 	Text Reference  :	fan from around the world are  in     qatar for the fifa world cup 
2024-02-04 19:10:56,758 	Text Hypothesis :	*** **** ****** the fans  were ranked based on  the **** ***** loss
2024-02-04 19:10:56,758 	Text Alignment  :	D   D    D          S     S    S      S     S       D    D     S   
2024-02-04 19:10:56,758 ========================================================================================================================
2024-02-04 19:10:56,758 Logging Sequence: 128_145.00
2024-02-04 19:10:56,758 	Gloss Reference :	A B+C+D+E
2024-02-04 19:10:56,759 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 19:10:56,759 	Gloss Alignment :	         
2024-02-04 19:10:56,759 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 19:10:56,759 	Text Reference  :	icc also uploaded a video of   the  same 
2024-02-04 19:10:56,760 	Text Hypothesis :	*** **** ******** * the   news went viral
2024-02-04 19:10:56,760 	Text Alignment  :	D   D    D        D S     S    S    S    
2024-02-04 19:10:56,760 ========================================================================================================================
2024-02-04 19:10:56,760 Logging Sequence: 165_192.00
2024-02-04 19:10:56,760 	Gloss Reference :	A B+C+D+E
2024-02-04 19:10:56,761 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 19:10:56,761 	Gloss Alignment :	         
2024-02-04 19:10:56,761 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 19:10:56,762 	Text Reference  :	** 3   ravichandran ashwin believes that his bag is      lucky
2024-02-04 19:10:56,763 	Text Hypothesis :	he won all          out    and      food at  the women's team 
2024-02-04 19:10:56,763 	Text Alignment  :	I  S   S            S      S        S    S   S   S       S    
2024-02-04 19:10:56,763 ========================================================================================================================
2024-02-04 19:10:56,763 Logging Sequence: 180_494.00
2024-02-04 19:10:56,763 	Gloss Reference :	A B+C+D+E
2024-02-04 19:10:56,764 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 19:10:56,764 	Gloss Alignment :	         
2024-02-04 19:10:56,764 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 19:10:56,766 	Text Reference  :	the     women wrestlers spoke angrily against the police and      the controversy in front  of  the media    
2024-02-04 19:10:56,766 	Text Hypothesis :	however an    wrestlers said  it      wasn't  a   minor  argument the *********** ** police hit the wrestlers
2024-02-04 19:10:56,766 	Text Alignment  :	S       S               S     S       S       S   S      S            D           D  S      S       S        
2024-02-04 19:10:56,766 ========================================================================================================================
2024-02-04 19:11:00,155 Epoch 667: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-04 19:11:00,156 EPOCH 668
2024-02-04 19:11:16,756 Epoch 668: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-04 19:11:16,757 EPOCH 669
2024-02-04 19:11:30,966 Epoch 669: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-04 19:11:30,966 EPOCH 670
2024-02-04 19:11:44,860 Epoch 670: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-04 19:11:44,860 EPOCH 671
2024-02-04 19:11:59,084 Epoch 671: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-04 19:11:59,084 EPOCH 672
2024-02-04 19:12:13,142 Epoch 672: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-04 19:12:13,143 EPOCH 673
2024-02-04 19:12:27,396 Epoch 673: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-04 19:12:27,397 EPOCH 674
2024-02-04 19:12:41,472 Epoch 674: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-04 19:12:41,472 EPOCH 675
2024-02-04 19:12:55,563 Epoch 675: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-04 19:12:55,564 EPOCH 676
2024-02-04 19:13:09,561 Epoch 676: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-04 19:13:09,561 EPOCH 677
2024-02-04 19:13:23,771 Epoch 677: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-04 19:13:23,772 EPOCH 678
2024-02-04 19:13:32,370 [Epoch: 678 Step: 00006100] Batch Recognition Loss:   0.000367 => Gls Tokens per Sec:      939 || Batch Translation Loss:   0.031375 => Txt Tokens per Sec:     2645 || Lr: 0.000100
2024-02-04 19:13:38,011 Epoch 678: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-04 19:13:38,011 EPOCH 679
2024-02-04 19:13:52,253 Epoch 679: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-04 19:13:52,254 EPOCH 680
2024-02-04 19:14:06,452 Epoch 680: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-04 19:14:06,452 EPOCH 681
2024-02-04 19:14:20,405 Epoch 681: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-04 19:14:20,406 EPOCH 682
2024-02-04 19:14:34,681 Epoch 682: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-04 19:14:34,682 EPOCH 683
2024-02-04 19:14:48,667 Epoch 683: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-04 19:14:48,668 EPOCH 684
2024-02-04 19:15:02,696 Epoch 684: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-04 19:15:02,697 EPOCH 685
2024-02-04 19:15:17,108 Epoch 685: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-04 19:15:17,108 EPOCH 686
2024-02-04 19:15:30,920 Epoch 686: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-04 19:15:30,920 EPOCH 687
2024-02-04 19:15:45,226 Epoch 687: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-04 19:15:45,226 EPOCH 688
2024-02-04 19:15:59,315 Epoch 688: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-04 19:15:59,316 EPOCH 689
2024-02-04 19:16:12,955 [Epoch: 689 Step: 00006200] Batch Recognition Loss:   0.000331 => Gls Tokens per Sec:      685 || Batch Translation Loss:   0.035923 => Txt Tokens per Sec:     1887 || Lr: 0.000100
2024-02-04 19:16:13,521 Epoch 689: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-04 19:16:13,521 EPOCH 690
2024-02-04 19:16:27,435 Epoch 690: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-04 19:16:27,435 EPOCH 691
2024-02-04 19:16:41,352 Epoch 691: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-04 19:16:41,353 EPOCH 692
2024-02-04 19:16:55,377 Epoch 692: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-04 19:16:55,378 EPOCH 693
2024-02-04 19:17:09,657 Epoch 693: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-04 19:17:09,658 EPOCH 694
2024-02-04 19:17:23,707 Epoch 694: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-04 19:17:23,707 EPOCH 695
2024-02-04 19:17:38,099 Epoch 695: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-04 19:17:38,099 EPOCH 696
2024-02-04 19:17:52,169 Epoch 696: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-04 19:17:52,170 EPOCH 697
2024-02-04 19:18:06,429 Epoch 697: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-04 19:18:06,430 EPOCH 698
2024-02-04 19:18:20,396 Epoch 698: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-04 19:18:20,396 EPOCH 699
2024-02-04 19:18:34,181 Epoch 699: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-04 19:18:34,182 EPOCH 700
2024-02-04 19:18:48,147 [Epoch: 700 Step: 00006300] Batch Recognition Loss:   0.000307 => Gls Tokens per Sec:      761 || Batch Translation Loss:   0.102774 => Txt Tokens per Sec:     2113 || Lr: 0.000100
2024-02-04 19:18:48,148 Epoch 700: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-04 19:18:48,148 EPOCH 701
2024-02-04 19:19:02,104 Epoch 701: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-04 19:19:02,105 EPOCH 702
2024-02-04 19:19:15,952 Epoch 702: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-04 19:19:15,953 EPOCH 703
2024-02-04 19:19:30,072 Epoch 703: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.64 
2024-02-04 19:19:30,072 EPOCH 704
2024-02-04 19:19:44,082 Epoch 704: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.74 
2024-02-04 19:19:44,083 EPOCH 705
2024-02-04 19:19:58,352 Epoch 705: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.64 
2024-02-04 19:19:58,353 EPOCH 706
2024-02-04 19:20:12,408 Epoch 706: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.66 
2024-02-04 19:20:12,408 EPOCH 707
2024-02-04 19:20:26,345 Epoch 707: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.80 
2024-02-04 19:20:26,345 EPOCH 708
2024-02-04 19:20:40,266 Epoch 708: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.78 
2024-02-04 19:20:40,267 EPOCH 709
2024-02-04 19:20:54,295 Epoch 709: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.69 
2024-02-04 19:20:54,296 EPOCH 710
2024-02-04 19:21:08,432 Epoch 710: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.60 
2024-02-04 19:21:08,433 EPOCH 711
2024-02-04 19:21:22,814 Epoch 711: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-04 19:21:22,815 EPOCH 712
2024-02-04 19:21:23,411 [Epoch: 712 Step: 00006400] Batch Recognition Loss:   0.000384 => Gls Tokens per Sec:     2154 || Batch Translation Loss:   0.059867 => Txt Tokens per Sec:     6317 || Lr: 0.000100
2024-02-04 19:21:36,785 Epoch 712: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-04 19:21:36,785 EPOCH 713
2024-02-04 19:21:51,152 Epoch 713: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-04 19:21:51,153 EPOCH 714
2024-02-04 19:22:05,088 Epoch 714: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-04 19:22:05,089 EPOCH 715
2024-02-04 19:22:18,998 Epoch 715: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-04 19:22:18,999 EPOCH 716
2024-02-04 19:22:32,882 Epoch 716: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-04 19:22:32,883 EPOCH 717
2024-02-04 19:22:46,986 Epoch 717: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-04 19:22:46,987 EPOCH 718
2024-02-04 19:23:00,902 Epoch 718: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-04 19:23:00,903 EPOCH 719
2024-02-04 19:23:15,151 Epoch 719: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-04 19:23:15,152 EPOCH 720
2024-02-04 19:23:29,247 Epoch 720: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-04 19:23:29,248 EPOCH 721
2024-02-04 19:23:43,299 Epoch 721: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-04 19:23:43,300 EPOCH 722
2024-02-04 19:23:57,147 Epoch 722: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-04 19:23:57,147 EPOCH 723
2024-02-04 19:24:03,902 [Epoch: 723 Step: 00006500] Batch Recognition Loss:   0.000330 => Gls Tokens per Sec:      379 || Batch Translation Loss:   0.047989 => Txt Tokens per Sec:     1269 || Lr: 0.000100
2024-02-04 19:24:11,033 Epoch 723: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-04 19:24:11,034 EPOCH 724
2024-02-04 19:24:24,564 Epoch 724: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-04 19:24:24,565 EPOCH 725
2024-02-04 19:24:38,588 Epoch 725: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.58 
2024-02-04 19:24:38,589 EPOCH 726
2024-02-04 19:24:52,768 Epoch 726: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-04 19:24:52,769 EPOCH 727
2024-02-04 19:25:06,472 Epoch 727: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-04 19:25:06,473 EPOCH 728
2024-02-04 19:25:20,532 Epoch 728: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-04 19:25:20,533 EPOCH 729
2024-02-04 19:25:34,464 Epoch 729: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.86 
2024-02-04 19:25:34,464 EPOCH 730
2024-02-04 19:25:48,371 Epoch 730: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.20 
2024-02-04 19:25:48,372 EPOCH 731
2024-02-04 19:26:02,366 Epoch 731: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.07 
2024-02-04 19:26:02,366 EPOCH 732
2024-02-04 19:26:16,238 Epoch 732: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.11 
2024-02-04 19:26:16,238 EPOCH 733
2024-02-04 19:26:31,567 Epoch 733: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.07 
2024-02-04 19:26:31,568 EPOCH 734
2024-02-04 19:26:37,352 [Epoch: 734 Step: 00006600] Batch Recognition Loss:   0.001279 => Gls Tokens per Sec:      510 || Batch Translation Loss:   0.633219 => Txt Tokens per Sec:     1440 || Lr: 0.000100
2024-02-04 19:26:46,360 Epoch 734: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.66 
2024-02-04 19:26:46,360 EPOCH 735
2024-02-04 19:27:00,068 Epoch 735: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.49 
2024-02-04 19:27:00,069 EPOCH 736
2024-02-04 19:27:14,180 Epoch 736: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.55 
2024-02-04 19:27:14,180 EPOCH 737
2024-02-04 19:27:28,322 Epoch 737: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.14 
2024-02-04 19:27:28,322 EPOCH 738
2024-02-04 19:27:42,395 Epoch 738: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.97 
2024-02-04 19:27:42,395 EPOCH 739
2024-02-04 19:27:56,378 Epoch 739: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-04 19:27:56,378 EPOCH 740
2024-02-04 19:28:10,402 Epoch 740: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-04 19:28:10,403 EPOCH 741
2024-02-04 19:28:24,479 Epoch 741: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-04 19:28:24,480 EPOCH 742
2024-02-04 19:28:38,389 Epoch 742: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.56 
2024-02-04 19:28:38,389 EPOCH 743
2024-02-04 19:28:52,286 Epoch 743: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-04 19:28:52,287 EPOCH 744
2024-02-04 19:29:06,230 Epoch 744: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-04 19:29:06,231 EPOCH 745
2024-02-04 19:29:12,139 [Epoch: 745 Step: 00006700] Batch Recognition Loss:   0.000446 => Gls Tokens per Sec:      716 || Batch Translation Loss:   0.046480 => Txt Tokens per Sec:     1965 || Lr: 0.000100
2024-02-04 19:29:20,480 Epoch 745: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-04 19:29:20,481 EPOCH 746
2024-02-04 19:29:34,132 Epoch 746: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.48 
2024-02-04 19:29:34,133 EPOCH 747
2024-02-04 19:29:48,516 Epoch 747: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.48 
2024-02-04 19:29:48,516 EPOCH 748
2024-02-04 19:30:02,440 Epoch 748: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-04 19:30:02,441 EPOCH 749
2024-02-04 19:30:16,303 Epoch 749: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.48 
2024-02-04 19:30:16,304 EPOCH 750
2024-02-04 19:30:30,337 Epoch 750: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-04 19:30:30,338 EPOCH 751
2024-02-04 19:30:44,301 Epoch 751: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-04 19:30:44,302 EPOCH 752
2024-02-04 19:30:58,393 Epoch 752: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-04 19:30:58,394 EPOCH 753
2024-02-04 19:31:12,458 Epoch 753: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-04 19:31:12,459 EPOCH 754
2024-02-04 19:31:26,616 Epoch 754: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-04 19:31:26,617 EPOCH 755
2024-02-04 19:31:40,397 Epoch 755: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-04 19:31:40,398 EPOCH 756
2024-02-04 19:31:50,989 [Epoch: 756 Step: 00006800] Batch Recognition Loss:   0.000411 => Gls Tokens per Sec:      520 || Batch Translation Loss:   0.070158 => Txt Tokens per Sec:     1468 || Lr: 0.000100
2024-02-04 19:31:54,576 Epoch 756: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.48 
2024-02-04 19:31:54,576 EPOCH 757
2024-02-04 19:32:08,678 Epoch 757: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-04 19:32:08,679 EPOCH 758
2024-02-04 19:32:22,664 Epoch 758: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-04 19:32:22,665 EPOCH 759
2024-02-04 19:32:36,809 Epoch 759: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-04 19:32:36,810 EPOCH 760
2024-02-04 19:32:50,655 Epoch 760: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-04 19:32:50,655 EPOCH 761
2024-02-04 19:33:04,395 Epoch 761: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-04 19:33:04,395 EPOCH 762
2024-02-04 19:33:18,542 Epoch 762: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-04 19:33:18,543 EPOCH 763
2024-02-04 19:33:32,561 Epoch 763: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-04 19:33:32,562 EPOCH 764
2024-02-04 19:33:46,357 Epoch 764: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-04 19:33:46,358 EPOCH 765
2024-02-04 19:34:00,244 Epoch 765: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-04 19:34:00,244 EPOCH 766
2024-02-04 19:34:13,967 Epoch 766: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-04 19:34:13,967 EPOCH 767
2024-02-04 19:34:22,732 [Epoch: 767 Step: 00006900] Batch Recognition Loss:   0.000303 => Gls Tokens per Sec:      876 || Batch Translation Loss:   0.039726 => Txt Tokens per Sec:     2471 || Lr: 0.000100
2024-02-04 19:34:28,043 Epoch 767: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-04 19:34:28,043 EPOCH 768
2024-02-04 19:34:42,011 Epoch 768: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-04 19:34:42,011 EPOCH 769
2024-02-04 19:34:55,743 Epoch 769: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-04 19:34:55,743 EPOCH 770
2024-02-04 19:35:09,665 Epoch 770: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-04 19:35:09,665 EPOCH 771
2024-02-04 19:35:23,470 Epoch 771: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-04 19:35:23,470 EPOCH 772
2024-02-04 19:35:37,468 Epoch 772: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.61 
2024-02-04 19:35:37,469 EPOCH 773
2024-02-04 19:35:51,155 Epoch 773: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.82 
2024-02-04 19:35:51,155 EPOCH 774
2024-02-04 19:36:04,840 Epoch 774: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.96 
2024-02-04 19:36:04,840 EPOCH 775
2024-02-04 19:36:18,935 Epoch 775: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.82 
2024-02-04 19:36:18,936 EPOCH 776
2024-02-04 19:36:32,575 Epoch 776: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.84 
2024-02-04 19:36:32,576 EPOCH 777
2024-02-04 19:36:46,424 Epoch 777: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.85 
2024-02-04 19:36:46,425 EPOCH 778
2024-02-04 19:36:59,692 [Epoch: 778 Step: 00007000] Batch Recognition Loss:   0.000474 => Gls Tokens per Sec:      608 || Batch Translation Loss:   0.082304 => Txt Tokens per Sec:     1812 || Lr: 0.000100
2024-02-04 19:37:00,315 Epoch 778: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.67 
2024-02-04 19:37:00,315 EPOCH 779
2024-02-04 19:37:13,943 Epoch 779: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.66 
2024-02-04 19:37:13,943 EPOCH 780
2024-02-04 19:37:27,945 Epoch 780: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-04 19:37:27,945 EPOCH 781
2024-02-04 19:37:41,724 Epoch 781: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-04 19:37:41,724 EPOCH 782
2024-02-04 19:37:55,439 Epoch 782: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.48 
2024-02-04 19:37:55,440 EPOCH 783
2024-02-04 19:38:09,651 Epoch 783: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-04 19:38:09,652 EPOCH 784
2024-02-04 19:38:23,361 Epoch 784: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.48 
2024-02-04 19:38:23,362 EPOCH 785
2024-02-04 19:38:37,567 Epoch 785: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-04 19:38:37,567 EPOCH 786
2024-02-04 19:38:51,468 Epoch 786: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-04 19:38:51,468 EPOCH 787
2024-02-04 19:39:05,296 Epoch 787: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.56 
2024-02-04 19:39:05,296 EPOCH 788
2024-02-04 19:39:19,239 Epoch 788: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.65 
2024-02-04 19:39:19,240 EPOCH 789
2024-02-04 19:39:32,565 [Epoch: 789 Step: 00007100] Batch Recognition Loss:   0.000320 => Gls Tokens per Sec:      702 || Batch Translation Loss:   0.045858 => Txt Tokens per Sec:     1934 || Lr: 0.000100
2024-02-04 19:39:33,134 Epoch 789: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-04 19:39:33,134 EPOCH 790
2024-02-04 19:39:46,968 Epoch 790: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.76 
2024-02-04 19:39:46,969 EPOCH 791
2024-02-04 19:40:00,740 Epoch 791: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.20 
2024-02-04 19:40:00,740 EPOCH 792
2024-02-04 19:40:14,499 Epoch 792: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.50 
2024-02-04 19:40:14,499 EPOCH 793
2024-02-04 19:40:28,512 Epoch 793: Total Training Recognition Loss 0.00  Total Training Translation Loss 2.09 
2024-02-04 19:40:28,512 EPOCH 794
2024-02-04 19:40:42,587 Epoch 794: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.80 
2024-02-04 19:40:42,588 EPOCH 795
2024-02-04 19:40:56,582 Epoch 795: Total Training Recognition Loss 0.01  Total Training Translation Loss 5.69 
2024-02-04 19:40:56,582 EPOCH 796
2024-02-04 19:41:10,554 Epoch 796: Total Training Recognition Loss 0.04  Total Training Translation Loss 5.25 
2024-02-04 19:41:10,555 EPOCH 797
2024-02-04 19:41:24,661 Epoch 797: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.55 
2024-02-04 19:41:24,662 EPOCH 798
2024-02-04 19:41:38,383 Epoch 798: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.47 
2024-02-04 19:41:38,384 EPOCH 799
2024-02-04 19:41:52,497 Epoch 799: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.20 
2024-02-04 19:41:52,497 EPOCH 800
2024-02-04 19:42:06,243 [Epoch: 800 Step: 00007200] Batch Recognition Loss:   0.000723 => Gls Tokens per Sec:      773 || Batch Translation Loss:   0.183796 => Txt Tokens per Sec:     2147 || Lr: 0.000100
2024-02-04 19:42:06,244 Epoch 800: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.60 
2024-02-04 19:42:06,244 EPOCH 801
2024-02-04 19:42:20,304 Epoch 801: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.10 
2024-02-04 19:42:20,304 EPOCH 802
2024-02-04 19:42:34,090 Epoch 802: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.79 
2024-02-04 19:42:34,090 EPOCH 803
2024-02-04 19:42:47,942 Epoch 803: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-04 19:42:47,943 EPOCH 804
2024-02-04 19:43:01,938 Epoch 804: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-04 19:43:01,938 EPOCH 805
2024-02-04 19:43:15,876 Epoch 805: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-04 19:43:15,877 EPOCH 806
2024-02-04 19:43:29,925 Epoch 806: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-04 19:43:29,926 EPOCH 807
2024-02-04 19:43:43,503 Epoch 807: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-04 19:43:43,503 EPOCH 808
2024-02-04 19:43:57,637 Epoch 808: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-04 19:43:57,637 EPOCH 809
2024-02-04 19:44:11,536 Epoch 809: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-04 19:44:11,537 EPOCH 810
2024-02-04 19:44:25,347 Epoch 810: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-04 19:44:25,348 EPOCH 811
2024-02-04 19:44:39,455 Epoch 811: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-04 19:44:39,456 EPOCH 812
2024-02-04 19:44:43,738 [Epoch: 812 Step: 00007300] Batch Recognition Loss:   0.000410 => Gls Tokens per Sec:       91 || Batch Translation Loss:   0.015017 => Txt Tokens per Sec:      326 || Lr: 0.000100
2024-02-04 19:44:53,471 Epoch 812: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-04 19:44:53,471 EPOCH 813
2024-02-04 19:45:07,547 Epoch 813: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-04 19:45:07,548 EPOCH 814
2024-02-04 19:45:21,697 Epoch 814: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-04 19:45:21,698 EPOCH 815
2024-02-04 19:45:35,791 Epoch 815: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-04 19:45:35,791 EPOCH 816
2024-02-04 19:45:49,831 Epoch 816: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-04 19:45:49,831 EPOCH 817
2024-02-04 19:46:03,742 Epoch 817: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-04 19:46:03,742 EPOCH 818
2024-02-04 19:46:17,246 Epoch 818: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-04 19:46:17,247 EPOCH 819
2024-02-04 19:46:31,281 Epoch 819: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-04 19:46:31,281 EPOCH 820
2024-02-04 19:46:45,286 Epoch 820: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-04 19:46:45,286 EPOCH 821
2024-02-04 19:46:59,295 Epoch 821: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-04 19:46:59,296 EPOCH 822
2024-02-04 19:47:12,842 Epoch 822: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-04 19:47:12,842 EPOCH 823
2024-02-04 19:47:17,625 [Epoch: 823 Step: 00007400] Batch Recognition Loss:   0.000486 => Gls Tokens per Sec:      349 || Batch Translation Loss:   0.013761 => Txt Tokens per Sec:     1068 || Lr: 0.000100
2024-02-04 19:47:26,479 Epoch 823: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-04 19:47:26,480 EPOCH 824
2024-02-04 19:47:40,330 Epoch 824: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-04 19:47:40,331 EPOCH 825
2024-02-04 19:47:54,421 Epoch 825: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-04 19:47:54,421 EPOCH 826
2024-02-04 19:48:08,205 Epoch 826: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-04 19:48:08,205 EPOCH 827
2024-02-04 19:48:22,297 Epoch 827: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-04 19:48:22,298 EPOCH 828
2024-02-04 19:48:36,279 Epoch 828: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-04 19:48:36,280 EPOCH 829
2024-02-04 19:48:50,386 Epoch 829: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-04 19:48:50,386 EPOCH 830
2024-02-04 19:49:04,360 Epoch 830: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-04 19:49:04,360 EPOCH 831
2024-02-04 19:49:18,361 Epoch 831: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-04 19:49:18,361 EPOCH 832
2024-02-04 19:49:32,482 Epoch 832: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-04 19:49:32,483 EPOCH 833
2024-02-04 19:49:46,422 Epoch 833: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-04 19:49:46,422 EPOCH 834
2024-02-04 19:49:57,616 [Epoch: 834 Step: 00007500] Batch Recognition Loss:   0.000177 => Gls Tokens per Sec:      264 || Batch Translation Loss:   0.013245 => Txt Tokens per Sec:      891 || Lr: 0.000100
2024-02-04 19:50:00,429 Epoch 834: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-04 19:50:00,430 EPOCH 835
2024-02-04 19:50:14,512 Epoch 835: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-04 19:50:14,513 EPOCH 836
2024-02-04 19:50:28,048 Epoch 836: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-04 19:50:28,048 EPOCH 837
2024-02-04 19:50:41,965 Epoch 837: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-04 19:50:41,965 EPOCH 838
2024-02-04 19:50:55,815 Epoch 838: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-04 19:50:55,815 EPOCH 839
2024-02-04 19:51:09,872 Epoch 839: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-04 19:51:09,873 EPOCH 840
2024-02-04 19:51:23,965 Epoch 840: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-04 19:51:23,965 EPOCH 841
2024-02-04 19:51:37,782 Epoch 841: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-04 19:51:37,783 EPOCH 842
2024-02-04 19:51:51,803 Epoch 842: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-04 19:51:51,804 EPOCH 843
2024-02-04 19:52:05,482 Epoch 843: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-04 19:52:05,482 EPOCH 844
2024-02-04 19:52:19,525 Epoch 844: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-04 19:52:19,525 EPOCH 845
2024-02-04 19:52:21,344 [Epoch: 845 Step: 00007600] Batch Recognition Loss:   0.000333 => Gls Tokens per Sec:     2819 || Batch Translation Loss:   0.024730 => Txt Tokens per Sec:     7091 || Lr: 0.000100
2024-02-04 19:52:33,342 Epoch 845: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-04 19:52:33,342 EPOCH 846
2024-02-04 19:52:47,025 Epoch 846: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-04 19:52:47,026 EPOCH 847
2024-02-04 19:53:00,859 Epoch 847: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-04 19:53:00,860 EPOCH 848
2024-02-04 19:53:14,753 Epoch 848: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-04 19:53:14,754 EPOCH 849
2024-02-04 19:53:28,685 Epoch 849: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-04 19:53:28,686 EPOCH 850
2024-02-04 19:53:42,780 Epoch 850: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-04 19:53:42,781 EPOCH 851
2024-02-04 19:53:56,912 Epoch 851: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-04 19:53:56,912 EPOCH 852
2024-02-04 19:54:10,968 Epoch 852: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-04 19:54:10,969 EPOCH 853
2024-02-04 19:54:24,722 Epoch 853: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-04 19:54:24,723 EPOCH 854
2024-02-04 19:54:38,854 Epoch 854: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-04 19:54:38,855 EPOCH 855
2024-02-04 19:54:52,412 Epoch 855: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-04 19:54:52,413 EPOCH 856
2024-02-04 19:55:00,679 [Epoch: 856 Step: 00007700] Batch Recognition Loss:   0.000221 => Gls Tokens per Sec:      774 || Batch Translation Loss:   0.038172 => Txt Tokens per Sec:     2196 || Lr: 0.000100
2024-02-04 19:55:06,416 Epoch 856: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-04 19:55:06,417 EPOCH 857
2024-02-04 19:55:20,578 Epoch 857: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-04 19:55:20,578 EPOCH 858
2024-02-04 19:55:34,552 Epoch 858: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-04 19:55:34,552 EPOCH 859
2024-02-04 19:55:48,541 Epoch 859: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-04 19:55:48,542 EPOCH 860
2024-02-04 19:56:02,152 Epoch 860: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-04 19:56:02,153 EPOCH 861
2024-02-04 19:56:16,200 Epoch 861: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-04 19:56:16,201 EPOCH 862
2024-02-04 19:56:29,925 Epoch 862: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-04 19:56:29,926 EPOCH 863
2024-02-04 19:56:43,822 Epoch 863: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-04 19:56:43,822 EPOCH 864
2024-02-04 19:56:57,524 Epoch 864: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-04 19:56:57,525 EPOCH 865
2024-02-04 19:57:11,462 Epoch 865: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-04 19:57:11,463 EPOCH 866
2024-02-04 19:57:25,125 Epoch 866: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-04 19:57:25,126 EPOCH 867
2024-02-04 19:57:33,352 [Epoch: 867 Step: 00007800] Batch Recognition Loss:   0.000218 => Gls Tokens per Sec:      934 || Batch Translation Loss:   0.051071 => Txt Tokens per Sec:     2490 || Lr: 0.000100
2024-02-04 19:57:38,870 Epoch 867: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-04 19:57:38,871 EPOCH 868
2024-02-04 19:57:52,772 Epoch 868: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-04 19:57:52,772 EPOCH 869
2024-02-04 19:58:06,773 Epoch 869: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-04 19:58:06,774 EPOCH 870
2024-02-04 19:58:20,978 Epoch 870: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-04 19:58:20,979 EPOCH 871
2024-02-04 19:58:34,807 Epoch 871: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-04 19:58:34,808 EPOCH 872
2024-02-04 19:58:48,759 Epoch 872: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-04 19:58:48,760 EPOCH 873
2024-02-04 19:59:02,633 Epoch 873: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-04 19:59:02,633 EPOCH 874
2024-02-04 19:59:16,780 Epoch 874: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-04 19:59:16,781 EPOCH 875
2024-02-04 19:59:30,456 Epoch 875: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-04 19:59:30,457 EPOCH 876
2024-02-04 19:59:44,334 Epoch 876: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.61 
2024-02-04 19:59:44,334 EPOCH 877
2024-02-04 19:59:58,321 Epoch 877: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.66 
2024-02-04 19:59:58,322 EPOCH 878
2024-02-04 20:00:06,716 [Epoch: 878 Step: 00007900] Batch Recognition Loss:   0.000221 => Gls Tokens per Sec:      962 || Batch Translation Loss:   0.064659 => Txt Tokens per Sec:     2600 || Lr: 0.000100
2024-02-04 20:00:12,077 Epoch 878: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.56 
2024-02-04 20:00:12,077 EPOCH 879
2024-02-04 20:00:26,136 Epoch 879: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.67 
2024-02-04 20:00:26,136 EPOCH 880
2024-02-04 20:00:39,913 Epoch 880: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.59 
2024-02-04 20:00:39,914 EPOCH 881
2024-02-04 20:00:53,861 Epoch 881: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.59 
2024-02-04 20:00:53,862 EPOCH 882
2024-02-04 20:01:07,947 Epoch 882: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.77 
2024-02-04 20:01:07,948 EPOCH 883
2024-02-04 20:01:21,394 Epoch 883: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.81 
2024-02-04 20:01:21,394 EPOCH 884
2024-02-04 20:01:35,242 Epoch 884: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.64 
2024-02-04 20:01:35,243 EPOCH 885
2024-02-04 20:01:48,838 Epoch 885: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.32 
2024-02-04 20:01:48,839 EPOCH 886
2024-02-04 20:02:02,858 Epoch 886: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.46 
2024-02-04 20:02:02,858 EPOCH 887
2024-02-04 20:02:16,679 Epoch 887: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.13 
2024-02-04 20:02:16,680 EPOCH 888
2024-02-04 20:02:30,658 Epoch 888: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.02 
2024-02-04 20:02:30,659 EPOCH 889
2024-02-04 20:02:42,669 [Epoch: 889 Step: 00008000] Batch Recognition Loss:   0.000288 => Gls Tokens per Sec:      779 || Batch Translation Loss:   0.159812 => Txt Tokens per Sec:     2115 || Lr: 0.000100
2024-02-04 20:03:12,959 Validation result at epoch 889, step     8000: duration: 30.2888s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00025	Translation Loss: 91530.74219	PPL: 9504.16895
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.87	(BLEU-1: 12.01,	BLEU-2: 4.25,	BLEU-3: 1.74,	BLEU-4: 0.87)
	CHRF 18.10	ROUGE 10.31
2024-02-04 20:03:12,961 Logging Recognition and Translation Outputs
2024-02-04 20:03:12,961 ========================================================================================================================
2024-02-04 20:03:12,961 Logging Sequence: 88_57.00
2024-02-04 20:03:12,962 	Gloss Reference :	A B+C+D+E
2024-02-04 20:03:12,963 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 20:03:12,963 	Gloss Alignment :	         
2024-02-04 20:03:12,963 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 20:03:12,965 	Text Reference  :	which   stated  messi we're  waiting for  you     to   come here      you  will be finished when    you come
2024-02-04 20:03:12,966 	Text Hypothesis :	notably rosario has   become the     most violent city in   argentina with 250  to 300      murders in  2022
2024-02-04 20:03:12,966 	Text Alignment  :	S       S       S     S      S       S    S       S    S    S         S    S    S  S        S       S   S   
2024-02-04 20:03:12,966 ========================================================================================================================
2024-02-04 20:03:12,966 Logging Sequence: 171_142.00
2024-02-04 20:03:12,966 	Gloss Reference :	A B+C+D+E
2024-02-04 20:03:12,966 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 20:03:12,967 	Gloss Alignment :	         
2024-02-04 20:03:12,967 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 20:03:12,969 	Text Reference  :	***** *** this   decision on        dhoni made   a   significant impact as    pathirana claimed two           tough wickets
2024-02-04 20:03:12,969 	Text Hypothesis :	since the couple were     residents of    mumbai the mumbai      police cyber cell      began   investigating the   matter 
2024-02-04 20:03:12,969 	Text Alignment  :	I     I   S      S        S         S     S      S   S           S      S     S         S       S             S     S      
2024-02-04 20:03:12,970 ========================================================================================================================
2024-02-04 20:03:12,970 Logging Sequence: 125_207.00
2024-02-04 20:03:12,970 	Gloss Reference :	A B+C+D+E
2024-02-04 20:03:12,970 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 20:03:12,970 	Gloss Alignment :	         
2024-02-04 20:03:12,970 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 20:03:12,972 	Text Reference  :	he had not ***** **** *** ******* ** practised since he  returned and he          had also fallen sick
2024-02-04 20:03:12,973 	Text Hypothesis :	i  am  not happy with the javelin at the       2020  but decided  to  participate in  the  indian love
2024-02-04 20:03:12,973 	Text Alignment  :	S  S       I     I    I   I       I  S         S     S   S        S   S           S   S    S      S   
2024-02-04 20:03:12,973 ========================================================================================================================
2024-02-04 20:03:12,973 Logging Sequence: 68_230.00
2024-02-04 20:03:12,973 	Gloss Reference :	A B+C+D+E
2024-02-04 20:03:12,974 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 20:03:12,974 	Gloss Alignment :	         
2024-02-04 20:03:12,974 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 20:03:12,976 	Text Reference  :	**** **** **** * ****** *** ****** **** **** let us know what      you think in the comments  below   
2024-02-04 20:03:12,976 	Text Hypothesis :	they also sent a person who bowled very well and he is   extremely fit but   he is  extremely personal
2024-02-04 20:03:12,976 	Text Alignment  :	I    I    I    I I      I   I      I    I    S   S  S    S         S   S     S  S   S         S       
2024-02-04 20:03:12,976 ========================================================================================================================
2024-02-04 20:03:12,976 Logging Sequence: 126_82.00
2024-02-04 20:03:12,976 	Gloss Reference :	A B+C+D+E
2024-02-04 20:03:12,977 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 20:03:12,977 	Gloss Alignment :	         
2024-02-04 20:03:12,977 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 20:03:12,979 	Text Reference  :	** ** * ******** neeraj  also dedicated his gold   medal ** to      former indian olympians who     came close to  winning medals
2024-02-04 20:03:12,980 	Text Hypothesis :	he is a talented players who  won       a   silver medal in javelin throw  at     the       winning you  did   not winning ******
2024-02-04 20:03:12,980 	Text Alignment  :	I  I  I I        S       S    S         S   S            I  S       S      S      S         S       S    S     S           D     
2024-02-04 20:03:12,980 ========================================================================================================================
2024-02-04 20:03:14,977 Epoch 889: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.08 
2024-02-04 20:03:14,978 EPOCH 890
2024-02-04 20:03:31,864 Epoch 890: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.24 
2024-02-04 20:03:31,865 EPOCH 891
2024-02-04 20:03:46,680 Epoch 891: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.04 
2024-02-04 20:03:46,681 EPOCH 892
2024-02-04 20:04:00,739 Epoch 892: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.79 
2024-02-04 20:04:00,739 EPOCH 893
2024-02-04 20:04:14,860 Epoch 893: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.63 
2024-02-04 20:04:14,861 EPOCH 894
2024-02-04 20:04:28,949 Epoch 894: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-04 20:04:28,950 EPOCH 895
2024-02-04 20:04:43,145 Epoch 895: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-04 20:04:43,146 EPOCH 896
2024-02-04 20:04:57,238 Epoch 896: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-04 20:04:57,239 EPOCH 897
2024-02-04 20:05:11,374 Epoch 897: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-04 20:05:11,375 EPOCH 898
2024-02-04 20:05:25,625 Epoch 898: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-04 20:05:25,626 EPOCH 899
2024-02-04 20:05:39,698 Epoch 899: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-04 20:05:39,699 EPOCH 900
2024-02-04 20:05:53,614 [Epoch: 900 Step: 00008100] Batch Recognition Loss:   0.000221 => Gls Tokens per Sec:      764 || Batch Translation Loss:   0.075569 => Txt Tokens per Sec:     2121 || Lr: 0.000100
2024-02-04 20:05:53,615 Epoch 900: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-04 20:05:53,615 EPOCH 901
2024-02-04 20:06:07,478 Epoch 901: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-04 20:06:07,479 EPOCH 902
2024-02-04 20:06:21,574 Epoch 902: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-04 20:06:21,575 EPOCH 903
2024-02-04 20:06:35,594 Epoch 903: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-04 20:06:35,595 EPOCH 904
2024-02-04 20:06:49,801 Epoch 904: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-04 20:06:49,801 EPOCH 905
2024-02-04 20:07:03,600 Epoch 905: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-04 20:07:03,600 EPOCH 906
2024-02-04 20:07:17,693 Epoch 906: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-04 20:07:17,694 EPOCH 907
2024-02-04 20:07:31,816 Epoch 907: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-04 20:07:31,816 EPOCH 908
2024-02-04 20:07:46,034 Epoch 908: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-04 20:07:46,035 EPOCH 909
2024-02-04 20:08:00,261 Epoch 909: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-04 20:08:00,261 EPOCH 910
2024-02-04 20:08:14,246 Epoch 910: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-04 20:08:14,246 EPOCH 911
2024-02-04 20:08:28,193 Epoch 911: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-04 20:08:28,194 EPOCH 912
2024-02-04 20:08:29,884 [Epoch: 912 Step: 00008200] Batch Recognition Loss:   0.000248 => Gls Tokens per Sec:      758 || Batch Translation Loss:   0.031945 => Txt Tokens per Sec:     2424 || Lr: 0.000100
2024-02-04 20:08:42,028 Epoch 912: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-04 20:08:42,028 EPOCH 913
2024-02-04 20:08:56,238 Epoch 913: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-04 20:08:56,239 EPOCH 914
2024-02-04 20:09:10,043 Epoch 914: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-04 20:09:10,043 EPOCH 915
2024-02-04 20:09:23,993 Epoch 915: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-04 20:09:23,994 EPOCH 916
2024-02-04 20:09:38,151 Epoch 916: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-04 20:09:38,151 EPOCH 917
2024-02-04 20:09:52,349 Epoch 917: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-04 20:09:52,350 EPOCH 918
2024-02-04 20:10:06,356 Epoch 918: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-04 20:10:06,357 EPOCH 919
2024-02-04 20:10:20,379 Epoch 919: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-04 20:10:20,379 EPOCH 920
2024-02-04 20:10:34,141 Epoch 920: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.48 
2024-02-04 20:10:34,141 EPOCH 921
2024-02-04 20:10:48,385 Epoch 921: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-04 20:10:48,386 EPOCH 922
2024-02-04 20:11:02,069 Epoch 922: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-04 20:11:02,069 EPOCH 923
2024-02-04 20:11:03,113 [Epoch: 923 Step: 00008300] Batch Recognition Loss:   0.000211 => Gls Tokens per Sec:     2456 || Batch Translation Loss:   0.059013 => Txt Tokens per Sec:     5867 || Lr: 0.000100
2024-02-04 20:11:16,078 Epoch 923: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.81 
2024-02-04 20:11:16,079 EPOCH 924
2024-02-04 20:11:30,061 Epoch 924: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.74 
2024-02-04 20:11:30,062 EPOCH 925
2024-02-04 20:11:44,067 Epoch 925: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.56 
2024-02-04 20:11:44,067 EPOCH 926
2024-02-04 20:11:58,169 Epoch 926: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.55 
2024-02-04 20:11:58,170 EPOCH 927
2024-02-04 20:12:12,166 Epoch 927: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-04 20:12:12,166 EPOCH 928
2024-02-04 20:12:25,697 Epoch 928: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.64 
2024-02-04 20:12:25,697 EPOCH 929
2024-02-04 20:12:39,331 Epoch 929: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.78 
2024-02-04 20:12:39,332 EPOCH 930
2024-02-04 20:12:53,639 Epoch 930: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.74 
2024-02-04 20:12:53,640 EPOCH 931
2024-02-04 20:13:07,406 Epoch 931: Total Training Recognition Loss 0.01  Total Training Translation Loss 5.11 
2024-02-04 20:13:07,406 EPOCH 932
2024-02-04 20:13:21,548 Epoch 932: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.76 
2024-02-04 20:13:21,549 EPOCH 933
2024-02-04 20:13:35,585 Epoch 933: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.64 
2024-02-04 20:13:35,586 EPOCH 934
2024-02-04 20:13:42,022 [Epoch: 934 Step: 00008400] Batch Recognition Loss:   0.000827 => Gls Tokens per Sec:      458 || Batch Translation Loss:   0.322264 => Txt Tokens per Sec:     1349 || Lr: 0.000100
2024-02-04 20:13:49,705 Epoch 934: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.58 
2024-02-04 20:13:49,706 EPOCH 935
2024-02-04 20:14:03,531 Epoch 935: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.96 
2024-02-04 20:14:03,531 EPOCH 936
2024-02-04 20:14:17,379 Epoch 936: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.25 
2024-02-04 20:14:17,380 EPOCH 937
2024-02-04 20:14:31,392 Epoch 937: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.00 
2024-02-04 20:14:31,393 EPOCH 938
2024-02-04 20:14:45,642 Epoch 938: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.72 
2024-02-04 20:14:45,642 EPOCH 939
2024-02-04 20:14:59,496 Epoch 939: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.65 
2024-02-04 20:14:59,497 EPOCH 940
2024-02-04 20:15:13,615 Epoch 940: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-04 20:15:13,616 EPOCH 941
2024-02-04 20:15:27,624 Epoch 941: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.60 
2024-02-04 20:15:27,625 EPOCH 942
2024-02-04 20:15:41,355 Epoch 942: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-04 20:15:41,355 EPOCH 943
2024-02-04 20:15:55,148 Epoch 943: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-04 20:15:55,149 EPOCH 944
2024-02-04 20:16:08,885 Epoch 944: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-04 20:16:08,885 EPOCH 945
2024-02-04 20:16:14,939 [Epoch: 945 Step: 00008500] Batch Recognition Loss:   0.000354 => Gls Tokens per Sec:      699 || Batch Translation Loss:   0.051728 => Txt Tokens per Sec:     2070 || Lr: 0.000100
2024-02-04 20:16:22,723 Epoch 945: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-04 20:16:22,723 EPOCH 946
2024-02-04 20:16:36,698 Epoch 946: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-04 20:16:36,698 EPOCH 947
2024-02-04 20:16:50,185 Epoch 947: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-04 20:16:50,185 EPOCH 948
2024-02-04 20:17:04,284 Epoch 948: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-04 20:17:04,285 EPOCH 949
2024-02-04 20:17:18,369 Epoch 949: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-04 20:17:18,369 EPOCH 950
2024-02-04 20:17:32,288 Epoch 950: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-04 20:17:32,288 EPOCH 951
2024-02-04 20:17:46,412 Epoch 951: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-04 20:17:46,413 EPOCH 952
2024-02-04 20:18:00,238 Epoch 952: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-04 20:18:00,239 EPOCH 953
2024-02-04 20:18:14,172 Epoch 953: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-04 20:18:14,172 EPOCH 954
2024-02-04 20:18:27,936 Epoch 954: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-04 20:18:27,937 EPOCH 955
2024-02-04 20:18:41,911 Epoch 955: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-04 20:18:41,912 EPOCH 956
2024-02-04 20:18:48,852 [Epoch: 956 Step: 00008600] Batch Recognition Loss:   0.000235 => Gls Tokens per Sec:      922 || Batch Translation Loss:   0.030239 => Txt Tokens per Sec:     2526 || Lr: 0.000100
2024-02-04 20:18:55,688 Epoch 956: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-04 20:18:55,688 EPOCH 957
2024-02-04 20:19:09,217 Epoch 957: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-04 20:19:09,218 EPOCH 958
2024-02-04 20:19:23,225 Epoch 958: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-04 20:19:23,226 EPOCH 959
2024-02-04 20:19:37,280 Epoch 959: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-04 20:19:37,281 EPOCH 960
2024-02-04 20:19:51,123 Epoch 960: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-04 20:19:51,124 EPOCH 961
2024-02-04 20:20:05,176 Epoch 961: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-04 20:20:05,176 EPOCH 962
2024-02-04 20:20:18,898 Epoch 962: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-04 20:20:18,899 EPOCH 963
2024-02-04 20:20:32,875 Epoch 963: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-04 20:20:32,876 EPOCH 964
2024-02-04 20:20:46,885 Epoch 964: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-04 20:20:46,885 EPOCH 965
2024-02-04 20:21:01,217 Epoch 965: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-04 20:21:01,217 EPOCH 966
2024-02-04 20:21:15,118 Epoch 966: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-04 20:21:15,119 EPOCH 967
2024-02-04 20:21:23,200 [Epoch: 967 Step: 00008700] Batch Recognition Loss:   0.000179 => Gls Tokens per Sec:      840 || Batch Translation Loss:   0.022141 => Txt Tokens per Sec:     2268 || Lr: 0.000100
2024-02-04 20:21:29,238 Epoch 967: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-04 20:21:29,238 EPOCH 968
2024-02-04 20:21:43,159 Epoch 968: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-04 20:21:43,159 EPOCH 969
2024-02-04 20:21:57,036 Epoch 969: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-04 20:21:57,036 EPOCH 970
2024-02-04 20:22:10,989 Epoch 970: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-04 20:22:10,989 EPOCH 971
2024-02-04 20:22:24,973 Epoch 971: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-04 20:22:24,973 EPOCH 972
2024-02-04 20:22:38,702 Epoch 972: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-04 20:22:38,703 EPOCH 973
2024-02-04 20:22:52,575 Epoch 973: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-04 20:22:52,576 EPOCH 974
2024-02-04 20:23:06,531 Epoch 974: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-04 20:23:06,532 EPOCH 975
2024-02-04 20:23:19,938 Epoch 975: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-04 20:23:19,939 EPOCH 976
2024-02-04 20:23:34,065 Epoch 976: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-04 20:23:34,066 EPOCH 977
2024-02-04 20:23:48,052 Epoch 977: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-04 20:23:48,053 EPOCH 978
2024-02-04 20:23:57,363 [Epoch: 978 Step: 00008800] Batch Recognition Loss:   0.000218 => Gls Tokens per Sec:      963 || Batch Translation Loss:   0.025152 => Txt Tokens per Sec:     2704 || Lr: 0.000100
2024-02-04 20:24:01,993 Epoch 978: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-04 20:24:01,993 EPOCH 979
2024-02-04 20:24:16,150 Epoch 979: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-04 20:24:16,150 EPOCH 980
2024-02-04 20:24:30,070 Epoch 980: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-04 20:24:30,070 EPOCH 981
2024-02-04 20:24:43,823 Epoch 981: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-04 20:24:43,823 EPOCH 982
2024-02-04 20:24:57,807 Epoch 982: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-04 20:24:57,807 EPOCH 983
2024-02-04 20:25:11,628 Epoch 983: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-04 20:25:11,629 EPOCH 984
2024-02-04 20:25:25,480 Epoch 984: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-04 20:25:25,481 EPOCH 985
2024-02-04 20:25:39,453 Epoch 985: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-04 20:25:39,453 EPOCH 986
2024-02-04 20:25:53,215 Epoch 986: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-04 20:25:53,216 EPOCH 987
2024-02-04 20:26:06,939 Epoch 987: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-04 20:26:06,939 EPOCH 988
2024-02-04 20:26:20,639 Epoch 988: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-04 20:26:20,640 EPOCH 989
2024-02-04 20:26:33,955 [Epoch: 989 Step: 00008900] Batch Recognition Loss:   0.000162 => Gls Tokens per Sec:      702 || Batch Translation Loss:   0.022579 => Txt Tokens per Sec:     1955 || Lr: 0.000100
2024-02-04 20:26:34,418 Epoch 989: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-04 20:26:34,418 EPOCH 990
2024-02-04 20:26:48,174 Epoch 990: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-04 20:26:48,175 EPOCH 991
2024-02-04 20:27:02,148 Epoch 991: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-04 20:27:02,148 EPOCH 992
2024-02-04 20:27:16,009 Epoch 992: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-04 20:27:16,009 EPOCH 993
2024-02-04 20:27:29,608 Epoch 993: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-04 20:27:29,609 EPOCH 994
2024-02-04 20:27:43,940 Epoch 994: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-04 20:27:43,941 EPOCH 995
2024-02-04 20:27:57,788 Epoch 995: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-04 20:27:57,789 EPOCH 996
2024-02-04 20:28:11,688 Epoch 996: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-04 20:28:11,689 EPOCH 997
2024-02-04 20:28:25,189 Epoch 997: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.87 
2024-02-04 20:28:25,189 EPOCH 998
2024-02-04 20:28:39,385 Epoch 998: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.82 
2024-02-04 20:28:39,385 EPOCH 999
2024-02-04 20:28:52,938 Epoch 999: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.63 
2024-02-04 20:28:52,939 EPOCH 1000
2024-02-04 20:29:06,733 [Epoch: 1000 Step: 00009000] Batch Recognition Loss:   0.000314 => Gls Tokens per Sec:      771 || Batch Translation Loss:   0.081418 => Txt Tokens per Sec:     2140 || Lr: 0.000100
2024-02-04 20:29:06,733 Epoch 1000: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.57 
2024-02-04 20:29:06,734 EPOCH 1001
2024-02-04 20:29:20,506 Epoch 1001: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.48 
2024-02-04 20:29:20,506 EPOCH 1002
2024-02-04 20:29:34,506 Epoch 1002: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.61 
2024-02-04 20:29:34,506 EPOCH 1003
2024-02-04 20:29:48,078 Epoch 1003: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.88 
2024-02-04 20:29:48,078 EPOCH 1004
2024-02-04 20:30:01,797 Epoch 1004: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.76 
2024-02-04 20:30:01,798 EPOCH 1005
2024-02-04 20:30:15,827 Epoch 1005: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.67 
2024-02-04 20:30:15,828 EPOCH 1006
2024-02-04 20:30:29,406 Epoch 1006: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.67 
2024-02-04 20:30:29,407 EPOCH 1007
2024-02-04 20:30:43,479 Epoch 1007: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.62 
2024-02-04 20:30:43,480 EPOCH 1008
2024-02-04 20:30:57,275 Epoch 1008: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-04 20:30:57,276 EPOCH 1009
2024-02-04 20:31:11,235 Epoch 1009: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-04 20:31:11,236 EPOCH 1010
2024-02-04 20:31:25,025 Epoch 1010: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-04 20:31:25,025 EPOCH 1011
2024-02-04 20:31:38,836 Epoch 1011: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-04 20:31:38,837 EPOCH 1012
2024-02-04 20:31:39,153 [Epoch: 1012 Step: 00009100] Batch Recognition Loss:   0.000234 => Gls Tokens per Sec:     4064 || Batch Translation Loss:   0.018251 => Txt Tokens per Sec:     7251 || Lr: 0.000100
2024-02-04 20:31:52,679 Epoch 1012: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-04 20:31:52,680 EPOCH 1013
2024-02-04 20:32:06,675 Epoch 1013: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.57 
2024-02-04 20:32:06,676 EPOCH 1014
2024-02-04 20:32:20,695 Epoch 1014: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.57 
2024-02-04 20:32:20,696 EPOCH 1015
2024-02-04 20:32:34,377 Epoch 1015: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.60 
2024-02-04 20:32:34,378 EPOCH 1016
2024-02-04 20:32:48,313 Epoch 1016: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-04 20:32:48,313 EPOCH 1017
2024-02-04 20:33:02,319 Epoch 1017: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.62 
2024-02-04 20:33:02,319 EPOCH 1018
2024-02-04 20:33:16,247 Epoch 1018: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.59 
2024-02-04 20:33:16,248 EPOCH 1019
2024-02-04 20:33:30,170 Epoch 1019: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.64 
2024-02-04 20:33:30,171 EPOCH 1020
2024-02-04 20:33:44,175 Epoch 1020: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.78 
2024-02-04 20:33:44,175 EPOCH 1021
2024-02-04 20:33:58,019 Epoch 1021: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.74 
2024-02-04 20:33:58,020 EPOCH 1022
2024-02-04 20:34:12,038 Epoch 1022: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.64 
2024-02-04 20:34:12,039 EPOCH 1023
2024-02-04 20:34:13,004 [Epoch: 1023 Step: 00009200] Batch Recognition Loss:   0.000302 => Gls Tokens per Sec:     2655 || Batch Translation Loss:   0.039296 => Txt Tokens per Sec:     6365 || Lr: 0.000100
2024-02-04 20:34:25,881 Epoch 1023: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.82 
2024-02-04 20:34:25,881 EPOCH 1024
2024-02-04 20:34:39,543 Epoch 1024: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.85 
2024-02-04 20:34:39,543 EPOCH 1025
2024-02-04 20:34:53,332 Epoch 1025: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.62 
2024-02-04 20:34:53,333 EPOCH 1026
2024-02-04 20:35:07,394 Epoch 1026: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-04 20:35:07,395 EPOCH 1027
2024-02-04 20:35:21,223 Epoch 1027: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-04 20:35:21,224 EPOCH 1028
2024-02-04 20:35:35,186 Epoch 1028: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.66 
2024-02-04 20:35:35,187 EPOCH 1029
2024-02-04 20:35:49,165 Epoch 1029: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.79 
2024-02-04 20:35:49,166 EPOCH 1030
2024-02-04 20:36:03,190 Epoch 1030: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.56 
2024-02-04 20:36:03,191 EPOCH 1031
2024-02-04 20:36:16,992 Epoch 1031: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.83 
2024-02-04 20:36:16,993 EPOCH 1032
2024-02-04 20:36:30,834 Epoch 1032: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.75 
2024-02-04 20:36:30,834 EPOCH 1033
2024-02-04 20:36:44,916 Epoch 1033: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-04 20:36:44,917 EPOCH 1034
2024-02-04 20:36:50,911 [Epoch: 1034 Step: 00009300] Batch Recognition Loss:   0.000267 => Gls Tokens per Sec:      641 || Batch Translation Loss:   0.042777 => Txt Tokens per Sec:     1714 || Lr: 0.000100
2024-02-04 20:36:58,784 Epoch 1034: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-04 20:36:58,785 EPOCH 1035
2024-02-04 20:37:12,799 Epoch 1035: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-04 20:37:12,799 EPOCH 1036
2024-02-04 20:37:26,429 Epoch 1036: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-04 20:37:26,429 EPOCH 1037
2024-02-04 20:37:40,214 Epoch 1037: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-04 20:37:40,215 EPOCH 1038
2024-02-04 20:37:54,195 Epoch 1038: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-04 20:37:54,195 EPOCH 1039
2024-02-04 20:38:07,913 Epoch 1039: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-04 20:38:07,914 EPOCH 1040
2024-02-04 20:38:21,802 Epoch 1040: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-04 20:38:21,802 EPOCH 1041
2024-02-04 20:38:35,762 Epoch 1041: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-04 20:38:35,763 EPOCH 1042
2024-02-04 20:38:49,648 Epoch 1042: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-04 20:38:49,649 EPOCH 1043
2024-02-04 20:39:03,861 Epoch 1043: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-04 20:39:03,862 EPOCH 1044
2024-02-04 20:39:17,731 Epoch 1044: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-04 20:39:17,731 EPOCH 1045
2024-02-04 20:39:21,371 [Epoch: 1045 Step: 00009400] Batch Recognition Loss:   0.000228 => Gls Tokens per Sec:     1408 || Batch Translation Loss:   0.031540 => Txt Tokens per Sec:     4023 || Lr: 0.000100
2024-02-04 20:39:31,807 Epoch 1045: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-04 20:39:31,808 EPOCH 1046
2024-02-04 20:39:45,894 Epoch 1046: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-04 20:39:45,895 EPOCH 1047
2024-02-04 20:40:00,004 Epoch 1047: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-04 20:40:00,004 EPOCH 1048
2024-02-04 20:40:13,696 Epoch 1048: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-04 20:40:13,696 EPOCH 1049
2024-02-04 20:40:27,555 Epoch 1049: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-04 20:40:27,556 EPOCH 1050
2024-02-04 20:40:41,609 Epoch 1050: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-04 20:40:41,610 EPOCH 1051
2024-02-04 20:40:55,319 Epoch 1051: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-04 20:40:55,320 EPOCH 1052
2024-02-04 20:41:09,152 Epoch 1052: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-04 20:41:09,152 EPOCH 1053
2024-02-04 20:41:23,024 Epoch 1053: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-04 20:41:23,025 EPOCH 1054
2024-02-04 20:41:37,152 Epoch 1054: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-04 20:41:37,153 EPOCH 1055
2024-02-04 20:41:50,946 Epoch 1055: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-04 20:41:50,947 EPOCH 1056
2024-02-04 20:41:59,080 [Epoch: 1056 Step: 00009500] Batch Recognition Loss:   0.000181 => Gls Tokens per Sec:      787 || Batch Translation Loss:   0.027211 => Txt Tokens per Sec:     2226 || Lr: 0.000100
2024-02-04 20:42:04,931 Epoch 1056: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-04 20:42:04,932 EPOCH 1057
2024-02-04 20:42:18,888 Epoch 1057: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-04 20:42:18,888 EPOCH 1058
2024-02-04 20:42:32,677 Epoch 1058: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-04 20:42:32,677 EPOCH 1059
2024-02-04 20:42:46,402 Epoch 1059: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-04 20:42:46,402 EPOCH 1060
2024-02-04 20:43:00,411 Epoch 1060: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-04 20:43:00,412 EPOCH 1061
2024-02-04 20:43:14,053 Epoch 1061: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-04 20:43:14,054 EPOCH 1062
2024-02-04 20:43:28,399 Epoch 1062: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-04 20:43:28,399 EPOCH 1063
2024-02-04 20:43:42,188 Epoch 1063: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.72 
2024-02-04 20:43:42,189 EPOCH 1064
2024-02-04 20:43:56,177 Epoch 1064: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.97 
2024-02-04 20:43:56,178 EPOCH 1065
2024-02-04 20:44:10,037 Epoch 1065: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.12 
2024-02-04 20:44:10,038 EPOCH 1066
2024-02-04 20:44:23,898 Epoch 1066: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.98 
2024-02-04 20:44:23,899 EPOCH 1067
2024-02-04 20:44:36,335 [Epoch: 1067 Step: 00009600] Batch Recognition Loss:   0.000436 => Gls Tokens per Sec:      546 || Batch Translation Loss:   0.053083 => Txt Tokens per Sec:     1502 || Lr: 0.000100
2024-02-04 20:44:38,031 Epoch 1067: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.22 
2024-02-04 20:44:38,031 EPOCH 1068
2024-02-04 20:44:52,018 Epoch 1068: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.92 
2024-02-04 20:44:52,019 EPOCH 1069
2024-02-04 20:45:05,881 Epoch 1069: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.96 
2024-02-04 20:45:05,882 EPOCH 1070
2024-02-04 20:45:19,977 Epoch 1070: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.77 
2024-02-04 20:45:19,977 EPOCH 1071
2024-02-04 20:45:34,285 Epoch 1071: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.61 
2024-02-04 20:45:34,285 EPOCH 1072
2024-02-04 20:45:48,051 Epoch 1072: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-04 20:45:48,051 EPOCH 1073
2024-02-04 20:46:01,992 Epoch 1073: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-04 20:46:01,993 EPOCH 1074
2024-02-04 20:46:15,786 Epoch 1074: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-04 20:46:15,787 EPOCH 1075
2024-02-04 20:46:29,608 Epoch 1075: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-04 20:46:29,608 EPOCH 1076
2024-02-04 20:46:43,401 Epoch 1076: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-04 20:46:43,402 EPOCH 1077
2024-02-04 20:46:56,800 Epoch 1077: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-04 20:46:56,801 EPOCH 1078
2024-02-04 20:47:08,719 [Epoch: 1078 Step: 00009700] Batch Recognition Loss:   0.000271 => Gls Tokens per Sec:      677 || Batch Translation Loss:   0.050651 => Txt Tokens per Sec:     1837 || Lr: 0.000100
2024-02-04 20:47:10,916 Epoch 1078: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-04 20:47:10,916 EPOCH 1079
2024-02-04 20:47:24,478 Epoch 1079: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-04 20:47:24,478 EPOCH 1080
2024-02-04 20:47:38,228 Epoch 1080: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-04 20:47:38,229 EPOCH 1081
2024-02-04 20:47:51,982 Epoch 1081: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-04 20:47:51,983 EPOCH 1082
2024-02-04 20:48:05,941 Epoch 1082: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-04 20:48:05,942 EPOCH 1083
2024-02-04 20:48:19,754 Epoch 1083: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-04 20:48:19,754 EPOCH 1084
2024-02-04 20:48:33,727 Epoch 1084: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-04 20:48:33,728 EPOCH 1085
2024-02-04 20:48:47,380 Epoch 1085: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-04 20:48:47,381 EPOCH 1086
2024-02-04 20:49:01,146 Epoch 1086: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-04 20:49:01,147 EPOCH 1087
2024-02-04 20:49:15,137 Epoch 1087: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-04 20:49:15,138 EPOCH 1088
2024-02-04 20:49:28,897 Epoch 1088: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-04 20:49:28,898 EPOCH 1089
2024-02-04 20:49:42,352 [Epoch: 1089 Step: 00009800] Batch Recognition Loss:   0.000195 => Gls Tokens per Sec:      695 || Batch Translation Loss:   0.035953 => Txt Tokens per Sec:     1914 || Lr: 0.000100
2024-02-04 20:49:42,917 Epoch 1089: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-04 20:49:42,917 EPOCH 1090
2024-02-04 20:49:56,901 Epoch 1090: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-04 20:49:56,901 EPOCH 1091
2024-02-04 20:50:10,917 Epoch 1091: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-04 20:50:10,918 EPOCH 1092
2024-02-04 20:50:24,601 Epoch 1092: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-04 20:50:24,602 EPOCH 1093
2024-02-04 20:50:38,624 Epoch 1093: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-04 20:50:38,625 EPOCH 1094
2024-02-04 20:50:52,729 Epoch 1094: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-04 20:50:52,730 EPOCH 1095
2024-02-04 20:51:06,383 Epoch 1095: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-04 20:51:06,383 EPOCH 1096
2024-02-04 20:51:20,526 Epoch 1096: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-04 20:51:20,526 EPOCH 1097
2024-02-04 20:51:34,545 Epoch 1097: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-04 20:51:34,546 EPOCH 1098
2024-02-04 20:51:48,627 Epoch 1098: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-04 20:51:48,628 EPOCH 1099
2024-02-04 20:52:02,332 Epoch 1099: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-04 20:52:02,332 EPOCH 1100
2024-02-04 20:52:16,395 [Epoch: 1100 Step: 00009900] Batch Recognition Loss:   0.000188 => Gls Tokens per Sec:      756 || Batch Translation Loss:   0.018491 => Txt Tokens per Sec:     2098 || Lr: 0.000100
2024-02-04 20:52:16,396 Epoch 1100: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-04 20:52:16,396 EPOCH 1101
2024-02-04 20:52:30,208 Epoch 1101: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-04 20:52:30,209 EPOCH 1102
2024-02-04 20:52:43,870 Epoch 1102: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-04 20:52:43,871 EPOCH 1103
2024-02-04 20:52:58,172 Epoch 1103: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-04 20:52:58,173 EPOCH 1104
2024-02-04 20:53:11,843 Epoch 1104: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-04 20:53:11,844 EPOCH 1105
2024-02-04 20:53:25,696 Epoch 1105: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-04 20:53:25,697 EPOCH 1106
2024-02-04 20:53:39,604 Epoch 1106: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-04 20:53:39,604 EPOCH 1107
2024-02-04 20:53:53,391 Epoch 1107: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-04 20:53:53,392 EPOCH 1108
2024-02-04 20:54:07,239 Epoch 1108: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-04 20:54:07,240 EPOCH 1109
2024-02-04 20:54:21,207 Epoch 1109: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-04 20:54:21,208 EPOCH 1110
2024-02-04 20:54:35,313 Epoch 1110: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-04 20:54:35,314 EPOCH 1111
2024-02-04 20:54:49,026 Epoch 1111: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-04 20:54:49,028 EPOCH 1112
2024-02-04 20:54:53,942 [Epoch: 1112 Step: 00010000] Batch Recognition Loss:   0.000184 => Gls Tokens per Sec:      261 || Batch Translation Loss:   0.029726 => Txt Tokens per Sec:      913 || Lr: 0.000100
2024-02-04 20:55:23,386 Validation result at epoch 1112, step    10000: duration: 29.4433s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00008	Translation Loss: 93878.45312	PPL: 12021.13477
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.95	(BLEU-1: 12.16,	BLEU-2: 4.17,	BLEU-3: 1.84,	BLEU-4: 0.95)
	CHRF 17.79	ROUGE 10.63
2024-02-04 20:55:23,388 Logging Recognition and Translation Outputs
2024-02-04 20:55:23,388 ========================================================================================================================
2024-02-04 20:55:23,389 Logging Sequence: 159_139.00
2024-02-04 20:55:23,389 	Gloss Reference :	A B+C+D+E
2024-02-04 20:55:23,389 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 20:55:23,389 	Gloss Alignment :	         
2024-02-04 20:55:23,390 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 20:55:23,391 	Text Reference  :	he took time and finally   was ready for the ********** asia    cup   where he     scored the  century 
2024-02-04 20:55:23,391 	Text Hypothesis :	** **** **** *** yesterday was ***** *** the semi-final between india and   hardik pandya were watching
2024-02-04 20:55:23,391 	Text Alignment  :	D  D    D    D   S             D     D       I          S       S     S     S      S      S    S       
2024-02-04 20:55:23,391 ========================================================================================================================
2024-02-04 20:55:23,391 Logging Sequence: 159_159.00
2024-02-04 20:55:23,392 	Gloss Reference :	A B+C+D+E
2024-02-04 20:55:23,392 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 20:55:23,392 	Gloss Alignment :	         
2024-02-04 20:55:23,392 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 20:55:23,394 	Text Reference  :	he said it wasn't easy the mind has   to  be     focussed and he   is  glad   that he is back in form with the asia cup century
2024-02-04 20:55:23,394 	Text Hypothesis :	** **** ** kohli  is   the **** first day people will     not find any takers and  he is **** ** **** **** *** **** *** *******
2024-02-04 20:55:23,394 	Text Alignment  :	D  D    D  S      S        D    S     S   S      S        S   S    S   S      S          D    D  D    D    D   D    D   D      
2024-02-04 20:55:23,395 ========================================================================================================================
2024-02-04 20:55:23,395 Logging Sequence: 103_8.00
2024-02-04 20:55:23,395 	Gloss Reference :	A B+C+D+E
2024-02-04 20:55:23,395 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 20:55:23,395 	Gloss Alignment :	         
2024-02-04 20:55:23,395 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 20:55:23,396 	Text Reference  :	were going on in    birmingham england from  28th july   to  8th     august 2022 
2024-02-04 20:55:23,396 	Text Hypothesis :	**** ***** ** since then       2022    india had  booked the british empire games
2024-02-04 20:55:23,396 	Text Alignment  :	D    D     D  S     S          S       S     S    S      S   S       S      S    
2024-02-04 20:55:23,396 ========================================================================================================================
2024-02-04 20:55:23,397 Logging Sequence: 164_546.00
2024-02-04 20:55:23,397 	Gloss Reference :	A B+C+D+E
2024-02-04 20:55:23,397 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 20:55:23,397 	Gloss Alignment :	         
2024-02-04 20:55:23,397 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 20:55:23,398 	Text Reference  :	reliance has turned  out    to    be the strongest company  
2024-02-04 20:55:23,398 	Text Hypothesis :	after    the british empire games it has been      postponed
2024-02-04 20:55:23,398 	Text Alignment  :	S        S   S       S      S     S  S   S         S        
2024-02-04 20:55:23,398 ========================================================================================================================
2024-02-04 20:55:23,398 Logging Sequence: 132_173.00
2024-02-04 20:55:23,399 	Gloss Reference :	A B+C+D+E
2024-02-04 20:55:23,399 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 20:55:23,399 	Gloss Alignment :	         
2024-02-04 20:55:23,399 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 20:55:23,400 	Text Reference  :	usman is ******** ** *** **** ***** australia' first muslim player  
2024-02-04 20:55:23,400 	Text Hypothesis :	he    is survived by his wife their son        and   a      daughter
2024-02-04 20:55:23,400 	Text Alignment  :	S        I        I  I   I    I     S          S     S      S       
2024-02-04 20:55:23,400 ========================================================================================================================
2024-02-04 20:55:32,525 Epoch 1112: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-04 20:55:32,526 EPOCH 1113
2024-02-04 20:55:46,368 Epoch 1113: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-04 20:55:46,368 EPOCH 1114
2024-02-04 20:56:00,230 Epoch 1114: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-04 20:56:00,231 EPOCH 1115
2024-02-04 20:56:14,017 Epoch 1115: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-04 20:56:14,018 EPOCH 1116
2024-02-04 20:56:28,001 Epoch 1116: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-04 20:56:28,002 EPOCH 1117
2024-02-04 20:56:41,607 Epoch 1117: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-04 20:56:41,607 EPOCH 1118
2024-02-04 20:56:55,712 Epoch 1118: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-04 20:56:55,713 EPOCH 1119
2024-02-04 20:57:09,789 Epoch 1119: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-04 20:57:09,789 EPOCH 1120
2024-02-04 20:57:23,490 Epoch 1120: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-04 20:57:23,490 EPOCH 1121
2024-02-04 20:57:37,588 Epoch 1121: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-04 20:57:37,588 EPOCH 1122
2024-02-04 20:57:51,643 Epoch 1122: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-04 20:57:51,644 EPOCH 1123
2024-02-04 20:57:56,463 [Epoch: 1123 Step: 00010100] Batch Recognition Loss:   0.000159 => Gls Tokens per Sec:      347 || Batch Translation Loss:   0.030053 => Txt Tokens per Sec:      902 || Lr: 0.000100
2024-02-04 20:58:05,761 Epoch 1123: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-04 20:58:05,762 EPOCH 1124
2024-02-04 20:58:19,748 Epoch 1124: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-04 20:58:19,749 EPOCH 1125
2024-02-04 20:58:33,696 Epoch 1125: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-04 20:58:33,696 EPOCH 1126
2024-02-04 20:58:47,730 Epoch 1126: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-04 20:58:47,731 EPOCH 1127
2024-02-04 20:59:01,402 Epoch 1127: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.83 
2024-02-04 20:59:01,402 EPOCH 1128
2024-02-04 20:59:15,608 Epoch 1128: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.56 
2024-02-04 20:59:15,608 EPOCH 1129
2024-02-04 20:59:29,668 Epoch 1129: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.67 
2024-02-04 20:59:29,669 EPOCH 1130
2024-02-04 20:59:43,747 Epoch 1130: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.75 
2024-02-04 20:59:43,748 EPOCH 1131
2024-02-04 20:59:57,396 Epoch 1131: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.69 
2024-02-04 20:59:57,396 EPOCH 1132
2024-02-04 21:00:11,615 Epoch 1132: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.58 
2024-02-04 21:00:11,616 EPOCH 1133
2024-02-04 21:00:25,233 Epoch 1133: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-04 21:00:25,234 EPOCH 1134
2024-02-04 21:00:32,249 [Epoch: 1134 Step: 00010200] Batch Recognition Loss:   0.000415 => Gls Tokens per Sec:      547 || Batch Translation Loss:   0.078397 => Txt Tokens per Sec:     1646 || Lr: 0.000100
2024-02-04 21:00:39,154 Epoch 1134: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-04 21:00:39,155 EPOCH 1135
2024-02-04 21:00:52,878 Epoch 1135: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.59 
2024-02-04 21:00:52,878 EPOCH 1136
2024-02-04 21:01:06,939 Epoch 1136: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.77 
2024-02-04 21:01:06,940 EPOCH 1137
2024-02-04 21:01:21,004 Epoch 1137: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.31 
2024-02-04 21:01:21,005 EPOCH 1138
2024-02-04 21:01:34,852 Epoch 1138: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.92 
2024-02-04 21:01:34,853 EPOCH 1139
2024-02-04 21:01:48,742 Epoch 1139: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.37 
2024-02-04 21:01:48,743 EPOCH 1140
2024-02-04 21:02:02,743 Epoch 1140: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.76 
2024-02-04 21:02:02,744 EPOCH 1141
2024-02-04 21:02:16,295 Epoch 1141: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.18 
2024-02-04 21:02:16,295 EPOCH 1142
2024-02-04 21:02:29,968 Epoch 1142: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.93 
2024-02-04 21:02:29,969 EPOCH 1143
2024-02-04 21:02:43,925 Epoch 1143: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.96 
2024-02-04 21:02:43,926 EPOCH 1144
2024-02-04 21:02:57,727 Epoch 1144: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.91 
2024-02-04 21:02:57,728 EPOCH 1145
2024-02-04 21:02:59,289 [Epoch: 1145 Step: 00010300] Batch Recognition Loss:   0.000286 => Gls Tokens per Sec:     3282 || Batch Translation Loss:   0.051484 => Txt Tokens per Sec:     8574 || Lr: 0.000100
2024-02-04 21:03:11,798 Epoch 1145: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.89 
2024-02-04 21:03:11,798 EPOCH 1146
2024-02-04 21:03:25,549 Epoch 1146: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.94 
2024-02-04 21:03:25,549 EPOCH 1147
2024-02-04 21:03:39,501 Epoch 1147: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.78 
2024-02-04 21:03:39,502 EPOCH 1148
2024-02-04 21:03:53,363 Epoch 1148: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.83 
2024-02-04 21:03:53,363 EPOCH 1149
2024-02-04 21:04:07,010 Epoch 1149: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.75 
2024-02-04 21:04:07,011 EPOCH 1150
2024-02-04 21:04:21,073 Epoch 1150: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.69 
2024-02-04 21:04:21,073 EPOCH 1151
2024-02-04 21:04:34,804 Epoch 1151: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-04 21:04:34,805 EPOCH 1152
2024-02-04 21:04:48,865 Epoch 1152: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-04 21:04:48,866 EPOCH 1153
2024-02-04 21:05:02,840 Epoch 1153: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-04 21:05:02,840 EPOCH 1154
2024-02-04 21:05:16,600 Epoch 1154: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-04 21:05:16,600 EPOCH 1155
2024-02-04 21:05:30,703 Epoch 1155: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-04 21:05:30,703 EPOCH 1156
2024-02-04 21:05:38,999 [Epoch: 1156 Step: 00010400] Batch Recognition Loss:   0.000225 => Gls Tokens per Sec:      772 || Batch Translation Loss:   0.019433 => Txt Tokens per Sec:     2160 || Lr: 0.000100
2024-02-04 21:05:44,848 Epoch 1156: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-04 21:05:44,849 EPOCH 1157
2024-02-04 21:05:58,644 Epoch 1157: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-04 21:05:58,644 EPOCH 1158
2024-02-04 21:06:12,352 Epoch 1158: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-04 21:06:12,353 EPOCH 1159
2024-02-04 21:06:26,363 Epoch 1159: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-04 21:06:26,363 EPOCH 1160
2024-02-04 21:06:40,248 Epoch 1160: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-04 21:06:40,248 EPOCH 1161
2024-02-04 21:06:54,062 Epoch 1161: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-04 21:06:54,063 EPOCH 1162
2024-02-04 21:07:07,984 Epoch 1162: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-04 21:07:07,984 EPOCH 1163
2024-02-04 21:07:21,942 Epoch 1163: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-04 21:07:21,943 EPOCH 1164
2024-02-04 21:07:35,939 Epoch 1164: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-04 21:07:35,939 EPOCH 1165
2024-02-04 21:07:49,989 Epoch 1165: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-04 21:07:49,990 EPOCH 1166
2024-02-04 21:08:03,652 Epoch 1166: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-04 21:08:03,653 EPOCH 1167
2024-02-04 21:08:16,582 [Epoch: 1167 Step: 00010500] Batch Recognition Loss:   0.000173 => Gls Tokens per Sec:      525 || Batch Translation Loss:   0.020456 => Txt Tokens per Sec:     1587 || Lr: 0.000100
2024-02-04 21:08:17,653 Epoch 1167: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-04 21:08:17,653 EPOCH 1168
2024-02-04 21:08:31,521 Epoch 1168: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-04 21:08:31,522 EPOCH 1169
2024-02-04 21:08:45,234 Epoch 1169: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-04 21:08:45,234 EPOCH 1170
2024-02-04 21:08:59,157 Epoch 1170: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-04 21:08:59,158 EPOCH 1171
2024-02-04 21:09:13,025 Epoch 1171: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-04 21:09:13,025 EPOCH 1172
2024-02-04 21:09:27,052 Epoch 1172: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-04 21:09:27,052 EPOCH 1173
2024-02-04 21:09:40,711 Epoch 1173: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-04 21:09:40,712 EPOCH 1174
2024-02-04 21:09:54,437 Epoch 1174: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-04 21:09:54,437 EPOCH 1175
2024-02-04 21:10:08,589 Epoch 1175: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-04 21:10:08,590 EPOCH 1176
2024-02-04 21:10:22,311 Epoch 1176: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-04 21:10:22,312 EPOCH 1177
2024-02-04 21:10:36,382 Epoch 1177: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-04 21:10:36,382 EPOCH 1178
2024-02-04 21:10:49,386 [Epoch: 1178 Step: 00010600] Batch Recognition Loss:   0.000140 => Gls Tokens per Sec:      621 || Batch Translation Loss:   0.020724 => Txt Tokens per Sec:     1736 || Lr: 0.000100
2024-02-04 21:10:50,412 Epoch 1178: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-04 21:10:50,412 EPOCH 1179
2024-02-04 21:11:04,313 Epoch 1179: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-04 21:11:04,313 EPOCH 1180
2024-02-04 21:11:18,164 Epoch 1180: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-04 21:11:18,164 EPOCH 1181
2024-02-04 21:11:32,043 Epoch 1181: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-04 21:11:32,044 EPOCH 1182
2024-02-04 21:11:46,058 Epoch 1182: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-04 21:11:46,059 EPOCH 1183
2024-02-04 21:11:59,701 Epoch 1183: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-04 21:11:59,702 EPOCH 1184
2024-02-04 21:12:13,591 Epoch 1184: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-04 21:12:13,591 EPOCH 1185
2024-02-04 21:12:27,263 Epoch 1185: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-04 21:12:27,264 EPOCH 1186
2024-02-04 21:12:41,284 Epoch 1186: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-04 21:12:41,285 EPOCH 1187
2024-02-04 21:12:55,190 Epoch 1187: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-04 21:12:55,190 EPOCH 1188
2024-02-04 21:13:09,309 Epoch 1188: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-04 21:13:09,310 EPOCH 1189
2024-02-04 21:13:22,917 [Epoch: 1189 Step: 00010700] Batch Recognition Loss:   0.000143 => Gls Tokens per Sec:      687 || Batch Translation Loss:   0.075714 => Txt Tokens per Sec:     1934 || Lr: 0.000100
2024-02-04 21:13:23,285 Epoch 1189: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-04 21:13:23,285 EPOCH 1190
2024-02-04 21:13:37,229 Epoch 1190: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-04 21:13:37,230 EPOCH 1191
2024-02-04 21:13:50,877 Epoch 1191: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-04 21:13:50,878 EPOCH 1192
2024-02-04 21:14:04,614 Epoch 1192: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-04 21:14:04,614 EPOCH 1193
2024-02-04 21:14:18,729 Epoch 1193: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-04 21:14:18,730 EPOCH 1194
2024-02-04 21:14:32,794 Epoch 1194: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-04 21:14:32,795 EPOCH 1195
2024-02-04 21:14:46,278 Epoch 1195: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-04 21:14:46,278 EPOCH 1196
2024-02-04 21:15:00,435 Epoch 1196: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-04 21:15:00,436 EPOCH 1197
2024-02-04 21:15:14,147 Epoch 1197: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-04 21:15:14,148 EPOCH 1198
2024-02-04 21:15:27,878 Epoch 1198: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-04 21:15:27,879 EPOCH 1199
2024-02-04 21:15:42,229 Epoch 1199: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-04 21:15:42,229 EPOCH 1200
2024-02-04 21:15:56,013 [Epoch: 1200 Step: 00010800] Batch Recognition Loss:   0.000132 => Gls Tokens per Sec:      771 || Batch Translation Loss:   0.007933 => Txt Tokens per Sec:     2141 || Lr: 0.000100
2024-02-04 21:15:56,014 Epoch 1200: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-04 21:15:56,014 EPOCH 1201
2024-02-04 21:16:09,981 Epoch 1201: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-04 21:16:09,982 EPOCH 1202
2024-02-04 21:16:23,466 Epoch 1202: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-04 21:16:23,467 EPOCH 1203
2024-02-04 21:16:37,158 Epoch 1203: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-04 21:16:37,158 EPOCH 1204
2024-02-04 21:16:50,928 Epoch 1204: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-04 21:16:50,928 EPOCH 1205
2024-02-04 21:17:04,518 Epoch 1205: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-04 21:17:04,519 EPOCH 1206
2024-02-04 21:17:18,712 Epoch 1206: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-04 21:17:18,713 EPOCH 1207
2024-02-04 21:17:32,482 Epoch 1207: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-04 21:17:32,482 EPOCH 1208
2024-02-04 21:17:46,504 Epoch 1208: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-04 21:17:46,505 EPOCH 1209
2024-02-04 21:18:00,487 Epoch 1209: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.66 
2024-02-04 21:18:00,488 EPOCH 1210
2024-02-04 21:18:14,649 Epoch 1210: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.00 
2024-02-04 21:18:14,649 EPOCH 1211
2024-02-04 21:18:28,827 Epoch 1211: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.21 
2024-02-04 21:18:28,827 EPOCH 1212
2024-02-04 21:18:29,139 [Epoch: 1212 Step: 00010900] Batch Recognition Loss:   0.000255 => Gls Tokens per Sec:     4116 || Batch Translation Loss:   0.054064 => Txt Tokens per Sec:     9524 || Lr: 0.000100
2024-02-04 21:18:42,912 Epoch 1212: Total Training Recognition Loss 0.00  Total Training Translation Loss 2.23 
2024-02-04 21:18:42,913 EPOCH 1213
2024-02-04 21:18:56,781 Epoch 1213: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.83 
2024-02-04 21:18:56,781 EPOCH 1214
2024-02-04 21:19:10,867 Epoch 1214: Total Training Recognition Loss 0.01  Total Training Translation Loss 5.39 
2024-02-04 21:19:10,867 EPOCH 1215
2024-02-04 21:19:24,832 Epoch 1215: Total Training Recognition Loss 0.02  Total Training Translation Loss 8.83 
2024-02-04 21:19:24,833 EPOCH 1216
2024-02-04 21:19:38,885 Epoch 1216: Total Training Recognition Loss 0.01  Total Training Translation Loss 5.34 
2024-02-04 21:19:38,885 EPOCH 1217
2024-02-04 21:19:52,663 Epoch 1217: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.29 
2024-02-04 21:19:52,664 EPOCH 1218
2024-02-04 21:20:06,388 Epoch 1218: Total Training Recognition Loss 0.03  Total Training Translation Loss 10.00 
2024-02-04 21:20:06,388 EPOCH 1219
2024-02-04 21:20:20,411 Epoch 1219: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.14 
2024-02-04 21:20:20,411 EPOCH 1220
2024-02-04 21:20:34,204 Epoch 1220: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.63 
2024-02-04 21:20:34,205 EPOCH 1221
2024-02-04 21:20:48,258 Epoch 1221: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.07 
2024-02-04 21:20:48,258 EPOCH 1222
2024-02-04 21:21:02,234 Epoch 1222: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-04 21:21:02,235 EPOCH 1223
2024-02-04 21:21:04,422 [Epoch: 1223 Step: 00011000] Batch Recognition Loss:   0.000456 => Gls Tokens per Sec:     1170 || Batch Translation Loss:   0.062044 => Txt Tokens per Sec:     3474 || Lr: 0.000100
2024-02-04 21:21:15,912 Epoch 1223: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-04 21:21:15,912 EPOCH 1224
2024-02-04 21:21:30,107 Epoch 1224: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-04 21:21:30,108 EPOCH 1225
2024-02-04 21:21:44,079 Epoch 1225: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-04 21:21:44,080 EPOCH 1226
2024-02-04 21:21:57,771 Epoch 1226: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-04 21:21:57,772 EPOCH 1227
2024-02-04 21:22:11,581 Epoch 1227: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-04 21:22:11,582 EPOCH 1228
2024-02-04 21:22:25,719 Epoch 1228: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-04 21:22:25,720 EPOCH 1229
2024-02-04 21:22:39,389 Epoch 1229: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-04 21:22:39,389 EPOCH 1230
2024-02-04 21:22:53,606 Epoch 1230: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-04 21:22:53,607 EPOCH 1231
2024-02-04 21:23:07,473 Epoch 1231: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-04 21:23:07,474 EPOCH 1232
2024-02-04 21:23:21,282 Epoch 1232: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-04 21:23:21,282 EPOCH 1233
2024-02-04 21:23:35,223 Epoch 1233: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-04 21:23:35,224 EPOCH 1234
2024-02-04 21:23:41,575 [Epoch: 1234 Step: 00011100] Batch Recognition Loss:   0.000328 => Gls Tokens per Sec:      465 || Batch Translation Loss:   0.033285 => Txt Tokens per Sec:     1454 || Lr: 0.000100
2024-02-04 21:23:48,946 Epoch 1234: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-04 21:23:48,947 EPOCH 1235
2024-02-04 21:24:02,805 Epoch 1235: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-04 21:24:02,806 EPOCH 1236
2024-02-04 21:24:16,546 Epoch 1236: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-04 21:24:16,546 EPOCH 1237
2024-02-04 21:24:30,427 Epoch 1237: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-04 21:24:30,428 EPOCH 1238
2024-02-04 21:24:44,660 Epoch 1238: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-04 21:24:44,660 EPOCH 1239
2024-02-04 21:24:58,363 Epoch 1239: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-04 21:24:58,363 EPOCH 1240
2024-02-04 21:25:12,335 Epoch 1240: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-04 21:25:12,336 EPOCH 1241
2024-02-04 21:25:25,976 Epoch 1241: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-04 21:25:25,977 EPOCH 1242
2024-02-04 21:25:39,950 Epoch 1242: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-04 21:25:39,950 EPOCH 1243
2024-02-04 21:25:53,835 Epoch 1243: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-04 21:25:53,835 EPOCH 1244
2024-02-04 21:26:07,756 Epoch 1244: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-04 21:26:07,756 EPOCH 1245
2024-02-04 21:26:10,729 [Epoch: 1245 Step: 00011200] Batch Recognition Loss:   0.000220 => Gls Tokens per Sec:     1723 || Batch Translation Loss:   0.025438 => Txt Tokens per Sec:     4439 || Lr: 0.000100
2024-02-04 21:26:21,653 Epoch 1245: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-04 21:26:21,654 EPOCH 1246
2024-02-04 21:26:35,503 Epoch 1246: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-04 21:26:35,503 EPOCH 1247
2024-02-04 21:26:49,412 Epoch 1247: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-04 21:26:49,413 EPOCH 1248
2024-02-04 21:27:03,108 Epoch 1248: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-04 21:27:03,108 EPOCH 1249
2024-02-04 21:27:17,102 Epoch 1249: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-04 21:27:17,102 EPOCH 1250
2024-02-04 21:27:31,223 Epoch 1250: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-04 21:27:31,224 EPOCH 1251
2024-02-04 21:27:45,102 Epoch 1251: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-04 21:27:45,103 EPOCH 1252
2024-02-04 21:27:59,259 Epoch 1252: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-04 21:27:59,260 EPOCH 1253
2024-02-04 21:28:13,422 Epoch 1253: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-04 21:28:13,422 EPOCH 1254
2024-02-04 21:28:27,266 Epoch 1254: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-04 21:28:27,267 EPOCH 1255
2024-02-04 21:28:41,148 Epoch 1255: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-04 21:28:41,148 EPOCH 1256
2024-02-04 21:28:48,239 [Epoch: 1256 Step: 00011300] Batch Recognition Loss:   0.000179 => Gls Tokens per Sec:      903 || Batch Translation Loss:   0.019442 => Txt Tokens per Sec:     2443 || Lr: 0.000100
2024-02-04 21:28:55,210 Epoch 1256: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-04 21:28:55,211 EPOCH 1257
2024-02-04 21:29:08,765 Epoch 1257: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-04 21:29:08,766 EPOCH 1258
2024-02-04 21:29:22,647 Epoch 1258: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-04 21:29:22,647 EPOCH 1259
2024-02-04 21:29:36,493 Epoch 1259: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-04 21:29:36,493 EPOCH 1260
2024-02-04 21:29:50,616 Epoch 1260: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-04 21:29:50,617 EPOCH 1261
2024-02-04 21:30:04,194 Epoch 1261: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-04 21:30:04,195 EPOCH 1262
2024-02-04 21:30:18,174 Epoch 1262: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-04 21:30:18,175 EPOCH 1263
2024-02-04 21:30:31,997 Epoch 1263: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-04 21:30:31,998 EPOCH 1264
2024-02-04 21:30:46,272 Epoch 1264: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-04 21:30:46,273 EPOCH 1265
2024-02-04 21:31:00,360 Epoch 1265: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-04 21:31:00,360 EPOCH 1266
2024-02-04 21:31:14,334 Epoch 1266: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-04 21:31:14,334 EPOCH 1267
2024-02-04 21:31:26,694 [Epoch: 1267 Step: 00011400] Batch Recognition Loss:   0.000106 => Gls Tokens per Sec:      549 || Batch Translation Loss:   0.007551 => Txt Tokens per Sec:     1611 || Lr: 0.000100
2024-02-04 21:31:28,142 Epoch 1267: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-04 21:31:28,142 EPOCH 1268
2024-02-04 21:31:42,091 Epoch 1268: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-04 21:31:42,091 EPOCH 1269
2024-02-04 21:31:56,216 Epoch 1269: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-04 21:31:56,217 EPOCH 1270
2024-02-04 21:32:10,393 Epoch 1270: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-04 21:32:10,394 EPOCH 1271
2024-02-04 21:32:24,275 Epoch 1271: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-04 21:32:24,276 EPOCH 1272
2024-02-04 21:32:38,007 Epoch 1272: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-04 21:32:38,007 EPOCH 1273
2024-02-04 21:32:51,879 Epoch 1273: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-04 21:32:51,880 EPOCH 1274
2024-02-04 21:33:05,705 Epoch 1274: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-04 21:33:05,706 EPOCH 1275
2024-02-04 21:33:19,861 Epoch 1275: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-04 21:33:19,861 EPOCH 1276
2024-02-04 21:33:33,708 Epoch 1276: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-04 21:33:33,708 EPOCH 1277
2024-02-04 21:33:47,823 Epoch 1277: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-04 21:33:47,823 EPOCH 1278
2024-02-04 21:33:55,589 [Epoch: 1278 Step: 00011500] Batch Recognition Loss:   0.000131 => Gls Tokens per Sec:     1154 || Batch Translation Loss:   0.013904 => Txt Tokens per Sec:     3096 || Lr: 0.000100
2024-02-04 21:34:01,576 Epoch 1278: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-04 21:34:01,577 EPOCH 1279
2024-02-04 21:34:15,579 Epoch 1279: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-04 21:34:15,580 EPOCH 1280
2024-02-04 21:34:29,473 Epoch 1280: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-04 21:34:29,473 EPOCH 1281
2024-02-04 21:34:43,523 Epoch 1281: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-04 21:34:43,524 EPOCH 1282
2024-02-04 21:34:57,416 Epoch 1282: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-04 21:34:57,417 EPOCH 1283
2024-02-04 21:35:11,211 Epoch 1283: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-04 21:35:11,211 EPOCH 1284
2024-02-04 21:35:25,284 Epoch 1284: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-04 21:35:25,285 EPOCH 1285
2024-02-04 21:35:39,170 Epoch 1285: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-04 21:35:39,170 EPOCH 1286
2024-02-04 21:35:53,209 Epoch 1286: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-04 21:35:53,209 EPOCH 1287
2024-02-04 21:36:07,138 Epoch 1287: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-04 21:36:07,139 EPOCH 1288
2024-02-04 21:36:21,249 Epoch 1288: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-04 21:36:21,250 EPOCH 1289
2024-02-04 21:36:30,263 [Epoch: 1289 Step: 00011600] Batch Recognition Loss:   0.000199 => Gls Tokens per Sec:     1038 || Batch Translation Loss:   0.076654 => Txt Tokens per Sec:     2777 || Lr: 0.000100
2024-02-04 21:36:35,287 Epoch 1289: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.75 
2024-02-04 21:36:35,288 EPOCH 1290
2024-02-04 21:36:49,394 Epoch 1290: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.66 
2024-02-04 21:36:49,394 EPOCH 1291
2024-02-04 21:37:03,367 Epoch 1291: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.57 
2024-02-04 21:37:03,367 EPOCH 1292
2024-02-04 21:37:17,353 Epoch 1292: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-04 21:37:17,353 EPOCH 1293
2024-02-04 21:37:31,371 Epoch 1293: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-04 21:37:31,372 EPOCH 1294
2024-02-04 21:37:45,532 Epoch 1294: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-04 21:37:45,533 EPOCH 1295
2024-02-04 21:37:59,311 Epoch 1295: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-04 21:37:59,312 EPOCH 1296
2024-02-04 21:38:13,647 Epoch 1296: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-04 21:38:13,648 EPOCH 1297
2024-02-04 21:38:27,264 Epoch 1297: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-04 21:38:27,265 EPOCH 1298
2024-02-04 21:38:41,293 Epoch 1298: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-04 21:38:41,294 EPOCH 1299
2024-02-04 21:38:55,044 Epoch 1299: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-04 21:38:55,044 EPOCH 1300
2024-02-04 21:39:08,944 [Epoch: 1300 Step: 00011700] Batch Recognition Loss:   0.000143 => Gls Tokens per Sec:      765 || Batch Translation Loss:   0.020966 => Txt Tokens per Sec:     2123 || Lr: 0.000100
2024-02-04 21:39:08,945 Epoch 1300: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-04 21:39:08,945 EPOCH 1301
2024-02-04 21:39:22,827 Epoch 1301: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-04 21:39:22,827 EPOCH 1302
2024-02-04 21:39:36,655 Epoch 1302: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-04 21:39:36,655 EPOCH 1303
2024-02-04 21:39:50,603 Epoch 1303: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-04 21:39:50,604 EPOCH 1304
2024-02-04 21:40:04,370 Epoch 1304: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-04 21:40:04,370 EPOCH 1305
2024-02-04 21:40:18,206 Epoch 1305: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-04 21:40:18,207 EPOCH 1306
2024-02-04 21:40:32,007 Epoch 1306: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-04 21:40:32,007 EPOCH 1307
2024-02-04 21:40:46,243 Epoch 1307: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-04 21:40:46,244 EPOCH 1308
2024-02-04 21:41:00,218 Epoch 1308: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-04 21:41:00,218 EPOCH 1309
2024-02-04 21:41:13,628 Epoch 1309: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-04 21:41:13,628 EPOCH 1310
2024-02-04 21:41:27,607 Epoch 1310: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-04 21:41:27,608 EPOCH 1311
2024-02-04 21:41:41,360 Epoch 1311: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-04 21:41:41,361 EPOCH 1312
2024-02-04 21:41:42,009 [Epoch: 1312 Step: 00011800] Batch Recognition Loss:   0.000178 => Gls Tokens per Sec:     1978 || Batch Translation Loss:   0.031671 => Txt Tokens per Sec:     5938 || Lr: 0.000100
2024-02-04 21:41:55,397 Epoch 1312: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-04 21:41:55,398 EPOCH 1313
2024-02-04 21:42:09,049 Epoch 1313: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-04 21:42:09,050 EPOCH 1314
2024-02-04 21:42:22,888 Epoch 1314: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-04 21:42:22,888 EPOCH 1315
2024-02-04 21:42:36,948 Epoch 1315: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-04 21:42:36,948 EPOCH 1316
2024-02-04 21:42:50,612 Epoch 1316: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-04 21:42:50,613 EPOCH 1317
2024-02-04 21:43:04,693 Epoch 1317: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-04 21:43:04,693 EPOCH 1318
2024-02-04 21:43:18,407 Epoch 1318: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-04 21:43:18,408 EPOCH 1319
2024-02-04 21:43:32,247 Epoch 1319: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-04 21:43:32,248 EPOCH 1320
2024-02-04 21:43:46,217 Epoch 1320: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-04 21:43:46,218 EPOCH 1321
2024-02-04 21:43:59,961 Epoch 1321: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-04 21:43:59,961 EPOCH 1322
2024-02-04 21:44:13,771 Epoch 1322: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-04 21:44:13,772 EPOCH 1323
2024-02-04 21:44:15,899 [Epoch: 1323 Step: 00011900] Batch Recognition Loss:   0.000127 => Gls Tokens per Sec:     1203 || Batch Translation Loss:   0.015677 => Txt Tokens per Sec:     3565 || Lr: 0.000100
2024-02-04 21:44:27,733 Epoch 1323: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-04 21:44:27,734 EPOCH 1324
2024-02-04 21:44:41,555 Epoch 1324: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-04 21:44:41,556 EPOCH 1325
2024-02-04 21:44:55,389 Epoch 1325: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-04 21:44:55,390 EPOCH 1326
2024-02-04 21:45:09,503 Epoch 1326: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-04 21:45:09,504 EPOCH 1327
2024-02-04 21:45:23,223 Epoch 1327: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-04 21:45:23,224 EPOCH 1328
2024-02-04 21:45:37,046 Epoch 1328: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-04 21:45:37,046 EPOCH 1329
2024-02-04 21:45:51,061 Epoch 1329: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-04 21:45:51,062 EPOCH 1330
2024-02-04 21:46:04,993 Epoch 1330: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-04 21:46:04,994 EPOCH 1331
2024-02-04 21:46:19,297 Epoch 1331: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-04 21:46:19,297 EPOCH 1332
2024-02-04 21:46:33,232 Epoch 1332: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-04 21:46:33,233 EPOCH 1333
2024-02-04 21:46:46,931 Epoch 1333: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-04 21:46:46,932 EPOCH 1334
2024-02-04 21:46:53,823 [Epoch: 1334 Step: 00012000] Batch Recognition Loss:   0.000165 => Gls Tokens per Sec:      428 || Batch Translation Loss:   0.030633 => Txt Tokens per Sec:     1355 || Lr: 0.000100
2024-02-04 21:47:22,829 Validation result at epoch 1334, step    12000: duration: 29.0066s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00016	Translation Loss: 91902.53125	PPL: 9864.42871
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 1.07	(BLEU-1: 10.93,	BLEU-2: 3.80,	BLEU-3: 1.87,	BLEU-4: 1.07)
	CHRF 17.41	ROUGE 9.55
2024-02-04 21:47:22,831 Logging Recognition and Translation Outputs
2024-02-04 21:47:22,831 ========================================================================================================================
2024-02-04 21:47:22,831 Logging Sequence: 177_50.00
2024-02-04 21:47:22,831 	Gloss Reference :	A B+C+D+E
2024-02-04 21:47:22,831 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 21:47:22,832 	Gloss Alignment :	         
2024-02-04 21:47:22,832 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 21:47:22,833 	Text Reference  :	a similar reward of    rs    50000  was announced    for    information against his associate ajay   kumar
2024-02-04 21:47:22,833 	Text Hypothesis :	* after   the    delhi court issued a   non-bailable arrest warrant     against *** ********* sushil kumar
2024-02-04 21:47:22,833 	Text Alignment  :	D S       S      S     S     S      S   S            S      S                   D   D         S           
2024-02-04 21:47:22,833 ========================================================================================================================
2024-02-04 21:47:22,834 Logging Sequence: 122_86.00
2024-02-04 21:47:22,834 	Gloss Reference :	A B+C+D+E
2024-02-04 21:47:22,834 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 21:47:22,834 	Gloss Alignment :	         
2024-02-04 21:47:22,834 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 21:47:22,835 	Text Reference  :	after winning chanu spoke to  the media and said  
2024-02-04 21:47:22,835 	Text Hypothesis :	after ******* ***** india won the 1st   odi series
2024-02-04 21:47:22,835 	Text Alignment  :	      D       D     S     S       S     S   S     
2024-02-04 21:47:22,835 ========================================================================================================================
2024-02-04 21:47:22,835 Logging Sequence: 165_27.00
2024-02-04 21:47:22,835 	Gloss Reference :	A B+C+D+E
2024-02-04 21:47:22,836 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 21:47:22,836 	Gloss Alignment :	         
2024-02-04 21:47:22,836 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 21:47:22,837 	Text Reference  :	so then they change their routes some people believe in   this while some don't
2024-02-04 21:47:22,837 	Text Hypothesis :	** **** **** ****** ***** ****** many teams  will    have been from  6    balls
2024-02-04 21:47:22,837 	Text Alignment  :	D  D    D    D      D     D      S    S      S       S    S    S     S    S    
2024-02-04 21:47:22,837 ========================================================================================================================
2024-02-04 21:47:22,837 Logging Sequence: 70_65.00
2024-02-04 21:47:22,838 	Gloss Reference :	A B+C+D+E
2024-02-04 21:47:22,838 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 21:47:22,838 	Gloss Alignment :	         
2024-02-04 21:47:22,838 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 21:47:22,839 	Text Reference  :	during the press conference a    table was placed in        front of the ***** media   
2024-02-04 21:47:22,839 	Text Hypothesis :	****** the ***** ********** euro 2020  has been   postponed due   to the covid pandemic
2024-02-04 21:47:22,839 	Text Alignment  :	D          D     D          S    S     S   S      S         S     S      I     S       
2024-02-04 21:47:22,839 ========================================================================================================================
2024-02-04 21:47:22,839 Logging Sequence: 149_65.00
2024-02-04 21:47:22,840 	Gloss Reference :	A B+C+D+E
2024-02-04 21:47:22,840 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 21:47:22,840 	Gloss Alignment :	         
2024-02-04 21:47:22,840 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 21:47:22,842 	Text Reference  :	at 6am on 6th november 2022  the police reached sri  lankan   team's hotel  in          sydney australia's central business district cbd  
2024-02-04 21:47:22,842 	Text Hypothesis :	** *** ** *** ******** after the woman  alleged that danushka had    sexual intercourse with   her         without any      sporting arena
2024-02-04 21:47:22,842 	Text Alignment  :	D  D   D  D   D        S         S      S       S    S        S      S      S           S      S           S       S        S        S    
2024-02-04 21:47:22,842 ========================================================================================================================
2024-02-04 21:47:30,295 Epoch 1334: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.55 
2024-02-04 21:47:30,296 EPOCH 1335
2024-02-04 21:47:44,280 Epoch 1335: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-04 21:47:44,281 EPOCH 1336
2024-02-04 21:47:58,013 Epoch 1336: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-04 21:47:58,013 EPOCH 1337
2024-02-04 21:48:11,930 Epoch 1337: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-04 21:48:11,930 EPOCH 1338
2024-02-04 21:48:26,079 Epoch 1338: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-04 21:48:26,079 EPOCH 1339
2024-02-04 21:48:39,991 Epoch 1339: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-04 21:48:39,992 EPOCH 1340
2024-02-04 21:48:54,094 Epoch 1340: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-04 21:48:54,094 EPOCH 1341
2024-02-04 21:49:07,861 Epoch 1341: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-04 21:49:07,861 EPOCH 1342
2024-02-04 21:49:21,462 Epoch 1342: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-04 21:49:21,463 EPOCH 1343
2024-02-04 21:49:35,327 Epoch 1343: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-04 21:49:35,328 EPOCH 1344
2024-02-04 21:49:49,097 Epoch 1344: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-04 21:49:49,097 EPOCH 1345
2024-02-04 21:49:59,393 [Epoch: 1345 Step: 00012100] Batch Recognition Loss:   0.000184 => Gls Tokens per Sec:      411 || Batch Translation Loss:   0.034802 => Txt Tokens per Sec:     1156 || Lr: 0.000100
2024-02-04 21:50:03,156 Epoch 1345: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-04 21:50:03,156 EPOCH 1346
2024-02-04 21:50:17,075 Epoch 1346: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-04 21:50:17,076 EPOCH 1347
2024-02-04 21:50:30,958 Epoch 1347: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-04 21:50:30,958 EPOCH 1348
2024-02-04 21:50:44,675 Epoch 1348: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-04 21:50:44,676 EPOCH 1349
2024-02-04 21:50:58,722 Epoch 1349: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-04 21:50:58,722 EPOCH 1350
2024-02-04 21:51:12,188 Epoch 1350: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-04 21:51:12,188 EPOCH 1351
2024-02-04 21:51:26,218 Epoch 1351: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-04 21:51:26,219 EPOCH 1352
2024-02-04 21:51:40,147 Epoch 1352: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-04 21:51:40,147 EPOCH 1353
2024-02-04 21:51:54,028 Epoch 1353: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-04 21:51:54,028 EPOCH 1354
2024-02-04 21:52:07,789 Epoch 1354: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-04 21:52:07,790 EPOCH 1355
2024-02-04 21:52:21,719 Epoch 1355: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-04 21:52:21,720 EPOCH 1356
2024-02-04 21:52:32,625 [Epoch: 1356 Step: 00012200] Batch Recognition Loss:   0.000124 => Gls Tokens per Sec:      505 || Batch Translation Loss:   0.014803 => Txt Tokens per Sec:     1488 || Lr: 0.000100
2024-02-04 21:52:35,578 Epoch 1356: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-04 21:52:35,578 EPOCH 1357
2024-02-04 21:52:49,608 Epoch 1357: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-04 21:52:49,608 EPOCH 1358
2024-02-04 21:53:03,380 Epoch 1358: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-04 21:53:03,380 EPOCH 1359
2024-02-04 21:53:17,540 Epoch 1359: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-04 21:53:17,540 EPOCH 1360
2024-02-04 21:53:31,396 Epoch 1360: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-04 21:53:31,396 EPOCH 1361
2024-02-04 21:53:45,420 Epoch 1361: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-04 21:53:45,420 EPOCH 1362
2024-02-04 21:53:59,171 Epoch 1362: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-04 21:53:59,172 EPOCH 1363
2024-02-04 21:54:13,173 Epoch 1363: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-04 21:54:13,173 EPOCH 1364
2024-02-04 21:54:27,125 Epoch 1364: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-04 21:54:27,125 EPOCH 1365
2024-02-04 21:54:40,973 Epoch 1365: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-04 21:54:40,974 EPOCH 1366
2024-02-04 21:54:54,995 Epoch 1366: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-04 21:54:54,995 EPOCH 1367
2024-02-04 21:55:02,153 [Epoch: 1367 Step: 00012300] Batch Recognition Loss:   0.000165 => Gls Tokens per Sec:     1073 || Batch Translation Loss:   0.049966 => Txt Tokens per Sec:     2912 || Lr: 0.000100
2024-02-04 21:55:08,953 Epoch 1367: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-04 21:55:08,954 EPOCH 1368
2024-02-04 21:55:22,756 Epoch 1368: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.63 
2024-02-04 21:55:22,757 EPOCH 1369
2024-02-04 21:55:36,601 Epoch 1369: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-04 21:55:36,602 EPOCH 1370
2024-02-04 21:55:50,762 Epoch 1370: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-04 21:55:50,763 EPOCH 1371
2024-02-04 21:56:04,460 Epoch 1371: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-04 21:56:04,460 EPOCH 1372
2024-02-04 21:56:18,702 Epoch 1372: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-04 21:56:18,702 EPOCH 1373
2024-02-04 21:56:32,731 Epoch 1373: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-04 21:56:32,731 EPOCH 1374
2024-02-04 21:56:46,789 Epoch 1374: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-04 21:56:46,790 EPOCH 1375
2024-02-04 21:57:00,896 Epoch 1375: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-04 21:57:00,896 EPOCH 1376
2024-02-04 21:57:14,875 Epoch 1376: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-04 21:57:14,876 EPOCH 1377
2024-02-04 21:57:29,033 Epoch 1377: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-04 21:57:29,033 EPOCH 1378
2024-02-04 21:57:42,237 [Epoch: 1378 Step: 00012400] Batch Recognition Loss:   0.000323 => Gls Tokens per Sec:      611 || Batch Translation Loss:   0.050614 => Txt Tokens per Sec:     1710 || Lr: 0.000100
2024-02-04 21:57:43,165 Epoch 1378: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-04 21:57:43,165 EPOCH 1379
2024-02-04 21:57:57,088 Epoch 1379: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-04 21:57:57,088 EPOCH 1380
2024-02-04 21:58:10,680 Epoch 1380: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-04 21:58:10,681 EPOCH 1381
2024-02-04 21:58:24,705 Epoch 1381: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-04 21:58:24,705 EPOCH 1382
2024-02-04 21:58:38,532 Epoch 1382: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-04 21:58:38,532 EPOCH 1383
2024-02-04 21:58:52,201 Epoch 1383: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-04 21:58:52,202 EPOCH 1384
2024-02-04 21:59:06,485 Epoch 1384: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-04 21:59:06,485 EPOCH 1385
2024-02-04 21:59:20,335 Epoch 1385: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-04 21:59:20,336 EPOCH 1386
2024-02-04 21:59:34,085 Epoch 1386: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-04 21:59:34,086 EPOCH 1387
2024-02-04 21:59:47,909 Epoch 1387: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-04 21:59:47,909 EPOCH 1388
2024-02-04 22:00:01,793 Epoch 1388: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-04 22:00:01,794 EPOCH 1389
2024-02-04 22:00:15,645 [Epoch: 1389 Step: 00012500] Batch Recognition Loss:   0.000183 => Gls Tokens per Sec:      675 || Batch Translation Loss:   0.043963 => Txt Tokens per Sec:     1919 || Lr: 0.000100
2024-02-04 22:00:16,005 Epoch 1389: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-04 22:00:16,006 EPOCH 1390
2024-02-04 22:00:29,910 Epoch 1390: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-04 22:00:29,911 EPOCH 1391
2024-02-04 22:00:44,058 Epoch 1391: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-04 22:00:44,059 EPOCH 1392
2024-02-04 22:00:57,934 Epoch 1392: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-04 22:00:57,935 EPOCH 1393
2024-02-04 22:01:11,728 Epoch 1393: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-04 22:01:11,728 EPOCH 1394
2024-02-04 22:01:25,873 Epoch 1394: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-04 22:01:25,874 EPOCH 1395
2024-02-04 22:01:39,917 Epoch 1395: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-04 22:01:39,917 EPOCH 1396
2024-02-04 22:01:53,655 Epoch 1396: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-04 22:01:53,655 EPOCH 1397
2024-02-04 22:02:07,590 Epoch 1397: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-04 22:02:07,590 EPOCH 1398
2024-02-04 22:02:21,638 Epoch 1398: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-04 22:02:21,639 EPOCH 1399
2024-02-04 22:02:35,463 Epoch 1399: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-04 22:02:35,464 EPOCH 1400
2024-02-04 22:02:49,502 [Epoch: 1400 Step: 00012600] Batch Recognition Loss:   0.000211 => Gls Tokens per Sec:      757 || Batch Translation Loss:   0.064725 => Txt Tokens per Sec:     2102 || Lr: 0.000100
2024-02-04 22:02:49,503 Epoch 1400: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-04 22:02:49,503 EPOCH 1401
2024-02-04 22:03:03,667 Epoch 1401: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-04 22:03:03,667 EPOCH 1402
2024-02-04 22:03:17,732 Epoch 1402: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-04 22:03:17,733 EPOCH 1403
2024-02-04 22:03:31,603 Epoch 1403: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-04 22:03:31,603 EPOCH 1404
2024-02-04 22:03:45,356 Epoch 1404: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-04 22:03:45,357 EPOCH 1405
2024-02-04 22:03:59,035 Epoch 1405: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-04 22:03:59,035 EPOCH 1406
2024-02-04 22:04:13,118 Epoch 1406: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-04 22:04:13,118 EPOCH 1407
2024-02-04 22:04:26,817 Epoch 1407: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-04 22:04:26,818 EPOCH 1408
2024-02-04 22:04:40,776 Epoch 1408: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-04 22:04:40,777 EPOCH 1409
2024-02-04 22:04:54,971 Epoch 1409: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-04 22:04:54,972 EPOCH 1410
2024-02-04 22:05:08,994 Epoch 1410: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.68 
2024-02-04 22:05:08,995 EPOCH 1411
2024-02-04 22:05:22,893 Epoch 1411: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.33 
2024-02-04 22:05:22,894 EPOCH 1412
2024-02-04 22:05:23,218 [Epoch: 1412 Step: 00012700] Batch Recognition Loss:   0.000415 => Gls Tokens per Sec:     3951 || Batch Translation Loss:   0.050357 => Txt Tokens per Sec:     9111 || Lr: 0.000100
2024-02-04 22:05:36,904 Epoch 1412: Total Training Recognition Loss 0.00  Total Training Translation Loss 2.45 
2024-02-04 22:05:36,904 EPOCH 1413
2024-02-04 22:05:50,765 Epoch 1413: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.00 
2024-02-04 22:05:50,766 EPOCH 1414
2024-02-04 22:06:04,669 Epoch 1414: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.71 
2024-02-04 22:06:04,669 EPOCH 1415
2024-02-04 22:06:18,579 Epoch 1415: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.71 
2024-02-04 22:06:18,580 EPOCH 1416
2024-02-04 22:06:32,570 Epoch 1416: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.70 
2024-02-04 22:06:32,571 EPOCH 1417
2024-02-04 22:06:46,408 Epoch 1417: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.78 
2024-02-04 22:06:46,408 EPOCH 1418
2024-02-04 22:07:00,364 Epoch 1418: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.50 
2024-02-04 22:07:00,365 EPOCH 1419
2024-02-04 22:07:14,156 Epoch 1419: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.99 
2024-02-04 22:07:14,157 EPOCH 1420
2024-02-04 22:07:28,052 Epoch 1420: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-04 22:07:28,052 EPOCH 1421
2024-02-04 22:07:42,021 Epoch 1421: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-04 22:07:42,022 EPOCH 1422
2024-02-04 22:07:55,907 Epoch 1422: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-04 22:07:55,907 EPOCH 1423
2024-02-04 22:07:57,976 [Epoch: 1423 Step: 00012800] Batch Recognition Loss:   0.000372 => Gls Tokens per Sec:     1238 || Batch Translation Loss:   0.045223 => Txt Tokens per Sec:     3529 || Lr: 0.000100
2024-02-04 22:08:09,867 Epoch 1423: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-04 22:08:09,867 EPOCH 1424
2024-02-04 22:08:24,007 Epoch 1424: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-04 22:08:24,008 EPOCH 1425
2024-02-04 22:08:37,528 Epoch 1425: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-04 22:08:37,528 EPOCH 1426
2024-02-04 22:08:51,150 Epoch 1426: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-04 22:08:51,151 EPOCH 1427
2024-02-04 22:09:05,037 Epoch 1427: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-04 22:09:05,037 EPOCH 1428
2024-02-04 22:09:19,080 Epoch 1428: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-04 22:09:19,081 EPOCH 1429
2024-02-04 22:09:33,300 Epoch 1429: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-04 22:09:33,301 EPOCH 1430
2024-02-04 22:09:47,162 Epoch 1430: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-04 22:09:47,163 EPOCH 1431
2024-02-04 22:10:01,180 Epoch 1431: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-04 22:10:01,181 EPOCH 1432
2024-02-04 22:10:15,186 Epoch 1432: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-04 22:10:15,187 EPOCH 1433
2024-02-04 22:10:28,967 Epoch 1433: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-04 22:10:28,967 EPOCH 1434
2024-02-04 22:10:33,788 [Epoch: 1434 Step: 00012900] Batch Recognition Loss:   0.000113 => Gls Tokens per Sec:      612 || Batch Translation Loss:   0.013972 => Txt Tokens per Sec:     1491 || Lr: 0.000100
2024-02-04 22:10:42,594 Epoch 1434: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-04 22:10:42,595 EPOCH 1435
2024-02-04 22:10:56,697 Epoch 1435: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-04 22:10:56,698 EPOCH 1436
2024-02-04 22:11:10,698 Epoch 1436: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-04 22:11:10,698 EPOCH 1437
2024-02-04 22:11:24,712 Epoch 1437: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-04 22:11:24,713 EPOCH 1438
2024-02-04 22:11:38,486 Epoch 1438: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-04 22:11:38,487 EPOCH 1439
2024-02-04 22:11:52,232 Epoch 1439: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-04 22:11:52,233 EPOCH 1440
2024-02-04 22:12:06,004 Epoch 1440: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-04 22:12:06,004 EPOCH 1441
2024-02-04 22:12:19,805 Epoch 1441: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-04 22:12:19,806 EPOCH 1442
2024-02-04 22:12:33,751 Epoch 1442: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-04 22:12:33,752 EPOCH 1443
2024-02-04 22:12:47,708 Epoch 1443: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-04 22:12:47,709 EPOCH 1444
2024-02-04 22:13:01,598 Epoch 1444: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-04 22:13:01,598 EPOCH 1445
2024-02-04 22:13:09,190 [Epoch: 1445 Step: 00013000] Batch Recognition Loss:   0.000156 => Gls Tokens per Sec:      675 || Batch Translation Loss:   0.017197 => Txt Tokens per Sec:     1885 || Lr: 0.000100
2024-02-04 22:13:15,467 Epoch 1445: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-04 22:13:15,467 EPOCH 1446
2024-02-04 22:13:29,428 Epoch 1446: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-04 22:13:29,429 EPOCH 1447
2024-02-04 22:13:43,348 Epoch 1447: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-04 22:13:43,348 EPOCH 1448
2024-02-04 22:13:57,243 Epoch 1448: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-04 22:13:57,244 EPOCH 1449
2024-02-04 22:14:11,437 Epoch 1449: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-04 22:14:11,437 EPOCH 1450
2024-02-04 22:14:25,260 Epoch 1450: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-04 22:14:25,260 EPOCH 1451
2024-02-04 22:14:39,151 Epoch 1451: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-04 22:14:39,151 EPOCH 1452
2024-02-04 22:14:53,104 Epoch 1452: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-04 22:14:53,105 EPOCH 1453
2024-02-04 22:15:06,818 Epoch 1453: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-04 22:15:06,819 EPOCH 1454
2024-02-04 22:15:20,958 Epoch 1454: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-04 22:15:20,958 EPOCH 1455
2024-02-04 22:15:34,801 Epoch 1455: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-04 22:15:34,801 EPOCH 1456
2024-02-04 22:15:41,199 [Epoch: 1456 Step: 00013100] Batch Recognition Loss:   0.000106 => Gls Tokens per Sec:      862 || Batch Translation Loss:   0.007740 => Txt Tokens per Sec:     2457 || Lr: 0.000100
2024-02-04 22:15:48,773 Epoch 1456: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-04 22:15:48,773 EPOCH 1457
2024-02-04 22:16:02,832 Epoch 1457: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-04 22:16:02,832 EPOCH 1458
2024-02-04 22:16:16,710 Epoch 1458: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-04 22:16:16,710 EPOCH 1459
2024-02-04 22:16:30,486 Epoch 1459: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-04 22:16:30,487 EPOCH 1460
2024-02-04 22:16:44,418 Epoch 1460: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-04 22:16:44,418 EPOCH 1461
2024-02-04 22:16:58,205 Epoch 1461: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-04 22:16:58,206 EPOCH 1462
2024-02-04 22:17:12,065 Epoch 1462: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-04 22:17:12,066 EPOCH 1463
2024-02-04 22:17:26,122 Epoch 1463: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-04 22:17:26,123 EPOCH 1464
2024-02-04 22:17:39,888 Epoch 1464: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-04 22:17:39,889 EPOCH 1465
2024-02-04 22:17:54,102 Epoch 1465: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-04 22:17:54,103 EPOCH 1466
2024-02-04 22:18:07,929 Epoch 1466: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-04 22:18:07,930 EPOCH 1467
2024-02-04 22:18:15,782 [Epoch: 1467 Step: 00013200] Batch Recognition Loss:   0.000120 => Gls Tokens per Sec:      865 || Batch Translation Loss:   0.018573 => Txt Tokens per Sec:     2401 || Lr: 0.000100
2024-02-04 22:18:21,886 Epoch 1467: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-04 22:18:21,886 EPOCH 1468
2024-02-04 22:18:35,763 Epoch 1468: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-04 22:18:35,764 EPOCH 1469
2024-02-04 22:18:49,546 Epoch 1469: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-04 22:18:49,547 EPOCH 1470
2024-02-04 22:19:03,444 Epoch 1470: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-04 22:19:03,445 EPOCH 1471
2024-02-04 22:19:17,533 Epoch 1471: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-04 22:19:17,534 EPOCH 1472
2024-02-04 22:19:31,395 Epoch 1472: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-04 22:19:31,396 EPOCH 1473
2024-02-04 22:19:45,354 Epoch 1473: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-04 22:19:45,355 EPOCH 1474
2024-02-04 22:19:59,320 Epoch 1474: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-04 22:19:59,321 EPOCH 1475
2024-02-04 22:20:13,281 Epoch 1475: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-04 22:20:13,282 EPOCH 1476
2024-02-04 22:20:27,140 Epoch 1476: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-04 22:20:27,140 EPOCH 1477
2024-02-04 22:20:41,326 Epoch 1477: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-04 22:20:41,327 EPOCH 1478
2024-02-04 22:20:50,369 [Epoch: 1478 Step: 00013300] Batch Recognition Loss:   0.000147 => Gls Tokens per Sec:      991 || Batch Translation Loss:   0.067464 => Txt Tokens per Sec:     2858 || Lr: 0.000100
2024-02-04 22:20:55,074 Epoch 1478: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.65 
2024-02-04 22:20:55,074 EPOCH 1479
2024-02-04 22:21:08,978 Epoch 1479: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.57 
2024-02-04 22:21:08,979 EPOCH 1480
2024-02-04 22:21:22,720 Epoch 1480: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.85 
2024-02-04 22:21:22,720 EPOCH 1481
2024-02-04 22:21:36,908 Epoch 1481: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-04 22:21:36,908 EPOCH 1482
2024-02-04 22:21:51,125 Epoch 1482: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-04 22:21:51,125 EPOCH 1483
2024-02-04 22:22:05,251 Epoch 1483: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-04 22:22:05,251 EPOCH 1484
2024-02-04 22:22:19,237 Epoch 1484: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-04 22:22:19,238 EPOCH 1485
2024-02-04 22:22:32,994 Epoch 1485: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-04 22:22:32,995 EPOCH 1486
2024-02-04 22:22:47,063 Epoch 1486: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-04 22:22:47,064 EPOCH 1487
2024-02-04 22:23:01,032 Epoch 1487: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-04 22:23:01,032 EPOCH 1488
2024-02-04 22:23:14,832 Epoch 1488: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-04 22:23:14,832 EPOCH 1489
2024-02-04 22:23:28,196 [Epoch: 1489 Step: 00013400] Batch Recognition Loss:   0.000218 => Gls Tokens per Sec:      700 || Batch Translation Loss:   0.053868 => Txt Tokens per Sec:     1927 || Lr: 0.000100
2024-02-04 22:23:28,847 Epoch 1489: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-04 22:23:28,848 EPOCH 1490
2024-02-04 22:23:42,686 Epoch 1490: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-04 22:23:42,686 EPOCH 1491
2024-02-04 22:23:56,451 Epoch 1491: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-04 22:23:56,451 EPOCH 1492
2024-02-04 22:24:10,543 Epoch 1492: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-04 22:24:10,543 EPOCH 1493
2024-02-04 22:24:24,282 Epoch 1493: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.48 
2024-02-04 22:24:24,283 EPOCH 1494
2024-02-04 22:24:38,188 Epoch 1494: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.73 
2024-02-04 22:24:38,188 EPOCH 1495
2024-02-04 22:24:51,844 Epoch 1495: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.57 
2024-02-04 22:24:51,844 EPOCH 1496
2024-02-04 22:25:05,961 Epoch 1496: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.99 
2024-02-04 22:25:05,961 EPOCH 1497
2024-02-04 22:25:19,677 Epoch 1497: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.80 
2024-02-04 22:25:19,678 EPOCH 1498
2024-02-04 22:25:33,515 Epoch 1498: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-04 22:25:33,515 EPOCH 1499
2024-02-04 22:25:47,448 Epoch 1499: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-04 22:25:47,449 EPOCH 1500
2024-02-04 22:26:01,628 [Epoch: 1500 Step: 00013500] Batch Recognition Loss:   0.000136 => Gls Tokens per Sec:      750 || Batch Translation Loss:   0.025946 => Txt Tokens per Sec:     2081 || Lr: 0.000100
2024-02-04 22:26:01,628 Epoch 1500: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-04 22:26:01,629 EPOCH 1501
2024-02-04 22:26:15,591 Epoch 1501: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-04 22:26:15,591 EPOCH 1502
2024-02-04 22:26:29,345 Epoch 1502: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-04 22:26:29,346 EPOCH 1503
2024-02-04 22:26:43,417 Epoch 1503: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-04 22:26:43,418 EPOCH 1504
2024-02-04 22:26:57,432 Epoch 1504: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-04 22:26:57,432 EPOCH 1505
2024-02-04 22:27:11,398 Epoch 1505: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-04 22:27:11,398 EPOCH 1506
2024-02-04 22:27:25,416 Epoch 1506: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-04 22:27:25,417 EPOCH 1507
2024-02-04 22:27:39,553 Epoch 1507: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-04 22:27:39,554 EPOCH 1508
2024-02-04 22:27:53,501 Epoch 1508: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-04 22:27:53,501 EPOCH 1509
2024-02-04 22:28:07,313 Epoch 1509: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-04 22:28:07,313 EPOCH 1510
2024-02-04 22:28:20,864 Epoch 1510: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-04 22:28:20,864 EPOCH 1511
2024-02-04 22:28:34,847 Epoch 1511: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-04 22:28:34,847 EPOCH 1512
2024-02-04 22:28:35,485 [Epoch: 1512 Step: 00013600] Batch Recognition Loss:   0.000194 => Gls Tokens per Sec:     2009 || Batch Translation Loss:   0.042119 => Txt Tokens per Sec:     6071 || Lr: 0.000100
2024-02-04 22:28:48,398 Epoch 1512: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-04 22:28:48,398 EPOCH 1513
2024-02-04 22:29:02,431 Epoch 1513: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-04 22:29:02,432 EPOCH 1514
2024-02-04 22:29:16,429 Epoch 1514: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.71 
2024-02-04 22:29:16,429 EPOCH 1515
2024-02-04 22:29:30,083 Epoch 1515: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.61 
2024-02-04 22:29:30,083 EPOCH 1516
2024-02-04 22:29:44,284 Epoch 1516: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.68 
2024-02-04 22:29:44,285 EPOCH 1517
2024-02-04 22:29:58,085 Epoch 1517: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.97 
2024-02-04 22:29:58,086 EPOCH 1518
2024-02-04 22:30:12,150 Epoch 1518: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.64 
2024-02-04 22:30:12,151 EPOCH 1519
2024-02-04 22:30:26,090 Epoch 1519: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.94 
2024-02-04 22:30:26,090 EPOCH 1520
2024-02-04 22:30:40,050 Epoch 1520: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.73 
2024-02-04 22:30:40,051 EPOCH 1521
2024-02-04 22:30:53,977 Epoch 1521: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.57 
2024-02-04 22:30:53,977 EPOCH 1522
2024-02-04 22:31:07,967 Epoch 1522: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-04 22:31:07,967 EPOCH 1523
2024-02-04 22:31:08,574 [Epoch: 1523 Step: 00013700] Batch Recognition Loss:   0.000177 => Gls Tokens per Sec:     4224 || Batch Translation Loss:   0.038019 => Txt Tokens per Sec:     8924 || Lr: 0.000100
2024-02-04 22:31:21,787 Epoch 1523: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-04 22:31:21,788 EPOCH 1524
2024-02-04 22:31:35,749 Epoch 1524: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-04 22:31:35,750 EPOCH 1525
2024-02-04 22:31:49,670 Epoch 1525: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-04 22:31:49,671 EPOCH 1526
2024-02-04 22:32:03,809 Epoch 1526: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-04 22:32:03,810 EPOCH 1527
2024-02-04 22:32:17,722 Epoch 1527: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-04 22:32:17,722 EPOCH 1528
2024-02-04 22:32:31,649 Epoch 1528: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-04 22:32:31,649 EPOCH 1529
2024-02-04 22:32:45,548 Epoch 1529: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-04 22:32:45,549 EPOCH 1530
2024-02-04 22:32:59,131 Epoch 1530: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-04 22:32:59,132 EPOCH 1531
2024-02-04 22:33:13,321 Epoch 1531: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-04 22:33:13,321 EPOCH 1532
2024-02-04 22:33:27,087 Epoch 1532: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-04 22:33:27,088 EPOCH 1533
2024-02-04 22:33:41,119 Epoch 1533: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-04 22:33:41,119 EPOCH 1534
2024-02-04 22:33:46,495 [Epoch: 1534 Step: 00013800] Batch Recognition Loss:   0.000146 => Gls Tokens per Sec:      549 || Batch Translation Loss:   0.029931 => Txt Tokens per Sec:     1508 || Lr: 0.000100
2024-02-04 22:33:55,303 Epoch 1534: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-04 22:33:55,304 EPOCH 1535
2024-02-04 22:34:09,037 Epoch 1535: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-04 22:34:09,038 EPOCH 1536
2024-02-04 22:34:22,814 Epoch 1536: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-04 22:34:22,815 EPOCH 1537
2024-02-04 22:34:36,686 Epoch 1537: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-04 22:34:36,687 EPOCH 1538
2024-02-04 22:34:50,572 Epoch 1538: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-04 22:34:50,572 EPOCH 1539
2024-02-04 22:35:04,295 Epoch 1539: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-04 22:35:04,296 EPOCH 1540
2024-02-04 22:35:18,365 Epoch 1540: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-04 22:35:18,366 EPOCH 1541
2024-02-04 22:35:32,207 Epoch 1541: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-04 22:35:32,208 EPOCH 1542
2024-02-04 22:35:45,931 Epoch 1542: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-04 22:35:45,932 EPOCH 1543
2024-02-04 22:35:59,965 Epoch 1543: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-04 22:35:59,966 EPOCH 1544
2024-02-04 22:36:13,958 Epoch 1544: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-04 22:36:13,959 EPOCH 1545
2024-02-04 22:36:15,742 [Epoch: 1545 Step: 00013900] Batch Recognition Loss:   0.000170 => Gls Tokens per Sec:     2872 || Batch Translation Loss:   0.025644 => Txt Tokens per Sec:     7510 || Lr: 0.000100
2024-02-04 22:36:27,678 Epoch 1545: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-04 22:36:27,678 EPOCH 1546
2024-02-04 22:36:41,710 Epoch 1546: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-04 22:36:41,711 EPOCH 1547
2024-02-04 22:36:55,502 Epoch 1547: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-04 22:36:55,503 EPOCH 1548
2024-02-04 22:37:09,525 Epoch 1548: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-04 22:37:09,526 EPOCH 1549
2024-02-04 22:37:23,555 Epoch 1549: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-04 22:37:23,556 EPOCH 1550
2024-02-04 22:37:37,401 Epoch 1550: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-04 22:37:37,402 EPOCH 1551
2024-02-04 22:37:51,248 Epoch 1551: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.60 
2024-02-04 22:37:51,248 EPOCH 1552
2024-02-04 22:38:05,263 Epoch 1552: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.84 
2024-02-04 22:38:05,264 EPOCH 1553
2024-02-04 22:38:19,305 Epoch 1553: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.72 
2024-02-04 22:38:19,306 EPOCH 1554
2024-02-04 22:38:33,232 Epoch 1554: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.57 
2024-02-04 22:38:33,233 EPOCH 1555
2024-02-04 22:38:47,188 Epoch 1555: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-04 22:38:47,189 EPOCH 1556
2024-02-04 22:38:54,368 [Epoch: 1556 Step: 00014000] Batch Recognition Loss:   0.000385 => Gls Tokens per Sec:      768 || Batch Translation Loss:   0.067654 => Txt Tokens per Sec:     2046 || Lr: 0.000100
2024-02-04 22:39:23,492 Validation result at epoch 1556, step    14000: duration: 29.1221s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00020	Translation Loss: 93516.82812	PPL: 11593.88867
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.81	(BLEU-1: 11.20,	BLEU-2: 3.73,	BLEU-3: 1.55,	BLEU-4: 0.81)
	CHRF 17.51	ROUGE 9.82
2024-02-04 22:39:23,493 Logging Recognition and Translation Outputs
2024-02-04 22:39:23,493 ========================================================================================================================
2024-02-04 22:39:23,495 Logging Sequence: 141_40.00
2024-02-04 22:39:23,495 	Gloss Reference :	A B+C+D+E
2024-02-04 22:39:23,496 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 22:39:23,496 	Gloss Alignment :	         
2024-02-04 22:39:23,496 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 22:39:23,498 	Text Reference  :	got infected with covid-19 he    was quarantined and   could     not take    part  in ******* ****** the warmup match
2024-02-04 22:39:23,498 	Text Hypothesis :	he  won      a    gold     medal in  javelin     throw including an  olympic games in mirabai called me  tell   you  
2024-02-04 22:39:23,498 	Text Alignment  :	S   S        S    S        S     S   S           S     S         S   S       S        I       I      S   S      S    
2024-02-04 22:39:23,498 ========================================================================================================================
2024-02-04 22:39:23,498 Logging Sequence: 117_37.00
2024-02-04 22:39:23,499 	Gloss Reference :	A B+C+D+E
2024-02-04 22:39:23,499 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 22:39:23,499 	Gloss Alignment :	         
2024-02-04 22:39:23,499 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 22:39:23,500 	Text Reference  :	****** shikhar dhawan put  up   a      wonderful performance scoring 98     runs
2024-02-04 22:39:23,500 	Text Hypothesis :	during the     match  ajaz took played very      well        and     scored 3175
2024-02-04 22:39:23,500 	Text Alignment  :	I      S       S      S    S    S      S         S           S       S      S   
2024-02-04 22:39:23,500 ========================================================================================================================
2024-02-04 22:39:23,500 Logging Sequence: 64_13.00
2024-02-04 22:39:23,500 	Gloss Reference :	A B+C+D+E
2024-02-04 22:39:23,501 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 22:39:23,501 	Gloss Alignment :	         
2024-02-04 22:39:23,501 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 22:39:23,502 	Text Reference  :	arrangements were made to  move    all the **** ipl matches to the wankhede stadium in     mumbai  
2024-02-04 22:39:23,502 	Text Hypothesis :	************ **** for  the players won the toss and decided to *** ******** bowl    indian athletes
2024-02-04 22:39:23,502 	Text Alignment  :	D            D    S    S   S       S       I    S   S          D   D        S       S      S       
2024-02-04 22:39:23,502 ========================================================================================================================
2024-02-04 22:39:23,503 Logging Sequence: 98_121.00
2024-02-04 22:39:23,503 	Gloss Reference :	A B+C+D+E
2024-02-04 22:39:23,503 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 22:39:23,503 	Gloss Alignment :	         
2024-02-04 22:39:23,503 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 22:39:23,504 	Text Reference  :	** **** so      then england legends    and bangladesh legends were added to the  tournament
2024-02-04 22:39:23,504 	Text Hypothesis :	we were present at   the     tournament was played     between 3    days  to take place     
2024-02-04 22:39:23,505 	Text Alignment  :	I  I    S       S    S       S          S   S          S       S    S        S    S         
2024-02-04 22:39:23,505 ========================================================================================================================
2024-02-04 22:39:23,505 Logging Sequence: 179_414.00
2024-02-04 22:39:23,505 	Gloss Reference :	A B+C+D+E
2024-02-04 22:39:23,505 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 22:39:23,505 	Gloss Alignment :	         
2024-02-04 22:39:23,505 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 22:39:23,506 	Text Reference  :	we could not travel to delhi as there was a lockdown in     our home town     haryana
2024-02-04 22:39:23,506 	Text Hypothesis :	** ***** *** ****** ** ***** ** ***** *** * ******** phogat got her  schedule changed
2024-02-04 22:39:23,506 	Text Alignment  :	D  D     D   D      D  D     D  D     D   D D        S      S   S    S        S      
2024-02-04 22:39:23,506 ========================================================================================================================
2024-02-04 22:39:30,250 Epoch 1556: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-04 22:39:30,251 EPOCH 1557
2024-02-04 22:39:43,939 Epoch 1557: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-04 22:39:43,940 EPOCH 1558
2024-02-04 22:39:57,828 Epoch 1558: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-04 22:39:57,828 EPOCH 1559
2024-02-04 22:40:11,682 Epoch 1559: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-04 22:40:11,683 EPOCH 1560
2024-02-04 22:40:25,852 Epoch 1560: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-04 22:40:25,853 EPOCH 1561
2024-02-04 22:40:39,825 Epoch 1561: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-04 22:40:39,826 EPOCH 1562
2024-02-04 22:40:53,999 Epoch 1562: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-04 22:40:53,999 EPOCH 1563
2024-02-04 22:41:07,809 Epoch 1563: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-04 22:41:07,810 EPOCH 1564
2024-02-04 22:41:21,991 Epoch 1564: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.81 
2024-02-04 22:41:21,991 EPOCH 1565
2024-02-04 22:41:35,729 Epoch 1565: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.68 
2024-02-04 22:41:35,729 EPOCH 1566
2024-02-04 22:41:49,445 Epoch 1566: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.63 
2024-02-04 22:41:49,446 EPOCH 1567
2024-02-04 22:41:56,636 [Epoch: 1567 Step: 00014100] Batch Recognition Loss:   0.000184 => Gls Tokens per Sec:     1068 || Batch Translation Loss:   0.042615 => Txt Tokens per Sec:     2797 || Lr: 0.000100
2024-02-04 22:42:03,453 Epoch 1567: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.96 
2024-02-04 22:42:03,453 EPOCH 1568
2024-02-04 22:42:17,687 Epoch 1568: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.16 
2024-02-04 22:42:17,688 EPOCH 1569
2024-02-04 22:42:31,615 Epoch 1569: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.06 
2024-02-04 22:42:31,615 EPOCH 1570
2024-02-04 22:42:45,560 Epoch 1570: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.87 
2024-02-04 22:42:45,561 EPOCH 1571
2024-02-04 22:42:59,349 Epoch 1571: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.74 
2024-02-04 22:42:59,350 EPOCH 1572
2024-02-04 22:43:13,469 Epoch 1572: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.56 
2024-02-04 22:43:13,470 EPOCH 1573
2024-02-04 22:43:27,480 Epoch 1573: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-04 22:43:27,480 EPOCH 1574
2024-02-04 22:43:41,375 Epoch 1574: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-04 22:43:41,375 EPOCH 1575
2024-02-04 22:43:55,188 Epoch 1575: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-04 22:43:55,188 EPOCH 1576
2024-02-04 22:44:09,751 Epoch 1576: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-04 22:44:09,752 EPOCH 1577
2024-02-04 22:44:23,497 Epoch 1577: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-04 22:44:23,498 EPOCH 1578
2024-02-04 22:44:32,709 [Epoch: 1578 Step: 00014200] Batch Recognition Loss:   0.000265 => Gls Tokens per Sec:      973 || Batch Translation Loss:   0.033344 => Txt Tokens per Sec:     2733 || Lr: 0.000100
2024-02-04 22:44:37,395 Epoch 1578: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-04 22:44:37,395 EPOCH 1579
2024-02-04 22:44:51,307 Epoch 1579: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-04 22:44:51,308 EPOCH 1580
2024-02-04 22:45:05,345 Epoch 1580: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-04 22:45:05,345 EPOCH 1581
2024-02-04 22:45:19,150 Epoch 1581: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-04 22:45:19,150 EPOCH 1582
2024-02-04 22:45:33,226 Epoch 1582: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-04 22:45:33,226 EPOCH 1583
2024-02-04 22:45:47,023 Epoch 1583: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-04 22:45:47,023 EPOCH 1584
2024-02-04 22:46:01,294 Epoch 1584: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-04 22:46:01,295 EPOCH 1585
2024-02-04 22:46:15,290 Epoch 1585: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-04 22:46:15,290 EPOCH 1586
2024-02-04 22:46:29,426 Epoch 1586: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-04 22:46:29,427 EPOCH 1587
2024-02-04 22:46:43,256 Epoch 1587: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-04 22:46:43,256 EPOCH 1588
2024-02-04 22:46:57,225 Epoch 1588: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-04 22:46:57,225 EPOCH 1589
2024-02-04 22:47:10,774 [Epoch: 1589 Step: 00014300] Batch Recognition Loss:   0.000111 => Gls Tokens per Sec:      690 || Batch Translation Loss:   0.019288 => Txt Tokens per Sec:     1903 || Lr: 0.000100
2024-02-04 22:47:11,344 Epoch 1589: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-04 22:47:11,345 EPOCH 1590
2024-02-04 22:47:25,394 Epoch 1590: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-04 22:47:25,394 EPOCH 1591
2024-02-04 22:47:39,210 Epoch 1591: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-04 22:47:39,211 EPOCH 1592
2024-02-04 22:47:52,922 Epoch 1592: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-04 22:47:52,923 EPOCH 1593
2024-02-04 22:48:07,258 Epoch 1593: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-04 22:48:07,259 EPOCH 1594
2024-02-04 22:48:20,971 Epoch 1594: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-04 22:48:20,971 EPOCH 1595
2024-02-04 22:48:34,881 Epoch 1595: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-04 22:48:34,882 EPOCH 1596
2024-02-04 22:48:48,766 Epoch 1596: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-04 22:48:48,767 EPOCH 1597
2024-02-04 22:49:02,445 Epoch 1597: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-04 22:49:02,445 EPOCH 1598
2024-02-04 22:49:16,317 Epoch 1598: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-04 22:49:16,318 EPOCH 1599
2024-02-04 22:49:30,237 Epoch 1599: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-04 22:49:30,238 EPOCH 1600
2024-02-04 22:49:43,941 [Epoch: 1600 Step: 00014400] Batch Recognition Loss:   0.000176 => Gls Tokens per Sec:      776 || Batch Translation Loss:   0.023988 => Txt Tokens per Sec:     2154 || Lr: 0.000100
2024-02-04 22:49:43,942 Epoch 1600: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-04 22:49:43,942 EPOCH 1601
2024-02-04 22:49:58,253 Epoch 1601: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-04 22:49:58,254 EPOCH 1602
2024-02-04 22:50:12,171 Epoch 1602: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-04 22:50:12,171 EPOCH 1603
2024-02-04 22:50:25,821 Epoch 1603: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-04 22:50:25,822 EPOCH 1604
2024-02-04 22:50:39,446 Epoch 1604: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-04 22:50:39,447 EPOCH 1605
2024-02-04 22:50:53,254 Epoch 1605: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-04 22:50:53,254 EPOCH 1606
2024-02-04 22:51:07,155 Epoch 1606: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-04 22:51:07,156 EPOCH 1607
2024-02-04 22:51:20,882 Epoch 1607: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-04 22:51:20,882 EPOCH 1608
2024-02-04 22:51:35,052 Epoch 1608: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-04 22:51:35,052 EPOCH 1609
2024-02-04 22:51:48,832 Epoch 1609: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-04 22:51:48,833 EPOCH 1610
2024-02-04 22:52:02,729 Epoch 1610: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-04 22:52:02,729 EPOCH 1611
2024-02-04 22:52:16,735 Epoch 1611: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-04 22:52:16,735 EPOCH 1612
2024-02-04 22:52:21,001 [Epoch: 1612 Step: 00014500] Batch Recognition Loss:   0.000125 => Gls Tokens per Sec:       91 || Batch Translation Loss:   0.006852 => Txt Tokens per Sec:      329 || Lr: 0.000100
2024-02-04 22:52:30,501 Epoch 1612: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-04 22:52:30,502 EPOCH 1613
2024-02-04 22:52:44,462 Epoch 1613: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-04 22:52:44,463 EPOCH 1614
2024-02-04 22:52:57,940 Epoch 1614: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-04 22:52:57,941 EPOCH 1615
2024-02-04 22:53:11,481 Epoch 1615: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-04 22:53:11,482 EPOCH 1616
2024-02-04 22:53:25,240 Epoch 1616: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-04 22:53:25,240 EPOCH 1617
2024-02-04 22:53:39,219 Epoch 1617: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-04 22:53:39,220 EPOCH 1618
2024-02-04 22:53:53,031 Epoch 1618: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-04 22:53:53,032 EPOCH 1619
2024-02-04 22:54:07,107 Epoch 1619: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-04 22:54:07,107 EPOCH 1620
2024-02-04 22:54:21,065 Epoch 1620: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-04 22:54:21,066 EPOCH 1621
2024-02-04 22:54:34,948 Epoch 1621: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-04 22:54:34,949 EPOCH 1622
2024-02-04 22:54:48,904 Epoch 1622: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-04 22:54:48,905 EPOCH 1623
2024-02-04 22:54:54,179 [Epoch: 1623 Step: 00014600] Batch Recognition Loss:   0.000227 => Gls Tokens per Sec:      485 || Batch Translation Loss:   0.030791 => Txt Tokens per Sec:     1278 || Lr: 0.000100
2024-02-04 22:55:02,992 Epoch 1623: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-04 22:55:02,993 EPOCH 1624
2024-02-04 22:55:16,887 Epoch 1624: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-04 22:55:16,888 EPOCH 1625
2024-02-04 22:55:30,548 Epoch 1625: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-04 22:55:30,549 EPOCH 1626
2024-02-04 22:55:44,644 Epoch 1626: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-04 22:55:44,644 EPOCH 1627
2024-02-04 22:55:58,509 Epoch 1627: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-04 22:55:58,510 EPOCH 1628
2024-02-04 22:56:12,648 Epoch 1628: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-04 22:56:12,649 EPOCH 1629
2024-02-04 22:56:26,544 Epoch 1629: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-04 22:56:26,544 EPOCH 1630
2024-02-04 22:56:40,629 Epoch 1630: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.68 
2024-02-04 22:56:40,630 EPOCH 1631
2024-02-04 22:56:54,335 Epoch 1631: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.18 
2024-02-04 22:56:54,336 EPOCH 1632
2024-02-04 22:57:08,097 Epoch 1632: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.15 
2024-02-04 22:57:08,098 EPOCH 1633
2024-02-04 22:57:22,281 Epoch 1633: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.85 
2024-02-04 22:57:22,281 EPOCH 1634
2024-02-04 22:57:28,031 [Epoch: 1634 Step: 00014700] Batch Recognition Loss:   0.000233 => Gls Tokens per Sec:      668 || Batch Translation Loss:   0.095660 => Txt Tokens per Sec:     1683 || Lr: 0.000100
2024-02-04 22:57:36,202 Epoch 1634: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.14 
2024-02-04 22:57:36,202 EPOCH 1635
2024-02-04 22:57:50,358 Epoch 1635: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.95 
2024-02-04 22:57:50,358 EPOCH 1636
2024-02-04 22:58:04,151 Epoch 1636: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.48 
2024-02-04 22:58:04,151 EPOCH 1637
2024-02-04 22:58:18,005 Epoch 1637: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.53 
2024-02-04 22:58:18,005 EPOCH 1638
2024-02-04 22:58:31,724 Epoch 1638: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.73 
2024-02-04 22:58:31,725 EPOCH 1639
2024-02-04 22:58:45,251 Epoch 1639: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.50 
2024-02-04 22:58:45,252 EPOCH 1640
2024-02-04 22:58:59,249 Epoch 1640: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.20 
2024-02-04 22:58:59,249 EPOCH 1641
2024-02-04 22:59:13,373 Epoch 1641: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-04 22:59:13,374 EPOCH 1642
2024-02-04 22:59:27,158 Epoch 1642: Total Training Recognition Loss 0.00  Total Training Translation Loss 2.08 
2024-02-04 22:59:27,158 EPOCH 1643
2024-02-04 22:59:41,077 Epoch 1643: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.95 
2024-02-04 22:59:41,077 EPOCH 1644
2024-02-04 22:59:54,823 Epoch 1644: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.50 
2024-02-04 22:59:54,823 EPOCH 1645
2024-02-04 23:00:04,956 [Epoch: 1645 Step: 00014800] Batch Recognition Loss:   0.001789 => Gls Tokens per Sec:      418 || Batch Translation Loss:   0.249592 => Txt Tokens per Sec:     1242 || Lr: 0.000100
2024-02-04 23:00:08,655 Epoch 1645: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.39 
2024-02-04 23:00:08,655 EPOCH 1646
2024-02-04 23:00:22,315 Epoch 1646: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.13 
2024-02-04 23:00:22,316 EPOCH 1647
2024-02-04 23:00:36,250 Epoch 1647: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.75 
2024-02-04 23:00:36,250 EPOCH 1648
2024-02-04 23:00:50,286 Epoch 1648: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-04 23:00:50,287 EPOCH 1649
2024-02-04 23:01:04,150 Epoch 1649: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-04 23:01:04,150 EPOCH 1650
2024-02-04 23:01:18,046 Epoch 1650: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-04 23:01:18,046 EPOCH 1651
2024-02-04 23:01:32,012 Epoch 1651: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-04 23:01:32,013 EPOCH 1652
2024-02-04 23:01:46,131 Epoch 1652: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-04 23:01:46,132 EPOCH 1653
2024-02-04 23:02:00,160 Epoch 1653: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-04 23:02:00,160 EPOCH 1654
2024-02-04 23:02:14,286 Epoch 1654: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-04 23:02:14,286 EPOCH 1655
2024-02-04 23:02:28,436 Epoch 1655: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-04 23:02:28,437 EPOCH 1656
2024-02-04 23:02:36,068 [Epoch: 1656 Step: 00014900] Batch Recognition Loss:   0.000285 => Gls Tokens per Sec:      839 || Batch Translation Loss:   0.033602 => Txt Tokens per Sec:     2391 || Lr: 0.000100
2024-02-04 23:02:42,194 Epoch 1656: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-04 23:02:42,194 EPOCH 1657
2024-02-04 23:02:56,098 Epoch 1657: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-04 23:02:56,099 EPOCH 1658
2024-02-04 23:03:10,080 Epoch 1658: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-04 23:03:10,080 EPOCH 1659
2024-02-04 23:03:23,993 Epoch 1659: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-04 23:03:23,994 EPOCH 1660
2024-02-04 23:03:37,861 Epoch 1660: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-04 23:03:37,862 EPOCH 1661
2024-02-04 23:03:51,905 Epoch 1661: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-04 23:03:51,906 EPOCH 1662
2024-02-04 23:04:05,704 Epoch 1662: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-04 23:04:05,704 EPOCH 1663
2024-02-04 23:04:19,779 Epoch 1663: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-04 23:04:19,780 EPOCH 1664
2024-02-04 23:04:33,828 Epoch 1664: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-04 23:04:33,829 EPOCH 1665
2024-02-04 23:04:47,605 Epoch 1665: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-04 23:04:47,606 EPOCH 1666
2024-02-04 23:05:01,236 Epoch 1666: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-04 23:05:01,236 EPOCH 1667
2024-02-04 23:05:13,917 [Epoch: 1667 Step: 00015000] Batch Recognition Loss:   0.000198 => Gls Tokens per Sec:      536 || Batch Translation Loss:   0.026388 => Txt Tokens per Sec:     1610 || Lr: 0.000100
2024-02-04 23:05:15,311 Epoch 1667: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-04 23:05:15,311 EPOCH 1668
2024-02-04 23:05:29,331 Epoch 1668: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-04 23:05:29,332 EPOCH 1669
2024-02-04 23:05:43,204 Epoch 1669: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-04 23:05:43,205 EPOCH 1670
2024-02-04 23:05:57,282 Epoch 1670: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-04 23:05:57,282 EPOCH 1671
2024-02-04 23:06:11,335 Epoch 1671: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-04 23:06:11,336 EPOCH 1672
2024-02-04 23:06:25,383 Epoch 1672: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-04 23:06:25,383 EPOCH 1673
2024-02-04 23:06:39,242 Epoch 1673: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-04 23:06:39,243 EPOCH 1674
2024-02-04 23:06:53,369 Epoch 1674: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-04 23:06:53,370 EPOCH 1675
2024-02-04 23:07:07,328 Epoch 1675: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-04 23:07:07,328 EPOCH 1676
2024-02-04 23:07:21,460 Epoch 1676: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-04 23:07:21,461 EPOCH 1677
2024-02-04 23:07:35,157 Epoch 1677: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-04 23:07:35,158 EPOCH 1678
2024-02-04 23:07:46,642 [Epoch: 1678 Step: 00015100] Batch Recognition Loss:   0.000163 => Gls Tokens per Sec:      703 || Batch Translation Loss:   0.027825 => Txt Tokens per Sec:     1909 || Lr: 0.000100
2024-02-04 23:07:48,855 Epoch 1678: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-04 23:07:48,855 EPOCH 1679
2024-02-04 23:08:02,751 Epoch 1679: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-04 23:08:02,752 EPOCH 1680
2024-02-04 23:08:16,779 Epoch 1680: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-04 23:08:16,780 EPOCH 1681
2024-02-04 23:08:30,525 Epoch 1681: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-04 23:08:30,525 EPOCH 1682
2024-02-04 23:08:44,469 Epoch 1682: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-04 23:08:44,469 EPOCH 1683
2024-02-04 23:08:58,295 Epoch 1683: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-04 23:08:58,296 EPOCH 1684
2024-02-04 23:09:12,730 Epoch 1684: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-04 23:09:12,731 EPOCH 1685
2024-02-04 23:09:26,741 Epoch 1685: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-04 23:09:26,742 EPOCH 1686
2024-02-04 23:09:40,602 Epoch 1686: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-04 23:09:40,602 EPOCH 1687
2024-02-04 23:09:54,575 Epoch 1687: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-04 23:09:54,576 EPOCH 1688
2024-02-04 23:10:08,415 Epoch 1688: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-04 23:10:08,415 EPOCH 1689
2024-02-04 23:10:18,051 [Epoch: 1689 Step: 00015200] Batch Recognition Loss:   0.000131 => Gls Tokens per Sec:     1063 || Batch Translation Loss:   0.011482 => Txt Tokens per Sec:     2918 || Lr: 0.000100
2024-02-04 23:10:22,441 Epoch 1689: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-04 23:10:22,442 EPOCH 1690
2024-02-04 23:10:36,438 Epoch 1690: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-04 23:10:36,438 EPOCH 1691
2024-02-04 23:10:50,505 Epoch 1691: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-04 23:10:50,505 EPOCH 1692
2024-02-04 23:11:04,550 Epoch 1692: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-04 23:11:04,550 EPOCH 1693
2024-02-04 23:11:18,658 Epoch 1693: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-04 23:11:18,658 EPOCH 1694
2024-02-04 23:11:32,596 Epoch 1694: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-04 23:11:32,597 EPOCH 1695
2024-02-04 23:11:46,572 Epoch 1695: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-04 23:11:46,572 EPOCH 1696
2024-02-04 23:12:00,407 Epoch 1696: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-04 23:12:00,408 EPOCH 1697
2024-02-04 23:12:14,554 Epoch 1697: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-04 23:12:14,554 EPOCH 1698
2024-02-04 23:12:28,446 Epoch 1698: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-04 23:12:28,447 EPOCH 1699
2024-02-04 23:12:42,519 Epoch 1699: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-04 23:12:42,520 EPOCH 1700
2024-02-04 23:12:56,436 [Epoch: 1700 Step: 00015300] Batch Recognition Loss:   0.000126 => Gls Tokens per Sec:      764 || Batch Translation Loss:   0.014129 => Txt Tokens per Sec:     2121 || Lr: 0.000100
2024-02-04 23:12:56,437 Epoch 1700: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-04 23:12:56,437 EPOCH 1701
2024-02-04 23:13:10,088 Epoch 1701: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-04 23:13:10,088 EPOCH 1702
2024-02-04 23:13:24,002 Epoch 1702: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-04 23:13:24,003 EPOCH 1703
2024-02-04 23:13:37,980 Epoch 1703: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-04 23:13:37,981 EPOCH 1704
2024-02-04 23:13:51,901 Epoch 1704: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-04 23:13:51,901 EPOCH 1705
2024-02-04 23:14:05,635 Epoch 1705: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-04 23:14:05,635 EPOCH 1706
2024-02-04 23:14:19,449 Epoch 1706: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-04 23:14:19,450 EPOCH 1707
2024-02-04 23:14:33,174 Epoch 1707: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-04 23:14:33,175 EPOCH 1708
2024-02-04 23:14:47,104 Epoch 1708: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-04 23:14:47,104 EPOCH 1709
2024-02-04 23:15:01,227 Epoch 1709: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-04 23:15:01,228 EPOCH 1710
2024-02-04 23:15:15,154 Epoch 1710: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-04 23:15:15,154 EPOCH 1711
2024-02-04 23:15:29,450 Epoch 1711: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-04 23:15:29,451 EPOCH 1712
2024-02-04 23:15:29,789 [Epoch: 1712 Step: 00015400] Batch Recognition Loss:   0.000136 => Gls Tokens per Sec:     3810 || Batch Translation Loss:   0.013773 => Txt Tokens per Sec:     8810 || Lr: 0.000100
2024-02-04 23:15:43,543 Epoch 1712: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-04 23:15:43,543 EPOCH 1713
2024-02-04 23:15:57,059 Epoch 1713: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-04 23:15:57,060 EPOCH 1714
2024-02-04 23:16:10,968 Epoch 1714: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-04 23:16:10,968 EPOCH 1715
2024-02-04 23:16:24,968 Epoch 1715: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-04 23:16:24,968 EPOCH 1716
2024-02-04 23:16:39,011 Epoch 1716: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-04 23:16:39,012 EPOCH 1717
2024-02-04 23:16:52,820 Epoch 1717: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-04 23:16:52,821 EPOCH 1718
2024-02-04 23:17:06,743 Epoch 1718: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-04 23:17:06,744 EPOCH 1719
2024-02-04 23:17:20,587 Epoch 1719: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-04 23:17:20,587 EPOCH 1720
2024-02-04 23:17:34,493 Epoch 1720: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-04 23:17:34,494 EPOCH 1721
2024-02-04 23:17:48,306 Epoch 1721: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-04 23:17:48,306 EPOCH 1722
2024-02-04 23:18:02,300 Epoch 1722: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-04 23:18:02,301 EPOCH 1723
2024-02-04 23:18:04,443 [Epoch: 1723 Step: 00015500] Batch Recognition Loss:   0.000265 => Gls Tokens per Sec:     1195 || Batch Translation Loss:   0.032247 => Txt Tokens per Sec:     2984 || Lr: 0.000100
2024-02-04 23:18:16,394 Epoch 1723: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-04 23:18:16,395 EPOCH 1724
2024-02-04 23:18:30,193 Epoch 1724: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-04 23:18:30,194 EPOCH 1725
2024-02-04 23:18:44,380 Epoch 1725: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.10 
2024-02-04 23:18:44,381 EPOCH 1726
2024-02-04 23:18:58,500 Epoch 1726: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.67 
2024-02-04 23:18:58,501 EPOCH 1727
2024-02-04 23:19:12,377 Epoch 1727: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.88 
2024-02-04 23:19:12,378 EPOCH 1728
2024-02-04 23:19:26,392 Epoch 1728: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.66 
2024-02-04 23:19:26,392 EPOCH 1729
2024-02-04 23:19:40,470 Epoch 1729: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.87 
2024-02-04 23:19:40,471 EPOCH 1730
2024-02-04 23:19:54,193 Epoch 1730: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.78 
2024-02-04 23:19:54,193 EPOCH 1731
2024-02-04 23:20:07,942 Epoch 1731: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.62 
2024-02-04 23:20:07,943 EPOCH 1732
2024-02-04 23:20:22,000 Epoch 1732: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.58 
2024-02-04 23:20:22,000 EPOCH 1733
2024-02-04 23:20:35,791 Epoch 1733: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-04 23:20:35,792 EPOCH 1734
2024-02-04 23:20:38,709 [Epoch: 1734 Step: 00015600] Batch Recognition Loss:   0.000257 => Gls Tokens per Sec:     1317 || Batch Translation Loss:   0.045403 => Txt Tokens per Sec:     3925 || Lr: 0.000100
2024-02-04 23:20:49,736 Epoch 1734: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.73 
2024-02-04 23:20:49,736 EPOCH 1735
2024-02-04 23:21:03,579 Epoch 1735: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.84 
2024-02-04 23:21:03,580 EPOCH 1736
2024-02-04 23:21:17,405 Epoch 1736: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.58 
2024-02-04 23:21:17,406 EPOCH 1737
2024-02-04 23:21:31,288 Epoch 1737: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.72 
2024-02-04 23:21:31,288 EPOCH 1738
2024-02-04 23:21:45,195 Epoch 1738: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.62 
2024-02-04 23:21:45,195 EPOCH 1739
2024-02-04 23:21:59,224 Epoch 1739: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-04 23:21:59,225 EPOCH 1740
2024-02-04 23:22:12,995 Epoch 1740: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-04 23:22:12,996 EPOCH 1741
2024-02-04 23:22:26,898 Epoch 1741: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-04 23:22:26,899 EPOCH 1742
2024-02-04 23:22:40,883 Epoch 1742: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-04 23:22:40,883 EPOCH 1743
2024-02-04 23:22:54,392 Epoch 1743: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-04 23:22:54,392 EPOCH 1744
2024-02-04 23:23:08,459 Epoch 1744: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-04 23:23:08,459 EPOCH 1745
2024-02-04 23:23:14,033 [Epoch: 1745 Step: 00015700] Batch Recognition Loss:   0.000184 => Gls Tokens per Sec:      759 || Batch Translation Loss:   0.010546 => Txt Tokens per Sec:     1972 || Lr: 0.000100
2024-02-04 23:23:22,293 Epoch 1745: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-04 23:23:22,294 EPOCH 1746
2024-02-04 23:23:36,230 Epoch 1746: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-04 23:23:36,230 EPOCH 1747
2024-02-04 23:23:50,333 Epoch 1747: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-04 23:23:50,334 EPOCH 1748
2024-02-04 23:24:04,240 Epoch 1748: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-04 23:24:04,241 EPOCH 1749
2024-02-04 23:24:18,270 Epoch 1749: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-04 23:24:18,271 EPOCH 1750
2024-02-04 23:24:32,213 Epoch 1750: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-04 23:24:32,214 EPOCH 1751
2024-02-04 23:24:46,065 Epoch 1751: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-04 23:24:46,066 EPOCH 1752
2024-02-04 23:24:59,803 Epoch 1752: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-04 23:24:59,803 EPOCH 1753
2024-02-04 23:25:13,592 Epoch 1753: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-04 23:25:13,593 EPOCH 1754
2024-02-04 23:25:27,525 Epoch 1754: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-04 23:25:27,525 EPOCH 1755
2024-02-04 23:25:41,610 Epoch 1755: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-04 23:25:41,611 EPOCH 1756
2024-02-04 23:25:53,582 [Epoch: 1756 Step: 00015800] Batch Recognition Loss:   0.000174 => Gls Tokens per Sec:      460 || Batch Translation Loss:   0.023002 => Txt Tokens per Sec:     1406 || Lr: 0.000100
2024-02-04 23:25:55,288 Epoch 1756: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-04 23:25:55,288 EPOCH 1757
2024-02-04 23:26:09,098 Epoch 1757: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-04 23:26:09,099 EPOCH 1758
2024-02-04 23:26:23,189 Epoch 1758: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-04 23:26:23,190 EPOCH 1759
2024-02-04 23:26:37,004 Epoch 1759: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-04 23:26:37,005 EPOCH 1760
2024-02-04 23:26:50,860 Epoch 1760: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-04 23:26:50,860 EPOCH 1761
2024-02-04 23:27:04,740 Epoch 1761: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-04 23:27:04,741 EPOCH 1762
2024-02-04 23:27:18,740 Epoch 1762: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-04 23:27:18,741 EPOCH 1763
2024-02-04 23:27:32,577 Epoch 1763: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-04 23:27:32,578 EPOCH 1764
2024-02-04 23:27:46,610 Epoch 1764: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-04 23:27:46,611 EPOCH 1765
2024-02-04 23:28:00,260 Epoch 1765: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-04 23:28:00,261 EPOCH 1766
2024-02-04 23:28:14,234 Epoch 1766: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-04 23:28:14,234 EPOCH 1767
2024-02-04 23:28:21,663 [Epoch: 1767 Step: 00015900] Batch Recognition Loss:   0.000132 => Gls Tokens per Sec:     1034 || Batch Translation Loss:   0.007562 => Txt Tokens per Sec:     2766 || Lr: 0.000100
2024-02-04 23:28:28,269 Epoch 1767: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-04 23:28:28,269 EPOCH 1768
2024-02-04 23:28:41,942 Epoch 1768: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-04 23:28:41,942 EPOCH 1769
2024-02-04 23:28:55,691 Epoch 1769: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-04 23:28:55,691 EPOCH 1770
2024-02-04 23:29:09,877 Epoch 1770: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-04 23:29:09,877 EPOCH 1771
2024-02-04 23:29:23,729 Epoch 1771: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-04 23:29:23,730 EPOCH 1772
2024-02-04 23:29:37,887 Epoch 1772: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-04 23:29:37,887 EPOCH 1773
2024-02-04 23:29:51,529 Epoch 1773: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-04 23:29:51,530 EPOCH 1774
2024-02-04 23:30:05,250 Epoch 1774: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-04 23:30:05,251 EPOCH 1775
2024-02-04 23:30:19,220 Epoch 1775: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-04 23:30:19,220 EPOCH 1776
2024-02-04 23:30:33,105 Epoch 1776: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-04 23:30:33,105 EPOCH 1777
2024-02-04 23:30:47,094 Epoch 1777: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-04 23:30:47,094 EPOCH 1778
2024-02-04 23:31:00,064 [Epoch: 1778 Step: 00016000] Batch Recognition Loss:   0.000129 => Gls Tokens per Sec:      622 || Batch Translation Loss:   0.023845 => Txt Tokens per Sec:     1828 || Lr: 0.000100
2024-02-04 23:31:29,091 Validation result at epoch 1778, step    16000: duration: 29.0271s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00010	Translation Loss: 94747.35938	PPL: 13113.17676
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 1.03	(BLEU-1: 12.53,	BLEU-2: 4.29,	BLEU-3: 1.94,	BLEU-4: 1.03)
	CHRF 18.05	ROUGE 10.80
2024-02-04 23:31:29,092 Logging Recognition and Translation Outputs
2024-02-04 23:31:29,092 ========================================================================================================================
2024-02-04 23:31:29,092 Logging Sequence: 147_132.00
2024-02-04 23:31:29,093 	Gloss Reference :	A B+C+D+E
2024-02-04 23:31:29,093 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 23:31:29,093 	Gloss Alignment :	         
2024-02-04 23:31:29,093 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 23:31:29,095 	Text Reference  :	** i  can not ***** ** *** **** earlier i      used to    have    fun   in  gymnastics
2024-02-04 23:31:29,095 	Text Hypothesis :	if it is  not known as the same even    during the  match between india and england   
2024-02-04 23:31:29,095 	Text Alignment  :	I  S  S       I     I  I   I    S       S      S    S     S       S     S   S         
2024-02-04 23:31:29,095 ========================================================================================================================
2024-02-04 23:31:29,095 Logging Sequence: 116_162.00
2024-02-04 23:31:29,095 	Gloss Reference :	A B+C+D+E
2024-02-04 23:31:29,096 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 23:31:29,096 	Gloss Alignment :	         
2024-02-04 23:31:29,096 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 23:31:29,097 	Text Reference  :	* ***** *** **** turned  out the    video was  shared on social media by    a staff at the  hotel    
2024-02-04 23:31:29,097 	Text Hypothesis :	a babar fan said 'œthere is  speech but   went viral  on ****** this  means a ***** ** poor household
2024-02-04 23:31:29,098 	Text Alignment  :	I I     I   I    S       S   S      S     S    S         D      S     S       D     D  S    S        
2024-02-04 23:31:29,098 ========================================================================================================================
2024-02-04 23:31:29,098 Logging Sequence: 73_79.00
2024-02-04 23:31:29,098 	Gloss Reference :	A B+C+D+E
2024-02-04 23:31:29,098 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 23:31:29,098 	Gloss Alignment :	         
2024-02-04 23:31:29,098 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 23:31:29,100 	Text Reference  :	raina resturant has food  from the rich spices     of ****** north india to   the aromatic curries of south india  
2024-02-04 23:31:29,100 	Text Hypothesis :	***** ********* *** there were a   very protective of vamika and   never lets the ******** ******* ** ***** country
2024-02-04 23:31:29,100 	Text Alignment  :	D     D         D   S     S    S   S    S             I      S     S     S        D        D       D  D     S      
2024-02-04 23:31:29,100 ========================================================================================================================
2024-02-04 23:31:29,100 Logging Sequence: 165_523.00
2024-02-04 23:31:29,100 	Gloss Reference :	A B+C+D+E
2024-02-04 23:31:29,100 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 23:31:29,101 	Gloss Alignment :	         
2024-02-04 23:31:29,101 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 23:31:29,102 	Text Reference  :	*** **** ********* as     he  believed that his   team    might lose ** if     he takes off his batting pads
2024-02-04 23:31:29,102 	Text Hypothesis :	but then rajasthan royals has won      the  match however now   lose at jantar of just  off *** ******* ****
2024-02-04 23:31:29,102 	Text Alignment  :	I   I    I         S      S   S        S    S     S       S          I  S      S  S         D   D       D   
2024-02-04 23:31:29,103 ========================================================================================================================
2024-02-04 23:31:29,103 Logging Sequence: 125_72.00
2024-02-04 23:31:29,103 	Gloss Reference :	A B+C+D+E
2024-02-04 23:31:29,103 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 23:31:29,103 	Gloss Alignment :	         
2024-02-04 23:31:29,103 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 23:31:29,105 	Text Reference  :	****** some   said the pakistani javelineer had milicious intentions of     tampering with the javelin out of jealousy
2024-02-04 23:31:29,105 	Text Hypothesis :	neeraj chopra at   the ********* ********** *** game      many       people blamed    for  the ******* *** ** finals  
2024-02-04 23:31:29,105 	Text Alignment  :	I      S      S        D         D          D   S         S          S      S         S        D       D   D  S       
2024-02-04 23:31:29,105 ========================================================================================================================
2024-02-04 23:31:29,916 Epoch 1778: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-04 23:31:29,917 EPOCH 1779
2024-02-04 23:31:43,905 Epoch 1779: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-04 23:31:43,906 EPOCH 1780
2024-02-04 23:31:57,845 Epoch 1780: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-04 23:31:57,846 EPOCH 1781
2024-02-04 23:32:11,798 Epoch 1781: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-04 23:32:11,799 EPOCH 1782
2024-02-04 23:32:26,117 Epoch 1782: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-04 23:32:26,118 EPOCH 1783
2024-02-04 23:32:39,907 Epoch 1783: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-04 23:32:39,907 EPOCH 1784
2024-02-04 23:32:53,693 Epoch 1784: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-04 23:32:53,693 EPOCH 1785
2024-02-04 23:33:07,598 Epoch 1785: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-04 23:33:07,598 EPOCH 1786
2024-02-04 23:33:21,276 Epoch 1786: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-04 23:33:21,276 EPOCH 1787
2024-02-04 23:33:35,556 Epoch 1787: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-04 23:33:35,557 EPOCH 1788
2024-02-04 23:33:49,640 Epoch 1788: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-04 23:33:49,641 EPOCH 1789
2024-02-04 23:34:03,237 [Epoch: 1789 Step: 00016100] Batch Recognition Loss:   0.000145 => Gls Tokens per Sec:      688 || Batch Translation Loss:   0.066549 => Txt Tokens per Sec:     1950 || Lr: 0.000100
2024-02-04 23:34:03,546 Epoch 1789: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-04 23:34:03,547 EPOCH 1790
2024-02-04 23:34:16,550 Epoch 1790: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-04 23:34:16,550 EPOCH 1791
2024-02-04 23:34:30,962 Epoch 1791: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-04 23:34:30,962 EPOCH 1792
2024-02-04 23:34:44,932 Epoch 1792: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-04 23:34:44,933 EPOCH 1793
2024-02-04 23:34:59,014 Epoch 1793: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-04 23:34:59,014 EPOCH 1794
2024-02-04 23:35:12,929 Epoch 1794: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.88 
2024-02-04 23:35:12,930 EPOCH 1795
2024-02-04 23:35:27,228 Epoch 1795: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.10 
2024-02-04 23:35:27,228 EPOCH 1796
2024-02-04 23:35:41,450 Epoch 1796: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.86 
2024-02-04 23:35:41,450 EPOCH 1797
2024-02-04 23:35:55,615 Epoch 1797: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.83 
2024-02-04 23:35:55,616 EPOCH 1798
2024-02-04 23:36:09,782 Epoch 1798: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.00 
2024-02-04 23:36:09,782 EPOCH 1799
2024-02-04 23:36:23,956 Epoch 1799: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.73 
2024-02-04 23:36:23,957 EPOCH 1800
2024-02-04 23:36:38,137 [Epoch: 1800 Step: 00016200] Batch Recognition Loss:   0.000383 => Gls Tokens per Sec:      750 || Batch Translation Loss:   0.068545 => Txt Tokens per Sec:     2081 || Lr: 0.000100
2024-02-04 23:36:38,138 Epoch 1800: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.65 
2024-02-04 23:36:38,138 EPOCH 1801
2024-02-04 23:36:51,860 Epoch 1801: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.72 
2024-02-04 23:36:51,861 EPOCH 1802
2024-02-04 23:37:05,857 Epoch 1802: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.64 
2024-02-04 23:37:05,858 EPOCH 1803
2024-02-04 23:37:19,741 Epoch 1803: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.56 
2024-02-04 23:37:19,741 EPOCH 1804
2024-02-04 23:37:33,487 Epoch 1804: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-04 23:37:33,488 EPOCH 1805
2024-02-04 23:37:47,391 Epoch 1805: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.61 
2024-02-04 23:37:47,391 EPOCH 1806
2024-02-04 23:38:01,242 Epoch 1806: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-04 23:38:01,242 EPOCH 1807
2024-02-04 23:38:15,328 Epoch 1807: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-04 23:38:15,329 EPOCH 1808
2024-02-04 23:38:29,044 Epoch 1808: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-04 23:38:29,045 EPOCH 1809
2024-02-04 23:38:42,842 Epoch 1809: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-04 23:38:42,843 EPOCH 1810
2024-02-04 23:38:56,770 Epoch 1810: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-04 23:38:56,770 EPOCH 1811
2024-02-04 23:39:10,651 Epoch 1811: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-04 23:39:10,651 EPOCH 1812
2024-02-04 23:39:11,384 [Epoch: 1812 Step: 00016300] Batch Recognition Loss:   0.000196 => Gls Tokens per Sec:     1751 || Batch Translation Loss:   0.032210 => Txt Tokens per Sec:     5161 || Lr: 0.000100
2024-02-04 23:39:24,722 Epoch 1812: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-04 23:39:24,723 EPOCH 1813
2024-02-04 23:39:38,419 Epoch 1813: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-04 23:39:38,420 EPOCH 1814
2024-02-04 23:39:52,065 Epoch 1814: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-04 23:39:52,066 EPOCH 1815
2024-02-04 23:40:05,835 Epoch 1815: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-04 23:40:05,835 EPOCH 1816
2024-02-04 23:40:20,008 Epoch 1816: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-04 23:40:20,009 EPOCH 1817
2024-02-04 23:40:33,941 Epoch 1817: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-04 23:40:33,942 EPOCH 1818
2024-02-04 23:40:47,941 Epoch 1818: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-04 23:40:47,942 EPOCH 1819
2024-02-04 23:41:01,863 Epoch 1819: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-04 23:41:01,864 EPOCH 1820
2024-02-04 23:41:16,023 Epoch 1820: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-04 23:41:16,023 EPOCH 1821
2024-02-04 23:41:29,577 Epoch 1821: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-04 23:41:29,577 EPOCH 1822
2024-02-04 23:41:43,578 Epoch 1822: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-04 23:41:43,579 EPOCH 1823
2024-02-04 23:41:48,994 [Epoch: 1823 Step: 00016400] Batch Recognition Loss:   0.000206 => Gls Tokens per Sec:      473 || Batch Translation Loss:   0.034532 => Txt Tokens per Sec:     1411 || Lr: 0.000100
2024-02-04 23:41:57,747 Epoch 1823: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-04 23:41:57,747 EPOCH 1824
2024-02-04 23:42:11,902 Epoch 1824: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-04 23:42:11,902 EPOCH 1825
2024-02-04 23:42:25,760 Epoch 1825: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-04 23:42:25,761 EPOCH 1826
2024-02-04 23:42:39,552 Epoch 1826: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-04 23:42:39,552 EPOCH 1827
2024-02-04 23:42:53,688 Epoch 1827: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-04 23:42:53,689 EPOCH 1828
2024-02-04 23:43:07,567 Epoch 1828: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-04 23:43:07,568 EPOCH 1829
2024-02-04 23:43:21,346 Epoch 1829: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-04 23:43:21,347 EPOCH 1830
2024-02-04 23:43:35,472 Epoch 1830: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-04 23:43:35,473 EPOCH 1831
2024-02-04 23:43:49,563 Epoch 1831: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-04 23:43:49,564 EPOCH 1832
2024-02-04 23:44:03,533 Epoch 1832: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-04 23:44:03,534 EPOCH 1833
2024-02-04 23:44:17,667 Epoch 1833: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-04 23:44:17,668 EPOCH 1834
2024-02-04 23:44:19,933 [Epoch: 1834 Step: 00016500] Batch Recognition Loss:   0.000112 => Gls Tokens per Sec:     1696 || Batch Translation Loss:   0.012729 => Txt Tokens per Sec:     4092 || Lr: 0.000100
2024-02-04 23:44:31,670 Epoch 1834: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-04 23:44:31,671 EPOCH 1835
2024-02-04 23:44:45,295 Epoch 1835: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-04 23:44:45,296 EPOCH 1836
2024-02-04 23:44:59,222 Epoch 1836: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-04 23:44:59,223 EPOCH 1837
2024-02-04 23:45:12,955 Epoch 1837: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-04 23:45:12,956 EPOCH 1838
2024-02-04 23:45:27,016 Epoch 1838: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-04 23:45:27,016 EPOCH 1839
2024-02-04 23:45:41,173 Epoch 1839: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-04 23:45:41,173 EPOCH 1840
2024-02-04 23:45:55,012 Epoch 1840: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-04 23:45:55,013 EPOCH 1841
2024-02-04 23:46:08,791 Epoch 1841: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-04 23:46:08,791 EPOCH 1842
2024-02-04 23:46:22,586 Epoch 1842: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-04 23:46:22,586 EPOCH 1843
2024-02-04 23:46:36,244 Epoch 1843: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-04 23:46:36,245 EPOCH 1844
2024-02-04 23:46:50,258 Epoch 1844: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-04 23:46:50,259 EPOCH 1845
2024-02-04 23:46:57,665 [Epoch: 1845 Step: 00016600] Batch Recognition Loss:   0.000173 => Gls Tokens per Sec:      691 || Batch Translation Loss:   0.021152 => Txt Tokens per Sec:     2028 || Lr: 0.000100
2024-02-04 23:47:04,028 Epoch 1845: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-04 23:47:04,028 EPOCH 1846
2024-02-04 23:47:17,861 Epoch 1846: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-04 23:47:17,861 EPOCH 1847
2024-02-04 23:47:31,681 Epoch 1847: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-04 23:47:31,682 EPOCH 1848
2024-02-04 23:47:45,333 Epoch 1848: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-04 23:47:45,334 EPOCH 1849
2024-02-04 23:47:59,511 Epoch 1849: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-04 23:47:59,512 EPOCH 1850
2024-02-04 23:48:13,380 Epoch 1850: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-04 23:48:13,381 EPOCH 1851
2024-02-04 23:48:27,241 Epoch 1851: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-04 23:48:27,242 EPOCH 1852
2024-02-04 23:48:41,043 Epoch 1852: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-04 23:48:41,043 EPOCH 1853
2024-02-04 23:48:55,009 Epoch 1853: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-04 23:48:55,009 EPOCH 1854
2024-02-04 23:49:08,875 Epoch 1854: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-04 23:49:08,876 EPOCH 1855
2024-02-04 23:49:22,666 Epoch 1855: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-04 23:49:22,667 EPOCH 1856
2024-02-04 23:49:31,422 [Epoch: 1856 Step: 00016700] Batch Recognition Loss:   0.000177 => Gls Tokens per Sec:      731 || Batch Translation Loss:   0.085870 => Txt Tokens per Sec:     2183 || Lr: 0.000100
2024-02-04 23:49:36,946 Epoch 1856: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.56 
2024-02-04 23:49:36,946 EPOCH 1857
2024-02-04 23:49:50,936 Epoch 1857: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.77 
2024-02-04 23:49:50,937 EPOCH 1858
2024-02-04 23:50:04,851 Epoch 1858: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.74 
2024-02-04 23:50:04,851 EPOCH 1859
2024-02-04 23:50:19,136 Epoch 1859: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.89 
2024-02-04 23:50:19,136 EPOCH 1860
2024-02-04 23:50:33,143 Epoch 1860: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.50 
2024-02-04 23:50:33,144 EPOCH 1861
2024-02-04 23:50:46,946 Epoch 1861: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.33 
2024-02-04 23:50:46,947 EPOCH 1862
2024-02-04 23:51:01,059 Epoch 1862: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.77 
2024-02-04 23:51:01,059 EPOCH 1863
2024-02-04 23:51:15,022 Epoch 1863: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.33 
2024-02-04 23:51:15,023 EPOCH 1864
2024-02-04 23:51:28,629 Epoch 1864: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.11 
2024-02-04 23:51:28,630 EPOCH 1865
2024-02-04 23:51:42,721 Epoch 1865: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.88 
2024-02-04 23:51:42,721 EPOCH 1866
2024-02-04 23:51:56,464 Epoch 1866: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.76 
2024-02-04 23:51:56,465 EPOCH 1867
2024-02-04 23:52:04,655 [Epoch: 1867 Step: 00016800] Batch Recognition Loss:   0.000466 => Gls Tokens per Sec:      829 || Batch Translation Loss:   0.062162 => Txt Tokens per Sec:     2350 || Lr: 0.000100
2024-02-04 23:52:10,399 Epoch 1867: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.55 
2024-02-04 23:52:10,400 EPOCH 1868
2024-02-04 23:52:24,649 Epoch 1868: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-04 23:52:24,650 EPOCH 1869
2024-02-04 23:52:38,667 Epoch 1869: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-04 23:52:38,668 EPOCH 1870
2024-02-04 23:52:52,704 Epoch 1870: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-04 23:52:52,705 EPOCH 1871
2024-02-04 23:53:06,362 Epoch 1871: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-04 23:53:06,362 EPOCH 1872
2024-02-04 23:53:20,352 Epoch 1872: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-04 23:53:20,352 EPOCH 1873
2024-02-04 23:53:34,120 Epoch 1873: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-04 23:53:34,121 EPOCH 1874
2024-02-04 23:53:48,018 Epoch 1874: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-04 23:53:48,018 EPOCH 1875
2024-02-04 23:54:01,638 Epoch 1875: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-04 23:54:01,639 EPOCH 1876
2024-02-04 23:54:15,755 Epoch 1876: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-04 23:54:15,756 EPOCH 1877
2024-02-04 23:54:29,573 Epoch 1877: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-04 23:54:29,573 EPOCH 1878
2024-02-04 23:54:41,001 [Epoch: 1878 Step: 00016900] Batch Recognition Loss:   0.000187 => Gls Tokens per Sec:      706 || Batch Translation Loss:   0.025842 => Txt Tokens per Sec:     1883 || Lr: 0.000100
2024-02-04 23:54:43,691 Epoch 1878: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-04 23:54:43,691 EPOCH 1879
2024-02-04 23:54:57,338 Epoch 1879: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-04 23:54:57,338 EPOCH 1880
2024-02-04 23:55:11,418 Epoch 1880: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-04 23:55:11,419 EPOCH 1881
2024-02-04 23:55:25,283 Epoch 1881: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-04 23:55:25,284 EPOCH 1882
2024-02-04 23:55:39,324 Epoch 1882: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-04 23:55:39,324 EPOCH 1883
2024-02-04 23:55:53,162 Epoch 1883: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-04 23:55:53,163 EPOCH 1884
2024-02-04 23:56:07,108 Epoch 1884: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-04 23:56:07,109 EPOCH 1885
2024-02-04 23:56:20,807 Epoch 1885: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-04 23:56:20,807 EPOCH 1886
2024-02-04 23:56:34,987 Epoch 1886: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-04 23:56:34,988 EPOCH 1887
2024-02-04 23:56:48,866 Epoch 1887: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-04 23:56:48,866 EPOCH 1888
2024-02-04 23:57:02,960 Epoch 1888: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-04 23:57:02,960 EPOCH 1889
2024-02-04 23:57:11,863 [Epoch: 1889 Step: 00017000] Batch Recognition Loss:   0.000130 => Gls Tokens per Sec:     1050 || Batch Translation Loss:   0.012697 => Txt Tokens per Sec:     2811 || Lr: 0.000100
2024-02-04 23:57:16,825 Epoch 1889: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-04 23:57:16,826 EPOCH 1890
2024-02-04 23:57:30,612 Epoch 1890: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-04 23:57:30,613 EPOCH 1891
2024-02-04 23:57:44,550 Epoch 1891: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-04 23:57:44,551 EPOCH 1892
2024-02-04 23:57:58,719 Epoch 1892: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-04 23:57:58,719 EPOCH 1893
2024-02-04 23:58:12,658 Epoch 1893: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-04 23:58:12,659 EPOCH 1894
2024-02-04 23:58:26,194 Epoch 1894: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-04 23:58:26,194 EPOCH 1895
2024-02-04 23:58:40,239 Epoch 1895: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-04 23:58:40,240 EPOCH 1896
2024-02-04 23:58:54,097 Epoch 1896: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-04 23:58:54,097 EPOCH 1897
2024-02-04 23:59:08,129 Epoch 1897: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-04 23:59:08,130 EPOCH 1898
2024-02-04 23:59:22,173 Epoch 1898: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-04 23:59:22,174 EPOCH 1899
2024-02-04 23:59:36,169 Epoch 1899: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-04 23:59:36,170 EPOCH 1900
2024-02-04 23:59:50,165 [Epoch: 1900 Step: 00017100] Batch Recognition Loss:   0.000130 => Gls Tokens per Sec:      760 || Batch Translation Loss:   0.015692 => Txt Tokens per Sec:     2109 || Lr: 0.000100
2024-02-04 23:59:50,166 Epoch 1900: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-04 23:59:50,166 EPOCH 1901
2024-02-05 00:00:03,865 Epoch 1901: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 00:00:03,865 EPOCH 1902
2024-02-05 00:00:18,036 Epoch 1902: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 00:00:18,037 EPOCH 1903
2024-02-05 00:00:31,698 Epoch 1903: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 00:00:31,699 EPOCH 1904
2024-02-05 00:00:45,607 Epoch 1904: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 00:00:45,607 EPOCH 1905
2024-02-05 00:00:59,499 Epoch 1905: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 00:00:59,500 EPOCH 1906
2024-02-05 00:01:13,387 Epoch 1906: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 00:01:13,387 EPOCH 1907
2024-02-05 00:01:27,420 Epoch 1907: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 00:01:27,420 EPOCH 1908
2024-02-05 00:01:41,189 Epoch 1908: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 00:01:41,189 EPOCH 1909
2024-02-05 00:01:55,134 Epoch 1909: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 00:01:55,134 EPOCH 1910
2024-02-05 00:02:08,952 Epoch 1910: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 00:02:08,953 EPOCH 1911
2024-02-05 00:02:22,906 Epoch 1911: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 00:02:22,906 EPOCH 1912
2024-02-05 00:02:23,175 [Epoch: 1912 Step: 00017200] Batch Recognition Loss:   0.000137 => Gls Tokens per Sec:     4776 || Batch Translation Loss:   0.009930 => Txt Tokens per Sec:     8507 || Lr: 0.000100
2024-02-05 00:02:36,751 Epoch 1912: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 00:02:36,752 EPOCH 1913
2024-02-05 00:02:50,614 Epoch 1913: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 00:02:50,614 EPOCH 1914
2024-02-05 00:03:04,747 Epoch 1914: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 00:03:04,748 EPOCH 1915
2024-02-05 00:03:19,102 Epoch 1915: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 00:03:19,102 EPOCH 1916
2024-02-05 00:03:33,084 Epoch 1916: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 00:03:33,084 EPOCH 1917
2024-02-05 00:03:46,941 Epoch 1917: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 00:03:46,941 EPOCH 1918
2024-02-05 00:04:00,736 Epoch 1918: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 00:04:00,737 EPOCH 1919
2024-02-05 00:04:14,400 Epoch 1919: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 00:04:14,400 EPOCH 1920
2024-02-05 00:04:28,328 Epoch 1920: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 00:04:28,329 EPOCH 1921
2024-02-05 00:04:42,093 Epoch 1921: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 00:04:42,093 EPOCH 1922
2024-02-05 00:04:56,164 Epoch 1922: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 00:04:56,164 EPOCH 1923
2024-02-05 00:04:57,274 [Epoch: 1923 Step: 00017300] Batch Recognition Loss:   0.000130 => Gls Tokens per Sec:     2311 || Batch Translation Loss:   0.020910 => Txt Tokens per Sec:     6682 || Lr: 0.000100
2024-02-05 00:05:09,921 Epoch 1923: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 00:05:09,922 EPOCH 1924
2024-02-05 00:05:24,022 Epoch 1924: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 00:05:24,022 EPOCH 1925
2024-02-05 00:05:37,715 Epoch 1925: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 00:05:37,716 EPOCH 1926
2024-02-05 00:05:51,878 Epoch 1926: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 00:05:51,879 EPOCH 1927
2024-02-05 00:06:05,995 Epoch 1927: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 00:06:05,996 EPOCH 1928
2024-02-05 00:06:19,865 Epoch 1928: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 00:06:19,866 EPOCH 1929
2024-02-05 00:06:33,445 Epoch 1929: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-05 00:06:33,446 EPOCH 1930
2024-02-05 00:06:47,505 Epoch 1930: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 00:06:47,506 EPOCH 1931
2024-02-05 00:07:01,314 Epoch 1931: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-05 00:07:01,315 EPOCH 1932
2024-02-05 00:07:15,310 Epoch 1932: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-05 00:07:15,310 EPOCH 1933
2024-02-05 00:07:29,212 Epoch 1933: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 00:07:29,213 EPOCH 1934
2024-02-05 00:07:30,295 [Epoch: 1934 Step: 00017400] Batch Recognition Loss:   0.000183 => Gls Tokens per Sec:     3554 || Batch Translation Loss:   0.023252 => Txt Tokens per Sec:     8272 || Lr: 0.000100
2024-02-05 00:07:43,200 Epoch 1934: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 00:07:43,200 EPOCH 1935
2024-02-05 00:07:57,198 Epoch 1935: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 00:07:57,199 EPOCH 1936
2024-02-05 00:08:11,249 Epoch 1936: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-05 00:08:11,249 EPOCH 1937
2024-02-05 00:08:25,007 Epoch 1937: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 00:08:25,008 EPOCH 1938
2024-02-05 00:08:39,201 Epoch 1938: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-05 00:08:39,202 EPOCH 1939
2024-02-05 00:08:52,692 Epoch 1939: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-05 00:08:52,693 EPOCH 1940
2024-02-05 00:09:06,653 Epoch 1940: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-05 00:09:06,653 EPOCH 1941
2024-02-05 00:09:20,459 Epoch 1941: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-05 00:09:20,459 EPOCH 1942
2024-02-05 00:09:34,305 Epoch 1942: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-05 00:09:34,305 EPOCH 1943
2024-02-05 00:09:47,633 Epoch 1943: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-05 00:09:47,633 EPOCH 1944
2024-02-05 00:10:01,472 Epoch 1944: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-05 00:10:01,472 EPOCH 1945
2024-02-05 00:10:11,882 [Epoch: 1945 Step: 00017500] Batch Recognition Loss:   0.000170 => Gls Tokens per Sec:      406 || Batch Translation Loss:   0.043858 => Txt Tokens per Sec:     1264 || Lr: 0.000100
2024-02-05 00:10:15,266 Epoch 1945: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-05 00:10:15,266 EPOCH 1946
2024-02-05 00:10:29,127 Epoch 1946: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-05 00:10:29,127 EPOCH 1947
2024-02-05 00:10:43,177 Epoch 1947: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-05 00:10:43,177 EPOCH 1948
2024-02-05 00:10:56,914 Epoch 1948: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-05 00:10:56,915 EPOCH 1949
2024-02-05 00:11:10,992 Epoch 1949: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-05 00:11:10,992 EPOCH 1950
2024-02-05 00:11:25,042 Epoch 1950: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.62 
2024-02-05 00:11:25,043 EPOCH 1951
2024-02-05 00:11:39,049 Epoch 1951: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.61 
2024-02-05 00:11:39,049 EPOCH 1952
2024-02-05 00:11:52,784 Epoch 1952: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.74 
2024-02-05 00:11:52,785 EPOCH 1953
2024-02-05 00:12:06,873 Epoch 1953: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.04 
2024-02-05 00:12:06,874 EPOCH 1954
2024-02-05 00:12:20,778 Epoch 1954: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.96 
2024-02-05 00:12:20,778 EPOCH 1955
2024-02-05 00:12:34,571 Epoch 1955: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.15 
2024-02-05 00:12:34,571 EPOCH 1956
2024-02-05 00:12:42,448 [Epoch: 1956 Step: 00017600] Batch Recognition Loss:   0.000887 => Gls Tokens per Sec:      700 || Batch Translation Loss:   0.332408 => Txt Tokens per Sec:     2106 || Lr: 0.000100
2024-02-05 00:12:48,614 Epoch 1956: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.67 
2024-02-05 00:12:48,615 EPOCH 1957
2024-02-05 00:13:02,722 Epoch 1957: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.35 
2024-02-05 00:13:02,723 EPOCH 1958
2024-02-05 00:13:16,379 Epoch 1958: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.14 
2024-02-05 00:13:16,380 EPOCH 1959
2024-02-05 00:13:30,384 Epoch 1959: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.49 
2024-02-05 00:13:30,385 EPOCH 1960
2024-02-05 00:13:44,090 Epoch 1960: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.10 
2024-02-05 00:13:44,091 EPOCH 1961
2024-02-05 00:13:58,085 Epoch 1961: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.88 
2024-02-05 00:13:58,085 EPOCH 1962
2024-02-05 00:14:12,020 Epoch 1962: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-05 00:14:12,021 EPOCH 1963
2024-02-05 00:14:25,803 Epoch 1963: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-05 00:14:25,804 EPOCH 1964
2024-02-05 00:14:39,818 Epoch 1964: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-05 00:14:39,819 EPOCH 1965
2024-02-05 00:14:53,772 Epoch 1965: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-05 00:14:53,772 EPOCH 1966
2024-02-05 00:15:07,645 Epoch 1966: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-05 00:15:07,645 EPOCH 1967
2024-02-05 00:15:14,122 [Epoch: 1967 Step: 00017700] Batch Recognition Loss:   0.000201 => Gls Tokens per Sec:     1049 || Batch Translation Loss:   0.027295 => Txt Tokens per Sec:     2739 || Lr: 0.000100
2024-02-05 00:15:21,472 Epoch 1967: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-05 00:15:21,472 EPOCH 1968
2024-02-05 00:15:35,553 Epoch 1968: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-05 00:15:35,554 EPOCH 1969
2024-02-05 00:15:49,577 Epoch 1969: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 00:15:49,577 EPOCH 1970
2024-02-05 00:16:03,242 Epoch 1970: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 00:16:03,243 EPOCH 1971
2024-02-05 00:16:17,091 Epoch 1971: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 00:16:17,091 EPOCH 1972
2024-02-05 00:16:31,046 Epoch 1972: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 00:16:31,047 EPOCH 1973
2024-02-05 00:16:44,877 Epoch 1973: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 00:16:44,877 EPOCH 1974
2024-02-05 00:16:58,796 Epoch 1974: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 00:16:58,796 EPOCH 1975
2024-02-05 00:17:12,759 Epoch 1975: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 00:17:12,759 EPOCH 1976
2024-02-05 00:17:26,851 Epoch 1976: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 00:17:26,851 EPOCH 1977
2024-02-05 00:17:40,696 Epoch 1977: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 00:17:40,697 EPOCH 1978
2024-02-05 00:17:49,500 [Epoch: 1978 Step: 00017800] Batch Recognition Loss:   0.000158 => Gls Tokens per Sec:     1018 || Batch Translation Loss:   0.014458 => Txt Tokens per Sec:     2752 || Lr: 0.000100
2024-02-05 00:17:54,641 Epoch 1978: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 00:17:54,641 EPOCH 1979
2024-02-05 00:18:08,537 Epoch 1979: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 00:18:08,538 EPOCH 1980
2024-02-05 00:18:22,034 Epoch 1980: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 00:18:22,034 EPOCH 1981
2024-02-05 00:18:36,200 Epoch 1981: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 00:18:36,201 EPOCH 1982
2024-02-05 00:18:50,079 Epoch 1982: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 00:18:50,080 EPOCH 1983
2024-02-05 00:19:04,263 Epoch 1983: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 00:19:04,263 EPOCH 1984
2024-02-05 00:19:18,197 Epoch 1984: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 00:19:18,198 EPOCH 1985
2024-02-05 00:19:32,239 Epoch 1985: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 00:19:32,239 EPOCH 1986
2024-02-05 00:19:46,515 Epoch 1986: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 00:19:46,516 EPOCH 1987
2024-02-05 00:20:00,249 Epoch 1987: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 00:20:00,250 EPOCH 1988
2024-02-05 00:20:14,412 Epoch 1988: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 00:20:14,412 EPOCH 1989
2024-02-05 00:20:28,105 [Epoch: 1989 Step: 00017900] Batch Recognition Loss:   0.000171 => Gls Tokens per Sec:      683 || Batch Translation Loss:   0.025128 => Txt Tokens per Sec:     1992 || Lr: 0.000100
2024-02-05 00:20:28,362 Epoch 1989: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 00:20:28,362 EPOCH 1990
2024-02-05 00:20:42,555 Epoch 1990: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 00:20:42,556 EPOCH 1991
2024-02-05 00:20:56,530 Epoch 1991: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 00:20:56,531 EPOCH 1992
2024-02-05 00:21:10,477 Epoch 1992: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 00:21:10,478 EPOCH 1993
2024-02-05 00:21:24,240 Epoch 1993: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 00:21:24,241 EPOCH 1994
2024-02-05 00:21:38,151 Epoch 1994: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 00:21:38,152 EPOCH 1995
2024-02-05 00:21:52,237 Epoch 1995: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 00:21:52,238 EPOCH 1996
2024-02-05 00:22:06,312 Epoch 1996: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 00:22:06,313 EPOCH 1997
2024-02-05 00:22:20,342 Epoch 1997: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 00:22:20,343 EPOCH 1998
2024-02-05 00:22:34,025 Epoch 1998: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 00:22:34,025 EPOCH 1999
2024-02-05 00:22:47,761 Epoch 1999: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 00:22:47,762 EPOCH 2000
2024-02-05 00:23:01,617 [Epoch: 2000 Step: 00018000] Batch Recognition Loss:   0.000132 => Gls Tokens per Sec:      767 || Batch Translation Loss:   0.016695 => Txt Tokens per Sec:     2130 || Lr: 0.000100
2024-02-05 00:23:30,746 Validation result at epoch 2000, step    18000: duration: 29.1286s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00011	Translation Loss: 96012.21875	PPL: 14882.58984
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.71	(BLEU-1: 11.47,	BLEU-2: 3.59,	BLEU-3: 1.44,	BLEU-4: 0.71)
	CHRF 17.23	ROUGE 9.90
2024-02-05 00:23:30,748 Logging Recognition and Translation Outputs
2024-02-05 00:23:30,748 ========================================================================================================================
2024-02-05 00:23:30,749 Logging Sequence: 155_119.00
2024-02-05 00:23:30,750 	Gloss Reference :	A B+C+D+E
2024-02-05 00:23:30,750 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 00:23:30,750 	Gloss Alignment :	         
2024-02-05 00:23:30,750 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 00:23:30,751 	Text Reference  :	a report said    that the taliban wanted icc to   replace the afghan     flag with its own     
2024-02-05 00:23:30,752 	Text Hypothesis :	* indian cricket team is  also    given  a   part of      the tournament on   the  tv  channels
2024-02-05 00:23:30,752 	Text Alignment  :	D S      S       S    S   S       S      S   S    S           S          S    S    S   S       
2024-02-05 00:23:30,752 ========================================================================================================================
2024-02-05 00:23:30,752 Logging Sequence: 153_43.00
2024-02-05 00:23:30,752 	Gloss Reference :	A B+C+D+E
2024-02-05 00:23:30,752 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 00:23:30,752 	Gloss Alignment :	         
2024-02-05 00:23:30,753 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 00:23:30,753 	Text Reference  :	these runs were all because of   hardik pandya  and  virat    kohli   
2024-02-05 00:23:30,753 	Text Hypothesis :	***** **** **** *** now     they got    several such baseless comments
2024-02-05 00:23:30,753 	Text Alignment  :	D     D    D    D   S       S    S      S       S    S        S       
2024-02-05 00:23:30,754 ========================================================================================================================
2024-02-05 00:23:30,754 Logging Sequence: 150_35.00
2024-02-05 00:23:30,754 	Gloss Reference :	A B+C+D+E
2024-02-05 00:23:30,754 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 00:23:30,754 	Gloss Alignment :	         
2024-02-05 00:23:30,754 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 00:23:30,755 	Text Reference  :	wow india football team  is   really strong 
2024-02-05 00:23:30,755 	Text Hypothesis :	*** ***** ******** let's wait for    updates
2024-02-05 00:23:30,755 	Text Alignment  :	D   D     D        S     S    S      S      
2024-02-05 00:23:30,755 ========================================================================================================================
2024-02-05 00:23:30,755 Logging Sequence: 146_154.00
2024-02-05 00:23:30,755 	Gloss Reference :	A B+C+D+E
2024-02-05 00:23:30,756 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 00:23:30,756 	Gloss Alignment :	         
2024-02-05 00:23:30,756 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 00:23:30,758 	Text Reference  :	*** bwf   said that testing protocols have been implemented to      ensure  the health    and ** safety of    all participants
2024-02-05 00:23:30,758 	Text Hypothesis :	the ashes is   a    special series    of   test matches     between england and australia and is held   every 2   years       
2024-02-05 00:23:30,758 	Text Alignment  :	I   S     S    S    S       S         S    S    S           S       S       S   S             I  S      S     S   S           
2024-02-05 00:23:30,758 ========================================================================================================================
2024-02-05 00:23:30,758 Logging Sequence: 76_79.00
2024-02-05 00:23:30,758 	Gloss Reference :	A B+C+D+E
2024-02-05 00:23:30,758 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 00:23:30,759 	Gloss Alignment :	         
2024-02-05 00:23:30,759 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 00:23:30,759 	Text Reference  :	** **** ******** **** *** speaking to   ani csk      ceo kasi viswanathan said  
2024-02-05 00:23:30,760 	Text Hypothesis :	on 13th february 2023 the indian   team was supposed to  play against     mumbai
2024-02-05 00:23:30,760 	Text Alignment  :	I  I    I        I    I   S        S    S   S        S   S    S           S     
2024-02-05 00:23:30,760 ========================================================================================================================
2024-02-05 00:23:30,764 Epoch 2000: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 00:23:30,764 EPOCH 2001
2024-02-05 00:23:44,911 Epoch 2001: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 00:23:44,912 EPOCH 2002
2024-02-05 00:23:58,974 Epoch 2002: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 00:23:58,974 EPOCH 2003
2024-02-05 00:24:13,244 Epoch 2003: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 00:24:13,244 EPOCH 2004
2024-02-05 00:24:27,017 Epoch 2004: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 00:24:27,018 EPOCH 2005
2024-02-05 00:24:40,885 Epoch 2005: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 00:24:40,885 EPOCH 2006
2024-02-05 00:24:54,950 Epoch 2006: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 00:24:54,951 EPOCH 2007
2024-02-05 00:25:08,737 Epoch 2007: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 00:25:08,738 EPOCH 2008
2024-02-05 00:25:22,581 Epoch 2008: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 00:25:22,581 EPOCH 2009
2024-02-05 00:25:36,413 Epoch 2009: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 00:25:36,413 EPOCH 2010
2024-02-05 00:25:50,364 Epoch 2010: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 00:25:50,365 EPOCH 2011
2024-02-05 00:26:04,114 Epoch 2011: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 00:26:04,114 EPOCH 2012
2024-02-05 00:26:05,989 [Epoch: 2012 Step: 00018100] Batch Recognition Loss:   0.000147 => Gls Tokens per Sec:      683 || Batch Translation Loss:   0.018707 => Txt Tokens per Sec:     2181 || Lr: 0.000100
2024-02-05 00:26:18,091 Epoch 2012: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 00:26:18,091 EPOCH 2013
2024-02-05 00:26:31,778 Epoch 2013: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 00:26:31,778 EPOCH 2014
2024-02-05 00:26:45,473 Epoch 2014: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 00:26:45,474 EPOCH 2015
2024-02-05 00:26:59,590 Epoch 2015: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 00:26:59,591 EPOCH 2016
2024-02-05 00:27:13,260 Epoch 2016: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 00:27:13,261 EPOCH 2017
2024-02-05 00:27:27,487 Epoch 2017: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 00:27:27,488 EPOCH 2018
2024-02-05 00:27:41,598 Epoch 2018: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 00:27:41,598 EPOCH 2019
2024-02-05 00:27:55,719 Epoch 2019: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 00:27:55,720 EPOCH 2020
2024-02-05 00:28:09,585 Epoch 2020: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 00:28:09,585 EPOCH 2021
2024-02-05 00:28:23,402 Epoch 2021: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 00:28:23,402 EPOCH 2022
2024-02-05 00:28:37,707 Epoch 2022: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 00:28:37,708 EPOCH 2023
2024-02-05 00:28:44,710 [Epoch: 2023 Step: 00018200] Batch Recognition Loss:   0.000154 => Gls Tokens per Sec:      366 || Batch Translation Loss:   0.021420 => Txt Tokens per Sec:     1224 || Lr: 0.000100
2024-02-05 00:28:51,700 Epoch 2023: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 00:28:51,700 EPOCH 2024
2024-02-05 00:29:05,656 Epoch 2024: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 00:29:05,657 EPOCH 2025
2024-02-05 00:29:19,281 Epoch 2025: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 00:29:19,281 EPOCH 2026
2024-02-05 00:29:33,415 Epoch 2026: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 00:29:33,416 EPOCH 2027
2024-02-05 00:29:47,573 Epoch 2027: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 00:29:47,573 EPOCH 2028
2024-02-05 00:30:01,345 Epoch 2028: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 00:30:01,345 EPOCH 2029
2024-02-05 00:30:15,441 Epoch 2029: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 00:30:15,442 EPOCH 2030
2024-02-05 00:30:29,448 Epoch 2030: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 00:30:29,449 EPOCH 2031
2024-02-05 00:30:43,392 Epoch 2031: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.61 
2024-02-05 00:30:43,393 EPOCH 2032
2024-02-05 00:30:57,509 Epoch 2032: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.94 
2024-02-05 00:30:57,510 EPOCH 2033
2024-02-05 00:31:11,373 Epoch 2033: Total Training Recognition Loss 0.00  Total Training Translation Loss 2.92 
2024-02-05 00:31:11,373 EPOCH 2034
2024-02-05 00:31:16,376 [Epoch: 2034 Step: 00018300] Batch Recognition Loss:   0.000735 => Gls Tokens per Sec:      590 || Batch Translation Loss:   0.042313 => Txt Tokens per Sec:     1616 || Lr: 0.000100
2024-02-05 00:31:25,376 Epoch 2034: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.93 
2024-02-05 00:31:25,377 EPOCH 2035
2024-02-05 00:31:39,119 Epoch 2035: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.32 
2024-02-05 00:31:39,119 EPOCH 2036
2024-02-05 00:31:52,854 Epoch 2036: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.23 
2024-02-05 00:31:52,855 EPOCH 2037
2024-02-05 00:32:06,868 Epoch 2037: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.83 
2024-02-05 00:32:06,868 EPOCH 2038
2024-02-05 00:32:20,645 Epoch 2038: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-05 00:32:20,646 EPOCH 2039
2024-02-05 00:32:34,879 Epoch 2039: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-05 00:32:34,879 EPOCH 2040
2024-02-05 00:32:48,480 Epoch 2040: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-05 00:32:48,480 EPOCH 2041
2024-02-05 00:33:02,163 Epoch 2041: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-05 00:33:02,164 EPOCH 2042
2024-02-05 00:33:16,419 Epoch 2042: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-05 00:33:16,419 EPOCH 2043
2024-02-05 00:33:30,432 Epoch 2043: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-05 00:33:30,433 EPOCH 2044
2024-02-05 00:33:44,170 Epoch 2044: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-05 00:33:44,170 EPOCH 2045
2024-02-05 00:33:56,059 [Epoch: 2045 Step: 00018400] Batch Recognition Loss:   0.000139 => Gls Tokens per Sec:      356 || Batch Translation Loss:   0.010787 => Txt Tokens per Sec:     1165 || Lr: 0.000100
2024-02-05 00:33:58,163 Epoch 2045: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 00:33:58,164 EPOCH 2046
2024-02-05 00:34:12,322 Epoch 2046: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-05 00:34:12,322 EPOCH 2047
2024-02-05 00:34:26,123 Epoch 2047: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 00:34:26,123 EPOCH 2048
2024-02-05 00:34:40,147 Epoch 2048: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 00:34:40,148 EPOCH 2049
2024-02-05 00:34:54,019 Epoch 2049: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 00:34:54,020 EPOCH 2050
2024-02-05 00:35:08,060 Epoch 2050: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 00:35:08,060 EPOCH 2051
2024-02-05 00:35:21,976 Epoch 2051: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 00:35:21,977 EPOCH 2052
2024-02-05 00:35:36,075 Epoch 2052: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-05 00:35:36,076 EPOCH 2053
2024-02-05 00:35:49,978 Epoch 2053: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-05 00:35:49,978 EPOCH 2054
2024-02-05 00:36:03,727 Epoch 2054: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 00:36:03,728 EPOCH 2055
2024-02-05 00:36:17,487 Epoch 2055: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 00:36:17,487 EPOCH 2056
2024-02-05 00:36:25,641 [Epoch: 2056 Step: 00018500] Batch Recognition Loss:   0.000165 => Gls Tokens per Sec:      785 || Batch Translation Loss:   0.018087 => Txt Tokens per Sec:     2334 || Lr: 0.000100
2024-02-05 00:36:31,320 Epoch 2056: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 00:36:31,321 EPOCH 2057
2024-02-05 00:36:44,967 Epoch 2057: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 00:36:44,968 EPOCH 2058
2024-02-05 00:36:59,009 Epoch 2058: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 00:36:59,009 EPOCH 2059
2024-02-05 00:37:13,122 Epoch 2059: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 00:37:13,123 EPOCH 2060
2024-02-05 00:37:27,103 Epoch 2060: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 00:37:27,104 EPOCH 2061
2024-02-05 00:37:41,164 Epoch 2061: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 00:37:41,165 EPOCH 2062
2024-02-05 00:37:54,951 Epoch 2062: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 00:37:54,951 EPOCH 2063
2024-02-05 00:38:08,948 Epoch 2063: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 00:38:08,949 EPOCH 2064
2024-02-05 00:38:23,103 Epoch 2064: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 00:38:23,104 EPOCH 2065
2024-02-05 00:38:36,980 Epoch 2065: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 00:38:36,981 EPOCH 2066
2024-02-05 00:38:50,873 Epoch 2066: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 00:38:50,874 EPOCH 2067
2024-02-05 00:39:03,347 [Epoch: 2067 Step: 00018600] Batch Recognition Loss:   0.000153 => Gls Tokens per Sec:      544 || Batch Translation Loss:   0.023062 => Txt Tokens per Sec:     1575 || Lr: 0.000100
2024-02-05 00:39:04,593 Epoch 2067: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 00:39:04,593 EPOCH 2068
2024-02-05 00:39:18,332 Epoch 2068: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 00:39:18,333 EPOCH 2069
2024-02-05 00:39:31,992 Epoch 2069: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 00:39:31,992 EPOCH 2070
2024-02-05 00:39:46,201 Epoch 2070: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 00:39:46,202 EPOCH 2071
2024-02-05 00:39:59,917 Epoch 2071: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 00:39:59,917 EPOCH 2072
2024-02-05 00:40:14,215 Epoch 2072: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 00:40:14,215 EPOCH 2073
2024-02-05 00:40:28,045 Epoch 2073: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 00:40:28,045 EPOCH 2074
2024-02-05 00:40:41,818 Epoch 2074: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 00:40:41,819 EPOCH 2075
2024-02-05 00:40:55,898 Epoch 2075: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 00:40:55,899 EPOCH 2076
2024-02-05 00:41:09,953 Epoch 2076: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 00:41:09,954 EPOCH 2077
2024-02-05 00:41:23,718 Epoch 2077: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 00:41:23,718 EPOCH 2078
2024-02-05 00:41:37,236 [Epoch: 2078 Step: 00018700] Batch Recognition Loss:   0.000133 => Gls Tokens per Sec:      597 || Batch Translation Loss:   0.016691 => Txt Tokens per Sec:     1755 || Lr: 0.000100
2024-02-05 00:41:37,944 Epoch 2078: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 00:41:37,944 EPOCH 2079
2024-02-05 00:41:51,750 Epoch 2079: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 00:41:51,751 EPOCH 2080
2024-02-05 00:42:05,531 Epoch 2080: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 00:42:05,531 EPOCH 2081
2024-02-05 00:42:19,590 Epoch 2081: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 00:42:19,590 EPOCH 2082
2024-02-05 00:42:33,297 Epoch 2082: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 00:42:33,298 EPOCH 2083
2024-02-05 00:42:47,294 Epoch 2083: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 00:42:47,295 EPOCH 2084
2024-02-05 00:43:01,428 Epoch 2084: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 00:43:01,429 EPOCH 2085
2024-02-05 00:43:15,201 Epoch 2085: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 00:43:15,201 EPOCH 2086
2024-02-05 00:43:29,316 Epoch 2086: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 00:43:29,317 EPOCH 2087
2024-02-05 00:43:43,186 Epoch 2087: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 00:43:43,187 EPOCH 2088
2024-02-05 00:43:56,951 Epoch 2088: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 00:43:56,951 EPOCH 2089
2024-02-05 00:44:05,622 [Epoch: 2089 Step: 00018800] Batch Recognition Loss:   0.000091 => Gls Tokens per Sec:     1079 || Batch Translation Loss:   0.006978 => Txt Tokens per Sec:     2885 || Lr: 0.000100
2024-02-05 00:44:10,588 Epoch 2089: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 00:44:10,589 EPOCH 2090
2024-02-05 00:44:24,703 Epoch 2090: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 00:44:24,703 EPOCH 2091
2024-02-05 00:44:38,952 Epoch 2091: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 00:44:38,953 EPOCH 2092
2024-02-05 00:44:53,047 Epoch 2092: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 00:44:53,047 EPOCH 2093
2024-02-05 00:45:07,143 Epoch 2093: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 00:45:07,144 EPOCH 2094
2024-02-05 00:45:21,198 Epoch 2094: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-05 00:45:21,199 EPOCH 2095
2024-02-05 00:45:35,282 Epoch 2095: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 00:45:35,283 EPOCH 2096
2024-02-05 00:45:49,267 Epoch 2096: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-05 00:45:49,268 EPOCH 2097
2024-02-05 00:46:03,451 Epoch 2097: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-05 00:46:03,452 EPOCH 2098
2024-02-05 00:46:17,305 Epoch 2098: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-05 00:46:17,305 EPOCH 2099
2024-02-05 00:46:31,151 Epoch 2099: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-05 00:46:31,151 EPOCH 2100
2024-02-05 00:46:45,251 [Epoch: 2100 Step: 00018900] Batch Recognition Loss:   0.000136 => Gls Tokens per Sec:      754 || Batch Translation Loss:   0.016569 => Txt Tokens per Sec:     2093 || Lr: 0.000100
2024-02-05 00:46:45,251 Epoch 2100: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 00:46:45,251 EPOCH 2101
2024-02-05 00:46:58,891 Epoch 2101: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-05 00:46:58,892 EPOCH 2102
2024-02-05 00:47:12,610 Epoch 2102: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-05 00:47:12,611 EPOCH 2103
2024-02-05 00:47:26,861 Epoch 2103: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-05 00:47:26,862 EPOCH 2104
2024-02-05 00:47:40,760 Epoch 2104: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-05 00:47:40,761 EPOCH 2105
2024-02-05 00:47:54,368 Epoch 2105: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-05 00:47:54,369 EPOCH 2106
2024-02-05 00:48:08,158 Epoch 2106: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 00:48:08,159 EPOCH 2107
2024-02-05 00:48:22,266 Epoch 2107: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-05 00:48:22,267 EPOCH 2108
2024-02-05 00:48:36,188 Epoch 2108: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-05 00:48:36,188 EPOCH 2109
2024-02-05 00:48:49,811 Epoch 2109: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-05 00:48:49,812 EPOCH 2110
2024-02-05 00:49:03,520 Epoch 2110: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-05 00:49:03,520 EPOCH 2111
2024-02-05 00:49:17,411 Epoch 2111: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-05 00:49:17,411 EPOCH 2112
2024-02-05 00:49:17,735 [Epoch: 2112 Step: 00019000] Batch Recognition Loss:   0.000161 => Gls Tokens per Sec:     3980 || Batch Translation Loss:   0.018166 => Txt Tokens per Sec:     9206 || Lr: 0.000100
2024-02-05 00:49:31,535 Epoch 2112: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-05 00:49:31,536 EPOCH 2113
2024-02-05 00:49:45,480 Epoch 2113: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-05 00:49:45,480 EPOCH 2114
2024-02-05 00:49:59,329 Epoch 2114: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-05 00:49:59,329 EPOCH 2115
2024-02-05 00:50:13,103 Epoch 2115: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-05 00:50:13,103 EPOCH 2116
2024-02-05 00:50:26,958 Epoch 2116: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-05 00:50:26,959 EPOCH 2117
2024-02-05 00:50:40,882 Epoch 2117: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 00:50:40,883 EPOCH 2118
2024-02-05 00:50:54,765 Epoch 2118: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 00:50:54,766 EPOCH 2119
2024-02-05 00:51:08,519 Epoch 2119: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 00:51:08,520 EPOCH 2120
2024-02-05 00:51:22,708 Epoch 2120: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 00:51:22,709 EPOCH 2121
2024-02-05 00:51:36,779 Epoch 2121: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-05 00:51:36,780 EPOCH 2122
2024-02-05 00:51:50,522 Epoch 2122: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-05 00:51:50,522 EPOCH 2123
2024-02-05 00:51:51,170 [Epoch: 2123 Step: 00019100] Batch Recognition Loss:   0.000132 => Gls Tokens per Sec:     3961 || Batch Translation Loss:   0.016973 => Txt Tokens per Sec:    10004 || Lr: 0.000100
2024-02-05 00:52:04,023 Epoch 2123: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 00:52:04,023 EPOCH 2124
2024-02-05 00:52:18,058 Epoch 2124: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 00:52:18,059 EPOCH 2125
2024-02-05 00:52:31,765 Epoch 2125: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 00:52:31,766 EPOCH 2126
2024-02-05 00:52:45,693 Epoch 2126: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-05 00:52:45,694 EPOCH 2127
2024-02-05 00:52:59,748 Epoch 2127: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-05 00:52:59,749 EPOCH 2128
2024-02-05 00:53:13,574 Epoch 2128: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-05 00:53:13,574 EPOCH 2129
2024-02-05 00:53:27,294 Epoch 2129: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.56 
2024-02-05 00:53:27,294 EPOCH 2130
2024-02-05 00:53:41,347 Epoch 2130: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.57 
2024-02-05 00:53:41,348 EPOCH 2131
2024-02-05 00:53:55,344 Epoch 2131: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.58 
2024-02-05 00:53:55,344 EPOCH 2132
2024-02-05 00:54:09,421 Epoch 2132: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.56 
2024-02-05 00:54:09,421 EPOCH 2133
2024-02-05 00:54:23,410 Epoch 2133: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.66 
2024-02-05 00:54:23,410 EPOCH 2134
2024-02-05 00:54:30,796 [Epoch: 2134 Step: 00019200] Batch Recognition Loss:   0.000277 => Gls Tokens per Sec:      520 || Batch Translation Loss:   0.063175 => Txt Tokens per Sec:     1633 || Lr: 0.000100
2024-02-05 00:54:37,217 Epoch 2134: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.72 
2024-02-05 00:54:37,217 EPOCH 2135
2024-02-05 00:54:51,294 Epoch 2135: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.60 
2024-02-05 00:54:51,295 EPOCH 2136
2024-02-05 00:55:05,233 Epoch 2136: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-05 00:55:05,233 EPOCH 2137
2024-02-05 00:55:19,362 Epoch 2137: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-05 00:55:19,363 EPOCH 2138
2024-02-05 00:55:33,190 Epoch 2138: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-05 00:55:33,191 EPOCH 2139
2024-02-05 00:55:46,947 Epoch 2139: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-05 00:55:46,948 EPOCH 2140
2024-02-05 00:56:01,012 Epoch 2140: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-05 00:56:01,013 EPOCH 2141
2024-02-05 00:56:14,798 Epoch 2141: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-05 00:56:14,799 EPOCH 2142
2024-02-05 00:56:28,840 Epoch 2142: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-05 00:56:28,841 EPOCH 2143
2024-02-05 00:56:42,680 Epoch 2143: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-05 00:56:42,680 EPOCH 2144
2024-02-05 00:56:56,655 Epoch 2144: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-05 00:56:56,655 EPOCH 2145
2024-02-05 00:57:07,204 [Epoch: 2145 Step: 00019300] Batch Recognition Loss:   0.000234 => Gls Tokens per Sec:      401 || Batch Translation Loss:   0.026754 => Txt Tokens per Sec:     1245 || Lr: 0.000100
2024-02-05 00:57:10,570 Epoch 2145: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-05 00:57:10,570 EPOCH 2146
2024-02-05 00:57:24,482 Epoch 2146: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-05 00:57:24,483 EPOCH 2147
2024-02-05 00:57:38,416 Epoch 2147: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-05 00:57:38,417 EPOCH 2148
2024-02-05 00:57:52,525 Epoch 2148: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-05 00:57:52,525 EPOCH 2149
2024-02-05 00:58:06,344 Epoch 2149: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-05 00:58:06,345 EPOCH 2150
2024-02-05 00:58:20,137 Epoch 2150: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.75 
2024-02-05 00:58:20,137 EPOCH 2151
2024-02-05 00:58:34,304 Epoch 2151: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.71 
2024-02-05 00:58:34,304 EPOCH 2152
2024-02-05 00:58:48,200 Epoch 2152: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.63 
2024-02-05 00:58:48,201 EPOCH 2153
2024-02-05 00:59:02,130 Epoch 2153: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.65 
2024-02-05 00:59:02,130 EPOCH 2154
2024-02-05 00:59:16,020 Epoch 2154: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-05 00:59:16,020 EPOCH 2155
2024-02-05 00:59:30,296 Epoch 2155: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.57 
2024-02-05 00:59:30,297 EPOCH 2156
2024-02-05 00:59:37,563 [Epoch: 2156 Step: 00019400] Batch Recognition Loss:   0.000279 => Gls Tokens per Sec:      881 || Batch Translation Loss:   0.080388 => Txt Tokens per Sec:     2585 || Lr: 0.000100
2024-02-05 00:59:44,347 Epoch 2156: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-05 00:59:44,347 EPOCH 2157
2024-02-05 00:59:58,191 Epoch 2157: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.64 
2024-02-05 00:59:58,191 EPOCH 2158
2024-02-05 01:00:12,269 Epoch 2158: Total Training Recognition Loss 0.01  Total Training Translation Loss 5.05 
2024-02-05 01:00:12,270 EPOCH 2159
2024-02-05 01:00:26,083 Epoch 2159: Total Training Recognition Loss 0.01  Total Training Translation Loss 5.14 
2024-02-05 01:00:26,084 EPOCH 2160
2024-02-05 01:00:40,147 Epoch 2160: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.18 
2024-02-05 01:00:40,147 EPOCH 2161
2024-02-05 01:00:53,947 Epoch 2161: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.43 
2024-02-05 01:00:53,948 EPOCH 2162
2024-02-05 01:01:07,802 Epoch 2162: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.74 
2024-02-05 01:01:07,802 EPOCH 2163
2024-02-05 01:01:21,516 Epoch 2163: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.55 
2024-02-05 01:01:21,516 EPOCH 2164
2024-02-05 01:01:35,238 Epoch 2164: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-05 01:01:35,239 EPOCH 2165
2024-02-05 01:01:49,197 Epoch 2165: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-05 01:01:49,198 EPOCH 2166
2024-02-05 01:02:03,170 Epoch 2166: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-05 01:02:03,171 EPOCH 2167
2024-02-05 01:02:09,723 [Epoch: 2167 Step: 00019500] Batch Recognition Loss:   0.000274 => Gls Tokens per Sec:     1036 || Batch Translation Loss:   0.021739 => Txt Tokens per Sec:     2847 || Lr: 0.000100
2024-02-05 01:02:16,794 Epoch 2167: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-05 01:02:16,794 EPOCH 2168
2024-02-05 01:02:30,476 Epoch 2168: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 01:02:30,477 EPOCH 2169
2024-02-05 01:02:44,316 Epoch 2169: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 01:02:44,317 EPOCH 2170
2024-02-05 01:02:58,225 Epoch 2170: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 01:02:58,226 EPOCH 2171
2024-02-05 01:03:11,975 Epoch 2171: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 01:03:11,975 EPOCH 2172
2024-02-05 01:03:26,084 Epoch 2172: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 01:03:26,084 EPOCH 2173
2024-02-05 01:03:40,013 Epoch 2173: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-05 01:03:40,014 EPOCH 2174
2024-02-05 01:03:53,751 Epoch 2174: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-05 01:03:53,751 EPOCH 2175
2024-02-05 01:04:07,612 Epoch 2175: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-05 01:04:07,613 EPOCH 2176
2024-02-05 01:04:21,569 Epoch 2176: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 01:04:21,570 EPOCH 2177
2024-02-05 01:04:35,322 Epoch 2177: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 01:04:35,323 EPOCH 2178
2024-02-05 01:04:48,390 [Epoch: 2178 Step: 00019600] Batch Recognition Loss:   0.000233 => Gls Tokens per Sec:      618 || Batch Translation Loss:   0.021171 => Txt Tokens per Sec:     1764 || Lr: 0.000100
2024-02-05 01:04:49,137 Epoch 2178: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 01:04:49,138 EPOCH 2179
2024-02-05 01:05:02,979 Epoch 2179: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 01:05:02,980 EPOCH 2180
2024-02-05 01:05:16,963 Epoch 2180: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 01:05:16,964 EPOCH 2181
2024-02-05 01:05:30,937 Epoch 2181: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 01:05:30,938 EPOCH 2182
2024-02-05 01:05:44,556 Epoch 2182: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 01:05:44,557 EPOCH 2183
2024-02-05 01:05:58,618 Epoch 2183: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 01:05:58,619 EPOCH 2184
2024-02-05 01:06:12,420 Epoch 2184: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 01:06:12,421 EPOCH 2185
2024-02-05 01:06:26,301 Epoch 2185: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 01:06:26,302 EPOCH 2186
2024-02-05 01:06:40,403 Epoch 2186: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 01:06:40,404 EPOCH 2187
2024-02-05 01:06:54,171 Epoch 2187: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 01:06:54,171 EPOCH 2188
2024-02-05 01:07:07,997 Epoch 2188: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 01:07:07,997 EPOCH 2189
2024-02-05 01:07:21,320 [Epoch: 2189 Step: 00019700] Batch Recognition Loss:   0.000134 => Gls Tokens per Sec:      702 || Batch Translation Loss:   0.013697 => Txt Tokens per Sec:     1927 || Lr: 0.000100
2024-02-05 01:07:22,016 Epoch 2189: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 01:07:22,016 EPOCH 2190
2024-02-05 01:07:35,603 Epoch 2190: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 01:07:35,604 EPOCH 2191
2024-02-05 01:07:49,907 Epoch 2191: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 01:07:49,908 EPOCH 2192
2024-02-05 01:08:03,822 Epoch 2192: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 01:08:03,822 EPOCH 2193
2024-02-05 01:08:17,617 Epoch 2193: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 01:08:17,618 EPOCH 2194
2024-02-05 01:08:31,477 Epoch 2194: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 01:08:31,477 EPOCH 2195
2024-02-05 01:08:45,261 Epoch 2195: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 01:08:45,261 EPOCH 2196
2024-02-05 01:08:59,221 Epoch 2196: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 01:08:59,221 EPOCH 2197
2024-02-05 01:09:12,962 Epoch 2197: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 01:09:12,963 EPOCH 2198
2024-02-05 01:09:26,740 Epoch 2198: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 01:09:26,741 EPOCH 2199
2024-02-05 01:09:40,731 Epoch 2199: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 01:09:40,732 EPOCH 2200
2024-02-05 01:09:54,356 [Epoch: 2200 Step: 00019800] Batch Recognition Loss:   0.000106 => Gls Tokens per Sec:      780 || Batch Translation Loss:   0.006443 => Txt Tokens per Sec:     2166 || Lr: 0.000100
2024-02-05 01:09:54,357 Epoch 2200: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 01:09:54,357 EPOCH 2201
2024-02-05 01:10:08,277 Epoch 2201: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 01:10:08,278 EPOCH 2202
2024-02-05 01:10:22,221 Epoch 2202: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 01:10:22,222 EPOCH 2203
2024-02-05 01:10:36,016 Epoch 2203: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 01:10:36,016 EPOCH 2204
2024-02-05 01:10:49,980 Epoch 2204: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 01:10:49,980 EPOCH 2205
2024-02-05 01:11:03,951 Epoch 2205: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 01:11:03,952 EPOCH 2206
2024-02-05 01:11:17,738 Epoch 2206: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 01:11:17,739 EPOCH 2207
2024-02-05 01:11:31,450 Epoch 2207: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 01:11:31,451 EPOCH 2208
2024-02-05 01:11:45,704 Epoch 2208: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 01:11:45,704 EPOCH 2209
2024-02-05 01:11:59,436 Epoch 2209: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 01:11:59,437 EPOCH 2210
2024-02-05 01:12:13,111 Epoch 2210: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 01:12:13,111 EPOCH 2211
2024-02-05 01:12:27,458 Epoch 2211: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 01:12:27,458 EPOCH 2212
2024-02-05 01:12:27,744 [Epoch: 2212 Step: 00019900] Batch Recognition Loss:   0.000125 => Gls Tokens per Sec:     4500 || Batch Translation Loss:   0.009845 => Txt Tokens per Sec:    10223 || Lr: 0.000100
2024-02-05 01:12:41,278 Epoch 2212: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 01:12:41,279 EPOCH 2213
2024-02-05 01:12:55,172 Epoch 2213: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 01:12:55,173 EPOCH 2214
2024-02-05 01:13:09,329 Epoch 2214: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 01:13:09,330 EPOCH 2215
2024-02-05 01:13:23,037 Epoch 2215: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 01:13:23,037 EPOCH 2216
2024-02-05 01:13:37,013 Epoch 2216: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 01:13:37,014 EPOCH 2217
2024-02-05 01:13:50,943 Epoch 2217: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 01:13:50,943 EPOCH 2218
2024-02-05 01:14:04,717 Epoch 2218: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 01:14:04,718 EPOCH 2219
2024-02-05 01:14:18,882 Epoch 2219: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-05 01:14:18,883 EPOCH 2220
2024-02-05 01:14:32,497 Epoch 2220: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-05 01:14:32,497 EPOCH 2221
2024-02-05 01:14:46,113 Epoch 2221: Total Training Recognition Loss 0.00  Total Training Translation Loss 2.25 
2024-02-05 01:14:46,113 EPOCH 2222
2024-02-05 01:15:00,170 Epoch 2222: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.98 
2024-02-05 01:15:00,171 EPOCH 2223
2024-02-05 01:15:01,564 [Epoch: 2223 Step: 00020000] Batch Recognition Loss:   0.001066 => Gls Tokens per Sec:     1839 || Batch Translation Loss:   0.272406 => Txt Tokens per Sec:     5476 || Lr: 0.000100
2024-02-05 01:15:30,729 Validation result at epoch 2223, step    20000: duration: 29.1646s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00058	Translation Loss: 91713.09375	PPL: 9679.19922
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.71	(BLEU-1: 12.04,	BLEU-2: 3.65,	BLEU-3: 1.45,	BLEU-4: 0.71)
	CHRF 17.55	ROUGE 10.47
2024-02-05 01:15:30,731 Logging Recognition and Translation Outputs
2024-02-05 01:15:30,731 ========================================================================================================================
2024-02-05 01:15:30,733 Logging Sequence: 174_121.00
2024-02-05 01:15:30,733 	Gloss Reference :	A B+C+D+E
2024-02-05 01:15:30,734 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 01:15:30,734 	Gloss Alignment :	         
2024-02-05 01:15:30,734 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 01:15:30,735 	Text Reference  :	there was a    strong competition and a       difficult auction for      the     5       franchise owners     
2024-02-05 01:15:30,735 	Text Hypothesis :	***** *** even social media       was flooded with      vicious comments against india's abysmal   performance
2024-02-05 01:15:30,735 	Text Alignment  :	D     D   S    S      S           S   S       S         S       S        S       S       S         S          
2024-02-05 01:15:30,735 ========================================================================================================================
2024-02-05 01:15:30,736 Logging Sequence: 170_24.00
2024-02-05 01:15:30,736 	Gloss Reference :	A B+C+D+E
2024-02-05 01:15:30,736 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 01:15:30,736 	Gloss Alignment :	         
2024-02-05 01:15:30,736 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 01:15:30,737 	Text Reference  :	let me tell you about it
2024-02-05 01:15:30,737 	Text Hypothesis :	let me tell you about it
2024-02-05 01:15:30,737 	Text Alignment  :	                        
2024-02-05 01:15:30,737 ========================================================================================================================
2024-02-05 01:15:30,737 Logging Sequence: 73_79.00
2024-02-05 01:15:30,737 	Gloss Reference :	A B+C+D+E
2024-02-05 01:15:30,738 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 01:15:30,738 	Gloss Alignment :	         
2024-02-05 01:15:30,738 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 01:15:30,739 	Text Reference  :	raina resturant has food from the   rich spices of   north india     to the aromatic curries of south india 
2024-02-05 01:15:30,739 	Text Hypothesis :	***** ********* *** **** **** there was  a      huge fan   following in the ******** ******* ** ***** league
2024-02-05 01:15:30,739 	Text Alignment  :	D     D         D   D    D    S     S    S      S    S     S         S      D        D       D  D     S     
2024-02-05 01:15:30,739 ========================================================================================================================
2024-02-05 01:15:30,739 Logging Sequence: 140_2.00
2024-02-05 01:15:30,740 	Gloss Reference :	A B+C+D+E
2024-02-05 01:15:30,740 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 01:15:30,740 	Gloss Alignment :	         
2024-02-05 01:15:30,740 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 01:15:30,741 	Text Reference  :	** *** **** ********* *** *** indian batsman-wicket keeper   rishabh pant  has outstanding skills in  cricket
2024-02-05 01:15:30,741 	Text Hypothesis :	he has also following the ban are    spreading      everyone are     given a   proud       of     the person 
2024-02-05 01:15:30,741 	Text Alignment  :	I  I   I    I         I   I   S      S              S        S       S     S   S           S      S   S      
2024-02-05 01:15:30,741 ========================================================================================================================
2024-02-05 01:15:30,742 Logging Sequence: 81_470.00
2024-02-05 01:15:30,742 	Gloss Reference :	A B+C+D+E
2024-02-05 01:15:30,742 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 01:15:30,742 	Gloss Alignment :	         
2024-02-05 01:15:30,742 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 01:15:30,743 	Text Reference  :	or you don't know if  you  do       let us      know   in      the  comments
2024-02-05 01:15:30,743 	Text Hypothesis :	** *** ***** the  two runs suffered a   similar defeat against each other   
2024-02-05 01:15:30,743 	Text Alignment  :	D  D   D     S    S   S    S        S   S       S      S       S    S       
2024-02-05 01:15:30,743 ========================================================================================================================
2024-02-05 01:15:43,514 Epoch 2223: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.76 
2024-02-05 01:15:43,514 EPOCH 2224
2024-02-05 01:15:57,365 Epoch 2224: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.44 
2024-02-05 01:15:57,365 EPOCH 2225
2024-02-05 01:16:11,173 Epoch 2225: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.56 
2024-02-05 01:16:11,174 EPOCH 2226
2024-02-05 01:16:24,976 Epoch 2226: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.21 
2024-02-05 01:16:24,977 EPOCH 2227
2024-02-05 01:16:39,014 Epoch 2227: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.77 
2024-02-05 01:16:39,015 EPOCH 2228
2024-02-05 01:16:52,766 Epoch 2228: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.55 
2024-02-05 01:16:52,767 EPOCH 2229
2024-02-05 01:17:06,897 Epoch 2229: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-05 01:17:06,897 EPOCH 2230
2024-02-05 01:17:20,649 Epoch 2230: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-05 01:17:20,649 EPOCH 2231
2024-02-05 01:17:34,809 Epoch 2231: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-05 01:17:34,809 EPOCH 2232
2024-02-05 01:17:48,676 Epoch 2232: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-05 01:17:48,676 EPOCH 2233
2024-02-05 01:18:02,379 Epoch 2233: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-05 01:18:02,380 EPOCH 2234
2024-02-05 01:18:08,475 [Epoch: 2234 Step: 00020100] Batch Recognition Loss:   0.000207 => Gls Tokens per Sec:      630 || Batch Translation Loss:   0.020002 => Txt Tokens per Sec:     1881 || Lr: 0.000100
2024-02-05 01:18:16,286 Epoch 2234: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 01:18:16,286 EPOCH 2235
2024-02-05 01:18:30,079 Epoch 2235: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 01:18:30,080 EPOCH 2236
2024-02-05 01:18:44,426 Epoch 2236: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 01:18:44,427 EPOCH 2237
2024-02-05 01:18:58,516 Epoch 2237: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 01:18:58,516 EPOCH 2238
2024-02-05 01:19:12,576 Epoch 2238: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 01:19:12,577 EPOCH 2239
2024-02-05 01:19:26,515 Epoch 2239: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 01:19:26,516 EPOCH 2240
2024-02-05 01:19:40,598 Epoch 2240: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 01:19:40,599 EPOCH 2241
2024-02-05 01:19:54,427 Epoch 2241: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 01:19:54,428 EPOCH 2242
2024-02-05 01:20:08,181 Epoch 2242: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 01:20:08,181 EPOCH 2243
2024-02-05 01:20:22,151 Epoch 2243: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 01:20:22,152 EPOCH 2244
2024-02-05 01:20:36,129 Epoch 2244: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 01:20:36,129 EPOCH 2245
2024-02-05 01:20:42,105 [Epoch: 2245 Step: 00020200] Batch Recognition Loss:   0.000155 => Gls Tokens per Sec:      708 || Batch Translation Loss:   0.016512 => Txt Tokens per Sec:     2052 || Lr: 0.000100
2024-02-05 01:20:50,258 Epoch 2245: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 01:20:50,258 EPOCH 2246
2024-02-05 01:21:04,323 Epoch 2246: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 01:21:04,324 EPOCH 2247
2024-02-05 01:21:18,390 Epoch 2247: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 01:21:18,390 EPOCH 2248
2024-02-05 01:21:32,187 Epoch 2248: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 01:21:32,188 EPOCH 2249
2024-02-05 01:21:45,836 Epoch 2249: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 01:21:45,836 EPOCH 2250
2024-02-05 01:21:59,804 Epoch 2250: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 01:21:59,805 EPOCH 2251
2024-02-05 01:22:13,706 Epoch 2251: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 01:22:13,707 EPOCH 2252
2024-02-05 01:22:27,643 Epoch 2252: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 01:22:27,644 EPOCH 2253
2024-02-05 01:22:41,568 Epoch 2253: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 01:22:41,569 EPOCH 2254
2024-02-05 01:22:55,952 Epoch 2254: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 01:22:55,953 EPOCH 2255
2024-02-05 01:23:09,654 Epoch 2255: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 01:23:09,655 EPOCH 2256
2024-02-05 01:23:20,192 [Epoch: 2256 Step: 00020300] Batch Recognition Loss:   0.000136 => Gls Tokens per Sec:      523 || Batch Translation Loss:   0.008624 => Txt Tokens per Sec:     1405 || Lr: 0.000100
2024-02-05 01:23:23,610 Epoch 2256: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 01:23:23,611 EPOCH 2257
2024-02-05 01:23:37,280 Epoch 2257: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 01:23:37,280 EPOCH 2258
2024-02-05 01:23:51,042 Epoch 2258: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 01:23:51,043 EPOCH 2259
2024-02-05 01:24:05,109 Epoch 2259: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 01:24:05,110 EPOCH 2260
2024-02-05 01:24:18,897 Epoch 2260: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 01:24:18,898 EPOCH 2261
2024-02-05 01:24:33,000 Epoch 2261: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 01:24:33,001 EPOCH 2262
2024-02-05 01:24:46,768 Epoch 2262: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 01:24:46,769 EPOCH 2263
2024-02-05 01:25:00,546 Epoch 2263: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 01:25:00,547 EPOCH 2264
2024-02-05 01:25:14,617 Epoch 2264: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 01:25:14,618 EPOCH 2265
2024-02-05 01:25:28,408 Epoch 2265: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 01:25:28,408 EPOCH 2266
2024-02-05 01:25:42,013 Epoch 2266: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 01:25:42,013 EPOCH 2267
2024-02-05 01:25:50,672 [Epoch: 2267 Step: 00020400] Batch Recognition Loss:   0.000126 => Gls Tokens per Sec:      887 || Batch Translation Loss:   0.007422 => Txt Tokens per Sec:     2434 || Lr: 0.000100
2024-02-05 01:25:56,120 Epoch 2267: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 01:25:56,120 EPOCH 2268
2024-02-05 01:26:10,133 Epoch 2268: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 01:26:10,134 EPOCH 2269
2024-02-05 01:26:23,813 Epoch 2269: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 01:26:23,814 EPOCH 2270
2024-02-05 01:26:38,180 Epoch 2270: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 01:26:38,181 EPOCH 2271
2024-02-05 01:26:52,057 Epoch 2271: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 01:26:52,058 EPOCH 2272
2024-02-05 01:27:05,752 Epoch 2272: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 01:27:05,753 EPOCH 2273
2024-02-05 01:27:19,574 Epoch 2273: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 01:27:19,574 EPOCH 2274
2024-02-05 01:27:33,529 Epoch 2274: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 01:27:33,530 EPOCH 2275
2024-02-05 01:27:47,306 Epoch 2275: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 01:27:47,306 EPOCH 2276
2024-02-05 01:28:01,372 Epoch 2276: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 01:28:01,372 EPOCH 2277
2024-02-05 01:28:15,324 Epoch 2277: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 01:28:15,325 EPOCH 2278
2024-02-05 01:28:28,404 [Epoch: 2278 Step: 00020500] Batch Recognition Loss:   0.000129 => Gls Tokens per Sec:      617 || Batch Translation Loss:   0.007261 => Txt Tokens per Sec:     1745 || Lr: 0.000100
2024-02-05 01:28:29,378 Epoch 2278: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 01:28:29,378 EPOCH 2279
2024-02-05 01:28:43,245 Epoch 2279: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 01:28:43,245 EPOCH 2280
2024-02-05 01:28:56,935 Epoch 2280: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 01:28:56,936 EPOCH 2281
2024-02-05 01:29:10,739 Epoch 2281: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 01:29:10,740 EPOCH 2282
2024-02-05 01:29:24,595 Epoch 2282: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 01:29:24,595 EPOCH 2283
2024-02-05 01:29:38,627 Epoch 2283: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 01:29:38,628 EPOCH 2284
2024-02-05 01:29:52,393 Epoch 2284: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 01:29:52,393 EPOCH 2285
2024-02-05 01:30:06,514 Epoch 2285: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 01:30:06,514 EPOCH 2286
2024-02-05 01:30:20,477 Epoch 2286: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 01:30:20,477 EPOCH 2287
2024-02-05 01:30:34,094 Epoch 2287: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 01:30:34,094 EPOCH 2288
2024-02-05 01:30:48,323 Epoch 2288: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 01:30:48,324 EPOCH 2289
2024-02-05 01:30:57,005 [Epoch: 2289 Step: 00020600] Batch Recognition Loss:   0.000119 => Gls Tokens per Sec:     1077 || Batch Translation Loss:   0.013999 => Txt Tokens per Sec:     2884 || Lr: 0.000100
2024-02-05 01:31:01,983 Epoch 2289: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 01:31:01,984 EPOCH 2290
2024-02-05 01:31:16,150 Epoch 2290: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 01:31:16,151 EPOCH 2291
2024-02-05 01:31:29,941 Epoch 2291: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 01:31:29,941 EPOCH 2292
2024-02-05 01:31:43,843 Epoch 2292: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 01:31:43,844 EPOCH 2293
2024-02-05 01:31:57,848 Epoch 2293: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 01:31:57,849 EPOCH 2294
2024-02-05 01:32:11,564 Epoch 2294: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 01:32:11,565 EPOCH 2295
2024-02-05 01:32:25,546 Epoch 2295: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 01:32:25,547 EPOCH 2296
2024-02-05 01:32:39,426 Epoch 2296: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 01:32:39,426 EPOCH 2297
2024-02-05 01:32:53,093 Epoch 2297: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 01:32:53,094 EPOCH 2298
2024-02-05 01:33:07,328 Epoch 2298: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 01:33:07,329 EPOCH 2299
2024-02-05 01:33:21,081 Epoch 2299: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 01:33:21,082 EPOCH 2300
2024-02-05 01:33:35,098 [Epoch: 2300 Step: 00020700] Batch Recognition Loss:   0.000121 => Gls Tokens per Sec:      758 || Batch Translation Loss:   0.013609 => Txt Tokens per Sec:     2106 || Lr: 0.000100
2024-02-05 01:33:35,099 Epoch 2300: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 01:33:35,099 EPOCH 2301
2024-02-05 01:33:48,683 Epoch 2301: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 01:33:48,683 EPOCH 2302
2024-02-05 01:34:02,431 Epoch 2302: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 01:34:02,432 EPOCH 2303
2024-02-05 01:34:16,541 Epoch 2303: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 01:34:16,541 EPOCH 2304
2024-02-05 01:34:30,209 Epoch 2304: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 01:34:30,209 EPOCH 2305
2024-02-05 01:34:43,816 Epoch 2305: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 01:34:43,816 EPOCH 2306
2024-02-05 01:34:57,808 Epoch 2306: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 01:34:57,809 EPOCH 2307
2024-02-05 01:35:11,730 Epoch 2307: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 01:35:11,731 EPOCH 2308
2024-02-05 01:35:25,746 Epoch 2308: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 01:35:25,747 EPOCH 2309
2024-02-05 01:35:39,705 Epoch 2309: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 01:35:39,705 EPOCH 2310
2024-02-05 01:35:53,836 Epoch 2310: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 01:35:53,837 EPOCH 2311
2024-02-05 01:36:07,544 Epoch 2311: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-05 01:36:07,545 EPOCH 2312
2024-02-05 01:36:09,293 [Epoch: 2312 Step: 00020800] Batch Recognition Loss:   0.000177 => Gls Tokens per Sec:      733 || Batch Translation Loss:   0.062498 => Txt Tokens per Sec:     2335 || Lr: 0.000100
2024-02-05 01:36:21,151 Epoch 2312: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-05 01:36:21,151 EPOCH 2313
2024-02-05 01:36:35,579 Epoch 2313: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 01:36:35,580 EPOCH 2314
2024-02-05 01:36:49,198 Epoch 2314: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-05 01:36:49,199 EPOCH 2315
2024-02-05 01:37:03,000 Epoch 2315: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 01:37:03,000 EPOCH 2316
2024-02-05 01:37:16,915 Epoch 2316: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 01:37:16,916 EPOCH 2317
2024-02-05 01:37:30,817 Epoch 2317: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 01:37:30,818 EPOCH 2318
2024-02-05 01:37:44,983 Epoch 2318: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 01:37:44,984 EPOCH 2319
2024-02-05 01:37:58,846 Epoch 2319: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-05 01:37:58,846 EPOCH 2320
2024-02-05 01:38:12,809 Epoch 2320: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-05 01:38:12,810 EPOCH 2321
2024-02-05 01:38:26,689 Epoch 2321: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-05 01:38:26,689 EPOCH 2322
2024-02-05 01:38:40,517 Epoch 2322: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-05 01:38:40,517 EPOCH 2323
2024-02-05 01:38:41,080 [Epoch: 2323 Step: 00020900] Batch Recognition Loss:   0.000186 => Gls Tokens per Sec:     4555 || Batch Translation Loss:   0.021107 => Txt Tokens per Sec:     9335 || Lr: 0.000100
2024-02-05 01:38:54,349 Epoch 2323: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.06 
2024-02-05 01:38:54,350 EPOCH 2324
2024-02-05 01:39:07,761 Epoch 2324: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.29 
2024-02-05 01:39:07,761 EPOCH 2325
2024-02-05 01:39:21,563 Epoch 2325: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.64 
2024-02-05 01:39:21,563 EPOCH 2326
2024-02-05 01:39:35,529 Epoch 2326: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.30 
2024-02-05 01:39:35,530 EPOCH 2327
2024-02-05 01:39:49,649 Epoch 2327: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.40 
2024-02-05 01:39:49,649 EPOCH 2328
2024-02-05 01:40:03,687 Epoch 2328: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.15 
2024-02-05 01:40:03,687 EPOCH 2329
2024-02-05 01:40:17,711 Epoch 2329: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.98 
2024-02-05 01:40:17,712 EPOCH 2330
2024-02-05 01:40:31,418 Epoch 2330: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-05 01:40:31,419 EPOCH 2331
2024-02-05 01:40:45,168 Epoch 2331: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.48 
2024-02-05 01:40:45,168 EPOCH 2332
2024-02-05 01:40:59,043 Epoch 2332: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-05 01:40:59,044 EPOCH 2333
2024-02-05 01:41:13,045 Epoch 2333: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-05 01:41:13,045 EPOCH 2334
2024-02-05 01:41:20,270 [Epoch: 2334 Step: 00021000] Batch Recognition Loss:   0.000368 => Gls Tokens per Sec:      532 || Batch Translation Loss:   0.034203 => Txt Tokens per Sec:     1499 || Lr: 0.000100
2024-02-05 01:41:26,893 Epoch 2334: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-05 01:41:26,894 EPOCH 2335
2024-02-05 01:41:41,103 Epoch 2335: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-05 01:41:41,103 EPOCH 2336
2024-02-05 01:41:54,996 Epoch 2336: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 01:41:54,996 EPOCH 2337
2024-02-05 01:42:08,784 Epoch 2337: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 01:42:08,784 EPOCH 2338
2024-02-05 01:42:22,794 Epoch 2338: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 01:42:22,794 EPOCH 2339
2024-02-05 01:42:36,289 Epoch 2339: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 01:42:36,290 EPOCH 2340
2024-02-05 01:42:50,062 Epoch 2340: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 01:42:50,062 EPOCH 2341
2024-02-05 01:43:04,082 Epoch 2341: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 01:43:04,082 EPOCH 2342
2024-02-05 01:43:18,027 Epoch 2342: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 01:43:18,028 EPOCH 2343
2024-02-05 01:43:32,067 Epoch 2343: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 01:43:32,068 EPOCH 2344
2024-02-05 01:43:46,086 Epoch 2344: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 01:43:46,086 EPOCH 2345
2024-02-05 01:43:56,642 [Epoch: 2345 Step: 00021100] Batch Recognition Loss:   0.000181 => Gls Tokens per Sec:      401 || Batch Translation Loss:   0.023431 => Txt Tokens per Sec:     1256 || Lr: 0.000100
2024-02-05 01:43:59,859 Epoch 2345: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 01:43:59,859 EPOCH 2346
2024-02-05 01:44:13,940 Epoch 2346: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 01:44:13,941 EPOCH 2347
2024-02-05 01:44:27,832 Epoch 2347: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 01:44:27,832 EPOCH 2348
2024-02-05 01:44:41,651 Epoch 2348: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 01:44:41,651 EPOCH 2349
2024-02-05 01:44:55,624 Epoch 2349: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 01:44:55,625 EPOCH 2350
2024-02-05 01:45:09,442 Epoch 2350: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 01:45:09,442 EPOCH 2351
2024-02-05 01:45:23,636 Epoch 2351: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 01:45:23,637 EPOCH 2352
2024-02-05 01:45:37,234 Epoch 2352: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 01:45:37,235 EPOCH 2353
2024-02-05 01:45:51,079 Epoch 2353: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 01:45:51,079 EPOCH 2354
2024-02-05 01:46:05,188 Epoch 2354: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 01:46:05,188 EPOCH 2355
2024-02-05 01:46:19,260 Epoch 2355: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 01:46:19,261 EPOCH 2356
2024-02-05 01:46:26,220 [Epoch: 2356 Step: 00021200] Batch Recognition Loss:   0.000142 => Gls Tokens per Sec:      920 || Batch Translation Loss:   0.010465 => Txt Tokens per Sec:     2457 || Lr: 0.000100
2024-02-05 01:46:33,411 Epoch 2356: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 01:46:33,411 EPOCH 2357
2024-02-05 01:46:47,049 Epoch 2357: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 01:46:47,049 EPOCH 2358
2024-02-05 01:47:01,199 Epoch 2358: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 01:47:01,200 EPOCH 2359
2024-02-05 01:47:14,836 Epoch 2359: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 01:47:14,836 EPOCH 2360
2024-02-05 01:47:28,598 Epoch 2360: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 01:47:28,599 EPOCH 2361
2024-02-05 01:47:42,807 Epoch 2361: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 01:47:42,807 EPOCH 2362
2024-02-05 01:47:56,707 Epoch 2362: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 01:47:56,707 EPOCH 2363
2024-02-05 01:48:10,767 Epoch 2363: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 01:48:10,768 EPOCH 2364
2024-02-05 01:48:24,687 Epoch 2364: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 01:48:24,688 EPOCH 2365
2024-02-05 01:48:38,508 Epoch 2365: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 01:48:38,509 EPOCH 2366
2024-02-05 01:48:52,540 Epoch 2366: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 01:48:52,540 EPOCH 2367
2024-02-05 01:49:03,863 [Epoch: 2367 Step: 00021300] Batch Recognition Loss:   0.000122 => Gls Tokens per Sec:      600 || Batch Translation Loss:   0.013231 => Txt Tokens per Sec:     1702 || Lr: 0.000100
2024-02-05 01:49:06,461 Epoch 2367: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 01:49:06,462 EPOCH 2368
2024-02-05 01:49:20,235 Epoch 2368: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 01:49:20,236 EPOCH 2369
2024-02-05 01:49:33,923 Epoch 2369: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 01:49:33,923 EPOCH 2370
2024-02-05 01:49:47,820 Epoch 2370: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 01:49:47,820 EPOCH 2371
2024-02-05 01:50:01,441 Epoch 2371: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 01:50:01,441 EPOCH 2372
2024-02-05 01:50:15,552 Epoch 2372: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 01:50:15,552 EPOCH 2373
2024-02-05 01:50:29,555 Epoch 2373: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 01:50:29,555 EPOCH 2374
2024-02-05 01:50:43,546 Epoch 2374: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 01:50:43,546 EPOCH 2375
2024-02-05 01:50:57,158 Epoch 2375: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 01:50:57,159 EPOCH 2376
2024-02-05 01:51:11,094 Epoch 2376: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 01:51:11,095 EPOCH 2377
2024-02-05 01:51:25,231 Epoch 2377: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 01:51:25,232 EPOCH 2378
2024-02-05 01:51:32,639 [Epoch: 2378 Step: 00021400] Batch Recognition Loss:   0.000160 => Gls Tokens per Sec:     1090 || Batch Translation Loss:   0.008371 => Txt Tokens per Sec:     2831 || Lr: 0.000100
2024-02-05 01:51:40,433 Epoch 2378: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 01:51:40,433 EPOCH 2379
2024-02-05 01:51:54,421 Epoch 2379: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 01:51:54,421 EPOCH 2380
2024-02-05 01:52:08,498 Epoch 2380: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 01:52:08,499 EPOCH 2381
2024-02-05 01:52:22,229 Epoch 2381: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 01:52:22,230 EPOCH 2382
2024-02-05 01:52:36,178 Epoch 2382: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 01:52:36,178 EPOCH 2383
2024-02-05 01:52:50,362 Epoch 2383: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 01:52:50,362 EPOCH 2384
2024-02-05 01:53:04,420 Epoch 2384: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 01:53:04,421 EPOCH 2385
2024-02-05 01:53:17,973 Epoch 2385: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 01:53:17,973 EPOCH 2386
2024-02-05 01:53:31,749 Epoch 2386: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 01:53:31,750 EPOCH 2387
2024-02-05 01:53:45,824 Epoch 2387: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 01:53:45,824 EPOCH 2388
2024-02-05 01:54:00,182 Epoch 2388: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 01:54:00,182 EPOCH 2389
2024-02-05 01:54:11,852 [Epoch: 2389 Step: 00021500] Batch Recognition Loss:   0.000140 => Gls Tokens per Sec:      801 || Batch Translation Loss:   0.026173 => Txt Tokens per Sec:     2178 || Lr: 0.000100
2024-02-05 01:54:13,814 Epoch 2389: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 01:54:13,815 EPOCH 2390
2024-02-05 01:54:27,842 Epoch 2390: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 01:54:27,843 EPOCH 2391
2024-02-05 01:54:41,783 Epoch 2391: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 01:54:41,784 EPOCH 2392
2024-02-05 01:54:55,746 Epoch 2392: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 01:54:55,747 EPOCH 2393
2024-02-05 01:55:09,761 Epoch 2393: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 01:55:09,761 EPOCH 2394
2024-02-05 01:55:23,825 Epoch 2394: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 01:55:23,826 EPOCH 2395
2024-02-05 01:55:37,407 Epoch 2395: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 01:55:37,407 EPOCH 2396
2024-02-05 01:55:51,396 Epoch 2396: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 01:55:51,397 EPOCH 2397
2024-02-05 01:56:05,220 Epoch 2397: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 01:56:05,221 EPOCH 2398
2024-02-05 01:56:19,154 Epoch 2398: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 01:56:19,154 EPOCH 2399
2024-02-05 01:56:32,575 Epoch 2399: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 01:56:32,576 EPOCH 2400
2024-02-05 01:56:46,244 [Epoch: 2400 Step: 00021600] Batch Recognition Loss:   0.000154 => Gls Tokens per Sec:      778 || Batch Translation Loss:   0.021814 => Txt Tokens per Sec:     2159 || Lr: 0.000100
2024-02-05 01:56:46,245 Epoch 2400: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 01:56:46,245 EPOCH 2401
2024-02-05 01:57:00,267 Epoch 2401: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 01:57:00,268 EPOCH 2402
2024-02-05 01:57:14,043 Epoch 2402: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 01:57:14,043 EPOCH 2403
2024-02-05 01:57:27,887 Epoch 2403: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 01:57:27,887 EPOCH 2404
2024-02-05 01:57:41,916 Epoch 2404: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 01:57:41,917 EPOCH 2405
2024-02-05 01:57:55,515 Epoch 2405: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 01:57:55,516 EPOCH 2406
2024-02-05 01:58:09,603 Epoch 2406: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 01:58:09,604 EPOCH 2407
2024-02-05 01:58:23,385 Epoch 2407: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 01:58:23,386 EPOCH 2408
2024-02-05 01:58:37,227 Epoch 2408: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 01:58:37,228 EPOCH 2409
2024-02-05 01:58:51,418 Epoch 2409: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-05 01:58:51,419 EPOCH 2410
2024-02-05 01:59:05,286 Epoch 2410: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-05 01:59:05,287 EPOCH 2411
2024-02-05 01:59:19,308 Epoch 2411: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-05 01:59:19,309 EPOCH 2412
2024-02-05 01:59:19,558 [Epoch: 2412 Step: 00021700] Batch Recognition Loss:   0.000153 => Gls Tokens per Sec:     5161 || Batch Translation Loss:   0.011523 => Txt Tokens per Sec:     9198 || Lr: 0.000100
2024-02-05 01:59:33,184 Epoch 2412: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-05 01:59:33,185 EPOCH 2413
2024-02-05 01:59:47,053 Epoch 2413: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-05 01:59:47,053 EPOCH 2414
2024-02-05 02:00:00,898 Epoch 2414: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.00 
2024-02-05 02:00:00,899 EPOCH 2415
2024-02-05 02:00:14,772 Epoch 2415: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.29 
2024-02-05 02:00:14,772 EPOCH 2416
2024-02-05 02:00:28,759 Epoch 2416: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.27 
2024-02-05 02:00:28,759 EPOCH 2417
2024-02-05 02:00:42,622 Epoch 2417: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.61 
2024-02-05 02:00:42,622 EPOCH 2418
2024-02-05 02:00:56,399 Epoch 2418: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.39 
2024-02-05 02:00:56,399 EPOCH 2419
2024-02-05 02:01:10,411 Epoch 2419: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.16 
2024-02-05 02:01:10,411 EPOCH 2420
2024-02-05 02:01:24,238 Epoch 2420: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.95 
2024-02-05 02:01:24,239 EPOCH 2421
2024-02-05 02:01:38,263 Epoch 2421: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.09 
2024-02-05 02:01:38,263 EPOCH 2422
2024-02-05 02:01:52,133 Epoch 2422: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.81 
2024-02-05 02:01:52,133 EPOCH 2423
2024-02-05 02:01:56,614 [Epoch: 2423 Step: 00021800] Batch Recognition Loss:   0.000568 => Gls Tokens per Sec:      373 || Batch Translation Loss:   0.051913 => Txt Tokens per Sec:      973 || Lr: 0.000100
2024-02-05 02:02:05,821 Epoch 2423: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.63 
2024-02-05 02:02:05,821 EPOCH 2424
2024-02-05 02:02:19,901 Epoch 2424: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-05 02:02:19,902 EPOCH 2425
2024-02-05 02:02:33,697 Epoch 2425: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.72 
2024-02-05 02:02:33,697 EPOCH 2426
2024-02-05 02:02:47,637 Epoch 2426: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.67 
2024-02-05 02:02:47,638 EPOCH 2427
2024-02-05 02:03:01,842 Epoch 2427: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.79 
2024-02-05 02:03:01,843 EPOCH 2428
2024-02-05 02:03:15,715 Epoch 2428: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-05 02:03:15,716 EPOCH 2429
2024-02-05 02:03:29,479 Epoch 2429: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.48 
2024-02-05 02:03:29,480 EPOCH 2430
2024-02-05 02:03:43,217 Epoch 2430: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-05 02:03:43,217 EPOCH 2431
2024-02-05 02:03:57,251 Epoch 2431: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-05 02:03:57,252 EPOCH 2432
2024-02-05 02:04:10,847 Epoch 2432: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-05 02:04:10,847 EPOCH 2433
2024-02-05 02:04:24,992 Epoch 2433: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-05 02:04:24,992 EPOCH 2434
2024-02-05 02:04:30,687 [Epoch: 2434 Step: 00021900] Batch Recognition Loss:   0.000190 => Gls Tokens per Sec:      518 || Batch Translation Loss:   0.029943 => Txt Tokens per Sec:     1579 || Lr: 0.000100
2024-02-05 02:04:39,105 Epoch 2434: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-05 02:04:39,105 EPOCH 2435
2024-02-05 02:04:52,967 Epoch 2435: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 02:04:52,967 EPOCH 2436
2024-02-05 02:05:06,765 Epoch 2436: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 02:05:06,765 EPOCH 2437
2024-02-05 02:05:20,519 Epoch 2437: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 02:05:20,520 EPOCH 2438
2024-02-05 02:05:34,526 Epoch 2438: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 02:05:34,527 EPOCH 2439
2024-02-05 02:05:48,120 Epoch 2439: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 02:05:48,121 EPOCH 2440
2024-02-05 02:06:02,026 Epoch 2440: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 02:06:02,026 EPOCH 2441
2024-02-05 02:06:15,983 Epoch 2441: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 02:06:15,984 EPOCH 2442
2024-02-05 02:06:29,949 Epoch 2442: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 02:06:29,949 EPOCH 2443
2024-02-05 02:06:43,857 Epoch 2443: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 02:06:43,858 EPOCH 2444
2024-02-05 02:06:57,819 Epoch 2444: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 02:06:57,820 EPOCH 2445
2024-02-05 02:07:05,242 [Epoch: 2445 Step: 00022000] Batch Recognition Loss:   0.000182 => Gls Tokens per Sec:      690 || Batch Translation Loss:   0.020014 => Txt Tokens per Sec:     1941 || Lr: 0.000100
2024-02-05 02:07:34,783 Validation result at epoch 2445, step    22000: duration: 29.5395s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00016	Translation Loss: 96149.03125	PPL: 15087.74707
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.58	(BLEU-1: 11.10,	BLEU-2: 3.22,	BLEU-3: 1.26,	BLEU-4: 0.58)
	CHRF 17.14	ROUGE 9.29
2024-02-05 02:07:34,785 Logging Recognition and Translation Outputs
2024-02-05 02:07:34,785 ========================================================================================================================
2024-02-05 02:07:34,785 Logging Sequence: 146_56.00
2024-02-05 02:07:34,785 	Gloss Reference :	A B+C+D+E
2024-02-05 02:07:34,785 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 02:07:34,786 	Gloss Alignment :	         
2024-02-05 02:07:34,786 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 02:07:34,788 	Text Reference  :	when the players go      back   to   the hotel as per rules all of    them have    to      undergo rtpcr test      for covid-19 everyday
2024-02-05 02:07:34,788 	Text Hypothesis :	**** the indian  premier league 2023 the ***** ** *** ***** *** world test matches between kkr     and   australia and the      match   
2024-02-05 02:07:34,788 	Text Alignment  :	D        S       S       S      S        D     D  D   D     D   S     S    S       S       S       S     S         S   S        S       
2024-02-05 02:07:34,788 ========================================================================================================================
2024-02-05 02:07:34,788 Logging Sequence: 118_338.00
2024-02-05 02:07:34,789 	Gloss Reference :	A B+C+D+E
2024-02-05 02:07:34,789 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 02:07:34,789 	Gloss Alignment :	         
2024-02-05 02:07:34,789 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 02:07:34,789 	Text Reference  :	*** *** this   is         why even messi wore it        
2024-02-05 02:07:34,790 	Text Hypothesis :	and the second federation or  left for   the  tournament
2024-02-05 02:07:34,790 	Text Alignment  :	I   I   S      S          S   S    S     S    S         
2024-02-05 02:07:34,790 ========================================================================================================================
2024-02-05 02:07:34,790 Logging Sequence: 66_61.00
2024-02-05 02:07:34,791 	Gloss Reference :	A B+C+D+E
2024-02-05 02:07:34,791 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 02:07:34,791 	Gloss Alignment :	         
2024-02-05 02:07:34,791 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 02:07:34,792 	Text Reference  :	instead of returning back      to     his homeland because of     his injury
2024-02-05 02:07:34,792 	Text Hypothesis :	******* ** ********* rajasthan royals ben stokes   had     broken his finger
2024-02-05 02:07:34,792 	Text Alignment  :	D       D  D         S         S      S   S        S       S          S     
2024-02-05 02:07:34,792 ========================================================================================================================
2024-02-05 02:07:34,792 Logging Sequence: 81_278.00
2024-02-05 02:07:34,793 	Gloss Reference :	A B+C+D+E
2024-02-05 02:07:34,793 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 02:07:34,793 	Gloss Alignment :	         
2024-02-05 02:07:34,793 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 02:07:34,794 	Text Reference  :	of this amrapali group paid rs 3570 crore the remaining rs 652 crore was paid by      amrapali sapphire developers a ******** subsidiary of amrapali group   
2024-02-05 02:07:34,795 	Text Hypothesis :	** **** ******** ***** **** ** **** ***** the ********* ** *** ***** *** **** supreme court    then     ordered    a forensic audit      of amrapali builders
2024-02-05 02:07:34,795 	Text Alignment  :	D  D    D        D     D    D  D    D         D         D  D   D     D   D    S       S        S        S            I        S                      S       
2024-02-05 02:07:34,795 ========================================================================================================================
2024-02-05 02:07:34,795 Logging Sequence: 162_125.00
2024-02-05 02:07:34,795 	Gloss Reference :	A B+C+D+E
2024-02-05 02:07:34,795 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 02:07:34,795 	Gloss Alignment :	         
2024-02-05 02:07:34,795 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 02:07:34,797 	Text Reference  :	**** *** *** ********* *** in response to  this kohli     received many  hate comments on   social media  
2024-02-05 02:07:34,797 	Text Hypothesis :	fans are now wondering why he did      not have returning to       india and  he       then the    country
2024-02-05 02:07:34,797 	Text Alignment  :	I    I   I   I         I   S  S        S   S    S         S        S     S    S        S    S      S      
2024-02-05 02:07:34,797 ========================================================================================================================
2024-02-05 02:07:41,219 Epoch 2445: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 02:07:41,219 EPOCH 2446
2024-02-05 02:07:55,014 Epoch 2446: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 02:07:55,014 EPOCH 2447
2024-02-05 02:08:08,897 Epoch 2447: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 02:08:08,898 EPOCH 2448
2024-02-05 02:08:22,591 Epoch 2448: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 02:08:22,591 EPOCH 2449
2024-02-05 02:08:36,501 Epoch 2449: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 02:08:36,501 EPOCH 2450
2024-02-05 02:08:50,515 Epoch 2450: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 02:08:50,515 EPOCH 2451
2024-02-05 02:09:04,494 Epoch 2451: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 02:09:04,495 EPOCH 2452
2024-02-05 02:09:18,413 Epoch 2452: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 02:09:18,414 EPOCH 2453
2024-02-05 02:09:32,608 Epoch 2453: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 02:09:32,609 EPOCH 2454
2024-02-05 02:09:46,606 Epoch 2454: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 02:09:46,607 EPOCH 2455
2024-02-05 02:10:00,730 Epoch 2455: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 02:10:00,731 EPOCH 2456
2024-02-05 02:10:04,489 [Epoch: 2456 Step: 00022100] Batch Recognition Loss:   0.000145 => Gls Tokens per Sec:     1703 || Batch Translation Loss:   0.021152 => Txt Tokens per Sec:     4750 || Lr: 0.000100
2024-02-05 02:10:14,555 Epoch 2456: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 02:10:14,556 EPOCH 2457
2024-02-05 02:10:28,344 Epoch 2457: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-05 02:10:28,345 EPOCH 2458
2024-02-05 02:10:42,545 Epoch 2458: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-05 02:10:42,546 EPOCH 2459
2024-02-05 02:10:56,334 Epoch 2459: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 02:10:56,335 EPOCH 2460
2024-02-05 02:11:10,270 Epoch 2460: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 02:11:10,271 EPOCH 2461
2024-02-05 02:11:24,320 Epoch 2461: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 02:11:24,321 EPOCH 2462
2024-02-05 02:11:37,901 Epoch 2462: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 02:11:37,901 EPOCH 2463
2024-02-05 02:11:51,942 Epoch 2463: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 02:11:51,943 EPOCH 2464
2024-02-05 02:12:05,848 Epoch 2464: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 02:12:05,849 EPOCH 2465
2024-02-05 02:12:19,824 Epoch 2465: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 02:12:19,825 EPOCH 2466
2024-02-05 02:12:33,779 Epoch 2466: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 02:12:33,780 EPOCH 2467
2024-02-05 02:12:42,576 [Epoch: 2467 Step: 00022200] Batch Recognition Loss:   0.000162 => Gls Tokens per Sec:      873 || Batch Translation Loss:   0.020720 => Txt Tokens per Sec:     2501 || Lr: 0.000100
2024-02-05 02:12:47,653 Epoch 2467: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 02:12:47,653 EPOCH 2468
2024-02-05 02:13:01,410 Epoch 2468: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 02:13:01,411 EPOCH 2469
2024-02-05 02:13:15,445 Epoch 2469: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-05 02:13:15,445 EPOCH 2470
2024-02-05 02:13:29,587 Epoch 2470: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-05 02:13:29,588 EPOCH 2471
2024-02-05 02:13:43,689 Epoch 2471: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-05 02:13:43,690 EPOCH 2472
2024-02-05 02:13:57,473 Epoch 2472: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-05 02:13:57,474 EPOCH 2473
2024-02-05 02:14:11,575 Epoch 2473: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 02:14:11,576 EPOCH 2474
2024-02-05 02:14:25,198 Epoch 2474: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-05 02:14:25,199 EPOCH 2475
2024-02-05 02:14:38,892 Epoch 2475: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-05 02:14:38,893 EPOCH 2476
2024-02-05 02:14:52,835 Epoch 2476: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 02:14:52,836 EPOCH 2477
2024-02-05 02:15:06,663 Epoch 2477: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 02:15:06,664 EPOCH 2478
2024-02-05 02:15:16,141 [Epoch: 2478 Step: 00022300] Batch Recognition Loss:   0.000188 => Gls Tokens per Sec:      946 || Batch Translation Loss:   0.025845 => Txt Tokens per Sec:     2726 || Lr: 0.000100
2024-02-05 02:15:20,835 Epoch 2478: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 02:15:20,835 EPOCH 2479
2024-02-05 02:15:34,864 Epoch 2479: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 02:15:34,865 EPOCH 2480
2024-02-05 02:15:48,599 Epoch 2480: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 02:15:48,600 EPOCH 2481
2024-02-05 02:16:02,368 Epoch 2481: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 02:16:02,369 EPOCH 2482
2024-02-05 02:16:16,510 Epoch 2482: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 02:16:16,510 EPOCH 2483
2024-02-05 02:16:30,666 Epoch 2483: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 02:16:30,666 EPOCH 2484
2024-02-05 02:16:44,793 Epoch 2484: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 02:16:44,794 EPOCH 2485
2024-02-05 02:16:58,936 Epoch 2485: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 02:16:58,936 EPOCH 2486
2024-02-05 02:17:12,619 Epoch 2486: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 02:17:12,619 EPOCH 2487
2024-02-05 02:17:26,534 Epoch 2487: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 02:17:26,535 EPOCH 2488
2024-02-05 02:17:40,496 Epoch 2488: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 02:17:40,497 EPOCH 2489
2024-02-05 02:17:52,819 [Epoch: 2489 Step: 00022400] Batch Recognition Loss:   0.000144 => Gls Tokens per Sec:      759 || Batch Translation Loss:   0.025987 => Txt Tokens per Sec:     2063 || Lr: 0.000100
2024-02-05 02:17:54,571 Epoch 2489: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 02:17:54,571 EPOCH 2490
2024-02-05 02:18:08,518 Epoch 2490: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 02:18:08,518 EPOCH 2491
2024-02-05 02:18:22,360 Epoch 2491: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 02:18:22,360 EPOCH 2492
2024-02-05 02:18:35,960 Epoch 2492: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-05 02:18:35,960 EPOCH 2493
2024-02-05 02:18:49,777 Epoch 2493: Total Training Recognition Loss 0.00  Total Training Translation Loss 2.12 
2024-02-05 02:18:49,778 EPOCH 2494
2024-02-05 02:19:03,579 Epoch 2494: Total Training Recognition Loss 0.04  Total Training Translation Loss 5.75 
2024-02-05 02:19:03,579 EPOCH 2495
2024-02-05 02:19:17,482 Epoch 2495: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.81 
2024-02-05 02:19:17,483 EPOCH 2496
2024-02-05 02:19:31,158 Epoch 2496: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.73 
2024-02-05 02:19:31,158 EPOCH 2497
2024-02-05 02:19:45,182 Epoch 2497: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.33 
2024-02-05 02:19:45,183 EPOCH 2498
2024-02-05 02:19:58,939 Epoch 2498: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.46 
2024-02-05 02:19:58,940 EPOCH 2499
2024-02-05 02:20:13,099 Epoch 2499: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.07 
2024-02-05 02:20:13,100 EPOCH 2500
2024-02-05 02:20:27,049 [Epoch: 2500 Step: 00022500] Batch Recognition Loss:   0.000792 => Gls Tokens per Sec:      762 || Batch Translation Loss:   0.759501 => Txt Tokens per Sec:     2115 || Lr: 0.000100
2024-02-05 02:20:27,050 Epoch 2500: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.74 
2024-02-05 02:20:27,050 EPOCH 2501
2024-02-05 02:20:41,097 Epoch 2501: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.34 
2024-02-05 02:20:41,097 EPOCH 2502
2024-02-05 02:20:54,996 Epoch 2502: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.99 
2024-02-05 02:20:54,997 EPOCH 2503
2024-02-05 02:21:08,739 Epoch 2503: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-05 02:21:08,739 EPOCH 2504
2024-02-05 02:21:22,706 Epoch 2504: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-05 02:21:22,706 EPOCH 2505
2024-02-05 02:21:36,530 Epoch 2505: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-05 02:21:36,531 EPOCH 2506
2024-02-05 02:21:50,418 Epoch 2506: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-05 02:21:50,418 EPOCH 2507
2024-02-05 02:22:04,419 Epoch 2507: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-05 02:22:04,420 EPOCH 2508
2024-02-05 02:22:18,344 Epoch 2508: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 02:22:18,344 EPOCH 2509
2024-02-05 02:22:32,579 Epoch 2509: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 02:22:32,579 EPOCH 2510
2024-02-05 02:22:46,476 Epoch 2510: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 02:22:46,477 EPOCH 2511
2024-02-05 02:23:00,092 Epoch 2511: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 02:23:00,092 EPOCH 2512
2024-02-05 02:23:05,063 [Epoch: 2512 Step: 00022600] Batch Recognition Loss:   0.000341 => Gls Tokens per Sec:      258 || Batch Translation Loss:   0.025315 => Txt Tokens per Sec:      902 || Lr: 0.000100
2024-02-05 02:23:14,045 Epoch 2512: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 02:23:14,046 EPOCH 2513
2024-02-05 02:23:27,832 Epoch 2513: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 02:23:27,833 EPOCH 2514
2024-02-05 02:23:41,958 Epoch 2514: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 02:23:41,959 EPOCH 2515
2024-02-05 02:23:55,870 Epoch 2515: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 02:23:55,870 EPOCH 2516
2024-02-05 02:24:09,704 Epoch 2516: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 02:24:09,705 EPOCH 2517
2024-02-05 02:24:24,196 Epoch 2517: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 02:24:24,197 EPOCH 2518
2024-02-05 02:24:38,093 Epoch 2518: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 02:24:38,093 EPOCH 2519
2024-02-05 02:24:52,016 Epoch 2519: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 02:24:52,017 EPOCH 2520
2024-02-05 02:25:05,908 Epoch 2520: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 02:25:05,909 EPOCH 2521
2024-02-05 02:25:19,523 Epoch 2521: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 02:25:19,523 EPOCH 2522
2024-02-05 02:25:33,360 Epoch 2522: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 02:25:33,361 EPOCH 2523
2024-02-05 02:25:34,423 [Epoch: 2523 Step: 00022700] Batch Recognition Loss:   0.000175 => Gls Tokens per Sec:     2414 || Batch Translation Loss:   0.013956 => Txt Tokens per Sec:     6252 || Lr: 0.000100
2024-02-05 02:25:47,322 Epoch 2523: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 02:25:47,322 EPOCH 2524
2024-02-05 02:26:01,470 Epoch 2524: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 02:26:01,470 EPOCH 2525
2024-02-05 02:26:15,426 Epoch 2525: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 02:26:15,427 EPOCH 2526
2024-02-05 02:26:29,174 Epoch 2526: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 02:26:29,175 EPOCH 2527
2024-02-05 02:26:43,099 Epoch 2527: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 02:26:43,100 EPOCH 2528
2024-02-05 02:26:57,107 Epoch 2528: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 02:26:57,108 EPOCH 2529
2024-02-05 02:27:10,959 Epoch 2529: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 02:27:10,959 EPOCH 2530
2024-02-05 02:27:24,816 Epoch 2530: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 02:27:24,817 EPOCH 2531
2024-02-05 02:27:38,491 Epoch 2531: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 02:27:38,492 EPOCH 2532
2024-02-05 02:27:52,460 Epoch 2532: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 02:27:52,461 EPOCH 2533
2024-02-05 02:28:06,303 Epoch 2533: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 02:28:06,304 EPOCH 2534
2024-02-05 02:28:07,489 [Epoch: 2534 Step: 00022800] Batch Recognition Loss:   0.000135 => Gls Tokens per Sec:     3240 || Batch Translation Loss:   0.015292 => Txt Tokens per Sec:     8342 || Lr: 0.000100
2024-02-05 02:28:20,073 Epoch 2534: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 02:28:20,074 EPOCH 2535
2024-02-05 02:28:33,905 Epoch 2535: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 02:28:33,906 EPOCH 2536
2024-02-05 02:28:48,111 Epoch 2536: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 02:28:48,112 EPOCH 2537
2024-02-05 02:29:02,122 Epoch 2537: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 02:29:02,123 EPOCH 2538
2024-02-05 02:29:16,081 Epoch 2538: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 02:29:16,082 EPOCH 2539
2024-02-05 02:29:29,791 Epoch 2539: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 02:29:29,792 EPOCH 2540
2024-02-05 02:29:43,815 Epoch 2540: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 02:29:43,816 EPOCH 2541
2024-02-05 02:29:57,990 Epoch 2541: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 02:29:57,990 EPOCH 2542
2024-02-05 02:30:11,806 Epoch 2542: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 02:30:11,807 EPOCH 2543
2024-02-05 02:30:25,770 Epoch 2543: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 02:30:25,771 EPOCH 2544
2024-02-05 02:30:39,712 Epoch 2544: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 02:30:39,713 EPOCH 2545
2024-02-05 02:30:46,614 [Epoch: 2545 Step: 00022900] Batch Recognition Loss:   0.000145 => Gls Tokens per Sec:      613 || Batch Translation Loss:   0.006943 => Txt Tokens per Sec:     1686 || Lr: 0.000100
2024-02-05 02:30:53,469 Epoch 2545: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 02:30:53,469 EPOCH 2546
2024-02-05 02:31:07,602 Epoch 2546: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 02:31:07,603 EPOCH 2547
2024-02-05 02:31:21,517 Epoch 2547: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 02:31:21,517 EPOCH 2548
2024-02-05 02:31:35,266 Epoch 2548: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 02:31:35,267 EPOCH 2549
2024-02-05 02:31:49,316 Epoch 2549: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 02:31:49,316 EPOCH 2550
2024-02-05 02:32:03,217 Epoch 2550: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 02:32:03,217 EPOCH 2551
2024-02-05 02:32:17,023 Epoch 2551: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 02:32:17,024 EPOCH 2552
2024-02-05 02:32:30,928 Epoch 2552: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 02:32:30,928 EPOCH 2553
2024-02-05 02:32:44,882 Epoch 2553: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 02:32:44,883 EPOCH 2554
2024-02-05 02:32:58,697 Epoch 2554: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 02:32:58,698 EPOCH 2555
2024-02-05 02:33:12,721 Epoch 2555: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 02:33:12,721 EPOCH 2556
2024-02-05 02:33:19,798 [Epoch: 2556 Step: 00023000] Batch Recognition Loss:   0.000146 => Gls Tokens per Sec:      905 || Batch Translation Loss:   0.014045 => Txt Tokens per Sec:     2444 || Lr: 0.000100
2024-02-05 02:33:26,720 Epoch 2556: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 02:33:26,721 EPOCH 2557
2024-02-05 02:33:40,558 Epoch 2557: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 02:33:40,559 EPOCH 2558
2024-02-05 02:33:54,008 Epoch 2558: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 02:33:54,008 EPOCH 2559
2024-02-05 02:34:08,191 Epoch 2559: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 02:34:08,191 EPOCH 2560
2024-02-05 02:34:21,991 Epoch 2560: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 02:34:21,991 EPOCH 2561
2024-02-05 02:34:36,084 Epoch 2561: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 02:34:36,085 EPOCH 2562
2024-02-05 02:34:50,121 Epoch 2562: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 02:34:50,121 EPOCH 2563
2024-02-05 02:35:03,825 Epoch 2563: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 02:35:03,825 EPOCH 2564
2024-02-05 02:35:17,674 Epoch 2564: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 02:35:17,675 EPOCH 2565
2024-02-05 02:35:31,859 Epoch 2565: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 02:35:31,860 EPOCH 2566
2024-02-05 02:35:45,697 Epoch 2566: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 02:35:45,697 EPOCH 2567
2024-02-05 02:35:57,339 [Epoch: 2567 Step: 00023100] Batch Recognition Loss:   0.000138 => Gls Tokens per Sec:      583 || Batch Translation Loss:   0.021824 => Txt Tokens per Sec:     1737 || Lr: 0.000100
2024-02-05 02:35:59,660 Epoch 2567: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 02:35:59,660 EPOCH 2568
2024-02-05 02:36:13,413 Epoch 2568: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 02:36:13,414 EPOCH 2569
2024-02-05 02:36:27,041 Epoch 2569: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 02:36:27,041 EPOCH 2570
2024-02-05 02:36:41,187 Epoch 2570: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 02:36:41,187 EPOCH 2571
2024-02-05 02:36:55,373 Epoch 2571: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 02:36:55,374 EPOCH 2572
2024-02-05 02:37:09,233 Epoch 2572: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 02:37:09,234 EPOCH 2573
2024-02-05 02:37:23,180 Epoch 2573: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 02:37:23,180 EPOCH 2574
2024-02-05 02:37:37,077 Epoch 2574: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 02:37:37,078 EPOCH 2575
2024-02-05 02:37:51,052 Epoch 2575: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 02:37:51,052 EPOCH 2576
2024-02-05 02:38:04,730 Epoch 2576: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 02:38:04,731 EPOCH 2577
2024-02-05 02:38:18,486 Epoch 2577: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 02:38:18,487 EPOCH 2578
2024-02-05 02:38:29,846 [Epoch: 2578 Step: 00023200] Batch Recognition Loss:   0.000117 => Gls Tokens per Sec:      710 || Batch Translation Loss:   0.008239 => Txt Tokens per Sec:     1895 || Lr: 0.000100
2024-02-05 02:38:32,406 Epoch 2578: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 02:38:32,407 EPOCH 2579
2024-02-05 02:38:46,331 Epoch 2579: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 02:38:46,332 EPOCH 2580
2024-02-05 02:39:00,084 Epoch 2580: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 02:39:00,085 EPOCH 2581
2024-02-05 02:39:13,824 Epoch 2581: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 02:39:13,824 EPOCH 2582
2024-02-05 02:39:27,954 Epoch 2582: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 02:39:27,954 EPOCH 2583
2024-02-05 02:39:41,824 Epoch 2583: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-05 02:39:41,825 EPOCH 2584
2024-02-05 02:39:55,878 Epoch 2584: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-05 02:39:55,878 EPOCH 2585
2024-02-05 02:40:09,792 Epoch 2585: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.76 
2024-02-05 02:40:09,793 EPOCH 2586
2024-02-05 02:40:23,510 Epoch 2586: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.63 
2024-02-05 02:40:23,510 EPOCH 2587
2024-02-05 02:40:37,162 Epoch 2587: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.75 
2024-02-05 02:40:37,163 EPOCH 2588
2024-02-05 02:40:51,022 Epoch 2588: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.56 
2024-02-05 02:40:51,023 EPOCH 2589
2024-02-05 02:40:59,965 [Epoch: 2589 Step: 00023300] Batch Recognition Loss:   0.000383 => Gls Tokens per Sec:     1046 || Batch Translation Loss:   0.095416 => Txt Tokens per Sec:     2799 || Lr: 0.000100
2024-02-05 02:41:04,980 Epoch 2589: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-05 02:41:04,980 EPOCH 2590
2024-02-05 02:41:18,642 Epoch 2590: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-05 02:41:18,643 EPOCH 2591
2024-02-05 02:41:32,798 Epoch 2591: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.57 
2024-02-05 02:41:32,798 EPOCH 2592
2024-02-05 02:41:46,508 Epoch 2592: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.56 
2024-02-05 02:41:46,509 EPOCH 2593
2024-02-05 02:42:00,229 Epoch 2593: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.69 
2024-02-05 02:42:00,230 EPOCH 2594
2024-02-05 02:42:14,291 Epoch 2594: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.63 
2024-02-05 02:42:14,292 EPOCH 2595
2024-02-05 02:42:28,230 Epoch 2595: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.59 
2024-02-05 02:42:28,230 EPOCH 2596
2024-02-05 02:42:42,258 Epoch 2596: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-05 02:42:42,258 EPOCH 2597
2024-02-05 02:42:55,988 Epoch 2597: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-05 02:42:55,989 EPOCH 2598
2024-02-05 02:43:09,321 Epoch 2598: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-05 02:43:09,321 EPOCH 2599
2024-02-05 02:43:23,191 Epoch 2599: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-05 02:43:23,192 EPOCH 2600
2024-02-05 02:43:37,164 [Epoch: 2600 Step: 00023400] Batch Recognition Loss:   0.000254 => Gls Tokens per Sec:      761 || Batch Translation Loss:   0.020542 => Txt Tokens per Sec:     2112 || Lr: 0.000100
2024-02-05 02:43:37,165 Epoch 2600: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-05 02:43:37,165 EPOCH 2601
2024-02-05 02:43:51,092 Epoch 2601: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-05 02:43:51,092 EPOCH 2602
2024-02-05 02:44:05,152 Epoch 2602: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-05 02:44:05,152 EPOCH 2603
2024-02-05 02:44:18,996 Epoch 2603: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-05 02:44:18,997 EPOCH 2604
2024-02-05 02:44:32,904 Epoch 2604: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-05 02:44:32,905 EPOCH 2605
2024-02-05 02:44:46,773 Epoch 2605: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-05 02:44:46,774 EPOCH 2606
2024-02-05 02:45:00,654 Epoch 2606: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-05 02:45:00,655 EPOCH 2607
2024-02-05 02:45:14,488 Epoch 2607: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-05 02:45:14,489 EPOCH 2608
2024-02-05 02:45:28,271 Epoch 2608: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 02:45:28,272 EPOCH 2609
2024-02-05 02:45:42,278 Epoch 2609: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 02:45:42,278 EPOCH 2610
2024-02-05 02:45:56,080 Epoch 2610: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 02:45:56,080 EPOCH 2611
2024-02-05 02:46:09,961 Epoch 2611: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 02:46:09,962 EPOCH 2612
2024-02-05 02:46:10,431 [Epoch: 2612 Step: 00023500] Batch Recognition Loss:   0.000162 => Gls Tokens per Sec:     2735 || Batch Translation Loss:   0.016181 => Txt Tokens per Sec:     6771 || Lr: 0.000100
2024-02-05 02:46:24,019 Epoch 2612: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 02:46:24,019 EPOCH 2613
2024-02-05 02:46:37,866 Epoch 2613: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 02:46:37,867 EPOCH 2614
2024-02-05 02:46:52,031 Epoch 2614: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 02:46:52,031 EPOCH 2615
2024-02-05 02:47:05,811 Epoch 2615: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 02:47:05,811 EPOCH 2616
2024-02-05 02:47:19,425 Epoch 2616: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 02:47:19,426 EPOCH 2617
2024-02-05 02:47:32,889 Epoch 2617: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 02:47:32,889 EPOCH 2618
2024-02-05 02:47:46,961 Epoch 2618: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 02:47:46,961 EPOCH 2619
2024-02-05 02:48:00,669 Epoch 2619: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 02:48:00,669 EPOCH 2620
2024-02-05 02:48:14,801 Epoch 2620: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 02:48:14,801 EPOCH 2621
2024-02-05 02:48:28,792 Epoch 2621: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 02:48:28,793 EPOCH 2622
2024-02-05 02:48:42,898 Epoch 2622: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 02:48:42,898 EPOCH 2623
2024-02-05 02:48:48,763 [Epoch: 2623 Step: 00023600] Batch Recognition Loss:   0.000159 => Gls Tokens per Sec:      437 || Batch Translation Loss:   0.026941 => Txt Tokens per Sec:     1421 || Lr: 0.000100
2024-02-05 02:48:56,842 Epoch 2623: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 02:48:56,842 EPOCH 2624
2024-02-05 02:49:10,381 Epoch 2624: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 02:49:10,381 EPOCH 2625
2024-02-05 02:49:24,294 Epoch 2625: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 02:49:24,294 EPOCH 2626
2024-02-05 02:49:38,488 Epoch 2626: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 02:49:38,488 EPOCH 2627
2024-02-05 02:49:52,358 Epoch 2627: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 02:49:52,359 EPOCH 2628
2024-02-05 02:50:06,246 Epoch 2628: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 02:50:06,246 EPOCH 2629
2024-02-05 02:50:20,088 Epoch 2629: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 02:50:20,088 EPOCH 2630
2024-02-05 02:50:33,928 Epoch 2630: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 02:50:33,929 EPOCH 2631
2024-02-05 02:50:47,825 Epoch 2631: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 02:50:47,825 EPOCH 2632
2024-02-05 02:51:01,849 Epoch 2632: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 02:51:01,850 EPOCH 2633
2024-02-05 02:51:15,412 Epoch 2633: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 02:51:15,412 EPOCH 2634
2024-02-05 02:51:20,691 [Epoch: 2634 Step: 00023700] Batch Recognition Loss:   0.000126 => Gls Tokens per Sec:      559 || Batch Translation Loss:   0.006925 => Txt Tokens per Sec:     1434 || Lr: 0.000100
2024-02-05 02:51:29,290 Epoch 2634: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 02:51:29,290 EPOCH 2635
2024-02-05 02:51:43,487 Epoch 2635: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 02:51:43,488 EPOCH 2636
2024-02-05 02:51:57,281 Epoch 2636: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 02:51:57,281 EPOCH 2637
2024-02-05 02:52:11,334 Epoch 2637: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 02:52:11,335 EPOCH 2638
2024-02-05 02:52:25,203 Epoch 2638: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 02:52:25,203 EPOCH 2639
2024-02-05 02:52:39,340 Epoch 2639: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 02:52:39,341 EPOCH 2640
2024-02-05 02:52:53,052 Epoch 2640: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 02:52:53,052 EPOCH 2641
2024-02-05 02:53:06,936 Epoch 2641: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 02:53:06,937 EPOCH 2642
2024-02-05 02:53:20,480 Epoch 2642: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 02:53:20,480 EPOCH 2643
2024-02-05 02:53:34,743 Epoch 2643: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 02:53:34,744 EPOCH 2644
2024-02-05 02:53:48,607 Epoch 2644: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 02:53:48,607 EPOCH 2645
2024-02-05 02:53:58,941 [Epoch: 2645 Step: 00023800] Batch Recognition Loss:   0.000120 => Gls Tokens per Sec:      409 || Batch Translation Loss:   0.008128 => Txt Tokens per Sec:     1164 || Lr: 0.000100
2024-02-05 02:54:02,513 Epoch 2645: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 02:54:02,513 EPOCH 2646
2024-02-05 02:54:16,403 Epoch 2646: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 02:54:16,403 EPOCH 2647
2024-02-05 02:54:30,294 Epoch 2647: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 02:54:30,294 EPOCH 2648
2024-02-05 02:54:44,096 Epoch 2648: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 02:54:44,096 EPOCH 2649
2024-02-05 02:54:58,329 Epoch 2649: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 02:54:58,329 EPOCH 2650
2024-02-05 02:55:12,341 Epoch 2650: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 02:55:12,341 EPOCH 2651
2024-02-05 02:55:26,172 Epoch 2651: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 02:55:26,172 EPOCH 2652
2024-02-05 02:55:40,151 Epoch 2652: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 02:55:40,152 EPOCH 2653
2024-02-05 02:55:54,007 Epoch 2653: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 02:55:54,007 EPOCH 2654
2024-02-05 02:56:07,833 Epoch 2654: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 02:56:07,834 EPOCH 2655
2024-02-05 02:56:21,694 Epoch 2655: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 02:56:21,695 EPOCH 2656
2024-02-05 02:56:29,359 [Epoch: 2656 Step: 00023900] Batch Recognition Loss:   0.000124 => Gls Tokens per Sec:      719 || Batch Translation Loss:   0.011215 => Txt Tokens per Sec:     1926 || Lr: 0.000100
2024-02-05 02:56:35,721 Epoch 2656: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 02:56:35,721 EPOCH 2657
2024-02-05 02:56:49,441 Epoch 2657: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 02:56:49,441 EPOCH 2658
2024-02-05 02:57:03,136 Epoch 2658: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 02:57:03,137 EPOCH 2659
2024-02-05 02:57:17,195 Epoch 2659: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 02:57:17,196 EPOCH 2660
2024-02-05 02:57:31,080 Epoch 2660: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 02:57:31,081 EPOCH 2661
2024-02-05 02:57:45,041 Epoch 2661: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 02:57:45,041 EPOCH 2662
2024-02-05 02:57:58,801 Epoch 2662: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 02:57:58,801 EPOCH 2663
2024-02-05 02:58:12,713 Epoch 2663: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 02:58:12,714 EPOCH 2664
2024-02-05 02:58:26,905 Epoch 2664: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 02:58:26,906 EPOCH 2665
2024-02-05 02:58:40,727 Epoch 2665: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 02:58:40,728 EPOCH 2666
2024-02-05 02:58:54,425 Epoch 2666: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-05 02:58:54,425 EPOCH 2667
2024-02-05 02:59:07,337 [Epoch: 2667 Step: 00024000] Batch Recognition Loss:   0.000167 => Gls Tokens per Sec:      526 || Batch Translation Loss:   0.033311 => Txt Tokens per Sec:     1538 || Lr: 0.000100
2024-02-05 02:59:36,217 Validation result at epoch 2667, step    24000: duration: 28.8787s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00017	Translation Loss: 96143.27344	PPL: 15079.05957
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.83	(BLEU-1: 11.51,	BLEU-2: 3.78,	BLEU-3: 1.61,	BLEU-4: 0.83)
	CHRF 17.32	ROUGE 9.93
2024-02-05 02:59:36,218 Logging Recognition and Translation Outputs
2024-02-05 02:59:36,218 ========================================================================================================================
2024-02-05 02:59:36,219 Logging Sequence: 169_165.00
2024-02-05 02:59:36,219 	Gloss Reference :	A B+C+D+E
2024-02-05 02:59:36,219 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 02:59:36,219 	Gloss Alignment :	         
2024-02-05 02:59:36,220 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 02:59:36,221 	Text Reference  :	** the indian government was       outraged by          the incident and  these changes were undone by         wikipedia
2024-02-05 02:59:36,221 	Text Hypothesis :	do you know   that       wikipedia provides information on  celebs   like their height  age  family background etc      
2024-02-05 02:59:36,221 	Text Alignment  :	I  S   S      S          S         S        S           S   S        S    S     S       S    S      S          S        
2024-02-05 02:59:36,221 ========================================================================================================================
2024-02-05 02:59:36,222 Logging Sequence: 175_60.00
2024-02-05 02:59:36,222 	Gloss Reference :	A B+C+D+E
2024-02-05 02:59:36,222 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 02:59:36,222 	Gloss Alignment :	         
2024-02-05 02:59:36,222 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 02:59:36,223 	Text Reference  :	******* that is    how    india bagged 9   medals in       the youth tournament
2024-02-05 02:59:36,223 	Text Hypothesis :	despite the  delay people were  all    out and    pakistan has been  made      
2024-02-05 02:59:36,223 	Text Alignment  :	I       S    S     S      S     S      S   S      S        S   S     S         
2024-02-05 02:59:36,224 ========================================================================================================================
2024-02-05 02:59:36,224 Logging Sequence: 61_255.00
2024-02-05 02:59:36,224 	Gloss Reference :	A B+C+D+E
2024-02-05 02:59:36,224 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 02:59:36,224 	Gloss Alignment :	         
2024-02-05 02:59:36,225 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 02:59:36,225 	Text Reference  :	**** *** in   2011 we    decided to marry and    informed our families
2024-02-05 02:59:36,226 	Text Hypothesis :	when rcb lost the  match fees    by the   couple were     all out     
2024-02-05 02:59:36,226 	Text Alignment  :	I    I   S    S    S     S       S  S     S      S        S   S       
2024-02-05 02:59:36,226 ========================================================================================================================
2024-02-05 02:59:36,226 Logging Sequence: 173_39.00
2024-02-05 02:59:36,226 	Gloss Reference :	A B+C+D+E
2024-02-05 02:59:36,226 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 02:59:36,227 	Gloss Alignment :	         
2024-02-05 02:59:36,227 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 02:59:36,227 	Text Reference  :	** *** ******* kohli will step      down as   india' captain
2024-02-05 02:59:36,227 	Text Hypothesis :	as two opening game  was  extremely fit  when the    captain
2024-02-05 02:59:36,227 	Text Alignment  :	I  I   I       S     S    S         S    S    S             
2024-02-05 02:59:36,228 ========================================================================================================================
2024-02-05 02:59:36,228 Logging Sequence: 172_82.00
2024-02-05 02:59:36,228 	Gloss Reference :	A B+C+D+E
2024-02-05 02:59:36,228 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 02:59:36,228 	Gloss Alignment :	         
2024-02-05 02:59:36,228 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 02:59:36,230 	Text Reference  :	you all know that the toss was about to start at 700 pm   but it   started raining at around 630 pm  
2024-02-05 02:59:36,230 	Text Hypothesis :	*** *** **** that *** **** *** ***** ** ***** ** *** time she said 'i      am      a  match  as  well
2024-02-05 02:59:36,230 	Text Alignment  :	D   D   D         D   D    D   D     D  D     D  D   S    S   S    S       S       S  S      S   S   
2024-02-05 02:59:36,230 ========================================================================================================================
2024-02-05 02:59:37,519 Epoch 2667: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-05 02:59:37,519 EPOCH 2668
2024-02-05 02:59:51,508 Epoch 2668: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-05 02:59:51,508 EPOCH 2669
2024-02-05 03:00:05,253 Epoch 2669: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-05 03:00:05,253 EPOCH 2670
2024-02-05 03:00:19,229 Epoch 2670: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-05 03:00:19,230 EPOCH 2671
2024-02-05 03:00:33,255 Epoch 2671: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.18 
2024-02-05 03:00:33,255 EPOCH 2672
2024-02-05 03:00:47,154 Epoch 2672: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.06 
2024-02-05 03:00:47,155 EPOCH 2673
2024-02-05 03:01:00,790 Epoch 2673: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.00 
2024-02-05 03:01:00,791 EPOCH 2674
2024-02-05 03:01:14,477 Epoch 2674: Total Training Recognition Loss 0.04  Total Training Translation Loss 4.70 
2024-02-05 03:01:14,478 EPOCH 2675
2024-02-05 03:01:28,162 Epoch 2675: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.93 
2024-02-05 03:01:28,162 EPOCH 2676
2024-02-05 03:01:42,265 Epoch 2676: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.14 
2024-02-05 03:01:42,265 EPOCH 2677
2024-02-05 03:01:56,153 Epoch 2677: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.43 
2024-02-05 03:01:56,153 EPOCH 2678
2024-02-05 03:02:07,383 [Epoch: 2678 Step: 00024100] Batch Recognition Loss:   0.006642 => Gls Tokens per Sec:      719 || Batch Translation Loss:   0.347374 => Txt Tokens per Sec:     1930 || Lr: 0.000100
2024-02-05 03:02:09,853 Epoch 2678: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.41 
2024-02-05 03:02:09,854 EPOCH 2679
2024-02-05 03:02:23,634 Epoch 2679: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.56 
2024-02-05 03:02:23,635 EPOCH 2680
2024-02-05 03:02:37,587 Epoch 2680: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.92 
2024-02-05 03:02:37,588 EPOCH 2681
2024-02-05 03:02:51,577 Epoch 2681: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-05 03:02:51,577 EPOCH 2682
2024-02-05 03:03:05,067 Epoch 2682: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-05 03:03:05,068 EPOCH 2683
2024-02-05 03:03:19,077 Epoch 2683: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-05 03:03:19,077 EPOCH 2684
2024-02-05 03:03:32,816 Epoch 2684: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-05 03:03:32,816 EPOCH 2685
2024-02-05 03:03:46,852 Epoch 2685: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-05 03:03:46,852 EPOCH 2686
2024-02-05 03:04:00,611 Epoch 2686: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-05 03:04:00,612 EPOCH 2687
2024-02-05 03:04:14,379 Epoch 2687: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-05 03:04:14,379 EPOCH 2688
2024-02-05 03:04:28,044 Epoch 2688: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-05 03:04:28,045 EPOCH 2689
2024-02-05 03:04:41,732 [Epoch: 2689 Step: 00024200] Batch Recognition Loss:   0.000310 => Gls Tokens per Sec:      683 || Batch Translation Loss:   0.030315 => Txt Tokens per Sec:     1900 || Lr: 0.000100
2024-02-05 03:04:42,202 Epoch 2689: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 03:04:42,203 EPOCH 2690
2024-02-05 03:04:56,182 Epoch 2690: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 03:04:56,182 EPOCH 2691
2024-02-05 03:05:10,001 Epoch 2691: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 03:05:10,002 EPOCH 2692
2024-02-05 03:05:23,991 Epoch 2692: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 03:05:23,992 EPOCH 2693
2024-02-05 03:05:37,710 Epoch 2693: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 03:05:37,710 EPOCH 2694
2024-02-05 03:05:51,646 Epoch 2694: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 03:05:51,646 EPOCH 2695
2024-02-05 03:06:05,525 Epoch 2695: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 03:06:05,525 EPOCH 2696
2024-02-05 03:06:19,208 Epoch 2696: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 03:06:19,209 EPOCH 2697
2024-02-05 03:06:33,077 Epoch 2697: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 03:06:33,078 EPOCH 2698
2024-02-05 03:06:47,204 Epoch 2698: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 03:06:47,204 EPOCH 2699
2024-02-05 03:07:01,204 Epoch 2699: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 03:07:01,204 EPOCH 2700
2024-02-05 03:07:15,083 [Epoch: 2700 Step: 00024300] Batch Recognition Loss:   0.000190 => Gls Tokens per Sec:      766 || Batch Translation Loss:   0.008995 => Txt Tokens per Sec:     2127 || Lr: 0.000100
2024-02-05 03:07:15,083 Epoch 2700: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 03:07:15,084 EPOCH 2701
2024-02-05 03:07:29,002 Epoch 2701: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 03:07:29,002 EPOCH 2702
2024-02-05 03:07:42,974 Epoch 2702: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 03:07:42,975 EPOCH 2703
2024-02-05 03:07:56,889 Epoch 2703: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 03:07:56,890 EPOCH 2704
2024-02-05 03:08:10,960 Epoch 2704: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 03:08:10,960 EPOCH 2705
2024-02-05 03:08:24,912 Epoch 2705: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 03:08:24,912 EPOCH 2706
2024-02-05 03:08:38,534 Epoch 2706: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 03:08:38,535 EPOCH 2707
2024-02-05 03:08:52,753 Epoch 2707: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 03:08:52,754 EPOCH 2708
2024-02-05 03:09:06,383 Epoch 2708: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 03:09:06,384 EPOCH 2709
2024-02-05 03:09:20,501 Epoch 2709: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 03:09:20,502 EPOCH 2710
2024-02-05 03:09:34,159 Epoch 2710: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 03:09:34,160 EPOCH 2711
2024-02-05 03:09:48,190 Epoch 2711: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 03:09:48,191 EPOCH 2712
2024-02-05 03:09:50,120 [Epoch: 2712 Step: 00024400] Batch Recognition Loss:   0.000197 => Gls Tokens per Sec:      664 || Batch Translation Loss:   0.017542 => Txt Tokens per Sec:     2112 || Lr: 0.000100
2024-02-05 03:10:02,279 Epoch 2712: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 03:10:02,280 EPOCH 2713
2024-02-05 03:10:16,018 Epoch 2713: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 03:10:16,019 EPOCH 2714
2024-02-05 03:10:29,873 Epoch 2714: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 03:10:29,874 EPOCH 2715
2024-02-05 03:10:43,629 Epoch 2715: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 03:10:43,629 EPOCH 2716
2024-02-05 03:10:57,575 Epoch 2716: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 03:10:57,576 EPOCH 2717
2024-02-05 03:11:11,221 Epoch 2717: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 03:11:11,222 EPOCH 2718
2024-02-05 03:11:25,515 Epoch 2718: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 03:11:25,516 EPOCH 2719
2024-02-05 03:11:39,038 Epoch 2719: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 03:11:39,039 EPOCH 2720
2024-02-05 03:11:52,948 Epoch 2720: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 03:11:52,948 EPOCH 2721
2024-02-05 03:12:06,936 Epoch 2721: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 03:12:06,936 EPOCH 2722
2024-02-05 03:12:20,698 Epoch 2722: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 03:12:20,698 EPOCH 2723
2024-02-05 03:12:21,444 [Epoch: 2723 Step: 00024500] Batch Recognition Loss:   0.000144 => Gls Tokens per Sec:     3443 || Batch Translation Loss:   0.014023 => Txt Tokens per Sec:     8988 || Lr: 0.000100
2024-02-05 03:12:34,435 Epoch 2723: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 03:12:34,436 EPOCH 2724
2024-02-05 03:12:48,146 Epoch 2724: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 03:12:48,146 EPOCH 2725
2024-02-05 03:13:02,379 Epoch 2725: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 03:13:02,379 EPOCH 2726
2024-02-05 03:13:16,455 Epoch 2726: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 03:13:16,456 EPOCH 2727
2024-02-05 03:13:30,378 Epoch 2727: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 03:13:30,379 EPOCH 2728
2024-02-05 03:13:44,380 Epoch 2728: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 03:13:44,381 EPOCH 2729
2024-02-05 03:13:58,347 Epoch 2729: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 03:13:58,348 EPOCH 2730
2024-02-05 03:14:12,432 Epoch 2730: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-05 03:14:12,433 EPOCH 2731
2024-02-05 03:14:26,381 Epoch 2731: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 03:14:26,381 EPOCH 2732
2024-02-05 03:14:40,165 Epoch 2732: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-05 03:14:40,166 EPOCH 2733
2024-02-05 03:14:54,001 Epoch 2733: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 03:14:54,002 EPOCH 2734
2024-02-05 03:14:59,833 [Epoch: 2734 Step: 00024600] Batch Recognition Loss:   0.000161 => Gls Tokens per Sec:      659 || Batch Translation Loss:   0.014384 => Txt Tokens per Sec:     1824 || Lr: 0.000100
2024-02-05 03:15:07,922 Epoch 2734: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 03:15:07,923 EPOCH 2735
2024-02-05 03:15:21,823 Epoch 2735: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 03:15:21,823 EPOCH 2736
2024-02-05 03:15:35,675 Epoch 2736: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 03:15:35,676 EPOCH 2737
2024-02-05 03:15:49,656 Epoch 2737: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 03:15:49,656 EPOCH 2738
2024-02-05 03:16:03,853 Epoch 2738: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 03:16:03,854 EPOCH 2739
2024-02-05 03:16:17,880 Epoch 2739: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 03:16:17,881 EPOCH 2740
2024-02-05 03:16:31,735 Epoch 2740: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 03:16:31,735 EPOCH 2741
2024-02-05 03:16:45,256 Epoch 2741: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 03:16:45,257 EPOCH 2742
2024-02-05 03:16:59,033 Epoch 2742: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-05 03:16:59,034 EPOCH 2743
2024-02-05 03:17:12,993 Epoch 2743: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 03:17:12,993 EPOCH 2744
2024-02-05 03:17:27,055 Epoch 2744: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 03:17:27,056 EPOCH 2745
2024-02-05 03:17:34,250 [Epoch: 2745 Step: 00024700] Batch Recognition Loss:   0.000185 => Gls Tokens per Sec:      712 || Batch Translation Loss:   0.019546 => Txt Tokens per Sec:     1918 || Lr: 0.000100
2024-02-05 03:17:40,733 Epoch 2745: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 03:17:40,733 EPOCH 2746
2024-02-05 03:17:54,326 Epoch 2746: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 03:17:54,327 EPOCH 2747
2024-02-05 03:18:08,071 Epoch 2747: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 03:18:08,072 EPOCH 2748
2024-02-05 03:18:21,935 Epoch 2748: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 03:18:21,936 EPOCH 2749
2024-02-05 03:18:36,062 Epoch 2749: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 03:18:36,063 EPOCH 2750
2024-02-05 03:18:49,940 Epoch 2750: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 03:18:49,940 EPOCH 2751
2024-02-05 03:19:03,678 Epoch 2751: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 03:19:03,678 EPOCH 2752
2024-02-05 03:19:17,673 Epoch 2752: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 03:19:17,674 EPOCH 2753
2024-02-05 03:19:31,789 Epoch 2753: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 03:19:31,789 EPOCH 2754
2024-02-05 03:19:45,805 Epoch 2754: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 03:19:45,805 EPOCH 2755
2024-02-05 03:19:59,468 Epoch 2755: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 03:19:59,468 EPOCH 2756
2024-02-05 03:20:06,425 [Epoch: 2756 Step: 00024800] Batch Recognition Loss:   0.000126 => Gls Tokens per Sec:      920 || Batch Translation Loss:   0.012787 => Txt Tokens per Sec:     2583 || Lr: 0.000100
2024-02-05 03:20:13,343 Epoch 2756: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 03:20:13,343 EPOCH 2757
2024-02-05 03:20:27,585 Epoch 2757: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 03:20:27,585 EPOCH 2758
2024-02-05 03:20:41,228 Epoch 2758: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 03:20:41,228 EPOCH 2759
2024-02-05 03:20:55,288 Epoch 2759: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 03:20:55,289 EPOCH 2760
2024-02-05 03:21:09,205 Epoch 2760: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 03:21:09,205 EPOCH 2761
2024-02-05 03:21:22,936 Epoch 2761: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 03:21:22,936 EPOCH 2762
2024-02-05 03:21:36,500 Epoch 2762: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 03:21:36,501 EPOCH 2763
2024-02-05 03:21:50,422 Epoch 2763: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 03:21:50,423 EPOCH 2764
2024-02-05 03:22:04,285 Epoch 2764: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 03:22:04,285 EPOCH 2765
2024-02-05 03:22:18,079 Epoch 2765: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 03:22:18,080 EPOCH 2766
2024-02-05 03:22:32,146 Epoch 2766: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 03:22:32,147 EPOCH 2767
2024-02-05 03:22:39,863 [Epoch: 2767 Step: 00024900] Batch Recognition Loss:   0.000130 => Gls Tokens per Sec:      880 || Batch Translation Loss:   0.007519 => Txt Tokens per Sec:     2344 || Lr: 0.000100
2024-02-05 03:22:45,807 Epoch 2767: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 03:22:45,808 EPOCH 2768
2024-02-05 03:22:59,565 Epoch 2768: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 03:22:59,566 EPOCH 2769
2024-02-05 03:23:13,630 Epoch 2769: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 03:23:13,631 EPOCH 2770
2024-02-05 03:23:27,397 Epoch 2770: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 03:23:27,398 EPOCH 2771
2024-02-05 03:23:41,450 Epoch 2771: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 03:23:41,451 EPOCH 2772
2024-02-05 03:23:55,301 Epoch 2772: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 03:23:55,302 EPOCH 2773
2024-02-05 03:24:09,102 Epoch 2773: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 03:24:09,103 EPOCH 2774
2024-02-05 03:24:23,066 Epoch 2774: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 03:24:23,068 EPOCH 2775
2024-02-05 03:24:37,149 Epoch 2775: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 03:24:37,150 EPOCH 2776
2024-02-05 03:24:51,202 Epoch 2776: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 03:24:51,203 EPOCH 2777
2024-02-05 03:25:04,990 Epoch 2777: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 03:25:04,990 EPOCH 2778
2024-02-05 03:25:14,663 [Epoch: 2778 Step: 00025000] Batch Recognition Loss:   0.000142 => Gls Tokens per Sec:      926 || Batch Translation Loss:   0.025998 => Txt Tokens per Sec:     2671 || Lr: 0.000100
2024-02-05 03:25:19,212 Epoch 2778: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 03:25:19,212 EPOCH 2779
2024-02-05 03:25:32,898 Epoch 2779: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 03:25:32,898 EPOCH 2780
2024-02-05 03:25:47,010 Epoch 2780: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 03:25:47,011 EPOCH 2781
2024-02-05 03:26:00,749 Epoch 2781: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 03:26:00,749 EPOCH 2782
2024-02-05 03:26:14,699 Epoch 2782: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 03:26:14,699 EPOCH 2783
2024-02-05 03:26:28,609 Epoch 2783: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 03:26:28,610 EPOCH 2784
2024-02-05 03:26:42,464 Epoch 2784: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 03:26:42,465 EPOCH 2785
2024-02-05 03:26:56,374 Epoch 2785: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 03:26:56,375 EPOCH 2786
2024-02-05 03:27:10,294 Epoch 2786: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 03:27:10,295 EPOCH 2787
2024-02-05 03:27:24,140 Epoch 2787: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 03:27:24,141 EPOCH 2788
2024-02-05 03:27:38,190 Epoch 2788: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 03:27:38,190 EPOCH 2789
2024-02-05 03:27:46,922 [Epoch: 2789 Step: 00025100] Batch Recognition Loss:   0.000116 => Gls Tokens per Sec:     1071 || Batch Translation Loss:   0.020449 => Txt Tokens per Sec:     2866 || Lr: 0.000100
2024-02-05 03:27:51,879 Epoch 2789: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 03:27:51,879 EPOCH 2790
2024-02-05 03:28:05,814 Epoch 2790: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 03:28:05,815 EPOCH 2791
2024-02-05 03:28:19,972 Epoch 2791: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 03:28:19,973 EPOCH 2792
2024-02-05 03:28:34,075 Epoch 2792: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 03:28:34,075 EPOCH 2793
2024-02-05 03:28:48,105 Epoch 2793: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 03:28:48,105 EPOCH 2794
2024-02-05 03:29:01,800 Epoch 2794: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 03:29:01,801 EPOCH 2795
2024-02-05 03:29:15,742 Epoch 2795: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 03:29:15,743 EPOCH 2796
2024-02-05 03:29:29,462 Epoch 2796: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 03:29:29,462 EPOCH 2797
2024-02-05 03:29:43,319 Epoch 2797: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-05 03:29:43,319 EPOCH 2798
2024-02-05 03:29:57,358 Epoch 2798: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-05 03:29:57,359 EPOCH 2799
2024-02-05 03:30:10,825 Epoch 2799: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-05 03:30:10,825 EPOCH 2800
2024-02-05 03:30:24,881 [Epoch: 2800 Step: 00025200] Batch Recognition Loss:   0.000136 => Gls Tokens per Sec:      756 || Batch Translation Loss:   0.085832 => Txt Tokens per Sec:     2100 || Lr: 0.000100
2024-02-05 03:30:24,881 Epoch 2800: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-05 03:30:24,882 EPOCH 2801
2024-02-05 03:30:38,614 Epoch 2801: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-05 03:30:38,615 EPOCH 2802
2024-02-05 03:30:52,728 Epoch 2802: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.67 
2024-02-05 03:30:52,729 EPOCH 2803
2024-02-05 03:31:06,694 Epoch 2803: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.72 
2024-02-05 03:31:06,695 EPOCH 2804
2024-02-05 03:31:20,538 Epoch 2804: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.87 
2024-02-05 03:31:20,539 EPOCH 2805
2024-02-05 03:31:34,329 Epoch 2805: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.90 
2024-02-05 03:31:34,329 EPOCH 2806
2024-02-05 03:31:48,156 Epoch 2806: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.57 
2024-02-05 03:31:48,156 EPOCH 2807
2024-02-05 03:32:02,265 Epoch 2807: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.35 
2024-02-05 03:32:02,266 EPOCH 2808
2024-02-05 03:32:16,209 Epoch 2808: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.17 
2024-02-05 03:32:16,210 EPOCH 2809
2024-02-05 03:32:29,914 Epoch 2809: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.02 
2024-02-05 03:32:29,914 EPOCH 2810
2024-02-05 03:32:43,743 Epoch 2810: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.92 
2024-02-05 03:32:43,744 EPOCH 2811
2024-02-05 03:32:57,964 Epoch 2811: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.64 
2024-02-05 03:32:57,964 EPOCH 2812
2024-02-05 03:32:59,612 [Epoch: 2812 Step: 00025300] Batch Recognition Loss:   0.000560 => Gls Tokens per Sec:      777 || Batch Translation Loss:   0.071802 => Txt Tokens per Sec:     2480 || Lr: 0.000100
2024-02-05 03:33:11,643 Epoch 2812: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-05 03:33:11,643 EPOCH 2813
2024-02-05 03:33:25,511 Epoch 2813: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-05 03:33:25,512 EPOCH 2814
2024-02-05 03:33:39,493 Epoch 2814: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-05 03:33:39,493 EPOCH 2815
2024-02-05 03:33:53,233 Epoch 2815: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-05 03:33:53,234 EPOCH 2816
2024-02-05 03:34:07,083 Epoch 2816: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-05 03:34:07,084 EPOCH 2817
2024-02-05 03:34:21,017 Epoch 2817: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-05 03:34:21,017 EPOCH 2818
2024-02-05 03:34:35,106 Epoch 2818: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-05 03:34:35,106 EPOCH 2819
2024-02-05 03:34:48,912 Epoch 2819: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-05 03:34:48,913 EPOCH 2820
2024-02-05 03:35:02,800 Epoch 2820: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-05 03:35:02,800 EPOCH 2821
2024-02-05 03:35:16,930 Epoch 2821: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 03:35:16,931 EPOCH 2822
2024-02-05 03:35:30,866 Epoch 2822: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 03:35:30,867 EPOCH 2823
2024-02-05 03:35:35,518 [Epoch: 2823 Step: 00025400] Batch Recognition Loss:   0.000200 => Gls Tokens per Sec:      359 || Batch Translation Loss:   0.011279 => Txt Tokens per Sec:      928 || Lr: 0.000100
2024-02-05 03:35:44,795 Epoch 2823: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 03:35:44,796 EPOCH 2824
2024-02-05 03:35:59,002 Epoch 2824: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 03:35:59,002 EPOCH 2825
2024-02-05 03:36:12,790 Epoch 2825: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 03:36:12,791 EPOCH 2826
2024-02-05 03:36:26,391 Epoch 2826: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 03:36:26,392 EPOCH 2827
2024-02-05 03:36:40,541 Epoch 2827: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 03:36:40,541 EPOCH 2828
2024-02-05 03:36:54,196 Epoch 2828: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 03:36:54,197 EPOCH 2829
2024-02-05 03:37:08,214 Epoch 2829: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 03:37:08,215 EPOCH 2830
2024-02-05 03:37:22,393 Epoch 2830: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 03:37:22,394 EPOCH 2831
2024-02-05 03:37:36,408 Epoch 2831: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 03:37:36,408 EPOCH 2832
2024-02-05 03:37:50,267 Epoch 2832: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 03:37:50,268 EPOCH 2833
2024-02-05 03:38:03,926 Epoch 2833: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 03:38:03,926 EPOCH 2834
2024-02-05 03:38:10,015 [Epoch: 2834 Step: 00025500] Batch Recognition Loss:   0.000132 => Gls Tokens per Sec:      631 || Batch Translation Loss:   0.008761 => Txt Tokens per Sec:     1739 || Lr: 0.000100
2024-02-05 03:38:18,046 Epoch 2834: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 03:38:18,046 EPOCH 2835
2024-02-05 03:38:31,922 Epoch 2835: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 03:38:31,923 EPOCH 2836
2024-02-05 03:38:45,784 Epoch 2836: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 03:38:45,785 EPOCH 2837
2024-02-05 03:38:59,536 Epoch 2837: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-05 03:38:59,537 EPOCH 2838
2024-02-05 03:39:13,681 Epoch 2838: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.55 
2024-02-05 03:39:13,681 EPOCH 2839
2024-02-05 03:39:27,523 Epoch 2839: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-05 03:39:27,524 EPOCH 2840
2024-02-05 03:39:41,291 Epoch 2840: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-05 03:39:41,292 EPOCH 2841
2024-02-05 03:39:55,344 Epoch 2841: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-05 03:39:55,345 EPOCH 2842
2024-02-05 03:40:09,228 Epoch 2842: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-05 03:40:09,229 EPOCH 2843
2024-02-05 03:40:22,869 Epoch 2843: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-05 03:40:22,869 EPOCH 2844
2024-02-05 03:40:36,680 Epoch 2844: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 03:40:36,681 EPOCH 2845
2024-02-05 03:40:42,371 [Epoch: 2845 Step: 00025600] Batch Recognition Loss:   0.000206 => Gls Tokens per Sec:      744 || Batch Translation Loss:   0.029002 => Txt Tokens per Sec:     2037 || Lr: 0.000100
2024-02-05 03:40:50,686 Epoch 2845: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 03:40:50,686 EPOCH 2846
2024-02-05 03:41:04,673 Epoch 2846: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 03:41:04,674 EPOCH 2847
2024-02-05 03:41:18,556 Epoch 2847: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 03:41:18,557 EPOCH 2848
2024-02-05 03:41:32,639 Epoch 2848: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 03:41:32,640 EPOCH 2849
2024-02-05 03:41:46,718 Epoch 2849: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 03:41:46,719 EPOCH 2850
2024-02-05 03:42:00,758 Epoch 2850: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 03:42:00,759 EPOCH 2851
2024-02-05 03:42:14,418 Epoch 2851: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 03:42:14,419 EPOCH 2852
2024-02-05 03:42:28,433 Epoch 2852: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 03:42:28,434 EPOCH 2853
2024-02-05 03:42:42,178 Epoch 2853: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 03:42:42,179 EPOCH 2854
2024-02-05 03:42:56,258 Epoch 2854: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 03:42:56,259 EPOCH 2855
2024-02-05 03:43:10,064 Epoch 2855: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 03:43:10,065 EPOCH 2856
2024-02-05 03:43:13,128 [Epoch: 2856 Step: 00025700] Batch Recognition Loss:   0.000126 => Gls Tokens per Sec:     2090 || Batch Translation Loss:   0.007244 => Txt Tokens per Sec:     5239 || Lr: 0.000100
2024-02-05 03:43:23,763 Epoch 2856: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 03:43:23,763 EPOCH 2857
2024-02-05 03:43:37,868 Epoch 2857: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 03:43:37,868 EPOCH 2858
2024-02-05 03:43:51,737 Epoch 2858: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 03:43:51,738 EPOCH 2859
2024-02-05 03:44:05,770 Epoch 2859: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 03:44:05,771 EPOCH 2860
2024-02-05 03:44:19,955 Epoch 2860: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 03:44:19,956 EPOCH 2861
2024-02-05 03:44:33,913 Epoch 2861: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 03:44:33,914 EPOCH 2862
2024-02-05 03:44:47,885 Epoch 2862: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 03:44:47,886 EPOCH 2863
2024-02-05 03:45:01,753 Epoch 2863: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 03:45:01,754 EPOCH 2864
2024-02-05 03:45:15,807 Epoch 2864: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 03:45:15,807 EPOCH 2865
2024-02-05 03:45:29,509 Epoch 2865: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 03:45:29,510 EPOCH 2866
2024-02-05 03:45:43,688 Epoch 2866: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 03:45:43,689 EPOCH 2867
2024-02-05 03:45:52,247 [Epoch: 2867 Step: 00025800] Batch Recognition Loss:   0.000126 => Gls Tokens per Sec:      897 || Batch Translation Loss:   0.008461 => Txt Tokens per Sec:     2508 || Lr: 0.000100
2024-02-05 03:45:57,514 Epoch 2867: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 03:45:57,514 EPOCH 2868
2024-02-05 03:46:11,713 Epoch 2868: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 03:46:11,713 EPOCH 2869
2024-02-05 03:46:25,782 Epoch 2869: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 03:46:25,782 EPOCH 2870
2024-02-05 03:46:39,825 Epoch 2870: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 03:46:39,826 EPOCH 2871
2024-02-05 03:46:53,685 Epoch 2871: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 03:46:53,686 EPOCH 2872
2024-02-05 03:47:07,873 Epoch 2872: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 03:47:07,874 EPOCH 2873
2024-02-05 03:47:21,621 Epoch 2873: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 03:47:21,621 EPOCH 2874
2024-02-05 03:47:35,464 Epoch 2874: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 03:47:35,464 EPOCH 2875
2024-02-05 03:47:49,646 Epoch 2875: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-05 03:47:49,646 EPOCH 2876
2024-02-05 03:48:03,698 Epoch 2876: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-05 03:48:03,699 EPOCH 2877
2024-02-05 03:48:17,562 Epoch 2877: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-05 03:48:17,563 EPOCH 2878
2024-02-05 03:48:26,583 [Epoch: 2878 Step: 00025900] Batch Recognition Loss:   0.000368 => Gls Tokens per Sec:      993 || Batch Translation Loss:   0.062979 => Txt Tokens per Sec:     2696 || Lr: 0.000100
2024-02-05 03:48:31,319 Epoch 2878: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.81 
2024-02-05 03:48:31,319 EPOCH 2879
2024-02-05 03:48:45,488 Epoch 2879: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-05 03:48:45,488 EPOCH 2880
2024-02-05 03:48:59,706 Epoch 2880: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-05 03:48:59,706 EPOCH 2881
2024-02-05 03:49:13,883 Epoch 2881: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-05 03:49:13,883 EPOCH 2882
2024-02-05 03:49:27,699 Epoch 2882: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-05 03:49:27,700 EPOCH 2883
2024-02-05 03:49:41,458 Epoch 2883: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 03:49:41,459 EPOCH 2884
2024-02-05 03:49:55,407 Epoch 2884: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-05 03:49:55,407 EPOCH 2885
2024-02-05 03:50:09,636 Epoch 2885: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 03:50:09,637 EPOCH 2886
2024-02-05 03:50:23,442 Epoch 2886: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 03:50:23,443 EPOCH 2887
2024-02-05 03:50:37,065 Epoch 2887: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-05 03:50:37,066 EPOCH 2888
2024-02-05 03:50:50,873 Epoch 2888: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 03:50:50,873 EPOCH 2889
2024-02-05 03:51:04,013 [Epoch: 2889 Step: 00026000] Batch Recognition Loss:   0.000145 => Gls Tokens per Sec:      712 || Batch Translation Loss:   0.029482 => Txt Tokens per Sec:     1950 || Lr: 0.000100
2024-02-05 03:51:33,188 Validation result at epoch 2889, step    26000: duration: 29.1736s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00018	Translation Loss: 98434.63281	PPL: 18965.16211
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.77	(BLEU-1: 11.66,	BLEU-2: 3.81,	BLEU-3: 1.62,	BLEU-4: 0.77)
	CHRF 17.48	ROUGE 10.09
2024-02-05 03:51:33,189 Logging Recognition and Translation Outputs
2024-02-05 03:51:33,190 ========================================================================================================================
2024-02-05 03:51:33,190 Logging Sequence: 130_139.00
2024-02-05 03:51:33,190 	Gloss Reference :	A B+C+D+E
2024-02-05 03:51:33,190 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 03:51:33,190 	Gloss Alignment :	         
2024-02-05 03:51:33,191 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 03:51:33,194 	Text Reference  :	he shared a ***** picture of   a  little pouch he  knit   for his olympic gold medal with uk flag on    one side  and  japanese flag on the  other
2024-02-05 03:51:33,194 	Text Hypothesis :	he played a diver but     what 10 2021   and   the reason for *** ******* **** ***** **** ** the  comes to  india when the      2012 to when it   
2024-02-05 03:51:33,194 	Text Alignment  :	   S        I     S       S    S  S      S     S   S          D   D       D    D     D    D  S    S     S   S     S    S        S    S  S    S    
2024-02-05 03:51:33,194 ========================================================================================================================
2024-02-05 03:51:33,194 Logging Sequence: 72_194.00
2024-02-05 03:51:33,195 	Gloss Reference :	A B+C+D+E
2024-02-05 03:51:33,195 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 03:51:33,195 	Gloss Alignment :	         
2024-02-05 03:51:33,195 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 03:51:33,196 	Text Reference  :	shah told her to do    what she wants and filed a    police complaint against her 
2024-02-05 03:51:33,196 	Text Hypothesis :	**** **** *** ** babar kept me  tell  you all   know in     the       same    team
2024-02-05 03:51:33,196 	Text Alignment  :	D    D    D   D  S     S    S   S     S   S     S    S      S         S       S   
2024-02-05 03:51:33,197 ========================================================================================================================
2024-02-05 03:51:33,197 Logging Sequence: 69_177.00
2024-02-05 03:51:33,197 	Gloss Reference :	A B+C+D+E
2024-02-05 03:51:33,197 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 03:51:33,197 	Gloss Alignment :	         
2024-02-05 03:51:33,197 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 03:51:33,199 	Text Reference  :	he said 'i will continue playing i        know it's  about time    i   retire i   also    have a     knee condition
2024-02-05 03:51:33,200 	Text Hypothesis :	** **** ** when csk      have    captured the  first 9     minutes the score  was delayed the  match in   mumbai   
2024-02-05 03:51:33,200 	Text Alignment  :	D  D    D  S    S        S       S        S    S     S     S       S   S      S   S       S    S     S    S        
2024-02-05 03:51:33,200 ========================================================================================================================
2024-02-05 03:51:33,200 Logging Sequence: 95_118.00
2024-02-05 03:51:33,200 	Gloss Reference :	A B+C+D+E
2024-02-05 03:51:33,200 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 03:51:33,200 	Gloss Alignment :	         
2024-02-05 03:51:33,201 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 03:51:33,201 	Text Reference  :	******* ** ***** ******** ******** ** the ***** ******* game was  stopped strangely due    to excessive sunlight
2024-02-05 03:51:33,202 	Text Hypothesis :	however an undue incident occurred on the pitch scoring 81   runs for     all       thanks to ********* the     
2024-02-05 03:51:33,202 	Text Alignment  :	I       I  I     I        I        I      I     I       S    S    S       S         S         D         S       
2024-02-05 03:51:33,202 ========================================================================================================================
2024-02-05 03:51:33,202 Logging Sequence: 112_8.00
2024-02-05 03:51:33,202 	Gloss Reference :	A B+C+D+E
2024-02-05 03:51:33,202 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 03:51:33,202 	Gloss Alignment :	         
2024-02-05 03:51:33,203 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 03:51:33,204 	Text Reference  :	before there were 8 teams such as mumbai indians delhi capitals  punjab kings etc and  now there will be  10   teams in   2022     
2024-02-05 03:51:33,205 	Text Hypothesis :	****** ***** **** * ***** **** ** ****** but     you   practised very   hard  to  know in  the   ipl  the bcci has   been postponed
2024-02-05 03:51:33,205 	Text Alignment  :	D      D     D    D D     D    D  D      S       S     S         S      S     S   S    S   S     S    S   S    S     S    S        
2024-02-05 03:51:33,205 ========================================================================================================================
2024-02-05 03:51:34,063 Epoch 2889: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-05 03:51:34,064 EPOCH 2890
2024-02-05 03:51:48,160 Epoch 2890: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-05 03:51:48,161 EPOCH 2891
2024-02-05 03:52:02,198 Epoch 2891: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-05 03:52:02,198 EPOCH 2892
2024-02-05 03:52:15,587 Epoch 2892: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 03:52:15,588 EPOCH 2893
2024-02-05 03:52:29,662 Epoch 2893: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-05 03:52:29,662 EPOCH 2894
2024-02-05 03:52:43,439 Epoch 2894: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-05 03:52:43,440 EPOCH 2895
2024-02-05 03:52:57,656 Epoch 2895: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.68 
2024-02-05 03:52:57,656 EPOCH 2896
2024-02-05 03:53:11,531 Epoch 2896: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-05 03:53:11,532 EPOCH 2897
2024-02-05 03:53:25,855 Epoch 2897: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.83 
2024-02-05 03:53:25,856 EPOCH 2898
2024-02-05 03:53:39,537 Epoch 2898: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.97 
2024-02-05 03:53:39,538 EPOCH 2899
2024-02-05 03:53:53,446 Epoch 2899: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.90 
2024-02-05 03:53:53,447 EPOCH 2900
2024-02-05 03:54:07,369 [Epoch: 2900 Step: 00026100] Batch Recognition Loss:   0.000571 => Gls Tokens per Sec:      764 || Batch Translation Loss:   0.286708 => Txt Tokens per Sec:     2120 || Lr: 0.000100
2024-02-05 03:54:07,369 Epoch 2900: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.96 
2024-02-05 03:54:07,370 EPOCH 2901
2024-02-05 03:54:21,391 Epoch 2901: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.70 
2024-02-05 03:54:21,392 EPOCH 2902
2024-02-05 03:54:35,349 Epoch 2902: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.65 
2024-02-05 03:54:35,349 EPOCH 2903
2024-02-05 03:54:49,424 Epoch 2903: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-05 03:54:49,424 EPOCH 2904
2024-02-05 03:55:03,057 Epoch 2904: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-05 03:55:03,058 EPOCH 2905
2024-02-05 03:55:16,987 Epoch 2905: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-05 03:55:16,988 EPOCH 2906
2024-02-05 03:55:30,821 Epoch 2906: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-05 03:55:30,822 EPOCH 2907
2024-02-05 03:55:44,975 Epoch 2907: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-05 03:55:44,976 EPOCH 2908
2024-02-05 03:55:59,318 Epoch 2908: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-05 03:55:59,318 EPOCH 2909
2024-02-05 03:56:13,047 Epoch 2909: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-05 03:56:13,048 EPOCH 2910
2024-02-05 03:56:27,017 Epoch 2910: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-05 03:56:27,017 EPOCH 2911
2024-02-05 03:56:41,356 Epoch 2911: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-05 03:56:41,356 EPOCH 2912
2024-02-05 03:56:41,737 [Epoch: 2912 Step: 00026200] Batch Recognition Loss:   0.000186 => Gls Tokens per Sec:     3368 || Batch Translation Loss:   0.027294 => Txt Tokens per Sec:     8400 || Lr: 0.000100
2024-02-05 03:56:55,337 Epoch 2912: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-05 03:56:55,338 EPOCH 2913
2024-02-05 03:57:09,138 Epoch 2913: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-05 03:57:09,139 EPOCH 2914
2024-02-05 03:57:23,567 Epoch 2914: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-05 03:57:23,567 EPOCH 2915
2024-02-05 03:57:37,621 Epoch 2915: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-05 03:57:37,621 EPOCH 2916
2024-02-05 03:57:51,444 Epoch 2916: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 03:57:51,445 EPOCH 2917
2024-02-05 03:58:05,432 Epoch 2917: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 03:58:05,433 EPOCH 2918
2024-02-05 03:58:19,312 Epoch 2918: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 03:58:19,312 EPOCH 2919
2024-02-05 03:58:33,349 Epoch 2919: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-05 03:58:33,350 EPOCH 2920
2024-02-05 03:58:47,324 Epoch 2920: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 03:58:47,325 EPOCH 2921
2024-02-05 03:59:00,960 Epoch 2921: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-05 03:59:00,960 EPOCH 2922
2024-02-05 03:59:15,162 Epoch 2922: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.84 
2024-02-05 03:59:15,163 EPOCH 2923
2024-02-05 03:59:15,753 [Epoch: 2923 Step: 00026300] Batch Recognition Loss:   0.000222 => Gls Tokens per Sec:     4354 || Batch Translation Loss:   0.261432 => Txt Tokens per Sec:     9274 || Lr: 0.000100
2024-02-05 03:59:28,851 Epoch 2923: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.86 
2024-02-05 03:59:28,852 EPOCH 2924
2024-02-05 03:59:42,989 Epoch 2924: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.99 
2024-02-05 03:59:42,990 EPOCH 2925
2024-02-05 03:59:57,041 Epoch 2925: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.97 
2024-02-05 03:59:57,042 EPOCH 2926
2024-02-05 04:00:11,136 Epoch 2926: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.03 
2024-02-05 04:00:11,136 EPOCH 2927
2024-02-05 04:00:24,959 Epoch 2927: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-05 04:00:24,959 EPOCH 2928
2024-02-05 04:00:39,038 Epoch 2928: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-05 04:00:39,039 EPOCH 2929
2024-02-05 04:00:52,909 Epoch 2929: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-05 04:00:52,909 EPOCH 2930
2024-02-05 04:01:06,922 Epoch 2930: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-05 04:01:06,922 EPOCH 2931
2024-02-05 04:01:21,037 Epoch 2931: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-05 04:01:21,038 EPOCH 2932
2024-02-05 04:01:35,013 Epoch 2932: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-05 04:01:35,013 EPOCH 2933
2024-02-05 04:01:48,824 Epoch 2933: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-05 04:01:48,825 EPOCH 2934
2024-02-05 04:01:53,968 [Epoch: 2934 Step: 00026400] Batch Recognition Loss:   0.000210 => Gls Tokens per Sec:      574 || Batch Translation Loss:   0.023167 => Txt Tokens per Sec:     1575 || Lr: 0.000100
2024-02-05 04:02:02,871 Epoch 2934: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 04:02:02,871 EPOCH 2935
2024-02-05 04:02:16,439 Epoch 2935: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-05 04:02:16,439 EPOCH 2936
2024-02-05 04:02:30,386 Epoch 2936: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-05 04:02:30,387 EPOCH 2937
2024-02-05 04:02:44,119 Epoch 2937: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 04:02:44,119 EPOCH 2938
2024-02-05 04:02:58,121 Epoch 2938: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 04:02:58,122 EPOCH 2939
2024-02-05 04:03:12,102 Epoch 2939: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 04:03:12,103 EPOCH 2940
2024-02-05 04:03:25,762 Epoch 2940: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 04:03:25,763 EPOCH 2941
2024-02-05 04:03:39,630 Epoch 2941: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 04:03:39,631 EPOCH 2942
2024-02-05 04:03:53,543 Epoch 2942: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 04:03:53,544 EPOCH 2943
2024-02-05 04:04:07,610 Epoch 2943: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 04:04:07,611 EPOCH 2944
2024-02-05 04:04:21,095 Epoch 2944: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 04:04:21,096 EPOCH 2945
2024-02-05 04:04:29,030 [Epoch: 2945 Step: 00026500] Batch Recognition Loss:   0.000216 => Gls Tokens per Sec:      645 || Batch Translation Loss:   0.020122 => Txt Tokens per Sec:     1971 || Lr: 0.000100
2024-02-05 04:04:35,123 Epoch 2945: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 04:04:35,124 EPOCH 2946
2024-02-05 04:04:48,959 Epoch 2946: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 04:04:48,959 EPOCH 2947
2024-02-05 04:05:02,990 Epoch 2947: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 04:05:02,990 EPOCH 2948
2024-02-05 04:05:16,658 Epoch 2948: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 04:05:16,659 EPOCH 2949
2024-02-05 04:05:30,636 Epoch 2949: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 04:05:30,637 EPOCH 2950
2024-02-05 04:05:44,180 Epoch 2950: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 04:05:44,181 EPOCH 2951
2024-02-05 04:05:58,460 Epoch 2951: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 04:05:58,461 EPOCH 2952
2024-02-05 04:06:12,475 Epoch 2952: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 04:06:12,475 EPOCH 2953
2024-02-05 04:06:26,440 Epoch 2953: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 04:06:26,441 EPOCH 2954
2024-02-05 04:06:40,300 Epoch 2954: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 04:06:40,301 EPOCH 2955
2024-02-05 04:06:54,280 Epoch 2955: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 04:06:54,280 EPOCH 2956
2024-02-05 04:07:01,414 [Epoch: 2956 Step: 00026600] Batch Recognition Loss:   0.000143 => Gls Tokens per Sec:      897 || Batch Translation Loss:   0.013551 => Txt Tokens per Sec:     2638 || Lr: 0.000100
2024-02-05 04:07:08,166 Epoch 2956: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 04:07:08,166 EPOCH 2957
2024-02-05 04:07:21,981 Epoch 2957: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 04:07:21,982 EPOCH 2958
2024-02-05 04:07:35,861 Epoch 2958: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 04:07:35,862 EPOCH 2959
2024-02-05 04:07:50,016 Epoch 2959: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 04:07:50,017 EPOCH 2960
2024-02-05 04:08:03,754 Epoch 2960: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 04:08:03,754 EPOCH 2961
2024-02-05 04:08:17,888 Epoch 2961: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 04:08:17,888 EPOCH 2962
2024-02-05 04:08:31,935 Epoch 2962: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 04:08:31,936 EPOCH 2963
2024-02-05 04:08:45,963 Epoch 2963: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 04:08:45,964 EPOCH 2964
2024-02-05 04:08:59,914 Epoch 2964: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 04:08:59,914 EPOCH 2965
2024-02-05 04:09:13,789 Epoch 2965: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 04:09:13,789 EPOCH 2966
2024-02-05 04:09:27,399 Epoch 2966: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 04:09:27,399 EPOCH 2967
2024-02-05 04:09:39,590 [Epoch: 2967 Step: 00026700] Batch Recognition Loss:   0.000134 => Gls Tokens per Sec:      557 || Batch Translation Loss:   0.025034 => Txt Tokens per Sec:     1552 || Lr: 0.000100
2024-02-05 04:09:41,213 Epoch 2967: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 04:09:41,213 EPOCH 2968
2024-02-05 04:09:55,336 Epoch 2968: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 04:09:55,336 EPOCH 2969
2024-02-05 04:10:09,272 Epoch 2969: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 04:10:09,273 EPOCH 2970
2024-02-05 04:10:23,299 Epoch 2970: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 04:10:23,299 EPOCH 2971
2024-02-05 04:10:37,180 Epoch 2971: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 04:10:37,181 EPOCH 2972
2024-02-05 04:10:51,254 Epoch 2972: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 04:10:51,254 EPOCH 2973
2024-02-05 04:11:05,059 Epoch 2973: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 04:11:05,060 EPOCH 2974
2024-02-05 04:11:18,926 Epoch 2974: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 04:11:18,926 EPOCH 2975
2024-02-05 04:11:33,160 Epoch 2975: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 04:11:33,161 EPOCH 2976
2024-02-05 04:11:47,242 Epoch 2976: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 04:11:47,242 EPOCH 2977
2024-02-05 04:12:01,118 Epoch 2977: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 04:12:01,119 EPOCH 2978
2024-02-05 04:12:09,622 [Epoch: 2978 Step: 00026800] Batch Recognition Loss:   0.000110 => Gls Tokens per Sec:      949 || Batch Translation Loss:   0.013086 => Txt Tokens per Sec:     2676 || Lr: 0.000100
2024-02-05 04:12:14,871 Epoch 2978: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 04:12:14,871 EPOCH 2979
2024-02-05 04:12:28,532 Epoch 2979: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 04:12:28,533 EPOCH 2980
2024-02-05 04:12:42,665 Epoch 2980: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 04:12:42,665 EPOCH 2981
2024-02-05 04:12:56,747 Epoch 2981: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 04:12:56,748 EPOCH 2982
2024-02-05 04:13:10,749 Epoch 2982: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 04:13:10,750 EPOCH 2983
2024-02-05 04:13:24,723 Epoch 2983: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 04:13:24,724 EPOCH 2984
2024-02-05 04:13:38,581 Epoch 2984: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 04:13:38,582 EPOCH 2985
2024-02-05 04:13:52,456 Epoch 2985: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 04:13:52,457 EPOCH 2986
2024-02-05 04:14:06,300 Epoch 2986: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 04:14:06,301 EPOCH 2987
2024-02-05 04:14:20,233 Epoch 2987: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 04:14:20,233 EPOCH 2988
2024-02-05 04:14:33,999 Epoch 2988: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 04:14:34,000 EPOCH 2989
2024-02-05 04:14:47,282 [Epoch: 2989 Step: 00026900] Batch Recognition Loss:   0.000130 => Gls Tokens per Sec:      704 || Batch Translation Loss:   0.020196 => Txt Tokens per Sec:     1929 || Lr: 0.000100
2024-02-05 04:14:48,073 Epoch 2989: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 04:14:48,073 EPOCH 2990
2024-02-05 04:15:01,931 Epoch 2990: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 04:15:01,932 EPOCH 2991
2024-02-05 04:15:15,856 Epoch 2991: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 04:15:15,857 EPOCH 2992
2024-02-05 04:15:29,982 Epoch 2992: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 04:15:29,983 EPOCH 2993
2024-02-05 04:15:43,685 Epoch 2993: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 04:15:43,686 EPOCH 2994
2024-02-05 04:15:57,782 Epoch 2994: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 04:15:57,782 EPOCH 2995
2024-02-05 04:16:11,813 Epoch 2995: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 04:16:11,813 EPOCH 2996
2024-02-05 04:16:25,820 Epoch 2996: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 04:16:25,820 EPOCH 2997
2024-02-05 04:16:39,827 Epoch 2997: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 04:16:39,828 EPOCH 2998
2024-02-05 04:16:53,884 Epoch 2998: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 04:16:53,884 EPOCH 2999
2024-02-05 04:17:07,724 Epoch 2999: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 04:17:07,725 EPOCH 3000
2024-02-05 04:17:21,980 [Epoch: 3000 Step: 00027000] Batch Recognition Loss:   0.000110 => Gls Tokens per Sec:      746 || Batch Translation Loss:   0.013536 => Txt Tokens per Sec:     2070 || Lr: 0.000100
2024-02-05 04:17:21,981 Epoch 3000: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 04:17:21,981 EPOCH 3001
2024-02-05 04:17:35,852 Epoch 3001: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 04:17:35,852 EPOCH 3002
2024-02-05 04:17:49,769 Epoch 3002: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 04:17:49,770 EPOCH 3003
2024-02-05 04:18:03,756 Epoch 3003: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 04:18:03,756 EPOCH 3004
2024-02-05 04:18:17,897 Epoch 3004: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 04:18:17,898 EPOCH 3005
2024-02-05 04:18:31,865 Epoch 3005: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 04:18:31,866 EPOCH 3006
2024-02-05 04:18:45,946 Epoch 3006: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 04:18:45,947 EPOCH 3007
2024-02-05 04:18:59,959 Epoch 3007: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 04:18:59,960 EPOCH 3008
2024-02-05 04:19:13,729 Epoch 3008: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 04:19:13,729 EPOCH 3009
2024-02-05 04:19:27,641 Epoch 3009: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 04:19:27,641 EPOCH 3010
2024-02-05 04:19:41,592 Epoch 3010: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 04:19:41,592 EPOCH 3011
2024-02-05 04:19:55,756 Epoch 3011: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 04:19:55,757 EPOCH 3012
2024-02-05 04:20:00,124 [Epoch: 3012 Step: 00027100] Batch Recognition Loss:   0.000166 => Gls Tokens per Sec:       89 || Batch Translation Loss:   0.007776 => Txt Tokens per Sec:      320 || Lr: 0.000100
2024-02-05 04:20:09,612 Epoch 3012: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-05 04:20:09,612 EPOCH 3013
2024-02-05 04:20:23,254 Epoch 3013: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-05 04:20:23,254 EPOCH 3014
2024-02-05 04:20:37,250 Epoch 3014: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-05 04:20:37,251 EPOCH 3015
2024-02-05 04:20:51,252 Epoch 3015: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-05 04:20:51,252 EPOCH 3016
2024-02-05 04:21:05,055 Epoch 3016: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-05 04:21:05,056 EPOCH 3017
2024-02-05 04:21:18,850 Epoch 3017: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-05 04:21:18,851 EPOCH 3018
2024-02-05 04:21:32,844 Epoch 3018: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-05 04:21:32,845 EPOCH 3019
2024-02-05 04:21:46,611 Epoch 3019: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-05 04:21:46,611 EPOCH 3020
2024-02-05 04:22:00,791 Epoch 3020: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-05 04:22:00,792 EPOCH 3021
2024-02-05 04:22:14,728 Epoch 3021: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 04:22:14,728 EPOCH 3022
2024-02-05 04:22:28,318 Epoch 3022: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 04:22:28,319 EPOCH 3023
2024-02-05 04:22:34,966 [Epoch: 3023 Step: 00027200] Batch Recognition Loss:   0.000195 => Gls Tokens per Sec:      385 || Batch Translation Loss:   0.028831 => Txt Tokens per Sec:     1291 || Lr: 0.000100
2024-02-05 04:22:42,203 Epoch 3023: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 04:22:42,203 EPOCH 3024
2024-02-05 04:22:56,094 Epoch 3024: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 04:22:56,094 EPOCH 3025
2024-02-05 04:23:10,205 Epoch 3025: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-05 04:23:10,205 EPOCH 3026
2024-02-05 04:23:24,299 Epoch 3026: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-05 04:23:24,299 EPOCH 3027
2024-02-05 04:23:37,930 Epoch 3027: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-05 04:23:37,931 EPOCH 3028
2024-02-05 04:23:52,112 Epoch 3028: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-05 04:23:52,112 EPOCH 3029
2024-02-05 04:24:05,787 Epoch 3029: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.62 
2024-02-05 04:24:05,788 EPOCH 3030
2024-02-05 04:24:19,815 Epoch 3030: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-05 04:24:19,816 EPOCH 3031
2024-02-05 04:24:33,932 Epoch 3031: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-05 04:24:33,932 EPOCH 3032
2024-02-05 04:24:47,580 Epoch 3032: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-05 04:24:47,581 EPOCH 3033
2024-02-05 04:25:01,810 Epoch 3033: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-05 04:25:01,810 EPOCH 3034
2024-02-05 04:25:04,528 [Epoch: 3034 Step: 00027300] Batch Recognition Loss:   0.000340 => Gls Tokens per Sec:     1413 || Batch Translation Loss:   0.046570 => Txt Tokens per Sec:     3974 || Lr: 0.000100
2024-02-05 04:25:15,787 Epoch 3034: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-05 04:25:15,788 EPOCH 3035
2024-02-05 04:25:29,785 Epoch 3035: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-05 04:25:29,786 EPOCH 3036
2024-02-05 04:25:43,687 Epoch 3036: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-05 04:25:43,688 EPOCH 3037
2024-02-05 04:25:57,460 Epoch 3037: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-05 04:25:57,461 EPOCH 3038
2024-02-05 04:26:11,415 Epoch 3038: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-05 04:26:11,415 EPOCH 3039
2024-02-05 04:26:25,401 Epoch 3039: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-05 04:26:25,401 EPOCH 3040
2024-02-05 04:26:39,089 Epoch 3040: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-05 04:26:39,089 EPOCH 3041
2024-02-05 04:26:53,088 Epoch 3041: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 04:26:53,089 EPOCH 3042
2024-02-05 04:27:06,961 Epoch 3042: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 04:27:06,962 EPOCH 3043
2024-02-05 04:27:20,780 Epoch 3043: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 04:27:20,781 EPOCH 3044
2024-02-05 04:27:34,606 Epoch 3044: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 04:27:34,606 EPOCH 3045
2024-02-05 04:27:40,334 [Epoch: 3045 Step: 00027400] Batch Recognition Loss:   0.000203 => Gls Tokens per Sec:      739 || Batch Translation Loss:   0.039280 => Txt Tokens per Sec:     1991 || Lr: 0.000100
2024-02-05 04:27:48,291 Epoch 3045: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-05 04:27:48,291 EPOCH 3046
2024-02-05 04:28:02,239 Epoch 3046: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 04:28:02,239 EPOCH 3047
2024-02-05 04:28:16,195 Epoch 3047: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 04:28:16,196 EPOCH 3048
2024-02-05 04:28:30,070 Epoch 3048: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 04:28:30,070 EPOCH 3049
2024-02-05 04:28:44,067 Epoch 3049: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 04:28:44,067 EPOCH 3050
2024-02-05 04:28:57,990 Epoch 3050: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 04:28:57,991 EPOCH 3051
2024-02-05 04:29:12,036 Epoch 3051: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 04:29:12,036 EPOCH 3052
2024-02-05 04:29:25,893 Epoch 3052: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 04:29:25,894 EPOCH 3053
2024-02-05 04:29:39,697 Epoch 3053: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 04:29:39,698 EPOCH 3054
2024-02-05 04:29:53,851 Epoch 3054: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 04:29:53,852 EPOCH 3055
2024-02-05 04:30:07,609 Epoch 3055: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 04:30:07,610 EPOCH 3056
2024-02-05 04:30:10,785 [Epoch: 3056 Step: 00027500] Batch Recognition Loss:   0.000126 => Gls Tokens per Sec:     2016 || Batch Translation Loss:   0.019695 => Txt Tokens per Sec:     5036 || Lr: 0.000100
2024-02-05 04:30:21,462 Epoch 3056: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 04:30:21,462 EPOCH 3057
2024-02-05 04:30:35,471 Epoch 3057: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 04:30:35,471 EPOCH 3058
2024-02-05 04:30:49,525 Epoch 3058: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 04:30:49,526 EPOCH 3059
2024-02-05 04:31:03,379 Epoch 3059: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 04:31:03,380 EPOCH 3060
2024-02-05 04:31:16,552 Epoch 3060: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-05 04:31:16,552 EPOCH 3061
2024-02-05 04:31:30,732 Epoch 3061: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 04:31:30,732 EPOCH 3062
2024-02-05 04:31:44,488 Epoch 3062: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 04:31:44,488 EPOCH 3063
2024-02-05 04:31:58,309 Epoch 3063: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 04:31:58,309 EPOCH 3064
2024-02-05 04:32:12,008 Epoch 3064: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 04:32:12,009 EPOCH 3065
2024-02-05 04:32:25,867 Epoch 3065: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 04:32:25,868 EPOCH 3066
2024-02-05 04:32:39,779 Epoch 3066: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 04:32:39,780 EPOCH 3067
2024-02-05 04:32:51,243 [Epoch: 3067 Step: 00027600] Batch Recognition Loss:   0.000137 => Gls Tokens per Sec:      592 || Batch Translation Loss:   0.015722 => Txt Tokens per Sec:     1655 || Lr: 0.000100
2024-02-05 04:32:54,022 Epoch 3067: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 04:32:54,023 EPOCH 3068
2024-02-05 04:33:07,712 Epoch 3068: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-05 04:33:07,712 EPOCH 3069
2024-02-05 04:33:21,561 Epoch 3069: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 04:33:21,562 EPOCH 3070
2024-02-05 04:33:35,655 Epoch 3070: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 04:33:35,656 EPOCH 3071
2024-02-05 04:33:49,722 Epoch 3071: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-05 04:33:49,723 EPOCH 3072
2024-02-05 04:34:03,490 Epoch 3072: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 04:34:03,491 EPOCH 3073
2024-02-05 04:34:17,138 Epoch 3073: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-05 04:34:17,139 EPOCH 3074
2024-02-05 04:34:31,230 Epoch 3074: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-05 04:34:31,230 EPOCH 3075
2024-02-05 04:34:45,359 Epoch 3075: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-05 04:34:45,360 EPOCH 3076
2024-02-05 04:34:59,371 Epoch 3076: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-05 04:34:59,372 EPOCH 3077
2024-02-05 04:35:13,305 Epoch 3077: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.11 
2024-02-05 04:35:13,306 EPOCH 3078
2024-02-05 04:35:26,659 [Epoch: 3078 Step: 00027700] Batch Recognition Loss:   0.000274 => Gls Tokens per Sec:      604 || Batch Translation Loss:   0.138610 => Txt Tokens per Sec:     1729 || Lr: 0.000100
2024-02-05 04:35:27,415 Epoch 3078: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.69 
2024-02-05 04:35:27,415 EPOCH 3079
2024-02-05 04:35:41,180 Epoch 3079: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.84 
2024-02-05 04:35:41,180 EPOCH 3080
2024-02-05 04:35:55,384 Epoch 3080: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.57 
2024-02-05 04:35:55,384 EPOCH 3081
2024-02-05 04:36:09,387 Epoch 3081: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.40 
2024-02-05 04:36:09,388 EPOCH 3082
2024-02-05 04:36:23,457 Epoch 3082: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.98 
2024-02-05 04:36:23,457 EPOCH 3083
2024-02-05 04:36:37,470 Epoch 3083: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.75 
2024-02-05 04:36:37,470 EPOCH 3084
2024-02-05 04:36:51,547 Epoch 3084: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.59 
2024-02-05 04:36:51,548 EPOCH 3085
2024-02-05 04:37:05,094 Epoch 3085: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-05 04:37:05,095 EPOCH 3086
2024-02-05 04:37:19,093 Epoch 3086: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-05 04:37:19,093 EPOCH 3087
2024-02-05 04:37:33,072 Epoch 3087: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-05 04:37:33,072 EPOCH 3088
2024-02-05 04:37:46,979 Epoch 3088: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.63 
2024-02-05 04:37:46,980 EPOCH 3089
2024-02-05 04:38:00,261 [Epoch: 3089 Step: 00027800] Batch Recognition Loss:   0.000263 => Gls Tokens per Sec:      704 || Batch Translation Loss:   0.045148 => Txt Tokens per Sec:     1940 || Lr: 0.000100
2024-02-05 04:38:00,830 Epoch 3089: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-05 04:38:00,830 EPOCH 3090
2024-02-05 04:38:14,686 Epoch 3090: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-05 04:38:14,687 EPOCH 3091
2024-02-05 04:38:28,324 Epoch 3091: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-05 04:38:28,324 EPOCH 3092
2024-02-05 04:38:42,423 Epoch 3092: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-05 04:38:42,424 EPOCH 3093
2024-02-05 04:38:56,465 Epoch 3093: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 04:38:56,466 EPOCH 3094
2024-02-05 04:39:10,309 Epoch 3094: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 04:39:10,309 EPOCH 3095
2024-02-05 04:39:24,374 Epoch 3095: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 04:39:24,375 EPOCH 3096
2024-02-05 04:39:38,321 Epoch 3096: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 04:39:38,322 EPOCH 3097
2024-02-05 04:39:52,486 Epoch 3097: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 04:39:52,487 EPOCH 3098
2024-02-05 04:40:06,371 Epoch 3098: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 04:40:06,371 EPOCH 3099
2024-02-05 04:40:20,468 Epoch 3099: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 04:40:20,469 EPOCH 3100
2024-02-05 04:40:34,201 [Epoch: 3100 Step: 00027900] Batch Recognition Loss:   0.000142 => Gls Tokens per Sec:      774 || Batch Translation Loss:   0.008953 => Txt Tokens per Sec:     2149 || Lr: 0.000100
2024-02-05 04:40:34,201 Epoch 3100: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 04:40:34,202 EPOCH 3101
2024-02-05 04:40:48,260 Epoch 3101: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 04:40:48,260 EPOCH 3102
2024-02-05 04:41:02,211 Epoch 3102: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 04:41:02,212 EPOCH 3103
2024-02-05 04:41:16,132 Epoch 3103: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 04:41:16,133 EPOCH 3104
2024-02-05 04:41:30,031 Epoch 3104: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 04:41:30,032 EPOCH 3105
2024-02-05 04:41:43,986 Epoch 3105: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 04:41:43,986 EPOCH 3106
2024-02-05 04:41:57,652 Epoch 3106: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 04:41:57,653 EPOCH 3107
2024-02-05 04:42:11,608 Epoch 3107: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 04:42:11,609 EPOCH 3108
2024-02-05 04:42:25,345 Epoch 3108: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 04:42:25,345 EPOCH 3109
2024-02-05 04:42:39,177 Epoch 3109: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 04:42:39,178 EPOCH 3110
2024-02-05 04:42:53,245 Epoch 3110: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 04:42:53,246 EPOCH 3111
2024-02-05 04:43:07,608 Epoch 3111: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 04:43:07,608 EPOCH 3112
2024-02-05 04:43:07,858 [Epoch: 3112 Step: 00028000] Batch Recognition Loss:   0.000118 => Gls Tokens per Sec:     5141 || Batch Translation Loss:   0.009688 => Txt Tokens per Sec:     9217 || Lr: 0.000100
2024-02-05 04:43:36,473 Validation result at epoch 3112, step    28000: duration: 28.6146s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00020	Translation Loss: 98706.78125	PPL: 19488.75781
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.67	(BLEU-1: 10.35,	BLEU-2: 3.15,	BLEU-3: 1.27,	BLEU-4: 0.67)
	CHRF 17.15	ROUGE 8.96
2024-02-05 04:43:36,474 Logging Recognition and Translation Outputs
2024-02-05 04:43:36,474 ========================================================================================================================
2024-02-05 04:43:36,474 Logging Sequence: 67_98.00
2024-02-05 04:43:36,475 	Gloss Reference :	A B+C+D+E
2024-02-05 04:43:36,475 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 04:43:36,475 	Gloss Alignment :	         
2024-02-05 04:43:36,475 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 04:43:36,476 	Text Reference  :	it saddens me to  see  people suffering and dying due   to  lack of   oxygen 
2024-02-05 04:43:36,476 	Text Hypothesis :	** ******* ** she said that   there     was a     world cup in   fast bowlers
2024-02-05 04:43:36,476 	Text Alignment  :	D  D       D  S   S    S      S         S   S     S     S   S    S    S      
2024-02-05 04:43:36,476 ========================================================================================================================
2024-02-05 04:43:36,477 Logging Sequence: 157_83.00
2024-02-05 04:43:36,477 	Gloss Reference :	A B+C+D+E
2024-02-05 04:43:36,477 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 04:43:36,477 	Gloss Alignment :	         
2024-02-05 04:43:36,477 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 04:43:36,479 	Text Reference  :	also when you eat sandwich at a streetside hawker or    stall the **** sandwich maker will first apply butter with a        knife
2024-02-05 04:43:36,479 	Text Hypothesis :	**** **** *** *** ******** ** * ********** the    final of    the asia cup      2023  will be    held  on     5th  february 2022 
2024-02-05 04:43:36,479 	Text Alignment  :	D    D    D   D   D        D  D D          S      S     S         I    S        S          S     S     S      S    S        S    
2024-02-05 04:43:36,479 ========================================================================================================================
2024-02-05 04:43:36,480 Logging Sequence: 76_35.00
2024-02-05 04:43:36,480 	Gloss Reference :	A B+C+D+E
2024-02-05 04:43:36,480 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 04:43:36,480 	Gloss Alignment :	         
2024-02-05 04:43:36,480 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 04:43:36,481 	Text Reference  :	bcci president sourav ganguly along with board secretary jay shah
2024-02-05 04:43:36,481 	Text Hypothesis :	**** this      is     because the   toss and   chose     to  bowl
2024-02-05 04:43:36,481 	Text Alignment  :	D    S         S      S       S     S    S     S         S   S   
2024-02-05 04:43:36,481 ========================================================================================================================
2024-02-05 04:43:36,482 Logging Sequence: 139_180.00
2024-02-05 04:43:36,482 	Gloss Reference :	A B+C+D+E
2024-02-05 04:43:36,482 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 04:43:36,482 	Gloss Alignment :	         
2024-02-05 04:43:36,482 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 04:43:36,483 	Text Reference  :	********* ********* **** ****** ******** netherlands also faced     similar riots     
2024-02-05 04:43:36,483 	Text Hypothesis :	ahmedabad witnesses huge crowds everyone is          also following the     tournament
2024-02-05 04:43:36,483 	Text Alignment  :	I         I         I    I      I        S                S         S       S         
2024-02-05 04:43:36,483 ========================================================================================================================
2024-02-05 04:43:36,483 Logging Sequence: 98_87.00
2024-02-05 04:43:36,483 	Gloss Reference :	A B+C+D+E
2024-02-05 04:43:36,484 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 04:43:36,484 	Gloss Alignment :	         
2024-02-05 04:43:36,484 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 04:43:36,485 	Text Reference  :	instead of starting afresh in 2021 the organizers opted to       resume with     the     previous edition   
2024-02-05 04:43:36,485 	Text Hypothesis :	******* ** ******** ****** ** **** *** and        was   consoled by     denmark' captain and      goalkeeper
2024-02-05 04:43:36,485 	Text Alignment  :	D       D  D        D      D  D    D   S          S     S        S      S        S       S        S         
2024-02-05 04:43:36,485 ========================================================================================================================
2024-02-05 04:43:50,296 Epoch 3112: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 04:43:50,296 EPOCH 3113
2024-02-05 04:44:04,089 Epoch 3113: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 04:44:04,090 EPOCH 3114
2024-02-05 04:44:17,825 Epoch 3114: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 04:44:17,826 EPOCH 3115
2024-02-05 04:44:31,741 Epoch 3115: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 04:44:31,741 EPOCH 3116
2024-02-05 04:44:45,617 Epoch 3116: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 04:44:45,617 EPOCH 3117
2024-02-05 04:44:59,428 Epoch 3117: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 04:44:59,428 EPOCH 3118
2024-02-05 04:45:13,357 Epoch 3118: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 04:45:13,358 EPOCH 3119
2024-02-05 04:45:26,891 Epoch 3119: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 04:45:26,892 EPOCH 3120
2024-02-05 04:45:40,964 Epoch 3120: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 04:45:40,965 EPOCH 3121
2024-02-05 04:45:54,956 Epoch 3121: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 04:45:54,957 EPOCH 3122
2024-02-05 04:46:09,101 Epoch 3122: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 04:46:09,102 EPOCH 3123
2024-02-05 04:46:10,258 [Epoch: 3123 Step: 00028100] Batch Recognition Loss:   0.000099 => Gls Tokens per Sec:     2220 || Batch Translation Loss:   0.008894 => Txt Tokens per Sec:     5917 || Lr: 0.000050
2024-02-05 04:46:22,679 Epoch 3123: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 04:46:22,680 EPOCH 3124
2024-02-05 04:46:36,844 Epoch 3124: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 04:46:36,845 EPOCH 3125
2024-02-05 04:46:50,936 Epoch 3125: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 04:46:50,936 EPOCH 3126
2024-02-05 04:47:05,100 Epoch 3126: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 04:47:05,100 EPOCH 3127
2024-02-05 04:47:18,989 Epoch 3127: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 04:47:18,989 EPOCH 3128
2024-02-05 04:47:32,946 Epoch 3128: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 04:47:32,947 EPOCH 3129
2024-02-05 04:47:46,865 Epoch 3129: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 04:47:46,866 EPOCH 3130
2024-02-05 04:48:00,493 Epoch 3130: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 04:48:00,494 EPOCH 3131
2024-02-05 04:48:14,498 Epoch 3131: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 04:48:14,498 EPOCH 3132
2024-02-05 04:48:28,296 Epoch 3132: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 04:48:28,296 EPOCH 3133
2024-02-05 04:48:42,200 Epoch 3133: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 04:48:42,201 EPOCH 3134
2024-02-05 04:48:47,354 [Epoch: 3134 Step: 00028200] Batch Recognition Loss:   0.000114 => Gls Tokens per Sec:      573 || Batch Translation Loss:   0.012753 => Txt Tokens per Sec:     1435 || Lr: 0.000050
2024-02-05 04:48:56,168 Epoch 3134: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 04:48:56,169 EPOCH 3135
2024-02-05 04:49:10,137 Epoch 3135: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 04:49:10,137 EPOCH 3136
2024-02-05 04:49:24,276 Epoch 3136: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 04:49:24,276 EPOCH 3137
2024-02-05 04:49:38,019 Epoch 3137: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 04:49:38,020 EPOCH 3138
2024-02-05 04:49:51,846 Epoch 3138: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 04:49:51,847 EPOCH 3139
2024-02-05 04:50:05,922 Epoch 3139: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 04:50:05,923 EPOCH 3140
2024-02-05 04:50:19,941 Epoch 3140: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 04:50:19,942 EPOCH 3141
2024-02-05 04:50:33,982 Epoch 3141: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 04:50:33,982 EPOCH 3142
2024-02-05 04:50:47,753 Epoch 3142: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 04:50:47,753 EPOCH 3143
2024-02-05 04:51:01,660 Epoch 3143: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 04:51:01,661 EPOCH 3144
2024-02-05 04:51:15,562 Epoch 3144: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 04:51:15,563 EPOCH 3145
2024-02-05 04:51:23,313 [Epoch: 3145 Step: 00028300] Batch Recognition Loss:   0.000130 => Gls Tokens per Sec:      661 || Batch Translation Loss:   0.016071 => Txt Tokens per Sec:     1900 || Lr: 0.000050
2024-02-05 04:51:29,744 Epoch 3145: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 04:51:29,744 EPOCH 3146
2024-02-05 04:51:43,741 Epoch 3146: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 04:51:43,742 EPOCH 3147
2024-02-05 04:51:57,952 Epoch 3147: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 04:51:57,952 EPOCH 3148
2024-02-05 04:52:12,226 Epoch 3148: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 04:52:12,227 EPOCH 3149
2024-02-05 04:52:25,996 Epoch 3149: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 04:52:25,996 EPOCH 3150
2024-02-05 04:52:40,182 Epoch 3150: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 04:52:40,183 EPOCH 3151
2024-02-05 04:52:54,050 Epoch 3151: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 04:52:54,051 EPOCH 3152
2024-02-05 04:53:08,080 Epoch 3152: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 04:53:08,081 EPOCH 3153
2024-02-05 04:53:22,098 Epoch 3153: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 04:53:22,098 EPOCH 3154
2024-02-05 04:53:36,052 Epoch 3154: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 04:53:36,053 EPOCH 3155
2024-02-05 04:53:49,917 Epoch 3155: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 04:53:49,918 EPOCH 3156
2024-02-05 04:53:57,283 [Epoch: 3156 Step: 00028400] Batch Recognition Loss:   0.000140 => Gls Tokens per Sec:      748 || Batch Translation Loss:   0.013816 => Txt Tokens per Sec:     2092 || Lr: 0.000050
2024-02-05 04:54:03,804 Epoch 3156: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 04:54:03,805 EPOCH 3157
2024-02-05 04:54:17,569 Epoch 3157: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 04:54:17,570 EPOCH 3158
2024-02-05 04:54:31,746 Epoch 3158: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 04:54:31,746 EPOCH 3159
2024-02-05 04:54:45,677 Epoch 3159: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 04:54:45,678 EPOCH 3160
2024-02-05 04:54:59,623 Epoch 3160: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 04:54:59,623 EPOCH 3161
2024-02-05 04:55:13,770 Epoch 3161: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 04:55:13,771 EPOCH 3162
2024-02-05 04:55:27,600 Epoch 3162: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 04:55:27,601 EPOCH 3163
2024-02-05 04:55:41,057 Epoch 3163: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 04:55:41,058 EPOCH 3164
2024-02-05 04:55:54,803 Epoch 3164: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 04:55:54,803 EPOCH 3165
2024-02-05 04:56:08,800 Epoch 3165: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 04:56:08,800 EPOCH 3166
2024-02-05 04:56:22,784 Epoch 3166: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 04:56:22,785 EPOCH 3167
2024-02-05 04:56:32,266 [Epoch: 3167 Step: 00028500] Batch Recognition Loss:   0.000098 => Gls Tokens per Sec:      810 || Batch Translation Loss:   0.011491 => Txt Tokens per Sec:     2390 || Lr: 0.000050
2024-02-05 04:56:37,147 Epoch 3167: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 04:56:37,147 EPOCH 3168
2024-02-05 04:56:51,056 Epoch 3168: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 04:56:51,057 EPOCH 3169
2024-02-05 04:57:05,143 Epoch 3169: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 04:57:05,143 EPOCH 3170
2024-02-05 04:57:19,120 Epoch 3170: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 04:57:19,121 EPOCH 3171
2024-02-05 04:57:33,190 Epoch 3171: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 04:57:33,191 EPOCH 3172
2024-02-05 04:57:47,122 Epoch 3172: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 04:57:47,122 EPOCH 3173
2024-02-05 04:58:00,853 Epoch 3173: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 04:58:00,853 EPOCH 3174
2024-02-05 04:58:15,075 Epoch 3174: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 04:58:15,075 EPOCH 3175
2024-02-05 04:58:28,631 Epoch 3175: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 04:58:28,631 EPOCH 3176
2024-02-05 04:58:42,592 Epoch 3176: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 04:58:42,593 EPOCH 3177
2024-02-05 04:58:56,627 Epoch 3177: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 04:58:56,628 EPOCH 3178
2024-02-05 04:59:09,489 [Epoch: 3178 Step: 00028600] Batch Recognition Loss:   0.000105 => Gls Tokens per Sec:      628 || Batch Translation Loss:   0.010388 => Txt Tokens per Sec:     1717 || Lr: 0.000050
2024-02-05 04:59:10,627 Epoch 3178: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 04:59:10,628 EPOCH 3179
2024-02-05 04:59:24,235 Epoch 3179: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 04:59:24,236 EPOCH 3180
2024-02-05 04:59:38,194 Epoch 3180: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 04:59:38,194 EPOCH 3181
2024-02-05 04:59:51,987 Epoch 3181: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 04:59:51,987 EPOCH 3182
2024-02-05 05:00:06,070 Epoch 3182: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 05:00:06,070 EPOCH 3183
2024-02-05 05:00:20,264 Epoch 3183: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 05:00:20,265 EPOCH 3184
2024-02-05 05:00:34,138 Epoch 3184: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 05:00:34,139 EPOCH 3185
2024-02-05 05:00:48,121 Epoch 3185: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 05:00:48,122 EPOCH 3186
2024-02-05 05:01:01,895 Epoch 3186: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 05:01:01,896 EPOCH 3187
2024-02-05 05:01:15,711 Epoch 3187: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 05:01:15,712 EPOCH 3188
2024-02-05 05:01:29,651 Epoch 3188: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 05:01:29,652 EPOCH 3189
2024-02-05 05:01:43,228 [Epoch: 3189 Step: 00028700] Batch Recognition Loss:   0.000099 => Gls Tokens per Sec:      689 || Batch Translation Loss:   0.011199 => Txt Tokens per Sec:     1959 || Lr: 0.000050
2024-02-05 05:01:43,538 Epoch 3189: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 05:01:43,538 EPOCH 3190
2024-02-05 05:01:57,356 Epoch 3190: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 05:01:57,356 EPOCH 3191
2024-02-05 05:02:11,348 Epoch 3191: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 05:02:11,348 EPOCH 3192
2024-02-05 05:02:25,443 Epoch 3192: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 05:02:25,443 EPOCH 3193
2024-02-05 05:02:39,313 Epoch 3193: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 05:02:39,313 EPOCH 3194
2024-02-05 05:02:53,265 Epoch 3194: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 05:02:53,265 EPOCH 3195
2024-02-05 05:03:06,803 Epoch 3195: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 05:03:06,804 EPOCH 3196
2024-02-05 05:03:20,793 Epoch 3196: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 05:03:20,793 EPOCH 3197
2024-02-05 05:03:34,801 Epoch 3197: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 05:03:34,801 EPOCH 3198
2024-02-05 05:03:48,727 Epoch 3198: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 05:03:48,727 EPOCH 3199
2024-02-05 05:04:02,512 Epoch 3199: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 05:04:02,513 EPOCH 3200
2024-02-05 05:04:16,617 [Epoch: 3200 Step: 00028800] Batch Recognition Loss:   0.000099 => Gls Tokens per Sec:      754 || Batch Translation Loss:   0.012667 => Txt Tokens per Sec:     2093 || Lr: 0.000050
2024-02-05 05:04:16,617 Epoch 3200: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 05:04:16,617 EPOCH 3201
2024-02-05 05:04:30,814 Epoch 3201: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 05:04:30,815 EPOCH 3202
2024-02-05 05:04:44,544 Epoch 3202: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 05:04:44,544 EPOCH 3203
2024-02-05 05:04:58,442 Epoch 3203: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 05:04:58,442 EPOCH 3204
2024-02-05 05:05:12,323 Epoch 3204: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 05:05:12,323 EPOCH 3205
2024-02-05 05:05:26,169 Epoch 3205: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 05:05:26,170 EPOCH 3206
2024-02-05 05:05:39,844 Epoch 3206: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 05:05:39,844 EPOCH 3207
2024-02-05 05:05:53,712 Epoch 3207: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 05:05:53,712 EPOCH 3208
2024-02-05 05:06:07,930 Epoch 3208: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 05:06:07,931 EPOCH 3209
2024-02-05 05:06:21,906 Epoch 3209: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 05:06:21,906 EPOCH 3210
2024-02-05 05:06:35,865 Epoch 3210: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 05:06:35,866 EPOCH 3211
2024-02-05 05:06:49,742 Epoch 3211: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 05:06:49,743 EPOCH 3212
2024-02-05 05:06:50,244 [Epoch: 3212 Step: 00028900] Batch Recognition Loss:   0.000106 => Gls Tokens per Sec:     2560 || Batch Translation Loss:   0.013315 => Txt Tokens per Sec:     7582 || Lr: 0.000050
2024-02-05 05:07:03,602 Epoch 3212: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 05:07:03,603 EPOCH 3213
2024-02-05 05:07:17,417 Epoch 3213: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 05:07:17,418 EPOCH 3214
2024-02-05 05:07:31,201 Epoch 3214: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 05:07:31,202 EPOCH 3215
2024-02-05 05:07:45,025 Epoch 3215: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 05:07:45,026 EPOCH 3216
2024-02-05 05:07:59,080 Epoch 3216: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 05:07:59,081 EPOCH 3217
2024-02-05 05:08:12,817 Epoch 3217: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 05:08:12,818 EPOCH 3218
2024-02-05 05:08:26,749 Epoch 3218: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 05:08:26,749 EPOCH 3219
2024-02-05 05:08:40,729 Epoch 3219: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 05:08:40,730 EPOCH 3220
2024-02-05 05:08:54,637 Epoch 3220: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 05:08:54,638 EPOCH 3221
2024-02-05 05:09:08,267 Epoch 3221: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 05:09:08,268 EPOCH 3222
2024-02-05 05:09:22,265 Epoch 3222: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 05:09:22,265 EPOCH 3223
2024-02-05 05:09:31,830 [Epoch: 3223 Step: 00029000] Batch Recognition Loss:   0.000139 => Gls Tokens per Sec:      175 || Batch Translation Loss:   0.020013 => Txt Tokens per Sec:      615 || Lr: 0.000050
2024-02-05 05:09:36,493 Epoch 3223: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 05:09:36,493 EPOCH 3224
2024-02-05 05:09:50,307 Epoch 3224: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 05:09:50,307 EPOCH 3225
2024-02-05 05:10:04,363 Epoch 3225: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 05:10:04,363 EPOCH 3226
2024-02-05 05:10:18,382 Epoch 3226: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 05:10:18,382 EPOCH 3227
2024-02-05 05:10:32,256 Epoch 3227: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 05:10:32,256 EPOCH 3228
2024-02-05 05:10:46,074 Epoch 3228: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 05:10:46,074 EPOCH 3229
2024-02-05 05:10:59,704 Epoch 3229: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 05:10:59,705 EPOCH 3230
2024-02-05 05:11:13,615 Epoch 3230: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 05:11:13,616 EPOCH 3231
2024-02-05 05:11:27,667 Epoch 3231: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 05:11:27,668 EPOCH 3232
2024-02-05 05:11:41,272 Epoch 3232: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 05:11:41,273 EPOCH 3233
2024-02-05 05:11:55,354 Epoch 3233: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 05:11:55,355 EPOCH 3234
2024-02-05 05:12:00,852 [Epoch: 3234 Step: 00029100] Batch Recognition Loss:   0.000126 => Gls Tokens per Sec:      537 || Batch Translation Loss:   0.021062 => Txt Tokens per Sec:     1598 || Lr: 0.000050
2024-02-05 05:12:09,403 Epoch 3234: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 05:12:09,403 EPOCH 3235
2024-02-05 05:12:23,321 Epoch 3235: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 05:12:23,321 EPOCH 3236
2024-02-05 05:12:37,218 Epoch 3236: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 05:12:37,219 EPOCH 3237
2024-02-05 05:12:51,206 Epoch 3237: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 05:12:51,206 EPOCH 3238
2024-02-05 05:13:05,261 Epoch 3238: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 05:13:05,261 EPOCH 3239
2024-02-05 05:13:19,110 Epoch 3239: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 05:13:19,111 EPOCH 3240
2024-02-05 05:13:32,956 Epoch 3240: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-05 05:13:32,956 EPOCH 3241
2024-02-05 05:13:46,993 Epoch 3241: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.82 
2024-02-05 05:13:46,993 EPOCH 3242
2024-02-05 05:14:01,361 Epoch 3242: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.99 
2024-02-05 05:14:01,361 EPOCH 3243
2024-02-05 05:14:15,281 Epoch 3243: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.14 
2024-02-05 05:14:15,282 EPOCH 3244
2024-02-05 05:14:29,042 Epoch 3244: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-05 05:14:29,043 EPOCH 3245
2024-02-05 05:14:35,045 [Epoch: 3245 Step: 00029200] Batch Recognition Loss:   0.000183 => Gls Tokens per Sec:      705 || Batch Translation Loss:   0.028326 => Txt Tokens per Sec:     1882 || Lr: 0.000050
2024-02-05 05:14:43,222 Epoch 3245: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-05 05:14:43,223 EPOCH 3246
2024-02-05 05:14:57,026 Epoch 3246: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-05 05:14:57,027 EPOCH 3247
2024-02-05 05:15:10,912 Epoch 3247: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-05 05:15:10,913 EPOCH 3248
2024-02-05 05:15:24,552 Epoch 3248: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-05 05:15:24,552 EPOCH 3249
2024-02-05 05:15:38,547 Epoch 3249: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 05:15:38,547 EPOCH 3250
2024-02-05 05:15:52,587 Epoch 3250: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 05:15:52,587 EPOCH 3251
2024-02-05 05:16:06,393 Epoch 3251: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 05:16:06,393 EPOCH 3252
2024-02-05 05:16:20,484 Epoch 3252: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 05:16:20,485 EPOCH 3253
2024-02-05 05:16:34,272 Epoch 3253: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 05:16:34,273 EPOCH 3254
2024-02-05 05:16:48,437 Epoch 3254: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 05:16:48,437 EPOCH 3255
2024-02-05 05:17:02,431 Epoch 3255: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 05:17:02,431 EPOCH 3256
2024-02-05 05:17:14,712 [Epoch: 3256 Step: 00029300] Batch Recognition Loss:   0.000185 => Gls Tokens per Sec:      449 || Batch Translation Loss:   0.024634 => Txt Tokens per Sec:     1367 || Lr: 0.000050
2024-02-05 05:17:16,331 Epoch 3256: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 05:17:16,331 EPOCH 3257
2024-02-05 05:17:30,302 Epoch 3257: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 05:17:30,303 EPOCH 3258
2024-02-05 05:17:44,236 Epoch 3258: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 05:17:44,237 EPOCH 3259
2024-02-05 05:17:58,069 Epoch 3259: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 05:17:58,070 EPOCH 3260
2024-02-05 05:18:11,967 Epoch 3260: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 05:18:11,967 EPOCH 3261
2024-02-05 05:18:25,818 Epoch 3261: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 05:18:25,819 EPOCH 3262
2024-02-05 05:18:40,012 Epoch 3262: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 05:18:40,013 EPOCH 3263
2024-02-05 05:18:53,947 Epoch 3263: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 05:18:53,947 EPOCH 3264
2024-02-05 05:19:07,755 Epoch 3264: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 05:19:07,755 EPOCH 3265
2024-02-05 05:19:21,877 Epoch 3265: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 05:19:21,878 EPOCH 3266
2024-02-05 05:19:35,733 Epoch 3266: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 05:19:35,734 EPOCH 3267
2024-02-05 05:19:43,476 [Epoch: 3267 Step: 00029400] Batch Recognition Loss:   0.000121 => Gls Tokens per Sec:      877 || Batch Translation Loss:   0.013080 => Txt Tokens per Sec:     2442 || Lr: 0.000050
2024-02-05 05:19:49,556 Epoch 3267: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 05:19:49,557 EPOCH 3268
2024-02-05 05:20:03,596 Epoch 3268: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 05:20:03,597 EPOCH 3269
2024-02-05 05:20:17,458 Epoch 3269: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 05:20:17,459 EPOCH 3270
2024-02-05 05:20:31,582 Epoch 3270: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 05:20:31,583 EPOCH 3271
2024-02-05 05:20:45,325 Epoch 3271: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 05:20:45,325 EPOCH 3272
2024-02-05 05:20:59,375 Epoch 3272: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 05:20:59,375 EPOCH 3273
2024-02-05 05:21:13,357 Epoch 3273: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 05:21:13,358 EPOCH 3274
2024-02-05 05:21:27,142 Epoch 3274: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 05:21:27,143 EPOCH 3275
2024-02-05 05:21:41,461 Epoch 3275: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 05:21:41,462 EPOCH 3276
2024-02-05 05:21:55,071 Epoch 3276: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 05:21:55,072 EPOCH 3277
2024-02-05 05:22:09,152 Epoch 3277: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 05:22:09,152 EPOCH 3278
2024-02-05 05:22:21,831 [Epoch: 3278 Step: 00029500] Batch Recognition Loss:   0.000146 => Gls Tokens per Sec:      637 || Batch Translation Loss:   0.018990 => Txt Tokens per Sec:     1775 || Lr: 0.000050
2024-02-05 05:22:23,299 Epoch 3278: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 05:22:23,299 EPOCH 3279
2024-02-05 05:22:37,236 Epoch 3279: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 05:22:37,236 EPOCH 3280
2024-02-05 05:22:51,324 Epoch 3280: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 05:22:51,324 EPOCH 3281
2024-02-05 05:23:05,329 Epoch 3281: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 05:23:05,330 EPOCH 3282
2024-02-05 05:23:19,309 Epoch 3282: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 05:23:19,309 EPOCH 3283
2024-02-05 05:23:33,289 Epoch 3283: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 05:23:33,289 EPOCH 3284
2024-02-05 05:23:47,154 Epoch 3284: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 05:23:47,155 EPOCH 3285
2024-02-05 05:24:01,136 Epoch 3285: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 05:24:01,137 EPOCH 3286
2024-02-05 05:24:14,977 Epoch 3286: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 05:24:14,977 EPOCH 3287
2024-02-05 05:24:28,965 Epoch 3287: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 05:24:28,965 EPOCH 3288
2024-02-05 05:24:42,896 Epoch 3288: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 05:24:42,897 EPOCH 3289
2024-02-05 05:24:51,865 [Epoch: 3289 Step: 00029600] Batch Recognition Loss:   0.000115 => Gls Tokens per Sec:     1043 || Batch Translation Loss:   0.010776 => Txt Tokens per Sec:     2791 || Lr: 0.000050
2024-02-05 05:24:56,824 Epoch 3289: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 05:24:56,825 EPOCH 3290
2024-02-05 05:25:10,532 Epoch 3290: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 05:25:10,532 EPOCH 3291
2024-02-05 05:25:24,680 Epoch 3291: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 05:25:24,681 EPOCH 3292
2024-02-05 05:25:38,614 Epoch 3292: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 05:25:38,615 EPOCH 3293
2024-02-05 05:25:52,559 Epoch 3293: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 05:25:52,560 EPOCH 3294
2024-02-05 05:26:06,900 Epoch 3294: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 05:26:06,900 EPOCH 3295
2024-02-05 05:26:20,654 Epoch 3295: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 05:26:20,654 EPOCH 3296
2024-02-05 05:26:34,850 Epoch 3296: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 05:26:34,850 EPOCH 3297
2024-02-05 05:26:48,969 Epoch 3297: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 05:26:48,970 EPOCH 3298
2024-02-05 05:27:02,433 Epoch 3298: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 05:27:02,434 EPOCH 3299
2024-02-05 05:27:16,535 Epoch 3299: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 05:27:16,536 EPOCH 3300
2024-02-05 05:27:30,335 [Epoch: 3300 Step: 00029700] Batch Recognition Loss:   0.000110 => Gls Tokens per Sec:      770 || Batch Translation Loss:   0.010395 => Txt Tokens per Sec:     2139 || Lr: 0.000050
2024-02-05 05:27:30,336 Epoch 3300: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 05:27:30,336 EPOCH 3301
2024-02-05 05:27:44,006 Epoch 3301: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 05:27:44,007 EPOCH 3302
2024-02-05 05:27:58,033 Epoch 3302: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 05:27:58,033 EPOCH 3303
2024-02-05 05:28:12,071 Epoch 3303: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 05:28:12,072 EPOCH 3304
2024-02-05 05:28:26,067 Epoch 3304: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-05 05:28:26,068 EPOCH 3305
2024-02-05 05:28:39,841 Epoch 3305: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-05 05:28:39,842 EPOCH 3306
2024-02-05 05:28:53,792 Epoch 3306: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-05 05:28:53,793 EPOCH 3307
2024-02-05 05:29:07,816 Epoch 3307: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-05 05:29:07,816 EPOCH 3308
2024-02-05 05:29:21,879 Epoch 3308: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-05 05:29:21,880 EPOCH 3309
2024-02-05 05:29:35,827 Epoch 3309: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-05 05:29:35,828 EPOCH 3310
2024-02-05 05:29:49,688 Epoch 3310: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 05:29:49,689 EPOCH 3311
2024-02-05 05:30:03,560 Epoch 3311: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 05:30:03,561 EPOCH 3312
2024-02-05 05:30:07,859 [Epoch: 3312 Step: 00029800] Batch Recognition Loss:   0.000337 => Gls Tokens per Sec:       91 || Batch Translation Loss:   0.009616 => Txt Tokens per Sec:      325 || Lr: 0.000050
2024-02-05 05:30:17,502 Epoch 3312: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 05:30:17,502 EPOCH 3313
2024-02-05 05:30:31,386 Epoch 3313: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 05:30:31,387 EPOCH 3314
2024-02-05 05:30:45,425 Epoch 3314: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 05:30:45,426 EPOCH 3315
2024-02-05 05:30:59,020 Epoch 3315: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 05:30:59,021 EPOCH 3316
2024-02-05 05:31:13,249 Epoch 3316: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 05:31:13,249 EPOCH 3317
2024-02-05 05:31:27,127 Epoch 3317: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 05:31:27,128 EPOCH 3318
2024-02-05 05:31:41,048 Epoch 3318: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 05:31:41,049 EPOCH 3319
2024-02-05 05:31:55,103 Epoch 3319: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 05:31:55,104 EPOCH 3320
2024-02-05 05:32:09,246 Epoch 3320: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 05:32:09,247 EPOCH 3321
2024-02-05 05:32:22,793 Epoch 3321: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 05:32:22,794 EPOCH 3322
2024-02-05 05:32:36,834 Epoch 3322: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 05:32:36,834 EPOCH 3323
2024-02-05 05:32:41,505 [Epoch: 3323 Step: 00029900] Batch Recognition Loss:   0.000107 => Gls Tokens per Sec:      358 || Batch Translation Loss:   0.011335 => Txt Tokens per Sec:      930 || Lr: 0.000050
2024-02-05 05:32:50,988 Epoch 3323: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 05:32:50,988 EPOCH 3324
2024-02-05 05:33:05,071 Epoch 3324: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 05:33:05,072 EPOCH 3325
2024-02-05 05:33:19,064 Epoch 3325: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 05:33:19,065 EPOCH 3326
2024-02-05 05:33:33,027 Epoch 3326: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 05:33:33,028 EPOCH 3327
2024-02-05 05:33:46,968 Epoch 3327: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 05:33:46,969 EPOCH 3328
2024-02-05 05:34:00,797 Epoch 3328: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 05:34:00,798 EPOCH 3329
2024-02-05 05:34:14,660 Epoch 3329: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 05:34:14,661 EPOCH 3330
2024-02-05 05:34:28,575 Epoch 3330: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 05:34:28,576 EPOCH 3331
2024-02-05 05:34:42,538 Epoch 3331: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 05:34:42,538 EPOCH 3332
2024-02-05 05:34:56,588 Epoch 3332: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 05:34:56,588 EPOCH 3333
2024-02-05 05:35:10,459 Epoch 3333: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 05:35:10,459 EPOCH 3334
2024-02-05 05:35:12,123 [Epoch: 3334 Step: 00030000] Batch Recognition Loss:   0.000107 => Gls Tokens per Sec:     2309 || Batch Translation Loss:   0.011402 => Txt Tokens per Sec:     6201 || Lr: 0.000050
2024-02-05 05:35:40,964 Validation result at epoch 3334, step    30000: duration: 28.8398s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00019	Translation Loss: 98192.26562	PPL: 18510.71680
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.58	(BLEU-1: 10.38,	BLEU-2: 3.02,	BLEU-3: 1.18,	BLEU-4: 0.58)
	CHRF 17.09	ROUGE 8.79
2024-02-05 05:35:40,965 Logging Recognition and Translation Outputs
2024-02-05 05:35:40,965 ========================================================================================================================
2024-02-05 05:35:40,966 Logging Sequence: 165_502.00
2024-02-05 05:35:40,966 	Gloss Reference :	A B+C+D+E
2024-02-05 05:35:40,966 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 05:35:40,966 	Gloss Alignment :	         
2024-02-05 05:35:40,967 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 05:35:40,968 	Text Reference  :	****** tendulkar would sit in   the  pavilion wearing both     his batting pads even after he     got out  
2024-02-05 05:35:40,968 	Text Hypothesis :	sachin tendulkar ***** *** also been an       auction happened as  they    did  not  lose  before any match
2024-02-05 05:35:40,968 	Text Alignment  :	I                D     D   S    S    S        S       S        S   S       S    S    S     S      S   S    
2024-02-05 05:35:40,968 ========================================================================================================================
2024-02-05 05:35:40,969 Logging Sequence: 127_57.00
2024-02-05 05:35:40,969 	Gloss Reference :	A B+C+D+E
2024-02-05 05:35:40,969 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 05:35:40,969 	Gloss Alignment :	         
2024-02-05 05:35:40,969 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 05:35:40,971 	Text Reference  :	*** till date india had won only 2 medals at the championships which like the   olympics is the highest level championship
2024-02-05 05:35:40,971 	Text Hypothesis :	the here is   india *** *** **** * ****** ** *** i             am    very first time     in the ******* world cup         
2024-02-05 05:35:40,971 	Text Alignment  :	I   S    S          D   D   D    D D      D  D   S             S     S    S     S        S      D       S     S           
2024-02-05 05:35:40,971 ========================================================================================================================
2024-02-05 05:35:40,971 Logging Sequence: 169_10.00
2024-02-05 05:35:40,972 	Gloss Reference :	A B+C+D+E
2024-02-05 05:35:40,972 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 05:35:40,972 	Gloss Alignment :	         
2024-02-05 05:35:40,972 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 05:35:40,974 	Text Reference  :	the 18th  over was  bowled by       ravi    bishnoi with  khushdil shah  and     asif ali     on the ********* crease
2024-02-05 05:35:40,974 	Text Hypothesis :	*** kohli said that though arshdeep dropped the     catch he       still removed it   because of the khalistan cause 
2024-02-05 05:35:40,974 	Text Alignment  :	D   S     S    S    S      S        S       S       S     S        S     S       S    S       S      I         S     
2024-02-05 05:35:40,974 ========================================================================================================================
2024-02-05 05:35:40,974 Logging Sequence: 64_89.00
2024-02-05 05:35:40,974 	Gloss Reference :	A B+C+D+E
2024-02-05 05:35:40,974 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 05:35:40,975 	Gloss Alignment :	         
2024-02-05 05:35:40,975 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 05:35:40,976 	Text Reference  :	but this can not go on   amidst the rising cases    human lives need to  be   safeguarded
2024-02-05 05:35:40,976 	Text Hypothesis :	*** **** *** *** ** with is     ipl will   continue to    play  he   was very time       
2024-02-05 05:35:40,976 	Text Alignment  :	D   D    D   D   D  S    S      S   S      S        S     S     S    S   S    S          
2024-02-05 05:35:40,976 ========================================================================================================================
2024-02-05 05:35:40,976 Logging Sequence: 166_261.00
2024-02-05 05:35:40,976 	Gloss Reference :	A B+C+D+E
2024-02-05 05:35:40,977 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 05:35:40,977 	Gloss Alignment :	         
2024-02-05 05:35:40,977 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 05:35:40,978 	Text Reference  :	for       all organizational matters  and  the **** *** ****** ** * *** *** schedule
2024-02-05 05:35:40,978 	Text Hypothesis :	yesterday on  18th           december 2021 the team are graded in a day and england 
2024-02-05 05:35:40,978 	Text Alignment  :	S         S   S              S        S        I    I   I      I  I I   I   S       
2024-02-05 05:35:40,978 ========================================================================================================================
2024-02-05 05:35:53,474 Epoch 3334: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 05:35:53,475 EPOCH 3335
2024-02-05 05:36:07,549 Epoch 3335: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 05:36:07,550 EPOCH 3336
2024-02-05 05:36:21,420 Epoch 3336: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 05:36:21,421 EPOCH 3337
2024-02-05 05:36:35,325 Epoch 3337: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 05:36:35,326 EPOCH 3338
2024-02-05 05:36:49,430 Epoch 3338: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 05:36:49,431 EPOCH 3339
2024-02-05 05:37:03,152 Epoch 3339: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 05:37:03,152 EPOCH 3340
2024-02-05 05:37:16,807 Epoch 3340: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 05:37:16,808 EPOCH 3341
2024-02-05 05:37:31,242 Epoch 3341: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 05:37:31,242 EPOCH 3342
2024-02-05 05:37:45,039 Epoch 3342: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 05:37:45,040 EPOCH 3343
2024-02-05 05:37:59,133 Epoch 3343: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 05:37:59,134 EPOCH 3344
2024-02-05 05:38:12,831 Epoch 3344: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 05:38:12,832 EPOCH 3345
2024-02-05 05:38:19,174 [Epoch: 3345 Step: 00030100] Batch Recognition Loss:   0.000144 => Gls Tokens per Sec:      808 || Batch Translation Loss:   0.022173 => Txt Tokens per Sec:     2086 || Lr: 0.000050
2024-02-05 05:38:27,070 Epoch 3345: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 05:38:27,071 EPOCH 3346
2024-02-05 05:38:41,157 Epoch 3346: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 05:38:41,157 EPOCH 3347
2024-02-05 05:38:54,962 Epoch 3347: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 05:38:54,963 EPOCH 3348
2024-02-05 05:39:08,947 Epoch 3348: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 05:39:08,948 EPOCH 3349
2024-02-05 05:39:22,819 Epoch 3349: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 05:39:22,819 EPOCH 3350
2024-02-05 05:39:36,925 Epoch 3350: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 05:39:36,926 EPOCH 3351
2024-02-05 05:39:51,000 Epoch 3351: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 05:39:51,001 EPOCH 3352
2024-02-05 05:40:05,049 Epoch 3352: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 05:40:05,050 EPOCH 3353
2024-02-05 05:40:19,108 Epoch 3353: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 05:40:19,109 EPOCH 3354
2024-02-05 05:40:33,166 Epoch 3354: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 05:40:33,167 EPOCH 3355
2024-02-05 05:40:47,502 Epoch 3355: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 05:40:47,502 EPOCH 3356
2024-02-05 05:40:55,754 [Epoch: 3356 Step: 00030200] Batch Recognition Loss:   0.000106 => Gls Tokens per Sec:      776 || Batch Translation Loss:   0.008560 => Txt Tokens per Sec:     2178 || Lr: 0.000050
2024-02-05 05:41:01,406 Epoch 3356: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 05:41:01,407 EPOCH 3357
2024-02-05 05:41:15,323 Epoch 3357: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 05:41:15,323 EPOCH 3358
2024-02-05 05:41:29,194 Epoch 3358: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 05:41:29,194 EPOCH 3359
2024-02-05 05:41:43,431 Epoch 3359: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 05:41:43,432 EPOCH 3360
2024-02-05 05:41:57,612 Epoch 3360: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 05:41:57,613 EPOCH 3361
2024-02-05 05:42:11,387 Epoch 3361: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 05:42:11,387 EPOCH 3362
2024-02-05 05:42:25,494 Epoch 3362: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 05:42:25,495 EPOCH 3363
2024-02-05 05:42:39,428 Epoch 3363: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 05:42:39,429 EPOCH 3364
2024-02-05 05:42:53,193 Epoch 3364: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 05:42:53,193 EPOCH 3365
2024-02-05 05:43:07,096 Epoch 3365: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 05:43:07,097 EPOCH 3366
2024-02-05 05:43:20,729 Epoch 3366: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 05:43:20,730 EPOCH 3367
2024-02-05 05:43:33,131 [Epoch: 3367 Step: 00030300] Batch Recognition Loss:   0.000100 => Gls Tokens per Sec:      548 || Batch Translation Loss:   0.011611 => Txt Tokens per Sec:     1543 || Lr: 0.000050
2024-02-05 05:43:34,586 Epoch 3367: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 05:43:34,586 EPOCH 3368
2024-02-05 05:43:48,608 Epoch 3368: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 05:43:48,609 EPOCH 3369
2024-02-05 05:44:02,521 Epoch 3369: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 05:44:02,521 EPOCH 3370
2024-02-05 05:44:16,576 Epoch 3370: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 05:44:16,577 EPOCH 3371
2024-02-05 05:44:30,413 Epoch 3371: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 05:44:30,413 EPOCH 3372
2024-02-05 05:44:44,439 Epoch 3372: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 05:44:44,439 EPOCH 3373
2024-02-05 05:44:58,542 Epoch 3373: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 05:44:58,542 EPOCH 3374
2024-02-05 05:45:12,251 Epoch 3374: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 05:45:12,252 EPOCH 3375
2024-02-05 05:45:26,308 Epoch 3375: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 05:45:26,308 EPOCH 3376
2024-02-05 05:45:40,097 Epoch 3376: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 05:45:40,098 EPOCH 3377
2024-02-05 05:45:54,149 Epoch 3377: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 05:45:54,150 EPOCH 3378
2024-02-05 05:46:07,170 [Epoch: 3378 Step: 00030400] Batch Recognition Loss:   0.000211 => Gls Tokens per Sec:      620 || Batch Translation Loss:   0.027259 => Txt Tokens per Sec:     1742 || Lr: 0.000050
2024-02-05 05:46:08,293 Epoch 3378: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-05 05:46:08,293 EPOCH 3379
2024-02-05 05:46:22,069 Epoch 3379: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-05 05:46:22,069 EPOCH 3380
2024-02-05 05:46:36,413 Epoch 3380: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-05 05:46:36,414 EPOCH 3381
2024-02-05 05:46:50,742 Epoch 3381: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 05:46:50,743 EPOCH 3382
2024-02-05 05:47:04,669 Epoch 3382: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 05:47:04,669 EPOCH 3383
2024-02-05 05:47:18,783 Epoch 3383: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 05:47:18,784 EPOCH 3384
2024-02-05 05:47:32,463 Epoch 3384: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 05:47:32,464 EPOCH 3385
2024-02-05 05:47:46,577 Epoch 3385: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 05:47:46,577 EPOCH 3386
2024-02-05 05:48:00,528 Epoch 3386: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 05:48:00,528 EPOCH 3387
2024-02-05 05:48:14,628 Epoch 3387: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 05:48:14,629 EPOCH 3388
2024-02-05 05:48:28,555 Epoch 3388: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 05:48:28,555 EPOCH 3389
2024-02-05 05:48:41,863 [Epoch: 3389 Step: 00030500] Batch Recognition Loss:   0.000126 => Gls Tokens per Sec:      703 || Batch Translation Loss:   0.017582 => Txt Tokens per Sec:     1927 || Lr: 0.000050
2024-02-05 05:48:42,576 Epoch 3389: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 05:48:42,576 EPOCH 3390
2024-02-05 05:48:56,341 Epoch 3390: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 05:48:56,342 EPOCH 3391
2024-02-05 05:49:10,428 Epoch 3391: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 05:49:10,428 EPOCH 3392
2024-02-05 05:49:24,342 Epoch 3392: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 05:49:24,343 EPOCH 3393
2024-02-05 05:49:38,339 Epoch 3393: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 05:49:38,339 EPOCH 3394
2024-02-05 05:49:52,396 Epoch 3394: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 05:49:52,396 EPOCH 3395
2024-02-05 05:50:06,280 Epoch 3395: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 05:50:06,281 EPOCH 3396
2024-02-05 05:50:19,955 Epoch 3396: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 05:50:19,955 EPOCH 3397
2024-02-05 05:50:34,100 Epoch 3397: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 05:50:34,101 EPOCH 3398
2024-02-05 05:50:48,074 Epoch 3398: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 05:50:48,074 EPOCH 3399
2024-02-05 05:51:01,944 Epoch 3399: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 05:51:01,945 EPOCH 3400
2024-02-05 05:51:15,917 [Epoch: 3400 Step: 00030600] Batch Recognition Loss:   0.000166 => Gls Tokens per Sec:      761 || Batch Translation Loss:   0.029337 => Txt Tokens per Sec:     2112 || Lr: 0.000050
2024-02-05 05:51:15,917 Epoch 3400: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 05:51:15,917 EPOCH 3401
2024-02-05 05:51:29,854 Epoch 3401: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 05:51:29,855 EPOCH 3402
2024-02-05 05:51:43,901 Epoch 3402: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 05:51:43,901 EPOCH 3403
2024-02-05 05:51:57,994 Epoch 3403: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 05:51:57,995 EPOCH 3404
2024-02-05 05:52:11,639 Epoch 3404: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 05:52:11,639 EPOCH 3405
2024-02-05 05:52:25,894 Epoch 3405: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 05:52:25,895 EPOCH 3406
2024-02-05 05:52:39,719 Epoch 3406: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 05:52:39,719 EPOCH 3407
2024-02-05 05:52:53,885 Epoch 3407: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 05:52:53,885 EPOCH 3408
2024-02-05 05:53:07,868 Epoch 3408: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 05:53:07,868 EPOCH 3409
2024-02-05 05:53:21,457 Epoch 3409: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 05:53:21,458 EPOCH 3410
2024-02-05 05:53:35,539 Epoch 3410: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 05:53:35,540 EPOCH 3411
2024-02-05 05:53:49,510 Epoch 3411: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 05:53:49,510 EPOCH 3412
2024-02-05 05:53:51,189 [Epoch: 3412 Step: 00030700] Batch Recognition Loss:   0.000160 => Gls Tokens per Sec:      763 || Batch Translation Loss:   0.022289 => Txt Tokens per Sec:     2438 || Lr: 0.000050
2024-02-05 05:54:03,618 Epoch 3412: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 05:54:03,618 EPOCH 3413
2024-02-05 05:54:17,312 Epoch 3413: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 05:54:17,312 EPOCH 3414
2024-02-05 05:54:31,409 Epoch 3414: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 05:54:31,409 EPOCH 3415
2024-02-05 05:54:45,083 Epoch 3415: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 05:54:45,084 EPOCH 3416
2024-02-05 05:54:58,475 Epoch 3416: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 05:54:58,475 EPOCH 3417
2024-02-05 05:55:12,625 Epoch 3417: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 05:55:12,626 EPOCH 3418
2024-02-05 05:55:26,587 Epoch 3418: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 05:55:26,587 EPOCH 3419
2024-02-05 05:55:40,481 Epoch 3419: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 05:55:40,482 EPOCH 3420
2024-02-05 05:55:54,679 Epoch 3420: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 05:55:54,679 EPOCH 3421
2024-02-05 05:56:08,645 Epoch 3421: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 05:56:08,646 EPOCH 3422
2024-02-05 05:56:22,646 Epoch 3422: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 05:56:22,647 EPOCH 3423
2024-02-05 05:56:23,352 [Epoch: 3423 Step: 00030800] Batch Recognition Loss:   0.000127 => Gls Tokens per Sec:     3627 || Batch Translation Loss:   0.015796 => Txt Tokens per Sec:     9170 || Lr: 0.000050
2024-02-05 05:56:36,472 Epoch 3423: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-05 05:56:36,473 EPOCH 3424
2024-02-05 05:56:50,614 Epoch 3424: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-05 05:56:50,615 EPOCH 3425
2024-02-05 05:57:04,519 Epoch 3425: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-05 05:57:04,519 EPOCH 3426
2024-02-05 05:57:18,256 Epoch 3426: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-05 05:57:18,257 EPOCH 3427
2024-02-05 05:57:32,038 Epoch 3427: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-05 05:57:32,038 EPOCH 3428
2024-02-05 05:57:46,235 Epoch 3428: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-05 05:57:46,236 EPOCH 3429
2024-02-05 05:57:59,999 Epoch 3429: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 05:57:59,999 EPOCH 3430
2024-02-05 05:58:13,985 Epoch 3430: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 05:58:13,986 EPOCH 3431
2024-02-05 05:58:27,900 Epoch 3431: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 05:58:27,901 EPOCH 3432
2024-02-05 05:58:42,201 Epoch 3432: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 05:58:42,201 EPOCH 3433
2024-02-05 05:58:56,266 Epoch 3433: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 05:58:56,267 EPOCH 3434
2024-02-05 05:59:02,769 [Epoch: 3434 Step: 00030900] Batch Recognition Loss:   0.000170 => Gls Tokens per Sec:      454 || Batch Translation Loss:   0.022854 => Txt Tokens per Sec:     1421 || Lr: 0.000050
2024-02-05 05:59:10,015 Epoch 3434: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 05:59:10,015 EPOCH 3435
2024-02-05 05:59:24,214 Epoch 3435: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 05:59:24,215 EPOCH 3436
2024-02-05 05:59:38,160 Epoch 3436: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 05:59:38,161 EPOCH 3437
2024-02-05 05:59:52,229 Epoch 3437: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 05:59:52,229 EPOCH 3438
2024-02-05 06:00:06,063 Epoch 3438: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 06:00:06,063 EPOCH 3439
2024-02-05 06:00:20,000 Epoch 3439: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-05 06:00:20,001 EPOCH 3440
2024-02-05 06:00:33,721 Epoch 3440: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-05 06:00:33,722 EPOCH 3441
2024-02-05 06:00:47,904 Epoch 3441: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-05 06:00:47,904 EPOCH 3442
2024-02-05 06:01:01,801 Epoch 3442: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 06:01:01,802 EPOCH 3443
2024-02-05 06:01:15,605 Epoch 3443: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-05 06:01:15,605 EPOCH 3444
2024-02-05 06:01:29,643 Epoch 3444: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 06:01:29,643 EPOCH 3445
2024-02-05 06:01:32,984 [Epoch: 3445 Step: 00031000] Batch Recognition Loss:   0.000194 => Gls Tokens per Sec:     1533 || Batch Translation Loss:   0.027083 => Txt Tokens per Sec:     4386 || Lr: 0.000050
2024-02-05 06:01:43,530 Epoch 3445: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-05 06:01:43,531 EPOCH 3446
2024-02-05 06:01:57,604 Epoch 3446: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-05 06:01:57,605 EPOCH 3447
2024-02-05 06:02:11,195 Epoch 3447: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-05 06:02:11,195 EPOCH 3448
2024-02-05 06:02:25,566 Epoch 3448: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-05 06:02:25,567 EPOCH 3449
2024-02-05 06:02:39,413 Epoch 3449: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 06:02:39,414 EPOCH 3450
2024-02-05 06:02:53,616 Epoch 3450: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 06:02:53,617 EPOCH 3451
2024-02-05 06:03:07,618 Epoch 3451: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 06:03:07,619 EPOCH 3452
2024-02-05 06:03:21,486 Epoch 3452: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 06:03:21,486 EPOCH 3453
2024-02-05 06:03:35,604 Epoch 3453: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 06:03:35,604 EPOCH 3454
2024-02-05 06:03:49,760 Epoch 3454: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-05 06:03:49,760 EPOCH 3455
2024-02-05 06:04:03,587 Epoch 3455: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-05 06:04:03,588 EPOCH 3456
2024-02-05 06:04:14,658 [Epoch: 3456 Step: 00031100] Batch Recognition Loss:   0.000233 => Gls Tokens per Sec:      498 || Batch Translation Loss:   0.014801 => Txt Tokens per Sec:     1513 || Lr: 0.000050
2024-02-05 06:04:17,581 Epoch 3456: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-05 06:04:17,582 EPOCH 3457
2024-02-05 06:04:31,267 Epoch 3457: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-05 06:04:31,267 EPOCH 3458
2024-02-05 06:04:45,272 Epoch 3458: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-05 06:04:45,273 EPOCH 3459
2024-02-05 06:04:59,123 Epoch 3459: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-05 06:04:59,124 EPOCH 3460
2024-02-05 06:05:13,461 Epoch 3460: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 06:05:13,461 EPOCH 3461
2024-02-05 06:05:27,544 Epoch 3461: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 06:05:27,545 EPOCH 3462
2024-02-05 06:05:41,340 Epoch 3462: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 06:05:41,341 EPOCH 3463
2024-02-05 06:05:55,183 Epoch 3463: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 06:05:55,183 EPOCH 3464
2024-02-05 06:06:08,912 Epoch 3464: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 06:06:08,913 EPOCH 3465
2024-02-05 06:06:23,114 Epoch 3465: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 06:06:23,115 EPOCH 3466
2024-02-05 06:06:36,982 Epoch 3466: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 06:06:36,982 EPOCH 3467
2024-02-05 06:06:45,342 [Epoch: 3467 Step: 00031200] Batch Recognition Loss:   0.000118 => Gls Tokens per Sec:      919 || Batch Translation Loss:   0.010643 => Txt Tokens per Sec:     2453 || Lr: 0.000050
2024-02-05 06:06:51,129 Epoch 3467: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 06:06:51,130 EPOCH 3468
2024-02-05 06:07:05,108 Epoch 3468: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 06:07:05,108 EPOCH 3469
2024-02-05 06:07:18,777 Epoch 3469: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 06:07:18,777 EPOCH 3470
2024-02-05 06:07:32,921 Epoch 3470: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 06:07:32,922 EPOCH 3471
2024-02-05 06:07:46,882 Epoch 3471: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 06:07:46,883 EPOCH 3472
2024-02-05 06:08:01,141 Epoch 3472: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 06:08:01,141 EPOCH 3473
2024-02-05 06:08:14,930 Epoch 3473: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 06:08:14,931 EPOCH 3474
2024-02-05 06:08:29,005 Epoch 3474: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 06:08:29,005 EPOCH 3475
2024-02-05 06:08:42,759 Epoch 3475: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 06:08:42,760 EPOCH 3476
2024-02-05 06:08:56,538 Epoch 3476: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 06:08:56,539 EPOCH 3477
2024-02-05 06:09:10,677 Epoch 3477: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 06:09:10,678 EPOCH 3478
2024-02-05 06:09:19,573 [Epoch: 3478 Step: 00031300] Batch Recognition Loss:   0.000158 => Gls Tokens per Sec:     1007 || Batch Translation Loss:   0.021291 => Txt Tokens per Sec:     2737 || Lr: 0.000050
2024-02-05 06:09:24,513 Epoch 3478: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 06:09:24,513 EPOCH 3479
2024-02-05 06:09:38,580 Epoch 3479: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 06:09:38,581 EPOCH 3480
2024-02-05 06:09:52,658 Epoch 3480: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 06:09:52,659 EPOCH 3481
2024-02-05 06:10:06,664 Epoch 3481: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 06:10:06,664 EPOCH 3482
2024-02-05 06:10:20,919 Epoch 3482: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 06:10:20,919 EPOCH 3483
2024-02-05 06:10:34,809 Epoch 3483: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 06:10:34,810 EPOCH 3484
2024-02-05 06:10:48,554 Epoch 3484: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 06:10:48,555 EPOCH 3485
2024-02-05 06:11:02,809 Epoch 3485: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 06:11:02,810 EPOCH 3486
2024-02-05 06:11:16,882 Epoch 3486: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 06:11:16,883 EPOCH 3487
2024-02-05 06:11:30,687 Epoch 3487: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 06:11:30,688 EPOCH 3488
2024-02-05 06:11:44,445 Epoch 3488: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 06:11:44,445 EPOCH 3489
2024-02-05 06:11:57,908 [Epoch: 3489 Step: 00031400] Batch Recognition Loss:   0.000104 => Gls Tokens per Sec:      695 || Batch Translation Loss:   0.014584 => Txt Tokens per Sec:     1954 || Lr: 0.000050
2024-02-05 06:11:58,290 Epoch 3489: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 06:11:58,290 EPOCH 3490
2024-02-05 06:12:12,269 Epoch 3490: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 06:12:12,270 EPOCH 3491
2024-02-05 06:12:26,072 Epoch 3491: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 06:12:26,072 EPOCH 3492
2024-02-05 06:12:39,889 Epoch 3492: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 06:12:39,889 EPOCH 3493
2024-02-05 06:12:53,975 Epoch 3493: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 06:12:53,976 EPOCH 3494
2024-02-05 06:13:07,750 Epoch 3494: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 06:13:07,750 EPOCH 3495
2024-02-05 06:13:21,515 Epoch 3495: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 06:13:21,516 EPOCH 3496
2024-02-05 06:13:35,762 Epoch 3496: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-05 06:13:35,762 EPOCH 3497
2024-02-05 06:13:49,777 Epoch 3497: Total Training Recognition Loss 0.00  Total Training Translation Loss 3.81 
2024-02-05 06:13:49,778 EPOCH 3498
2024-02-05 06:14:03,628 Epoch 3498: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.87 
2024-02-05 06:14:03,629 EPOCH 3499
2024-02-05 06:14:17,739 Epoch 3499: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.77 
2024-02-05 06:14:17,740 EPOCH 3500
2024-02-05 06:14:31,809 [Epoch: 3500 Step: 00031500] Batch Recognition Loss:   0.002565 => Gls Tokens per Sec:      756 || Batch Translation Loss:   0.500239 => Txt Tokens per Sec:     2098 || Lr: 0.000050
2024-02-05 06:14:31,810 Epoch 3500: Total Training Recognition Loss 0.01  Total Training Translation Loss 6.63 
2024-02-05 06:14:31,810 EPOCH 3501
2024-02-05 06:14:45,799 Epoch 3501: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.56 
2024-02-05 06:14:45,800 EPOCH 3502
2024-02-05 06:14:59,299 Epoch 3502: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.83 
2024-02-05 06:14:59,300 EPOCH 3503
2024-02-05 06:15:13,298 Epoch 3503: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-05 06:15:13,299 EPOCH 3504
2024-02-05 06:15:27,306 Epoch 3504: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-05 06:15:27,307 EPOCH 3505
2024-02-05 06:15:41,277 Epoch 3505: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-05 06:15:41,278 EPOCH 3506
2024-02-05 06:15:55,020 Epoch 3506: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-05 06:15:55,020 EPOCH 3507
2024-02-05 06:16:09,097 Epoch 3507: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 06:16:09,097 EPOCH 3508
2024-02-05 06:16:23,230 Epoch 3508: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 06:16:23,231 EPOCH 3509
2024-02-05 06:16:36,934 Epoch 3509: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 06:16:36,935 EPOCH 3510
2024-02-05 06:16:50,445 Epoch 3510: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 06:16:50,445 EPOCH 3511
2024-02-05 06:17:05,008 Epoch 3511: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 06:17:05,008 EPOCH 3512
2024-02-05 06:17:05,589 [Epoch: 3512 Step: 00031600] Batch Recognition Loss:   0.000265 => Gls Tokens per Sec:     2207 || Batch Translation Loss:   0.019225 => Txt Tokens per Sec:     6464 || Lr: 0.000050
2024-02-05 06:17:19,080 Epoch 3512: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 06:17:19,081 EPOCH 3513
2024-02-05 06:17:32,803 Epoch 3513: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 06:17:32,803 EPOCH 3514
2024-02-05 06:17:46,420 Epoch 3514: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 06:17:46,420 EPOCH 3515
2024-02-05 06:18:00,319 Epoch 3515: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 06:18:00,319 EPOCH 3516
2024-02-05 06:18:14,265 Epoch 3516: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 06:18:14,266 EPOCH 3517
2024-02-05 06:18:28,149 Epoch 3517: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 06:18:28,150 EPOCH 3518
2024-02-05 06:18:41,926 Epoch 3518: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 06:18:41,926 EPOCH 3519
2024-02-05 06:18:55,804 Epoch 3519: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 06:18:55,804 EPOCH 3520
2024-02-05 06:19:09,642 Epoch 3520: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 06:19:09,643 EPOCH 3521
2024-02-05 06:19:23,820 Epoch 3521: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 06:19:23,821 EPOCH 3522
2024-02-05 06:19:37,888 Epoch 3522: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 06:19:37,888 EPOCH 3523
2024-02-05 06:19:42,709 [Epoch: 3523 Step: 00031700] Batch Recognition Loss:   0.000246 => Gls Tokens per Sec:      346 || Batch Translation Loss:   0.007673 => Txt Tokens per Sec:      901 || Lr: 0.000050
2024-02-05 06:19:51,884 Epoch 3523: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 06:19:51,885 EPOCH 3524
2024-02-05 06:20:05,777 Epoch 3524: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 06:20:05,778 EPOCH 3525
2024-02-05 06:20:19,630 Epoch 3525: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 06:20:19,631 EPOCH 3526
2024-02-05 06:20:33,776 Epoch 3526: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 06:20:33,776 EPOCH 3527
2024-02-05 06:20:47,756 Epoch 3527: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 06:20:47,756 EPOCH 3528
2024-02-05 06:21:01,751 Epoch 3528: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 06:21:01,751 EPOCH 3529
2024-02-05 06:21:15,597 Epoch 3529: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 06:21:15,597 EPOCH 3530
2024-02-05 06:21:29,796 Epoch 3530: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 06:21:29,797 EPOCH 3531
2024-02-05 06:21:43,756 Epoch 3531: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 06:21:43,756 EPOCH 3532
2024-02-05 06:21:57,690 Epoch 3532: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 06:21:57,690 EPOCH 3533
2024-02-05 06:22:11,913 Epoch 3533: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 06:22:11,914 EPOCH 3534
2024-02-05 06:22:23,008 [Epoch: 3534 Step: 00031800] Batch Recognition Loss:   0.000193 => Gls Tokens per Sec:      266 || Batch Translation Loss:   0.017274 => Txt Tokens per Sec:      899 || Lr: 0.000050
2024-02-05 06:22:25,892 Epoch 3534: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 06:22:25,892 EPOCH 3535
2024-02-05 06:22:39,662 Epoch 3535: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 06:22:39,662 EPOCH 3536
2024-02-05 06:22:53,851 Epoch 3536: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 06:22:53,852 EPOCH 3537
2024-02-05 06:23:07,803 Epoch 3537: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 06:23:07,803 EPOCH 3538
2024-02-05 06:23:21,794 Epoch 3538: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 06:23:21,795 EPOCH 3539
2024-02-05 06:23:36,131 Epoch 3539: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 06:23:36,132 EPOCH 3540
2024-02-05 06:23:50,172 Epoch 3540: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 06:23:50,173 EPOCH 3541
2024-02-05 06:24:04,325 Epoch 3541: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 06:24:04,326 EPOCH 3542
2024-02-05 06:24:18,257 Epoch 3542: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 06:24:18,257 EPOCH 3543
2024-02-05 06:24:32,153 Epoch 3543: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 06:24:32,153 EPOCH 3544
2024-02-05 06:24:46,037 Epoch 3544: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 06:24:46,037 EPOCH 3545
2024-02-05 06:24:52,273 [Epoch: 3545 Step: 00031900] Batch Recognition Loss:   0.000117 => Gls Tokens per Sec:      678 || Batch Translation Loss:   0.011298 => Txt Tokens per Sec:     1966 || Lr: 0.000050
2024-02-05 06:25:00,272 Epoch 3545: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 06:25:00,273 EPOCH 3546
2024-02-05 06:25:14,080 Epoch 3546: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 06:25:14,080 EPOCH 3547
2024-02-05 06:25:27,938 Epoch 3547: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 06:25:27,939 EPOCH 3548
2024-02-05 06:25:41,713 Epoch 3548: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 06:25:41,713 EPOCH 3549
2024-02-05 06:25:55,722 Epoch 3549: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 06:25:55,723 EPOCH 3550
2024-02-05 06:26:09,938 Epoch 3550: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 06:26:09,938 EPOCH 3551
2024-02-05 06:26:23,648 Epoch 3551: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 06:26:23,648 EPOCH 3552
2024-02-05 06:26:37,583 Epoch 3552: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 06:26:37,584 EPOCH 3553
2024-02-05 06:26:51,388 Epoch 3553: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 06:26:51,388 EPOCH 3554
2024-02-05 06:27:05,323 Epoch 3554: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 06:27:05,323 EPOCH 3555
2024-02-05 06:27:19,123 Epoch 3555: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 06:27:19,124 EPOCH 3556
2024-02-05 06:27:27,269 [Epoch: 3556 Step: 00032000] Batch Recognition Loss:   0.000149 => Gls Tokens per Sec:      786 || Batch Translation Loss:   0.016707 => Txt Tokens per Sec:     2228 || Lr: 0.000050
2024-02-05 06:27:55,903 Validation result at epoch 3556, step    32000: duration: 28.6335s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00021	Translation Loss: 98957.46094	PPL: 19983.82422
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.70	(BLEU-1: 11.03,	BLEU-2: 3.33,	BLEU-3: 1.34,	BLEU-4: 0.70)
	CHRF 17.26	ROUGE 9.41
2024-02-05 06:27:55,904 Logging Recognition and Translation Outputs
2024-02-05 06:27:55,905 ========================================================================================================================
2024-02-05 06:27:55,905 Logging Sequence: 86_11.00
2024-02-05 06:27:55,905 	Gloss Reference :	A B+C+D+E
2024-02-05 06:27:55,905 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 06:27:55,905 	Gloss Alignment :	         
2024-02-05 06:27:55,905 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 06:27:55,906 	Text Reference  :	he was ** ***** ********* *** **** 66 years     old   
2024-02-05 06:27:55,906 	Text Hypothesis :	** was to score extremely fit when a  statement saying
2024-02-05 06:27:55,906 	Text Alignment  :	D      I  I     I         I   I    S  S         S     
2024-02-05 06:27:55,906 ========================================================================================================================
2024-02-05 06:27:55,907 Logging Sequence: 67_16.00
2024-02-05 06:27:55,907 	Gloss Reference :	A B+C+D+E
2024-02-05 06:27:55,907 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 06:27:55,907 	Gloss Alignment :	         
2024-02-05 06:27:55,907 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 06:27:55,908 	Text Reference  :	**** * ***** ****** ****** to        help india's   fight    against the covid-19 pandemic
2024-02-05 06:27:55,908 	Text Hypothesis :	what a close friend andrew neophitou and  extremely saddened by      his his      life    
2024-02-05 06:27:55,908 	Text Alignment  :	I    I I     I      I      S         S    S         S        S       S   S        S       
2024-02-05 06:27:55,908 ========================================================================================================================
2024-02-05 06:27:55,908 Logging Sequence: 69_177.00
2024-02-05 06:27:55,909 	Gloss Reference :	A B+C+D+E
2024-02-05 06:27:55,909 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 06:27:55,909 	Gloss Alignment :	         
2024-02-05 06:27:55,909 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 06:27:55,911 	Text Reference  :	he said 'i will continue playing i       know    it's       about time   i       retire i   also    have   a   knee   condition
2024-02-05 06:27:55,911 	Text Hypothesis :	** **** ** **** ******** when    england started protesting was   played between mumbai and gujarat titans and punjab kings    
2024-02-05 06:27:55,911 	Text Alignment  :	D  D    D  D    D        S       S       S       S          S     S      S       S      S   S       S      S   S      S        
2024-02-05 06:27:55,911 ========================================================================================================================
2024-02-05 06:27:55,911 Logging Sequence: 165_615.00
2024-02-05 06:27:55,911 	Gloss Reference :	A B+C+D+E
2024-02-05 06:27:55,911 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 06:27:55,912 	Gloss Alignment :	         
2024-02-05 06:27:55,912 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 06:27:55,912 	Text Reference  :	** ** ** *********** ******* we defeated pakistan too  
2024-02-05 06:27:55,912 	Text Hypothesis :	it is an interesting history of india    and      japan
2024-02-05 06:27:55,912 	Text Alignment  :	I  I  I  I           I       S  S        S        S    
2024-02-05 06:27:55,912 ========================================================================================================================
2024-02-05 06:27:55,913 Logging Sequence: 61_5.00
2024-02-05 06:27:55,913 	Gloss Reference :	A B+C+D+E
2024-02-05 06:27:55,913 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 06:27:55,913 	Gloss Alignment :	         
2024-02-05 06:27:55,913 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 06:27:55,914 	Text Reference  :	******* *** they rivalry is   seen the ************** ***** ***** ** ****** *** most    during india pakistan cricket matches  
2024-02-05 06:27:55,915 	Text Hypothesis :	however you all  know    that at   the india-pakistan match would be played and excited to     see   the      fan     following
2024-02-05 06:27:55,915 	Text Alignment  :	I       I   S    S       S    S        I              I     I     I  I      I   S       S      S     S        S       S        
2024-02-05 06:27:55,915 ========================================================================================================================
2024-02-05 06:28:01,855 Epoch 3556: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 06:28:01,856 EPOCH 3557
2024-02-05 06:28:15,968 Epoch 3557: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 06:28:15,968 EPOCH 3558
2024-02-05 06:28:29,985 Epoch 3558: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 06:28:29,986 EPOCH 3559
2024-02-05 06:28:43,929 Epoch 3559: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 06:28:43,930 EPOCH 3560
2024-02-05 06:28:57,736 Epoch 3560: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 06:28:57,737 EPOCH 3561
2024-02-05 06:29:11,752 Epoch 3561: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 06:29:11,753 EPOCH 3562
2024-02-05 06:29:25,586 Epoch 3562: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 06:29:25,586 EPOCH 3563
2024-02-05 06:29:39,478 Epoch 3563: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 06:29:39,478 EPOCH 3564
2024-02-05 06:29:53,522 Epoch 3564: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 06:29:53,522 EPOCH 3565
2024-02-05 06:30:07,353 Epoch 3565: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 06:30:07,354 EPOCH 3566
2024-02-05 06:30:21,619 Epoch 3566: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 06:30:21,620 EPOCH 3567
2024-02-05 06:30:29,576 [Epoch: 3567 Step: 00032100] Batch Recognition Loss:   0.000111 => Gls Tokens per Sec:      854 || Batch Translation Loss:   0.011588 => Txt Tokens per Sec:     2234 || Lr: 0.000050
2024-02-05 06:30:35,599 Epoch 3567: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 06:30:35,599 EPOCH 3568
2024-02-05 06:30:49,793 Epoch 3568: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 06:30:49,793 EPOCH 3569
2024-02-05 06:31:03,945 Epoch 3569: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 06:31:03,945 EPOCH 3570
2024-02-05 06:31:17,750 Epoch 3570: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 06:31:17,750 EPOCH 3571
2024-02-05 06:31:31,862 Epoch 3571: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 06:31:31,863 EPOCH 3572
2024-02-05 06:31:45,972 Epoch 3572: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 06:31:45,973 EPOCH 3573
2024-02-05 06:32:00,064 Epoch 3573: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 06:32:00,065 EPOCH 3574
2024-02-05 06:32:14,004 Epoch 3574: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 06:32:14,005 EPOCH 3575
2024-02-05 06:32:27,763 Epoch 3575: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 06:32:27,764 EPOCH 3576
2024-02-05 06:32:41,903 Epoch 3576: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 06:32:41,904 EPOCH 3577
2024-02-05 06:32:55,915 Epoch 3577: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 06:32:55,916 EPOCH 3578
2024-02-05 06:33:04,138 [Epoch: 3578 Step: 00032200] Batch Recognition Loss:   0.000124 => Gls Tokens per Sec:      982 || Batch Translation Loss:   0.021357 => Txt Tokens per Sec:     2589 || Lr: 0.000050
2024-02-05 06:33:09,791 Epoch 3578: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 06:33:09,791 EPOCH 3579
2024-02-05 06:33:23,605 Epoch 3579: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 06:33:23,606 EPOCH 3580
2024-02-05 06:33:37,708 Epoch 3580: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 06:33:37,709 EPOCH 3581
2024-02-05 06:33:51,751 Epoch 3581: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 06:33:51,751 EPOCH 3582
2024-02-05 06:34:05,583 Epoch 3582: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 06:34:05,583 EPOCH 3583
2024-02-05 06:34:19,515 Epoch 3583: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 06:34:19,516 EPOCH 3584
2024-02-05 06:34:33,511 Epoch 3584: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 06:34:33,512 EPOCH 3585
2024-02-05 06:34:47,633 Epoch 3585: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 06:34:47,634 EPOCH 3586
2024-02-05 06:35:01,610 Epoch 3586: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 06:35:01,610 EPOCH 3587
2024-02-05 06:35:15,494 Epoch 3587: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 06:35:15,495 EPOCH 3588
2024-02-05 06:35:29,637 Epoch 3588: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 06:35:29,637 EPOCH 3589
2024-02-05 06:35:38,379 [Epoch: 3589 Step: 00032300] Batch Recognition Loss:   0.000101 => Gls Tokens per Sec:     1070 || Batch Translation Loss:   0.012616 => Txt Tokens per Sec:     2863 || Lr: 0.000050
2024-02-05 06:35:43,399 Epoch 3589: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 06:35:43,399 EPOCH 3590
2024-02-05 06:35:57,417 Epoch 3590: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 06:35:57,417 EPOCH 3591
2024-02-05 06:36:11,201 Epoch 3591: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 06:36:11,202 EPOCH 3592
2024-02-05 06:36:25,470 Epoch 3592: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 06:36:25,470 EPOCH 3593
2024-02-05 06:36:39,368 Epoch 3593: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 06:36:39,369 EPOCH 3594
2024-02-05 06:36:53,478 Epoch 3594: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 06:36:53,478 EPOCH 3595
2024-02-05 06:37:07,281 Epoch 3595: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 06:37:07,282 EPOCH 3596
2024-02-05 06:37:21,603 Epoch 3596: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 06:37:21,603 EPOCH 3597
2024-02-05 06:37:35,583 Epoch 3597: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 06:37:35,584 EPOCH 3598
2024-02-05 06:37:49,445 Epoch 3598: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 06:37:49,445 EPOCH 3599
2024-02-05 06:38:03,436 Epoch 3599: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 06:38:03,437 EPOCH 3600
2024-02-05 06:38:17,254 [Epoch: 3600 Step: 00032400] Batch Recognition Loss:   0.000105 => Gls Tokens per Sec:      769 || Batch Translation Loss:   0.014812 => Txt Tokens per Sec:     2136 || Lr: 0.000050
2024-02-05 06:38:17,255 Epoch 3600: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 06:38:17,255 EPOCH 3601
2024-02-05 06:38:31,245 Epoch 3601: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 06:38:31,245 EPOCH 3602
2024-02-05 06:38:45,129 Epoch 3602: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 06:38:45,129 EPOCH 3603
2024-02-05 06:38:58,960 Epoch 3603: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 06:38:58,960 EPOCH 3604
2024-02-05 06:39:12,875 Epoch 3604: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 06:39:12,876 EPOCH 3605
2024-02-05 06:39:26,984 Epoch 3605: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 06:39:26,984 EPOCH 3606
2024-02-05 06:39:40,918 Epoch 3606: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 06:39:40,919 EPOCH 3607
2024-02-05 06:39:54,855 Epoch 3607: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 06:39:54,856 EPOCH 3608
2024-02-05 06:40:08,692 Epoch 3608: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 06:40:08,693 EPOCH 3609
2024-02-05 06:40:22,508 Epoch 3609: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-05 06:40:22,509 EPOCH 3610
2024-02-05 06:40:36,015 Epoch 3610: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 06:40:36,016 EPOCH 3611
2024-02-05 06:40:49,858 Epoch 3611: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 06:40:49,858 EPOCH 3612
2024-02-05 06:40:50,574 [Epoch: 3612 Step: 00032500] Batch Recognition Loss:   0.000159 => Gls Tokens per Sec:     1790 || Batch Translation Loss:   0.042435 => Txt Tokens per Sec:     5417 || Lr: 0.000050
2024-02-05 06:41:03,819 Epoch 3612: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 06:41:03,820 EPOCH 3613
2024-02-05 06:41:17,903 Epoch 3613: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 06:41:17,903 EPOCH 3614
2024-02-05 06:41:31,925 Epoch 3614: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 06:41:31,926 EPOCH 3615
2024-02-05 06:41:46,081 Epoch 3615: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 06:41:46,082 EPOCH 3616
2024-02-05 06:41:59,791 Epoch 3616: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 06:41:59,792 EPOCH 3617
2024-02-05 06:42:13,596 Epoch 3617: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 06:42:13,597 EPOCH 3618
2024-02-05 06:42:27,566 Epoch 3618: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 06:42:27,566 EPOCH 3619
2024-02-05 06:42:41,518 Epoch 3619: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 06:42:41,518 EPOCH 3620
2024-02-05 06:42:55,438 Epoch 3620: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 06:42:55,439 EPOCH 3621
2024-02-05 06:43:09,388 Epoch 3621: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 06:43:09,388 EPOCH 3622
2024-02-05 06:43:23,358 Epoch 3622: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 06:43:23,359 EPOCH 3623
2024-02-05 06:43:24,325 [Epoch: 3623 Step: 00032600] Batch Recognition Loss:   0.000139 => Gls Tokens per Sec:     2652 || Batch Translation Loss:   0.028192 => Txt Tokens per Sec:     7078 || Lr: 0.000050
2024-02-05 06:43:37,281 Epoch 3623: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 06:43:37,281 EPOCH 3624
2024-02-05 06:43:51,154 Epoch 3624: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 06:43:51,155 EPOCH 3625
2024-02-05 06:44:05,452 Epoch 3625: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 06:44:05,452 EPOCH 3626
2024-02-05 06:44:19,787 Epoch 3626: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 06:44:19,787 EPOCH 3627
2024-02-05 06:44:33,612 Epoch 3627: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 06:44:33,613 EPOCH 3628
2024-02-05 06:44:47,507 Epoch 3628: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 06:44:47,507 EPOCH 3629
2024-02-05 06:45:01,617 Epoch 3629: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 06:45:01,617 EPOCH 3630
2024-02-05 06:45:15,518 Epoch 3630: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 06:45:15,518 EPOCH 3631
2024-02-05 06:45:29,289 Epoch 3631: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-05 06:45:29,290 EPOCH 3632
2024-02-05 06:45:43,240 Epoch 3632: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 06:45:43,241 EPOCH 3633
2024-02-05 06:45:57,141 Epoch 3633: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 06:45:57,141 EPOCH 3634
2024-02-05 06:46:03,179 [Epoch: 3634 Step: 00032700] Batch Recognition Loss:   0.000123 => Gls Tokens per Sec:      636 || Batch Translation Loss:   0.013203 => Txt Tokens per Sec:     1737 || Lr: 0.000050
2024-02-05 06:46:11,180 Epoch 3634: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 06:46:11,181 EPOCH 3635
2024-02-05 06:46:25,235 Epoch 3635: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 06:46:25,236 EPOCH 3636
2024-02-05 06:46:39,056 Epoch 3636: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 06:46:39,057 EPOCH 3637
2024-02-05 06:46:53,263 Epoch 3637: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 06:46:53,264 EPOCH 3638
2024-02-05 06:47:07,129 Epoch 3638: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 06:47:07,130 EPOCH 3639
2024-02-05 06:47:21,238 Epoch 3639: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 06:47:21,239 EPOCH 3640
2024-02-05 06:47:35,347 Epoch 3640: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 06:47:35,348 EPOCH 3641
2024-02-05 06:47:49,330 Epoch 3641: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 06:47:49,331 EPOCH 3642
2024-02-05 06:48:03,294 Epoch 3642: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 06:48:03,295 EPOCH 3643
2024-02-05 06:48:17,317 Epoch 3643: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 06:48:17,317 EPOCH 3644
2024-02-05 06:48:31,359 Epoch 3644: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 06:48:31,360 EPOCH 3645
2024-02-05 06:48:38,010 [Epoch: 3645 Step: 00032800] Batch Recognition Loss:   0.000173 => Gls Tokens per Sec:      636 || Batch Translation Loss:   0.023962 => Txt Tokens per Sec:     1829 || Lr: 0.000050
2024-02-05 06:48:44,970 Epoch 3645: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 06:48:44,970 EPOCH 3646
2024-02-05 06:48:58,923 Epoch 3646: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 06:48:58,923 EPOCH 3647
2024-02-05 06:49:12,523 Epoch 3647: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 06:49:12,524 EPOCH 3648
2024-02-05 06:49:26,167 Epoch 3648: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 06:49:26,167 EPOCH 3649
2024-02-05 06:49:40,714 Epoch 3649: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 06:49:40,714 EPOCH 3650
2024-02-05 06:49:54,755 Epoch 3650: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 06:49:54,756 EPOCH 3651
2024-02-05 06:50:08,461 Epoch 3651: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 06:50:08,462 EPOCH 3652
2024-02-05 06:50:22,556 Epoch 3652: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 06:50:22,556 EPOCH 3653
2024-02-05 06:50:36,627 Epoch 3653: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 06:50:36,628 EPOCH 3654
2024-02-05 06:50:50,895 Epoch 3654: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 06:50:50,895 EPOCH 3655
2024-02-05 06:51:05,086 Epoch 3655: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 06:51:05,086 EPOCH 3656
2024-02-05 06:51:12,089 [Epoch: 3656 Step: 00032900] Batch Recognition Loss:   0.000146 => Gls Tokens per Sec:      787 || Batch Translation Loss:   0.017969 => Txt Tokens per Sec:     1986 || Lr: 0.000050
2024-02-05 06:51:19,103 Epoch 3656: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 06:51:19,103 EPOCH 3657
2024-02-05 06:51:32,959 Epoch 3657: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 06:51:32,960 EPOCH 3658
2024-02-05 06:51:46,648 Epoch 3658: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 06:51:46,649 EPOCH 3659
2024-02-05 06:52:00,549 Epoch 3659: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 06:52:00,550 EPOCH 3660
2024-02-05 06:52:14,423 Epoch 3660: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 06:52:14,424 EPOCH 3661
2024-02-05 06:52:28,278 Epoch 3661: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 06:52:28,278 EPOCH 3662
2024-02-05 06:52:42,252 Epoch 3662: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.90 
2024-02-05 06:52:42,253 EPOCH 3663
2024-02-05 06:52:55,979 Epoch 3663: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.76 
2024-02-05 06:52:55,979 EPOCH 3664
2024-02-05 06:53:10,180 Epoch 3664: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-05 06:53:10,181 EPOCH 3665
2024-02-05 06:53:24,168 Epoch 3665: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-05 06:53:24,168 EPOCH 3666
2024-02-05 06:53:37,960 Epoch 3666: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-05 06:53:37,961 EPOCH 3667
2024-02-05 06:53:44,837 [Epoch: 3667 Step: 00033000] Batch Recognition Loss:   0.000183 => Gls Tokens per Sec:     1117 || Batch Translation Loss:   0.016013 => Txt Tokens per Sec:     2994 || Lr: 0.000050
2024-02-05 06:53:51,462 Epoch 3667: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-05 06:53:51,462 EPOCH 3668
2024-02-05 06:54:05,356 Epoch 3668: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 06:54:05,357 EPOCH 3669
2024-02-05 06:54:19,241 Epoch 3669: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 06:54:19,241 EPOCH 3670
2024-02-05 06:54:33,095 Epoch 3670: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 06:54:33,095 EPOCH 3671
2024-02-05 06:54:47,046 Epoch 3671: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 06:54:47,047 EPOCH 3672
2024-02-05 06:55:00,992 Epoch 3672: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 06:55:00,993 EPOCH 3673
2024-02-05 06:55:14,741 Epoch 3673: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 06:55:14,741 EPOCH 3674
2024-02-05 06:55:28,652 Epoch 3674: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 06:55:28,652 EPOCH 3675
2024-02-05 06:55:42,820 Epoch 3675: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 06:55:42,821 EPOCH 3676
2024-02-05 06:55:56,719 Epoch 3676: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 06:55:56,719 EPOCH 3677
2024-02-05 06:56:10,701 Epoch 3677: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 06:56:10,702 EPOCH 3678
2024-02-05 06:56:22,514 [Epoch: 3678 Step: 00033100] Batch Recognition Loss:   0.000112 => Gls Tokens per Sec:      683 || Batch Translation Loss:   0.015118 => Txt Tokens per Sec:     1962 || Lr: 0.000050
2024-02-05 06:56:24,495 Epoch 3678: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 06:56:24,496 EPOCH 3679
2024-02-05 06:56:38,486 Epoch 3679: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 06:56:38,487 EPOCH 3680
2024-02-05 06:56:52,398 Epoch 3680: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 06:56:52,399 EPOCH 3681
2024-02-05 06:57:06,298 Epoch 3681: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 06:57:06,299 EPOCH 3682
2024-02-05 06:57:20,224 Epoch 3682: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 06:57:20,224 EPOCH 3683
2024-02-05 06:57:34,292 Epoch 3683: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 06:57:34,293 EPOCH 3684
2024-02-05 06:57:48,077 Epoch 3684: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 06:57:48,078 EPOCH 3685
2024-02-05 06:58:01,786 Epoch 3685: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 06:58:01,787 EPOCH 3686
2024-02-05 06:58:15,688 Epoch 3686: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 06:58:15,688 EPOCH 3687
2024-02-05 06:58:29,628 Epoch 3687: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 06:58:29,628 EPOCH 3688
2024-02-05 06:58:43,364 Epoch 3688: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 06:58:43,364 EPOCH 3689
2024-02-05 06:58:53,072 [Epoch: 3689 Step: 00033200] Batch Recognition Loss:   0.000115 => Gls Tokens per Sec:     1055 || Batch Translation Loss:   0.008193 => Txt Tokens per Sec:     2897 || Lr: 0.000050
2024-02-05 06:58:57,378 Epoch 3689: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 06:58:57,378 EPOCH 3690
2024-02-05 06:59:11,191 Epoch 3690: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 06:59:11,192 EPOCH 3691
2024-02-05 06:59:25,178 Epoch 3691: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 06:59:25,178 EPOCH 3692
2024-02-05 06:59:39,312 Epoch 3692: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 06:59:39,313 EPOCH 3693
2024-02-05 06:59:53,437 Epoch 3693: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 06:59:53,438 EPOCH 3694
2024-02-05 07:00:07,454 Epoch 3694: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 07:00:07,455 EPOCH 3695
2024-02-05 07:00:21,586 Epoch 3695: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 07:00:21,587 EPOCH 3696
2024-02-05 07:00:35,517 Epoch 3696: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 07:00:35,518 EPOCH 3697
2024-02-05 07:00:49,400 Epoch 3697: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 07:00:49,401 EPOCH 3698
2024-02-05 07:01:03,324 Epoch 3698: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 07:01:03,324 EPOCH 3699
2024-02-05 07:01:17,367 Epoch 3699: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 07:01:17,367 EPOCH 3700
2024-02-05 07:01:31,193 [Epoch: 3700 Step: 00033300] Batch Recognition Loss:   0.000104 => Gls Tokens per Sec:      769 || Batch Translation Loss:   0.011446 => Txt Tokens per Sec:     2135 || Lr: 0.000050
2024-02-05 07:01:31,194 Epoch 3700: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 07:01:31,194 EPOCH 3701
2024-02-05 07:01:45,146 Epoch 3701: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 07:01:45,147 EPOCH 3702
2024-02-05 07:01:59,118 Epoch 3702: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 07:01:59,119 EPOCH 3703
2024-02-05 07:02:12,939 Epoch 3703: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 07:02:12,939 EPOCH 3704
2024-02-05 07:02:27,272 Epoch 3704: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 07:02:27,273 EPOCH 3705
2024-02-05 07:02:41,274 Epoch 3705: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 07:02:41,275 EPOCH 3706
2024-02-05 07:02:55,349 Epoch 3706: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 07:02:55,350 EPOCH 3707
2024-02-05 07:03:09,306 Epoch 3707: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 07:03:09,307 EPOCH 3708
2024-02-05 07:03:23,077 Epoch 3708: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 07:03:23,077 EPOCH 3709
2024-02-05 07:03:36,896 Epoch 3709: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 07:03:36,896 EPOCH 3710
2024-02-05 07:03:50,876 Epoch 3710: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 07:03:50,876 EPOCH 3711
2024-02-05 07:04:04,778 Epoch 3711: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 07:04:04,779 EPOCH 3712
2024-02-05 07:04:05,375 [Epoch: 3712 Step: 00033400] Batch Recognition Loss:   0.000117 => Gls Tokens per Sec:     2155 || Batch Translation Loss:   0.013985 => Txt Tokens per Sec:     6283 || Lr: 0.000050
2024-02-05 07:04:18,787 Epoch 3712: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 07:04:18,788 EPOCH 3713
2024-02-05 07:04:32,808 Epoch 3713: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 07:04:32,809 EPOCH 3714
2024-02-05 07:04:46,797 Epoch 3714: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 07:04:46,798 EPOCH 3715
2024-02-05 07:05:00,702 Epoch 3715: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 07:05:00,702 EPOCH 3716
2024-02-05 07:05:14,957 Epoch 3716: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 07:05:14,958 EPOCH 3717
2024-02-05 07:05:29,030 Epoch 3717: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 07:05:29,030 EPOCH 3718
2024-02-05 07:05:42,738 Epoch 3718: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 07:05:42,739 EPOCH 3719
2024-02-05 07:05:56,657 Epoch 3719: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 07:05:56,658 EPOCH 3720
2024-02-05 07:06:10,795 Epoch 3720: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 07:06:10,796 EPOCH 3721
2024-02-05 07:06:24,815 Epoch 3721: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 07:06:24,816 EPOCH 3722
2024-02-05 07:06:38,480 Epoch 3722: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 07:06:38,480 EPOCH 3723
2024-02-05 07:06:43,158 [Epoch: 3723 Step: 00033500] Batch Recognition Loss:   0.000140 => Gls Tokens per Sec:      357 || Batch Translation Loss:   0.007756 => Txt Tokens per Sec:     1052 || Lr: 0.000050
2024-02-05 07:06:52,566 Epoch 3723: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 07:06:52,567 EPOCH 3724
2024-02-05 07:07:06,470 Epoch 3724: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 07:07:06,470 EPOCH 3725
2024-02-05 07:07:20,619 Epoch 3725: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 07:07:20,619 EPOCH 3726
2024-02-05 07:07:34,483 Epoch 3726: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 07:07:34,483 EPOCH 3727
2024-02-05 07:07:48,748 Epoch 3727: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 07:07:48,748 EPOCH 3728
2024-02-05 07:08:02,723 Epoch 3728: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 07:08:02,723 EPOCH 3729
2024-02-05 07:08:16,933 Epoch 3729: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 07:08:16,933 EPOCH 3730
2024-02-05 07:08:30,784 Epoch 3730: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 07:08:30,784 EPOCH 3731
2024-02-05 07:08:44,762 Epoch 3731: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-05 07:08:44,762 EPOCH 3732
2024-02-05 07:08:58,222 Epoch 3732: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-05 07:08:58,223 EPOCH 3733
2024-02-05 07:09:12,208 Epoch 3733: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.67 
2024-02-05 07:09:12,209 EPOCH 3734
2024-02-05 07:09:13,315 [Epoch: 3734 Step: 00033600] Batch Recognition Loss:   0.000178 => Gls Tokens per Sec:     3472 || Batch Translation Loss:   0.044099 => Txt Tokens per Sec:     8082 || Lr: 0.000050
2024-02-05 07:09:26,044 Epoch 3734: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.84 
2024-02-05 07:09:26,045 EPOCH 3735
2024-02-05 07:09:40,115 Epoch 3735: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.81 
2024-02-05 07:09:40,115 EPOCH 3736
2024-02-05 07:09:54,051 Epoch 3736: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.64 
2024-02-05 07:09:54,051 EPOCH 3737
2024-02-05 07:10:08,124 Epoch 3737: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.93 
2024-02-05 07:10:08,125 EPOCH 3738
2024-02-05 07:10:22,056 Epoch 3738: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-05 07:10:22,057 EPOCH 3739
2024-02-05 07:10:36,269 Epoch 3739: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-05 07:10:36,270 EPOCH 3740
2024-02-05 07:10:49,977 Epoch 3740: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-05 07:10:49,978 EPOCH 3741
2024-02-05 07:11:03,821 Epoch 3741: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-05 07:11:03,822 EPOCH 3742
2024-02-05 07:11:17,588 Epoch 3742: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 07:11:17,589 EPOCH 3743
2024-02-05 07:11:31,689 Epoch 3743: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 07:11:31,690 EPOCH 3744
2024-02-05 07:11:45,386 Epoch 3744: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 07:11:45,387 EPOCH 3745
2024-02-05 07:11:52,340 [Epoch: 3745 Step: 00033700] Batch Recognition Loss:   0.000170 => Gls Tokens per Sec:      608 || Batch Translation Loss:   0.017259 => Txt Tokens per Sec:     1624 || Lr: 0.000050
2024-02-05 07:11:59,734 Epoch 3745: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 07:11:59,734 EPOCH 3746
2024-02-05 07:12:13,658 Epoch 3746: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 07:12:13,659 EPOCH 3747
2024-02-05 07:12:27,359 Epoch 3747: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 07:12:27,360 EPOCH 3748
2024-02-05 07:12:41,535 Epoch 3748: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 07:12:41,536 EPOCH 3749
2024-02-05 07:12:55,568 Epoch 3749: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 07:12:55,568 EPOCH 3750
2024-02-05 07:13:09,468 Epoch 3750: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 07:13:09,469 EPOCH 3751
2024-02-05 07:13:23,292 Epoch 3751: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 07:13:23,292 EPOCH 3752
2024-02-05 07:13:37,329 Epoch 3752: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 07:13:37,330 EPOCH 3753
2024-02-05 07:13:51,181 Epoch 3753: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 07:13:51,182 EPOCH 3754
2024-02-05 07:14:05,119 Epoch 3754: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 07:14:05,119 EPOCH 3755
2024-02-05 07:14:18,753 Epoch 3755: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 07:14:18,754 EPOCH 3756
2024-02-05 07:14:31,113 [Epoch: 3756 Step: 00033800] Batch Recognition Loss:   0.000186 => Gls Tokens per Sec:      446 || Batch Translation Loss:   0.009141 => Txt Tokens per Sec:     1407 || Lr: 0.000050
2024-02-05 07:14:32,717 Epoch 3756: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 07:14:32,717 EPOCH 3757
2024-02-05 07:14:46,655 Epoch 3757: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 07:14:46,656 EPOCH 3758
2024-02-05 07:15:00,643 Epoch 3758: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 07:15:00,643 EPOCH 3759
2024-02-05 07:15:14,365 Epoch 3759: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 07:15:14,366 EPOCH 3760
2024-02-05 07:15:27,776 Epoch 3760: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 07:15:27,776 EPOCH 3761
2024-02-05 07:15:41,886 Epoch 3761: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 07:15:41,887 EPOCH 3762
2024-02-05 07:15:55,555 Epoch 3762: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 07:15:55,555 EPOCH 3763
2024-02-05 07:16:09,743 Epoch 3763: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 07:16:09,743 EPOCH 3764
2024-02-05 07:16:23,544 Epoch 3764: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 07:16:23,544 EPOCH 3765
2024-02-05 07:16:37,343 Epoch 3765: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 07:16:37,344 EPOCH 3766
2024-02-05 07:16:50,977 Epoch 3766: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 07:16:50,977 EPOCH 3767
2024-02-05 07:17:02,112 [Epoch: 3767 Step: 00033900] Batch Recognition Loss:   0.000116 => Gls Tokens per Sec:      610 || Batch Translation Loss:   0.011738 => Txt Tokens per Sec:     1618 || Lr: 0.000050
2024-02-05 07:17:05,056 Epoch 3767: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 07:17:05,056 EPOCH 3768
2024-02-05 07:17:19,050 Epoch 3768: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 07:17:19,050 EPOCH 3769
2024-02-05 07:17:32,794 Epoch 3769: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 07:17:32,795 EPOCH 3770
2024-02-05 07:17:46,732 Epoch 3770: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 07:17:46,732 EPOCH 3771
2024-02-05 07:18:00,597 Epoch 3771: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 07:18:00,598 EPOCH 3772
2024-02-05 07:18:14,372 Epoch 3772: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 07:18:14,373 EPOCH 3773
2024-02-05 07:18:28,094 Epoch 3773: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 07:18:28,094 EPOCH 3774
2024-02-05 07:18:42,099 Epoch 3774: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 07:18:42,100 EPOCH 3775
2024-02-05 07:18:56,272 Epoch 3775: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 07:18:56,273 EPOCH 3776
2024-02-05 07:19:10,149 Epoch 3776: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 07:19:10,150 EPOCH 3777
2024-02-05 07:19:23,978 Epoch 3777: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 07:19:23,979 EPOCH 3778
2024-02-05 07:19:36,052 [Epoch: 3778 Step: 00034000] Batch Recognition Loss:   0.000127 => Gls Tokens per Sec:      669 || Batch Translation Loss:   0.014797 => Txt Tokens per Sec:     1861 || Lr: 0.000050
2024-02-05 07:20:05,077 Validation result at epoch 3778, step    34000: duration: 29.0251s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00023	Translation Loss: 100281.07031	PPL: 22814.03906
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.66	(BLEU-1: 10.52,	BLEU-2: 3.10,	BLEU-3: 1.30,	BLEU-4: 0.66)
	CHRF 16.96	ROUGE 9.01
2024-02-05 07:20:05,079 Logging Recognition and Translation Outputs
2024-02-05 07:20:05,079 ========================================================================================================================
2024-02-05 07:20:05,079 Logging Sequence: 92_199.00
2024-02-05 07:20:05,079 	Gloss Reference :	A B+C+D+E
2024-02-05 07:20:05,079 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 07:20:05,080 	Gloss Alignment :	         
2024-02-05 07:20:05,080 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 07:20:05,080 	Text Reference  :	*** people on     social media  said that  
2024-02-05 07:20:05,080 	Text Hypothesis :	and the    police still  played very sushil
2024-02-05 07:20:05,080 	Text Alignment  :	I   S      S      S      S      S    S     
2024-02-05 07:20:05,081 ========================================================================================================================
2024-02-05 07:20:05,081 Logging Sequence: 109_64.00
2024-02-05 07:20:05,081 	Gloss Reference :	A B+C+D+E
2024-02-05 07:20:05,081 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 07:20:05,081 	Gloss Alignment :	         
2024-02-05 07:20:05,081 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 07:20:05,082 	Text Reference  :	the 2 players as      well       as the entire kkr      team have   been quarantined
2024-02-05 07:20:05,082 	Text Hypothesis :	*** * to      discuss management of the ****** upcoming team physio the  fir        
2024-02-05 07:20:05,082 	Text Alignment  :	D   D S       S       S          S      D      S             S      S    S          
2024-02-05 07:20:05,083 ========================================================================================================================
2024-02-05 07:20:05,083 Logging Sequence: 84_108.00
2024-02-05 07:20:05,083 	Gloss Reference :	A B+C+D+E
2024-02-05 07:20:05,083 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 07:20:05,083 	Gloss Alignment :	         
2024-02-05 07:20:05,083 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 07:20:05,085 	Text Reference  :	so in order to show their protest they covered  their     mouth   in the    photos which then   went     viral
2024-02-05 07:20:05,085 	Text Hypothesis :	** ** ***** ** **** ***** ******* the  incident triggered outrage on social media  with  people pointing out  
2024-02-05 07:20:05,085 	Text Alignment  :	D  D  D     D  D    D     D       S    S        S         S       S  S      S      S     S      S        S    
2024-02-05 07:20:05,085 ========================================================================================================================
2024-02-05 07:20:05,085 Logging Sequence: 115_24.00
2024-02-05 07:20:05,085 	Gloss Reference :	A B+C+D+E
2024-02-05 07:20:05,085 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 07:20:05,086 	Gloss Alignment :	         
2024-02-05 07:20:05,086 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 07:20:05,086 	Text Reference  :	bumrah also did not participate in  the  5        match t20        series
2024-02-05 07:20:05,087 	Text Hypothesis :	****** **** *** *** loss        are many negative and   merseyside police
2024-02-05 07:20:05,087 	Text Alignment  :	D      D    D   D   S           S   S    S        S     S          S     
2024-02-05 07:20:05,087 ========================================================================================================================
2024-02-05 07:20:05,087 Logging Sequence: 96_129.00
2024-02-05 07:20:05,087 	Gloss Reference :	A B+C+D+E
2024-02-05 07:20:05,087 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 07:20:05,088 	Gloss Alignment :	         
2024-02-05 07:20:05,088 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 07:20:05,088 	Text Reference  :	***** *** ********* viewers were very stressed
2024-02-05 07:20:05,088 	Text Hypothesis :	while the remaining can     play a    wicket  
2024-02-05 07:20:05,088 	Text Alignment  :	I     I   I         S       S    S    S       
2024-02-05 07:20:05,088 ========================================================================================================================
2024-02-05 07:20:07,480 Epoch 3778: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 07:20:07,480 EPOCH 3779
2024-02-05 07:20:21,344 Epoch 3779: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 07:20:21,344 EPOCH 3780
2024-02-05 07:20:35,164 Epoch 3780: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 07:20:35,165 EPOCH 3781
2024-02-05 07:20:49,208 Epoch 3781: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 07:20:49,208 EPOCH 3782
2024-02-05 07:21:03,438 Epoch 3782: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 07:21:03,439 EPOCH 3783
2024-02-05 07:21:17,334 Epoch 3783: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 07:21:17,335 EPOCH 3784
2024-02-05 07:21:31,469 Epoch 3784: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 07:21:31,470 EPOCH 3785
2024-02-05 07:21:45,385 Epoch 3785: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 07:21:45,386 EPOCH 3786
2024-02-05 07:21:59,152 Epoch 3786: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 07:21:59,152 EPOCH 3787
2024-02-05 07:22:13,051 Epoch 3787: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 07:22:13,052 EPOCH 3788
2024-02-05 07:22:26,983 Epoch 3788: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 07:22:26,983 EPOCH 3789
2024-02-05 07:22:39,499 [Epoch: 3789 Step: 00034100] Batch Recognition Loss:   0.000110 => Gls Tokens per Sec:      747 || Batch Translation Loss:   0.013690 => Txt Tokens per Sec:     2031 || Lr: 0.000050
2024-02-05 07:22:41,278 Epoch 3789: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 07:22:41,278 EPOCH 3790
2024-02-05 07:22:55,176 Epoch 3790: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 07:22:55,177 EPOCH 3791
2024-02-05 07:23:09,134 Epoch 3791: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 07:23:09,134 EPOCH 3792
2024-02-05 07:23:23,083 Epoch 3792: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 07:23:23,084 EPOCH 3793
2024-02-05 07:23:36,713 Epoch 3793: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 07:23:36,713 EPOCH 3794
2024-02-05 07:23:50,926 Epoch 3794: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 07:23:50,926 EPOCH 3795
2024-02-05 07:24:04,802 Epoch 3795: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 07:24:04,803 EPOCH 3796
2024-02-05 07:24:18,814 Epoch 3796: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 07:24:18,815 EPOCH 3797
2024-02-05 07:24:32,560 Epoch 3797: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 07:24:32,561 EPOCH 3798
2024-02-05 07:24:46,686 Epoch 3798: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 07:24:46,686 EPOCH 3799
2024-02-05 07:25:00,245 Epoch 3799: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 07:25:00,245 EPOCH 3800
2024-02-05 07:25:13,969 [Epoch: 3800 Step: 00034200] Batch Recognition Loss:   0.000101 => Gls Tokens per Sec:      775 || Batch Translation Loss:   0.014129 => Txt Tokens per Sec:     2150 || Lr: 0.000050
2024-02-05 07:25:13,970 Epoch 3800: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 07:25:13,970 EPOCH 3801
2024-02-05 07:25:27,566 Epoch 3801: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 07:25:27,567 EPOCH 3802
2024-02-05 07:25:41,516 Epoch 3802: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 07:25:41,516 EPOCH 3803
2024-02-05 07:25:55,602 Epoch 3803: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 07:25:55,603 EPOCH 3804
2024-02-05 07:26:09,463 Epoch 3804: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 07:26:09,464 EPOCH 3805
2024-02-05 07:26:23,539 Epoch 3805: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 07:26:23,539 EPOCH 3806
2024-02-05 07:26:37,413 Epoch 3806: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 07:26:37,414 EPOCH 3807
2024-02-05 07:26:51,198 Epoch 3807: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 07:26:51,199 EPOCH 3808
2024-02-05 07:27:05,056 Epoch 3808: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 07:27:05,057 EPOCH 3809
2024-02-05 07:27:19,303 Epoch 3809: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 07:27:19,303 EPOCH 3810
2024-02-05 07:27:33,297 Epoch 3810: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 07:27:33,298 EPOCH 3811
2024-02-05 07:27:46,953 Epoch 3811: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 07:27:46,954 EPOCH 3812
2024-02-05 07:27:52,113 [Epoch: 3812 Step: 00034300] Batch Recognition Loss:   0.000131 => Gls Tokens per Sec:      248 || Batch Translation Loss:   0.030670 => Txt Tokens per Sec:      869 || Lr: 0.000050
2024-02-05 07:28:00,997 Epoch 3812: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 07:28:00,997 EPOCH 3813
2024-02-05 07:28:14,742 Epoch 3813: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 07:28:14,743 EPOCH 3814
2024-02-05 07:28:28,632 Epoch 3814: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 07:28:28,632 EPOCH 3815
2024-02-05 07:28:42,564 Epoch 3815: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 07:28:42,565 EPOCH 3816
2024-02-05 07:28:56,347 Epoch 3816: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 07:28:56,348 EPOCH 3817
2024-02-05 07:29:10,388 Epoch 3817: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 07:29:10,389 EPOCH 3818
2024-02-05 07:29:24,603 Epoch 3818: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 07:29:24,604 EPOCH 3819
2024-02-05 07:29:38,398 Epoch 3819: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-05 07:29:38,399 EPOCH 3820
2024-02-05 07:29:52,325 Epoch 3820: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-05 07:29:52,326 EPOCH 3821
2024-02-05 07:30:06,266 Epoch 3821: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-05 07:30:06,267 EPOCH 3822
2024-02-05 07:30:20,181 Epoch 3822: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-05 07:30:20,181 EPOCH 3823
2024-02-05 07:30:21,089 [Epoch: 3823 Step: 00034400] Batch Recognition Loss:   0.000231 => Gls Tokens per Sec:     2822 || Batch Translation Loss:   0.067575 => Txt Tokens per Sec:     7461 || Lr: 0.000050
2024-02-05 07:30:33,974 Epoch 3823: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-05 07:30:33,975 EPOCH 3824
2024-02-05 07:30:47,710 Epoch 3824: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.62 
2024-02-05 07:30:47,710 EPOCH 3825
2024-02-05 07:31:01,913 Epoch 3825: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-05 07:31:01,913 EPOCH 3826
2024-02-05 07:31:15,966 Epoch 3826: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-05 07:31:15,967 EPOCH 3827
2024-02-05 07:31:30,067 Epoch 3827: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-05 07:31:30,068 EPOCH 3828
2024-02-05 07:31:43,752 Epoch 3828: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-05 07:31:43,753 EPOCH 3829
2024-02-05 07:31:57,584 Epoch 3829: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 07:31:57,584 EPOCH 3830
2024-02-05 07:32:11,636 Epoch 3830: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 07:32:11,637 EPOCH 3831
2024-02-05 07:32:25,303 Epoch 3831: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 07:32:25,304 EPOCH 3832
2024-02-05 07:32:39,360 Epoch 3832: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 07:32:39,361 EPOCH 3833
2024-02-05 07:32:53,356 Epoch 3833: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 07:32:53,356 EPOCH 3834
2024-02-05 07:32:58,558 [Epoch: 3834 Step: 00034500] Batch Recognition Loss:   0.000109 => Gls Tokens per Sec:      567 || Batch Translation Loss:   0.014460 => Txt Tokens per Sec:     1573 || Lr: 0.000050
2024-02-05 07:33:06,974 Epoch 3834: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 07:33:06,974 EPOCH 3835
2024-02-05 07:33:20,695 Epoch 3835: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 07:33:20,696 EPOCH 3836
2024-02-05 07:33:34,608 Epoch 3836: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 07:33:34,609 EPOCH 3837
2024-02-05 07:33:48,423 Epoch 3837: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 07:33:48,424 EPOCH 3838
2024-02-05 07:34:02,461 Epoch 3838: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 07:34:02,462 EPOCH 3839
2024-02-05 07:34:16,374 Epoch 3839: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 07:34:16,375 EPOCH 3840
2024-02-05 07:34:30,193 Epoch 3840: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 07:34:30,194 EPOCH 3841
2024-02-05 07:34:44,477 Epoch 3841: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 07:34:44,477 EPOCH 3842
2024-02-05 07:34:58,562 Epoch 3842: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 07:34:58,562 EPOCH 3843
2024-02-05 07:35:12,609 Epoch 3843: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 07:35:12,610 EPOCH 3844
2024-02-05 07:35:26,612 Epoch 3844: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 07:35:26,612 EPOCH 3845
2024-02-05 07:35:29,648 [Epoch: 3845 Step: 00034600] Batch Recognition Loss:   0.000146 => Gls Tokens per Sec:     1687 || Batch Translation Loss:   0.021081 => Txt Tokens per Sec:     4309 || Lr: 0.000050
2024-02-05 07:35:40,557 Epoch 3845: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 07:35:40,558 EPOCH 3846
2024-02-05 07:35:54,492 Epoch 3846: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 07:35:54,493 EPOCH 3847
2024-02-05 07:36:08,258 Epoch 3847: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 07:36:08,259 EPOCH 3848
2024-02-05 07:36:22,588 Epoch 3848: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 07:36:22,588 EPOCH 3849
2024-02-05 07:36:36,543 Epoch 3849: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 07:36:36,544 EPOCH 3850
2024-02-05 07:36:50,127 Epoch 3850: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 07:36:50,127 EPOCH 3851
2024-02-05 07:37:03,751 Epoch 3851: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 07:37:03,751 EPOCH 3852
2024-02-05 07:37:17,296 Epoch 3852: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 07:37:17,297 EPOCH 3853
2024-02-05 07:37:31,429 Epoch 3853: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 07:37:31,429 EPOCH 3854
2024-02-05 07:37:45,385 Epoch 3854: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 07:37:45,386 EPOCH 3855
2024-02-05 07:37:59,253 Epoch 3855: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 07:37:59,254 EPOCH 3856
2024-02-05 07:38:06,705 [Epoch: 3856 Step: 00034700] Batch Recognition Loss:   0.000161 => Gls Tokens per Sec:      740 || Batch Translation Loss:   0.007279 => Txt Tokens per Sec:     2127 || Lr: 0.000050
2024-02-05 07:38:13,277 Epoch 3856: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 07:38:13,277 EPOCH 3857
2024-02-05 07:38:27,129 Epoch 3857: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 07:38:27,129 EPOCH 3858
2024-02-05 07:38:41,073 Epoch 3858: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 07:38:41,074 EPOCH 3859
2024-02-05 07:38:55,117 Epoch 3859: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 07:38:55,117 EPOCH 3860
2024-02-05 07:39:09,050 Epoch 3860: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 07:39:09,051 EPOCH 3861
2024-02-05 07:39:23,123 Epoch 3861: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 07:39:23,124 EPOCH 3862
2024-02-05 07:39:36,898 Epoch 3862: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 07:39:36,899 EPOCH 3863
2024-02-05 07:39:50,734 Epoch 3863: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 07:39:50,734 EPOCH 3864
2024-02-05 07:40:03,817 Epoch 3864: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 07:40:03,818 EPOCH 3865
2024-02-05 07:40:17,570 Epoch 3865: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 07:40:17,571 EPOCH 3866
2024-02-05 07:40:31,954 Epoch 3866: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 07:40:31,954 EPOCH 3867
2024-02-05 07:40:44,345 [Epoch: 3867 Step: 00034800] Batch Recognition Loss:   0.000128 => Gls Tokens per Sec:      548 || Batch Translation Loss:   0.007728 => Txt Tokens per Sec:     1607 || Lr: 0.000050
2024-02-05 07:40:45,715 Epoch 3867: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 07:40:45,716 EPOCH 3868
2024-02-05 07:40:59,540 Epoch 3868: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 07:40:59,541 EPOCH 3869
2024-02-05 07:41:13,151 Epoch 3869: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 07:41:13,152 EPOCH 3870
2024-02-05 07:41:27,088 Epoch 3870: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 07:41:27,089 EPOCH 3871
2024-02-05 07:41:41,068 Epoch 3871: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-05 07:41:41,068 EPOCH 3872
2024-02-05 07:41:54,930 Epoch 3872: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 07:41:54,930 EPOCH 3873
2024-02-05 07:42:09,006 Epoch 3873: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-05 07:42:09,006 EPOCH 3874
2024-02-05 07:42:22,766 Epoch 3874: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 07:42:22,767 EPOCH 3875
2024-02-05 07:42:36,405 Epoch 3875: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 07:42:36,406 EPOCH 3876
2024-02-05 07:42:50,588 Epoch 3876: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 07:42:50,589 EPOCH 3877
2024-02-05 07:43:04,571 Epoch 3877: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 07:43:04,571 EPOCH 3878
2024-02-05 07:43:12,933 [Epoch: 3878 Step: 00034900] Batch Recognition Loss:   0.000203 => Gls Tokens per Sec:      965 || Batch Translation Loss:   0.020741 => Txt Tokens per Sec:     2722 || Lr: 0.000050
2024-02-05 07:43:18,330 Epoch 3878: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 07:43:18,331 EPOCH 3879
2024-02-05 07:43:32,381 Epoch 3879: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 07:43:32,381 EPOCH 3880
2024-02-05 07:43:46,170 Epoch 3880: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 07:43:46,171 EPOCH 3881
2024-02-05 07:44:00,036 Epoch 3881: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 07:44:00,036 EPOCH 3882
2024-02-05 07:44:13,897 Epoch 3882: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 07:44:13,898 EPOCH 3883
2024-02-05 07:44:27,709 Epoch 3883: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 07:44:27,710 EPOCH 3884
2024-02-05 07:44:41,822 Epoch 3884: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 07:44:41,822 EPOCH 3885
2024-02-05 07:44:55,883 Epoch 3885: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 07:44:55,884 EPOCH 3886
2024-02-05 07:45:10,013 Epoch 3886: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 07:45:10,014 EPOCH 3887
2024-02-05 07:45:24,018 Epoch 3887: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-05 07:45:24,018 EPOCH 3888
2024-02-05 07:45:38,015 Epoch 3888: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.57 
2024-02-05 07:45:38,016 EPOCH 3889
2024-02-05 07:45:50,197 [Epoch: 3889 Step: 00035000] Batch Recognition Loss:   0.001165 => Gls Tokens per Sec:      768 || Batch Translation Loss:   0.062106 => Txt Tokens per Sec:     2090 || Lr: 0.000050
2024-02-05 07:45:52,067 Epoch 3889: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.58 
2024-02-05 07:45:52,067 EPOCH 3890
2024-02-05 07:46:05,810 Epoch 3890: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-05 07:46:05,811 EPOCH 3891
2024-02-05 07:46:19,984 Epoch 3891: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-05 07:46:19,985 EPOCH 3892
2024-02-05 07:46:34,200 Epoch 3892: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-05 07:46:34,201 EPOCH 3893
2024-02-05 07:46:48,079 Epoch 3893: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-05 07:46:48,080 EPOCH 3894
2024-02-05 07:47:01,997 Epoch 3894: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 07:47:01,997 EPOCH 3895
2024-02-05 07:47:15,899 Epoch 3895: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 07:47:15,900 EPOCH 3896
2024-02-05 07:47:29,890 Epoch 3896: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 07:47:29,890 EPOCH 3897
2024-02-05 07:47:43,880 Epoch 3897: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 07:47:43,881 EPOCH 3898
2024-02-05 07:47:57,738 Epoch 3898: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 07:47:57,739 EPOCH 3899
2024-02-05 07:48:11,689 Epoch 3899: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 07:48:11,689 EPOCH 3900
2024-02-05 07:48:25,608 [Epoch: 3900 Step: 00035100] Batch Recognition Loss:   0.000110 => Gls Tokens per Sec:      764 || Batch Translation Loss:   0.009105 => Txt Tokens per Sec:     2120 || Lr: 0.000050
2024-02-05 07:48:25,609 Epoch 3900: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 07:48:25,609 EPOCH 3901
2024-02-05 07:48:39,520 Epoch 3901: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 07:48:39,520 EPOCH 3902
2024-02-05 07:48:53,544 Epoch 3902: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 07:48:53,545 EPOCH 3903
2024-02-05 07:49:07,217 Epoch 3903: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 07:49:07,218 EPOCH 3904
2024-02-05 07:49:21,137 Epoch 3904: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 07:49:21,137 EPOCH 3905
2024-02-05 07:49:35,119 Epoch 3905: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 07:49:35,120 EPOCH 3906
2024-02-05 07:49:48,758 Epoch 3906: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 07:49:48,759 EPOCH 3907
2024-02-05 07:50:02,858 Epoch 3907: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 07:50:02,859 EPOCH 3908
2024-02-05 07:50:16,934 Epoch 3908: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 07:50:16,935 EPOCH 3909
2024-02-05 07:50:31,109 Epoch 3909: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 07:50:31,109 EPOCH 3910
2024-02-05 07:50:45,140 Epoch 3910: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 07:50:45,141 EPOCH 3911
2024-02-05 07:50:59,099 Epoch 3911: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 07:50:59,100 EPOCH 3912
2024-02-05 07:50:59,462 [Epoch: 3912 Step: 00035200] Batch Recognition Loss:   0.000113 => Gls Tokens per Sec:     3542 || Batch Translation Loss:   0.015493 => Txt Tokens per Sec:     9786 || Lr: 0.000050
2024-02-05 07:51:13,340 Epoch 3912: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 07:51:13,340 EPOCH 3913
2024-02-05 07:51:27,184 Epoch 3913: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 07:51:27,185 EPOCH 3914
2024-02-05 07:51:40,963 Epoch 3914: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 07:51:40,964 EPOCH 3915
2024-02-05 07:51:54,791 Epoch 3915: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 07:51:54,792 EPOCH 3916
2024-02-05 07:52:08,551 Epoch 3916: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 07:52:08,551 EPOCH 3917
2024-02-05 07:52:22,585 Epoch 3917: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 07:52:22,586 EPOCH 3918
2024-02-05 07:52:36,672 Epoch 3918: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 07:52:36,673 EPOCH 3919
2024-02-05 07:52:50,684 Epoch 3919: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 07:52:50,685 EPOCH 3920
2024-02-05 07:53:04,378 Epoch 3920: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 07:53:04,379 EPOCH 3921
2024-02-05 07:53:18,474 Epoch 3921: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 07:53:18,475 EPOCH 3922
2024-02-05 07:53:32,511 Epoch 3922: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 07:53:32,512 EPOCH 3923
2024-02-05 07:53:33,318 [Epoch: 3923 Step: 00035300] Batch Recognition Loss:   0.000100 => Gls Tokens per Sec:     3180 || Batch Translation Loss:   0.011991 => Txt Tokens per Sec:     7496 || Lr: 0.000050
2024-02-05 07:53:46,474 Epoch 3923: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 07:53:46,475 EPOCH 3924
2024-02-05 07:54:00,270 Epoch 3924: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 07:54:00,271 EPOCH 3925
2024-02-05 07:54:14,348 Epoch 3925: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 07:54:14,348 EPOCH 3926
2024-02-05 07:54:28,211 Epoch 3926: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 07:54:28,211 EPOCH 3927
2024-02-05 07:54:42,042 Epoch 3927: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 07:54:42,043 EPOCH 3928
2024-02-05 07:54:56,206 Epoch 3928: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 07:54:56,206 EPOCH 3929
2024-02-05 07:55:10,003 Epoch 3929: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 07:55:10,004 EPOCH 3930
2024-02-05 07:55:23,850 Epoch 3930: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 07:55:23,850 EPOCH 3931
2024-02-05 07:55:37,903 Epoch 3931: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 07:55:37,904 EPOCH 3932
2024-02-05 07:55:51,840 Epoch 3932: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 07:55:51,841 EPOCH 3933
2024-02-05 07:56:05,781 Epoch 3933: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-05 07:56:05,781 EPOCH 3934
2024-02-05 07:56:07,035 [Epoch: 3934 Step: 00035400] Batch Recognition Loss:   0.000107 => Gls Tokens per Sec:     3065 || Batch Translation Loss:   0.018939 => Txt Tokens per Sec:     7903 || Lr: 0.000050
2024-02-05 07:56:19,676 Epoch 3934: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-05 07:56:19,678 EPOCH 3935
2024-02-05 07:56:33,655 Epoch 3935: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.64 
2024-02-05 07:56:33,655 EPOCH 3936
2024-02-05 07:56:47,932 Epoch 3936: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-05 07:56:47,933 EPOCH 3937
2024-02-05 07:57:01,699 Epoch 3937: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-05 07:57:01,699 EPOCH 3938
2024-02-05 07:57:15,658 Epoch 3938: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-05 07:57:15,659 EPOCH 3939
2024-02-05 07:57:29,412 Epoch 3939: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-05 07:57:29,413 EPOCH 3940
2024-02-05 07:57:43,225 Epoch 3940: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-05 07:57:43,225 EPOCH 3941
2024-02-05 07:57:56,825 Epoch 3941: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-05 07:57:56,825 EPOCH 3942
2024-02-05 07:58:10,818 Epoch 3942: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-05 07:58:10,818 EPOCH 3943
2024-02-05 07:58:24,880 Epoch 3943: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-05 07:58:24,881 EPOCH 3944
2024-02-05 07:58:38,640 Epoch 3944: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 07:58:38,640 EPOCH 3945
2024-02-05 07:58:49,296 [Epoch: 3945 Step: 00035500] Batch Recognition Loss:   0.000231 => Gls Tokens per Sec:      397 || Batch Translation Loss:   0.031250 => Txt Tokens per Sec:     1266 || Lr: 0.000050
2024-02-05 07:58:52,636 Epoch 3945: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-05 07:58:52,636 EPOCH 3946
2024-02-05 07:59:06,519 Epoch 3946: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 07:59:06,519 EPOCH 3947
2024-02-05 07:59:20,742 Epoch 3947: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 07:59:20,742 EPOCH 3948
2024-02-05 07:59:34,525 Epoch 3948: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 07:59:34,525 EPOCH 3949
2024-02-05 07:59:48,525 Epoch 3949: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 07:59:48,526 EPOCH 3950
2024-02-05 08:00:02,479 Epoch 3950: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 08:00:02,480 EPOCH 3951
2024-02-05 08:00:16,568 Epoch 3951: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 08:00:16,569 EPOCH 3952
2024-02-05 08:00:30,373 Epoch 3952: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 08:00:30,374 EPOCH 3953
2024-02-05 08:00:44,090 Epoch 3953: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 08:00:44,091 EPOCH 3954
2024-02-05 08:00:58,101 Epoch 3954: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 08:00:58,101 EPOCH 3955
2024-02-05 08:01:12,078 Epoch 3955: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 08:01:12,078 EPOCH 3956
2024-02-05 08:01:20,355 [Epoch: 3956 Step: 00035600] Batch Recognition Loss:   0.000128 => Gls Tokens per Sec:      773 || Batch Translation Loss:   0.016362 => Txt Tokens per Sec:     2267 || Lr: 0.000050
2024-02-05 08:01:26,212 Epoch 3956: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 08:01:26,213 EPOCH 3957
2024-02-05 08:01:40,046 Epoch 3957: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 08:01:40,047 EPOCH 3958
2024-02-05 08:01:53,725 Epoch 3958: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 08:01:53,726 EPOCH 3959
2024-02-05 08:02:07,747 Epoch 3959: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 08:02:07,747 EPOCH 3960
2024-02-05 08:02:21,682 Epoch 3960: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 08:02:21,683 EPOCH 3961
2024-02-05 08:02:35,795 Epoch 3961: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 08:02:35,795 EPOCH 3962
2024-02-05 08:02:49,899 Epoch 3962: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 08:02:49,900 EPOCH 3963
2024-02-05 08:03:03,701 Epoch 3963: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 08:03:03,701 EPOCH 3964
2024-02-05 08:03:17,584 Epoch 3964: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 08:03:17,585 EPOCH 3965
2024-02-05 08:03:31,596 Epoch 3965: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 08:03:31,597 EPOCH 3966
2024-02-05 08:03:45,563 Epoch 3966: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 08:03:45,563 EPOCH 3967
2024-02-05 08:03:49,592 [Epoch: 3967 Step: 00035700] Batch Recognition Loss:   0.000104 => Gls Tokens per Sec:     1907 || Batch Translation Loss:   0.011133 => Txt Tokens per Sec:     5303 || Lr: 0.000050
2024-02-05 08:03:59,136 Epoch 3967: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 08:03:59,136 EPOCH 3968
2024-02-05 08:04:13,224 Epoch 3968: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 08:04:13,225 EPOCH 3969
2024-02-05 08:04:27,155 Epoch 3969: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 08:04:27,156 EPOCH 3970
2024-02-05 08:04:41,009 Epoch 3970: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 08:04:41,009 EPOCH 3971
2024-02-05 08:04:55,124 Epoch 3971: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 08:04:55,125 EPOCH 3972
2024-02-05 08:05:09,253 Epoch 3972: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 08:05:09,254 EPOCH 3973
2024-02-05 08:05:23,260 Epoch 3973: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 08:05:23,260 EPOCH 3974
2024-02-05 08:05:36,948 Epoch 3974: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 08:05:36,948 EPOCH 3975
2024-02-05 08:05:50,968 Epoch 3975: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 08:05:50,969 EPOCH 3976
2024-02-05 08:06:05,289 Epoch 3976: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 08:06:05,290 EPOCH 3977
2024-02-05 08:06:19,150 Epoch 3977: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 08:06:19,150 EPOCH 3978
2024-02-05 08:06:27,489 [Epoch: 3978 Step: 00035800] Batch Recognition Loss:   0.000140 => Gls Tokens per Sec:      968 || Batch Translation Loss:   0.027745 => Txt Tokens per Sec:     2647 || Lr: 0.000050
2024-02-05 08:06:33,046 Epoch 3978: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 08:06:33,046 EPOCH 3979
2024-02-05 08:06:46,931 Epoch 3979: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 08:06:46,932 EPOCH 3980
2024-02-05 08:07:01,131 Epoch 3980: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 08:07:01,131 EPOCH 3981
2024-02-05 08:07:15,038 Epoch 3981: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 08:07:15,038 EPOCH 3982
2024-02-05 08:07:28,933 Epoch 3982: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 08:07:28,933 EPOCH 3983
2024-02-05 08:07:42,894 Epoch 3983: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 08:07:42,895 EPOCH 3984
2024-02-05 08:07:56,483 Epoch 3984: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 08:07:56,483 EPOCH 3985
2024-02-05 08:08:10,683 Epoch 3985: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 08:08:10,684 EPOCH 3986
2024-02-05 08:08:24,775 Epoch 3986: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 08:08:24,776 EPOCH 3987
2024-02-05 08:08:38,853 Epoch 3987: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 08:08:38,853 EPOCH 3988
2024-02-05 08:08:52,874 Epoch 3988: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 08:08:52,875 EPOCH 3989
2024-02-05 08:09:06,576 [Epoch: 3989 Step: 00035900] Batch Recognition Loss:   0.000112 => Gls Tokens per Sec:      683 || Batch Translation Loss:   0.014232 => Txt Tokens per Sec:     1898 || Lr: 0.000050
2024-02-05 08:09:07,032 Epoch 3989: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 08:09:07,033 EPOCH 3990
2024-02-05 08:09:20,792 Epoch 3990: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 08:09:20,792 EPOCH 3991
2024-02-05 08:09:34,271 Epoch 3991: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 08:09:34,271 EPOCH 3992
2024-02-05 08:09:48,327 Epoch 3992: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 08:09:48,328 EPOCH 3993
2024-02-05 08:10:02,021 Epoch 3993: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 08:10:02,021 EPOCH 3994
2024-02-05 08:10:15,886 Epoch 3994: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 08:10:15,886 EPOCH 3995
2024-02-05 08:10:29,762 Epoch 3995: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 08:10:29,763 EPOCH 3996
2024-02-05 08:10:43,751 Epoch 3996: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 08:10:43,752 EPOCH 3997
2024-02-05 08:10:57,897 Epoch 3997: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 08:10:57,897 EPOCH 3998
2024-02-05 08:11:11,837 Epoch 3998: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 08:11:11,838 EPOCH 3999
2024-02-05 08:11:25,518 Epoch 3999: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 08:11:25,519 EPOCH 4000
2024-02-05 08:11:39,857 [Epoch: 4000 Step: 00036000] Batch Recognition Loss:   0.000126 => Gls Tokens per Sec:      741 || Batch Translation Loss:   0.014710 => Txt Tokens per Sec:     2058 || Lr: 0.000050
2024-02-05 08:12:08,933 Validation result at epoch 4000, step    36000: duration: 29.0761s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00025	Translation Loss: 100479.09375	PPL: 23270.64062
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.70	(BLEU-1: 10.61,	BLEU-2: 3.32,	BLEU-3: 1.34,	BLEU-4: 0.70)
	CHRF 17.06	ROUGE 9.16
2024-02-05 08:12:08,934 Logging Recognition and Translation Outputs
2024-02-05 08:12:08,934 ========================================================================================================================
2024-02-05 08:12:08,935 Logging Sequence: 78_198.00
2024-02-05 08:12:08,935 	Gloss Reference :	A B+C+D+E
2024-02-05 08:12:08,935 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 08:12:08,935 	Gloss Alignment :	         
2024-02-05 08:12:08,935 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 08:12:08,936 	Text Reference  :	*** ****** *** * **** ** **** they   have been flooded with congratulations comments
2024-02-05 08:12:08,937 	Text Hypothesis :	and behind him a girl is seen laying on   the  bed     by   her             lingerie
2024-02-05 08:12:08,937 	Text Alignment  :	I   I      I   I I    I  I    S      S    S    S       S    S               S       
2024-02-05 08:12:08,937 ========================================================================================================================
2024-02-05 08:12:08,937 Logging Sequence: 145_216.00
2024-02-05 08:12:08,937 	Gloss Reference :	A B+C+D+E
2024-02-05 08:12:08,938 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 08:12:08,938 	Gloss Alignment :	         
2024-02-05 08:12:08,938 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 08:12:08,939 	Text Reference  :	asking him to include sameeha in the world championship as       she  was a   talented athlete
2024-02-05 08:12:08,939 	Text Hypothesis :	****** *** ** ******* ******* ** *** ***** he           believed that his bag brought  luck   
2024-02-05 08:12:08,939 	Text Alignment  :	D      D   D  D       D       D  D   D     S            S        S    S   S   S        S      
2024-02-05 08:12:08,939 ========================================================================================================================
2024-02-05 08:12:08,939 Logging Sequence: 70_137.00
2024-02-05 08:12:08,940 	Gloss Reference :	A B+C+D+E
2024-02-05 08:12:08,940 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 08:12:08,940 	Gloss Alignment :	         
2024-02-05 08:12:08,940 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 08:12:08,941 	Text Reference  :	the small gesture appeared to encourage people to drink water   instead of         aerated drinks  
2024-02-05 08:12:08,941 	Text Hypothesis :	*** ***** ******* ******** ** ********* ****** ** ***** however no      disrespect was     intended
2024-02-05 08:12:08,941 	Text Alignment  :	D   D     D       D        D  D         D      D  D     S       S       S          S       S       
2024-02-05 08:12:08,941 ========================================================================================================================
2024-02-05 08:12:08,941 Logging Sequence: 119_20.00
2024-02-05 08:12:08,941 	Gloss Reference :	A B+C+D+E
2024-02-05 08:12:08,941 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 08:12:08,942 	Gloss Alignment :	         
2024-02-05 08:12:08,942 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 08:12:08,943 	Text Reference  :	messi intended to gift something to  all    the   players and ******* the   staff    to special to    celebrate the moment 
2024-02-05 08:12:08,943 	Text Hypothesis :	***** ******** ** **** ********* the reason being played  and idesign gold' official to ******* messi receiving the iphones
2024-02-05 08:12:08,943 	Text Alignment  :	D     D        D  D    D         S   S      S     S           I       S     S           D       S     S             S      
2024-02-05 08:12:08,944 ========================================================================================================================
2024-02-05 08:12:08,944 Logging Sequence: 106_15.00
2024-02-05 08:12:08,944 	Gloss Reference :	A B+C+D+E
2024-02-05 08:12:08,944 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 08:12:08,944 	Gloss Alignment :	         
2024-02-05 08:12:08,944 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 08:12:08,945 	Text Reference  :	*** **** ******** ** but  what about women's cricket earlier we   never spoke about it       
2024-02-05 08:12:08,945 	Text Hypothesis :	and were selected to play he   was   given   a       fine    such a     huge  fan   following
2024-02-05 08:12:08,946 	Text Alignment  :	I   I    I        I  S    S    S     S       S       S       S    S     S     S     S        
2024-02-05 08:12:08,946 ========================================================================================================================
2024-02-05 08:12:08,950 Epoch 4000: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 08:12:08,950 EPOCH 4001
2024-02-05 08:12:23,116 Epoch 4001: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 08:12:23,116 EPOCH 4002
2024-02-05 08:12:37,140 Epoch 4002: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 08:12:37,141 EPOCH 4003
2024-02-05 08:12:50,854 Epoch 4003: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 08:12:50,855 EPOCH 4004
2024-02-05 08:13:04,880 Epoch 4004: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 08:13:04,880 EPOCH 4005
2024-02-05 08:13:18,626 Epoch 4005: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 08:13:18,626 EPOCH 4006
2024-02-05 08:13:32,684 Epoch 4006: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 08:13:32,684 EPOCH 4007
2024-02-05 08:13:46,638 Epoch 4007: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 08:13:46,639 EPOCH 4008
2024-02-05 08:14:00,723 Epoch 4008: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 08:14:00,723 EPOCH 4009
2024-02-05 08:14:14,798 Epoch 4009: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 08:14:14,798 EPOCH 4010
2024-02-05 08:14:28,667 Epoch 4010: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 08:14:28,668 EPOCH 4011
2024-02-05 08:14:42,770 Epoch 4011: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 08:14:42,770 EPOCH 4012
2024-02-05 08:14:47,747 [Epoch: 4012 Step: 00036100] Batch Recognition Loss:   0.000155 => Gls Tokens per Sec:      257 || Batch Translation Loss:   0.021970 => Txt Tokens per Sec:      901 || Lr: 0.000050
2024-02-05 08:14:56,538 Epoch 4012: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 08:14:56,538 EPOCH 4013
2024-02-05 08:15:10,655 Epoch 4013: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 08:15:10,655 EPOCH 4014
2024-02-05 08:15:24,472 Epoch 4014: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 08:15:24,472 EPOCH 4015
2024-02-05 08:15:38,497 Epoch 4015: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 08:15:38,497 EPOCH 4016
2024-02-05 08:15:52,517 Epoch 4016: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 08:15:52,518 EPOCH 4017
2024-02-05 08:16:06,443 Epoch 4017: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 08:16:06,443 EPOCH 4018
2024-02-05 08:16:20,548 Epoch 4018: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 08:16:20,548 EPOCH 4019
2024-02-05 08:16:34,226 Epoch 4019: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 08:16:34,226 EPOCH 4020
2024-02-05 08:16:48,430 Epoch 4020: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 08:16:48,431 EPOCH 4021
2024-02-05 08:17:02,301 Epoch 4021: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 08:17:02,302 EPOCH 4022
2024-02-05 08:17:16,285 Epoch 4022: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 08:17:16,285 EPOCH 4023
2024-02-05 08:17:18,234 [Epoch: 4023 Step: 00036200] Batch Recognition Loss:   0.000099 => Gls Tokens per Sec:     1314 || Batch Translation Loss:   0.010737 => Txt Tokens per Sec:     3591 || Lr: 0.000050
2024-02-05 08:17:30,334 Epoch 4023: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 08:17:30,335 EPOCH 4024
2024-02-05 08:17:44,105 Epoch 4024: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 08:17:44,106 EPOCH 4025
2024-02-05 08:17:58,078 Epoch 4025: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 08:17:58,079 EPOCH 4026
2024-02-05 08:18:12,012 Epoch 4026: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 08:18:12,013 EPOCH 4027
2024-02-05 08:18:26,023 Epoch 4027: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 08:18:26,023 EPOCH 4028
2024-02-05 08:18:39,792 Epoch 4028: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 08:18:39,792 EPOCH 4029
2024-02-05 08:18:53,575 Epoch 4029: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 08:18:53,576 EPOCH 4030
2024-02-05 08:19:07,812 Epoch 4030: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-05 08:19:07,813 EPOCH 4031
2024-02-05 08:19:21,798 Epoch 4031: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-05 08:19:21,799 EPOCH 4032
2024-02-05 08:19:35,872 Epoch 4032: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.25 
2024-02-05 08:19:35,872 EPOCH 4033
2024-02-05 08:19:49,952 Epoch 4033: Total Training Recognition Loss 0.04  Total Training Translation Loss 4.84 
2024-02-05 08:19:49,953 EPOCH 4034
2024-02-05 08:19:51,442 [Epoch: 4034 Step: 00036300] Batch Recognition Loss:   0.000634 => Gls Tokens per Sec:     2582 || Batch Translation Loss:   0.118735 => Txt Tokens per Sec:     7043 || Lr: 0.000050
2024-02-05 08:20:03,923 Epoch 4034: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.61 
2024-02-05 08:20:03,923 EPOCH 4035
2024-02-05 08:20:17,846 Epoch 4035: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.95 
2024-02-05 08:20:17,847 EPOCH 4036
2024-02-05 08:20:31,614 Epoch 4036: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-05 08:20:31,615 EPOCH 4037
2024-02-05 08:20:45,671 Epoch 4037: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-05 08:20:45,672 EPOCH 4038
2024-02-05 08:20:59,635 Epoch 4038: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-05 08:20:59,636 EPOCH 4039
2024-02-05 08:21:13,437 Epoch 4039: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-05 08:21:13,437 EPOCH 4040
2024-02-05 08:21:27,583 Epoch 4040: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-05 08:21:27,584 EPOCH 4041
2024-02-05 08:21:41,478 Epoch 4041: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 08:21:41,479 EPOCH 4042
2024-02-05 08:21:55,289 Epoch 4042: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 08:21:55,289 EPOCH 4043
2024-02-05 08:22:09,261 Epoch 4043: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 08:22:09,261 EPOCH 4044
2024-02-05 08:22:22,975 Epoch 4044: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 08:22:22,975 EPOCH 4045
2024-02-05 08:22:28,718 [Epoch: 4045 Step: 00036400] Batch Recognition Loss:   0.000141 => Gls Tokens per Sec:      737 || Batch Translation Loss:   0.011778 => Txt Tokens per Sec:     1818 || Lr: 0.000050
2024-02-05 08:22:37,090 Epoch 4045: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 08:22:37,090 EPOCH 4046
2024-02-05 08:22:51,022 Epoch 4046: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 08:22:51,023 EPOCH 4047
2024-02-05 08:23:05,002 Epoch 4047: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 08:23:05,003 EPOCH 4048
2024-02-05 08:23:18,987 Epoch 4048: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 08:23:18,988 EPOCH 4049
2024-02-05 08:23:33,028 Epoch 4049: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 08:23:33,028 EPOCH 4050
2024-02-05 08:23:47,139 Epoch 4050: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 08:23:47,139 EPOCH 4051
2024-02-05 08:24:01,059 Epoch 4051: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 08:24:01,059 EPOCH 4052
2024-02-05 08:24:15,044 Epoch 4052: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 08:24:15,045 EPOCH 4053
2024-02-05 08:24:29,036 Epoch 4053: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 08:24:29,037 EPOCH 4054
2024-02-05 08:24:43,028 Epoch 4054: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 08:24:43,029 EPOCH 4055
2024-02-05 08:24:57,076 Epoch 4055: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 08:24:57,076 EPOCH 4056
2024-02-05 08:25:03,924 [Epoch: 4056 Step: 00036500] Batch Recognition Loss:   0.000122 => Gls Tokens per Sec:      935 || Batch Translation Loss:   0.012924 => Txt Tokens per Sec:     2525 || Lr: 0.000050
2024-02-05 08:25:10,933 Epoch 4056: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 08:25:10,934 EPOCH 4057
2024-02-05 08:25:24,666 Epoch 4057: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 08:25:24,666 EPOCH 4058
2024-02-05 08:25:38,904 Epoch 4058: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 08:25:38,905 EPOCH 4059
2024-02-05 08:25:53,022 Epoch 4059: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 08:25:53,022 EPOCH 4060
2024-02-05 08:26:06,728 Epoch 4060: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 08:26:06,728 EPOCH 4061
2024-02-05 08:26:20,713 Epoch 4061: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 08:26:20,713 EPOCH 4062
2024-02-05 08:26:34,718 Epoch 4062: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 08:26:34,718 EPOCH 4063
2024-02-05 08:26:48,691 Epoch 4063: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 08:26:48,692 EPOCH 4064
2024-02-05 08:27:03,078 Epoch 4064: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 08:27:03,078 EPOCH 4065
2024-02-05 08:27:16,952 Epoch 4065: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 08:27:16,952 EPOCH 4066
2024-02-05 08:27:30,753 Epoch 4066: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 08:27:30,753 EPOCH 4067
2024-02-05 08:27:42,074 [Epoch: 4067 Step: 00036600] Batch Recognition Loss:   0.000147 => Gls Tokens per Sec:      600 || Batch Translation Loss:   0.016289 => Txt Tokens per Sec:     1646 || Lr: 0.000050
2024-02-05 08:27:44,888 Epoch 4067: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 08:27:44,889 EPOCH 4068
2024-02-05 08:27:58,607 Epoch 4068: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 08:27:58,608 EPOCH 4069
2024-02-05 08:28:12,537 Epoch 4069: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 08:28:12,538 EPOCH 4070
2024-02-05 08:28:26,578 Epoch 4070: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 08:28:26,579 EPOCH 4071
2024-02-05 08:28:40,316 Epoch 4071: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 08:28:40,317 EPOCH 4072
2024-02-05 08:28:54,103 Epoch 4072: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 08:28:54,104 EPOCH 4073
2024-02-05 08:29:08,181 Epoch 4073: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 08:29:08,182 EPOCH 4074
2024-02-05 08:29:22,081 Epoch 4074: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 08:29:22,082 EPOCH 4075
2024-02-05 08:29:35,845 Epoch 4075: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 08:29:35,846 EPOCH 4076
2024-02-05 08:29:50,059 Epoch 4076: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 08:29:50,060 EPOCH 4077
2024-02-05 08:30:04,335 Epoch 4077: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 08:30:04,336 EPOCH 4078
2024-02-05 08:30:16,108 [Epoch: 4078 Step: 00036700] Batch Recognition Loss:   0.000171 => Gls Tokens per Sec:      686 || Batch Translation Loss:   0.019657 => Txt Tokens per Sec:     1884 || Lr: 0.000050
2024-02-05 08:30:18,415 Epoch 4078: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 08:30:18,415 EPOCH 4079
2024-02-05 08:30:32,558 Epoch 4079: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 08:30:32,558 EPOCH 4080
2024-02-05 08:30:46,478 Epoch 4080: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 08:30:46,478 EPOCH 4081
2024-02-05 08:31:00,615 Epoch 4081: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 08:31:00,616 EPOCH 4082
2024-02-05 08:31:14,768 Epoch 4082: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 08:31:14,768 EPOCH 4083
2024-02-05 08:31:28,839 Epoch 4083: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 08:31:28,839 EPOCH 4084
2024-02-05 08:31:42,707 Epoch 4084: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 08:31:42,707 EPOCH 4085
2024-02-05 08:31:56,740 Epoch 4085: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 08:31:56,741 EPOCH 4086
2024-02-05 08:32:10,633 Epoch 4086: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 08:32:10,634 EPOCH 4087
2024-02-05 08:32:24,767 Epoch 4087: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 08:32:24,768 EPOCH 4088
2024-02-05 08:32:38,472 Epoch 4088: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 08:32:38,472 EPOCH 4089
2024-02-05 08:32:51,755 [Epoch: 4089 Step: 00036800] Batch Recognition Loss:   0.000140 => Gls Tokens per Sec:      704 || Batch Translation Loss:   0.006444 => Txt Tokens per Sec:     1930 || Lr: 0.000050
2024-02-05 08:32:52,545 Epoch 4089: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 08:32:52,545 EPOCH 4090
2024-02-05 08:33:06,319 Epoch 4090: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 08:33:06,320 EPOCH 4091
2024-02-05 08:33:20,150 Epoch 4091: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 08:33:20,151 EPOCH 4092
2024-02-05 08:33:34,071 Epoch 4092: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 08:33:34,072 EPOCH 4093
2024-02-05 08:33:48,114 Epoch 4093: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 08:33:48,114 EPOCH 4094
2024-02-05 08:34:01,802 Epoch 4094: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 08:34:01,802 EPOCH 4095
2024-02-05 08:34:15,896 Epoch 4095: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 08:34:15,896 EPOCH 4096
2024-02-05 08:34:29,797 Epoch 4096: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 08:34:29,797 EPOCH 4097
2024-02-05 08:34:44,007 Epoch 4097: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 08:34:44,007 EPOCH 4098
2024-02-05 08:34:58,154 Epoch 4098: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 08:34:58,154 EPOCH 4099
2024-02-05 08:35:11,981 Epoch 4099: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 08:35:11,982 EPOCH 4100
2024-02-05 08:35:26,056 [Epoch: 4100 Step: 00036900] Batch Recognition Loss:   0.000117 => Gls Tokens per Sec:      755 || Batch Translation Loss:   0.015100 => Txt Tokens per Sec:     2097 || Lr: 0.000050
2024-02-05 08:35:26,057 Epoch 4100: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 08:35:26,057 EPOCH 4101
2024-02-05 08:35:39,633 Epoch 4101: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 08:35:39,633 EPOCH 4102
2024-02-05 08:35:53,402 Epoch 4102: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 08:35:53,403 EPOCH 4103
2024-02-05 08:36:07,338 Epoch 4103: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 08:36:07,338 EPOCH 4104
2024-02-05 08:36:21,448 Epoch 4104: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 08:36:21,449 EPOCH 4105
2024-02-05 08:36:35,448 Epoch 4105: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 08:36:35,449 EPOCH 4106
2024-02-05 08:36:49,233 Epoch 4106: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 08:36:49,234 EPOCH 4107
2024-02-05 08:37:03,505 Epoch 4107: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 08:37:03,506 EPOCH 4108
2024-02-05 08:37:17,263 Epoch 4108: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 08:37:17,263 EPOCH 4109
2024-02-05 08:37:31,394 Epoch 4109: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 08:37:31,394 EPOCH 4110
2024-02-05 08:37:45,298 Epoch 4110: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 08:37:45,298 EPOCH 4111
2024-02-05 08:37:59,063 Epoch 4111: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 08:37:59,063 EPOCH 4112
2024-02-05 08:37:59,619 [Epoch: 4112 Step: 00037000] Batch Recognition Loss:   0.000101 => Gls Tokens per Sec:     2309 || Batch Translation Loss:   0.015269 => Txt Tokens per Sec:     6288 || Lr: 0.000050
2024-02-05 08:38:12,989 Epoch 4112: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 08:38:12,989 EPOCH 4113
2024-02-05 08:38:27,319 Epoch 4113: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 08:38:27,320 EPOCH 4114
2024-02-05 08:38:40,936 Epoch 4114: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 08:38:40,937 EPOCH 4115
2024-02-05 08:38:55,012 Epoch 4115: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 08:38:55,012 EPOCH 4116
2024-02-05 08:39:08,562 Epoch 4116: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 08:39:08,562 EPOCH 4117
2024-02-05 08:39:22,356 Epoch 4117: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 08:39:22,356 EPOCH 4118
2024-02-05 08:39:36,103 Epoch 4118: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 08:39:36,104 EPOCH 4119
2024-02-05 08:39:50,295 Epoch 4119: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 08:39:50,295 EPOCH 4120
2024-02-05 08:40:04,124 Epoch 4120: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 08:40:04,125 EPOCH 4121
2024-02-05 08:40:18,296 Epoch 4121: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 08:40:18,297 EPOCH 4122
2024-02-05 08:40:32,049 Epoch 4122: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 08:40:32,050 EPOCH 4123
2024-02-05 08:40:32,784 [Epoch: 4123 Step: 00037100] Batch Recognition Loss:   0.000106 => Gls Tokens per Sec:     3492 || Batch Translation Loss:   0.013551 => Txt Tokens per Sec:     7841 || Lr: 0.000050
2024-02-05 08:40:46,040 Epoch 4123: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 08:40:46,041 EPOCH 4124
2024-02-05 08:41:00,279 Epoch 4124: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 08:41:00,280 EPOCH 4125
2024-02-05 08:41:14,411 Epoch 4125: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 08:41:14,412 EPOCH 4126
2024-02-05 08:41:28,443 Epoch 4126: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 08:41:28,444 EPOCH 4127
2024-02-05 08:41:42,421 Epoch 4127: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 08:41:42,422 EPOCH 4128
2024-02-05 08:41:56,573 Epoch 4128: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 08:41:56,574 EPOCH 4129
2024-02-05 08:42:10,388 Epoch 4129: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 08:42:10,388 EPOCH 4130
2024-02-05 08:42:24,279 Epoch 4130: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 08:42:24,280 EPOCH 4131
2024-02-05 08:42:38,497 Epoch 4131: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 08:42:38,497 EPOCH 4132
2024-02-05 08:42:52,625 Epoch 4132: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 08:42:52,626 EPOCH 4133
2024-02-05 08:43:06,577 Epoch 4133: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 08:43:06,578 EPOCH 4134
2024-02-05 08:43:09,010 [Epoch: 4134 Step: 00037200] Batch Recognition Loss:   0.000154 => Gls Tokens per Sec:     1580 || Batch Translation Loss:   0.018877 => Txt Tokens per Sec:     3814 || Lr: 0.000050
2024-02-05 08:43:20,683 Epoch 4134: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 08:43:20,684 EPOCH 4135
2024-02-05 08:43:34,614 Epoch 4135: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 08:43:34,614 EPOCH 4136
2024-02-05 08:43:48,536 Epoch 4136: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 08:43:48,537 EPOCH 4137
2024-02-05 08:44:02,657 Epoch 4137: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 08:44:02,657 EPOCH 4138
2024-02-05 08:44:16,646 Epoch 4138: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 08:44:16,646 EPOCH 4139
2024-02-05 08:44:30,409 Epoch 4139: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 08:44:30,409 EPOCH 4140
2024-02-05 08:44:44,228 Epoch 4140: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 08:44:44,229 EPOCH 4141
2024-02-05 08:44:58,230 Epoch 4141: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 08:44:58,230 EPOCH 4142
2024-02-05 08:45:12,183 Epoch 4142: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 08:45:12,184 EPOCH 4143
2024-02-05 08:45:26,164 Epoch 4143: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 08:45:26,164 EPOCH 4144
2024-02-05 08:45:40,053 Epoch 4144: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 08:45:40,053 EPOCH 4145
2024-02-05 08:45:46,380 [Epoch: 4145 Step: 00037300] Batch Recognition Loss:   0.000110 => Gls Tokens per Sec:      809 || Batch Translation Loss:   0.015303 => Txt Tokens per Sec:     2215 || Lr: 0.000050
2024-02-05 08:45:54,002 Epoch 4145: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 08:45:54,003 EPOCH 4146
2024-02-05 08:46:07,988 Epoch 4146: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 08:46:07,988 EPOCH 4147
2024-02-05 08:46:21,880 Epoch 4147: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 08:46:21,880 EPOCH 4148
2024-02-05 08:46:36,015 Epoch 4148: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 08:46:36,015 EPOCH 4149
2024-02-05 08:46:49,960 Epoch 4149: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 08:46:49,961 EPOCH 4150
2024-02-05 08:47:04,014 Epoch 4150: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 08:47:04,015 EPOCH 4151
2024-02-05 08:47:17,794 Epoch 4151: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 08:47:17,794 EPOCH 4152
2024-02-05 08:47:32,159 Epoch 4152: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 08:47:32,160 EPOCH 4153
2024-02-05 08:47:46,073 Epoch 4153: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 08:47:46,074 EPOCH 4154
2024-02-05 08:48:00,214 Epoch 4154: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 08:48:00,215 EPOCH 4155
2024-02-05 08:48:14,328 Epoch 4155: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 08:48:14,329 EPOCH 4156
2024-02-05 08:48:18,120 [Epoch: 4156 Step: 00037400] Batch Recognition Loss:   0.000098 => Gls Tokens per Sec:     1689 || Batch Translation Loss:   0.011729 => Txt Tokens per Sec:     4615 || Lr: 0.000050
2024-02-05 08:48:28,358 Epoch 4156: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 08:48:28,358 EPOCH 4157
2024-02-05 08:48:42,551 Epoch 4157: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 08:48:42,551 EPOCH 4158
2024-02-05 08:48:56,849 Epoch 4158: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 08:48:56,850 EPOCH 4159
2024-02-05 08:49:10,592 Epoch 4159: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 08:49:10,593 EPOCH 4160
2024-02-05 08:49:24,808 Epoch 4160: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 08:49:24,808 EPOCH 4161
2024-02-05 08:49:38,674 Epoch 4161: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 08:49:38,675 EPOCH 4162
2024-02-05 08:49:52,466 Epoch 4162: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 08:49:52,467 EPOCH 4163
2024-02-05 08:50:06,289 Epoch 4163: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 08:50:06,290 EPOCH 4164
2024-02-05 08:50:20,422 Epoch 4164: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 08:50:20,422 EPOCH 4165
2024-02-05 08:50:34,553 Epoch 4165: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 08:50:34,554 EPOCH 4166
2024-02-05 08:50:48,655 Epoch 4166: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 08:50:48,656 EPOCH 4167
2024-02-05 08:51:00,691 [Epoch: 4167 Step: 00037500] Batch Recognition Loss:   0.000137 => Gls Tokens per Sec:      564 || Batch Translation Loss:   0.024200 => Txt Tokens per Sec:     1577 || Lr: 0.000050
2024-02-05 08:51:02,469 Epoch 4167: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 08:51:02,469 EPOCH 4168
2024-02-05 08:51:16,482 Epoch 4168: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 08:51:16,483 EPOCH 4169
2024-02-05 08:51:30,479 Epoch 4169: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 08:51:30,480 EPOCH 4170
2024-02-05 08:51:44,443 Epoch 4170: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 08:51:44,443 EPOCH 4171
2024-02-05 08:51:57,819 Epoch 4171: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 08:51:57,819 EPOCH 4172
2024-02-05 08:52:11,799 Epoch 4172: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 08:52:11,800 EPOCH 4173
2024-02-05 08:52:25,739 Epoch 4173: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 08:52:25,740 EPOCH 4174
2024-02-05 08:52:39,749 Epoch 4174: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 08:52:39,750 EPOCH 4175
2024-02-05 08:52:53,610 Epoch 4175: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 08:52:53,610 EPOCH 4176
2024-02-05 08:53:07,656 Epoch 4176: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-05 08:53:07,656 EPOCH 4177
2024-02-05 08:53:21,554 Epoch 4177: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-05 08:53:21,555 EPOCH 4178
2024-02-05 08:53:33,530 [Epoch: 4178 Step: 00037600] Batch Recognition Loss:   0.000135 => Gls Tokens per Sec:      674 || Batch Translation Loss:   0.122201 => Txt Tokens per Sec:     1835 || Lr: 0.000050
2024-02-05 08:53:35,732 Epoch 4178: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-05 08:53:35,732 EPOCH 4179
2024-02-05 08:53:49,467 Epoch 4179: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-05 08:53:49,468 EPOCH 4180
2024-02-05 08:54:03,210 Epoch 4180: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-05 08:54:03,211 EPOCH 4181
2024-02-05 08:54:16,966 Epoch 4181: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 08:54:16,966 EPOCH 4182
2024-02-05 08:54:31,232 Epoch 4182: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 08:54:31,233 EPOCH 4183
2024-02-05 08:54:44,950 Epoch 4183: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 08:54:44,951 EPOCH 4184
2024-02-05 08:54:58,621 Epoch 4184: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 08:54:58,621 EPOCH 4185
2024-02-05 08:55:12,782 Epoch 4185: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 08:55:12,783 EPOCH 4186
2024-02-05 08:55:26,658 Epoch 4186: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 08:55:26,659 EPOCH 4187
2024-02-05 08:55:40,535 Epoch 4187: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 08:55:40,535 EPOCH 4188
2024-02-05 08:55:54,575 Epoch 4188: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 08:55:54,575 EPOCH 4189
2024-02-05 08:56:08,416 [Epoch: 4189 Step: 00037700] Batch Recognition Loss:   0.000104 => Gls Tokens per Sec:      676 || Batch Translation Loss:   0.013292 => Txt Tokens per Sec:     1968 || Lr: 0.000050
2024-02-05 08:56:08,664 Epoch 4189: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 08:56:08,664 EPOCH 4190
2024-02-05 08:56:22,291 Epoch 4190: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 08:56:22,292 EPOCH 4191
2024-02-05 08:56:36,639 Epoch 4191: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 08:56:36,640 EPOCH 4192
2024-02-05 08:56:50,461 Epoch 4192: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 08:56:50,462 EPOCH 4193
2024-02-05 08:57:04,155 Epoch 4193: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 08:57:04,156 EPOCH 4194
2024-02-05 08:57:17,945 Epoch 4194: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-05 08:57:17,946 EPOCH 4195
2024-02-05 08:57:31,925 Epoch 4195: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-05 08:57:31,925 EPOCH 4196
2024-02-05 08:57:45,621 Epoch 4196: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-05 08:57:45,622 EPOCH 4197
2024-02-05 08:57:59,788 Epoch 4197: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-05 08:57:59,789 EPOCH 4198
2024-02-05 08:58:13,535 Epoch 4198: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-05 08:58:13,536 EPOCH 4199
2024-02-05 08:58:27,479 Epoch 4199: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-05 08:58:27,480 EPOCH 4200
2024-02-05 08:58:41,565 [Epoch: 4200 Step: 00037800] Batch Recognition Loss:   0.000257 => Gls Tokens per Sec:      755 || Batch Translation Loss:   0.010186 => Txt Tokens per Sec:     2095 || Lr: 0.000050
2024-02-05 08:58:41,566 Epoch 4200: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-05 08:58:41,566 EPOCH 4201
2024-02-05 08:58:55,769 Epoch 4201: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-05 08:58:55,769 EPOCH 4202
2024-02-05 08:59:09,599 Epoch 4202: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 08:59:09,599 EPOCH 4203
2024-02-05 08:59:23,342 Epoch 4203: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 08:59:23,343 EPOCH 4204
2024-02-05 08:59:37,252 Epoch 4204: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 08:59:37,253 EPOCH 4205
2024-02-05 08:59:51,030 Epoch 4205: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 08:59:51,031 EPOCH 4206
2024-02-05 09:00:05,267 Epoch 4206: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 09:00:05,268 EPOCH 4207
2024-02-05 09:00:19,070 Epoch 4207: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 09:00:19,071 EPOCH 4208
2024-02-05 09:00:33,100 Epoch 4208: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 09:00:33,100 EPOCH 4209
2024-02-05 09:00:47,097 Epoch 4209: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 09:00:47,098 EPOCH 4210
2024-02-05 09:01:01,098 Epoch 4210: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 09:01:01,098 EPOCH 4211
2024-02-05 09:01:15,297 Epoch 4211: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 09:01:15,298 EPOCH 4212
2024-02-05 09:01:15,551 [Epoch: 4212 Step: 00037900] Batch Recognition Loss:   0.000119 => Gls Tokens per Sec:     5079 || Batch Translation Loss:   0.009947 => Txt Tokens per Sec:     9016 || Lr: 0.000050
2024-02-05 09:01:29,467 Epoch 4212: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 09:01:29,468 EPOCH 4213
2024-02-05 09:01:43,432 Epoch 4213: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 09:01:43,433 EPOCH 4214
2024-02-05 09:01:57,401 Epoch 4214: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 09:01:57,402 EPOCH 4215
2024-02-05 09:02:11,187 Epoch 4215: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 09:02:11,188 EPOCH 4216
2024-02-05 09:02:25,347 Epoch 4216: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 09:02:25,348 EPOCH 4217
2024-02-05 09:02:39,375 Epoch 4217: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 09:02:39,376 EPOCH 4218
2024-02-05 09:02:53,289 Epoch 4218: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 09:02:53,290 EPOCH 4219
2024-02-05 09:03:07,193 Epoch 4219: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 09:03:07,193 EPOCH 4220
2024-02-05 09:03:21,452 Epoch 4220: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 09:03:21,453 EPOCH 4221
2024-02-05 09:03:35,556 Epoch 4221: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 09:03:35,557 EPOCH 4222
2024-02-05 09:03:49,654 Epoch 4222: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 09:03:49,654 EPOCH 4223
2024-02-05 09:03:54,437 [Epoch: 4223 Step: 00038000] Batch Recognition Loss:   0.000135 => Gls Tokens per Sec:      349 || Batch Translation Loss:   0.018255 => Txt Tokens per Sec:     1025 || Lr: 0.000050
2024-02-05 09:04:23,287 Validation result at epoch 4223, step    38000: duration: 28.8486s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00036	Translation Loss: 102211.31250	PPL: 27675.18555
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.75	(BLEU-1: 11.30,	BLEU-2: 3.47,	BLEU-3: 1.47,	BLEU-4: 0.75)
	CHRF 17.59	ROUGE 9.40
2024-02-05 09:04:23,288 Logging Recognition and Translation Outputs
2024-02-05 09:04:23,288 ========================================================================================================================
2024-02-05 09:04:23,289 Logging Sequence: 72_194.00
2024-02-05 09:04:23,289 	Gloss Reference :	A B+C+D+E
2024-02-05 09:04:23,289 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 09:04:23,289 	Gloss Alignment :	         
2024-02-05 09:04:23,289 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 09:04:23,291 	Text Reference  :	***** **** shah told her to    do  what she  wants   and filed a police complaint against her      
2024-02-05 09:04:23,291 	Text Hypothesis :	babar kept me   tell you there and they have decided to  such  a ****** huge      fan     following
2024-02-05 09:04:23,291 	Text Alignment  :	I     I    S    S    S   S     S   S    S    S       S   S       D      S         S       S        
2024-02-05 09:04:23,292 ========================================================================================================================
2024-02-05 09:04:23,292 Logging Sequence: 108_59.00
2024-02-05 09:04:23,292 	Gloss Reference :	A B+C+D+E
2024-02-05 09:04:23,292 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 09:04:23,292 	Gloss Alignment :	         
2024-02-05 09:04:23,292 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 09:04:23,294 	Text Reference  :	ishan kishan remained the  biggest buy    of  ipl     as  mumbai indians paid    a    whopping rs 1525 crore to keep him
2024-02-05 09:04:23,294 	Text Hypothesis :	in    the    end      rpsg group   placed the winning bid for    the     lucknow team for      rs 7090 crore ** **** ***
2024-02-05 09:04:23,295 	Text Alignment  :	S     S      S        S    S       S      S   S       S   S      S       S       S    S           S          D  D    D  
2024-02-05 09:04:23,295 ========================================================================================================================
2024-02-05 09:04:23,295 Logging Sequence: 109_10.00
2024-02-05 09:04:23,295 	Gloss Reference :	A B+C+D+E
2024-02-05 09:04:23,295 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 09:04:23,295 	Gloss Alignment :	         
2024-02-05 09:04:23,295 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 09:04:23,297 	Text Reference  :	was     scheduled to *** ***** ** **** *** be played at   the narendra modi stadium in  ahmedabad
2024-02-05 09:04:23,297 	Text Hypothesis :	however up        to the match we have kkr ms dhoni  will be  banned   are  on      the players  
2024-02-05 09:04:23,297 	Text Alignment  :	S       S            I   I     I  I    I   S  S      S    S   S        S    S       S   S        
2024-02-05 09:04:23,297 ========================================================================================================================
2024-02-05 09:04:23,297 Logging Sequence: 103_202.00
2024-02-05 09:04:23,297 	Gloss Reference :	A B+C+D+E
2024-02-05 09:04:23,298 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 09:04:23,298 	Gloss Alignment :	         
2024-02-05 09:04:23,298 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 09:04:23,300 	Text Reference  :	***** **** ** *** ******** **** india    in    total has won     61 medals including 22 gold medals 16  silver medals 23    bronze medals
2024-02-05 09:04:23,300 	Text Hypothesis :	since then on 8th december 2021 replaced kohli as    odi captain as well   let       me tell you    can watch  the    right pad    bcci  
2024-02-05 09:04:23,300 	Text Alignment  :	I     I    I  I   I        I    S        S     S     S   S       S  S      S         S  S    S      S   S      S      S     S      S     
2024-02-05 09:04:23,300 ========================================================================================================================
2024-02-05 09:04:23,300 Logging Sequence: 149_77.00
2024-02-05 09:04:23,301 	Gloss Reference :	A B+C+D+E
2024-02-05 09:04:23,301 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 09:04:23,301 	Gloss Alignment :	         
2024-02-05 09:04:23,301 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 09:04:23,303 	Text Reference  :	and arrested danushka for alleged sexual assault of    a   29   year old       woman whose name has      not been disclosed
2024-02-05 09:04:23,303 	Text Hypothesis :	*** since    there    was no      one    day     which has left her  happiness while kaif  and  everyone is  also started' 
2024-02-05 09:04:23,303 	Text Alignment  :	D   S        S        S   S       S      S       S     S   S    S    S         S     S     S    S        S   S    S        
2024-02-05 09:04:23,303 ========================================================================================================================
2024-02-05 09:04:32,706 Epoch 4223: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 09:04:32,707 EPOCH 4224
2024-02-05 09:04:46,398 Epoch 4224: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 09:04:46,399 EPOCH 4225
2024-02-05 09:05:00,476 Epoch 4225: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 09:05:00,476 EPOCH 4226
2024-02-05 09:05:14,069 Epoch 4226: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 09:05:14,070 EPOCH 4227
2024-02-05 09:05:28,040 Epoch 4227: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 09:05:28,040 EPOCH 4228
2024-02-05 09:05:42,007 Epoch 4228: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 09:05:42,007 EPOCH 4229
2024-02-05 09:05:55,841 Epoch 4229: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 09:05:55,841 EPOCH 4230
2024-02-05 09:06:09,518 Epoch 4230: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 09:06:09,518 EPOCH 4231
2024-02-05 09:06:23,665 Epoch 4231: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-05 09:06:23,666 EPOCH 4232
2024-02-05 09:06:37,591 Epoch 4232: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-05 09:06:37,592 EPOCH 4233
2024-02-05 09:06:51,509 Epoch 4233: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 09:06:51,509 EPOCH 4234
2024-02-05 09:06:53,117 [Epoch: 4234 Step: 00038100] Batch Recognition Loss:   0.000172 => Gls Tokens per Sec:     2390 || Batch Translation Loss:   0.041350 => Txt Tokens per Sec:     6624 || Lr: 0.000050
2024-02-05 09:07:05,288 Epoch 4234: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-05 09:07:05,289 EPOCH 4235
2024-02-05 09:07:19,385 Epoch 4235: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-05 09:07:19,386 EPOCH 4236
2024-02-05 09:07:33,170 Epoch 4236: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-05 09:07:33,170 EPOCH 4237
2024-02-05 09:07:47,252 Epoch 4237: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 09:07:47,253 EPOCH 4238
2024-02-05 09:08:01,275 Epoch 4238: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-05 09:08:01,276 EPOCH 4239
2024-02-05 09:08:15,194 Epoch 4239: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 09:08:15,195 EPOCH 4240
2024-02-05 09:08:29,286 Epoch 4240: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 09:08:29,287 EPOCH 4241
2024-02-05 09:08:43,177 Epoch 4241: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 09:08:43,178 EPOCH 4242
2024-02-05 09:08:56,996 Epoch 4242: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 09:08:56,996 EPOCH 4243
2024-02-05 09:09:10,311 Epoch 4243: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 09:09:10,311 EPOCH 4244
2024-02-05 09:09:24,627 Epoch 4244: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 09:09:24,628 EPOCH 4245
2024-02-05 09:09:31,288 [Epoch: 4245 Step: 00038200] Batch Recognition Loss:   0.000117 => Gls Tokens per Sec:      635 || Batch Translation Loss:   0.012727 => Txt Tokens per Sec:     1644 || Lr: 0.000050
2024-02-05 09:09:38,609 Epoch 4245: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 09:09:38,610 EPOCH 4246
2024-02-05 09:09:52,368 Epoch 4246: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 09:09:52,368 EPOCH 4247
2024-02-05 09:10:06,368 Epoch 4247: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 09:10:06,368 EPOCH 4248
2024-02-05 09:10:20,205 Epoch 4248: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 09:10:20,206 EPOCH 4249
2024-02-05 09:10:33,858 Epoch 4249: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 09:10:33,859 EPOCH 4250
2024-02-05 09:10:47,971 Epoch 4250: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 09:10:47,971 EPOCH 4251
2024-02-05 09:11:01,995 Epoch 4251: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 09:11:01,996 EPOCH 4252
2024-02-05 09:11:15,959 Epoch 4252: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 09:11:15,960 EPOCH 4253
2024-02-05 09:11:30,197 Epoch 4253: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 09:11:30,198 EPOCH 4254
2024-02-05 09:11:43,983 Epoch 4254: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 09:11:43,984 EPOCH 4255
2024-02-05 09:11:57,882 Epoch 4255: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 09:11:57,883 EPOCH 4256
2024-02-05 09:12:04,508 [Epoch: 4256 Step: 00038300] Batch Recognition Loss:   0.000107 => Gls Tokens per Sec:      966 || Batch Translation Loss:   0.012764 => Txt Tokens per Sec:     2528 || Lr: 0.000050
2024-02-05 09:12:11,623 Epoch 4256: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 09:12:11,623 EPOCH 4257
2024-02-05 09:12:25,516 Epoch 4257: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 09:12:25,516 EPOCH 4258
2024-02-05 09:12:39,329 Epoch 4258: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 09:12:39,330 EPOCH 4259
2024-02-05 09:12:53,230 Epoch 4259: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 09:12:53,231 EPOCH 4260
2024-02-05 09:13:07,210 Epoch 4260: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 09:13:07,211 EPOCH 4261
2024-02-05 09:13:21,215 Epoch 4261: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 09:13:21,215 EPOCH 4262
2024-02-05 09:13:35,204 Epoch 4262: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 09:13:35,204 EPOCH 4263
2024-02-05 09:13:49,131 Epoch 4263: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 09:13:49,132 EPOCH 4264
2024-02-05 09:14:02,950 Epoch 4264: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 09:14:02,950 EPOCH 4265
2024-02-05 09:14:17,114 Epoch 4265: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 09:14:17,115 EPOCH 4266
2024-02-05 09:14:31,012 Epoch 4266: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 09:14:31,013 EPOCH 4267
2024-02-05 09:14:38,730 [Epoch: 4267 Step: 00038400] Batch Recognition Loss:   0.000123 => Gls Tokens per Sec:      880 || Batch Translation Loss:   0.016472 => Txt Tokens per Sec:     2375 || Lr: 0.000050
2024-02-05 09:14:44,556 Epoch 4267: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 09:14:44,557 EPOCH 4268
2024-02-05 09:14:58,554 Epoch 4268: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 09:14:58,554 EPOCH 4269
2024-02-05 09:15:12,170 Epoch 4269: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 09:15:12,170 EPOCH 4270
2024-02-05 09:15:26,233 Epoch 4270: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 09:15:26,234 EPOCH 4271
2024-02-05 09:15:40,299 Epoch 4271: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-05 09:15:40,300 EPOCH 4272
2024-02-05 09:15:54,370 Epoch 4272: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-05 09:15:54,371 EPOCH 4273
2024-02-05 09:16:08,356 Epoch 4273: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 09:16:08,356 EPOCH 4274
2024-02-05 09:16:22,258 Epoch 4274: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 09:16:22,259 EPOCH 4275
2024-02-05 09:16:36,293 Epoch 4275: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 09:16:36,293 EPOCH 4276
2024-02-05 09:16:50,172 Epoch 4276: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 09:16:50,173 EPOCH 4277
2024-02-05 09:17:04,251 Epoch 4277: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 09:17:04,251 EPOCH 4278
2024-02-05 09:17:15,824 [Epoch: 4278 Step: 00038500] Batch Recognition Loss:   0.000153 => Gls Tokens per Sec:      697 || Batch Translation Loss:   0.016063 => Txt Tokens per Sec:     1921 || Lr: 0.000050
2024-02-05 09:17:17,954 Epoch 4278: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 09:17:17,955 EPOCH 4279
2024-02-05 09:17:32,328 Epoch 4279: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 09:17:32,328 EPOCH 4280
2024-02-05 09:17:46,552 Epoch 4280: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 09:17:46,552 EPOCH 4281
2024-02-05 09:18:00,384 Epoch 4281: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 09:18:00,384 EPOCH 4282
2024-02-05 09:18:14,360 Epoch 4282: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 09:18:14,360 EPOCH 4283
2024-02-05 09:18:28,098 Epoch 4283: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 09:18:28,099 EPOCH 4284
2024-02-05 09:18:42,186 Epoch 4284: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 09:18:42,186 EPOCH 4285
2024-02-05 09:18:55,910 Epoch 4285: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 09:18:55,911 EPOCH 4286
2024-02-05 09:19:09,892 Epoch 4286: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 09:19:09,893 EPOCH 4287
2024-02-05 09:19:23,823 Epoch 4287: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 09:19:23,824 EPOCH 4288
2024-02-05 09:19:37,594 Epoch 4288: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 09:19:37,595 EPOCH 4289
2024-02-05 09:19:50,895 [Epoch: 4289 Step: 00038600] Batch Recognition Loss:   0.000146 => Gls Tokens per Sec:      703 || Batch Translation Loss:   0.020898 => Txt Tokens per Sec:     1955 || Lr: 0.000050
2024-02-05 09:19:51,508 Epoch 4289: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 09:19:51,509 EPOCH 4290
2024-02-05 09:20:05,512 Epoch 4290: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 09:20:05,512 EPOCH 4291
2024-02-05 09:20:19,769 Epoch 4291: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 09:20:19,770 EPOCH 4292
2024-02-05 09:20:33,605 Epoch 4292: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 09:20:33,606 EPOCH 4293
2024-02-05 09:20:47,541 Epoch 4293: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 09:20:47,542 EPOCH 4294
2024-02-05 09:21:01,471 Epoch 4294: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 09:21:01,471 EPOCH 4295
2024-02-05 09:21:15,212 Epoch 4295: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 09:21:15,213 EPOCH 4296
2024-02-05 09:21:29,087 Epoch 4296: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 09:21:29,087 EPOCH 4297
2024-02-05 09:21:42,870 Epoch 4297: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 09:21:42,870 EPOCH 4298
2024-02-05 09:21:56,861 Epoch 4298: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-05 09:21:56,862 EPOCH 4299
2024-02-05 09:22:10,770 Epoch 4299: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-05 09:22:10,770 EPOCH 4300
2024-02-05 09:22:24,650 [Epoch: 4300 Step: 00038700] Batch Recognition Loss:   0.000259 => Gls Tokens per Sec:      766 || Batch Translation Loss:   0.013730 => Txt Tokens per Sec:     2126 || Lr: 0.000050
2024-02-05 09:22:24,651 Epoch 4300: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-05 09:22:24,651 EPOCH 4301
2024-02-05 09:22:38,637 Epoch 4301: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-05 09:22:38,637 EPOCH 4302
2024-02-05 09:22:52,853 Epoch 4302: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-05 09:22:52,853 EPOCH 4303
2024-02-05 09:23:06,817 Epoch 4303: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-05 09:23:06,818 EPOCH 4304
2024-02-05 09:23:21,086 Epoch 4304: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-05 09:23:21,086 EPOCH 4305
2024-02-05 09:23:35,086 Epoch 4305: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-05 09:23:35,086 EPOCH 4306
2024-02-05 09:23:49,294 Epoch 4306: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 09:23:49,294 EPOCH 4307
2024-02-05 09:24:03,302 Epoch 4307: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-05 09:24:03,302 EPOCH 4308
2024-02-05 09:24:17,568 Epoch 4308: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-05 09:24:17,569 EPOCH 4309
2024-02-05 09:24:31,398 Epoch 4309: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-05 09:24:31,398 EPOCH 4310
2024-02-05 09:24:45,351 Epoch 4310: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.87 
2024-02-05 09:24:45,352 EPOCH 4311
2024-02-05 09:24:59,507 Epoch 4311: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.19 
2024-02-05 09:24:59,507 EPOCH 4312
2024-02-05 09:25:01,247 [Epoch: 4312 Step: 00038800] Batch Recognition Loss:   0.002160 => Gls Tokens per Sec:      736 || Batch Translation Loss:   0.398802 => Txt Tokens per Sec:     2380 || Lr: 0.000050
2024-02-05 09:25:13,399 Epoch 4312: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.64 
2024-02-05 09:25:13,399 EPOCH 4313
2024-02-05 09:25:27,406 Epoch 4313: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-05 09:25:27,407 EPOCH 4314
2024-02-05 09:25:41,527 Epoch 4314: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-05 09:25:41,527 EPOCH 4315
2024-02-05 09:25:55,699 Epoch 4315: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-05 09:25:55,700 EPOCH 4316
2024-02-05 09:26:09,560 Epoch 4316: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-05 09:26:09,561 EPOCH 4317
2024-02-05 09:26:23,375 Epoch 4317: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-05 09:26:23,376 EPOCH 4318
2024-02-05 09:26:37,225 Epoch 4318: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 09:26:37,226 EPOCH 4319
2024-02-05 09:26:50,978 Epoch 4319: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 09:26:50,979 EPOCH 4320
2024-02-05 09:27:05,198 Epoch 4320: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 09:27:05,198 EPOCH 4321
2024-02-05 09:27:18,977 Epoch 4321: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 09:27:18,977 EPOCH 4322
2024-02-05 09:27:32,881 Epoch 4322: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 09:27:32,881 EPOCH 4323
2024-02-05 09:27:33,963 [Epoch: 4323 Step: 00038900] Batch Recognition Loss:   0.000150 => Gls Tokens per Sec:     2368 || Batch Translation Loss:   0.016815 => Txt Tokens per Sec:     6504 || Lr: 0.000050
2024-02-05 09:27:46,805 Epoch 4323: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 09:27:46,805 EPOCH 4324
2024-02-05 09:28:00,711 Epoch 4324: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 09:28:00,711 EPOCH 4325
2024-02-05 09:28:14,639 Epoch 4325: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 09:28:14,640 EPOCH 4326
2024-02-05 09:28:28,475 Epoch 4326: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-05 09:28:28,475 EPOCH 4327
2024-02-05 09:28:42,675 Epoch 4327: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 09:28:42,676 EPOCH 4328
2024-02-05 09:28:56,752 Epoch 4328: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 09:28:56,753 EPOCH 4329
2024-02-05 09:29:10,265 Epoch 4329: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 09:29:10,266 EPOCH 4330
2024-02-05 09:29:24,313 Epoch 4330: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 09:29:24,313 EPOCH 4331
2024-02-05 09:29:37,845 Epoch 4331: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 09:29:37,846 EPOCH 4332
2024-02-05 09:29:51,824 Epoch 4332: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 09:29:51,825 EPOCH 4333
2024-02-05 09:30:06,148 Epoch 4333: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 09:30:06,149 EPOCH 4334
2024-02-05 09:30:11,227 [Epoch: 4334 Step: 00039000] Batch Recognition Loss:   0.000134 => Gls Tokens per Sec:      581 || Batch Translation Loss:   0.018345 => Txt Tokens per Sec:     1410 || Lr: 0.000050
2024-02-05 09:30:20,035 Epoch 4334: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 09:30:20,036 EPOCH 4335
2024-02-05 09:30:34,065 Epoch 4335: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 09:30:34,065 EPOCH 4336
2024-02-05 09:30:47,973 Epoch 4336: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 09:30:47,974 EPOCH 4337
2024-02-05 09:31:02,155 Epoch 4337: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 09:31:02,155 EPOCH 4338
2024-02-05 09:31:15,899 Epoch 4338: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 09:31:15,900 EPOCH 4339
2024-02-05 09:31:29,670 Epoch 4339: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 09:31:29,671 EPOCH 4340
2024-02-05 09:31:43,407 Epoch 4340: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 09:31:43,408 EPOCH 4341
2024-02-05 09:31:57,462 Epoch 4341: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 09:31:57,462 EPOCH 4342
2024-02-05 09:32:11,115 Epoch 4342: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 09:32:11,116 EPOCH 4343
2024-02-05 09:32:25,167 Epoch 4343: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-05 09:32:25,168 EPOCH 4344
2024-02-05 09:32:39,202 Epoch 4344: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-05 09:32:39,203 EPOCH 4345
2024-02-05 09:32:42,189 [Epoch: 4345 Step: 00039100] Batch Recognition Loss:   0.000120 => Gls Tokens per Sec:     1715 || Batch Translation Loss:   0.014221 => Txt Tokens per Sec:     4193 || Lr: 0.000050
2024-02-05 09:32:53,503 Epoch 4345: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-05 09:32:53,504 EPOCH 4346
2024-02-05 09:33:07,495 Epoch 4346: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 09:33:07,496 EPOCH 4347
2024-02-05 09:33:21,281 Epoch 4347: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 09:33:21,281 EPOCH 4348
2024-02-05 09:33:35,122 Epoch 4348: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 09:33:35,123 EPOCH 4349
2024-02-05 09:33:48,916 Epoch 4349: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 09:33:48,916 EPOCH 4350
2024-02-05 09:34:02,936 Epoch 4350: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 09:34:02,937 EPOCH 4351
2024-02-05 09:34:16,819 Epoch 4351: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 09:34:16,819 EPOCH 4352
2024-02-05 09:34:30,890 Epoch 4352: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 09:34:30,891 EPOCH 4353
2024-02-05 09:34:44,950 Epoch 4353: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 09:34:44,951 EPOCH 4354
2024-02-05 09:34:59,018 Epoch 4354: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 09:34:59,019 EPOCH 4355
2024-02-05 09:35:13,064 Epoch 4355: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 09:35:13,064 EPOCH 4356
2024-02-05 09:35:21,616 [Epoch: 4356 Step: 00039200] Batch Recognition Loss:   0.000167 => Gls Tokens per Sec:      749 || Batch Translation Loss:   0.020741 => Txt Tokens per Sec:     2159 || Lr: 0.000050
2024-02-05 09:35:27,378 Epoch 4356: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 09:35:27,379 EPOCH 4357
2024-02-05 09:35:41,383 Epoch 4357: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 09:35:41,383 EPOCH 4358
2024-02-05 09:35:55,005 Epoch 4358: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 09:35:55,006 EPOCH 4359
2024-02-05 09:36:08,824 Epoch 4359: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 09:36:08,825 EPOCH 4360
2024-02-05 09:36:23,044 Epoch 4360: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 09:36:23,044 EPOCH 4361
2024-02-05 09:36:37,308 Epoch 4361: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 09:36:37,309 EPOCH 4362
2024-02-05 09:36:51,335 Epoch 4362: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 09:36:51,336 EPOCH 4363
2024-02-05 09:37:05,003 Epoch 4363: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 09:37:05,004 EPOCH 4364
2024-02-05 09:37:19,128 Epoch 4364: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 09:37:19,129 EPOCH 4365
2024-02-05 09:37:33,430 Epoch 4365: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 09:37:33,430 EPOCH 4366
2024-02-05 09:37:47,375 Epoch 4366: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 09:37:47,376 EPOCH 4367
2024-02-05 09:37:58,572 [Epoch: 4367 Step: 00039300] Batch Recognition Loss:   0.000148 => Gls Tokens per Sec:      607 || Batch Translation Loss:   0.017964 => Txt Tokens per Sec:     1721 || Lr: 0.000050
2024-02-05 09:38:01,201 Epoch 4367: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 09:38:01,202 EPOCH 4368
2024-02-05 09:38:15,336 Epoch 4368: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 09:38:15,337 EPOCH 4369
2024-02-05 09:38:29,065 Epoch 4369: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 09:38:29,066 EPOCH 4370
2024-02-05 09:38:43,388 Epoch 4370: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 09:38:43,388 EPOCH 4371
2024-02-05 09:38:57,215 Epoch 4371: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 09:38:57,216 EPOCH 4372
2024-02-05 09:39:11,126 Epoch 4372: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 09:39:11,126 EPOCH 4373
2024-02-05 09:39:25,045 Epoch 4373: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 09:39:25,046 EPOCH 4374
2024-02-05 09:39:39,427 Epoch 4374: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 09:39:39,428 EPOCH 4375
2024-02-05 09:39:53,409 Epoch 4375: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 09:39:53,409 EPOCH 4376
2024-02-05 09:40:07,518 Epoch 4376: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 09:40:07,518 EPOCH 4377
2024-02-05 09:40:21,351 Epoch 4377: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 09:40:21,352 EPOCH 4378
2024-02-05 09:40:33,165 [Epoch: 4378 Step: 00039400] Batch Recognition Loss:   0.000175 => Gls Tokens per Sec:      683 || Batch Translation Loss:   0.006694 => Txt Tokens per Sec:     1882 || Lr: 0.000050
2024-02-05 09:40:35,335 Epoch 4378: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 09:40:35,335 EPOCH 4379
2024-02-05 09:40:49,244 Epoch 4379: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 09:40:49,245 EPOCH 4380
2024-02-05 09:41:03,225 Epoch 4380: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 09:41:03,226 EPOCH 4381
2024-02-05 09:41:17,374 Epoch 4381: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 09:41:17,375 EPOCH 4382
2024-02-05 09:41:31,545 Epoch 4382: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 09:41:31,545 EPOCH 4383
2024-02-05 09:41:45,365 Epoch 4383: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 09:41:45,366 EPOCH 4384
2024-02-05 09:41:59,350 Epoch 4384: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 09:41:59,350 EPOCH 4385
2024-02-05 09:42:13,286 Epoch 4385: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 09:42:13,286 EPOCH 4386
2024-02-05 09:42:27,036 Epoch 4386: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 09:42:27,037 EPOCH 4387
2024-02-05 09:42:41,081 Epoch 4387: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 09:42:41,081 EPOCH 4388
2024-02-05 09:42:55,192 Epoch 4388: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 09:42:55,192 EPOCH 4389
2024-02-05 09:43:04,730 [Epoch: 4389 Step: 00039500] Batch Recognition Loss:   0.000121 => Gls Tokens per Sec:     1074 || Batch Translation Loss:   0.021118 => Txt Tokens per Sec:     2948 || Lr: 0.000050
2024-02-05 09:43:09,004 Epoch 4389: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 09:43:09,004 EPOCH 4390
2024-02-05 09:43:22,898 Epoch 4390: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 09:43:22,899 EPOCH 4391
2024-02-05 09:43:36,807 Epoch 4391: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 09:43:36,807 EPOCH 4392
2024-02-05 09:43:50,917 Epoch 4392: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 09:43:50,918 EPOCH 4393
2024-02-05 09:44:04,878 Epoch 4393: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 09:44:04,878 EPOCH 4394
2024-02-05 09:44:19,128 Epoch 4394: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 09:44:19,128 EPOCH 4395
2024-02-05 09:44:33,274 Epoch 4395: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 09:44:33,274 EPOCH 4396
2024-02-05 09:44:46,849 Epoch 4396: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 09:44:46,850 EPOCH 4397
2024-02-05 09:45:00,833 Epoch 4397: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 09:45:00,834 EPOCH 4398
2024-02-05 09:45:14,701 Epoch 4398: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 09:45:14,702 EPOCH 4399
2024-02-05 09:45:28,563 Epoch 4399: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 09:45:28,563 EPOCH 4400
2024-02-05 09:45:42,585 [Epoch: 4400 Step: 00039600] Batch Recognition Loss:   0.000089 => Gls Tokens per Sec:      758 || Batch Translation Loss:   0.010725 => Txt Tokens per Sec:     2105 || Lr: 0.000050
2024-02-05 09:45:42,585 Epoch 4400: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 09:45:42,586 EPOCH 4401
2024-02-05 09:45:56,592 Epoch 4401: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 09:45:56,593 EPOCH 4402
2024-02-05 09:46:10,564 Epoch 4402: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 09:46:10,565 EPOCH 4403
2024-02-05 09:46:24,281 Epoch 4403: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 09:46:24,282 EPOCH 4404
2024-02-05 09:46:38,119 Epoch 4404: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 09:46:38,120 EPOCH 4405
2024-02-05 09:46:52,240 Epoch 4405: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 09:46:52,241 EPOCH 4406
2024-02-05 09:47:06,261 Epoch 4406: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 09:47:06,262 EPOCH 4407
2024-02-05 09:47:20,241 Epoch 4407: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 09:47:20,241 EPOCH 4408
2024-02-05 09:47:34,322 Epoch 4408: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 09:47:34,323 EPOCH 4409
2024-02-05 09:47:48,384 Epoch 4409: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 09:47:48,384 EPOCH 4410
2024-02-05 09:48:02,920 Epoch 4410: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 09:48:02,921 EPOCH 4411
2024-02-05 09:48:16,735 Epoch 4411: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 09:48:16,735 EPOCH 4412
2024-02-05 09:48:18,694 [Epoch: 4412 Step: 00039700] Batch Recognition Loss:   0.000126 => Gls Tokens per Sec:      654 || Batch Translation Loss:   0.014466 => Txt Tokens per Sec:     2084 || Lr: 0.000050
2024-02-05 09:48:30,847 Epoch 4412: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 09:48:30,848 EPOCH 4413
2024-02-05 09:48:44,932 Epoch 4413: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 09:48:44,932 EPOCH 4414
2024-02-05 09:48:58,870 Epoch 4414: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 09:48:58,871 EPOCH 4415
2024-02-05 09:49:12,857 Epoch 4415: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 09:49:12,858 EPOCH 4416
2024-02-05 09:49:26,676 Epoch 4416: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 09:49:26,677 EPOCH 4417
2024-02-05 09:49:40,683 Epoch 4417: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 09:49:40,684 EPOCH 4418
2024-02-05 09:49:54,650 Epoch 4418: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 09:49:54,650 EPOCH 4419
2024-02-05 09:50:08,481 Epoch 4419: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 09:50:08,482 EPOCH 4420
2024-02-05 09:50:22,567 Epoch 4420: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 09:50:22,568 EPOCH 4421
2024-02-05 09:50:36,584 Epoch 4421: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 09:50:36,585 EPOCH 4422
2024-02-05 09:50:50,465 Epoch 4422: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 09:50:50,466 EPOCH 4423
2024-02-05 09:50:51,559 [Epoch: 4423 Step: 00039800] Batch Recognition Loss:   0.000125 => Gls Tokens per Sec:     2345 || Batch Translation Loss:   0.024249 => Txt Tokens per Sec:     6290 || Lr: 0.000050
2024-02-05 09:51:04,491 Epoch 4423: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 09:51:04,491 EPOCH 4424
2024-02-05 09:51:18,513 Epoch 4424: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 09:51:18,514 EPOCH 4425
2024-02-05 09:51:32,485 Epoch 4425: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 09:51:32,485 EPOCH 4426
2024-02-05 09:51:46,432 Epoch 4426: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 09:51:46,433 EPOCH 4427
2024-02-05 09:52:00,546 Epoch 4427: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 09:52:00,547 EPOCH 4428
2024-02-05 09:52:14,323 Epoch 4428: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 09:52:14,324 EPOCH 4429
2024-02-05 09:52:28,586 Epoch 4429: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 09:52:28,586 EPOCH 4430
2024-02-05 09:52:42,720 Epoch 4430: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 09:52:42,720 EPOCH 4431
2024-02-05 09:52:56,489 Epoch 4431: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 09:52:56,490 EPOCH 4432
2024-02-05 09:53:10,557 Epoch 4432: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 09:53:10,558 EPOCH 4433
2024-02-05 09:53:24,237 Epoch 4433: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 09:53:24,237 EPOCH 4434
2024-02-05 09:53:30,882 [Epoch: 4434 Step: 00039900] Batch Recognition Loss:   0.000108 => Gls Tokens per Sec:      444 || Batch Translation Loss:   0.011815 => Txt Tokens per Sec:     1350 || Lr: 0.000050
2024-02-05 09:53:38,245 Epoch 4434: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 09:53:38,246 EPOCH 4435
2024-02-05 09:53:52,003 Epoch 4435: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 09:53:52,004 EPOCH 4436
2024-02-05 09:54:06,164 Epoch 4436: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 09:54:06,166 EPOCH 4437
2024-02-05 09:54:19,822 Epoch 4437: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 09:54:19,823 EPOCH 4438
2024-02-05 09:54:33,018 Epoch 4438: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 09:54:33,019 EPOCH 4439
2024-02-05 09:54:46,830 Epoch 4439: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 09:54:46,830 EPOCH 4440
2024-02-05 09:55:00,751 Epoch 4440: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 09:55:00,752 EPOCH 4441
2024-02-05 09:55:14,614 Epoch 4441: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 09:55:14,615 EPOCH 4442
2024-02-05 09:55:28,175 Epoch 4442: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 09:55:28,175 EPOCH 4443
2024-02-05 09:55:42,189 Epoch 4443: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 09:55:42,189 EPOCH 4444
2024-02-05 09:55:55,973 Epoch 4444: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 09:55:55,974 EPOCH 4445
2024-02-05 09:56:02,371 [Epoch: 4445 Step: 00040000] Batch Recognition Loss:   0.000109 => Gls Tokens per Sec:      801 || Batch Translation Loss:   0.014042 => Txt Tokens per Sec:     2189 || Lr: 0.000050
2024-02-05 09:56:31,189 Validation result at epoch 4445, step    40000: duration: 28.8178s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00036	Translation Loss: 102199.35938	PPL: 27642.08398
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.37	(BLEU-1: 10.76,	BLEU-2: 3.03,	BLEU-3: 1.09,	BLEU-4: 0.37)
	CHRF 16.77	ROUGE 9.10
2024-02-05 09:56:31,191 Logging Recognition and Translation Outputs
2024-02-05 09:56:31,191 ========================================================================================================================
2024-02-05 09:56:31,191 Logging Sequence: 123_104.00
2024-02-05 09:56:31,192 	Gloss Reference :	A B+C+D+E
2024-02-05 09:56:31,192 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 09:56:31,192 	Gloss Alignment :	         
2024-02-05 09:56:31,192 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 09:56:31,193 	Text Reference  :	the car    was   presented to  the former india cricketer from an   unknown person 
2024-02-05 09:56:31,194 	Text Hypothesis :	the former india had       won the ****** ***** toss      and  left out     captain
2024-02-05 09:56:31,194 	Text Alignment  :	    S      S     S         S       D      D     S         S    S    S       S      
2024-02-05 09:56:31,194 ========================================================================================================================
2024-02-05 09:56:31,194 Logging Sequence: 107_23.00
2024-02-05 09:56:31,194 	Gloss Reference :	A B+C+D+E
2024-02-05 09:56:31,194 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 09:56:31,195 	Gloss Alignment :	         
2024-02-05 09:56:31,195 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 09:56:31,195 	Text Reference  :	and viktor lilov who is  also    from the    usa   
2024-02-05 09:56:31,195 	Text Hypothesis :	*** ****** ***** if  the matches were became second
2024-02-05 09:56:31,195 	Text Alignment  :	D   D      D     S   S   S       S    S      S     
2024-02-05 09:56:31,196 ========================================================================================================================
2024-02-05 09:56:31,196 Logging Sequence: 134_212.00
2024-02-05 09:56:31,196 	Gloss Reference :	A B+C+D+E
2024-02-05 09:56:31,196 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 09:56:31,196 	Gloss Alignment :	         
2024-02-05 09:56:31,196 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 09:56:31,197 	Text Reference  :	*** dhanush said that he   practises little  yoga   
2024-02-05 09:56:31,197 	Text Hypothesis :	the video   of   the  deaf to        sanjana ganesan
2024-02-05 09:56:31,197 	Text Alignment  :	I   S       S    S    S    S         S       S      
2024-02-05 09:56:31,197 ========================================================================================================================
2024-02-05 09:56:31,197 Logging Sequence: 165_577.00
2024-02-05 09:56:31,198 	Gloss Reference :	A B+C+D+E
2024-02-05 09:56:31,198 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 09:56:31,198 	Gloss Alignment :	         
2024-02-05 09:56:31,198 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 09:56:31,199 	Text Reference  :	** then after 28    years india won the    world cup   again in  2011
2024-02-05 09:56:31,200 	Text Hypothesis :	it is   not   known as    god   of  vamika has   never lets  the team
2024-02-05 09:56:31,200 	Text Alignment  :	I  S    S     S     S     S     S   S      S     S     S     S   S   
2024-02-05 09:56:31,200 ========================================================================================================================
2024-02-05 09:56:31,200 Logging Sequence: 88_142.00
2024-02-05 09:56:31,200 	Gloss Reference :	A B+C+D+E
2024-02-05 09:56:31,200 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 09:56:31,200 	Gloss Alignment :	         
2024-02-05 09:56:31,201 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 09:56:31,201 	Text Reference  :	*** this        is      because the ****** ** ****** police  does  not  do  anything
2024-02-05 09:56:31,202 	Text Hypothesis :	the supermarket belongs to      the family of lionel messi's first over the police  
2024-02-05 09:56:31,202 	Text Alignment  :	I   S           S       S           I      I  I      S       S     S    S   S       
2024-02-05 09:56:31,202 ========================================================================================================================
2024-02-05 09:56:38,898 Epoch 4445: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 09:56:38,898 EPOCH 4446
2024-02-05 09:56:52,915 Epoch 4446: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 09:56:52,915 EPOCH 4447
2024-02-05 09:57:07,000 Epoch 4447: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 09:57:07,001 EPOCH 4448
2024-02-05 09:57:21,067 Epoch 4448: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 09:57:21,068 EPOCH 4449
2024-02-05 09:57:35,045 Epoch 4449: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-05 09:57:35,046 EPOCH 4450
2024-02-05 09:57:48,980 Epoch 4450: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 09:57:48,981 EPOCH 4451
2024-02-05 09:58:02,801 Epoch 4451: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 09:58:02,801 EPOCH 4452
2024-02-05 09:58:16,684 Epoch 4452: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-05 09:58:16,684 EPOCH 4453
2024-02-05 09:58:30,616 Epoch 4453: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-05 09:58:30,617 EPOCH 4454
2024-02-05 09:58:44,588 Epoch 4454: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-05 09:58:44,589 EPOCH 4455
2024-02-05 09:58:58,501 Epoch 4455: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-05 09:58:58,501 EPOCH 4456
2024-02-05 09:59:04,624 [Epoch: 4456 Step: 00040100] Batch Recognition Loss:   0.000157 => Gls Tokens per Sec:      900 || Batch Translation Loss:   0.068795 => Txt Tokens per Sec:     2470 || Lr: 0.000050
2024-02-05 09:59:12,220 Epoch 4456: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-05 09:59:12,220 EPOCH 4457
2024-02-05 09:59:26,422 Epoch 4457: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-05 09:59:26,423 EPOCH 4458
2024-02-05 09:59:40,161 Epoch 4458: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-05 09:59:40,161 EPOCH 4459
2024-02-05 09:59:54,249 Epoch 4459: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-05 09:59:54,249 EPOCH 4460
2024-02-05 10:00:08,570 Epoch 4460: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-05 10:00:08,571 EPOCH 4461
2024-02-05 10:00:22,261 Epoch 4461: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-05 10:00:22,261 EPOCH 4462
2024-02-05 10:00:36,326 Epoch 4462: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-05 10:00:36,326 EPOCH 4463
2024-02-05 10:00:50,179 Epoch 4463: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-05 10:00:50,180 EPOCH 4464
2024-02-05 10:01:03,937 Epoch 4464: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-05 10:01:03,937 EPOCH 4465
2024-02-05 10:01:17,891 Epoch 4465: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-05 10:01:17,892 EPOCH 4466
2024-02-05 10:01:31,681 Epoch 4466: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-05 10:01:31,681 EPOCH 4467
2024-02-05 10:01:44,301 [Epoch: 4467 Step: 00040200] Batch Recognition Loss:   0.000262 => Gls Tokens per Sec:      538 || Batch Translation Loss:   0.041423 => Txt Tokens per Sec:     1606 || Lr: 0.000050
2024-02-05 10:01:45,496 Epoch 4467: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-05 10:01:45,496 EPOCH 4468
2024-02-05 10:01:59,207 Epoch 4468: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-05 10:01:59,207 EPOCH 4469
2024-02-05 10:02:13,373 Epoch 4469: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 10:02:13,373 EPOCH 4470
2024-02-05 10:02:26,959 Epoch 4470: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-05 10:02:26,960 EPOCH 4471
2024-02-05 10:02:41,018 Epoch 4471: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-05 10:02:41,019 EPOCH 4472
2024-02-05 10:02:55,038 Epoch 4472: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-05 10:02:55,038 EPOCH 4473
2024-02-05 10:03:09,103 Epoch 4473: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-05 10:03:09,103 EPOCH 4474
2024-02-05 10:03:23,105 Epoch 4474: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 10:03:23,105 EPOCH 4475
2024-02-05 10:03:37,187 Epoch 4475: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 10:03:37,188 EPOCH 4476
2024-02-05 10:03:51,334 Epoch 4476: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 10:03:51,335 EPOCH 4477
2024-02-05 10:04:05,599 Epoch 4477: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 10:04:05,599 EPOCH 4478
2024-02-05 10:04:13,884 [Epoch: 4478 Step: 00040300] Batch Recognition Loss:   0.000150 => Gls Tokens per Sec:      974 || Batch Translation Loss:   0.025107 => Txt Tokens per Sec:     2568 || Lr: 0.000050
2024-02-05 10:04:19,469 Epoch 4478: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 10:04:19,469 EPOCH 4479
2024-02-05 10:04:33,445 Epoch 4479: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 10:04:33,446 EPOCH 4480
2024-02-05 10:04:47,048 Epoch 4480: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 10:04:47,049 EPOCH 4481
2024-02-05 10:05:01,406 Epoch 4481: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 10:05:01,406 EPOCH 4482
2024-02-05 10:05:15,419 Epoch 4482: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 10:05:15,420 EPOCH 4483
2024-02-05 10:05:29,316 Epoch 4483: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 10:05:29,316 EPOCH 4484
2024-02-05 10:05:43,273 Epoch 4484: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 10:05:43,274 EPOCH 4485
2024-02-05 10:05:57,201 Epoch 4485: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 10:05:57,201 EPOCH 4486
2024-02-05 10:06:11,348 Epoch 4486: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 10:06:11,349 EPOCH 4487
2024-02-05 10:06:25,294 Epoch 4487: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 10:06:25,294 EPOCH 4488
2024-02-05 10:06:39,554 Epoch 4488: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 10:06:39,554 EPOCH 4489
2024-02-05 10:06:53,208 [Epoch: 4489 Step: 00040400] Batch Recognition Loss:   0.000103 => Gls Tokens per Sec:      685 || Batch Translation Loss:   0.014392 => Txt Tokens per Sec:     1943 || Lr: 0.000050
2024-02-05 10:06:53,522 Epoch 4489: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 10:06:53,523 EPOCH 4490
2024-02-05 10:07:07,553 Epoch 4490: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 10:07:07,554 EPOCH 4491
2024-02-05 10:07:21,553 Epoch 4491: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 10:07:21,554 EPOCH 4492
2024-02-05 10:07:35,133 Epoch 4492: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 10:07:35,134 EPOCH 4493
2024-02-05 10:07:49,153 Epoch 4493: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 10:07:49,153 EPOCH 4494
2024-02-05 10:08:03,190 Epoch 4494: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 10:08:03,191 EPOCH 4495
2024-02-05 10:08:17,117 Epoch 4495: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 10:08:17,117 EPOCH 4496
2024-02-05 10:08:31,108 Epoch 4496: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 10:08:31,109 EPOCH 4497
2024-02-05 10:08:44,466 Epoch 4497: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 10:08:44,466 EPOCH 4498
2024-02-05 10:08:58,499 Epoch 4498: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 10:08:58,499 EPOCH 4499
2024-02-05 10:09:12,587 Epoch 4499: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 10:09:12,588 EPOCH 4500
2024-02-05 10:09:26,258 [Epoch: 4500 Step: 00040500] Batch Recognition Loss:   0.000115 => Gls Tokens per Sec:      778 || Batch Translation Loss:   0.013936 => Txt Tokens per Sec:     2159 || Lr: 0.000050
2024-02-05 10:09:26,259 Epoch 4500: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 10:09:26,259 EPOCH 4501
2024-02-05 10:09:40,003 Epoch 4501: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 10:09:40,004 EPOCH 4502
2024-02-05 10:09:54,184 Epoch 4502: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 10:09:54,184 EPOCH 4503
2024-02-05 10:10:08,174 Epoch 4503: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 10:10:08,174 EPOCH 4504
2024-02-05 10:10:21,852 Epoch 4504: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 10:10:21,852 EPOCH 4505
2024-02-05 10:10:36,001 Epoch 4505: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 10:10:36,001 EPOCH 4506
2024-02-05 10:10:49,751 Epoch 4506: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 10:10:49,752 EPOCH 4507
2024-02-05 10:11:03,865 Epoch 4507: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 10:11:03,866 EPOCH 4508
2024-02-05 10:11:17,690 Epoch 4508: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 10:11:17,691 EPOCH 4509
2024-02-05 10:11:31,466 Epoch 4509: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 10:11:31,466 EPOCH 4510
2024-02-05 10:11:45,298 Epoch 4510: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 10:11:45,298 EPOCH 4511
2024-02-05 10:11:59,056 Epoch 4511: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 10:11:59,057 EPOCH 4512
2024-02-05 10:11:59,602 [Epoch: 4512 Step: 00040600] Batch Recognition Loss:   0.000123 => Gls Tokens per Sec:     2356 || Batch Translation Loss:   0.016030 => Txt Tokens per Sec:     6942 || Lr: 0.000050
2024-02-05 10:12:12,928 Epoch 4512: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 10:12:12,928 EPOCH 4513
2024-02-05 10:12:26,770 Epoch 4513: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 10:12:26,771 EPOCH 4514
2024-02-05 10:12:40,711 Epoch 4514: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 10:12:40,712 EPOCH 4515
2024-02-05 10:12:54,705 Epoch 4515: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 10:12:54,706 EPOCH 4516
2024-02-05 10:13:08,710 Epoch 4516: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 10:13:08,710 EPOCH 4517
2024-02-05 10:13:22,378 Epoch 4517: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 10:13:22,379 EPOCH 4518
2024-02-05 10:13:36,564 Epoch 4518: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 10:13:36,564 EPOCH 4519
2024-02-05 10:13:50,439 Epoch 4519: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 10:13:50,440 EPOCH 4520
2024-02-05 10:14:04,351 Epoch 4520: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 10:14:04,352 EPOCH 4521
2024-02-05 10:14:18,087 Epoch 4521: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 10:14:18,088 EPOCH 4522
2024-02-05 10:14:32,090 Epoch 4522: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 10:14:32,091 EPOCH 4523
2024-02-05 10:14:33,060 [Epoch: 4523 Step: 00040700] Batch Recognition Loss:   0.000147 => Gls Tokens per Sec:     2645 || Batch Translation Loss:   0.021304 => Txt Tokens per Sec:     7575 || Lr: 0.000050
2024-02-05 10:14:45,959 Epoch 4523: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 10:14:45,960 EPOCH 4524
2024-02-05 10:14:59,758 Epoch 4524: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 10:14:59,758 EPOCH 4525
2024-02-05 10:15:13,804 Epoch 4525: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 10:15:13,805 EPOCH 4526
2024-02-05 10:15:27,659 Epoch 4526: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-05 10:15:27,659 EPOCH 4527
2024-02-05 10:15:41,712 Epoch 4527: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-05 10:15:41,712 EPOCH 4528
2024-02-05 10:15:55,718 Epoch 4528: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-05 10:15:55,719 EPOCH 4529
2024-02-05 10:16:09,683 Epoch 4529: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 10:16:09,684 EPOCH 4530
2024-02-05 10:16:23,852 Epoch 4530: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-05 10:16:23,853 EPOCH 4531
2024-02-05 10:16:37,826 Epoch 4531: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-05 10:16:37,827 EPOCH 4532
2024-02-05 10:16:51,794 Epoch 4532: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-05 10:16:51,794 EPOCH 4533
2024-02-05 10:17:05,717 Epoch 4533: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-05 10:17:05,717 EPOCH 4534
2024-02-05 10:17:11,147 [Epoch: 4534 Step: 00040800] Batch Recognition Loss:   0.000163 => Gls Tokens per Sec:      543 || Batch Translation Loss:   0.020763 => Txt Tokens per Sec:     1598 || Lr: 0.000050
2024-02-05 10:17:19,749 Epoch 4534: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-05 10:17:19,750 EPOCH 4535
2024-02-05 10:17:33,409 Epoch 4535: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-05 10:17:33,410 EPOCH 4536
2024-02-05 10:17:47,405 Epoch 4536: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 10:17:47,405 EPOCH 4537
2024-02-05 10:18:01,593 Epoch 4537: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 10:18:01,593 EPOCH 4538
2024-02-05 10:18:15,509 Epoch 4538: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 10:18:15,510 EPOCH 4539
2024-02-05 10:18:29,393 Epoch 4539: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 10:18:29,393 EPOCH 4540
2024-02-05 10:18:43,432 Epoch 4540: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 10:18:43,432 EPOCH 4541
2024-02-05 10:18:57,291 Epoch 4541: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 10:18:57,292 EPOCH 4542
2024-02-05 10:19:11,159 Epoch 4542: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 10:19:11,159 EPOCH 4543
2024-02-05 10:19:25,105 Epoch 4543: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 10:19:25,106 EPOCH 4544
2024-02-05 10:19:39,011 Epoch 4544: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 10:19:39,012 EPOCH 4545
2024-02-05 10:19:44,663 [Epoch: 4545 Step: 00040900] Batch Recognition Loss:   0.000095 => Gls Tokens per Sec:      749 || Batch Translation Loss:   0.010987 => Txt Tokens per Sec:     1996 || Lr: 0.000050
2024-02-05 10:19:53,065 Epoch 4545: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 10:19:53,065 EPOCH 4546
2024-02-05 10:20:07,146 Epoch 4546: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 10:20:07,147 EPOCH 4547
2024-02-05 10:20:20,821 Epoch 4547: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 10:20:20,821 EPOCH 4548
2024-02-05 10:20:34,622 Epoch 4548: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 10:20:34,623 EPOCH 4549
2024-02-05 10:20:48,643 Epoch 4549: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 10:20:48,644 EPOCH 4550
2024-02-05 10:21:02,350 Epoch 4550: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 10:21:02,351 EPOCH 4551
2024-02-05 10:21:16,241 Epoch 4551: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 10:21:16,241 EPOCH 4552
2024-02-05 10:21:30,103 Epoch 4552: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 10:21:30,103 EPOCH 4553
2024-02-05 10:21:43,911 Epoch 4553: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 10:21:43,912 EPOCH 4554
2024-02-05 10:21:57,865 Epoch 4554: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 10:21:57,865 EPOCH 4555
2024-02-05 10:22:11,887 Epoch 4555: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 10:22:11,888 EPOCH 4556
2024-02-05 10:22:23,918 [Epoch: 4556 Step: 00041000] Batch Recognition Loss:   0.000114 => Gls Tokens per Sec:      458 || Batch Translation Loss:   0.022978 => Txt Tokens per Sec:     1282 || Lr: 0.000050
2024-02-05 10:22:26,078 Epoch 4556: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 10:22:26,078 EPOCH 4557
2024-02-05 10:22:39,599 Epoch 4557: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 10:22:39,599 EPOCH 4558
2024-02-05 10:22:53,701 Epoch 4558: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 10:22:53,702 EPOCH 4559
2024-02-05 10:23:07,645 Epoch 4559: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 10:23:07,646 EPOCH 4560
2024-02-05 10:23:21,805 Epoch 4560: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 10:23:21,805 EPOCH 4561
2024-02-05 10:23:35,565 Epoch 4561: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 10:23:35,566 EPOCH 4562
2024-02-05 10:23:49,671 Epoch 4562: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 10:23:49,672 EPOCH 4563
2024-02-05 10:24:03,615 Epoch 4563: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 10:24:03,615 EPOCH 4564
2024-02-05 10:24:17,463 Epoch 4564: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 10:24:17,464 EPOCH 4565
2024-02-05 10:24:31,113 Epoch 4565: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 10:24:31,113 EPOCH 4566
2024-02-05 10:24:45,177 Epoch 4566: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 10:24:45,178 EPOCH 4567
2024-02-05 10:24:49,454 [Epoch: 4567 Step: 00041100] Batch Recognition Loss:   0.000174 => Gls Tokens per Sec:     1796 || Batch Translation Loss:   0.024394 => Txt Tokens per Sec:     4835 || Lr: 0.000050
2024-02-05 10:24:59,071 Epoch 4567: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-05 10:24:59,072 EPOCH 4568
2024-02-05 10:25:13,187 Epoch 4568: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-05 10:25:13,187 EPOCH 4569
2024-02-05 10:25:26,926 Epoch 4569: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.74 
2024-02-05 10:25:26,927 EPOCH 4570
2024-02-05 10:25:40,915 Epoch 4570: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.18 
2024-02-05 10:25:40,915 EPOCH 4571
2024-02-05 10:25:54,528 Epoch 4571: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.79 
2024-02-05 10:25:54,529 EPOCH 4572
2024-02-05 10:26:08,797 Epoch 4572: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.84 
2024-02-05 10:26:08,798 EPOCH 4573
2024-02-05 10:26:22,732 Epoch 4573: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.96 
2024-02-05 10:26:22,732 EPOCH 4574
2024-02-05 10:26:36,667 Epoch 4574: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.83 
2024-02-05 10:26:36,668 EPOCH 4575
2024-02-05 10:26:50,654 Epoch 4575: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.58 
2024-02-05 10:26:50,654 EPOCH 4576
2024-02-05 10:27:04,386 Epoch 4576: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-05 10:27:04,386 EPOCH 4577
2024-02-05 10:27:18,225 Epoch 4577: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-05 10:27:18,225 EPOCH 4578
2024-02-05 10:27:22,418 [Epoch: 4578 Step: 00041200] Batch Recognition Loss:   0.000174 => Gls Tokens per Sec:     2137 || Batch Translation Loss:   0.014413 => Txt Tokens per Sec:     5636 || Lr: 0.000050
2024-02-05 10:27:31,784 Epoch 4578: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-05 10:27:31,784 EPOCH 4579
2024-02-05 10:27:45,636 Epoch 4579: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 10:27:45,637 EPOCH 4580
2024-02-05 10:27:59,538 Epoch 4580: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 10:27:59,539 EPOCH 4581
2024-02-05 10:28:13,168 Epoch 4581: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 10:28:13,169 EPOCH 4582
2024-02-05 10:28:27,293 Epoch 4582: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 10:28:27,293 EPOCH 4583
2024-02-05 10:28:41,440 Epoch 4583: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 10:28:41,441 EPOCH 4584
2024-02-05 10:28:55,558 Epoch 4584: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 10:28:55,559 EPOCH 4585
2024-02-05 10:29:09,509 Epoch 4585: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 10:29:09,509 EPOCH 4586
2024-02-05 10:29:23,275 Epoch 4586: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 10:29:23,276 EPOCH 4587
2024-02-05 10:29:37,550 Epoch 4587: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 10:29:37,550 EPOCH 4588
2024-02-05 10:29:51,459 Epoch 4588: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 10:29:51,459 EPOCH 4589
2024-02-05 10:30:03,470 [Epoch: 4589 Step: 00041300] Batch Recognition Loss:   0.000128 => Gls Tokens per Sec:      779 || Batch Translation Loss:   0.029619 => Txt Tokens per Sec:     2115 || Lr: 0.000050
2024-02-05 10:30:05,377 Epoch 4589: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 10:30:05,377 EPOCH 4590
2024-02-05 10:30:19,432 Epoch 4590: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 10:30:19,433 EPOCH 4591
2024-02-05 10:30:33,408 Epoch 4591: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 10:30:33,409 EPOCH 4592
2024-02-05 10:30:47,368 Epoch 4592: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 10:30:47,369 EPOCH 4593
2024-02-05 10:31:01,352 Epoch 4593: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 10:31:01,352 EPOCH 4594
2024-02-05 10:31:15,324 Epoch 4594: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 10:31:15,324 EPOCH 4595
2024-02-05 10:31:29,293 Epoch 4595: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 10:31:29,293 EPOCH 4596
2024-02-05 10:31:42,969 Epoch 4596: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 10:31:42,970 EPOCH 4597
2024-02-05 10:31:56,958 Epoch 4597: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 10:31:56,958 EPOCH 4598
2024-02-05 10:32:10,808 Epoch 4598: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 10:32:10,808 EPOCH 4599
2024-02-05 10:32:24,913 Epoch 4599: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 10:32:24,913 EPOCH 4600
2024-02-05 10:32:38,728 [Epoch: 4600 Step: 00041400] Batch Recognition Loss:   0.000127 => Gls Tokens per Sec:      769 || Batch Translation Loss:   0.015744 => Txt Tokens per Sec:     2136 || Lr: 0.000050
2024-02-05 10:32:38,728 Epoch 4600: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 10:32:38,729 EPOCH 4601
2024-02-05 10:32:52,768 Epoch 4601: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 10:32:52,769 EPOCH 4602
2024-02-05 10:33:06,765 Epoch 4602: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 10:33:06,766 EPOCH 4603
2024-02-05 10:33:20,680 Epoch 4603: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 10:33:20,681 EPOCH 4604
2024-02-05 10:33:34,756 Epoch 4604: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 10:33:34,757 EPOCH 4605
2024-02-05 10:33:48,496 Epoch 4605: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 10:33:48,497 EPOCH 4606
2024-02-05 10:34:02,538 Epoch 4606: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 10:34:02,539 EPOCH 4607
2024-02-05 10:34:16,657 Epoch 4607: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 10:34:16,658 EPOCH 4608
2024-02-05 10:34:30,642 Epoch 4608: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 10:34:30,643 EPOCH 4609
2024-02-05 10:34:44,587 Epoch 4609: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 10:34:44,588 EPOCH 4610
2024-02-05 10:34:58,663 Epoch 4610: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 10:34:58,663 EPOCH 4611
2024-02-05 10:35:12,526 Epoch 4611: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 10:35:12,526 EPOCH 4612
2024-02-05 10:35:12,935 [Epoch: 4612 Step: 00041500] Batch Recognition Loss:   0.000099 => Gls Tokens per Sec:     3137 || Batch Translation Loss:   0.012467 => Txt Tokens per Sec:     7868 || Lr: 0.000050
2024-02-05 10:35:26,337 Epoch 4612: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 10:35:26,338 EPOCH 4613
2024-02-05 10:35:40,317 Epoch 4613: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 10:35:40,317 EPOCH 4614
2024-02-05 10:35:54,233 Epoch 4614: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 10:35:54,234 EPOCH 4615
2024-02-05 10:36:08,072 Epoch 4615: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 10:36:08,073 EPOCH 4616
2024-02-05 10:36:22,194 Epoch 4616: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 10:36:22,195 EPOCH 4617
2024-02-05 10:36:36,106 Epoch 4617: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 10:36:36,107 EPOCH 4618
2024-02-05 10:36:49,965 Epoch 4618: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 10:36:49,965 EPOCH 4619
2024-02-05 10:37:03,926 Epoch 4619: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 10:37:03,927 EPOCH 4620
2024-02-05 10:37:17,924 Epoch 4620: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 10:37:17,925 EPOCH 4621
2024-02-05 10:37:31,871 Epoch 4621: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 10:37:31,872 EPOCH 4622
2024-02-05 10:37:45,558 Epoch 4622: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 10:37:45,558 EPOCH 4623
2024-02-05 10:37:51,434 [Epoch: 4623 Step: 00041600] Batch Recognition Loss:   0.000133 => Gls Tokens per Sec:      436 || Batch Translation Loss:   0.019033 => Txt Tokens per Sec:     1418 || Lr: 0.000050
2024-02-05 10:37:59,533 Epoch 4623: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 10:37:59,534 EPOCH 4624
2024-02-05 10:38:13,345 Epoch 4624: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 10:38:13,345 EPOCH 4625
2024-02-05 10:38:27,400 Epoch 4625: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 10:38:27,401 EPOCH 4626
2024-02-05 10:38:41,358 Epoch 4626: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 10:38:41,359 EPOCH 4627
2024-02-05 10:38:55,242 Epoch 4627: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 10:38:55,242 EPOCH 4628
2024-02-05 10:39:09,280 Epoch 4628: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 10:39:09,281 EPOCH 4629
2024-02-05 10:39:23,308 Epoch 4629: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 10:39:23,309 EPOCH 4630
2024-02-05 10:39:36,991 Epoch 4630: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 10:39:36,991 EPOCH 4631
2024-02-05 10:39:51,121 Epoch 4631: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 10:39:51,121 EPOCH 4632
2024-02-05 10:40:05,058 Epoch 4632: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 10:40:05,059 EPOCH 4633
2024-02-05 10:40:18,850 Epoch 4633: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 10:40:18,851 EPOCH 4634
2024-02-05 10:40:21,806 [Epoch: 4634 Step: 00041700] Batch Recognition Loss:   0.000110 => Gls Tokens per Sec:     1300 || Batch Translation Loss:   0.012907 => Txt Tokens per Sec:     3856 || Lr: 0.000050
2024-02-05 10:40:32,896 Epoch 4634: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 10:40:32,896 EPOCH 4635
2024-02-05 10:40:46,465 Epoch 4635: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 10:40:46,466 EPOCH 4636
2024-02-05 10:41:00,134 Epoch 4636: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 10:41:00,134 EPOCH 4637
2024-02-05 10:41:14,364 Epoch 4637: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 10:41:14,364 EPOCH 4638
2024-02-05 10:41:28,403 Epoch 4638: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 10:41:28,403 EPOCH 4639
2024-02-05 10:41:42,371 Epoch 4639: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 10:41:42,372 EPOCH 4640
2024-02-05 10:41:56,099 Epoch 4640: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 10:41:56,099 EPOCH 4641
2024-02-05 10:42:10,171 Epoch 4641: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 10:42:10,171 EPOCH 4642
2024-02-05 10:42:24,145 Epoch 4642: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 10:42:24,146 EPOCH 4643
2024-02-05 10:42:38,059 Epoch 4643: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 10:42:38,060 EPOCH 4644
2024-02-05 10:42:52,059 Epoch 4644: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 10:42:52,060 EPOCH 4645
2024-02-05 10:43:03,514 [Epoch: 4645 Step: 00041800] Batch Recognition Loss:   0.000139 => Gls Tokens per Sec:      369 || Batch Translation Loss:   0.005972 => Txt Tokens per Sec:     1176 || Lr: 0.000050
2024-02-05 10:43:05,670 Epoch 4645: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 10:43:05,670 EPOCH 4646
2024-02-05 10:43:19,664 Epoch 4646: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 10:43:19,664 EPOCH 4647
2024-02-05 10:43:33,670 Epoch 4647: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 10:43:33,671 EPOCH 4648
2024-02-05 10:43:47,635 Epoch 4648: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 10:43:47,635 EPOCH 4649
2024-02-05 10:44:01,782 Epoch 4649: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 10:44:01,783 EPOCH 4650
2024-02-05 10:44:15,734 Epoch 4650: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 10:44:15,735 EPOCH 4651
2024-02-05 10:44:29,687 Epoch 4651: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 10:44:29,687 EPOCH 4652
2024-02-05 10:44:43,561 Epoch 4652: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 10:44:43,561 EPOCH 4653
2024-02-05 10:44:57,315 Epoch 4653: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 10:44:57,315 EPOCH 4654
2024-02-05 10:45:11,287 Epoch 4654: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 10:45:11,288 EPOCH 4655
2024-02-05 10:45:25,162 Epoch 4655: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 10:45:25,162 EPOCH 4656
2024-02-05 10:45:32,232 [Epoch: 4656 Step: 00041900] Batch Recognition Loss:   0.000138 => Gls Tokens per Sec:      779 || Batch Translation Loss:   0.014591 => Txt Tokens per Sec:     1968 || Lr: 0.000050
2024-02-05 10:45:39,052 Epoch 4656: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 10:45:39,053 EPOCH 4657
2024-02-05 10:45:52,940 Epoch 4657: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 10:45:52,941 EPOCH 4658
2024-02-05 10:46:06,470 Epoch 4658: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 10:46:06,470 EPOCH 4659
2024-02-05 10:46:20,510 Epoch 4659: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 10:46:20,511 EPOCH 4660
2024-02-05 10:46:34,652 Epoch 4660: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 10:46:34,652 EPOCH 4661
2024-02-05 10:46:48,656 Epoch 4661: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 10:46:48,656 EPOCH 4662
2024-02-05 10:47:02,741 Epoch 4662: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 10:47:02,741 EPOCH 4663
2024-02-05 10:47:16,749 Epoch 4663: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 10:47:16,749 EPOCH 4664
2024-02-05 10:47:30,956 Epoch 4664: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 10:47:30,956 EPOCH 4665
2024-02-05 10:47:44,728 Epoch 4665: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 10:47:44,729 EPOCH 4666
2024-02-05 10:47:58,925 Epoch 4666: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 10:47:58,926 EPOCH 4667
2024-02-05 10:48:10,309 [Epoch: 4667 Step: 00042000] Batch Recognition Loss:   0.000108 => Gls Tokens per Sec:      597 || Batch Translation Loss:   0.013010 => Txt Tokens per Sec:     1696 || Lr: 0.000050
2024-02-05 10:48:39,416 Validation result at epoch 4667, step    42000: duration: 29.1061s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00033	Translation Loss: 103435.53125	PPL: 31282.01367
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.58	(BLEU-1: 10.44,	BLEU-2: 3.16,	BLEU-3: 1.27,	BLEU-4: 0.58)
	CHRF 16.70	ROUGE 8.83
2024-02-05 10:48:39,417 Logging Recognition and Translation Outputs
2024-02-05 10:48:39,417 ========================================================================================================================
2024-02-05 10:48:39,418 Logging Sequence: 81_8.00
2024-02-05 10:48:39,418 	Gloss Reference :	A B+C+D+E
2024-02-05 10:48:39,418 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 10:48:39,418 	Gloss Alignment :	         
2024-02-05 10:48:39,419 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 10:48:39,421 	Text Reference  :	have been involved in  a     huge    controversy in       connection to    real estate developer amrapali group since     last 7       years
2024-02-05 10:48:39,421 	Text Hypothesis :	**** **** ******** the woman alleged that        danushka on         media if   even   train     and      has   completed the  cricket kit  
2024-02-05 10:48:39,421 	Text Alignment  :	D    D    D        S   S     S       S           S        S          S     S    S      S         S        S     S         S    S       S    
2024-02-05 10:48:39,421 ========================================================================================================================
2024-02-05 10:48:39,422 Logging Sequence: 148_239.00
2024-02-05 10:48:39,422 	Gloss Reference :	A B+C+D+E
2024-02-05 10:48:39,423 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 10:48:39,423 	Gloss Alignment :	         
2024-02-05 10:48:39,423 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 10:48:39,425 	Text Reference  :	**** ***** ******* *** *** ** ** ******** the  ground staff were very  happy and    thanked the  bowler for his kind gesture   
2024-02-05 10:48:39,425 	Text Hypothesis :	test match between rcb and 50 30 november 2022 was    a     huge round of    hardik pandya  away he     is  his **** girlfriend
2024-02-05 10:48:39,426 	Text Alignment  :	I    I     I       I   I   I  I  I        S    S      S     S    S     S     S      S       S    S      S       D    S         
2024-02-05 10:48:39,426 ========================================================================================================================
2024-02-05 10:48:39,426 Logging Sequence: 165_8.00
2024-02-05 10:48:39,426 	Gloss Reference :	A B+C+D+E
2024-02-05 10:48:39,427 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 10:48:39,427 	Gloss Alignment :	         
2024-02-05 10:48:39,427 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 10:48:39,428 	Text Reference  :	however many don't believe in     it  it    varies among  people
2024-02-05 10:48:39,428 	Text Hypothesis :	******* **** ***** they    played 194 balls and    pandya won   
2024-02-05 10:48:39,428 	Text Alignment  :	D       D    D     S       S      S   S     S      S      S     
2024-02-05 10:48:39,428 ========================================================================================================================
2024-02-05 10:48:39,429 Logging Sequence: 93_93.00
2024-02-05 10:48:39,429 	Gloss Reference :	A B+C+D+E
2024-02-05 10:48:39,429 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 10:48:39,429 	Gloss Alignment :	         
2024-02-05 10:48:39,429 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 10:48:39,430 	Text Reference  :	*** ********** rooney was at        the        club   as       well
2024-02-05 10:48:39,430 	Text Hypothesis :	and federation runs   to  england's manchester united football team
2024-02-05 10:48:39,430 	Text Alignment  :	I   I          S      S   S         S          S      S        S   
2024-02-05 10:48:39,430 ========================================================================================================================
2024-02-05 10:48:39,430 Logging Sequence: 96_129.00
2024-02-05 10:48:39,431 	Gloss Reference :	A B+C+D+E
2024-02-05 10:48:39,431 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 10:48:39,431 	Gloss Alignment :	         
2024-02-05 10:48:39,431 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 10:48:39,431 	Text Reference  :	***** *** viewers   were very stressed
2024-02-05 10:48:39,432 	Text Hypothesis :	while the remaining can  win  said    
2024-02-05 10:48:39,432 	Text Alignment  :	I     I   S         S    S    S       
2024-02-05 10:48:39,432 ========================================================================================================================
2024-02-05 10:48:42,191 Epoch 4667: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 10:48:42,192 EPOCH 4668
2024-02-05 10:48:57,618 Epoch 4668: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 10:48:57,619 EPOCH 4669
2024-02-05 10:49:12,290 Epoch 4669: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 10:49:12,291 EPOCH 4670
2024-02-05 10:49:26,606 Epoch 4670: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 10:49:26,606 EPOCH 4671
2024-02-05 10:49:40,735 Epoch 4671: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 10:49:40,736 EPOCH 4672
2024-02-05 10:49:54,416 Epoch 4672: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 10:49:54,417 EPOCH 4673
2024-02-05 10:50:08,669 Epoch 4673: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 10:50:08,669 EPOCH 4674
2024-02-05 10:50:23,029 Epoch 4674: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 10:50:23,030 EPOCH 4675
2024-02-05 10:50:37,017 Epoch 4675: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 10:50:37,018 EPOCH 4676
2024-02-05 10:50:51,611 Epoch 4676: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 10:50:51,612 EPOCH 4677
2024-02-05 10:51:05,419 Epoch 4677: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 10:51:05,420 EPOCH 4678
2024-02-05 10:51:17,412 [Epoch: 4678 Step: 00042100] Batch Recognition Loss:   0.000146 => Gls Tokens per Sec:      673 || Batch Translation Loss:   0.022449 => Txt Tokens per Sec:     1828 || Lr: 0.000050
2024-02-05 10:51:19,811 Epoch 4678: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 10:51:19,812 EPOCH 4679
2024-02-05 10:51:33,541 Epoch 4679: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 10:51:33,541 EPOCH 4680
2024-02-05 10:51:47,496 Epoch 4680: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 10:51:47,496 EPOCH 4681
2024-02-05 10:52:01,477 Epoch 4681: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-05 10:52:01,477 EPOCH 4682
2024-02-05 10:52:15,473 Epoch 4682: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 10:52:15,473 EPOCH 4683
2024-02-05 10:52:29,638 Epoch 4683: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 10:52:29,639 EPOCH 4684
2024-02-05 10:52:43,735 Epoch 4684: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 10:52:43,735 EPOCH 4685
2024-02-05 10:52:57,972 Epoch 4685: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 10:52:57,973 EPOCH 4686
2024-02-05 10:53:12,023 Epoch 4686: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-05 10:53:12,023 EPOCH 4687
2024-02-05 10:53:25,826 Epoch 4687: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-05 10:53:25,827 EPOCH 4688
2024-02-05 10:53:40,235 Epoch 4688: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.56 
2024-02-05 10:53:40,235 EPOCH 4689
2024-02-05 10:53:52,387 [Epoch: 4689 Step: 00042200] Batch Recognition Loss:   0.000668 => Gls Tokens per Sec:      770 || Batch Translation Loss:   0.092784 => Txt Tokens per Sec:     2093 || Lr: 0.000050
2024-02-05 10:53:54,364 Epoch 4689: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.68 
2024-02-05 10:53:54,365 EPOCH 4690
2024-02-05 10:54:08,361 Epoch 4690: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-05 10:54:08,361 EPOCH 4691
2024-02-05 10:54:22,380 Epoch 4691: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-05 10:54:22,381 EPOCH 4692
2024-02-05 10:54:36,280 Epoch 4692: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.73 
2024-02-05 10:54:36,280 EPOCH 4693
2024-02-05 10:54:50,381 Epoch 4693: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-05 10:54:50,382 EPOCH 4694
2024-02-05 10:55:04,356 Epoch 4694: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-05 10:55:04,356 EPOCH 4695
2024-02-05 10:55:18,203 Epoch 4695: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-05 10:55:18,204 EPOCH 4696
2024-02-05 10:55:31,791 Epoch 4696: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 10:55:31,792 EPOCH 4697
2024-02-05 10:55:45,762 Epoch 4697: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 10:55:45,762 EPOCH 4698
2024-02-05 10:55:59,998 Epoch 4698: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 10:55:59,999 EPOCH 4699
2024-02-05 10:56:13,737 Epoch 4699: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-05 10:56:13,738 EPOCH 4700
2024-02-05 10:56:27,411 [Epoch: 4700 Step: 00042300] Batch Recognition Loss:   0.000199 => Gls Tokens per Sec:      778 || Batch Translation Loss:   0.028242 => Txt Tokens per Sec:     2158 || Lr: 0.000050
2024-02-05 10:56:27,411 Epoch 4700: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 10:56:27,411 EPOCH 4701
2024-02-05 10:56:41,528 Epoch 4701: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-05 10:56:41,528 EPOCH 4702
2024-02-05 10:56:55,620 Epoch 4702: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-05 10:56:55,621 EPOCH 4703
2024-02-05 10:57:09,591 Epoch 4703: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 10:57:09,592 EPOCH 4704
2024-02-05 10:57:23,265 Epoch 4704: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 10:57:23,266 EPOCH 4705
2024-02-05 10:57:37,359 Epoch 4705: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 10:57:37,359 EPOCH 4706
2024-02-05 10:57:51,206 Epoch 4706: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 10:57:51,207 EPOCH 4707
2024-02-05 10:58:05,170 Epoch 4707: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 10:58:05,171 EPOCH 4708
2024-02-05 10:58:19,109 Epoch 4708: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 10:58:19,109 EPOCH 4709
2024-02-05 10:58:33,189 Epoch 4709: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 10:58:33,189 EPOCH 4710
2024-02-05 10:58:46,843 Epoch 4710: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 10:58:46,844 EPOCH 4711
2024-02-05 10:59:00,873 Epoch 4711: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 10:59:00,874 EPOCH 4712
2024-02-05 10:59:01,198 [Epoch: 4712 Step: 00042400] Batch Recognition Loss:   0.000121 => Gls Tokens per Sec:     3975 || Batch Translation Loss:   0.014769 => Txt Tokens per Sec:    10025 || Lr: 0.000050
2024-02-05 10:59:14,660 Epoch 4712: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 10:59:14,660 EPOCH 4713
2024-02-05 10:59:28,488 Epoch 4713: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 10:59:28,489 EPOCH 4714
2024-02-05 10:59:42,345 Epoch 4714: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 10:59:42,346 EPOCH 4715
2024-02-05 10:59:56,183 Epoch 4715: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 10:59:56,184 EPOCH 4716
2024-02-05 11:00:10,400 Epoch 4716: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 11:00:10,400 EPOCH 4717
2024-02-05 11:00:24,277 Epoch 4717: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 11:00:24,278 EPOCH 4718
2024-02-05 11:00:37,940 Epoch 4718: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 11:00:37,941 EPOCH 4719
2024-02-05 11:00:51,597 Epoch 4719: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 11:00:51,597 EPOCH 4720
2024-02-05 11:01:05,731 Epoch 4720: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 11:01:05,731 EPOCH 4721
2024-02-05 11:01:19,677 Epoch 4721: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 11:01:19,678 EPOCH 4722
2024-02-05 11:01:33,446 Epoch 4722: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 11:01:33,446 EPOCH 4723
2024-02-05 11:01:34,209 [Epoch: 4723 Step: 00042500] Batch Recognition Loss:   0.000132 => Gls Tokens per Sec:     3360 || Batch Translation Loss:   0.016522 => Txt Tokens per Sec:     8824 || Lr: 0.000050
2024-02-05 11:01:47,016 Epoch 4723: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 11:01:47,016 EPOCH 4724
2024-02-05 11:02:01,326 Epoch 4724: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 11:02:01,327 EPOCH 4725
2024-02-05 11:02:15,203 Epoch 4725: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 11:02:15,204 EPOCH 4726
2024-02-05 11:02:29,108 Epoch 4726: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-05 11:02:29,109 EPOCH 4727
2024-02-05 11:02:42,745 Epoch 4727: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-05 11:02:42,746 EPOCH 4728
2024-02-05 11:02:56,748 Epoch 4728: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-05 11:02:56,748 EPOCH 4729
2024-02-05 11:03:10,658 Epoch 4729: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-05 11:03:10,658 EPOCH 4730
2024-02-05 11:03:24,406 Epoch 4730: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-05 11:03:24,406 EPOCH 4731
2024-02-05 11:03:38,392 Epoch 4731: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-05 11:03:38,393 EPOCH 4732
2024-02-05 11:03:52,136 Epoch 4732: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-05 11:03:52,136 EPOCH 4733
2024-02-05 11:04:06,328 Epoch 4733: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-05 11:04:06,328 EPOCH 4734
2024-02-05 11:04:11,224 [Epoch: 4734 Step: 00042600] Batch Recognition Loss:   0.000130 => Gls Tokens per Sec:      603 || Batch Translation Loss:   0.016054 => Txt Tokens per Sec:     1356 || Lr: 0.000050
2024-02-05 11:04:20,305 Epoch 4734: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 11:04:20,306 EPOCH 4735
2024-02-05 11:04:34,430 Epoch 4735: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 11:04:34,431 EPOCH 4736
2024-02-05 11:04:48,197 Epoch 4736: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 11:04:48,198 EPOCH 4737
2024-02-05 11:05:02,091 Epoch 4737: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 11:05:02,092 EPOCH 4738
2024-02-05 11:05:15,876 Epoch 4738: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 11:05:15,876 EPOCH 4739
2024-02-05 11:05:29,692 Epoch 4739: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 11:05:29,692 EPOCH 4740
2024-02-05 11:05:43,502 Epoch 4740: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 11:05:43,503 EPOCH 4741
2024-02-05 11:05:57,349 Epoch 4741: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 11:05:57,350 EPOCH 4742
2024-02-05 11:06:11,304 Epoch 4742: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 11:06:11,304 EPOCH 4743
2024-02-05 11:06:25,154 Epoch 4743: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 11:06:25,154 EPOCH 4744
2024-02-05 11:06:39,024 Epoch 4744: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 11:06:39,025 EPOCH 4745
2024-02-05 11:06:45,427 [Epoch: 4745 Step: 00042700] Batch Recognition Loss:   0.000130 => Gls Tokens per Sec:      800 || Batch Translation Loss:   0.016616 => Txt Tokens per Sec:     2096 || Lr: 0.000050
2024-02-05 11:06:53,217 Epoch 4745: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 11:06:53,218 EPOCH 4746
2024-02-05 11:07:07,202 Epoch 4746: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 11:07:07,203 EPOCH 4747
2024-02-05 11:07:21,198 Epoch 4747: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 11:07:21,198 EPOCH 4748
2024-02-05 11:07:34,800 Epoch 4748: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 11:07:34,801 EPOCH 4749
2024-02-05 11:07:48,877 Epoch 4749: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 11:07:48,878 EPOCH 4750
2024-02-05 11:08:02,553 Epoch 4750: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 11:08:02,554 EPOCH 4751
2024-02-05 11:08:16,580 Epoch 4751: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 11:08:16,580 EPOCH 4752
2024-02-05 11:08:30,266 Epoch 4752: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 11:08:30,266 EPOCH 4753
2024-02-05 11:08:44,543 Epoch 4753: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 11:08:44,544 EPOCH 4754
2024-02-05 11:08:58,500 Epoch 4754: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 11:08:58,501 EPOCH 4755
2024-02-05 11:09:12,648 Epoch 4755: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 11:09:12,648 EPOCH 4756
2024-02-05 11:09:23,492 [Epoch: 4756 Step: 00042800] Batch Recognition Loss:   0.000102 => Gls Tokens per Sec:      508 || Batch Translation Loss:   0.010847 => Txt Tokens per Sec:     1382 || Lr: 0.000050
2024-02-05 11:09:26,622 Epoch 4756: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 11:09:26,622 EPOCH 4757
2024-02-05 11:09:40,630 Epoch 4757: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 11:09:40,631 EPOCH 4758
2024-02-05 11:09:54,596 Epoch 4758: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 11:09:54,596 EPOCH 4759
2024-02-05 11:10:08,613 Epoch 4759: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 11:10:08,614 EPOCH 4760
2024-02-05 11:10:22,393 Epoch 4760: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 11:10:22,393 EPOCH 4761
2024-02-05 11:10:36,381 Epoch 4761: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 11:10:36,381 EPOCH 4762
2024-02-05 11:10:50,172 Epoch 4762: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 11:10:50,173 EPOCH 4763
2024-02-05 11:11:04,190 Epoch 4763: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 11:11:04,191 EPOCH 4764
2024-02-05 11:11:18,048 Epoch 4764: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 11:11:18,049 EPOCH 4765
2024-02-05 11:11:32,389 Epoch 4765: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 11:11:32,390 EPOCH 4766
2024-02-05 11:11:46,524 Epoch 4766: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 11:11:46,525 EPOCH 4767
2024-02-05 11:11:54,079 [Epoch: 4767 Step: 00042900] Batch Recognition Loss:   0.000098 => Gls Tokens per Sec:     1017 || Batch Translation Loss:   0.011240 => Txt Tokens per Sec:     2790 || Lr: 0.000050
2024-02-05 11:12:00,387 Epoch 4767: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 11:12:00,387 EPOCH 4768
2024-02-05 11:12:14,246 Epoch 4768: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 11:12:14,246 EPOCH 4769
2024-02-05 11:12:28,442 Epoch 4769: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 11:12:28,443 EPOCH 4770
2024-02-05 11:12:42,353 Epoch 4770: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 11:12:42,353 EPOCH 4771
2024-02-05 11:12:56,187 Epoch 4771: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 11:12:56,187 EPOCH 4772
2024-02-05 11:13:10,214 Epoch 4772: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 11:13:10,215 EPOCH 4773
2024-02-05 11:13:24,179 Epoch 4773: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 11:13:24,180 EPOCH 4774
2024-02-05 11:13:38,193 Epoch 4774: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 11:13:38,193 EPOCH 4775
2024-02-05 11:13:51,789 Epoch 4775: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 11:13:51,790 EPOCH 4776
2024-02-05 11:14:05,768 Epoch 4776: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 11:14:05,769 EPOCH 4777
2024-02-05 11:14:19,680 Epoch 4777: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 11:14:19,681 EPOCH 4778
2024-02-05 11:14:28,609 [Epoch: 4778 Step: 00043000] Batch Recognition Loss:   0.000107 => Gls Tokens per Sec:     1004 || Batch Translation Loss:   0.013332 => Txt Tokens per Sec:     2730 || Lr: 0.000050
2024-02-05 11:14:33,397 Epoch 4778: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 11:14:33,398 EPOCH 4779
2024-02-05 11:14:47,498 Epoch 4779: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 11:14:47,498 EPOCH 4780
2024-02-05 11:15:01,494 Epoch 4780: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 11:15:01,495 EPOCH 4781
2024-02-05 11:15:15,477 Epoch 4781: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 11:15:15,478 EPOCH 4782
2024-02-05 11:15:29,180 Epoch 4782: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 11:15:29,181 EPOCH 4783
2024-02-05 11:15:43,223 Epoch 4783: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 11:15:43,224 EPOCH 4784
2024-02-05 11:15:57,278 Epoch 4784: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 11:15:57,278 EPOCH 4785
2024-02-05 11:16:11,244 Epoch 4785: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 11:16:11,244 EPOCH 4786
2024-02-05 11:16:25,612 Epoch 4786: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 11:16:25,613 EPOCH 4787
2024-02-05 11:16:39,737 Epoch 4787: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 11:16:39,737 EPOCH 4788
2024-02-05 11:16:53,828 Epoch 4788: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 11:16:53,828 EPOCH 4789
2024-02-05 11:17:05,869 [Epoch: 4789 Step: 00043100] Batch Recognition Loss:   0.000131 => Gls Tokens per Sec:      777 || Batch Translation Loss:   0.015477 => Txt Tokens per Sec:     2110 || Lr: 0.000050
2024-02-05 11:17:07,608 Epoch 4789: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 11:17:07,608 EPOCH 4790
2024-02-05 11:17:21,315 Epoch 4790: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 11:17:21,315 EPOCH 4791
2024-02-05 11:17:35,173 Epoch 4791: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 11:17:35,173 EPOCH 4792
2024-02-05 11:17:49,259 Epoch 4792: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 11:17:49,259 EPOCH 4793
2024-02-05 11:18:03,296 Epoch 4793: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 11:18:03,296 EPOCH 4794
2024-02-05 11:18:17,171 Epoch 4794: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 11:18:17,172 EPOCH 4795
2024-02-05 11:18:30,991 Epoch 4795: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 11:18:30,991 EPOCH 4796
2024-02-05 11:18:44,944 Epoch 4796: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 11:18:44,945 EPOCH 4797
2024-02-05 11:18:58,848 Epoch 4797: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 11:18:58,849 EPOCH 4798
2024-02-05 11:19:12,648 Epoch 4798: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 11:19:12,649 EPOCH 4799
2024-02-05 11:19:26,362 Epoch 4799: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 11:19:26,363 EPOCH 4800
2024-02-05 11:19:40,307 [Epoch: 4800 Step: 00043200] Batch Recognition Loss:   0.000179 => Gls Tokens per Sec:      762 || Batch Translation Loss:   0.008650 => Txt Tokens per Sec:     2116 || Lr: 0.000050
2024-02-05 11:19:40,308 Epoch 4800: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 11:19:40,308 EPOCH 4801
2024-02-05 11:19:54,248 Epoch 4801: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-05 11:19:54,249 EPOCH 4802
2024-02-05 11:20:08,245 Epoch 4802: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 11:20:08,245 EPOCH 4803
2024-02-05 11:20:21,978 Epoch 4803: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 11:20:21,978 EPOCH 4804
2024-02-05 11:20:35,910 Epoch 4804: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 11:20:35,910 EPOCH 4805
2024-02-05 11:20:50,067 Epoch 4805: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 11:20:50,067 EPOCH 4806
2024-02-05 11:21:03,914 Epoch 4806: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 11:21:03,915 EPOCH 4807
2024-02-05 11:21:18,174 Epoch 4807: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 11:21:18,174 EPOCH 4808
2024-02-05 11:21:32,299 Epoch 4808: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 11:21:32,300 EPOCH 4809
2024-02-05 11:21:45,982 Epoch 4809: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 11:21:45,983 EPOCH 4810
2024-02-05 11:21:59,821 Epoch 4810: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 11:21:59,821 EPOCH 4811
2024-02-05 11:22:13,828 Epoch 4811: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 11:22:13,829 EPOCH 4812
2024-02-05 11:22:14,318 [Epoch: 4812 Step: 00043300] Batch Recognition Loss:   0.000144 => Gls Tokens per Sec:     2615 || Batch Translation Loss:   0.021480 => Txt Tokens per Sec:     7695 || Lr: 0.000050
2024-02-05 11:22:27,570 Epoch 4812: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 11:22:27,571 EPOCH 4813
2024-02-05 11:22:41,484 Epoch 4813: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 11:22:41,484 EPOCH 4814
2024-02-05 11:22:55,475 Epoch 4814: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 11:22:55,476 EPOCH 4815
2024-02-05 11:23:09,201 Epoch 4815: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 11:23:09,201 EPOCH 4816
2024-02-05 11:23:23,229 Epoch 4816: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 11:23:23,230 EPOCH 4817
2024-02-05 11:23:37,172 Epoch 4817: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 11:23:37,173 EPOCH 4818
2024-02-05 11:23:51,490 Epoch 4818: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 11:23:51,491 EPOCH 4819
2024-02-05 11:24:05,596 Epoch 4819: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 11:24:05,597 EPOCH 4820
2024-02-05 11:24:19,208 Epoch 4820: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 11:24:19,209 EPOCH 4821
2024-02-05 11:24:33,509 Epoch 4821: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 11:24:33,509 EPOCH 4822
2024-02-05 11:24:47,398 Epoch 4822: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-05 11:24:47,399 EPOCH 4823
2024-02-05 11:24:53,399 [Epoch: 4823 Step: 00043400] Batch Recognition Loss:   0.000164 => Gls Tokens per Sec:      278 || Batch Translation Loss:   0.008167 => Txt Tokens per Sec:      915 || Lr: 0.000050
2024-02-05 11:25:01,172 Epoch 4823: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 11:25:01,173 EPOCH 4824
2024-02-05 11:25:15,078 Epoch 4824: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 11:25:15,079 EPOCH 4825
2024-02-05 11:25:28,844 Epoch 4825: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 11:25:28,844 EPOCH 4826
2024-02-05 11:25:42,791 Epoch 4826: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 11:25:42,791 EPOCH 4827
2024-02-05 11:25:56,348 Epoch 4827: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 11:25:56,348 EPOCH 4828
2024-02-05 11:26:10,578 Epoch 4828: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 11:26:10,579 EPOCH 4829
2024-02-05 11:26:24,583 Epoch 4829: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 11:26:24,584 EPOCH 4830
2024-02-05 11:26:38,640 Epoch 4830: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 11:26:38,641 EPOCH 4831
2024-02-05 11:26:52,545 Epoch 4831: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 11:26:52,545 EPOCH 4832
2024-02-05 11:27:06,507 Epoch 4832: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 11:27:06,507 EPOCH 4833
2024-02-05 11:27:20,346 Epoch 4833: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 11:27:20,346 EPOCH 4834
2024-02-05 11:27:25,641 [Epoch: 4834 Step: 00043500] Batch Recognition Loss:   0.000116 => Gls Tokens per Sec:      557 || Batch Translation Loss:   0.014545 => Txt Tokens per Sec:     1638 || Lr: 0.000050
2024-02-05 11:27:34,279 Epoch 4834: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 11:27:34,279 EPOCH 4835
2024-02-05 11:27:48,249 Epoch 4835: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 11:27:48,250 EPOCH 4836
2024-02-05 11:28:02,048 Epoch 4836: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 11:28:02,049 EPOCH 4837
2024-02-05 11:28:16,034 Epoch 4837: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 11:28:16,034 EPOCH 4838
2024-02-05 11:28:30,007 Epoch 4838: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 11:28:30,008 EPOCH 4839
2024-02-05 11:28:43,897 Epoch 4839: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 11:28:43,898 EPOCH 4840
2024-02-05 11:28:57,730 Epoch 4840: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 11:28:57,730 EPOCH 4841
2024-02-05 11:29:11,595 Epoch 4841: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 11:29:11,596 EPOCH 4842
2024-02-05 11:29:25,346 Epoch 4842: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 11:29:25,347 EPOCH 4843
2024-02-05 11:29:39,315 Epoch 4843: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 11:29:39,315 EPOCH 4844
2024-02-05 11:29:53,591 Epoch 4844: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 11:29:53,592 EPOCH 4845
2024-02-05 11:29:55,147 [Epoch: 4845 Step: 00043600] Batch Recognition Loss:   0.000112 => Gls Tokens per Sec:     3297 || Batch Translation Loss:   0.014658 => Txt Tokens per Sec:     7829 || Lr: 0.000050
2024-02-05 11:30:07,560 Epoch 4845: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 11:30:07,560 EPOCH 4846
2024-02-05 11:30:21,399 Epoch 4846: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 11:30:21,400 EPOCH 4847
2024-02-05 11:30:35,358 Epoch 4847: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 11:30:35,358 EPOCH 4848
2024-02-05 11:30:49,288 Epoch 4848: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 11:30:49,288 EPOCH 4849
2024-02-05 11:31:03,292 Epoch 4849: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 11:31:03,293 EPOCH 4850
2024-02-05 11:31:17,212 Epoch 4850: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 11:31:17,213 EPOCH 4851
2024-02-05 11:31:30,919 Epoch 4851: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 11:31:30,919 EPOCH 4852
2024-02-05 11:31:44,339 Epoch 4852: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 11:31:44,340 EPOCH 4853
2024-02-05 11:31:58,373 Epoch 4853: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 11:31:58,373 EPOCH 4854
2024-02-05 11:32:12,288 Epoch 4854: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 11:32:12,289 EPOCH 4855
2024-02-05 11:32:26,369 Epoch 4855: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 11:32:26,369 EPOCH 4856
2024-02-05 11:32:37,366 [Epoch: 4856 Step: 00043700] Batch Recognition Loss:   0.000118 => Gls Tokens per Sec:      501 || Batch Translation Loss:   0.015084 => Txt Tokens per Sec:     1474 || Lr: 0.000050
2024-02-05 11:32:40,553 Epoch 4856: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 11:32:40,554 EPOCH 4857
2024-02-05 11:32:54,166 Epoch 4857: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 11:32:54,167 EPOCH 4858
2024-02-05 11:33:08,193 Epoch 4858: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 11:33:08,194 EPOCH 4859
2024-02-05 11:33:22,184 Epoch 4859: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 11:33:22,185 EPOCH 4860
2024-02-05 11:33:36,003 Epoch 4860: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 11:33:36,003 EPOCH 4861
2024-02-05 11:33:49,805 Epoch 4861: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 11:33:49,806 EPOCH 4862
2024-02-05 11:34:03,749 Epoch 4862: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 11:34:03,749 EPOCH 4863
2024-02-05 11:34:17,272 Epoch 4863: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 11:34:17,272 EPOCH 4864
2024-02-05 11:34:31,278 Epoch 4864: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 11:34:31,278 EPOCH 4865
2024-02-05 11:34:45,255 Epoch 4865: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 11:34:45,255 EPOCH 4866
2024-02-05 11:34:59,116 Epoch 4866: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-05 11:34:59,117 EPOCH 4867
2024-02-05 11:35:07,957 [Epoch: 4867 Step: 00043800] Batch Recognition Loss:   0.000159 => Gls Tokens per Sec:      869 || Batch Translation Loss:   0.019228 => Txt Tokens per Sec:     2375 || Lr: 0.000050
2024-02-05 11:35:13,220 Epoch 4867: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 11:35:13,221 EPOCH 4868
2024-02-05 11:35:27,544 Epoch 4868: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-05 11:35:27,544 EPOCH 4869
2024-02-05 11:35:41,513 Epoch 4869: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 11:35:41,514 EPOCH 4870
2024-02-05 11:35:55,353 Epoch 4870: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-05 11:35:55,354 EPOCH 4871
2024-02-05 11:36:09,103 Epoch 4871: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-05 11:36:09,103 EPOCH 4872
2024-02-05 11:36:22,799 Epoch 4872: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-05 11:36:22,800 EPOCH 4873
2024-02-05 11:36:36,833 Epoch 4873: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-05 11:36:36,834 EPOCH 4874
2024-02-05 11:36:50,459 Epoch 4874: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-05 11:36:50,459 EPOCH 4875
2024-02-05 11:37:04,175 Epoch 4875: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-05 11:37:04,175 EPOCH 4876
2024-02-05 11:37:17,999 Epoch 4876: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-05 11:37:18,000 EPOCH 4877
2024-02-05 11:37:31,720 Epoch 4877: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-05 11:37:31,721 EPOCH 4878
2024-02-05 11:37:39,678 [Epoch: 4878 Step: 00043900] Batch Recognition Loss:   0.000168 => Gls Tokens per Sec:     1014 || Batch Translation Loss:   0.019989 => Txt Tokens per Sec:     2660 || Lr: 0.000050
2024-02-05 11:37:45,366 Epoch 4878: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-05 11:37:45,367 EPOCH 4879
2024-02-05 11:37:59,354 Epoch 4879: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-05 11:37:59,355 EPOCH 4880
2024-02-05 11:38:12,988 Epoch 4880: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-05 11:38:12,989 EPOCH 4881
2024-02-05 11:38:26,669 Epoch 4881: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-05 11:38:26,669 EPOCH 4882
2024-02-05 11:38:40,577 Epoch 4882: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-05 11:38:40,578 EPOCH 4883
2024-02-05 11:38:54,705 Epoch 4883: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 11:38:54,706 EPOCH 4884
2024-02-05 11:39:08,546 Epoch 4884: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 11:39:08,547 EPOCH 4885
2024-02-05 11:39:22,846 Epoch 4885: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 11:39:22,846 EPOCH 4886
2024-02-05 11:39:36,816 Epoch 4886: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 11:39:36,817 EPOCH 4887
2024-02-05 11:39:50,851 Epoch 4887: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 11:39:50,852 EPOCH 4888
2024-02-05 11:40:04,525 Epoch 4888: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 11:40:04,526 EPOCH 4889
2024-02-05 11:40:17,653 [Epoch: 4889 Step: 00044000] Batch Recognition Loss:   0.000147 => Gls Tokens per Sec:      712 || Batch Translation Loss:   0.022601 => Txt Tokens per Sec:     2006 || Lr: 0.000050
2024-02-05 11:40:46,919 Validation result at epoch 4889, step    44000: duration: 29.2641s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00061	Translation Loss: 105562.34375	PPL: 38701.32812
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.53	(BLEU-1: 10.39,	BLEU-2: 3.00,	BLEU-3: 1.11,	BLEU-4: 0.53)
	CHRF 16.51	ROUGE 8.97
2024-02-05 11:40:46,921 Logging Recognition and Translation Outputs
2024-02-05 11:40:46,921 ========================================================================================================================
2024-02-05 11:40:46,922 Logging Sequence: 117_29.00
2024-02-05 11:40:46,923 	Gloss Reference :	A B+C+D+E
2024-02-05 11:40:46,923 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 11:40:46,923 	Gloss Alignment :	         
2024-02-05 11:40:46,923 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 11:40:46,924 	Text Reference  :	however england was unable to reach the target they   were all   out  lost   by  66     runs
2024-02-05 11:40:46,924 	Text Hypothesis :	******* ******* *** ****** ** ***** *** ****** krunal and  rahul took charge and scored 3175
2024-02-05 11:40:46,925 	Text Alignment  :	D       D       D   D      D  D     D   D      S      S    S     S    S      S   S      S   
2024-02-05 11:40:46,925 ========================================================================================================================
2024-02-05 11:40:46,925 Logging Sequence: 84_176.00
2024-02-05 11:40:46,925 	Gloss Reference :	A B+C+D+E
2024-02-05 11:40:46,925 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 11:40:46,925 	Gloss Alignment :	         
2024-02-05 11:40:46,926 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 11:40:46,927 	Text Reference  :	**** germany's nancy faeser who attended the game    in  doha     against japan said
2024-02-05 11:40:46,927 	Text Hypothesis :	when yuvraj    singh into   one day      pm  support and arshdeep with    the   team
2024-02-05 11:40:46,927 	Text Alignment  :	I    S         S     S      S   S        S   S       S   S        S       S     S   
2024-02-05 11:40:46,927 ========================================================================================================================
2024-02-05 11:40:46,928 Logging Sequence: 172_98.00
2024-02-05 11:40:46,928 	Gloss Reference :	A B+C+D+E
2024-02-05 11:40:46,928 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 11:40:46,928 	Gloss Alignment :	         
2024-02-05 11:40:46,928 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 11:40:46,930 	Text Reference  :	******* since    700   pm  it       kept raining the intensity plunged around 915 pm        
2024-02-05 11:40:46,930 	Text Hypothesis :	however arshdeep match the narendra modi stadium the ********* final   before the tournament
2024-02-05 11:40:46,930 	Text Alignment  :	I       S        S     S   S        S    S           D         S       S      S   S         
2024-02-05 11:40:46,930 ========================================================================================================================
2024-02-05 11:40:46,930 Logging Sequence: 135_92.00
2024-02-05 11:40:46,931 	Gloss Reference :	A B+C+D+E
2024-02-05 11:40:46,931 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 11:40:46,931 	Gloss Alignment :	         
2024-02-05 11:40:46,931 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 11:40:46,932 	Text Reference  :	she wrote that half had already been  raised by   the     family's online fundraiser
2024-02-05 11:40:46,932 	Text Hypothesis :	*** ***** **** **** it  is      world cup    that because of       her    sample    
2024-02-05 11:40:46,932 	Text Alignment  :	D   D     D    D    S   S       S     S      S    S       S        S      S         
2024-02-05 11:40:46,932 ========================================================================================================================
2024-02-05 11:40:46,932 Logging Sequence: 180_332.00
2024-02-05 11:40:46,933 	Gloss Reference :	A B+C+D+E
2024-02-05 11:40:46,933 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 11:40:46,933 	Gloss Alignment :	         
2024-02-05 11:40:46,933 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 11:40:46,934 	Text Reference  :	did i eat roti made of shilajit that   i     got  energy to assault so     many girls
2024-02-05 11:40:46,934 	Text Hypothesis :	*** * *** **** **** ** for      taking india gave birth  to ******* donate as   well 
2024-02-05 11:40:46,934 	Text Alignment  :	D   D D   D    D    D  S        S      S     S    S         D       S      S    S    
2024-02-05 11:40:46,934 ========================================================================================================================
2024-02-05 11:40:47,466 Epoch 4889: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 11:40:47,466 EPOCH 4890
2024-02-05 11:41:01,602 Epoch 4890: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 11:41:01,603 EPOCH 4891
2024-02-05 11:41:15,707 Epoch 4891: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 11:41:15,707 EPOCH 4892
2024-02-05 11:41:29,481 Epoch 4892: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 11:41:29,482 EPOCH 4893
2024-02-05 11:41:43,206 Epoch 4893: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 11:41:43,206 EPOCH 4894
2024-02-05 11:41:57,240 Epoch 4894: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 11:41:57,241 EPOCH 4895
2024-02-05 11:42:10,850 Epoch 4895: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 11:42:10,850 EPOCH 4896
2024-02-05 11:42:24,721 Epoch 4896: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-05 11:42:24,721 EPOCH 4897
2024-02-05 11:42:38,758 Epoch 4897: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-05 11:42:38,759 EPOCH 4898
2024-02-05 11:42:52,718 Epoch 4898: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 11:42:52,719 EPOCH 4899
2024-02-05 11:43:06,560 Epoch 4899: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-05 11:43:06,561 EPOCH 4900
2024-02-05 11:43:20,394 [Epoch: 4900 Step: 00044100] Batch Recognition Loss:   0.000106 => Gls Tokens per Sec:      768 || Batch Translation Loss:   0.011832 => Txt Tokens per Sec:     2133 || Lr: 0.000050
2024-02-05 11:43:20,395 Epoch 4900: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 11:43:20,395 EPOCH 4901
2024-02-05 11:43:34,458 Epoch 4901: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 11:43:34,458 EPOCH 4902
2024-02-05 11:43:48,465 Epoch 4902: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 11:43:48,466 EPOCH 4903
2024-02-05 11:44:02,106 Epoch 4903: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 11:44:02,107 EPOCH 4904
2024-02-05 11:44:16,058 Epoch 4904: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 11:44:16,058 EPOCH 4905
2024-02-05 11:44:29,991 Epoch 4905: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-05 11:44:29,992 EPOCH 4906
2024-02-05 11:44:43,934 Epoch 4906: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 11:44:43,934 EPOCH 4907
2024-02-05 11:44:57,819 Epoch 4907: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 11:44:57,820 EPOCH 4908
2024-02-05 11:45:11,991 Epoch 4908: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 11:45:11,991 EPOCH 4909
2024-02-05 11:45:25,602 Epoch 4909: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 11:45:25,603 EPOCH 4910
2024-02-05 11:45:39,860 Epoch 4910: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 11:45:39,861 EPOCH 4911
2024-02-05 11:45:53,735 Epoch 4911: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 11:45:53,735 EPOCH 4912
2024-02-05 11:45:58,811 [Epoch: 4912 Step: 00044200] Batch Recognition Loss:   0.000165 => Gls Tokens per Sec:      252 || Batch Translation Loss:   0.017675 => Txt Tokens per Sec:      885 || Lr: 0.000050
2024-02-05 11:46:07,635 Epoch 4912: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 11:46:07,635 EPOCH 4913
2024-02-05 11:46:21,454 Epoch 4913: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 11:46:21,455 EPOCH 4914
2024-02-05 11:46:35,316 Epoch 4914: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 11:46:35,317 EPOCH 4915
2024-02-05 11:46:48,980 Epoch 4915: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 11:46:48,980 EPOCH 4916
2024-02-05 11:47:02,936 Epoch 4916: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 11:47:02,937 EPOCH 4917
2024-02-05 11:47:16,758 Epoch 4917: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 11:47:16,759 EPOCH 4918
2024-02-05 11:47:30,836 Epoch 4918: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 11:47:30,837 EPOCH 4919
2024-02-05 11:47:44,569 Epoch 4919: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 11:47:44,569 EPOCH 4920
2024-02-05 11:47:58,216 Epoch 4920: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 11:47:58,216 EPOCH 4921
2024-02-05 11:48:12,386 Epoch 4921: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 11:48:12,387 EPOCH 4922
2024-02-05 11:48:26,327 Epoch 4922: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 11:48:26,328 EPOCH 4923
2024-02-05 11:48:26,907 [Epoch: 4923 Step: 00044300] Batch Recognition Loss:   0.000103 => Gls Tokens per Sec:     4429 || Batch Translation Loss:   0.010620 => Txt Tokens per Sec:    10045 || Lr: 0.000050
2024-02-05 11:48:40,398 Epoch 4923: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 11:48:40,399 EPOCH 4924
2024-02-05 11:48:54,349 Epoch 4924: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 11:48:54,349 EPOCH 4925
2024-02-05 11:49:08,378 Epoch 4925: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 11:49:08,379 EPOCH 4926
2024-02-05 11:49:22,329 Epoch 4926: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 11:49:22,330 EPOCH 4927
2024-02-05 11:49:36,238 Epoch 4927: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 11:49:36,239 EPOCH 4928
2024-02-05 11:49:50,378 Epoch 4928: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 11:49:50,379 EPOCH 4929
2024-02-05 11:50:04,122 Epoch 4929: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 11:50:04,123 EPOCH 4930
2024-02-05 11:50:18,037 Epoch 4930: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 11:50:18,038 EPOCH 4931
2024-02-05 11:50:32,507 Epoch 4931: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 11:50:32,508 EPOCH 4932
2024-02-05 11:50:46,203 Epoch 4932: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 11:50:46,204 EPOCH 4933
2024-02-05 11:51:00,193 Epoch 4933: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 11:51:00,194 EPOCH 4934
2024-02-05 11:51:02,611 [Epoch: 4934 Step: 00044400] Batch Recognition Loss:   0.000135 => Gls Tokens per Sec:     1590 || Batch Translation Loss:   0.019868 => Txt Tokens per Sec:     4111 || Lr: 0.000050
2024-02-05 11:51:14,072 Epoch 4934: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 11:51:14,072 EPOCH 4935
2024-02-05 11:51:28,134 Epoch 4935: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 11:51:28,134 EPOCH 4936
2024-02-05 11:51:42,205 Epoch 4936: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 11:51:42,206 EPOCH 4937
2024-02-05 11:51:56,264 Epoch 4937: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 11:51:56,265 EPOCH 4938
2024-02-05 11:52:10,145 Epoch 4938: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 11:52:10,146 EPOCH 4939
2024-02-05 11:52:24,237 Epoch 4939: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 11:52:24,237 EPOCH 4940
2024-02-05 11:52:38,060 Epoch 4940: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 11:52:38,061 EPOCH 4941
2024-02-05 11:52:52,074 Epoch 4941: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 11:52:52,075 EPOCH 4942
2024-02-05 11:53:06,114 Epoch 4942: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 11:53:06,115 EPOCH 4943
2024-02-05 11:53:20,030 Epoch 4943: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 11:53:20,030 EPOCH 4944
2024-02-05 11:53:33,739 Epoch 4944: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 11:53:33,740 EPOCH 4945
2024-02-05 11:53:40,619 [Epoch: 4945 Step: 00044500] Batch Recognition Loss:   0.000142 => Gls Tokens per Sec:      745 || Batch Translation Loss:   0.017808 => Txt Tokens per Sec:     2188 || Lr: 0.000050
2024-02-05 11:53:47,749 Epoch 4945: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 11:53:47,750 EPOCH 4946
2024-02-05 11:54:01,610 Epoch 4946: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 11:54:01,610 EPOCH 4947
2024-02-05 11:54:15,515 Epoch 4947: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 11:54:15,516 EPOCH 4948
2024-02-05 11:54:29,516 Epoch 4948: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 11:54:29,516 EPOCH 4949
2024-02-05 11:54:43,353 Epoch 4949: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 11:54:43,353 EPOCH 4950
2024-02-05 11:54:57,341 Epoch 4950: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 11:54:57,341 EPOCH 4951
2024-02-05 11:55:10,992 Epoch 4951: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 11:55:10,992 EPOCH 4952
2024-02-05 11:55:25,195 Epoch 4952: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 11:55:25,195 EPOCH 4953
2024-02-05 11:55:38,938 Epoch 4953: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 11:55:38,939 EPOCH 4954
2024-02-05 11:55:52,992 Epoch 4954: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 11:55:52,993 EPOCH 4955
2024-02-05 11:56:06,652 Epoch 4955: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 11:56:06,653 EPOCH 4956
2024-02-05 11:56:13,224 [Epoch: 4956 Step: 00044600] Batch Recognition Loss:   0.000096 => Gls Tokens per Sec:      839 || Batch Translation Loss:   0.009896 => Txt Tokens per Sec:     2392 || Lr: 0.000050
2024-02-05 11:56:20,820 Epoch 4956: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 11:56:20,821 EPOCH 4957
2024-02-05 11:56:34,762 Epoch 4957: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 11:56:34,762 EPOCH 4958
2024-02-05 11:56:48,626 Epoch 4958: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 11:56:48,626 EPOCH 4959
2024-02-05 11:57:02,443 Epoch 4959: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 11:57:02,444 EPOCH 4960
2024-02-05 11:57:16,334 Epoch 4960: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 11:57:16,335 EPOCH 4961
2024-02-05 11:57:30,408 Epoch 4961: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 11:57:30,408 EPOCH 4962
2024-02-05 11:57:44,215 Epoch 4962: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 11:57:44,215 EPOCH 4963
2024-02-05 11:57:58,367 Epoch 4963: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 11:57:58,367 EPOCH 4964
2024-02-05 11:58:12,324 Epoch 4964: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 11:58:12,325 EPOCH 4965
2024-02-05 11:58:26,516 Epoch 4965: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 11:58:26,517 EPOCH 4966
2024-02-05 11:58:40,483 Epoch 4966: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 11:58:40,484 EPOCH 4967
2024-02-05 11:58:51,987 [Epoch: 4967 Step: 00044700] Batch Recognition Loss:   0.000142 => Gls Tokens per Sec:      590 || Batch Translation Loss:   0.018978 => Txt Tokens per Sec:     1755 || Lr: 0.000050
2024-02-05 11:58:54,509 Epoch 4967: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 11:58:54,509 EPOCH 4968
2024-02-05 11:59:08,371 Epoch 4968: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 11:59:08,372 EPOCH 4969
2024-02-05 11:59:22,369 Epoch 4969: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 11:59:22,370 EPOCH 4970
2024-02-05 11:59:36,109 Epoch 4970: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 11:59:36,109 EPOCH 4971
2024-02-05 11:59:50,034 Epoch 4971: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 11:59:50,034 EPOCH 4972
2024-02-05 12:00:03,625 Epoch 4972: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 12:00:03,625 EPOCH 4973
2024-02-05 12:00:17,775 Epoch 4973: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 12:00:17,775 EPOCH 4974
2024-02-05 12:00:31,700 Epoch 4974: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 12:00:31,701 EPOCH 4975
2024-02-05 12:00:45,847 Epoch 4975: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 12:00:45,848 EPOCH 4976
2024-02-05 12:00:59,765 Epoch 4976: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 12:00:59,765 EPOCH 4977
2024-02-05 12:01:13,708 Epoch 4977: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 12:01:13,709 EPOCH 4978
2024-02-05 12:01:25,093 [Epoch: 4978 Step: 00044800] Batch Recognition Loss:   0.000165 => Gls Tokens per Sec:      709 || Batch Translation Loss:   0.026282 => Txt Tokens per Sec:     1892 || Lr: 0.000050
2024-02-05 12:01:27,805 Epoch 4978: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 12:01:27,806 EPOCH 4979
2024-02-05 12:01:41,712 Epoch 4979: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-05 12:01:41,713 EPOCH 4980
2024-02-05 12:01:55,820 Epoch 4980: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 12:01:55,821 EPOCH 4981
2024-02-05 12:02:09,441 Epoch 4981: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 12:02:09,442 EPOCH 4982
2024-02-05 12:02:23,490 Epoch 4982: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 12:02:23,490 EPOCH 4983
2024-02-05 12:02:37,262 Epoch 4983: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 12:02:37,263 EPOCH 4984
2024-02-05 12:02:51,560 Epoch 4984: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-05 12:02:51,560 EPOCH 4985
2024-02-05 12:03:05,584 Epoch 4985: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 12:03:05,585 EPOCH 4986
2024-02-05 12:03:19,713 Epoch 4986: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 12:03:19,713 EPOCH 4987
2024-02-05 12:03:33,334 Epoch 4987: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 12:03:33,335 EPOCH 4988
2024-02-05 12:03:47,467 Epoch 4988: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-05 12:03:47,467 EPOCH 4989
2024-02-05 12:04:00,498 [Epoch: 4989 Step: 00044900] Batch Recognition Loss:   0.000284 => Gls Tokens per Sec:      718 || Batch Translation Loss:   0.051931 => Txt Tokens per Sec:     1970 || Lr: 0.000050
2024-02-05 12:04:01,299 Epoch 4989: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-05 12:04:01,300 EPOCH 4990
2024-02-05 12:04:15,187 Epoch 4990: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-05 12:04:15,188 EPOCH 4991
2024-02-05 12:04:28,774 Epoch 4991: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-05 12:04:28,775 EPOCH 4992
2024-02-05 12:04:42,941 Epoch 4992: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-05 12:04:42,942 EPOCH 4993
2024-02-05 12:04:56,963 Epoch 4993: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-05 12:04:56,964 EPOCH 4994
2024-02-05 12:05:11,208 Epoch 4994: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-05 12:05:11,209 EPOCH 4995
2024-02-05 12:05:25,190 Epoch 4995: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-05 12:05:25,191 EPOCH 4996
2024-02-05 12:05:39,061 Epoch 4996: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-05 12:05:39,062 EPOCH 4997
2024-02-05 12:05:52,646 Epoch 4997: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 12:05:52,646 EPOCH 4998
2024-02-05 12:06:06,578 Epoch 4998: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 12:06:06,579 EPOCH 4999
2024-02-05 12:06:20,505 Epoch 4999: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 12:06:20,505 EPOCH 5000
2024-02-05 12:06:34,122 [Epoch: 5000 Step: 00045000] Batch Recognition Loss:   0.000168 => Gls Tokens per Sec:      781 || Batch Translation Loss:   0.024396 => Txt Tokens per Sec:     2167 || Lr: 0.000050
2024-02-05 12:06:34,122 Epoch 5000: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 12:06:34,122 EPOCH 5001
2024-02-05 12:06:47,897 Epoch 5001: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 12:06:47,898 EPOCH 5002
2024-02-05 12:07:01,890 Epoch 5002: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 12:07:01,891 EPOCH 5003
2024-02-05 12:07:15,709 Epoch 5003: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 12:07:15,710 EPOCH 5004
2024-02-05 12:07:29,535 Epoch 5004: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 12:07:29,536 EPOCH 5005
2024-02-05 12:07:43,876 Epoch 5005: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 12:07:43,876 EPOCH 5006
2024-02-05 12:07:57,906 Epoch 5006: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 12:07:57,907 EPOCH 5007
2024-02-05 12:08:11,926 Epoch 5007: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 12:08:11,927 EPOCH 5008
2024-02-05 12:08:25,729 Epoch 5008: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 12:08:25,730 EPOCH 5009
2024-02-05 12:08:39,766 Epoch 5009: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 12:08:39,766 EPOCH 5010
2024-02-05 12:08:53,727 Epoch 5010: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 12:08:53,728 EPOCH 5011
2024-02-05 12:09:07,562 Epoch 5011: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 12:09:07,562 EPOCH 5012
2024-02-05 12:09:09,220 [Epoch: 5012 Step: 00045100] Batch Recognition Loss:   0.000165 => Gls Tokens per Sec:      773 || Batch Translation Loss:   0.019787 => Txt Tokens per Sec:     2474 || Lr: 0.000050
2024-02-05 12:09:21,482 Epoch 5012: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 12:09:21,483 EPOCH 5013
2024-02-05 12:09:35,595 Epoch 5013: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 12:09:35,596 EPOCH 5014
2024-02-05 12:09:49,719 Epoch 5014: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 12:09:49,719 EPOCH 5015
2024-02-05 12:10:03,500 Epoch 5015: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 12:10:03,501 EPOCH 5016
2024-02-05 12:10:17,265 Epoch 5016: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 12:10:17,266 EPOCH 5017
2024-02-05 12:10:31,166 Epoch 5017: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 12:10:31,167 EPOCH 5018
2024-02-05 12:10:44,954 Epoch 5018: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 12:10:44,955 EPOCH 5019
2024-02-05 12:10:58,775 Epoch 5019: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 12:10:58,775 EPOCH 5020
2024-02-05 12:11:12,690 Epoch 5020: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 12:11:12,691 EPOCH 5021
2024-02-05 12:11:26,881 Epoch 5021: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 12:11:26,882 EPOCH 5022
2024-02-05 12:11:41,073 Epoch 5022: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 12:11:41,073 EPOCH 5023
2024-02-05 12:11:45,735 [Epoch: 5023 Step: 00045200] Batch Recognition Loss:   0.000101 => Gls Tokens per Sec:      358 || Batch Translation Loss:   0.009776 => Txt Tokens per Sec:      788 || Lr: 0.000050
2024-02-05 12:11:55,000 Epoch 5023: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 12:11:55,001 EPOCH 5024
2024-02-05 12:12:09,012 Epoch 5024: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 12:12:09,013 EPOCH 5025
2024-02-05 12:12:22,889 Epoch 5025: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 12:12:22,889 EPOCH 5026
2024-02-05 12:12:36,907 Epoch 5026: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 12:12:36,908 EPOCH 5027
2024-02-05 12:12:51,077 Epoch 5027: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 12:12:51,078 EPOCH 5028
2024-02-05 12:13:05,026 Epoch 5028: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 12:13:05,027 EPOCH 5029
2024-02-05 12:13:18,779 Epoch 5029: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 12:13:18,780 EPOCH 5030
2024-02-05 12:13:32,875 Epoch 5030: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 12:13:32,876 EPOCH 5031
2024-02-05 12:13:46,623 Epoch 5031: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 12:13:46,623 EPOCH 5032
2024-02-05 12:14:00,584 Epoch 5032: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 12:14:00,584 EPOCH 5033
2024-02-05 12:14:14,594 Epoch 5033: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 12:14:14,595 EPOCH 5034
2024-02-05 12:14:21,586 [Epoch: 5034 Step: 00045300] Batch Recognition Loss:   0.000190 => Gls Tokens per Sec:      549 || Batch Translation Loss:   0.023852 => Txt Tokens per Sec:     1683 || Lr: 0.000050
2024-02-05 12:14:28,347 Epoch 5034: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 12:14:28,348 EPOCH 5035
2024-02-05 12:14:42,233 Epoch 5035: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 12:14:42,233 EPOCH 5036
2024-02-05 12:14:56,077 Epoch 5036: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 12:14:56,078 EPOCH 5037
2024-02-05 12:15:10,210 Epoch 5037: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 12:15:10,211 EPOCH 5038
2024-02-05 12:15:23,699 Epoch 5038: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 12:15:23,699 EPOCH 5039
2024-02-05 12:15:37,738 Epoch 5039: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-05 12:15:37,738 EPOCH 5040
2024-02-05 12:15:51,722 Epoch 5040: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-05 12:15:51,722 EPOCH 5041
2024-02-05 12:16:05,950 Epoch 5041: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 12:16:05,951 EPOCH 5042
2024-02-05 12:16:19,650 Epoch 5042: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 12:16:19,651 EPOCH 5043
2024-02-05 12:16:33,785 Epoch 5043: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 12:16:33,786 EPOCH 5044
2024-02-05 12:16:48,010 Epoch 5044: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 12:16:48,010 EPOCH 5045
2024-02-05 12:16:53,903 [Epoch: 5045 Step: 00045400] Batch Recognition Loss:   0.000144 => Gls Tokens per Sec:      718 || Batch Translation Loss:   0.007004 => Txt Tokens per Sec:     1916 || Lr: 0.000050
2024-02-05 12:17:02,002 Epoch 5045: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 12:17:02,003 EPOCH 5046
2024-02-05 12:17:16,035 Epoch 5046: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 12:17:16,036 EPOCH 5047
2024-02-05 12:17:29,911 Epoch 5047: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 12:17:29,911 EPOCH 5048
2024-02-05 12:17:43,628 Epoch 5048: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 12:17:43,629 EPOCH 5049
2024-02-05 12:17:57,308 Epoch 5049: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 12:17:57,309 EPOCH 5050
2024-02-05 12:18:11,436 Epoch 5050: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 12:18:11,437 EPOCH 5051
2024-02-05 12:18:25,034 Epoch 5051: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 12:18:25,034 EPOCH 5052
2024-02-05 12:18:39,076 Epoch 5052: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 12:18:39,076 EPOCH 5053
2024-02-05 12:18:53,133 Epoch 5053: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 12:18:53,133 EPOCH 5054
2024-02-05 12:19:07,252 Epoch 5054: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 12:19:07,253 EPOCH 5055
2024-02-05 12:19:21,092 Epoch 5055: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 12:19:21,093 EPOCH 5056
2024-02-05 12:19:25,049 [Epoch: 5056 Step: 00045500] Batch Recognition Loss:   0.000123 => Gls Tokens per Sec:     1618 || Batch Translation Loss:   0.014766 => Txt Tokens per Sec:     4414 || Lr: 0.000050
2024-02-05 12:19:35,140 Epoch 5056: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 12:19:35,141 EPOCH 5057
2024-02-05 12:19:49,446 Epoch 5057: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 12:19:49,447 EPOCH 5058
2024-02-05 12:20:03,429 Epoch 5058: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 12:20:03,430 EPOCH 5059
2024-02-05 12:20:17,332 Epoch 5059: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 12:20:17,333 EPOCH 5060
2024-02-05 12:20:31,265 Epoch 5060: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 12:20:31,266 EPOCH 5061
2024-02-05 12:20:45,295 Epoch 5061: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 12:20:45,296 EPOCH 5062
2024-02-05 12:20:59,101 Epoch 5062: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 12:20:59,102 EPOCH 5063
2024-02-05 12:21:12,713 Epoch 5063: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 12:21:12,714 EPOCH 5064
2024-02-05 12:21:26,617 Epoch 5064: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 12:21:26,618 EPOCH 5065
2024-02-05 12:21:40,092 Epoch 5065: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 12:21:40,093 EPOCH 5066
2024-02-05 12:21:54,216 Epoch 5066: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 12:21:54,216 EPOCH 5067
2024-02-05 12:22:02,076 [Epoch: 5067 Step: 00045600] Batch Recognition Loss:   0.000092 => Gls Tokens per Sec:      864 || Batch Translation Loss:   0.009615 => Txt Tokens per Sec:     2408 || Lr: 0.000050
2024-02-05 12:22:08,067 Epoch 5067: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 12:22:08,068 EPOCH 5068
2024-02-05 12:22:22,086 Epoch 5068: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 12:22:22,086 EPOCH 5069
2024-02-05 12:22:35,881 Epoch 5069: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 12:22:35,881 EPOCH 5070
2024-02-05 12:22:49,899 Epoch 5070: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 12:22:49,900 EPOCH 5071
2024-02-05 12:23:03,966 Epoch 5071: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 12:23:03,966 EPOCH 5072
2024-02-05 12:23:17,666 Epoch 5072: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 12:23:17,667 EPOCH 5073
2024-02-05 12:23:31,608 Epoch 5073: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 12:23:31,609 EPOCH 5074
2024-02-05 12:23:45,765 Epoch 5074: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 12:23:45,765 EPOCH 5075
2024-02-05 12:23:59,884 Epoch 5075: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 12:23:59,885 EPOCH 5076
2024-02-05 12:24:13,950 Epoch 5076: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 12:24:13,950 EPOCH 5077
2024-02-05 12:24:27,895 Epoch 5077: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 12:24:27,896 EPOCH 5078
2024-02-05 12:24:37,059 [Epoch: 5078 Step: 00045700] Batch Recognition Loss:   0.000128 => Gls Tokens per Sec:      978 || Batch Translation Loss:   0.019494 => Txt Tokens per Sec:     2658 || Lr: 0.000050
2024-02-05 12:24:41,933 Epoch 5078: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 12:24:41,934 EPOCH 5079
2024-02-05 12:24:55,777 Epoch 5079: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 12:24:55,777 EPOCH 5080
2024-02-05 12:25:09,890 Epoch 5080: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 12:25:09,890 EPOCH 5081
2024-02-05 12:25:23,686 Epoch 5081: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 12:25:23,687 EPOCH 5082
2024-02-05 12:25:37,565 Epoch 5082: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 12:25:37,566 EPOCH 5083
2024-02-05 12:25:51,610 Epoch 5083: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 12:25:51,611 EPOCH 5084
2024-02-05 12:26:05,762 Epoch 5084: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 12:26:05,763 EPOCH 5085
2024-02-05 12:26:19,903 Epoch 5085: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 12:26:19,904 EPOCH 5086
2024-02-05 12:26:34,271 Epoch 5086: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 12:26:34,272 EPOCH 5087
2024-02-05 12:26:48,130 Epoch 5087: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 12:26:48,131 EPOCH 5088
2024-02-05 12:27:01,977 Epoch 5088: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-05 12:27:01,978 EPOCH 5089
2024-02-05 12:27:15,184 [Epoch: 5089 Step: 00045800] Batch Recognition Loss:   0.000336 => Gls Tokens per Sec:      708 || Batch Translation Loss:   0.622410 => Txt Tokens per Sec:     1940 || Lr: 0.000050
2024-02-05 12:27:15,869 Epoch 5089: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.34 
2024-02-05 12:27:15,869 EPOCH 5090
2024-02-05 12:27:29,644 Epoch 5090: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.63 
2024-02-05 12:27:29,644 EPOCH 5091
2024-02-05 12:27:43,827 Epoch 5091: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.68 
2024-02-05 12:27:43,828 EPOCH 5092
2024-02-05 12:27:57,439 Epoch 5092: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.20 
2024-02-05 12:27:57,440 EPOCH 5093
2024-02-05 12:28:11,286 Epoch 5093: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-05 12:28:11,287 EPOCH 5094
2024-02-05 12:28:25,366 Epoch 5094: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-05 12:28:25,366 EPOCH 5095
2024-02-05 12:28:38,918 Epoch 5095: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-05 12:28:38,919 EPOCH 5096
2024-02-05 12:28:53,054 Epoch 5096: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-05 12:28:53,054 EPOCH 5097
2024-02-05 12:29:06,993 Epoch 5097: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-05 12:29:06,994 EPOCH 5098
2024-02-05 12:29:20,714 Epoch 5098: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-05 12:29:20,714 EPOCH 5099
2024-02-05 12:29:34,543 Epoch 5099: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-05 12:29:34,543 EPOCH 5100
2024-02-05 12:29:48,425 [Epoch: 5100 Step: 00045900] Batch Recognition Loss:   0.000302 => Gls Tokens per Sec:      766 || Batch Translation Loss:   0.028658 => Txt Tokens per Sec:     2126 || Lr: 0.000050
2024-02-05 12:29:48,426 Epoch 5100: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 12:29:48,426 EPOCH 5101
2024-02-05 12:30:02,491 Epoch 5101: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 12:30:02,491 EPOCH 5102
2024-02-05 12:30:16,326 Epoch 5102: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 12:30:16,326 EPOCH 5103
2024-02-05 12:30:30,423 Epoch 5103: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 12:30:30,424 EPOCH 5104
2024-02-05 12:30:44,223 Epoch 5104: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 12:30:44,224 EPOCH 5105
2024-02-05 12:30:58,045 Epoch 5105: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 12:30:58,046 EPOCH 5106
2024-02-05 12:31:12,146 Epoch 5106: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 12:31:12,147 EPOCH 5107
2024-02-05 12:31:26,213 Epoch 5107: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 12:31:26,213 EPOCH 5108
2024-02-05 12:31:40,349 Epoch 5108: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 12:31:40,349 EPOCH 5109
2024-02-05 12:31:54,148 Epoch 5109: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 12:31:54,148 EPOCH 5110
2024-02-05 12:32:08,135 Epoch 5110: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 12:32:08,135 EPOCH 5111
2024-02-05 12:32:22,118 Epoch 5111: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 12:32:22,118 EPOCH 5112
2024-02-05 12:32:22,666 [Epoch: 5112 Step: 00046000] Batch Recognition Loss:   0.000153 => Gls Tokens per Sec:     2339 || Batch Translation Loss:   0.017071 => Txt Tokens per Sec:     6867 || Lr: 0.000050
2024-02-05 12:32:51,859 Validation result at epoch 5112, step    46000: duration: 29.1931s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00063	Translation Loss: 105109.58594	PPL: 36987.00781
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.50	(BLEU-1: 10.52,	BLEU-2: 3.18,	BLEU-3: 1.12,	BLEU-4: 0.50)
	CHRF 16.69	ROUGE 9.01
2024-02-05 12:32:51,860 Logging Recognition and Translation Outputs
2024-02-05 12:32:51,861 ========================================================================================================================
2024-02-05 12:32:51,862 Logging Sequence: 126_121.00
2024-02-05 12:32:51,862 	Gloss Reference :	A B+C+D+E
2024-02-05 12:32:51,862 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 12:32:51,863 	Gloss Alignment :	         
2024-02-05 12:32:51,863 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 12:32:51,863 	Text Reference  :	**** **** * everyone was very    happy by  his victory
2024-02-05 12:32:51,864 	Text Hypothesis :	high such a video    of  decided to    win a   it     
2024-02-05 12:32:51,864 	Text Alignment  :	I    I    I S        S   S       S     S   S   S      
2024-02-05 12:32:51,864 ========================================================================================================================
2024-02-05 12:32:51,864 Logging Sequence: 73_79.00
2024-02-05 12:32:51,864 	Gloss Reference :	A B+C+D+E
2024-02-05 12:32:51,865 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 12:32:51,865 	Gloss Alignment :	         
2024-02-05 12:32:51,865 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 12:32:51,867 	Text Reference  :	raina resturant has food  from the     rich  spices of   north india to   the   aromatic curries of south    india  
2024-02-05 12:32:51,867 	Text Hypothesis :	***** ********* *** there were chicken kebab pani   puri sev   puri  aloo chaat etc      it      is absolute rubbish
2024-02-05 12:32:51,867 	Text Alignment  :	D     D         D   S     S    S       S     S      S    S     S     S    S     S        S       S  S        S      
2024-02-05 12:32:51,867 ========================================================================================================================
2024-02-05 12:32:51,867 Logging Sequence: 95_152.00
2024-02-05 12:32:51,867 	Gloss Reference :	A B+C+D+E
2024-02-05 12:32:51,868 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 12:32:51,868 	Gloss Alignment :	         
2024-02-05 12:32:51,868 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 12:32:51,868 	Text Reference  :	******** *** *** ***** *** ******* ** how strange
2024-02-05 12:32:51,868 	Text Hypothesis :	pakistan are the score was granted by the reason 
2024-02-05 12:32:51,868 	Text Alignment  :	I        I   I   I     I   I       I  S   S      
2024-02-05 12:32:51,869 ========================================================================================================================
2024-02-05 12:32:51,869 Logging Sequence: 135_39.00
2024-02-05 12:32:51,869 	Gloss Reference :	A B+C+D+E
2024-02-05 12:32:51,869 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 12:32:51,869 	Gloss Alignment :	         
2024-02-05 12:32:51,869 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 12:32:51,870 	Text Reference  :	who needs to  travel from poland  to       stanford university in  california
2024-02-05 12:32:51,870 	Text Hypothesis :	but it    was able   to   restart training for      saving     his life      
2024-02-05 12:32:51,870 	Text Alignment  :	S   S     S   S      S    S       S        S        S          S   S         
2024-02-05 12:32:51,870 ========================================================================================================================
2024-02-05 12:32:51,871 Logging Sequence: 87_2.00
2024-02-05 12:32:51,871 	Gloss Reference :	A B+C+D+E
2024-02-05 12:32:51,871 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 12:32:51,871 	Gloss Alignment :	         
2024-02-05 12:32:51,871 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 12:32:51,873 	Text Reference  :	cricketer gautam gambhir's jealousy against ms    dhoni and virat kohli has   been increasing day by     day  
2024-02-05 12:32:51,873 	Text Hypothesis :	support   the    camera    captured their   fifth video and the   same  since the  same       has called dhoni
2024-02-05 12:32:51,873 	Text Alignment  :	S         S      S         S        S       S     S         S     S     S     S    S          S   S      S    
2024-02-05 12:32:51,873 ========================================================================================================================
2024-02-05 12:33:05,661 Epoch 5112: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 12:33:05,662 EPOCH 5113
2024-02-05 12:33:19,579 Epoch 5113: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 12:33:19,580 EPOCH 5114
2024-02-05 12:33:33,436 Epoch 5114: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 12:33:33,437 EPOCH 5115
2024-02-05 12:33:47,194 Epoch 5115: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 12:33:47,195 EPOCH 5116
2024-02-05 12:34:01,173 Epoch 5116: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 12:34:01,173 EPOCH 5117
2024-02-05 12:34:15,324 Epoch 5117: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 12:34:15,324 EPOCH 5118
2024-02-05 12:34:29,263 Epoch 5118: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 12:34:29,264 EPOCH 5119
2024-02-05 12:34:43,057 Epoch 5119: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 12:34:43,057 EPOCH 5120
2024-02-05 12:34:57,371 Epoch 5120: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 12:34:57,372 EPOCH 5121
2024-02-05 12:35:11,342 Epoch 5121: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 12:35:11,343 EPOCH 5122
2024-02-05 12:35:25,402 Epoch 5122: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 12:35:25,403 EPOCH 5123
2024-02-05 12:35:30,694 [Epoch: 5123 Step: 00046100] Batch Recognition Loss:   0.000165 => Gls Tokens per Sec:      484 || Batch Translation Loss:   0.017222 => Txt Tokens per Sec:     1453 || Lr: 0.000050
2024-02-05 12:35:39,321 Epoch 5123: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 12:35:39,322 EPOCH 5124
2024-02-05 12:35:52,855 Epoch 5124: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 12:35:52,856 EPOCH 5125
2024-02-05 12:36:06,550 Epoch 5125: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 12:36:06,551 EPOCH 5126
2024-02-05 12:36:20,511 Epoch 5126: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 12:36:20,512 EPOCH 5127
2024-02-05 12:36:34,543 Epoch 5127: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 12:36:34,544 EPOCH 5128
2024-02-05 12:36:48,443 Epoch 5128: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 12:36:48,443 EPOCH 5129
2024-02-05 12:37:02,280 Epoch 5129: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 12:37:02,281 EPOCH 5130
2024-02-05 12:37:16,425 Epoch 5130: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 12:37:16,425 EPOCH 5131
2024-02-05 12:37:29,912 Epoch 5131: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 12:37:29,913 EPOCH 5132
2024-02-05 12:37:44,045 Epoch 5132: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 12:37:44,045 EPOCH 5133
2024-02-05 12:37:57,951 Epoch 5133: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 12:37:57,951 EPOCH 5134
2024-02-05 12:38:05,678 [Epoch: 5134 Step: 00046200] Batch Recognition Loss:   0.000150 => Gls Tokens per Sec:      497 || Batch Translation Loss:   0.016089 => Txt Tokens per Sec:     1608 || Lr: 0.000050
2024-02-05 12:38:12,286 Epoch 5134: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 12:38:12,287 EPOCH 5135
2024-02-05 12:38:25,969 Epoch 5135: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 12:38:25,970 EPOCH 5136
2024-02-05 12:38:40,016 Epoch 5136: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 12:38:40,017 EPOCH 5137
2024-02-05 12:38:53,745 Epoch 5137: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 12:38:53,746 EPOCH 5138
2024-02-05 12:39:07,758 Epoch 5138: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 12:39:07,758 EPOCH 5139
2024-02-05 12:39:21,502 Epoch 5139: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 12:39:21,503 EPOCH 5140
2024-02-05 12:39:35,767 Epoch 5140: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 12:39:35,767 EPOCH 5141
2024-02-05 12:39:49,671 Epoch 5141: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 12:39:49,672 EPOCH 5142
2024-02-05 12:40:03,720 Epoch 5142: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 12:40:03,721 EPOCH 5143
2024-02-05 12:40:17,637 Epoch 5143: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 12:40:17,638 EPOCH 5144
2024-02-05 12:40:31,501 Epoch 5144: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 12:40:31,501 EPOCH 5145
2024-02-05 12:40:33,453 [Epoch: 5145 Step: 00046300] Batch Recognition Loss:   0.000093 => Gls Tokens per Sec:     2625 || Batch Translation Loss:   0.006126 => Txt Tokens per Sec:     6708 || Lr: 0.000050
2024-02-05 12:40:45,733 Epoch 5145: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 12:40:45,733 EPOCH 5146
2024-02-05 12:40:59,554 Epoch 5146: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 12:40:59,555 EPOCH 5147
2024-02-05 12:41:13,597 Epoch 5147: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 12:41:13,598 EPOCH 5148
2024-02-05 12:41:27,651 Epoch 5148: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 12:41:27,652 EPOCH 5149
2024-02-05 12:41:41,705 Epoch 5149: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 12:41:41,706 EPOCH 5150
2024-02-05 12:41:55,526 Epoch 5150: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 12:41:55,527 EPOCH 5151
2024-02-05 12:42:09,296 Epoch 5151: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 12:42:09,296 EPOCH 5152
2024-02-05 12:42:23,058 Epoch 5152: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 12:42:23,059 EPOCH 5153
2024-02-05 12:42:36,932 Epoch 5153: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 12:42:36,933 EPOCH 5154
2024-02-05 12:42:50,724 Epoch 5154: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 12:42:50,724 EPOCH 5155
2024-02-05 12:43:04,878 Epoch 5155: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 12:43:04,878 EPOCH 5156
2024-02-05 12:43:15,794 [Epoch: 5156 Step: 00046400] Batch Recognition Loss:   0.000115 => Gls Tokens per Sec:      505 || Batch Translation Loss:   0.006054 => Txt Tokens per Sec:     1484 || Lr: 0.000050
2024-02-05 12:43:18,842 Epoch 5156: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 12:43:18,842 EPOCH 5157
2024-02-05 12:43:32,454 Epoch 5157: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 12:43:32,455 EPOCH 5158
2024-02-05 12:43:46,486 Epoch 5158: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 12:43:46,486 EPOCH 5159
2024-02-05 12:44:00,024 Epoch 5159: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 12:44:00,025 EPOCH 5160
2024-02-05 12:44:14,379 Epoch 5160: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 12:44:14,380 EPOCH 5161
2024-02-05 12:44:28,115 Epoch 5161: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 12:44:28,116 EPOCH 5162
2024-02-05 12:44:42,230 Epoch 5162: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 12:44:42,230 EPOCH 5163
2024-02-05 12:44:55,961 Epoch 5163: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 12:44:55,962 EPOCH 5164
2024-02-05 12:45:09,644 Epoch 5164: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 12:45:09,644 EPOCH 5165
2024-02-05 12:45:23,724 Epoch 5165: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 12:45:23,725 EPOCH 5166
2024-02-05 12:45:37,894 Epoch 5166: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 12:45:37,895 EPOCH 5167
2024-02-05 12:45:50,370 [Epoch: 5167 Step: 00046500] Batch Recognition Loss:   0.000094 => Gls Tokens per Sec:      544 || Batch Translation Loss:   0.010211 => Txt Tokens per Sec:     1550 || Lr: 0.000050
2024-02-05 12:45:51,881 Epoch 5167: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 12:45:51,881 EPOCH 5168
2024-02-05 12:46:05,743 Epoch 5168: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 12:46:05,744 EPOCH 5169
2024-02-05 12:46:19,714 Epoch 5169: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 12:46:19,715 EPOCH 5170
2024-02-05 12:46:33,715 Epoch 5170: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 12:46:33,715 EPOCH 5171
2024-02-05 12:46:47,715 Epoch 5171: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 12:46:47,716 EPOCH 5172
2024-02-05 12:47:01,464 Epoch 5172: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 12:47:01,464 EPOCH 5173
2024-02-05 12:47:15,211 Epoch 5173: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 12:47:15,211 EPOCH 5174
2024-02-05 12:47:29,152 Epoch 5174: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 12:47:29,152 EPOCH 5175
2024-02-05 12:47:42,764 Epoch 5175: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 12:47:42,764 EPOCH 5176
2024-02-05 12:47:56,903 Epoch 5176: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 12:47:56,904 EPOCH 5177
2024-02-05 12:48:10,916 Epoch 5177: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 12:48:10,917 EPOCH 5178
2024-02-05 12:48:19,275 [Epoch: 5178 Step: 00046600] Batch Recognition Loss:   0.000158 => Gls Tokens per Sec:      966 || Batch Translation Loss:   0.008291 => Txt Tokens per Sec:     2724 || Lr: 0.000050
2024-02-05 12:48:24,866 Epoch 5178: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 12:48:24,866 EPOCH 5179
2024-02-05 12:48:39,053 Epoch 5179: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 12:48:39,054 EPOCH 5180
2024-02-05 12:48:52,983 Epoch 5180: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 12:48:52,983 EPOCH 5181
2024-02-05 12:49:07,051 Epoch 5181: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 12:49:07,051 EPOCH 5182
2024-02-05 12:49:20,855 Epoch 5182: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 12:49:20,856 EPOCH 5183
2024-02-05 12:49:35,138 Epoch 5183: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 12:49:35,139 EPOCH 5184
2024-02-05 12:49:48,936 Epoch 5184: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 12:49:48,937 EPOCH 5185
2024-02-05 12:50:02,903 Epoch 5185: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 12:50:02,903 EPOCH 5186
2024-02-05 12:50:16,566 Epoch 5186: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 12:50:16,566 EPOCH 5187
2024-02-05 12:50:30,336 Epoch 5187: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 12:50:30,336 EPOCH 5188
2024-02-05 12:50:44,310 Epoch 5188: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 12:50:44,310 EPOCH 5189
2024-02-05 12:50:57,538 [Epoch: 5189 Step: 00046700] Batch Recognition Loss:   0.000108 => Gls Tokens per Sec:      707 || Batch Translation Loss:   0.079800 => Txt Tokens per Sec:     1940 || Lr: 0.000050
2024-02-05 12:50:58,259 Epoch 5189: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-05 12:50:58,260 EPOCH 5190
2024-02-05 12:51:12,461 Epoch 5190: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-05 12:51:12,461 EPOCH 5191
2024-02-05 12:51:26,469 Epoch 5191: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-05 12:51:26,470 EPOCH 5192
2024-02-05 12:51:40,375 Epoch 5192: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-05 12:51:40,375 EPOCH 5193
2024-02-05 12:51:54,360 Epoch 5193: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-05 12:51:54,360 EPOCH 5194
2024-02-05 12:52:08,246 Epoch 5194: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-05 12:52:08,247 EPOCH 5195
2024-02-05 12:52:22,148 Epoch 5195: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 12:52:22,148 EPOCH 5196
2024-02-05 12:52:35,952 Epoch 5196: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 12:52:35,953 EPOCH 5197
2024-02-05 12:52:50,016 Epoch 5197: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 12:52:50,017 EPOCH 5198
2024-02-05 12:53:03,895 Epoch 5198: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 12:53:03,896 EPOCH 5199
2024-02-05 12:53:17,915 Epoch 5199: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 12:53:17,915 EPOCH 5200
2024-02-05 12:53:31,468 [Epoch: 5200 Step: 00046800] Batch Recognition Loss:   0.000151 => Gls Tokens per Sec:      784 || Batch Translation Loss:   0.020826 => Txt Tokens per Sec:     2177 || Lr: 0.000050
2024-02-05 12:53:31,469 Epoch 5200: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 12:53:31,469 EPOCH 5201
2024-02-05 12:53:45,583 Epoch 5201: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 12:53:45,584 EPOCH 5202
2024-02-05 12:53:59,617 Epoch 5202: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 12:53:59,617 EPOCH 5203
2024-02-05 12:54:13,694 Epoch 5203: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 12:54:13,694 EPOCH 5204
2024-02-05 12:54:27,643 Epoch 5204: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 12:54:27,643 EPOCH 5205
2024-02-05 12:54:41,591 Epoch 5205: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 12:54:41,592 EPOCH 5206
2024-02-05 12:54:55,352 Epoch 5206: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 12:54:55,352 EPOCH 5207
2024-02-05 12:55:09,577 Epoch 5207: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 12:55:09,577 EPOCH 5208
2024-02-05 12:55:23,702 Epoch 5208: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 12:55:23,703 EPOCH 5209
2024-02-05 12:55:37,745 Epoch 5209: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 12:55:37,746 EPOCH 5210
2024-02-05 12:55:51,601 Epoch 5210: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 12:55:51,601 EPOCH 5211
2024-02-05 12:56:05,596 Epoch 5211: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 12:56:05,597 EPOCH 5212
2024-02-05 12:56:05,909 [Epoch: 5212 Step: 00046900] Batch Recognition Loss:   0.000110 => Gls Tokens per Sec:     4103 || Batch Translation Loss:   0.012353 => Txt Tokens per Sec:    10397 || Lr: 0.000050
2024-02-05 12:56:19,335 Epoch 5212: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 12:56:19,336 EPOCH 5213
2024-02-05 12:56:33,479 Epoch 5213: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 12:56:33,479 EPOCH 5214
2024-02-05 12:56:47,331 Epoch 5214: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 12:56:47,332 EPOCH 5215
2024-02-05 12:57:01,350 Epoch 5215: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 12:57:01,350 EPOCH 5216
2024-02-05 12:57:15,049 Epoch 5216: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 12:57:15,049 EPOCH 5217
2024-02-05 12:57:29,256 Epoch 5217: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 12:57:29,256 EPOCH 5218
2024-02-05 12:57:42,942 Epoch 5218: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 12:57:42,942 EPOCH 5219
2024-02-05 12:57:57,010 Epoch 5219: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 12:57:57,011 EPOCH 5220
2024-02-05 12:58:10,849 Epoch 5220: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 12:58:10,850 EPOCH 5221
2024-02-05 12:58:24,592 Epoch 5221: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 12:58:24,592 EPOCH 5222
2024-02-05 12:58:37,844 Epoch 5222: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 12:58:37,844 EPOCH 5223
2024-02-05 12:58:38,597 [Epoch: 5223 Step: 00047000] Batch Recognition Loss:   0.000094 => Gls Tokens per Sec:     3409 || Batch Translation Loss:   0.012006 => Txt Tokens per Sec:     6945 || Lr: 0.000050
2024-02-05 12:58:51,918 Epoch 5223: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 12:58:51,918 EPOCH 5224
2024-02-05 12:59:05,947 Epoch 5224: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 12:59:05,948 EPOCH 5225
2024-02-05 12:59:19,921 Epoch 5225: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 12:59:19,921 EPOCH 5226
2024-02-05 12:59:33,763 Epoch 5226: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 12:59:33,764 EPOCH 5227
2024-02-05 12:59:47,764 Epoch 5227: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 12:59:47,764 EPOCH 5228
2024-02-05 13:00:01,611 Epoch 5228: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 13:00:01,612 EPOCH 5229
2024-02-05 13:00:15,502 Epoch 5229: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 13:00:15,503 EPOCH 5230
2024-02-05 13:00:29,496 Epoch 5230: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 13:00:29,496 EPOCH 5231
2024-02-05 13:00:43,428 Epoch 5231: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 13:00:43,429 EPOCH 5232
2024-02-05 13:00:57,345 Epoch 5232: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 13:00:57,346 EPOCH 5233
2024-02-05 13:01:11,248 Epoch 5233: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 13:01:11,248 EPOCH 5234
2024-02-05 13:01:17,016 [Epoch: 5234 Step: 00047100] Batch Recognition Loss:   0.000150 => Gls Tokens per Sec:      666 || Batch Translation Loss:   0.020336 => Txt Tokens per Sec:     1948 || Lr: 0.000050
2024-02-05 13:01:25,086 Epoch 5234: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 13:01:25,087 EPOCH 5235
2024-02-05 13:01:38,940 Epoch 5235: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 13:01:38,941 EPOCH 5236
2024-02-05 13:01:53,015 Epoch 5236: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 13:01:53,016 EPOCH 5237
2024-02-05 13:02:06,839 Epoch 5237: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 13:02:06,839 EPOCH 5238
2024-02-05 13:02:20,703 Epoch 5238: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 13:02:20,704 EPOCH 5239
2024-02-05 13:02:34,864 Epoch 5239: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 13:02:34,865 EPOCH 5240
2024-02-05 13:02:48,889 Epoch 5240: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 13:02:48,890 EPOCH 5241
2024-02-05 13:03:02,812 Epoch 5241: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-05 13:03:02,813 EPOCH 5242
2024-02-05 13:03:16,791 Epoch 5242: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 13:03:16,792 EPOCH 5243
2024-02-05 13:03:30,610 Epoch 5243: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 13:03:30,611 EPOCH 5244
2024-02-05 13:03:44,322 Epoch 5244: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 13:03:44,323 EPOCH 5245
2024-02-05 13:03:45,894 [Epoch: 5245 Step: 00047200] Batch Recognition Loss:   0.000128 => Gls Tokens per Sec:     3260 || Batch Translation Loss:   0.011112 => Txt Tokens per Sec:     8537 || Lr: 0.000050
2024-02-05 13:03:58,071 Epoch 5245: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 13:03:58,072 EPOCH 5246
2024-02-05 13:04:11,633 Epoch 5246: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 13:04:11,634 EPOCH 5247
2024-02-05 13:04:25,442 Epoch 5247: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 13:04:25,443 EPOCH 5248
2024-02-05 13:04:39,197 Epoch 5248: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 13:04:39,198 EPOCH 5249
2024-02-05 13:04:53,158 Epoch 5249: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 13:04:53,159 EPOCH 5250
2024-02-05 13:05:06,973 Epoch 5250: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 13:05:06,973 EPOCH 5251
2024-02-05 13:05:20,743 Epoch 5251: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 13:05:20,744 EPOCH 5252
2024-02-05 13:05:34,789 Epoch 5252: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 13:05:34,790 EPOCH 5253
2024-02-05 13:05:48,773 Epoch 5253: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 13:05:48,774 EPOCH 5254
2024-02-05 13:06:02,586 Epoch 5254: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 13:06:02,587 EPOCH 5255
2024-02-05 13:06:16,421 Epoch 5255: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 13:06:16,421 EPOCH 5256
2024-02-05 13:06:22,702 [Epoch: 5256 Step: 00047300] Batch Recognition Loss:   0.000134 => Gls Tokens per Sec:      877 || Batch Translation Loss:   0.030841 => Txt Tokens per Sec:     2354 || Lr: 0.000050
2024-02-05 13:06:30,366 Epoch 5256: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-05 13:06:30,367 EPOCH 5257
2024-02-05 13:06:44,251 Epoch 5257: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-05 13:06:44,252 EPOCH 5258
2024-02-05 13:06:58,250 Epoch 5258: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-05 13:06:58,250 EPOCH 5259
2024-02-05 13:07:12,298 Epoch 5259: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-05 13:07:12,299 EPOCH 5260
2024-02-05 13:07:26,209 Epoch 5260: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.58 
2024-02-05 13:07:26,210 EPOCH 5261
2024-02-05 13:07:40,302 Epoch 5261: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-05 13:07:40,302 EPOCH 5262
2024-02-05 13:07:54,493 Epoch 5262: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-05 13:07:54,494 EPOCH 5263
2024-02-05 13:08:08,359 Epoch 5263: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-05 13:08:08,360 EPOCH 5264
2024-02-05 13:08:22,414 Epoch 5264: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-05 13:08:22,414 EPOCH 5265
2024-02-05 13:08:36,512 Epoch 5265: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-05 13:08:36,512 EPOCH 5266
2024-02-05 13:08:50,251 Epoch 5266: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-05 13:08:50,251 EPOCH 5267
2024-02-05 13:08:59,142 [Epoch: 5267 Step: 00047400] Batch Recognition Loss:   0.000159 => Gls Tokens per Sec:      864 || Batch Translation Loss:   0.022608 => Txt Tokens per Sec:     2352 || Lr: 0.000050
2024-02-05 13:09:04,607 Epoch 5267: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-05 13:09:04,607 EPOCH 5268
2024-02-05 13:09:18,236 Epoch 5268: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-05 13:09:18,236 EPOCH 5269
2024-02-05 13:09:31,984 Epoch 5269: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-05 13:09:31,985 EPOCH 5270
2024-02-05 13:09:45,962 Epoch 5270: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-05 13:09:45,963 EPOCH 5271
2024-02-05 13:09:59,945 Epoch 5271: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-05 13:09:59,945 EPOCH 5272
2024-02-05 13:10:13,667 Epoch 5272: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-05 13:10:13,668 EPOCH 5273
2024-02-05 13:10:27,814 Epoch 5273: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 13:10:27,814 EPOCH 5274
2024-02-05 13:10:41,573 Epoch 5274: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 13:10:41,573 EPOCH 5275
2024-02-05 13:10:55,579 Epoch 5275: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 13:10:55,580 EPOCH 5276
2024-02-05 13:11:09,457 Epoch 5276: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 13:11:09,457 EPOCH 5277
2024-02-05 13:11:23,491 Epoch 5277: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 13:11:23,492 EPOCH 5278
2024-02-05 13:11:36,349 [Epoch: 5278 Step: 00047500] Batch Recognition Loss:   0.000234 => Gls Tokens per Sec:      628 || Batch Translation Loss:   0.030425 => Txt Tokens per Sec:     1814 || Lr: 0.000050
2024-02-05 13:11:37,410 Epoch 5278: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 13:11:37,410 EPOCH 5279
2024-02-05 13:11:51,404 Epoch 5279: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 13:11:51,404 EPOCH 5280
2024-02-05 13:12:05,071 Epoch 5280: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 13:12:05,072 EPOCH 5281
2024-02-05 13:12:19,253 Epoch 5281: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 13:12:19,254 EPOCH 5282
2024-02-05 13:12:33,381 Epoch 5282: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 13:12:33,382 EPOCH 5283
2024-02-05 13:12:47,162 Epoch 5283: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 13:12:47,163 EPOCH 5284
2024-02-05 13:13:01,114 Epoch 5284: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 13:13:01,115 EPOCH 5285
2024-02-05 13:13:15,028 Epoch 5285: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 13:13:15,028 EPOCH 5286
2024-02-05 13:13:28,740 Epoch 5286: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 13:13:28,740 EPOCH 5287
2024-02-05 13:13:42,565 Epoch 5287: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 13:13:42,565 EPOCH 5288
2024-02-05 13:13:56,712 Epoch 5288: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 13:13:56,713 EPOCH 5289
2024-02-05 13:14:10,372 [Epoch: 5289 Step: 00047600] Batch Recognition Loss:   0.000187 => Gls Tokens per Sec:      685 || Batch Translation Loss:   0.007660 => Txt Tokens per Sec:     1994 || Lr: 0.000050
2024-02-05 13:14:10,666 Epoch 5289: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 13:14:10,666 EPOCH 5290
2024-02-05 13:14:24,501 Epoch 5290: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 13:14:24,502 EPOCH 5291
2024-02-05 13:14:38,509 Epoch 5291: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 13:14:38,510 EPOCH 5292
2024-02-05 13:14:51,936 Epoch 5292: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 13:14:51,937 EPOCH 5293
2024-02-05 13:15:05,922 Epoch 5293: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 13:15:05,923 EPOCH 5294
2024-02-05 13:15:19,678 Epoch 5294: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 13:15:19,679 EPOCH 5295
2024-02-05 13:15:33,601 Epoch 5295: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 13:15:33,601 EPOCH 5296
2024-02-05 13:15:47,572 Epoch 5296: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 13:15:47,572 EPOCH 5297
2024-02-05 13:16:01,407 Epoch 5297: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 13:16:01,407 EPOCH 5298
2024-02-05 13:16:15,454 Epoch 5298: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 13:16:15,455 EPOCH 5299
2024-02-05 13:16:29,650 Epoch 5299: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 13:16:29,650 EPOCH 5300
2024-02-05 13:16:43,770 [Epoch: 5300 Step: 00047700] Batch Recognition Loss:   0.000085 => Gls Tokens per Sec:      753 || Batch Translation Loss:   0.009651 => Txt Tokens per Sec:     2090 || Lr: 0.000050
2024-02-05 13:16:43,771 Epoch 5300: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 13:16:43,771 EPOCH 5301
2024-02-05 13:16:57,624 Epoch 5301: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 13:16:57,625 EPOCH 5302
2024-02-05 13:17:11,838 Epoch 5302: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 13:17:11,838 EPOCH 5303
2024-02-05 13:17:25,632 Epoch 5303: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 13:17:25,633 EPOCH 5304
2024-02-05 13:17:39,618 Epoch 5304: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 13:17:39,619 EPOCH 5305
2024-02-05 13:17:53,672 Epoch 5305: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 13:17:53,672 EPOCH 5306
2024-02-05 13:18:07,349 Epoch 5306: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 13:18:07,350 EPOCH 5307
2024-02-05 13:18:21,253 Epoch 5307: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 13:18:21,254 EPOCH 5308
2024-02-05 13:18:35,475 Epoch 5308: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 13:18:35,475 EPOCH 5309
2024-02-05 13:18:49,324 Epoch 5309: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 13:18:49,324 EPOCH 5310
2024-02-05 13:19:03,300 Epoch 5310: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 13:19:03,301 EPOCH 5311
2024-02-05 13:19:17,364 Epoch 5311: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 13:19:17,364 EPOCH 5312
2024-02-05 13:19:17,817 [Epoch: 5312 Step: 00047800] Batch Recognition Loss:   0.000099 => Gls Tokens per Sec:     2838 || Batch Translation Loss:   0.010175 => Txt Tokens per Sec:     7823 || Lr: 0.000050
2024-02-05 13:19:31,580 Epoch 5312: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 13:19:31,580 EPOCH 5313
2024-02-05 13:19:45,488 Epoch 5313: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 13:19:45,488 EPOCH 5314
2024-02-05 13:19:59,619 Epoch 5314: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 13:19:59,619 EPOCH 5315
2024-02-05 13:20:13,546 Epoch 5315: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 13:20:13,546 EPOCH 5316
2024-02-05 13:20:27,616 Epoch 5316: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 13:20:27,616 EPOCH 5317
2024-02-05 13:20:41,652 Epoch 5317: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 13:20:41,652 EPOCH 5318
2024-02-05 13:20:55,454 Epoch 5318: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 13:20:55,455 EPOCH 5319
2024-02-05 13:21:09,389 Epoch 5319: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 13:21:09,389 EPOCH 5320
2024-02-05 13:21:23,353 Epoch 5320: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 13:21:23,354 EPOCH 5321
2024-02-05 13:21:37,359 Epoch 5321: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 13:21:37,360 EPOCH 5322
2024-02-05 13:21:51,177 Epoch 5322: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 13:21:51,178 EPOCH 5323
2024-02-05 13:22:00,431 [Epoch: 5323 Step: 00047900] Batch Recognition Loss:   0.000130 => Gls Tokens per Sec:      180 || Batch Translation Loss:   0.006947 => Txt Tokens per Sec:      635 || Lr: 0.000050
2024-02-05 13:22:05,072 Epoch 5323: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 13:22:05,072 EPOCH 5324
2024-02-05 13:22:18,515 Epoch 5324: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 13:22:18,516 EPOCH 5325
2024-02-05 13:22:32,558 Epoch 5325: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 13:22:32,559 EPOCH 5326
2024-02-05 13:22:46,231 Epoch 5326: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 13:22:46,231 EPOCH 5327
2024-02-05 13:23:00,480 Epoch 5327: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 13:23:00,481 EPOCH 5328
2024-02-05 13:23:14,358 Epoch 5328: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 13:23:14,358 EPOCH 5329
2024-02-05 13:23:28,207 Epoch 5329: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 13:23:28,208 EPOCH 5330
2024-02-05 13:23:42,104 Epoch 5330: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 13:23:42,104 EPOCH 5331
2024-02-05 13:23:56,276 Epoch 5331: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 13:23:56,277 EPOCH 5332
2024-02-05 13:24:10,178 Epoch 5332: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 13:24:10,179 EPOCH 5333
2024-02-05 13:24:24,000 Epoch 5333: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 13:24:24,001 EPOCH 5334
2024-02-05 13:24:25,180 [Epoch: 5334 Step: 00048000] Batch Recognition Loss:   0.000089 => Gls Tokens per Sec:     3265 || Batch Translation Loss:   0.006847 => Txt Tokens per Sec:     7422 || Lr: 0.000050
2024-02-05 13:24:53,954 Validation result at epoch 5334, step    48000: duration: 28.7740s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00036	Translation Loss: 106898.40625	PPL: 44237.51953
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.50	(BLEU-1: 11.04,	BLEU-2: 3.33,	BLEU-3: 1.16,	BLEU-4: 0.50)
	CHRF 17.00	ROUGE 9.38
2024-02-05 13:24:53,955 Logging Recognition and Translation Outputs
2024-02-05 13:24:53,955 ========================================================================================================================
2024-02-05 13:24:53,956 Logging Sequence: 88_159.00
2024-02-05 13:24:53,956 	Gloss Reference :	A B+C+D+E
2024-02-05 13:24:53,956 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 13:24:53,956 	Gloss Alignment :	         
2024-02-05 13:24:53,956 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 13:24:53,957 	Text Reference  :	however he  often  comes to    the town to meet his    relatives
2024-02-05 13:24:53,957 	Text Hypothesis :	******* the police never filed the **** ** fir  sushil kumar    
2024-02-05 13:24:53,957 	Text Alignment  :	D       S   S      S     S         D    D  S    S      S        
2024-02-05 13:24:53,957 ========================================================================================================================
2024-02-05 13:24:53,958 Logging Sequence: 180_53.00
2024-02-05 13:24:53,958 	Gloss Reference :	A B+C+D+E
2024-02-05 13:24:53,958 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 13:24:53,958 	Gloss Alignment :	         
2024-02-05 13:24:53,958 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 13:24:53,959 	Text Reference  :	******* ** the ******** ***** **** **** ** ** ******* ** protest is  against singh again
2024-02-05 13:24:53,959 	Text Hypothesis :	because of the olympics since they have to be decided to be      has landed  in    india
2024-02-05 13:24:53,959 	Text Alignment  :	I       I      I        I     I    I    I  I  I       I  S       S   S       S     S    
2024-02-05 13:24:53,959 ========================================================================================================================
2024-02-05 13:24:53,960 Logging Sequence: 163_30.00
2024-02-05 13:24:53,960 	Gloss Reference :	A B+C+D+E
2024-02-05 13:24:53,960 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 13:24:53,960 	Gloss Alignment :	         
2024-02-05 13:24:53,960 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 13:24:53,961 	Text Reference  :	**** **** ***** ******* they never permitted anyone to       reveal her face
2024-02-05 13:24:53,961 	Text Hypothesis :	that time usman khawaja who  is    virat     kohli  involved from   the team
2024-02-05 13:24:53,961 	Text Alignment  :	I    I    I     I       S    S     S         S      S        S      S   S   
2024-02-05 13:24:53,961 ========================================================================================================================
2024-02-05 13:24:53,962 Logging Sequence: 51_110.00
2024-02-05 13:24:53,962 	Gloss Reference :	A B+C+D+E
2024-02-05 13:24:53,962 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 13:24:53,962 	Gloss Alignment :	         
2024-02-05 13:24:53,962 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 13:24:53,963 	Text Reference  :	the aussies were very happy with    their victory
2024-02-05 13:24:53,963 	Text Hypothesis :	*** he      took a    new   zealand in    india  
2024-02-05 13:24:53,963 	Text Alignment  :	D   S       S    S    S     S       S     S      
2024-02-05 13:24:53,963 ========================================================================================================================
2024-02-05 13:24:53,963 Logging Sequence: 70_249.00
2024-02-05 13:24:53,964 	Gloss Reference :	A B+C+D+E
2024-02-05 13:24:53,964 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 13:24:53,964 	Gloss Alignment :	         
2024-02-05 13:24:53,964 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 13:24:53,965 	Text Reference  :	******* **** ** have a       look at   this    video   
2024-02-05 13:24:53,965 	Text Hypothesis :	however then on 16th october 2022 very nothing happened
2024-02-05 13:24:53,965 	Text Alignment  :	I       I    I  S    S       S    S    S       S       
2024-02-05 13:24:53,965 ========================================================================================================================
2024-02-05 13:25:07,075 Epoch 5334: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 13:25:07,076 EPOCH 5335
2024-02-05 13:25:20,940 Epoch 5335: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 13:25:20,941 EPOCH 5336
2024-02-05 13:25:35,007 Epoch 5336: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 13:25:35,007 EPOCH 5337
2024-02-05 13:25:49,048 Epoch 5337: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 13:25:49,048 EPOCH 5338
2024-02-05 13:26:02,755 Epoch 5338: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 13:26:02,756 EPOCH 5339
2024-02-05 13:26:16,697 Epoch 5339: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 13:26:16,698 EPOCH 5340
2024-02-05 13:26:30,454 Epoch 5340: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 13:26:30,455 EPOCH 5341
2024-02-05 13:26:44,507 Epoch 5341: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 13:26:44,508 EPOCH 5342
2024-02-05 13:26:58,006 Epoch 5342: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 13:26:58,007 EPOCH 5343
2024-02-05 13:27:11,960 Epoch 5343: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 13:27:11,961 EPOCH 5344
2024-02-05 13:27:26,095 Epoch 5344: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 13:27:26,096 EPOCH 5345
2024-02-05 13:27:29,167 [Epoch: 5345 Step: 00048100] Batch Recognition Loss:   0.000112 => Gls Tokens per Sec:     1668 || Batch Translation Loss:   0.012738 => Txt Tokens per Sec:     4548 || Lr: 0.000050
2024-02-05 13:27:40,038 Epoch 5345: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 13:27:40,039 EPOCH 5346
2024-02-05 13:27:53,763 Epoch 5346: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 13:27:53,764 EPOCH 5347
2024-02-05 13:28:07,819 Epoch 5347: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 13:28:07,819 EPOCH 5348
2024-02-05 13:28:21,641 Epoch 5348: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 13:28:21,641 EPOCH 5349
2024-02-05 13:28:35,622 Epoch 5349: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 13:28:35,623 EPOCH 5350
2024-02-05 13:28:49,602 Epoch 5350: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 13:28:49,603 EPOCH 5351
2024-02-05 13:29:03,437 Epoch 5351: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 13:29:03,438 EPOCH 5352
2024-02-05 13:29:17,214 Epoch 5352: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 13:29:17,215 EPOCH 5353
2024-02-05 13:29:31,119 Epoch 5353: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 13:29:31,120 EPOCH 5354
2024-02-05 13:29:45,143 Epoch 5354: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 13:29:45,143 EPOCH 5355
2024-02-05 13:29:59,003 Epoch 5355: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 13:29:59,004 EPOCH 5356
2024-02-05 13:30:06,204 [Epoch: 5356 Step: 00048200] Batch Recognition Loss:   0.000176 => Gls Tokens per Sec:      765 || Batch Translation Loss:   0.008594 => Txt Tokens per Sec:     2139 || Lr: 0.000050
2024-02-05 13:30:12,826 Epoch 5356: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 13:30:12,826 EPOCH 5357
2024-02-05 13:30:26,912 Epoch 5357: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 13:30:26,913 EPOCH 5358
2024-02-05 13:30:40,981 Epoch 5358: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 13:30:40,982 EPOCH 5359
2024-02-05 13:30:54,904 Epoch 5359: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 13:30:54,905 EPOCH 5360
2024-02-05 13:31:09,022 Epoch 5360: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 13:31:09,023 EPOCH 5361
2024-02-05 13:31:23,063 Epoch 5361: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 13:31:23,064 EPOCH 5362
2024-02-05 13:31:36,929 Epoch 5362: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 13:31:36,930 EPOCH 5363
2024-02-05 13:31:50,738 Epoch 5363: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 13:31:50,738 EPOCH 5364
2024-02-05 13:32:04,775 Epoch 5364: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 13:32:04,775 EPOCH 5365
2024-02-05 13:32:18,711 Epoch 5365: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 13:32:18,712 EPOCH 5366
2024-02-05 13:32:32,179 Epoch 5366: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 13:32:32,180 EPOCH 5367
2024-02-05 13:32:44,896 [Epoch: 5367 Step: 00048300] Batch Recognition Loss:   0.000152 => Gls Tokens per Sec:      534 || Batch Translation Loss:   0.015398 => Txt Tokens per Sec:     1507 || Lr: 0.000050
2024-02-05 13:32:46,357 Epoch 5367: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 13:32:46,358 EPOCH 5368
2024-02-05 13:33:00,399 Epoch 5368: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 13:33:00,400 EPOCH 5369
2024-02-05 13:33:14,142 Epoch 5369: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 13:33:14,143 EPOCH 5370
2024-02-05 13:33:28,122 Epoch 5370: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 13:33:28,123 EPOCH 5371
2024-02-05 13:33:42,063 Epoch 5371: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 13:33:42,064 EPOCH 5372
2024-02-05 13:33:56,001 Epoch 5372: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 13:33:56,002 EPOCH 5373
2024-02-05 13:34:09,943 Epoch 5373: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 13:34:09,944 EPOCH 5374
2024-02-05 13:34:23,686 Epoch 5374: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 13:34:23,687 EPOCH 5375
2024-02-05 13:34:37,769 Epoch 5375: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 13:34:37,770 EPOCH 5376
2024-02-05 13:34:52,057 Epoch 5376: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 13:34:52,058 EPOCH 5377
2024-02-05 13:35:05,979 Epoch 5377: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 13:35:05,979 EPOCH 5378
2024-02-05 13:35:19,048 [Epoch: 5378 Step: 00048400] Batch Recognition Loss:   0.000166 => Gls Tokens per Sec:      618 || Batch Translation Loss:   0.007950 => Txt Tokens per Sec:     1792 || Lr: 0.000050
2024-02-05 13:35:20,135 Epoch 5378: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 13:35:20,136 EPOCH 5379
2024-02-05 13:35:34,043 Epoch 5379: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 13:35:34,044 EPOCH 5380
2024-02-05 13:35:47,995 Epoch 5380: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 13:35:47,996 EPOCH 5381
2024-02-05 13:36:01,820 Epoch 5381: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 13:36:01,820 EPOCH 5382
2024-02-05 13:36:15,819 Epoch 5382: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 13:36:15,819 EPOCH 5383
2024-02-05 13:36:29,759 Epoch 5383: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 13:36:29,759 EPOCH 5384
2024-02-05 13:36:43,877 Epoch 5384: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 13:36:43,877 EPOCH 5385
2024-02-05 13:36:57,676 Epoch 5385: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 13:36:57,677 EPOCH 5386
2024-02-05 13:37:11,805 Epoch 5386: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 13:37:11,805 EPOCH 5387
2024-02-05 13:37:25,595 Epoch 5387: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 13:37:25,596 EPOCH 5388
2024-02-05 13:37:40,057 Epoch 5388: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 13:37:40,058 EPOCH 5389
2024-02-05 13:37:53,134 [Epoch: 5389 Step: 00048500] Batch Recognition Loss:   0.000182 => Gls Tokens per Sec:      715 || Batch Translation Loss:   0.033529 => Txt Tokens per Sec:     1963 || Lr: 0.000050
2024-02-05 13:37:53,867 Epoch 5389: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.57 
2024-02-05 13:37:53,867 EPOCH 5390
2024-02-05 13:38:07,975 Epoch 5390: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.32 
2024-02-05 13:38:07,975 EPOCH 5391
2024-02-05 13:38:21,592 Epoch 5391: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.35 
2024-02-05 13:38:21,593 EPOCH 5392
2024-02-05 13:38:35,495 Epoch 5392: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.20 
2024-02-05 13:38:35,496 EPOCH 5393
2024-02-05 13:38:49,359 Epoch 5393: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.00 
2024-02-05 13:38:49,360 EPOCH 5394
2024-02-05 13:39:03,487 Epoch 5394: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.61 
2024-02-05 13:39:03,488 EPOCH 5395
2024-02-05 13:39:17,166 Epoch 5395: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-05 13:39:17,167 EPOCH 5396
2024-02-05 13:39:30,968 Epoch 5396: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-05 13:39:30,969 EPOCH 5397
2024-02-05 13:39:44,914 Epoch 5397: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-05 13:39:44,915 EPOCH 5398
2024-02-05 13:39:58,883 Epoch 5398: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-05 13:39:58,884 EPOCH 5399
2024-02-05 13:40:13,054 Epoch 5399: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 13:40:13,054 EPOCH 5400
2024-02-05 13:40:27,033 [Epoch: 5400 Step: 00048600] Batch Recognition Loss:   0.000214 => Gls Tokens per Sec:      760 || Batch Translation Loss:   0.030921 => Txt Tokens per Sec:     2111 || Lr: 0.000050
2024-02-05 13:40:27,034 Epoch 5400: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 13:40:27,034 EPOCH 5401
2024-02-05 13:40:40,966 Epoch 5401: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 13:40:40,967 EPOCH 5402
2024-02-05 13:40:54,695 Epoch 5402: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 13:40:54,696 EPOCH 5403
2024-02-05 13:41:08,647 Epoch 5403: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 13:41:08,648 EPOCH 5404
2024-02-05 13:41:22,283 Epoch 5404: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 13:41:22,283 EPOCH 5405
2024-02-05 13:41:36,269 Epoch 5405: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 13:41:36,270 EPOCH 5406
2024-02-05 13:41:50,079 Epoch 5406: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 13:41:50,079 EPOCH 5407
2024-02-05 13:42:04,123 Epoch 5407: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 13:42:04,124 EPOCH 5408
2024-02-05 13:42:18,134 Epoch 5408: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 13:42:18,135 EPOCH 5409
2024-02-05 13:42:32,024 Epoch 5409: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 13:42:32,024 EPOCH 5410
2024-02-05 13:42:46,031 Epoch 5410: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 13:42:46,031 EPOCH 5411
2024-02-05 13:42:59,962 Epoch 5411: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 13:42:59,963 EPOCH 5412
2024-02-05 13:43:00,527 [Epoch: 5412 Step: 00048700] Batch Recognition Loss:   0.000142 => Gls Tokens per Sec:     2270 || Batch Translation Loss:   0.014743 => Txt Tokens per Sec:     6642 || Lr: 0.000050
2024-02-05 13:43:13,385 Epoch 5412: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 13:43:13,385 EPOCH 5413
2024-02-05 13:43:27,295 Epoch 5413: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 13:43:27,296 EPOCH 5414
2024-02-05 13:43:41,619 Epoch 5414: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 13:43:41,620 EPOCH 5415
2024-02-05 13:43:55,358 Epoch 5415: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 13:43:55,358 EPOCH 5416
2024-02-05 13:44:09,386 Epoch 5416: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 13:44:09,387 EPOCH 5417
2024-02-05 13:44:23,099 Epoch 5417: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 13:44:23,099 EPOCH 5418
2024-02-05 13:44:37,157 Epoch 5418: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 13:44:37,157 EPOCH 5419
2024-02-05 13:44:51,043 Epoch 5419: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 13:44:51,044 EPOCH 5420
2024-02-05 13:45:04,959 Epoch 5420: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 13:45:04,960 EPOCH 5421
2024-02-05 13:45:18,844 Epoch 5421: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 13:45:18,845 EPOCH 5422
2024-02-05 13:45:32,627 Epoch 5422: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 13:45:32,628 EPOCH 5423
2024-02-05 13:45:38,349 [Epoch: 5423 Step: 00048800] Batch Recognition Loss:   0.000127 => Gls Tokens per Sec:      448 || Batch Translation Loss:   0.020452 => Txt Tokens per Sec:     1459 || Lr: 0.000050
2024-02-05 13:45:46,475 Epoch 5423: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 13:45:46,476 EPOCH 5424
2024-02-05 13:46:00,456 Epoch 5424: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 13:46:00,457 EPOCH 5425
2024-02-05 13:46:14,515 Epoch 5425: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 13:46:14,515 EPOCH 5426
2024-02-05 13:46:28,387 Epoch 5426: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 13:46:28,388 EPOCH 5427
2024-02-05 13:46:42,250 Epoch 5427: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 13:46:42,251 EPOCH 5428
2024-02-05 13:46:56,101 Epoch 5428: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 13:46:56,102 EPOCH 5429
2024-02-05 13:47:09,823 Epoch 5429: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 13:47:09,824 EPOCH 5430
2024-02-05 13:47:23,795 Epoch 5430: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 13:47:23,796 EPOCH 5431
2024-02-05 13:47:37,555 Epoch 5431: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 13:47:37,556 EPOCH 5432
2024-02-05 13:47:51,717 Epoch 5432: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 13:47:51,718 EPOCH 5433
2024-02-05 13:48:05,547 Epoch 5433: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 13:48:05,548 EPOCH 5434
2024-02-05 13:48:06,965 [Epoch: 5434 Step: 00048900] Batch Recognition Loss:   0.000093 => Gls Tokens per Sec:     2712 || Batch Translation Loss:   0.006506 => Txt Tokens per Sec:     6948 || Lr: 0.000050
2024-02-05 13:48:19,334 Epoch 5434: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 13:48:19,335 EPOCH 5435
2024-02-05 13:48:33,156 Epoch 5435: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 13:48:33,157 EPOCH 5436
2024-02-05 13:48:47,366 Epoch 5436: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 13:48:47,367 EPOCH 5437
2024-02-05 13:49:01,372 Epoch 5437: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 13:49:01,373 EPOCH 5438
2024-02-05 13:49:15,381 Epoch 5438: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 13:49:15,382 EPOCH 5439
2024-02-05 13:49:29,137 Epoch 5439: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 13:49:29,138 EPOCH 5440
2024-02-05 13:49:43,084 Epoch 5440: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 13:49:43,084 EPOCH 5441
2024-02-05 13:49:57,033 Epoch 5441: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 13:49:57,033 EPOCH 5442
2024-02-05 13:50:10,894 Epoch 5442: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 13:50:10,895 EPOCH 5443
2024-02-05 13:50:25,118 Epoch 5443: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 13:50:25,119 EPOCH 5444
2024-02-05 13:50:38,857 Epoch 5444: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 13:50:38,858 EPOCH 5445
2024-02-05 13:50:40,937 [Epoch: 5445 Step: 00049000] Batch Recognition Loss:   0.000112 => Gls Tokens per Sec:     2463 || Batch Translation Loss:   0.021067 => Txt Tokens per Sec:     6783 || Lr: 0.000050
2024-02-05 13:50:52,819 Epoch 5445: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 13:50:52,819 EPOCH 5446
2024-02-05 13:51:06,698 Epoch 5446: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 13:51:06,699 EPOCH 5447
2024-02-05 13:51:20,678 Epoch 5447: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 13:51:20,679 EPOCH 5448
2024-02-05 13:51:34,432 Epoch 5448: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 13:51:34,433 EPOCH 5449
2024-02-05 13:51:48,175 Epoch 5449: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 13:51:48,175 EPOCH 5450
2024-02-05 13:52:02,248 Epoch 5450: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 13:52:02,248 EPOCH 5451
2024-02-05 13:52:16,228 Epoch 5451: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 13:52:16,229 EPOCH 5452
2024-02-05 13:52:30,271 Epoch 5452: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 13:52:30,271 EPOCH 5453
2024-02-05 13:52:44,172 Epoch 5453: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 13:52:44,172 EPOCH 5454
2024-02-05 13:52:57,986 Epoch 5454: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 13:52:57,986 EPOCH 5455
2024-02-05 13:53:11,739 Epoch 5455: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 13:53:11,740 EPOCH 5456
2024-02-05 13:53:22,374 [Epoch: 5456 Step: 00049100] Batch Recognition Loss:   0.000160 => Gls Tokens per Sec:      518 || Batch Translation Loss:   0.021515 => Txt Tokens per Sec:     1408 || Lr: 0.000050
2024-02-05 13:53:25,780 Epoch 5456: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 13:53:25,781 EPOCH 5457
2024-02-05 13:53:39,405 Epoch 5457: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 13:53:39,406 EPOCH 5458
2024-02-05 13:53:53,704 Epoch 5458: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 13:53:53,705 EPOCH 5459
2024-02-05 13:54:07,367 Epoch 5459: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 13:54:07,368 EPOCH 5460
2024-02-05 13:54:21,457 Epoch 5460: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 13:54:21,457 EPOCH 5461
2024-02-05 13:54:35,049 Epoch 5461: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 13:54:35,050 EPOCH 5462
2024-02-05 13:54:49,287 Epoch 5462: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 13:54:49,287 EPOCH 5463
2024-02-05 13:55:03,266 Epoch 5463: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 13:55:03,266 EPOCH 5464
2024-02-05 13:55:17,192 Epoch 5464: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 13:55:17,193 EPOCH 5465
2024-02-05 13:55:31,139 Epoch 5465: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 13:55:31,140 EPOCH 5466
2024-02-05 13:55:44,884 Epoch 5466: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 13:55:44,884 EPOCH 5467
2024-02-05 13:55:52,657 [Epoch: 5467 Step: 00049200] Batch Recognition Loss:   0.000105 => Gls Tokens per Sec:      874 || Batch Translation Loss:   0.013287 => Txt Tokens per Sec:     2248 || Lr: 0.000050
2024-02-05 13:55:58,929 Epoch 5467: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 13:55:58,929 EPOCH 5468
2024-02-05 13:56:13,142 Epoch 5468: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 13:56:13,143 EPOCH 5469
2024-02-05 13:56:26,964 Epoch 5469: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 13:56:26,965 EPOCH 5470
2024-02-05 13:56:41,067 Epoch 5470: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 13:56:41,068 EPOCH 5471
2024-02-05 13:56:54,833 Epoch 5471: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 13:56:54,834 EPOCH 5472
2024-02-05 13:57:08,991 Epoch 5472: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 13:57:08,992 EPOCH 5473
2024-02-05 13:57:22,708 Epoch 5473: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 13:57:22,708 EPOCH 5474
2024-02-05 13:57:36,353 Epoch 5474: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 13:57:36,354 EPOCH 5475
2024-02-05 13:57:50,212 Epoch 5475: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 13:57:50,213 EPOCH 5476
2024-02-05 13:58:03,926 Epoch 5476: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 13:58:03,926 EPOCH 5477
2024-02-05 13:58:18,067 Epoch 5477: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 13:58:18,068 EPOCH 5478
2024-02-05 13:58:26,530 [Epoch: 5478 Step: 00049300] Batch Recognition Loss:   0.000086 => Gls Tokens per Sec:      954 || Batch Translation Loss:   0.007010 => Txt Tokens per Sec:     2511 || Lr: 0.000050
2024-02-05 13:58:32,203 Epoch 5478: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 13:58:32,203 EPOCH 5479
2024-02-05 13:58:46,048 Epoch 5479: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 13:58:46,049 EPOCH 5480
2024-02-05 13:59:00,009 Epoch 5480: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 13:59:00,010 EPOCH 5481
2024-02-05 13:59:13,869 Epoch 5481: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 13:59:13,869 EPOCH 5482
2024-02-05 13:59:27,926 Epoch 5482: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 13:59:27,926 EPOCH 5483
2024-02-05 13:59:41,864 Epoch 5483: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 13:59:41,864 EPOCH 5484
2024-02-05 13:59:55,693 Epoch 5484: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 13:59:55,693 EPOCH 5485
2024-02-05 14:00:09,475 Epoch 5485: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 14:00:09,476 EPOCH 5486
2024-02-05 14:00:23,316 Epoch 5486: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 14:00:23,317 EPOCH 5487
2024-02-05 14:00:37,279 Epoch 5487: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-05 14:00:37,280 EPOCH 5488
2024-02-05 14:00:51,129 Epoch 5488: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-05 14:00:51,129 EPOCH 5489
2024-02-05 14:01:00,028 [Epoch: 5489 Step: 00049400] Batch Recognition Loss:   0.000151 => Gls Tokens per Sec:     1051 || Batch Translation Loss:   0.038059 => Txt Tokens per Sec:     2813 || Lr: 0.000050
2024-02-05 14:01:05,028 Epoch 5489: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-05 14:01:05,028 EPOCH 5490
2024-02-05 14:01:19,015 Epoch 5490: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-05 14:01:19,015 EPOCH 5491
2024-02-05 14:01:32,783 Epoch 5491: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 14:01:32,784 EPOCH 5492
2024-02-05 14:01:46,663 Epoch 5492: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 14:01:46,663 EPOCH 5493
2024-02-05 14:02:00,469 Epoch 5493: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 14:02:00,470 EPOCH 5494
2024-02-05 14:02:14,480 Epoch 5494: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 14:02:14,481 EPOCH 5495
2024-02-05 14:02:28,487 Epoch 5495: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 14:02:28,488 EPOCH 5496
2024-02-05 14:02:42,409 Epoch 5496: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 14:02:42,410 EPOCH 5497
2024-02-05 14:02:56,171 Epoch 5497: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 14:02:56,171 EPOCH 5498
2024-02-05 14:03:09,793 Epoch 5498: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 14:03:09,795 EPOCH 5499
2024-02-05 14:03:24,147 Epoch 5499: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 14:03:24,148 EPOCH 5500
2024-02-05 14:03:38,384 [Epoch: 5500 Step: 00049500] Batch Recognition Loss:   0.000120 => Gls Tokens per Sec:      747 || Batch Translation Loss:   0.020435 => Txt Tokens per Sec:     2073 || Lr: 0.000050
2024-02-05 14:03:38,385 Epoch 5500: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 14:03:38,385 EPOCH 5501
2024-02-05 14:03:52,304 Epoch 5501: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 14:03:52,305 EPOCH 5502
2024-02-05 14:04:06,248 Epoch 5502: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 14:04:06,249 EPOCH 5503
2024-02-05 14:04:20,298 Epoch 5503: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 14:04:20,299 EPOCH 5504
2024-02-05 14:04:34,106 Epoch 5504: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-05 14:04:34,106 EPOCH 5505
2024-02-05 14:04:48,439 Epoch 5505: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-05 14:04:48,440 EPOCH 5506
2024-02-05 14:05:02,276 Epoch 5506: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.94 
2024-02-05 14:05:02,276 EPOCH 5507
2024-02-05 14:05:16,298 Epoch 5507: Total Training Recognition Loss 0.00  Total Training Translation Loss 5.29 
2024-02-05 14:05:16,299 EPOCH 5508
2024-02-05 14:05:30,006 Epoch 5508: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.07 
2024-02-05 14:05:30,007 EPOCH 5509
2024-02-05 14:05:43,860 Epoch 5509: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.50 
2024-02-05 14:05:43,861 EPOCH 5510
2024-02-05 14:05:57,902 Epoch 5510: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.54 
2024-02-05 14:05:57,902 EPOCH 5511
2024-02-05 14:06:11,695 Epoch 5511: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-05 14:06:11,696 EPOCH 5512
2024-02-05 14:06:11,919 [Epoch: 5512 Step: 00049600] Batch Recognition Loss:   0.000323 => Gls Tokens per Sec:     5766 || Batch Translation Loss:   0.067144 => Txt Tokens per Sec:    10090 || Lr: 0.000050
2024-02-05 14:06:25,576 Epoch 5512: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-05 14:06:25,576 EPOCH 5513
2024-02-05 14:06:39,629 Epoch 5513: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-05 14:06:39,629 EPOCH 5514
2024-02-05 14:06:53,442 Epoch 5514: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 14:06:53,443 EPOCH 5515
2024-02-05 14:07:07,519 Epoch 5515: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-05 14:07:07,520 EPOCH 5516
2024-02-05 14:07:21,122 Epoch 5516: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 14:07:21,123 EPOCH 5517
2024-02-05 14:07:35,023 Epoch 5517: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 14:07:35,024 EPOCH 5518
2024-02-05 14:07:48,837 Epoch 5518: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 14:07:48,837 EPOCH 5519
2024-02-05 14:08:02,882 Epoch 5519: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 14:08:02,883 EPOCH 5520
2024-02-05 14:08:16,492 Epoch 5520: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 14:08:16,493 EPOCH 5521
2024-02-05 14:08:30,370 Epoch 5521: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 14:08:30,371 EPOCH 5522
2024-02-05 14:08:44,376 Epoch 5522: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 14:08:44,376 EPOCH 5523
2024-02-05 14:08:46,274 [Epoch: 5523 Step: 00049700] Batch Recognition Loss:   0.000161 => Gls Tokens per Sec:     1350 || Batch Translation Loss:   0.011805 => Txt Tokens per Sec:     3689 || Lr: 0.000050
2024-02-05 14:08:58,128 Epoch 5523: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 14:08:58,129 EPOCH 5524
2024-02-05 14:09:12,267 Epoch 5524: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 14:09:12,268 EPOCH 5525
2024-02-05 14:09:26,484 Epoch 5525: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 14:09:26,484 EPOCH 5526
2024-02-05 14:09:40,267 Epoch 5526: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 14:09:40,267 EPOCH 5527
2024-02-05 14:09:53,908 Epoch 5527: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 14:09:53,909 EPOCH 5528
2024-02-05 14:10:07,910 Epoch 5528: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 14:10:07,910 EPOCH 5529
2024-02-05 14:10:21,719 Epoch 5529: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 14:10:21,720 EPOCH 5530
2024-02-05 14:10:35,637 Epoch 5530: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 14:10:35,638 EPOCH 5531
2024-02-05 14:10:49,685 Epoch 5531: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 14:10:49,686 EPOCH 5532
2024-02-05 14:11:03,442 Epoch 5532: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 14:11:03,443 EPOCH 5533
2024-02-05 14:11:17,375 Epoch 5533: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 14:11:17,376 EPOCH 5534
2024-02-05 14:11:22,539 [Epoch: 5534 Step: 00049800] Batch Recognition Loss:   0.000145 => Gls Tokens per Sec:      571 || Batch Translation Loss:   0.013600 => Txt Tokens per Sec:     1568 || Lr: 0.000050
2024-02-05 14:11:30,971 Epoch 5534: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 14:11:30,971 EPOCH 5535
2024-02-05 14:11:44,970 Epoch 5535: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 14:11:44,970 EPOCH 5536
2024-02-05 14:11:58,890 Epoch 5536: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 14:11:58,890 EPOCH 5537
2024-02-05 14:12:12,750 Epoch 5537: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 14:12:12,751 EPOCH 5538
2024-02-05 14:12:26,668 Epoch 5538: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 14:12:26,669 EPOCH 5539
2024-02-05 14:12:40,553 Epoch 5539: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 14:12:40,553 EPOCH 5540
2024-02-05 14:12:54,081 Epoch 5540: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 14:12:54,081 EPOCH 5541
2024-02-05 14:13:07,944 Epoch 5541: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 14:13:07,944 EPOCH 5542
2024-02-05 14:13:21,885 Epoch 5542: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 14:13:21,885 EPOCH 5543
2024-02-05 14:13:35,935 Epoch 5543: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 14:13:35,935 EPOCH 5544
2024-02-05 14:13:49,834 Epoch 5544: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 14:13:49,835 EPOCH 5545
2024-02-05 14:13:55,564 [Epoch: 5545 Step: 00049900] Batch Recognition Loss:   0.000111 => Gls Tokens per Sec:      738 || Batch Translation Loss:   0.011345 => Txt Tokens per Sec:     2063 || Lr: 0.000050
2024-02-05 14:14:03,799 Epoch 5545: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 14:14:03,799 EPOCH 5546
2024-02-05 14:14:17,686 Epoch 5546: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 14:14:17,687 EPOCH 5547
2024-02-05 14:14:31,535 Epoch 5547: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 14:14:31,535 EPOCH 5548
2024-02-05 14:14:45,747 Epoch 5548: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 14:14:45,748 EPOCH 5549
2024-02-05 14:14:59,755 Epoch 5549: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 14:14:59,755 EPOCH 5550
2024-02-05 14:15:13,593 Epoch 5550: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 14:15:13,593 EPOCH 5551
2024-02-05 14:15:27,216 Epoch 5551: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 14:15:27,217 EPOCH 5552
2024-02-05 14:15:41,157 Epoch 5552: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 14:15:41,157 EPOCH 5553
2024-02-05 14:15:55,123 Epoch 5553: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 14:15:55,123 EPOCH 5554
2024-02-05 14:16:09,042 Epoch 5554: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 14:16:09,043 EPOCH 5555
2024-02-05 14:16:22,631 Epoch 5555: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 14:16:22,632 EPOCH 5556
2024-02-05 14:16:29,186 [Epoch: 5556 Step: 00050000] Batch Recognition Loss:   0.000140 => Gls Tokens per Sec:      977 || Batch Translation Loss:   0.015502 => Txt Tokens per Sec:     2582 || Lr: 0.000050
2024-02-05 14:16:58,090 Validation result at epoch 5556, step    50000: duration: 28.9037s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00040	Translation Loss: 106799.76562	PPL: 43803.01953
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.49	(BLEU-1: 10.39,	BLEU-2: 3.10,	BLEU-3: 1.11,	BLEU-4: 0.49)
	CHRF 16.63	ROUGE 8.74
2024-02-05 14:16:58,091 Logging Recognition and Translation Outputs
2024-02-05 14:16:58,092 ========================================================================================================================
2024-02-05 14:16:58,094 Logging Sequence: 59_58.00
2024-02-05 14:16:58,094 	Gloss Reference :	A B+C+D+E
2024-02-05 14:16:58,094 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 14:16:58,094 	Gloss Alignment :	         
2024-02-05 14:16:58,094 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 14:16:58,095 	Text Reference  :	** to    fix the damage they did not have a       lot of  time      
2024-02-05 14:16:58,095 	Text Hypothesis :	23 years old the ****** **** *** *** **** players who are devastated
2024-02-05 14:16:58,095 	Text Alignment  :	I  S     S       D      D    D   D   D    S       S   S   S         
2024-02-05 14:16:58,096 ========================================================================================================================
2024-02-05 14:16:58,096 Logging Sequence: 165_2.00
2024-02-05 14:16:58,096 	Gloss Reference :	A B+C+D+E
2024-02-05 14:16:58,096 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 14:16:58,096 	Gloss Alignment :	         
2024-02-05 14:16:58,096 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 14:16:58,098 	Text Reference  :	many people believe in  superstitions and think it        brings  good luck   and bad luck 
2024-02-05 14:16:58,098 	Text Hypothesis :	**** in     india   won the           ipl to    celebrate without any  number of  20  overs
2024-02-05 14:16:58,098 	Text Alignment  :	D    S      S       S   S             S   S     S         S       S    S      S   S   S    
2024-02-05 14:16:58,098 ========================================================================================================================
2024-02-05 14:16:58,098 Logging Sequence: 58_147.00
2024-02-05 14:16:58,098 	Gloss Reference :	A B+C+D+E
2024-02-05 14:16:58,098 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 14:16:58,099 	Gloss Alignment :	         
2024-02-05 14:16:58,099 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 14:16:58,100 	Text Reference  :	the women's cricket team grabbed gold by   beating sri lanka in    the finals what a historic win  
2024-02-05 14:16:58,100 	Text Hypothesis :	*** ******* ******* **** let     me   tell you     and know  about the ****** **** * asian    games
2024-02-05 14:16:58,100 	Text Alignment  :	D   D       D       D    S       S    S    S       S   S     S         D      D    D S        S    
2024-02-05 14:16:58,100 ========================================================================================================================
2024-02-05 14:16:58,100 Logging Sequence: 81_139.00
2024-02-05 14:16:58,101 	Gloss Reference :	A B+C+D+E
2024-02-05 14:16:58,101 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 14:16:58,101 	Gloss Alignment :	         
2024-02-05 14:16:58,101 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 14:16:58,102 	Text Reference  :	in 2017 the case was filed first in delhi high    court by rhiti sports   management on       behalf of     dhoni       
2024-02-05 14:16:58,102 	Text Hypothesis :	** now  the **** *** ***** ***** ** ***** supreme court ** ***** canceled the        builder' real   estate registration
2024-02-05 14:16:58,102 	Text Alignment  :	D  S        D    D   D     D     D  D     S             D  D     S        S          S        S      S      S           
2024-02-05 14:16:58,103 ========================================================================================================================
2024-02-05 14:16:58,103 Logging Sequence: 125_72.00
2024-02-05 14:16:58,103 	Gloss Reference :	A B+C+D+E
2024-02-05 14:16:58,103 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 14:16:58,103 	Gloss Alignment :	         
2024-02-05 14:16:58,103 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 14:16:58,104 	Text Reference  :	some      said the pakistani javelineer had milicious intentions of tampering with     the       javelin out of     jealousy
2024-02-05 14:16:58,104 	Text Hypothesis :	yesterday on   the ********* ********** *** ********* ********** ** ********* incident triggered outrage on  social media   
2024-02-05 14:16:58,105 	Text Alignment  :	S         S        D         D          D   D         D          D  D         S        S         S       S   S      S       
2024-02-05 14:16:58,105 ========================================================================================================================
2024-02-05 14:17:05,466 Epoch 5556: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 14:17:05,467 EPOCH 5557
2024-02-05 14:17:19,757 Epoch 5557: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 14:17:19,758 EPOCH 5558
2024-02-05 14:17:33,852 Epoch 5558: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 14:17:33,852 EPOCH 5559
2024-02-05 14:17:47,869 Epoch 5559: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 14:17:47,870 EPOCH 5560
2024-02-05 14:18:01,653 Epoch 5560: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 14:18:01,653 EPOCH 5561
2024-02-05 14:18:15,764 Epoch 5561: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 14:18:15,765 EPOCH 5562
2024-02-05 14:18:29,496 Epoch 5562: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 14:18:29,497 EPOCH 5563
2024-02-05 14:18:43,795 Epoch 5563: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 14:18:43,796 EPOCH 5564
2024-02-05 14:18:57,671 Epoch 5564: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 14:18:57,671 EPOCH 5565
2024-02-05 14:19:11,777 Epoch 5565: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 14:19:11,778 EPOCH 5566
2024-02-05 14:19:25,912 Epoch 5566: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 14:19:25,913 EPOCH 5567
2024-02-05 14:19:33,620 [Epoch: 5567 Step: 00050100] Batch Recognition Loss:   0.000131 => Gls Tokens per Sec:      881 || Batch Translation Loss:   0.012794 => Txt Tokens per Sec:     2449 || Lr: 0.000050
2024-02-05 14:19:39,749 Epoch 5567: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 14:19:39,750 EPOCH 5568
2024-02-05 14:19:53,791 Epoch 5568: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 14:19:53,792 EPOCH 5569
2024-02-05 14:20:07,638 Epoch 5569: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 14:20:07,639 EPOCH 5570
2024-02-05 14:20:21,645 Epoch 5570: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 14:20:21,646 EPOCH 5571
2024-02-05 14:20:35,488 Epoch 5571: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 14:20:35,489 EPOCH 5572
2024-02-05 14:20:49,579 Epoch 5572: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 14:20:49,580 EPOCH 5573
2024-02-05 14:21:03,317 Epoch 5573: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 14:21:03,317 EPOCH 5574
2024-02-05 14:21:17,135 Epoch 5574: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 14:21:17,136 EPOCH 5575
2024-02-05 14:21:31,060 Epoch 5575: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 14:21:31,060 EPOCH 5576
2024-02-05 14:21:44,971 Epoch 5576: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 14:21:44,972 EPOCH 5577
2024-02-05 14:21:58,938 Epoch 5577: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 14:21:58,939 EPOCH 5578
2024-02-05 14:22:11,870 [Epoch: 5578 Step: 00050200] Batch Recognition Loss:   0.000100 => Gls Tokens per Sec:      624 || Batch Translation Loss:   0.007548 => Txt Tokens per Sec:     1744 || Lr: 0.000050
2024-02-05 14:22:12,841 Epoch 5578: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 14:22:12,841 EPOCH 5579
2024-02-05 14:22:26,569 Epoch 5579: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 14:22:26,569 EPOCH 5580
2024-02-05 14:22:40,372 Epoch 5580: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 14:22:40,373 EPOCH 5581
2024-02-05 14:22:54,682 Epoch 5581: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 14:22:54,682 EPOCH 5582
2024-02-05 14:23:08,423 Epoch 5582: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 14:23:08,423 EPOCH 5583
2024-02-05 14:23:22,406 Epoch 5583: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 14:23:22,406 EPOCH 5584
2024-02-05 14:23:35,791 Epoch 5584: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 14:23:35,792 EPOCH 5585
2024-02-05 14:23:49,971 Epoch 5585: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 14:23:49,971 EPOCH 5586
2024-02-05 14:24:03,851 Epoch 5586: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 14:24:03,851 EPOCH 5587
2024-02-05 14:24:18,114 Epoch 5587: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 14:24:18,114 EPOCH 5588
2024-02-05 14:24:32,216 Epoch 5588: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 14:24:32,216 EPOCH 5589
2024-02-05 14:24:41,704 [Epoch: 5589 Step: 00050300] Batch Recognition Loss:   0.000096 => Gls Tokens per Sec:     1079 || Batch Translation Loss:   0.007706 => Txt Tokens per Sec:     2964 || Lr: 0.000050
2024-02-05 14:24:46,012 Epoch 5589: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 14:24:46,012 EPOCH 5590
2024-02-05 14:24:59,946 Epoch 5590: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 14:24:59,947 EPOCH 5591
2024-02-05 14:25:13,683 Epoch 5591: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 14:25:13,683 EPOCH 5592
2024-02-05 14:25:27,493 Epoch 5592: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 14:25:27,493 EPOCH 5593
2024-02-05 14:25:41,390 Epoch 5593: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 14:25:41,390 EPOCH 5594
2024-02-05 14:25:55,222 Epoch 5594: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 14:25:55,222 EPOCH 5595
2024-02-05 14:26:09,121 Epoch 5595: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 14:26:09,122 EPOCH 5596
2024-02-05 14:26:23,221 Epoch 5596: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 14:26:23,221 EPOCH 5597
2024-02-05 14:26:37,038 Epoch 5597: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 14:26:37,039 EPOCH 5598
2024-02-05 14:26:50,764 Epoch 5598: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 14:26:50,764 EPOCH 5599
2024-02-05 14:27:04,619 Epoch 5599: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 14:27:04,620 EPOCH 5600
2024-02-05 14:27:18,699 [Epoch: 5600 Step: 00050400] Batch Recognition Loss:   0.000102 => Gls Tokens per Sec:      755 || Batch Translation Loss:   0.046024 => Txt Tokens per Sec:     2096 || Lr: 0.000050
2024-02-05 14:27:18,699 Epoch 5600: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 14:27:18,700 EPOCH 5601
2024-02-05 14:27:32,674 Epoch 5601: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 14:27:32,675 EPOCH 5602
2024-02-05 14:27:46,676 Epoch 5602: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 14:27:46,677 EPOCH 5603
2024-02-05 14:28:00,459 Epoch 5603: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-05 14:28:00,459 EPOCH 5604
2024-02-05 14:28:14,754 Epoch 5604: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 14:28:14,755 EPOCH 5605
2024-02-05 14:28:28,445 Epoch 5605: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-05 14:28:28,445 EPOCH 5606
2024-02-05 14:28:42,607 Epoch 5606: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-05 14:28:42,608 EPOCH 5607
2024-02-05 14:28:56,583 Epoch 5607: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 14:28:56,584 EPOCH 5608
2024-02-05 14:29:10,437 Epoch 5608: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-05 14:29:10,438 EPOCH 5609
2024-02-05 14:29:24,146 Epoch 5609: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-05 14:29:24,146 EPOCH 5610
2024-02-05 14:29:38,255 Epoch 5610: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 14:29:38,256 EPOCH 5611
2024-02-05 14:29:52,540 Epoch 5611: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 14:29:52,541 EPOCH 5612
2024-02-05 14:29:54,281 [Epoch: 5612 Step: 00050500] Batch Recognition Loss:   0.000222 => Gls Tokens per Sec:      736 || Batch Translation Loss:   0.022541 => Txt Tokens per Sec:     2357 || Lr: 0.000050
2024-02-05 14:30:06,213 Epoch 5612: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 14:30:06,214 EPOCH 5613
2024-02-05 14:30:20,144 Epoch 5613: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 14:30:20,145 EPOCH 5614
2024-02-05 14:30:33,896 Epoch 5614: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 14:30:33,897 EPOCH 5615
2024-02-05 14:30:47,816 Epoch 5615: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 14:30:47,816 EPOCH 5616
2024-02-05 14:31:01,329 Epoch 5616: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 14:31:01,330 EPOCH 5617
2024-02-05 14:31:15,323 Epoch 5617: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 14:31:15,323 EPOCH 5618
2024-02-05 14:31:29,196 Epoch 5618: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 14:31:29,196 EPOCH 5619
2024-02-05 14:31:43,106 Epoch 5619: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 14:31:43,107 EPOCH 5620
2024-02-05 14:31:56,921 Epoch 5620: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 14:31:56,922 EPOCH 5621
2024-02-05 14:32:11,277 Epoch 5621: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 14:32:11,277 EPOCH 5622
2024-02-05 14:32:24,906 Epoch 5622: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 14:32:24,906 EPOCH 5623
2024-02-05 14:32:30,349 [Epoch: 5623 Step: 00050600] Batch Recognition Loss:   0.000100 => Gls Tokens per Sec:      470 || Batch Translation Loss:   0.006402 => Txt Tokens per Sec:     1241 || Lr: 0.000050
2024-02-05 14:32:38,918 Epoch 5623: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 14:32:38,918 EPOCH 5624
2024-02-05 14:32:52,542 Epoch 5624: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 14:32:52,543 EPOCH 5625
2024-02-05 14:33:06,499 Epoch 5625: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 14:33:06,500 EPOCH 5626
2024-02-05 14:33:20,293 Epoch 5626: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 14:33:20,294 EPOCH 5627
2024-02-05 14:33:34,290 Epoch 5627: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 14:33:34,291 EPOCH 5628
2024-02-05 14:33:47,868 Epoch 5628: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 14:33:47,868 EPOCH 5629
2024-02-05 14:34:01,524 Epoch 5629: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 14:34:01,524 EPOCH 5630
2024-02-05 14:34:15,186 Epoch 5630: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 14:34:15,187 EPOCH 5631
2024-02-05 14:34:29,213 Epoch 5631: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 14:34:29,214 EPOCH 5632
2024-02-05 14:34:42,975 Epoch 5632: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 14:34:42,975 EPOCH 5633
2024-02-05 14:34:56,803 Epoch 5633: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 14:34:56,804 EPOCH 5634
2024-02-05 14:35:02,590 [Epoch: 5634 Step: 00050700] Batch Recognition Loss:   0.000123 => Gls Tokens per Sec:      664 || Batch Translation Loss:   0.019749 => Txt Tokens per Sec:     1779 || Lr: 0.000050
2024-02-05 14:35:10,833 Epoch 5634: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 14:35:10,834 EPOCH 5635
2024-02-05 14:35:25,044 Epoch 5635: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 14:35:25,045 EPOCH 5636
2024-02-05 14:35:39,058 Epoch 5636: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 14:35:39,059 EPOCH 5637
2024-02-05 14:35:53,045 Epoch 5637: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 14:35:53,045 EPOCH 5638
2024-02-05 14:36:07,234 Epoch 5638: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 14:36:07,235 EPOCH 5639
2024-02-05 14:36:20,904 Epoch 5639: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 14:36:20,904 EPOCH 5640
2024-02-05 14:36:35,009 Epoch 5640: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 14:36:35,009 EPOCH 5641
2024-02-05 14:36:49,096 Epoch 5641: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 14:36:49,097 EPOCH 5642
2024-02-05 14:37:03,166 Epoch 5642: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 14:37:03,166 EPOCH 5643
2024-02-05 14:37:17,093 Epoch 5643: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 14:37:17,094 EPOCH 5644
2024-02-05 14:37:31,139 Epoch 5644: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 14:37:31,140 EPOCH 5645
2024-02-05 14:37:33,683 [Epoch: 5645 Step: 00050800] Batch Recognition Loss:   0.000091 => Gls Tokens per Sec:     2014 || Batch Translation Loss:   0.011205 => Txt Tokens per Sec:     5127 || Lr: 0.000050
2024-02-05 14:37:44,661 Epoch 5645: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 14:37:44,662 EPOCH 5646
2024-02-05 14:37:58,605 Epoch 5646: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-05 14:37:58,606 EPOCH 5647
2024-02-05 14:38:12,378 Epoch 5647: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-05 14:38:12,379 EPOCH 5648
2024-02-05 14:38:26,534 Epoch 5648: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-05 14:38:26,534 EPOCH 5649
2024-02-05 14:38:40,383 Epoch 5649: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-05 14:38:40,384 EPOCH 5650
2024-02-05 14:38:54,675 Epoch 5650: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 14:38:54,676 EPOCH 5651
2024-02-05 14:39:08,498 Epoch 5651: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 14:39:08,498 EPOCH 5652
2024-02-05 14:39:22,286 Epoch 5652: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 14:39:22,287 EPOCH 5653
2024-02-05 14:39:36,374 Epoch 5653: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 14:39:36,375 EPOCH 5654
2024-02-05 14:39:50,332 Epoch 5654: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 14:39:50,332 EPOCH 5655
2024-02-05 14:40:04,141 Epoch 5655: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 14:40:04,142 EPOCH 5656
2024-02-05 14:40:07,368 [Epoch: 5656 Step: 00050900] Batch Recognition Loss:   0.000102 => Gls Tokens per Sec:     1985 || Batch Translation Loss:   0.006632 => Txt Tokens per Sec:     5030 || Lr: 0.000050
2024-02-05 14:40:18,235 Epoch 5656: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 14:40:18,236 EPOCH 5657
2024-02-05 14:40:31,984 Epoch 5657: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 14:40:31,984 EPOCH 5658
2024-02-05 14:40:46,224 Epoch 5658: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 14:40:46,225 EPOCH 5659
2024-02-05 14:40:59,962 Epoch 5659: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 14:40:59,962 EPOCH 5660
2024-02-05 14:41:14,611 Epoch 5660: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 14:41:14,612 EPOCH 5661
2024-02-05 14:41:29,101 Epoch 5661: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 14:41:29,101 EPOCH 5662
2024-02-05 14:41:43,376 Epoch 5662: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 14:41:43,377 EPOCH 5663
2024-02-05 14:41:57,882 Epoch 5663: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 14:41:57,883 EPOCH 5664
2024-02-05 14:42:11,987 Epoch 5664: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 14:42:11,988 EPOCH 5665
2024-02-05 14:42:26,220 Epoch 5665: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 14:42:26,220 EPOCH 5666
2024-02-05 14:42:40,215 Epoch 5666: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 14:42:40,216 EPOCH 5667
2024-02-05 14:42:47,163 [Epoch: 5667 Step: 00051000] Batch Recognition Loss:   0.000106 => Gls Tokens per Sec:      978 || Batch Translation Loss:   0.012034 => Txt Tokens per Sec:     2692 || Lr: 0.000050
2024-02-05 14:42:54,434 Epoch 5667: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 14:42:54,434 EPOCH 5668
2024-02-05 14:43:08,822 Epoch 5668: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 14:43:08,822 EPOCH 5669
2024-02-05 14:43:22,792 Epoch 5669: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 14:43:22,792 EPOCH 5670
2024-02-05 14:43:36,784 Epoch 5670: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 14:43:36,785 EPOCH 5671
2024-02-05 14:43:50,731 Epoch 5671: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 14:43:50,732 EPOCH 5672
2024-02-05 14:44:04,686 Epoch 5672: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 14:44:04,687 EPOCH 5673
2024-02-05 14:44:18,833 Epoch 5673: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 14:44:18,834 EPOCH 5674
2024-02-05 14:44:33,094 Epoch 5674: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 14:44:33,094 EPOCH 5675
2024-02-05 14:44:47,153 Epoch 5675: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 14:44:47,154 EPOCH 5676
2024-02-05 14:45:01,368 Epoch 5676: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 14:45:01,369 EPOCH 5677
2024-02-05 14:45:15,210 Epoch 5677: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 14:45:15,211 EPOCH 5678
2024-02-05 14:45:26,883 [Epoch: 5678 Step: 00051100] Batch Recognition Loss:   0.000156 => Gls Tokens per Sec:      691 || Batch Translation Loss:   0.008350 => Txt Tokens per Sec:     1846 || Lr: 0.000050
2024-02-05 14:45:29,429 Epoch 5678: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 14:45:29,429 EPOCH 5679
2024-02-05 14:45:43,432 Epoch 5679: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 14:45:43,433 EPOCH 5680
2024-02-05 14:45:57,738 Epoch 5680: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 14:45:57,738 EPOCH 5681
2024-02-05 14:46:11,826 Epoch 5681: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 14:46:11,827 EPOCH 5682
2024-02-05 14:46:25,785 Epoch 5682: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 14:46:25,786 EPOCH 5683
2024-02-05 14:46:40,261 Epoch 5683: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 14:46:40,262 EPOCH 5684
2024-02-05 14:46:54,487 Epoch 5684: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 14:46:54,487 EPOCH 5685
2024-02-05 14:47:08,949 Epoch 5685: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 14:47:08,949 EPOCH 5686
2024-02-05 14:47:23,089 Epoch 5686: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 14:47:23,090 EPOCH 5687
2024-02-05 14:47:37,263 Epoch 5687: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 14:47:37,264 EPOCH 5688
2024-02-05 14:47:51,460 Epoch 5688: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 14:47:51,461 EPOCH 5689
2024-02-05 14:48:04,868 [Epoch: 5689 Step: 00051200] Batch Recognition Loss:   0.000144 => Gls Tokens per Sec:      697 || Batch Translation Loss:   0.017099 => Txt Tokens per Sec:     1923 || Lr: 0.000050
2024-02-05 14:48:05,595 Epoch 5689: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 14:48:05,595 EPOCH 5690
2024-02-05 14:48:19,539 Epoch 5690: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 14:48:19,540 EPOCH 5691
2024-02-05 14:48:33,545 Epoch 5691: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 14:48:33,546 EPOCH 5692
2024-02-05 14:48:47,191 Epoch 5692: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 14:48:47,192 EPOCH 5693
2024-02-05 14:49:00,961 Epoch 5693: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 14:49:00,961 EPOCH 5694
2024-02-05 14:49:15,107 Epoch 5694: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 14:49:15,108 EPOCH 5695
2024-02-05 14:49:29,491 Epoch 5695: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 14:49:29,492 EPOCH 5696
2024-02-05 14:49:43,382 Epoch 5696: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 14:49:43,382 EPOCH 5697
2024-02-05 14:49:57,426 Epoch 5697: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 14:49:57,427 EPOCH 5698
2024-02-05 14:50:11,323 Epoch 5698: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 14:50:11,324 EPOCH 5699
2024-02-05 14:50:25,072 Epoch 5699: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 14:50:25,073 EPOCH 5700
2024-02-05 14:50:39,182 [Epoch: 5700 Step: 00051300] Batch Recognition Loss:   0.000096 => Gls Tokens per Sec:      753 || Batch Translation Loss:   0.010061 => Txt Tokens per Sec:     2092 || Lr: 0.000050
2024-02-05 14:50:39,183 Epoch 5700: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 14:50:39,183 EPOCH 5701
2024-02-05 14:50:53,352 Epoch 5701: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 14:50:53,353 EPOCH 5702
2024-02-05 14:51:07,197 Epoch 5702: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 14:51:07,198 EPOCH 5703
2024-02-05 14:51:21,504 Epoch 5703: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 14:51:21,505 EPOCH 5704
2024-02-05 14:51:35,482 Epoch 5704: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-05 14:51:35,482 EPOCH 5705
2024-02-05 14:51:49,437 Epoch 5705: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-05 14:51:49,438 EPOCH 5706
2024-02-05 14:52:03,410 Epoch 5706: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-05 14:52:03,410 EPOCH 5707
2024-02-05 14:52:17,327 Epoch 5707: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-05 14:52:17,328 EPOCH 5708
2024-02-05 14:52:30,956 Epoch 5708: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.59 
2024-02-05 14:52:30,957 EPOCH 5709
2024-02-05 14:52:44,935 Epoch 5709: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-05 14:52:44,936 EPOCH 5710
2024-02-05 14:52:58,640 Epoch 5710: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.00 
2024-02-05 14:52:58,641 EPOCH 5711
2024-02-05 14:53:13,078 Epoch 5711: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.81 
2024-02-05 14:53:13,078 EPOCH 5712
2024-02-05 14:53:13,402 [Epoch: 5712 Step: 00051400] Batch Recognition Loss:   0.000346 => Gls Tokens per Sec:     3963 || Batch Translation Loss:   0.053159 => Txt Tokens per Sec:     9102 || Lr: 0.000050
2024-02-05 14:53:27,220 Epoch 5712: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.48 
2024-02-05 14:53:27,221 EPOCH 5713
2024-02-05 14:53:41,562 Epoch 5713: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.93 
2024-02-05 14:53:41,562 EPOCH 5714
2024-02-05 14:53:55,849 Epoch 5714: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.90 
2024-02-05 14:53:55,850 EPOCH 5715
2024-02-05 14:54:09,677 Epoch 5715: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-05 14:54:09,678 EPOCH 5716
2024-02-05 14:54:24,135 Epoch 5716: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-05 14:54:24,136 EPOCH 5717
2024-02-05 14:54:38,331 Epoch 5717: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-05 14:54:38,331 EPOCH 5718
2024-02-05 14:54:52,528 Epoch 5718: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 14:54:52,529 EPOCH 5719
2024-02-05 14:55:06,647 Epoch 5719: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 14:55:06,648 EPOCH 5720
2024-02-05 14:55:20,428 Epoch 5720: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 14:55:20,429 EPOCH 5721
2024-02-05 14:55:34,678 Epoch 5721: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 14:55:34,679 EPOCH 5722
2024-02-05 14:55:48,693 Epoch 5722: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 14:55:48,693 EPOCH 5723
2024-02-05 14:55:50,969 [Epoch: 5723 Step: 00051500] Batch Recognition Loss:   0.000278 => Gls Tokens per Sec:     1125 || Batch Translation Loss:   0.019206 => Txt Tokens per Sec:     3499 || Lr: 0.000050
2024-02-05 14:56:02,735 Epoch 5723: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 14:56:02,735 EPOCH 5724
2024-02-05 14:56:17,350 Epoch 5724: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 14:56:17,350 EPOCH 5725
2024-02-05 14:56:31,307 Epoch 5725: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 14:56:31,308 EPOCH 5726
2024-02-05 14:56:45,687 Epoch 5726: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 14:56:45,688 EPOCH 5727
2024-02-05 14:56:59,833 Epoch 5727: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 14:56:59,833 EPOCH 5728
2024-02-05 14:57:14,133 Epoch 5728: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 14:57:14,134 EPOCH 5729
2024-02-05 14:57:27,823 Epoch 5729: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 14:57:27,823 EPOCH 5730
2024-02-05 14:57:42,629 Epoch 5730: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 14:57:42,629 EPOCH 5731
2024-02-05 14:57:56,718 Epoch 5731: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 14:57:56,719 EPOCH 5732
2024-02-05 14:58:10,824 Epoch 5732: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 14:58:10,825 EPOCH 5733
2024-02-05 14:58:24,793 Epoch 5733: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 14:58:24,794 EPOCH 5734
2024-02-05 14:58:25,652 [Epoch: 5734 Step: 00051600] Batch Recognition Loss:   0.000111 => Gls Tokens per Sec:     4484 || Batch Translation Loss:   0.006296 => Txt Tokens per Sec:     9801 || Lr: 0.000050
2024-02-05 14:58:38,548 Epoch 5734: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 14:58:38,549 EPOCH 5735
2024-02-05 14:58:52,333 Epoch 5735: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 14:58:52,334 EPOCH 5736
2024-02-05 14:59:06,532 Epoch 5736: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 14:59:06,533 EPOCH 5737
2024-02-05 14:59:20,622 Epoch 5737: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 14:59:20,623 EPOCH 5738
2024-02-05 14:59:34,442 Epoch 5738: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 14:59:34,443 EPOCH 5739
2024-02-05 14:59:48,555 Epoch 5739: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 14:59:48,556 EPOCH 5740
2024-02-05 15:00:02,807 Epoch 5740: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 15:00:02,808 EPOCH 5741
2024-02-05 15:00:16,707 Epoch 5741: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 15:00:16,707 EPOCH 5742
2024-02-05 15:00:30,850 Epoch 5742: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 15:00:30,850 EPOCH 5743
2024-02-05 15:00:44,592 Epoch 5743: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 15:00:44,592 EPOCH 5744
2024-02-05 15:00:58,839 Epoch 5744: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 15:00:58,840 EPOCH 5745
2024-02-05 15:01:05,167 [Epoch: 5745 Step: 00051700] Batch Recognition Loss:   0.000094 => Gls Tokens per Sec:      809 || Batch Translation Loss:   0.006977 => Txt Tokens per Sec:     2124 || Lr: 0.000050
2024-02-05 15:01:12,910 Epoch 5745: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 15:01:12,911 EPOCH 5746
2024-02-05 15:01:26,990 Epoch 5746: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 15:01:26,990 EPOCH 5747
2024-02-05 15:01:40,736 Epoch 5747: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 15:01:40,736 EPOCH 5748
2024-02-05 15:01:54,779 Epoch 5748: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 15:01:54,780 EPOCH 5749
2024-02-05 15:02:08,940 Epoch 5749: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 15:02:08,940 EPOCH 5750
2024-02-05 15:02:22,716 Epoch 5750: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 15:02:22,717 EPOCH 5751
2024-02-05 15:02:36,687 Epoch 5751: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 15:02:36,687 EPOCH 5752
2024-02-05 15:02:50,840 Epoch 5752: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 15:02:50,840 EPOCH 5753
2024-02-05 15:03:04,629 Epoch 5753: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 15:03:04,630 EPOCH 5754
2024-02-05 15:03:18,887 Epoch 5754: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 15:03:18,888 EPOCH 5755
2024-02-05 15:03:32,820 Epoch 5755: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 15:03:32,820 EPOCH 5756
2024-02-05 15:03:43,275 [Epoch: 5756 Step: 00051800] Batch Recognition Loss:   0.000134 => Gls Tokens per Sec:      527 || Batch Translation Loss:   0.016418 => Txt Tokens per Sec:     1398 || Lr: 0.000050
2024-02-05 15:03:47,014 Epoch 5756: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 15:03:47,014 EPOCH 5757
2024-02-05 15:04:00,756 Epoch 5757: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 15:04:00,756 EPOCH 5758
2024-02-05 15:04:14,727 Epoch 5758: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 15:04:14,728 EPOCH 5759
2024-02-05 15:04:28,725 Epoch 5759: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 15:04:28,726 EPOCH 5760
2024-02-05 15:04:42,775 Epoch 5760: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 15:04:42,775 EPOCH 5761
2024-02-05 15:04:56,916 Epoch 5761: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 15:04:56,916 EPOCH 5762
2024-02-05 15:05:10,807 Epoch 5762: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-05 15:05:10,808 EPOCH 5763
2024-02-05 15:05:24,611 Epoch 5763: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 15:05:24,611 EPOCH 5764
2024-02-05 15:05:38,530 Epoch 5764: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 15:05:38,531 EPOCH 5765
2024-02-05 15:05:52,623 Epoch 5765: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 15:05:52,624 EPOCH 5766
2024-02-05 15:06:06,222 Epoch 5766: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 15:06:06,222 EPOCH 5767
2024-02-05 15:06:17,454 [Epoch: 5767 Step: 00051900] Batch Recognition Loss:   0.000086 => Gls Tokens per Sec:      605 || Batch Translation Loss:   0.009739 => Txt Tokens per Sec:     1584 || Lr: 0.000050
2024-02-05 15:06:20,555 Epoch 5767: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 15:06:20,555 EPOCH 5768
2024-02-05 15:06:34,436 Epoch 5768: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-05 15:06:34,436 EPOCH 5769
2024-02-05 15:06:48,656 Epoch 5769: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 15:06:48,657 EPOCH 5770
2024-02-05 15:07:02,478 Epoch 5770: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 15:07:02,479 EPOCH 5771
2024-02-05 15:07:16,801 Epoch 5771: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 15:07:16,802 EPOCH 5772
2024-02-05 15:07:30,626 Epoch 5772: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 15:07:30,626 EPOCH 5773
2024-02-05 15:07:44,708 Epoch 5773: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 15:07:44,708 EPOCH 5774
2024-02-05 15:07:58,278 Epoch 5774: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 15:07:58,279 EPOCH 5775
2024-02-05 15:08:12,171 Epoch 5775: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 15:08:12,171 EPOCH 5776
2024-02-05 15:08:26,185 Epoch 5776: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 15:08:26,186 EPOCH 5777
2024-02-05 15:08:40,079 Epoch 5777: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 15:08:40,080 EPOCH 5778
2024-02-05 15:08:44,634 [Epoch: 5778 Step: 00052000] Batch Recognition Loss:   0.000101 => Gls Tokens per Sec:     1968 || Batch Translation Loss:   0.011611 => Txt Tokens per Sec:     5186 || Lr: 0.000050
2024-02-05 15:09:13,279 Validation result at epoch 5778, step    52000: duration: 28.6452s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00031	Translation Loss: 106646.02344	PPL: 43134.28516
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.58	(BLEU-1: 11.18,	BLEU-2: 3.37,	BLEU-3: 1.26,	BLEU-4: 0.58)
	CHRF 17.18	ROUGE 9.30
2024-02-05 15:09:13,281 Logging Recognition and Translation Outputs
2024-02-05 15:09:13,281 ========================================================================================================================
2024-02-05 15:09:13,281 Logging Sequence: 87_229.00
2024-02-05 15:09:13,281 	Gloss Reference :	A B+C+D+E
2024-02-05 15:09:13,281 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 15:09:13,281 	Gloss Alignment :	         
2024-02-05 15:09:13,282 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 15:09:13,282 	Text Reference  :	** ***** *** it  was  not  against dhoni or  kohli
2024-02-05 15:09:13,282 	Text Hypothesis :	in india has now they have used    to    sri lanka
2024-02-05 15:09:13,282 	Text Alignment  :	I  I     I   S   S    S    S       S     S   S    
2024-02-05 15:09:13,283 ========================================================================================================================
2024-02-05 15:09:13,283 Logging Sequence: 134_153.00
2024-02-05 15:09:13,283 	Gloss Reference :	A B+C+D+E
2024-02-05 15:09:13,283 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 15:09:13,283 	Gloss Alignment :	         
2024-02-05 15:09:13,283 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 15:09:13,285 	Text Reference  :	pm modi in his interaction said that deaf athletes must fight for their goals and       never give up    despite  the    losses
2024-02-05 15:09:13,285 	Text Hypothesis :	** **** ** *** *********** **** **** **** ******** **** ***** *** it    was   diagnosed with  a    prime minister anurag thakur
2024-02-05 15:09:13,285 	Text Alignment  :	D  D    D  D   D           D    D    D    D        D    D     D   S     S     S         S     S    S     S        S      S     
2024-02-05 15:09:13,285 ========================================================================================================================
2024-02-05 15:09:13,285 Logging Sequence: 137_155.00
2024-02-05 15:09:13,285 	Gloss Reference :	A B+C+D+E
2024-02-05 15:09:13,286 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 15:09:13,286 	Gloss Alignment :	         
2024-02-05 15:09:13,286 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 15:09:13,287 	Text Reference  :	** **** ******** an   extremely high      tax    named as  sin  tax will  be  applied
2024-02-05 15:09:13,287 	Text Hypothesis :	on 12th november 2022 india     champions trophy he    was held in  qatar and 2022   
2024-02-05 15:09:13,287 	Text Alignment  :	I  I    I        S    S         S         S      S     S   S    S   S     S   S      
2024-02-05 15:09:13,287 ========================================================================================================================
2024-02-05 15:09:13,287 Logging Sequence: 59_18.00
2024-02-05 15:09:13,287 	Gloss Reference :	A B+C+D+E
2024-02-05 15:09:13,288 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 15:09:13,288 	Gloss Alignment :	         
2024-02-05 15:09:13,288 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 15:09:13,289 	Text Reference  :	*** *** 27-year-old jessica fox    from    australia won  a    bronze a        gold medal     in   canoeing
2024-02-05 15:09:13,289 	Text Hypothesis :	all the streets     of      london english football  fans were seen   fighting like hooligans with cops    
2024-02-05 15:09:13,289 	Text Alignment  :	I   I   S           S       S      S       S         S    S    S      S        S    S         S    S       
2024-02-05 15:09:13,289 ========================================================================================================================
2024-02-05 15:09:13,290 Logging Sequence: 173_103.00
2024-02-05 15:09:13,290 	Gloss Reference :	A B+C+D+E
2024-02-05 15:09:13,290 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 15:09:13,290 	Gloss Alignment :	         
2024-02-05 15:09:13,290 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 15:09:13,291 	Text Reference  :	** these rumours are absolutely rubbish
2024-02-05 15:09:13,291 	Text Hypothesis :	in the   end     of  the        month  
2024-02-05 15:09:13,291 	Text Alignment  :	I  S     S       S   S          S      
2024-02-05 15:09:13,291 ========================================================================================================================
2024-02-05 15:09:23,034 Epoch 5778: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 15:09:23,035 EPOCH 5779
2024-02-05 15:09:36,825 Epoch 5779: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 15:09:36,826 EPOCH 5780
2024-02-05 15:09:50,840 Epoch 5780: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 15:09:50,841 EPOCH 5781
2024-02-05 15:10:04,754 Epoch 5781: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 15:10:04,754 EPOCH 5782
2024-02-05 15:10:18,435 Epoch 5782: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 15:10:18,435 EPOCH 5783
2024-02-05 15:10:32,556 Epoch 5783: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 15:10:32,556 EPOCH 5784
2024-02-05 15:10:46,497 Epoch 5784: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-05 15:10:46,498 EPOCH 5785
2024-02-05 15:11:00,255 Epoch 5785: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 15:11:00,256 EPOCH 5786
2024-02-05 15:11:14,806 Epoch 5786: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 15:11:14,806 EPOCH 5787
2024-02-05 15:11:30,471 Epoch 5787: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 15:11:30,471 EPOCH 5788
2024-02-05 15:11:45,065 Epoch 5788: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 15:11:45,066 EPOCH 5789
2024-02-05 15:11:59,512 [Epoch: 5789 Step: 00052100] Batch Recognition Loss:   0.000106 => Gls Tokens per Sec:      647 || Batch Translation Loss:   0.021124 => Txt Tokens per Sec:     1886 || Lr: 0.000050
2024-02-05 15:11:59,759 Epoch 5789: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 15:11:59,759 EPOCH 5790
2024-02-05 15:12:14,177 Epoch 5790: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 15:12:14,178 EPOCH 5791
2024-02-05 15:12:28,008 Epoch 5791: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 15:12:28,009 EPOCH 5792
2024-02-05 15:12:42,051 Epoch 5792: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 15:12:42,051 EPOCH 5793
2024-02-05 15:12:55,879 Epoch 5793: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 15:12:55,880 EPOCH 5794
2024-02-05 15:13:09,813 Epoch 5794: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 15:13:09,813 EPOCH 5795
2024-02-05 15:13:23,427 Epoch 5795: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 15:13:23,428 EPOCH 5796
2024-02-05 15:13:37,775 Epoch 5796: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 15:13:37,775 EPOCH 5797
2024-02-05 15:13:51,800 Epoch 5797: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 15:13:51,800 EPOCH 5798
2024-02-05 15:14:06,086 Epoch 5798: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 15:14:06,086 EPOCH 5799
2024-02-05 15:14:20,006 Epoch 5799: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 15:14:20,006 EPOCH 5800
2024-02-05 15:14:34,238 [Epoch: 5800 Step: 00052200] Batch Recognition Loss:   0.000093 => Gls Tokens per Sec:      747 || Batch Translation Loss:   0.009196 => Txt Tokens per Sec:     2074 || Lr: 0.000050
2024-02-05 15:14:34,239 Epoch 5800: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 15:14:34,239 EPOCH 5801
2024-02-05 15:14:48,012 Epoch 5801: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 15:14:48,013 EPOCH 5802
2024-02-05 15:15:02,300 Epoch 5802: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 15:15:02,301 EPOCH 5803
2024-02-05 15:15:16,131 Epoch 5803: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 15:15:16,131 EPOCH 5804
2024-02-05 15:15:30,028 Epoch 5804: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 15:15:30,029 EPOCH 5805
2024-02-05 15:15:44,227 Epoch 5805: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 15:15:44,227 EPOCH 5806
2024-02-05 15:15:58,161 Epoch 5806: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 15:15:58,162 EPOCH 5807
2024-02-05 15:16:12,083 Epoch 5807: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 15:16:12,084 EPOCH 5808
2024-02-05 15:16:26,022 Epoch 5808: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 15:16:26,022 EPOCH 5809
2024-02-05 15:16:39,924 Epoch 5809: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 15:16:39,925 EPOCH 5810
2024-02-05 15:16:54,147 Epoch 5810: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 15:16:54,148 EPOCH 5811
2024-02-05 15:17:07,999 Epoch 5811: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 15:17:07,999 EPOCH 5812
2024-02-05 15:17:13,065 [Epoch: 5812 Step: 00052300] Batch Recognition Loss:   0.000128 => Gls Tokens per Sec:      253 || Batch Translation Loss:   0.016308 => Txt Tokens per Sec:      887 || Lr: 0.000050
2024-02-05 15:17:21,835 Epoch 5812: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 15:17:21,836 EPOCH 5813
2024-02-05 15:17:35,272 Epoch 5813: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 15:17:35,272 EPOCH 5814
2024-02-05 15:17:49,151 Epoch 5814: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 15:17:49,152 EPOCH 5815
2024-02-05 15:18:03,034 Epoch 5815: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 15:18:03,034 EPOCH 5816
2024-02-05 15:18:17,097 Epoch 5816: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 15:18:17,098 EPOCH 5817
2024-02-05 15:18:31,127 Epoch 5817: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 15:18:31,127 EPOCH 5818
2024-02-05 15:18:44,988 Epoch 5818: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 15:18:44,988 EPOCH 5819
2024-02-05 15:18:58,998 Epoch 5819: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 15:18:58,998 EPOCH 5820
2024-02-05 15:19:13,101 Epoch 5820: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 15:19:13,102 EPOCH 5821
2024-02-05 15:19:26,694 Epoch 5821: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 15:19:26,695 EPOCH 5822
2024-02-05 15:19:40,646 Epoch 5822: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 15:19:40,646 EPOCH 5823
2024-02-05 15:19:45,092 [Epoch: 5823 Step: 00052400] Batch Recognition Loss:   0.000162 => Gls Tokens per Sec:      376 || Batch Translation Loss:   0.018444 => Txt Tokens per Sec:      826 || Lr: 0.000050
2024-02-05 15:19:54,373 Epoch 5823: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 15:19:54,373 EPOCH 5824
2024-02-05 15:20:08,477 Epoch 5824: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.75 
2024-02-05 15:20:08,477 EPOCH 5825
2024-02-05 15:20:22,350 Epoch 5825: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.99 
2024-02-05 15:20:22,351 EPOCH 5826
2024-02-05 15:20:36,101 Epoch 5826: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.28 
2024-02-05 15:20:36,101 EPOCH 5827
2024-02-05 15:20:49,907 Epoch 5827: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.62 
2024-02-05 15:20:49,908 EPOCH 5828
2024-02-05 15:21:03,643 Epoch 5828: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.07 
2024-02-05 15:21:03,644 EPOCH 5829
2024-02-05 15:21:17,649 Epoch 5829: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.87 
2024-02-05 15:21:17,650 EPOCH 5830
2024-02-05 15:21:31,761 Epoch 5830: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-05 15:21:31,762 EPOCH 5831
2024-02-05 15:21:45,531 Epoch 5831: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-05 15:21:45,532 EPOCH 5832
2024-02-05 15:21:59,755 Epoch 5832: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-05 15:21:59,756 EPOCH 5833
2024-02-05 15:22:13,684 Epoch 5833: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 15:22:13,684 EPOCH 5834
2024-02-05 15:22:19,163 [Epoch: 5834 Step: 00052500] Batch Recognition Loss:   0.000349 => Gls Tokens per Sec:      538 || Batch Translation Loss:   0.029851 => Txt Tokens per Sec:     1540 || Lr: 0.000050
2024-02-05 15:22:27,720 Epoch 5834: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 15:22:27,720 EPOCH 5835
2024-02-05 15:22:41,262 Epoch 5835: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 15:22:41,263 EPOCH 5836
2024-02-05 15:22:55,338 Epoch 5836: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 15:22:55,339 EPOCH 5837
2024-02-05 15:23:08,924 Epoch 5837: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 15:23:08,925 EPOCH 5838
2024-02-05 15:23:23,140 Epoch 5838: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 15:23:23,141 EPOCH 5839
2024-02-05 15:23:37,176 Epoch 5839: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 15:23:37,177 EPOCH 5840
2024-02-05 15:23:51,114 Epoch 5840: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 15:23:51,114 EPOCH 5841
2024-02-05 15:24:05,105 Epoch 5841: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 15:24:05,106 EPOCH 5842
2024-02-05 15:24:18,987 Epoch 5842: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 15:24:18,988 EPOCH 5843
2024-02-05 15:24:32,878 Epoch 5843: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 15:24:32,878 EPOCH 5844
2024-02-05 15:24:46,641 Epoch 5844: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 15:24:46,641 EPOCH 5845
2024-02-05 15:24:52,681 [Epoch: 5845 Step: 00052600] Batch Recognition Loss:   0.000124 => Gls Tokens per Sec:      848 || Batch Translation Loss:   0.010960 => Txt Tokens per Sec:     2346 || Lr: 0.000050
2024-02-05 15:25:00,365 Epoch 5845: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 15:25:00,365 EPOCH 5846
2024-02-05 15:25:14,188 Epoch 5846: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 15:25:14,189 EPOCH 5847
2024-02-05 15:25:28,200 Epoch 5847: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 15:25:28,201 EPOCH 5848
2024-02-05 15:25:42,071 Epoch 5848: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 15:25:42,072 EPOCH 5849
2024-02-05 15:25:55,814 Epoch 5849: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 15:25:55,815 EPOCH 5850
2024-02-05 15:26:10,066 Epoch 5850: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 15:26:10,067 EPOCH 5851
2024-02-05 15:26:24,019 Epoch 5851: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 15:26:24,019 EPOCH 5852
2024-02-05 15:26:37,777 Epoch 5852: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 15:26:37,777 EPOCH 5853
2024-02-05 15:26:51,580 Epoch 5853: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 15:26:51,580 EPOCH 5854
2024-02-05 15:27:05,297 Epoch 5854: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 15:27:05,298 EPOCH 5855
2024-02-05 15:27:19,185 Epoch 5855: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 15:27:19,186 EPOCH 5856
2024-02-05 15:27:25,587 [Epoch: 5856 Step: 00052700] Batch Recognition Loss:   0.000141 => Gls Tokens per Sec:      861 || Batch Translation Loss:   0.019493 => Txt Tokens per Sec:     2332 || Lr: 0.000050
2024-02-05 15:27:33,094 Epoch 5856: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 15:27:33,094 EPOCH 5857
2024-02-05 15:27:46,982 Epoch 5857: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 15:27:46,982 EPOCH 5858
2024-02-05 15:28:00,789 Epoch 5858: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 15:28:00,790 EPOCH 5859
2024-02-05 15:28:14,463 Epoch 5859: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 15:28:14,463 EPOCH 5860
2024-02-05 15:28:28,504 Epoch 5860: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 15:28:28,504 EPOCH 5861
2024-02-05 15:28:42,500 Epoch 5861: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 15:28:42,501 EPOCH 5862
2024-02-05 15:28:56,281 Epoch 5862: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 15:28:56,281 EPOCH 5863
2024-02-05 15:29:10,272 Epoch 5863: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 15:29:10,273 EPOCH 5864
2024-02-05 15:29:24,163 Epoch 5864: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 15:29:24,164 EPOCH 5865
2024-02-05 15:29:37,708 Epoch 5865: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 15:29:37,708 EPOCH 5866
2024-02-05 15:29:51,740 Epoch 5866: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 15:29:51,740 EPOCH 5867
2024-02-05 15:30:00,025 [Epoch: 5867 Step: 00052800] Batch Recognition Loss:   0.000079 => Gls Tokens per Sec:      927 || Batch Translation Loss:   0.009158 => Txt Tokens per Sec:     2471 || Lr: 0.000050
2024-02-05 15:30:05,734 Epoch 5867: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 15:30:05,734 EPOCH 5868
2024-02-05 15:30:19,570 Epoch 5868: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 15:30:19,570 EPOCH 5869
2024-02-05 15:30:33,373 Epoch 5869: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 15:30:33,374 EPOCH 5870
2024-02-05 15:30:47,223 Epoch 5870: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 15:30:47,224 EPOCH 5871
2024-02-05 15:31:01,050 Epoch 5871: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 15:31:01,051 EPOCH 5872
2024-02-05 15:31:14,986 Epoch 5872: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 15:31:14,987 EPOCH 5873
2024-02-05 15:31:28,903 Epoch 5873: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 15:31:28,904 EPOCH 5874
2024-02-05 15:31:42,625 Epoch 5874: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 15:31:42,625 EPOCH 5875
2024-02-05 15:31:56,674 Epoch 5875: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 15:31:56,674 EPOCH 5876
2024-02-05 15:32:10,476 Epoch 5876: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 15:32:10,477 EPOCH 5877
2024-02-05 15:32:24,441 Epoch 5877: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 15:32:24,442 EPOCH 5878
2024-02-05 15:32:37,561 [Epoch: 5878 Step: 00052900] Batch Recognition Loss:   0.000104 => Gls Tokens per Sec:      615 || Batch Translation Loss:   0.010959 => Txt Tokens per Sec:     1783 || Lr: 0.000050
2024-02-05 15:32:38,403 Epoch 5878: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 15:32:38,404 EPOCH 5879
2024-02-05 15:32:52,278 Epoch 5879: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 15:32:52,279 EPOCH 5880
2024-02-05 15:33:06,178 Epoch 5880: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 15:33:06,178 EPOCH 5881
2024-02-05 15:33:20,058 Epoch 5881: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 15:33:20,059 EPOCH 5882
2024-02-05 15:33:33,869 Epoch 5882: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-05 15:33:33,869 EPOCH 5883
2024-02-05 15:33:47,733 Epoch 5883: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 15:33:47,733 EPOCH 5884
2024-02-05 15:34:01,799 Epoch 5884: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 15:34:01,800 EPOCH 5885
2024-02-05 15:34:15,843 Epoch 5885: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 15:34:15,844 EPOCH 5886
2024-02-05 15:34:29,538 Epoch 5886: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 15:34:29,538 EPOCH 5887
2024-02-05 15:34:43,352 Epoch 5887: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 15:34:43,353 EPOCH 5888
2024-02-05 15:34:57,335 Epoch 5888: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 15:34:57,335 EPOCH 5889
2024-02-05 15:35:11,138 [Epoch: 5889 Step: 00053000] Batch Recognition Loss:   0.000095 => Gls Tokens per Sec:      677 || Batch Translation Loss:   0.014682 => Txt Tokens per Sec:     1974 || Lr: 0.000050
2024-02-05 15:35:11,383 Epoch 5889: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 15:35:11,384 EPOCH 5890
2024-02-05 15:35:25,162 Epoch 5890: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 15:35:25,163 EPOCH 5891
2024-02-05 15:35:39,256 Epoch 5891: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 15:35:39,257 EPOCH 5892
2024-02-05 15:35:53,338 Epoch 5892: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 15:35:53,338 EPOCH 5893
2024-02-05 15:36:07,204 Epoch 5893: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 15:36:07,205 EPOCH 5894
2024-02-05 15:36:20,960 Epoch 5894: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 15:36:20,961 EPOCH 5895
2024-02-05 15:36:34,776 Epoch 5895: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 15:36:34,776 EPOCH 5896
2024-02-05 15:36:48,623 Epoch 5896: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 15:36:48,624 EPOCH 5897
2024-02-05 15:37:02,443 Epoch 5897: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 15:37:02,444 EPOCH 5898
2024-02-05 15:37:16,390 Epoch 5898: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 15:37:16,390 EPOCH 5899
2024-02-05 15:37:30,421 Epoch 5899: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 15:37:30,422 EPOCH 5900
2024-02-05 15:37:44,197 [Epoch: 5900 Step: 00053100] Batch Recognition Loss:   0.000188 => Gls Tokens per Sec:      772 || Batch Translation Loss:   0.022140 => Txt Tokens per Sec:     2142 || Lr: 0.000050
2024-02-05 15:37:44,197 Epoch 5900: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 15:37:44,197 EPOCH 5901
2024-02-05 15:37:58,465 Epoch 5901: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 15:37:58,465 EPOCH 5902
2024-02-05 15:38:12,143 Epoch 5902: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 15:38:12,144 EPOCH 5903
2024-02-05 15:38:26,225 Epoch 5903: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 15:38:26,225 EPOCH 5904
2024-02-05 15:38:39,847 Epoch 5904: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 15:38:39,847 EPOCH 5905
2024-02-05 15:38:53,964 Epoch 5905: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 15:38:53,965 EPOCH 5906
2024-02-05 15:39:07,746 Epoch 5906: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 15:39:07,747 EPOCH 5907
2024-02-05 15:39:21,573 Epoch 5907: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 15:39:21,574 EPOCH 5908
2024-02-05 15:39:35,416 Epoch 5908: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 15:39:35,416 EPOCH 5909
2024-02-05 15:39:49,358 Epoch 5909: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 15:39:49,359 EPOCH 5910
2024-02-05 15:40:03,430 Epoch 5910: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 15:40:03,431 EPOCH 5911
2024-02-05 15:40:17,362 Epoch 5911: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 15:40:17,362 EPOCH 5912
2024-02-05 15:40:17,961 [Epoch: 5912 Step: 00053200] Batch Recognition Loss:   0.000105 => Gls Tokens per Sec:     2144 || Batch Translation Loss:   0.014436 => Txt Tokens per Sec:     5881 || Lr: 0.000050
2024-02-05 15:40:31,369 Epoch 5912: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 15:40:31,370 EPOCH 5913
2024-02-05 15:40:44,798 Epoch 5913: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 15:40:44,799 EPOCH 5914
2024-02-05 15:40:58,920 Epoch 5914: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 15:40:58,920 EPOCH 5915
2024-02-05 15:41:12,697 Epoch 5915: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 15:41:12,698 EPOCH 5916
2024-02-05 15:41:26,419 Epoch 5916: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 15:41:26,419 EPOCH 5917
2024-02-05 15:41:40,595 Epoch 5917: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 15:41:40,596 EPOCH 5918
2024-02-05 15:41:54,487 Epoch 5918: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 15:41:54,488 EPOCH 5919
2024-02-05 15:42:08,135 Epoch 5919: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 15:42:08,135 EPOCH 5920
2024-02-05 15:42:21,815 Epoch 5920: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 15:42:21,815 EPOCH 5921
2024-02-05 15:42:36,118 Epoch 5921: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 15:42:36,119 EPOCH 5922
2024-02-05 15:42:49,898 Epoch 5922: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 15:42:49,899 EPOCH 5923
2024-02-05 15:42:55,951 [Epoch: 5923 Step: 00053300] Batch Recognition Loss:   0.000150 => Gls Tokens per Sec:      276 || Batch Translation Loss:   0.007395 => Txt Tokens per Sec:      908 || Lr: 0.000050
2024-02-05 15:43:03,920 Epoch 5923: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 15:43:03,921 EPOCH 5924
2024-02-05 15:43:17,960 Epoch 5924: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 15:43:17,961 EPOCH 5925
2024-02-05 15:43:31,830 Epoch 5925: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 15:43:31,831 EPOCH 5926
2024-02-05 15:43:45,774 Epoch 5926: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 15:43:45,775 EPOCH 5927
2024-02-05 15:43:59,658 Epoch 5927: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 15:43:59,658 EPOCH 5928
2024-02-05 15:44:13,770 Epoch 5928: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 15:44:13,770 EPOCH 5929
2024-02-05 15:44:27,511 Epoch 5929: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 15:44:27,512 EPOCH 5930
2024-02-05 15:44:41,538 Epoch 5930: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 15:44:41,538 EPOCH 5931
2024-02-05 15:44:54,934 Epoch 5931: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 15:44:54,935 EPOCH 5932
2024-02-05 15:45:08,802 Epoch 5932: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 15:45:08,802 EPOCH 5933
2024-02-05 15:45:22,443 Epoch 5933: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 15:45:22,444 EPOCH 5934
2024-02-05 15:45:27,597 [Epoch: 5934 Step: 00053400] Batch Recognition Loss:   0.000096 => Gls Tokens per Sec:      572 || Batch Translation Loss:   0.011744 => Txt Tokens per Sec:     1334 || Lr: 0.000050
2024-02-05 15:45:36,416 Epoch 5934: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 15:45:36,416 EPOCH 5935
2024-02-05 15:45:50,439 Epoch 5935: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 15:45:50,440 EPOCH 5936
2024-02-05 15:46:04,582 Epoch 5936: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 15:46:04,583 EPOCH 5937
2024-02-05 15:46:18,259 Epoch 5937: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 15:46:18,260 EPOCH 5938
2024-02-05 15:46:32,053 Epoch 5938: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 15:46:32,054 EPOCH 5939
2024-02-05 15:46:46,157 Epoch 5939: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 15:46:46,158 EPOCH 5940
2024-02-05 15:47:00,314 Epoch 5940: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 15:47:00,314 EPOCH 5941
2024-02-05 15:47:14,227 Epoch 5941: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-05 15:47:14,228 EPOCH 5942
2024-02-05 15:47:28,332 Epoch 5942: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 15:47:28,332 EPOCH 5943
2024-02-05 15:47:42,221 Epoch 5943: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-05 15:47:42,222 EPOCH 5944
2024-02-05 15:47:56,072 Epoch 5944: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 15:47:56,072 EPOCH 5945
2024-02-05 15:48:01,800 [Epoch: 5945 Step: 00053500] Batch Recognition Loss:   0.000097 => Gls Tokens per Sec:      739 || Batch Translation Loss:   0.011389 => Txt Tokens per Sec:     2064 || Lr: 0.000050
2024-02-05 15:48:10,107 Epoch 5945: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 15:48:10,108 EPOCH 5946
2024-02-05 15:48:24,123 Epoch 5946: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-05 15:48:24,124 EPOCH 5947
2024-02-05 15:48:38,083 Epoch 5947: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 15:48:38,083 EPOCH 5948
2024-02-05 15:48:52,099 Epoch 5948: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 15:48:52,100 EPOCH 5949
2024-02-05 15:49:06,114 Epoch 5949: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 15:49:06,115 EPOCH 5950
2024-02-05 15:49:20,284 Epoch 5950: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 15:49:20,285 EPOCH 5951
2024-02-05 15:49:34,014 Epoch 5951: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 15:49:34,015 EPOCH 5952
2024-02-05 15:49:47,613 Epoch 5952: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-05 15:49:47,613 EPOCH 5953
2024-02-05 15:50:01,669 Epoch 5953: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 15:50:01,669 EPOCH 5954
2024-02-05 15:50:15,674 Epoch 5954: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 15:50:15,675 EPOCH 5955
2024-02-05 15:50:29,822 Epoch 5955: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 15:50:29,823 EPOCH 5956
2024-02-05 15:50:36,276 [Epoch: 5956 Step: 00053600] Batch Recognition Loss:   0.000102 => Gls Tokens per Sec:      854 || Batch Translation Loss:   0.013716 => Txt Tokens per Sec:     2393 || Lr: 0.000050
2024-02-05 15:50:43,663 Epoch 5956: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-05 15:50:43,663 EPOCH 5957
2024-02-05 15:50:57,809 Epoch 5957: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 15:50:57,810 EPOCH 5958
2024-02-05 15:51:11,705 Epoch 5958: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 15:51:11,706 EPOCH 5959
2024-02-05 15:51:26,030 Epoch 5959: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 15:51:26,031 EPOCH 5960
2024-02-05 15:51:39,696 Epoch 5960: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 15:51:39,696 EPOCH 5961
2024-02-05 15:51:53,438 Epoch 5961: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 15:51:53,438 EPOCH 5962
2024-02-05 15:52:07,273 Epoch 5962: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-05 15:52:07,274 EPOCH 5963
2024-02-05 15:52:21,360 Epoch 5963: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 15:52:21,360 EPOCH 5964
2024-02-05 15:52:35,181 Epoch 5964: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 15:52:35,181 EPOCH 5965
2024-02-05 15:52:49,310 Epoch 5965: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 15:52:49,310 EPOCH 5966
2024-02-05 15:53:03,323 Epoch 5966: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 15:53:03,324 EPOCH 5967
2024-02-05 15:53:10,211 [Epoch: 5967 Step: 00053700] Batch Recognition Loss:   0.000141 => Gls Tokens per Sec:      986 || Batch Translation Loss:   0.040631 => Txt Tokens per Sec:     2712 || Lr: 0.000050
2024-02-05 15:53:17,479 Epoch 5967: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 15:53:17,479 EPOCH 5968
2024-02-05 15:53:31,490 Epoch 5968: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 15:53:31,491 EPOCH 5969
2024-02-05 15:53:45,551 Epoch 5969: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 15:53:45,551 EPOCH 5970
2024-02-05 15:53:59,530 Epoch 5970: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 15:53:59,531 EPOCH 5971
2024-02-05 15:54:13,326 Epoch 5971: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 15:54:13,327 EPOCH 5972
2024-02-05 15:54:27,180 Epoch 5972: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 15:54:27,181 EPOCH 5973
2024-02-05 15:54:40,924 Epoch 5973: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-05 15:54:40,924 EPOCH 5974
2024-02-05 15:54:54,849 Epoch 5974: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-05 15:54:54,850 EPOCH 5975
2024-02-05 15:55:09,076 Epoch 5975: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-05 15:55:09,077 EPOCH 5976
2024-02-05 15:55:22,739 Epoch 5976: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-05 15:55:22,739 EPOCH 5977
2024-02-05 15:55:36,610 Epoch 5977: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-05 15:55:36,610 EPOCH 5978
2024-02-05 15:55:44,767 [Epoch: 5978 Step: 00053800] Batch Recognition Loss:   0.000199 => Gls Tokens per Sec:      989 || Batch Translation Loss:   0.021429 => Txt Tokens per Sec:     2706 || Lr: 0.000050
2024-02-05 15:55:50,222 Epoch 5978: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 15:55:50,223 EPOCH 5979
2024-02-05 15:56:04,259 Epoch 5979: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 15:56:04,259 EPOCH 5980
2024-02-05 15:56:18,185 Epoch 5980: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 15:56:18,185 EPOCH 5981
2024-02-05 15:56:32,002 Epoch 5981: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 15:56:32,002 EPOCH 5982
2024-02-05 15:56:45,870 Epoch 5982: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 15:56:45,870 EPOCH 5983
2024-02-05 15:56:59,772 Epoch 5983: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 15:56:59,772 EPOCH 5984
2024-02-05 15:57:13,752 Epoch 5984: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 15:57:13,752 EPOCH 5985
2024-02-05 15:57:27,755 Epoch 5985: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 15:57:27,755 EPOCH 5986
2024-02-05 15:57:41,296 Epoch 5986: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 15:57:41,296 EPOCH 5987
2024-02-05 15:57:55,362 Epoch 5987: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-05 15:57:55,362 EPOCH 5988
2024-02-05 15:58:09,347 Epoch 5988: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-05 15:58:09,347 EPOCH 5989
2024-02-05 15:58:23,008 [Epoch: 5989 Step: 00053900] Batch Recognition Loss:   0.000128 => Gls Tokens per Sec:      684 || Batch Translation Loss:   0.015537 => Txt Tokens per Sec:     1902 || Lr: 0.000050
2024-02-05 15:58:23,454 Epoch 5989: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-05 15:58:23,455 EPOCH 5990
2024-02-05 15:58:37,268 Epoch 5990: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 15:58:37,268 EPOCH 5991
2024-02-05 15:58:51,024 Epoch 5991: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 15:58:51,024 EPOCH 5992
2024-02-05 15:59:04,944 Epoch 5992: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 15:59:04,945 EPOCH 5993
2024-02-05 15:59:18,997 Epoch 5993: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 15:59:18,998 EPOCH 5994
2024-02-05 15:59:33,012 Epoch 5994: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-05 15:59:33,012 EPOCH 5995
2024-02-05 15:59:46,932 Epoch 5995: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-05 15:59:46,933 EPOCH 5996
2024-02-05 16:00:00,996 Epoch 5996: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 16:00:00,996 EPOCH 5997
2024-02-05 16:00:14,972 Epoch 5997: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-05 16:00:14,973 EPOCH 5998
2024-02-05 16:00:29,014 Epoch 5998: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-05 16:00:29,015 EPOCH 5999
2024-02-05 16:00:42,728 Epoch 5999: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-05 16:00:42,728 EPOCH 6000
2024-02-05 16:00:56,851 [Epoch: 6000 Step: 00054000] Batch Recognition Loss:   0.000208 => Gls Tokens per Sec:      753 || Batch Translation Loss:   0.025632 => Txt Tokens per Sec:     2090 || Lr: 0.000050
2024-02-05 16:01:25,766 Validation result at epoch 6000, step    54000: duration: 28.9140s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00065	Translation Loss: 108155.46094	PPL: 50167.46484
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.68	(BLEU-1: 10.81,	BLEU-2: 3.30,	BLEU-3: 1.36,	BLEU-4: 0.68)
	CHRF 16.86	ROUGE 9.03
2024-02-05 16:01:25,768 Logging Recognition and Translation Outputs
2024-02-05 16:01:25,768 ========================================================================================================================
2024-02-05 16:01:25,768 Logging Sequence: 130_139.00
2024-02-05 16:01:25,768 	Gloss Reference :	A B+C+D+E
2024-02-05 16:01:25,769 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 16:01:25,769 	Gloss Alignment :	         
2024-02-05 16:01:25,769 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 16:01:25,772 	Text Reference  :	he shared a ***** picture of   a   little pouch he knit for his olympic gold medal with uk flag on one side  and     japanese flag    on the ******* other
2024-02-05 16:01:25,772 	Text Hypothesis :	he played a diver but     what the age    old   he **** *** *** ******* **** ***** **** ** **** ** *** still removed it       because of the beijing china
2024-02-05 16:01:25,772 	Text Alignment  :	   S        I     S       S    S   S      S        D    D   D   D       D    D     D    D  D    D  D   S     S       S        S       S      I       S    
2024-02-05 16:01:25,772 ========================================================================================================================
2024-02-05 16:01:25,772 Logging Sequence: 148_155.00
2024-02-05 16:01:25,772 	Gloss Reference :	A B+C+D+E
2024-02-05 16:01:25,772 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 16:01:25,773 	Gloss Alignment :	         
2024-02-05 16:01:25,773 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 16:01:25,774 	Text Reference  :	india *** won the **** *** ****** ** **** ***** match with 263     balls remaining and without losing any wicket
2024-02-05 16:01:25,774 	Text Hypothesis :	india has won the toss and choose to bowl first time  in   however now   pakistan  or  playing in     20  overs 
2024-02-05 16:01:25,775 	Text Alignment  :	      I           I    I   I      I  I    I     S     S    S       S     S         S   S       S      S   S     
2024-02-05 16:01:25,775 ========================================================================================================================
2024-02-05 16:01:25,775 Logging Sequence: 126_99.00
2024-02-05 16:01:25,775 	Gloss Reference :	A B+C+D+E
2024-02-05 16:01:25,775 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 16:01:25,775 	Gloss Alignment :	         
2024-02-05 16:01:25,776 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 16:01:25,776 	Text Reference  :	he        dedicated the medal to sprinter milkha singh
2024-02-05 16:01:25,776 	Text Hypothesis :	yesterday on        the age   of 6        crore  whoa 
2024-02-05 16:01:25,776 	Text Alignment  :	S         S             S     S  S        S      S    
2024-02-05 16:01:25,777 ========================================================================================================================
2024-02-05 16:01:25,777 Logging Sequence: 149_77.00
2024-02-05 16:01:25,777 	Gloss Reference :	A B+C+D+E
2024-02-05 16:01:25,777 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 16:01:25,777 	Gloss Alignment :	         
2024-02-05 16:01:25,777 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 16:01:25,779 	Text Reference  :	and arrested danushka for   alleged sexual assault of  a   29  year old    woman whose name has not ***** been disclosed
2024-02-05 16:01:25,779 	Text Hypothesis :	*** ******** ******** since there   was    no      one who has no   longer india as    he   is  not loved to   play     
2024-02-05 16:01:25,780 	Text Alignment  :	D   D        D        S     S       S      S       S   S   S   S    S      S     S     S    S       I     S    S        
2024-02-05 16:01:25,780 ========================================================================================================================
2024-02-05 16:01:25,780 Logging Sequence: 168_15.00
2024-02-05 16:01:25,780 	Gloss Reference :	A B+C+D+E
2024-02-05 16:01:25,780 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 16:01:25,780 	Gloss Alignment :	         
2024-02-05 16:01:25,781 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 16:01:25,781 	Text Reference  :	when in public the   couple are    always approached for  photographys and autographs
2024-02-05 16:01:25,781 	Text Hypothesis :	**** ** ****** india and    people were   present    that in           the league    
2024-02-05 16:01:25,782 	Text Alignment  :	D    D  D      S     S      S      S      S          S    S            S   S         
2024-02-05 16:01:25,782 ========================================================================================================================
2024-02-05 16:01:25,786 Training ended since there were no improvements inthe last learning rate step: 0.000050
2024-02-05 16:01:25,787 Best validation result at step     2000:   1.14 eval_metric.
2024-02-05 16:01:51,294 ------------------------------------------------------------
2024-02-05 16:01:51,295 [DEV] partition [RECOGNITION] experiment [BW]: 1
2024-02-05 16:02:26,650 finished in 35.3556s 
2024-02-05 16:02:26,650 ************************************************************
2024-02-05 16:02:26,651 [DEV] partition [RECOGNITION] results:
	New Best CTC Decode Beam Size: 1
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
2024-02-05 16:02:26,651 ************************************************************
2024-02-05 16:02:26,651 ------------------------------------------------------------
2024-02-05 16:02:26,651 [DEV] partition [RECOGNITION] experiment [BW]: 2
2024-02-05 16:02:59,712 finished in 33.0604s 
2024-02-05 16:02:59,712 ------------------------------------------------------------
2024-02-05 16:02:59,712 [DEV] partition [RECOGNITION] experiment [BW]: 3
2024-02-05 16:03:29,532 finished in 29.8191s 
2024-02-05 16:03:29,533 ------------------------------------------------------------
2024-02-05 16:03:29,533 [DEV] partition [RECOGNITION] experiment [BW]: 4
2024-02-05 16:04:00,969 finished in 31.4363s 
2024-02-05 16:04:00,969 ------------------------------------------------------------
2024-02-05 16:04:00,970 [DEV] partition [RECOGNITION] experiment [BW]: 5
2024-02-05 16:04:30,048 finished in 29.0784s 
2024-02-05 16:04:30,049 ------------------------------------------------------------
2024-02-05 16:04:30,050 [DEV] partition [RECOGNITION] experiment [BW]: 6
2024-02-05 16:04:58,872 finished in 28.8226s 
2024-02-05 16:04:58,873 ------------------------------------------------------------
2024-02-05 16:04:58,873 [DEV] partition [RECOGNITION] experiment [BW]: 7
2024-02-05 16:05:27,231 finished in 28.3580s 
2024-02-05 16:05:27,232 ------------------------------------------------------------
2024-02-05 16:05:27,232 [DEV] partition [RECOGNITION] experiment [BW]: 8
2024-02-05 16:05:55,648 finished in 28.4166s 
2024-02-05 16:05:55,649 ------------------------------------------------------------
2024-02-05 16:05:55,649 [DEV] partition [RECOGNITION] experiment [BW]: 9
2024-02-05 16:06:24,078 finished in 28.4284s 
2024-02-05 16:06:24,079 ------------------------------------------------------------
2024-02-05 16:06:24,079 [DEV] partition [RECOGNITION] experiment [BW]: 10
2024-02-05 16:06:52,615 finished in 28.5366s 
2024-02-05 16:06:52,616 ============================================================
2024-02-05 16:07:20,818 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 1 and Alpha: -1
	BLEU-4 1.14	(BLEU-1: 12.71,	BLEU-2: 4.82,	BLEU-3: 2.14,	BLEU-4: 1.14)
	CHRF 17.87	ROUGE 11.04
2024-02-05 16:07:20,819 ------------------------------------------------------------
2024-02-05 16:23:08,617 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 5 and Alpha: 1
	BLEU-4 1.20	(BLEU-1: 11.73,	BLEU-2: 4.67,	BLEU-3: 2.23,	BLEU-4: 1.20)
	CHRF 16.88	ROUGE 10.20
2024-02-05 16:23:08,618 ------------------------------------------------------------
2024-02-05 16:27:21,830 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 6 and Alpha: 1
	BLEU-4 1.24	(BLEU-1: 11.77,	BLEU-2: 4.69,	BLEU-3: 2.30,	BLEU-4: 1.24)
	CHRF 16.84	ROUGE 10.16
2024-02-05 16:27:21,831 ------------------------------------------------------------
2024-02-05 16:31:54,007 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 7 and Alpha: 1
	BLEU-4 1.33	(BLEU-1: 11.69,	BLEU-2: 4.73,	BLEU-3: 2.36,	BLEU-4: 1.33)
	CHRF 16.93	ROUGE 9.96
2024-02-05 16:31:54,007 ------------------------------------------------------------
2024-02-05 16:39:35,635 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 8 and Alpha: 1
	BLEU-4 1.37	(BLEU-1: 11.78,	BLEU-2: 4.76,	BLEU-3: 2.41,	BLEU-4: 1.37)
	CHRF 16.91	ROUGE 10.01
2024-02-05 16:39:35,635 ------------------------------------------------------------
2024-02-05 17:11:46,312 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 10 and Alpha: 1
	BLEU-4 1.37	(BLEU-1: 11.66,	BLEU-2: 4.59,	BLEU-3: 2.33,	BLEU-4: 1.37)
	CHRF 16.69	ROUGE 9.87
2024-02-05 17:11:46,313 ------------------------------------------------------------
2024-02-05 17:23:41,191 ************************************************************
2024-02-05 17:23:41,192 [DEV] partition [Recognition & Translation] results:
	Best CTC Decode Beam Size: 1
	Best Translation Beam Size: 10 and Alpha: 1
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 1.37	(BLEU-1: 11.66,	BLEU-2: 4.59,	BLEU-3: 2.33,	BLEU-4: 1.37)
	CHRF 16.69	ROUGE 9.87
2024-02-05 17:23:41,192 ************************************************************
2024-02-05 17:25:29,536 [TEST] partition [Recognition & Translation] results:
	Best CTC Decode Beam Size: 1
	Best Translation Beam Size: 10 and Alpha: 1
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 1.02	(BLEU-1: 10.88,	BLEU-2: 4.13,	BLEU-3: 1.92,	BLEU-4: 1.02)
	CHRF 16.40	ROUGE 9.33
2024-02-05 17:25:29,536 ************************************************************
