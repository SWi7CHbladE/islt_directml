2024-02-03 07:44:58,847 Hello! This is Joey-NMT.
2024-02-03 07:44:58,855 Total params: 25618440
2024-02-03 07:44:58,857 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'gloss_output_layer.bias', 'gloss_output_layer.weight', 'sgn_embed.ln.bias', 'sgn_embed.ln.weight', 'sgn_embed.norm.norm.bias', 'sgn_embed.norm.norm.weight', 'txt_embed.norm.norm.bias', 'txt_embed.norm.norm.weight']
2024-02-03 07:45:00,005 cfg.name                           : sign_experiment
2024-02-03 07:45:00,006 cfg.data.data_path                 : ./data/Sports_dataset/1/
2024-02-03 07:45:00,006 cfg.data.version                   : phoenix_2014_trans
2024-02-03 07:45:00,006 cfg.data.sgn                       : sign
2024-02-03 07:45:00,006 cfg.data.txt                       : text
2024-02-03 07:45:00,006 cfg.data.gls                       : gloss
2024-02-03 07:45:00,006 cfg.data.train                     : excel_data.train
2024-02-03 07:45:00,007 cfg.data.dev                       : excel_data.dev
2024-02-03 07:45:00,007 cfg.data.test                      : excel_data.test
2024-02-03 07:45:00,007 cfg.data.feature_size              : 2560
2024-02-03 07:45:00,007 cfg.data.level                     : word
2024-02-03 07:45:00,007 cfg.data.txt_lowercase             : True
2024-02-03 07:45:00,007 cfg.data.max_sent_length           : 500
2024-02-03 07:45:00,007 cfg.data.random_train_subset       : -1
2024-02-03 07:45:00,008 cfg.data.random_dev_subset         : -1
2024-02-03 07:45:00,008 cfg.testing.recognition_beam_sizes : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
2024-02-03 07:45:00,008 cfg.testing.translation_beam_sizes : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
2024-02-03 07:45:00,008 cfg.testing.translation_beam_alphas : [-1, 0, 1, 2, 3, 4, 5]
2024-02-03 07:45:00,008 cfg.training.reset_best_ckpt       : False
2024-02-03 07:45:00,008 cfg.training.reset_scheduler       : False
2024-02-03 07:45:00,008 cfg.training.reset_optimizer       : False
2024-02-03 07:45:00,008 cfg.training.random_seed           : 42
2024-02-03 07:45:00,008 cfg.training.model_dir             : ./sign_sample_model/fold1/32head/64batch
2024-02-03 07:45:00,009 cfg.training.recognition_loss_weight : 1.0
2024-02-03 07:45:00,009 cfg.training.translation_loss_weight : 1.0
2024-02-03 07:45:00,009 cfg.training.eval_metric           : bleu
2024-02-03 07:45:00,009 cfg.training.optimizer             : adam
2024-02-03 07:45:00,009 cfg.training.learning_rate         : 0.0001
2024-02-03 07:45:00,009 cfg.training.batch_size            : 64
2024-02-03 07:45:00,009 cfg.training.num_valid_log         : 5
2024-02-03 07:45:00,009 cfg.training.epochs                : 50000
2024-02-03 07:45:00,009 cfg.training.early_stopping_metric : eval_metric
2024-02-03 07:45:00,011 cfg.training.batch_type            : sentence
2024-02-03 07:45:00,011 cfg.training.translation_normalization : batch
2024-02-03 07:45:00,011 cfg.training.eval_recognition_beam_size : 1
2024-02-03 07:45:00,011 cfg.training.eval_translation_beam_size : 1
2024-02-03 07:45:00,011 cfg.training.eval_translation_beam_alpha : -1
2024-02-03 07:45:00,011 cfg.training.overwrite             : True
2024-02-03 07:45:00,011 cfg.training.shuffle               : True
2024-02-03 07:45:00,011 cfg.training.use_cuda              : True
2024-02-03 07:45:00,011 cfg.training.translation_max_output_length : 40
2024-02-03 07:45:00,012 cfg.training.keep_last_ckpts       : 1
2024-02-03 07:45:00,012 cfg.training.batch_multiplier      : 1
2024-02-03 07:45:00,012 cfg.training.logging_freq          : 100
2024-02-03 07:45:00,012 cfg.training.validation_freq       : 2000
2024-02-03 07:45:00,012 cfg.training.betas                 : [0.9, 0.998]
2024-02-03 07:45:00,012 cfg.training.scheduling            : plateau
2024-02-03 07:45:00,012 cfg.training.learning_rate_min     : 1e-08
2024-02-03 07:45:00,012 cfg.training.weight_decay          : 0.0001
2024-02-03 07:45:00,013 cfg.training.patience              : 12
2024-02-03 07:45:00,013 cfg.training.decrease_factor       : 0.5
2024-02-03 07:45:00,013 cfg.training.label_smoothing       : 0.0
2024-02-03 07:45:00,013 cfg.model.initializer              : xavier
2024-02-03 07:45:00,013 cfg.model.bias_initializer         : zeros
2024-02-03 07:45:00,013 cfg.model.init_gain                : 1.0
2024-02-03 07:45:00,013 cfg.model.embed_initializer        : xavier
2024-02-03 07:45:00,013 cfg.model.embed_init_gain          : 1.0
2024-02-03 07:45:00,013 cfg.model.tied_softmax             : True
2024-02-03 07:45:00,014 cfg.model.encoder.type             : transformer
2024-02-03 07:45:00,014 cfg.model.encoder.num_layers       : 3
2024-02-03 07:45:00,014 cfg.model.encoder.num_heads        : 32
2024-02-03 07:45:00,014 cfg.model.encoder.embeddings.embedding_dim : 512
2024-02-03 07:45:00,014 cfg.model.encoder.embeddings.scale : False
2024-02-03 07:45:00,014 cfg.model.encoder.embeddings.dropout : 0.1
2024-02-03 07:45:00,014 cfg.model.encoder.embeddings.norm_type : batch
2024-02-03 07:45:00,014 cfg.model.encoder.embeddings.activation_type : softsign
2024-02-03 07:45:00,015 cfg.model.encoder.hidden_size      : 512
2024-02-03 07:45:00,015 cfg.model.encoder.ff_size          : 2048
2024-02-03 07:45:00,015 cfg.model.encoder.dropout          : 0.1
2024-02-03 07:45:00,015 cfg.model.decoder.type             : transformer
2024-02-03 07:45:00,015 cfg.model.decoder.num_layers       : 3
2024-02-03 07:45:00,015 cfg.model.decoder.num_heads        : 32
2024-02-03 07:45:00,015 cfg.model.decoder.embeddings.embedding_dim : 512
2024-02-03 07:45:00,015 cfg.model.decoder.embeddings.scale : False
2024-02-03 07:45:00,015 cfg.model.decoder.embeddings.dropout : 0.1
2024-02-03 07:45:00,016 cfg.model.decoder.embeddings.norm_type : batch
2024-02-03 07:45:00,016 cfg.model.decoder.embeddings.activation_type : softsign
2024-02-03 07:45:00,016 cfg.model.decoder.hidden_size      : 512
2024-02-03 07:45:00,016 cfg.model.decoder.ff_size          : 2048
2024-02-03 07:45:00,016 cfg.model.decoder.dropout          : 0.1
2024-02-03 07:45:00,016 Data set sizes: 
	train 2126,
	valid 707,
	test 708
2024-02-03 07:45:00,016 First training example:
	[GLS] A B C D E
	[TXT] another problem is that because of ipl and t20 world cup many players may not be available for the matches
2024-02-03 07:45:00,016 First 10 words (gls): (0) <si> (1) <unk> (2) <pad> (3) A (4) B (5) C (6) D (7) E
2024-02-03 07:45:00,017 First 10 words (txt): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) the (5) to (6) and (7) in (8) a (9) of
2024-02-03 07:45:00,017 Number of unique glosses (types): 8
2024-02-03 07:45:00,017 Number of unique words (types): 4355
2024-02-03 07:45:00,017 SignModel(
	encoder=TransformerEncoder(num_layers=3, num_heads=32),
	decoder=TransformerDecoder(num_layers=3, num_heads=32),
	sgn_embed=SpatialEmbeddings(embedding_dim=512, input_size=2560),
	txt_embed=Embeddings(embedding_dim=512, vocab_size=4355))
2024-02-03 07:45:00,021 EPOCH 1
2024-02-03 07:45:05,502 Epoch   1: Total Training Recognition Loss 233.82  Total Training Translation Loss 3458.20 
2024-02-03 07:45:05,503 EPOCH 2
2024-02-03 07:45:10,021 Epoch   2: Total Training Recognition Loss 117.03  Total Training Translation Loss 3110.55 
2024-02-03 07:45:10,021 EPOCH 3
2024-02-03 07:45:14,497 [Epoch: 003 Step: 00000100] Batch Recognition Loss:   2.390118 => Gls Tokens per Sec:     2232 || Batch Translation Loss:  63.612709 => Txt Tokens per Sec:     6193 || Lr: 0.000100
2024-02-03 07:45:14,805 Epoch   3: Total Training Recognition Loss 94.35  Total Training Translation Loss 3035.48 
2024-02-03 07:45:14,806 EPOCH 4
2024-02-03 07:45:19,260 Epoch   4: Total Training Recognition Loss 84.60  Total Training Translation Loss 3005.08 
2024-02-03 07:45:19,260 EPOCH 5
2024-02-03 07:45:23,898 Epoch   5: Total Training Recognition Loss 77.36  Total Training Translation Loss 2969.47 
2024-02-03 07:45:23,898 EPOCH 6
2024-02-03 07:45:28,119 [Epoch: 006 Step: 00000200] Batch Recognition Loss:   0.818436 => Gls Tokens per Sec:     2216 || Batch Translation Loss:  79.479568 => Txt Tokens per Sec:     6208 || Lr: 0.000100
2024-02-03 07:45:28,668 Epoch   6: Total Training Recognition Loss 69.79  Total Training Translation Loss 2907.65 
2024-02-03 07:45:28,668 EPOCH 7
2024-02-03 07:45:33,288 Epoch   7: Total Training Recognition Loss 50.42  Total Training Translation Loss 2825.52 
2024-02-03 07:45:33,288 EPOCH 8
2024-02-03 07:45:37,935 Epoch   8: Total Training Recognition Loss 50.14  Total Training Translation Loss 2743.02 
2024-02-03 07:45:37,936 EPOCH 9
2024-02-03 07:45:41,416 [Epoch: 009 Step: 00000300] Batch Recognition Loss:   1.526824 => Gls Tokens per Sec:     2576 || Batch Translation Loss:  48.519302 => Txt Tokens per Sec:     7070 || Lr: 0.000100
2024-02-03 07:45:42,466 Epoch   9: Total Training Recognition Loss 43.00  Total Training Translation Loss 2659.38 
2024-02-03 07:45:42,467 EPOCH 10
2024-02-03 07:45:47,374 Epoch  10: Total Training Recognition Loss 32.62  Total Training Translation Loss 2582.88 
2024-02-03 07:45:47,375 EPOCH 11
2024-02-03 07:45:51,619 Epoch  11: Total Training Recognition Loss 25.24  Total Training Translation Loss 2494.25 
2024-02-03 07:45:51,619 EPOCH 12
2024-02-03 07:45:55,203 [Epoch: 012 Step: 00000400] Batch Recognition Loss:   0.129025 => Gls Tokens per Sec:     2253 || Batch Translation Loss:  67.199745 => Txt Tokens per Sec:     6133 || Lr: 0.000100
2024-02-03 07:45:56,476 Epoch  12: Total Training Recognition Loss 21.66  Total Training Translation Loss 2425.48 
2024-02-03 07:45:56,476 EPOCH 13
2024-02-03 07:46:00,879 Epoch  13: Total Training Recognition Loss 20.06  Total Training Translation Loss 2349.75 
2024-02-03 07:46:00,880 EPOCH 14
2024-02-03 07:46:05,758 Epoch  14: Total Training Recognition Loss 17.95  Total Training Translation Loss 2284.11 
2024-02-03 07:46:05,759 EPOCH 15
2024-02-03 07:46:08,566 [Epoch: 015 Step: 00000500] Batch Recognition Loss:   0.074063 => Gls Tokens per Sec:     2737 || Batch Translation Loss:  57.925694 => Txt Tokens per Sec:     7568 || Lr: 0.000100
2024-02-03 07:46:10,007 Epoch  15: Total Training Recognition Loss 16.04  Total Training Translation Loss 2212.59 
2024-02-03 07:46:10,007 EPOCH 16
2024-02-03 07:46:14,894 Epoch  16: Total Training Recognition Loss 15.61  Total Training Translation Loss 2159.45 
2024-02-03 07:46:14,894 EPOCH 17
2024-02-03 07:46:18,972 Epoch  17: Total Training Recognition Loss 14.77  Total Training Translation Loss 2091.54 
2024-02-03 07:46:18,973 EPOCH 18
2024-02-03 07:46:22,405 [Epoch: 018 Step: 00000600] Batch Recognition Loss:   1.164127 => Gls Tokens per Sec:     1979 || Batch Translation Loss:  83.695114 => Txt Tokens per Sec:     5668 || Lr: 0.000100
2024-02-03 07:46:23,886 Epoch  18: Total Training Recognition Loss 12.33  Total Training Translation Loss 2052.46 
2024-02-03 07:46:23,887 EPOCH 19
2024-02-03 07:46:28,289 Epoch  19: Total Training Recognition Loss 10.39  Total Training Translation Loss 1983.70 
2024-02-03 07:46:28,289 EPOCH 20
2024-02-03 07:46:33,181 Epoch  20: Total Training Recognition Loss 10.21  Total Training Translation Loss 1938.48 
2024-02-03 07:46:33,182 EPOCH 21
2024-02-03 07:46:35,626 [Epoch: 021 Step: 00000700] Batch Recognition Loss:   0.279311 => Gls Tokens per Sec:     2517 || Batch Translation Loss:  40.716251 => Txt Tokens per Sec:     7205 || Lr: 0.000100
2024-02-03 07:46:37,395 Epoch  21: Total Training Recognition Loss 9.53  Total Training Translation Loss 1876.97 
2024-02-03 07:46:37,396 EPOCH 22
2024-02-03 07:46:42,293 Epoch  22: Total Training Recognition Loss 8.90  Total Training Translation Loss 1819.70 
2024-02-03 07:46:42,293 EPOCH 23
2024-02-03 07:46:46,539 Epoch  23: Total Training Recognition Loss 8.12  Total Training Translation Loss 1770.76 
2024-02-03 07:46:46,539 EPOCH 24
2024-02-03 07:46:49,287 [Epoch: 024 Step: 00000800] Batch Recognition Loss:   0.171117 => Gls Tokens per Sec:     2007 || Batch Translation Loss:  51.747356 => Txt Tokens per Sec:     5449 || Lr: 0.000100
2024-02-03 07:46:51,487 Epoch  24: Total Training Recognition Loss 7.80  Total Training Translation Loss 1708.46 
2024-02-03 07:46:51,487 EPOCH 25
2024-02-03 07:46:55,708 Epoch  25: Total Training Recognition Loss 7.80  Total Training Translation Loss 1655.65 
2024-02-03 07:46:55,709 EPOCH 26
2024-02-03 07:47:00,633 Epoch  26: Total Training Recognition Loss 7.19  Total Training Translation Loss 1613.25 
2024-02-03 07:47:00,634 EPOCH 27
2024-02-03 07:47:02,534 [Epoch: 027 Step: 00000900] Batch Recognition Loss:   0.085333 => Gls Tokens per Sec:     2563 || Batch Translation Loss:  42.783634 => Txt Tokens per Sec:     7382 || Lr: 0.000100
2024-02-03 07:47:04,881 Epoch  27: Total Training Recognition Loss 7.36  Total Training Translation Loss 1573.09 
2024-02-03 07:47:04,882 EPOCH 28
2024-02-03 07:47:09,885 Epoch  28: Total Training Recognition Loss 7.12  Total Training Translation Loss 1519.64 
2024-02-03 07:47:09,886 EPOCH 29
2024-02-03 07:47:14,155 Epoch  29: Total Training Recognition Loss 6.88  Total Training Translation Loss 1473.51 
2024-02-03 07:47:14,156 EPOCH 30
2024-02-03 07:47:16,126 [Epoch: 030 Step: 00001000] Batch Recognition Loss:   0.419418 => Gls Tokens per Sec:     2275 || Batch Translation Loss:  17.296520 => Txt Tokens per Sec:     6130 || Lr: 0.000100
2024-02-03 07:47:19,035 Epoch  30: Total Training Recognition Loss 6.51  Total Training Translation Loss 1423.05 
2024-02-03 07:47:19,035 EPOCH 31
2024-02-03 07:47:23,143 Epoch  31: Total Training Recognition Loss 6.20  Total Training Translation Loss 1377.94 
2024-02-03 07:47:23,144 EPOCH 32
2024-02-03 07:47:27,170 Epoch  32: Total Training Recognition Loss 6.11  Total Training Translation Loss 1322.13 
2024-02-03 07:47:27,170 EPOCH 33
2024-02-03 07:47:28,409 [Epoch: 033 Step: 00001100] Batch Recognition Loss:   0.172444 => Gls Tokens per Sec:     2898 || Batch Translation Loss:  47.001083 => Txt Tokens per Sec:     7793 || Lr: 0.000100
2024-02-03 07:47:31,617 Epoch  33: Total Training Recognition Loss 6.31  Total Training Translation Loss 1274.85 
2024-02-03 07:47:31,617 EPOCH 34
2024-02-03 07:47:36,551 Epoch  34: Total Training Recognition Loss 6.16  Total Training Translation Loss 1268.00 
2024-02-03 07:47:36,551 EPOCH 35
2024-02-03 07:47:41,199 Epoch  35: Total Training Recognition Loss 5.89  Total Training Translation Loss 1201.51 
2024-02-03 07:47:41,199 EPOCH 36
2024-02-03 07:47:42,539 [Epoch: 036 Step: 00001200] Batch Recognition Loss:   0.134284 => Gls Tokens per Sec:     2202 || Batch Translation Loss:  39.251709 => Txt Tokens per Sec:     6331 || Lr: 0.000100
2024-02-03 07:47:45,779 Epoch  36: Total Training Recognition Loss 5.63  Total Training Translation Loss 1164.85 
2024-02-03 07:47:45,780 EPOCH 37
2024-02-03 07:47:50,422 Epoch  37: Total Training Recognition Loss 5.42  Total Training Translation Loss 1111.27 
2024-02-03 07:47:50,422 EPOCH 38
2024-02-03 07:47:55,272 Epoch  38: Total Training Recognition Loss 5.53  Total Training Translation Loss 1071.03 
2024-02-03 07:47:55,273 EPOCH 39
2024-02-03 07:47:56,241 [Epoch: 039 Step: 00001300] Batch Recognition Loss:   0.062155 => Gls Tokens per Sec:     2649 || Batch Translation Loss:  30.649158 => Txt Tokens per Sec:     7422 || Lr: 0.000100
2024-02-03 07:47:59,676 Epoch  39: Total Training Recognition Loss 5.13  Total Training Translation Loss 1020.42 
2024-02-03 07:47:59,676 EPOCH 40
2024-02-03 07:48:04,497 Epoch  40: Total Training Recognition Loss 5.46  Total Training Translation Loss 989.99 
2024-02-03 07:48:04,498 EPOCH 41
2024-02-03 07:48:08,835 Epoch  41: Total Training Recognition Loss 5.20  Total Training Translation Loss 945.37 
2024-02-03 07:48:08,835 EPOCH 42
2024-02-03 07:48:09,729 [Epoch: 042 Step: 00001400] Batch Recognition Loss:   0.083899 => Gls Tokens per Sec:     2150 || Batch Translation Loss:  19.871384 => Txt Tokens per Sec:     5382 || Lr: 0.000100
2024-02-03 07:48:13,469 Epoch  42: Total Training Recognition Loss 5.03  Total Training Translation Loss 903.08 
2024-02-03 07:48:13,470 EPOCH 43
2024-02-03 07:48:18,030 Epoch  43: Total Training Recognition Loss 4.95  Total Training Translation Loss 866.54 
2024-02-03 07:48:18,030 EPOCH 44
2024-02-03 07:48:22,991 Epoch  44: Total Training Recognition Loss 4.97  Total Training Translation Loss 830.43 
2024-02-03 07:48:22,992 EPOCH 45
2024-02-03 07:48:23,524 [Epoch: 045 Step: 00001500] Batch Recognition Loss:   0.180988 => Gls Tokens per Sec:     1941 || Batch Translation Loss:  29.357361 => Txt Tokens per Sec:     5681 || Lr: 0.000100
2024-02-03 07:48:27,305 Epoch  45: Total Training Recognition Loss 4.89  Total Training Translation Loss 786.76 
2024-02-03 07:48:27,306 EPOCH 46
2024-02-03 07:48:32,163 Epoch  46: Total Training Recognition Loss 4.86  Total Training Translation Loss 760.37 
2024-02-03 07:48:32,164 EPOCH 47
2024-02-03 07:48:36,411 Epoch  47: Total Training Recognition Loss 4.70  Total Training Translation Loss 737.39 
2024-02-03 07:48:36,411 EPOCH 48
2024-02-03 07:48:36,652 [Epoch: 048 Step: 00001600] Batch Recognition Loss:   0.066117 => Gls Tokens per Sec:     2672 || Batch Translation Loss:  16.011759 => Txt Tokens per Sec:     7131 || Lr: 0.000100
2024-02-03 07:48:41,510 Epoch  48: Total Training Recognition Loss 4.66  Total Training Translation Loss 697.64 
2024-02-03 07:48:41,510 EPOCH 49
2024-02-03 07:48:46,239 Epoch  49: Total Training Recognition Loss 4.58  Total Training Translation Loss 665.96 
2024-02-03 07:48:46,239 EPOCH 50
2024-02-03 07:48:51,071 [Epoch: 050 Step: 00001700] Batch Recognition Loss:   0.086702 => Gls Tokens per Sec:     2201 || Batch Translation Loss:  20.757441 => Txt Tokens per Sec:     6120 || Lr: 0.000100
2024-02-03 07:48:51,071 Epoch  50: Total Training Recognition Loss 4.81  Total Training Translation Loss 641.52 
2024-02-03 07:48:51,071 EPOCH 51
2024-02-03 07:48:55,315 Epoch  51: Total Training Recognition Loss 4.74  Total Training Translation Loss 611.52 
2024-02-03 07:48:55,315 EPOCH 52
2024-02-03 07:49:00,197 Epoch  52: Total Training Recognition Loss 4.48  Total Training Translation Loss 563.53 
2024-02-03 07:49:00,197 EPOCH 53
2024-02-03 07:49:04,598 [Epoch: 053 Step: 00001800] Batch Recognition Loss:   0.139533 => Gls Tokens per Sec:     2270 || Batch Translation Loss:   9.768851 => Txt Tokens per Sec:     6315 || Lr: 0.000100
2024-02-03 07:49:04,828 Epoch  53: Total Training Recognition Loss 4.33  Total Training Translation Loss 542.68 
2024-02-03 07:49:04,828 EPOCH 54
2024-02-03 07:49:09,395 Epoch  54: Total Training Recognition Loss 4.37  Total Training Translation Loss 505.33 
2024-02-03 07:49:09,396 EPOCH 55
2024-02-03 07:49:13,938 Epoch  55: Total Training Recognition Loss 4.20  Total Training Translation Loss 478.06 
2024-02-03 07:49:13,939 EPOCH 56
2024-02-03 07:49:18,146 [Epoch: 056 Step: 00001900] Batch Recognition Loss:   0.157443 => Gls Tokens per Sec:     2223 || Batch Translation Loss:  11.074793 => Txt Tokens per Sec:     6306 || Lr: 0.000100
2024-02-03 07:49:18,517 Epoch  56: Total Training Recognition Loss 4.14  Total Training Translation Loss 451.05 
2024-02-03 07:49:18,517 EPOCH 57
2024-02-03 07:49:23,267 Epoch  57: Total Training Recognition Loss 4.05  Total Training Translation Loss 436.95 
2024-02-03 07:49:23,267 EPOCH 58
2024-02-03 07:49:28,286 Epoch  58: Total Training Recognition Loss 4.30  Total Training Translation Loss 408.42 
2024-02-03 07:49:28,286 EPOCH 59
2024-02-03 07:49:32,109 [Epoch: 059 Step: 00002000] Batch Recognition Loss:   0.082553 => Gls Tokens per Sec:     2345 || Batch Translation Loss:  12.887936 => Txt Tokens per Sec:     6444 || Lr: 0.000100
2024-02-03 07:49:40,999 Hooray! New best validation result [eval_metric]!
2024-02-03 07:49:41,000 Saving new checkpoint.
2024-02-03 07:49:41,259 Validation result at epoch  59, step     2000: duration: 9.1497s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 3.03687	Translation Loss: 62955.53125	PPL: 691.45355
	Eval Metric: BLEU
	WER 8.20	(DEL: 0.14,	INS: 0.00,	SUB: 8.06)
	BLEU-4 0.63	(BLEU-1: 12.45,	BLEU-2: 4.03,	BLEU-3: 1.38,	BLEU-4: 0.63)
	CHRF 16.95	ROUGE 10.24
2024-02-03 07:49:41,260 Logging Recognition and Translation Outputs
2024-02-03 07:49:41,260 ========================================================================================================================
2024-02-03 07:49:41,260 Logging Sequence: 143_161.00
2024-02-03 07:49:41,260 	Gloss Reference :	A B+C+D+E
2024-02-03 07:49:41,260 	Gloss Hypothesis:	A B+C+E  
2024-02-03 07:49:41,260 	Gloss Alignment :	  S      
2024-02-03 07:49:41,261 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 07:49:41,261 	Text Reference  :	there is *** *** no   response from  them as  yet   
2024-02-03 07:49:41,261 	Text Hypothesis :	it    is why why they are      upset with the reason
2024-02-03 07:49:41,261 	Text Alignment  :	S        I   I   S    S        S     S    S   S     
2024-02-03 07:49:41,262 ========================================================================================================================
2024-02-03 07:49:41,262 Logging Sequence: 63_21.00
2024-02-03 07:49:41,262 	Gloss Reference :	A B+C+D+E
2024-02-03 07:49:41,262 	Gloss Hypothesis:	A B+C+D  
2024-02-03 07:49:41,262 	Gloss Alignment :	  S      
2024-02-03 07:49:41,262 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 07:49:41,263 	Text Reference  :	however the teams will be **** ** ***** *** *** announced only on  25th october 2021   
2024-02-03 07:49:41,264 	Text Hypothesis :	******* the ipl   will be held in dubai and csk to        bowl for a    new     zealand
2024-02-03 07:49:41,264 	Text Alignment  :	D           S             I    I  I     I   I   S         S    S   S    S       S      
2024-02-03 07:49:41,264 ========================================================================================================================
2024-02-03 07:49:41,264 Logging Sequence: 122_147.00
2024-02-03 07:49:41,264 	Gloss Reference :	A B+C+D+E
2024-02-03 07:49:41,264 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 07:49:41,264 	Gloss Alignment :	         
2024-02-03 07:49:41,264 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 07:49:41,265 	Text Reference  :	chanu had   been working as         a ****** ** tickets inspector in the indian railways 
2024-02-03 07:49:41,266 	Text Hypothesis :	***** after the  press   conference a bottle of water   that      i  am  very   emotional
2024-02-03 07:49:41,266 	Text Alignment  :	D     S     S    S       S            I      I  S       S         S  S   S      S        
2024-02-03 07:49:41,266 ========================================================================================================================
2024-02-03 07:49:41,266 Logging Sequence: 87_196.00
2024-02-03 07:49:41,266 	Gloss Reference :	A B+C+D+E
2024-02-03 07:49:41,266 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 07:49:41,266 	Gloss Alignment :	         
2024-02-03 07:49:41,267 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 07:49:41,267 	Text Reference  :	** **** my expression was **** ** **** ** ********* not against dhoni    or    kohli
2024-02-03 07:49:41,267 	Text Hypothesis :	so what a  video      was that we will be completed for the     upcoming world cup  
2024-02-03 07:49:41,268 	Text Alignment  :	I  I    S  S              I    I  I    I  I         S   S       S        S     S    
2024-02-03 07:49:41,268 ========================================================================================================================
2024-02-03 07:49:41,268 Logging Sequence: 114_153.00
2024-02-03 07:49:41,268 	Gloss Reference :	A B+C+D+E
2024-02-03 07:49:41,268 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 07:49:41,268 	Gloss Alignment :	         
2024-02-03 07:49:41,268 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 07:49:41,269 	Text Reference  :	***** ** *** * **** another big football news is     that the   copa america final 
2024-02-03 07:49:41,269 	Text Hypothesis :	since it was a huge shock   as  it       was  played in   front of   the     finals
2024-02-03 07:49:41,270 	Text Alignment  :	I     I  I   I I    S       S   S        S    S      S    S     S    S       S     
2024-02-03 07:49:41,270 ========================================================================================================================
2024-02-03 07:49:42,332 Epoch  59: Total Training Recognition Loss 3.96  Total Training Translation Loss 379.46 
2024-02-03 07:49:42,332 EPOCH 60
2024-02-03 07:49:47,333 Epoch  60: Total Training Recognition Loss 3.90  Total Training Translation Loss 363.05 
2024-02-03 07:49:47,334 EPOCH 61
2024-02-03 07:49:52,169 Epoch  61: Total Training Recognition Loss 3.97  Total Training Translation Loss 344.12 
2024-02-03 07:49:52,169 EPOCH 62
2024-02-03 07:49:55,788 [Epoch: 062 Step: 00002100] Batch Recognition Loss:   0.065961 => Gls Tokens per Sec:     2231 || Batch Translation Loss:   6.507982 => Txt Tokens per Sec:     6160 || Lr: 0.000100
2024-02-03 07:49:56,873 Epoch  62: Total Training Recognition Loss 3.72  Total Training Translation Loss 316.01 
2024-02-03 07:49:56,874 EPOCH 63
2024-02-03 07:50:01,820 Epoch  63: Total Training Recognition Loss 3.71  Total Training Translation Loss 293.01 
2024-02-03 07:50:01,821 EPOCH 64
2024-02-03 07:50:06,373 Epoch  64: Total Training Recognition Loss 3.60  Total Training Translation Loss 278.14 
2024-02-03 07:50:06,373 EPOCH 65
2024-02-03 07:50:09,772 [Epoch: 065 Step: 00002200] Batch Recognition Loss:   0.149756 => Gls Tokens per Sec:     2187 || Batch Translation Loss:   1.642989 => Txt Tokens per Sec:     6245 || Lr: 0.000100
2024-02-03 07:50:10,925 Epoch  65: Total Training Recognition Loss 3.69  Total Training Translation Loss 276.44 
2024-02-03 07:50:10,925 EPOCH 66
2024-02-03 07:50:15,581 Epoch  66: Total Training Recognition Loss 3.54  Total Training Translation Loss 248.97 
2024-02-03 07:50:15,581 EPOCH 67
2024-02-03 07:50:20,023 Epoch  67: Total Training Recognition Loss 3.50  Total Training Translation Loss 231.59 
2024-02-03 07:50:20,023 EPOCH 68
2024-02-03 07:50:23,341 [Epoch: 068 Step: 00002300] Batch Recognition Loss:   0.106347 => Gls Tokens per Sec:     2047 || Batch Translation Loss:   4.201169 => Txt Tokens per Sec:     5717 || Lr: 0.000100
2024-02-03 07:50:24,780 Epoch  68: Total Training Recognition Loss 3.55  Total Training Translation Loss 213.73 
2024-02-03 07:50:24,780 EPOCH 69
2024-02-03 07:50:29,465 Epoch  69: Total Training Recognition Loss 3.27  Total Training Translation Loss 203.71 
2024-02-03 07:50:29,466 EPOCH 70
2024-02-03 07:50:33,942 Epoch  70: Total Training Recognition Loss 3.25  Total Training Translation Loss 190.64 
2024-02-03 07:50:33,943 EPOCH 71
2024-02-03 07:50:36,441 [Epoch: 071 Step: 00002400] Batch Recognition Loss:   0.203925 => Gls Tokens per Sec:     2462 || Batch Translation Loss:   6.728468 => Txt Tokens per Sec:     6826 || Lr: 0.000100
2024-02-03 07:50:38,669 Epoch  71: Total Training Recognition Loss 3.07  Total Training Translation Loss 171.27 
2024-02-03 07:50:38,670 EPOCH 72
2024-02-03 07:50:43,150 Epoch  72: Total Training Recognition Loss 2.98  Total Training Translation Loss 156.54 
2024-02-03 07:50:43,151 EPOCH 73
2024-02-03 07:50:47,831 Epoch  73: Total Training Recognition Loss 3.01  Total Training Translation Loss 147.26 
2024-02-03 07:50:47,831 EPOCH 74
2024-02-03 07:50:50,240 [Epoch: 074 Step: 00002500] Batch Recognition Loss:   0.039356 => Gls Tokens per Sec:     2288 || Batch Translation Loss:   3.488623 => Txt Tokens per Sec:     6301 || Lr: 0.000100
2024-02-03 07:50:52,290 Epoch  74: Total Training Recognition Loss 2.84  Total Training Translation Loss 141.51 
2024-02-03 07:50:52,290 EPOCH 75
2024-02-03 07:50:56,925 Epoch  75: Total Training Recognition Loss 2.89  Total Training Translation Loss 141.12 
2024-02-03 07:50:56,926 EPOCH 76
2024-02-03 07:51:01,447 Epoch  76: Total Training Recognition Loss 2.73  Total Training Translation Loss 129.04 
2024-02-03 07:51:01,447 EPOCH 77
2024-02-03 07:51:03,315 [Epoch: 077 Step: 00002600] Batch Recognition Loss:   0.034575 => Gls Tokens per Sec:     2609 || Batch Translation Loss:   3.327646 => Txt Tokens per Sec:     7133 || Lr: 0.000100
2024-02-03 07:51:06,108 Epoch  77: Total Training Recognition Loss 2.67  Total Training Translation Loss 121.14 
2024-02-03 07:51:06,109 EPOCH 78
2024-02-03 07:51:10,609 Epoch  78: Total Training Recognition Loss 2.70  Total Training Translation Loss 113.61 
2024-02-03 07:51:10,610 EPOCH 79
2024-02-03 07:51:15,270 Epoch  79: Total Training Recognition Loss 2.67  Total Training Translation Loss 109.25 
2024-02-03 07:51:15,270 EPOCH 80
2024-02-03 07:51:17,228 [Epoch: 080 Step: 00002700] Batch Recognition Loss:   0.022432 => Gls Tokens per Sec:     2162 || Batch Translation Loss:   2.479401 => Txt Tokens per Sec:     5783 || Lr: 0.000100
2024-02-03 07:51:19,785 Epoch  80: Total Training Recognition Loss 2.53  Total Training Translation Loss 101.99 
2024-02-03 07:51:19,785 EPOCH 81
2024-02-03 07:51:24,420 Epoch  81: Total Training Recognition Loss 2.62  Total Training Translation Loss 101.33 
2024-02-03 07:51:24,421 EPOCH 82
2024-02-03 07:51:28,952 Epoch  82: Total Training Recognition Loss 2.38  Total Training Translation Loss 93.23 
2024-02-03 07:51:28,953 EPOCH 83
2024-02-03 07:51:30,396 [Epoch: 083 Step: 00002800] Batch Recognition Loss:   0.161083 => Gls Tokens per Sec:     2662 || Batch Translation Loss:   3.470251 => Txt Tokens per Sec:     7913 || Lr: 0.000100
2024-02-03 07:51:33,483 Epoch  83: Total Training Recognition Loss 2.35  Total Training Translation Loss 86.69 
2024-02-03 07:51:33,484 EPOCH 84
2024-02-03 07:51:37,992 Epoch  84: Total Training Recognition Loss 2.22  Total Training Translation Loss 81.19 
2024-02-03 07:51:37,992 EPOCH 85
2024-02-03 07:51:42,527 Epoch  85: Total Training Recognition Loss 2.19  Total Training Translation Loss 78.43 
2024-02-03 07:51:42,528 EPOCH 86
2024-02-03 07:51:44,292 [Epoch: 086 Step: 00002900] Batch Recognition Loss:   0.027147 => Gls Tokens per Sec:     1815 || Batch Translation Loss:   1.892705 => Txt Tokens per Sec:     5480 || Lr: 0.000100
2024-02-03 07:51:47,086 Epoch  86: Total Training Recognition Loss 2.15  Total Training Translation Loss 73.75 
2024-02-03 07:51:47,086 EPOCH 87
2024-02-03 07:51:51,652 Epoch  87: Total Training Recognition Loss 2.14  Total Training Translation Loss 71.06 
2024-02-03 07:51:51,653 EPOCH 88
2024-02-03 07:51:56,252 Epoch  88: Total Training Recognition Loss 2.12  Total Training Translation Loss 67.97 
2024-02-03 07:51:56,252 EPOCH 89
2024-02-03 07:51:57,098 [Epoch: 089 Step: 00003000] Batch Recognition Loss:   0.071730 => Gls Tokens per Sec:     2736 || Batch Translation Loss:   1.786650 => Txt Tokens per Sec:     7287 || Lr: 0.000100
2024-02-03 07:52:00,815 Epoch  89: Total Training Recognition Loss 2.14  Total Training Translation Loss 64.44 
2024-02-03 07:52:00,816 EPOCH 90
2024-02-03 07:52:05,435 Epoch  90: Total Training Recognition Loss 1.89  Total Training Translation Loss 61.05 
2024-02-03 07:52:05,436 EPOCH 91
2024-02-03 07:52:09,639 Epoch  91: Total Training Recognition Loss 1.83  Total Training Translation Loss 57.74 
2024-02-03 07:52:09,639 EPOCH 92
2024-02-03 07:52:10,328 [Epoch: 092 Step: 00003100] Batch Recognition Loss:   0.067501 => Gls Tokens per Sec:     2795 || Batch Translation Loss:   1.779986 => Txt Tokens per Sec:     6658 || Lr: 0.000100
2024-02-03 07:52:14,546 Epoch  92: Total Training Recognition Loss 1.83  Total Training Translation Loss 57.09 
2024-02-03 07:52:14,547 EPOCH 93
2024-02-03 07:52:19,303 Epoch  93: Total Training Recognition Loss 1.79  Total Training Translation Loss 55.65 
2024-02-03 07:52:19,303 EPOCH 94
2024-02-03 07:52:23,641 Epoch  94: Total Training Recognition Loss 1.83  Total Training Translation Loss 53.19 
2024-02-03 07:52:23,642 EPOCH 95
2024-02-03 07:52:24,367 [Epoch: 095 Step: 00003200] Batch Recognition Loss:   0.017512 => Gls Tokens per Sec:     1768 || Batch Translation Loss:   1.280314 => Txt Tokens per Sec:     5659 || Lr: 0.000100
2024-02-03 07:52:28,533 Epoch  95: Total Training Recognition Loss 1.69  Total Training Translation Loss 50.65 
2024-02-03 07:52:28,534 EPOCH 96
2024-02-03 07:52:32,838 Epoch  96: Total Training Recognition Loss 1.66  Total Training Translation Loss 49.95 
2024-02-03 07:52:32,838 EPOCH 97
2024-02-03 07:52:37,597 Epoch  97: Total Training Recognition Loss 1.67  Total Training Translation Loss 46.56 
2024-02-03 07:52:37,597 EPOCH 98
2024-02-03 07:52:37,838 [Epoch: 098 Step: 00003300] Batch Recognition Loss:   0.022067 => Gls Tokens per Sec:     2667 || Batch Translation Loss:   1.087550 => Txt Tokens per Sec:     7233 || Lr: 0.000100
2024-02-03 07:52:42,647 Epoch  98: Total Training Recognition Loss 1.69  Total Training Translation Loss 44.78 
2024-02-03 07:52:42,648 EPOCH 99
2024-02-03 07:52:47,328 Epoch  99: Total Training Recognition Loss 1.61  Total Training Translation Loss 44.73 
2024-02-03 07:52:47,328 EPOCH 100
2024-02-03 07:52:52,033 [Epoch: 100 Step: 00003400] Batch Recognition Loss:   0.070989 => Gls Tokens per Sec:     2260 || Batch Translation Loss:   1.782927 => Txt Tokens per Sec:     6284 || Lr: 0.000100
2024-02-03 07:52:52,034 Epoch 100: Total Training Recognition Loss 1.51  Total Training Translation Loss 41.53 
2024-02-03 07:52:52,034 EPOCH 101
2024-02-03 07:52:56,487 Epoch 101: Total Training Recognition Loss 1.46  Total Training Translation Loss 41.39 
2024-02-03 07:52:56,488 EPOCH 102
2024-02-03 07:53:01,148 Epoch 102: Total Training Recognition Loss 1.58  Total Training Translation Loss 38.51 
2024-02-03 07:53:01,149 EPOCH 103
2024-02-03 07:53:05,611 [Epoch: 103 Step: 00003500] Batch Recognition Loss:   0.043162 => Gls Tokens per Sec:     2239 || Batch Translation Loss:   1.449707 => Txt Tokens per Sec:     6197 || Lr: 0.000100
2024-02-03 07:53:05,938 Epoch 103: Total Training Recognition Loss 1.49  Total Training Translation Loss 37.65 
2024-02-03 07:53:05,938 EPOCH 104
2024-02-03 07:53:10,679 Epoch 104: Total Training Recognition Loss 1.52  Total Training Translation Loss 36.31 
2024-02-03 07:53:10,680 EPOCH 105
2024-02-03 07:53:15,628 Epoch 105: Total Training Recognition Loss 1.48  Total Training Translation Loss 34.99 
2024-02-03 07:53:15,629 EPOCH 106
2024-02-03 07:53:19,764 [Epoch: 106 Step: 00003600] Batch Recognition Loss:   0.012189 => Gls Tokens per Sec:     2262 || Batch Translation Loss:   0.931031 => Txt Tokens per Sec:     6248 || Lr: 0.000100
2024-02-03 07:53:20,629 Epoch 106: Total Training Recognition Loss 1.32  Total Training Translation Loss 34.86 
2024-02-03 07:53:20,629 EPOCH 107
2024-02-03 07:53:25,622 Epoch 107: Total Training Recognition Loss 1.32  Total Training Translation Loss 35.10 
2024-02-03 07:53:25,623 EPOCH 108
2024-02-03 07:53:30,351 Epoch 108: Total Training Recognition Loss 1.42  Total Training Translation Loss 36.52 
2024-02-03 07:53:30,351 EPOCH 109
2024-02-03 07:53:34,267 [Epoch: 109 Step: 00003700] Batch Recognition Loss:   0.017222 => Gls Tokens per Sec:     2225 || Batch Translation Loss:   0.939069 => Txt Tokens per Sec:     6262 || Lr: 0.000100
2024-02-03 07:53:35,251 Epoch 109: Total Training Recognition Loss 1.33  Total Training Translation Loss 32.55 
2024-02-03 07:53:35,251 EPOCH 110
2024-02-03 07:53:39,673 Epoch 110: Total Training Recognition Loss 1.29  Total Training Translation Loss 30.62 
2024-02-03 07:53:39,673 EPOCH 111
2024-02-03 07:53:44,597 Epoch 111: Total Training Recognition Loss 1.31  Total Training Translation Loss 28.95 
2024-02-03 07:53:44,597 EPOCH 112
2024-02-03 07:53:48,276 [Epoch: 112 Step: 00003800] Batch Recognition Loss:   0.022046 => Gls Tokens per Sec:     2194 || Batch Translation Loss:   0.849729 => Txt Tokens per Sec:     6106 || Lr: 0.000100
2024-02-03 07:53:49,296 Epoch 112: Total Training Recognition Loss 1.30  Total Training Translation Loss 28.89 
2024-02-03 07:53:49,296 EPOCH 113
2024-02-03 07:53:54,176 Epoch 113: Total Training Recognition Loss 1.28  Total Training Translation Loss 27.56 
2024-02-03 07:53:54,176 EPOCH 114
2024-02-03 07:53:58,989 Epoch 114: Total Training Recognition Loss 1.13  Total Training Translation Loss 27.31 
2024-02-03 07:53:58,990 EPOCH 115
2024-02-03 07:54:02,784 [Epoch: 115 Step: 00003900] Batch Recognition Loss:   0.017423 => Gls Tokens per Sec:     1959 || Batch Translation Loss:   0.872783 => Txt Tokens per Sec:     5721 || Lr: 0.000100
2024-02-03 07:54:03,966 Epoch 115: Total Training Recognition Loss 1.15  Total Training Translation Loss 25.14 
2024-02-03 07:54:03,966 EPOCH 116
2024-02-03 07:54:08,284 Epoch 116: Total Training Recognition Loss 1.02  Total Training Translation Loss 26.78 
2024-02-03 07:54:08,284 EPOCH 117
2024-02-03 07:54:13,264 Epoch 117: Total Training Recognition Loss 1.39  Total Training Translation Loss 25.50 
2024-02-03 07:54:13,265 EPOCH 118
2024-02-03 07:54:16,038 [Epoch: 118 Step: 00004000] Batch Recognition Loss:   0.012418 => Gls Tokens per Sec:     2541 || Batch Translation Loss:   0.730925 => Txt Tokens per Sec:     6883 || Lr: 0.000100
2024-02-03 07:54:24,629 Hooray! New best validation result [eval_metric]!
2024-02-03 07:54:24,630 Saving new checkpoint.
2024-02-03 07:54:24,924 Validation result at epoch 118, step     4000: duration: 8.8862s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 3.31250	Translation Loss: 74335.11719	PPL: 2254.58716
	Eval Metric: BLEU
	WER 6.29	(DEL: 0.00,	INS: 0.00,	SUB: 6.29)
	BLEU-4 0.63	(BLEU-1: 11.65,	BLEU-2: 3.81,	BLEU-3: 1.52,	BLEU-4: 0.63)
	CHRF 17.36	ROUGE 9.65
2024-02-03 07:54:24,924 Logging Recognition and Translation Outputs
2024-02-03 07:54:24,925 ========================================================================================================================
2024-02-03 07:54:24,925 Logging Sequence: 52_208.00
2024-02-03 07:54:24,925 	Gloss Reference :	A B+C+D+E
2024-02-03 07:54:24,925 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 07:54:24,925 	Gloss Alignment :	         
2024-02-03 07:54:24,925 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 07:54:24,927 	Text Reference  :	after *** seeing  dhoni   play within 3   hours     36      lakh people downloaded candy crush   
2024-02-03 07:54:24,927 	Text Hypothesis :	after the intense bidding fans are    now excitedly waiting for  the    ipl        to    commence
2024-02-03 07:54:24,927 	Text Alignment  :	      I   S       S       S    S      S   S         S       S    S      S          S     S       
2024-02-03 07:54:24,927 ========================================================================================================================
2024-02-03 07:54:24,927 Logging Sequence: 177_79.00
2024-02-03 07:54:24,927 	Gloss Reference :	A B+C+D+E
2024-02-03 07:54:24,927 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 07:54:24,928 	Gloss Alignment :	         
2024-02-03 07:54:24,928 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 07:54:24,929 	Text Reference  :	finally on 23rd may both     sushil and   ajay were  arrested in ****** delhi' mundka area    
2024-02-03 07:54:24,929 	Text Hypothesis :	******* as per  the olympics are    going to   delhi capitals in search of     the    wrestler
2024-02-03 07:54:24,929 	Text Alignment  :	D       S  S    S   S        S      S     S    S     S           I      S      S      S       
2024-02-03 07:54:24,929 ========================================================================================================================
2024-02-03 07:54:24,930 Logging Sequence: 107_94.00
2024-02-03 07:54:24,930 	Gloss Reference :	A B+C+D+E
2024-02-03 07:54:24,930 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 07:54:24,930 	Gloss Alignment :	         
2024-02-03 07:54:24,930 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 07:54:24,931 	Text Reference  :	and currently development officer of the       bengal tennis association bta   said   
2024-02-03 07:54:24,931 	Text Hypothesis :	*** will      be          played  31 september 2023   or     for         three formats
2024-02-03 07:54:24,931 	Text Alignment  :	D   S         S           S       S  S         S      S      S           S     S      
2024-02-03 07:54:24,931 ========================================================================================================================
2024-02-03 07:54:24,931 Logging Sequence: 114_153.00
2024-02-03 07:54:24,932 	Gloss Reference :	A B+C+D+E
2024-02-03 07:54:24,932 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 07:54:24,932 	Gloss Alignment :	         
2024-02-03 07:54:24,932 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 07:54:24,933 	Text Reference  :	another big   football news  is        that the copa america final
2024-02-03 07:54:24,933 	Text Hypothesis :	******* after 28       years argentina won  the copa america *****
2024-02-03 07:54:24,933 	Text Alignment  :	D       S     S        S     S         S                     D    
2024-02-03 07:54:24,933 ========================================================================================================================
2024-02-03 07:54:24,933 Logging Sequence: 52_36.00
2024-02-03 07:54:24,933 	Gloss Reference :	A B+C+D+E    
2024-02-03 07:54:24,933 	Gloss Hypothesis:	A B+C+D+E+D+E
2024-02-03 07:54:24,934 	Gloss Alignment :	  S          
2024-02-03 07:54:24,934 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 07:54:24,934 	Text Reference  :	** recently dhoni was  travellin on an indigo   flight
2024-02-03 07:54:24,934 	Text Hypothesis :	he said     'i    will continue  to my personal crush 
2024-02-03 07:54:24,934 	Text Alignment  :	I  S        S     S    S         S  S  S        S     
2024-02-03 07:54:24,935 ========================================================================================================================
2024-02-03 07:54:26,795 Epoch 118: Total Training Recognition Loss 1.34  Total Training Translation Loss 24.76 
2024-02-03 07:54:26,796 EPOCH 119
2024-02-03 07:54:31,652 Epoch 119: Total Training Recognition Loss 1.15  Total Training Translation Loss 25.09 
2024-02-03 07:54:31,653 EPOCH 120
2024-02-03 07:54:35,782 Epoch 120: Total Training Recognition Loss 1.04  Total Training Translation Loss 24.27 
2024-02-03 07:54:35,782 EPOCH 121
2024-02-03 07:54:39,041 [Epoch: 121 Step: 00004100] Batch Recognition Loss:   0.034610 => Gls Tokens per Sec:     1964 || Batch Translation Loss:   0.587456 => Txt Tokens per Sec:     5702 || Lr: 0.000100
2024-02-03 07:54:40,749 Epoch 121: Total Training Recognition Loss 1.08  Total Training Translation Loss 23.38 
2024-02-03 07:54:40,750 EPOCH 122
2024-02-03 07:54:45,501 Epoch 122: Total Training Recognition Loss 0.99  Total Training Translation Loss 28.01 
2024-02-03 07:54:45,502 EPOCH 123
2024-02-03 07:54:49,907 Epoch 123: Total Training Recognition Loss 1.08  Total Training Translation Loss 25.27 
2024-02-03 07:54:49,908 EPOCH 124
2024-02-03 07:54:52,152 [Epoch: 124 Step: 00004200] Batch Recognition Loss:   0.099905 => Gls Tokens per Sec:     2458 || Batch Translation Loss:   0.257467 => Txt Tokens per Sec:     6537 || Lr: 0.000100
2024-02-03 07:54:54,758 Epoch 124: Total Training Recognition Loss 1.09  Total Training Translation Loss 23.38 
2024-02-03 07:54:54,759 EPOCH 125
2024-02-03 07:54:59,093 Epoch 125: Total Training Recognition Loss 1.34  Total Training Translation Loss 22.37 
2024-02-03 07:54:59,093 EPOCH 126
2024-02-03 07:55:03,137 Epoch 126: Total Training Recognition Loss 1.04  Total Training Translation Loss 23.86 
2024-02-03 07:55:03,137 EPOCH 127
2024-02-03 07:55:05,802 [Epoch: 127 Step: 00004300] Batch Recognition Loss:   0.017119 => Gls Tokens per Sec:     1828 || Batch Translation Loss:   0.525152 => Txt Tokens per Sec:     5405 || Lr: 0.000100
2024-02-03 07:55:08,043 Epoch 127: Total Training Recognition Loss 1.16  Total Training Translation Loss 25.44 
2024-02-03 07:55:08,043 EPOCH 128
2024-02-03 07:55:12,304 Epoch 128: Total Training Recognition Loss 1.07  Total Training Translation Loss 21.67 
2024-02-03 07:55:12,305 EPOCH 129
2024-02-03 07:55:17,254 Epoch 129: Total Training Recognition Loss 0.99  Total Training Translation Loss 22.35 
2024-02-03 07:55:17,255 EPOCH 130
2024-02-03 07:55:19,037 [Epoch: 130 Step: 00004400] Batch Recognition Loss:   0.026885 => Gls Tokens per Sec:     2515 || Batch Translation Loss:   1.065286 => Txt Tokens per Sec:     7065 || Lr: 0.000100
2024-02-03 07:55:21,840 Epoch 130: Total Training Recognition Loss 0.88  Total Training Translation Loss 20.23 
2024-02-03 07:55:21,841 EPOCH 131
2024-02-03 07:55:26,490 Epoch 131: Total Training Recognition Loss 1.06  Total Training Translation Loss 24.32 
2024-02-03 07:55:26,490 EPOCH 132
2024-02-03 07:55:30,943 Epoch 132: Total Training Recognition Loss 1.22  Total Training Translation Loss 33.59 
2024-02-03 07:55:30,944 EPOCH 133
2024-02-03 07:55:32,567 [Epoch: 133 Step: 00004500] Batch Recognition Loss:   0.013193 => Gls Tokens per Sec:     2367 || Batch Translation Loss:   0.563177 => Txt Tokens per Sec:     6692 || Lr: 0.000100
2024-02-03 07:55:35,619 Epoch 133: Total Training Recognition Loss 1.15  Total Training Translation Loss 27.49 
2024-02-03 07:55:35,619 EPOCH 134
2024-02-03 07:55:39,840 Epoch 134: Total Training Recognition Loss 1.19  Total Training Translation Loss 22.54 
2024-02-03 07:55:39,840 EPOCH 135
2024-02-03 07:55:44,754 Epoch 135: Total Training Recognition Loss 1.06  Total Training Translation Loss 20.86 
2024-02-03 07:55:44,755 EPOCH 136
2024-02-03 07:55:45,957 [Epoch: 136 Step: 00004600] Batch Recognition Loss:   0.016126 => Gls Tokens per Sec:     2666 || Batch Translation Loss:   0.416643 => Txt Tokens per Sec:     7690 || Lr: 0.000100
2024-02-03 07:55:48,796 Epoch 136: Total Training Recognition Loss 1.00  Total Training Translation Loss 18.83 
2024-02-03 07:55:48,796 EPOCH 137
2024-02-03 07:55:52,808 Epoch 137: Total Training Recognition Loss 1.02  Total Training Translation Loss 22.77 
2024-02-03 07:55:52,808 EPOCH 138
2024-02-03 07:55:57,673 Epoch 138: Total Training Recognition Loss 0.79  Total Training Translation Loss 18.40 
2024-02-03 07:55:57,674 EPOCH 139
2024-02-03 07:55:59,046 [Epoch: 139 Step: 00004700] Batch Recognition Loss:   0.012131 => Gls Tokens per Sec:     1867 || Batch Translation Loss:   0.442737 => Txt Tokens per Sec:     5462 || Lr: 0.000100
2024-02-03 07:56:02,074 Epoch 139: Total Training Recognition Loss 0.83  Total Training Translation Loss 17.08 
2024-02-03 07:56:02,074 EPOCH 140
2024-02-03 07:56:06,952 Epoch 140: Total Training Recognition Loss 0.79  Total Training Translation Loss 14.56 
2024-02-03 07:56:06,953 EPOCH 141
2024-02-03 07:56:11,294 Epoch 141: Total Training Recognition Loss 0.76  Total Training Translation Loss 13.74 
2024-02-03 07:56:11,294 EPOCH 142
2024-02-03 07:56:12,122 [Epoch: 142 Step: 00004800] Batch Recognition Loss:   0.039770 => Gls Tokens per Sec:     2324 || Batch Translation Loss:   0.552582 => Txt Tokens per Sec:     6758 || Lr: 0.000100
2024-02-03 07:56:16,166 Epoch 142: Total Training Recognition Loss 0.67  Total Training Translation Loss 12.82 
2024-02-03 07:56:16,166 EPOCH 143
2024-02-03 07:56:21,205 Epoch 143: Total Training Recognition Loss 0.63  Total Training Translation Loss 11.69 
2024-02-03 07:56:21,205 EPOCH 144
2024-02-03 07:56:25,876 Epoch 144: Total Training Recognition Loss 0.62  Total Training Translation Loss 11.25 
2024-02-03 07:56:25,877 EPOCH 145
2024-02-03 07:56:26,483 [Epoch: 145 Step: 00004900] Batch Recognition Loss:   0.016814 => Gls Tokens per Sec:     2116 || Batch Translation Loss:   0.163379 => Txt Tokens per Sec:     6197 || Lr: 0.000100
2024-02-03 07:56:30,752 Epoch 145: Total Training Recognition Loss 0.60  Total Training Translation Loss 10.59 
2024-02-03 07:56:30,753 EPOCH 146
2024-02-03 07:56:35,359 Epoch 146: Total Training Recognition Loss 0.65  Total Training Translation Loss 9.40 
2024-02-03 07:56:35,359 EPOCH 147
2024-02-03 07:56:40,215 Epoch 147: Total Training Recognition Loss 0.68  Total Training Translation Loss 9.09 
2024-02-03 07:56:40,215 EPOCH 148
2024-02-03 07:56:40,550 [Epoch: 148 Step: 00005000] Batch Recognition Loss:   0.012147 => Gls Tokens per Sec:     1919 || Batch Translation Loss:   0.312595 => Txt Tokens per Sec:     6305 || Lr: 0.000100
2024-02-03 07:56:45,185 Epoch 148: Total Training Recognition Loss 0.55  Total Training Translation Loss 8.95 
2024-02-03 07:56:45,185 EPOCH 149
2024-02-03 07:56:49,703 Epoch 149: Total Training Recognition Loss 0.62  Total Training Translation Loss 9.94 
2024-02-03 07:56:49,703 EPOCH 150
2024-02-03 07:56:54,161 [Epoch: 150 Step: 00005100] Batch Recognition Loss:   0.024554 => Gls Tokens per Sec:     2385 || Batch Translation Loss:   0.154037 => Txt Tokens per Sec:     6634 || Lr: 0.000100
2024-02-03 07:56:54,161 Epoch 150: Total Training Recognition Loss 0.65  Total Training Translation Loss 11.74 
2024-02-03 07:56:54,161 EPOCH 151
2024-02-03 07:56:58,887 Epoch 151: Total Training Recognition Loss 0.60  Total Training Translation Loss 11.32 
2024-02-03 07:56:58,887 EPOCH 152
2024-02-03 07:57:03,079 Epoch 152: Total Training Recognition Loss 0.60  Total Training Translation Loss 11.51 
2024-02-03 07:57:03,080 EPOCH 153
2024-02-03 07:57:07,790 [Epoch: 153 Step: 00005200] Batch Recognition Loss:   0.006258 => Gls Tokens per Sec:     2121 || Batch Translation Loss:   0.320655 => Txt Tokens per Sec:     5948 || Lr: 0.000100
2024-02-03 07:57:07,974 Epoch 153: Total Training Recognition Loss 0.57  Total Training Translation Loss 11.34 
2024-02-03 07:57:07,974 EPOCH 154
2024-02-03 07:57:12,755 Epoch 154: Total Training Recognition Loss 0.63  Total Training Translation Loss 12.33 
2024-02-03 07:57:12,755 EPOCH 155
2024-02-03 07:57:17,116 Epoch 155: Total Training Recognition Loss 0.61  Total Training Translation Loss 12.10 
2024-02-03 07:57:17,116 EPOCH 156
2024-02-03 07:57:21,507 [Epoch: 156 Step: 00005300] Batch Recognition Loss:   0.034295 => Gls Tokens per Sec:     2130 || Batch Translation Loss:   0.498948 => Txt Tokens per Sec:     5935 || Lr: 0.000100
2024-02-03 07:57:22,014 Epoch 156: Total Training Recognition Loss 0.63  Total Training Translation Loss 12.59 
2024-02-03 07:57:22,014 EPOCH 157
2024-02-03 07:57:26,283 Epoch 157: Total Training Recognition Loss 0.54  Total Training Translation Loss 13.09 
2024-02-03 07:57:26,283 EPOCH 158
2024-02-03 07:57:31,203 Epoch 158: Total Training Recognition Loss 0.61  Total Training Translation Loss 12.04 
2024-02-03 07:57:31,203 EPOCH 159
2024-02-03 07:57:35,129 [Epoch: 159 Step: 00005400] Batch Recognition Loss:   0.037350 => Gls Tokens per Sec:     2219 || Batch Translation Loss:   0.530583 => Txt Tokens per Sec:     6172 || Lr: 0.000100
2024-02-03 07:57:35,873 Epoch 159: Total Training Recognition Loss 0.57  Total Training Translation Loss 12.50 
2024-02-03 07:57:35,873 EPOCH 160
2024-02-03 07:57:39,905 Epoch 160: Total Training Recognition Loss 0.59  Total Training Translation Loss 12.05 
2024-02-03 07:57:39,905 EPOCH 161
2024-02-03 07:57:43,932 Epoch 161: Total Training Recognition Loss 0.55  Total Training Translation Loss 11.93 
2024-02-03 07:57:43,932 EPOCH 162
2024-02-03 07:57:47,633 [Epoch: 162 Step: 00005500] Batch Recognition Loss:   0.014102 => Gls Tokens per Sec:     2249 || Batch Translation Loss:   0.379068 => Txt Tokens per Sec:     6294 || Lr: 0.000100
2024-02-03 07:57:48,662 Epoch 162: Total Training Recognition Loss 0.55  Total Training Translation Loss 11.95 
2024-02-03 07:57:48,663 EPOCH 163
2024-02-03 07:57:53,513 Epoch 163: Total Training Recognition Loss 0.61  Total Training Translation Loss 11.84 
2024-02-03 07:57:53,513 EPOCH 164
2024-02-03 07:57:58,287 Epoch 164: Total Training Recognition Loss 0.54  Total Training Translation Loss 12.05 
2024-02-03 07:57:58,288 EPOCH 165
2024-02-03 07:58:01,354 [Epoch: 165 Step: 00005600] Batch Recognition Loss:   0.011940 => Gls Tokens per Sec:     2424 || Batch Translation Loss:   0.765800 => Txt Tokens per Sec:     6624 || Lr: 0.000100
2024-02-03 07:58:02,517 Epoch 165: Total Training Recognition Loss 0.61  Total Training Translation Loss 12.21 
2024-02-03 07:58:02,517 EPOCH 166
2024-02-03 07:58:06,941 Epoch 166: Total Training Recognition Loss 0.63  Total Training Translation Loss 14.18 
2024-02-03 07:58:06,942 EPOCH 167
2024-02-03 07:58:11,877 Epoch 167: Total Training Recognition Loss 0.57  Total Training Translation Loss 12.98 
2024-02-03 07:58:11,878 EPOCH 168
2024-02-03 07:58:14,949 [Epoch: 168 Step: 00005700] Batch Recognition Loss:   0.055710 => Gls Tokens per Sec:     2212 || Batch Translation Loss:   0.558141 => Txt Tokens per Sec:     6179 || Lr: 0.000100
2024-02-03 07:58:16,812 Epoch 168: Total Training Recognition Loss 0.68  Total Training Translation Loss 15.85 
2024-02-03 07:58:16,813 EPOCH 169
2024-02-03 07:58:21,625 Epoch 169: Total Training Recognition Loss 0.56  Total Training Translation Loss 16.55 
2024-02-03 07:58:21,626 EPOCH 170
2024-02-03 07:58:26,532 Epoch 170: Total Training Recognition Loss 0.70  Total Training Translation Loss 17.06 
2024-02-03 07:58:26,532 EPOCH 171
2024-02-03 07:58:28,821 [Epoch: 171 Step: 00005800] Batch Recognition Loss:   0.004463 => Gls Tokens per Sec:     2797 || Batch Translation Loss:   0.297142 => Txt Tokens per Sec:     7525 || Lr: 0.000100
2024-02-03 07:58:30,898 Epoch 171: Total Training Recognition Loss 0.62  Total Training Translation Loss 13.99 
2024-02-03 07:58:30,898 EPOCH 172
2024-02-03 07:58:35,586 Epoch 172: Total Training Recognition Loss 0.60  Total Training Translation Loss 16.40 
2024-02-03 07:58:35,586 EPOCH 173
2024-02-03 07:58:40,661 Epoch 173: Total Training Recognition Loss 0.62  Total Training Translation Loss 18.40 
2024-02-03 07:58:40,662 EPOCH 174
2024-02-03 07:58:43,309 [Epoch: 174 Step: 00005900] Batch Recognition Loss:   0.009092 => Gls Tokens per Sec:     2178 || Batch Translation Loss:   0.315540 => Txt Tokens per Sec:     6200 || Lr: 0.000100
2024-02-03 07:58:45,270 Epoch 174: Total Training Recognition Loss 0.60  Total Training Translation Loss 15.43 
2024-02-03 07:58:45,270 EPOCH 175
2024-02-03 07:58:50,246 Epoch 175: Total Training Recognition Loss 0.76  Total Training Translation Loss 16.52 
2024-02-03 07:58:50,246 EPOCH 176
2024-02-03 07:58:54,862 Epoch 176: Total Training Recognition Loss 1.33  Total Training Translation Loss 17.46 
2024-02-03 07:58:54,862 EPOCH 177
2024-02-03 07:58:56,839 [Epoch: 177 Step: 00006000] Batch Recognition Loss:   0.012705 => Gls Tokens per Sec:     2591 || Batch Translation Loss:   0.322124 => Txt Tokens per Sec:     6655 || Lr: 0.000100
2024-02-03 07:59:05,561 Hooray! New best validation result [eval_metric]!
2024-02-03 07:59:05,562 Saving new checkpoint.
2024-02-03 07:59:05,839 Validation result at epoch 177, step     6000: duration: 9.0000s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 3.58024	Translation Loss: 79949.07031	PPL: 4039.22534
	Eval Metric: BLEU
	WER 6.15	(DEL: 0.00,	INS: 0.00,	SUB: 6.15)
	BLEU-4 0.87	(BLEU-1: 11.78,	BLEU-2: 4.04,	BLEU-3: 1.71,	BLEU-4: 0.87)
	CHRF 17.89	ROUGE 9.62
2024-02-03 07:59:05,840 Logging Recognition and Translation Outputs
2024-02-03 07:59:05,840 ========================================================================================================================
2024-02-03 07:59:05,840 Logging Sequence: 101_39.00
2024-02-03 07:59:05,840 	Gloss Reference :	A B+C+D+E
2024-02-03 07:59:05,840 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 07:59:05,840 	Gloss Alignment :	         
2024-02-03 07:59:05,841 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 07:59:05,842 	Text Reference  :	star batsmen and bowlers of     the *** indian team  were infected by  the    coronavirus
2024-02-03 07:59:05,842 	Text Hypothesis :	**** ******* *** they    played the 3rd ball   india and  pakistan had scored 3175       
2024-02-03 07:59:05,842 	Text Alignment  :	D    D       D   S       S          I   S      S     S    S        S   S      S          
2024-02-03 07:59:05,842 ========================================================================================================================
2024-02-03 07:59:05,842 Logging Sequence: 105_139.00
2024-02-03 07:59:05,842 	Gloss Reference :	A B+C+D+E
2024-02-03 07:59:05,843 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 07:59:05,843 	Gloss Alignment :	         
2024-02-03 07:59:05,843 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 07:59:05,844 	Text Reference  :	and now he       has finally achieved his ******** **** dream by       defeating carlsen  
2024-02-03 07:59:05,844 	Text Hypothesis :	in  the response to  be      played   his religion from the   chapagne was       overjoyed
2024-02-03 07:59:05,844 	Text Alignment  :	S   S   S        S   S       S            I        I    S     S        S         S        
2024-02-03 07:59:05,844 ========================================================================================================================
2024-02-03 07:59:05,844 Logging Sequence: 85_17.00
2024-02-03 07:59:05,845 	Gloss Reference :	A B+C+D+E
2024-02-03 07:59:05,845 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 07:59:05,845 	Gloss Alignment :	         
2024-02-03 07:59:05,845 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 07:59:05,846 	Text Reference  :	in the 2003 world      cup  symonds scored an  unbeaten  143 against pakistan
2024-02-03 07:59:05,846 	Text Hypothesis :	** *** **** australian fast bowler  brett  lee announced on  twitter that    
2024-02-03 07:59:05,846 	Text Alignment  :	D  D   D    S          S    S       S      S   S         S   S       S       
2024-02-03 07:59:05,846 ========================================================================================================================
2024-02-03 07:59:05,846 Logging Sequence: 150_98.00
2024-02-03 07:59:05,847 	Gloss Reference :	A B+C+D+E        
2024-02-03 07:59:05,847 	Gloss Hypothesis:	A B+C+B+E+B+C+D+E
2024-02-03 07:59:05,847 	Gloss Alignment :	  S              
2024-02-03 07:59:05,847 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 07:59:05,847 	Text Reference  :	** **** **** *********** chhetri was   the captain 
2024-02-03 07:59:05,848 	Text Hypothesis :	we also have medications to      treat the symptoms
2024-02-03 07:59:05,848 	Text Alignment  :	I  I    I    I           S       S         S       
2024-02-03 07:59:05,848 ========================================================================================================================
2024-02-03 07:59:05,848 Logging Sequence: 147_76.00
2024-02-03 07:59:05,848 	Gloss Reference :	A B+C+D+E
2024-02-03 07:59:05,848 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 07:59:05,848 	Gloss Alignment :	         
2024-02-03 07:59:05,849 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 07:59:05,850 	Text Reference  :	however on     28     july she announced her    withdrawal   from   the olympic games  
2024-02-03 07:59:05,850 	Text Hypothesis :	******* zaheer always has  his lucky     yellow handkerchief during the ******* matches
2024-02-03 07:59:05,850 	Text Alignment  :	D       S      S      S    S   S         S      S            S          D       S      
2024-02-03 07:59:05,850 ========================================================================================================================
2024-02-03 07:59:08,901 Epoch 177: Total Training Recognition Loss 1.57  Total Training Translation Loss 11.97 
2024-02-03 07:59:08,901 EPOCH 178
2024-02-03 07:59:13,522 Epoch 178: Total Training Recognition Loss 1.60  Total Training Translation Loss 10.90 
2024-02-03 07:59:13,522 EPOCH 179
2024-02-03 07:59:18,414 Epoch 179: Total Training Recognition Loss 0.85  Total Training Translation Loss 8.20 
2024-02-03 07:59:18,414 EPOCH 180
2024-02-03 07:59:20,102 [Epoch: 180 Step: 00006100] Batch Recognition Loss:   0.002665 => Gls Tokens per Sec:     2508 || Batch Translation Loss:   0.186219 => Txt Tokens per Sec:     7017 || Lr: 0.000100
2024-02-03 07:59:22,631 Epoch 180: Total Training Recognition Loss 0.56  Total Training Translation Loss 6.11 
2024-02-03 07:59:22,631 EPOCH 181
2024-02-03 07:59:27,517 Epoch 181: Total Training Recognition Loss 0.46  Total Training Translation Loss 5.46 
2024-02-03 07:59:27,518 EPOCH 182
2024-02-03 07:59:31,826 Epoch 182: Total Training Recognition Loss 0.40  Total Training Translation Loss 5.10 
2024-02-03 07:59:31,826 EPOCH 183
2024-02-03 07:59:33,180 [Epoch: 183 Step: 00006200] Batch Recognition Loss:   0.009536 => Gls Tokens per Sec:     2653 || Batch Translation Loss:   0.193744 => Txt Tokens per Sec:     7664 || Lr: 0.000100
2024-02-03 07:59:36,263 Epoch 183: Total Training Recognition Loss 0.46  Total Training Translation Loss 5.42 
2024-02-03 07:59:36,264 EPOCH 184
2024-02-03 07:59:40,975 Epoch 184: Total Training Recognition Loss 0.44  Total Training Translation Loss 5.21 
2024-02-03 07:59:40,975 EPOCH 185
2024-02-03 07:59:45,730 Epoch 185: Total Training Recognition Loss 0.38  Total Training Translation Loss 5.48 
2024-02-03 07:59:45,731 EPOCH 186
2024-02-03 07:59:47,350 [Epoch: 186 Step: 00006300] Batch Recognition Loss:   0.012407 => Gls Tokens per Sec:     1978 || Batch Translation Loss:   0.151905 => Txt Tokens per Sec:     5734 || Lr: 0.000100
2024-02-03 07:59:50,179 Epoch 186: Total Training Recognition Loss 0.50  Total Training Translation Loss 5.35 
2024-02-03 07:59:50,180 EPOCH 187
2024-02-03 07:59:54,883 Epoch 187: Total Training Recognition Loss 0.57  Total Training Translation Loss 5.43 
2024-02-03 07:59:54,883 EPOCH 188
2024-02-03 07:59:59,797 Epoch 188: Total Training Recognition Loss 0.33  Total Training Translation Loss 4.72 
2024-02-03 07:59:59,798 EPOCH 189
2024-02-03 08:00:00,902 [Epoch: 189 Step: 00006400] Batch Recognition Loss:   0.003886 => Gls Tokens per Sec:     2321 || Batch Translation Loss:   0.266591 => Txt Tokens per Sec:     6245 || Lr: 0.000100
2024-02-03 08:00:04,092 Epoch 189: Total Training Recognition Loss 0.34  Total Training Translation Loss 5.80 
2024-02-03 08:00:04,093 EPOCH 190
2024-02-03 08:00:08,679 Epoch 190: Total Training Recognition Loss 0.99  Total Training Translation Loss 18.29 
2024-02-03 08:00:08,679 EPOCH 191
2024-02-03 08:00:13,213 Epoch 191: Total Training Recognition Loss 6.47  Total Training Translation Loss 19.30 
2024-02-03 08:00:13,213 EPOCH 192
2024-02-03 08:00:13,970 [Epoch: 192 Step: 00006500] Batch Recognition Loss:   0.013527 => Gls Tokens per Sec:     2540 || Batch Translation Loss:   0.451355 => Txt Tokens per Sec:     7253 || Lr: 0.000100
2024-02-03 08:00:18,071 Epoch 192: Total Training Recognition Loss 3.31  Total Training Translation Loss 17.71 
2024-02-03 08:00:18,072 EPOCH 193
2024-02-03 08:00:22,409 Epoch 193: Total Training Recognition Loss 1.93  Total Training Translation Loss 12.50 
2024-02-03 08:00:22,409 EPOCH 194
2024-02-03 08:00:27,258 Epoch 194: Total Training Recognition Loss 0.84  Total Training Translation Loss 9.82 
2024-02-03 08:00:27,258 EPOCH 195
2024-02-03 08:00:27,984 [Epoch: 195 Step: 00006600] Batch Recognition Loss:   0.014413 => Gls Tokens per Sec:     1766 || Batch Translation Loss:   0.200411 => Txt Tokens per Sec:     5200 || Lr: 0.000100
2024-02-03 08:00:31,564 Epoch 195: Total Training Recognition Loss 0.50  Total Training Translation Loss 7.59 
2024-02-03 08:00:31,564 EPOCH 196
2024-02-03 08:00:36,409 Epoch 196: Total Training Recognition Loss 0.50  Total Training Translation Loss 6.20 
2024-02-03 08:00:36,409 EPOCH 197
2024-02-03 08:00:40,733 Epoch 197: Total Training Recognition Loss 0.47  Total Training Translation Loss 5.60 
2024-02-03 08:00:40,734 EPOCH 198
2024-02-03 08:00:40,907 [Epoch: 198 Step: 00006700] Batch Recognition Loss:   0.009945 => Gls Tokens per Sec:     3699 || Batch Translation Loss:   0.136846 => Txt Tokens per Sec:     9052 || Lr: 0.000100
2024-02-03 08:00:45,553 Epoch 198: Total Training Recognition Loss 0.37  Total Training Translation Loss 5.90 
2024-02-03 08:00:45,553 EPOCH 199
2024-02-03 08:00:49,859 Epoch 199: Total Training Recognition Loss 0.34  Total Training Translation Loss 5.24 
2024-02-03 08:00:49,859 EPOCH 200
2024-02-03 08:00:54,656 [Epoch: 200 Step: 00006800] Batch Recognition Loss:   0.033677 => Gls Tokens per Sec:     2217 || Batch Translation Loss:   0.232798 => Txt Tokens per Sec:     6165 || Lr: 0.000100
2024-02-03 08:00:54,656 Epoch 200: Total Training Recognition Loss 0.35  Total Training Translation Loss 5.41 
2024-02-03 08:00:54,656 EPOCH 201
2024-02-03 08:00:59,027 Epoch 201: Total Training Recognition Loss 0.32  Total Training Translation Loss 5.72 
2024-02-03 08:00:59,027 EPOCH 202
2024-02-03 08:01:03,792 Epoch 202: Total Training Recognition Loss 0.30  Total Training Translation Loss 4.55 
2024-02-03 08:01:03,793 EPOCH 203
2024-02-03 08:01:07,881 [Epoch: 203 Step: 00006900] Batch Recognition Loss:   0.015471 => Gls Tokens per Sec:     2444 || Batch Translation Loss:   0.140258 => Txt Tokens per Sec:     6734 || Lr: 0.000100
2024-02-03 08:01:08,212 Epoch 203: Total Training Recognition Loss 0.31  Total Training Translation Loss 4.37 
2024-02-03 08:01:08,212 EPOCH 204
2024-02-03 08:01:13,013 Epoch 204: Total Training Recognition Loss 0.27  Total Training Translation Loss 4.02 
2024-02-03 08:01:13,014 EPOCH 205
2024-02-03 08:01:17,339 Epoch 205: Total Training Recognition Loss 0.29  Total Training Translation Loss 5.47 
2024-02-03 08:01:17,339 EPOCH 206
2024-02-03 08:01:21,668 [Epoch: 206 Step: 00007000] Batch Recognition Loss:   0.014522 => Gls Tokens per Sec:     2161 || Batch Translation Loss:   0.395428 => Txt Tokens per Sec:     6004 || Lr: 0.000100
2024-02-03 08:01:22,113 Epoch 206: Total Training Recognition Loss 0.25  Total Training Translation Loss 5.28 
2024-02-03 08:01:22,113 EPOCH 207
2024-02-03 08:01:26,534 Epoch 207: Total Training Recognition Loss 0.31  Total Training Translation Loss 5.75 
2024-02-03 08:01:26,534 EPOCH 208
2024-02-03 08:01:31,260 Epoch 208: Total Training Recognition Loss 0.29  Total Training Translation Loss 4.87 
2024-02-03 08:01:31,261 EPOCH 209
2024-02-03 08:01:34,757 [Epoch: 209 Step: 00007100] Batch Recognition Loss:   0.003161 => Gls Tokens per Sec:     2491 || Batch Translation Loss:   0.192704 => Txt Tokens per Sec:     6805 || Lr: 0.000100
2024-02-03 08:01:35,628 Epoch 209: Total Training Recognition Loss 0.28  Total Training Translation Loss 4.25 
2024-02-03 08:01:35,628 EPOCH 210
2024-02-03 08:01:40,369 Epoch 210: Total Training Recognition Loss 0.27  Total Training Translation Loss 4.36 
2024-02-03 08:01:40,370 EPOCH 211
2024-02-03 08:01:45,063 Epoch 211: Total Training Recognition Loss 0.23  Total Training Translation Loss 5.17 
2024-02-03 08:01:45,064 EPOCH 212
2024-02-03 08:01:48,858 [Epoch: 212 Step: 00007200] Batch Recognition Loss:   0.006934 => Gls Tokens per Sec:     2128 || Batch Translation Loss:   0.206827 => Txt Tokens per Sec:     5952 || Lr: 0.000100
2024-02-03 08:01:49,942 Epoch 212: Total Training Recognition Loss 0.26  Total Training Translation Loss 5.50 
2024-02-03 08:01:49,942 EPOCH 213
2024-02-03 08:01:54,847 Epoch 213: Total Training Recognition Loss 0.25  Total Training Translation Loss 6.25 
2024-02-03 08:01:54,848 EPOCH 214
2024-02-03 08:01:59,681 Epoch 214: Total Training Recognition Loss 0.25  Total Training Translation Loss 7.92 
2024-02-03 08:01:59,682 EPOCH 215
2024-02-03 08:02:02,948 [Epoch: 215 Step: 00007300] Batch Recognition Loss:   0.004814 => Gls Tokens per Sec:     2276 || Batch Translation Loss:   0.210799 => Txt Tokens per Sec:     6141 || Lr: 0.000100
2024-02-03 08:02:04,663 Epoch 215: Total Training Recognition Loss 0.29  Total Training Translation Loss 8.10 
2024-02-03 08:02:04,663 EPOCH 216
2024-02-03 08:02:09,179 Epoch 216: Total Training Recognition Loss 0.31  Total Training Translation Loss 7.37 
2024-02-03 08:02:09,179 EPOCH 217
2024-02-03 08:02:14,065 Epoch 217: Total Training Recognition Loss 0.30  Total Training Translation Loss 7.96 
2024-02-03 08:02:14,065 EPOCH 218
2024-02-03 08:02:16,630 [Epoch: 218 Step: 00007400] Batch Recognition Loss:   0.007650 => Gls Tokens per Sec:     2648 || Batch Translation Loss:   0.136668 => Txt Tokens per Sec:     7318 || Lr: 0.000100
2024-02-03 08:02:18,288 Epoch 218: Total Training Recognition Loss 0.33  Total Training Translation Loss 6.23 
2024-02-03 08:02:18,288 EPOCH 219
2024-02-03 08:02:23,173 Epoch 219: Total Training Recognition Loss 0.30  Total Training Translation Loss 6.29 
2024-02-03 08:02:23,173 EPOCH 220
2024-02-03 08:02:27,919 Epoch 220: Total Training Recognition Loss 0.25  Total Training Translation Loss 6.76 
2024-02-03 08:02:27,919 EPOCH 221
2024-02-03 08:02:30,952 [Epoch: 221 Step: 00007500] Batch Recognition Loss:   0.007877 => Gls Tokens per Sec:     2030 || Batch Translation Loss:   0.223082 => Txt Tokens per Sec:     5620 || Lr: 0.000100
2024-02-03 08:02:32,793 Epoch 221: Total Training Recognition Loss 0.31  Total Training Translation Loss 9.24 
2024-02-03 08:02:32,793 EPOCH 222
2024-02-03 08:02:37,124 Epoch 222: Total Training Recognition Loss 0.47  Total Training Translation Loss 23.81 
2024-02-03 08:02:37,125 EPOCH 223
2024-02-03 08:02:41,973 Epoch 223: Total Training Recognition Loss 0.51  Total Training Translation Loss 15.44 
2024-02-03 08:02:41,973 EPOCH 224
2024-02-03 08:02:44,432 [Epoch: 224 Step: 00007600] Batch Recognition Loss:   0.006468 => Gls Tokens per Sec:     2343 || Batch Translation Loss:   0.206383 => Txt Tokens per Sec:     6570 || Lr: 0.000100
2024-02-03 08:02:46,566 Epoch 224: Total Training Recognition Loss 0.51  Total Training Translation Loss 11.05 
2024-02-03 08:02:46,567 EPOCH 225
2024-02-03 08:02:51,121 Epoch 225: Total Training Recognition Loss 0.39  Total Training Translation Loss 7.43 
2024-02-03 08:02:51,121 EPOCH 226
2024-02-03 08:02:55,755 Epoch 226: Total Training Recognition Loss 0.38  Total Training Translation Loss 5.26 
2024-02-03 08:02:55,756 EPOCH 227
2024-02-03 08:02:57,779 [Epoch: 227 Step: 00007700] Batch Recognition Loss:   0.012143 => Gls Tokens per Sec:     2532 || Batch Translation Loss:   0.110019 => Txt Tokens per Sec:     7032 || Lr: 0.000100
2024-02-03 08:03:00,343 Epoch 227: Total Training Recognition Loss 0.35  Total Training Translation Loss 4.23 
2024-02-03 08:03:00,344 EPOCH 228
2024-02-03 08:03:05,379 Epoch 228: Total Training Recognition Loss 0.33  Total Training Translation Loss 4.40 
2024-02-03 08:03:05,380 EPOCH 229
2024-02-03 08:03:10,156 Epoch 229: Total Training Recognition Loss 0.29  Total Training Translation Loss 4.44 
2024-02-03 08:03:10,157 EPOCH 230
2024-02-03 08:03:11,988 [Epoch: 230 Step: 00007800] Batch Recognition Loss:   0.002068 => Gls Tokens per Sec:     2448 || Batch Translation Loss:   0.182321 => Txt Tokens per Sec:     7090 || Lr: 0.000100
2024-02-03 08:03:14,866 Epoch 230: Total Training Recognition Loss 0.32  Total Training Translation Loss 3.60 
2024-02-03 08:03:14,867 EPOCH 231
2024-02-03 08:03:19,327 Epoch 231: Total Training Recognition Loss 0.29  Total Training Translation Loss 3.80 
2024-02-03 08:03:19,327 EPOCH 232
2024-02-03 08:03:23,977 Epoch 232: Total Training Recognition Loss 0.26  Total Training Translation Loss 3.76 
2024-02-03 08:03:23,977 EPOCH 233
2024-02-03 08:03:25,800 [Epoch: 233 Step: 00007900] Batch Recognition Loss:   0.004837 => Gls Tokens per Sec:     1970 || Batch Translation Loss:   0.024087 => Txt Tokens per Sec:     5948 || Lr: 0.000100
2024-02-03 08:03:28,535 Epoch 233: Total Training Recognition Loss 0.22  Total Training Translation Loss 3.29 
2024-02-03 08:03:28,536 EPOCH 234
2024-02-03 08:03:33,383 Epoch 234: Total Training Recognition Loss 0.28  Total Training Translation Loss 3.88 
2024-02-03 08:03:33,383 EPOCH 235
2024-02-03 08:03:37,686 Epoch 235: Total Training Recognition Loss 0.28  Total Training Translation Loss 4.68 
2024-02-03 08:03:37,686 EPOCH 236
2024-02-03 08:03:39,020 [Epoch: 236 Step: 00008000] Batch Recognition Loss:   0.006661 => Gls Tokens per Sec:     2401 || Batch Translation Loss:   0.139389 => Txt Tokens per Sec:     6362 || Lr: 0.000100
2024-02-03 08:03:47,443 Validation result at epoch 236, step     8000: duration: 8.4220s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.96531	Translation Loss: 84294.42188	PPL: 6343.16211
	Eval Metric: BLEU
	WER 5.52	(DEL: 0.07,	INS: 0.00,	SUB: 5.45)
	BLEU-4 0.76	(BLEU-1: 11.42,	BLEU-2: 3.60,	BLEU-3: 1.49,	BLEU-4: 0.76)
	CHRF 17.29	ROUGE 9.55
2024-02-03 08:03:47,445 Logging Recognition and Translation Outputs
2024-02-03 08:03:47,445 ========================================================================================================================
2024-02-03 08:03:47,445 Logging Sequence: 109_16.00
2024-02-03 08:03:47,445 	Gloss Reference :	A B+C+D+E
2024-02-03 08:03:47,445 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 08:03:47,445 	Gloss Alignment :	         
2024-02-03 08:03:47,445 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 08:03:47,446 	Text Reference  :	however * *** ** the      match was          rescheduled as  two     kkr  players -      
2024-02-03 08:03:47,446 	Text Hypothesis :	however a few of football team  participated in          the results were very    popular
2024-02-03 08:03:47,447 	Text Alignment  :	        I I   I  S        S     S            S           S   S       S    S       S      
2024-02-03 08:03:47,447 ========================================================================================================================
2024-02-03 08:03:47,447 Logging Sequence: 156_272.00
2024-02-03 08:03:47,447 	Gloss Reference :	A B+C+D+E
2024-02-03 08:03:47,447 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 08:03:47,447 	Gloss Alignment :	         
2024-02-03 08:03:47,447 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 08:03:47,449 	Text Reference  :	miny' original captain was kieron pollard nicholas pooran was  stand-in captain in * place of him   
2024-02-03 08:03:47,449 	Text Hypothesis :	the   most     of      mlc began  on      13th     july   2023 and      ended   in a visit to poland
2024-02-03 08:03:47,449 	Text Alignment  :	S     S        S       S   S      S       S        S      S    S        S          I S     S  S     
2024-02-03 08:03:47,449 ========================================================================================================================
2024-02-03 08:03:47,449 Logging Sequence: 115_59.00
2024-02-03 08:03:47,450 	Gloss Reference :	A B+C+D+E
2024-02-03 08:03:47,450 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 08:03:47,450 	Gloss Alignment :	         
2024-02-03 08:03:47,450 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 08:03:47,451 	Text Reference  :	** *** **** ***** **** ****** ****** ** she   now hosts   several cricket programmes
2024-02-03 08:03:47,451 	Text Hypothesis :	on the 15th march 2021 bumrah posted an image on  twitter which   went    viral     
2024-02-03 08:03:47,451 	Text Alignment  :	I  I   I    I     I    I      I      I  S     S   S       S       S       S         
2024-02-03 08:03:47,451 ========================================================================================================================
2024-02-03 08:03:47,451 Logging Sequence: 63_35.00
2024-02-03 08:03:47,451 	Gloss Reference :	A B+C+D+E
2024-02-03 08:03:47,451 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 08:03:47,452 	Gloss Alignment :	         
2024-02-03 08:03:47,452 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 08:03:47,453 	Text Reference  :	***** companies interested in      buying the teams need to   fill    the tender form by  paying rs 10  lakh   
2024-02-03 08:03:47,453 	Text Hypothesis :	until it        was        dropped from   the ***** **** most awaited ipl as     he   has warned to the support
2024-02-03 08:03:47,453 	Text Alignment  :	I     S         S          S       S          D     D    S    S       S   S      S    S   S      S  S   S      
2024-02-03 08:03:47,454 ========================================================================================================================
2024-02-03 08:03:47,454 Logging Sequence: 100_97.00
2024-02-03 08:03:47,454 	Gloss Reference :	A B+C+D+E
2024-02-03 08:03:47,454 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 08:03:47,454 	Gloss Alignment :	         
2024-02-03 08:03:47,454 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 08:03:47,455 	Text Reference  :	india had to  play against pakistan in   the sahara cup    in ***** *** ********** canada
2024-02-03 08:03:47,455 	Text Hypothesis :	when  it  was the  first   time     when it  was    played in qatar for commercial flight
2024-02-03 08:03:47,456 	Text Alignment  :	S     S   S   S    S       S        S    S   S      S         I     I   I          S     
2024-02-03 08:03:47,456 ========================================================================================================================
2024-02-03 08:03:51,193 Epoch 236: Total Training Recognition Loss 0.24  Total Training Translation Loss 4.91 
2024-02-03 08:03:51,193 EPOCH 237
2024-02-03 08:03:55,645 Epoch 237: Total Training Recognition Loss 0.22  Total Training Translation Loss 4.58 
2024-02-03 08:03:55,645 EPOCH 238
2024-02-03 08:04:00,617 Epoch 238: Total Training Recognition Loss 0.25  Total Training Translation Loss 5.63 
2024-02-03 08:04:00,618 EPOCH 239
2024-02-03 08:04:01,813 [Epoch: 239 Step: 00008100] Batch Recognition Loss:   0.001814 => Gls Tokens per Sec:     2144 || Batch Translation Loss:   0.189321 => Txt Tokens per Sec:     5572 || Lr: 0.000100
2024-02-03 08:04:04,854 Epoch 239: Total Training Recognition Loss 0.24  Total Training Translation Loss 5.49 
2024-02-03 08:04:04,854 EPOCH 240
2024-02-03 08:04:09,785 Epoch 240: Total Training Recognition Loss 0.28  Total Training Translation Loss 5.82 
2024-02-03 08:04:09,785 EPOCH 241
2024-02-03 08:04:14,008 Epoch 241: Total Training Recognition Loss 0.25  Total Training Translation Loss 7.61 
2024-02-03 08:04:14,008 EPOCH 242
2024-02-03 08:04:14,736 [Epoch: 242 Step: 00008200] Batch Recognition Loss:   0.005543 => Gls Tokens per Sec:     2641 || Batch Translation Loss:   0.103992 => Txt Tokens per Sec:     6287 || Lr: 0.000100
2024-02-03 08:04:19,105 Epoch 242: Total Training Recognition Loss 0.35  Total Training Translation Loss 9.90 
2024-02-03 08:04:19,106 EPOCH 243
2024-02-03 08:04:24,010 Epoch 243: Total Training Recognition Loss 0.46  Total Training Translation Loss 8.26 
2024-02-03 08:04:24,010 EPOCH 244
2024-02-03 08:04:28,965 Epoch 244: Total Training Recognition Loss 0.36  Total Training Translation Loss 13.92 
2024-02-03 08:04:28,966 EPOCH 245
2024-02-03 08:04:29,619 [Epoch: 245 Step: 00008300] Batch Recognition Loss:   0.012065 => Gls Tokens per Sec:     1960 || Batch Translation Loss:   0.331974 => Txt Tokens per Sec:     5848 || Lr: 0.000100
2024-02-03 08:04:33,462 Epoch 245: Total Training Recognition Loss 0.40  Total Training Translation Loss 9.97 
2024-02-03 08:04:33,462 EPOCH 246
2024-02-03 08:04:38,090 Epoch 246: Total Training Recognition Loss 0.43  Total Training Translation Loss 18.47 
2024-02-03 08:04:38,091 EPOCH 247
2024-02-03 08:04:42,783 Epoch 247: Total Training Recognition Loss 2.87  Total Training Translation Loss 16.47 
2024-02-03 08:04:42,784 EPOCH 248
2024-02-03 08:04:43,088 [Epoch: 248 Step: 00008400] Batch Recognition Loss:   1.551283 => Gls Tokens per Sec:     2103 || Batch Translation Loss:   0.405335 => Txt Tokens per Sec:     6098 || Lr: 0.000100
2024-02-03 08:04:47,646 Epoch 248: Total Training Recognition Loss 4.44  Total Training Translation Loss 9.37 
2024-02-03 08:04:47,646 EPOCH 249
2024-02-03 08:04:51,841 Epoch 249: Total Training Recognition Loss 1.13  Total Training Translation Loss 5.96 
2024-02-03 08:04:51,841 EPOCH 250
2024-02-03 08:04:56,790 [Epoch: 250 Step: 00008500] Batch Recognition Loss:   0.007595 => Gls Tokens per Sec:     2148 || Batch Translation Loss:   0.043155 => Txt Tokens per Sec:     5974 || Lr: 0.000100
2024-02-03 08:04:56,791 Epoch 250: Total Training Recognition Loss 0.43  Total Training Translation Loss 3.93 
2024-02-03 08:04:56,791 EPOCH 251
2024-02-03 08:05:01,443 Epoch 251: Total Training Recognition Loss 0.27  Total Training Translation Loss 3.63 
2024-02-03 08:05:01,443 EPOCH 252
2024-02-03 08:05:06,475 Epoch 252: Total Training Recognition Loss 0.32  Total Training Translation Loss 3.37 
2024-02-03 08:05:06,476 EPOCH 253
2024-02-03 08:05:10,733 [Epoch: 253 Step: 00008600] Batch Recognition Loss:   0.002568 => Gls Tokens per Sec:     2347 || Batch Translation Loss:   0.065972 => Txt Tokens per Sec:     6485 || Lr: 0.000100
2024-02-03 08:05:11,058 Epoch 253: Total Training Recognition Loss 0.24  Total Training Translation Loss 2.83 
2024-02-03 08:05:11,059 EPOCH 254
2024-02-03 08:05:15,785 Epoch 254: Total Training Recognition Loss 0.27  Total Training Translation Loss 2.58 
2024-02-03 08:05:15,786 EPOCH 255
2024-02-03 08:05:20,138 Epoch 255: Total Training Recognition Loss 0.25  Total Training Translation Loss 2.47 
2024-02-03 08:05:20,138 EPOCH 256
2024-02-03 08:05:24,117 [Epoch: 256 Step: 00008700] Batch Recognition Loss:   0.001122 => Gls Tokens per Sec:     2413 || Batch Translation Loss:   0.082010 => Txt Tokens per Sec:     6622 || Lr: 0.000100
2024-02-03 08:05:24,940 Epoch 256: Total Training Recognition Loss 0.21  Total Training Translation Loss 2.90 
2024-02-03 08:05:24,940 EPOCH 257
2024-02-03 08:05:29,221 Epoch 257: Total Training Recognition Loss 0.24  Total Training Translation Loss 3.44 
2024-02-03 08:05:29,222 EPOCH 258
2024-02-03 08:05:34,070 Epoch 258: Total Training Recognition Loss 0.20  Total Training Translation Loss 3.27 
2024-02-03 08:05:34,071 EPOCH 259
2024-02-03 08:05:37,975 [Epoch: 259 Step: 00008800] Batch Recognition Loss:   0.005088 => Gls Tokens per Sec:     2232 || Batch Translation Loss:   0.049820 => Txt Tokens per Sec:     6361 || Lr: 0.000100
2024-02-03 08:05:38,674 Epoch 259: Total Training Recognition Loss 0.21  Total Training Translation Loss 2.43 
2024-02-03 08:05:38,674 EPOCH 260
2024-02-03 08:05:43,261 Epoch 260: Total Training Recognition Loss 0.19  Total Training Translation Loss 2.87 
2024-02-03 08:05:43,261 EPOCH 261
2024-02-03 08:05:48,083 Epoch 261: Total Training Recognition Loss 0.19  Total Training Translation Loss 3.37 
2024-02-03 08:05:48,083 EPOCH 262
2024-02-03 08:05:51,501 [Epoch: 262 Step: 00008900] Batch Recognition Loss:   0.016516 => Gls Tokens per Sec:     2362 || Batch Translation Loss:   0.091252 => Txt Tokens per Sec:     6609 || Lr: 0.000100
2024-02-03 08:05:52,521 Epoch 262: Total Training Recognition Loss 0.23  Total Training Translation Loss 4.71 
2024-02-03 08:05:52,521 EPOCH 263
2024-02-03 08:05:57,423 Epoch 263: Total Training Recognition Loss 0.21  Total Training Translation Loss 7.24 
2024-02-03 08:05:57,424 EPOCH 264
2024-02-03 08:06:01,999 Epoch 264: Total Training Recognition Loss 0.22  Total Training Translation Loss 6.43 
2024-02-03 08:06:01,999 EPOCH 265
2024-02-03 08:06:05,170 [Epoch: 265 Step: 00009000] Batch Recognition Loss:   0.018354 => Gls Tokens per Sec:     2344 || Batch Translation Loss:   0.434191 => Txt Tokens per Sec:     6474 || Lr: 0.000100
2024-02-03 08:06:06,712 Epoch 265: Total Training Recognition Loss 0.24  Total Training Translation Loss 7.63 
2024-02-03 08:06:06,713 EPOCH 266
2024-02-03 08:06:11,264 Epoch 266: Total Training Recognition Loss 0.24  Total Training Translation Loss 6.25 
2024-02-03 08:06:11,265 EPOCH 267
2024-02-03 08:06:16,120 Epoch 267: Total Training Recognition Loss 0.25  Total Training Translation Loss 5.38 
2024-02-03 08:06:16,121 EPOCH 268
2024-02-03 08:06:18,980 [Epoch: 268 Step: 00009100] Batch Recognition Loss:   0.007211 => Gls Tokens per Sec:     2376 || Batch Translation Loss:   0.327178 => Txt Tokens per Sec:     6818 || Lr: 0.000100
2024-02-03 08:06:20,577 Epoch 268: Total Training Recognition Loss 0.23  Total Training Translation Loss 5.21 
2024-02-03 08:06:20,577 EPOCH 269
2024-02-03 08:06:25,556 Epoch 269: Total Training Recognition Loss 0.24  Total Training Translation Loss 5.70 
2024-02-03 08:06:25,556 EPOCH 270
2024-02-03 08:06:29,676 Epoch 270: Total Training Recognition Loss 0.25  Total Training Translation Loss 5.52 
2024-02-03 08:06:29,676 EPOCH 271
2024-02-03 08:06:32,375 [Epoch: 271 Step: 00009200] Batch Recognition Loss:   0.004113 => Gls Tokens per Sec:     2279 || Batch Translation Loss:   0.169731 => Txt Tokens per Sec:     6444 || Lr: 0.000100
2024-02-03 08:06:34,448 Epoch 271: Total Training Recognition Loss 0.31  Total Training Translation Loss 3.85 
2024-02-03 08:06:34,448 EPOCH 272
2024-02-03 08:06:39,381 Epoch 272: Total Training Recognition Loss 0.20  Total Training Translation Loss 3.91 
2024-02-03 08:06:39,382 EPOCH 273
2024-02-03 08:06:44,362 Epoch 273: Total Training Recognition Loss 0.19  Total Training Translation Loss 4.29 
2024-02-03 08:06:44,363 EPOCH 274
2024-02-03 08:06:46,697 [Epoch: 274 Step: 00009300] Batch Recognition Loss:   0.003624 => Gls Tokens per Sec:     2361 || Batch Translation Loss:   0.053773 => Txt Tokens per Sec:     6731 || Lr: 0.000100
2024-02-03 08:06:48,911 Epoch 274: Total Training Recognition Loss 0.23  Total Training Translation Loss 4.91 
2024-02-03 08:06:48,911 EPOCH 275
2024-02-03 08:06:53,684 Epoch 275: Total Training Recognition Loss 0.19  Total Training Translation Loss 4.17 
2024-02-03 08:06:53,684 EPOCH 276
2024-02-03 08:06:58,333 Epoch 276: Total Training Recognition Loss 0.18  Total Training Translation Loss 3.85 
2024-02-03 08:06:58,334 EPOCH 277
2024-02-03 08:07:00,353 [Epoch: 277 Step: 00009400] Batch Recognition Loss:   0.002865 => Gls Tokens per Sec:     2539 || Batch Translation Loss:   0.074919 => Txt Tokens per Sec:     7282 || Lr: 0.000100
2024-02-03 08:07:02,827 Epoch 277: Total Training Recognition Loss 0.20  Total Training Translation Loss 3.85 
2024-02-03 08:07:02,827 EPOCH 278
2024-02-03 08:07:07,444 Epoch 278: Total Training Recognition Loss 0.20  Total Training Translation Loss 3.36 
2024-02-03 08:07:07,445 EPOCH 279
2024-02-03 08:07:11,952 Epoch 279: Total Training Recognition Loss 0.17  Total Training Translation Loss 3.57 
2024-02-03 08:07:11,952 EPOCH 280
2024-02-03 08:07:13,908 [Epoch: 280 Step: 00009500] Batch Recognition Loss:   0.002560 => Gls Tokens per Sec:     2164 || Batch Translation Loss:   0.046085 => Txt Tokens per Sec:     6336 || Lr: 0.000100
2024-02-03 08:07:16,661 Epoch 280: Total Training Recognition Loss 0.21  Total Training Translation Loss 3.58 
2024-02-03 08:07:16,661 EPOCH 281
2024-02-03 08:07:21,400 Epoch 281: Total Training Recognition Loss 0.23  Total Training Translation Loss 3.87 
2024-02-03 08:07:21,401 EPOCH 282
2024-02-03 08:07:26,147 Epoch 282: Total Training Recognition Loss 0.20  Total Training Translation Loss 3.36 
2024-02-03 08:07:26,147 EPOCH 283
2024-02-03 08:07:27,507 [Epoch: 283 Step: 00009600] Batch Recognition Loss:   0.002481 => Gls Tokens per Sec:     2642 || Batch Translation Loss:   0.096865 => Txt Tokens per Sec:     7181 || Lr: 0.000100
2024-02-03 08:07:30,524 Epoch 283: Total Training Recognition Loss 0.19  Total Training Translation Loss 3.36 
2024-02-03 08:07:30,525 EPOCH 284
2024-02-03 08:07:35,319 Epoch 284: Total Training Recognition Loss 0.17  Total Training Translation Loss 2.72 
2024-02-03 08:07:35,319 EPOCH 285
2024-02-03 08:07:39,963 Epoch 285: Total Training Recognition Loss 0.21  Total Training Translation Loss 3.65 
2024-02-03 08:07:39,964 EPOCH 286
2024-02-03 08:07:41,610 [Epoch: 286 Step: 00009700] Batch Recognition Loss:   0.001381 => Gls Tokens per Sec:     1794 || Batch Translation Loss:   0.071295 => Txt Tokens per Sec:     5043 || Lr: 0.000100
2024-02-03 08:07:44,445 Epoch 286: Total Training Recognition Loss 0.21  Total Training Translation Loss 3.78 
2024-02-03 08:07:44,446 EPOCH 287
2024-02-03 08:07:49,103 Epoch 287: Total Training Recognition Loss 0.30  Total Training Translation Loss 4.22 
2024-02-03 08:07:49,103 EPOCH 288
2024-02-03 08:07:53,685 Epoch 288: Total Training Recognition Loss 0.22  Total Training Translation Loss 5.05 
2024-02-03 08:07:53,686 EPOCH 289
2024-02-03 08:07:55,099 [Epoch: 289 Step: 00009800] Batch Recognition Loss:   0.008891 => Gls Tokens per Sec:     1811 || Batch Translation Loss:   0.142297 => Txt Tokens per Sec:     5251 || Lr: 0.000100
2024-02-03 08:07:58,569 Epoch 289: Total Training Recognition Loss 0.21  Total Training Translation Loss 5.19 
2024-02-03 08:07:58,569 EPOCH 290
2024-02-03 08:08:02,853 Epoch 290: Total Training Recognition Loss 0.35  Total Training Translation Loss 5.41 
2024-02-03 08:08:02,853 EPOCH 291
2024-02-03 08:08:07,716 Epoch 291: Total Training Recognition Loss 0.79  Total Training Translation Loss 7.09 
2024-02-03 08:08:07,717 EPOCH 292
2024-02-03 08:08:08,559 [Epoch: 292 Step: 00009900] Batch Recognition Loss:   0.005420 => Gls Tokens per Sec:     1986 || Batch Translation Loss:   0.168705 => Txt Tokens per Sec:     6316 || Lr: 0.000100
2024-02-03 08:08:12,175 Epoch 292: Total Training Recognition Loss 0.85  Total Training Translation Loss 18.41 
2024-02-03 08:08:12,175 EPOCH 293
2024-02-03 08:08:17,392 Epoch 293: Total Training Recognition Loss 0.74  Total Training Translation Loss 37.28 
2024-02-03 08:08:17,393 EPOCH 294
2024-02-03 08:08:21,930 Epoch 294: Total Training Recognition Loss 0.57  Total Training Translation Loss 16.00 
2024-02-03 08:08:21,931 EPOCH 295
2024-02-03 08:08:22,366 [Epoch: 295 Step: 00010000] Batch Recognition Loss:   0.005857 => Gls Tokens per Sec:     2943 || Batch Translation Loss:   0.215299 => Txt Tokens per Sec:     7947 || Lr: 0.000100
2024-02-03 08:08:30,995 Hooray! New best validation result [eval_metric]!
2024-02-03 08:08:30,996 Saving new checkpoint.
2024-02-03 08:08:31,270 Validation result at epoch 295, step    10000: duration: 8.9047s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 3.26711	Translation Loss: 86584.67188	PPL: 8046.61426
	Eval Metric: BLEU
	WER 5.66	(DEL: 0.07,	INS: 0.00,	SUB: 5.59)
	BLEU-4 0.91	(BLEU-1: 12.11,	BLEU-2: 3.82,	BLEU-3: 1.67,	BLEU-4: 0.91)
	CHRF 17.62	ROUGE 9.94
2024-02-03 08:08:31,271 Logging Recognition and Translation Outputs
2024-02-03 08:08:31,272 ========================================================================================================================
2024-02-03 08:08:31,272 Logging Sequence: 78_43.00
2024-02-03 08:08:31,272 	Gloss Reference :	A B+C+D+E
2024-02-03 08:08:31,272 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 08:08:31,272 	Gloss Alignment :	         
2024-02-03 08:08:31,272 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 08:08:31,273 	Text Reference  :	* *** *** ** however something   happened which ** ******* ** **** * ******** made the team very    happy
2024-02-03 08:08:31,273 	Text Hypothesis :	i was not at all     interesting ipl      which is talking so what a director in   the **** supreme time 
2024-02-03 08:08:31,274 	Text Alignment  :	I I   I   I  S       S           S              I  I       I  I    I I        S        D    S       S    
2024-02-03 08:08:31,274 ========================================================================================================================
2024-02-03 08:08:31,274 Logging Sequence: 98_97.00
2024-02-03 08:08:31,274 	Gloss Reference :	A B+C+D+E
2024-02-03 08:08:31,274 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 08:08:31,274 	Gloss Alignment :	         
2024-02-03 08:08:31,274 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 08:08:31,276 	Text Reference  :	the teams   in the    series were india legends england legends sri lanka legends   
2024-02-03 08:08:31,276 	Text Hypothesis :	*** however a  single girl   is   when  they    were    added   to  the   tournament
2024-02-03 08:08:31,276 	Text Alignment  :	D   S       S  S      S      S    S     S       S       S       S   S     S         
2024-02-03 08:08:31,276 ========================================================================================================================
2024-02-03 08:08:31,276 Logging Sequence: 143_11.00
2024-02-03 08:08:31,276 	Gloss Reference :	A B+C+D+E    
2024-02-03 08:08:31,276 	Gloss Hypothesis:	A B+C+B+C+D+E
2024-02-03 08:08:31,277 	Gloss Alignment :	  S          
2024-02-03 08:08:31,277 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 08:08:31,279 	Text Reference  :	ronaldo has    also   become the  first person to  have    500    million followers on  instagram he is      the  most loved footballer
2024-02-03 08:08:31,279 	Text Hypothesis :	the     camera stayed on     them for   about  ten seconds giving a       clear     cut view      of vamika' face for  the   fine      
2024-02-03 08:08:31,279 	Text Alignment  :	S       S      S      S      S    S     S      S   S       S      S       S         S   S         S  S       S    S    S     S         
2024-02-03 08:08:31,279 ========================================================================================================================
2024-02-03 08:08:31,279 Logging Sequence: 179_386.00
2024-02-03 08:08:31,280 	Gloss Reference :	A B+C+D+E
2024-02-03 08:08:31,280 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 08:08:31,280 	Gloss Alignment :	         
2024-02-03 08:08:31,280 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 08:08:31,281 	Text Reference  :	and     the ***** federation or sai    people are  their servants to   pick up   their passport
2024-02-03 08:08:31,281 	Text Hypothesis :	however the third charge     on vinesh is     that she   must     have a    kind of    stars   
2024-02-03 08:08:31,282 	Text Alignment  :	S           I     S          S  S      S      S    S     S        S    S    S    S     S       
2024-02-03 08:08:31,282 ========================================================================================================================
2024-02-03 08:08:31,282 Logging Sequence: 77_60.00
2024-02-03 08:08:31,282 	Gloss Reference :	A B+C+D+E
2024-02-03 08:08:31,282 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 08:08:31,282 	Gloss Alignment :	         
2024-02-03 08:08:31,283 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 08:08:31,283 	Text Reference  :	*** ****** he   remained not out     
2024-02-03 08:08:31,283 	Text Hypothesis :	the couple were crazy    for pictures
2024-02-03 08:08:31,283 	Text Alignment  :	I   I      S    S        S   S       
2024-02-03 08:08:31,283 ========================================================================================================================
2024-02-03 08:08:35,836 Epoch 295: Total Training Recognition Loss 0.46  Total Training Translation Loss 6.66 
2024-02-03 08:08:35,836 EPOCH 296
2024-02-03 08:08:40,418 Epoch 296: Total Training Recognition Loss 0.38  Total Training Translation Loss 5.39 
2024-02-03 08:08:40,418 EPOCH 297
2024-02-03 08:08:44,842 Epoch 297: Total Training Recognition Loss 0.26  Total Training Translation Loss 4.75 
2024-02-03 08:08:44,842 EPOCH 298
2024-02-03 08:08:45,085 [Epoch: 298 Step: 00010100] Batch Recognition Loss:   0.005634 => Gls Tokens per Sec:     1612 || Batch Translation Loss:   0.023705 => Txt Tokens per Sec:     4116 || Lr: 0.000100
2024-02-03 08:08:49,590 Epoch 298: Total Training Recognition Loss 0.27  Total Training Translation Loss 3.16 
2024-02-03 08:08:49,590 EPOCH 299
2024-02-03 08:08:54,011 Epoch 299: Total Training Recognition Loss 0.23  Total Training Translation Loss 2.76 
2024-02-03 08:08:54,012 EPOCH 300
2024-02-03 08:08:58,793 [Epoch: 300 Step: 00010200] Batch Recognition Loss:   0.001872 => Gls Tokens per Sec:     2224 || Batch Translation Loss:   0.135363 => Txt Tokens per Sec:     6186 || Lr: 0.000100
2024-02-03 08:08:58,793 Epoch 300: Total Training Recognition Loss 0.19  Total Training Translation Loss 2.58 
2024-02-03 08:08:58,793 EPOCH 301
2024-02-03 08:09:03,313 Epoch 301: Total Training Recognition Loss 0.21  Total Training Translation Loss 2.15 
2024-02-03 08:09:03,314 EPOCH 302
2024-02-03 08:09:07,990 Epoch 302: Total Training Recognition Loss 0.19  Total Training Translation Loss 2.36 
2024-02-03 08:09:07,991 EPOCH 303
2024-02-03 08:09:12,069 [Epoch: 303 Step: 00010300] Batch Recognition Loss:   0.004644 => Gls Tokens per Sec:     2450 || Batch Translation Loss:   0.087836 => Txt Tokens per Sec:     6799 || Lr: 0.000100
2024-02-03 08:09:12,351 Epoch 303: Total Training Recognition Loss 0.17  Total Training Translation Loss 2.42 
2024-02-03 08:09:12,352 EPOCH 304
2024-02-03 08:09:17,334 Epoch 304: Total Training Recognition Loss 0.21  Total Training Translation Loss 2.27 
2024-02-03 08:09:17,335 EPOCH 305
2024-02-03 08:09:22,196 Epoch 305: Total Training Recognition Loss 0.17  Total Training Translation Loss 2.90 
2024-02-03 08:09:22,197 EPOCH 306
2024-02-03 08:09:25,857 [Epoch: 306 Step: 00010400] Batch Recognition Loss:   0.005798 => Gls Tokens per Sec:     2556 || Batch Translation Loss:   0.072250 => Txt Tokens per Sec:     6987 || Lr: 0.000100
2024-02-03 08:09:26,405 Epoch 306: Total Training Recognition Loss 0.21  Total Training Translation Loss 2.86 
2024-02-03 08:09:26,406 EPOCH 307
2024-02-03 08:09:31,244 Epoch 307: Total Training Recognition Loss 0.16  Total Training Translation Loss 3.17 
2024-02-03 08:09:31,245 EPOCH 308
2024-02-03 08:09:35,546 Epoch 308: Total Training Recognition Loss 0.14  Total Training Translation Loss 2.27 
2024-02-03 08:09:35,546 EPOCH 309
2024-02-03 08:09:39,732 [Epoch: 309 Step: 00010500] Batch Recognition Loss:   0.002318 => Gls Tokens per Sec:     2081 || Batch Translation Loss:   0.083691 => Txt Tokens per Sec:     5988 || Lr: 0.000100
2024-02-03 08:09:40,385 Epoch 309: Total Training Recognition Loss 0.16  Total Training Translation Loss 2.52 
2024-02-03 08:09:40,385 EPOCH 310
2024-02-03 08:09:44,679 Epoch 310: Total Training Recognition Loss 0.16  Total Training Translation Loss 2.63 
2024-02-03 08:09:44,679 EPOCH 311
2024-02-03 08:09:49,517 Epoch 311: Total Training Recognition Loss 0.15  Total Training Translation Loss 2.98 
2024-02-03 08:09:49,518 EPOCH 312
2024-02-03 08:09:52,951 [Epoch: 312 Step: 00010600] Batch Recognition Loss:   0.004269 => Gls Tokens per Sec:     2351 || Batch Translation Loss:   0.110966 => Txt Tokens per Sec:     6648 || Lr: 0.000100
2024-02-03 08:09:53,822 Epoch 312: Total Training Recognition Loss 0.17  Total Training Translation Loss 3.67 
2024-02-03 08:09:53,822 EPOCH 313
2024-02-03 08:09:58,604 Epoch 313: Total Training Recognition Loss 0.19  Total Training Translation Loss 3.48 
2024-02-03 08:09:58,605 EPOCH 314
2024-02-03 08:10:03,289 Epoch 314: Total Training Recognition Loss 0.18  Total Training Translation Loss 3.66 
2024-02-03 08:10:03,289 EPOCH 315
2024-02-03 08:10:06,750 [Epoch: 315 Step: 00010700] Batch Recognition Loss:   0.004735 => Gls Tokens per Sec:     2147 || Batch Translation Loss:   0.060575 => Txt Tokens per Sec:     5952 || Lr: 0.000100
2024-02-03 08:10:08,149 Epoch 315: Total Training Recognition Loss 0.16  Total Training Translation Loss 3.82 
2024-02-03 08:10:08,150 EPOCH 316
2024-02-03 08:10:12,399 Epoch 316: Total Training Recognition Loss 0.16  Total Training Translation Loss 3.04 
2024-02-03 08:10:12,400 EPOCH 317
2024-02-03 08:10:17,312 Epoch 317: Total Training Recognition Loss 0.14  Total Training Translation Loss 2.89 
2024-02-03 08:10:17,313 EPOCH 318
2024-02-03 08:10:19,989 [Epoch: 318 Step: 00010800] Batch Recognition Loss:   0.000701 => Gls Tokens per Sec:     2537 || Batch Translation Loss:   0.044338 => Txt Tokens per Sec:     6872 || Lr: 0.000100
2024-02-03 08:10:21,831 Epoch 318: Total Training Recognition Loss 0.17  Total Training Translation Loss 2.52 
2024-02-03 08:10:21,832 EPOCH 319
2024-02-03 08:10:26,511 Epoch 319: Total Training Recognition Loss 0.11  Total Training Translation Loss 2.39 
2024-02-03 08:10:26,511 EPOCH 320
2024-02-03 08:10:30,983 Epoch 320: Total Training Recognition Loss 0.14  Total Training Translation Loss 2.19 
2024-02-03 08:10:30,984 EPOCH 321
2024-02-03 08:10:33,891 [Epoch: 321 Step: 00010900] Batch Recognition Loss:   0.002433 => Gls Tokens per Sec:     2116 || Batch Translation Loss:   0.048690 => Txt Tokens per Sec:     6011 || Lr: 0.000100
2024-02-03 08:10:35,766 Epoch 321: Total Training Recognition Loss 0.13  Total Training Translation Loss 2.51 
2024-02-03 08:10:35,766 EPOCH 322
2024-02-03 08:10:40,503 Epoch 322: Total Training Recognition Loss 0.12  Total Training Translation Loss 3.45 
2024-02-03 08:10:40,504 EPOCH 323
2024-02-03 08:10:44,958 Epoch 323: Total Training Recognition Loss 0.14  Total Training Translation Loss 6.75 
2024-02-03 08:10:44,958 EPOCH 324
2024-02-03 08:10:47,400 [Epoch: 324 Step: 00011000] Batch Recognition Loss:   0.000644 => Gls Tokens per Sec:     2257 || Batch Translation Loss:   0.136816 => Txt Tokens per Sec:     6651 || Lr: 0.000100
2024-02-03 08:10:49,005 Epoch 324: Total Training Recognition Loss 0.18  Total Training Translation Loss 6.73 
2024-02-03 08:10:49,005 EPOCH 325
2024-02-03 08:10:53,709 Epoch 325: Total Training Recognition Loss 0.23  Total Training Translation Loss 5.90 
2024-02-03 08:10:53,709 EPOCH 326
2024-02-03 08:10:58,549 Epoch 326: Total Training Recognition Loss 0.21  Total Training Translation Loss 5.78 
2024-02-03 08:10:58,550 EPOCH 327
2024-02-03 08:11:00,764 [Epoch: 327 Step: 00011100] Batch Recognition Loss:   0.001696 => Gls Tokens per Sec:     2313 || Batch Translation Loss:   0.095302 => Txt Tokens per Sec:     6608 || Lr: 0.000100
2024-02-03 08:11:03,082 Epoch 327: Total Training Recognition Loss 0.18  Total Training Translation Loss 6.51 
2024-02-03 08:11:03,082 EPOCH 328
2024-02-03 08:11:07,887 Epoch 328: Total Training Recognition Loss 0.17  Total Training Translation Loss 5.51 
2024-02-03 08:11:07,888 EPOCH 329
2024-02-03 08:11:12,279 Epoch 329: Total Training Recognition Loss 0.18  Total Training Translation Loss 4.32 
2024-02-03 08:11:12,280 EPOCH 330
2024-02-03 08:11:14,179 [Epoch: 330 Step: 00011200] Batch Recognition Loss:   0.010436 => Gls Tokens per Sec:     2361 || Batch Translation Loss:   0.171072 => Txt Tokens per Sec:     6802 || Lr: 0.000100
2024-02-03 08:11:16,797 Epoch 330: Total Training Recognition Loss 0.19  Total Training Translation Loss 3.23 
2024-02-03 08:11:16,798 EPOCH 331
2024-02-03 08:11:21,409 Epoch 331: Total Training Recognition Loss 0.18  Total Training Translation Loss 3.21 
2024-02-03 08:11:21,410 EPOCH 332
2024-02-03 08:11:26,199 Epoch 332: Total Training Recognition Loss 0.16  Total Training Translation Loss 2.83 
2024-02-03 08:11:26,200 EPOCH 333
2024-02-03 08:11:27,493 [Epoch: 333 Step: 00011300] Batch Recognition Loss:   0.002467 => Gls Tokens per Sec:     2973 || Batch Translation Loss:   0.049461 => Txt Tokens per Sec:     7659 || Lr: 0.000100
2024-02-03 08:11:30,544 Epoch 333: Total Training Recognition Loss 0.15  Total Training Translation Loss 3.55 
2024-02-03 08:11:30,544 EPOCH 334
2024-02-03 08:11:35,325 Epoch 334: Total Training Recognition Loss 0.14  Total Training Translation Loss 3.43 
2024-02-03 08:11:35,326 EPOCH 335
2024-02-03 08:11:39,969 Epoch 335: Total Training Recognition Loss 0.16  Total Training Translation Loss 3.22 
2024-02-03 08:11:39,970 EPOCH 336
2024-02-03 08:11:41,146 [Epoch: 336 Step: 00011400] Batch Recognition Loss:   0.002546 => Gls Tokens per Sec:     2508 || Batch Translation Loss:   0.059795 => Txt Tokens per Sec:     6421 || Lr: 0.000100
2024-02-03 08:11:44,661 Epoch 336: Total Training Recognition Loss 0.12  Total Training Translation Loss 2.89 
2024-02-03 08:11:44,661 EPOCH 337
2024-02-03 08:11:49,140 Epoch 337: Total Training Recognition Loss 0.14  Total Training Translation Loss 3.03 
2024-02-03 08:11:49,140 EPOCH 338
2024-02-03 08:11:53,873 Epoch 338: Total Training Recognition Loss 0.15  Total Training Translation Loss 3.67 
2024-02-03 08:11:53,874 EPOCH 339
2024-02-03 08:11:54,773 [Epoch: 339 Step: 00011500] Batch Recognition Loss:   0.004385 => Gls Tokens per Sec:     2848 || Batch Translation Loss:   0.164529 => Txt Tokens per Sec:     8125 || Lr: 0.000100
2024-02-03 08:11:58,560 Epoch 339: Total Training Recognition Loss 0.13  Total Training Translation Loss 4.02 
2024-02-03 08:11:58,561 EPOCH 340
2024-02-03 08:12:03,016 Epoch 340: Total Training Recognition Loss 0.13  Total Training Translation Loss 5.24 
2024-02-03 08:12:03,016 EPOCH 341
2024-02-03 08:12:07,673 Epoch 341: Total Training Recognition Loss 0.16  Total Training Translation Loss 7.04 
2024-02-03 08:12:07,674 EPOCH 342
2024-02-03 08:12:08,331 [Epoch: 342 Step: 00011600] Batch Recognition Loss:   0.001441 => Gls Tokens per Sec:     2927 || Batch Translation Loss:   0.119377 => Txt Tokens per Sec:     8066 || Lr: 0.000100
2024-02-03 08:12:12,186 Epoch 342: Total Training Recognition Loss 0.22  Total Training Translation Loss 7.28 
2024-02-03 08:12:12,186 EPOCH 343
2024-02-03 08:12:17,125 Epoch 343: Total Training Recognition Loss 0.22  Total Training Translation Loss 9.95 
2024-02-03 08:12:17,125 EPOCH 344
2024-02-03 08:12:21,390 Epoch 344: Total Training Recognition Loss 0.32  Total Training Translation Loss 9.39 
2024-02-03 08:12:21,391 EPOCH 345
2024-02-03 08:12:21,822 [Epoch: 345 Step: 00011700] Batch Recognition Loss:   0.003475 => Gls Tokens per Sec:     2977 || Batch Translation Loss:   0.136413 => Txt Tokens per Sec:     8463 || Lr: 0.000100
2024-02-03 08:12:26,242 Epoch 345: Total Training Recognition Loss 0.46  Total Training Translation Loss 6.05 
2024-02-03 08:12:26,243 EPOCH 346
2024-02-03 08:12:30,503 Epoch 346: Total Training Recognition Loss 0.41  Total Training Translation Loss 4.22 
2024-02-03 08:12:30,503 EPOCH 347
2024-02-03 08:12:35,472 Epoch 347: Total Training Recognition Loss 0.42  Total Training Translation Loss 3.43 
2024-02-03 08:12:35,472 EPOCH 348
2024-02-03 08:12:35,647 [Epoch: 348 Step: 00011800] Batch Recognition Loss:   0.002741 => Gls Tokens per Sec:     3678 || Batch Translation Loss:   0.141580 => Txt Tokens per Sec:     9420 || Lr: 0.000100
2024-02-03 08:12:40,198 Epoch 348: Total Training Recognition Loss 0.28  Total Training Translation Loss 3.05 
2024-02-03 08:12:40,198 EPOCH 349
2024-02-03 08:12:44,655 Epoch 349: Total Training Recognition Loss 0.36  Total Training Translation Loss 2.73 
2024-02-03 08:12:44,656 EPOCH 350
2024-02-03 08:12:49,334 [Epoch: 350 Step: 00011900] Batch Recognition Loss:   0.003332 => Gls Tokens per Sec:     2273 || Batch Translation Loss:   0.036527 => Txt Tokens per Sec:     6321 || Lr: 0.000100
2024-02-03 08:12:49,335 Epoch 350: Total Training Recognition Loss 0.17  Total Training Translation Loss 1.98 
2024-02-03 08:12:49,335 EPOCH 351
2024-02-03 08:12:54,315 Epoch 351: Total Training Recognition Loss 0.17  Total Training Translation Loss 1.93 
2024-02-03 08:12:54,316 EPOCH 352
2024-02-03 08:12:58,860 Epoch 352: Total Training Recognition Loss 0.12  Total Training Translation Loss 2.32 
2024-02-03 08:12:58,860 EPOCH 353
2024-02-03 08:13:03,151 [Epoch: 353 Step: 00012000] Batch Recognition Loss:   0.001108 => Gls Tokens per Sec:     2329 || Batch Translation Loss:   0.116543 => Txt Tokens per Sec:     6434 || Lr: 0.000100
2024-02-03 08:13:11,728 Validation result at epoch 353, step    12000: duration: 8.5760s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 3.06708	Translation Loss: 89393.77344	PPL: 10772.70605
	Eval Metric: BLEU
	WER 4.60	(DEL: 0.07,	INS: 0.00,	SUB: 4.53)
	BLEU-4 0.45	(BLEU-1: 10.43,	BLEU-2: 3.11,	BLEU-3: 1.13,	BLEU-4: 0.45)
	CHRF 17.10	ROUGE 8.92
2024-02-03 08:13:11,729 Logging Recognition and Translation Outputs
2024-02-03 08:13:11,729 ========================================================================================================================
2024-02-03 08:13:11,730 Logging Sequence: 180_138.00
2024-02-03 08:13:11,730 	Gloss Reference :	A B+C+D+E
2024-02-03 08:13:11,730 	Gloss Hypothesis:	A B+C+E  
2024-02-03 08:13:11,730 	Gloss Alignment :	  S      
2024-02-03 08:13:11,730 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 08:13:11,732 	Text Reference  :	ioa president p t usha constituted a seven-member panel which included world   champions from various sports to      inquire into the    allegations
2024-02-03 08:13:11,732 	Text Hypothesis :	*** ********* * * **** *********** * ************ ***** ***** brij     bhushan said      'i   did     not    assault even    a    single girl       
2024-02-03 08:13:11,732 	Text Alignment  :	D   D         D D D    D           D D            D     D     S        S       S         S    S       S      S       S       S    S      S          
2024-02-03 08:13:11,732 ========================================================================================================================
2024-02-03 08:13:11,732 Logging Sequence: 126_99.00
2024-02-03 08:13:11,732 	Gloss Reference :	A B+C+D+E
2024-02-03 08:13:11,732 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 08:13:11,733 	Gloss Alignment :	         
2024-02-03 08:13:11,733 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 08:13:11,733 	Text Reference  :	he dedicated the medal    to ** **** ** *** sprinter milkha singh   
2024-02-03 08:13:11,734 	Text Hypothesis :	** and       is  expected to be held at the 2012     london olympics
2024-02-03 08:13:11,734 	Text Alignment  :	D  S         S   S           I  I    I  I   S        S      S       
2024-02-03 08:13:11,734 ========================================================================================================================
2024-02-03 08:13:11,734 Logging Sequence: 90_146.00
2024-02-03 08:13:11,734 	Gloss Reference :	A B+C+D+E
2024-02-03 08:13:11,734 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 08:13:11,734 	Gloss Alignment :	         
2024-02-03 08:13:11,734 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 08:13:11,735 	Text Reference  :	**** ** *** similarly natasa and  hardik both decided to   renew their vows
2024-02-03 08:13:11,736 	Text Hypothesis :	what do you all       know   that there  are  a       huge fan   from  loss
2024-02-03 08:13:11,736 	Text Alignment  :	I    I  I   S         S      S    S      S    S       S    S     S     S   
2024-02-03 08:13:11,736 ========================================================================================================================
2024-02-03 08:13:11,736 Logging Sequence: 79_198.00
2024-02-03 08:13:11,736 	Gloss Reference :	A B+C+D+E
2024-02-03 08:13:11,736 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 08:13:11,736 	Gloss Alignment :	         
2024-02-03 08:13:11,737 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 08:13:11,738 	Text Reference  :	*** ***** ** * *********** ****** ** **** will ** try        to  reschedule the    match before the  finals now       hope the team tests negative
2024-02-03 08:13:11,739 	Text Hypothesis :	all there is a significant impact as they will be spectators and will       decide if    you    will be     available for  the **** ***** finals  
2024-02-03 08:13:11,739 	Text Alignment  :	I   I     I  I I           I      I  I         I  S          S   S          S      S     S      S    S      S         S        D    D     S       
2024-02-03 08:13:11,739 ========================================================================================================================
2024-02-03 08:13:11,739 Logging Sequence: 138_182.00
2024-02-03 08:13:11,739 	Gloss Reference :	A B+C+D+E
2024-02-03 08:13:11,739 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 08:13:11,739 	Gloss Alignment :	         
2024-02-03 08:13:11,740 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 08:13:11,741 	Text Reference  :	there is  a      mural     of ***** *** **** ******* ** ** marcus rashford's face    in    machester
2024-02-03 08:13:11,741 	Text Hypothesis :	***** the sports authority of india sai then tweeted as it rains  the        penalty shoot out      
2024-02-03 08:13:11,741 	Text Alignment  :	D     S   S      S            I     I   I    I       I  I  S      S          S       S     S        
2024-02-03 08:13:11,741 ========================================================================================================================
2024-02-03 08:13:12,153 Epoch 353: Total Training Recognition Loss 0.14  Total Training Translation Loss 2.46 
2024-02-03 08:13:12,153 EPOCH 354
2024-02-03 08:13:17,056 Epoch 354: Total Training Recognition Loss 0.10  Total Training Translation Loss 2.71 
2024-02-03 08:13:17,057 EPOCH 355
2024-02-03 08:13:21,784 Epoch 355: Total Training Recognition Loss 0.12  Total Training Translation Loss 2.32 
2024-02-03 08:13:21,784 EPOCH 356
2024-02-03 08:13:25,595 [Epoch: 356 Step: 00012100] Batch Recognition Loss:   0.007116 => Gls Tokens per Sec:     2454 || Batch Translation Loss:   0.095939 => Txt Tokens per Sec:     6790 || Lr: 0.000100
2024-02-03 08:13:26,304 Epoch 356: Total Training Recognition Loss 0.13  Total Training Translation Loss 1.91 
2024-02-03 08:13:26,305 EPOCH 357
2024-02-03 08:13:31,047 Epoch 357: Total Training Recognition Loss 0.13  Total Training Translation Loss 1.68 
2024-02-03 08:13:31,048 EPOCH 358
2024-02-03 08:13:35,788 Epoch 358: Total Training Recognition Loss 0.14  Total Training Translation Loss 1.45 
2024-02-03 08:13:35,789 EPOCH 359
2024-02-03 08:13:39,494 [Epoch: 359 Step: 00012200] Batch Recognition Loss:   0.001630 => Gls Tokens per Sec:     2351 || Batch Translation Loss:   0.049922 => Txt Tokens per Sec:     6595 || Lr: 0.000100
2024-02-03 08:13:40,214 Epoch 359: Total Training Recognition Loss 0.20  Total Training Translation Loss 1.55 
2024-02-03 08:13:40,214 EPOCH 360
2024-02-03 08:13:45,184 Epoch 360: Total Training Recognition Loss 0.23  Total Training Translation Loss 2.23 
2024-02-03 08:13:45,184 EPOCH 361
2024-02-03 08:13:49,440 Epoch 361: Total Training Recognition Loss 0.67  Total Training Translation Loss 1.77 
2024-02-03 08:13:49,440 EPOCH 362
2024-02-03 08:13:53,213 [Epoch: 362 Step: 00012300] Batch Recognition Loss:   0.001601 => Gls Tokens per Sec:     2206 || Batch Translation Loss:   0.032620 => Txt Tokens per Sec:     6071 || Lr: 0.000100
2024-02-03 08:13:54,306 Epoch 362: Total Training Recognition Loss 0.13  Total Training Translation Loss 2.01 
2024-02-03 08:13:54,306 EPOCH 363
2024-02-03 08:13:58,815 Epoch 363: Total Training Recognition Loss 0.15  Total Training Translation Loss 1.63 
2024-02-03 08:13:58,816 EPOCH 364
2024-02-03 08:14:03,678 Epoch 364: Total Training Recognition Loss 0.15  Total Training Translation Loss 2.10 
2024-02-03 08:14:03,678 EPOCH 365
2024-02-03 08:14:06,471 [Epoch: 365 Step: 00012400] Batch Recognition Loss:   0.004451 => Gls Tokens per Sec:     2661 || Batch Translation Loss:   0.071623 => Txt Tokens per Sec:     7168 || Lr: 0.000100
2024-02-03 08:14:07,888 Epoch 365: Total Training Recognition Loss 0.14  Total Training Translation Loss 2.56 
2024-02-03 08:14:07,888 EPOCH 366
2024-02-03 08:14:12,844 Epoch 366: Total Training Recognition Loss 0.11  Total Training Translation Loss 2.63 
2024-02-03 08:14:12,844 EPOCH 367
2024-02-03 08:14:16,927 Epoch 367: Total Training Recognition Loss 0.15  Total Training Translation Loss 5.91 
2024-02-03 08:14:16,928 EPOCH 368
2024-02-03 08:14:19,995 [Epoch: 368 Step: 00012500] Batch Recognition Loss:   0.008638 => Gls Tokens per Sec:     2215 || Batch Translation Loss:   0.906218 => Txt Tokens per Sec:     6010 || Lr: 0.000100
2024-02-03 08:14:21,899 Epoch 368: Total Training Recognition Loss 0.43  Total Training Translation Loss 19.27 
2024-02-03 08:14:21,899 EPOCH 369
2024-02-03 08:14:26,354 Epoch 369: Total Training Recognition Loss 0.75  Total Training Translation Loss 15.16 
2024-02-03 08:14:26,355 EPOCH 370
2024-02-03 08:14:31,090 Epoch 370: Total Training Recognition Loss 0.32  Total Training Translation Loss 12.11 
2024-02-03 08:14:31,090 EPOCH 371
2024-02-03 08:14:33,657 [Epoch: 371 Step: 00012600] Batch Recognition Loss:   0.002294 => Gls Tokens per Sec:     2396 || Batch Translation Loss:   0.611239 => Txt Tokens per Sec:     6948 || Lr: 0.000100
2024-02-03 08:14:35,487 Epoch 371: Total Training Recognition Loss 0.26  Total Training Translation Loss 9.70 
2024-02-03 08:14:35,488 EPOCH 372
2024-02-03 08:14:40,310 Epoch 372: Total Training Recognition Loss 1.00  Total Training Translation Loss 7.04 
2024-02-03 08:14:40,310 EPOCH 373
2024-02-03 08:14:44,623 Epoch 373: Total Training Recognition Loss 3.21  Total Training Translation Loss 5.13 
2024-02-03 08:14:44,623 EPOCH 374
2024-02-03 08:14:46,991 [Epoch: 374 Step: 00012700] Batch Recognition Loss:   0.008580 => Gls Tokens per Sec:     2328 || Batch Translation Loss:   0.079014 => Txt Tokens per Sec:     6269 || Lr: 0.000100
2024-02-03 08:14:49,436 Epoch 374: Total Training Recognition Loss 1.38  Total Training Translation Loss 5.33 
2024-02-03 08:14:49,437 EPOCH 375
2024-02-03 08:14:53,750 Epoch 375: Total Training Recognition Loss 0.52  Total Training Translation Loss 2.37 
2024-02-03 08:14:53,751 EPOCH 376
2024-02-03 08:14:58,643 Epoch 376: Total Training Recognition Loss 0.24  Total Training Translation Loss 1.88 
2024-02-03 08:14:58,643 EPOCH 377
2024-02-03 08:15:00,913 [Epoch: 377 Step: 00012800] Batch Recognition Loss:   0.009659 => Gls Tokens per Sec:     2146 || Batch Translation Loss:   0.050828 => Txt Tokens per Sec:     6322 || Lr: 0.000100
2024-02-03 08:15:02,721 Epoch 377: Total Training Recognition Loss 0.18  Total Training Translation Loss 1.91 
2024-02-03 08:15:02,721 EPOCH 378
2024-02-03 08:15:07,563 Epoch 378: Total Training Recognition Loss 0.15  Total Training Translation Loss 1.65 
2024-02-03 08:15:07,564 EPOCH 379
2024-02-03 08:15:12,333 Epoch 379: Total Training Recognition Loss 0.16  Total Training Translation Loss 1.57 
2024-02-03 08:15:12,334 EPOCH 380
2024-02-03 08:15:14,421 [Epoch: 380 Step: 00012900] Batch Recognition Loss:   0.001841 => Gls Tokens per Sec:     2028 || Batch Translation Loss:   0.030016 => Txt Tokens per Sec:     5815 || Lr: 0.000100
2024-02-03 08:15:17,119 Epoch 380: Total Training Recognition Loss 0.10  Total Training Translation Loss 1.50 
2024-02-03 08:15:17,119 EPOCH 381
2024-02-03 08:15:22,153 Epoch 381: Total Training Recognition Loss 0.10  Total Training Translation Loss 1.90 
2024-02-03 08:15:22,154 EPOCH 382
2024-02-03 08:15:26,901 Epoch 382: Total Training Recognition Loss 0.13  Total Training Translation Loss 1.61 
2024-02-03 08:15:26,901 EPOCH 383
2024-02-03 08:15:28,259 [Epoch: 383 Step: 00013000] Batch Recognition Loss:   0.001538 => Gls Tokens per Sec:     2833 || Batch Translation Loss:   0.039692 => Txt Tokens per Sec:     7062 || Lr: 0.000100
2024-02-03 08:15:31,759 Epoch 383: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.69 
2024-02-03 08:15:31,760 EPOCH 384
2024-02-03 08:15:35,916 Epoch 384: Total Training Recognition Loss 0.11  Total Training Translation Loss 1.52 
2024-02-03 08:15:35,917 EPOCH 385
2024-02-03 08:15:40,796 Epoch 385: Total Training Recognition Loss 0.10  Total Training Translation Loss 1.24 
2024-02-03 08:15:40,797 EPOCH 386
2024-02-03 08:15:42,518 [Epoch: 386 Step: 00013100] Batch Recognition Loss:   0.001435 => Gls Tokens per Sec:     1860 || Batch Translation Loss:   0.034084 => Txt Tokens per Sec:     5444 || Lr: 0.000100
2024-02-03 08:15:45,159 Epoch 386: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.44 
2024-02-03 08:15:45,160 EPOCH 387
2024-02-03 08:15:50,052 Epoch 387: Total Training Recognition Loss 0.09  Total Training Translation Loss 1.37 
2024-02-03 08:15:50,053 EPOCH 388
2024-02-03 08:15:54,287 Epoch 388: Total Training Recognition Loss 0.10  Total Training Translation Loss 1.15 
2024-02-03 08:15:54,287 EPOCH 389
2024-02-03 08:15:55,615 [Epoch: 389 Step: 00013200] Batch Recognition Loss:   0.006693 => Gls Tokens per Sec:     1930 || Batch Translation Loss:   0.034968 => Txt Tokens per Sec:     5825 || Lr: 0.000100
2024-02-03 08:15:58,974 Epoch 389: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.26 
2024-02-03 08:15:58,975 EPOCH 390
2024-02-03 08:16:03,532 Epoch 390: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.00 
2024-02-03 08:16:03,533 EPOCH 391
2024-02-03 08:16:08,452 Epoch 391: Total Training Recognition Loss 0.09  Total Training Translation Loss 1.20 
2024-02-03 08:16:08,452 EPOCH 392
2024-02-03 08:16:09,072 [Epoch: 392 Step: 00013300] Batch Recognition Loss:   0.002644 => Gls Tokens per Sec:     3100 || Batch Translation Loss:   0.063461 => Txt Tokens per Sec:     8120 || Lr: 0.000100
2024-02-03 08:16:12,566 Epoch 392: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.00 
2024-02-03 08:16:12,566 EPOCH 393
2024-02-03 08:16:17,474 Epoch 393: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.42 
2024-02-03 08:16:17,475 EPOCH 394
2024-02-03 08:16:22,027 Epoch 394: Total Training Recognition Loss 0.09  Total Training Translation Loss 1.89 
2024-02-03 08:16:22,027 EPOCH 395
2024-02-03 08:16:22,790 [Epoch: 395 Step: 00013400] Batch Recognition Loss:   0.001363 => Gls Tokens per Sec:     1679 || Batch Translation Loss:   0.036281 => Txt Tokens per Sec:     4544 || Lr: 0.000100
2024-02-03 08:16:26,854 Epoch 395: Total Training Recognition Loss 0.10  Total Training Translation Loss 1.82 
2024-02-03 08:16:26,854 EPOCH 396
2024-02-03 08:16:31,185 Epoch 396: Total Training Recognition Loss 0.10  Total Training Translation Loss 1.64 
2024-02-03 08:16:31,185 EPOCH 397
2024-02-03 08:16:36,049 Epoch 397: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.31 
2024-02-03 08:16:36,049 EPOCH 398
2024-02-03 08:16:36,545 [Epoch: 398 Step: 00013500] Batch Recognition Loss:   0.003507 => Gls Tokens per Sec:     1296 || Batch Translation Loss:   0.111202 => Txt Tokens per Sec:     4265 || Lr: 0.000100
2024-02-03 08:16:40,219 Epoch 398: Total Training Recognition Loss 0.08  Total Training Translation Loss 3.26 
2024-02-03 08:16:40,220 EPOCH 399
2024-02-03 08:16:45,087 Epoch 399: Total Training Recognition Loss 0.09  Total Training Translation Loss 4.01 
2024-02-03 08:16:45,088 EPOCH 400
2024-02-03 08:16:49,354 [Epoch: 400 Step: 00013600] Batch Recognition Loss:   0.017150 => Gls Tokens per Sec:     2493 || Batch Translation Loss:   0.078443 => Txt Tokens per Sec:     6932 || Lr: 0.000100
2024-02-03 08:16:49,355 Epoch 400: Total Training Recognition Loss 0.11  Total Training Translation Loss 6.80 
2024-02-03 08:16:49,355 EPOCH 401
2024-02-03 08:16:54,216 Epoch 401: Total Training Recognition Loss 0.20  Total Training Translation Loss 11.90 
2024-02-03 08:16:54,217 EPOCH 402
2024-02-03 08:16:58,748 Epoch 402: Total Training Recognition Loss 0.46  Total Training Translation Loss 11.03 
2024-02-03 08:16:58,749 EPOCH 403
2024-02-03 08:17:03,035 [Epoch: 403 Step: 00013700] Batch Recognition Loss:   0.001367 => Gls Tokens per Sec:     2390 || Batch Translation Loss:   0.200478 => Txt Tokens per Sec:     6583 || Lr: 0.000100
2024-02-03 08:17:03,360 Epoch 403: Total Training Recognition Loss 0.34  Total Training Translation Loss 8.35 
2024-02-03 08:17:03,361 EPOCH 404
2024-02-03 08:17:07,664 Epoch 404: Total Training Recognition Loss 0.32  Total Training Translation Loss 6.13 
2024-02-03 08:17:07,664 EPOCH 405
2024-02-03 08:17:12,519 Epoch 405: Total Training Recognition Loss 0.20  Total Training Translation Loss 5.08 
2024-02-03 08:17:12,519 EPOCH 406
2024-02-03 08:17:16,670 [Epoch: 406 Step: 00013800] Batch Recognition Loss:   0.000639 => Gls Tokens per Sec:     2254 || Batch Translation Loss:   0.079561 => Txt Tokens per Sec:     6300 || Lr: 0.000100
2024-02-03 08:17:17,132 Epoch 406: Total Training Recognition Loss 0.17  Total Training Translation Loss 3.81 
2024-02-03 08:17:17,133 EPOCH 407
2024-02-03 08:17:21,674 Epoch 407: Total Training Recognition Loss 0.17  Total Training Translation Loss 3.23 
2024-02-03 08:17:21,674 EPOCH 408
2024-02-03 08:17:26,306 Epoch 408: Total Training Recognition Loss 0.13  Total Training Translation Loss 2.47 
2024-02-03 08:17:26,307 EPOCH 409
2024-02-03 08:17:30,098 [Epoch: 409 Step: 00013900] Batch Recognition Loss:   0.008373 => Gls Tokens per Sec:     2298 || Batch Translation Loss:   0.025920 => Txt Tokens per Sec:     6323 || Lr: 0.000100
2024-02-03 08:17:30,909 Epoch 409: Total Training Recognition Loss 0.12  Total Training Translation Loss 3.14 
2024-02-03 08:17:30,909 EPOCH 410
2024-02-03 08:17:35,482 Epoch 410: Total Training Recognition Loss 0.09  Total Training Translation Loss 1.69 
2024-02-03 08:17:35,483 EPOCH 411
2024-02-03 08:17:40,078 Epoch 411: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.58 
2024-02-03 08:17:40,078 EPOCH 412
2024-02-03 08:17:43,705 [Epoch: 412 Step: 00014000] Batch Recognition Loss:   0.002904 => Gls Tokens per Sec:     2227 || Batch Translation Loss:   0.027308 => Txt Tokens per Sec:     6254 || Lr: 0.000100
2024-02-03 08:17:52,105 Validation result at epoch 412, step    14000: duration: 8.3998s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.76570	Translation Loss: 89593.94531	PPL: 10999.01562
	Eval Metric: BLEU
	WER 4.03	(DEL: 0.00,	INS: 0.00,	SUB: 4.03)
	BLEU-4 0.75	(BLEU-1: 11.66,	BLEU-2: 3.81,	BLEU-3: 1.52,	BLEU-4: 0.75)
	CHRF 17.31	ROUGE 9.79
2024-02-03 08:17:52,107 Logging Recognition and Translation Outputs
2024-02-03 08:17:52,107 ========================================================================================================================
2024-02-03 08:17:52,107 Logging Sequence: 123_76.00
2024-02-03 08:17:52,108 	Gloss Reference :	A B+C+D+E
2024-02-03 08:17:52,108 	Gloss Hypothesis:	A B      
2024-02-03 08:17:52,108 	Gloss Alignment :	  S      
2024-02-03 08:17:52,108 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 08:17:52,108 	Text Reference  :	** ** here there are    
2024-02-03 08:17:52,108 	Text Hypothesis :	he is from what  amazing
2024-02-03 08:17:52,109 	Text Alignment  :	I  I  S    S     S      
2024-02-03 08:17:52,109 ========================================================================================================================
2024-02-03 08:17:52,109 Logging Sequence: 87_224.00
2024-02-03 08:17:52,109 	Gloss Reference :	A B+C+D+E
2024-02-03 08:17:52,109 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 08:17:52,109 	Gloss Alignment :	         
2024-02-03 08:17:52,109 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 08:17:52,110 	Text Reference  :	so  i  lost my     cool   and it   was          my natural reaction
2024-02-03 08:17:52,111 	Text Hypothesis :	due to the  entire nation is  very disappointed to see     him     
2024-02-03 08:17:52,111 	Text Alignment  :	S   S  S    S      S      S   S    S            S  S       S       
2024-02-03 08:17:52,111 ========================================================================================================================
2024-02-03 08:17:52,111 Logging Sequence: 182_20.00
2024-02-03 08:17:52,111 	Gloss Reference :	A B+C+D+E
2024-02-03 08:17:52,111 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 08:17:52,111 	Gloss Alignment :	         
2024-02-03 08:17:52,112 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 08:17:52,113 	Text Reference  :	**** ** in     2019    june yuvraj shocked the world when he  announced his international retirement many people were sad with the news
2024-02-03 08:17:52,113 	Text Hypothesis :	soon he played against i    am     going   to  thank the  god of        his ************* retirement **** ****** **** *** **** *** ****
2024-02-03 08:17:52,114 	Text Alignment  :	I    I  S      S       S    S      S       S   S     S    S   S             D                        D    D      D    D   D    D   D   
2024-02-03 08:17:52,114 ========================================================================================================================
2024-02-03 08:17:52,114 Logging Sequence: 116_133.00
2024-02-03 08:17:52,114 	Gloss Reference :	A B+C+D+E
2024-02-03 08:17:52,114 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 08:17:52,114 	Gloss Alignment :	         
2024-02-03 08:17:52,115 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 08:17:52,115 	Text Reference  :	**** ** *** he expressed his sadness in the   caption
2024-02-03 08:17:52,115 	Text Hypothesis :	this is why i  am        not go      to their hotel  
2024-02-03 08:17:52,115 	Text Alignment  :	I    I  I   S  S         S   S       S  S     S      
2024-02-03 08:17:52,115 ========================================================================================================================
2024-02-03 08:17:52,116 Logging Sequence: 180_409.00
2024-02-03 08:17:52,116 	Gloss Reference :	A B+C+D+E
2024-02-03 08:17:52,116 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 08:17:52,116 	Gloss Alignment :	         
2024-02-03 08:17:52,116 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 08:17:52,118 	Text Reference  :	** cricketer kapil dev etc have also        shown support to    the  wrestlers through their  tweets
2024-02-03 08:17:52,118 	Text Hypothesis :	he said      there was a   huge controversy on    23rd    april 2023 night     at      jantar mantar
2024-02-03 08:17:52,118 	Text Alignment  :	I  S         S     S   S   S    S           S     S       S     S    S         S       S      S     
2024-02-03 08:17:52,118 ========================================================================================================================
2024-02-03 08:17:53,116 Epoch 412: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.42 
2024-02-03 08:17:53,117 EPOCH 413
2024-02-03 08:17:57,792 Epoch 413: Total Training Recognition Loss 0.09  Total Training Translation Loss 1.86 
2024-02-03 08:17:57,792 EPOCH 414
2024-02-03 08:18:02,515 Epoch 414: Total Training Recognition Loss 0.11  Total Training Translation Loss 2.48 
2024-02-03 08:18:02,515 EPOCH 415
2024-02-03 08:18:05,768 [Epoch: 415 Step: 00014100] Batch Recognition Loss:   0.009189 => Gls Tokens per Sec:     2285 || Batch Translation Loss:   0.199820 => Txt Tokens per Sec:     6307 || Lr: 0.000100
2024-02-03 08:18:07,011 Epoch 415: Total Training Recognition Loss 0.12  Total Training Translation Loss 5.02 
2024-02-03 08:18:07,011 EPOCH 416
2024-02-03 08:18:11,893 Epoch 416: Total Training Recognition Loss 0.15  Total Training Translation Loss 4.87 
2024-02-03 08:18:11,894 EPOCH 417
2024-02-03 08:18:16,143 Epoch 417: Total Training Recognition Loss 0.15  Total Training Translation Loss 4.45 
2024-02-03 08:18:16,143 EPOCH 418
2024-02-03 08:18:19,344 [Epoch: 418 Step: 00014200] Batch Recognition Loss:   0.011245 => Gls Tokens per Sec:     2200 || Batch Translation Loss:   0.064707 => Txt Tokens per Sec:     6196 || Lr: 0.000100
2024-02-03 08:18:20,855 Epoch 418: Total Training Recognition Loss 0.17  Total Training Translation Loss 8.94 
2024-02-03 08:18:20,855 EPOCH 419
2024-02-03 08:18:25,311 Epoch 419: Total Training Recognition Loss 0.23  Total Training Translation Loss 6.86 
2024-02-03 08:18:25,312 EPOCH 420
2024-02-03 08:18:30,310 Epoch 420: Total Training Recognition Loss 0.19  Total Training Translation Loss 7.70 
2024-02-03 08:18:30,311 EPOCH 421
2024-02-03 08:18:32,817 [Epoch: 421 Step: 00014300] Batch Recognition Loss:   0.005350 => Gls Tokens per Sec:     2456 || Batch Translation Loss:   0.134503 => Txt Tokens per Sec:     6902 || Lr: 0.000100
2024-02-03 08:18:34,823 Epoch 421: Total Training Recognition Loss 0.21  Total Training Translation Loss 3.24 
2024-02-03 08:18:34,824 EPOCH 422
2024-02-03 08:18:39,552 Epoch 422: Total Training Recognition Loss 0.14  Total Training Translation Loss 2.21 
2024-02-03 08:18:39,552 EPOCH 423
2024-02-03 08:18:43,757 Epoch 423: Total Training Recognition Loss 0.11  Total Training Translation Loss 1.82 
2024-02-03 08:18:43,757 EPOCH 424
2024-02-03 08:18:46,336 [Epoch: 424 Step: 00014400] Batch Recognition Loss:   0.003452 => Gls Tokens per Sec:     2235 || Batch Translation Loss:   0.068832 => Txt Tokens per Sec:     6275 || Lr: 0.000100
2024-02-03 08:18:48,637 Epoch 424: Total Training Recognition Loss 0.13  Total Training Translation Loss 1.42 
2024-02-03 08:18:48,637 EPOCH 425
2024-02-03 08:18:53,187 Epoch 425: Total Training Recognition Loss 0.09  Total Training Translation Loss 1.58 
2024-02-03 08:18:53,187 EPOCH 426
2024-02-03 08:18:57,771 Epoch 426: Total Training Recognition Loss 0.09  Total Training Translation Loss 1.60 
2024-02-03 08:18:57,771 EPOCH 427
2024-02-03 08:18:59,718 [Epoch: 427 Step: 00014500] Batch Recognition Loss:   0.002864 => Gls Tokens per Sec:     2631 || Batch Translation Loss:   0.285589 => Txt Tokens per Sec:     7575 || Lr: 0.000100
2024-02-03 08:19:02,501 Epoch 427: Total Training Recognition Loss 0.09  Total Training Translation Loss 1.81 
2024-02-03 08:19:02,502 EPOCH 428
2024-02-03 08:19:06,911 Epoch 428: Total Training Recognition Loss 0.09  Total Training Translation Loss 1.33 
2024-02-03 08:19:06,911 EPOCH 429
2024-02-03 08:19:11,548 Epoch 429: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.10 
2024-02-03 08:19:11,548 EPOCH 430
2024-02-03 08:19:13,373 [Epoch: 430 Step: 00014600] Batch Recognition Loss:   0.004254 => Gls Tokens per Sec:     2456 || Batch Translation Loss:   0.021131 => Txt Tokens per Sec:     6840 || Lr: 0.000100
2024-02-03 08:19:16,259 Epoch 430: Total Training Recognition Loss 0.09  Total Training Translation Loss 1.15 
2024-02-03 08:19:16,259 EPOCH 431
2024-02-03 08:19:21,138 Epoch 431: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.26 
2024-02-03 08:19:21,138 EPOCH 432
2024-02-03 08:19:25,517 Epoch 432: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.22 
2024-02-03 08:19:25,518 EPOCH 433
2024-02-03 08:19:27,486 [Epoch: 433 Step: 00014700] Batch Recognition Loss:   0.001200 => Gls Tokens per Sec:     1952 || Batch Translation Loss:   0.041505 => Txt Tokens per Sec:     5705 || Lr: 0.000100
2024-02-03 08:19:30,313 Epoch 433: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.81 
2024-02-03 08:19:30,313 EPOCH 434
2024-02-03 08:19:34,819 Epoch 434: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.72 
2024-02-03 08:19:34,819 EPOCH 435
2024-02-03 08:19:39,508 Epoch 435: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.60 
2024-02-03 08:19:39,509 EPOCH 436
2024-02-03 08:19:40,823 [Epoch: 436 Step: 00014800] Batch Recognition Loss:   0.000880 => Gls Tokens per Sec:     2435 || Batch Translation Loss:   0.135981 => Txt Tokens per Sec:     6878 || Lr: 0.000100
2024-02-03 08:19:44,142 Epoch 436: Total Training Recognition Loss 0.11  Total Training Translation Loss 3.44 
2024-02-03 08:19:44,142 EPOCH 437
2024-02-03 08:19:49,039 Epoch 437: Total Training Recognition Loss 0.11  Total Training Translation Loss 5.33 
2024-02-03 08:19:49,039 EPOCH 438
2024-02-03 08:19:53,662 Epoch 438: Total Training Recognition Loss 0.12  Total Training Translation Loss 4.96 
2024-02-03 08:19:53,662 EPOCH 439
2024-02-03 08:19:54,454 [Epoch: 439 Step: 00014900] Batch Recognition Loss:   0.002589 => Gls Tokens per Sec:     3236 || Batch Translation Loss:   0.294883 => Txt Tokens per Sec:     8606 || Lr: 0.000100
2024-02-03 08:19:58,435 Epoch 439: Total Training Recognition Loss 0.11  Total Training Translation Loss 5.42 
2024-02-03 08:19:58,436 EPOCH 440
2024-02-03 08:20:02,866 Epoch 440: Total Training Recognition Loss 0.13  Total Training Translation Loss 6.45 
2024-02-03 08:20:02,866 EPOCH 441
2024-02-03 08:20:07,373 Epoch 441: Total Training Recognition Loss 0.16  Total Training Translation Loss 5.67 
2024-02-03 08:20:07,374 EPOCH 442
2024-02-03 08:20:08,057 [Epoch: 442 Step: 00015000] Batch Recognition Loss:   0.004046 => Gls Tokens per Sec:     2819 || Batch Translation Loss:   0.128781 => Txt Tokens per Sec:     7253 || Lr: 0.000100
2024-02-03 08:20:12,028 Epoch 442: Total Training Recognition Loss 0.15  Total Training Translation Loss 5.21 
2024-02-03 08:20:12,028 EPOCH 443
2024-02-03 08:20:16,774 Epoch 443: Total Training Recognition Loss 0.17  Total Training Translation Loss 4.03 
2024-02-03 08:20:16,775 EPOCH 444
2024-02-03 08:20:21,195 Epoch 444: Total Training Recognition Loss 0.14  Total Training Translation Loss 5.37 
2024-02-03 08:20:21,195 EPOCH 445
2024-02-03 08:20:21,579 [Epoch: 445 Step: 00015100] Batch Recognition Loss:   0.002627 => Gls Tokens per Sec:     3342 || Batch Translation Loss:   0.440710 => Txt Tokens per Sec:     7796 || Lr: 0.000100
2024-02-03 08:20:25,920 Epoch 445: Total Training Recognition Loss 0.16  Total Training Translation Loss 7.79 
2024-02-03 08:20:25,921 EPOCH 446
2024-02-03 08:20:30,573 Epoch 446: Total Training Recognition Loss 0.19  Total Training Translation Loss 4.30 
2024-02-03 08:20:30,574 EPOCH 447
2024-02-03 08:20:35,294 Epoch 447: Total Training Recognition Loss 0.14  Total Training Translation Loss 3.20 
2024-02-03 08:20:35,295 EPOCH 448
2024-02-03 08:20:35,601 [Epoch: 448 Step: 00015200] Batch Recognition Loss:   0.008985 => Gls Tokens per Sec:     2091 || Batch Translation Loss:   0.058252 => Txt Tokens per Sec:     6742 || Lr: 0.000100
2024-02-03 08:20:39,907 Epoch 448: Total Training Recognition Loss 0.13  Total Training Translation Loss 2.27 
2024-02-03 08:20:39,907 EPOCH 449
2024-02-03 08:20:44,562 Epoch 449: Total Training Recognition Loss 0.14  Total Training Translation Loss 4.07 
2024-02-03 08:20:44,563 EPOCH 450
2024-02-03 08:20:49,016 [Epoch: 450 Step: 00015300] Batch Recognition Loss:   0.001338 => Gls Tokens per Sec:     2387 || Batch Translation Loss:   0.100644 => Txt Tokens per Sec:     6638 || Lr: 0.000100
2024-02-03 08:20:49,017 Epoch 450: Total Training Recognition Loss 0.15  Total Training Translation Loss 2.51 
2024-02-03 08:20:49,017 EPOCH 451
2024-02-03 08:20:53,614 Epoch 451: Total Training Recognition Loss 0.12  Total Training Translation Loss 2.40 
2024-02-03 08:20:53,614 EPOCH 452
2024-02-03 08:20:58,024 Epoch 452: Total Training Recognition Loss 0.10  Total Training Translation Loss 1.98 
2024-02-03 08:20:58,025 EPOCH 453
2024-02-03 08:21:02,481 [Epoch: 453 Step: 00015400] Batch Recognition Loss:   0.000699 => Gls Tokens per Sec:     2243 || Batch Translation Loss:   0.038521 => Txt Tokens per Sec:     6221 || Lr: 0.000100
2024-02-03 08:21:02,781 Epoch 453: Total Training Recognition Loss 0.14  Total Training Translation Loss 2.60 
2024-02-03 08:21:02,781 EPOCH 454
2024-02-03 08:21:07,563 Epoch 454: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.99 
2024-02-03 08:21:07,563 EPOCH 455
2024-02-03 08:21:11,961 Epoch 455: Total Training Recognition Loss 0.09  Total Training Translation Loss 1.99 
2024-02-03 08:21:11,961 EPOCH 456
2024-02-03 08:21:15,950 [Epoch: 456 Step: 00015500] Batch Recognition Loss:   0.000880 => Gls Tokens per Sec:     2345 || Batch Translation Loss:   0.043727 => Txt Tokens per Sec:     6469 || Lr: 0.000100
2024-02-03 08:21:16,655 Epoch 456: Total Training Recognition Loss 0.11  Total Training Translation Loss 3.25 
2024-02-03 08:21:16,655 EPOCH 457
2024-02-03 08:21:21,209 Epoch 457: Total Training Recognition Loss 0.14  Total Training Translation Loss 5.14 
2024-02-03 08:21:21,210 EPOCH 458
2024-02-03 08:21:26,036 Epoch 458: Total Training Recognition Loss 0.22  Total Training Translation Loss 4.00 
2024-02-03 08:21:26,037 EPOCH 459
2024-02-03 08:21:29,385 [Epoch: 459 Step: 00015600] Batch Recognition Loss:   0.002471 => Gls Tokens per Sec:     2676 || Batch Translation Loss:   0.049463 => Txt Tokens per Sec:     7297 || Lr: 0.000100
2024-02-03 08:21:30,408 Epoch 459: Total Training Recognition Loss 0.27  Total Training Translation Loss 3.70 
2024-02-03 08:21:30,408 EPOCH 460
2024-02-03 08:21:35,195 Epoch 460: Total Training Recognition Loss 0.42  Total Training Translation Loss 6.37 
2024-02-03 08:21:35,196 EPOCH 461
2024-02-03 08:21:39,522 Epoch 461: Total Training Recognition Loss 0.43  Total Training Translation Loss 3.61 
2024-02-03 08:21:39,522 EPOCH 462
2024-02-03 08:21:43,375 [Epoch: 462 Step: 00015700] Batch Recognition Loss:   0.004215 => Gls Tokens per Sec:     2160 || Batch Translation Loss:   0.025698 => Txt Tokens per Sec:     5843 || Lr: 0.000100
2024-02-03 08:21:44,492 Epoch 462: Total Training Recognition Loss 0.33  Total Training Translation Loss 3.72 
2024-02-03 08:21:44,493 EPOCH 463
2024-02-03 08:21:48,993 Epoch 463: Total Training Recognition Loss 0.17  Total Training Translation Loss 5.61 
2024-02-03 08:21:48,994 EPOCH 464
2024-02-03 08:21:53,794 Epoch 464: Total Training Recognition Loss 0.13  Total Training Translation Loss 3.99 
2024-02-03 08:21:53,794 EPOCH 465
2024-02-03 08:21:56,491 [Epoch: 465 Step: 00015800] Batch Recognition Loss:   0.002260 => Gls Tokens per Sec:     2755 || Batch Translation Loss:   0.083813 => Txt Tokens per Sec:     7462 || Lr: 0.000100
2024-02-03 08:21:57,873 Epoch 465: Total Training Recognition Loss 0.12  Total Training Translation Loss 2.01 
2024-02-03 08:21:57,874 EPOCH 466
2024-02-03 08:22:02,767 Epoch 466: Total Training Recognition Loss 0.12  Total Training Translation Loss 1.99 
2024-02-03 08:22:02,768 EPOCH 467
2024-02-03 08:22:07,299 Epoch 467: Total Training Recognition Loss 0.10  Total Training Translation Loss 1.92 
2024-02-03 08:22:07,299 EPOCH 468
2024-02-03 08:22:10,749 [Epoch: 468 Step: 00015900] Batch Recognition Loss:   0.003672 => Gls Tokens per Sec:     1969 || Batch Translation Loss:   0.038344 => Txt Tokens per Sec:     5765 || Lr: 0.000100
2024-02-03 08:22:12,038 Epoch 468: Total Training Recognition Loss 0.10  Total Training Translation Loss 1.64 
2024-02-03 08:22:12,038 EPOCH 469
2024-02-03 08:22:16,426 Epoch 469: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.45 
2024-02-03 08:22:16,427 EPOCH 470
2024-02-03 08:22:21,187 Epoch 470: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.43 
2024-02-03 08:22:21,188 EPOCH 471
2024-02-03 08:22:23,436 [Epoch: 471 Step: 00016000] Batch Recognition Loss:   0.001114 => Gls Tokens per Sec:     2847 || Batch Translation Loss:   0.017737 => Txt Tokens per Sec:     7651 || Lr: 0.000100
2024-02-03 08:22:32,313 Validation result at epoch 471, step    16000: duration: 8.8769s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.91522	Translation Loss: 88870.61719	PPL: 10202.96289
	Eval Metric: BLEU
	WER 4.03	(DEL: 0.07,	INS: 0.00,	SUB: 3.96)
	BLEU-4 0.68	(BLEU-1: 10.74,	BLEU-2: 3.22,	BLEU-3: 1.33,	BLEU-4: 0.68)
	CHRF 17.26	ROUGE 8.75
2024-02-03 08:22:32,314 Logging Recognition and Translation Outputs
2024-02-03 08:22:32,314 ========================================================================================================================
2024-02-03 08:22:32,314 Logging Sequence: 182_20.00
2024-02-03 08:22:32,315 	Gloss Reference :	A B+C+D+E
2024-02-03 08:22:32,315 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 08:22:32,315 	Gloss Alignment :	         
2024-02-03 08:22:32,315 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 08:22:32,317 	Text Reference  :	in 2019 june yuvraj shocked the world when he announced his  international retirement many    people were sad with   the       news
2024-02-03 08:22:32,317 	Text Hypothesis :	** **** **** ****** ******* *** ***** soon he ********* even spoke         angrily    against this   and  the police announced that
2024-02-03 08:22:32,317 	Text Alignment  :	D  D    D    D      D       D   D     S       D         S    S             S          S       S      S    S   S      S         S   
2024-02-03 08:22:32,317 ========================================================================================================================
2024-02-03 08:22:32,317 Logging Sequence: 180_409.00
2024-02-03 08:22:32,317 	Gloss Reference :	A B+C+D+E
2024-02-03 08:22:32,317 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 08:22:32,317 	Gloss Alignment :	         
2024-02-03 08:22:32,318 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 08:22:32,319 	Text Reference  :	cricketer kapil dev etc       have  also shown   support to      the          wrestlers through their   tweets
2024-02-03 08:22:32,319 	Text Hypothesis :	********* ***** *** wrestlers moved the  supreme court   seeking registration of        fir     against singh 
2024-02-03 08:22:32,319 	Text Alignment  :	D         D     D   S         S     S    S       S       S       S            S         S       S       S     
2024-02-03 08:22:32,319 ========================================================================================================================
2024-02-03 08:22:32,319 Logging Sequence: 59_2.00
2024-02-03 08:22:32,319 	Gloss Reference :	A B+C+D+E
2024-02-03 08:22:32,320 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 08:22:32,320 	Gloss Alignment :	         
2024-02-03 08:22:32,320 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 08:22:32,320 	Text Reference  :	at the 2020 tokyo olympics in   japan
2024-02-03 08:22:32,320 	Text Hypothesis :	** *** well let   me       tell you  
2024-02-03 08:22:32,320 	Text Alignment  :	D  D   S    S     S        S    S    
2024-02-03 08:22:32,321 ========================================================================================================================
2024-02-03 08:22:32,321 Logging Sequence: 118_100.00
2024-02-03 08:22:32,321 	Gloss Reference :	A B+C+D+E
2024-02-03 08:22:32,321 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 08:22:32,321 	Gloss Alignment :	         
2024-02-03 08:22:32,321 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 08:22:32,322 	Text Reference  :	********* ** while the ***** *** french were very heartbroken by     this  
2024-02-03 08:22:32,322 	Text Hypothesis :	according to see   the match was taken  due  to   his         recent demise
2024-02-03 08:22:32,322 	Text Alignment  :	I         I  S         I     I   S      S    S    S           S      S     
2024-02-03 08:22:32,322 ========================================================================================================================
2024-02-03 08:22:32,322 Logging Sequence: 69_204.00
2024-02-03 08:22:32,323 	Gloss Reference :	A B+C+D+E
2024-02-03 08:22:32,323 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 08:22:32,323 	Gloss Alignment :	         
2024-02-03 08:22:32,323 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 08:22:32,323 	Text Reference  :	however i am hoping to     play     the   next  season'
2024-02-03 08:22:32,323 	Text Hypothesis :	******* * ** ****** bhogle directly asked about dhoni  
2024-02-03 08:22:32,324 	Text Alignment  :	D       D D  D      S      S        S     S     S      
2024-02-03 08:22:32,324 ========================================================================================================================
2024-02-03 08:22:34,526 Epoch 471: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.13 
2024-02-03 08:22:34,526 EPOCH 472
2024-02-03 08:22:39,008 Epoch 472: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.08 
2024-02-03 08:22:39,009 EPOCH 473
2024-02-03 08:22:43,772 Epoch 473: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.59 
2024-02-03 08:22:43,773 EPOCH 474
2024-02-03 08:22:46,108 [Epoch: 474 Step: 00016100] Batch Recognition Loss:   0.005533 => Gls Tokens per Sec:     2468 || Batch Translation Loss:   0.110556 => Txt Tokens per Sec:     6710 || Lr: 0.000100
2024-02-03 08:22:48,177 Epoch 474: Total Training Recognition Loss 0.09  Total Training Translation Loss 1.61 
2024-02-03 08:22:48,178 EPOCH 475
2024-02-03 08:22:52,837 Epoch 475: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.79 
2024-02-03 08:22:52,838 EPOCH 476
2024-02-03 08:22:57,498 Epoch 476: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.26 
2024-02-03 08:22:57,498 EPOCH 477
2024-02-03 08:22:59,934 [Epoch: 477 Step: 00016200] Batch Recognition Loss:   0.001866 => Gls Tokens per Sec:     2103 || Batch Translation Loss:   0.059627 => Txt Tokens per Sec:     5858 || Lr: 0.000100
2024-02-03 08:23:02,413 Epoch 477: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.51 
2024-02-03 08:23:02,414 EPOCH 478
2024-02-03 08:23:06,860 Epoch 478: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.01 
2024-02-03 08:23:06,861 EPOCH 479
2024-02-03 08:23:11,629 Epoch 479: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.13 
2024-02-03 08:23:11,629 EPOCH 480
2024-02-03 08:23:13,586 [Epoch: 480 Step: 00016300] Batch Recognition Loss:   0.011635 => Gls Tokens per Sec:     2290 || Batch Translation Loss:   0.063751 => Txt Tokens per Sec:     6477 || Lr: 0.000100
2024-02-03 08:23:16,283 Epoch 480: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.41 
2024-02-03 08:23:16,283 EPOCH 481
2024-02-03 08:23:20,780 Epoch 481: Total Training Recognition Loss 0.09  Total Training Translation Loss 2.94 
2024-02-03 08:23:20,780 EPOCH 482
2024-02-03 08:23:25,464 Epoch 482: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.63 
2024-02-03 08:23:25,465 EPOCH 483
2024-02-03 08:23:26,755 [Epoch: 483 Step: 00016400] Batch Recognition Loss:   0.003775 => Gls Tokens per Sec:     2787 || Batch Translation Loss:   0.039100 => Txt Tokens per Sec:     7360 || Lr: 0.000100
2024-02-03 08:23:30,265 Epoch 483: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.98 
2024-02-03 08:23:30,266 EPOCH 484
2024-02-03 08:23:35,158 Epoch 484: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.93 
2024-02-03 08:23:35,159 EPOCH 485
2024-02-03 08:23:39,778 Epoch 485: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.85 
2024-02-03 08:23:39,778 EPOCH 486
2024-02-03 08:23:41,380 [Epoch: 486 Step: 00016500] Batch Recognition Loss:   0.004685 => Gls Tokens per Sec:     2000 || Batch Translation Loss:   0.054368 => Txt Tokens per Sec:     5614 || Lr: 0.000100
2024-02-03 08:23:44,630 Epoch 486: Total Training Recognition Loss 0.09  Total Training Translation Loss 2.94 
2024-02-03 08:23:44,631 EPOCH 487
2024-02-03 08:23:49,452 Epoch 487: Total Training Recognition Loss 0.10  Total Training Translation Loss 2.67 
2024-02-03 08:23:49,453 EPOCH 488
2024-02-03 08:23:54,344 Epoch 488: Total Training Recognition Loss 0.09  Total Training Translation Loss 3.68 
2024-02-03 08:23:54,345 EPOCH 489
2024-02-03 08:23:55,360 [Epoch: 489 Step: 00016600] Batch Recognition Loss:   0.001849 => Gls Tokens per Sec:     2525 || Batch Translation Loss:   0.123286 => Txt Tokens per Sec:     6598 || Lr: 0.000100
2024-02-03 08:23:59,209 Epoch 489: Total Training Recognition Loss 0.22  Total Training Translation Loss 12.32 
2024-02-03 08:23:59,209 EPOCH 490
2024-02-03 08:24:03,777 Epoch 490: Total Training Recognition Loss 0.36  Total Training Translation Loss 10.75 
2024-02-03 08:24:03,777 EPOCH 491
2024-02-03 08:24:08,669 Epoch 491: Total Training Recognition Loss 0.53  Total Training Translation Loss 6.60 
2024-02-03 08:24:08,669 EPOCH 492
2024-02-03 08:24:09,587 [Epoch: 492 Step: 00016700] Batch Recognition Loss:   0.025554 => Gls Tokens per Sec:     2094 || Batch Translation Loss:   0.159341 => Txt Tokens per Sec:     5587 || Lr: 0.000100
2024-02-03 08:24:13,387 Epoch 492: Total Training Recognition Loss 0.27  Total Training Translation Loss 4.64 
2024-02-03 08:24:13,387 EPOCH 493
2024-02-03 08:24:18,217 Epoch 493: Total Training Recognition Loss 0.17  Total Training Translation Loss 3.43 
2024-02-03 08:24:18,217 EPOCH 494
2024-02-03 08:24:23,159 Epoch 494: Total Training Recognition Loss 0.13  Total Training Translation Loss 4.24 
2024-02-03 08:24:23,160 EPOCH 495
2024-02-03 08:24:23,582 [Epoch: 495 Step: 00016800] Batch Recognition Loss:   0.000880 => Gls Tokens per Sec:     3040 || Batch Translation Loss:   0.076608 => Txt Tokens per Sec:     8648 || Lr: 0.000100
2024-02-03 08:24:27,949 Epoch 495: Total Training Recognition Loss 0.11  Total Training Translation Loss 5.07 
2024-02-03 08:24:27,949 EPOCH 496
2024-02-03 08:24:32,827 Epoch 496: Total Training Recognition Loss 0.17  Total Training Translation Loss 4.00 
2024-02-03 08:24:32,827 EPOCH 497
2024-02-03 08:24:37,681 Epoch 497: Total Training Recognition Loss 0.14  Total Training Translation Loss 2.29 
2024-02-03 08:24:37,682 EPOCH 498
2024-02-03 08:24:38,055 [Epoch: 498 Step: 00016900] Batch Recognition Loss:   0.005236 => Gls Tokens per Sec:     1716 || Batch Translation Loss:   0.036154 => Txt Tokens per Sec:     5461 || Lr: 0.000100
2024-02-03 08:24:42,467 Epoch 498: Total Training Recognition Loss 0.08  Total Training Translation Loss 4.46 
2024-02-03 08:24:42,467 EPOCH 499
2024-02-03 08:24:47,176 Epoch 499: Total Training Recognition Loss 0.16  Total Training Translation Loss 4.23 
2024-02-03 08:24:47,177 EPOCH 500
2024-02-03 08:24:51,910 [Epoch: 500 Step: 00017000] Batch Recognition Loss:   0.002448 => Gls Tokens per Sec:     2246 || Batch Translation Loss:   0.103473 => Txt Tokens per Sec:     6247 || Lr: 0.000100
2024-02-03 08:24:51,911 Epoch 500: Total Training Recognition Loss 0.10  Total Training Translation Loss 3.08 
2024-02-03 08:24:51,911 EPOCH 501
2024-02-03 08:24:56,816 Epoch 501: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.03 
2024-02-03 08:24:56,816 EPOCH 502
2024-02-03 08:25:01,487 Epoch 502: Total Training Recognition Loss 0.10  Total Training Translation Loss 1.55 
2024-02-03 08:25:01,487 EPOCH 503
2024-02-03 08:25:06,211 [Epoch: 503 Step: 00017100] Batch Recognition Loss:   0.003139 => Gls Tokens per Sec:     2115 || Batch Translation Loss:   0.053668 => Txt Tokens per Sec:     5872 || Lr: 0.000100
2024-02-03 08:25:06,453 Epoch 503: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.50 
2024-02-03 08:25:06,453 EPOCH 504
2024-02-03 08:25:11,432 Epoch 504: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.88 
2024-02-03 08:25:11,433 EPOCH 505
2024-02-03 08:25:16,090 Epoch 505: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.40 
2024-02-03 08:25:16,091 EPOCH 506
2024-02-03 08:25:20,049 [Epoch: 506 Step: 00017200] Batch Recognition Loss:   0.000493 => Gls Tokens per Sec:     2363 || Batch Translation Loss:   0.032655 => Txt Tokens per Sec:     6499 || Lr: 0.000100
2024-02-03 08:25:20,610 Epoch 506: Total Training Recognition Loss 0.10  Total Training Translation Loss 1.64 
2024-02-03 08:25:20,610 EPOCH 507
2024-02-03 08:25:25,211 Epoch 507: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.76 
2024-02-03 08:25:25,212 EPOCH 508
2024-02-03 08:25:29,787 Epoch 508: Total Training Recognition Loss 0.12  Total Training Translation Loss 1.36 
2024-02-03 08:25:29,787 EPOCH 509
2024-02-03 08:25:33,151 [Epoch: 509 Step: 00017300] Batch Recognition Loss:   0.001611 => Gls Tokens per Sec:     2589 || Batch Translation Loss:   0.022724 => Txt Tokens per Sec:     7136 || Lr: 0.000100
2024-02-03 08:25:34,151 Epoch 509: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.40 
2024-02-03 08:25:34,151 EPOCH 510
2024-02-03 08:25:38,921 Epoch 510: Total Training Recognition Loss 0.10  Total Training Translation Loss 1.13 
2024-02-03 08:25:38,922 EPOCH 511
2024-02-03 08:25:43,555 Epoch 511: Total Training Recognition Loss 0.07  Total Training Translation Loss 0.99 
2024-02-03 08:25:43,555 EPOCH 512
2024-02-03 08:25:46,974 [Epoch: 512 Step: 00017400] Batch Recognition Loss:   0.000798 => Gls Tokens per Sec:     2361 || Batch Translation Loss:   0.039291 => Txt Tokens per Sec:     6534 || Lr: 0.000100
2024-02-03 08:25:48,058 Epoch 512: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.16 
2024-02-03 08:25:48,058 EPOCH 513
2024-02-03 08:25:52,705 Epoch 513: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.05 
2024-02-03 08:25:52,706 EPOCH 514
2024-02-03 08:25:57,232 Epoch 514: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.93 
2024-02-03 08:25:57,232 EPOCH 515
2024-02-03 08:26:00,482 [Epoch: 515 Step: 00017500] Batch Recognition Loss:   0.000391 => Gls Tokens per Sec:     2287 || Batch Translation Loss:   0.028292 => Txt Tokens per Sec:     6394 || Lr: 0.000100
2024-02-03 08:26:01,966 Epoch 515: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.07 
2024-02-03 08:26:01,966 EPOCH 516
2024-02-03 08:26:06,412 Epoch 516: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.12 
2024-02-03 08:26:06,413 EPOCH 517
2024-02-03 08:26:11,048 Epoch 517: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.59 
2024-02-03 08:26:11,049 EPOCH 518
2024-02-03 08:26:13,694 [Epoch: 518 Step: 00017600] Batch Recognition Loss:   0.002860 => Gls Tokens per Sec:     2569 || Batch Translation Loss:   0.012941 => Txt Tokens per Sec:     6804 || Lr: 0.000100
2024-02-03 08:26:15,652 Epoch 518: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.74 
2024-02-03 08:26:15,652 EPOCH 519
2024-02-03 08:26:20,371 Epoch 519: Total Training Recognition Loss 0.09  Total Training Translation Loss 1.88 
2024-02-03 08:26:20,371 EPOCH 520
2024-02-03 08:26:24,789 Epoch 520: Total Training Recognition Loss 0.10  Total Training Translation Loss 1.70 
2024-02-03 08:26:24,789 EPOCH 521
2024-02-03 08:26:27,443 [Epoch: 521 Step: 00017700] Batch Recognition Loss:   0.001051 => Gls Tokens per Sec:     2318 || Batch Translation Loss:   0.233380 => Txt Tokens per Sec:     6816 || Lr: 0.000100
2024-02-03 08:26:29,324 Epoch 521: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.18 
2024-02-03 08:26:29,324 EPOCH 522
2024-02-03 08:26:34,084 Epoch 522: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.27 
2024-02-03 08:26:34,084 EPOCH 523
2024-02-03 08:26:39,011 Epoch 523: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.26 
2024-02-03 08:26:39,011 EPOCH 524
2024-02-03 08:26:41,127 [Epoch: 524 Step: 00017800] Batch Recognition Loss:   0.008121 => Gls Tokens per Sec:     2724 || Batch Translation Loss:   0.096157 => Txt Tokens per Sec:     7182 || Lr: 0.000100
2024-02-03 08:26:43,779 Epoch 524: Total Training Recognition Loss 0.10  Total Training Translation Loss 3.08 
2024-02-03 08:26:43,779 EPOCH 525
2024-02-03 08:26:48,551 Epoch 525: Total Training Recognition Loss 0.10  Total Training Translation Loss 2.40 
2024-02-03 08:26:48,551 EPOCH 526
2024-02-03 08:26:53,346 Epoch 526: Total Training Recognition Loss 0.09  Total Training Translation Loss 2.56 
2024-02-03 08:26:53,346 EPOCH 527
2024-02-03 08:26:55,469 [Epoch: 527 Step: 00017900] Batch Recognition Loss:   0.001812 => Gls Tokens per Sec:     2294 || Batch Translation Loss:   0.108531 => Txt Tokens per Sec:     6369 || Lr: 0.000100
2024-02-03 08:26:58,046 Epoch 527: Total Training Recognition Loss 0.13  Total Training Translation Loss 10.76 
2024-02-03 08:26:58,046 EPOCH 528
2024-02-03 08:27:02,935 Epoch 528: Total Training Recognition Loss 0.27  Total Training Translation Loss 16.80 
2024-02-03 08:27:02,936 EPOCH 529
2024-02-03 08:27:07,597 Epoch 529: Total Training Recognition Loss 0.33  Total Training Translation Loss 14.52 
2024-02-03 08:27:07,597 EPOCH 530
2024-02-03 08:27:09,707 [Epoch: 530 Step: 00018000] Batch Recognition Loss:   0.005198 => Gls Tokens per Sec:     2124 || Batch Translation Loss:   0.127121 => Txt Tokens per Sec:     6106 || Lr: 0.000100
2024-02-03 08:27:18,577 Validation result at epoch 530, step    18000: duration: 8.8692s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 3.04559	Translation Loss: 88886.95312	PPL: 10220.28809
	Eval Metric: BLEU
	WER 4.53	(DEL: 0.14,	INS: 0.00,	SUB: 4.38)
	BLEU-4 0.76	(BLEU-1: 10.51,	BLEU-2: 3.21,	BLEU-3: 1.36,	BLEU-4: 0.76)
	CHRF 16.73	ROUGE 9.02
2024-02-03 08:27:18,578 Logging Recognition and Translation Outputs
2024-02-03 08:27:18,578 ========================================================================================================================
2024-02-03 08:27:18,578 Logging Sequence: 64_53.00
2024-02-03 08:27:18,579 	Gloss Reference :	A B+C+D+E
2024-02-03 08:27:18,579 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 08:27:18,579 	Gloss Alignment :	         
2024-02-03 08:27:18,579 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 08:27:18,580 	Text Reference  :	the ******** bcci and ipl  does      not want     to compromise on      the  safety of     the players     
2024-02-03 08:27:18,581 	Text Hypothesis :	the decision was  not been postponed and everyone to play       against this may    called all participants
2024-02-03 08:27:18,581 	Text Alignment  :	    I        S    S   S    S         S   S           S          S       S    S      S      S   S           
2024-02-03 08:27:18,581 ========================================================================================================================
2024-02-03 08:27:18,581 Logging Sequence: 165_252.00
2024-02-03 08:27:18,581 	Gloss Reference :	A B+C+D+E
2024-02-03 08:27:18,581 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 08:27:18,582 	Gloss Alignment :	         
2024-02-03 08:27:18,582 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 08:27:18,582 	Text Reference  :	********* ***** ****** 4    rahul dravid  also  has     his        own  superstitions
2024-02-03 08:27:18,582 	Text Hypothesis :	tendulkar would always made it    brought about dhoni's retirement from pakistan     
2024-02-03 08:27:18,583 	Text Alignment  :	I         I     I      S    S     S       S     S       S          S    S            
2024-02-03 08:27:18,583 ========================================================================================================================
2024-02-03 08:27:18,583 Logging Sequence: 90_7.00
2024-02-03 08:27:18,583 	Gloss Reference :	A B+C+D+E
2024-02-03 08:27:18,583 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 08:27:18,583 	Gloss Alignment :	         
2024-02-03 08:27:18,583 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 08:27:18,584 	Text Reference  :	let me tell you the exciting story of how they       were introduced
2024-02-03 08:27:18,584 	Text Hypothesis :	*** ** **** *** *** ******** ***** i  am  devastated by   this      
2024-02-03 08:27:18,584 	Text Alignment  :	D   D  D    D   D   D        D     S  S   S          S    S         
2024-02-03 08:27:18,584 ========================================================================================================================
2024-02-03 08:27:18,584 Logging Sequence: 137_307.00
2024-02-03 08:27:18,584 	Gloss Reference :	A B+C+D+E
2024-02-03 08:27:18,585 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 08:27:18,585 	Gloss Alignment :	         
2024-02-03 08:27:18,585 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 08:27:18,586 	Text Reference  :	so to be extra careful the player's flight was escorted by the flighter jets  
2024-02-03 08:27:18,586 	Text Hypothesis :	** ** ** ***** ******* *** we       won    6   medal    in the ******** women'
2024-02-03 08:27:18,586 	Text Alignment  :	D  D  D  D     D       D   S        S      S   S        S      D        S     
2024-02-03 08:27:18,586 ========================================================================================================================
2024-02-03 08:27:18,586 Logging Sequence: 165_200.00
2024-02-03 08:27:18,586 	Gloss Reference :	A B+C+D+E
2024-02-03 08:27:18,586 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 08:27:18,587 	Gloss Alignment :	         
2024-02-03 08:27:18,587 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 08:27:18,588 	Text Reference  :	*** you  may  be    wondering what    bag  remember in  2011 when india won the       world cup 
2024-02-03 08:27:18,588 	Text Hypothesis :	now they gave birth to        propose when csk      had a    semi final and captaincy to    bowl
2024-02-03 08:27:18,588 	Text Alignment  :	I   S    S    S     S         S       S    S        S   S    S    S     S   S         S     S   
2024-02-03 08:27:18,588 ========================================================================================================================
2024-02-03 08:27:21,329 Epoch 530: Total Training Recognition Loss 0.24  Total Training Translation Loss 4.37 
2024-02-03 08:27:21,329 EPOCH 531
2024-02-03 08:27:26,016 Epoch 531: Total Training Recognition Loss 0.15  Total Training Translation Loss 3.97 
2024-02-03 08:27:26,017 EPOCH 532
2024-02-03 08:27:30,875 Epoch 532: Total Training Recognition Loss 0.20  Total Training Translation Loss 3.21 
2024-02-03 08:27:30,875 EPOCH 533
2024-02-03 08:27:32,081 [Epoch: 533 Step: 00018100] Batch Recognition Loss:   0.002318 => Gls Tokens per Sec:     3186 || Batch Translation Loss:   0.270261 => Txt Tokens per Sec:     8384 || Lr: 0.000100
2024-02-03 08:27:35,366 Epoch 533: Total Training Recognition Loss 0.20  Total Training Translation Loss 3.06 
2024-02-03 08:27:35,367 EPOCH 534
2024-02-03 08:27:40,225 Epoch 534: Total Training Recognition Loss 0.10  Total Training Translation Loss 2.27 
2024-02-03 08:27:40,225 EPOCH 535
2024-02-03 08:27:44,428 Epoch 535: Total Training Recognition Loss 0.11  Total Training Translation Loss 1.80 
2024-02-03 08:27:44,428 EPOCH 536
2024-02-03 08:27:45,555 [Epoch: 536 Step: 00018200] Batch Recognition Loss:   0.001142 => Gls Tokens per Sec:     2842 || Batch Translation Loss:   0.028830 => Txt Tokens per Sec:     7421 || Lr: 0.000100
2024-02-03 08:27:49,277 Epoch 536: Total Training Recognition Loss 0.09  Total Training Translation Loss 1.44 
2024-02-03 08:27:49,277 EPOCH 537
2024-02-03 08:27:53,591 Epoch 537: Total Training Recognition Loss 0.09  Total Training Translation Loss 1.45 
2024-02-03 08:27:53,591 EPOCH 538
2024-02-03 08:27:58,580 Epoch 538: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.62 
2024-02-03 08:27:58,580 EPOCH 539
2024-02-03 08:27:59,284 [Epoch: 539 Step: 00018300] Batch Recognition Loss:   0.002454 => Gls Tokens per Sec:     3640 || Batch Translation Loss:   0.020618 => Txt Tokens per Sec:     8267 || Lr: 0.000100
2024-02-03 08:28:02,898 Epoch 539: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.49 
2024-02-03 08:28:02,899 EPOCH 540
2024-02-03 08:28:07,773 Epoch 540: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.84 
2024-02-03 08:28:07,774 EPOCH 541
2024-02-03 08:28:12,358 Epoch 541: Total Training Recognition Loss 0.11  Total Training Translation Loss 1.37 
2024-02-03 08:28:12,358 EPOCH 542
2024-02-03 08:28:13,131 [Epoch: 542 Step: 00018400] Batch Recognition Loss:   0.001361 => Gls Tokens per Sec:     2492 || Batch Translation Loss:   0.019433 => Txt Tokens per Sec:     7067 || Lr: 0.000100
2024-02-03 08:28:16,985 Epoch 542: Total Training Recognition Loss 0.10  Total Training Translation Loss 2.00 
2024-02-03 08:28:16,985 EPOCH 543
2024-02-03 08:28:21,751 Epoch 543: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.23 
2024-02-03 08:28:21,752 EPOCH 544
2024-02-03 08:28:26,134 Epoch 544: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.11 
2024-02-03 08:28:26,134 EPOCH 545
2024-02-03 08:28:26,609 [Epoch: 545 Step: 00018500] Batch Recognition Loss:   0.000226 => Gls Tokens per Sec:     2700 || Batch Translation Loss:   0.039188 => Txt Tokens per Sec:     7985 || Lr: 0.000100
2024-02-03 08:28:30,974 Epoch 545: Total Training Recognition Loss 0.06  Total Training Translation Loss 0.98 
2024-02-03 08:28:30,975 EPOCH 546
2024-02-03 08:28:35,460 Epoch 546: Total Training Recognition Loss 0.07  Total Training Translation Loss 0.85 
2024-02-03 08:28:35,460 EPOCH 547
2024-02-03 08:28:40,450 Epoch 547: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.79 
2024-02-03 08:28:40,451 EPOCH 548
2024-02-03 08:28:40,605 [Epoch: 548 Step: 00018600] Batch Recognition Loss:   0.001987 => Gls Tokens per Sec:     4183 || Batch Translation Loss:   0.016096 => Txt Tokens per Sec:     8987 || Lr: 0.000100
2024-02-03 08:28:44,616 Epoch 548: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.07 
2024-02-03 08:28:44,616 EPOCH 549
2024-02-03 08:28:49,534 Epoch 549: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.95 
2024-02-03 08:28:49,535 EPOCH 550
2024-02-03 08:28:53,761 [Epoch: 550 Step: 00018700] Batch Recognition Loss:   0.000308 => Gls Tokens per Sec:     2516 || Batch Translation Loss:   0.020921 => Txt Tokens per Sec:     6997 || Lr: 0.000100
2024-02-03 08:28:53,761 Epoch 550: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.75 
2024-02-03 08:28:53,761 EPOCH 551
2024-02-03 08:28:58,569 Epoch 551: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.24 
2024-02-03 08:28:58,570 EPOCH 552
2024-02-03 08:29:02,905 Epoch 552: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.34 
2024-02-03 08:29:02,906 EPOCH 553
2024-02-03 08:29:07,392 [Epoch: 553 Step: 00018800] Batch Recognition Loss:   0.002936 => Gls Tokens per Sec:     2227 || Batch Translation Loss:   0.122801 => Txt Tokens per Sec:     6254 || Lr: 0.000100
2024-02-03 08:29:07,672 Epoch 553: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.38 
2024-02-03 08:29:07,672 EPOCH 554
2024-02-03 08:29:12,325 Epoch 554: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.57 
2024-02-03 08:29:12,325 EPOCH 555
2024-02-03 08:29:16,935 Epoch 555: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.97 
2024-02-03 08:29:16,935 EPOCH 556
2024-02-03 08:29:21,157 [Epoch: 556 Step: 00018900] Batch Recognition Loss:   0.001424 => Gls Tokens per Sec:     2275 || Batch Translation Loss:   0.060439 => Txt Tokens per Sec:     6428 || Lr: 0.000100
2024-02-03 08:29:21,695 Epoch 556: Total Training Recognition Loss 0.09  Total Training Translation Loss 3.11 
2024-02-03 08:29:21,696 EPOCH 557
2024-02-03 08:29:26,581 Epoch 557: Total Training Recognition Loss 0.09  Total Training Translation Loss 4.33 
2024-02-03 08:29:26,581 EPOCH 558
2024-02-03 08:29:30,815 Epoch 558: Total Training Recognition Loss 0.07  Total Training Translation Loss 5.60 
2024-02-03 08:29:30,815 EPOCH 559
2024-02-03 08:29:34,937 [Epoch: 559 Step: 00019000] Batch Recognition Loss:   0.000676 => Gls Tokens per Sec:     2114 || Batch Translation Loss:   0.127896 => Txt Tokens per Sec:     5928 || Lr: 0.000100
2024-02-03 08:29:35,691 Epoch 559: Total Training Recognition Loss 0.11  Total Training Translation Loss 6.06 
2024-02-03 08:29:35,691 EPOCH 560
2024-02-03 08:29:39,783 Epoch 560: Total Training Recognition Loss 0.11  Total Training Translation Loss 4.82 
2024-02-03 08:29:39,784 EPOCH 561
2024-02-03 08:29:44,733 Epoch 561: Total Training Recognition Loss 0.10  Total Training Translation Loss 4.15 
2024-02-03 08:29:44,733 EPOCH 562
2024-02-03 08:29:48,108 [Epoch: 562 Step: 00019100] Batch Recognition Loss:   0.000997 => Gls Tokens per Sec:     2392 || Batch Translation Loss:   0.054633 => Txt Tokens per Sec:     6711 || Lr: 0.000100
2024-02-03 08:29:49,205 Epoch 562: Total Training Recognition Loss 0.10  Total Training Translation Loss 3.20 
2024-02-03 08:29:49,206 EPOCH 563
2024-02-03 08:29:53,923 Epoch 563: Total Training Recognition Loss 0.09  Total Training Translation Loss 2.72 
2024-02-03 08:29:53,923 EPOCH 564
2024-02-03 08:29:58,267 Epoch 564: Total Training Recognition Loss 0.09  Total Training Translation Loss 2.22 
2024-02-03 08:29:58,268 EPOCH 565
2024-02-03 08:30:01,612 [Epoch: 565 Step: 00019200] Batch Recognition Loss:   0.002121 => Gls Tokens per Sec:     2223 || Batch Translation Loss:   0.251440 => Txt Tokens per Sec:     6108 || Lr: 0.000100
2024-02-03 08:30:03,093 Epoch 565: Total Training Recognition Loss 0.09  Total Training Translation Loss 2.05 
2024-02-03 08:30:03,093 EPOCH 566
2024-02-03 08:30:07,749 Epoch 566: Total Training Recognition Loss 0.11  Total Training Translation Loss 1.88 
2024-02-03 08:30:07,750 EPOCH 567
2024-02-03 08:30:12,245 Epoch 567: Total Training Recognition Loss 0.10  Total Training Translation Loss 1.91 
2024-02-03 08:30:12,246 EPOCH 568
2024-02-03 08:30:15,117 [Epoch: 568 Step: 00019300] Batch Recognition Loss:   0.000643 => Gls Tokens per Sec:     2453 || Batch Translation Loss:   0.250726 => Txt Tokens per Sec:     6713 || Lr: 0.000100
2024-02-03 08:30:17,088 Epoch 568: Total Training Recognition Loss 0.10  Total Training Translation Loss 1.72 
2024-02-03 08:30:17,089 EPOCH 569
2024-02-03 08:30:21,404 Epoch 569: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.43 
2024-02-03 08:30:21,404 EPOCH 570
2024-02-03 08:30:25,648 Epoch 570: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.71 
2024-02-03 08:30:25,649 EPOCH 571
2024-02-03 08:30:28,496 [Epoch: 571 Step: 00019400] Batch Recognition Loss:   0.001392 => Gls Tokens per Sec:     2160 || Batch Translation Loss:   0.034828 => Txt Tokens per Sec:     5893 || Lr: 0.000100
2024-02-03 08:30:30,555 Epoch 571: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.93 
2024-02-03 08:30:30,555 EPOCH 572
2024-02-03 08:30:35,326 Epoch 572: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.97 
2024-02-03 08:30:35,327 EPOCH 573
2024-02-03 08:30:39,699 Epoch 573: Total Training Recognition Loss 0.09  Total Training Translation Loss 2.70 
2024-02-03 08:30:39,699 EPOCH 574
2024-02-03 08:30:42,364 [Epoch: 574 Step: 00019500] Batch Recognition Loss:   0.000903 => Gls Tokens per Sec:     2069 || Batch Translation Loss:   0.024052 => Txt Tokens per Sec:     5680 || Lr: 0.000100
2024-02-03 08:30:44,616 Epoch 574: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.71 
2024-02-03 08:30:44,617 EPOCH 575
2024-02-03 08:30:48,897 Epoch 575: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.15 
2024-02-03 08:30:48,897 EPOCH 576
2024-02-03 08:30:53,683 Epoch 576: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.82 
2024-02-03 08:30:53,684 EPOCH 577
2024-02-03 08:30:55,511 [Epoch: 577 Step: 00019600] Batch Recognition Loss:   0.012321 => Gls Tokens per Sec:     2667 || Batch Translation Loss:   0.080643 => Txt Tokens per Sec:     7009 || Lr: 0.000100
2024-02-03 08:30:58,115 Epoch 577: Total Training Recognition Loss 0.08  Total Training Translation Loss 3.07 
2024-02-03 08:30:58,115 EPOCH 578
2024-02-03 08:31:02,793 Epoch 578: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.55 
2024-02-03 08:31:02,793 EPOCH 579
2024-02-03 08:31:07,274 Epoch 579: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.51 
2024-02-03 08:31:07,274 EPOCH 580
2024-02-03 08:31:09,292 [Epoch: 580 Step: 00019700] Batch Recognition Loss:   0.007366 => Gls Tokens per Sec:     2221 || Batch Translation Loss:   0.350076 => Txt Tokens per Sec:     6796 || Lr: 0.000100
2024-02-03 08:31:11,937 Epoch 580: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.24 
2024-02-03 08:31:11,938 EPOCH 581
2024-02-03 08:31:16,399 Epoch 581: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.66 
2024-02-03 08:31:16,399 EPOCH 582
2024-02-03 08:31:21,288 Epoch 582: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.52 
2024-02-03 08:31:21,288 EPOCH 583
2024-02-03 08:31:22,908 [Epoch: 583 Step: 00019800] Batch Recognition Loss:   0.003433 => Gls Tokens per Sec:     2372 || Batch Translation Loss:   0.096678 => Txt Tokens per Sec:     6964 || Lr: 0.000100
2024-02-03 08:31:25,733 Epoch 583: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.51 
2024-02-03 08:31:25,733 EPOCH 584
2024-02-03 08:31:30,819 Epoch 584: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.35 
2024-02-03 08:31:30,820 EPOCH 585
2024-02-03 08:31:35,796 Epoch 585: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.60 
2024-02-03 08:31:35,797 EPOCH 586
2024-02-03 08:31:37,077 [Epoch: 586 Step: 00019900] Batch Recognition Loss:   0.001484 => Gls Tokens per Sec:     2505 || Batch Translation Loss:   0.095723 => Txt Tokens per Sec:     7425 || Lr: 0.000100
2024-02-03 08:31:40,325 Epoch 586: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.41 
2024-02-03 08:31:40,326 EPOCH 587
2024-02-03 08:31:45,234 Epoch 587: Total Training Recognition Loss 0.08  Total Training Translation Loss 3.44 
2024-02-03 08:31:45,235 EPOCH 588
2024-02-03 08:31:49,823 Epoch 588: Total Training Recognition Loss 0.08  Total Training Translation Loss 3.26 
2024-02-03 08:31:49,823 EPOCH 589
2024-02-03 08:31:50,719 [Epoch: 589 Step: 00020000] Batch Recognition Loss:   0.000476 => Gls Tokens per Sec:     2859 || Batch Translation Loss:   0.068390 => Txt Tokens per Sec:     7627 || Lr: 0.000100
2024-02-03 08:31:59,271 Validation result at epoch 589, step    20000: duration: 8.5504s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.86895	Translation Loss: 89437.45312	PPL: 10821.68848
	Eval Metric: BLEU
	WER 3.82	(DEL: 0.00,	INS: 0.00,	SUB: 3.82)
	BLEU-4 0.78	(BLEU-1: 10.62,	BLEU-2: 3.45,	BLEU-3: 1.45,	BLEU-4: 0.78)
	CHRF 17.21	ROUGE 9.06
2024-02-03 08:31:59,272 Logging Recognition and Translation Outputs
2024-02-03 08:31:59,273 ========================================================================================================================
2024-02-03 08:31:59,273 Logging Sequence: 153_218.00
2024-02-03 08:31:59,273 	Gloss Reference :	A B+C+D+E
2024-02-03 08:31:59,273 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 08:31:59,273 	Gloss Alignment :	         
2024-02-03 08:31:59,274 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 08:31:59,275 	Text Reference  :	*** the 2022 final match is   being held in  the same melbourne  stadium  where the 1992 match was held     
2024-02-03 08:31:59,275 	Text Hypothesis :	now you are  aware the   game as    they had a   very aggressive attitude on    the **** ***** *** bengaluru
2024-02-03 08:31:59,275 	Text Alignment  :	I   S   S    S     S     S    S     S    S   S   S    S          S        S         D    D     D   S        
2024-02-03 08:31:59,276 ========================================================================================================================
2024-02-03 08:31:59,276 Logging Sequence: 168_115.00
2024-02-03 08:31:59,276 	Gloss Reference :	A B+C+D+E
2024-02-03 08:31:59,276 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 08:31:59,276 	Gloss Alignment :	         
2024-02-03 08:31:59,276 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 08:31:59,277 	Text Reference  :	******* ***** ******* this has sparked a ****** ******* major discussion on   social media 
2024-02-03 08:31:59,277 	Text Hypothesis :	however virat anushka have not shared  a single picture of    their      baby and    others
2024-02-03 08:31:59,277 	Text Alignment  :	I       I     I       S    S   S         I      I       S     S          S    S      S     
2024-02-03 08:31:59,278 ========================================================================================================================
2024-02-03 08:31:59,278 Logging Sequence: 180_82.00
2024-02-03 08:31:59,278 	Gloss Reference :	A B+C+D+E
2024-02-03 08:31:59,278 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 08:31:59,278 	Gloss Alignment :	         
2024-02-03 08:31:59,278 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 08:31:59,279 	Text Reference  :	let me tell you about the protest that has      been going on  since 23rd april       2023
2024-02-03 08:31:59,279 	Text Hypothesis :	*** ** **** *** ***** *** ******* **** speaking to   ani   csk ceo   kasi viswanathan said
2024-02-03 08:31:59,279 	Text Alignment  :	D   D  D    D   D     D   D       D    S        S    S     S   S     S    S           S   
2024-02-03 08:31:59,280 ========================================================================================================================
2024-02-03 08:31:59,280 Logging Sequence: 56_14.00
2024-02-03 08:31:59,280 	Gloss Reference :	A B+C+D+E
2024-02-03 08:31:59,280 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 08:31:59,280 	Gloss Alignment :	         
2024-02-03 08:31:59,280 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 08:31:59,281 	Text Reference  :	***** people were glued to their screens during the match  
2024-02-03 08:31:59,281 	Text Hypothesis :	these teams  were ***** ** ***** ******* sent   in  england
2024-02-03 08:31:59,281 	Text Alignment  :	I     S           D     D  D     D       S      S   S      
2024-02-03 08:31:59,281 ========================================================================================================================
2024-02-03 08:31:59,281 Logging Sequence: 62_135.00
2024-02-03 08:31:59,282 	Gloss Reference :	A B+C+D+E
2024-02-03 08:31:59,282 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 08:31:59,282 	Gloss Alignment :	         
2024-02-03 08:31:59,282 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 08:31:59,283 	Text Reference  :	*** *** ******* ** *** ***** **** a   grade player *** ** is       the one who 
2024-02-03 08:31:59,283 	Text Hypothesis :	and was stunned by the court when the woman player won by danushka on  3   days
2024-02-03 08:31:59,283 	Text Alignment  :	I   I   I       I  I   I     I    S   S            I   I  S        S   S   S   
2024-02-03 08:31:59,283 ========================================================================================================================
2024-02-03 08:32:03,165 Epoch 589: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.67 
2024-02-03 08:32:03,165 EPOCH 590
2024-02-03 08:32:07,553 Epoch 590: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.96 
2024-02-03 08:32:07,554 EPOCH 591
2024-02-03 08:32:12,459 Epoch 591: Total Training Recognition Loss 0.09  Total Training Translation Loss 2.62 
2024-02-03 08:32:12,460 EPOCH 592
2024-02-03 08:32:13,176 [Epoch: 592 Step: 00020100] Batch Recognition Loss:   0.000608 => Gls Tokens per Sec:     2685 || Batch Translation Loss:   0.092909 => Txt Tokens per Sec:     7810 || Lr: 0.000100
2024-02-03 08:32:17,067 Epoch 592: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.15 
2024-02-03 08:32:17,067 EPOCH 593
2024-02-03 08:32:21,663 Epoch 593: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.35 
2024-02-03 08:32:21,663 EPOCH 594
2024-02-03 08:32:26,180 Epoch 594: Total Training Recognition Loss 0.10  Total Training Translation Loss 2.71 
2024-02-03 08:32:26,181 EPOCH 595
2024-02-03 08:32:26,971 [Epoch: 595 Step: 00020200] Batch Recognition Loss:   0.001106 => Gls Tokens per Sec:     1624 || Batch Translation Loss:   0.037904 => Txt Tokens per Sec:     5440 || Lr: 0.000100
2024-02-03 08:32:30,849 Epoch 595: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.23 
2024-02-03 08:32:30,849 EPOCH 596
2024-02-03 08:32:35,616 Epoch 596: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.86 
2024-02-03 08:32:35,617 EPOCH 597
2024-02-03 08:32:40,282 Epoch 597: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.03 
2024-02-03 08:32:40,283 EPOCH 598
2024-02-03 08:32:40,578 [Epoch: 598 Step: 00020300] Batch Recognition Loss:   0.012067 => Gls Tokens per Sec:     2177 || Batch Translation Loss:   0.036252 => Txt Tokens per Sec:     6646 || Lr: 0.000100
2024-02-03 08:32:45,195 Epoch 598: Total Training Recognition Loss 0.09  Total Training Translation Loss 1.38 
2024-02-03 08:32:45,196 EPOCH 599
2024-02-03 08:32:49,816 Epoch 599: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.57 
2024-02-03 08:32:49,817 EPOCH 600
2024-02-03 08:32:54,720 [Epoch: 600 Step: 00020400] Batch Recognition Loss:   0.007086 => Gls Tokens per Sec:     2169 || Batch Translation Loss:   0.043379 => Txt Tokens per Sec:     6031 || Lr: 0.000100
2024-02-03 08:32:54,721 Epoch 600: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.43 
2024-02-03 08:32:54,721 EPOCH 601
2024-02-03 08:32:59,419 Epoch 601: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.60 
2024-02-03 08:32:59,419 EPOCH 602
2024-02-03 08:33:03,916 Epoch 602: Total Training Recognition Loss 0.14  Total Training Translation Loss 8.27 
2024-02-03 08:33:03,917 EPOCH 603
2024-02-03 08:33:08,264 [Epoch: 603 Step: 00020500] Batch Recognition Loss:   0.006491 => Gls Tokens per Sec:     2356 || Batch Translation Loss:   0.216395 => Txt Tokens per Sec:     6524 || Lr: 0.000100
2024-02-03 08:33:08,527 Epoch 603: Total Training Recognition Loss 0.18  Total Training Translation Loss 7.36 
2024-02-03 08:33:08,527 EPOCH 604
2024-02-03 08:33:13,133 Epoch 604: Total Training Recognition Loss 0.14  Total Training Translation Loss 5.42 
2024-02-03 08:33:13,133 EPOCH 605
2024-02-03 08:33:18,170 Epoch 605: Total Training Recognition Loss 0.15  Total Training Translation Loss 4.20 
2024-02-03 08:33:18,170 EPOCH 606
2024-02-03 08:33:22,579 [Epoch: 606 Step: 00020600] Batch Recognition Loss:   0.002681 => Gls Tokens per Sec:     2121 || Batch Translation Loss:   0.166818 => Txt Tokens per Sec:     5970 || Lr: 0.000100
2024-02-03 08:33:22,988 Epoch 606: Total Training Recognition Loss 0.19  Total Training Translation Loss 6.94 
2024-02-03 08:33:22,988 EPOCH 607
2024-02-03 08:33:27,686 Epoch 607: Total Training Recognition Loss 0.22  Total Training Translation Loss 4.41 
2024-02-03 08:33:27,686 EPOCH 608
2024-02-03 08:33:32,140 Epoch 608: Total Training Recognition Loss 0.19  Total Training Translation Loss 3.46 
2024-02-03 08:33:32,140 EPOCH 609
2024-02-03 08:33:36,118 [Epoch: 609 Step: 00020700] Batch Recognition Loss:   0.012556 => Gls Tokens per Sec:     2190 || Batch Translation Loss:   0.096564 => Txt Tokens per Sec:     5971 || Lr: 0.000100
2024-02-03 08:33:37,083 Epoch 609: Total Training Recognition Loss 0.09  Total Training Translation Loss 2.28 
2024-02-03 08:33:37,083 EPOCH 610
2024-02-03 08:33:41,267 Epoch 610: Total Training Recognition Loss 0.12  Total Training Translation Loss 1.98 
2024-02-03 08:33:41,267 EPOCH 611
2024-02-03 08:33:46,073 Epoch 611: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.75 
2024-02-03 08:33:46,074 EPOCH 612
2024-02-03 08:33:49,598 [Epoch: 612 Step: 00020800] Batch Recognition Loss:   0.001388 => Gls Tokens per Sec:     2362 || Batch Translation Loss:   0.034887 => Txt Tokens per Sec:     6743 || Lr: 0.000100
2024-02-03 08:33:50,502 Epoch 612: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.42 
2024-02-03 08:33:50,502 EPOCH 613
2024-02-03 08:33:55,462 Epoch 613: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.71 
2024-02-03 08:33:55,463 EPOCH 614
2024-02-03 08:33:59,993 Epoch 614: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.13 
2024-02-03 08:33:59,994 EPOCH 615
2024-02-03 08:34:03,607 [Epoch: 615 Step: 00020900] Batch Recognition Loss:   0.001297 => Gls Tokens per Sec:     2057 || Batch Translation Loss:   0.024389 => Txt Tokens per Sec:     5906 || Lr: 0.000100
2024-02-03 08:34:04,664 Epoch 615: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.25 
2024-02-03 08:34:04,664 EPOCH 616
2024-02-03 08:34:08,827 Epoch 616: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.10 
2024-02-03 08:34:08,828 EPOCH 617
2024-02-03 08:34:13,736 Epoch 617: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.27 
2024-02-03 08:34:13,737 EPOCH 618
2024-02-03 08:34:16,229 [Epoch: 618 Step: 00021000] Batch Recognition Loss:   0.000556 => Gls Tokens per Sec:     2826 || Batch Translation Loss:   0.015374 => Txt Tokens per Sec:     7721 || Lr: 0.000100
2024-02-03 08:34:18,342 Epoch 618: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.10 
2024-02-03 08:34:18,342 EPOCH 619
2024-02-03 08:34:22,969 Epoch 619: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.16 
2024-02-03 08:34:22,969 EPOCH 620
2024-02-03 08:34:27,853 Epoch 620: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.87 
2024-02-03 08:34:27,853 EPOCH 621
2024-02-03 08:34:30,706 [Epoch: 621 Step: 00021100] Batch Recognition Loss:   0.008723 => Gls Tokens per Sec:     2156 || Batch Translation Loss:   0.015886 => Txt Tokens per Sec:     5955 || Lr: 0.000100
2024-02-03 08:34:32,698 Epoch 621: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.19 
2024-02-03 08:34:32,698 EPOCH 622
2024-02-03 08:34:37,393 Epoch 622: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.39 
2024-02-03 08:34:37,394 EPOCH 623
2024-02-03 08:34:41,949 Epoch 623: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.77 
2024-02-03 08:34:41,949 EPOCH 624
2024-02-03 08:34:43,930 [Epoch: 624 Step: 00021200] Batch Recognition Loss:   0.000580 => Gls Tokens per Sec:     2909 || Batch Translation Loss:   0.046896 => Txt Tokens per Sec:     7782 || Lr: 0.000100
2024-02-03 08:34:46,504 Epoch 624: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.90 
2024-02-03 08:34:46,505 EPOCH 625
2024-02-03 08:34:51,037 Epoch 625: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.01 
2024-02-03 08:34:51,037 EPOCH 626
2024-02-03 08:34:55,645 Epoch 626: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.97 
2024-02-03 08:34:55,646 EPOCH 627
2024-02-03 08:34:57,530 [Epoch: 627 Step: 00021300] Batch Recognition Loss:   0.000394 => Gls Tokens per Sec:     2721 || Batch Translation Loss:   0.041248 => Txt Tokens per Sec:     7253 || Lr: 0.000100
2024-02-03 08:35:00,237 Epoch 627: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.56 
2024-02-03 08:35:00,237 EPOCH 628
2024-02-03 08:35:05,056 Epoch 628: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.97 
2024-02-03 08:35:05,057 EPOCH 629
2024-02-03 08:35:09,523 Epoch 629: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.09 
2024-02-03 08:35:09,523 EPOCH 630
2024-02-03 08:35:11,154 [Epoch: 630 Step: 00021400] Batch Recognition Loss:   0.002577 => Gls Tokens per Sec:     2749 || Batch Translation Loss:   0.250820 => Txt Tokens per Sec:     7104 || Lr: 0.000100
2024-02-03 08:35:14,392 Epoch 630: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.49 
2024-02-03 08:35:14,392 EPOCH 631
2024-02-03 08:35:19,249 Epoch 631: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.25 
2024-02-03 08:35:19,250 EPOCH 632
2024-02-03 08:35:23,922 Epoch 632: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.03 
2024-02-03 08:35:23,923 EPOCH 633
2024-02-03 08:35:25,472 [Epoch: 633 Step: 00021500] Batch Recognition Loss:   0.002438 => Gls Tokens per Sec:     2481 || Batch Translation Loss:   0.120918 => Txt Tokens per Sec:     7430 || Lr: 0.000100
2024-02-03 08:35:28,304 Epoch 633: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.24 
2024-02-03 08:35:28,305 EPOCH 634
2024-02-03 08:35:33,167 Epoch 634: Total Training Recognition Loss 0.13  Total Training Translation Loss 4.05 
2024-02-03 08:35:33,168 EPOCH 635
2024-02-03 08:35:37,899 Epoch 635: Total Training Recognition Loss 0.14  Total Training Translation Loss 6.77 
2024-02-03 08:35:37,899 EPOCH 636
2024-02-03 08:35:39,444 [Epoch: 636 Step: 00021600] Batch Recognition Loss:   0.005917 => Gls Tokens per Sec:     2072 || Batch Translation Loss:   0.131487 => Txt Tokens per Sec:     6180 || Lr: 0.000100
2024-02-03 08:35:42,286 Epoch 636: Total Training Recognition Loss 0.25  Total Training Translation Loss 9.74 
2024-02-03 08:35:42,287 EPOCH 637
2024-02-03 08:35:47,039 Epoch 637: Total Training Recognition Loss 0.11  Total Training Translation Loss 3.88 
2024-02-03 08:35:47,040 EPOCH 638
2024-02-03 08:35:51,492 Epoch 638: Total Training Recognition Loss 0.13  Total Training Translation Loss 3.73 
2024-02-03 08:35:51,493 EPOCH 639
2024-02-03 08:35:52,536 [Epoch: 639 Step: 00021700] Batch Recognition Loss:   0.002331 => Gls Tokens per Sec:     2454 || Batch Translation Loss:   0.100726 => Txt Tokens per Sec:     6218 || Lr: 0.000100
2024-02-03 08:35:56,482 Epoch 639: Total Training Recognition Loss 0.10  Total Training Translation Loss 3.28 
2024-02-03 08:35:56,482 EPOCH 640
2024-02-03 08:36:00,910 Epoch 640: Total Training Recognition Loss 0.12  Total Training Translation Loss 3.15 
2024-02-03 08:36:00,911 EPOCH 641
2024-02-03 08:36:05,996 Epoch 641: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.98 
2024-02-03 08:36:05,997 EPOCH 642
2024-02-03 08:36:06,701 [Epoch: 642 Step: 00021800] Batch Recognition Loss:   0.001598 => Gls Tokens per Sec:     2731 || Batch Translation Loss:   0.045212 => Txt Tokens per Sec:     7514 || Lr: 0.000100
2024-02-03 08:36:10,597 Epoch 642: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.75 
2024-02-03 08:36:10,597 EPOCH 643
2024-02-03 08:36:15,507 Epoch 643: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.06 
2024-02-03 08:36:15,508 EPOCH 644
2024-02-03 08:36:20,038 Epoch 644: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.36 
2024-02-03 08:36:20,039 EPOCH 645
2024-02-03 08:36:20,537 [Epoch: 645 Step: 00021900] Batch Recognition Loss:   0.000558 => Gls Tokens per Sec:     2573 || Batch Translation Loss:   0.025598 => Txt Tokens per Sec:     7351 || Lr: 0.000100
2024-02-03 08:36:24,679 Epoch 645: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.17 
2024-02-03 08:36:24,679 EPOCH 646
2024-02-03 08:36:28,838 Epoch 646: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.11 
2024-02-03 08:36:28,838 EPOCH 647
2024-02-03 08:36:33,744 Epoch 647: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.22 
2024-02-03 08:36:33,745 EPOCH 648
2024-02-03 08:36:33,949 [Epoch: 648 Step: 00022000] Batch Recognition Loss:   0.000678 => Gls Tokens per Sec:     3153 || Batch Translation Loss:   0.019268 => Txt Tokens per Sec:     8310 || Lr: 0.000100
2024-02-03 08:36:42,622 Validation result at epoch 648, step    22000: duration: 8.6731s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 3.05361	Translation Loss: 89951.41406	PPL: 11415.06738
	Eval Metric: BLEU
	WER 3.89	(DEL: 0.00,	INS: 0.00,	SUB: 3.89)
	BLEU-4 0.45	(BLEU-1: 10.05,	BLEU-2: 2.76,	BLEU-3: 0.97,	BLEU-4: 0.45)
	CHRF 16.64	ROUGE 8.48
2024-02-03 08:36:42,624 Logging Recognition and Translation Outputs
2024-02-03 08:36:42,624 ========================================================================================================================
2024-02-03 08:36:42,624 Logging Sequence: 85_58.00
2024-02-03 08:36:42,624 	Gloss Reference :	A B+C+D+E
2024-02-03 08:36:42,624 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 08:36:42,625 	Gloss Alignment :	         
2024-02-03 08:36:42,625 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 08:36:42,625 	Text Reference  :	symonds has      been an    amazing player 
2024-02-03 08:36:42,625 	Text Hypothesis :	they    followed the  delhi police  tweeted
2024-02-03 08:36:42,625 	Text Alignment  :	S       S        S    S     S       S      
2024-02-03 08:36:42,626 ========================================================================================================================
2024-02-03 08:36:42,626 Logging Sequence: 71_120.00
2024-02-03 08:36:42,626 	Gloss Reference :	A B+C+D+E
2024-02-03 08:36:42,626 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 08:36:42,626 	Gloss Alignment :	         
2024-02-03 08:36:42,626 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 08:36:42,627 	Text Reference  :	on 3rd august 2022 kartikeya met his family and   posted a       heartwarming picture with his   mother
2024-02-03 08:36:42,628 	Text Hypothesis :	** *** ****** **** ********* *** and take   stern action against the          player  if   found guilty
2024-02-03 08:36:42,628 	Text Alignment  :	D  D   D      D    D         D   S   S      S     S      S       S            S       S    S     S     
2024-02-03 08:36:42,628 ========================================================================================================================
2024-02-03 08:36:42,628 Logging Sequence: 174_64.00
2024-02-03 08:36:42,628 	Gloss Reference :	A B+C+D+E
2024-02-03 08:36:42,628 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 08:36:42,628 	Gloss Alignment :	         
2024-02-03 08:36:42,629 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 08:36:42,630 	Text Reference  :	a total of 22 matches will be   played at  only 2     stadiums and    not all  over          india  
2024-02-03 08:36:42,630 	Text Hypothesis :	* ***** ** ** ******* **** they also   own abu  dhabi knight   riders in  uae' international matches
2024-02-03 08:36:42,630 	Text Alignment  :	D D     D  D  D       D    S    S      S   S    S     S        S      S   S    S             S      
2024-02-03 08:36:42,630 ========================================================================================================================
2024-02-03 08:36:42,630 Logging Sequence: 93_2.00
2024-02-03 08:36:42,631 	Gloss Reference :	A B+C+D+E  
2024-02-03 08:36:42,631 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-03 08:36:42,631 	Gloss Alignment :	  S        
2024-02-03 08:36:42,631 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 08:36:42,632 	Text Reference  :	wayne rooney was   an     amazing football player ** ***** ******
2024-02-03 08:36:42,632 	Text Hypothesis :	and   take   stern action against the      player if found guilty
2024-02-03 08:36:42,632 	Text Alignment  :	S     S      S     S      S       S               I  I     I     
2024-02-03 08:36:42,632 ========================================================================================================================
2024-02-03 08:36:42,632 Logging Sequence: 172_270.00
2024-02-03 08:36:42,632 	Gloss Reference :	A B+C+D+E
2024-02-03 08:36:42,633 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 08:36:42,633 	Gloss Alignment :	         
2024-02-03 08:36:42,633 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 08:36:42,634 	Text Reference  :	***** if   it  continues to   rain  today   the  tickets will    be         refunded
2024-02-03 08:36:42,634 	Text Hypothesis :	since then the wrestlers were quiet however they have    started protesting again   
2024-02-03 08:36:42,634 	Text Alignment  :	I     S    S   S         S    S     S       S    S       S       S          S       
2024-02-03 08:36:42,634 ========================================================================================================================
2024-02-03 08:36:47,469 Epoch 648: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.70 
2024-02-03 08:36:47,470 EPOCH 649
2024-02-03 08:36:52,350 Epoch 649: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.70 
2024-02-03 08:36:52,351 EPOCH 650
2024-02-03 08:36:56,877 [Epoch: 650 Step: 00022100] Batch Recognition Loss:   0.000691 => Gls Tokens per Sec:     2349 || Batch Translation Loss:   0.074968 => Txt Tokens per Sec:     6532 || Lr: 0.000100
2024-02-03 08:36:56,877 Epoch 650: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.48 
2024-02-03 08:36:56,877 EPOCH 651
2024-02-03 08:37:01,492 Epoch 651: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.55 
2024-02-03 08:37:01,493 EPOCH 652
2024-02-03 08:37:06,268 Epoch 652: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.69 
2024-02-03 08:37:06,269 EPOCH 653
2024-02-03 08:37:10,389 [Epoch: 653 Step: 00022200] Batch Recognition Loss:   0.002001 => Gls Tokens per Sec:     2426 || Batch Translation Loss:   0.021229 => Txt Tokens per Sec:     6736 || Lr: 0.000100
2024-02-03 08:37:10,737 Epoch 653: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.01 
2024-02-03 08:37:10,737 EPOCH 654
2024-02-03 08:37:15,655 Epoch 654: Total Training Recognition Loss 0.11  Total Training Translation Loss 1.91 
2024-02-03 08:37:15,655 EPOCH 655
2024-02-03 08:37:19,808 Epoch 655: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.59 
2024-02-03 08:37:19,809 EPOCH 656
2024-02-03 08:37:24,183 [Epoch: 656 Step: 00022300] Batch Recognition Loss:   0.000315 => Gls Tokens per Sec:     2196 || Batch Translation Loss:   0.079249 => Txt Tokens per Sec:     6132 || Lr: 0.000100
2024-02-03 08:37:24,795 Epoch 656: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.79 
2024-02-03 08:37:24,795 EPOCH 657
2024-02-03 08:37:29,337 Epoch 657: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.90 
2024-02-03 08:37:29,338 EPOCH 658
2024-02-03 08:37:34,110 Epoch 658: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.90 
2024-02-03 08:37:34,111 EPOCH 659
2024-02-03 08:37:37,415 [Epoch: 659 Step: 00022400] Batch Recognition Loss:   0.000399 => Gls Tokens per Sec:     2712 || Batch Translation Loss:   0.132175 => Txt Tokens per Sec:     7446 || Lr: 0.000100
2024-02-03 08:37:38,270 Epoch 659: Total Training Recognition Loss 0.10  Total Training Translation Loss 4.29 
2024-02-03 08:37:38,271 EPOCH 660
2024-02-03 08:37:43,160 Epoch 660: Total Training Recognition Loss 0.11  Total Training Translation Loss 7.99 
2024-02-03 08:37:43,160 EPOCH 661
2024-02-03 08:37:47,802 Epoch 661: Total Training Recognition Loss 0.18  Total Training Translation Loss 5.54 
2024-02-03 08:37:47,803 EPOCH 662
2024-02-03 08:37:51,470 [Epoch: 662 Step: 00022500] Batch Recognition Loss:   0.025713 => Gls Tokens per Sec:     2269 || Batch Translation Loss:   0.392457 => Txt Tokens per Sec:     6265 || Lr: 0.000100
2024-02-03 08:37:52,460 Epoch 662: Total Training Recognition Loss 0.11  Total Training Translation Loss 4.08 
2024-02-03 08:37:52,460 EPOCH 663
2024-02-03 08:37:57,261 Epoch 663: Total Training Recognition Loss 0.09  Total Training Translation Loss 3.46 
2024-02-03 08:37:57,262 EPOCH 664
2024-02-03 08:38:01,648 Epoch 664: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.19 
2024-02-03 08:38:01,648 EPOCH 665
2024-02-03 08:38:05,046 [Epoch: 665 Step: 00022600] Batch Recognition Loss:   0.001887 => Gls Tokens per Sec:     2188 || Batch Translation Loss:   0.133920 => Txt Tokens per Sec:     6118 || Lr: 0.000100
2024-02-03 08:38:06,668 Epoch 665: Total Training Recognition Loss 0.09  Total Training Translation Loss 9.74 
2024-02-03 08:38:06,669 EPOCH 666
2024-02-03 08:38:10,903 Epoch 666: Total Training Recognition Loss 0.19  Total Training Translation Loss 6.32 
2024-02-03 08:38:10,903 EPOCH 667
2024-02-03 08:38:15,907 Epoch 667: Total Training Recognition Loss 0.17  Total Training Translation Loss 2.92 
2024-02-03 08:38:15,908 EPOCH 668
2024-02-03 08:38:18,174 [Epoch: 668 Step: 00022700] Batch Recognition Loss:   0.001933 => Gls Tokens per Sec:     2998 || Batch Translation Loss:   0.042602 => Txt Tokens per Sec:     7831 || Lr: 0.000100
2024-02-03 08:38:20,409 Epoch 668: Total Training Recognition Loss 0.10  Total Training Translation Loss 2.99 
2024-02-03 08:38:20,409 EPOCH 669
2024-02-03 08:38:25,217 Epoch 669: Total Training Recognition Loss 0.10  Total Training Translation Loss 3.14 
2024-02-03 08:38:25,217 EPOCH 670
2024-02-03 08:38:29,385 Epoch 670: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.66 
2024-02-03 08:38:29,385 EPOCH 671
2024-02-03 08:38:32,306 [Epoch: 671 Step: 00022800] Batch Recognition Loss:   0.001667 => Gls Tokens per Sec:     2108 || Batch Translation Loss:   0.034471 => Txt Tokens per Sec:     5904 || Lr: 0.000100
2024-02-03 08:38:34,302 Epoch 671: Total Training Recognition Loss 0.08  Total Training Translation Loss 6.10 
2024-02-03 08:38:34,302 EPOCH 672
2024-02-03 08:38:39,061 Epoch 672: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.67 
2024-02-03 08:38:39,061 EPOCH 673
2024-02-03 08:38:43,440 Epoch 673: Total Training Recognition Loss 0.90  Total Training Translation Loss 4.46 
2024-02-03 08:38:43,440 EPOCH 674
2024-02-03 08:38:45,925 [Epoch: 674 Step: 00022900] Batch Recognition Loss:   0.016016 => Gls Tokens per Sec:     2219 || Batch Translation Loss:   0.040678 => Txt Tokens per Sec:     6164 || Lr: 0.000100
2024-02-03 08:38:48,246 Epoch 674: Total Training Recognition Loss 0.60  Total Training Translation Loss 2.56 
2024-02-03 08:38:48,246 EPOCH 675
2024-02-03 08:38:52,745 Epoch 675: Total Training Recognition Loss 0.15  Total Training Translation Loss 2.22 
2024-02-03 08:38:52,745 EPOCH 676
2024-02-03 08:38:57,660 Epoch 676: Total Training Recognition Loss 0.10  Total Training Translation Loss 1.50 
2024-02-03 08:38:57,661 EPOCH 677
2024-02-03 08:38:59,565 [Epoch: 677 Step: 00023000] Batch Recognition Loss:   0.000969 => Gls Tokens per Sec:     2559 || Batch Translation Loss:   0.038054 => Txt Tokens per Sec:     7395 || Lr: 0.000100
2024-02-03 08:39:01,795 Epoch 677: Total Training Recognition Loss 0.10  Total Training Translation Loss 1.20 
2024-02-03 08:39:01,796 EPOCH 678
2024-02-03 08:39:06,691 Epoch 678: Total Training Recognition Loss 0.08  Total Training Translation Loss 0.89 
2024-02-03 08:39:06,691 EPOCH 679
2024-02-03 08:39:10,919 Epoch 679: Total Training Recognition Loss 0.07  Total Training Translation Loss 0.92 
2024-02-03 08:39:10,919 EPOCH 680
2024-02-03 08:39:12,900 [Epoch: 680 Step: 00023100] Batch Recognition Loss:   0.002128 => Gls Tokens per Sec:     2264 || Batch Translation Loss:   0.067274 => Txt Tokens per Sec:     6338 || Lr: 0.000100
2024-02-03 08:39:15,935 Epoch 680: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.68 
2024-02-03 08:39:15,936 EPOCH 681
2024-02-03 08:39:20,677 Epoch 681: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.18 
2024-02-03 08:39:20,678 EPOCH 682
2024-02-03 08:39:25,151 Epoch 682: Total Training Recognition Loss 0.12  Total Training Translation Loss 1.32 
2024-02-03 08:39:25,151 EPOCH 683
2024-02-03 08:39:26,612 [Epoch: 683 Step: 00023200] Batch Recognition Loss:   0.000250 => Gls Tokens per Sec:     2631 || Batch Translation Loss:   0.020368 => Txt Tokens per Sec:     7196 || Lr: 0.000100
2024-02-03 08:39:29,544 Epoch 683: Total Training Recognition Loss 0.26  Total Training Translation Loss 0.99 
2024-02-03 08:39:29,545 EPOCH 684
2024-02-03 08:39:34,310 Epoch 684: Total Training Recognition Loss 0.30  Total Training Translation Loss 1.25 
2024-02-03 08:39:34,310 EPOCH 685
2024-02-03 08:39:39,046 Epoch 685: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.10 
2024-02-03 08:39:39,047 EPOCH 686
2024-02-03 08:39:40,114 [Epoch: 686 Step: 00023300] Batch Recognition Loss:   0.001059 => Gls Tokens per Sec:     3001 || Batch Translation Loss:   0.040226 => Txt Tokens per Sec:     7875 || Lr: 0.000100
2024-02-03 08:39:43,969 Epoch 686: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.10 
2024-02-03 08:39:43,970 EPOCH 687
2024-02-03 08:39:48,698 Epoch 687: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.42 
2024-02-03 08:39:48,699 EPOCH 688
2024-02-03 08:39:53,548 Epoch 688: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.41 
2024-02-03 08:39:53,549 EPOCH 689
2024-02-03 08:39:54,630 [Epoch: 689 Step: 00023400] Batch Recognition Loss:   0.000448 => Gls Tokens per Sec:     2371 || Batch Translation Loss:   0.020951 => Txt Tokens per Sec:     6577 || Lr: 0.000100
2024-02-03 08:39:58,129 Epoch 689: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.32 
2024-02-03 08:39:58,129 EPOCH 690
2024-02-03 08:40:03,043 Epoch 690: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.23 
2024-02-03 08:40:03,043 EPOCH 691
2024-02-03 08:40:07,350 Epoch 691: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.99 
2024-02-03 08:40:07,350 EPOCH 692
2024-02-03 08:40:08,192 [Epoch: 692 Step: 00023500] Batch Recognition Loss:   0.001409 => Gls Tokens per Sec:     1986 || Batch Translation Loss:   0.006478 => Txt Tokens per Sec:     5698 || Lr: 0.000100
2024-02-03 08:40:11,580 Epoch 692: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.87 
2024-02-03 08:40:11,580 EPOCH 693
2024-02-03 08:40:16,437 Epoch 693: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.78 
2024-02-03 08:40:16,438 EPOCH 694
2024-02-03 08:40:20,984 Epoch 694: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.97 
2024-02-03 08:40:20,985 EPOCH 695
2024-02-03 08:40:21,440 [Epoch: 695 Step: 00023600] Batch Recognition Loss:   0.000897 => Gls Tokens per Sec:     2817 || Batch Translation Loss:   0.019278 => Txt Tokens per Sec:     7394 || Lr: 0.000100
2024-02-03 08:40:25,643 Epoch 695: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.92 
2024-02-03 08:40:25,643 EPOCH 696
2024-02-03 08:40:30,148 Epoch 696: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.70 
2024-02-03 08:40:30,149 EPOCH 697
2024-02-03 08:40:34,844 Epoch 697: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.68 
2024-02-03 08:40:34,844 EPOCH 698
2024-02-03 08:40:35,174 [Epoch: 698 Step: 00023700] Batch Recognition Loss:   0.004135 => Gls Tokens per Sec:     1945 || Batch Translation Loss:   0.019383 => Txt Tokens per Sec:     6611 || Lr: 0.000100
2024-02-03 08:40:39,614 Epoch 698: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.02 
2024-02-03 08:40:39,614 EPOCH 699
2024-02-03 08:40:43,948 Epoch 699: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.98 
2024-02-03 08:40:43,949 EPOCH 700
2024-02-03 08:40:48,754 [Epoch: 700 Step: 00023800] Batch Recognition Loss:   0.000836 => Gls Tokens per Sec:     2213 || Batch Translation Loss:   0.164185 => Txt Tokens per Sec:     6155 || Lr: 0.000100
2024-02-03 08:40:48,755 Epoch 700: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.23 
2024-02-03 08:40:48,755 EPOCH 701
2024-02-03 08:40:53,548 Epoch 701: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.40 
2024-02-03 08:40:53,548 EPOCH 702
2024-02-03 08:40:57,602 Epoch 702: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.97 
2024-02-03 08:40:57,602 EPOCH 703
2024-02-03 08:41:02,295 [Epoch: 703 Step: 00023900] Batch Recognition Loss:   0.000919 => Gls Tokens per Sec:     2129 || Batch Translation Loss:   0.095755 => Txt Tokens per Sec:     5969 || Lr: 0.000100
2024-02-03 08:41:02,549 Epoch 703: Total Training Recognition Loss 0.07  Total Training Translation Loss 4.18 
2024-02-03 08:41:02,549 EPOCH 704
2024-02-03 08:41:06,830 Epoch 704: Total Training Recognition Loss 0.08  Total Training Translation Loss 6.49 
2024-02-03 08:41:06,830 EPOCH 705
2024-02-03 08:41:11,808 Epoch 705: Total Training Recognition Loss 0.10  Total Training Translation Loss 5.40 
2024-02-03 08:41:11,809 EPOCH 706
2024-02-03 08:41:15,854 [Epoch: 706 Step: 00024000] Batch Recognition Loss:   0.002866 => Gls Tokens per Sec:     2312 || Batch Translation Loss:   0.065304 => Txt Tokens per Sec:     6264 || Lr: 0.000100
2024-02-03 08:41:24,220 Validation result at epoch 706, step    24000: duration: 8.3660s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.54759	Translation Loss: 87818.52344	PPL: 9146.80176
	Eval Metric: BLEU
	WER 3.75	(DEL: 0.07,	INS: 0.00,	SUB: 3.68)
	BLEU-4 0.58	(BLEU-1: 9.57,	BLEU-2: 2.61,	BLEU-3: 1.10,	BLEU-4: 0.58)
	CHRF 16.75	ROUGE 7.88
2024-02-03 08:41:24,221 Logging Recognition and Translation Outputs
2024-02-03 08:41:24,221 ========================================================================================================================
2024-02-03 08:41:24,221 Logging Sequence: 118_46.00
2024-02-03 08:41:24,221 	Gloss Reference :	A B+C+D+E
2024-02-03 08:41:24,222 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 08:41:24,222 	Gloss Alignment :	         
2024-02-03 08:41:24,222 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 08:41:24,223 	Text Reference  :	since there was a tie the match went into the penalty shootout where each team gets a        chance to       score   5   times     
2024-02-03 08:41:24,223 	Text Hypothesis :	***** and   was * *** *** ***** **** **** *** ******* ******** ***** **** **** **** consoled by     denmark' captain and goalkeeper
2024-02-03 08:41:24,223 	Text Alignment  :	D     S         D D   D   D     D    D    D   D       D        D     D    D    D    S        S      S        S       S   S         
2024-02-03 08:41:24,223 ========================================================================================================================
2024-02-03 08:41:24,224 Logging Sequence: 115_59.00
2024-02-03 08:41:24,224 	Gloss Reference :	A B+C+D+E
2024-02-03 08:41:24,224 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 08:41:24,224 	Gloss Alignment :	         
2024-02-03 08:41:24,224 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 08:41:24,225 	Text Reference  :	** **** ***** **** ****** ****** ** she   now hosts   several cricket programmes
2024-02-03 08:41:24,225 	Text Hypothesis :	on 15th march 2021 bumrah posted an image on  twitter which   went    viral     
2024-02-03 08:41:24,225 	Text Alignment  :	I  I    I     I    I      I      I  S     S   S       S       S       S         
2024-02-03 08:41:24,225 ========================================================================================================================
2024-02-03 08:41:24,225 Logging Sequence: 128_172.00
2024-02-03 08:41:24,225 	Gloss Reference :	A B+C+D+E
2024-02-03 08:41:24,226 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 08:41:24,226 	Gloss Alignment :	         
2024-02-03 08:41:24,226 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 08:41:24,228 	Text Reference  :	the best test team can't be  decided   with just one       match    and it  should    be a    best of   three contest
2024-02-03 08:41:24,228 	Text Hypothesis :	*** **** **** when he    was diagnosed with an   inflammed appendix and was scheduled to stay in   just 111   balls  
2024-02-03 08:41:24,228 	Text Alignment  :	D   D    D    S    S     S   S              S    S         S            S   S         S  S    S    S    S     S      
2024-02-03 08:41:24,228 ========================================================================================================================
2024-02-03 08:41:24,228 Logging Sequence: 119_151.00
2024-02-03 08:41:24,228 	Gloss Reference :	A B+C+D+E
2024-02-03 08:41:24,228 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 08:41:24,229 	Gloss Alignment :	         
2024-02-03 08:41:24,229 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 08:41:24,229 	Text Reference  :	messi loved the  idea and placed the     order
2024-02-03 08:41:24,229 	Text Hypothesis :	***** ***** whoa what a   big    hearted man  
2024-02-03 08:41:24,229 	Text Alignment  :	D     D     S    S    S   S      S       S    
2024-02-03 08:41:24,230 ========================================================================================================================
2024-02-03 08:41:24,230 Logging Sequence: 61_196.00
2024-02-03 08:41:24,230 	Gloss Reference :	A B+C+D+E
2024-02-03 08:41:24,230 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 08:41:24,230 	Gloss Alignment :	         
2024-02-03 08:41:24,230 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 08:41:24,231 	Text Reference  :	another said people should refrain from   spreading this        video       who knows if      that         video    is       fake
2024-02-03 08:41:24,231 	Text Hypothesis :	******* **** ****** ****** ******* hotels and       restaurants association hra '“    gujarat spokesperson abhijeet deshmukh said
2024-02-03 08:41:24,232 	Text Alignment  :	D       D    D      D      D       S      S         S           S           S   S     S       S            S        S        S   
2024-02-03 08:41:24,232 ========================================================================================================================
2024-02-03 08:41:24,997 Epoch 706: Total Training Recognition Loss 0.10  Total Training Translation Loss 4.05 
2024-02-03 08:41:24,998 EPOCH 707
2024-02-03 08:41:29,975 Epoch 707: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.96 
2024-02-03 08:41:29,976 EPOCH 708
2024-02-03 08:41:34,857 Epoch 708: Total Training Recognition Loss 0.11  Total Training Translation Loss 9.45 
2024-02-03 08:41:34,858 EPOCH 709
2024-02-03 08:41:38,188 [Epoch: 709 Step: 00024100] Batch Recognition Loss:   0.003324 => Gls Tokens per Sec:     2615 || Batch Translation Loss:   0.085963 => Txt Tokens per Sec:     7151 || Lr: 0.000100
2024-02-03 08:41:39,114 Epoch 709: Total Training Recognition Loss 0.15  Total Training Translation Loss 5.65 
2024-02-03 08:41:39,115 EPOCH 710
2024-02-03 08:41:44,080 Epoch 710: Total Training Recognition Loss 0.09  Total Training Translation Loss 3.54 
2024-02-03 08:41:44,080 EPOCH 711
2024-02-03 08:41:48,582 Epoch 711: Total Training Recognition Loss 0.10  Total Training Translation Loss 3.01 
2024-02-03 08:41:48,582 EPOCH 712
2024-02-03 08:41:52,276 [Epoch: 712 Step: 00024200] Batch Recognition Loss:   0.000702 => Gls Tokens per Sec:     2186 || Batch Translation Loss:   0.030975 => Txt Tokens per Sec:     6163 || Lr: 0.000100
2024-02-03 08:41:53,347 Epoch 712: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.93 
2024-02-03 08:41:53,348 EPOCH 713
2024-02-03 08:41:57,454 Epoch 713: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.46 
2024-02-03 08:41:57,455 EPOCH 714
2024-02-03 08:42:02,449 Epoch 714: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.23 
2024-02-03 08:42:02,449 EPOCH 715
2024-02-03 08:42:05,060 [Epoch: 715 Step: 00024300] Batch Recognition Loss:   0.001352 => Gls Tokens per Sec:     2847 || Batch Translation Loss:   0.051451 => Txt Tokens per Sec:     7735 || Lr: 0.000100
2024-02-03 08:42:06,884 Epoch 715: Total Training Recognition Loss 0.06  Total Training Translation Loss 0.95 
2024-02-03 08:42:06,884 EPOCH 716
2024-02-03 08:42:11,723 Epoch 716: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.87 
2024-02-03 08:42:11,723 EPOCH 717
2024-02-03 08:42:16,598 Epoch 717: Total Training Recognition Loss 0.06  Total Training Translation Loss 0.88 
2024-02-03 08:42:16,598 EPOCH 718
2024-02-03 08:42:19,315 [Epoch: 718 Step: 00024400] Batch Recognition Loss:   0.001581 => Gls Tokens per Sec:     2499 || Batch Translation Loss:   0.033335 => Txt Tokens per Sec:     7213 || Lr: 0.000100
2024-02-03 08:42:20,876 Epoch 718: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.03 
2024-02-03 08:42:20,877 EPOCH 719
2024-02-03 08:42:25,627 Epoch 719: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.98 
2024-02-03 08:42:25,628 EPOCH 720
2024-02-03 08:42:30,032 Epoch 720: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.80 
2024-02-03 08:42:30,032 EPOCH 721
2024-02-03 08:42:33,348 [Epoch: 721 Step: 00024500] Batch Recognition Loss:   0.000360 => Gls Tokens per Sec:     1856 || Batch Translation Loss:   0.014580 => Txt Tokens per Sec:     5578 || Lr: 0.000100
2024-02-03 08:42:34,943 Epoch 721: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.97 
2024-02-03 08:42:34,944 EPOCH 722
2024-02-03 08:42:39,113 Epoch 722: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.01 
2024-02-03 08:42:39,113 EPOCH 723
2024-02-03 08:42:43,940 Epoch 723: Total Training Recognition Loss 0.06  Total Training Translation Loss 0.67 
2024-02-03 08:42:43,940 EPOCH 724
2024-02-03 08:42:46,208 [Epoch: 724 Step: 00024600] Batch Recognition Loss:   0.000856 => Gls Tokens per Sec:     2541 || Batch Translation Loss:   0.044763 => Txt Tokens per Sec:     7055 || Lr: 0.000100
2024-02-03 08:42:48,648 Epoch 724: Total Training Recognition Loss 0.08  Total Training Translation Loss 0.65 
2024-02-03 08:42:48,648 EPOCH 725
2024-02-03 08:42:53,291 Epoch 725: Total Training Recognition Loss 2.63  Total Training Translation Loss 0.84 
2024-02-03 08:42:53,291 EPOCH 726
2024-02-03 08:42:57,931 Epoch 726: Total Training Recognition Loss 28.57  Total Training Translation Loss 11.07 
2024-02-03 08:42:57,932 EPOCH 727
2024-02-03 08:42:59,934 [Epoch: 727 Step: 00024700] Batch Recognition Loss:   0.077015 => Gls Tokens per Sec:     2559 || Batch Translation Loss:   0.279298 => Txt Tokens per Sec:     7030 || Lr: 0.000100
2024-02-03 08:43:02,451 Epoch 727: Total Training Recognition Loss 8.61  Total Training Translation Loss 17.29 
2024-02-03 08:43:02,452 EPOCH 728
2024-02-03 08:43:07,309 Epoch 728: Total Training Recognition Loss 3.08  Total Training Translation Loss 7.53 
2024-02-03 08:43:07,309 EPOCH 729
2024-02-03 08:43:11,718 Epoch 729: Total Training Recognition Loss 0.68  Total Training Translation Loss 3.26 
2024-02-03 08:43:11,718 EPOCH 730
2024-02-03 08:43:13,999 [Epoch: 730 Step: 00024800] Batch Recognition Loss:   0.000516 => Gls Tokens per Sec:     1966 || Batch Translation Loss:   0.029633 => Txt Tokens per Sec:     5652 || Lr: 0.000100
2024-02-03 08:43:16,637 Epoch 730: Total Training Recognition Loss 0.32  Total Training Translation Loss 1.83 
2024-02-03 08:43:16,637 EPOCH 731
2024-02-03 08:43:20,833 Epoch 731: Total Training Recognition Loss 0.29  Total Training Translation Loss 1.60 
2024-02-03 08:43:20,834 EPOCH 732
2024-02-03 08:43:25,787 Epoch 732: Total Training Recognition Loss 0.18  Total Training Translation Loss 1.10 
2024-02-03 08:43:25,787 EPOCH 733
2024-02-03 08:43:27,257 [Epoch: 733 Step: 00024900] Batch Recognition Loss:   0.000870 => Gls Tokens per Sec:     2615 || Batch Translation Loss:   0.031263 => Txt Tokens per Sec:     7457 || Lr: 0.000100
2024-02-03 08:43:30,349 Epoch 733: Total Training Recognition Loss 0.11  Total Training Translation Loss 1.04 
2024-02-03 08:43:30,350 EPOCH 734
2024-02-03 08:43:35,047 Epoch 734: Total Training Recognition Loss 0.13  Total Training Translation Loss 0.96 
2024-02-03 08:43:35,048 EPOCH 735
2024-02-03 08:43:39,309 Epoch 735: Total Training Recognition Loss 0.10  Total Training Translation Loss 0.99 
2024-02-03 08:43:39,310 EPOCH 736
2024-02-03 08:43:40,599 [Epoch: 736 Step: 00025000] Batch Recognition Loss:   0.001408 => Gls Tokens per Sec:     2486 || Batch Translation Loss:   0.027686 => Txt Tokens per Sec:     6762 || Lr: 0.000100
2024-02-03 08:43:44,205 Epoch 736: Total Training Recognition Loss 0.07  Total Training Translation Loss 0.76 
2024-02-03 08:43:44,205 EPOCH 737
2024-02-03 08:43:48,745 Epoch 737: Total Training Recognition Loss 0.09  Total Training Translation Loss 0.93 
2024-02-03 08:43:48,746 EPOCH 738
2024-02-03 08:43:53,381 Epoch 738: Total Training Recognition Loss 0.10  Total Training Translation Loss 1.18 
2024-02-03 08:43:53,381 EPOCH 739
2024-02-03 08:43:54,268 [Epoch: 739 Step: 00025100] Batch Recognition Loss:   0.003791 => Gls Tokens per Sec:     2609 || Batch Translation Loss:   0.014760 => Txt Tokens per Sec:     7317 || Lr: 0.000100
2024-02-03 08:43:58,209 Epoch 739: Total Training Recognition Loss 0.08  Total Training Translation Loss 0.91 
2024-02-03 08:43:58,210 EPOCH 740
2024-02-03 08:44:02,549 Epoch 740: Total Training Recognition Loss 0.07  Total Training Translation Loss 0.76 
2024-02-03 08:44:02,549 EPOCH 741
2024-02-03 08:44:07,223 Epoch 741: Total Training Recognition Loss 0.07  Total Training Translation Loss 0.83 
2024-02-03 08:44:07,223 EPOCH 742
2024-02-03 08:44:07,775 [Epoch: 742 Step: 00025200] Batch Recognition Loss:   0.001844 => Gls Tokens per Sec:     3491 || Batch Translation Loss:   0.015837 => Txt Tokens per Sec:     8247 || Lr: 0.000100
2024-02-03 08:44:11,726 Epoch 742: Total Training Recognition Loss 0.06  Total Training Translation Loss 0.76 
2024-02-03 08:44:11,727 EPOCH 743
2024-02-03 08:44:16,616 Epoch 743: Total Training Recognition Loss 0.07  Total Training Translation Loss 0.78 
2024-02-03 08:44:16,617 EPOCH 744
2024-02-03 08:44:21,160 Epoch 744: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.85 
2024-02-03 08:44:21,160 EPOCH 745
2024-02-03 08:44:21,934 [Epoch: 745 Step: 00025300] Batch Recognition Loss:   0.004529 => Gls Tokens per Sec:     1656 || Batch Translation Loss:   0.051353 => Txt Tokens per Sec:     4296 || Lr: 0.000100
2024-02-03 08:44:26,104 Epoch 745: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.32 
2024-02-03 08:44:26,104 EPOCH 746
2024-02-03 08:44:30,183 Epoch 746: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.88 
2024-02-03 08:44:30,184 EPOCH 747
2024-02-03 08:44:35,150 Epoch 747: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.08 
2024-02-03 08:44:35,151 EPOCH 748
2024-02-03 08:44:35,412 [Epoch: 748 Step: 00025400] Batch Recognition Loss:   0.001943 => Gls Tokens per Sec:     2468 || Batch Translation Loss:   0.035991 => Txt Tokens per Sec:     6942 || Lr: 0.000100
2024-02-03 08:44:39,663 Epoch 748: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.30 
2024-02-03 08:44:39,664 EPOCH 749
2024-02-03 08:44:44,444 Epoch 749: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.93 
2024-02-03 08:44:44,445 EPOCH 750
2024-02-03 08:44:49,057 [Epoch: 750 Step: 00025500] Batch Recognition Loss:   0.000679 => Gls Tokens per Sec:     2305 || Batch Translation Loss:   0.047899 => Txt Tokens per Sec:     6411 || Lr: 0.000100
2024-02-03 08:44:49,057 Epoch 750: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.78 
2024-02-03 08:44:49,058 EPOCH 751
2024-02-03 08:44:53,573 Epoch 751: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.20 
2024-02-03 08:44:53,573 EPOCH 752
2024-02-03 08:44:58,393 Epoch 752: Total Training Recognition Loss 0.06  Total Training Translation Loss 8.74 
2024-02-03 08:44:58,394 EPOCH 753
2024-02-03 08:45:02,342 [Epoch: 753 Step: 00025600] Batch Recognition Loss:   0.014434 => Gls Tokens per Sec:     2531 || Batch Translation Loss:   0.108553 => Txt Tokens per Sec:     6924 || Lr: 0.000100
2024-02-03 08:45:02,684 Epoch 753: Total Training Recognition Loss 0.64  Total Training Translation Loss 6.71 
2024-02-03 08:45:02,684 EPOCH 754
2024-02-03 08:45:07,433 Epoch 754: Total Training Recognition Loss 0.45  Total Training Translation Loss 5.96 
2024-02-03 08:45:07,434 EPOCH 755
2024-02-03 08:45:11,940 Epoch 755: Total Training Recognition Loss 0.26  Total Training Translation Loss 6.32 
2024-02-03 08:45:11,941 EPOCH 756
2024-02-03 08:45:16,441 [Epoch: 756 Step: 00025700] Batch Recognition Loss:   0.001917 => Gls Tokens per Sec:     2134 || Batch Translation Loss:   0.089266 => Txt Tokens per Sec:     6029 || Lr: 0.000100
2024-02-03 08:45:16,856 Epoch 756: Total Training Recognition Loss 0.13  Total Training Translation Loss 3.16 
2024-02-03 08:45:16,857 EPOCH 757
2024-02-03 08:45:21,399 Epoch 757: Total Training Recognition Loss 0.09  Total Training Translation Loss 2.25 
2024-02-03 08:45:21,399 EPOCH 758
2024-02-03 08:45:26,125 Epoch 758: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.67 
2024-02-03 08:45:26,125 EPOCH 759
2024-02-03 08:45:29,820 [Epoch: 759 Step: 00025800] Batch Recognition Loss:   0.000392 => Gls Tokens per Sec:     2425 || Batch Translation Loss:   0.062153 => Txt Tokens per Sec:     6854 || Lr: 0.000100
2024-02-03 08:45:30,547 Epoch 759: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.35 
2024-02-03 08:45:30,548 EPOCH 760
2024-02-03 08:45:35,304 Epoch 760: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.32 
2024-02-03 08:45:35,305 EPOCH 761
2024-02-03 08:45:39,928 Epoch 761: Total Training Recognition Loss 0.06  Total Training Translation Loss 0.86 
2024-02-03 08:45:39,929 EPOCH 762
2024-02-03 08:45:43,407 [Epoch: 762 Step: 00025900] Batch Recognition Loss:   0.000997 => Gls Tokens per Sec:     2322 || Batch Translation Loss:   0.026722 => Txt Tokens per Sec:     6409 || Lr: 0.000100
2024-02-03 08:45:44,413 Epoch 762: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.92 
2024-02-03 08:45:44,413 EPOCH 763
2024-02-03 08:45:49,149 Epoch 763: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.80 
2024-02-03 08:45:49,150 EPOCH 764
2024-02-03 08:45:53,626 Epoch 764: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.75 
2024-02-03 08:45:53,626 EPOCH 765
2024-02-03 08:45:57,430 [Epoch: 765 Step: 00026000] Batch Recognition Loss:   0.000252 => Gls Tokens per Sec:     1954 || Batch Translation Loss:   0.018981 => Txt Tokens per Sec:     5436 || Lr: 0.000100
2024-02-03 08:46:06,291 Validation result at epoch 765, step    26000: duration: 8.8596s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.98708	Translation Loss: 88561.75000	PPL: 9880.84961
	Eval Metric: BLEU
	WER 3.39	(DEL: 0.07,	INS: 0.00,	SUB: 3.32)
	BLEU-4 0.77	(BLEU-1: 10.89,	BLEU-2: 3.56,	BLEU-3: 1.53,	BLEU-4: 0.77)
	CHRF 17.56	ROUGE 9.05
2024-02-03 08:46:06,292 Logging Recognition and Translation Outputs
2024-02-03 08:46:06,292 ========================================================================================================================
2024-02-03 08:46:06,292 Logging Sequence: 140_34.00
2024-02-03 08:46:06,293 	Gloss Reference :	A B+C+D+E
2024-02-03 08:46:06,293 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 08:46:06,293 	Gloss Alignment :	         
2024-02-03 08:46:06,293 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 08:46:06,295 	Text Reference  :	pant made his debut in international cricket  in   february 2017       against england india won the match because  of  pant 
2024-02-03 08:46:06,295 	Text Hypothesis :	pant is   a   sense of growing       optimism over an       impressive outing  for     india in  the ***** upcoming afc match
2024-02-03 08:46:06,295 	Text Alignment  :	     S    S   S     S  S             S        S    S        S          S       S             S       D     S        S   S    
2024-02-03 08:46:06,295 ========================================================================================================================
2024-02-03 08:46:06,296 Logging Sequence: 142_111.00
2024-02-03 08:46:06,296 	Gloss Reference :	A B+C+D+E
2024-02-03 08:46:06,296 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 08:46:06,296 	Gloss Alignment :	         
2024-02-03 08:46:06,296 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 08:46:06,297 	Text Reference  :	*** oh   god how embarassing will the young boy be able to pay such a huge  amount
2024-02-03 08:46:06,297 	Text Hypothesis :	and that is  how *********** **** *** ***** *** ** **** ** *** **** * india proud 
2024-02-03 08:46:06,297 	Text Alignment  :	I   S    S       D           D    D   D     D   D  D    D  D   D    D S     S     
2024-02-03 08:46:06,297 ========================================================================================================================
2024-02-03 08:46:06,298 Logging Sequence: 135_9.00
2024-02-03 08:46:06,298 	Gloss Reference :	A B+C+D+E
2024-02-03 08:46:06,298 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 08:46:06,298 	Gloss Alignment :	         
2024-02-03 08:46:06,298 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 08:46:06,299 	Text Reference  :	flew to the tokyo olympics won a silver medal in javelin throw 
2024-02-03 08:46:06,299 	Text Hypothesis :	**** ** *** ***** ******** *** * ****** that  is roughly 385000
2024-02-03 08:46:06,299 	Text Alignment  :	D    D  D   D     D        D   D D      S     S  S       S     
2024-02-03 08:46:06,299 ========================================================================================================================
2024-02-03 08:46:06,299 Logging Sequence: 118_444.00
2024-02-03 08:46:06,299 	Gloss Reference :	A B+C+D+E
2024-02-03 08:46:06,299 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 08:46:06,299 	Gloss Alignment :	         
2024-02-03 08:46:06,299 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 08:46:06,300 	Text Reference  :	******** what    a  match but cheers to argentina for winning the        match
2024-02-03 08:46:06,300 	Text Hypothesis :	everyone thought we will  be  hooked to ********* *** their   respective teams
2024-02-03 08:46:06,301 	Text Alignment  :	I        S       S  S     S   S         D         D   S       S          S    
2024-02-03 08:46:06,301 ========================================================================================================================
2024-02-03 08:46:06,301 Logging Sequence: 52_36.00
2024-02-03 08:46:06,301 	Gloss Reference :	A B+C+D+E    
2024-02-03 08:46:06,301 	Gloss Hypothesis:	A B+C+D+E+D+E
2024-02-03 08:46:06,301 	Gloss Alignment :	  S          
2024-02-03 08:46:06,301 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 08:46:06,302 	Text Reference  :	** ** * ****** *********** recently dhoni was travellin on  an  indigo flight 
2024-02-03 08:46:06,302 	Text Hypothesis :	he is a strong competition and      a     few days      for the ipl    seasons
2024-02-03 08:46:06,302 	Text Alignment  :	I  I  I I      I           S        S     S   S         S   S   S      S      
2024-02-03 08:46:06,302 ========================================================================================================================
2024-02-03 08:46:07,523 Epoch 765: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.80 
2024-02-03 08:46:07,523 EPOCH 766
2024-02-03 08:46:12,470 Epoch 766: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.81 
2024-02-03 08:46:12,470 EPOCH 767
2024-02-03 08:46:16,981 Epoch 767: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.03 
2024-02-03 08:46:16,982 EPOCH 768
2024-02-03 08:46:19,807 [Epoch: 768 Step: 00026100] Batch Recognition Loss:   0.000902 => Gls Tokens per Sec:     2492 || Batch Translation Loss:   0.011124 => Txt Tokens per Sec:     6829 || Lr: 0.000100
2024-02-03 08:46:21,576 Epoch 768: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.22 
2024-02-03 08:46:21,577 EPOCH 769
2024-02-03 08:46:26,171 Epoch 769: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.45 
2024-02-03 08:46:26,171 EPOCH 770
2024-02-03 08:46:30,993 Epoch 770: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.17 
2024-02-03 08:46:30,994 EPOCH 771
2024-02-03 08:46:33,793 [Epoch: 771 Step: 00026200] Batch Recognition Loss:   0.000794 => Gls Tokens per Sec:     2197 || Batch Translation Loss:   0.044014 => Txt Tokens per Sec:     6337 || Lr: 0.000100
2024-02-03 08:46:35,436 Epoch 771: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.76 
2024-02-03 08:46:35,436 EPOCH 772
2024-02-03 08:46:40,335 Epoch 772: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.50 
2024-02-03 08:46:40,336 EPOCH 773
2024-02-03 08:46:44,502 Epoch 773: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.49 
2024-02-03 08:46:44,502 EPOCH 774
2024-02-03 08:46:47,164 [Epoch: 774 Step: 00026300] Batch Recognition Loss:   0.000339 => Gls Tokens per Sec:     2070 || Batch Translation Loss:   0.080195 => Txt Tokens per Sec:     5812 || Lr: 0.000100
2024-02-03 08:46:49,493 Epoch 774: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.20 
2024-02-03 08:46:49,493 EPOCH 775
2024-02-03 08:46:53,979 Epoch 775: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.09 
2024-02-03 08:46:53,979 EPOCH 776
2024-02-03 08:46:58,664 Epoch 776: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.14 
2024-02-03 08:46:58,664 EPOCH 777
2024-02-03 08:47:00,733 [Epoch: 777 Step: 00026400] Batch Recognition Loss:   0.000414 => Gls Tokens per Sec:     2355 || Batch Translation Loss:   0.039687 => Txt Tokens per Sec:     6922 || Lr: 0.000100
2024-02-03 08:47:02,876 Epoch 777: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.62 
2024-02-03 08:47:02,877 EPOCH 778
2024-02-03 08:47:07,736 Epoch 778: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.19 
2024-02-03 08:47:07,736 EPOCH 779
2024-02-03 08:47:12,541 Epoch 779: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.92 
2024-02-03 08:47:12,542 EPOCH 780
2024-02-03 08:47:14,568 [Epoch: 780 Step: 00026500] Batch Recognition Loss:   0.001394 => Gls Tokens per Sec:     2089 || Batch Translation Loss:   0.037292 => Txt Tokens per Sec:     6157 || Lr: 0.000100
2024-02-03 08:47:16,922 Epoch 780: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.00 
2024-02-03 08:47:16,923 EPOCH 781
2024-02-03 08:47:21,782 Epoch 781: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.01 
2024-02-03 08:47:21,783 EPOCH 782
2024-02-03 08:47:26,254 Epoch 782: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.21 
2024-02-03 08:47:26,254 EPOCH 783
2024-02-03 08:47:28,013 [Epoch: 783 Step: 00026600] Batch Recognition Loss:   0.000199 => Gls Tokens per Sec:     2184 || Batch Translation Loss:   0.039041 => Txt Tokens per Sec:     6099 || Lr: 0.000100
2024-02-03 08:47:31,177 Epoch 783: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.47 
2024-02-03 08:47:31,177 EPOCH 784
2024-02-03 08:47:35,754 Epoch 784: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.98 
2024-02-03 08:47:35,754 EPOCH 785
2024-02-03 08:47:40,407 Epoch 785: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.21 
2024-02-03 08:47:40,408 EPOCH 786
2024-02-03 08:47:41,461 [Epoch: 786 Step: 00026700] Batch Recognition Loss:   0.000539 => Gls Tokens per Sec:     2805 || Batch Translation Loss:   0.089104 => Txt Tokens per Sec:     7495 || Lr: 0.000100
2024-02-03 08:47:44,845 Epoch 786: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.77 
2024-02-03 08:47:44,845 EPOCH 787
2024-02-03 08:47:49,579 Epoch 787: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.34 
2024-02-03 08:47:49,579 EPOCH 788
2024-02-03 08:47:54,265 Epoch 788: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.18 
2024-02-03 08:47:54,266 EPOCH 789
2024-02-03 08:47:55,237 [Epoch: 789 Step: 00026800] Batch Recognition Loss:   0.000655 => Gls Tokens per Sec:     2639 || Batch Translation Loss:   0.028576 => Txt Tokens per Sec:     7584 || Lr: 0.000100
2024-02-03 08:47:58,715 Epoch 789: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.37 
2024-02-03 08:47:58,715 EPOCH 790
2024-02-03 08:48:03,599 Epoch 790: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.50 
2024-02-03 08:48:03,599 EPOCH 791
2024-02-03 08:48:07,857 Epoch 791: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.65 
2024-02-03 08:48:07,857 EPOCH 792
2024-02-03 08:48:08,445 [Epoch: 792 Step: 00026900] Batch Recognition Loss:   0.000259 => Gls Tokens per Sec:     3274 || Batch Translation Loss:   0.015700 => Txt Tokens per Sec:     8602 || Lr: 0.000100
2024-02-03 08:48:12,731 Epoch 792: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.47 
2024-02-03 08:48:12,732 EPOCH 793
2024-02-03 08:48:17,166 Epoch 793: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.35 
2024-02-03 08:48:17,166 EPOCH 794
2024-02-03 08:48:22,075 Epoch 794: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.27 
2024-02-03 08:48:22,075 EPOCH 795
2024-02-03 08:48:22,445 [Epoch: 795 Step: 00027000] Batch Recognition Loss:   0.000763 => Gls Tokens per Sec:     3469 || Batch Translation Loss:   0.044159 => Txt Tokens per Sec:     8412 || Lr: 0.000100
2024-02-03 08:48:26,143 Epoch 795: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.43 
2024-02-03 08:48:26,143 EPOCH 796
2024-02-03 08:48:31,016 Epoch 796: Total Training Recognition Loss 0.09  Total Training Translation Loss 3.11 
2024-02-03 08:48:31,017 EPOCH 797
2024-02-03 08:48:35,546 Epoch 797: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.47 
2024-02-03 08:48:35,547 EPOCH 798
2024-02-03 08:48:35,789 [Epoch: 798 Step: 00027100] Batch Recognition Loss:   0.000777 => Gls Tokens per Sec:     2656 || Batch Translation Loss:   0.047358 => Txt Tokens per Sec:     6755 || Lr: 0.000100
2024-02-03 08:48:40,416 Epoch 798: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.61 
2024-02-03 08:48:40,417 EPOCH 799
2024-02-03 08:48:45,046 Epoch 799: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.26 
2024-02-03 08:48:45,047 EPOCH 800
2024-02-03 08:48:49,597 [Epoch: 800 Step: 00027200] Batch Recognition Loss:   0.000427 => Gls Tokens per Sec:     2337 || Batch Translation Loss:   0.023109 => Txt Tokens per Sec:     6499 || Lr: 0.000100
2024-02-03 08:48:49,598 Epoch 800: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.59 
2024-02-03 08:48:49,598 EPOCH 801
2024-02-03 08:48:53,989 Epoch 801: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.72 
2024-02-03 08:48:53,989 EPOCH 802
2024-02-03 08:48:58,724 Epoch 802: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.95 
2024-02-03 08:48:58,724 EPOCH 803
2024-02-03 08:49:03,218 [Epoch: 803 Step: 00027300] Batch Recognition Loss:   0.005402 => Gls Tokens per Sec:     2223 || Batch Translation Loss:   0.041303 => Txt Tokens per Sec:     6231 || Lr: 0.000100
2024-02-03 08:49:03,426 Epoch 803: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.69 
2024-02-03 08:49:03,426 EPOCH 804
2024-02-03 08:49:07,924 Epoch 804: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.11 
2024-02-03 08:49:07,924 EPOCH 805
2024-02-03 08:49:12,837 Epoch 805: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.14 
2024-02-03 08:49:12,838 EPOCH 806
2024-02-03 08:49:16,585 [Epoch: 806 Step: 00027400] Batch Recognition Loss:   0.000538 => Gls Tokens per Sec:     2496 || Batch Translation Loss:   0.020285 => Txt Tokens per Sec:     6975 || Lr: 0.000100
2024-02-03 08:49:17,088 Epoch 806: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.99 
2024-02-03 08:49:17,088 EPOCH 807
2024-02-03 08:49:21,944 Epoch 807: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.70 
2024-02-03 08:49:21,945 EPOCH 808
2024-02-03 08:49:26,403 Epoch 808: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.78 
2024-02-03 08:49:26,403 EPOCH 809
2024-02-03 08:49:30,500 [Epoch: 809 Step: 00027500] Batch Recognition Loss:   0.001060 => Gls Tokens per Sec:     2188 || Batch Translation Loss:   0.050592 => Txt Tokens per Sec:     6112 || Lr: 0.000100
2024-02-03 08:49:31,272 Epoch 809: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.24 
2024-02-03 08:49:31,272 EPOCH 810
2024-02-03 08:49:35,901 Epoch 810: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.04 
2024-02-03 08:49:35,902 EPOCH 811
2024-02-03 08:49:40,562 Epoch 811: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.77 
2024-02-03 08:49:40,562 EPOCH 812
2024-02-03 08:49:43,807 [Epoch: 812 Step: 00027600] Batch Recognition Loss:   0.000666 => Gls Tokens per Sec:     2489 || Batch Translation Loss:   0.256310 => Txt Tokens per Sec:     6812 || Lr: 0.000100
2024-02-03 08:49:45,016 Epoch 812: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.61 
2024-02-03 08:49:45,017 EPOCH 813
2024-02-03 08:49:49,710 Epoch 813: Total Training Recognition Loss 0.08  Total Training Translation Loss 10.62 
2024-02-03 08:49:49,710 EPOCH 814
2024-02-03 08:49:53,907 Epoch 814: Total Training Recognition Loss 0.19  Total Training Translation Loss 12.77 
2024-02-03 08:49:53,907 EPOCH 815
2024-02-03 08:49:57,288 [Epoch: 815 Step: 00027700] Batch Recognition Loss:   0.001618 => Gls Tokens per Sec:     2198 || Batch Translation Loss:   0.127990 => Txt Tokens per Sec:     6037 || Lr: 0.000100
2024-02-03 08:49:58,795 Epoch 815: Total Training Recognition Loss 0.32  Total Training Translation Loss 6.23 
2024-02-03 08:49:58,796 EPOCH 816
2024-02-03 08:50:03,536 Epoch 816: Total Training Recognition Loss 0.11  Total Training Translation Loss 2.77 
2024-02-03 08:50:03,537 EPOCH 817
2024-02-03 08:50:08,041 Epoch 817: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.46 
2024-02-03 08:50:08,041 EPOCH 818
2024-02-03 08:50:11,049 [Epoch: 818 Step: 00027800] Batch Recognition Loss:   0.000871 => Gls Tokens per Sec:     2259 || Batch Translation Loss:   0.020916 => Txt Tokens per Sec:     6441 || Lr: 0.000100
2024-02-03 08:50:12,799 Epoch 818: Total Training Recognition Loss 0.09  Total Training Translation Loss 1.41 
2024-02-03 08:50:12,799 EPOCH 819
2024-02-03 08:50:17,456 Epoch 819: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.52 
2024-02-03 08:50:17,456 EPOCH 820
2024-02-03 08:50:22,146 Epoch 820: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.00 
2024-02-03 08:50:22,146 EPOCH 821
2024-02-03 08:50:24,659 [Epoch: 821 Step: 00027900] Batch Recognition Loss:   0.009306 => Gls Tokens per Sec:     2548 || Batch Translation Loss:   0.053360 => Txt Tokens per Sec:     7122 || Lr: 0.000100
2024-02-03 08:50:26,524 Epoch 821: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.91 
2024-02-03 08:50:26,524 EPOCH 822
2024-02-03 08:50:31,331 Epoch 822: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.20 
2024-02-03 08:50:31,332 EPOCH 823
2024-02-03 08:50:36,009 Epoch 823: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.65 
2024-02-03 08:50:36,009 EPOCH 824
2024-02-03 08:50:38,006 [Epoch: 824 Step: 00028000] Batch Recognition Loss:   0.001272 => Gls Tokens per Sec:     2763 || Batch Translation Loss:   0.010815 => Txt Tokens per Sec:     7246 || Lr: 0.000100
2024-02-03 08:50:46,567 Validation result at epoch 824, step    28000: duration: 8.5614s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.61372	Translation Loss: 88790.33594	PPL: 10118.24512
	Eval Metric: BLEU
	WER 3.11	(DEL: 0.00,	INS: 0.00,	SUB: 3.11)
	BLEU-4 0.66	(BLEU-1: 10.65,	BLEU-2: 3.25,	BLEU-3: 1.28,	BLEU-4: 0.66)
	CHRF 17.23	ROUGE 8.94
2024-02-03 08:50:46,568 Logging Recognition and Translation Outputs
2024-02-03 08:50:46,568 ========================================================================================================================
2024-02-03 08:50:46,568 Logging Sequence: 79_33.00
2024-02-03 08:50:46,569 	Gloss Reference :	A B+C+D+E
2024-02-03 08:50:46,569 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 08:50:46,569 	Gloss Alignment :	         
2024-02-03 08:50:46,569 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 08:50:46,570 	Text Reference  :	result in cancellation and other hassles this is  something the bcci does not   want     and    is  taking all precautions
2024-02-03 08:50:46,570 	Text Hypothesis :	****** ** ************ *** ***** ******* **** due to        the **** **** covid pandemic people are going  to  defeat     
2024-02-03 08:50:46,571 	Text Alignment  :	D      D  D            D   D     D       D    S   S             D    D    S     S        S      S   S      S   S          
2024-02-03 08:50:46,571 ========================================================================================================================
2024-02-03 08:50:46,571 Logging Sequence: 113_75.00
2024-02-03 08:50:46,571 	Gloss Reference :	A B+C+D+E
2024-02-03 08:50:46,571 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 08:50:46,571 	Gloss Alignment :	         
2024-02-03 08:50:46,571 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 08:50:46,573 	Text Reference  :	mirza responded 'œpeople can    date a     person for       years but    break up          as soon   as         they get   married  
2024-02-03 08:50:46,573 	Text Hypothesis :	***** ********* ******** within 72   hours singh  responsed and   denied all   allegations of sexual harassment of   women wrestlers
2024-02-03 08:50:46,573 	Text Alignment  :	D     D         D        S      S    S     S      S         S     S      S     S           S  S      S          S    S     S        
2024-02-03 08:50:46,574 ========================================================================================================================
2024-02-03 08:50:46,574 Logging Sequence: 96_165.00
2024-02-03 08:50:46,574 	Gloss Reference :	A B+C+D+E
2024-02-03 08:50:46,574 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 08:50:46,574 	Gloss Alignment :	         
2024-02-03 08:50:46,574 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 08:50:46,575 	Text Reference  :	7 runs were needed from 6 balls and     india   had lost    a      wicket
2024-02-03 08:50:46,575 	Text Hypothesis :	* **** **** ****** **** * the   winning skipper was finally called upon  
2024-02-03 08:50:46,575 	Text Alignment  :	D D    D    D      D    D S     S       S       S   S       S      S     
2024-02-03 08:50:46,575 ========================================================================================================================
2024-02-03 08:50:46,576 Logging Sequence: 91_133.00
2024-02-03 08:50:46,576 	Gloss Reference :	A B+C+D+E
2024-02-03 08:50:46,576 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 08:50:46,576 	Gloss Alignment :	         
2024-02-03 08:50:46,576 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 08:50:46,577 	Text Reference  :	all the   umpires in the tournament were    from     bangalesh'
2024-02-03 08:50:46,577 	Text Hypothesis :	*** there needs   to be  a          neutral umpiring system    
2024-02-03 08:50:46,577 	Text Alignment  :	D   S     S       S  S   S          S       S        S         
2024-02-03 08:50:46,577 ========================================================================================================================
2024-02-03 08:50:46,577 Logging Sequence: 105_139.00
2024-02-03 08:50:46,578 	Gloss Reference :	A B+C+D+E
2024-02-03 08:50:46,578 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 08:50:46,578 	Gloss Alignment :	         
2024-02-03 08:50:46,578 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 08:50:46,578 	Text Reference  :	and now he has finally achieved his  dream  by defeating carlsen       
2024-02-03 08:50:46,579 	Text Hypothesis :	*** *** ** *** ******* symonds  also played 14 t20       internationals
2024-02-03 08:50:46,579 	Text Alignment  :	D   D   D  D   D       S        S    S      S  S         S             
2024-02-03 08:50:46,579 ========================================================================================================================
2024-02-03 08:50:49,259 Epoch 824: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.53 
2024-02-03 08:50:49,260 EPOCH 825
2024-02-03 08:50:54,181 Epoch 825: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.54 
2024-02-03 08:50:54,182 EPOCH 826
2024-02-03 08:50:58,620 Epoch 826: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.54 
2024-02-03 08:50:58,620 EPOCH 827
2024-02-03 08:51:01,418 [Epoch: 827 Step: 00028100] Batch Recognition Loss:   0.000378 => Gls Tokens per Sec:     1741 || Batch Translation Loss:   0.013775 => Txt Tokens per Sec:     5163 || Lr: 0.000100
2024-02-03 08:51:03,574 Epoch 827: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.74 
2024-02-03 08:51:03,574 EPOCH 828
2024-02-03 08:51:07,693 Epoch 828: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.66 
2024-02-03 08:51:07,693 EPOCH 829
2024-02-03 08:51:12,686 Epoch 829: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.07 
2024-02-03 08:51:12,687 EPOCH 830
2024-02-03 08:51:14,233 [Epoch: 830 Step: 00028200] Batch Recognition Loss:   0.002870 => Gls Tokens per Sec:     2736 || Batch Translation Loss:   0.017582 => Txt Tokens per Sec:     7596 || Lr: 0.000100
2024-02-03 08:51:17,256 Epoch 830: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.21 
2024-02-03 08:51:17,257 EPOCH 831
2024-02-03 08:51:21,995 Epoch 831: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.35 
2024-02-03 08:51:21,995 EPOCH 832
2024-02-03 08:51:26,742 Epoch 832: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.27 
2024-02-03 08:51:26,743 EPOCH 833
2024-02-03 08:51:28,050 [Epoch: 833 Step: 00028300] Batch Recognition Loss:   0.000699 => Gls Tokens per Sec:     2936 || Batch Translation Loss:   0.050757 => Txt Tokens per Sec:     7901 || Lr: 0.000100
2024-02-03 08:51:31,242 Epoch 833: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.79 
2024-02-03 08:51:31,242 EPOCH 834
2024-02-03 08:51:35,847 Epoch 834: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.06 
2024-02-03 08:51:35,848 EPOCH 835
2024-02-03 08:51:40,398 Epoch 835: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.81 
2024-02-03 08:51:40,398 EPOCH 836
2024-02-03 08:51:41,537 [Epoch: 836 Step: 00028400] Batch Recognition Loss:   0.000479 => Gls Tokens per Sec:     2812 || Batch Translation Loss:   0.050330 => Txt Tokens per Sec:     7336 || Lr: 0.000100
2024-02-03 08:51:45,224 Epoch 836: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.53 
2024-02-03 08:51:45,225 EPOCH 837
2024-02-03 08:51:49,679 Epoch 837: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.30 
2024-02-03 08:51:49,680 EPOCH 838
2024-02-03 08:51:54,581 Epoch 838: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.58 
2024-02-03 08:51:54,582 EPOCH 839
2024-02-03 08:51:55,453 [Epoch: 839 Step: 00028500] Batch Recognition Loss:   0.000403 => Gls Tokens per Sec:     2941 || Batch Translation Loss:   0.388643 => Txt Tokens per Sec:     8019 || Lr: 0.000100
2024-02-03 08:51:58,745 Epoch 839: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.48 
2024-02-03 08:51:58,745 EPOCH 840
2024-02-03 08:52:03,733 Epoch 840: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.95 
2024-02-03 08:52:03,734 EPOCH 841
2024-02-03 08:52:07,905 Epoch 841: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.95 
2024-02-03 08:52:07,905 EPOCH 842
2024-02-03 08:52:08,986 [Epoch: 842 Step: 00028600] Batch Recognition Loss:   0.001052 => Gls Tokens per Sec:     1779 || Batch Translation Loss:   0.048689 => Txt Tokens per Sec:     5438 || Lr: 0.000100
2024-02-03 08:52:11,965 Epoch 842: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.42 
2024-02-03 08:52:11,965 EPOCH 843
2024-02-03 08:52:16,996 Epoch 843: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.81 
2024-02-03 08:52:16,996 EPOCH 844
2024-02-03 08:52:21,777 Epoch 844: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.68 
2024-02-03 08:52:21,778 EPOCH 845
2024-02-03 08:52:22,296 [Epoch: 845 Step: 00028700] Batch Recognition Loss:   0.005257 => Gls Tokens per Sec:     2476 || Batch Translation Loss:   0.098733 => Txt Tokens per Sec:     6654 || Lr: 0.000100
2024-02-03 08:52:26,676 Epoch 845: Total Training Recognition Loss 0.05  Total Training Translation Loss 4.38 
2024-02-03 08:52:26,677 EPOCH 846
2024-02-03 08:52:30,947 Epoch 846: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.62 
2024-02-03 08:52:30,947 EPOCH 847
2024-02-03 08:52:36,008 Epoch 847: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.05 
2024-02-03 08:52:36,009 EPOCH 848
2024-02-03 08:52:36,276 [Epoch: 848 Step: 00028800] Batch Recognition Loss:   0.008802 => Gls Tokens per Sec:     2406 || Batch Translation Loss:   0.067383 => Txt Tokens per Sec:     7109 || Lr: 0.000100
2024-02-03 08:52:40,961 Epoch 848: Total Training Recognition Loss 0.10  Total Training Translation Loss 2.22 
2024-02-03 08:52:40,962 EPOCH 849
2024-02-03 08:52:45,649 Epoch 849: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.64 
2024-02-03 08:52:45,650 EPOCH 850
2024-02-03 08:52:50,586 [Epoch: 850 Step: 00028900] Batch Recognition Loss:   0.000651 => Gls Tokens per Sec:     2154 || Batch Translation Loss:   0.031730 => Txt Tokens per Sec:     5990 || Lr: 0.000100
2024-02-03 08:52:50,587 Epoch 850: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.37 
2024-02-03 08:52:50,587 EPOCH 851
2024-02-03 08:52:55,596 Epoch 851: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.36 
2024-02-03 08:52:55,596 EPOCH 852
2024-02-03 08:53:00,222 Epoch 852: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.26 
2024-02-03 08:53:00,223 EPOCH 853
2024-02-03 08:53:05,031 [Epoch: 853 Step: 00029000] Batch Recognition Loss:   0.001940 => Gls Tokens per Sec:     2078 || Batch Translation Loss:   0.048695 => Txt Tokens per Sec:     5804 || Lr: 0.000100
2024-02-03 08:53:05,244 Epoch 853: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.75 
2024-02-03 08:53:05,244 EPOCH 854
2024-02-03 08:53:09,929 Epoch 854: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.60 
2024-02-03 08:53:09,930 EPOCH 855
2024-02-03 08:53:14,799 Epoch 855: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.01 
2024-02-03 08:53:14,800 EPOCH 856
2024-02-03 08:53:19,018 [Epoch: 856 Step: 00029100] Batch Recognition Loss:   0.001978 => Gls Tokens per Sec:     2217 || Batch Translation Loss:   0.095871 => Txt Tokens per Sec:     6153 || Lr: 0.000100
2024-02-03 08:53:19,522 Epoch 856: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.81 
2024-02-03 08:53:19,522 EPOCH 857
2024-02-03 08:53:24,415 Epoch 857: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.57 
2024-02-03 08:53:24,416 EPOCH 858
2024-02-03 08:53:29,534 Epoch 858: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.77 
2024-02-03 08:53:29,534 EPOCH 859
2024-02-03 08:53:33,324 [Epoch: 859 Step: 00029200] Batch Recognition Loss:   0.002396 => Gls Tokens per Sec:     2298 || Batch Translation Loss:   0.229217 => Txt Tokens per Sec:     6241 || Lr: 0.000100
2024-02-03 08:53:34,215 Epoch 859: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.47 
2024-02-03 08:53:34,215 EPOCH 860
2024-02-03 08:53:38,921 Epoch 860: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.79 
2024-02-03 08:53:38,922 EPOCH 861
2024-02-03 08:53:43,408 Epoch 861: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.33 
2024-02-03 08:53:43,408 EPOCH 862
2024-02-03 08:53:47,342 [Epoch: 862 Step: 00029300] Batch Recognition Loss:   0.000658 => Gls Tokens per Sec:     2052 || Batch Translation Loss:   0.077071 => Txt Tokens per Sec:     5827 || Lr: 0.000100
2024-02-03 08:53:48,323 Epoch 862: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.23 
2024-02-03 08:53:48,323 EPOCH 863
2024-02-03 08:53:52,758 Epoch 863: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.84 
2024-02-03 08:53:52,758 EPOCH 864
2024-02-03 08:53:57,610 Epoch 864: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.33 
2024-02-03 08:53:57,610 EPOCH 865
2024-02-03 08:54:00,230 [Epoch: 865 Step: 00029400] Batch Recognition Loss:   0.000435 => Gls Tokens per Sec:     2837 || Batch Translation Loss:   0.021203 => Txt Tokens per Sec:     7710 || Lr: 0.000100
2024-02-03 08:54:01,918 Epoch 865: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.60 
2024-02-03 08:54:01,919 EPOCH 866
2024-02-03 08:54:06,818 Epoch 866: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.19 
2024-02-03 08:54:06,818 EPOCH 867
2024-02-03 08:54:11,336 Epoch 867: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.93 
2024-02-03 08:54:11,337 EPOCH 868
2024-02-03 08:54:14,303 [Epoch: 868 Step: 00029500] Batch Recognition Loss:   0.000822 => Gls Tokens per Sec:     2375 || Batch Translation Loss:   0.018702 => Txt Tokens per Sec:     6484 || Lr: 0.000100
2024-02-03 08:54:15,926 Epoch 868: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.05 
2024-02-03 08:54:15,926 EPOCH 869
2024-02-03 08:54:20,777 Epoch 869: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.09 
2024-02-03 08:54:20,778 EPOCH 870
2024-02-03 08:54:25,706 Epoch 870: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.48 
2024-02-03 08:54:25,707 EPOCH 871
2024-02-03 08:54:28,547 [Epoch: 871 Step: 00029600] Batch Recognition Loss:   0.000523 => Gls Tokens per Sec:     2254 || Batch Translation Loss:   0.019021 => Txt Tokens per Sec:     6316 || Lr: 0.000100
2024-02-03 08:54:30,477 Epoch 871: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.11 
2024-02-03 08:54:30,478 EPOCH 872
2024-02-03 08:54:34,984 Epoch 872: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.75 
2024-02-03 08:54:34,984 EPOCH 873
2024-02-03 08:54:39,860 Epoch 873: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.81 
2024-02-03 08:54:39,861 EPOCH 874
2024-02-03 08:54:42,165 [Epoch: 874 Step: 00029700] Batch Recognition Loss:   0.002169 => Gls Tokens per Sec:     2392 || Batch Translation Loss:   0.055495 => Txt Tokens per Sec:     6525 || Lr: 0.000100
2024-02-03 08:54:44,283 Epoch 874: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.17 
2024-02-03 08:54:44,283 EPOCH 875
2024-02-03 08:54:49,231 Epoch 875: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.75 
2024-02-03 08:54:49,231 EPOCH 876
2024-02-03 08:54:53,448 Epoch 876: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.54 
2024-02-03 08:54:53,449 EPOCH 877
2024-02-03 08:54:55,993 [Epoch: 877 Step: 00029800] Batch Recognition Loss:   0.000792 => Gls Tokens per Sec:     1915 || Batch Translation Loss:   0.008941 => Txt Tokens per Sec:     5374 || Lr: 0.000100
2024-02-03 08:54:58,498 Epoch 877: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.55 
2024-02-03 08:54:58,499 EPOCH 878
2024-02-03 08:55:02,935 Epoch 878: Total Training Recognition Loss 0.31  Total Training Translation Loss 0.68 
2024-02-03 08:55:02,936 EPOCH 879
2024-02-03 08:55:07,844 Epoch 879: Total Training Recognition Loss 6.14  Total Training Translation Loss 2.73 
2024-02-03 08:55:07,845 EPOCH 880
2024-02-03 08:55:09,214 [Epoch: 880 Step: 00029900] Batch Recognition Loss:   0.006671 => Gls Tokens per Sec:     3275 || Batch Translation Loss:   0.277835 => Txt Tokens per Sec:     8473 || Lr: 0.000100
2024-02-03 08:55:12,452 Epoch 880: Total Training Recognition Loss 47.00  Total Training Translation Loss 12.73 
2024-02-03 08:55:12,452 EPOCH 881
2024-02-03 08:55:17,090 Epoch 881: Total Training Recognition Loss 8.32  Total Training Translation Loss 17.66 
2024-02-03 08:55:17,090 EPOCH 882
2024-02-03 08:55:21,354 Epoch 882: Total Training Recognition Loss 5.57  Total Training Translation Loss 6.26 
2024-02-03 08:55:21,355 EPOCH 883
2024-02-03 08:55:22,958 [Epoch: 883 Step: 00030000] Batch Recognition Loss:   0.000581 => Gls Tokens per Sec:     2399 || Batch Translation Loss:   0.071019 => Txt Tokens per Sec:     6567 || Lr: 0.000100
2024-02-03 08:55:31,642 Validation result at epoch 883, step    30000: duration: 8.6833s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 3.35786	Translation Loss: 88644.47656	PPL: 9966.10840
	Eval Metric: BLEU
	WER 3.61	(DEL: 0.21,	INS: 0.00,	SUB: 3.39)
	BLEU-4 0.53	(BLEU-1: 9.54,	BLEU-2: 2.69,	BLEU-3: 1.02,	BLEU-4: 0.53)
	CHRF 16.72	ROUGE 7.76
2024-02-03 08:55:31,643 Logging Recognition and Translation Outputs
2024-02-03 08:55:31,643 ========================================================================================================================
2024-02-03 08:55:31,643 Logging Sequence: 177_16.00
2024-02-03 08:55:31,644 	Gloss Reference :	A B+C+D+E
2024-02-03 08:55:31,644 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 08:55:31,644 	Gloss Alignment :	         
2024-02-03 08:55:31,644 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 08:55:31,645 	Text Reference  :	***** ** ***** *** ***** **** ************ *** **** and severely injured 2   others  
2024-02-03 08:55:31,645 	Text Hypothesis :	after 28 years the world test championship was held in  london   from    the olympics
2024-02-03 08:55:31,645 	Text Alignment  :	I     I  I     I   I     I    I            I   I    S   S        S       S   S       
2024-02-03 08:55:31,645 ========================================================================================================================
2024-02-03 08:55:31,645 Logging Sequence: 165_200.00
2024-02-03 08:55:31,645 	Gloss Reference :	A B+C+D+E
2024-02-03 08:55:31,645 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 08:55:31,646 	Gloss Alignment :	         
2024-02-03 08:55:31,646 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 08:55:31,647 	Text Reference  :	you may be wondering what bag remember in   2011 when  india won  the  world cup   
2024-02-03 08:55:31,647 	Text Hypothesis :	*** *** ** ********* **** *** now      they gave birth to    baby girl named aliana
2024-02-03 08:55:31,647 	Text Alignment  :	D   D   D  D         D    D   S        S    S    S     S     S    S    S     S     
2024-02-03 08:55:31,647 ========================================================================================================================
2024-02-03 08:55:31,647 Logging Sequence: 125_119.00
2024-02-03 08:55:31,647 	Gloss Reference :	A B+C+D+E
2024-02-03 08:55:31,647 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 08:55:31,648 	Gloss Alignment :	         
2024-02-03 08:55:31,648 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 08:55:31,648 	Text Reference  :	people must not post such baseless comments on   social media 
2024-02-03 08:55:31,648 	Text Hypothesis :	****** **** *** **** i    have     retired  from all    player
2024-02-03 08:55:31,649 	Text Alignment  :	D      D    D   D    S    S        S        S    S      S     
2024-02-03 08:55:31,649 ========================================================================================================================
2024-02-03 08:55:31,649 Logging Sequence: 53_25.00
2024-02-03 08:55:31,649 	Gloss Reference :	A B+C+D+E
2024-02-03 08:55:31,649 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 08:55:31,650 	Gloss Alignment :	         
2024-02-03 08:55:31,650 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 08:55:31,651 	Text Reference  :	there is absolute chaos on     the         country as     residents are trying to        flee     
2024-02-03 08:55:31,651 	Text Hypothesis :	***** it was      a     strong competition and     people including the ipl    technical committee
2024-02-03 08:55:31,651 	Text Alignment  :	D     S  S        S     S      S           S       S      S         S   S      S         S        
2024-02-03 08:55:31,651 ========================================================================================================================
2024-02-03 08:55:31,651 Logging Sequence: 70_81.00
2024-02-03 08:55:31,652 	Gloss Reference :	A B+C+D+E
2024-02-03 08:55:31,652 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 08:55:31,652 	Gloss Alignment :	         
2024-02-03 08:55:31,652 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 08:55:31,653 	Text Reference  :	euro 2020 has   many sponsors but          coca-cola is the     official sponsor for     the   tournament
2024-02-03 08:55:31,653 	Text Hypothesis :	**** **** after her  no       weightlifter secured   an olympic victory  finally mirabai chanu won       
2024-02-03 08:55:31,653 	Text Alignment  :	D    D    S     S    S        S            S         S  S       S        S       S       S     S         
2024-02-03 08:55:31,654 ========================================================================================================================
2024-02-03 08:55:34,991 Epoch 883: Total Training Recognition Loss 1.56  Total Training Translation Loss 2.87 
2024-02-03 08:55:34,991 EPOCH 884
2024-02-03 08:55:39,863 Epoch 884: Total Training Recognition Loss 0.43  Total Training Translation Loss 2.45 
2024-02-03 08:55:39,864 EPOCH 885
2024-02-03 08:55:44,287 Epoch 885: Total Training Recognition Loss 0.24  Total Training Translation Loss 2.11 
2024-02-03 08:55:44,287 EPOCH 886
2024-02-03 08:55:45,350 [Epoch: 886 Step: 00030100] Batch Recognition Loss:   0.000822 => Gls Tokens per Sec:     2778 || Batch Translation Loss:   0.024355 => Txt Tokens per Sec:     7276 || Lr: 0.000100
2024-02-03 08:55:48,383 Epoch 886: Total Training Recognition Loss 0.16  Total Training Translation Loss 1.62 
2024-02-03 08:55:48,384 EPOCH 887
2024-02-03 08:55:53,042 Epoch 887: Total Training Recognition Loss 0.15  Total Training Translation Loss 1.21 
2024-02-03 08:55:53,043 EPOCH 888
2024-02-03 08:55:57,717 Epoch 888: Total Training Recognition Loss 0.13  Total Training Translation Loss 1.00 
2024-02-03 08:55:57,718 EPOCH 889
2024-02-03 08:55:59,145 [Epoch: 889 Step: 00030200] Batch Recognition Loss:   0.001834 => Gls Tokens per Sec:     1795 || Batch Translation Loss:   0.027815 => Txt Tokens per Sec:     5292 || Lr: 0.000100
2024-02-03 08:56:02,556 Epoch 889: Total Training Recognition Loss 0.13  Total Training Translation Loss 1.16 
2024-02-03 08:56:02,557 EPOCH 890
2024-02-03 08:56:07,007 Epoch 890: Total Training Recognition Loss 0.10  Total Training Translation Loss 1.09 
2024-02-03 08:56:07,007 EPOCH 891
2024-02-03 08:56:11,719 Epoch 891: Total Training Recognition Loss 0.12  Total Training Translation Loss 0.97 
2024-02-03 08:56:11,719 EPOCH 892
2024-02-03 08:56:12,301 [Epoch: 892 Step: 00030300] Batch Recognition Loss:   0.000515 => Gls Tokens per Sec:     3305 || Batch Translation Loss:   0.019069 => Txt Tokens per Sec:     8470 || Lr: 0.000100
2024-02-03 08:56:16,368 Epoch 892: Total Training Recognition Loss 0.08  Total Training Translation Loss 0.81 
2024-02-03 08:56:16,369 EPOCH 893
2024-02-03 08:56:21,387 Epoch 893: Total Training Recognition Loss 0.07  Total Training Translation Loss 0.80 
2024-02-03 08:56:21,387 EPOCH 894
2024-02-03 08:56:25,989 Epoch 894: Total Training Recognition Loss 0.08  Total Training Translation Loss 0.73 
2024-02-03 08:56:25,990 EPOCH 895
2024-02-03 08:56:26,546 [Epoch: 895 Step: 00030400] Batch Recognition Loss:   0.004726 => Gls Tokens per Sec:     1856 || Batch Translation Loss:   0.009303 => Txt Tokens per Sec:     5216 || Lr: 0.000100
2024-02-03 08:56:30,694 Epoch 895: Total Training Recognition Loss 0.06  Total Training Translation Loss 0.71 
2024-02-03 08:56:30,694 EPOCH 896
2024-02-03 08:56:35,498 Epoch 896: Total Training Recognition Loss 0.06  Total Training Translation Loss 0.77 
2024-02-03 08:56:35,498 EPOCH 897
2024-02-03 08:56:39,979 Epoch 897: Total Training Recognition Loss 0.09  Total Training Translation Loss 0.76 
2024-02-03 08:56:39,980 EPOCH 898
2024-02-03 08:56:40,286 [Epoch: 898 Step: 00030500] Batch Recognition Loss:   0.000398 => Gls Tokens per Sec:     2098 || Batch Translation Loss:   0.023797 => Txt Tokens per Sec:     5997 || Lr: 0.000100
2024-02-03 08:56:44,923 Epoch 898: Total Training Recognition Loss 0.06  Total Training Translation Loss 0.83 
2024-02-03 08:56:44,924 EPOCH 899
2024-02-03 08:56:49,120 Epoch 899: Total Training Recognition Loss 0.06  Total Training Translation Loss 0.70 
2024-02-03 08:56:49,120 EPOCH 900
2024-02-03 08:56:54,110 [Epoch: 900 Step: 00030600] Batch Recognition Loss:   0.009341 => Gls Tokens per Sec:     2132 || Batch Translation Loss:   0.062482 => Txt Tokens per Sec:     5928 || Lr: 0.000100
2024-02-03 08:56:54,110 Epoch 900: Total Training Recognition Loss 0.08  Total Training Translation Loss 0.64 
2024-02-03 08:56:54,110 EPOCH 901
2024-02-03 08:56:58,562 Epoch 901: Total Training Recognition Loss 0.07  Total Training Translation Loss 0.61 
2024-02-03 08:56:58,563 EPOCH 902
2024-02-03 08:57:03,303 Epoch 902: Total Training Recognition Loss 0.07  Total Training Translation Loss 0.91 
2024-02-03 08:57:03,303 EPOCH 903
2024-02-03 08:57:07,691 [Epoch: 903 Step: 00030700] Batch Recognition Loss:   0.001136 => Gls Tokens per Sec:     2278 || Batch Translation Loss:   0.025468 => Txt Tokens per Sec:     6304 || Lr: 0.000100
2024-02-03 08:57:07,983 Epoch 903: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.09 
2024-02-03 08:57:07,983 EPOCH 904
2024-02-03 08:57:12,512 Epoch 904: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.02 
2024-02-03 08:57:12,512 EPOCH 905
2024-02-03 08:57:17,098 Epoch 905: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.52 
2024-02-03 08:57:17,098 EPOCH 906
2024-02-03 08:57:21,268 [Epoch: 906 Step: 00030800] Batch Recognition Loss:   0.000599 => Gls Tokens per Sec:     2303 || Batch Translation Loss:   0.042322 => Txt Tokens per Sec:     6542 || Lr: 0.000100
2024-02-03 08:57:21,694 Epoch 906: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.59 
2024-02-03 08:57:21,694 EPOCH 907
2024-02-03 08:57:26,564 Epoch 907: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.01 
2024-02-03 08:57:26,565 EPOCH 908
2024-02-03 08:57:30,973 Epoch 908: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.30 
2024-02-03 08:57:30,973 EPOCH 909
2024-02-03 08:57:35,057 [Epoch: 909 Step: 00030900] Batch Recognition Loss:   0.004179 => Gls Tokens per Sec:     2194 || Batch Translation Loss:   0.025977 => Txt Tokens per Sec:     5992 || Lr: 0.000100
2024-02-03 08:57:35,934 Epoch 909: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.65 
2024-02-03 08:57:35,934 EPOCH 910
2024-02-03 08:57:40,074 Epoch 910: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.91 
2024-02-03 08:57:40,075 EPOCH 911
2024-02-03 08:57:45,032 Epoch 911: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.85 
2024-02-03 08:57:45,033 EPOCH 912
2024-02-03 08:57:48,080 [Epoch: 912 Step: 00031000] Batch Recognition Loss:   0.001890 => Gls Tokens per Sec:     2650 || Batch Translation Loss:   0.018928 => Txt Tokens per Sec:     7273 || Lr: 0.000100
2024-02-03 08:57:49,486 Epoch 912: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.20 
2024-02-03 08:57:49,487 EPOCH 913
2024-02-03 08:57:54,242 Epoch 913: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.87 
2024-02-03 08:57:54,242 EPOCH 914
2024-02-03 08:57:58,596 Epoch 914: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.97 
2024-02-03 08:57:58,597 EPOCH 915
2024-02-03 08:58:02,144 [Epoch: 915 Step: 00031100] Batch Recognition Loss:   0.000155 => Gls Tokens per Sec:     2095 || Batch Translation Loss:   0.017205 => Txt Tokens per Sec:     5882 || Lr: 0.000100
2024-02-03 08:58:03,424 Epoch 915: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.75 
2024-02-03 08:58:03,424 EPOCH 916
2024-02-03 08:58:07,997 Epoch 916: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.85 
2024-02-03 08:58:07,998 EPOCH 917
2024-02-03 08:58:12,612 Epoch 917: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.94 
2024-02-03 08:58:12,612 EPOCH 918
2024-02-03 08:58:15,281 [Epoch: 918 Step: 00031200] Batch Recognition Loss:   0.000098 => Gls Tokens per Sec:     2546 || Batch Translation Loss:   0.011305 => Txt Tokens per Sec:     6601 || Lr: 0.000100
2024-02-03 08:58:17,500 Epoch 918: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.96 
2024-02-03 08:58:17,501 EPOCH 919
2024-02-03 08:58:21,770 Epoch 919: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.88 
2024-02-03 08:58:21,770 EPOCH 920
2024-02-03 08:58:26,553 Epoch 920: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.54 
2024-02-03 08:58:26,554 EPOCH 921
2024-02-03 08:58:28,978 [Epoch: 921 Step: 00031300] Batch Recognition Loss:   0.000736 => Gls Tokens per Sec:     2642 || Batch Translation Loss:   0.042415 => Txt Tokens per Sec:     7149 || Lr: 0.000100
2024-02-03 08:58:30,941 Epoch 921: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.30 
2024-02-03 08:58:30,942 EPOCH 922
2024-02-03 08:58:35,857 Epoch 922: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.55 
2024-02-03 08:58:35,857 EPOCH 923
2024-02-03 08:58:40,052 Epoch 923: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.59 
2024-02-03 08:58:40,052 EPOCH 924
2024-02-03 08:58:42,102 [Epoch: 924 Step: 00031400] Batch Recognition Loss:   0.000205 => Gls Tokens per Sec:     2811 || Batch Translation Loss:   0.029186 => Txt Tokens per Sec:     7747 || Lr: 0.000100
2024-02-03 08:58:44,090 Epoch 924: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.12 
2024-02-03 08:58:44,090 EPOCH 925
2024-02-03 08:58:48,298 Epoch 925: Total Training Recognition Loss 0.05  Total Training Translation Loss 4.00 
2024-02-03 08:58:48,298 EPOCH 926
2024-02-03 08:58:53,258 Epoch 926: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.59 
2024-02-03 08:58:53,259 EPOCH 927
2024-02-03 08:58:55,779 [Epoch: 927 Step: 00031500] Batch Recognition Loss:   0.001063 => Gls Tokens per Sec:     1934 || Batch Translation Loss:   0.060042 => Txt Tokens per Sec:     5426 || Lr: 0.000100
2024-02-03 08:58:58,218 Epoch 927: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.44 
2024-02-03 08:58:58,218 EPOCH 928
2024-02-03 08:59:02,810 Epoch 928: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.77 
2024-02-03 08:59:02,810 EPOCH 929
2024-02-03 08:59:07,684 Epoch 929: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.25 
2024-02-03 08:59:07,685 EPOCH 930
2024-02-03 08:59:09,844 [Epoch: 930 Step: 00031600] Batch Recognition Loss:   0.000859 => Gls Tokens per Sec:     1960 || Batch Translation Loss:   0.070457 => Txt Tokens per Sec:     5689 || Lr: 0.000100
2024-02-03 08:59:12,626 Epoch 930: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.41 
2024-02-03 08:59:12,627 EPOCH 931
2024-02-03 08:59:17,327 Epoch 931: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.99 
2024-02-03 08:59:17,327 EPOCH 932
2024-02-03 08:59:21,849 Epoch 932: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.31 
2024-02-03 08:59:21,849 EPOCH 933
2024-02-03 08:59:23,564 [Epoch: 933 Step: 00031700] Batch Recognition Loss:   0.001339 => Gls Tokens per Sec:     2095 || Batch Translation Loss:   0.058790 => Txt Tokens per Sec:     5961 || Lr: 0.000100
2024-02-03 08:59:26,461 Epoch 933: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.63 
2024-02-03 08:59:26,461 EPOCH 934
2024-02-03 08:59:31,249 Epoch 934: Total Training Recognition Loss 0.09  Total Training Translation Loss 7.34 
2024-02-03 08:59:31,249 EPOCH 935
2024-02-03 08:59:35,628 Epoch 935: Total Training Recognition Loss 0.12  Total Training Translation Loss 7.22 
2024-02-03 08:59:35,628 EPOCH 936
2024-02-03 08:59:37,051 [Epoch: 936 Step: 00031800] Batch Recognition Loss:   0.001292 => Gls Tokens per Sec:     2252 || Batch Translation Loss:   0.117160 => Txt Tokens per Sec:     6179 || Lr: 0.000100
2024-02-03 08:59:40,544 Epoch 936: Total Training Recognition Loss 1.35  Total Training Translation Loss 4.28 
2024-02-03 08:59:40,545 EPOCH 937
2024-02-03 08:59:44,746 Epoch 937: Total Training Recognition Loss 1.94  Total Training Translation Loss 2.85 
2024-02-03 08:59:44,746 EPOCH 938
2024-02-03 08:59:49,856 Epoch 938: Total Training Recognition Loss 1.07  Total Training Translation Loss 1.41 
2024-02-03 08:59:49,857 EPOCH 939
2024-02-03 08:59:50,792 [Epoch: 939 Step: 00031900] Batch Recognition Loss:   0.001260 => Gls Tokens per Sec:     2472 || Batch Translation Loss:   0.034970 => Txt Tokens per Sec:     6616 || Lr: 0.000100
2024-02-03 08:59:54,324 Epoch 939: Total Training Recognition Loss 0.18  Total Training Translation Loss 1.44 
2024-02-03 08:59:54,324 EPOCH 940
2024-02-03 08:59:59,158 Epoch 940: Total Training Recognition Loss 0.61  Total Training Translation Loss 0.93 
2024-02-03 08:59:59,159 EPOCH 941
2024-02-03 09:00:03,546 Epoch 941: Total Training Recognition Loss 0.77  Total Training Translation Loss 1.04 
2024-02-03 09:00:03,546 EPOCH 942
2024-02-03 09:00:04,614 [Epoch: 942 Step: 00032000] Batch Recognition Loss:   0.000969 => Gls Tokens per Sec:     1798 || Batch Translation Loss:   0.019876 => Txt Tokens per Sec:     5675 || Lr: 0.000100
2024-02-03 09:00:13,195 Validation result at epoch 942, step    32000: duration: 8.5795s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 3.18633	Translation Loss: 89072.00000	PPL: 10418.62598
	Eval Metric: BLEU
	WER 3.75	(DEL: 0.00,	INS: 0.00,	SUB: 3.75)
	BLEU-4 0.79	(BLEU-1: 10.31,	BLEU-2: 3.35,	BLEU-3: 1.49,	BLEU-4: 0.79)
	CHRF 17.10	ROUGE 8.69
2024-02-03 09:00:13,196 Logging Recognition and Translation Outputs
2024-02-03 09:00:13,196 ========================================================================================================================
2024-02-03 09:00:13,196 Logging Sequence: 73_57.00
2024-02-03 09:00:13,197 	Gloss Reference :	A B+C+D+E
2024-02-03 09:00:13,197 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 09:00:13,197 	Gloss Alignment :	         
2024-02-03 09:00:13,197 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 09:00:13,199 	Text Reference  :	he made this announcement on  instagram with    a     photo  which  had    raina standing along with  the chefs at restaurant
2024-02-03 09:00:13,199 	Text Hypothesis :	** **** **** ************ ipl team      chennai super kings' player suresh raina has      a     video of  rs    1  crore     
2024-02-03 09:00:13,199 	Text Alignment  :	D  D    D    D            S   S         S       S     S      S      S            S        S     S     S   S     S  S         
2024-02-03 09:00:13,199 ========================================================================================================================
2024-02-03 09:00:13,199 Logging Sequence: 177_16.00
2024-02-03 09:00:13,200 	Gloss Reference :	A B+C+D+E
2024-02-03 09:00:13,200 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 09:00:13,200 	Gloss Alignment :	         
2024-02-03 09:00:13,200 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 09:00:13,201 	Text Reference  :	***** ** ***** *** ***** *** ** ********* ***** and severely injured 2      others
2024-02-03 09:00:13,201 	Text Hypothesis :	after 28 years the world cup is currently going to  play     against sushil kumar 
2024-02-03 09:00:13,201 	Text Alignment  :	I     I  I     I   I     I   I  I         I     S   S        S       S      S     
2024-02-03 09:00:13,201 ========================================================================================================================
2024-02-03 09:00:13,201 Logging Sequence: 124_134.00
2024-02-03 09:00:13,201 	Gloss Reference :	A B+C+D+E
2024-02-03 09:00:13,202 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 09:00:13,202 	Gloss Alignment :	         
2024-02-03 09:00:13,202 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 09:00:13,203 	Text Reference  :	fans are now wondering why he       did      not     say     anything about dhoni as he   is very           close  to  dhoni 
2024-02-03 09:00:13,204 	Text Hypothesis :	**** *** *** ********* *** farewell messages started pouring in       for   dhoni as well as congratulatory wishes for jadeja
2024-02-03 09:00:13,204 	Text Alignment  :	D    D   D   D         D   S        S        S       S       S        S              S    S  S              S      S   S     
2024-02-03 09:00:13,204 ========================================================================================================================
2024-02-03 09:00:13,204 Logging Sequence: 117_50.00
2024-02-03 09:00:13,204 	Gloss Reference :	A B+C+D+E
2024-02-03 09:00:13,204 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 09:00:13,204 	Gloss Alignment :	         
2024-02-03 09:00:13,205 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 09:00:13,205 	Text Reference  :	** **** ***** **** ********* ****** *** ****** this  was  krunal  pandya's maiden odi  
2024-02-03 09:00:13,205 	Text Hypothesis :	on 12th april 2021 rajasthan royals and punjab kings were playing the      punjab kings
2024-02-03 09:00:13,206 	Text Alignment  :	I  I    I     I    I         I      I   I      S     S    S       S        S      S    
2024-02-03 09:00:13,206 ========================================================================================================================
2024-02-03 09:00:13,206 Logging Sequence: 136_55.00
2024-02-03 09:00:13,206 	Gloss Reference :	A B+C+D+E
2024-02-03 09:00:13,206 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 09:00:13,206 	Gloss Alignment :	         
2024-02-03 09:00:13,206 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 09:00:13,208 	Text Reference  :	****** she  had earlier won a ****** silver at the rio de janeiro olympics in     2016   
2024-02-03 09:00:13,208 	Text Hypothesis :	sindhu beat her and     won a bronze medal  in the *** ** ******* ******** women' singles
2024-02-03 09:00:13,208 	Text Alignment  :	I      S    S   S             I      S      S      D   D  D       D        S      S      
2024-02-03 09:00:13,208 ========================================================================================================================
2024-02-03 09:00:17,133 Epoch 942: Total Training Recognition Loss 0.40  Total Training Translation Loss 1.18 
2024-02-03 09:00:17,133 EPOCH 943
2024-02-03 09:00:21,439 Epoch 943: Total Training Recognition Loss 0.60  Total Training Translation Loss 1.45 
2024-02-03 09:00:21,440 EPOCH 944
2024-02-03 09:00:26,333 Epoch 944: Total Training Recognition Loss 1.14  Total Training Translation Loss 0.95 
2024-02-03 09:00:26,333 EPOCH 945
2024-02-03 09:00:26,796 [Epoch: 945 Step: 00032100] Batch Recognition Loss:   0.000183 => Gls Tokens per Sec:     2777 || Batch Translation Loss:   0.018195 => Txt Tokens per Sec:     7925 || Lr: 0.000100
2024-02-03 09:00:30,897 Epoch 945: Total Training Recognition Loss 0.15  Total Training Translation Loss 1.45 
2024-02-03 09:00:30,898 EPOCH 946
2024-02-03 09:00:35,547 Epoch 946: Total Training Recognition Loss 0.16  Total Training Translation Loss 0.94 
2024-02-03 09:00:35,547 EPOCH 947
2024-02-03 09:00:39,777 Epoch 947: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.63 
2024-02-03 09:00:39,778 EPOCH 948
2024-02-03 09:00:40,175 [Epoch: 948 Step: 00032200] Batch Recognition Loss:   0.037127 => Gls Tokens per Sec:      983 || Batch Translation Loss:   0.342921 => Txt Tokens per Sec:     3461 || Lr: 0.000100
2024-02-03 09:00:44,677 Epoch 948: Total Training Recognition Loss 0.09  Total Training Translation Loss 3.16 
2024-02-03 09:00:44,677 EPOCH 949
2024-02-03 09:00:48,997 Epoch 949: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.49 
2024-02-03 09:00:48,998 EPOCH 950
2024-02-03 09:00:53,817 [Epoch: 950 Step: 00032300] Batch Recognition Loss:   0.000458 => Gls Tokens per Sec:     2206 || Batch Translation Loss:   0.032157 => Txt Tokens per Sec:     6136 || Lr: 0.000100
2024-02-03 09:00:53,818 Epoch 950: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.59 
2024-02-03 09:00:53,818 EPOCH 951
2024-02-03 09:00:58,399 Epoch 951: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.98 
2024-02-03 09:00:58,399 EPOCH 952
2024-02-03 09:01:02,981 Epoch 952: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.71 
2024-02-03 09:01:02,982 EPOCH 953
2024-02-03 09:01:07,666 [Epoch: 953 Step: 00032400] Batch Recognition Loss:   0.001596 => Gls Tokens per Sec:     2134 || Batch Translation Loss:   0.044601 => Txt Tokens per Sec:     6007 || Lr: 0.000100
2024-02-03 09:01:07,834 Epoch 953: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.47 
2024-02-03 09:01:07,835 EPOCH 954
2024-02-03 09:01:12,315 Epoch 954: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.39 
2024-02-03 09:01:12,315 EPOCH 955
2024-02-03 09:01:17,249 Epoch 955: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.81 
2024-02-03 09:01:17,249 EPOCH 956
2024-02-03 09:01:20,575 [Epoch: 956 Step: 00032500] Batch Recognition Loss:   0.001378 => Gls Tokens per Sec:     2812 || Batch Translation Loss:   0.097169 => Txt Tokens per Sec:     7749 || Lr: 0.000100
2024-02-03 09:01:21,358 Epoch 956: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.59 
2024-02-03 09:01:21,358 EPOCH 957
2024-02-03 09:01:26,332 Epoch 957: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.31 
2024-02-03 09:01:26,333 EPOCH 958
2024-02-03 09:01:30,848 Epoch 958: Total Training Recognition Loss 0.12  Total Training Translation Loss 5.89 
2024-02-03 09:01:30,848 EPOCH 959
2024-02-03 09:01:34,775 [Epoch: 959 Step: 00032600] Batch Recognition Loss:   0.000943 => Gls Tokens per Sec:     2283 || Batch Translation Loss:   0.036209 => Txt Tokens per Sec:     6259 || Lr: 0.000100
2024-02-03 09:01:35,576 Epoch 959: Total Training Recognition Loss 0.05  Total Training Translation Loss 4.20 
2024-02-03 09:01:35,576 EPOCH 960
2024-02-03 09:01:39,716 Epoch 960: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.98 
2024-02-03 09:01:39,717 EPOCH 961
2024-02-03 09:01:44,622 Epoch 961: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.58 
2024-02-03 09:01:44,623 EPOCH 962
2024-02-03 09:01:48,094 [Epoch: 962 Step: 00032700] Batch Recognition Loss:   0.000468 => Gls Tokens per Sec:     2398 || Batch Translation Loss:   0.056621 => Txt Tokens per Sec:     6815 || Lr: 0.000100
2024-02-03 09:01:49,251 Epoch 962: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.53 
2024-02-03 09:01:49,252 EPOCH 963
2024-02-03 09:01:53,814 Epoch 963: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.10 
2024-02-03 09:01:53,815 EPOCH 964
2024-02-03 09:01:58,673 Epoch 964: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.12 
2024-02-03 09:01:58,674 EPOCH 965
2024-02-03 09:02:01,693 [Epoch: 965 Step: 00032800] Batch Recognition Loss:   0.000464 => Gls Tokens per Sec:     2461 || Batch Translation Loss:   0.022005 => Txt Tokens per Sec:     6768 || Lr: 0.000100
2024-02-03 09:02:03,174 Epoch 965: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.28 
2024-02-03 09:02:03,174 EPOCH 966
2024-02-03 09:02:08,062 Epoch 966: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.41 
2024-02-03 09:02:08,063 EPOCH 967
2024-02-03 09:02:12,673 Epoch 967: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.03 
2024-02-03 09:02:12,674 EPOCH 968
2024-02-03 09:02:16,083 [Epoch: 968 Step: 00032900] Batch Recognition Loss:   0.001332 => Gls Tokens per Sec:     1992 || Batch Translation Loss:   0.021882 => Txt Tokens per Sec:     5721 || Lr: 0.000100
2024-02-03 09:02:17,423 Epoch 968: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.32 
2024-02-03 09:02:17,423 EPOCH 969
2024-02-03 09:02:22,039 Epoch 969: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.46 
2024-02-03 09:02:22,040 EPOCH 970
2024-02-03 09:02:26,609 Epoch 970: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.49 
2024-02-03 09:02:26,610 EPOCH 971
2024-02-03 09:02:29,061 [Epoch: 971 Step: 00033000] Batch Recognition Loss:   0.002028 => Gls Tokens per Sec:     2509 || Batch Translation Loss:   0.027048 => Txt Tokens per Sec:     7130 || Lr: 0.000100
2024-02-03 09:02:31,203 Epoch 971: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.53 
2024-02-03 09:02:31,204 EPOCH 972
2024-02-03 09:02:35,821 Epoch 972: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.41 
2024-02-03 09:02:35,821 EPOCH 973
2024-02-03 09:02:40,699 Epoch 973: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.81 
2024-02-03 09:02:40,700 EPOCH 974
2024-02-03 09:02:43,113 [Epoch: 974 Step: 00033100] Batch Recognition Loss:   0.001080 => Gls Tokens per Sec:     2285 || Batch Translation Loss:   0.070741 => Txt Tokens per Sec:     6316 || Lr: 0.000100
2024-02-03 09:02:45,135 Epoch 974: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.52 
2024-02-03 09:02:45,136 EPOCH 975
2024-02-03 09:02:50,047 Epoch 975: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.70 
2024-02-03 09:02:50,048 EPOCH 976
2024-02-03 09:02:54,163 Epoch 976: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.83 
2024-02-03 09:02:54,163 EPOCH 977
2024-02-03 09:02:55,798 [Epoch: 977 Step: 00033200] Batch Recognition Loss:   0.000565 => Gls Tokens per Sec:     3135 || Batch Translation Loss:   0.046225 => Txt Tokens per Sec:     8147 || Lr: 0.000100
2024-02-03 09:02:58,893 Epoch 977: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.39 
2024-02-03 09:02:58,894 EPOCH 978
2024-02-03 09:03:03,296 Epoch 978: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.44 
2024-02-03 09:03:03,297 EPOCH 979
2024-02-03 09:03:07,992 Epoch 979: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.23 
2024-02-03 09:03:07,993 EPOCH 980
2024-02-03 09:03:09,729 [Epoch: 980 Step: 00033300] Batch Recognition Loss:   0.000893 => Gls Tokens per Sec:     2437 || Batch Translation Loss:   0.025272 => Txt Tokens per Sec:     6652 || Lr: 0.000100
2024-02-03 09:03:12,413 Epoch 980: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.10 
2024-02-03 09:03:12,413 EPOCH 981
2024-02-03 09:03:17,316 Epoch 981: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.04 
2024-02-03 09:03:17,316 EPOCH 982
2024-02-03 09:03:21,736 Epoch 982: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.18 
2024-02-03 09:03:21,737 EPOCH 983
2024-02-03 09:03:23,585 [Epoch: 983 Step: 00033400] Batch Recognition Loss:   0.000324 => Gls Tokens per Sec:     1944 || Batch Translation Loss:   0.019938 => Txt Tokens per Sec:     5159 || Lr: 0.000100
2024-02-03 09:03:26,613 Epoch 983: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.38 
2024-02-03 09:03:26,614 EPOCH 984
2024-02-03 09:03:31,163 Epoch 984: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.32 
2024-02-03 09:03:31,163 EPOCH 985
2024-02-03 09:03:35,772 Epoch 985: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.32 
2024-02-03 09:03:35,773 EPOCH 986
2024-02-03 09:03:36,994 [Epoch: 986 Step: 00033500] Batch Recognition Loss:   0.002875 => Gls Tokens per Sec:     2621 || Batch Translation Loss:   0.080352 => Txt Tokens per Sec:     7057 || Lr: 0.000100
2024-02-03 09:03:40,312 Epoch 986: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.48 
2024-02-03 09:03:40,312 EPOCH 987
2024-02-03 09:03:44,987 Epoch 987: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.13 
2024-02-03 09:03:44,987 EPOCH 988
2024-02-03 09:03:49,736 Epoch 988: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.35 
2024-02-03 09:03:49,737 EPOCH 989
2024-02-03 09:03:50,927 [Epoch: 989 Step: 00033600] Batch Recognition Loss:   0.000354 => Gls Tokens per Sec:     2155 || Batch Translation Loss:   0.025707 => Txt Tokens per Sec:     6396 || Lr: 0.000100
2024-02-03 09:03:54,282 Epoch 989: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.44 
2024-02-03 09:03:54,282 EPOCH 990
2024-02-03 09:03:59,178 Epoch 990: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.46 
2024-02-03 09:03:59,179 EPOCH 991
2024-02-03 09:04:03,347 Epoch 991: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.97 
2024-02-03 09:04:03,347 EPOCH 992
2024-02-03 09:04:04,149 [Epoch: 992 Step: 00033700] Batch Recognition Loss:   0.002576 => Gls Tokens per Sec:     2400 || Batch Translation Loss:   0.030416 => Txt Tokens per Sec:     6523 || Lr: 0.000100
2024-02-03 09:04:08,315 Epoch 992: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.86 
2024-02-03 09:04:08,316 EPOCH 993
2024-02-03 09:04:12,838 Epoch 993: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.11 
2024-02-03 09:04:12,839 EPOCH 994
2024-02-03 09:04:17,561 Epoch 994: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.45 
2024-02-03 09:04:17,561 EPOCH 995
2024-02-03 09:04:17,936 [Epoch: 995 Step: 00033800] Batch Recognition Loss:   0.000532 => Gls Tokens per Sec:     3432 || Batch Translation Loss:   0.055917 => Txt Tokens per Sec:     9158 || Lr: 0.000100
2024-02-03 09:04:22,143 Epoch 995: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.15 
2024-02-03 09:04:22,143 EPOCH 996
2024-02-03 09:04:27,063 Epoch 996: Total Training Recognition Loss 0.31  Total Training Translation Loss 11.68 
2024-02-03 09:04:27,064 EPOCH 997
2024-02-03 09:04:31,625 Epoch 997: Total Training Recognition Loss 0.59  Total Training Translation Loss 6.28 
2024-02-03 09:04:31,626 EPOCH 998
2024-02-03 09:04:31,828 [Epoch: 998 Step: 00033900] Batch Recognition Loss:   0.001739 => Gls Tokens per Sec:     3168 || Batch Translation Loss:   0.084628 => Txt Tokens per Sec:     7416 || Lr: 0.000100
2024-02-03 09:04:36,020 Epoch 998: Total Training Recognition Loss 0.26  Total Training Translation Loss 4.80 
2024-02-03 09:04:36,021 EPOCH 999
2024-02-03 09:04:40,754 Epoch 999: Total Training Recognition Loss 0.37  Total Training Translation Loss 4.91 
2024-02-03 09:04:40,754 EPOCH 1000
2024-02-03 09:04:45,273 [Epoch: 1000 Step: 00034000] Batch Recognition Loss:   0.016502 => Gls Tokens per Sec:     2354 || Batch Translation Loss:   2.652065 => Txt Tokens per Sec:     6545 || Lr: 0.000100
2024-02-03 09:04:54,040 Validation result at epoch 1000, step    34000: duration: 8.7672s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.90063	Translation Loss: 88529.53125	PPL: 9847.83887
	Eval Metric: BLEU
	WER 3.54	(DEL: 0.14,	INS: 0.00,	SUB: 3.39)
	BLEU-4 0.43	(BLEU-1: 10.68,	BLEU-2: 2.95,	BLEU-3: 1.05,	BLEU-4: 0.43)
	CHRF 17.04	ROUGE 8.86
2024-02-03 09:04:54,041 Logging Recognition and Translation Outputs
2024-02-03 09:04:54,041 ========================================================================================================================
2024-02-03 09:04:54,041 Logging Sequence: 92_117.00
2024-02-03 09:04:54,041 	Gloss Reference :	A B+C+D+E
2024-02-03 09:04:54,041 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 09:04:54,042 	Gloss Alignment :	         
2024-02-03 09:04:54,042 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 09:04:54,042 	Text Reference  :	vandana's brother shekhar  filed  a   complaint against the   2      men  
2024-02-03 09:04:54,042 	Text Hypothesis :	********* ******* recently during the india     vs      south africa match
2024-02-03 09:04:54,043 	Text Alignment  :	D         D       S        S      S   S         S       S     S      S    
2024-02-03 09:04:54,043 ========================================================================================================================
2024-02-03 09:04:54,043 Logging Sequence: 70_219.00
2024-02-03 09:04:54,043 	Gloss Reference :	A B+C+D+E
2024-02-03 09:04:54,043 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 09:04:54,043 	Gloss Alignment :	         
2024-02-03 09:04:54,043 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 09:04:54,044 	Text Reference  :	** *** * **** ***** ** after   this  there was another similar incident
2024-02-03 09:04:54,044 	Text Hypothesis :	he won a gold medal in javelin throw at    the 2020    tokyo   olympics
2024-02-03 09:04:54,044 	Text Alignment  :	I  I   I I    I     I  S       S     S     S   S       S       S       
2024-02-03 09:04:54,044 ========================================================================================================================
2024-02-03 09:04:54,044 Logging Sequence: 181_101.00
2024-02-03 09:04:54,045 	Gloss Reference :	A B+C+D+E
2024-02-03 09:04:54,045 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 09:04:54,045 	Gloss Alignment :	         
2024-02-03 09:04:54,045 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 09:04:54,047 	Text Reference  :	****** ***** ** * the cutest comment was     by    yuvraj' father yograj singh who is   a  former cricketer and actor
2024-02-03 09:04:54,047 	Text Hypothesis :	yuvraj singh in a lot of     anger   smashed sixes for     each   bowl   in    an  over to see    smashing  35  runs 
2024-02-03 09:04:54,047 	Text Alignment  :	I      I     I  I S   S      S       S       S     S       S      S      S     S   S    S  S      S         S   S    
2024-02-03 09:04:54,047 ========================================================================================================================
2024-02-03 09:04:54,047 Logging Sequence: 112_117.00
2024-02-03 09:04:54,048 	Gloss Reference :	A B+C+D+E
2024-02-03 09:04:54,048 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 09:04:54,048 	Gloss Alignment :	         
2024-02-03 09:04:54,048 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 09:04:54,049 	Text Reference  :	the ******* *** **** ******** **** rpsg group previously owned the ****** rising pune     supergiant in  2016     and 2017
2024-02-03 09:04:54,050 	Text Hypothesis :	the taliban has been infected with icc  to    uae        oman  the indian will   continue to         see smashing 35  runs
2024-02-03 09:04:54,050 	Text Alignment  :	    I       I   I    I        I    S    S     S          S         I      S      S        S          S   S        S   S   
2024-02-03 09:04:54,050 ========================================================================================================================
2024-02-03 09:04:54,050 Logging Sequence: 156_248.00
2024-02-03 09:04:54,050 	Gloss Reference :	A B+C+D+E
2024-02-03 09:04:54,050 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 09:04:54,050 	Gloss Alignment :	         
2024-02-03 09:04:54,050 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 09:04:54,052 	Text Reference  :	miny reached 1843 in 16         overs they won the  match       this was        very shocking
2024-02-03 09:04:54,052 	Text Hypothesis :	**** ******* it   is disgusting that  such a   huge controversy in   connection to   bowl    
2024-02-03 09:04:54,052 	Text Alignment  :	D    D       S    S  S          S     S    S   S    S           S    S          S    S       
2024-02-03 09:04:54,052 ========================================================================================================================
2024-02-03 09:04:54,056 Epoch 1000: Total Training Recognition Loss 0.11  Total Training Translation Loss 5.47 
2024-02-03 09:04:54,056 EPOCH 1001
2024-02-03 09:04:58,829 Epoch 1001: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.95 
2024-02-03 09:04:58,830 EPOCH 1002
2024-02-03 09:05:03,762 Epoch 1002: Total Training Recognition Loss 0.09  Total Training Translation Loss 2.70 
2024-02-03 09:05:03,762 EPOCH 1003
2024-02-03 09:05:08,051 [Epoch: 1003 Step: 00034100] Batch Recognition Loss:   0.000706 => Gls Tokens per Sec:     2330 || Batch Translation Loss:   0.063854 => Txt Tokens per Sec:     6421 || Lr: 0.000100
2024-02-03 09:05:08,384 Epoch 1003: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.17 
2024-02-03 09:05:08,384 EPOCH 1004
2024-02-03 09:05:13,075 Epoch 1004: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.49 
2024-02-03 09:05:13,075 EPOCH 1005
2024-02-03 09:05:17,804 Epoch 1005: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.44 
2024-02-03 09:05:17,805 EPOCH 1006
2024-02-03 09:05:21,566 [Epoch: 1006 Step: 00034200] Batch Recognition Loss:   0.001756 => Gls Tokens per Sec:     2487 || Batch Translation Loss:   0.037602 => Txt Tokens per Sec:     7001 || Lr: 0.000100
2024-02-03 09:05:22,309 Epoch 1006: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.21 
2024-02-03 09:05:22,309 EPOCH 1007
2024-02-03 09:05:27,024 Epoch 1007: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.21 
2024-02-03 09:05:27,025 EPOCH 1008
2024-02-03 09:05:31,511 Epoch 1008: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.14 
2024-02-03 09:05:31,511 EPOCH 1009
2024-02-03 09:05:35,030 [Epoch: 1009 Step: 00034300] Batch Recognition Loss:   0.000951 => Gls Tokens per Sec:     2547 || Batch Translation Loss:   0.025430 => Txt Tokens per Sec:     7343 || Lr: 0.000100
2024-02-03 09:05:35,559 Epoch 1009: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.87 
2024-02-03 09:05:35,560 EPOCH 1010
2024-02-03 09:05:39,870 Epoch 1010: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.32 
2024-02-03 09:05:39,871 EPOCH 1011
2024-02-03 09:05:44,799 Epoch 1011: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.90 
2024-02-03 09:05:44,799 EPOCH 1012
2024-02-03 09:05:48,260 [Epoch: 1012 Step: 00034400] Batch Recognition Loss:   0.000523 => Gls Tokens per Sec:     2333 || Batch Translation Loss:   0.019737 => Txt Tokens per Sec:     6387 || Lr: 0.000100
2024-02-03 09:05:49,526 Epoch 1012: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.93 
2024-02-03 09:05:49,526 EPOCH 1013
2024-02-03 09:05:53,931 Epoch 1013: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.12 
2024-02-03 09:05:53,931 EPOCH 1014
2024-02-03 09:05:58,608 Epoch 1014: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.82 
2024-02-03 09:05:58,608 EPOCH 1015
2024-02-03 09:06:01,957 [Epoch: 1015 Step: 00034500] Batch Recognition Loss:   0.000458 => Gls Tokens per Sec:     2294 || Batch Translation Loss:   0.035450 => Txt Tokens per Sec:     6379 || Lr: 0.000100
2024-02-03 09:06:03,163 Epoch 1015: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.65 
2024-02-03 09:06:03,163 EPOCH 1016
2024-02-03 09:06:07,776 Epoch 1016: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.60 
2024-02-03 09:06:07,776 EPOCH 1017
2024-02-03 09:06:12,447 Epoch 1017: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.54 
2024-02-03 09:06:12,447 EPOCH 1018
2024-02-03 09:06:15,781 [Epoch: 1018 Step: 00034600] Batch Recognition Loss:   0.000318 => Gls Tokens per Sec:     2112 || Batch Translation Loss:   0.015745 => Txt Tokens per Sec:     5840 || Lr: 0.000100
2024-02-03 09:06:17,608 Epoch 1018: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.69 
2024-02-03 09:06:17,608 EPOCH 1019
2024-02-03 09:06:22,125 Epoch 1019: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.26 
2024-02-03 09:06:22,126 EPOCH 1020
2024-02-03 09:06:26,707 Epoch 1020: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.31 
2024-02-03 09:06:26,707 EPOCH 1021
2024-02-03 09:06:29,516 [Epoch: 1021 Step: 00034700] Batch Recognition Loss:   0.000146 => Gls Tokens per Sec:     2190 || Batch Translation Loss:   0.028794 => Txt Tokens per Sec:     6197 || Lr: 0.000100
2024-02-03 09:06:31,349 Epoch 1021: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.07 
2024-02-03 09:06:31,349 EPOCH 1022
2024-02-03 09:06:36,309 Epoch 1022: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.84 
2024-02-03 09:06:36,309 EPOCH 1023
2024-02-03 09:06:40,801 Epoch 1023: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.84 
2024-02-03 09:06:40,802 EPOCH 1024
2024-02-03 09:06:43,596 [Epoch: 1024 Step: 00034800] Batch Recognition Loss:   0.002118 => Gls Tokens per Sec:     1973 || Batch Translation Loss:   0.172967 => Txt Tokens per Sec:     5926 || Lr: 0.000100
2024-02-03 09:06:45,644 Epoch 1024: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.48 
2024-02-03 09:06:45,644 EPOCH 1025
2024-02-03 09:06:50,007 Epoch 1025: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.27 
2024-02-03 09:06:50,008 EPOCH 1026
2024-02-03 09:06:55,019 Epoch 1026: Total Training Recognition Loss 0.15  Total Training Translation Loss 8.90 
2024-02-03 09:06:55,019 EPOCH 1027
2024-02-03 09:06:56,800 [Epoch: 1027 Step: 00034900] Batch Recognition Loss:   0.002313 => Gls Tokens per Sec:     2736 || Batch Translation Loss:   0.337793 => Txt Tokens per Sec:     7407 || Lr: 0.000100
2024-02-03 09:06:59,132 Epoch 1027: Total Training Recognition Loss 0.24  Total Training Translation Loss 8.52 
2024-02-03 09:06:59,133 EPOCH 1028
2024-02-03 09:07:04,103 Epoch 1028: Total Training Recognition Loss 0.13  Total Training Translation Loss 3.06 
2024-02-03 09:07:04,103 EPOCH 1029
2024-02-03 09:07:08,691 Epoch 1029: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.10 
2024-02-03 09:07:08,692 EPOCH 1030
2024-02-03 09:07:10,222 [Epoch: 1030 Step: 00035000] Batch Recognition Loss:   0.000706 => Gls Tokens per Sec:     2767 || Batch Translation Loss:   0.028516 => Txt Tokens per Sec:     7225 || Lr: 0.000100
2024-02-03 09:07:13,424 Epoch 1030: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.08 
2024-02-03 09:07:13,425 EPOCH 1031
2024-02-03 09:07:18,373 Epoch 1031: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.60 
2024-02-03 09:07:18,373 EPOCH 1032
2024-02-03 09:07:22,633 Epoch 1032: Total Training Recognition Loss 0.06  Total Training Translation Loss 0.96 
2024-02-03 09:07:22,634 EPOCH 1033
2024-02-03 09:07:24,427 [Epoch: 1033 Step: 00035100] Batch Recognition Loss:   0.002166 => Gls Tokens per Sec:     2003 || Batch Translation Loss:   0.026172 => Txt Tokens per Sec:     5728 || Lr: 0.000100
2024-02-03 09:07:27,503 Epoch 1033: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.07 
2024-02-03 09:07:27,503 EPOCH 1034
2024-02-03 09:07:32,033 Epoch 1034: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.83 
2024-02-03 09:07:32,034 EPOCH 1035
2024-02-03 09:07:36,647 Epoch 1035: Total Training Recognition Loss 0.07  Total Training Translation Loss 0.68 
2024-02-03 09:07:36,648 EPOCH 1036
2024-02-03 09:07:37,826 [Epoch: 1036 Step: 00035200] Batch Recognition Loss:   0.000561 => Gls Tokens per Sec:     2718 || Batch Translation Loss:   0.013019 => Txt Tokens per Sec:     7151 || Lr: 0.000100
2024-02-03 09:07:41,444 Epoch 1036: Total Training Recognition Loss 0.06  Total Training Translation Loss 0.70 
2024-02-03 09:07:41,444 EPOCH 1037
2024-02-03 09:07:45,829 Epoch 1037: Total Training Recognition Loss 0.08  Total Training Translation Loss 0.61 
2024-02-03 09:07:45,830 EPOCH 1038
2024-02-03 09:07:50,507 Epoch 1038: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.52 
2024-02-03 09:07:50,508 EPOCH 1039
2024-02-03 09:07:51,511 [Epoch: 1039 Step: 00035300] Batch Recognition Loss:   0.000575 => Gls Tokens per Sec:     2306 || Batch Translation Loss:   0.009593 => Txt Tokens per Sec:     6370 || Lr: 0.000100
2024-02-03 09:07:54,959 Epoch 1039: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.52 
2024-02-03 09:07:54,960 EPOCH 1040
2024-02-03 09:08:00,001 Epoch 1040: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.54 
2024-02-03 09:08:00,002 EPOCH 1041
2024-02-03 09:08:04,537 Epoch 1041: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.56 
2024-02-03 09:08:04,538 EPOCH 1042
2024-02-03 09:08:05,314 [Epoch: 1042 Step: 00035400] Batch Recognition Loss:   0.000341 => Gls Tokens per Sec:     2477 || Batch Translation Loss:   0.009974 => Txt Tokens per Sec:     6804 || Lr: 0.000100
2024-02-03 09:08:09,351 Epoch 1042: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.59 
2024-02-03 09:08:09,352 EPOCH 1043
2024-02-03 09:08:13,560 Epoch 1043: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.67 
2024-02-03 09:08:13,561 EPOCH 1044
2024-02-03 09:08:18,485 Epoch 1044: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.59 
2024-02-03 09:08:18,485 EPOCH 1045
2024-02-03 09:08:18,845 [Epoch: 1045 Step: 00035500] Batch Recognition Loss:   0.001887 => Gls Tokens per Sec:     3580 || Batch Translation Loss:   0.012989 => Txt Tokens per Sec:     8965 || Lr: 0.000100
2024-02-03 09:08:23,053 Epoch 1045: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.60 
2024-02-03 09:08:23,054 EPOCH 1046
2024-02-03 09:08:27,726 Epoch 1046: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.92 
2024-02-03 09:08:27,726 EPOCH 1047
2024-02-03 09:08:32,609 Epoch 1047: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.11 
2024-02-03 09:08:32,609 EPOCH 1048
2024-02-03 09:08:32,776 [Epoch: 1048 Step: 00035600] Batch Recognition Loss:   0.001783 => Gls Tokens per Sec:     3855 || Batch Translation Loss:   0.061547 => Txt Tokens per Sec:     9042 || Lr: 0.000100
2024-02-03 09:08:36,956 Epoch 1048: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.23 
2024-02-03 09:08:36,956 EPOCH 1049
2024-02-03 09:08:41,757 Epoch 1049: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.37 
2024-02-03 09:08:41,758 EPOCH 1050
2024-02-03 09:08:46,209 [Epoch: 1050 Step: 00035700] Batch Recognition Loss:   0.001374 => Gls Tokens per Sec:     2388 || Batch Translation Loss:   0.136231 => Txt Tokens per Sec:     6642 || Lr: 0.000100
2024-02-03 09:08:46,209 Epoch 1050: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.52 
2024-02-03 09:08:46,210 EPOCH 1051
2024-02-03 09:08:50,778 Epoch 1051: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.06 
2024-02-03 09:08:50,778 EPOCH 1052
2024-02-03 09:08:55,321 Epoch 1052: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.09 
2024-02-03 09:08:55,321 EPOCH 1053
2024-02-03 09:08:59,792 [Epoch: 1053 Step: 00035800] Batch Recognition Loss:   0.000879 => Gls Tokens per Sec:     2235 || Batch Translation Loss:   0.128716 => Txt Tokens per Sec:     6220 || Lr: 0.000100
2024-02-03 09:09:00,291 Epoch 1053: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.35 
2024-02-03 09:09:00,291 EPOCH 1054
2024-02-03 09:09:04,791 Epoch 1054: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.86 
2024-02-03 09:09:04,792 EPOCH 1055
2024-02-03 09:09:09,623 Epoch 1055: Total Training Recognition Loss 0.05  Total Training Translation Loss 4.28 
2024-02-03 09:09:09,623 EPOCH 1056
2024-02-03 09:09:13,755 [Epoch: 1056 Step: 00035900] Batch Recognition Loss:   0.002119 => Gls Tokens per Sec:     2263 || Batch Translation Loss:   0.161072 => Txt Tokens per Sec:     6326 || Lr: 0.000100
2024-02-03 09:09:14,216 Epoch 1056: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.22 
2024-02-03 09:09:14,217 EPOCH 1057
2024-02-03 09:09:19,161 Epoch 1057: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.99 
2024-02-03 09:09:19,162 EPOCH 1058
2024-02-03 09:09:23,749 Epoch 1058: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.33 
2024-02-03 09:09:23,749 EPOCH 1059
2024-02-03 09:09:27,866 [Epoch: 1059 Step: 00036000] Batch Recognition Loss:   0.003319 => Gls Tokens per Sec:     2116 || Batch Translation Loss:   0.055926 => Txt Tokens per Sec:     6073 || Lr: 0.000100
2024-02-03 09:09:36,422 Validation result at epoch 1059, step    36000: duration: 8.5558s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.87912	Translation Loss: 88154.15625	PPL: 9471.27930
	Eval Metric: BLEU
	WER 3.32	(DEL: 0.07,	INS: 0.00,	SUB: 3.25)
	BLEU-4 0.58	(BLEU-1: 9.85,	BLEU-2: 2.80,	BLEU-3: 1.16,	BLEU-4: 0.58)
	CHRF 16.86	ROUGE 7.96
2024-02-03 09:09:36,424 Logging Recognition and Translation Outputs
2024-02-03 09:09:36,424 ========================================================================================================================
2024-02-03 09:09:36,424 Logging Sequence: 109_16.00
2024-02-03 09:09:36,424 	Gloss Reference :	A B+C+D+E
2024-02-03 09:09:36,424 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 09:09:36,424 	Gloss Alignment :	         
2024-02-03 09:09:36,425 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 09:09:36,425 	Text Reference  :	* *** however the match was  rescheduled as two kkr     players -       
2024-02-03 09:09:36,426 	Text Hypothesis :	a new date    has not   been moved       to the results were    negative
2024-02-03 09:09:36,426 	Text Alignment  :	I I   S       S   S     S    S           S  S   S       S       S       
2024-02-03 09:09:36,426 ========================================================================================================================
2024-02-03 09:09:36,426 Logging Sequence: 146_120.00
2024-02-03 09:09:36,426 	Gloss Reference :	A B+C+D+E
2024-02-03 09:09:36,426 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 09:09:36,426 	Gloss Alignment :	         
2024-02-03 09:09:36,426 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 09:09:36,428 	Text Reference  :	bwf   also announced that the    doubles partners of the players infected with  the virus    would also be       
2024-02-03 09:09:36,428 	Text Hypothesis :	hazel also ********* **** posted a       photo    of *** ******* orion    lying on  yuvraj's chest on   instagram
2024-02-03 09:09:36,428 	Text Alignment  :	S          D         D    S      S       S           D   D       S        S     S   S        S     S    S        
2024-02-03 09:09:36,428 ========================================================================================================================
2024-02-03 09:09:36,428 Logging Sequence: 130_38.00
2024-02-03 09:09:36,428 	Gloss Reference :	A B+C+D+E
2024-02-03 09:09:36,429 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 09:09:36,429 	Gloss Alignment :	         
2024-02-03 09:09:36,429 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 09:09:36,430 	Text Reference  :	and is * ******* ******* now married to the *** ** ** ** oscar-winning film-maker dustin lance black   
2024-02-03 09:09:36,430 	Text Hypothesis :	he  is a fitness fanatic as  even    at the age of 36 he is            extremely  fit    and   handsome
2024-02-03 09:09:36,430 	Text Alignment  :	S      I I       I       S   S       S      I   I  I  I  S             S          S      S     S       
2024-02-03 09:09:36,431 ========================================================================================================================
2024-02-03 09:09:36,431 Logging Sequence: 161_170.00
2024-02-03 09:09:36,431 	Gloss Reference :	A B+C+D+E
2024-02-03 09:09:36,431 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 09:09:36,431 	Gloss Alignment :	         
2024-02-03 09:09:36,431 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 09:09:36,433 	Text Reference  :	** however when the      bcci       replaced him with rohit sharma he decided to step down as     test captain as  well
2024-02-03 09:09:36,433 	Text Hypothesis :	it is      a    100-ball tournament called   him **** ***** ****** ** ******* ** **** **** bowled in   the     1st time
2024-02-03 09:09:36,433 	Text Alignment  :	I  S       S    S        S          S            D    D     D      D  D       D  D    D    S      S    S       S   S   
2024-02-03 09:09:36,433 ========================================================================================================================
2024-02-03 09:09:36,433 Logging Sequence: 89_14.00
2024-02-03 09:09:36,433 	Gloss Reference :	A B+C+D+E
2024-02-03 09:09:36,433 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 09:09:36,434 	Gloss Alignment :	         
2024-02-03 09:09:36,434 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 09:09:36,434 	Text Reference  :	*** ***** he      has  completed 23   years of playing cricket   
2024-02-03 09:09:36,434 	Text Hypothesis :	the men's cricket team members   were only  by four    tournament
2024-02-03 09:09:36,435 	Text Alignment  :	I   I     S       S    S         S    S     S  S       S         
2024-02-03 09:09:36,435 ========================================================================================================================
2024-02-03 09:09:37,083 Epoch 1059: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.50 
2024-02-03 09:09:37,083 EPOCH 1060
2024-02-03 09:09:42,012 Epoch 1060: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.36 
2024-02-03 09:09:42,013 EPOCH 1061
2024-02-03 09:09:46,841 Epoch 1061: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.89 
2024-02-03 09:09:46,841 EPOCH 1062
2024-02-03 09:09:50,217 [Epoch: 1062 Step: 00036100] Batch Recognition Loss:   0.000275 => Gls Tokens per Sec:     2466 || Batch Translation Loss:   0.021014 => Txt Tokens per Sec:     6889 || Lr: 0.000050
2024-02-03 09:09:51,482 Epoch 1062: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.66 
2024-02-03 09:09:51,482 EPOCH 1063
2024-02-03 09:09:56,073 Epoch 1063: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.60 
2024-02-03 09:09:56,073 EPOCH 1064
2024-02-03 09:10:00,961 Epoch 1064: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.59 
2024-02-03 09:10:00,962 EPOCH 1065
2024-02-03 09:10:03,772 [Epoch: 1065 Step: 00036200] Batch Recognition Loss:   0.000297 => Gls Tokens per Sec:     2734 || Batch Translation Loss:   0.010205 => Txt Tokens per Sec:     7464 || Lr: 0.000050
2024-02-03 09:10:05,246 Epoch 1065: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.58 
2024-02-03 09:10:05,246 EPOCH 1066
2024-02-03 09:10:09,923 Epoch 1066: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.51 
2024-02-03 09:10:09,924 EPOCH 1067
2024-02-03 09:10:14,417 Epoch 1067: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.49 
2024-02-03 09:10:14,418 EPOCH 1068
2024-02-03 09:10:17,018 [Epoch: 1068 Step: 00036300] Batch Recognition Loss:   0.000558 => Gls Tokens per Sec:     2612 || Batch Translation Loss:   0.013414 => Txt Tokens per Sec:     7234 || Lr: 0.000050
2024-02-03 09:10:18,459 Epoch 1068: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.49 
2024-02-03 09:10:18,459 EPOCH 1069
2024-02-03 09:10:23,421 Epoch 1069: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.48 
2024-02-03 09:10:23,421 EPOCH 1070
2024-02-03 09:10:27,827 Epoch 1070: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.49 
2024-02-03 09:10:27,827 EPOCH 1071
2024-02-03 09:10:30,305 [Epoch: 1071 Step: 00036400] Batch Recognition Loss:   0.000953 => Gls Tokens per Sec:     2484 || Batch Translation Loss:   0.015447 => Txt Tokens per Sec:     6513 || Lr: 0.000050
2024-02-03 09:10:32,692 Epoch 1071: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.45 
2024-02-03 09:10:32,692 EPOCH 1072
2024-02-03 09:10:36,786 Epoch 1072: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.48 
2024-02-03 09:10:36,786 EPOCH 1073
2024-02-03 09:10:40,864 Epoch 1073: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.50 
2024-02-03 09:10:40,865 EPOCH 1074
2024-02-03 09:10:43,605 [Epoch: 1074 Step: 00036500] Batch Recognition Loss:   0.001134 => Gls Tokens per Sec:     2011 || Batch Translation Loss:   0.008155 => Txt Tokens per Sec:     5431 || Lr: 0.000050
2024-02-03 09:10:46,039 Epoch 1074: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.51 
2024-02-03 09:10:46,039 EPOCH 1075
2024-02-03 09:10:50,530 Epoch 1075: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.48 
2024-02-03 09:10:50,530 EPOCH 1076
2024-02-03 09:10:55,429 Epoch 1076: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-03 09:10:55,429 EPOCH 1077
2024-02-03 09:10:57,701 [Epoch: 1077 Step: 00036600] Batch Recognition Loss:   0.001139 => Gls Tokens per Sec:     2255 || Batch Translation Loss:   0.032559 => Txt Tokens per Sec:     6458 || Lr: 0.000050
2024-02-03 09:10:59,877 Epoch 1077: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-03 09:10:59,878 EPOCH 1078
2024-02-03 09:11:04,769 Epoch 1078: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.47 
2024-02-03 09:11:04,769 EPOCH 1079
2024-02-03 09:11:09,312 Epoch 1079: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.53 
2024-02-03 09:11:09,312 EPOCH 1080
2024-02-03 09:11:10,840 [Epoch: 1080 Step: 00036700] Batch Recognition Loss:   0.000149 => Gls Tokens per Sec:     2770 || Batch Translation Loss:   0.007729 => Txt Tokens per Sec:     7154 || Lr: 0.000050
2024-02-03 09:11:13,939 Epoch 1080: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.54 
2024-02-03 09:11:13,939 EPOCH 1081
2024-02-03 09:11:18,439 Epoch 1081: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.55 
2024-02-03 09:11:18,439 EPOCH 1082
2024-02-03 09:11:23,085 Epoch 1082: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.48 
2024-02-03 09:11:23,085 EPOCH 1083
2024-02-03 09:11:24,580 [Epoch: 1083 Step: 00036800] Batch Recognition Loss:   0.000191 => Gls Tokens per Sec:     2405 || Batch Translation Loss:   0.009637 => Txt Tokens per Sec:     7045 || Lr: 0.000050
2024-02-03 09:11:27,831 Epoch 1083: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.46 
2024-02-03 09:11:27,831 EPOCH 1084
2024-02-03 09:11:32,297 Epoch 1084: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.44 
2024-02-03 09:11:32,297 EPOCH 1085
2024-02-03 09:11:37,468 Epoch 1085: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.46 
2024-02-03 09:11:37,469 EPOCH 1086
2024-02-03 09:11:39,102 [Epoch: 1086 Step: 00036900] Batch Recognition Loss:   0.000220 => Gls Tokens per Sec:     1961 || Batch Translation Loss:   0.007415 => Txt Tokens per Sec:     5479 || Lr: 0.000050
2024-02-03 09:11:42,176 Epoch 1086: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.43 
2024-02-03 09:11:42,177 EPOCH 1087
2024-02-03 09:11:47,013 Epoch 1087: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.44 
2024-02-03 09:11:47,014 EPOCH 1088
2024-02-03 09:11:51,491 Epoch 1088: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.54 
2024-02-03 09:11:51,491 EPOCH 1089
2024-02-03 09:11:52,661 [Epoch: 1089 Step: 00037000] Batch Recognition Loss:   0.000093 => Gls Tokens per Sec:     2190 || Batch Translation Loss:   0.009190 => Txt Tokens per Sec:     5896 || Lr: 0.000050
2024-02-03 09:11:56,404 Epoch 1089: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-03 09:11:56,404 EPOCH 1090
2024-02-03 09:12:00,867 Epoch 1090: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-03 09:12:00,868 EPOCH 1091
2024-02-03 09:12:05,557 Epoch 1091: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-03 09:12:05,557 EPOCH 1092
2024-02-03 09:12:06,342 [Epoch: 1092 Step: 00037100] Batch Recognition Loss:   0.000188 => Gls Tokens per Sec:     2450 || Batch Translation Loss:   0.027067 => Txt Tokens per Sec:     7151 || Lr: 0.000050
2024-02-03 09:12:10,016 Epoch 1092: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-03 09:12:10,016 EPOCH 1093
2024-02-03 09:12:14,782 Epoch 1093: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.54 
2024-02-03 09:12:14,782 EPOCH 1094
2024-02-03 09:12:18,850 Epoch 1094: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-03 09:12:18,850 EPOCH 1095
2024-02-03 09:12:19,388 [Epoch: 1095 Step: 00037200] Batch Recognition Loss:   0.000357 => Gls Tokens per Sec:     2384 || Batch Translation Loss:   0.018493 => Txt Tokens per Sec:     6816 || Lr: 0.000050
2024-02-03 09:12:23,600 Epoch 1095: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.24 
2024-02-03 09:12:23,601 EPOCH 1096
2024-02-03 09:12:28,355 Epoch 1096: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.91 
2024-02-03 09:12:28,355 EPOCH 1097
2024-02-03 09:12:32,942 Epoch 1097: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.79 
2024-02-03 09:12:32,942 EPOCH 1098
2024-02-03 09:12:33,214 [Epoch: 1098 Step: 00037300] Batch Recognition Loss:   0.000368 => Gls Tokens per Sec:     1444 || Batch Translation Loss:   0.004730 => Txt Tokens per Sec:     4256 || Lr: 0.000050
2024-02-03 09:12:37,599 Epoch 1098: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.85 
2024-02-03 09:12:37,600 EPOCH 1099
2024-02-03 09:12:42,129 Epoch 1099: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.79 
2024-02-03 09:12:42,129 EPOCH 1100
2024-02-03 09:12:47,023 [Epoch: 1100 Step: 00037400] Batch Recognition Loss:   0.000203 => Gls Tokens per Sec:     2173 || Batch Translation Loss:   0.013573 => Txt Tokens per Sec:     6042 || Lr: 0.000050
2024-02-03 09:12:47,024 Epoch 1100: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.58 
2024-02-03 09:12:47,024 EPOCH 1101
2024-02-03 09:12:51,325 Epoch 1101: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.52 
2024-02-03 09:12:51,325 EPOCH 1102
2024-02-03 09:12:56,220 Epoch 1102: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.47 
2024-02-03 09:12:56,221 EPOCH 1103
2024-02-03 09:13:00,656 [Epoch: 1103 Step: 00037500] Batch Recognition Loss:   0.000149 => Gls Tokens per Sec:     2253 || Batch Translation Loss:   0.011424 => Txt Tokens per Sec:     6245 || Lr: 0.000050
2024-02-03 09:13:00,966 Epoch 1103: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.47 
2024-02-03 09:13:00,966 EPOCH 1104
2024-02-03 09:13:05,567 Epoch 1104: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.46 
2024-02-03 09:13:05,567 EPOCH 1105
2024-02-03 09:13:09,993 Epoch 1105: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.46 
2024-02-03 09:13:09,994 EPOCH 1106
2024-02-03 09:13:14,053 [Epoch: 1106 Step: 00037600] Batch Recognition Loss:   0.000248 => Gls Tokens per Sec:     2304 || Batch Translation Loss:   0.011554 => Txt Tokens per Sec:     6455 || Lr: 0.000050
2024-02-03 09:13:14,652 Epoch 1106: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.49 
2024-02-03 09:13:14,652 EPOCH 1107
2024-02-03 09:13:19,424 Epoch 1107: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.69 
2024-02-03 09:13:19,424 EPOCH 1108
2024-02-03 09:13:23,883 Epoch 1108: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.87 
2024-02-03 09:13:23,884 EPOCH 1109
2024-02-03 09:13:28,180 [Epoch: 1109 Step: 00037700] Batch Recognition Loss:   0.000506 => Gls Tokens per Sec:     2028 || Batch Translation Loss:   0.023423 => Txt Tokens per Sec:     5742 || Lr: 0.000050
2024-02-03 09:13:28,847 Epoch 1109: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.07 
2024-02-03 09:13:28,847 EPOCH 1110
2024-02-03 09:13:33,026 Epoch 1110: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.94 
2024-02-03 09:13:33,027 EPOCH 1111
2024-02-03 09:13:37,590 Epoch 1111: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.78 
2024-02-03 09:13:37,590 EPOCH 1112
2024-02-03 09:13:41,250 [Epoch: 1112 Step: 00037800] Batch Recognition Loss:   0.000308 => Gls Tokens per Sec:     2274 || Batch Translation Loss:   0.022110 => Txt Tokens per Sec:     6278 || Lr: 0.000050
2024-02-03 09:13:42,580 Epoch 1112: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.87 
2024-02-03 09:13:42,580 EPOCH 1113
2024-02-03 09:13:47,208 Epoch 1113: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.96 
2024-02-03 09:13:47,208 EPOCH 1114
2024-02-03 09:13:51,261 Epoch 1114: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.86 
2024-02-03 09:13:51,261 EPOCH 1115
2024-02-03 09:13:54,917 [Epoch: 1115 Step: 00037900] Batch Recognition Loss:   0.001892 => Gls Tokens per Sec:     2034 || Batch Translation Loss:   0.136983 => Txt Tokens per Sec:     5855 || Lr: 0.000050
2024-02-03 09:13:56,229 Epoch 1115: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.95 
2024-02-03 09:13:56,229 EPOCH 1116
2024-02-03 09:14:00,906 Epoch 1116: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.46 
2024-02-03 09:14:00,906 EPOCH 1117
2024-02-03 09:14:05,484 Epoch 1117: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.52 
2024-02-03 09:14:05,485 EPOCH 1118
2024-02-03 09:14:08,283 [Epoch: 1118 Step: 00038000] Batch Recognition Loss:   0.000380 => Gls Tokens per Sec:     2427 || Batch Translation Loss:   0.046436 => Txt Tokens per Sec:     6676 || Lr: 0.000050
2024-02-03 09:14:16,732 Validation result at epoch 1118, step    38000: duration: 8.4480s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 3.02020	Translation Loss: 88708.10938	PPL: 10032.20117
	Eval Metric: BLEU
	WER 3.39	(DEL: 0.00,	INS: 0.00,	SUB: 3.39)
	BLEU-4 0.51	(BLEU-1: 9.70,	BLEU-2: 2.90,	BLEU-3: 1.04,	BLEU-4: 0.51)
	CHRF 16.17	ROUGE 8.38
2024-02-03 09:14:16,733 Logging Recognition and Translation Outputs
2024-02-03 09:14:16,734 ========================================================================================================================
2024-02-03 09:14:16,734 Logging Sequence: 180_124.00
2024-02-03 09:14:16,734 	Gloss Reference :	A B+C+D+E
2024-02-03 09:14:16,734 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 09:14:16,734 	Gloss Alignment :	         
2024-02-03 09:14:16,734 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 09:14:16,736 	Text Reference  :	*** ********* **** ***** ******* *** demanding singh' resignation dissolution of    wfi  committee and   appointing a  new  committee
2024-02-03 09:14:16,736 	Text Hypothesis :	the wrestlers were quiet however now they      have   started     protesting  again from 23rd      april 2023       to join sports   
2024-02-03 09:14:16,736 	Text Alignment  :	I   I         I    I     I       I   S         S      S           S           S     S    S         S     S          S  S    S        
2024-02-03 09:14:16,736 ========================================================================================================================
2024-02-03 09:14:16,736 Logging Sequence: 67_16.00
2024-02-03 09:14:16,736 	Gloss Reference :	A B+C+D+E
2024-02-03 09:14:16,737 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 09:14:16,737 	Gloss Alignment :	         
2024-02-03 09:14:16,737 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 09:14:16,737 	Text Reference  :	** to  help india's fight    against the  covid-19 pandemic
2024-02-03 09:14:16,738 	Text Hypothesis :	he has now  been    preponed to      11th november 2023    
2024-02-03 09:14:16,738 	Text Alignment  :	I  S   S    S       S        S       S    S        S       
2024-02-03 09:14:16,738 ========================================================================================================================
2024-02-03 09:14:16,738 Logging Sequence: 79_57.00
2024-02-03 09:14:16,738 	Gloss Reference :	A B+C+D+E
2024-02-03 09:14:16,738 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 09:14:16,738 	Gloss Alignment :	         
2024-02-03 09:14:16,739 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 09:14:16,740 	Text Reference  :	*** the    board organised the matches only     in    mumbai navi mumbai and pune  
2024-02-03 09:14:16,740 	Text Hypothesis :	now people to    see       the sports  minister based on     the  dalit  and others
2024-02-03 09:14:16,740 	Text Alignment  :	I   S      S     S             S       S        S     S      S    S          S     
2024-02-03 09:14:16,740 ========================================================================================================================
2024-02-03 09:14:16,740 Logging Sequence: 109_161.00
2024-02-03 09:14:16,740 	Gloss Reference :	A B+C+D+E
2024-02-03 09:14:16,741 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 09:14:16,741 	Gloss Alignment :	         
2024-02-03 09:14:16,741 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 09:14:16,741 	Text Reference  :	he had ****** severe   pain in       his lower abdomen
2024-02-03 09:14:16,741 	Text Hypothesis :	he had tested positive for  covid-19 on  may   2023   
2024-02-03 09:14:16,742 	Text Alignment  :	       I      S        S    S        S   S     S      
2024-02-03 09:14:16,742 ========================================================================================================================
2024-02-03 09:14:16,742 Logging Sequence: 135_136.00
2024-02-03 09:14:16,742 	Gloss Reference :	A B+C+D+E
2024-02-03 09:14:16,742 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 09:14:16,742 	Gloss Alignment :	         
2024-02-03 09:14:16,742 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 09:14:16,744 	Text Reference  :	***** ***** **** in 2018  maria was   diagnosed with cancer         and ******* required surgery she    is a       cancer suvivor
2024-02-03 09:14:16,744 	Text Hypothesis :	maria wrote that to cover the   costs of        his  transportation and medical care     maå‚ysa needed 15 million polish zlotys 
2024-02-03 09:14:16,744 	Text Alignment  :	I     I     I    S  S     S     S     S         S    S                  I       S        S       S      S  S       S      S      
2024-02-03 09:14:16,744 ========================================================================================================================
2024-02-03 09:14:18,724 Epoch 1118: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.30 
2024-02-03 09:14:18,725 EPOCH 1119
2024-02-03 09:14:23,287 Epoch 1119: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.02 
2024-02-03 09:14:23,288 EPOCH 1120
2024-02-03 09:14:28,213 Epoch 1120: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.82 
2024-02-03 09:14:28,214 EPOCH 1121
2024-02-03 09:14:30,812 [Epoch: 1121 Step: 00038100] Batch Recognition Loss:   0.000532 => Gls Tokens per Sec:     2368 || Batch Translation Loss:   0.011644 => Txt Tokens per Sec:     6761 || Lr: 0.000050
2024-02-03 09:14:32,366 Epoch 1121: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.83 
2024-02-03 09:14:32,366 EPOCH 1122
2024-02-03 09:14:37,380 Epoch 1122: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.83 
2024-02-03 09:14:37,381 EPOCH 1123
2024-02-03 09:14:41,919 Epoch 1123: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.61 
2024-02-03 09:14:41,919 EPOCH 1124
2024-02-03 09:14:44,078 [Epoch: 1124 Step: 00038200] Batch Recognition Loss:   0.000099 => Gls Tokens per Sec:     2669 || Batch Translation Loss:   0.014171 => Txt Tokens per Sec:     7241 || Lr: 0.000050
2024-02-03 09:14:46,693 Epoch 1124: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.58 
2024-02-03 09:14:46,694 EPOCH 1125
2024-02-03 09:14:51,344 Epoch 1125: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-03 09:14:51,344 EPOCH 1126
2024-02-03 09:14:55,850 Epoch 1126: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.58 
2024-02-03 09:14:55,850 EPOCH 1127
2024-02-03 09:14:57,864 [Epoch: 1127 Step: 00038300] Batch Recognition Loss:   0.000380 => Gls Tokens per Sec:     2544 || Batch Translation Loss:   0.014300 => Txt Tokens per Sec:     6895 || Lr: 0.000050
2024-02-03 09:15:00,725 Epoch 1127: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.75 
2024-02-03 09:15:00,726 EPOCH 1128
2024-02-03 09:15:05,099 Epoch 1128: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.69 
2024-02-03 09:15:05,099 EPOCH 1129
2024-02-03 09:15:09,911 Epoch 1129: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.95 
2024-02-03 09:15:09,912 EPOCH 1130
2024-02-03 09:15:11,721 [Epoch: 1130 Step: 00038400] Batch Recognition Loss:   0.000296 => Gls Tokens per Sec:     2478 || Batch Translation Loss:   0.016972 => Txt Tokens per Sec:     6956 || Lr: 0.000050
2024-02-03 09:15:14,352 Epoch 1130: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.70 
2024-02-03 09:15:14,353 EPOCH 1131
2024-02-03 09:15:19,270 Epoch 1131: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.89 
2024-02-03 09:15:19,271 EPOCH 1132
2024-02-03 09:15:23,786 Epoch 1132: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.10 
2024-02-03 09:15:23,786 EPOCH 1133
2024-02-03 09:15:25,309 [Epoch: 1133 Step: 00038500] Batch Recognition Loss:   0.000129 => Gls Tokens per Sec:     2524 || Batch Translation Loss:   0.028636 => Txt Tokens per Sec:     6899 || Lr: 0.000050
2024-02-03 09:15:28,539 Epoch 1133: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.34 
2024-02-03 09:15:28,540 EPOCH 1134
2024-02-03 09:15:32,931 Epoch 1134: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.16 
2024-02-03 09:15:32,932 EPOCH 1135
2024-02-03 09:15:37,695 Epoch 1135: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.85 
2024-02-03 09:15:37,696 EPOCH 1136
2024-02-03 09:15:39,050 [Epoch: 1136 Step: 00038600] Batch Recognition Loss:   0.000817 => Gls Tokens per Sec:     2365 || Batch Translation Loss:   0.014246 => Txt Tokens per Sec:     6984 || Lr: 0.000050
2024-02-03 09:15:42,369 Epoch 1136: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.76 
2024-02-03 09:15:42,369 EPOCH 1137
2024-02-03 09:15:46,904 Epoch 1137: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.83 
2024-02-03 09:15:46,905 EPOCH 1138
2024-02-03 09:15:51,797 Epoch 1138: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.56 
2024-02-03 09:15:51,798 EPOCH 1139
2024-02-03 09:15:52,787 [Epoch: 1139 Step: 00038700] Batch Recognition Loss:   0.000196 => Gls Tokens per Sec:     2593 || Batch Translation Loss:   0.012579 => Txt Tokens per Sec:     7565 || Lr: 0.000050
2024-02-03 09:15:56,052 Epoch 1139: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.51 
2024-02-03 09:15:56,052 EPOCH 1140
2024-02-03 09:16:00,824 Epoch 1140: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-03 09:16:00,825 EPOCH 1141
2024-02-03 09:16:05,321 Epoch 1141: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-03 09:16:05,321 EPOCH 1142
2024-02-03 09:16:06,005 [Epoch: 1142 Step: 00038800] Batch Recognition Loss:   0.000079 => Gls Tokens per Sec:     2810 || Batch Translation Loss:   0.008236 => Txt Tokens per Sec:     7583 || Lr: 0.000050
2024-02-03 09:16:10,192 Epoch 1142: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-03 09:16:10,192 EPOCH 1143
2024-02-03 09:16:14,714 Epoch 1143: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-03 09:16:14,715 EPOCH 1144
2024-02-03 09:16:19,382 Epoch 1144: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-03 09:16:19,382 EPOCH 1145
2024-02-03 09:16:19,845 [Epoch: 1145 Step: 00038900] Batch Recognition Loss:   0.000173 => Gls Tokens per Sec:     2774 || Batch Translation Loss:   0.010800 => Txt Tokens per Sec:     7955 || Lr: 0.000050
2024-02-03 09:16:23,791 Epoch 1145: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-03 09:16:23,792 EPOCH 1146
2024-02-03 09:16:28,577 Epoch 1146: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.47 
2024-02-03 09:16:28,577 EPOCH 1147
2024-02-03 09:16:33,246 Epoch 1147: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.52 
2024-02-03 09:16:33,247 EPOCH 1148
2024-02-03 09:16:33,668 [Epoch: 1148 Step: 00039000] Batch Recognition Loss:   0.000782 => Gls Tokens per Sec:     1520 || Batch Translation Loss:   0.015510 => Txt Tokens per Sec:     5171 || Lr: 0.000050
2024-02-03 09:16:37,716 Epoch 1148: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.40 
2024-02-03 09:16:37,716 EPOCH 1149
2024-02-03 09:16:42,581 Epoch 1149: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.14 
2024-02-03 09:16:42,582 EPOCH 1150
2024-02-03 09:16:47,056 [Epoch: 1150 Step: 00039100] Batch Recognition Loss:   0.000332 => Gls Tokens per Sec:     2377 || Batch Translation Loss:   0.026027 => Txt Tokens per Sec:     6609 || Lr: 0.000050
2024-02-03 09:16:47,057 Epoch 1150: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.54 
2024-02-03 09:16:47,057 EPOCH 1151
2024-02-03 09:16:51,928 Epoch 1151: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.45 
2024-02-03 09:16:51,929 EPOCH 1152
2024-02-03 09:16:56,171 Epoch 1152: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.80 
2024-02-03 09:16:56,172 EPOCH 1153
2024-02-03 09:17:00,877 [Epoch: 1153 Step: 00039200] Batch Recognition Loss:   0.000267 => Gls Tokens per Sec:     2124 || Batch Translation Loss:   0.110085 => Txt Tokens per Sec:     5916 || Lr: 0.000050
2024-02-03 09:17:01,093 Epoch 1153: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.66 
2024-02-03 09:17:01,093 EPOCH 1154
2024-02-03 09:17:05,592 Epoch 1154: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.19 
2024-02-03 09:17:05,593 EPOCH 1155
2024-02-03 09:17:10,350 Epoch 1155: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.38 
2024-02-03 09:17:10,350 EPOCH 1156
2024-02-03 09:17:13,997 [Epoch: 1156 Step: 00039300] Batch Recognition Loss:   0.000266 => Gls Tokens per Sec:     2564 || Batch Translation Loss:   0.045638 => Txt Tokens per Sec:     7173 || Lr: 0.000050
2024-02-03 09:17:14,418 Epoch 1156: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.02 
2024-02-03 09:17:14,418 EPOCH 1157
2024-02-03 09:17:18,457 Epoch 1157: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.34 
2024-02-03 09:17:18,457 EPOCH 1158
2024-02-03 09:17:23,414 Epoch 1158: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.12 
2024-02-03 09:17:23,414 EPOCH 1159
2024-02-03 09:17:27,081 [Epoch: 1159 Step: 00039400] Batch Recognition Loss:   0.000233 => Gls Tokens per Sec:     2376 || Batch Translation Loss:   0.015373 => Txt Tokens per Sec:     6609 || Lr: 0.000050
2024-02-03 09:17:28,026 Epoch 1159: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.90 
2024-02-03 09:17:28,027 EPOCH 1160
2024-02-03 09:17:32,634 Epoch 1160: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.92 
2024-02-03 09:17:32,635 EPOCH 1161
2024-02-03 09:17:37,401 Epoch 1161: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.91 
2024-02-03 09:17:37,402 EPOCH 1162
2024-02-03 09:17:41,135 [Epoch: 1162 Step: 00039500] Batch Recognition Loss:   0.000173 => Gls Tokens per Sec:     2162 || Batch Translation Loss:   0.010179 => Txt Tokens per Sec:     6055 || Lr: 0.000050
2024-02-03 09:17:42,212 Epoch 1162: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.94 
2024-02-03 09:17:42,213 EPOCH 1163
2024-02-03 09:17:47,035 Epoch 1163: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.97 
2024-02-03 09:17:47,036 EPOCH 1164
2024-02-03 09:17:51,773 Epoch 1164: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.82 
2024-02-03 09:17:51,773 EPOCH 1165
2024-02-03 09:17:55,234 [Epoch: 1165 Step: 00039600] Batch Recognition Loss:   0.000226 => Gls Tokens per Sec:     2219 || Batch Translation Loss:   0.022672 => Txt Tokens per Sec:     6180 || Lr: 0.000050
2024-02-03 09:17:56,666 Epoch 1165: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.07 
2024-02-03 09:17:56,667 EPOCH 1166
2024-02-03 09:18:01,691 Epoch 1166: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.94 
2024-02-03 09:18:01,692 EPOCH 1167
2024-02-03 09:18:06,209 Epoch 1167: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.69 
2024-02-03 09:18:06,209 EPOCH 1168
2024-02-03 09:18:08,804 [Epoch: 1168 Step: 00039700] Batch Recognition Loss:   0.000266 => Gls Tokens per Sec:     2617 || Batch Translation Loss:   0.032506 => Txt Tokens per Sec:     6937 || Lr: 0.000050
2024-02-03 09:18:10,932 Epoch 1168: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.66 
2024-02-03 09:18:10,932 EPOCH 1169
2024-02-03 09:18:15,914 Epoch 1169: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.64 
2024-02-03 09:18:15,914 EPOCH 1170
2024-02-03 09:18:20,679 Epoch 1170: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.59 
2024-02-03 09:18:20,679 EPOCH 1171
2024-02-03 09:18:23,083 [Epoch: 1171 Step: 00039800] Batch Recognition Loss:   0.000333 => Gls Tokens per Sec:     2664 || Batch Translation Loss:   0.018164 => Txt Tokens per Sec:     7402 || Lr: 0.000050
2024-02-03 09:18:25,167 Epoch 1171: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.58 
2024-02-03 09:18:25,168 EPOCH 1172
2024-02-03 09:18:29,896 Epoch 1172: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.58 
2024-02-03 09:18:29,897 EPOCH 1173
2024-02-03 09:18:34,400 Epoch 1173: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.53 
2024-02-03 09:18:34,401 EPOCH 1174
2024-02-03 09:18:36,959 [Epoch: 1174 Step: 00039900] Batch Recognition Loss:   0.006778 => Gls Tokens per Sec:     2155 || Batch Translation Loss:   0.008409 => Txt Tokens per Sec:     5916 || Lr: 0.000050
2024-02-03 09:18:38,984 Epoch 1174: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.50 
2024-02-03 09:18:38,984 EPOCH 1175
2024-02-03 09:18:43,769 Epoch 1175: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.49 
2024-02-03 09:18:43,769 EPOCH 1176
2024-02-03 09:18:48,202 Epoch 1176: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.55 
2024-02-03 09:18:48,203 EPOCH 1177
2024-02-03 09:18:50,469 [Epoch: 1177 Step: 00040000] Batch Recognition Loss:   0.000244 => Gls Tokens per Sec:     2150 || Batch Translation Loss:   0.010847 => Txt Tokens per Sec:     5879 || Lr: 0.000050
2024-02-03 09:18:59,136 Validation result at epoch 1177, step    40000: duration: 8.6659s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.66152	Translation Loss: 88001.97656	PPL: 9322.75098
	Eval Metric: BLEU
	WER 3.39	(DEL: 0.07,	INS: 0.00,	SUB: 3.32)
	BLEU-4 0.55	(BLEU-1: 9.68,	BLEU-2: 2.73,	BLEU-3: 1.06,	BLEU-4: 0.55)
	CHRF 16.59	ROUGE 8.17
2024-02-03 09:18:59,137 Logging Recognition and Translation Outputs
2024-02-03 09:18:59,138 ========================================================================================================================
2024-02-03 09:18:59,138 Logging Sequence: 142_182.00
2024-02-03 09:18:59,138 	Gloss Reference :	A B+C+D+E
2024-02-03 09:18:59,138 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 09:18:59,138 	Gloss Alignment :	         
2024-02-03 09:18:59,138 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 09:18:59,139 	Text Reference  :	many indians commented that hope he   does  not    miss    the match as he is important while  playing against england
2024-02-03 09:18:59,140 	Text Hypothesis :	**** ******* ********* **** and  take stern action against the ***** ** ** ** ********* player if      found   guilty 
2024-02-03 09:18:59,140 	Text Alignment  :	D    D       D         D    S    S    S     S      S           D     D  D  D  D         S      S       S       S      
2024-02-03 09:18:59,140 ========================================================================================================================
2024-02-03 09:18:59,140 Logging Sequence: 58_97.00
2024-02-03 09:18:59,140 	Gloss Reference :	A B+C+D+E
2024-02-03 09:18:59,140 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 09:18:59,140 	Gloss Alignment :	         
2024-02-03 09:18:59,141 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 09:18:59,141 	Text Reference  :	the indian athletes have already bagged a   total    of    28 medals
2024-02-03 09:18:59,141 	Text Hypothesis :	the ****** talents  and  skills  of     our athletes knows no bounds
2024-02-03 09:18:59,142 	Text Alignment  :	    D      S        S    S       S      S   S        S     S  S     
2024-02-03 09:18:59,142 ========================================================================================================================
2024-02-03 09:18:59,142 Logging Sequence: 62_76.00
2024-02-03 09:18:59,142 	Gloss Reference :	A B+C+D+E
2024-02-03 09:18:59,142 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 09:18:59,142 	Gloss Alignment :	         
2024-02-03 09:18:59,142 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 09:18:59,144 	Text Reference  :	for t20 the male   player is paid rs 3       lakh information is   not available for        women     players
2024-02-03 09:18:59,144 	Text Hypothesis :	*** *** the indian team   is **** in england for  a           loss and the       australian stadium's rule   
2024-02-03 09:18:59,144 	Text Alignment  :	D   D       S      S         D    S  S       S    S           S    S   S         S          S         S      
2024-02-03 09:18:59,144 ========================================================================================================================
2024-02-03 09:18:59,144 Logging Sequence: 171_52.00
2024-02-03 09:18:59,144 	Gloss Reference :	A B+C+D+E
2024-02-03 09:18:59,145 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 09:18:59,145 	Gloss Alignment :	         
2024-02-03 09:18:59,145 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 09:18:59,147 	Text Reference  :	******** ******* during the   second half of  the   match ******* ******* ** ** when    gujarat titans were    batting sometime during the 14th-15th over 
2024-02-03 09:18:59,147 	Text Hypothesis :	everyone thought it     would be     a    one sided match because morocco is an amateur team    and    belgium ranks   2nd      in     the ********* world
2024-02-03 09:18:59,147 	Text Alignment  :	I        I       S      S     S      S    S   S           I       I       I  I  S       S       S      S       S       S        S          D         S    
2024-02-03 09:18:59,147 ========================================================================================================================
2024-02-03 09:18:59,147 Logging Sequence: 92_22.00
2024-02-03 09:18:59,147 	Gloss Reference :	A B+C+D+E
2024-02-03 09:18:59,148 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 09:18:59,148 	Gloss Alignment :	         
2024-02-03 09:18:59,148 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 09:18:59,149 	Text Reference  :	******* the team was devastated as they were hoping to win and   secure  a     gold medal in the  finals  
2024-02-03 09:18:59,149 	Text Hypothesis :	despite the **** *** ********** ** **** **** ****** ** *** first penalty shoot by   i     am very saddened
2024-02-03 09:18:59,149 	Text Alignment  :	I           D    D   D          D  D    D    D      D  D   S     S       S     S    S     S  S    S       
2024-02-03 09:18:59,149 ========================================================================================================================
2024-02-03 09:19:02,012 Epoch 1177: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.52 
2024-02-03 09:19:02,013 EPOCH 1178
2024-02-03 09:19:06,750 Epoch 1178: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-03 09:19:06,751 EPOCH 1179
2024-02-03 09:19:11,591 Epoch 1179: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-03 09:19:11,591 EPOCH 1180
2024-02-03 09:19:13,175 [Epoch: 1180 Step: 00040100] Batch Recognition Loss:   0.000064 => Gls Tokens per Sec:     2675 || Batch Translation Loss:   0.011423 => Txt Tokens per Sec:     7403 || Lr: 0.000050
2024-02-03 09:19:16,405 Epoch 1180: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-03 09:19:16,405 EPOCH 1181
2024-02-03 09:19:21,153 Epoch 1181: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.71 
2024-02-03 09:19:21,153 EPOCH 1182
2024-02-03 09:19:26,075 Epoch 1182: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.69 
2024-02-03 09:19:26,075 EPOCH 1183
2024-02-03 09:19:27,771 [Epoch: 1183 Step: 00040200] Batch Recognition Loss:   0.000241 => Gls Tokens per Sec:     2266 || Batch Translation Loss:   0.124671 => Txt Tokens per Sec:     6081 || Lr: 0.000050
2024-02-03 09:19:30,729 Epoch 1183: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.15 
2024-02-03 09:19:30,730 EPOCH 1184
2024-02-03 09:19:35,203 Epoch 1184: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.87 
2024-02-03 09:19:35,204 EPOCH 1185
2024-02-03 09:19:39,890 Epoch 1185: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.63 
2024-02-03 09:19:39,890 EPOCH 1186
2024-02-03 09:19:41,281 [Epoch: 1186 Step: 00040300] Batch Recognition Loss:   0.001454 => Gls Tokens per Sec:     2303 || Batch Translation Loss:   0.040923 => Txt Tokens per Sec:     6535 || Lr: 0.000050
2024-02-03 09:19:44,654 Epoch 1186: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.33 
2024-02-03 09:19:44,655 EPOCH 1187
2024-02-03 09:19:49,082 Epoch 1187: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.38 
2024-02-03 09:19:49,082 EPOCH 1188
2024-02-03 09:19:54,015 Epoch 1188: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.07 
2024-02-03 09:19:54,015 EPOCH 1189
2024-02-03 09:19:55,016 [Epoch: 1189 Step: 00040400] Batch Recognition Loss:   0.000723 => Gls Tokens per Sec:     2312 || Batch Translation Loss:   0.396242 => Txt Tokens per Sec:     6568 || Lr: 0.000050
2024-02-03 09:19:58,190 Epoch 1189: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.13 
2024-02-03 09:19:58,190 EPOCH 1190
2024-02-03 09:20:03,097 Epoch 1190: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.69 
2024-02-03 09:20:03,098 EPOCH 1191
2024-02-03 09:20:07,590 Epoch 1191: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.70 
2024-02-03 09:20:07,591 EPOCH 1192
2024-02-03 09:20:08,340 [Epoch: 1192 Step: 00040500] Batch Recognition Loss:   0.000706 => Gls Tokens per Sec:     2567 || Batch Translation Loss:   0.012870 => Txt Tokens per Sec:     6468 || Lr: 0.000050
2024-02-03 09:20:12,461 Epoch 1192: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.74 
2024-02-03 09:20:12,461 EPOCH 1193
2024-02-03 09:20:17,051 Epoch 1193: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.96 
2024-02-03 09:20:17,052 EPOCH 1194
2024-02-03 09:20:21,655 Epoch 1194: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.24 
2024-02-03 09:20:21,655 EPOCH 1195
2024-02-03 09:20:22,256 [Epoch: 1195 Step: 00040600] Batch Recognition Loss:   0.000275 => Gls Tokens per Sec:     2136 || Batch Translation Loss:   0.020572 => Txt Tokens per Sec:     6522 || Lr: 0.000050
2024-02-03 09:20:26,499 Epoch 1195: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.15 
2024-02-03 09:20:26,499 EPOCH 1196
2024-02-03 09:20:31,014 Epoch 1196: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.30 
2024-02-03 09:20:31,015 EPOCH 1197
2024-02-03 09:20:36,153 Epoch 1197: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.69 
2024-02-03 09:20:36,154 EPOCH 1198
2024-02-03 09:20:36,460 [Epoch: 1198 Step: 00040700] Batch Recognition Loss:   0.001506 => Gls Tokens per Sec:     2098 || Batch Translation Loss:   0.043009 => Txt Tokens per Sec:     6518 || Lr: 0.000050
2024-02-03 09:20:40,970 Epoch 1198: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.81 
2024-02-03 09:20:40,971 EPOCH 1199
2024-02-03 09:20:45,862 Epoch 1199: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.69 
2024-02-03 09:20:45,863 EPOCH 1200
2024-02-03 09:20:50,105 [Epoch: 1200 Step: 00040800] Batch Recognition Loss:   0.002811 => Gls Tokens per Sec:     2507 || Batch Translation Loss:   0.058354 => Txt Tokens per Sec:     6971 || Lr: 0.000050
2024-02-03 09:20:50,106 Epoch 1200: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.94 
2024-02-03 09:20:50,106 EPOCH 1201
2024-02-03 09:20:55,057 Epoch 1201: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.80 
2024-02-03 09:20:55,057 EPOCH 1202
2024-02-03 09:20:59,476 Epoch 1202: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.89 
2024-02-03 09:20:59,477 EPOCH 1203
2024-02-03 09:21:03,853 [Epoch: 1203 Step: 00040900] Batch Recognition Loss:   0.000155 => Gls Tokens per Sec:     2284 || Batch Translation Loss:   0.013855 => Txt Tokens per Sec:     6292 || Lr: 0.000050
2024-02-03 09:21:04,211 Epoch 1203: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.74 
2024-02-03 09:21:04,211 EPOCH 1204
2024-02-03 09:21:08,855 Epoch 1204: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.66 
2024-02-03 09:21:08,855 EPOCH 1205
2024-02-03 09:21:13,393 Epoch 1205: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.63 
2024-02-03 09:21:13,393 EPOCH 1206
2024-02-03 09:21:17,359 [Epoch: 1206 Step: 00041000] Batch Recognition Loss:   0.000544 => Gls Tokens per Sec:     2359 || Batch Translation Loss:   0.014374 => Txt Tokens per Sec:     6649 || Lr: 0.000050
2024-02-03 09:21:17,851 Epoch 1206: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.55 
2024-02-03 09:21:17,852 EPOCH 1207
2024-02-03 09:21:22,598 Epoch 1207: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.51 
2024-02-03 09:21:22,598 EPOCH 1208
2024-02-03 09:21:27,303 Epoch 1208: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-03 09:21:27,303 EPOCH 1209
2024-02-03 09:21:31,105 [Epoch: 1209 Step: 00041100] Batch Recognition Loss:   0.000187 => Gls Tokens per Sec:     2292 || Batch Translation Loss:   0.009935 => Txt Tokens per Sec:     6312 || Lr: 0.000050
2024-02-03 09:21:32,178 Epoch 1209: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.54 
2024-02-03 09:21:32,178 EPOCH 1210
2024-02-03 09:21:37,097 Epoch 1210: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.53 
2024-02-03 09:21:37,097 EPOCH 1211
2024-02-03 09:21:41,149 Epoch 1211: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.51 
2024-02-03 09:21:41,150 EPOCH 1212
2024-02-03 09:21:44,878 [Epoch: 1212 Step: 00041200] Batch Recognition Loss:   0.000241 => Gls Tokens per Sec:     2165 || Batch Translation Loss:   0.018102 => Txt Tokens per Sec:     5876 || Lr: 0.000050
2024-02-03 09:21:46,070 Epoch 1212: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.60 
2024-02-03 09:21:46,070 EPOCH 1213
2024-02-03 09:21:50,612 Epoch 1213: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.58 
2024-02-03 09:21:50,613 EPOCH 1214
2024-02-03 09:21:55,533 Epoch 1214: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.55 
2024-02-03 09:21:55,534 EPOCH 1215
2024-02-03 09:21:58,908 [Epoch: 1215 Step: 00041300] Batch Recognition Loss:   0.000200 => Gls Tokens per Sec:     2204 || Batch Translation Loss:   0.014033 => Txt Tokens per Sec:     6184 || Lr: 0.000050
2024-02-03 09:22:00,186 Epoch 1215: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.60 
2024-02-03 09:22:00,186 EPOCH 1216
2024-02-03 09:22:04,766 Epoch 1216: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.66 
2024-02-03 09:22:04,766 EPOCH 1217
2024-02-03 09:22:09,576 Epoch 1217: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.99 
2024-02-03 09:22:09,577 EPOCH 1218
2024-02-03 09:22:12,567 [Epoch: 1218 Step: 00041400] Batch Recognition Loss:   0.000298 => Gls Tokens per Sec:     2272 || Batch Translation Loss:   0.022082 => Txt Tokens per Sec:     6430 || Lr: 0.000050
2024-02-03 09:22:13,987 Epoch 1218: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.36 
2024-02-03 09:22:13,987 EPOCH 1219
2024-02-03 09:22:18,928 Epoch 1219: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.46 
2024-02-03 09:22:18,929 EPOCH 1220
2024-02-03 09:22:23,572 Epoch 1220: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.71 
2024-02-03 09:22:23,573 EPOCH 1221
2024-02-03 09:22:26,046 [Epoch: 1221 Step: 00041500] Batch Recognition Loss:   0.000261 => Gls Tokens per Sec:     2487 || Batch Translation Loss:   0.021186 => Txt Tokens per Sec:     6567 || Lr: 0.000050
2024-02-03 09:22:28,310 Epoch 1221: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.23 
2024-02-03 09:22:28,310 EPOCH 1222
2024-02-03 09:22:32,945 Epoch 1222: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.41 
2024-02-03 09:22:32,946 EPOCH 1223
2024-02-03 09:22:37,412 Epoch 1223: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.56 
2024-02-03 09:22:37,412 EPOCH 1224
2024-02-03 09:22:40,065 [Epoch: 1224 Step: 00041600] Batch Recognition Loss:   0.000274 => Gls Tokens per Sec:     2172 || Batch Translation Loss:   0.034171 => Txt Tokens per Sec:     6225 || Lr: 0.000050
2024-02-03 09:22:42,256 Epoch 1224: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.68 
2024-02-03 09:22:42,257 EPOCH 1225
2024-02-03 09:22:46,707 Epoch 1225: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.27 
2024-02-03 09:22:46,708 EPOCH 1226
2024-02-03 09:22:51,596 Epoch 1226: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.77 
2024-02-03 09:22:51,597 EPOCH 1227
2024-02-03 09:22:53,227 [Epoch: 1227 Step: 00041700] Batch Recognition Loss:   0.000771 => Gls Tokens per Sec:     2990 || Batch Translation Loss:   0.021193 => Txt Tokens per Sec:     8005 || Lr: 0.000050
2024-02-03 09:22:55,775 Epoch 1227: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.70 
2024-02-03 09:22:55,775 EPOCH 1228
2024-02-03 09:23:00,692 Epoch 1228: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.72 
2024-02-03 09:23:00,692 EPOCH 1229
2024-02-03 09:23:05,282 Epoch 1229: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.21 
2024-02-03 09:23:05,283 EPOCH 1230
2024-02-03 09:23:07,477 [Epoch: 1230 Step: 00041800] Batch Recognition Loss:   0.000106 => Gls Tokens per Sec:     1929 || Batch Translation Loss:   0.056702 => Txt Tokens per Sec:     5358 || Lr: 0.000050
2024-02-03 09:23:10,177 Epoch 1230: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.04 
2024-02-03 09:23:10,177 EPOCH 1231
2024-02-03 09:23:15,118 Epoch 1231: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.84 
2024-02-03 09:23:15,119 EPOCH 1232
2024-02-03 09:23:19,646 Epoch 1232: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.74 
2024-02-03 09:23:19,646 EPOCH 1233
2024-02-03 09:23:21,426 [Epoch: 1233 Step: 00041900] Batch Recognition Loss:   0.000170 => Gls Tokens per Sec:     2159 || Batch Translation Loss:   0.016528 => Txt Tokens per Sec:     6135 || Lr: 0.000050
2024-02-03 09:23:24,443 Epoch 1233: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.97 
2024-02-03 09:23:24,444 EPOCH 1234
2024-02-03 09:23:29,319 Epoch 1234: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.63 
2024-02-03 09:23:29,319 EPOCH 1235
2024-02-03 09:23:34,230 Epoch 1235: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-03 09:23:34,230 EPOCH 1236
2024-02-03 09:23:35,670 [Epoch: 1236 Step: 00042000] Batch Recognition Loss:   0.000123 => Gls Tokens per Sec:     2049 || Batch Translation Loss:   0.010332 => Txt Tokens per Sec:     5305 || Lr: 0.000050
2024-02-03 09:23:44,370 Validation result at epoch 1236, step    42000: duration: 8.7000s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.61166	Translation Loss: 88964.92969	PPL: 10303.40430
	Eval Metric: BLEU
	WER 3.47	(DEL: 0.21,	INS: 0.00,	SUB: 3.25)
	BLEU-4 0.50	(BLEU-1: 9.32,	BLEU-2: 2.70,	BLEU-3: 1.02,	BLEU-4: 0.50)
	CHRF 16.70	ROUGE 7.85
2024-02-03 09:23:44,371 Logging Recognition and Translation Outputs
2024-02-03 09:23:44,371 ========================================================================================================================
2024-02-03 09:23:44,371 Logging Sequence: 130_18.00
2024-02-03 09:23:44,372 	Gloss Reference :	A B+C+D+E
2024-02-03 09:23:44,372 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 09:23:44,372 	Gloss Alignment :	         
2024-02-03 09:23:44,372 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 09:23:44,374 	Text Reference  :	when he  was only 14   years old he participated in    the 2008 olympic games    in  beijing china  
2024-02-03 09:23:44,374 	Text Hypothesis :	**** you all know that daley is  a  fantastic    diver but what a       training for their   matches
2024-02-03 09:23:44,374 	Text Alignment  :	D    S   S   S    S    S     S   S  S            S     S   S    S       S        S   S       S      
2024-02-03 09:23:44,374 ========================================================================================================================
2024-02-03 09:23:44,374 Logging Sequence: 106_112.00
2024-02-03 09:23:44,374 	Gloss Reference :	A B+C+D+E
2024-02-03 09:23:44,375 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 09:23:44,375 	Gloss Alignment :	         
2024-02-03 09:23:44,375 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 09:23:44,376 	Text Reference  :	*** *** this ***** **** was indian team's   7th      asia          cup t20   win   
2024-02-03 09:23:44,376 	Text Hypothesis :	why did this match even in  the    national athletic championships for being played
2024-02-03 09:23:44,376 	Text Alignment  :	I   I        I     I    S   S      S        S        S             S   S     S     
2024-02-03 09:23:44,376 ========================================================================================================================
2024-02-03 09:23:44,376 Logging Sequence: 109_161.00
2024-02-03 09:23:44,376 	Gloss Reference :	A B+C+D+E
2024-02-03 09:23:44,377 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 09:23:44,377 	Gloss Alignment :	         
2024-02-03 09:23:44,377 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 09:23:44,377 	Text Reference  :	he had severe pain in    his     lower   abdomen
2024-02-03 09:23:44,377 	Text Hypothesis :	** *** ****** vs   jammu kashmir schools match  
2024-02-03 09:23:44,377 	Text Alignment  :	D  D   D      S    S     S       S       S      
2024-02-03 09:23:44,378 ========================================================================================================================
2024-02-03 09:23:44,378 Logging Sequence: 136_202.00
2024-02-03 09:23:44,378 	Gloss Reference :	A B+C+D+E
2024-02-03 09:23:44,378 	Gloss Hypothesis:	* A+B+E  
2024-02-03 09:23:44,378 	Gloss Alignment :	D S      
2024-02-03 09:23:44,378 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 09:23:44,379 	Text Reference  :	now we will have        to       wait    for          updates
2024-02-03 09:23:44,379 	Text Hypothesis :	*** ** **** cricketer's salaries airfare accomadation etc    
2024-02-03 09:23:44,379 	Text Alignment  :	D   D  D    S           S        S       S            S      
2024-02-03 09:23:44,379 ========================================================================================================================
2024-02-03 09:23:44,379 Logging Sequence: 140_2.00
2024-02-03 09:23:44,379 	Gloss Reference :	A B+C+D+E
2024-02-03 09:23:44,379 	Gloss Hypothesis:	A B+C+E+D
2024-02-03 09:23:44,380 	Gloss Alignment :	  S      
2024-02-03 09:23:44,380 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 09:23:44,381 	Text Reference  :	*** ** **** *** *** indian batsman-wicket keeper      rishabh pant     has     outstanding skills in   cricket
2024-02-03 09:23:44,381 	Text Hypothesis :	let me tell you why have   a              uttarakhand sports  minister pushkar singh       dhami  made india  
2024-02-03 09:23:44,381 	Text Alignment  :	I   I  I    I   I   S      S              S           S       S        S       S           S      S    S      
2024-02-03 09:23:44,381 ========================================================================================================================
2024-02-03 09:23:47,965 Epoch 1236: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.83 
2024-02-03 09:23:47,966 EPOCH 1237
2024-02-03 09:23:52,706 Epoch 1237: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.72 
2024-02-03 09:23:52,707 EPOCH 1238
2024-02-03 09:23:57,576 Epoch 1238: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.62 
2024-02-03 09:23:57,576 EPOCH 1239
2024-02-03 09:23:58,843 [Epoch: 1239 Step: 00042100] Batch Recognition Loss:   0.000778 => Gls Tokens per Sec:     2022 || Batch Translation Loss:   0.019469 => Txt Tokens per Sec:     6081 || Lr: 0.000050
2024-02-03 09:24:02,339 Epoch 1239: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-03 09:24:02,339 EPOCH 1240
2024-02-03 09:24:07,209 Epoch 1240: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.54 
2024-02-03 09:24:07,210 EPOCH 1241
2024-02-03 09:24:11,445 Epoch 1241: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.53 
2024-02-03 09:24:11,445 EPOCH 1242
2024-02-03 09:24:12,137 [Epoch: 1242 Step: 00042200] Batch Recognition Loss:   0.000439 => Gls Tokens per Sec:     2779 || Batch Translation Loss:   0.006681 => Txt Tokens per Sec:     7331 || Lr: 0.000050
2024-02-03 09:24:16,503 Epoch 1242: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.53 
2024-02-03 09:24:16,503 EPOCH 1243
2024-02-03 09:24:21,445 Epoch 1243: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.98 
2024-02-03 09:24:21,446 EPOCH 1244
2024-02-03 09:24:25,975 Epoch 1244: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.08 
2024-02-03 09:24:25,976 EPOCH 1245
2024-02-03 09:24:26,571 [Epoch: 1245 Step: 00042300] Batch Recognition Loss:   0.000771 => Gls Tokens per Sec:     1733 || Batch Translation Loss:   0.074062 => Txt Tokens per Sec:     5019 || Lr: 0.000050
2024-02-03 09:24:30,960 Epoch 1245: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.36 
2024-02-03 09:24:30,961 EPOCH 1246
2024-02-03 09:24:35,789 Epoch 1246: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.15 
2024-02-03 09:24:35,789 EPOCH 1247
2024-02-03 09:24:40,528 Epoch 1247: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.02 
2024-02-03 09:24:40,529 EPOCH 1248
2024-02-03 09:24:40,760 [Epoch: 1248 Step: 00042400] Batch Recognition Loss:   0.002792 => Gls Tokens per Sec:     2783 || Batch Translation Loss:   0.016731 => Txt Tokens per Sec:     7935 || Lr: 0.000050
2024-02-03 09:24:45,356 Epoch 1248: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.98 
2024-02-03 09:24:45,357 EPOCH 1249
2024-02-03 09:24:50,420 Epoch 1249: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.74 
2024-02-03 09:24:50,421 EPOCH 1250
2024-02-03 09:24:55,367 [Epoch: 1250 Step: 00042500] Batch Recognition Loss:   0.000324 => Gls Tokens per Sec:     2149 || Batch Translation Loss:   0.025128 => Txt Tokens per Sec:     5977 || Lr: 0.000050
2024-02-03 09:24:55,368 Epoch 1250: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.92 
2024-02-03 09:24:55,368 EPOCH 1251
2024-02-03 09:25:00,091 Epoch 1251: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.91 
2024-02-03 09:25:00,092 EPOCH 1252
2024-02-03 09:25:04,822 Epoch 1252: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.04 
2024-02-03 09:25:04,822 EPOCH 1253
2024-02-03 09:25:09,335 [Epoch: 1253 Step: 00042600] Batch Recognition Loss:   0.000206 => Gls Tokens per Sec:     2214 || Batch Translation Loss:   0.020824 => Txt Tokens per Sec:     6166 || Lr: 0.000050
2024-02-03 09:25:09,612 Epoch 1253: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.28 
2024-02-03 09:25:09,612 EPOCH 1254
2024-02-03 09:25:14,387 Epoch 1254: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.31 
2024-02-03 09:25:14,387 EPOCH 1255
2024-02-03 09:25:19,176 Epoch 1255: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.23 
2024-02-03 09:25:19,177 EPOCH 1256
2024-02-03 09:25:23,484 [Epoch: 1256 Step: 00042700] Batch Recognition Loss:   0.003972 => Gls Tokens per Sec:     2171 || Batch Translation Loss:   0.010145 => Txt Tokens per Sec:     6001 || Lr: 0.000050
2024-02-03 09:25:24,020 Epoch 1256: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.47 
2024-02-03 09:25:24,021 EPOCH 1257
2024-02-03 09:25:28,345 Epoch 1257: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.57 
2024-02-03 09:25:28,345 EPOCH 1258
2024-02-03 09:25:33,397 Epoch 1258: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.29 
2024-02-03 09:25:33,398 EPOCH 1259
2024-02-03 09:25:37,170 [Epoch: 1259 Step: 00042800] Batch Recognition Loss:   0.000555 => Gls Tokens per Sec:     2310 || Batch Translation Loss:   0.020594 => Txt Tokens per Sec:     6304 || Lr: 0.000050
2024-02-03 09:25:38,405 Epoch 1259: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.84 
2024-02-03 09:25:38,405 EPOCH 1260
2024-02-03 09:25:43,345 Epoch 1260: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.80 
2024-02-03 09:25:43,346 EPOCH 1261
2024-02-03 09:25:48,404 Epoch 1261: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.11 
2024-02-03 09:25:48,404 EPOCH 1262
2024-02-03 09:25:52,150 [Epoch: 1262 Step: 00042900] Batch Recognition Loss:   0.000611 => Gls Tokens per Sec:     2155 || Batch Translation Loss:   0.040041 => Txt Tokens per Sec:     6155 || Lr: 0.000050
2024-02-03 09:25:52,960 Epoch 1262: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.67 
2024-02-03 09:25:52,960 EPOCH 1263
2024-02-03 09:25:57,714 Epoch 1263: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.78 
2024-02-03 09:25:57,714 EPOCH 1264
2024-02-03 09:26:02,253 Epoch 1264: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.69 
2024-02-03 09:26:02,253 EPOCH 1265
2024-02-03 09:26:05,732 [Epoch: 1265 Step: 00043000] Batch Recognition Loss:   0.001068 => Gls Tokens per Sec:     2208 || Batch Translation Loss:   0.023403 => Txt Tokens per Sec:     6064 || Lr: 0.000050
2024-02-03 09:26:07,190 Epoch 1265: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.79 
2024-02-03 09:26:07,190 EPOCH 1266
2024-02-03 09:26:11,688 Epoch 1266: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.69 
2024-02-03 09:26:11,688 EPOCH 1267
2024-02-03 09:26:16,582 Epoch 1267: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-03 09:26:16,582 EPOCH 1268
2024-02-03 09:26:19,738 [Epoch: 1268 Step: 00043100] Batch Recognition Loss:   0.000346 => Gls Tokens per Sec:     2152 || Batch Translation Loss:   0.013046 => Txt Tokens per Sec:     6332 || Lr: 0.000050
2024-02-03 09:26:21,169 Epoch 1268: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.55 
2024-02-03 09:26:21,169 EPOCH 1269
2024-02-03 09:26:25,818 Epoch 1269: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.58 
2024-02-03 09:26:25,818 EPOCH 1270
2024-02-03 09:26:30,557 Epoch 1270: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.64 
2024-02-03 09:26:30,558 EPOCH 1271
2024-02-03 09:26:33,181 [Epoch: 1271 Step: 00043200] Batch Recognition Loss:   0.000431 => Gls Tokens per Sec:     2441 || Batch Translation Loss:   0.011573 => Txt Tokens per Sec:     6669 || Lr: 0.000050
2024-02-03 09:26:35,036 Epoch 1271: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.96 
2024-02-03 09:26:35,036 EPOCH 1272
2024-02-03 09:26:40,197 Epoch 1272: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.02 
2024-02-03 09:26:40,198 EPOCH 1273
2024-02-03 09:26:44,861 Epoch 1273: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.07 
2024-02-03 09:26:44,861 EPOCH 1274
2024-02-03 09:26:47,432 [Epoch: 1274 Step: 00043300] Batch Recognition Loss:   0.000420 => Gls Tokens per Sec:     2241 || Batch Translation Loss:   0.020627 => Txt Tokens per Sec:     6203 || Lr: 0.000050
2024-02-03 09:26:49,807 Epoch 1274: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.20 
2024-02-03 09:26:49,808 EPOCH 1275
2024-02-03 09:26:54,210 Epoch 1275: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.91 
2024-02-03 09:26:54,210 EPOCH 1276
2024-02-03 09:26:59,115 Epoch 1276: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.80 
2024-02-03 09:26:59,116 EPOCH 1277
2024-02-03 09:27:01,027 [Epoch: 1277 Step: 00043400] Batch Recognition Loss:   0.000827 => Gls Tokens per Sec:     2680 || Batch Translation Loss:   0.038389 => Txt Tokens per Sec:     7340 || Lr: 0.000050
2024-02-03 09:27:03,590 Epoch 1277: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.79 
2024-02-03 09:27:03,591 EPOCH 1278
2024-02-03 09:27:08,483 Epoch 1278: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.85 
2024-02-03 09:27:08,484 EPOCH 1279
2024-02-03 09:27:12,641 Epoch 1279: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.17 
2024-02-03 09:27:12,641 EPOCH 1280
2024-02-03 09:27:14,616 [Epoch: 1280 Step: 00043500] Batch Recognition Loss:   0.000176 => Gls Tokens per Sec:     2270 || Batch Translation Loss:   0.017458 => Txt Tokens per Sec:     6661 || Lr: 0.000050
2024-02-03 09:27:17,409 Epoch 1280: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.41 
2024-02-03 09:27:17,410 EPOCH 1281
2024-02-03 09:27:21,772 Epoch 1281: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.46 
2024-02-03 09:27:21,772 EPOCH 1282
2024-02-03 09:27:26,788 Epoch 1282: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.11 
2024-02-03 09:27:26,788 EPOCH 1283
2024-02-03 09:27:28,134 [Epoch: 1283 Step: 00043600] Batch Recognition Loss:   0.001210 => Gls Tokens per Sec:     2671 || Batch Translation Loss:   0.020845 => Txt Tokens per Sec:     7156 || Lr: 0.000050
2024-02-03 09:27:31,440 Epoch 1283: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.90 
2024-02-03 09:27:31,441 EPOCH 1284
2024-02-03 09:27:36,180 Epoch 1284: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.86 
2024-02-03 09:27:36,180 EPOCH 1285
2024-02-03 09:27:40,803 Epoch 1285: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.47 
2024-02-03 09:27:40,803 EPOCH 1286
2024-02-03 09:27:42,359 [Epoch: 1286 Step: 00043700] Batch Recognition Loss:   0.000781 => Gls Tokens per Sec:     1897 || Batch Translation Loss:   0.222785 => Txt Tokens per Sec:     5654 || Lr: 0.000050
2024-02-03 09:27:45,303 Epoch 1286: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.81 
2024-02-03 09:27:45,303 EPOCH 1287
2024-02-03 09:27:50,181 Epoch 1287: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.36 
2024-02-03 09:27:50,181 EPOCH 1288
2024-02-03 09:27:54,530 Epoch 1288: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.06 
2024-02-03 09:27:54,530 EPOCH 1289
2024-02-03 09:27:55,470 [Epoch: 1289 Step: 00043800] Batch Recognition Loss:   0.000601 => Gls Tokens per Sec:     2459 || Batch Translation Loss:   0.020909 => Txt Tokens per Sec:     6577 || Lr: 0.000050
2024-02-03 09:27:59,432 Epoch 1289: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.85 
2024-02-03 09:27:59,432 EPOCH 1290
2024-02-03 09:28:03,891 Epoch 1290: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.68 
2024-02-03 09:28:03,891 EPOCH 1291
2024-02-03 09:28:08,839 Epoch 1291: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.66 
2024-02-03 09:28:08,840 EPOCH 1292
2024-02-03 09:28:09,544 [Epoch: 1292 Step: 00043900] Batch Recognition Loss:   0.005682 => Gls Tokens per Sec:     2725 || Batch Translation Loss:   0.023163 => Txt Tokens per Sec:     7059 || Lr: 0.000050
2024-02-03 09:28:13,414 Epoch 1292: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.61 
2024-02-03 09:28:13,415 EPOCH 1293
2024-02-03 09:28:18,130 Epoch 1293: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.55 
2024-02-03 09:28:18,130 EPOCH 1294
2024-02-03 09:28:22,176 Epoch 1294: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.57 
2024-02-03 09:28:22,176 EPOCH 1295
2024-02-03 09:28:22,642 [Epoch: 1295 Step: 00044000] Batch Recognition Loss:   0.000746 => Gls Tokens per Sec:     2220 || Batch Translation Loss:   0.015881 => Txt Tokens per Sec:     6334 || Lr: 0.000050
2024-02-03 09:28:31,128 Validation result at epoch 1295, step    44000: duration: 8.4861s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.30636	Translation Loss: 88716.81250	PPL: 10041.26562
	Eval Metric: BLEU
	WER 3.25	(DEL: 0.14,	INS: 0.00,	SUB: 3.11)
	BLEU-4 0.32	(BLEU-1: 10.45,	BLEU-2: 2.56,	BLEU-3: 0.70,	BLEU-4: 0.32)
	CHRF 16.82	ROUGE 8.38
2024-02-03 09:28:31,129 Logging Recognition and Translation Outputs
2024-02-03 09:28:31,129 ========================================================================================================================
2024-02-03 09:28:31,130 Logging Sequence: 101_234.00
2024-02-03 09:28:31,130 	Gloss Reference :	A B+C+D+E        
2024-02-03 09:28:31,130 	Gloss Hypothesis:	A B+C+D+B+C+D+E+C
2024-02-03 09:28:31,130 	Gloss Alignment :	  S              
2024-02-03 09:28:31,130 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 09:28:31,132 	Text Reference  :	now similarly in       u-19    world   cup the winning runs were scored by             hitting a   six   
2024-02-03 09:28:31,132 	Text Hypothesis :	*** farewell  messages started pouring in  for dhoni   as   well as     congratulatory wishes  for jadeja
2024-02-03 09:28:31,132 	Text Alignment  :	D   S         S        S       S       S   S   S       S    S    S      S              S       S   S     
2024-02-03 09:28:31,132 ========================================================================================================================
2024-02-03 09:28:31,132 Logging Sequence: 156_260.00
2024-02-03 09:28:31,132 	Gloss Reference :	A B+C+D+E
2024-02-03 09:28:31,132 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 09:28:31,133 	Gloss Alignment :	         
2024-02-03 09:28:31,133 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 09:28:31,135 	Text Reference  :	this was because of some fine batting from nicholas pooran pooran smacked an     unbeaten knock of  137  runs off 55      balls and led miny to   victory
2024-02-03 09:28:31,136 	Text Hypothesis :	**** the final   of **** **** ******* **** ******** the    mlc    was     played between  mi    new york miny and seattle orcas sor on  30th july 2023   
2024-02-03 09:28:31,136 	Text Alignment  :	D    S   S          D    D    D       D    D        S      S      S       S      S        S     S   S    S    S   S       S     S   S   S    S    S      
2024-02-03 09:28:31,136 ========================================================================================================================
2024-02-03 09:28:31,136 Logging Sequence: 78_16.00
2024-02-03 09:28:31,136 	Gloss Reference :	A B+C+D+E
2024-02-03 09:28:31,136 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 09:28:31,136 	Gloss Alignment :	         
2024-02-03 09:28:31,137 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 09:28:31,137 	Text Reference  :	the full csk team    was very   sad
2024-02-03 09:28:31,137 	Text Hypothesis :	*** **** his bowling was really bad
2024-02-03 09:28:31,137 	Text Alignment  :	D   D    S   S           S      S  
2024-02-03 09:28:31,137 ========================================================================================================================
2024-02-03 09:28:31,137 Logging Sequence: 166_45.00
2024-02-03 09:28:31,138 	Gloss Reference :	A B+C+D+E
2024-02-03 09:28:31,138 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 09:28:31,138 	Gloss Alignment :	         
2024-02-03 09:28:31,138 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 09:28:31,139 	Text Reference  :	after an   odi     or   a   t20 ***** *** **** match the  next day    is a   rest day    
2024-02-03 09:28:31,139 	Text Hypothesis :	on    15th october 2022 the t20 world cup will be    held in   mumbai by 4th july cricket
2024-02-03 09:28:31,140 	Text Alignment  :	S     S    S       S    S       I     I   I    S     S    S    S      S  S   S    S      
2024-02-03 09:28:31,140 ========================================================================================================================
2024-02-03 09:28:31,140 Logging Sequence: 105_139.00
2024-02-03 09:28:31,140 	Gloss Reference :	A B+C+D+E
2024-02-03 09:28:31,140 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 09:28:31,140 	Gloss Alignment :	         
2024-02-03 09:28:31,140 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 09:28:31,141 	Text Reference  :	and now he has finally achieved his  dream  by defeating carlsen       
2024-02-03 09:28:31,141 	Text Hypothesis :	*** *** ** *** ******* symonds  also played 14 t20       internationals
2024-02-03 09:28:31,141 	Text Alignment  :	D   D   D  D   D       S        S    S      S  S         S             
2024-02-03 09:28:31,141 ========================================================================================================================
2024-02-03 09:28:34,803 Epoch 1295: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.69 
2024-02-03 09:28:34,804 EPOCH 1296
2024-02-03 09:28:39,795 Epoch 1296: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.85 
2024-02-03 09:28:39,795 EPOCH 1297
2024-02-03 09:28:44,410 Epoch 1297: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.01 
2024-02-03 09:28:44,411 EPOCH 1298
2024-02-03 09:28:44,640 [Epoch: 1298 Step: 00044100] Batch Recognition Loss:   0.000198 => Gls Tokens per Sec:     2819 || Batch Translation Loss:   0.010736 => Txt Tokens per Sec:     6449 || Lr: 0.000050
2024-02-03 09:28:49,124 Epoch 1298: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.51 
2024-02-03 09:28:49,124 EPOCH 1299
2024-02-03 09:28:53,852 Epoch 1299: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.94 
2024-02-03 09:28:53,853 EPOCH 1300
2024-02-03 09:28:58,291 [Epoch: 1300 Step: 00044200] Batch Recognition Loss:   0.000295 => Gls Tokens per Sec:     2396 || Batch Translation Loss:   0.018007 => Txt Tokens per Sec:     6662 || Lr: 0.000050
2024-02-03 09:28:58,291 Epoch 1300: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.79 
2024-02-03 09:28:58,292 EPOCH 1301
2024-02-03 09:29:03,246 Epoch 1301: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.84 
2024-02-03 09:29:03,247 EPOCH 1302
2024-02-03 09:29:07,818 Epoch 1302: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.84 
2024-02-03 09:29:07,819 EPOCH 1303
2024-02-03 09:29:12,287 [Epoch: 1303 Step: 00044300] Batch Recognition Loss:   0.000668 => Gls Tokens per Sec:     2237 || Batch Translation Loss:   0.062030 => Txt Tokens per Sec:     6158 || Lr: 0.000050
2024-02-03 09:29:12,617 Epoch 1303: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.85 
2024-02-03 09:29:12,617 EPOCH 1304
2024-02-03 09:29:17,252 Epoch 1304: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.87 
2024-02-03 09:29:17,252 EPOCH 1305
2024-02-03 09:29:21,784 Epoch 1305: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.26 
2024-02-03 09:29:21,784 EPOCH 1306
2024-02-03 09:29:26,058 [Epoch: 1306 Step: 00044400] Batch Recognition Loss:   0.000217 => Gls Tokens per Sec:     2188 || Batch Translation Loss:   0.023394 => Txt Tokens per Sec:     6008 || Lr: 0.000050
2024-02-03 09:29:26,708 Epoch 1306: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.71 
2024-02-03 09:29:26,708 EPOCH 1307
2024-02-03 09:29:31,084 Epoch 1307: Total Training Recognition Loss 0.08  Total Training Translation Loss 6.43 
2024-02-03 09:29:31,084 EPOCH 1308
2024-02-03 09:29:35,957 Epoch 1308: Total Training Recognition Loss 0.08  Total Training Translation Loss 4.14 
2024-02-03 09:29:35,958 EPOCH 1309
2024-02-03 09:29:39,638 [Epoch: 1309 Step: 00044500] Batch Recognition Loss:   0.000966 => Gls Tokens per Sec:     2369 || Batch Translation Loss:   0.028108 => Txt Tokens per Sec:     6590 || Lr: 0.000050
2024-02-03 09:29:40,337 Epoch 1309: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.28 
2024-02-03 09:29:40,337 EPOCH 1310
2024-02-03 09:29:44,956 Epoch 1310: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.83 
2024-02-03 09:29:44,957 EPOCH 1311
2024-02-03 09:29:49,417 Epoch 1311: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.70 
2024-02-03 09:29:49,417 EPOCH 1312
2024-02-03 09:29:52,720 [Epoch: 1312 Step: 00044600] Batch Recognition Loss:   0.000516 => Gls Tokens per Sec:     2445 || Batch Translation Loss:   0.014514 => Txt Tokens per Sec:     6442 || Lr: 0.000050
2024-02-03 09:29:54,166 Epoch 1312: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.65 
2024-02-03 09:29:54,166 EPOCH 1313
2024-02-03 09:29:58,610 Epoch 1313: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.62 
2024-02-03 09:29:58,610 EPOCH 1314
2024-02-03 09:30:03,542 Epoch 1314: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.56 
2024-02-03 09:30:03,542 EPOCH 1315
2024-02-03 09:30:06,683 [Epoch: 1315 Step: 00044700] Batch Recognition Loss:   0.001333 => Gls Tokens per Sec:     2366 || Batch Translation Loss:   0.005301 => Txt Tokens per Sec:     6699 || Lr: 0.000050
2024-02-03 09:30:07,813 Epoch 1315: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.56 
2024-02-03 09:30:07,813 EPOCH 1316
2024-02-03 09:30:12,798 Epoch 1316: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.69 
2024-02-03 09:30:12,798 EPOCH 1317
2024-02-03 09:30:17,447 Epoch 1317: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.73 
2024-02-03 09:30:17,448 EPOCH 1318
2024-02-03 09:30:20,613 [Epoch: 1318 Step: 00044800] Batch Recognition Loss:   0.000365 => Gls Tokens per Sec:     2225 || Batch Translation Loss:   0.013907 => Txt Tokens per Sec:     6193 || Lr: 0.000050
2024-02-03 09:30:22,083 Epoch 1318: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.75 
2024-02-03 09:30:22,083 EPOCH 1319
2024-02-03 09:30:26,841 Epoch 1319: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.64 
2024-02-03 09:30:26,841 EPOCH 1320
2024-02-03 09:30:31,294 Epoch 1320: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.61 
2024-02-03 09:30:31,294 EPOCH 1321
2024-02-03 09:30:34,245 [Epoch: 1321 Step: 00044900] Batch Recognition Loss:   0.000207 => Gls Tokens per Sec:     2170 || Batch Translation Loss:   0.012752 => Txt Tokens per Sec:     5916 || Lr: 0.000050
2024-02-03 09:30:36,232 Epoch 1321: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.63 
2024-02-03 09:30:36,232 EPOCH 1322
2024-02-03 09:30:40,728 Epoch 1322: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.81 
2024-02-03 09:30:40,729 EPOCH 1323
2024-02-03 09:30:45,448 Epoch 1323: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.69 
2024-02-03 09:30:45,448 EPOCH 1324
2024-02-03 09:30:47,702 [Epoch: 1324 Step: 00045000] Batch Recognition Loss:   0.001024 => Gls Tokens per Sec:     2556 || Batch Translation Loss:   0.019672 => Txt Tokens per Sec:     7188 || Lr: 0.000050
2024-02-03 09:30:49,792 Epoch 1324: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-03 09:30:49,793 EPOCH 1325
2024-02-03 09:30:54,628 Epoch 1325: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.64 
2024-02-03 09:30:54,629 EPOCH 1326
2024-02-03 09:30:58,909 Epoch 1326: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.78 
2024-02-03 09:30:58,910 EPOCH 1327
2024-02-03 09:31:01,524 [Epoch: 1327 Step: 00045100] Batch Recognition Loss:   0.000413 => Gls Tokens per Sec:     1959 || Batch Translation Loss:   0.021226 => Txt Tokens per Sec:     5608 || Lr: 0.000050
2024-02-03 09:31:03,795 Epoch 1327: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.65 
2024-02-03 09:31:03,795 EPOCH 1328
2024-02-03 09:31:08,507 Epoch 1328: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.83 
2024-02-03 09:31:08,507 EPOCH 1329
2024-02-03 09:31:13,005 Epoch 1329: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.71 
2024-02-03 09:31:13,006 EPOCH 1330
2024-02-03 09:31:14,925 [Epoch: 1330 Step: 00045200] Batch Recognition Loss:   0.000578 => Gls Tokens per Sec:     2204 || Batch Translation Loss:   0.013111 => Txt Tokens per Sec:     6090 || Lr: 0.000050
2024-02-03 09:31:17,937 Epoch 1330: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.64 
2024-02-03 09:31:17,937 EPOCH 1331
2024-02-03 09:31:22,382 Epoch 1331: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-03 09:31:22,383 EPOCH 1332
2024-02-03 09:31:27,277 Epoch 1332: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.62 
2024-02-03 09:31:27,278 EPOCH 1333
2024-02-03 09:31:28,486 [Epoch: 1333 Step: 00045300] Batch Recognition Loss:   0.000687 => Gls Tokens per Sec:     3180 || Batch Translation Loss:   0.008054 => Txt Tokens per Sec:     8415 || Lr: 0.000050
2024-02-03 09:31:31,838 Epoch 1333: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.72 
2024-02-03 09:31:31,839 EPOCH 1334
2024-02-03 09:31:36,744 Epoch 1334: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.07 
2024-02-03 09:31:36,744 EPOCH 1335
2024-02-03 09:31:41,295 Epoch 1335: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.58 
2024-02-03 09:31:41,295 EPOCH 1336
2024-02-03 09:31:43,059 [Epoch: 1336 Step: 00045400] Batch Recognition Loss:   0.000431 => Gls Tokens per Sec:     1815 || Batch Translation Loss:   0.063463 => Txt Tokens per Sec:     5466 || Lr: 0.000050
2024-02-03 09:31:46,093 Epoch 1336: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.67 
2024-02-03 09:31:46,093 EPOCH 1337
2024-02-03 09:31:50,552 Epoch 1337: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.13 
2024-02-03 09:31:50,553 EPOCH 1338
2024-02-03 09:31:55,242 Epoch 1338: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.92 
2024-02-03 09:31:55,242 EPOCH 1339
2024-02-03 09:31:56,433 [Epoch: 1339 Step: 00045500] Batch Recognition Loss:   0.000192 => Gls Tokens per Sec:     1944 || Batch Translation Loss:   0.015155 => Txt Tokens per Sec:     5540 || Lr: 0.000050
2024-02-03 09:31:59,733 Epoch 1339: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.85 
2024-02-03 09:31:59,733 EPOCH 1340
2024-02-03 09:32:04,434 Epoch 1340: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.79 
2024-02-03 09:32:04,435 EPOCH 1341
2024-02-03 09:32:09,360 Epoch 1341: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.77 
2024-02-03 09:32:09,361 EPOCH 1342
2024-02-03 09:32:10,331 [Epoch: 1342 Step: 00045600] Batch Recognition Loss:   0.002950 => Gls Tokens per Sec:     1981 || Batch Translation Loss:   0.010674 => Txt Tokens per Sec:     5805 || Lr: 0.000050
2024-02-03 09:32:13,823 Epoch 1342: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.68 
2024-02-03 09:32:13,823 EPOCH 1343
2024-02-03 09:32:18,709 Epoch 1343: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.74 
2024-02-03 09:32:18,710 EPOCH 1344
2024-02-03 09:32:23,207 Epoch 1344: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.78 
2024-02-03 09:32:23,208 EPOCH 1345
2024-02-03 09:32:23,900 [Epoch: 1345 Step: 00045700] Batch Recognition Loss:   0.002422 => Gls Tokens per Sec:     1853 || Batch Translation Loss:   0.023522 => Txt Tokens per Sec:     5770 || Lr: 0.000050
2024-02-03 09:32:27,849 Epoch 1345: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.06 
2024-02-03 09:32:27,850 EPOCH 1346
2024-02-03 09:32:32,405 Epoch 1346: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.71 
2024-02-03 09:32:32,405 EPOCH 1347
2024-02-03 09:32:37,102 Epoch 1347: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.83 
2024-02-03 09:32:37,102 EPOCH 1348
2024-02-03 09:32:37,399 [Epoch: 1348 Step: 00045800] Batch Recognition Loss:   0.000271 => Gls Tokens per Sec:     2162 || Batch Translation Loss:   0.017212 => Txt Tokens per Sec:     6622 || Lr: 0.000050
2024-02-03 09:32:41,842 Epoch 1348: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.88 
2024-02-03 09:32:41,842 EPOCH 1349
2024-02-03 09:32:46,321 Epoch 1349: Total Training Recognition Loss 0.06  Total Training Translation Loss 9.37 
2024-02-03 09:32:46,321 EPOCH 1350
2024-02-03 09:32:51,000 [Epoch: 1350 Step: 00045900] Batch Recognition Loss:   0.000966 => Gls Tokens per Sec:     2273 || Batch Translation Loss:   0.054289 => Txt Tokens per Sec:     6320 || Lr: 0.000050
2024-02-03 09:32:51,001 Epoch 1350: Total Training Recognition Loss 0.08  Total Training Translation Loss 4.18 
2024-02-03 09:32:51,001 EPOCH 1351
2024-02-03 09:32:55,525 Epoch 1351: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.86 
2024-02-03 09:32:55,525 EPOCH 1352
2024-02-03 09:33:00,389 Epoch 1352: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.01 
2024-02-03 09:33:00,390 EPOCH 1353
2024-02-03 09:33:04,485 [Epoch: 1353 Step: 00046000] Batch Recognition Loss:   0.004710 => Gls Tokens per Sec:     2441 || Batch Translation Loss:   0.009304 => Txt Tokens per Sec:     6737 || Lr: 0.000050
2024-02-03 09:33:12,984 Validation result at epoch 1353, step    46000: duration: 8.4991s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.24627	Translation Loss: 88096.55469	PPL: 9414.78711
	Eval Metric: BLEU
	WER 3.32	(DEL: 0.14,	INS: 0.00,	SUB: 3.18)
	BLEU-4 0.72	(BLEU-1: 11.13,	BLEU-2: 3.37,	BLEU-3: 1.32,	BLEU-4: 0.72)
	CHRF 17.31	ROUGE 9.11
2024-02-03 09:33:12,986 Logging Recognition and Translation Outputs
2024-02-03 09:33:12,986 ========================================================================================================================
2024-02-03 09:33:12,986 Logging Sequence: 83_108.00
2024-02-03 09:33:12,987 	Gloss Reference :	A B+C+D+E
2024-02-03 09:33:12,987 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 09:33:12,987 	Gloss Alignment :	         
2024-02-03 09:33:12,987 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 09:33:12,989 	Text Reference  :	eriksen' wife   sabrina kvist      jensen    rushed  to ****** ****** the pitch on    hearing about      her        husband
2024-02-03 09:33:12,989 	Text Hypothesis :	the      danish players surrounded christian eriksen to ensure during the ***** media for     constantly supporting him    
2024-02-03 09:33:12,989 	Text Alignment  :	S        S      S       S          S         S          I      I          D     S     S       S          S          S      
2024-02-03 09:33:12,989 ========================================================================================================================
2024-02-03 09:33:12,989 Logging Sequence: 171_150.00
2024-02-03 09:33:12,989 	Gloss Reference :	A B+C+D+E
2024-02-03 09:33:12,990 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 09:33:12,990 	Gloss Alignment :	         
2024-02-03 09:33:12,990 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 09:33:12,991 	Text Reference  :	as per rules players can   be  fined     for     purposely delaying a   match ** which runs    on   a       fixed schedule
2024-02-03 09:33:12,992 	Text Hypothesis :	** *** ***** ******* those who purchased tickets to        watch    the match at the   stadium were worried their support 
2024-02-03 09:33:12,992 	Text Alignment  :	D  D   D     D       S     S   S         S       S         S        S         I  S     S       S    S       S     S       
2024-02-03 09:33:12,992 ========================================================================================================================
2024-02-03 09:33:12,992 Logging Sequence: 72_139.00
2024-02-03 09:33:12,992 	Gloss Reference :	A B+C+D+E
2024-02-03 09:33:12,992 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 09:33:12,993 	Gloss Alignment :	         
2024-02-03 09:33:12,993 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 09:33:12,995 	Text Reference  :	when shah was taking a  u-turn all     those on         the   bike got down    and broke the     windshield of     the car     
2024-02-03 09:33:12,995 	Text Hypothesis :	**** **** *** 1      in the    stadium fans  especially women are  not allowed to  wear  clothes that       reveal the shoulder
2024-02-03 09:33:12,995 	Text Alignment  :	D    D    D   S      S  S      S       S     S          S     S    S   S       S   S     S       S          S          S       
2024-02-03 09:33:12,995 ========================================================================================================================
2024-02-03 09:33:12,995 Logging Sequence: 161_47.00
2024-02-03 09:33:12,995 	Gloss Reference :	A B+C+D+E
2024-02-03 09:33:12,996 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 09:33:12,996 	Gloss Alignment :	         
2024-02-03 09:33:12,996 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 09:33:12,997 	Text Reference  :	**** *** * ***** he  requested confidentiality as        he   was      planning to  make an    official announcement
2024-02-03 09:33:12,997 	Text Hypothesis :	they get a model for people    were            allegedly seen breaking tv       and get  total of       infection   
2024-02-03 09:33:12,997 	Text Alignment  :	I    I   I I     S   S         S               S         S    S        S        S   S    S     S        S           
2024-02-03 09:33:12,998 ========================================================================================================================
2024-02-03 09:33:12,998 Logging Sequence: 118_284.00
2024-02-03 09:33:12,998 	Gloss Reference :	A B+C+D+E
2024-02-03 09:33:12,998 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 09:33:12,998 	Gloss Alignment :	         
2024-02-03 09:33:12,998 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 09:33:12,999 	Text Reference  :	the second time they won in    1986  under the  leadership of   diego    maradona
2024-02-03 09:33:12,999 	Text Hypothesis :	the ****** **** **** *** third match was   held on         23rd november 2022    
2024-02-03 09:33:13,000 	Text Alignment  :	    D      D    D    D   S     S     S     S    S          S    S        S       
2024-02-03 09:33:13,000 ========================================================================================================================
2024-02-03 09:33:13,334 Epoch 1353: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.76 
2024-02-03 09:33:13,335 EPOCH 1354
2024-02-03 09:33:18,163 Epoch 1354: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.76 
2024-02-03 09:33:18,164 EPOCH 1355
2024-02-03 09:33:22,696 Epoch 1355: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.74 
2024-02-03 09:33:22,696 EPOCH 1356
2024-02-03 09:33:26,962 [Epoch: 1356 Step: 00046100] Batch Recognition Loss:   0.000430 => Gls Tokens per Sec:     2192 || Batch Translation Loss:   0.013997 => Txt Tokens per Sec:     6240 || Lr: 0.000050
2024-02-03 09:33:27,309 Epoch 1356: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.71 
2024-02-03 09:33:27,310 EPOCH 1357
2024-02-03 09:33:32,140 Epoch 1357: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.65 
2024-02-03 09:33:32,140 EPOCH 1358
2024-02-03 09:33:36,480 Epoch 1358: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.60 
2024-02-03 09:33:36,480 EPOCH 1359
2024-02-03 09:33:40,596 [Epoch: 1359 Step: 00046200] Batch Recognition Loss:   0.000909 => Gls Tokens per Sec:     2117 || Batch Translation Loss:   0.018141 => Txt Tokens per Sec:     5845 || Lr: 0.000050
2024-02-03 09:33:41,471 Epoch 1359: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.59 
2024-02-03 09:33:41,472 EPOCH 1360
2024-02-03 09:33:45,962 Epoch 1360: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.57 
2024-02-03 09:33:45,963 EPOCH 1361
2024-02-03 09:33:50,734 Epoch 1361: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.57 
2024-02-03 09:33:50,734 EPOCH 1362
2024-02-03 09:33:54,510 [Epoch: 1362 Step: 00046300] Batch Recognition Loss:   0.000260 => Gls Tokens per Sec:     2204 || Batch Translation Loss:   0.010843 => Txt Tokens per Sec:     6277 || Lr: 0.000050
2024-02-03 09:33:55,415 Epoch 1362: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.56 
2024-02-03 09:33:55,415 EPOCH 1363
2024-02-03 09:33:59,919 Epoch 1363: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.53 
2024-02-03 09:33:59,919 EPOCH 1364
2024-02-03 09:34:04,810 Epoch 1364: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-03 09:34:04,811 EPOCH 1365
2024-02-03 09:34:07,727 [Epoch: 1365 Step: 00046400] Batch Recognition Loss:   0.000427 => Gls Tokens per Sec:     2548 || Batch Translation Loss:   0.012926 => Txt Tokens per Sec:     7045 || Lr: 0.000050
2024-02-03 09:34:09,320 Epoch 1365: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.69 
2024-02-03 09:34:09,320 EPOCH 1366
2024-02-03 09:34:14,212 Epoch 1366: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.73 
2024-02-03 09:34:14,213 EPOCH 1367
2024-02-03 09:34:18,405 Epoch 1367: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.62 
2024-02-03 09:34:18,406 EPOCH 1368
2024-02-03 09:34:21,988 [Epoch: 1368 Step: 00046500] Batch Recognition Loss:   0.000731 => Gls Tokens per Sec:     1966 || Batch Translation Loss:   0.059252 => Txt Tokens per Sec:     5806 || Lr: 0.000050
2024-02-03 09:34:23,338 Epoch 1368: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.83 
2024-02-03 09:34:23,338 EPOCH 1369
2024-02-03 09:34:27,633 Epoch 1369: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.81 
2024-02-03 09:34:27,634 EPOCH 1370
2024-02-03 09:34:32,652 Epoch 1370: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.56 
2024-02-03 09:34:32,653 EPOCH 1371
2024-02-03 09:34:35,492 [Epoch: 1371 Step: 00046600] Batch Recognition Loss:   0.000223 => Gls Tokens per Sec:     2167 || Batch Translation Loss:   0.012488 => Txt Tokens per Sec:     5876 || Lr: 0.000050
2024-02-03 09:34:37,539 Epoch 1371: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-03 09:34:37,539 EPOCH 1372
2024-02-03 09:34:42,227 Epoch 1372: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.77 
2024-02-03 09:34:42,227 EPOCH 1373
2024-02-03 09:34:47,048 Epoch 1373: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.32 
2024-02-03 09:34:47,048 EPOCH 1374
2024-02-03 09:34:49,015 [Epoch: 1374 Step: 00046700] Batch Recognition Loss:   0.000137 => Gls Tokens per Sec:     2802 || Batch Translation Loss:   0.023247 => Txt Tokens per Sec:     7526 || Lr: 0.000050
2024-02-03 09:34:51,330 Epoch 1374: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.56 
2024-02-03 09:34:51,331 EPOCH 1375
2024-02-03 09:34:56,309 Epoch 1375: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.37 
2024-02-03 09:34:56,309 EPOCH 1376
2024-02-03 09:35:00,855 Epoch 1376: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.15 
2024-02-03 09:35:00,855 EPOCH 1377
2024-02-03 09:35:03,332 [Epoch: 1377 Step: 00046800] Batch Recognition Loss:   0.000459 => Gls Tokens per Sec:     2069 || Batch Translation Loss:   0.027876 => Txt Tokens per Sec:     5760 || Lr: 0.000050
2024-02-03 09:35:05,546 Epoch 1377: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.95 
2024-02-03 09:35:05,546 EPOCH 1378
2024-02-03 09:35:09,633 Epoch 1378: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.98 
2024-02-03 09:35:09,633 EPOCH 1379
2024-02-03 09:35:13,741 Epoch 1379: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.71 
2024-02-03 09:35:13,741 EPOCH 1380
2024-02-03 09:35:15,427 [Epoch: 1380 Step: 00046900] Batch Recognition Loss:   0.000150 => Gls Tokens per Sec:     2511 || Batch Translation Loss:   0.015599 => Txt Tokens per Sec:     7150 || Lr: 0.000050
2024-02-03 09:35:18,490 Epoch 1380: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.62 
2024-02-03 09:35:18,490 EPOCH 1381
2024-02-03 09:35:23,109 Epoch 1381: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.63 
2024-02-03 09:35:23,109 EPOCH 1382
2024-02-03 09:35:28,086 Epoch 1382: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.62 
2024-02-03 09:35:28,087 EPOCH 1383
2024-02-03 09:35:29,493 [Epoch: 1383 Step: 00047000] Batch Recognition Loss:   0.000196 => Gls Tokens per Sec:     2732 || Batch Translation Loss:   0.015711 => Txt Tokens per Sec:     7662 || Lr: 0.000050
2024-02-03 09:35:32,602 Epoch 1383: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.70 
2024-02-03 09:35:32,603 EPOCH 1384
2024-02-03 09:35:37,325 Epoch 1384: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.79 
2024-02-03 09:35:37,326 EPOCH 1385
2024-02-03 09:35:42,079 Epoch 1385: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.92 
2024-02-03 09:35:42,079 EPOCH 1386
2024-02-03 09:35:43,313 [Epoch: 1386 Step: 00047100] Batch Recognition Loss:   0.000221 => Gls Tokens per Sec:     2594 || Batch Translation Loss:   0.020835 => Txt Tokens per Sec:     6722 || Lr: 0.000050
2024-02-03 09:35:46,483 Epoch 1386: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.00 
2024-02-03 09:35:46,483 EPOCH 1387
2024-02-03 09:35:51,385 Epoch 1387: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.99 
2024-02-03 09:35:51,386 EPOCH 1388
2024-02-03 09:35:55,842 Epoch 1388: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.29 
2024-02-03 09:35:55,843 EPOCH 1389
2024-02-03 09:35:57,141 [Epoch: 1389 Step: 00047200] Batch Recognition Loss:   0.001153 => Gls Tokens per Sec:     1781 || Batch Translation Loss:   0.045708 => Txt Tokens per Sec:     4875 || Lr: 0.000050
2024-02-03 09:36:00,713 Epoch 1389: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.10 
2024-02-03 09:36:00,713 EPOCH 1390
2024-02-03 09:36:05,317 Epoch 1390: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.46 
2024-02-03 09:36:05,317 EPOCH 1391
2024-02-03 09:36:09,888 Epoch 1391: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.72 
2024-02-03 09:36:09,888 EPOCH 1392
2024-02-03 09:36:10,461 [Epoch: 1392 Step: 00047300] Batch Recognition Loss:   0.001756 => Gls Tokens per Sec:     3357 || Batch Translation Loss:   0.045147 => Txt Tokens per Sec:     8603 || Lr: 0.000050
2024-02-03 09:36:14,700 Epoch 1392: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.53 
2024-02-03 09:36:14,700 EPOCH 1393
2024-02-03 09:36:19,135 Epoch 1393: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.62 
2024-02-03 09:36:19,136 EPOCH 1394
2024-02-03 09:36:24,117 Epoch 1394: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.93 
2024-02-03 09:36:24,117 EPOCH 1395
2024-02-03 09:36:24,756 [Epoch: 1395 Step: 00047400] Batch Recognition Loss:   0.003804 => Gls Tokens per Sec:     2005 || Batch Translation Loss:   0.030876 => Txt Tokens per Sec:     6309 || Lr: 0.000050
2024-02-03 09:36:28,614 Epoch 1395: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.93 
2024-02-03 09:36:28,615 EPOCH 1396
2024-02-03 09:36:33,312 Epoch 1396: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.82 
2024-02-03 09:36:33,312 EPOCH 1397
2024-02-03 09:36:37,555 Epoch 1397: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.67 
2024-02-03 09:36:37,556 EPOCH 1398
2024-02-03 09:36:38,150 [Epoch: 1398 Step: 00047500] Batch Recognition Loss:   0.001420 => Gls Tokens per Sec:     1079 || Batch Translation Loss:   0.043071 => Txt Tokens per Sec:     3290 || Lr: 0.000050
2024-02-03 09:36:42,634 Epoch 1398: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.91 
2024-02-03 09:36:42,635 EPOCH 1399
2024-02-03 09:36:47,074 Epoch 1399: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.71 
2024-02-03 09:36:47,075 EPOCH 1400
2024-02-03 09:36:51,999 [Epoch: 1400 Step: 00047600] Batch Recognition Loss:   0.001076 => Gls Tokens per Sec:     2159 || Batch Translation Loss:   0.015945 => Txt Tokens per Sec:     6005 || Lr: 0.000050
2024-02-03 09:36:52,000 Epoch 1400: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.89 
2024-02-03 09:36:52,000 EPOCH 1401
2024-02-03 09:36:56,631 Epoch 1401: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.29 
2024-02-03 09:36:56,632 EPOCH 1402
2024-02-03 09:37:01,595 Epoch 1402: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.96 
2024-02-03 09:37:01,595 EPOCH 1403
2024-02-03 09:37:06,364 [Epoch: 1403 Step: 00047700] Batch Recognition Loss:   0.000411 => Gls Tokens per Sec:     2096 || Batch Translation Loss:   0.020260 => Txt Tokens per Sec:     5874 || Lr: 0.000050
2024-02-03 09:37:06,558 Epoch 1403: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.87 
2024-02-03 09:37:06,559 EPOCH 1404
2024-02-03 09:37:10,996 Epoch 1404: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.72 
2024-02-03 09:37:10,996 EPOCH 1405
2024-02-03 09:37:15,929 Epoch 1405: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.81 
2024-02-03 09:37:15,930 EPOCH 1406
2024-02-03 09:37:20,070 [Epoch: 1406 Step: 00047800] Batch Recognition Loss:   0.000325 => Gls Tokens per Sec:     2259 || Batch Translation Loss:   0.011230 => Txt Tokens per Sec:     6364 || Lr: 0.000050
2024-02-03 09:37:20,498 Epoch 1406: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.73 
2024-02-03 09:37:20,498 EPOCH 1407
2024-02-03 09:37:25,189 Epoch 1407: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.70 
2024-02-03 09:37:25,189 EPOCH 1408
2024-02-03 09:37:29,930 Epoch 1408: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.93 
2024-02-03 09:37:29,931 EPOCH 1409
2024-02-03 09:37:33,384 [Epoch: 1409 Step: 00047900] Batch Recognition Loss:   0.000328 => Gls Tokens per Sec:     2523 || Batch Translation Loss:   0.024162 => Txt Tokens per Sec:     6987 || Lr: 0.000050
2024-02-03 09:37:34,402 Epoch 1409: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.22 
2024-02-03 09:37:34,402 EPOCH 1410
2024-02-03 09:37:39,388 Epoch 1410: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.95 
2024-02-03 09:37:39,389 EPOCH 1411
2024-02-03 09:37:43,853 Epoch 1411: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.73 
2024-02-03 09:37:43,854 EPOCH 1412
2024-02-03 09:37:47,553 [Epoch: 1412 Step: 00048000] Batch Recognition Loss:   0.000179 => Gls Tokens per Sec:     2182 || Batch Translation Loss:   0.037964 => Txt Tokens per Sec:     6081 || Lr: 0.000050
2024-02-03 09:37:56,183 Validation result at epoch 1412, step    48000: duration: 8.6299s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.42702	Translation Loss: 90177.09375	PPL: 11685.79102
	Eval Metric: BLEU
	WER 3.25	(DEL: 0.14,	INS: 0.00,	SUB: 3.11)
	BLEU-4 0.62	(BLEU-1: 9.87,	BLEU-2: 3.01,	BLEU-3: 1.27,	BLEU-4: 0.62)
	CHRF 16.94	ROUGE 8.11
2024-02-03 09:37:56,184 Logging Recognition and Translation Outputs
2024-02-03 09:37:56,184 ========================================================================================================================
2024-02-03 09:37:56,185 Logging Sequence: 155_56.00
2024-02-03 09:37:56,185 	Gloss Reference :	A B+C+D+E
2024-02-03 09:37:56,185 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 09:37:56,185 	Gloss Alignment :	         
2024-02-03 09:37:56,185 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 09:37:56,186 	Text Reference  :	**** ***** ****** the *********** **** *** ********* team would have   been        disqualified
2024-02-03 09:37:56,186 	Text Hypothesis :	they never forget the interaction with our champions and  will  always accompanies him         
2024-02-03 09:37:56,186 	Text Alignment  :	I    I     I          I           I    I   I         S    S     S      S           S           
2024-02-03 09:37:56,186 ========================================================================================================================
2024-02-03 09:37:56,186 Logging Sequence: 161_170.00
2024-02-03 09:37:56,186 	Gloss Reference :	A B+C+D+E
2024-02-03 09:37:56,187 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 09:37:56,187 	Gloss Alignment :	         
2024-02-03 09:37:56,187 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 09:37:56,188 	Text Reference  :	**** however when the bcci replaced him with rohit sharma he decided to step down      as         test captain as  well
2024-02-03 09:37:56,188 	Text Hypothesis :	well known   as   the **** ******** *** **** ***** ****** ** ******* ** **** chartered accountant who  oversaw the game
2024-02-03 09:37:56,188 	Text Alignment  :	I    S       S        D    D        D   D    D     D      D  D       D  D    S         S          S    S       S   S   
2024-02-03 09:37:56,188 ========================================================================================================================
2024-02-03 09:37:56,189 Logging Sequence: 88_13.00
2024-02-03 09:37:56,189 	Gloss Reference :	A B+C+D+E
2024-02-03 09:37:56,189 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 09:37:56,189 	Gloss Alignment :	         
2024-02-03 09:37:56,189 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 09:37:56,191 	Text Reference  :	while his     fans were   elated  by the victory there were some who are     jealous and want    to ****** kill messi    
2024-02-03 09:37:56,191 	Text Hypothesis :	***** however a    police officer in the ******* ***** **** **** *** message was     not allowed to insult any  community
2024-02-03 09:37:56,191 	Text Alignment  :	D     S       S    S      S       S      D       D     D    D    D   S       S       S   S          I      S    S        
2024-02-03 09:37:56,191 ========================================================================================================================
2024-02-03 09:37:56,191 Logging Sequence: 103_25.00
2024-02-03 09:37:56,191 	Gloss Reference :	A B+C+D+E
2024-02-03 09:37:56,191 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 09:37:56,192 	Gloss Alignment :	         
2024-02-03 09:37:56,192 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 09:37:56,192 	Text Reference  :	earlier ish news had released a  video when  india had     won 6             medals       
2024-02-03 09:37:56,193 	Text Hypothesis :	******* *** **** *** ******** as the   world cup   started in  weightlifting championships
2024-02-03 09:37:56,193 	Text Alignment  :	D       D   D    D   D        S  S     S     S     S       S   S             S            
2024-02-03 09:37:56,193 ========================================================================================================================
2024-02-03 09:37:56,193 Logging Sequence: 85_36.00
2024-02-03 09:37:56,193 	Gloss Reference :	A B+C+D+E
2024-02-03 09:37:56,193 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 09:37:56,194 	Gloss Alignment :	         
2024-02-03 09:37:56,194 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 09:37:56,195 	Text Reference  :	symonds has scored 2   centuries in 26   tests that he played for his country
2024-02-03 09:37:56,195 	Text Hypothesis :	******* *** in     the tweet     he also said  that he played *** his injury 
2024-02-03 09:37:56,195 	Text Alignment  :	D       D   S      S   S         S  S    S                    D       S      
2024-02-03 09:37:56,195 ========================================================================================================================
2024-02-03 09:37:57,340 Epoch 1412: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.73 
2024-02-03 09:37:57,340 EPOCH 1413
2024-02-03 09:38:02,404 Epoch 1413: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.96 
2024-02-03 09:38:02,404 EPOCH 1414
2024-02-03 09:38:07,332 Epoch 1414: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.13 
2024-02-03 09:38:07,333 EPOCH 1415
2024-02-03 09:38:10,810 [Epoch: 1415 Step: 00048100] Batch Recognition Loss:   0.001155 => Gls Tokens per Sec:     2209 || Batch Translation Loss:   0.014140 => Txt Tokens per Sec:     6231 || Lr: 0.000050
2024-02-03 09:38:12,117 Epoch 1415: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.68 
2024-02-03 09:38:12,117 EPOCH 1416
2024-02-03 09:38:16,929 Epoch 1416: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.62 
2024-02-03 09:38:16,930 EPOCH 1417
2024-02-03 09:38:21,693 Epoch 1417: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.58 
2024-02-03 09:38:21,694 EPOCH 1418
2024-02-03 09:38:24,565 [Epoch: 1418 Step: 00048200] Batch Recognition Loss:   0.000164 => Gls Tokens per Sec:     2366 || Batch Translation Loss:   0.014845 => Txt Tokens per Sec:     6674 || Lr: 0.000050
2024-02-03 09:38:26,155 Epoch 1418: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-03 09:38:26,155 EPOCH 1419
2024-02-03 09:38:30,958 Epoch 1419: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.72 
2024-02-03 09:38:30,958 EPOCH 1420
2024-02-03 09:38:35,488 Epoch 1420: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.80 
2024-02-03 09:38:35,488 EPOCH 1421
2024-02-03 09:38:38,124 [Epoch: 1421 Step: 00048300] Batch Recognition Loss:   0.000225 => Gls Tokens per Sec:     2334 || Batch Translation Loss:   0.017494 => Txt Tokens per Sec:     6604 || Lr: 0.000050
2024-02-03 09:38:40,131 Epoch 1421: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.15 
2024-02-03 09:38:40,131 EPOCH 1422
2024-02-03 09:38:44,347 Epoch 1422: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.21 
2024-02-03 09:38:44,348 EPOCH 1423
2024-02-03 09:38:49,234 Epoch 1423: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.78 
2024-02-03 09:38:49,234 EPOCH 1424
2024-02-03 09:38:51,628 [Epoch: 1424 Step: 00048400] Batch Recognition Loss:   0.001346 => Gls Tokens per Sec:     2303 || Batch Translation Loss:   0.025289 => Txt Tokens per Sec:     6476 || Lr: 0.000050
2024-02-03 09:38:53,468 Epoch 1424: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.68 
2024-02-03 09:38:53,468 EPOCH 1425
2024-02-03 09:38:58,369 Epoch 1425: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.89 
2024-02-03 09:38:58,370 EPOCH 1426
2024-02-03 09:39:02,876 Epoch 1426: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.21 
2024-02-03 09:39:02,877 EPOCH 1427
2024-02-03 09:39:04,978 [Epoch: 1427 Step: 00048500] Batch Recognition Loss:   0.000616 => Gls Tokens per Sec:     2438 || Batch Translation Loss:   0.086296 => Txt Tokens per Sec:     6719 || Lr: 0.000050
2024-02-03 09:39:07,607 Epoch 1427: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.41 
2024-02-03 09:39:07,608 EPOCH 1428
2024-02-03 09:39:12,356 Epoch 1428: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.57 
2024-02-03 09:39:12,356 EPOCH 1429
2024-02-03 09:39:16,812 Epoch 1429: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.92 
2024-02-03 09:39:16,812 EPOCH 1430
2024-02-03 09:39:19,091 [Epoch: 1430 Step: 00048600] Batch Recognition Loss:   0.000433 => Gls Tokens per Sec:     1967 || Batch Translation Loss:   0.024512 => Txt Tokens per Sec:     5638 || Lr: 0.000050
2024-02-03 09:39:21,665 Epoch 1430: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.82 
2024-02-03 09:39:21,665 EPOCH 1431
2024-02-03 09:39:26,084 Epoch 1431: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.95 
2024-02-03 09:39:26,085 EPOCH 1432
2024-02-03 09:39:30,949 Epoch 1432: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.74 
2024-02-03 09:39:30,950 EPOCH 1433
2024-02-03 09:39:32,594 [Epoch: 1433 Step: 00048700] Batch Recognition Loss:   0.000271 => Gls Tokens per Sec:     2337 || Batch Translation Loss:   0.054510 => Txt Tokens per Sec:     6595 || Lr: 0.000050
2024-02-03 09:39:35,528 Epoch 1433: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.95 
2024-02-03 09:39:35,529 EPOCH 1434
2024-02-03 09:39:40,168 Epoch 1434: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.94 
2024-02-03 09:39:40,169 EPOCH 1435
2024-02-03 09:39:44,888 Epoch 1435: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.68 
2024-02-03 09:39:44,889 EPOCH 1436
2024-02-03 09:39:46,131 [Epoch: 1436 Step: 00048800] Batch Recognition Loss:   0.000180 => Gls Tokens per Sec:     2579 || Batch Translation Loss:   0.015350 => Txt Tokens per Sec:     7268 || Lr: 0.000050
2024-02-03 09:39:49,277 Epoch 1436: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.66 
2024-02-03 09:39:49,278 EPOCH 1437
2024-02-03 09:39:54,246 Epoch 1437: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.73 
2024-02-03 09:39:54,247 EPOCH 1438
2024-02-03 09:39:58,811 Epoch 1438: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.83 
2024-02-03 09:39:58,812 EPOCH 1439
2024-02-03 09:39:59,638 [Epoch: 1439 Step: 00048900] Batch Recognition Loss:   0.000200 => Gls Tokens per Sec:     3103 || Batch Translation Loss:   0.016226 => Txt Tokens per Sec:     6926 || Lr: 0.000050
2024-02-03 09:40:03,596 Epoch 1439: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.01 
2024-02-03 09:40:03,597 EPOCH 1440
2024-02-03 09:40:08,230 Epoch 1440: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.36 
2024-02-03 09:40:08,230 EPOCH 1441
2024-02-03 09:40:12,754 Epoch 1441: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.21 
2024-02-03 09:40:12,754 EPOCH 1442
2024-02-03 09:40:13,404 [Epoch: 1442 Step: 00049000] Batch Recognition Loss:   0.000866 => Gls Tokens per Sec:     2573 || Batch Translation Loss:   0.032669 => Txt Tokens per Sec:     7245 || Lr: 0.000050
2024-02-03 09:40:17,619 Epoch 1442: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.54 
2024-02-03 09:40:17,620 EPOCH 1443
2024-02-03 09:40:22,028 Epoch 1443: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.43 
2024-02-03 09:40:22,028 EPOCH 1444
2024-02-03 09:40:26,947 Epoch 1444: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.41 
2024-02-03 09:40:26,948 EPOCH 1445
2024-02-03 09:40:27,378 [Epoch: 1445 Step: 00049100] Batch Recognition Loss:   0.000769 => Gls Tokens per Sec:     2393 || Batch Translation Loss:   0.052941 => Txt Tokens per Sec:     7025 || Lr: 0.000050
2024-02-03 09:40:31,447 Epoch 1445: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.00 
2024-02-03 09:40:31,447 EPOCH 1446
2024-02-03 09:40:36,127 Epoch 1446: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.76 
2024-02-03 09:40:36,127 EPOCH 1447
2024-02-03 09:40:40,811 Epoch 1447: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.70 
2024-02-03 09:40:40,811 EPOCH 1448
2024-02-03 09:40:41,054 [Epoch: 1448 Step: 00049200] Batch Recognition Loss:   0.001453 => Gls Tokens per Sec:     1618 || Batch Translation Loss:   0.006030 => Txt Tokens per Sec:     3884 || Lr: 0.000050
2024-02-03 09:40:45,312 Epoch 1448: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.62 
2024-02-03 09:40:45,313 EPOCH 1449
2024-02-03 09:40:50,204 Epoch 1449: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.60 
2024-02-03 09:40:50,205 EPOCH 1450
2024-02-03 09:40:54,633 [Epoch: 1450 Step: 00049300] Batch Recognition Loss:   0.000666 => Gls Tokens per Sec:     2401 || Batch Translation Loss:   0.013731 => Txt Tokens per Sec:     6677 || Lr: 0.000050
2024-02-03 09:40:54,633 Epoch 1450: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.63 
2024-02-03 09:40:54,633 EPOCH 1451
2024-02-03 09:40:59,485 Epoch 1451: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.59 
2024-02-03 09:40:59,486 EPOCH 1452
2024-02-03 09:41:04,026 Epoch 1452: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.59 
2024-02-03 09:41:04,027 EPOCH 1453
2024-02-03 09:41:08,374 [Epoch: 1453 Step: 00049400] Batch Recognition Loss:   0.000551 => Gls Tokens per Sec:     2299 || Batch Translation Loss:   0.019271 => Txt Tokens per Sec:     6380 || Lr: 0.000050
2024-02-03 09:41:08,665 Epoch 1453: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.60 
2024-02-03 09:41:08,665 EPOCH 1454
2024-02-03 09:41:13,395 Epoch 1454: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.83 
2024-02-03 09:41:13,395 EPOCH 1455
2024-02-03 09:41:17,803 Epoch 1455: Total Training Recognition Loss 0.14  Total Training Translation Loss 0.73 
2024-02-03 09:41:17,803 EPOCH 1456
2024-02-03 09:41:22,017 [Epoch: 1456 Step: 00049500] Batch Recognition Loss:   0.000429 => Gls Tokens per Sec:     2219 || Batch Translation Loss:   0.032600 => Txt Tokens per Sec:     6168 || Lr: 0.000050
2024-02-03 09:41:22,775 Epoch 1456: Total Training Recognition Loss 0.34  Total Training Translation Loss 0.83 
2024-02-03 09:41:22,775 EPOCH 1457
2024-02-03 09:41:27,275 Epoch 1457: Total Training Recognition Loss 0.29  Total Training Translation Loss 0.85 
2024-02-03 09:41:27,276 EPOCH 1458
2024-02-03 09:41:32,081 Epoch 1458: Total Training Recognition Loss 8.57  Total Training Translation Loss 1.07 
2024-02-03 09:41:32,081 EPOCH 1459
2024-02-03 09:41:35,933 [Epoch: 1459 Step: 00049600] Batch Recognition Loss:   0.008681 => Gls Tokens per Sec:     2262 || Batch Translation Loss:   0.041317 => Txt Tokens per Sec:     6406 || Lr: 0.000050
2024-02-03 09:41:36,658 Epoch 1459: Total Training Recognition Loss 7.48  Total Training Translation Loss 3.40 
2024-02-03 09:41:36,658 EPOCH 1460
2024-02-03 09:41:41,210 Epoch 1460: Total Training Recognition Loss 0.68  Total Training Translation Loss 1.71 
2024-02-03 09:41:41,210 EPOCH 1461
2024-02-03 09:41:46,044 Epoch 1461: Total Training Recognition Loss 0.16  Total Training Translation Loss 1.73 
2024-02-03 09:41:46,044 EPOCH 1462
2024-02-03 09:41:49,624 [Epoch: 1462 Step: 00049700] Batch Recognition Loss:   0.011071 => Gls Tokens per Sec:     2325 || Batch Translation Loss:   0.055050 => Txt Tokens per Sec:     6603 || Lr: 0.000050
2024-02-03 09:41:50,520 Epoch 1462: Total Training Recognition Loss 0.09  Total Training Translation Loss 1.31 
2024-02-03 09:41:50,520 EPOCH 1463
2024-02-03 09:41:55,481 Epoch 1463: Total Training Recognition Loss 0.09  Total Training Translation Loss 1.19 
2024-02-03 09:41:55,481 EPOCH 1464
2024-02-03 09:41:59,982 Epoch 1464: Total Training Recognition Loss 0.12  Total Training Translation Loss 0.90 
2024-02-03 09:41:59,983 EPOCH 1465
2024-02-03 09:42:03,116 [Epoch: 1465 Step: 00049800] Batch Recognition Loss:   0.008629 => Gls Tokens per Sec:     2452 || Batch Translation Loss:   0.029455 => Txt Tokens per Sec:     6744 || Lr: 0.000050
2024-02-03 09:42:04,696 Epoch 1465: Total Training Recognition Loss 0.07  Total Training Translation Loss 0.77 
2024-02-03 09:42:04,696 EPOCH 1466
2024-02-03 09:42:09,389 Epoch 1466: Total Training Recognition Loss 0.08  Total Training Translation Loss 0.66 
2024-02-03 09:42:09,390 EPOCH 1467
2024-02-03 09:42:13,899 Epoch 1467: Total Training Recognition Loss 0.09  Total Training Translation Loss 0.61 
2024-02-03 09:42:13,899 EPOCH 1468
2024-02-03 09:42:16,698 [Epoch: 1468 Step: 00049900] Batch Recognition Loss:   0.001521 => Gls Tokens per Sec:     2427 || Batch Translation Loss:   0.017268 => Txt Tokens per Sec:     6229 || Lr: 0.000050
2024-02-03 09:42:18,769 Epoch 1468: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.70 
2024-02-03 09:42:18,770 EPOCH 1469
2024-02-03 09:42:23,278 Epoch 1469: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.07 
2024-02-03 09:42:23,278 EPOCH 1470
2024-02-03 09:42:28,144 Epoch 1470: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.87 
2024-02-03 09:42:28,145 EPOCH 1471
2024-02-03 09:42:30,283 [Epoch: 1471 Step: 00050000] Batch Recognition Loss:   0.002053 => Gls Tokens per Sec:     2994 || Batch Translation Loss:   0.012294 => Txt Tokens per Sec:     8145 || Lr: 0.000050
2024-02-03 09:42:39,091 Validation result at epoch 1471, step    50000: duration: 8.8065s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.83129	Translation Loss: 90394.75781	PPL: 11952.99023
	Eval Metric: BLEU
	WER 3.39	(DEL: 0.07,	INS: 0.00,	SUB: 3.32)
	BLEU-4 0.54	(BLEU-1: 9.78,	BLEU-2: 2.87,	BLEU-3: 1.13,	BLEU-4: 0.54)
	CHRF 16.94	ROUGE 8.20
2024-02-03 09:42:39,092 Logging Recognition and Translation Outputs
2024-02-03 09:42:39,093 ========================================================================================================================
2024-02-03 09:42:39,093 Logging Sequence: 154_33.00
2024-02-03 09:42:39,093 	Gloss Reference :	A B+C+D+E
2024-02-03 09:42:39,093 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 09:42:39,093 	Gloss Alignment :	         
2024-02-03 09:42:39,093 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 09:42:39,094 	Text Reference  :	the t20 world cup   will now be    held      in     uae oman   
2024-02-03 09:42:39,094 	Text Hypothesis :	*** but was   wrong and  kl  rahul continued during the players
2024-02-03 09:42:39,094 	Text Alignment  :	D   S   S     S     S    S   S     S         S      S   S      
2024-02-03 09:42:39,095 ========================================================================================================================
2024-02-03 09:42:39,095 Logging Sequence: 155_56.00
2024-02-03 09:42:39,095 	Gloss Reference :	A B+C+D+E
2024-02-03 09:42:39,095 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 09:42:39,095 	Gloss Alignment :	         
2024-02-03 09:42:39,095 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 09:42:39,096 	Text Reference  :	********* ** the ******* ****** *** team would have      been disqualified
2024-02-03 09:42:39,096 	Text Hypothesis :	meanwhile in the taliban wanted t20 if   the   over-rate is   slow        
2024-02-03 09:42:39,096 	Text Alignment  :	I         I      I       I      I   S    S     S         S    S           
2024-02-03 09:42:39,096 ========================================================================================================================
2024-02-03 09:42:39,096 Logging Sequence: 125_119.00
2024-02-03 09:42:39,097 	Gloss Reference :	A B+C+D+E
2024-02-03 09:42:39,097 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 09:42:39,097 	Gloss Alignment :	         
2024-02-03 09:42:39,097 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 09:42:39,098 	Text Reference  :	people must not     post such baseless comments on social media 
2024-02-03 09:42:39,098 	Text Hypothesis :	i      have retired from all  must     go       on 7th    august
2024-02-03 09:42:39,098 	Text Alignment  :	S      S    S       S    S    S        S           S      S     
2024-02-03 09:42:39,098 ========================================================================================================================
2024-02-03 09:42:39,099 Logging Sequence: 59_152.00
2024-02-03 09:42:39,099 	Gloss Reference :	A B+C+D+E
2024-02-03 09:42:39,099 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 09:42:39,099 	Gloss Alignment :	         
2024-02-03 09:42:39,099 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 09:42:39,101 	Text Reference  :	***** *** **** the organisers encouraged athletes    to  use the  condoms in    their home countries
2024-02-03 09:42:39,101 	Text Hypothesis :	after his fans did not        been       quarantined for his time and     ended up    a    condom   
2024-02-03 09:42:39,101 	Text Alignment  :	I     I   I    S   S          S          S           S   S   S    S       S     S     S    S        
2024-02-03 09:42:39,101 ========================================================================================================================
2024-02-03 09:42:39,101 Logging Sequence: 151_94.00
2024-02-03 09:42:39,101 	Gloss Reference :	A B+C+D+E
2024-02-03 09:42:39,101 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 09:42:39,102 	Gloss Alignment :	         
2024-02-03 09:42:39,102 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 09:42:39,103 	Text Reference  :	suresh raina was also known as  mr ipl   for   his            amazing  performance in  the tournament    
2024-02-03 09:42:39,103 	Text Hypothesis :	****** ***** *** **** ipl   has 10 media while praggnanandhaa defeated carlsen     and t20 internationals
2024-02-03 09:42:39,103 	Text Alignment  :	D      D     D   D    S     S   S  S     S     S              S        S           S   S   S             
2024-02-03 09:42:39,103 ========================================================================================================================
2024-02-03 09:42:41,500 Epoch 1471: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.21 
2024-02-03 09:42:41,500 EPOCH 1472
2024-02-03 09:42:46,396 Epoch 1472: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.94 
2024-02-03 09:42:46,396 EPOCH 1473
2024-02-03 09:42:51,254 Epoch 1473: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.68 
2024-02-03 09:42:51,255 EPOCH 1474
2024-02-03 09:42:53,584 [Epoch: 1474 Step: 00050100] Batch Recognition Loss:   0.000202 => Gls Tokens per Sec:     2474 || Batch Translation Loss:   0.011706 => Txt Tokens per Sec:     6690 || Lr: 0.000050
2024-02-03 09:42:55,994 Epoch 1474: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.72 
2024-02-03 09:42:55,994 EPOCH 1475
2024-02-03 09:43:00,736 Epoch 1475: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.70 
2024-02-03 09:43:00,736 EPOCH 1476
2024-02-03 09:43:05,199 Epoch 1476: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.64 
2024-02-03 09:43:05,199 EPOCH 1477
2024-02-03 09:43:07,927 [Epoch: 1477 Step: 00050200] Batch Recognition Loss:   0.001127 => Gls Tokens per Sec:     1786 || Batch Translation Loss:   0.009564 => Txt Tokens per Sec:     5337 || Lr: 0.000050
2024-02-03 09:43:10,160 Epoch 1477: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.57 
2024-02-03 09:43:10,160 EPOCH 1478
2024-02-03 09:43:14,658 Epoch 1478: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.63 
2024-02-03 09:43:14,659 EPOCH 1479
2024-02-03 09:43:19,481 Epoch 1479: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.71 
2024-02-03 09:43:19,481 EPOCH 1480
2024-02-03 09:43:21,329 [Epoch: 1480 Step: 00050300] Batch Recognition Loss:   0.000127 => Gls Tokens per Sec:     2291 || Batch Translation Loss:   0.021316 => Txt Tokens per Sec:     7021 || Lr: 0.000050
2024-02-03 09:43:24,046 Epoch 1480: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.61 
2024-02-03 09:43:24,047 EPOCH 1481
2024-02-03 09:43:28,636 Epoch 1481: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.96 
2024-02-03 09:43:28,637 EPOCH 1482
2024-02-03 09:43:33,457 Epoch 1482: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.12 
2024-02-03 09:43:33,458 EPOCH 1483
2024-02-03 09:43:34,808 [Epoch: 1483 Step: 00050400] Batch Recognition Loss:   0.000453 => Gls Tokens per Sec:     2845 || Batch Translation Loss:   0.018061 => Txt Tokens per Sec:     7548 || Lr: 0.000050
2024-02-03 09:43:37,898 Epoch 1483: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.39 
2024-02-03 09:43:37,899 EPOCH 1484
2024-02-03 09:43:42,859 Epoch 1484: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.78 
2024-02-03 09:43:42,860 EPOCH 1485
2024-02-03 09:43:47,408 Epoch 1485: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.18 
2024-02-03 09:43:47,409 EPOCH 1486
2024-02-03 09:43:48,810 [Epoch: 1486 Step: 00050500] Batch Recognition Loss:   0.000318 => Gls Tokens per Sec:     2287 || Batch Translation Loss:   0.019776 => Txt Tokens per Sec:     6446 || Lr: 0.000050
2024-02-03 09:43:52,160 Epoch 1486: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.07 
2024-02-03 09:43:52,160 EPOCH 1487
2024-02-03 09:43:56,864 Epoch 1487: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.15 
2024-02-03 09:43:56,865 EPOCH 1488
2024-02-03 09:44:01,380 Epoch 1488: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.70 
2024-02-03 09:44:01,380 EPOCH 1489
2024-02-03 09:44:02,446 [Epoch: 1489 Step: 00050600] Batch Recognition Loss:   0.000254 => Gls Tokens per Sec:     2404 || Batch Translation Loss:   0.029231 => Txt Tokens per Sec:     7077 || Lr: 0.000050
2024-02-03 09:44:06,220 Epoch 1489: Total Training Recognition Loss 0.04  Total Training Translation Loss 4.12 
2024-02-03 09:44:06,221 EPOCH 1490
2024-02-03 09:44:10,616 Epoch 1490: Total Training Recognition Loss 0.17  Total Training Translation Loss 4.11 
2024-02-03 09:44:10,617 EPOCH 1491
2024-02-03 09:44:15,497 Epoch 1491: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.14 
2024-02-03 09:44:15,498 EPOCH 1492
2024-02-03 09:44:16,405 [Epoch: 1492 Step: 00050700] Batch Recognition Loss:   0.001447 => Gls Tokens per Sec:     2116 || Batch Translation Loss:   0.035133 => Txt Tokens per Sec:     6439 || Lr: 0.000050
2024-02-03 09:44:20,074 Epoch 1492: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.91 
2024-02-03 09:44:20,075 EPOCH 1493
2024-02-03 09:44:24,783 Epoch 1493: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.85 
2024-02-03 09:44:24,783 EPOCH 1494
2024-02-03 09:44:29,496 Epoch 1494: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.68 
2024-02-03 09:44:29,497 EPOCH 1495
2024-02-03 09:44:30,119 [Epoch: 1495 Step: 00050800] Batch Recognition Loss:   0.000743 => Gls Tokens per Sec:     2059 || Batch Translation Loss:   0.020908 => Txt Tokens per Sec:     6564 || Lr: 0.000050
2024-02-03 09:44:33,920 Epoch 1495: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.62 
2024-02-03 09:44:33,921 EPOCH 1496
2024-02-03 09:44:38,885 Epoch 1496: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.61 
2024-02-03 09:44:38,885 EPOCH 1497
2024-02-03 09:44:43,325 Epoch 1497: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.67 
2024-02-03 09:44:43,326 EPOCH 1498
2024-02-03 09:44:43,511 [Epoch: 1498 Step: 00050900] Batch Recognition Loss:   0.000809 => Gls Tokens per Sec:     3491 || Batch Translation Loss:   0.011317 => Txt Tokens per Sec:     6485 || Lr: 0.000050
2024-02-03 09:44:48,193 Epoch 1498: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.64 
2024-02-03 09:44:48,194 EPOCH 1499
2024-02-03 09:44:52,738 Epoch 1499: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.68 
2024-02-03 09:44:52,738 EPOCH 1500
2024-02-03 09:44:57,391 [Epoch: 1500 Step: 00051000] Batch Recognition Loss:   0.000296 => Gls Tokens per Sec:     2285 || Batch Translation Loss:   0.018169 => Txt Tokens per Sec:     6356 || Lr: 0.000050
2024-02-03 09:44:57,391 Epoch 1500: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.81 
2024-02-03 09:44:57,391 EPOCH 1501
2024-02-03 09:45:02,223 Epoch 1501: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.67 
2024-02-03 09:45:02,224 EPOCH 1502
2024-02-03 09:45:06,701 Epoch 1502: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.65 
2024-02-03 09:45:06,702 EPOCH 1503
2024-02-03 09:45:11,514 [Epoch: 1503 Step: 00051100] Batch Recognition Loss:   0.000426 => Gls Tokens per Sec:     2077 || Batch Translation Loss:   0.016349 => Txt Tokens per Sec:     5835 || Lr: 0.000050
2024-02-03 09:45:11,677 Epoch 1503: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.60 
2024-02-03 09:45:11,677 EPOCH 1504
2024-02-03 09:45:16,219 Epoch 1504: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.66 
2024-02-03 09:45:16,219 EPOCH 1505
2024-02-03 09:45:20,948 Epoch 1505: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.55 
2024-02-03 09:45:20,948 EPOCH 1506
2024-02-03 09:45:25,233 [Epoch: 1506 Step: 00051200] Batch Recognition Loss:   0.000225 => Gls Tokens per Sec:     2183 || Batch Translation Loss:   0.013462 => Txt Tokens per Sec:     6168 || Lr: 0.000050
2024-02-03 09:45:25,649 Epoch 1506: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.54 
2024-02-03 09:45:25,650 EPOCH 1507
2024-02-03 09:45:30,220 Epoch 1507: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.54 
2024-02-03 09:45:30,220 EPOCH 1508
2024-02-03 09:45:35,136 Epoch 1508: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.54 
2024-02-03 09:45:35,137 EPOCH 1509
2024-02-03 09:45:38,759 [Epoch: 1509 Step: 00051300] Batch Recognition Loss:   0.000808 => Gls Tokens per Sec:     2405 || Batch Translation Loss:   0.020673 => Txt Tokens per Sec:     6704 || Lr: 0.000050
2024-02-03 09:45:39,612 Epoch 1509: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.64 
2024-02-03 09:45:39,612 EPOCH 1510
2024-02-03 09:45:44,529 Epoch 1510: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.88 
2024-02-03 09:45:44,530 EPOCH 1511
2024-02-03 09:45:49,087 Epoch 1511: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.95 
2024-02-03 09:45:49,088 EPOCH 1512
2024-02-03 09:45:52,671 [Epoch: 1512 Step: 00051400] Batch Recognition Loss:   0.000188 => Gls Tokens per Sec:     2253 || Batch Translation Loss:   0.007431 => Txt Tokens per Sec:     6171 || Lr: 0.000050
2024-02-03 09:45:53,721 Epoch 1512: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.15 
2024-02-03 09:45:53,722 EPOCH 1513
2024-02-03 09:45:57,765 Epoch 1513: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.27 
2024-02-03 09:45:57,765 EPOCH 1514
2024-02-03 09:46:01,777 Epoch 1514: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.17 
2024-02-03 09:46:01,777 EPOCH 1515
2024-02-03 09:46:04,752 [Epoch: 1515 Step: 00051500] Batch Recognition Loss:   0.000757 => Gls Tokens per Sec:     2498 || Batch Translation Loss:   0.035372 => Txt Tokens per Sec:     6953 || Lr: 0.000050
2024-02-03 09:46:05,826 Epoch 1515: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.16 
2024-02-03 09:46:05,826 EPOCH 1516
2024-02-03 09:46:09,900 Epoch 1516: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.64 
2024-02-03 09:46:09,900 EPOCH 1517
2024-02-03 09:46:13,952 Epoch 1517: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.17 
2024-02-03 09:46:13,953 EPOCH 1518
2024-02-03 09:46:16,434 [Epoch: 1518 Step: 00051600] Batch Recognition Loss:   0.000637 => Gls Tokens per Sec:     2738 || Batch Translation Loss:   0.026739 => Txt Tokens per Sec:     7489 || Lr: 0.000050
2024-02-03 09:46:18,003 Epoch 1518: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.87 
2024-02-03 09:46:18,004 EPOCH 1519
2024-02-03 09:46:22,999 Epoch 1519: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.04 
2024-02-03 09:46:23,000 EPOCH 1520
2024-02-03 09:46:27,161 Epoch 1520: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.73 
2024-02-03 09:46:27,161 EPOCH 1521
2024-02-03 09:46:29,256 [Epoch: 1521 Step: 00051700] Batch Recognition Loss:   0.000598 => Gls Tokens per Sec:     3056 || Batch Translation Loss:   0.022582 => Txt Tokens per Sec:     8182 || Lr: 0.000050
2024-02-03 09:46:31,198 Epoch 1521: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.70 
2024-02-03 09:46:31,198 EPOCH 1522
2024-02-03 09:46:35,276 Epoch 1522: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.63 
2024-02-03 09:46:35,276 EPOCH 1523
2024-02-03 09:46:39,363 Epoch 1523: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.56 
2024-02-03 09:46:39,363 EPOCH 1524
2024-02-03 09:46:41,349 [Epoch: 1524 Step: 00051800] Batch Recognition Loss:   0.000648 => Gls Tokens per Sec:     2901 || Batch Translation Loss:   0.022358 => Txt Tokens per Sec:     8107 || Lr: 0.000050
2024-02-03 09:46:43,733 Epoch 1524: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.60 
2024-02-03 09:46:43,734 EPOCH 1525
2024-02-03 09:46:48,528 Epoch 1525: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.64 
2024-02-03 09:46:48,528 EPOCH 1526
2024-02-03 09:46:53,353 Epoch 1526: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.59 
2024-02-03 09:46:53,354 EPOCH 1527
2024-02-03 09:46:55,279 [Epoch: 1527 Step: 00051900] Batch Recognition Loss:   0.000348 => Gls Tokens per Sec:     2661 || Batch Translation Loss:   0.012993 => Txt Tokens per Sec:     7359 || Lr: 0.000050
2024-02-03 09:46:57,658 Epoch 1527: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.76 
2024-02-03 09:46:57,658 EPOCH 1528
2024-02-03 09:47:01,757 Epoch 1528: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.73 
2024-02-03 09:47:01,757 EPOCH 1529
2024-02-03 09:47:06,242 Epoch 1529: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.68 
2024-02-03 09:47:06,242 EPOCH 1530
2024-02-03 09:47:08,243 [Epoch: 1530 Step: 00052000] Batch Recognition Loss:   0.003300 => Gls Tokens per Sec:     2116 || Batch Translation Loss:   0.007391 => Txt Tokens per Sec:     6218 || Lr: 0.000050
2024-02-03 09:47:17,050 Validation result at epoch 1530, step    52000: duration: 8.8059s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.49087	Translation Loss: 89473.03906	PPL: 10861.75391
	Eval Metric: BLEU
	WER 3.11	(DEL: 0.07,	INS: 0.00,	SUB: 3.04)
	BLEU-4 0.42	(BLEU-1: 9.65,	BLEU-2: 2.54,	BLEU-3: 0.89,	BLEU-4: 0.42)
	CHRF 16.72	ROUGE 8.02
2024-02-03 09:47:17,051 Logging Recognition and Translation Outputs
2024-02-03 09:47:17,051 ========================================================================================================================
2024-02-03 09:47:17,051 Logging Sequence: 84_206.00
2024-02-03 09:47:17,051 	Gloss Reference :	A B+C+D+E
2024-02-03 09:47:17,052 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 09:47:17,052 	Gloss Alignment :	         
2024-02-03 09:47:17,052 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 09:47:17,053 	Text Reference  :	football fans in t he stadium too agreed with the players and  wished  to   support the  lgbtqia community
2024-02-03 09:47:17,053 	Text Hypothesis :	******** **** ** * ** ******* *** ****** **** *** people  were shocked when this    news became  viral    
2024-02-03 09:47:17,053 	Text Alignment  :	D        D    D  D D  D       D   D      D    D   S       S    S       S    S       S    S       S        
2024-02-03 09:47:17,053 ========================================================================================================================
2024-02-03 09:47:17,054 Logging Sequence: 126_263.00
2024-02-03 09:47:17,054 	Gloss Reference :	A B+C+D+E
2024-02-03 09:47:17,054 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 09:47:17,054 	Gloss Alignment :	         
2024-02-03 09:47:17,054 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 09:47:17,055 	Text Reference  :	indian airline indigo has       announced unlimited free     year travel for the  gold   medalist
2024-02-03 09:47:17,055 	Text Hypothesis :	he     had     not    practised since     he        returned and  he     had also fallen sick    
2024-02-03 09:47:17,056 	Text Alignment  :	S      S       S      S         S         S         S        S    S      S   S    S      S       
2024-02-03 09:47:17,056 ========================================================================================================================
2024-02-03 09:47:17,056 Logging Sequence: 93_11.00
2024-02-03 09:47:17,056 	Gloss Reference :	A B+C+D+E
2024-02-03 09:47:17,056 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 09:47:17,056 	Gloss Alignment :	         
2024-02-03 09:47:17,056 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 09:47:17,057 	Text Reference  :	he also played for the ********** ******* ***** ***** **** ** manchester   united football team    
2024-02-03 09:47:17,057 	Text Hypothesis :	** **** ****** for the post-match meeting virat kohli made an announcement that   shocked  everyone
2024-02-03 09:47:17,058 	Text Alignment  :	D  D    D              I          I       I     I     I    I  S            S      S        S       
2024-02-03 09:47:17,058 ========================================================================================================================
2024-02-03 09:47:17,058 Logging Sequence: 155_62.00
2024-02-03 09:47:17,058 	Gloss Reference :	A B+C+D+E
2024-02-03 09:47:17,058 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 09:47:17,058 	Gloss Alignment :	         
2024-02-03 09:47:17,058 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 09:47:17,060 	Text Reference  :	however they were fortunate that the icc had drafted the match schedule  well  in       advance with the afghan team in the tournament
2024-02-03 09:47:17,060 	Text Hypothesis :	however **** **** ********* **** *** *** *** ******* *** ***** selectors never selected me      for  the ****** team ** *** **********
2024-02-03 09:47:17,060 	Text Alignment  :	        D    D    D         D    D   D   D   D       D   D     S         S     S        S       S        D           D  D   D         
2024-02-03 09:47:17,060 ========================================================================================================================
2024-02-03 09:47:17,060 Logging Sequence: 93_267.00
2024-02-03 09:47:17,060 	Gloss Reference :	A B+C+D+E
2024-02-03 09:47:17,061 	Gloss Hypothesis:	A B+E+B  
2024-02-03 09:47:17,061 	Gloss Alignment :	  S      
2024-02-03 09:47:17,061 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 09:47:17,063 	Text Reference  :	one girl who was wearing a      thong flashed her        bottom next   to rooney the image went   viral on    snapchat
2024-02-03 09:47:17,063 	Text Hypothesis :	*** this is  why he      walked into  the     tournament and    wanted to ****** *** ***** secure 11    covid crore   
2024-02-03 09:47:17,063 	Text Alignment  :	D   S    S   S   S       S      S     S       S          S      S         D      D   D     S      S     S     S       
2024-02-03 09:47:17,063 ========================================================================================================================
2024-02-03 09:47:19,865 Epoch 1530: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.58 
2024-02-03 09:47:19,865 EPOCH 1531
2024-02-03 09:47:24,894 Epoch 1531: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.57 
2024-02-03 09:47:24,895 EPOCH 1532
2024-02-03 09:47:29,726 Epoch 1532: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.51 
2024-02-03 09:47:29,726 EPOCH 1533
2024-02-03 09:47:30,992 [Epoch: 1533 Step: 00052100] Batch Recognition Loss:   0.000661 => Gls Tokens per Sec:     3039 || Batch Translation Loss:   0.064200 => Txt Tokens per Sec:     7722 || Lr: 0.000050
2024-02-03 09:47:34,638 Epoch 1533: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.61 
2024-02-03 09:47:34,639 EPOCH 1534
2024-02-03 09:47:39,373 Epoch 1534: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-03 09:47:39,374 EPOCH 1535
2024-02-03 09:47:44,264 Epoch 1535: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.92 
2024-02-03 09:47:44,265 EPOCH 1536
2024-02-03 09:47:45,450 [Epoch: 1536 Step: 00052200] Batch Recognition Loss:   0.001064 => Gls Tokens per Sec:     2490 || Batch Translation Loss:   0.006412 => Txt Tokens per Sec:     6611 || Lr: 0.000050
2024-02-03 09:47:48,529 Epoch 1536: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.62 
2024-02-03 09:47:48,530 EPOCH 1537
2024-02-03 09:47:53,431 Epoch 1537: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.05 
2024-02-03 09:47:53,431 EPOCH 1538
2024-02-03 09:47:57,867 Epoch 1538: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.68 
2024-02-03 09:47:57,868 EPOCH 1539
2024-02-03 09:47:59,079 [Epoch: 1539 Step: 00052300] Batch Recognition Loss:   0.000153 => Gls Tokens per Sec:     1908 || Batch Translation Loss:   0.053969 => Txt Tokens per Sec:     5506 || Lr: 0.000050
2024-02-03 09:48:02,563 Epoch 1539: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.19 
2024-02-03 09:48:02,563 EPOCH 1540
2024-02-03 09:48:07,270 Epoch 1540: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.46 
2024-02-03 09:48:07,271 EPOCH 1541
2024-02-03 09:48:11,765 Epoch 1541: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.05 
2024-02-03 09:48:11,765 EPOCH 1542
2024-02-03 09:48:12,625 [Epoch: 1542 Step: 00052400] Batch Recognition Loss:   0.000800 => Gls Tokens per Sec:     2235 || Batch Translation Loss:   0.050126 => Txt Tokens per Sec:     6973 || Lr: 0.000050
2024-02-03 09:48:16,696 Epoch 1542: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.21 
2024-02-03 09:48:16,696 EPOCH 1543
2024-02-03 09:48:21,692 Epoch 1543: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.68 
2024-02-03 09:48:21,692 EPOCH 1544
2024-02-03 09:48:26,647 Epoch 1544: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.30 
2024-02-03 09:48:26,647 EPOCH 1545
2024-02-03 09:48:27,154 [Epoch: 1545 Step: 00052500] Batch Recognition Loss:   0.001043 => Gls Tokens per Sec:     2527 || Batch Translation Loss:   0.030899 => Txt Tokens per Sec:     6962 || Lr: 0.000050
2024-02-03 09:48:30,926 Epoch 1545: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.02 
2024-02-03 09:48:30,926 EPOCH 1546
2024-02-03 09:48:35,818 Epoch 1546: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.77 
2024-02-03 09:48:35,819 EPOCH 1547
2024-02-03 09:48:40,360 Epoch 1547: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.71 
2024-02-03 09:48:40,360 EPOCH 1548
2024-02-03 09:48:40,657 [Epoch: 1548 Step: 00052600] Batch Recognition Loss:   0.000378 => Gls Tokens per Sec:     2169 || Batch Translation Loss:   0.017341 => Txt Tokens per Sec:     6580 || Lr: 0.000050
2024-02-03 09:48:45,014 Epoch 1548: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.63 
2024-02-03 09:48:45,014 EPOCH 1549
2024-02-03 09:48:49,471 Epoch 1549: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.69 
2024-02-03 09:48:49,471 EPOCH 1550
2024-02-03 09:48:54,211 [Epoch: 1550 Step: 00052700] Batch Recognition Loss:   0.001129 => Gls Tokens per Sec:     2243 || Batch Translation Loss:   0.042830 => Txt Tokens per Sec:     6238 || Lr: 0.000050
2024-02-03 09:48:54,212 Epoch 1550: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.77 
2024-02-03 09:48:54,212 EPOCH 1551
2024-02-03 09:48:58,727 Epoch 1551: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.94 
2024-02-03 09:48:58,727 EPOCH 1552
2024-02-03 09:49:03,413 Epoch 1552: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.71 
2024-02-03 09:49:03,413 EPOCH 1553
2024-02-03 09:49:07,891 [Epoch: 1553 Step: 00052800] Batch Recognition Loss:   0.000415 => Gls Tokens per Sec:     2231 || Batch Translation Loss:   0.014758 => Txt Tokens per Sec:     6167 || Lr: 0.000050
2024-02-03 09:49:08,192 Epoch 1553: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.69 
2024-02-03 09:49:08,193 EPOCH 1554
2024-02-03 09:49:12,668 Epoch 1554: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.91 
2024-02-03 09:49:12,669 EPOCH 1555
2024-02-03 09:49:17,640 Epoch 1555: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.90 
2024-02-03 09:49:17,641 EPOCH 1556
2024-02-03 09:49:21,571 [Epoch: 1556 Step: 00052900] Batch Recognition Loss:   0.001478 => Gls Tokens per Sec:     2380 || Batch Translation Loss:   0.007924 => Txt Tokens per Sec:     6617 || Lr: 0.000050
2024-02-03 09:49:22,109 Epoch 1556: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.68 
2024-02-03 09:49:22,109 EPOCH 1557
2024-02-03 09:49:26,919 Epoch 1557: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.62 
2024-02-03 09:49:26,919 EPOCH 1558
2024-02-03 09:49:31,532 Epoch 1558: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.70 
2024-02-03 09:49:31,532 EPOCH 1559
2024-02-03 09:49:35,285 [Epoch: 1559 Step: 00053000] Batch Recognition Loss:   0.000189 => Gls Tokens per Sec:     2321 || Batch Translation Loss:   0.016165 => Txt Tokens per Sec:     6356 || Lr: 0.000050
2024-02-03 09:49:36,093 Epoch 1559: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.87 
2024-02-03 09:49:36,093 EPOCH 1560
2024-02-03 09:49:40,662 Epoch 1560: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.26 
2024-02-03 09:49:40,663 EPOCH 1561
2024-02-03 09:49:45,280 Epoch 1561: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.91 
2024-02-03 09:49:45,281 EPOCH 1562
2024-02-03 09:49:48,498 [Epoch: 1562 Step: 00053100] Batch Recognition Loss:   0.000478 => Gls Tokens per Sec:     2509 || Batch Translation Loss:   0.022713 => Txt Tokens per Sec:     7050 || Lr: 0.000050
2024-02-03 09:49:49,293 Epoch 1562: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.07 
2024-02-03 09:49:49,293 EPOCH 1563
2024-02-03 09:49:54,129 Epoch 1563: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.64 
2024-02-03 09:49:54,129 EPOCH 1564
2024-02-03 09:49:58,852 Epoch 1564: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.42 
2024-02-03 09:49:58,853 EPOCH 1565
2024-02-03 09:50:01,932 [Epoch: 1565 Step: 00053200] Batch Recognition Loss:   0.000195 => Gls Tokens per Sec:     2414 || Batch Translation Loss:   0.024627 => Txt Tokens per Sec:     6609 || Lr: 0.000050
2024-02-03 09:50:03,561 Epoch 1565: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.97 
2024-02-03 09:50:03,562 EPOCH 1566
2024-02-03 09:50:08,117 Epoch 1566: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.88 
2024-02-03 09:50:08,117 EPOCH 1567
2024-02-03 09:50:12,714 Epoch 1567: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.71 
2024-02-03 09:50:12,715 EPOCH 1568
2024-02-03 09:50:15,573 [Epoch: 1568 Step: 00053300] Batch Recognition Loss:   0.000458 => Gls Tokens per Sec:     2377 || Batch Translation Loss:   0.017476 => Txt Tokens per Sec:     6582 || Lr: 0.000050
2024-02-03 09:50:17,666 Epoch 1568: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.82 
2024-02-03 09:50:17,666 EPOCH 1569
2024-02-03 09:50:22,294 Epoch 1569: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.68 
2024-02-03 09:50:22,294 EPOCH 1570
2024-02-03 09:50:27,062 Epoch 1570: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.67 
2024-02-03 09:50:27,062 EPOCH 1571
2024-02-03 09:50:29,614 [Epoch: 1571 Step: 00053400] Batch Recognition Loss:   0.001084 => Gls Tokens per Sec:     2412 || Batch Translation Loss:   0.008727 => Txt Tokens per Sec:     6778 || Lr: 0.000050
2024-02-03 09:50:31,472 Epoch 1571: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.80 
2024-02-03 09:50:31,473 EPOCH 1572
2024-02-03 09:50:36,443 Epoch 1572: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.83 
2024-02-03 09:50:36,444 EPOCH 1573
2024-02-03 09:50:40,956 Epoch 1573: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.02 
2024-02-03 09:50:40,956 EPOCH 1574
2024-02-03 09:50:43,213 [Epoch: 1574 Step: 00053500] Batch Recognition Loss:   0.000473 => Gls Tokens per Sec:     2553 || Batch Translation Loss:   0.083759 => Txt Tokens per Sec:     6955 || Lr: 0.000050
2024-02-03 09:50:45,798 Epoch 1574: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.47 
2024-02-03 09:50:45,798 EPOCH 1575
2024-02-03 09:50:50,332 Epoch 1575: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.23 
2024-02-03 09:50:50,333 EPOCH 1576
2024-02-03 09:50:54,897 Epoch 1576: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.29 
2024-02-03 09:50:54,897 EPOCH 1577
2024-02-03 09:50:57,397 [Epoch: 1577 Step: 00053600] Batch Recognition Loss:   0.000385 => Gls Tokens per Sec:     2049 || Batch Translation Loss:   0.023916 => Txt Tokens per Sec:     5896 || Lr: 0.000050
2024-02-03 09:50:59,698 Epoch 1577: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.27 
2024-02-03 09:50:59,698 EPOCH 1578
2024-02-03 09:51:04,034 Epoch 1578: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.46 
2024-02-03 09:51:04,034 EPOCH 1579
2024-02-03 09:51:08,072 Epoch 1579: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.46 
2024-02-03 09:51:08,072 EPOCH 1580
2024-02-03 09:51:10,437 [Epoch: 1580 Step: 00053700] Batch Recognition Loss:   0.000202 => Gls Tokens per Sec:     1791 || Batch Translation Loss:   0.025633 => Txt Tokens per Sec:     5393 || Lr: 0.000050
2024-02-03 09:51:13,201 Epoch 1580: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.22 
2024-02-03 09:51:13,201 EPOCH 1581
2024-02-03 09:51:18,118 Epoch 1581: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.40 
2024-02-03 09:51:18,119 EPOCH 1582
2024-02-03 09:51:22,162 Epoch 1582: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.48 
2024-02-03 09:51:22,163 EPOCH 1583
2024-02-03 09:51:23,806 [Epoch: 1583 Step: 00053800] Batch Recognition Loss:   0.005118 => Gls Tokens per Sec:     2186 || Batch Translation Loss:   0.108334 => Txt Tokens per Sec:     5734 || Lr: 0.000050
2024-02-03 09:51:27,181 Epoch 1583: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.23 
2024-02-03 09:51:27,181 EPOCH 1584
2024-02-03 09:51:31,991 Epoch 1584: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.96 
2024-02-03 09:51:31,992 EPOCH 1585
2024-02-03 09:51:36,294 Epoch 1585: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.94 
2024-02-03 09:51:36,295 EPOCH 1586
2024-02-03 09:51:37,484 [Epoch: 1586 Step: 00053900] Batch Recognition Loss:   0.000823 => Gls Tokens per Sec:     2692 || Batch Translation Loss:   0.032681 => Txt Tokens per Sec:     7712 || Lr: 0.000050
2024-02-03 09:51:40,972 Epoch 1586: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.94 
2024-02-03 09:51:40,972 EPOCH 1587
2024-02-03 09:51:45,404 Epoch 1587: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.81 
2024-02-03 09:51:45,404 EPOCH 1588
2024-02-03 09:51:50,278 Epoch 1588: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.79 
2024-02-03 09:51:50,279 EPOCH 1589
2024-02-03 09:51:51,283 [Epoch: 1589 Step: 00054000] Batch Recognition Loss:   0.000792 => Gls Tokens per Sec:     2552 || Batch Translation Loss:   0.010239 => Txt Tokens per Sec:     7440 || Lr: 0.000050
2024-02-03 09:51:59,973 Validation result at epoch 1589, step    54000: duration: 8.6904s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.45756	Translation Loss: 90571.07031	PPL: 12173.89551
	Eval Metric: BLEU
	WER 3.47	(DEL: 0.14,	INS: 0.00,	SUB: 3.32)
	BLEU-4 0.54	(BLEU-1: 9.68,	BLEU-2: 2.90,	BLEU-3: 1.13,	BLEU-4: 0.54)
	CHRF 16.56	ROUGE 8.16
2024-02-03 09:51:59,974 Logging Recognition and Translation Outputs
2024-02-03 09:51:59,975 ========================================================================================================================
2024-02-03 09:51:59,975 Logging Sequence: 77_140.00
2024-02-03 09:51:59,975 	Gloss Reference :	A B+C+D+E
2024-02-03 09:51:59,975 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 09:51:59,975 	Gloss Alignment :	         
2024-02-03 09:51:59,975 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 09:51:59,976 	Text Reference  :	he was up against chennai's ravindra jadeja who hit  multiple sixes 
2024-02-03 09:51:59,976 	Text Hypothesis :	** *** ** ******* however   kohli    tried  to  bowl first    wicket
2024-02-03 09:51:59,976 	Text Alignment  :	D  D   D  D       S         S        S      S   S    S        S     
2024-02-03 09:51:59,976 ========================================================================================================================
2024-02-03 09:51:59,977 Logging Sequence: 100_78.00
2024-02-03 09:51:59,977 	Gloss Reference :	A B+C+D+E
2024-02-03 09:51:59,977 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 09:51:59,977 	Gloss Alignment :	         
2024-02-03 09:51:59,977 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 09:51:59,979 	Text Reference  :	***** bcci had   sent two      separate indian teams to     compete in ***** the cwg and    the sahara cup  
2024-02-03 09:51:59,979 	Text Hypothesis :	after 6    years an   american has      won    the   silver medal   in asked the *** arrest of  sushil kumar
2024-02-03 09:51:59,979 	Text Alignment  :	I     S    S     S    S        S        S      S     S      S          I         D   S      S   S      S    
2024-02-03 09:51:59,979 ========================================================================================================================
2024-02-03 09:51:59,979 Logging Sequence: 153_230.00
2024-02-03 09:51:59,979 	Gloss Reference :	A B+C+D+E
2024-02-03 09:51:59,980 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 09:51:59,980 	Gloss Alignment :	         
2024-02-03 09:51:59,980 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 09:51:59,981 	Text Reference  :	the ***** imran khan-led the      pakistan side in 1992 and went on to become the pm    
2024-02-03 09:51:59,981 	Text Hypothesis :	the total count of       ronaldo' babies   was  in **** *** **** ** ** ****** *** mumbai
2024-02-03 09:51:59,981 	Text Alignment  :	    I     S     S        S        S        S       D    D   D    D  D  D      D   S     
2024-02-03 09:51:59,981 ========================================================================================================================
2024-02-03 09:51:59,981 Logging Sequence: 154_25.00
2024-02-03 09:51:59,982 	Gloss Reference :	A B+C+D+E
2024-02-03 09:51:59,982 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 09:51:59,982 	Gloss Alignment :	         
2024-02-03 09:51:59,982 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 09:51:59,983 	Text Reference  :	bcci chief  sourav ganguly on   28th        june said that      the  
2024-02-03 09:51:59,983 	Text Hypothesis :	even though they   have    been quarantined at   the  ahmedabad hotel
2024-02-03 09:51:59,983 	Text Alignment  :	S    S      S      S       S    S           S    S    S         S    
2024-02-03 09:51:59,983 ========================================================================================================================
2024-02-03 09:51:59,983 Logging Sequence: 81_394.00
2024-02-03 09:51:59,983 	Gloss Reference :	A B+C+D+E
2024-02-03 09:51:59,984 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 09:51:59,984 	Gloss Alignment :	         
2024-02-03 09:51:59,984 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 09:51:59,985 	Text Reference  :	the apex court is           going to    launch a strict investigation
2024-02-03 09:51:59,985 	Text Hypothesis :	*** **** with  speculations of    dhoni being  a good   headspace    
2024-02-03 09:51:59,985 	Text Alignment  :	D   D    S     S            S     S     S        S      S            
2024-02-03 09:51:59,985 ========================================================================================================================
2024-02-03 09:52:03,864 Epoch 1589: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.74 
2024-02-03 09:52:03,864 EPOCH 1590
2024-02-03 09:52:08,620 Epoch 1590: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.68 
2024-02-03 09:52:08,620 EPOCH 1591
2024-02-03 09:52:13,290 Epoch 1591: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.73 
2024-02-03 09:52:13,290 EPOCH 1592
2024-02-03 09:52:14,147 [Epoch: 1592 Step: 00054100] Batch Recognition Loss:   0.000340 => Gls Tokens per Sec:     2244 || Batch Translation Loss:   0.019625 => Txt Tokens per Sec:     6407 || Lr: 0.000050
2024-02-03 09:52:17,760 Epoch 1592: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.72 
2024-02-03 09:52:17,760 EPOCH 1593
2024-02-03 09:52:22,674 Epoch 1593: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.68 
2024-02-03 09:52:22,675 EPOCH 1594
2024-02-03 09:52:26,992 Epoch 1594: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.59 
2024-02-03 09:52:26,993 EPOCH 1595
2024-02-03 09:52:27,552 [Epoch: 1595 Step: 00054200] Batch Recognition Loss:   0.000261 => Gls Tokens per Sec:     2295 || Batch Translation Loss:   0.015733 => Txt Tokens per Sec:     7280 || Lr: 0.000050
2024-02-03 09:52:31,702 Epoch 1595: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.54 
2024-02-03 09:52:31,703 EPOCH 1596
2024-02-03 09:52:36,179 Epoch 1596: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.57 
2024-02-03 09:52:36,179 EPOCH 1597
2024-02-03 09:52:40,960 Epoch 1597: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.51 
2024-02-03 09:52:40,961 EPOCH 1598
2024-02-03 09:52:41,215 [Epoch: 1598 Step: 00054300] Batch Recognition Loss:   0.000502 => Gls Tokens per Sec:     2540 || Batch Translation Loss:   0.015438 => Txt Tokens per Sec:     6389 || Lr: 0.000050
2024-02-03 09:52:45,568 Epoch 1598: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.80 
2024-02-03 09:52:45,569 EPOCH 1599
2024-02-03 09:52:50,483 Epoch 1599: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.73 
2024-02-03 09:52:50,484 EPOCH 1600
2024-02-03 09:52:55,035 [Epoch: 1600 Step: 00054400] Batch Recognition Loss:   0.000522 => Gls Tokens per Sec:     2336 || Batch Translation Loss:   0.010818 => Txt Tokens per Sec:     6496 || Lr: 0.000050
2024-02-03 09:52:55,036 Epoch 1600: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.66 
2024-02-03 09:52:55,036 EPOCH 1601
2024-02-03 09:52:59,603 Epoch 1601: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.56 
2024-02-03 09:52:59,604 EPOCH 1602
2024-02-03 09:53:04,049 Epoch 1602: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-03 09:53:04,049 EPOCH 1603
2024-02-03 09:53:08,570 [Epoch: 1603 Step: 00054500] Batch Recognition Loss:   0.000820 => Gls Tokens per Sec:     2211 || Batch Translation Loss:   0.019277 => Txt Tokens per Sec:     6160 || Lr: 0.000050
2024-02-03 09:53:08,768 Epoch 1603: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.53 
2024-02-03 09:53:08,768 EPOCH 1604
2024-02-03 09:53:13,536 Epoch 1604: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.50 
2024-02-03 09:53:13,537 EPOCH 1605
2024-02-03 09:53:17,990 Epoch 1605: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.49 
2024-02-03 09:53:17,990 EPOCH 1606
2024-02-03 09:53:22,496 [Epoch: 1606 Step: 00054600] Batch Recognition Loss:   0.000722 => Gls Tokens per Sec:     2076 || Batch Translation Loss:   0.018288 => Txt Tokens per Sec:     5846 || Lr: 0.000050
2024-02-03 09:53:22,929 Epoch 1606: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.51 
2024-02-03 09:53:22,929 EPOCH 1607
2024-02-03 09:53:27,197 Epoch 1607: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.52 
2024-02-03 09:53:27,197 EPOCH 1608
2024-02-03 09:53:32,088 Epoch 1608: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.52 
2024-02-03 09:53:32,089 EPOCH 1609
2024-02-03 09:53:35,740 [Epoch: 1609 Step: 00054700] Batch Recognition Loss:   0.000210 => Gls Tokens per Sec:     2386 || Batch Translation Loss:   0.008279 => Txt Tokens per Sec:     6619 || Lr: 0.000050
2024-02-03 09:53:36,377 Epoch 1609: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.74 
2024-02-03 09:53:36,378 EPOCH 1610
2024-02-03 09:53:40,990 Epoch 1610: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-03 09:53:40,991 EPOCH 1611
2024-02-03 09:53:45,811 Epoch 1611: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.86 
2024-02-03 09:53:45,812 EPOCH 1612
2024-02-03 09:53:49,451 [Epoch: 1612 Step: 00054800] Batch Recognition Loss:   0.012281 => Gls Tokens per Sec:     2218 || Batch Translation Loss:   0.480231 => Txt Tokens per Sec:     5973 || Lr: 0.000050
2024-02-03 09:53:50,737 Epoch 1612: Total Training Recognition Loss 0.13  Total Training Translation Loss 16.37 
2024-02-03 09:53:50,737 EPOCH 1613
2024-02-03 09:53:55,066 Epoch 1613: Total Training Recognition Loss 0.17  Total Training Translation Loss 5.12 
2024-02-03 09:53:55,067 EPOCH 1614
2024-02-03 09:53:59,998 Epoch 1614: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.02 
2024-02-03 09:53:59,998 EPOCH 1615
2024-02-03 09:54:02,866 [Epoch: 1615 Step: 00054900] Batch Recognition Loss:   0.000751 => Gls Tokens per Sec:     2680 || Batch Translation Loss:   0.025873 => Txt Tokens per Sec:     7514 || Lr: 0.000050
2024-02-03 09:54:04,531 Epoch 1615: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.52 
2024-02-03 09:54:04,532 EPOCH 1616
2024-02-03 09:54:09,175 Epoch 1616: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.02 
2024-02-03 09:54:09,175 EPOCH 1617
2024-02-03 09:54:13,926 Epoch 1617: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.07 
2024-02-03 09:54:13,927 EPOCH 1618
2024-02-03 09:54:16,722 [Epoch: 1618 Step: 00055000] Batch Recognition Loss:   0.000425 => Gls Tokens per Sec:     2520 || Batch Translation Loss:   0.019902 => Txt Tokens per Sec:     6743 || Lr: 0.000050
2024-02-03 09:54:18,337 Epoch 1618: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.75 
2024-02-03 09:54:18,337 EPOCH 1619
2024-02-03 09:54:22,913 Epoch 1619: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.94 
2024-02-03 09:54:22,914 EPOCH 1620
2024-02-03 09:54:27,475 Epoch 1620: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.93 
2024-02-03 09:54:27,475 EPOCH 1621
2024-02-03 09:54:30,005 [Epoch: 1621 Step: 00055100] Batch Recognition Loss:   0.000414 => Gls Tokens per Sec:     2433 || Batch Translation Loss:   0.021737 => Txt Tokens per Sec:     6790 || Lr: 0.000050
2024-02-03 09:54:32,079 Epoch 1621: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.82 
2024-02-03 09:54:32,080 EPOCH 1622
2024-02-03 09:54:36,652 Epoch 1622: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.76 
2024-02-03 09:54:36,652 EPOCH 1623
2024-02-03 09:54:41,483 Epoch 1623: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.68 
2024-02-03 09:54:41,483 EPOCH 1624
2024-02-03 09:54:43,937 [Epoch: 1624 Step: 00055200] Batch Recognition Loss:   0.001072 => Gls Tokens per Sec:     2349 || Batch Translation Loss:   0.017199 => Txt Tokens per Sec:     6607 || Lr: 0.000050
2024-02-03 09:54:45,911 Epoch 1624: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.64 
2024-02-03 09:54:45,911 EPOCH 1625
2024-02-03 09:54:50,878 Epoch 1625: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.69 
2024-02-03 09:54:50,879 EPOCH 1626
2024-02-03 09:54:55,403 Epoch 1626: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.85 
2024-02-03 09:54:55,403 EPOCH 1627
2024-02-03 09:54:57,606 [Epoch: 1627 Step: 00055300] Batch Recognition Loss:   0.000677 => Gls Tokens per Sec:     2325 || Batch Translation Loss:   0.017436 => Txt Tokens per Sec:     6372 || Lr: 0.000050
2024-02-03 09:55:00,155 Epoch 1627: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.76 
2024-02-03 09:55:00,156 EPOCH 1628
2024-02-03 09:55:04,893 Epoch 1628: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.73 
2024-02-03 09:55:04,893 EPOCH 1629
2024-02-03 09:55:09,340 Epoch 1629: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.58 
2024-02-03 09:55:09,340 EPOCH 1630
2024-02-03 09:55:11,097 [Epoch: 1630 Step: 00055400] Batch Recognition Loss:   0.000551 => Gls Tokens per Sec:     2552 || Batch Translation Loss:   0.010274 => Txt Tokens per Sec:     7113 || Lr: 0.000050
2024-02-03 09:55:14,183 Epoch 1630: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.59 
2024-02-03 09:55:14,184 EPOCH 1631
2024-02-03 09:55:19,151 Epoch 1631: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.53 
2024-02-03 09:55:19,152 EPOCH 1632
2024-02-03 09:55:23,935 Epoch 1632: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.66 
2024-02-03 09:55:23,935 EPOCH 1633
2024-02-03 09:55:26,089 [Epoch: 1633 Step: 00055500] Batch Recognition Loss:   0.000724 => Gls Tokens per Sec:     1667 || Batch Translation Loss:   0.019286 => Txt Tokens per Sec:     4982 || Lr: 0.000050
2024-02-03 09:55:28,864 Epoch 1633: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.62 
2024-02-03 09:55:28,864 EPOCH 1634
2024-02-03 09:55:33,229 Epoch 1634: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.56 
2024-02-03 09:55:33,229 EPOCH 1635
2024-02-03 09:55:38,031 Epoch 1635: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.53 
2024-02-03 09:55:38,032 EPOCH 1636
2024-02-03 09:55:39,114 [Epoch: 1636 Step: 00055600] Batch Recognition Loss:   0.000243 => Gls Tokens per Sec:     2963 || Batch Translation Loss:   0.010011 => Txt Tokens per Sec:     7875 || Lr: 0.000050
2024-02-03 09:55:42,375 Epoch 1636: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.49 
2024-02-03 09:55:42,375 EPOCH 1637
2024-02-03 09:55:46,826 Epoch 1637: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.55 
2024-02-03 09:55:46,826 EPOCH 1638
2024-02-03 09:55:51,553 Epoch 1638: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.68 
2024-02-03 09:55:51,553 EPOCH 1639
2024-02-03 09:55:52,428 [Epoch: 1639 Step: 00055700] Batch Recognition Loss:   0.000256 => Gls Tokens per Sec:     2929 || Batch Translation Loss:   0.010649 => Txt Tokens per Sec:     7759 || Lr: 0.000050
2024-02-03 09:55:56,167 Epoch 1639: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.73 
2024-02-03 09:55:56,167 EPOCH 1640
2024-02-03 09:56:00,983 Epoch 1640: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.08 
2024-02-03 09:56:00,984 EPOCH 1641
2024-02-03 09:56:05,940 Epoch 1641: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.25 
2024-02-03 09:56:05,940 EPOCH 1642
2024-02-03 09:56:06,548 [Epoch: 1642 Step: 00055800] Batch Recognition Loss:   0.000586 => Gls Tokens per Sec:     3162 || Batch Translation Loss:   0.026121 => Txt Tokens per Sec:     7599 || Lr: 0.000050
2024-02-03 09:56:10,857 Epoch 1642: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.49 
2024-02-03 09:56:10,857 EPOCH 1643
2024-02-03 09:56:15,249 Epoch 1643: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.79 
2024-02-03 09:56:15,249 EPOCH 1644
2024-02-03 09:56:20,008 Epoch 1644: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.79 
2024-02-03 09:56:20,009 EPOCH 1645
2024-02-03 09:56:20,350 [Epoch: 1645 Step: 00055900] Batch Recognition Loss:   0.000303 => Gls Tokens per Sec:     3759 || Batch Translation Loss:   0.016742 => Txt Tokens per Sec:     9024 || Lr: 0.000050
2024-02-03 09:56:24,363 Epoch 1645: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.83 
2024-02-03 09:56:24,363 EPOCH 1646
2024-02-03 09:56:29,360 Epoch 1646: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.17 
2024-02-03 09:56:29,360 EPOCH 1647
2024-02-03 09:56:33,851 Epoch 1647: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.60 
2024-02-03 09:56:33,852 EPOCH 1648
2024-02-03 09:56:34,137 [Epoch: 1648 Step: 00056000] Batch Recognition Loss:   0.000150 => Gls Tokens per Sec:     2257 || Batch Translation Loss:   0.023448 => Txt Tokens per Sec:     5953 || Lr: 0.000050
2024-02-03 09:56:42,606 Validation result at epoch 1648, step    56000: duration: 8.4685s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.25135	Translation Loss: 90771.64844	PPL: 12430.17383
	Eval Metric: BLEU
	WER 3.11	(DEL: 0.00,	INS: 0.00,	SUB: 3.11)
	BLEU-4 0.40	(BLEU-1: 9.22,	BLEU-2: 2.30,	BLEU-3: 0.83,	BLEU-4: 0.40)
	CHRF 16.34	ROUGE 7.64
2024-02-03 09:56:42,607 Logging Recognition and Translation Outputs
2024-02-03 09:56:42,607 ========================================================================================================================
2024-02-03 09:56:42,607 Logging Sequence: 92_59.00
2024-02-03 09:56:42,607 	Gloss Reference :	A B+C+D+E
2024-02-03 09:56:42,608 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 09:56:42,608 	Gloss Alignment :	         
2024-02-03 09:56:42,608 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 09:56:42,609 	Text Reference  :	****** ***** ***** **** *** after   the     indian team's defeat 2    men dancing and   bursting   crackers outside her house
2024-02-03 09:56:42,610 	Text Hypothesis :	across india manyt deaf and hearing friends of     mine   had    gone for ipl     match screenings in       bars    and cafes
2024-02-03 09:56:42,610 	Text Alignment  :	I      I     I     I    I   S       S       S      S      S      S    S   S       S     S          S        S       S   S    
2024-02-03 09:56:42,610 ========================================================================================================================
2024-02-03 09:56:42,610 Logging Sequence: 81_73.00
2024-02-03 09:56:42,610 	Gloss Reference :	A B+C+D+E
2024-02-03 09:56:42,610 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 09:56:42,610 	Gloss Alignment :	         
2024-02-03 09:56:42,610 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 09:56:42,611 	Text Reference  :	buyers liked the ads and booked many    houses with     the builder sadly    they were   duped       
2024-02-03 09:56:42,611 	Text Hypothesis :	****** ***** the *** *** ****** supreme court  canceled the ******* builder' real estate registration
2024-02-03 09:56:42,612 	Text Alignment  :	D      D         D   D   D      S       S      S            D       S        S    S      S           
2024-02-03 09:56:42,612 ========================================================================================================================
2024-02-03 09:56:42,612 Logging Sequence: 166_41.00
2024-02-03 09:56:42,612 	Gloss Reference :	A B+C+D+E
2024-02-03 09:56:42,612 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 09:56:42,612 	Gloss Alignment :	         
2024-02-03 09:56:42,612 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 09:56:42,613 	Text Reference  :	test match is       played patiently till the opponents are all out       
2024-02-03 09:56:42,613 	Text Hypothesis :	i    am    thankful to     claim     over the ********* *** *** cricketers
2024-02-03 09:56:42,613 	Text Alignment  :	S    S     S        S      S         S        D         D   D   S         
2024-02-03 09:56:42,613 ========================================================================================================================
2024-02-03 09:56:42,614 Logging Sequence: 171_33.00
2024-02-03 09:56:42,614 	Gloss Reference :	A B+C+D+E
2024-02-03 09:56:42,614 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 09:56:42,614 	Gloss Alignment :	         
2024-02-03 09:56:42,614 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 09:56:42,616 	Text Reference  :	the ****** ****** gujarat titans vs  chennai super kings match *** ****** ******* *** *** **** ** ***** was held    on  23rd  may    
2024-02-03 09:56:42,616 	Text Hypothesis :	the couple wanted to      keep   the screens in    the   match and mumbai indians can can lead to watch the screens for men's cricket
2024-02-03 09:56:42,616 	Text Alignment  :	    I      I      S       S      S   S       S     S           I   I      I       I   I   I    I  I     S   S       S   S     S      
2024-02-03 09:56:42,616 ========================================================================================================================
2024-02-03 09:56:42,616 Logging Sequence: 140_2.00
2024-02-03 09:56:42,616 	Gloss Reference :	A B+C+D+E
2024-02-03 09:56:42,617 	Gloss Hypothesis:	A B+C+E+D
2024-02-03 09:56:42,617 	Gloss Alignment :	  S      
2024-02-03 09:56:42,617 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 09:56:42,618 	Text Reference  :	*** ** indian batsman-wicket keeper rishabh pant has outstanding skills in      cricket
2024-02-03 09:56:42,618 	Text Hypothesis :	let me tell   you            why    and     then she is          a      similar joke   
2024-02-03 09:56:42,618 	Text Alignment  :	I   I  S      S              S      S       S    S   S           S      S       S      
2024-02-03 09:56:42,618 ========================================================================================================================
2024-02-03 09:56:47,044 Epoch 1648: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.11 
2024-02-03 09:56:47,045 EPOCH 1649
2024-02-03 09:56:51,960 Epoch 1649: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.99 
2024-02-03 09:56:51,961 EPOCH 1650
2024-02-03 09:56:56,219 [Epoch: 1650 Step: 00056100] Batch Recognition Loss:   0.000864 => Gls Tokens per Sec:     2497 || Batch Translation Loss:   0.093507 => Txt Tokens per Sec:     6944 || Lr: 0.000050
2024-02-03 09:56:56,219 Epoch 1650: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.46 
2024-02-03 09:56:56,219 EPOCH 1651
2024-02-03 09:57:01,183 Epoch 1651: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.42 
2024-02-03 09:57:01,184 EPOCH 1652
2024-02-03 09:57:05,387 Epoch 1652: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.46 
2024-02-03 09:57:05,388 EPOCH 1653
2024-02-03 09:57:09,125 [Epoch: 1653 Step: 00056200] Batch Recognition Loss:   0.000203 => Gls Tokens per Sec:     2673 || Batch Translation Loss:   0.026706 => Txt Tokens per Sec:     7511 || Lr: 0.000050
2024-02-03 09:57:09,410 Epoch 1653: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.11 
2024-02-03 09:57:09,410 EPOCH 1654
2024-02-03 09:57:13,863 Epoch 1654: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.94 
2024-02-03 09:57:13,863 EPOCH 1655
2024-02-03 09:57:18,578 Epoch 1655: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.92 
2024-02-03 09:57:18,578 EPOCH 1656
2024-02-03 09:57:22,878 [Epoch: 1656 Step: 00056300] Batch Recognition Loss:   0.000249 => Gls Tokens per Sec:     2175 || Batch Translation Loss:   0.018172 => Txt Tokens per Sec:     6120 || Lr: 0.000050
2024-02-03 09:57:23,294 Epoch 1656: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.87 
2024-02-03 09:57:23,294 EPOCH 1657
2024-02-03 09:57:27,799 Epoch 1657: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.86 
2024-02-03 09:57:27,799 EPOCH 1658
2024-02-03 09:57:32,703 Epoch 1658: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.66 
2024-02-03 09:57:32,703 EPOCH 1659
2024-02-03 09:57:36,041 [Epoch: 1659 Step: 00056400] Batch Recognition Loss:   0.001302 => Gls Tokens per Sec:     2611 || Batch Translation Loss:   0.074223 => Txt Tokens per Sec:     7207 || Lr: 0.000050
2024-02-03 09:57:37,071 Epoch 1659: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.96 
2024-02-03 09:57:37,072 EPOCH 1660
2024-02-03 09:57:41,969 Epoch 1660: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.75 
2024-02-03 09:57:41,970 EPOCH 1661
2024-02-03 09:57:46,514 Epoch 1661: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.18 
2024-02-03 09:57:46,515 EPOCH 1662
2024-02-03 09:57:50,212 [Epoch: 1662 Step: 00056500] Batch Recognition Loss:   0.000392 => Gls Tokens per Sec:     2183 || Batch Translation Loss:   0.016408 => Txt Tokens per Sec:     6118 || Lr: 0.000050
2024-02-03 09:57:51,170 Epoch 1662: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.24 
2024-02-03 09:57:51,170 EPOCH 1663
2024-02-03 09:57:56,013 Epoch 1663: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.89 
2024-02-03 09:57:56,014 EPOCH 1664
2024-02-03 09:58:00,367 Epoch 1664: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.26 
2024-02-03 09:58:00,368 EPOCH 1665
2024-02-03 09:58:03,789 [Epoch: 1665 Step: 00056600] Batch Recognition Loss:   0.001083 => Gls Tokens per Sec:     2172 || Batch Translation Loss:   0.032549 => Txt Tokens per Sec:     5814 || Lr: 0.000050
2024-02-03 09:58:05,310 Epoch 1665: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.17 
2024-02-03 09:58:05,310 EPOCH 1666
2024-02-03 09:58:09,879 Epoch 1666: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.99 
2024-02-03 09:58:09,880 EPOCH 1667
2024-02-03 09:58:14,672 Epoch 1667: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.77 
2024-02-03 09:58:14,673 EPOCH 1668
2024-02-03 09:58:17,401 [Epoch: 1668 Step: 00056700] Batch Recognition Loss:   0.000309 => Gls Tokens per Sec:     2582 || Batch Translation Loss:   0.016572 => Txt Tokens per Sec:     7204 || Lr: 0.000050
2024-02-03 09:58:19,218 Epoch 1668: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.78 
2024-02-03 09:58:19,219 EPOCH 1669
2024-02-03 09:58:23,747 Epoch 1669: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.84 
2024-02-03 09:58:23,748 EPOCH 1670
2024-02-03 09:58:28,560 Epoch 1670: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.07 
2024-02-03 09:58:28,560 EPOCH 1671
2024-02-03 09:58:31,243 [Epoch: 1671 Step: 00056800] Batch Recognition Loss:   0.000290 => Gls Tokens per Sec:     2293 || Batch Translation Loss:   0.020896 => Txt Tokens per Sec:     6397 || Lr: 0.000050
2024-02-03 09:58:33,058 Epoch 1671: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.03 
2024-02-03 09:58:33,059 EPOCH 1672
2024-02-03 09:58:37,996 Epoch 1672: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.98 
2024-02-03 09:58:37,996 EPOCH 1673
2024-02-03 09:58:42,430 Epoch 1673: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.82 
2024-02-03 09:58:42,430 EPOCH 1674
2024-02-03 09:58:45,204 [Epoch: 1674 Step: 00056900] Batch Recognition Loss:   0.000463 => Gls Tokens per Sec:     2077 || Batch Translation Loss:   0.021943 => Txt Tokens per Sec:     5882 || Lr: 0.000050
2024-02-03 09:58:47,174 Epoch 1674: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.62 
2024-02-03 09:58:47,175 EPOCH 1675
2024-02-03 09:58:51,267 Epoch 1675: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.60 
2024-02-03 09:58:51,268 EPOCH 1676
2024-02-03 09:58:56,170 Epoch 1676: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.57 
2024-02-03 09:58:56,170 EPOCH 1677
2024-02-03 09:58:58,446 [Epoch: 1677 Step: 00057000] Batch Recognition Loss:   0.001129 => Gls Tokens per Sec:     2250 || Batch Translation Loss:   0.017981 => Txt Tokens per Sec:     6647 || Lr: 0.000050
2024-02-03 09:59:00,478 Epoch 1677: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.67 
2024-02-03 09:59:00,479 EPOCH 1678
2024-02-03 09:59:05,325 Epoch 1678: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.56 
2024-02-03 09:59:05,325 EPOCH 1679
2024-02-03 09:59:09,885 Epoch 1679: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.60 
2024-02-03 09:59:09,886 EPOCH 1680
2024-02-03 09:59:11,937 [Epoch: 1680 Step: 00057100] Batch Recognition Loss:   0.001610 => Gls Tokens per Sec:     2186 || Batch Translation Loss:   0.035482 => Txt Tokens per Sec:     5950 || Lr: 0.000050
2024-02-03 09:59:14,573 Epoch 1680: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.71 
2024-02-03 09:59:14,573 EPOCH 1681
2024-02-03 09:59:19,255 Epoch 1681: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.73 
2024-02-03 09:59:19,255 EPOCH 1682
2024-02-03 09:59:23,728 Epoch 1682: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.71 
2024-02-03 09:59:23,728 EPOCH 1683
2024-02-03 09:59:24,985 [Epoch: 1683 Step: 00057200] Batch Recognition Loss:   0.000339 => Gls Tokens per Sec:     2862 || Batch Translation Loss:   0.017906 => Txt Tokens per Sec:     7500 || Lr: 0.000050
2024-02-03 09:59:28,363 Epoch 1683: Total Training Recognition Loss 0.22  Total Training Translation Loss 3.76 
2024-02-03 09:59:28,363 EPOCH 1684
2024-02-03 09:59:32,881 Epoch 1684: Total Training Recognition Loss 0.55  Total Training Translation Loss 1.82 
2024-02-03 09:59:32,881 EPOCH 1685
2024-02-03 09:59:37,790 Epoch 1685: Total Training Recognition Loss 0.14  Total Training Translation Loss 1.62 
2024-02-03 09:59:37,790 EPOCH 1686
2024-02-03 09:59:38,994 [Epoch: 1686 Step: 00057300] Batch Recognition Loss:   0.002333 => Gls Tokens per Sec:     2660 || Batch Translation Loss:   0.024794 => Txt Tokens per Sec:     7510 || Lr: 0.000050
2024-02-03 09:59:42,298 Epoch 1686: Total Training Recognition Loss 0.09  Total Training Translation Loss 1.05 
2024-02-03 09:59:42,298 EPOCH 1687
2024-02-03 09:59:47,159 Epoch 1687: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.50 
2024-02-03 09:59:47,160 EPOCH 1688
2024-02-03 09:59:51,669 Epoch 1688: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.19 
2024-02-03 09:59:51,670 EPOCH 1689
2024-02-03 09:59:52,683 [Epoch: 1689 Step: 00057400] Batch Recognition Loss:   0.000559 => Gls Tokens per Sec:     2530 || Batch Translation Loss:   0.143334 => Txt Tokens per Sec:     7257 || Lr: 0.000050
2024-02-03 09:59:56,512 Epoch 1689: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.09 
2024-02-03 09:59:56,513 EPOCH 1690
2024-02-03 10:00:01,472 Epoch 1690: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.23 
2024-02-03 10:00:01,473 EPOCH 1691
2024-02-03 10:00:06,280 Epoch 1691: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.94 
2024-02-03 10:00:06,281 EPOCH 1692
2024-02-03 10:00:07,501 [Epoch: 1692 Step: 00057500] Batch Recognition Loss:   0.001239 => Gls Tokens per Sec:     1577 || Batch Translation Loss:   0.027137 => Txt Tokens per Sec:     4795 || Lr: 0.000050
2024-02-03 10:00:11,134 Epoch 1692: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.81 
2024-02-03 10:00:11,135 EPOCH 1693
2024-02-03 10:00:15,271 Epoch 1693: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.81 
2024-02-03 10:00:15,272 EPOCH 1694
2024-02-03 10:00:20,245 Epoch 1694: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.70 
2024-02-03 10:00:20,246 EPOCH 1695
2024-02-03 10:00:20,727 [Epoch: 1695 Step: 00057600] Batch Recognition Loss:   0.000971 => Gls Tokens per Sec:     2667 || Batch Translation Loss:   0.024593 => Txt Tokens per Sec:     7548 || Lr: 0.000050
2024-02-03 10:00:24,834 Epoch 1695: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.62 
2024-02-03 10:00:24,835 EPOCH 1696
2024-02-03 10:00:29,452 Epoch 1696: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.59 
2024-02-03 10:00:29,453 EPOCH 1697
2024-02-03 10:00:34,081 Epoch 1697: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.63 
2024-02-03 10:00:34,081 EPOCH 1698
2024-02-03 10:00:34,407 [Epoch: 1698 Step: 00057700] Batch Recognition Loss:   0.000480 => Gls Tokens per Sec:     1969 || Batch Translation Loss:   0.019682 => Txt Tokens per Sec:     6252 || Lr: 0.000050
2024-02-03 10:00:38,606 Epoch 1698: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.63 
2024-02-03 10:00:38,607 EPOCH 1699
2024-02-03 10:00:43,526 Epoch 1699: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.72 
2024-02-03 10:00:43,526 EPOCH 1700
2024-02-03 10:00:47,968 [Epoch: 1700 Step: 00057800] Batch Recognition Loss:   0.000409 => Gls Tokens per Sec:     2394 || Batch Translation Loss:   0.008745 => Txt Tokens per Sec:     6657 || Lr: 0.000050
2024-02-03 10:00:47,969 Epoch 1700: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.61 
2024-02-03 10:00:47,969 EPOCH 1701
2024-02-03 10:00:52,787 Epoch 1701: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.56 
2024-02-03 10:00:52,787 EPOCH 1702
2024-02-03 10:00:57,284 Epoch 1702: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.54 
2024-02-03 10:00:57,285 EPOCH 1703
2024-02-03 10:01:01,663 [Epoch: 1703 Step: 00057900] Batch Recognition Loss:   0.002339 => Gls Tokens per Sec:     2282 || Batch Translation Loss:   0.046402 => Txt Tokens per Sec:     6346 || Lr: 0.000050
2024-02-03 10:01:01,886 Epoch 1703: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.52 
2024-02-03 10:01:01,886 EPOCH 1704
2024-02-03 10:01:06,604 Epoch 1704: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.50 
2024-02-03 10:01:06,604 EPOCH 1705
2024-02-03 10:01:11,030 Epoch 1705: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.51 
2024-02-03 10:01:11,030 EPOCH 1706
2024-02-03 10:01:15,032 [Epoch: 1706 Step: 00058000] Batch Recognition Loss:   0.000602 => Gls Tokens per Sec:     2337 || Batch Translation Loss:   0.014430 => Txt Tokens per Sec:     6426 || Lr: 0.000050
2024-02-03 10:01:23,702 Validation result at epoch 1706, step    58000: duration: 8.6678s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.53184	Translation Loss: 90825.93750	PPL: 12500.45508
	Eval Metric: BLEU
	WER 3.18	(DEL: 0.07,	INS: 0.00,	SUB: 3.11)
	BLEU-4 0.00	(BLEU-1: 9.39,	BLEU-2: 2.39,	BLEU-3: 0.67,	BLEU-4: 0.00)
	CHRF 16.60	ROUGE 7.85
2024-02-03 10:01:23,703 Logging Recognition and Translation Outputs
2024-02-03 10:01:23,703 ========================================================================================================================
2024-02-03 10:01:23,703 Logging Sequence: 135_100.00
2024-02-03 10:01:23,703 	Gloss Reference :	A B+C+D+E
2024-02-03 10:01:23,703 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 10:01:23,704 	Gloss Alignment :	         
2024-02-03 10:01:23,704 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 10:01:23,705 	Text Reference  :	and that her goal was  to raise the  other half   through the **** medal auction
2024-02-03 10:01:23,705 	Text Hypothesis :	and that *** they want to ***** have been  raised by      the want the   hotel  
2024-02-03 10:01:23,705 	Text Alignment  :	         D   S    S       D     S    S     S      S           I    S     S      
2024-02-03 10:01:23,705 ========================================================================================================================
2024-02-03 10:01:23,706 Logging Sequence: 63_35.00
2024-02-03 10:01:23,706 	Gloss Reference :	A B+C+D+E
2024-02-03 10:01:23,706 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 10:01:23,706 	Gloss Alignment :	         
2024-02-03 10:01:23,707 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 10:01:23,708 	Text Reference  :	companies interested in buying the teams need to          fill the       tender form by  paying    rs   10      lakh
2024-02-03 10:01:23,708 	Text Hypothesis :	********* ********** ** ****** it  was   held alternately in   australia but    he   was extremely 15th october 2023
2024-02-03 10:01:23,708 	Text Alignment  :	D         D          D  D      S   S     S    S           S    S         S      S    S   S         S    S       S   
2024-02-03 10:01:23,708 ========================================================================================================================
2024-02-03 10:01:23,709 Logging Sequence: 103_122.00
2024-02-03 10:01:23,709 	Gloss Reference :	A B+C+D+E
2024-02-03 10:01:23,709 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 10:01:23,709 	Gloss Alignment :	         
2024-02-03 10:01:23,709 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 10:01:23,710 	Text Reference  :	** *** *** ***** like india    australia canada pakistan nigeria in        africa and many more
2024-02-03 10:01:23,711 	Text Hypothesis :	as per the rules the  original target    of     215      was     truncated to     171 runs in  
2024-02-03 10:01:23,711 	Text Alignment  :	I  I   I   I     S    S        S         S      S        S       S         S      S   S    S   
2024-02-03 10:01:23,711 ========================================================================================================================
2024-02-03 10:01:23,711 Logging Sequence: 106_175.00
2024-02-03 10:01:23,711 	Gloss Reference :	A B+C+D+E
2024-02-03 10:01:23,711 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 10:01:23,711 	Gloss Alignment :	         
2024-02-03 10:01:23,712 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 10:01:23,712 	Text Reference  :	*** ***** ** **** **** wonderful but what about the deaf women's team      
2024-02-03 10:01:23,712 	Text Hypothesis :	she tried to show that she       was not  with  the **** indian  contingent
2024-02-03 10:01:23,713 	Text Alignment  :	I   I     I  I    I    S         S   S    S         D    S       S         
2024-02-03 10:01:23,713 ========================================================================================================================
2024-02-03 10:01:23,713 Logging Sequence: 83_57.00
2024-02-03 10:01:23,713 	Gloss Reference :	A B+C+D+E
2024-02-03 10:01:23,713 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 10:01:23,713 	Gloss Alignment :	         
2024-02-03 10:01:23,713 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 10:01:23,714 	Text Reference  :	collapsed face first on      the     field he     was     completely unconscious
2024-02-03 10:01:23,714 	Text Hypothesis :	********* the  were  violent clashes and   abuses towards the        italians   
2024-02-03 10:01:23,714 	Text Alignment  :	D         S    S     S       S       S     S      S       S          S          
2024-02-03 10:01:23,715 ========================================================================================================================
2024-02-03 10:01:24,270 Epoch 1706: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.53 
2024-02-03 10:01:24,270 EPOCH 1707
2024-02-03 10:01:29,050 Epoch 1707: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.49 
2024-02-03 10:01:29,050 EPOCH 1708
2024-02-03 10:01:33,940 Epoch 1708: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.57 
2024-02-03 10:01:33,941 EPOCH 1709
2024-02-03 10:01:37,685 [Epoch: 1709 Step: 00058100] Batch Recognition Loss:   0.000893 => Gls Tokens per Sec:     2327 || Batch Translation Loss:   0.019115 => Txt Tokens per Sec:     6566 || Lr: 0.000050
2024-02-03 10:01:38,395 Epoch 1709: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.57 
2024-02-03 10:01:38,396 EPOCH 1710
2024-02-03 10:01:43,320 Epoch 1710: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.88 
2024-02-03 10:01:43,320 EPOCH 1711
2024-02-03 10:01:47,878 Epoch 1711: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.83 
2024-02-03 10:01:47,879 EPOCH 1712
2024-02-03 10:01:51,098 [Epoch: 1712 Step: 00058200] Batch Recognition Loss:   0.000147 => Gls Tokens per Sec:     2507 || Batch Translation Loss:   0.021582 => Txt Tokens per Sec:     6792 || Lr: 0.000050
2024-02-03 10:01:52,513 Epoch 1712: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.73 
2024-02-03 10:01:52,513 EPOCH 1713
2024-02-03 10:01:56,726 Epoch 1713: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-03 10:01:56,726 EPOCH 1714
2024-02-03 10:02:01,604 Epoch 1714: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.94 
2024-02-03 10:02:01,604 EPOCH 1715
2024-02-03 10:02:04,946 [Epoch: 1715 Step: 00058300] Batch Recognition Loss:   0.000483 => Gls Tokens per Sec:     2299 || Batch Translation Loss:   0.012080 => Txt Tokens per Sec:     6547 || Lr: 0.000050
2024-02-03 10:02:06,157 Epoch 1715: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.08 
2024-02-03 10:02:06,158 EPOCH 1716
2024-02-03 10:02:10,843 Epoch 1716: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.41 
2024-02-03 10:02:10,843 EPOCH 1717
2024-02-03 10:02:15,514 Epoch 1717: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.32 
2024-02-03 10:02:15,514 EPOCH 1718
2024-02-03 10:02:18,576 [Epoch: 1718 Step: 00058400] Batch Recognition Loss:   0.000967 => Gls Tokens per Sec:     2300 || Batch Translation Loss:   0.113642 => Txt Tokens per Sec:     6438 || Lr: 0.000050
2024-02-03 10:02:19,987 Epoch 1718: Total Training Recognition Loss 0.08  Total Training Translation Loss 6.07 
2024-02-03 10:02:19,987 EPOCH 1719
2024-02-03 10:02:24,886 Epoch 1719: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.29 
2024-02-03 10:02:24,887 EPOCH 1720
2024-02-03 10:02:29,362 Epoch 1720: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.77 
2024-02-03 10:02:29,363 EPOCH 1721
2024-02-03 10:02:32,353 [Epoch: 1721 Step: 00058500] Batch Recognition Loss:   0.000259 => Gls Tokens per Sec:     2057 || Batch Translation Loss:   0.025501 => Txt Tokens per Sec:     5580 || Lr: 0.000050
2024-02-03 10:02:34,263 Epoch 1721: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.34 
2024-02-03 10:02:34,263 EPOCH 1722
2024-02-03 10:02:38,789 Epoch 1722: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.93 
2024-02-03 10:02:38,790 EPOCH 1723
2024-02-03 10:02:43,453 Epoch 1723: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.83 
2024-02-03 10:02:43,454 EPOCH 1724
2024-02-03 10:02:45,710 [Epoch: 1724 Step: 00058600] Batch Recognition Loss:   0.000681 => Gls Tokens per Sec:     2445 || Batch Translation Loss:   0.018588 => Txt Tokens per Sec:     6868 || Lr: 0.000050
2024-02-03 10:02:47,744 Epoch 1724: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.72 
2024-02-03 10:02:47,745 EPOCH 1725
2024-02-03 10:02:52,803 Epoch 1725: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.68 
2024-02-03 10:02:52,803 EPOCH 1726
2024-02-03 10:02:57,668 Epoch 1726: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.68 
2024-02-03 10:02:57,669 EPOCH 1727
2024-02-03 10:02:59,937 [Epoch: 1727 Step: 00058700] Batch Recognition Loss:   0.000192 => Gls Tokens per Sec:     2258 || Batch Translation Loss:   0.013070 => Txt Tokens per Sec:     6401 || Lr: 0.000050
2024-02-03 10:03:02,396 Epoch 1727: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.63 
2024-02-03 10:03:02,397 EPOCH 1728
2024-02-03 10:03:07,238 Epoch 1728: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.60 
2024-02-03 10:03:07,238 EPOCH 1729
2024-02-03 10:03:11,641 Epoch 1729: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.64 
2024-02-03 10:03:11,642 EPOCH 1730
2024-02-03 10:03:13,433 [Epoch: 1730 Step: 00058800] Batch Recognition Loss:   0.000446 => Gls Tokens per Sec:     2502 || Batch Translation Loss:   0.012072 => Txt Tokens per Sec:     6604 || Lr: 0.000050
2024-02-03 10:03:16,492 Epoch 1730: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.61 
2024-02-03 10:03:16,492 EPOCH 1731
2024-02-03 10:03:21,276 Epoch 1731: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-03 10:03:21,277 EPOCH 1732
2024-02-03 10:03:26,099 Epoch 1732: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.59 
2024-02-03 10:03:26,100 EPOCH 1733
2024-02-03 10:03:27,533 [Epoch: 1733 Step: 00058900] Batch Recognition Loss:   0.000409 => Gls Tokens per Sec:     2505 || Batch Translation Loss:   0.005807 => Txt Tokens per Sec:     7008 || Lr: 0.000050
2024-02-03 10:03:30,350 Epoch 1733: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.61 
2024-02-03 10:03:30,350 EPOCH 1734
2024-02-03 10:03:35,276 Epoch 1734: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.57 
2024-02-03 10:03:35,276 EPOCH 1735
2024-02-03 10:03:39,822 Epoch 1735: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-03 10:03:39,823 EPOCH 1736
2024-02-03 10:03:41,350 [Epoch: 1736 Step: 00059000] Batch Recognition Loss:   0.000156 => Gls Tokens per Sec:     1932 || Batch Translation Loss:   0.016473 => Txt Tokens per Sec:     5584 || Lr: 0.000050
2024-02-03 10:03:44,469 Epoch 1736: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-03 10:03:44,469 EPOCH 1737
2024-02-03 10:03:48,530 Epoch 1737: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.85 
2024-02-03 10:03:48,530 EPOCH 1738
2024-02-03 10:03:53,496 Epoch 1738: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.73 
2024-02-03 10:03:53,497 EPOCH 1739
2024-02-03 10:03:54,519 [Epoch: 1739 Step: 00059100] Batch Recognition Loss:   0.000369 => Gls Tokens per Sec:     2506 || Batch Translation Loss:   0.019200 => Txt Tokens per Sec:     6683 || Lr: 0.000050
2024-02-03 10:03:57,984 Epoch 1739: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.88 
2024-02-03 10:03:57,985 EPOCH 1740
2024-02-03 10:04:02,824 Epoch 1740: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.26 
2024-02-03 10:04:02,825 EPOCH 1741
2024-02-03 10:04:07,391 Epoch 1741: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.27 
2024-02-03 10:04:07,392 EPOCH 1742
2024-02-03 10:04:07,960 [Epoch: 1742 Step: 00059200] Batch Recognition Loss:   0.000897 => Gls Tokens per Sec:     3383 || Batch Translation Loss:   0.139144 => Txt Tokens per Sec:     8115 || Lr: 0.000050
2024-02-03 10:04:11,967 Epoch 1742: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.14 
2024-02-03 10:04:11,967 EPOCH 1743
2024-02-03 10:04:16,752 Epoch 1743: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.17 
2024-02-03 10:04:16,753 EPOCH 1744
2024-02-03 10:04:21,470 Epoch 1744: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.43 
2024-02-03 10:04:21,471 EPOCH 1745
2024-02-03 10:04:21,874 [Epoch: 1745 Step: 00059300] Batch Recognition Loss:   0.001372 => Gls Tokens per Sec:     3192 || Batch Translation Loss:   0.028018 => Txt Tokens per Sec:     8150 || Lr: 0.000050
2024-02-03 10:04:26,072 Epoch 1745: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.94 
2024-02-03 10:04:26,073 EPOCH 1746
2024-02-03 10:04:30,121 Epoch 1746: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.79 
2024-02-03 10:04:30,121 EPOCH 1747
2024-02-03 10:04:34,979 Epoch 1747: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.78 
2024-02-03 10:04:34,980 EPOCH 1748
2024-02-03 10:04:35,136 [Epoch: 1748 Step: 00059400] Batch Recognition Loss:   0.000802 => Gls Tokens per Sec:     4129 || Batch Translation Loss:   0.012444 => Txt Tokens per Sec:     9148 || Lr: 0.000050
2024-02-03 10:04:39,634 Epoch 1748: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.89 
2024-02-03 10:04:39,635 EPOCH 1749
2024-02-03 10:04:44,573 Epoch 1749: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.80 
2024-02-03 10:04:44,573 EPOCH 1750
2024-02-03 10:04:49,450 [Epoch: 1750 Step: 00059500] Batch Recognition Loss:   0.003484 => Gls Tokens per Sec:     2180 || Batch Translation Loss:   0.031985 => Txt Tokens per Sec:     6063 || Lr: 0.000050
2024-02-03 10:04:49,451 Epoch 1750: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.71 
2024-02-03 10:04:49,451 EPOCH 1751
2024-02-03 10:04:53,764 Epoch 1751: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.71 
2024-02-03 10:04:53,765 EPOCH 1752
2024-02-03 10:04:58,635 Epoch 1752: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.57 
2024-02-03 10:04:58,635 EPOCH 1753
2024-02-03 10:05:03,115 [Epoch: 1753 Step: 00059600] Batch Recognition Loss:   0.001266 => Gls Tokens per Sec:     2230 || Batch Translation Loss:   0.019949 => Txt Tokens per Sec:     6172 || Lr: 0.000050
2024-02-03 10:05:03,358 Epoch 1753: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.77 
2024-02-03 10:05:03,359 EPOCH 1754
2024-02-03 10:05:08,211 Epoch 1754: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.65 
2024-02-03 10:05:08,212 EPOCH 1755
2024-02-03 10:05:13,146 Epoch 1755: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.56 
2024-02-03 10:05:13,147 EPOCH 1756
2024-02-03 10:05:17,184 [Epoch: 1756 Step: 00059700] Batch Recognition Loss:   0.000190 => Gls Tokens per Sec:     2317 || Batch Translation Loss:   0.007606 => Txt Tokens per Sec:     6547 || Lr: 0.000050
2024-02-03 10:05:17,617 Epoch 1756: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.65 
2024-02-03 10:05:17,617 EPOCH 1757
2024-02-03 10:05:22,350 Epoch 1757: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.91 
2024-02-03 10:05:22,350 EPOCH 1758
2024-02-03 10:05:26,984 Epoch 1758: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.58 
2024-02-03 10:05:26,985 EPOCH 1759
2024-02-03 10:05:31,153 [Epoch: 1759 Step: 00059800] Batch Recognition Loss:   0.000097 => Gls Tokens per Sec:     2091 || Batch Translation Loss:   0.009782 => Txt Tokens per Sec:     5834 || Lr: 0.000050
2024-02-03 10:05:31,905 Epoch 1759: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.56 
2024-02-03 10:05:31,905 EPOCH 1760
2024-02-03 10:05:36,796 Epoch 1760: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.52 
2024-02-03 10:05:36,797 EPOCH 1761
2024-02-03 10:05:41,592 Epoch 1761: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-03 10:05:41,593 EPOCH 1762
2024-02-03 10:05:44,493 [Epoch: 1762 Step: 00059900] Batch Recognition Loss:   0.001016 => Gls Tokens per Sec:     2783 || Batch Translation Loss:   0.007612 => Txt Tokens per Sec:     7409 || Lr: 0.000050
2024-02-03 10:05:45,980 Epoch 1762: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.62 
2024-02-03 10:05:45,981 EPOCH 1763
2024-02-03 10:05:50,792 Epoch 1763: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.72 
2024-02-03 10:05:50,792 EPOCH 1764
2024-02-03 10:05:55,363 Epoch 1764: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.99 
2024-02-03 10:05:55,364 EPOCH 1765
2024-02-03 10:05:58,405 [Epoch: 1765 Step: 00060000] Batch Recognition Loss:   0.000275 => Gls Tokens per Sec:     2444 || Batch Translation Loss:   0.090816 => Txt Tokens per Sec:     6639 || Lr: 0.000050
2024-02-03 10:06:07,049 Validation result at epoch 1765, step    60000: duration: 8.6436s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.47209	Translation Loss: 91648.17188	PPL: 13614.91113
	Eval Metric: BLEU
	WER 3.11	(DEL: 0.00,	INS: 0.00,	SUB: 3.11)
	BLEU-4 0.61	(BLEU-1: 9.07,	BLEU-2: 2.64,	BLEU-3: 1.09,	BLEU-4: 0.61)
	CHRF 16.23	ROUGE 7.66
2024-02-03 10:06:07,050 Logging Recognition and Translation Outputs
2024-02-03 10:06:07,050 ========================================================================================================================
2024-02-03 10:06:07,050 Logging Sequence: 161_210.00
2024-02-03 10:06:07,051 	Gloss Reference :	A B+C+D+E
2024-02-03 10:06:07,051 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 10:06:07,051 	Gloss Alignment :	         
2024-02-03 10:06:07,051 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 10:06:07,052 	Text Reference  :	*** we  hope    he   continues playing for     india  
2024-02-03 10:06:07,052 	Text Hypothesis :	the two players were both      the     bidding process
2024-02-03 10:06:07,052 	Text Alignment  :	I   S   S       S    S         S       S       S      
2024-02-03 10:06:07,052 ========================================================================================================================
2024-02-03 10:06:07,052 Logging Sequence: 59_152.00
2024-02-03 10:06:07,052 	Gloss Reference :	A B+C+D+E
2024-02-03 10:06:07,052 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 10:06:07,053 	Gloss Alignment :	         
2024-02-03 10:06:07,053 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 10:06:07,054 	Text Reference  :	the   organisers encouraged athletes to  use the  condoms in   their home    countries 
2024-02-03 10:06:07,054 	Text Hypothesis :	after his        world      cups     did not give you     more than  cricket tournament
2024-02-03 10:06:07,054 	Text Alignment  :	S     S          S          S        S   S   S    S       S    S     S       S         
2024-02-03 10:06:07,054 ========================================================================================================================
2024-02-03 10:06:07,054 Logging Sequence: 167_67.00
2024-02-03 10:06:07,054 	Gloss Reference :	A B+C+D+E  
2024-02-03 10:06:07,054 	Gloss Hypothesis:	A B+C+D+E+C
2024-02-03 10:06:07,055 	Gloss Alignment :	  S        
2024-02-03 10:06:07,055 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 10:06:07,056 	Text Reference  :	once infected the ****** incubation period of      the  virus   is   1-2  weeks after which symptoms begin to appear 
2024-02-03 10:06:07,057 	Text Hypothesis :	out  of       the couple announced  on     twitter that nothing july 2023 to    27    july  onwards  one   to qualify
2024-02-03 10:06:07,057 	Text Alignment  :	S    S            I      S          S      S       S    S       S    S    S     S     S     S        S        S      
2024-02-03 10:06:07,057 ========================================================================================================================
2024-02-03 10:06:07,057 Logging Sequence: 77_13.00
2024-02-03 10:06:07,057 	Gloss Reference :	A B+C+D+E
2024-02-03 10:06:07,057 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 10:06:07,057 	Gloss Alignment :	         
2024-02-03 10:06:07,058 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 10:06:07,059 	Text Reference  :	**** this   was the 1st match that ended in      a  tie in    this season's ipl 
2024-02-03 10:06:07,059 	Text Hypothesis :	when sharma was the *** ***** **** ***** captain he was fined rs   12       lakh
2024-02-03 10:06:07,059 	Text Alignment  :	I    S              D   D     D    D     S       S  S   S     S    S        S   
2024-02-03 10:06:07,059 ========================================================================================================================
2024-02-03 10:06:07,059 Logging Sequence: 169_245.00
2024-02-03 10:06:07,059 	Gloss Reference :	A B+C+D+E
2024-02-03 10:06:07,059 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 10:06:07,060 	Gloss Alignment :	         
2024-02-03 10:06:07,060 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 10:06:07,061 	Text Reference  :	**** *** **** ** *** *** * **** mohammed shami has    said these    trolls are  spreading hate  through fake accounts
2024-02-03 10:06:07,061 	Text Hypothesis :	they say that we may get a true glimpse  of    vamika in   february 2022   when she       turns 1       year old     
2024-02-03 10:06:07,061 	Text Alignment  :	I    I   I    I  I   I   I I    S        S     S      S    S        S      S    S         S     S       S    S       
2024-02-03 10:06:07,062 ========================================================================================================================
2024-02-03 10:06:08,670 Epoch 1765: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.86 
2024-02-03 10:06:08,671 EPOCH 1766
2024-02-03 10:06:13,663 Epoch 1766: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.79 
2024-02-03 10:06:13,664 EPOCH 1767
2024-02-03 10:06:18,350 Epoch 1767: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.16 
2024-02-03 10:06:18,351 EPOCH 1768
2024-02-03 10:06:21,877 [Epoch: 1768 Step: 00060100] Batch Recognition Loss:   0.000203 => Gls Tokens per Sec:     1927 || Batch Translation Loss:   0.017652 => Txt Tokens per Sec:     5548 || Lr: 0.000050
2024-02-03 10:06:23,272 Epoch 1768: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.53 
2024-02-03 10:06:23,273 EPOCH 1769
2024-02-03 10:06:27,562 Epoch 1769: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.21 
2024-02-03 10:06:27,563 EPOCH 1770
2024-02-03 10:06:32,402 Epoch 1770: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.08 
2024-02-03 10:06:32,402 EPOCH 1771
2024-02-03 10:06:34,763 [Epoch: 1771 Step: 00060200] Batch Recognition Loss:   0.003608 => Gls Tokens per Sec:     2608 || Batch Translation Loss:   0.012093 => Txt Tokens per Sec:     7308 || Lr: 0.000050
2024-02-03 10:06:36,994 Epoch 1771: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.35 
2024-02-03 10:06:36,994 EPOCH 1772
2024-02-03 10:06:41,578 Epoch 1772: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.61 
2024-02-03 10:06:41,578 EPOCH 1773
2024-02-03 10:06:46,409 Epoch 1773: Total Training Recognition Loss 0.10  Total Training Translation Loss 1.21 
2024-02-03 10:06:46,410 EPOCH 1774
2024-02-03 10:06:48,821 [Epoch: 1774 Step: 00060300] Batch Recognition Loss:   0.000782 => Gls Tokens per Sec:     2389 || Batch Translation Loss:   0.124992 => Txt Tokens per Sec:     6701 || Lr: 0.000050
2024-02-03 10:06:50,809 Epoch 1774: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.63 
2024-02-03 10:06:50,810 EPOCH 1775
2024-02-03 10:06:55,768 Epoch 1775: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.33 
2024-02-03 10:06:55,769 EPOCH 1776
2024-02-03 10:07:00,275 Epoch 1776: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.19 
2024-02-03 10:07:00,276 EPOCH 1777
2024-02-03 10:07:02,065 [Epoch: 1777 Step: 00060400] Batch Recognition Loss:   0.000692 => Gls Tokens per Sec:     2723 || Batch Translation Loss:   0.027575 => Txt Tokens per Sec:     6996 || Lr: 0.000050
2024-02-03 10:07:05,161 Epoch 1777: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.21 
2024-02-03 10:07:05,161 EPOCH 1778
2024-02-03 10:07:09,999 Epoch 1778: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.19 
2024-02-03 10:07:09,999 EPOCH 1779
2024-02-03 10:07:14,516 Epoch 1779: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.03 
2024-02-03 10:07:14,516 EPOCH 1780
2024-02-03 10:07:16,424 [Epoch: 1780 Step: 00060500] Batch Recognition Loss:   0.000121 => Gls Tokens per Sec:     2219 || Batch Translation Loss:   0.020048 => Txt Tokens per Sec:     6043 || Lr: 0.000050
2024-02-03 10:07:19,370 Epoch 1780: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.26 
2024-02-03 10:07:19,371 EPOCH 1781
2024-02-03 10:07:23,781 Epoch 1781: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.99 
2024-02-03 10:07:23,781 EPOCH 1782
2024-02-03 10:07:28,680 Epoch 1782: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.95 
2024-02-03 10:07:28,680 EPOCH 1783
2024-02-03 10:07:29,836 [Epoch: 1783 Step: 00060600] Batch Recognition Loss:   0.000295 => Gls Tokens per Sec:     3327 || Batch Translation Loss:   0.024748 => Txt Tokens per Sec:     8517 || Lr: 0.000050
2024-02-03 10:07:33,128 Epoch 1783: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.97 
2024-02-03 10:07:33,129 EPOCH 1784
2024-02-03 10:07:37,920 Epoch 1784: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.12 
2024-02-03 10:07:37,921 EPOCH 1785
2024-02-03 10:07:42,594 Epoch 1785: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.97 
2024-02-03 10:07:42,595 EPOCH 1786
2024-02-03 10:07:44,073 [Epoch: 1786 Step: 00060700] Batch Recognition Loss:   0.000217 => Gls Tokens per Sec:     2166 || Batch Translation Loss:   0.123380 => Txt Tokens per Sec:     6113 || Lr: 0.000050
2024-02-03 10:07:47,084 Epoch 1786: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.92 
2024-02-03 10:07:47,084 EPOCH 1787
2024-02-03 10:07:51,950 Epoch 1787: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.82 
2024-02-03 10:07:51,951 EPOCH 1788
2024-02-03 10:07:56,432 Epoch 1788: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.78 
2024-02-03 10:07:56,432 EPOCH 1789
2024-02-03 10:07:58,296 [Epoch: 1789 Step: 00060800] Batch Recognition Loss:   0.000368 => Gls Tokens per Sec:     1375 || Batch Translation Loss:   0.028694 => Txt Tokens per Sec:     4514 || Lr: 0.000050
2024-02-03 10:08:01,272 Epoch 1789: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.08 
2024-02-03 10:08:01,272 EPOCH 1790
2024-02-03 10:08:05,796 Epoch 1790: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.29 
2024-02-03 10:08:05,797 EPOCH 1791
2024-02-03 10:08:10,474 Epoch 1791: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.20 
2024-02-03 10:08:10,475 EPOCH 1792
2024-02-03 10:08:11,521 [Epoch: 1792 Step: 00060900] Batch Recognition Loss:   0.000357 => Gls Tokens per Sec:     1837 || Batch Translation Loss:   0.024523 => Txt Tokens per Sec:     5938 || Lr: 0.000050
2024-02-03 10:08:15,330 Epoch 1792: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.12 
2024-02-03 10:08:15,330 EPOCH 1793
2024-02-03 10:08:19,992 Epoch 1793: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.18 
2024-02-03 10:08:19,992 EPOCH 1794
2024-02-03 10:08:24,614 Epoch 1794: Total Training Recognition Loss 0.05  Total Training Translation Loss 7.92 
2024-02-03 10:08:24,614 EPOCH 1795
2024-02-03 10:08:25,332 [Epoch: 1795 Step: 00061000] Batch Recognition Loss:   0.001685 => Gls Tokens per Sec:     1787 || Batch Translation Loss:   2.750838 => Txt Tokens per Sec:     5744 || Lr: 0.000050
2024-02-03 10:08:29,105 Epoch 1795: Total Training Recognition Loss 0.11  Total Training Translation Loss 14.74 
2024-02-03 10:08:29,105 EPOCH 1796
2024-02-03 10:08:33,912 Epoch 1796: Total Training Recognition Loss 0.10  Total Training Translation Loss 3.97 
2024-02-03 10:08:33,913 EPOCH 1797
2024-02-03 10:08:38,449 Epoch 1797: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.30 
2024-02-03 10:08:38,450 EPOCH 1798
2024-02-03 10:08:38,736 [Epoch: 1798 Step: 00061100] Batch Recognition Loss:   0.000702 => Gls Tokens per Sec:     2254 || Batch Translation Loss:   0.030738 => Txt Tokens per Sec:     5345 || Lr: 0.000050
2024-02-03 10:08:43,441 Epoch 1798: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.93 
2024-02-03 10:08:43,441 EPOCH 1799
2024-02-03 10:08:47,932 Epoch 1799: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.78 
2024-02-03 10:08:47,933 EPOCH 1800
2024-02-03 10:08:52,618 [Epoch: 1800 Step: 00061200] Batch Recognition Loss:   0.001474 => Gls Tokens per Sec:     2269 || Batch Translation Loss:   0.020989 => Txt Tokens per Sec:     6311 || Lr: 0.000050
2024-02-03 10:08:52,618 Epoch 1800: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.70 
2024-02-03 10:08:52,619 EPOCH 1801
2024-02-03 10:08:56,968 Epoch 1801: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.69 
2024-02-03 10:08:56,969 EPOCH 1802
2024-02-03 10:09:01,806 Epoch 1802: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.63 
2024-02-03 10:09:01,807 EPOCH 1803
2024-02-03 10:09:06,587 [Epoch: 1803 Step: 00061300] Batch Recognition Loss:   0.000800 => Gls Tokens per Sec:     2143 || Batch Translation Loss:   0.012519 => Txt Tokens per Sec:     6004 || Lr: 0.000050
2024-02-03 10:09:06,814 Epoch 1803: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.63 
2024-02-03 10:09:06,814 EPOCH 1804
2024-02-03 10:09:11,098 Epoch 1804: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.59 
2024-02-03 10:09:11,098 EPOCH 1805
2024-02-03 10:09:15,995 Epoch 1805: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.59 
2024-02-03 10:09:15,996 EPOCH 1806
2024-02-03 10:09:20,038 [Epoch: 1806 Step: 00061400] Batch Recognition Loss:   0.000356 => Gls Tokens per Sec:     2315 || Batch Translation Loss:   0.010770 => Txt Tokens per Sec:     6531 || Lr: 0.000050
2024-02-03 10:09:20,467 Epoch 1806: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.54 
2024-02-03 10:09:20,468 EPOCH 1807
2024-02-03 10:09:25,167 Epoch 1807: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.52 
2024-02-03 10:09:25,167 EPOCH 1808
2024-02-03 10:09:29,849 Epoch 1808: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.50 
2024-02-03 10:09:29,850 EPOCH 1809
2024-02-03 10:09:33,644 [Epoch: 1809 Step: 00061500] Batch Recognition Loss:   0.001162 => Gls Tokens per Sec:     2297 || Batch Translation Loss:   0.018991 => Txt Tokens per Sec:     6442 || Lr: 0.000050
2024-02-03 10:09:34,337 Epoch 1809: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.52 
2024-02-03 10:09:34,337 EPOCH 1810
2024-02-03 10:09:39,170 Epoch 1810: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.52 
2024-02-03 10:09:39,171 EPOCH 1811
2024-02-03 10:09:43,492 Epoch 1811: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.54 
2024-02-03 10:09:43,492 EPOCH 1812
2024-02-03 10:09:46,790 [Epoch: 1812 Step: 00061600] Batch Recognition Loss:   0.001900 => Gls Tokens per Sec:     2524 || Batch Translation Loss:   0.038882 => Txt Tokens per Sec:     7007 || Lr: 0.000050
2024-02-03 10:09:48,025 Epoch 1812: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.52 
2024-02-03 10:09:48,025 EPOCH 1813
2024-02-03 10:09:52,661 Epoch 1813: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.59 
2024-02-03 10:09:52,661 EPOCH 1814
2024-02-03 10:09:57,417 Epoch 1814: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.57 
2024-02-03 10:09:57,418 EPOCH 1815
2024-02-03 10:10:00,275 [Epoch: 1815 Step: 00061700] Batch Recognition Loss:   0.000456 => Gls Tokens per Sec:     2602 || Batch Translation Loss:   0.018861 => Txt Tokens per Sec:     7148 || Lr: 0.000050
2024-02-03 10:10:01,798 Epoch 1815: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.70 
2024-02-03 10:10:01,798 EPOCH 1816
2024-02-03 10:10:06,753 Epoch 1816: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.59 
2024-02-03 10:10:06,754 EPOCH 1817
2024-02-03 10:10:11,210 Epoch 1817: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.51 
2024-02-03 10:10:11,211 EPOCH 1818
2024-02-03 10:10:14,298 [Epoch: 1818 Step: 00061800] Batch Recognition Loss:   0.000461 => Gls Tokens per Sec:     2200 || Batch Translation Loss:   0.039893 => Txt Tokens per Sec:     6111 || Lr: 0.000050
2024-02-03 10:10:16,031 Epoch 1818: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.56 
2024-02-03 10:10:16,031 EPOCH 1819
2024-02-03 10:10:20,083 Epoch 1819: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.52 
2024-02-03 10:10:20,084 EPOCH 1820
2024-02-03 10:10:24,867 Epoch 1820: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.53 
2024-02-03 10:10:24,868 EPOCH 1821
2024-02-03 10:10:27,235 [Epoch: 1821 Step: 00061900] Batch Recognition Loss:   0.000145 => Gls Tokens per Sec:     2599 || Batch Translation Loss:   0.012314 => Txt Tokens per Sec:     7273 || Lr: 0.000050
2024-02-03 10:10:29,426 Epoch 1821: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.59 
2024-02-03 10:10:29,427 EPOCH 1822
2024-02-03 10:10:34,498 Epoch 1822: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.01 
2024-02-03 10:10:34,499 EPOCH 1823
2024-02-03 10:10:39,066 Epoch 1823: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.82 
2024-02-03 10:10:39,067 EPOCH 1824
2024-02-03 10:10:41,335 [Epoch: 1824 Step: 00062000] Batch Recognition Loss:   0.000321 => Gls Tokens per Sec:     2430 || Batch Translation Loss:   0.010234 => Txt Tokens per Sec:     6606 || Lr: 0.000050
2024-02-03 10:10:49,966 Validation result at epoch 1824, step    62000: duration: 8.6306s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.07792	Translation Loss: 90773.40625	PPL: 12432.43848
	Eval Metric: BLEU
	WER 2.97	(DEL: 0.00,	INS: 0.00,	SUB: 2.97)
	BLEU-4 0.63	(BLEU-1: 10.09,	BLEU-2: 2.87,	BLEU-3: 1.15,	BLEU-4: 0.63)
	CHRF 16.60	ROUGE 8.35
2024-02-03 10:10:49,967 Logging Recognition and Translation Outputs
2024-02-03 10:10:49,967 ========================================================================================================================
2024-02-03 10:10:49,968 Logging Sequence: 162_86.00
2024-02-03 10:10:49,968 	Gloss Reference :	A B+C+D+E  
2024-02-03 10:10:49,968 	Gloss Hypothesis:	A B+C+D+E+B
2024-02-03 10:10:49,968 	Gloss Alignment :	  S        
2024-02-03 10:10:49,968 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 10:10:49,969 	Text Reference  :	do you know how   people **** ** **** ***** ********* **** *** **** ***** ** responded to **** *** ***** ****** ** ***** this   
2024-02-03 10:10:49,970 	Text Hypothesis :	** in  the  1900s people used to even drink champagne from the high heels of women     to show off their wealth at fancy parties
2024-02-03 10:10:49,970 	Text Alignment  :	D  S   S    S            I    I  I    I     I         I    I   I    I     I  S            I    I   I     I      I  I     S      
2024-02-03 10:10:49,970 ========================================================================================================================
2024-02-03 10:10:49,970 Logging Sequence: 98_113.00
2024-02-03 10:10:49,970 	Gloss Reference :	A B+C+D+E
2024-02-03 10:10:49,970 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 10:10:49,971 	Gloss Alignment :	         
2024-02-03 10:10:49,971 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 10:10:49,971 	Text Reference  :	however australia legends    opted out due     to   the covid situation
2024-02-03 10:10:49,972 	Text Hypothesis :	if      the       tournament from  8th october 2023 t0  11th  april    
2024-02-03 10:10:49,972 	Text Alignment  :	S       S         S          S     S   S       S    S   S     S        
2024-02-03 10:10:49,972 ========================================================================================================================
2024-02-03 10:10:49,972 Logging Sequence: 178_25.00
2024-02-03 10:10:49,972 	Gloss Reference :	A B+C+D+E
2024-02-03 10:10:49,972 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 10:10:49,972 	Gloss Alignment :	         
2024-02-03 10:10:49,973 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 10:10:49,973 	Text Reference  :	on  5th   may the ********** *** police filed   an  fir  
2024-02-03 10:10:49,973 	Text Hypothesis :	the final of  the tournament was played between sri lanka
2024-02-03 10:10:49,973 	Text Alignment  :	S   S     S       I          I   S      S       S   S    
2024-02-03 10:10:49,974 ========================================================================================================================
2024-02-03 10:10:49,974 Logging Sequence: 147_148.00
2024-02-03 10:10:49,974 	Gloss Reference :	A B+C+D+E
2024-02-03 10:10:49,974 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 10:10:49,974 	Gloss Alignment :	         
2024-02-03 10:10:49,974 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 10:10:49,975 	Text Reference  :	*** when   explained this    to my       teammates they understood my       feelings
2024-02-03 10:10:49,975 	Text Hypothesis :	the people were      shocked by decision but       it   will       continue playing 
2024-02-03 10:10:49,975 	Text Alignment  :	I   S      S         S       S  S        S         S    S          S        S       
2024-02-03 10:10:49,975 ========================================================================================================================
2024-02-03 10:10:49,976 Logging Sequence: 61_326.00
2024-02-03 10:10:49,976 	Gloss Reference :	A B+C+D+E
2024-02-03 10:10:49,976 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 10:10:49,976 	Gloss Alignment :	         
2024-02-03 10:10:49,976 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 10:10:49,977 	Text Reference  :	**** ******* **** ********* *** **** *** however no   confirmation has     been made    yet     
2024-02-03 10:10:49,977 	Text Hypothesis :	they thanked bcci secretary jay shah for his     care and          concern he   thanked everyone
2024-02-03 10:10:49,977 	Text Alignment  :	I    I       I    I         I   I    I   S       S    S            S       S    S       S       
2024-02-03 10:10:49,977 ========================================================================================================================
2024-02-03 10:10:49,981 Training ended since there were no improvements inthe last learning rate step: 0.000050
2024-02-03 10:10:49,982 Best validation result at step    10000:   0.91 eval_metric.
2024-02-03 10:11:17,260 ------------------------------------------------------------
2024-02-03 10:11:17,261 [DEV] partition [RECOGNITION] experiment [BW]: 1
2024-02-03 10:11:26,218 finished in 8.9574s 
2024-02-03 10:11:26,218 ************************************************************
2024-02-03 10:11:26,218 [DEV] partition [RECOGNITION] results:
	New Best CTC Decode Beam Size: 1
	WER 5.66	(DEL: 0.07,	INS: 0.00,	SUB: 5.59)
2024-02-03 10:11:26,218 ************************************************************
2024-02-03 10:11:26,219 ------------------------------------------------------------
2024-02-03 10:11:26,219 [DEV] partition [RECOGNITION] experiment [BW]: 2
2024-02-03 10:11:34,743 finished in 8.5242s 
2024-02-03 10:11:34,744 ************************************************************
2024-02-03 10:11:34,744 [DEV] partition [RECOGNITION] results:
	New Best CTC Decode Beam Size: 2
	WER 5.59	(DEL: 0.07,	INS: 0.00,	SUB: 5.52)
2024-02-03 10:11:34,744 ************************************************************
2024-02-03 10:11:34,744 ------------------------------------------------------------
2024-02-03 10:11:34,744 [DEV] partition [RECOGNITION] experiment [BW]: 3
2024-02-03 10:11:43,333 finished in 8.5888s 
2024-02-03 10:11:43,334 ************************************************************
2024-02-03 10:11:43,334 [DEV] partition [RECOGNITION] results:
	New Best CTC Decode Beam Size: 3
	WER 5.52	(DEL: 0.07,	INS: 0.00,	SUB: 5.45)
2024-02-03 10:11:43,334 ************************************************************
2024-02-03 10:11:43,334 ------------------------------------------------------------
2024-02-03 10:11:43,334 [DEV] partition [RECOGNITION] experiment [BW]: 4
2024-02-03 10:11:51,913 finished in 8.5786s 
2024-02-03 10:11:51,913 ------------------------------------------------------------
2024-02-03 10:11:51,913 [DEV] partition [RECOGNITION] experiment [BW]: 5
2024-02-03 10:12:00,456 finished in 8.5437s 
2024-02-03 10:12:00,457 ------------------------------------------------------------
2024-02-03 10:12:00,457 [DEV] partition [RECOGNITION] experiment [BW]: 6
2024-02-03 10:12:09,206 finished in 8.7486s 
2024-02-03 10:12:09,207 ------------------------------------------------------------
2024-02-03 10:12:09,207 [DEV] partition [RECOGNITION] experiment [BW]: 7
2024-02-03 10:12:17,870 finished in 8.6635s 
2024-02-03 10:12:17,871 ------------------------------------------------------------
2024-02-03 10:12:17,871 [DEV] partition [RECOGNITION] experiment [BW]: 8
2024-02-03 10:12:26,640 finished in 8.7684s 
2024-02-03 10:12:26,641 ------------------------------------------------------------
2024-02-03 10:12:26,641 [DEV] partition [RECOGNITION] experiment [BW]: 9
2024-02-03 10:12:35,067 finished in 8.4263s 
2024-02-03 10:12:35,068 ------------------------------------------------------------
2024-02-03 10:12:35,068 [DEV] partition [RECOGNITION] experiment [BW]: 10
2024-02-03 10:12:43,599 finished in 8.5313s 
2024-02-03 10:12:43,600 ============================================================
2024-02-03 10:12:52,000 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 1 and Alpha: -1
	BLEU-4 0.91	(BLEU-1: 12.11,	BLEU-2: 3.82,	BLEU-3: 1.67,	BLEU-4: 0.91)
	CHRF 17.62	ROUGE 9.94
2024-02-03 10:12:52,001 ------------------------------------------------------------
2024-02-03 10:30:15,278 ************************************************************
2024-02-03 10:30:15,278 [DEV] partition [Recognition & Translation] results:
	Best CTC Decode Beam Size: 3
	Best Translation Beam Size: 1 and Alpha: -1
	WER 5.52	(DEL: 0.07,	INS: 0.00,	SUB: 5.45)
	BLEU-4 0.91	(BLEU-1: 12.11,	BLEU-2: 3.82,	BLEU-3: 1.67,	BLEU-4: 0.91)
	CHRF 17.62	ROUGE 9.94
2024-02-03 10:30:15,279 ************************************************************
2024-02-03 10:30:23,730 [TEST] partition [Recognition & Translation] results:
	Best CTC Decode Beam Size: 3
	Best Translation Beam Size: 1 and Alpha: -1
	WER 5.72	(DEL: 0.28,	INS: 0.00,	SUB: 5.44)
	BLEU-4 0.60	(BLEU-1: 11.10,	BLEU-2: 3.47,	BLEU-3: 1.32,	BLEU-4: 0.60)
	CHRF 17.09	ROUGE 9.62
2024-02-03 10:30:23,730 ************************************************************
