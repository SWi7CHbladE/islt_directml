2024-02-11 18:19:59,441 Hello! This is Joey-NMT.
2024-02-11 18:19:59,449 Total params: 25640456
2024-02-11 18:19:59,449 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'gloss_output_layer.bias', 'gloss_output_layer.weight', 'sgn_embed.ln.bias', 'sgn_embed.ln.weight', 'sgn_embed.norm.norm.bias', 'sgn_embed.norm.norm.weight', 'txt_embed.norm.norm.bias', 'txt_embed.norm.norm.weight']
2024-02-11 18:20:00,463 cfg.name                           : sign_experiment
2024-02-11 18:20:00,464 cfg.data.data_path                 : ./data/Sports_dataset/6/
2024-02-11 18:20:00,464 cfg.data.version                   : phoenix_2014_trans
2024-02-11 18:20:00,464 cfg.data.sgn                       : sign
2024-02-11 18:20:00,464 cfg.data.txt                       : text
2024-02-11 18:20:00,464 cfg.data.gls                       : gloss
2024-02-11 18:20:00,464 cfg.data.train                     : excel_data.train
2024-02-11 18:20:00,465 cfg.data.dev                       : excel_data.dev
2024-02-11 18:20:00,465 cfg.data.test                      : excel_data.test
2024-02-11 18:20:00,465 cfg.data.feature_size              : 2560
2024-02-11 18:20:00,465 cfg.data.level                     : word
2024-02-11 18:20:00,465 cfg.data.txt_lowercase             : True
2024-02-11 18:20:00,465 cfg.data.max_sent_length           : 500
2024-02-11 18:20:00,465 cfg.data.random_train_subset       : -1
2024-02-11 18:20:00,465 cfg.data.random_dev_subset         : -1
2024-02-11 18:20:00,466 cfg.testing.recognition_beam_sizes : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
2024-02-11 18:20:00,466 cfg.testing.translation_beam_sizes : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
2024-02-11 18:20:00,466 cfg.testing.translation_beam_alphas : [-1, 0, 1, 2, 3, 4, 5]
2024-02-11 18:20:00,466 cfg.training.reset_best_ckpt       : False
2024-02-11 18:20:00,466 cfg.training.reset_scheduler       : False
2024-02-11 18:20:00,466 cfg.training.reset_optimizer       : False
2024-02-11 18:20:00,466 cfg.training.random_seed           : 42
2024-02-11 18:20:00,467 cfg.training.model_dir             : ./sign_sample_model/fold6/32head/16batch
2024-02-11 18:20:00,467 cfg.training.recognition_loss_weight : 1.0
2024-02-11 18:20:00,467 cfg.training.translation_loss_weight : 1.0
2024-02-11 18:20:00,467 cfg.training.eval_metric           : bleu
2024-02-11 18:20:00,467 cfg.training.optimizer             : adam
2024-02-11 18:20:00,467 cfg.training.learning_rate         : 0.0001
2024-02-11 18:20:00,467 cfg.training.batch_size            : 16
2024-02-11 18:20:00,467 cfg.training.num_valid_log         : 5
2024-02-11 18:20:00,468 cfg.training.epochs                : 50000
2024-02-11 18:20:00,468 cfg.training.early_stopping_metric : eval_metric
2024-02-11 18:20:00,468 cfg.training.batch_type            : sentence
2024-02-11 18:20:00,468 cfg.training.translation_normalization : batch
2024-02-11 18:20:00,468 cfg.training.eval_recognition_beam_size : 1
2024-02-11 18:20:00,468 cfg.training.eval_translation_beam_size : 1
2024-02-11 18:20:00,468 cfg.training.eval_translation_beam_alpha : -1
2024-02-11 18:20:00,468 cfg.training.overwrite             : True
2024-02-11 18:20:00,469 cfg.training.shuffle               : True
2024-02-11 18:20:00,469 cfg.training.use_cuda              : True
2024-02-11 18:20:00,469 cfg.training.translation_max_output_length : 40
2024-02-11 18:20:00,469 cfg.training.keep_last_ckpts       : 1
2024-02-11 18:20:00,469 cfg.training.batch_multiplier      : 1
2024-02-11 18:20:00,469 cfg.training.logging_freq          : 100
2024-02-11 18:20:00,469 cfg.training.validation_freq       : 2000
2024-02-11 18:20:00,469 cfg.training.betas                 : [0.9, 0.998]
2024-02-11 18:20:00,470 cfg.training.scheduling            : plateau
2024-02-11 18:20:00,470 cfg.training.learning_rate_min     : 1e-08
2024-02-11 18:20:00,470 cfg.training.weight_decay          : 0.0001
2024-02-11 18:20:00,470 cfg.training.patience              : 12
2024-02-11 18:20:00,470 cfg.training.decrease_factor       : 0.5
2024-02-11 18:20:00,470 cfg.training.label_smoothing       : 0.0
2024-02-11 18:20:00,470 cfg.model.initializer              : xavier
2024-02-11 18:20:00,470 cfg.model.bias_initializer         : zeros
2024-02-11 18:20:00,470 cfg.model.init_gain                : 1.0
2024-02-11 18:20:00,471 cfg.model.embed_initializer        : xavier
2024-02-11 18:20:00,471 cfg.model.embed_init_gain          : 1.0
2024-02-11 18:20:00,471 cfg.model.tied_softmax             : True
2024-02-11 18:20:00,471 cfg.model.encoder.type             : transformer
2024-02-11 18:20:00,471 cfg.model.encoder.num_layers       : 3
2024-02-11 18:20:00,471 cfg.model.encoder.num_heads        : 32
2024-02-11 18:20:00,471 cfg.model.encoder.embeddings.embedding_dim : 512
2024-02-11 18:20:00,471 cfg.model.encoder.embeddings.scale : False
2024-02-11 18:20:00,472 cfg.model.encoder.embeddings.dropout : 0.1
2024-02-11 18:20:00,472 cfg.model.encoder.embeddings.norm_type : batch
2024-02-11 18:20:00,472 cfg.model.encoder.embeddings.activation_type : softsign
2024-02-11 18:20:00,472 cfg.model.encoder.hidden_size      : 512
2024-02-11 18:20:00,472 cfg.model.encoder.ff_size          : 2048
2024-02-11 18:20:00,472 cfg.model.encoder.dropout          : 0.1
2024-02-11 18:20:00,472 cfg.model.decoder.type             : transformer
2024-02-11 18:20:00,472 cfg.model.decoder.num_layers       : 3
2024-02-11 18:20:00,472 cfg.model.decoder.num_heads        : 32
2024-02-11 18:20:00,473 cfg.model.decoder.embeddings.embedding_dim : 512
2024-02-11 18:20:00,473 cfg.model.decoder.embeddings.scale : False
2024-02-11 18:20:00,473 cfg.model.decoder.embeddings.dropout : 0.1
2024-02-11 18:20:00,473 cfg.model.decoder.embeddings.norm_type : batch
2024-02-11 18:20:00,473 cfg.model.decoder.embeddings.activation_type : softsign
2024-02-11 18:20:00,473 cfg.model.decoder.hidden_size      : 512
2024-02-11 18:20:00,473 cfg.model.decoder.ff_size          : 2048
2024-02-11 18:20:00,473 cfg.model.decoder.dropout          : 0.1
2024-02-11 18:20:00,474 Data set sizes: 
	train 2126,
	valid 707,
	test 707
2024-02-11 18:20:00,474 First training example:
	[GLS] A B C D E
	[TXT] you all know that daley is a strong diver but what i am about to tell you will surprise you
2024-02-11 18:20:00,474 First 10 words (gls): (0) <si> (1) <unk> (2) <pad> (3) A (4) B (5) C (6) D (7) E
2024-02-11 18:20:00,474 First 10 words (txt): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) the (5) to (6) and (7) in (8) a (9) of
2024-02-11 18:20:00,474 Number of unique glosses (types): 8
2024-02-11 18:20:00,474 Number of unique words (types): 4398
2024-02-11 18:20:00,474 SignModel(
	encoder=TransformerEncoder(num_layers=3, num_heads=32),
	decoder=TransformerDecoder(num_layers=3, num_heads=32),
	sgn_embed=SpatialEmbeddings(embedding_dim=512, input_size=2560),
	txt_embed=Embeddings(embedding_dim=512, vocab_size=4398))
2024-02-11 18:20:00,478 EPOCH 1
2024-02-11 18:20:06,871 [Epoch: 001 Step: 00000100] Batch Recognition Loss:   0.062682 => Gls Tokens per Sec:     1252 || Batch Translation Loss:  79.442139 => Txt Tokens per Sec:     3447 || Lr: 0.000100
2024-02-11 18:20:09,020 Epoch   1: Total Training Recognition Loss 157.48  Total Training Translation Loss 12520.26 
2024-02-11 18:20:09,020 EPOCH 2
2024-02-11 18:20:12,827 [Epoch: 002 Step: 00000200] Batch Recognition Loss:   0.006979 => Gls Tokens per Sec:     1409 || Batch Translation Loss:  80.057198 => Txt Tokens per Sec:     3926 || Lr: 0.000100
2024-02-11 18:20:16,728 Epoch   2: Total Training Recognition Loss 1.76  Total Training Translation Loss 11627.52 
2024-02-11 18:20:16,728 EPOCH 3
2024-02-11 18:20:18,555 [Epoch: 003 Step: 00000300] Batch Recognition Loss:   0.006476 => Gls Tokens per Sec:     1490 || Batch Translation Loss:  60.977798 => Txt Tokens per Sec:     4167 || Lr: 0.000100
2024-02-11 18:20:24,375 Epoch   3: Total Training Recognition Loss 1.00  Total Training Translation Loss 11085.67 
2024-02-11 18:20:24,376 EPOCH 4
2024-02-11 18:20:24,433 [Epoch: 004 Step: 00000400] Batch Recognition Loss:   0.004072 => Gls Tokens per Sec:     1429 || Batch Translation Loss: 103.150368 => Txt Tokens per Sec:     5107 || Lr: 0.000100
2024-02-11 18:20:29,767 [Epoch: 004 Step: 00000500] Batch Recognition Loss:   0.005029 => Gls Tokens per Sec:     1500 || Batch Translation Loss:  66.310791 => Txt Tokens per Sec:     4181 || Lr: 0.000100
2024-02-11 18:20:31,398 Epoch   4: Total Training Recognition Loss 0.88  Total Training Translation Loss 10544.93 
2024-02-11 18:20:31,398 EPOCH 5
2024-02-11 18:20:35,585 [Epoch: 005 Step: 00000600] Batch Recognition Loss:   0.002854 => Gls Tokens per Sec:     1300 || Batch Translation Loss:  71.855461 => Txt Tokens per Sec:     3660 || Lr: 0.000100
2024-02-11 18:20:39,376 Epoch   5: Total Training Recognition Loss 0.82  Total Training Translation Loss 10027.16 
2024-02-11 18:20:39,376 EPOCH 6
2024-02-11 18:20:41,220 [Epoch: 006 Step: 00000700] Batch Recognition Loss:   0.003277 => Gls Tokens per Sec:     1519 || Batch Translation Loss:  63.510586 => Txt Tokens per Sec:     4196 || Lr: 0.000100
2024-02-11 18:20:46,854 Epoch   6: Total Training Recognition Loss 0.81  Total Training Translation Loss 9539.62 
2024-02-11 18:20:46,854 EPOCH 7
2024-02-11 18:20:46,973 [Epoch: 007 Step: 00000800] Batch Recognition Loss:   0.005040 => Gls Tokens per Sec:     1356 || Batch Translation Loss:  50.546871 => Txt Tokens per Sec:     3619 || Lr: 0.000100
2024-02-11 18:20:52,684 [Epoch: 007 Step: 00000900] Batch Recognition Loss:   0.003477 => Gls Tokens per Sec:     1401 || Batch Translation Loss:  72.391380 => Txt Tokens per Sec:     3913 || Lr: 0.000100
2024-02-11 18:20:54,412 Epoch   7: Total Training Recognition Loss 0.79  Total Training Translation Loss 9053.51 
2024-02-11 18:20:54,412 EPOCH 8
2024-02-11 18:20:57,777 [Epoch: 008 Step: 00001000] Batch Recognition Loss:   0.004734 => Gls Tokens per Sec:     1641 || Batch Translation Loss:  48.725399 => Txt Tokens per Sec:     4334 || Lr: 0.000100
2024-02-11 18:21:01,831 Epoch   8: Total Training Recognition Loss 0.97  Total Training Translation Loss 8605.90 
2024-02-11 18:21:01,832 EPOCH 9
2024-02-11 18:21:03,749 [Epoch: 009 Step: 00001100] Batch Recognition Loss:   0.003087 => Gls Tokens per Sec:     1503 || Batch Translation Loss:  50.596661 => Txt Tokens per Sec:     4068 || Lr: 0.000100
2024-02-11 18:21:09,373 Epoch   9: Total Training Recognition Loss 0.99  Total Training Translation Loss 8157.76 
2024-02-11 18:21:09,373 EPOCH 10
2024-02-11 18:21:09,532 [Epoch: 010 Step: 00001200] Batch Recognition Loss:   0.005503 => Gls Tokens per Sec:     1529 || Batch Translation Loss:  43.041714 => Txt Tokens per Sec:     4357 || Lr: 0.000100
2024-02-11 18:21:14,977 [Epoch: 010 Step: 00001300] Batch Recognition Loss:   0.005493 => Gls Tokens per Sec:     1469 || Batch Translation Loss:  70.005005 => Txt Tokens per Sec:     4054 || Lr: 0.000100
2024-02-11 18:21:16,717 Epoch  10: Total Training Recognition Loss 1.18  Total Training Translation Loss 7723.85 
2024-02-11 18:21:16,717 EPOCH 11
2024-02-11 18:21:20,570 [Epoch: 011 Step: 00001400] Batch Recognition Loss:   0.005303 => Gls Tokens per Sec:     1454 || Batch Translation Loss:  55.442818 => Txt Tokens per Sec:     3967 || Lr: 0.000100
2024-02-11 18:21:24,275 Epoch  11: Total Training Recognition Loss 1.29  Total Training Translation Loss 7281.47 
2024-02-11 18:21:24,276 EPOCH 12
2024-02-11 18:21:26,521 [Epoch: 012 Step: 00001500] Batch Recognition Loss:   0.013035 => Gls Tokens per Sec:     1318 || Batch Translation Loss:  49.334011 => Txt Tokens per Sec:     3649 || Lr: 0.000100
2024-02-11 18:21:32,166 Epoch  12: Total Training Recognition Loss 1.27  Total Training Translation Loss 6874.68 
2024-02-11 18:21:32,166 EPOCH 13
2024-02-11 18:21:32,375 [Epoch: 013 Step: 00001600] Batch Recognition Loss:   0.010280 => Gls Tokens per Sec:     1538 || Batch Translation Loss:  45.695431 => Txt Tokens per Sec:     4168 || Lr: 0.000100
2024-02-11 18:21:38,068 [Epoch: 013 Step: 00001700] Batch Recognition Loss:   0.005628 => Gls Tokens per Sec:     1405 || Batch Translation Loss:  41.905724 => Txt Tokens per Sec:     3917 || Lr: 0.000100
2024-02-11 18:21:39,852 Epoch  13: Total Training Recognition Loss 1.49  Total Training Translation Loss 6451.07 
2024-02-11 18:21:39,853 EPOCH 14
2024-02-11 18:21:43,834 [Epoch: 014 Step: 00001800] Batch Recognition Loss:   0.008944 => Gls Tokens per Sec:     1427 || Batch Translation Loss:  32.539986 => Txt Tokens per Sec:     3966 || Lr: 0.000100
2024-02-11 18:21:47,611 Epoch  14: Total Training Recognition Loss 1.52  Total Training Translation Loss 6049.69 
2024-02-11 18:21:47,612 EPOCH 15
2024-02-11 18:21:49,728 [Epoch: 015 Step: 00001900] Batch Recognition Loss:   0.006808 => Gls Tokens per Sec:     1437 || Batch Translation Loss:  36.110268 => Txt Tokens per Sec:     4017 || Lr: 0.000100
2024-02-11 18:21:55,246 Epoch  15: Total Training Recognition Loss 1.55  Total Training Translation Loss 5655.12 
2024-02-11 18:21:55,246 EPOCH 16
2024-02-11 18:21:55,508 [Epoch: 016 Step: 00002000] Batch Recognition Loss:   0.007185 => Gls Tokens per Sec:     1533 || Batch Translation Loss:  27.858601 => Txt Tokens per Sec:     4410 || Lr: 0.000100
2024-02-11 18:22:07,053 Hooray! New best validation result [eval_metric]!
2024-02-11 18:22:07,054 Saving new checkpoint.
2024-02-11 18:22:07,304 Validation result at epoch  16, step     2000: duration: 11.7960s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.11889	Translation Loss: 58512.32422	PPL: 368.38995
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.79	(BLEU-1: 13.50,	BLEU-2: 4.97,	BLEU-3: 1.92,	BLEU-4: 0.79)
	CHRF 16.29	ROUGE 12.03
2024-02-11 18:22:07,305 Logging Recognition and Translation Outputs
2024-02-11 18:22:07,305 ========================================================================================================================
2024-02-11 18:22:07,305 Logging Sequence: 148_130.00
2024-02-11 18:22:07,305 	Gloss Reference :	A B+C+D+E
2024-02-11 18:22:07,306 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:22:07,306 	Gloss Alignment :	         
2024-02-11 18:22:07,306 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:22:07,307 	Text Reference  :	india has now taken it's revenge by dismissing the sri lankan team in    50  runs     what a   shame
2024-02-11 18:22:07,307 	Text Hypothesis :	india *** *** ***** **** ******* ** won        the *** ****** **** match and pakistan won  the match
2024-02-11 18:22:07,307 	Text Alignment  :	      D   D   D     D    D       D  S              D   D      D    S     S   S        S    S   S    
2024-02-11 18:22:07,307 ========================================================================================================================
2024-02-11 18:22:07,307 Logging Sequence: 57_204.00
2024-02-11 18:22:07,308 	Gloss Reference :	A B+C+D+E
2024-02-11 18:22:07,308 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:22:07,308 	Gloss Alignment :	         
2024-02-11 18:22:07,308 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:22:07,309 	Text Reference  :	******* surprisingly india had scored 3562 and pakistan had     scored just 128 runs 
2024-02-11 18:22:07,309 	Text Hypothesis :	however the          match was out    of   the match    between india  and  sri lanka
2024-02-11 18:22:07,309 	Text Alignment  :	I       S            S     S   S      S    S   S        S       S      S    S   S    
2024-02-11 18:22:07,309 ========================================================================================================================
2024-02-11 18:22:07,309 Logging Sequence: 130_18.00
2024-02-11 18:22:07,310 	Gloss Reference :	A B+C+D+E
2024-02-11 18:22:07,310 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:22:07,310 	Gloss Alignment :	         
2024-02-11 18:22:07,310 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:22:07,311 	Text Reference  :	when he was only 14 years old he   participated in the ******** ** 2008 olympic games in beijing china   
2024-02-11 18:22:07,312 	Text Hypothesis :	**** he *** **** ** is    a   gold medal        in the olympics is a    gold    medal in the     olympics
2024-02-11 18:22:07,312 	Text Alignment  :	D       D   D    D  S     S   S    S                   I        I  S    S       S        S       S       
2024-02-11 18:22:07,312 ========================================================================================================================
2024-02-11 18:22:07,312 Logging Sequence: 153_43.00
2024-02-11 18:22:07,312 	Gloss Reference :	A B+C+D+E
2024-02-11 18:22:07,312 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:22:07,312 	Gloss Alignment :	         
2024-02-11 18:22:07,312 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:22:07,314 	Text Reference  :	these runs  were **** ** ** all   because of    hardik  pandya and ******** **** **** ** virat kohli
2024-02-11 18:22:07,314 	Text Hypothesis :	the   match were held in 20 overs the     match between india  and pakistan were held in 20    overs
2024-02-11 18:22:07,314 	Text Alignment  :	S     S          I    I  I  S     S       S     S       S          I        I    I    I  S     S    
2024-02-11 18:22:07,314 ========================================================================================================================
2024-02-11 18:22:07,314 Logging Sequence: 102_197.00
2024-02-11 18:22:07,314 	Gloss Reference :	A B+C+D+E
2024-02-11 18:22:07,314 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:22:07,315 	Gloss Alignment :	         
2024-02-11 18:22:07,315 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:22:07,317 	Text Reference  :	in reply smriti mandhana scored 63 runs off 42 balls she   led the ***** chase as      and      secured  a        victory for   indian   in       just 114 overs
2024-02-11 18:22:07,317 	Text Hypothesis :	** ***** ****** ******** ****** ** **** *** ** ***** india won the first time  against pakistan pakistan pakistan in      india pakistan pakistan won  the match
2024-02-11 18:22:07,317 	Text Alignment  :	D  D     D      D        D      D  D    D   D  D     S     S       I     S     S       S        S        S        S       S     S        S        S    S   S    
2024-02-11 18:22:07,317 ========================================================================================================================
2024-02-11 18:22:13,046 [Epoch: 016 Step: 00002100] Batch Recognition Loss:   0.013468 => Gls Tokens per Sec:     1393 || Batch Translation Loss:  39.265022 => Txt Tokens per Sec:     3909 || Lr: 0.000100
2024-02-11 18:22:14,672 Epoch  16: Total Training Recognition Loss 1.57  Total Training Translation Loss 5288.02 
2024-02-11 18:22:14,673 EPOCH 17
2024-02-11 18:22:18,963 [Epoch: 017 Step: 00002200] Batch Recognition Loss:   0.011961 => Gls Tokens per Sec:     1343 || Batch Translation Loss:  30.770916 => Txt Tokens per Sec:     3747 || Lr: 0.000100
2024-02-11 18:22:22,588 Epoch  17: Total Training Recognition Loss 1.61  Total Training Translation Loss 4934.62 
2024-02-11 18:22:22,588 EPOCH 18
2024-02-11 18:22:24,658 [Epoch: 018 Step: 00002300] Batch Recognition Loss:   0.007609 => Gls Tokens per Sec:     1508 || Batch Translation Loss:  33.894478 => Txt Tokens per Sec:     4256 || Lr: 0.000100
2024-02-11 18:22:29,991 Epoch  18: Total Training Recognition Loss 1.47  Total Training Translation Loss 4547.93 
2024-02-11 18:22:29,991 EPOCH 19
2024-02-11 18:22:30,298 [Epoch: 019 Step: 00002400] Batch Recognition Loss:   0.015053 => Gls Tokens per Sec:     1579 || Batch Translation Loss:  36.586082 => Txt Tokens per Sec:     4132 || Lr: 0.000100
2024-02-11 18:22:35,872 [Epoch: 019 Step: 00002500] Batch Recognition Loss:   0.009543 => Gls Tokens per Sec:     1435 || Batch Translation Loss:  41.188271 => Txt Tokens per Sec:     4023 || Lr: 0.000100
2024-02-11 18:22:37,330 Epoch  19: Total Training Recognition Loss 1.56  Total Training Translation Loss 4174.56 
2024-02-11 18:22:37,330 EPOCH 20
2024-02-11 18:22:41,093 [Epoch: 020 Step: 00002600] Batch Recognition Loss:   0.012477 => Gls Tokens per Sec:     1552 || Batch Translation Loss:  37.245918 => Txt Tokens per Sec:     4294 || Lr: 0.000100
2024-02-11 18:22:44,730 Epoch  20: Total Training Recognition Loss 1.44  Total Training Translation Loss 3844.92 
2024-02-11 18:22:44,731 EPOCH 21
2024-02-11 18:22:47,232 [Epoch: 021 Step: 00002700] Batch Recognition Loss:   0.006231 => Gls Tokens per Sec:     1280 || Batch Translation Loss:  22.974596 => Txt Tokens per Sec:     3673 || Lr: 0.000100
2024-02-11 18:22:52,210 Epoch  21: Total Training Recognition Loss 1.38  Total Training Translation Loss 3496.57 
2024-02-11 18:22:52,210 EPOCH 22
2024-02-11 18:22:52,532 [Epoch: 022 Step: 00002800] Batch Recognition Loss:   0.009713 => Gls Tokens per Sec:     1739 || Batch Translation Loss:  18.748487 => Txt Tokens per Sec:     4609 || Lr: 0.000100
2024-02-11 18:22:58,111 [Epoch: 022 Step: 00002900] Batch Recognition Loss:   0.008208 => Gls Tokens per Sec:     1432 || Batch Translation Loss:  17.357553 => Txt Tokens per Sec:     3999 || Lr: 0.000100
2024-02-11 18:22:59,580 Epoch  22: Total Training Recognition Loss 1.42  Total Training Translation Loss 3188.12 
2024-02-11 18:22:59,580 EPOCH 23
2024-02-11 18:23:03,931 [Epoch: 023 Step: 00003000] Batch Recognition Loss:   0.009254 => Gls Tokens per Sec:     1361 || Batch Translation Loss:   5.871903 => Txt Tokens per Sec:     3797 || Lr: 0.000100
2024-02-11 18:23:07,254 Epoch  23: Total Training Recognition Loss 1.35  Total Training Translation Loss 2895.97 
2024-02-11 18:23:07,254 EPOCH 24
2024-02-11 18:23:09,866 [Epoch: 024 Step: 00003100] Batch Recognition Loss:   0.004938 => Gls Tokens per Sec:     1256 || Batch Translation Loss:  19.398024 => Txt Tokens per Sec:     3581 || Lr: 0.000100
2024-02-11 18:23:15,025 Epoch  24: Total Training Recognition Loss 1.29  Total Training Translation Loss 2612.37 
2024-02-11 18:23:15,025 EPOCH 25
2024-02-11 18:23:15,457 [Epoch: 025 Step: 00003200] Batch Recognition Loss:   0.006175 => Gls Tokens per Sec:     1485 || Batch Translation Loss:  15.055756 => Txt Tokens per Sec:     4241 || Lr: 0.000100
2024-02-11 18:23:20,529 [Epoch: 025 Step: 00003300] Batch Recognition Loss:   0.004491 => Gls Tokens per Sec:     1575 || Batch Translation Loss:  15.730503 => Txt Tokens per Sec:     4371 || Lr: 0.000100
2024-02-11 18:23:21,954 Epoch  25: Total Training Recognition Loss 1.19  Total Training Translation Loss 2309.76 
2024-02-11 18:23:21,954 EPOCH 26
2024-02-11 18:23:26,398 [Epoch: 026 Step: 00003400] Batch Recognition Loss:   0.011643 => Gls Tokens per Sec:     1350 || Batch Translation Loss:   5.075312 => Txt Tokens per Sec:     3690 || Lr: 0.000100
2024-02-11 18:23:29,950 Epoch  26: Total Training Recognition Loss 1.00  Total Training Translation Loss 2052.81 
2024-02-11 18:23:29,951 EPOCH 27
2024-02-11 18:23:32,075 [Epoch: 027 Step: 00003500] Batch Recognition Loss:   0.006243 => Gls Tokens per Sec:     1582 || Batch Translation Loss:  11.813847 => Txt Tokens per Sec:     4174 || Lr: 0.000100
2024-02-11 18:23:37,577 Epoch  27: Total Training Recognition Loss 1.04  Total Training Translation Loss 1844.66 
2024-02-11 18:23:37,578 EPOCH 28
2024-02-11 18:23:38,069 [Epoch: 028 Step: 00003600] Batch Recognition Loss:   0.009140 => Gls Tokens per Sec:     1469 || Batch Translation Loss:  10.911991 => Txt Tokens per Sec:     4112 || Lr: 0.000100
2024-02-11 18:23:43,840 [Epoch: 028 Step: 00003700] Batch Recognition Loss:   0.005404 => Gls Tokens per Sec:     1386 || Batch Translation Loss:  10.416897 => Txt Tokens per Sec:     3887 || Lr: 0.000100
2024-02-11 18:23:45,372 Epoch  28: Total Training Recognition Loss 0.92  Total Training Translation Loss 1617.44 
2024-02-11 18:23:45,373 EPOCH 29
2024-02-11 18:23:49,875 [Epoch: 029 Step: 00003800] Batch Recognition Loss:   0.007836 => Gls Tokens per Sec:     1351 || Batch Translation Loss:  17.620506 => Txt Tokens per Sec:     3766 || Lr: 0.000100
2024-02-11 18:23:53,192 Epoch  29: Total Training Recognition Loss 0.95  Total Training Translation Loss 1414.24 
2024-02-11 18:23:53,192 EPOCH 30
2024-02-11 18:23:55,834 [Epoch: 030 Step: 00003900] Batch Recognition Loss:   0.007050 => Gls Tokens per Sec:     1303 || Batch Translation Loss:  13.760205 => Txt Tokens per Sec:     3861 || Lr: 0.000100
2024-02-11 18:24:00,957 Epoch  30: Total Training Recognition Loss 0.78  Total Training Translation Loss 1241.11 
2024-02-11 18:24:00,958 EPOCH 31
2024-02-11 18:24:01,453 [Epoch: 031 Step: 00004000] Batch Recognition Loss:   0.005604 => Gls Tokens per Sec:     1619 || Batch Translation Loss:   3.160466 => Txt Tokens per Sec:     4069 || Lr: 0.000100
2024-02-11 18:24:12,417 Validation result at epoch  31, step     4000: duration: 10.9644s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.05536	Translation Loss: 67022.35938	PPL: 870.06512
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.76	(BLEU-1: 11.90,	BLEU-2: 4.02,	BLEU-3: 1.67,	BLEU-4: 0.76)
	CHRF 16.94	ROUGE 10.22
2024-02-11 18:24:12,418 Logging Recognition and Translation Outputs
2024-02-11 18:24:12,418 ========================================================================================================================
2024-02-11 18:24:12,418 Logging Sequence: 62_96.00
2024-02-11 18:24:12,418 	Gloss Reference :	A B+C+D+E
2024-02-11 18:24:12,419 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:24:12,419 	Gloss Alignment :	         
2024-02-11 18:24:12,419 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:24:12,420 	Text Reference  :	thats wonderful this means the women players will  now  get  rs  6    lakh for one-day international
2024-02-11 18:24:12,420 	Text Hypothesis :	***** ********* **** ***** the ***** supreme court said that the last over a   match   fee          
2024-02-11 18:24:12,420 	Text Alignment  :	D     D         D    D         D     S       S     S    S    S   S    S    S   S       S            
2024-02-11 18:24:12,420 ========================================================================================================================
2024-02-11 18:24:12,420 Logging Sequence: 103_54.00
2024-02-11 18:24:12,421 	Gloss Reference :	A B+C+D+E
2024-02-11 18:24:12,421 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:24:12,421 	Gloss Alignment :	         
2024-02-11 18:24:12,421 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:24:12,423 	Text Reference  :	what were the ****** reason     for these     games why     only 56  countries are   allowed to participate and  not  the  entire world  
2024-02-11 18:24:12,423 	Text Hypothesis :	**** **** the sports tournament was announced on    twitter that the other     teams were    on a           huge july 2023 on     twitter
2024-02-11 18:24:12,423 	Text Alignment  :	D    D        I      S          S   S         S     S       S    S   S         S     S       S  S           S    S    S    S      S      
2024-02-11 18:24:12,423 ========================================================================================================================
2024-02-11 18:24:12,424 Logging Sequence: 76_79.00
2024-02-11 18:24:12,424 	Gloss Reference :	A B+C+D+E
2024-02-11 18:24:12,424 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:24:12,424 	Gloss Alignment :	         
2024-02-11 18:24:12,424 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:24:12,425 	Text Reference  :	** **** ***** **** speaking to  ani  csk ceo  kasi viswanathan said     
2024-02-11 18:24:12,425 	Text Hypothesis :	on 13th april 2021 the      ipl team is  held in   dubai       yesterday
2024-02-11 18:24:12,425 	Text Alignment  :	I  I    I     I    S        S   S    S   S    S    S           S        
2024-02-11 18:24:12,426 ========================================================================================================================
2024-02-11 18:24:12,426 Logging Sequence: 102_197.00
2024-02-11 18:24:12,426 	Gloss Reference :	A B+C+D+E
2024-02-11 18:24:12,426 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:24:12,426 	Gloss Alignment :	         
2024-02-11 18:24:12,426 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:24:12,429 	Text Reference  :	in reply smriti mandhana scored 63 runs off    42   balls she    led the ****** chase  as and   secured a  victory for indian in just 114 overs
2024-02-11 18:24:12,429 	Text Hypothesis :	** ***** ****** ******** ****** ** **** indian team was   bowled by  the golden player as india chose   to bat     for ****** ** just 99  runs 
2024-02-11 18:24:12,429 	Text Alignment  :	D  D     D      D        D      D  D    S      S    S     S      S       I      S         S     S       S  S           D      D       S   S    
2024-02-11 18:24:12,429 ========================================================================================================================
2024-02-11 18:24:12,429 Logging Sequence: 61_181.00
2024-02-11 18:24:12,429 	Gloss Reference :	A B+C+D+E
2024-02-11 18:24:12,429 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:24:12,430 	Gloss Alignment :	         
2024-02-11 18:24:12,430 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:24:12,432 	Text Reference  :	one other fan   said it    is  babar's personal chat    we  should focuc on this cricketing career and not his  personal life
2024-02-11 18:24:12,432 	Text Hypothesis :	on  10th  april 2023 there was a       match    between rcb and    other on **** ********** ****** *** *** 15th october  2022
2024-02-11 18:24:12,432 	Text Alignment  :	S   S     S     S    S     S   S       S        S       S   S      S        D    D          D      D   D   S    S        S   
2024-02-11 18:24:12,432 ========================================================================================================================
2024-02-11 18:24:18,284 [Epoch: 031 Step: 00004100] Batch Recognition Loss:   0.003406 => Gls Tokens per Sec:     1364 || Batch Translation Loss:   7.799746 => Txt Tokens per Sec:     3809 || Lr: 0.000100
2024-02-11 18:24:19,736 Epoch  31: Total Training Recognition Loss 0.71  Total Training Translation Loss 1094.10 
2024-02-11 18:24:19,737 EPOCH 32
2024-02-11 18:24:24,436 [Epoch: 032 Step: 00004200] Batch Recognition Loss:   0.007989 => Gls Tokens per Sec:     1311 || Batch Translation Loss:   7.853653 => Txt Tokens per Sec:     3572 || Lr: 0.000100
2024-02-11 18:24:27,806 Epoch  32: Total Training Recognition Loss 0.75  Total Training Translation Loss 953.13 
2024-02-11 18:24:27,806 EPOCH 33
2024-02-11 18:24:30,399 [Epoch: 033 Step: 00004300] Batch Recognition Loss:   0.003663 => Gls Tokens per Sec:     1358 || Batch Translation Loss:  11.425006 => Txt Tokens per Sec:     3987 || Lr: 0.000100
2024-02-11 18:24:35,327 Epoch  33: Total Training Recognition Loss 0.70  Total Training Translation Loss 837.75 
2024-02-11 18:24:35,327 EPOCH 34
2024-02-11 18:24:35,991 [Epoch: 034 Step: 00004400] Batch Recognition Loss:   0.004879 => Gls Tokens per Sec:     1331 || Batch Translation Loss:   6.848400 => Txt Tokens per Sec:     3546 || Lr: 0.000100
2024-02-11 18:24:41,805 [Epoch: 034 Step: 00004500] Batch Recognition Loss:   0.006753 => Gls Tokens per Sec:     1376 || Batch Translation Loss:   5.755298 => Txt Tokens per Sec:     3799 || Lr: 0.000100
2024-02-11 18:24:43,264 Epoch  34: Total Training Recognition Loss 0.61  Total Training Translation Loss 739.74 
2024-02-11 18:24:43,265 EPOCH 35
2024-02-11 18:24:47,853 [Epoch: 035 Step: 00004600] Batch Recognition Loss:   0.004952 => Gls Tokens per Sec:     1361 || Batch Translation Loss:   4.609031 => Txt Tokens per Sec:     3770 || Lr: 0.000100
2024-02-11 18:24:51,173 Epoch  35: Total Training Recognition Loss 0.56  Total Training Translation Loss 644.00 
2024-02-11 18:24:51,173 EPOCH 36
2024-02-11 18:24:53,662 [Epoch: 036 Step: 00004700] Batch Recognition Loss:   0.002015 => Gls Tokens per Sec:     1447 || Batch Translation Loss:   3.998834 => Txt Tokens per Sec:     3954 || Lr: 0.000100
2024-02-11 18:24:58,739 Epoch  36: Total Training Recognition Loss 0.50  Total Training Translation Loss 575.58 
2024-02-11 18:24:58,739 EPOCH 37
2024-02-11 18:24:59,493 [Epoch: 037 Step: 00004800] Batch Recognition Loss:   0.005379 => Gls Tokens per Sec:     1275 || Batch Translation Loss:   4.175911 => Txt Tokens per Sec:     3724 || Lr: 0.000100
2024-02-11 18:25:05,375 [Epoch: 037 Step: 00004900] Batch Recognition Loss:   0.003822 => Gls Tokens per Sec:     1359 || Batch Translation Loss:   5.501241 => Txt Tokens per Sec:     3764 || Lr: 0.000100
2024-02-11 18:25:06,550 Epoch  37: Total Training Recognition Loss 0.51  Total Training Translation Loss 510.74 
2024-02-11 18:25:06,550 EPOCH 38
2024-02-11 18:25:11,374 [Epoch: 038 Step: 00005000] Batch Recognition Loss:   0.001408 => Gls Tokens per Sec:     1310 || Batch Translation Loss:   4.666159 => Txt Tokens per Sec:     3656 || Lr: 0.000100
2024-02-11 18:25:14,485 Epoch  38: Total Training Recognition Loss 0.44  Total Training Translation Loss 460.13 
2024-02-11 18:25:14,485 EPOCH 39
2024-02-11 18:25:16,848 [Epoch: 039 Step: 00005100] Batch Recognition Loss:   0.002530 => Gls Tokens per Sec:     1558 || Batch Translation Loss:   2.308326 => Txt Tokens per Sec:     4318 || Lr: 0.000100
2024-02-11 18:25:22,072 Epoch  39: Total Training Recognition Loss 0.38  Total Training Translation Loss 406.76 
2024-02-11 18:25:22,073 EPOCH 40
2024-02-11 18:25:23,032 [Epoch: 040 Step: 00005200] Batch Recognition Loss:   0.001543 => Gls Tokens per Sec:     1086 || Batch Translation Loss:   2.055376 => Txt Tokens per Sec:     3177 || Lr: 0.000100
2024-02-11 18:25:28,694 [Epoch: 040 Step: 00005300] Batch Recognition Loss:   0.003204 => Gls Tokens per Sec:     1413 || Batch Translation Loss:   2.610751 => Txt Tokens per Sec:     3865 || Lr: 0.000100
2024-02-11 18:25:30,005 Epoch  40: Total Training Recognition Loss 0.35  Total Training Translation Loss 366.52 
2024-02-11 18:25:30,006 EPOCH 41
2024-02-11 18:25:34,739 [Epoch: 041 Step: 00005400] Batch Recognition Loss:   0.003243 => Gls Tokens per Sec:     1352 || Batch Translation Loss:   3.057182 => Txt Tokens per Sec:     3683 || Lr: 0.000100
2024-02-11 18:25:38,036 Epoch  41: Total Training Recognition Loss 0.37  Total Training Translation Loss 338.41 
2024-02-11 18:25:38,036 EPOCH 42
2024-02-11 18:25:40,573 [Epoch: 042 Step: 00005500] Batch Recognition Loss:   0.002482 => Gls Tokens per Sec:     1483 || Batch Translation Loss:   2.872957 => Txt Tokens per Sec:     3993 || Lr: 0.000100
2024-02-11 18:25:45,143 Epoch  42: Total Training Recognition Loss 0.33  Total Training Translation Loss 323.83 
2024-02-11 18:25:45,143 EPOCH 43
2024-02-11 18:25:45,918 [Epoch: 043 Step: 00005600] Batch Recognition Loss:   0.002709 => Gls Tokens per Sec:     1447 || Batch Translation Loss:   2.777525 => Txt Tokens per Sec:     4174 || Lr: 0.000100
2024-02-11 18:25:52,272 [Epoch: 043 Step: 00005700] Batch Recognition Loss:   0.002140 => Gls Tokens per Sec:     1258 || Batch Translation Loss:   1.546272 => Txt Tokens per Sec:     3470 || Lr: 0.000100
2024-02-11 18:25:53,331 Epoch  43: Total Training Recognition Loss 0.36  Total Training Translation Loss 308.60 
2024-02-11 18:25:53,331 EPOCH 44
2024-02-11 18:25:57,997 [Epoch: 044 Step: 00005800] Batch Recognition Loss:   0.002744 => Gls Tokens per Sec:     1389 || Batch Translation Loss:   1.875270 => Txt Tokens per Sec:     3794 || Lr: 0.000100
2024-02-11 18:26:01,162 Epoch  44: Total Training Recognition Loss 0.30  Total Training Translation Loss 290.49 
2024-02-11 18:26:01,162 EPOCH 45
2024-02-11 18:26:04,060 [Epoch: 045 Step: 00005900] Batch Recognition Loss:   0.003686 => Gls Tokens per Sec:     1326 || Batch Translation Loss:   1.570850 => Txt Tokens per Sec:     3692 || Lr: 0.000100
2024-02-11 18:26:08,651 Epoch  45: Total Training Recognition Loss 0.27  Total Training Translation Loss 261.74 
2024-02-11 18:26:08,651 EPOCH 46
2024-02-11 18:26:09,391 [Epoch: 046 Step: 00006000] Batch Recognition Loss:   0.001161 => Gls Tokens per Sec:     1624 || Batch Translation Loss:   2.130118 => Txt Tokens per Sec:     4487 || Lr: 0.000100
2024-02-11 18:26:20,988 Validation result at epoch  46, step     6000: duration: 11.5962s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.01373	Translation Loss: 75766.89062	PPL: 2104.16846
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.77	(BLEU-1: 12.99,	BLEU-2: 4.38,	BLEU-3: 1.70,	BLEU-4: 0.77)
	CHRF 17.08	ROUGE 10.73
2024-02-11 18:26:20,989 Logging Recognition and Translation Outputs
2024-02-11 18:26:20,989 ========================================================================================================================
2024-02-11 18:26:20,990 Logging Sequence: 180_556.00
2024-02-11 18:26:20,990 	Gloss Reference :	A B+C+D+E
2024-02-11 18:26:20,990 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:26:20,990 	Gloss Alignment :	         
2024-02-11 18:26:20,990 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:26:20,992 	Text Reference  :	the agitated wrestlers also  asked the government to take back all       the  medals that    they have won  with      their ******* **** ** *** hard    work
2024-02-11 18:26:20,992 	Text Hypothesis :	*** ******** ********* since then  the ********** ** **** **** wrestlers were quiet  however they have been cancelled their protest site at the protest site
2024-02-11 18:26:20,993 	Text Alignment  :	D   D        D         S     S         D          D  D    D    S         S    S      S                 S    S               I       I    I  I   S       S   
2024-02-11 18:26:20,993 ========================================================================================================================
2024-02-11 18:26:20,993 Logging Sequence: 73_116.00
2024-02-11 18:26:20,993 	Gloss Reference :	A B+C+D+E
2024-02-11 18:26:20,993 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:26:20,993 	Gloss Alignment :	         
2024-02-11 18:26:20,993 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:26:20,995 	Text Reference  :	** the cheapest dish on  the *** ***** menu  is ********* raita  whereas the ****** ** costliest dish is   
2024-02-11 18:26:20,995 	Text Hypothesis :	on the ******** 4th  may the two times which is currently filled with    the couple of the       4th  match
2024-02-11 18:26:20,995 	Text Alignment  :	I      D        S    S       I   I     S        I         S      S           I      I  S         S    S    
2024-02-11 18:26:20,995 ========================================================================================================================
2024-02-11 18:26:20,995 Logging Sequence: 130_75.00
2024-02-11 18:26:20,995 	Gloss Reference :	A B+C+D+E
2024-02-11 18:26:20,996 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:26:20,996 	Gloss Alignment :	         
2024-02-11 18:26:20,996 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:26:20,996 	Text Reference  :	*** * ** **** ***** he is  a   fantastic diver   
2024-02-11 18:26:20,996 	Text Hypothesis :	and i am very proud of all the tokyo     olympics
2024-02-11 18:26:20,996 	Text Alignment  :	I   I I  I    I     S  S   S   S         S       
2024-02-11 18:26:20,997 ========================================================================================================================
2024-02-11 18:26:20,997 Logging Sequence: 58_39.00
2024-02-11 18:26:20,997 	Gloss Reference :	A B+C+D+E
2024-02-11 18:26:20,997 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:26:20,997 	Gloss Alignment :	         
2024-02-11 18:26:20,997 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:26:21,000 	Text Reference  :	but due to the covid pandemic they       were postponed and are now being held at the same place from    23rd september 2023 to 8th  october 2023
2024-02-11 18:26:21,000 	Text Hypothesis :	*** *** ** the games are      recognized by   india     and *** now ***** **** ** *** **** 4     matches they would     know on 15th october 2022
2024-02-11 18:26:21,000 	Text Alignment  :	D   D   D      S     S        S          S    S             D       D     D    D  D   D    S     S       S    S         S    S  S            S   
2024-02-11 18:26:21,000 ========================================================================================================================
2024-02-11 18:26:21,000 Logging Sequence: 138_60.00
2024-02-11 18:26:21,000 	Gloss Reference :	A B+C+D+E
2024-02-11 18:26:21,000 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:26:21,001 	Gloss Alignment :	         
2024-02-11 18:26:21,001 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:26:21,002 	Text Reference  :	fans were devastated over the    defeat and ****** * could not digest it    
2024-02-11 18:26:21,002 	Text Hypothesis :	**** the  england    team worked hard   and secure a part  of  the    finals
2024-02-11 18:26:21,002 	Text Alignment  :	D    S    S          S    S      S          I      I S     S   S      S     
2024-02-11 18:26:21,002 ========================================================================================================================
2024-02-11 18:26:27,199 [Epoch: 046 Step: 00006100] Batch Recognition Loss:   0.001209 => Gls Tokens per Sec:     1286 || Batch Translation Loss:   2.298772 => Txt Tokens per Sec:     3599 || Lr: 0.000100
2024-02-11 18:26:28,151 Epoch  46: Total Training Recognition Loss 0.27  Total Training Translation Loss 249.96 
2024-02-11 18:26:28,151 EPOCH 47
2024-02-11 18:26:32,921 [Epoch: 047 Step: 00006200] Batch Recognition Loss:   0.000648 => Gls Tokens per Sec:     1375 || Batch Translation Loss:   1.463343 => Txt Tokens per Sec:     3772 || Lr: 0.000100
2024-02-11 18:26:35,904 Epoch  47: Total Training Recognition Loss 0.25  Total Training Translation Loss 222.46 
2024-02-11 18:26:35,904 EPOCH 48
2024-02-11 18:26:38,518 [Epoch: 048 Step: 00006300] Batch Recognition Loss:   0.000815 => Gls Tokens per Sec:     1501 || Batch Translation Loss:   1.758625 => Txt Tokens per Sec:     4198 || Lr: 0.000100
2024-02-11 18:26:43,368 Epoch  48: Total Training Recognition Loss 0.24  Total Training Translation Loss 194.07 
2024-02-11 18:26:43,368 EPOCH 49
2024-02-11 18:26:44,279 [Epoch: 049 Step: 00006400] Batch Recognition Loss:   0.001908 => Gls Tokens per Sec:     1407 || Batch Translation Loss:   0.684908 => Txt Tokens per Sec:     3956 || Lr: 0.000100
2024-02-11 18:26:50,249 [Epoch: 049 Step: 00006500] Batch Recognition Loss:   0.000769 => Gls Tokens per Sec:     1340 || Batch Translation Loss:   1.205394 => Txt Tokens per Sec:     3692 || Lr: 0.000100
2024-02-11 18:26:51,441 Epoch  49: Total Training Recognition Loss 0.22  Total Training Translation Loss 186.96 
2024-02-11 18:26:51,441 EPOCH 50
2024-02-11 18:26:56,383 [Epoch: 050 Step: 00006600] Batch Recognition Loss:   0.000657 => Gls Tokens per Sec:     1344 || Batch Translation Loss:   0.452121 => Txt Tokens per Sec:     3763 || Lr: 0.000100
2024-02-11 18:26:59,407 Epoch  50: Total Training Recognition Loss 0.24  Total Training Translation Loss 177.75 
2024-02-11 18:26:59,408 EPOCH 51
2024-02-11 18:27:02,358 [Epoch: 051 Step: 00006700] Batch Recognition Loss:   0.004646 => Gls Tokens per Sec:     1356 || Batch Translation Loss:   1.092664 => Txt Tokens per Sec:     3777 || Lr: 0.000100
2024-02-11 18:27:07,100 Epoch  51: Total Training Recognition Loss 0.22  Total Training Translation Loss 169.03 
2024-02-11 18:27:07,101 EPOCH 52
2024-02-11 18:27:08,183 [Epoch: 052 Step: 00006800] Batch Recognition Loss:   0.001004 => Gls Tokens per Sec:     1258 || Batch Translation Loss:   2.006017 => Txt Tokens per Sec:     3479 || Lr: 0.000100
2024-02-11 18:27:13,808 [Epoch: 052 Step: 00006900] Batch Recognition Loss:   0.002254 => Gls Tokens per Sec:     1422 || Batch Translation Loss:   1.453329 => Txt Tokens per Sec:     3943 || Lr: 0.000100
2024-02-11 18:27:14,758 Epoch  52: Total Training Recognition Loss 0.20  Total Training Translation Loss 165.49 
2024-02-11 18:27:14,759 EPOCH 53
2024-02-11 18:27:19,916 [Epoch: 053 Step: 00007000] Batch Recognition Loss:   0.002655 => Gls Tokens per Sec:     1303 || Batch Translation Loss:   1.802992 => Txt Tokens per Sec:     3663 || Lr: 0.000100
2024-02-11 18:27:22,469 Epoch  53: Total Training Recognition Loss 0.18  Total Training Translation Loss 158.42 
2024-02-11 18:27:22,469 EPOCH 54
2024-02-11 18:27:25,078 [Epoch: 054 Step: 00007100] Batch Recognition Loss:   0.002742 => Gls Tokens per Sec:     1564 || Batch Translation Loss:   0.909437 => Txt Tokens per Sec:     4289 || Lr: 0.000100
2024-02-11 18:27:29,816 Epoch  54: Total Training Recognition Loss 0.22  Total Training Translation Loss 167.16 
2024-02-11 18:27:29,816 EPOCH 55
2024-02-11 18:27:30,888 [Epoch: 055 Step: 00007200] Batch Recognition Loss:   0.002302 => Gls Tokens per Sec:     1345 || Batch Translation Loss:   1.406903 => Txt Tokens per Sec:     3617 || Lr: 0.000100
2024-02-11 18:27:36,808 [Epoch: 055 Step: 00007300] Batch Recognition Loss:   0.002069 => Gls Tokens per Sec:     1350 || Batch Translation Loss:   1.320897 => Txt Tokens per Sec:     3760 || Lr: 0.000100
2024-02-11 18:27:37,712 Epoch  55: Total Training Recognition Loss 0.21  Total Training Translation Loss 148.14 
2024-02-11 18:27:37,712 EPOCH 56
2024-02-11 18:27:42,555 [Epoch: 056 Step: 00007400] Batch Recognition Loss:   0.000876 => Gls Tokens per Sec:     1404 || Batch Translation Loss:   0.962268 => Txt Tokens per Sec:     3898 || Lr: 0.000100
2024-02-11 18:27:45,483 Epoch  56: Total Training Recognition Loss 0.23  Total Training Translation Loss 143.26 
2024-02-11 18:27:45,484 EPOCH 57
2024-02-11 18:27:48,356 [Epoch: 057 Step: 00007500] Batch Recognition Loss:   0.004372 => Gls Tokens per Sec:     1449 || Batch Translation Loss:   0.863466 => Txt Tokens per Sec:     3951 || Lr: 0.000100
2024-02-11 18:27:52,865 Epoch  57: Total Training Recognition Loss 0.22  Total Training Translation Loss 143.21 
2024-02-11 18:27:52,865 EPOCH 58
2024-02-11 18:27:54,103 [Epoch: 058 Step: 00007600] Batch Recognition Loss:   0.001428 => Gls Tokens per Sec:     1229 || Batch Translation Loss:   0.384295 => Txt Tokens per Sec:     3553 || Lr: 0.000100
2024-02-11 18:28:00,090 [Epoch: 058 Step: 00007700] Batch Recognition Loss:   0.001026 => Gls Tokens per Sec:     1336 || Batch Translation Loss:   1.186480 => Txt Tokens per Sec:     3692 || Lr: 0.000100
2024-02-11 18:28:00,964 Epoch  58: Total Training Recognition Loss 0.17  Total Training Translation Loss 129.96 
2024-02-11 18:28:00,964 EPOCH 59
2024-02-11 18:28:05,971 [Epoch: 059 Step: 00007800] Batch Recognition Loss:   0.001079 => Gls Tokens per Sec:     1374 || Batch Translation Loss:   0.603774 => Txt Tokens per Sec:     3844 || Lr: 0.000100
2024-02-11 18:28:08,683 Epoch  59: Total Training Recognition Loss 0.19  Total Training Translation Loss 114.70 
2024-02-11 18:28:08,684 EPOCH 60
2024-02-11 18:28:11,872 [Epoch: 060 Step: 00007900] Batch Recognition Loss:   0.001154 => Gls Tokens per Sec:     1330 || Batch Translation Loss:   0.756767 => Txt Tokens per Sec:     3931 || Lr: 0.000100
2024-02-11 18:28:16,342 Epoch  60: Total Training Recognition Loss 0.17  Total Training Translation Loss 112.98 
2024-02-11 18:28:16,343 EPOCH 61
2024-02-11 18:28:17,443 [Epoch: 061 Step: 00008000] Batch Recognition Loss:   0.001928 => Gls Tokens per Sec:     1457 || Batch Translation Loss:   1.679422 => Txt Tokens per Sec:     3982 || Lr: 0.000100
2024-02-11 18:28:28,470 Validation result at epoch  61, step     8000: duration: 11.0270s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00744	Translation Loss: 82116.95312	PPL: 3995.67114
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.78	(BLEU-1: 11.04,	BLEU-2: 3.72,	BLEU-3: 1.63,	BLEU-4: 0.78)
	CHRF 16.79	ROUGE 9.73
2024-02-11 18:28:28,471 Logging Recognition and Translation Outputs
2024-02-11 18:28:28,471 ========================================================================================================================
2024-02-11 18:28:28,471 Logging Sequence: 162_2.00
2024-02-11 18:28:28,472 	Gloss Reference :	A B+C+D+E
2024-02-11 18:28:28,472 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:28:28,472 	Gloss Alignment :	         
2024-02-11 18:28:28,472 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:28:28,473 	Text Reference  :	*** *** ** *** india had   lost  the t20  world cup match against pakistan
2024-02-11 18:28:28,473 	Text Hypothesis :	the two do not want  their child to  step down  as  per   their   child   
2024-02-11 18:28:28,473 	Text Alignment  :	I   I   I  I   S     S     S     S   S    S     S   S     S       S       
2024-02-11 18:28:28,473 ========================================================================================================================
2024-02-11 18:28:28,473 Logging Sequence: 77_62.00
2024-02-11 18:28:28,474 	Gloss Reference :	A B+C+D+E
2024-02-11 18:28:28,474 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:28:28,474 	Gloss Alignment :	         
2024-02-11 18:28:28,474 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:28:28,475 	Text Reference  :	the match went  to   a *** ** ******** *** ***** 6-ball super  over 
2024-02-11 18:28:28,475 	Text Hypothesis :	*** also  there were a lot of violence and chaos on     social media
2024-02-11 18:28:28,475 	Text Alignment  :	D   S     S     S      I   I  I        I   I     S      S      S    
2024-02-11 18:28:28,475 ========================================================================================================================
2024-02-11 18:28:28,475 Logging Sequence: 84_98.00
2024-02-11 18:28:28,476 	Gloss Reference :	A B+C+D+E
2024-02-11 18:28:28,476 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:28:28,476 	Gloss Alignment :	         
2024-02-11 18:28:28,476 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:28:28,476 	Text Reference  :	everyone is expected to    follow the  law  
2024-02-11 18:28:28,476 	Text Hypothesis :	******** ** and      these images went viral
2024-02-11 18:28:28,477 	Text Alignment  :	D        D  S        S     S      S    S    
2024-02-11 18:28:28,477 ========================================================================================================================
2024-02-11 18:28:28,477 Logging Sequence: 181_43.00
2024-02-11 18:28:28,477 	Gloss Reference :	A B+C+D+E
2024-02-11 18:28:28,477 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:28:28,477 	Gloss Alignment :	         
2024-02-11 18:28:28,477 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:28:28,478 	Text Reference  :	**** he has been blessed    with    a   baby  boy
2024-02-11 18:28:28,478 	Text Hypothesis :	this is why the  government secured the world cup
2024-02-11 18:28:28,478 	Text Alignment  :	I    S  S   S    S          S       S   S     S  
2024-02-11 18:28:28,478 ========================================================================================================================
2024-02-11 18:28:28,478 Logging Sequence: 118_389.00
2024-02-11 18:28:28,479 	Gloss Reference :	A B+C+D+E
2024-02-11 18:28:28,479 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:28:28,479 	Gloss Alignment :	         
2024-02-11 18:28:28,479 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:28:28,480 	Text Reference  :	3 emi martinez from argentina recieved the golden  glove    for being the best goalkeeper in the match
2024-02-11 18:28:28,480 	Text Hypothesis :	* *** ******** **** argentina won      the penalty shootout for ***** *** **** ********** ** *** 2-2  
2024-02-11 18:28:28,480 	Text Alignment  :	D D   D        D              S            S       S            D     D   D    D          D  D   S    
2024-02-11 18:28:28,480 ========================================================================================================================
2024-02-11 18:28:34,409 [Epoch: 061 Step: 00008100] Batch Recognition Loss:   0.000706 => Gls Tokens per Sec:     1347 || Batch Translation Loss:   0.789749 => Txt Tokens per Sec:     3713 || Lr: 0.000100
2024-02-11 18:28:35,203 Epoch  61: Total Training Recognition Loss 0.20  Total Training Translation Loss 115.57 
2024-02-11 18:28:35,203 EPOCH 62
2024-02-11 18:28:40,295 [Epoch: 062 Step: 00008200] Batch Recognition Loss:   0.000759 => Gls Tokens per Sec:     1367 || Batch Translation Loss:   1.215775 => Txt Tokens per Sec:     3792 || Lr: 0.000100
2024-02-11 18:28:42,974 Epoch  62: Total Training Recognition Loss 0.22  Total Training Translation Loss 115.71 
2024-02-11 18:28:42,974 EPOCH 63
2024-02-11 18:28:46,104 [Epoch: 063 Step: 00008300] Batch Recognition Loss:   0.000557 => Gls Tokens per Sec:     1381 || Batch Translation Loss:   1.997204 => Txt Tokens per Sec:     3760 || Lr: 0.000100
2024-02-11 18:28:50,711 Epoch  63: Total Training Recognition Loss 0.16  Total Training Translation Loss 105.47 
2024-02-11 18:28:50,712 EPOCH 64
2024-02-11 18:28:51,920 [Epoch: 064 Step: 00008400] Batch Recognition Loss:   0.000333 => Gls Tokens per Sec:     1392 || Batch Translation Loss:   0.782351 => Txt Tokens per Sec:     3751 || Lr: 0.000100
2024-02-11 18:28:58,083 [Epoch: 064 Step: 00008500] Batch Recognition Loss:   0.000539 => Gls Tokens per Sec:     1296 || Batch Translation Loss:   0.917777 => Txt Tokens per Sec:     3621 || Lr: 0.000100
2024-02-11 18:28:58,817 Epoch  64: Total Training Recognition Loss 0.17  Total Training Translation Loss 106.15 
2024-02-11 18:28:58,817 EPOCH 65
2024-02-11 18:29:03,354 [Epoch: 065 Step: 00008600] Batch Recognition Loss:   0.000628 => Gls Tokens per Sec:     1552 || Batch Translation Loss:   0.396369 => Txt Tokens per Sec:     4279 || Lr: 0.000100
2024-02-11 18:29:06,048 Epoch  65: Total Training Recognition Loss 0.16  Total Training Translation Loss 110.40 
2024-02-11 18:29:06,049 EPOCH 66
2024-02-11 18:29:09,195 [Epoch: 066 Step: 00008700] Batch Recognition Loss:   0.000733 => Gls Tokens per Sec:     1399 || Batch Translation Loss:   0.762337 => Txt Tokens per Sec:     3848 || Lr: 0.000100
2024-02-11 18:29:13,707 Epoch  66: Total Training Recognition Loss 0.16  Total Training Translation Loss 97.90 
2024-02-11 18:29:13,707 EPOCH 67
2024-02-11 18:29:14,862 [Epoch: 067 Step: 00008800] Batch Recognition Loss:   0.002342 => Gls Tokens per Sec:     1526 || Batch Translation Loss:   0.998548 => Txt Tokens per Sec:     4378 || Lr: 0.000100
2024-02-11 18:29:20,716 [Epoch: 067 Step: 00008900] Batch Recognition Loss:   0.000920 => Gls Tokens per Sec:     1365 || Batch Translation Loss:   0.419656 => Txt Tokens per Sec:     3756 || Lr: 0.000100
2024-02-11 18:29:21,365 Epoch  67: Total Training Recognition Loss 0.16  Total Training Translation Loss 98.49 
2024-02-11 18:29:21,365 EPOCH 68
2024-02-11 18:29:26,406 [Epoch: 068 Step: 00009000] Batch Recognition Loss:   0.000886 => Gls Tokens per Sec:     1413 || Batch Translation Loss:   1.286321 => Txt Tokens per Sec:     3867 || Lr: 0.000100
2024-02-11 18:29:29,079 Epoch  68: Total Training Recognition Loss 0.16  Total Training Translation Loss 99.87 
2024-02-11 18:29:29,080 EPOCH 69
2024-02-11 18:29:32,236 [Epoch: 069 Step: 00009100] Batch Recognition Loss:   0.000391 => Gls Tokens per Sec:     1420 || Batch Translation Loss:   0.661555 => Txt Tokens per Sec:     3893 || Lr: 0.000100
2024-02-11 18:29:36,763 Epoch  69: Total Training Recognition Loss 0.16  Total Training Translation Loss 101.51 
2024-02-11 18:29:36,764 EPOCH 70
2024-02-11 18:29:38,112 [Epoch: 070 Step: 00009200] Batch Recognition Loss:   0.000899 => Gls Tokens per Sec:     1365 || Batch Translation Loss:   0.584045 => Txt Tokens per Sec:     3712 || Lr: 0.000100
2024-02-11 18:29:44,048 [Epoch: 070 Step: 00009300] Batch Recognition Loss:   0.000760 => Gls Tokens per Sec:     1346 || Batch Translation Loss:   0.698678 => Txt Tokens per Sec:     3725 || Lr: 0.000100
2024-02-11 18:29:44,633 Epoch  70: Total Training Recognition Loss 0.15  Total Training Translation Loss 82.73 
2024-02-11 18:29:44,634 EPOCH 71
2024-02-11 18:29:49,541 [Epoch: 071 Step: 00009400] Batch Recognition Loss:   0.000871 => Gls Tokens per Sec:     1468 || Batch Translation Loss:   0.333445 => Txt Tokens per Sec:     4038 || Lr: 0.000100
2024-02-11 18:29:51,966 Epoch  71: Total Training Recognition Loss 0.15  Total Training Translation Loss 79.28 
2024-02-11 18:29:51,966 EPOCH 72
2024-02-11 18:29:55,060 [Epoch: 072 Step: 00009500] Batch Recognition Loss:   0.002199 => Gls Tokens per Sec:     1474 || Batch Translation Loss:   0.354180 => Txt Tokens per Sec:     4057 || Lr: 0.000100
2024-02-11 18:29:59,571 Epoch  72: Total Training Recognition Loss 0.15  Total Training Translation Loss 78.04 
2024-02-11 18:29:59,571 EPOCH 73
2024-02-11 18:30:00,945 [Epoch: 073 Step: 00009600] Batch Recognition Loss:   0.003220 => Gls Tokens per Sec:     1398 || Batch Translation Loss:   0.374991 => Txt Tokens per Sec:     3921 || Lr: 0.000100
2024-02-11 18:30:06,674 [Epoch: 073 Step: 00009700] Batch Recognition Loss:   0.001321 => Gls Tokens per Sec:     1395 || Batch Translation Loss:   0.862561 => Txt Tokens per Sec:     3870 || Lr: 0.000100
2024-02-11 18:30:07,133 Epoch  73: Total Training Recognition Loss 0.17  Total Training Translation Loss 75.48 
2024-02-11 18:30:07,134 EPOCH 74
2024-02-11 18:30:12,311 [Epoch: 074 Step: 00009800] Batch Recognition Loss:   0.000379 => Gls Tokens per Sec:     1406 || Batch Translation Loss:   0.382030 => Txt Tokens per Sec:     3903 || Lr: 0.000100
2024-02-11 18:30:14,750 Epoch  74: Total Training Recognition Loss 0.16  Total Training Translation Loss 79.98 
2024-02-11 18:30:14,750 EPOCH 75
2024-02-11 18:30:17,904 [Epoch: 075 Step: 00009900] Batch Recognition Loss:   0.000857 => Gls Tokens per Sec:     1472 || Batch Translation Loss:   0.461113 => Txt Tokens per Sec:     4176 || Lr: 0.000100
2024-02-11 18:30:22,382 Epoch  75: Total Training Recognition Loss 0.12  Total Training Translation Loss 66.01 
2024-02-11 18:30:22,383 EPOCH 76
2024-02-11 18:30:24,081 [Epoch: 076 Step: 00010000] Batch Recognition Loss:   0.000375 => Gls Tokens per Sec:     1179 || Batch Translation Loss:   0.925011 => Txt Tokens per Sec:     3461 || Lr: 0.000100
2024-02-11 18:30:35,087 Hooray! New best validation result [eval_metric]!
2024-02-11 18:30:35,088 Saving new checkpoint.
2024-02-11 18:30:35,356 Validation result at epoch  76, step    10000: duration: 11.2733s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00735	Translation Loss: 86169.78125	PPL: 6016.50000
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.88	(BLEU-1: 10.96,	BLEU-2: 3.64,	BLEU-3: 1.60,	BLEU-4: 0.88)
	CHRF 17.21	ROUGE 9.43
2024-02-11 18:30:35,356 Logging Recognition and Translation Outputs
2024-02-11 18:30:35,357 ========================================================================================================================
2024-02-11 18:30:35,357 Logging Sequence: 166_61.00
2024-02-11 18:30:35,357 	Gloss Reference :	A B+C+D+E
2024-02-11 18:30:35,357 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:30:35,357 	Gloss Alignment :	         
2024-02-11 18:30:35,357 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:30:35,358 	Text Reference  :	*** it was  decided to    start one   
2024-02-11 18:30:35,358 	Text Hypothesis :	let me tell you     about the   rights
2024-02-11 18:30:35,358 	Text Alignment  :	I   S  S    S       S     S     S     
2024-02-11 18:30:35,358 ========================================================================================================================
2024-02-11 18:30:35,358 Logging Sequence: 103_19.00
2024-02-11 18:30:35,358 	Gloss Reference :	A B+C+D+E
2024-02-11 18:30:35,359 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:30:35,359 	Gloss Alignment :	         
2024-02-11 18:30:35,359 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:30:35,360 	Text Reference  :	*** *** **** **** **** * i   will tell you   the  full medals tally   in   short while
2024-02-11 18:30:35,360 	Text Hypothesis :	you all know that such a lot of   each other when it   is     already into the   video
2024-02-11 18:30:35,360 	Text Alignment  :	I   I   I    I    I    I S   S    S    S     S    S    S      S       S    S     S    
2024-02-11 18:30:35,360 ========================================================================================================================
2024-02-11 18:30:35,360 Logging Sequence: 119_94.00
2024-02-11 18:30:35,361 	Gloss Reference :	A B+C+D+E
2024-02-11 18:30:35,361 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:30:35,361 	Gloss Alignment :	         
2024-02-11 18:30:35,361 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:30:35,362 	Text Reference  :	***** **** ** messi  got in   touch with ben  lyons who is      the  ceo of   idesign gold
2024-02-11 18:30:35,363 	Text Hypothesis :	since then he wanted to  gift for   the  last over  new devices such a   gift for     them
2024-02-11 18:30:35,363 	Text Alignment  :	I     I    I  S      S   S    S     S    S    S     S   S       S    S   S    S       S   
2024-02-11 18:30:35,363 ========================================================================================================================
2024-02-11 18:30:35,363 Logging Sequence: 65_2.00
2024-02-11 18:30:35,363 	Gloss Reference :	A B+C+D+E
2024-02-11 18:30:35,363 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:30:35,363 	Gloss Alignment :	         
2024-02-11 18:30:35,363 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:30:35,364 	Text Reference  :	*** *** **** **** ***** bcci is         the governing board of  indian cricket
2024-02-11 18:30:35,364 	Text Hypothesis :	you all know that india and  bangladesh was going     on    the indian *******
2024-02-11 18:30:35,364 	Text Alignment  :	I   I   I    I    I     S    S          S   S         S     S          D      
2024-02-11 18:30:35,365 ========================================================================================================================
2024-02-11 18:30:35,365 Logging Sequence: 70_210.00
2024-02-11 18:30:35,365 	Gloss Reference :	A B+C+D+E
2024-02-11 18:30:35,365 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:30:35,365 	Gloss Alignment :	         
2024-02-11 18:30:35,365 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:30:35,366 	Text Reference  :	people were shocked as to how ronaldo  could endorse the    brand      for money and   now endorse water
2024-02-11 18:30:35,367 	Text Hypothesis :	****** **** ******* ** ** *** whenever a     top     famous malleswari had gone  viral on  social  media
2024-02-11 18:30:35,367 	Text Alignment  :	D      D    D       D  D  D   S        S     S       S      S          S   S     S     S   S       S    
2024-02-11 18:30:35,367 ========================================================================================================================
2024-02-11 18:30:41,433 [Epoch: 076 Step: 00010100] Batch Recognition Loss:   0.000466 => Gls Tokens per Sec:     1315 || Batch Translation Loss:   0.416873 => Txt Tokens per Sec:     3612 || Lr: 0.000100
2024-02-11 18:30:41,868 Epoch  76: Total Training Recognition Loss 0.15  Total Training Translation Loss 67.20 
2024-02-11 18:30:41,869 EPOCH 77
2024-02-11 18:30:47,162 [Epoch: 077 Step: 00010200] Batch Recognition Loss:   0.000449 => Gls Tokens per Sec:     1390 || Batch Translation Loss:   0.894971 => Txt Tokens per Sec:     3884 || Lr: 0.000100
2024-02-11 18:30:49,421 Epoch  77: Total Training Recognition Loss 0.13  Total Training Translation Loss 90.84 
2024-02-11 18:30:49,421 EPOCH 78
2024-02-11 18:30:52,888 [Epoch: 078 Step: 00010300] Batch Recognition Loss:   0.000360 => Gls Tokens per Sec:     1362 || Batch Translation Loss:   0.606326 => Txt Tokens per Sec:     3808 || Lr: 0.000100
2024-02-11 18:30:57,140 Epoch  78: Total Training Recognition Loss 0.18  Total Training Translation Loss 86.54 
2024-02-11 18:30:57,141 EPOCH 79
2024-02-11 18:30:58,547 [Epoch: 079 Step: 00010400] Batch Recognition Loss:   0.000682 => Gls Tokens per Sec:     1480 || Batch Translation Loss:   0.383651 => Txt Tokens per Sec:     4236 || Lr: 0.000100
2024-02-11 18:31:04,444 [Epoch: 079 Step: 00010500] Batch Recognition Loss:   0.023353 => Gls Tokens per Sec:     1355 || Batch Translation Loss:   1.606386 => Txt Tokens per Sec:     3768 || Lr: 0.000100
2024-02-11 18:31:04,811 Epoch  79: Total Training Recognition Loss 0.19  Total Training Translation Loss 81.97 
2024-02-11 18:31:04,811 EPOCH 80
2024-02-11 18:31:10,219 [Epoch: 080 Step: 00010600] Batch Recognition Loss:   0.002194 => Gls Tokens per Sec:     1376 || Batch Translation Loss:   0.436785 => Txt Tokens per Sec:     3787 || Lr: 0.000100
2024-02-11 18:31:12,455 Epoch  80: Total Training Recognition Loss 0.13  Total Training Translation Loss 68.82 
2024-02-11 18:31:12,455 EPOCH 81
2024-02-11 18:31:15,698 [Epoch: 081 Step: 00010700] Batch Recognition Loss:   0.001246 => Gls Tokens per Sec:     1481 || Batch Translation Loss:   0.168228 => Txt Tokens per Sec:     4049 || Lr: 0.000100
2024-02-11 18:31:20,246 Epoch  81: Total Training Recognition Loss 0.12  Total Training Translation Loss 56.41 
2024-02-11 18:31:20,246 EPOCH 82
2024-02-11 18:31:21,673 [Epoch: 082 Step: 00010800] Batch Recognition Loss:   0.001129 => Gls Tokens per Sec:     1516 || Batch Translation Loss:   0.208304 => Txt Tokens per Sec:     3874 || Lr: 0.000100
2024-02-11 18:31:27,687 [Epoch: 082 Step: 00010900] Batch Recognition Loss:   0.002748 => Gls Tokens per Sec:     1328 || Batch Translation Loss:   0.406839 => Txt Tokens per Sec:     3736 || Lr: 0.000100
2024-02-11 18:31:28,043 Epoch  82: Total Training Recognition Loss 0.14  Total Training Translation Loss 67.74 
2024-02-11 18:31:28,044 EPOCH 83
2024-02-11 18:31:33,774 [Epoch: 083 Step: 00011000] Batch Recognition Loss:   0.000314 => Gls Tokens per Sec:     1313 || Batch Translation Loss:   0.371524 => Txt Tokens per Sec:     3663 || Lr: 0.000100
2024-02-11 18:31:36,005 Epoch  83: Total Training Recognition Loss 0.12  Total Training Translation Loss 63.23 
2024-02-11 18:31:36,005 EPOCH 84
2024-02-11 18:31:39,314 [Epoch: 084 Step: 00011100] Batch Recognition Loss:   0.000619 => Gls Tokens per Sec:     1475 || Batch Translation Loss:   0.838247 => Txt Tokens per Sec:     4138 || Lr: 0.000100
2024-02-11 18:31:42,996 Epoch  84: Total Training Recognition Loss 0.11  Total Training Translation Loss 77.36 
2024-02-11 18:31:42,996 EPOCH 85
2024-02-11 18:31:44,571 [Epoch: 085 Step: 00011200] Batch Recognition Loss:   0.003051 => Gls Tokens per Sec:     1423 || Batch Translation Loss:   0.365205 => Txt Tokens per Sec:     4132 || Lr: 0.000100
2024-02-11 18:31:49,828 [Epoch: 085 Step: 00011300] Batch Recognition Loss:   0.001151 => Gls Tokens per Sec:     1520 || Batch Translation Loss:   0.828049 => Txt Tokens per Sec:     4177 || Lr: 0.000100
2024-02-11 18:31:50,047 Epoch  85: Total Training Recognition Loss 0.14  Total Training Translation Loss 74.68 
2024-02-11 18:31:50,047 EPOCH 86
2024-02-11 18:31:55,383 [Epoch: 086 Step: 00011400] Batch Recognition Loss:   0.000421 => Gls Tokens per Sec:     1425 || Batch Translation Loss:   0.289309 => Txt Tokens per Sec:     3962 || Lr: 0.000100
2024-02-11 18:31:57,496 Epoch  86: Total Training Recognition Loss 0.16  Total Training Translation Loss 65.91 
2024-02-11 18:31:57,496 EPOCH 87
2024-02-11 18:32:01,319 [Epoch: 087 Step: 00011500] Batch Recognition Loss:   0.000259 => Gls Tokens per Sec:     1298 || Batch Translation Loss:   0.227353 => Txt Tokens per Sec:     3596 || Lr: 0.000100
2024-02-11 18:32:05,507 Epoch  87: Total Training Recognition Loss 0.13  Total Training Translation Loss 63.36 
2024-02-11 18:32:05,507 EPOCH 88
2024-02-11 18:32:07,162 [Epoch: 088 Step: 00011600] Batch Recognition Loss:   0.000520 => Gls Tokens per Sec:     1403 || Batch Translation Loss:   0.541830 => Txt Tokens per Sec:     3814 || Lr: 0.000100
2024-02-11 18:32:12,995 [Epoch: 088 Step: 00011700] Batch Recognition Loss:   0.001398 => Gls Tokens per Sec:     1370 || Batch Translation Loss:   1.260316 => Txt Tokens per Sec:     3840 || Lr: 0.000100
2024-02-11 18:32:13,203 Epoch  88: Total Training Recognition Loss 0.14  Total Training Translation Loss 56.75 
2024-02-11 18:32:13,204 EPOCH 89
2024-02-11 18:32:18,619 [Epoch: 089 Step: 00011800] Batch Recognition Loss:   0.000643 => Gls Tokens per Sec:     1419 || Batch Translation Loss:   0.579714 => Txt Tokens per Sec:     3913 || Lr: 0.000100
2024-02-11 18:32:20,729 Epoch  89: Total Training Recognition Loss 0.14  Total Training Translation Loss 55.43 
2024-02-11 18:32:20,729 EPOCH 90
2024-02-11 18:32:24,116 [Epoch: 090 Step: 00011900] Batch Recognition Loss:   0.000455 => Gls Tokens per Sec:     1488 || Batch Translation Loss:   0.352828 => Txt Tokens per Sec:     4102 || Lr: 0.000100
2024-02-11 18:32:28,143 Epoch  90: Total Training Recognition Loss 0.11  Total Training Translation Loss 50.08 
2024-02-11 18:32:28,143 EPOCH 91
2024-02-11 18:32:29,835 [Epoch: 091 Step: 00012000] Batch Recognition Loss:   0.000165 => Gls Tokens per Sec:     1419 || Batch Translation Loss:   0.453495 => Txt Tokens per Sec:     3974 || Lr: 0.000100
2024-02-11 18:32:40,721 Validation result at epoch  91, step    12000: duration: 10.8860s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00451	Translation Loss: 87285.25000	PPL: 6733.91455
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.55	(BLEU-1: 11.51,	BLEU-2: 3.65,	BLEU-3: 1.30,	BLEU-4: 0.55)
	CHRF 17.96	ROUGE 9.65
2024-02-11 18:32:40,722 Logging Recognition and Translation Outputs
2024-02-11 18:32:40,722 ========================================================================================================================
2024-02-11 18:32:40,722 Logging Sequence: 153_239.00
2024-02-11 18:32:40,722 	Gloss Reference :	A B+C+D+E
2024-02-11 18:32:40,723 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:32:40,723 	Gloss Alignment :	         
2024-02-11 18:32:40,723 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:32:40,724 	Text Reference  :	now       people are predicted current skipped babar azam  to   become the prime  minister in the  future     
2024-02-11 18:32:40,724 	Text Hypothesis :	similarly natasa and shardul   thakur  they    took  their vows in     the finals what     a  huge achievement
2024-02-11 18:32:40,725 	Text Alignment  :	S         S      S   S         S       S       S     S     S    S          S      S        S  S    S          
2024-02-11 18:32:40,725 ========================================================================================================================
2024-02-11 18:32:40,725 Logging Sequence: 179_110.00
2024-02-11 18:32:40,725 	Gloss Reference :	A B+C+D+E
2024-02-11 18:32:40,725 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:32:40,725 	Gloss Alignment :	         
2024-02-11 18:32:40,725 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:32:40,727 	Text Reference  :	*** *** *** phogat      refused   to     stay   in the  same   room  with other  indian female wrestlers
2024-02-11 18:32:40,727 	Text Hypothesis :	the wfi has temporarily suspended vinesh phogat on 10th august after her  return from   the    olympics 
2024-02-11 18:32:40,727 	Text Alignment  :	I   I   I   S           S         S      S      S  S    S      S     S    S      S      S      S        
2024-02-11 18:32:40,727 ========================================================================================================================
2024-02-11 18:32:40,727 Logging Sequence: 132_91.00
2024-02-11 18:32:40,727 	Gloss Reference :	A B+C+D+E
2024-02-11 18:32:40,728 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:32:40,728 	Gloss Alignment :	         
2024-02-11 18:32:40,728 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:32:40,729 	Text Reference  :	that time pat cummins who is australia' captain noticed and   stopped it      he   then called  khwaja
2024-02-11 18:32:40,729 	Text Hypothesis :	**** **** *** ******* *** ** ********** he      heard   about the     players from 18th january 2023  
2024-02-11 18:32:40,729 	Text Alignment  :	D    D    D   D       D   D  D          S       S       S     S       S       S    S    S       S     
2024-02-11 18:32:40,729 ========================================================================================================================
2024-02-11 18:32:40,729 Logging Sequence: 144_161.00
2024-02-11 18:32:40,729 	Gloss Reference :	A B+C+D+E
2024-02-11 18:32:40,729 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:32:40,730 	Gloss Alignment :	         
2024-02-11 18:32:40,730 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:32:40,731 	Text Reference  :	***** **** **** really      amazed by  her enthusiasm hope she is   selected in  the    national team one  day
2024-02-11 18:32:40,731 	Text Hypothesis :	there were many participate to     see i   will       have a   huge argument and gautam gambhir  me   tell you
2024-02-11 18:32:40,732 	Text Alignment  :	I     I    I    S           S      S   S   S          S    S   S    S        S   S      S        S    S    S  
2024-02-11 18:32:40,732 ========================================================================================================================
2024-02-11 18:32:40,732 Logging Sequence: 93_267.00
2024-02-11 18:32:40,732 	Gloss Reference :	A B+C+D+E
2024-02-11 18:32:40,732 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:32:40,732 	Gloss Alignment :	         
2024-02-11 18:32:40,732 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:32:40,734 	Text Reference  :	one girl who    was wearing a thong flashed  her  bottom next          to     rooney the image went viral   on  snapchat
2024-02-11 18:32:40,734 	Text Hypothesis :	*** **** rooney was ******* * seen  speaking with a      scantily-clad blonde rooney *** and   has  reached the girls   
2024-02-11 18:32:40,734 	Text Alignment  :	D   D    S          D       D S     S        S    S      S             S             D   S     S    S       S   S       
2024-02-11 18:32:40,734 ========================================================================================================================
2024-02-11 18:32:46,714 [Epoch: 091 Step: 00012100] Batch Recognition Loss:   0.000204 => Gls Tokens per Sec:     1333 || Batch Translation Loss:   0.286436 => Txt Tokens per Sec:     3707 || Lr: 0.000100
2024-02-11 18:32:46,875 Epoch  91: Total Training Recognition Loss 0.10  Total Training Translation Loss 52.25 
2024-02-11 18:32:46,875 EPOCH 92
2024-02-11 18:32:51,779 [Epoch: 092 Step: 00012200] Batch Recognition Loss:   0.001164 => Gls Tokens per Sec:     1583 || Batch Translation Loss:   0.622186 => Txt Tokens per Sec:     4366 || Lr: 0.000100
2024-02-11 18:32:53,777 Epoch  92: Total Training Recognition Loss 0.11  Total Training Translation Loss 58.55 
2024-02-11 18:32:53,777 EPOCH 93
2024-02-11 18:32:57,426 [Epoch: 093 Step: 00012300] Batch Recognition Loss:   0.000609 => Gls Tokens per Sec:     1403 || Batch Translation Loss:   0.459379 => Txt Tokens per Sec:     3845 || Lr: 0.000100
2024-02-11 18:33:01,691 Epoch  93: Total Training Recognition Loss 0.13  Total Training Translation Loss 59.20 
2024-02-11 18:33:01,691 EPOCH 94
2024-02-11 18:33:03,460 [Epoch: 094 Step: 00012400] Batch Recognition Loss:   0.000986 => Gls Tokens per Sec:     1403 || Batch Translation Loss:   0.416389 => Txt Tokens per Sec:     4075 || Lr: 0.000100
2024-02-11 18:33:09,091 [Epoch: 094 Step: 00012500] Batch Recognition Loss:   0.002305 => Gls Tokens per Sec:     1419 || Batch Translation Loss:   0.276835 => Txt Tokens per Sec:     3873 || Lr: 0.000100
2024-02-11 18:33:09,214 Epoch  94: Total Training Recognition Loss 0.11  Total Training Translation Loss 53.98 
2024-02-11 18:33:09,214 EPOCH 95
2024-02-11 18:33:14,743 [Epoch: 095 Step: 00012600] Batch Recognition Loss:   0.000332 => Gls Tokens per Sec:     1418 || Batch Translation Loss:   0.183780 => Txt Tokens per Sec:     3966 || Lr: 0.000100
2024-02-11 18:33:16,870 Epoch  95: Total Training Recognition Loss 0.10  Total Training Translation Loss 50.79 
2024-02-11 18:33:16,870 EPOCH 96
2024-02-11 18:33:20,562 [Epoch: 096 Step: 00012700] Batch Recognition Loss:   0.001515 => Gls Tokens per Sec:     1409 || Batch Translation Loss:   0.221343 => Txt Tokens per Sec:     3980 || Lr: 0.000100
2024-02-11 18:33:24,472 Epoch  96: Total Training Recognition Loss 0.11  Total Training Translation Loss 54.15 
2024-02-11 18:33:24,472 EPOCH 97
2024-02-11 18:33:26,398 [Epoch: 097 Step: 00012800] Batch Recognition Loss:   0.000737 => Gls Tokens per Sec:     1330 || Batch Translation Loss:   0.528817 => Txt Tokens per Sec:     3849 || Lr: 0.000100
2024-02-11 18:33:32,081 [Epoch: 097 Step: 00012900] Batch Recognition Loss:   0.000930 => Gls Tokens per Sec:     1406 || Batch Translation Loss:   0.427969 => Txt Tokens per Sec:     3850 || Lr: 0.000100
2024-02-11 18:33:32,154 Epoch  97: Total Training Recognition Loss 0.11  Total Training Translation Loss 58.62 
2024-02-11 18:33:32,154 EPOCH 98
2024-02-11 18:33:37,835 [Epoch: 098 Step: 00013000] Batch Recognition Loss:   0.000272 => Gls Tokens per Sec:     1395 || Batch Translation Loss:   0.625784 => Txt Tokens per Sec:     3847 || Lr: 0.000100
2024-02-11 18:33:39,827 Epoch  98: Total Training Recognition Loss 0.13  Total Training Translation Loss 54.63 
2024-02-11 18:33:39,827 EPOCH 99
2024-02-11 18:33:43,492 [Epoch: 099 Step: 00013100] Batch Recognition Loss:   0.000386 => Gls Tokens per Sec:     1441 || Batch Translation Loss:   0.902853 => Txt Tokens per Sec:     3854 || Lr: 0.000100
2024-02-11 18:33:47,524 Epoch  99: Total Training Recognition Loss 0.13  Total Training Translation Loss 54.73 
2024-02-11 18:33:47,524 EPOCH 100
2024-02-11 18:33:49,567 [Epoch: 100 Step: 00013200] Batch Recognition Loss:   0.000234 => Gls Tokens per Sec:     1293 || Batch Translation Loss:   0.375348 => Txt Tokens per Sec:     3793 || Lr: 0.000100
2024-02-11 18:33:55,316 [Epoch: 100 Step: 00013300] Batch Recognition Loss:   0.000682 => Gls Tokens per Sec:     1390 || Batch Translation Loss:   0.297737 => Txt Tokens per Sec:     3791 || Lr: 0.000100
2024-02-11 18:33:55,317 Epoch 100: Total Training Recognition Loss 0.10  Total Training Translation Loss 47.72 
2024-02-11 18:33:55,317 EPOCH 101
2024-02-11 18:34:00,847 [Epoch: 101 Step: 00013400] Batch Recognition Loss:   0.000778 => Gls Tokens per Sec:     1447 || Batch Translation Loss:   0.260084 => Txt Tokens per Sec:     4042 || Lr: 0.000100
2024-02-11 18:34:02,684 Epoch 101: Total Training Recognition Loss 0.10  Total Training Translation Loss 48.22 
2024-02-11 18:34:02,684 EPOCH 102
2024-02-11 18:34:06,760 [Epoch: 102 Step: 00013500] Batch Recognition Loss:   0.000498 => Gls Tokens per Sec:     1316 || Batch Translation Loss:   0.138042 => Txt Tokens per Sec:     3635 || Lr: 0.000100
2024-02-11 18:34:10,778 Epoch 102: Total Training Recognition Loss 0.11  Total Training Translation Loss 44.66 
2024-02-11 18:34:10,778 EPOCH 103
2024-02-11 18:34:12,913 [Epoch: 103 Step: 00013600] Batch Recognition Loss:   0.000926 => Gls Tokens per Sec:     1275 || Batch Translation Loss:   0.228906 => Txt Tokens per Sec:     3548 || Lr: 0.000100
2024-02-11 18:34:18,385 Epoch 103: Total Training Recognition Loss 0.11  Total Training Translation Loss 45.76 
2024-02-11 18:34:18,385 EPOCH 104
2024-02-11 18:34:18,437 [Epoch: 104 Step: 00013700] Batch Recognition Loss:   0.000621 => Gls Tokens per Sec:     1600 || Batch Translation Loss:   0.139521 => Txt Tokens per Sec:     4560 || Lr: 0.000100
2024-02-11 18:34:23,644 [Epoch: 104 Step: 00013800] Batch Recognition Loss:   0.001081 => Gls Tokens per Sec:     1536 || Batch Translation Loss:   0.144772 => Txt Tokens per Sec:     4224 || Lr: 0.000100
2024-02-11 18:34:25,447 Epoch 104: Total Training Recognition Loss 0.11  Total Training Translation Loss 62.35 
2024-02-11 18:34:25,448 EPOCH 105
2024-02-11 18:34:29,348 [Epoch: 105 Step: 00013900] Batch Recognition Loss:   0.001045 => Gls Tokens per Sec:     1395 || Batch Translation Loss:   0.536964 => Txt Tokens per Sec:     3787 || Lr: 0.000100
2024-02-11 18:34:33,227 Epoch 105: Total Training Recognition Loss 0.11  Total Training Translation Loss 52.07 
2024-02-11 18:34:33,227 EPOCH 106
2024-02-11 18:34:35,097 [Epoch: 106 Step: 00014000] Batch Recognition Loss:   0.000402 => Gls Tokens per Sec:     1498 || Batch Translation Loss:   0.102567 => Txt Tokens per Sec:     3939 || Lr: 0.000100
2024-02-11 18:34:45,851 Validation result at epoch 106, step    14000: duration: 10.7540s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00402	Translation Loss: 89021.70312	PPL: 8024.66650
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.40	(BLEU-1: 10.94,	BLEU-2: 3.47,	BLEU-3: 1.20,	BLEU-4: 0.40)
	CHRF 16.98	ROUGE 9.35
2024-02-11 18:34:45,852 Logging Recognition and Translation Outputs
2024-02-11 18:34:45,853 ========================================================================================================================
2024-02-11 18:34:45,853 Logging Sequence: 152_34.00
2024-02-11 18:34:45,853 	Gloss Reference :	A B+C+D+E
2024-02-11 18:34:45,853 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:34:45,853 	Gloss Alignment :	         
2024-02-11 18:34:45,853 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:34:45,854 	Text Reference  :	whenever there is a   india-pakistan match people are          always glued to the screens
2024-02-11 18:34:45,854 	Text Hypothesis :	******** ***** ** did it             was   very   disappointed to     see   if the match  
2024-02-11 18:34:45,855 	Text Alignment  :	D        D     D  S   S              S     S      S            S      S     S      S      
2024-02-11 18:34:45,855 ========================================================================================================================
2024-02-11 18:34:45,855 Logging Sequence: 148_26.00
2024-02-11 18:34:45,855 	Gloss Reference :	A B+C+D+E
2024-02-11 18:34:45,855 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:34:45,855 	Gloss Alignment :	         
2024-02-11 18:34:45,855 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:34:45,857 	Text Reference  :	*** jasprit bumrah  took the    first wicket in the very first over of    the sri   lankan innings 
2024-02-11 18:34:45,857 	Text Hypothesis :	who then    bowling and  pandya came  out    of the **** ***** odi  world cup final and    pakistan
2024-02-11 18:34:45,857 	Text Alignment  :	I   S       S       S    S      S     S      S      D    D     S    S     S   S     S      S       
2024-02-11 18:34:45,857 ========================================================================================================================
2024-02-11 18:34:45,857 Logging Sequence: 70_65.00
2024-02-11 18:34:45,858 	Gloss Reference :	A B+C+D+E
2024-02-11 18:34:45,858 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:34:45,858 	Gloss Alignment :	         
2024-02-11 18:34:45,858 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:34:45,859 	Text Reference  :	during the    press conference a    table   was placed in    front of   the  media
2024-02-11 18:34:45,859 	Text Hypothesis :	****** people are   saying     that ronaldo is  a      viral on    14th june 2021 
2024-02-11 18:34:45,859 	Text Alignment  :	D      S      S     S          S    S       S   S      S     S     S    S    S    
2024-02-11 18:34:45,859 ========================================================================================================================
2024-02-11 18:34:45,859 Logging Sequence: 149_144.00
2024-02-11 18:34:45,860 	Gloss Reference :	A B+C+D+E
2024-02-11 18:34:45,860 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:34:45,860 	Gloss Alignment :	         
2024-02-11 18:34:45,860 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:34:45,862 	Text Reference  :	this was reported by fox sports australia which quoted   police superintendent jayne    doherty who    added      that ******** investigation is  going on    
2024-02-11 18:34:45,862 	Text Hypothesis :	**** *** ******** ** the woman  alleged   that  danushka had    a              suburban of      sydney complained that danushka happened      for the   crease
2024-02-11 18:34:45,862 	Text Alignment  :	D    D   D        D  S   S      S         S     S        S      S              S        S       S      S               I        S             S   S     S     
2024-02-11 18:34:45,862 ========================================================================================================================
2024-02-11 18:34:45,862 Logging Sequence: 96_110.00
2024-02-11 18:34:45,863 	Gloss Reference :	A B+C+D+E
2024-02-11 18:34:45,863 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:34:45,863 	Gloss Alignment :	         
2024-02-11 18:34:45,863 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:34:45,863 	Text Reference  :	*** ***** viewers were  extremely distressed
2024-02-11 18:34:45,863 	Text Hypothesis :	the match started after the       ceremony  
2024-02-11 18:34:45,863 	Text Alignment  :	I   I     S       S     S         S         
2024-02-11 18:34:45,864 ========================================================================================================================
2024-02-11 18:34:51,710 Epoch 106: Total Training Recognition Loss 0.11  Total Training Translation Loss 48.27 
2024-02-11 18:34:51,710 EPOCH 107
2024-02-11 18:34:51,830 [Epoch: 107 Step: 00014100] Batch Recognition Loss:   0.000616 => Gls Tokens per Sec:     1345 || Batch Translation Loss:   0.110874 => Txt Tokens per Sec:     3597 || Lr: 0.000100
2024-02-11 18:34:57,486 [Epoch: 107 Step: 00014200] Batch Recognition Loss:   0.001020 => Gls Tokens per Sec:     1414 || Batch Translation Loss:   0.254200 => Txt Tokens per Sec:     3929 || Lr: 0.000100
2024-02-11 18:34:59,197 Epoch 107: Total Training Recognition Loss 0.11  Total Training Translation Loss 47.20 
2024-02-11 18:34:59,197 EPOCH 108
2024-02-11 18:35:03,029 [Epoch: 108 Step: 00014300] Batch Recognition Loss:   0.000895 => Gls Tokens per Sec:     1441 || Batch Translation Loss:   0.147751 => Txt Tokens per Sec:     4023 || Lr: 0.000100
2024-02-11 18:35:06,964 Epoch 108: Total Training Recognition Loss 0.09  Total Training Translation Loss 43.22 
2024-02-11 18:35:06,965 EPOCH 109
2024-02-11 18:35:08,860 [Epoch: 109 Step: 00014400] Batch Recognition Loss:   0.000560 => Gls Tokens per Sec:     1521 || Batch Translation Loss:   0.459520 => Txt Tokens per Sec:     4060 || Lr: 0.000100
2024-02-11 18:35:14,171 Epoch 109: Total Training Recognition Loss 0.10  Total Training Translation Loss 41.34 
2024-02-11 18:35:14,171 EPOCH 110
2024-02-11 18:35:14,320 [Epoch: 110 Step: 00014500] Batch Recognition Loss:   0.000917 => Gls Tokens per Sec:     1622 || Batch Translation Loss:   0.281804 => Txt Tokens per Sec:     5041 || Lr: 0.000100
2024-02-11 18:35:19,981 [Epoch: 110 Step: 00014600] Batch Recognition Loss:   0.001529 => Gls Tokens per Sec:     1413 || Batch Translation Loss:   0.300928 => Txt Tokens per Sec:     3930 || Lr: 0.000100
2024-02-11 18:35:21,800 Epoch 110: Total Training Recognition Loss 0.07  Total Training Translation Loss 36.13 
2024-02-11 18:35:21,800 EPOCH 111
2024-02-11 18:35:25,458 [Epoch: 111 Step: 00014700] Batch Recognition Loss:   0.000243 => Gls Tokens per Sec:     1531 || Batch Translation Loss:   0.180562 => Txt Tokens per Sec:     4180 || Lr: 0.000100
2024-02-11 18:35:29,382 Epoch 111: Total Training Recognition Loss 0.10  Total Training Translation Loss 43.87 
2024-02-11 18:35:29,383 EPOCH 112
2024-02-11 18:35:31,443 [Epoch: 112 Step: 00014800] Batch Recognition Loss:   0.000399 => Gls Tokens per Sec:     1437 || Batch Translation Loss:   0.492109 => Txt Tokens per Sec:     3888 || Lr: 0.000100
2024-02-11 18:35:36,890 Epoch 112: Total Training Recognition Loss 0.11  Total Training Translation Loss 43.88 
2024-02-11 18:35:36,890 EPOCH 113
2024-02-11 18:35:37,080 [Epoch: 113 Step: 00014900] Batch Recognition Loss:   0.000266 => Gls Tokens per Sec:     1693 || Batch Translation Loss:   0.963072 => Txt Tokens per Sec:     4688 || Lr: 0.000100
2024-02-11 18:35:42,491 [Epoch: 113 Step: 00015000] Batch Recognition Loss:   0.000354 => Gls Tokens per Sec:     1478 || Batch Translation Loss:   0.717097 => Txt Tokens per Sec:     4133 || Lr: 0.000100
2024-02-11 18:35:44,232 Epoch 113: Total Training Recognition Loss 0.10  Total Training Translation Loss 53.78 
2024-02-11 18:35:44,233 EPOCH 114
2024-02-11 18:35:48,345 [Epoch: 114 Step: 00015100] Batch Recognition Loss:   0.000180 => Gls Tokens per Sec:     1382 || Batch Translation Loss:   0.340708 => Txt Tokens per Sec:     3857 || Lr: 0.000100
2024-02-11 18:35:51,895 Epoch 114: Total Training Recognition Loss 0.08  Total Training Translation Loss 47.92 
2024-02-11 18:35:51,895 EPOCH 115
2024-02-11 18:35:54,217 [Epoch: 115 Step: 00015200] Batch Recognition Loss:   0.000401 => Gls Tokens per Sec:     1310 || Batch Translation Loss:   0.369650 => Txt Tokens per Sec:     3660 || Lr: 0.000100
2024-02-11 18:35:59,579 Epoch 115: Total Training Recognition Loss 0.09  Total Training Translation Loss 34.16 
2024-02-11 18:35:59,580 EPOCH 116
2024-02-11 18:35:59,869 [Epoch: 116 Step: 00015300] Batch Recognition Loss:   0.000165 => Gls Tokens per Sec:     1384 || Batch Translation Loss:   0.250786 => Txt Tokens per Sec:     4069 || Lr: 0.000100
2024-02-11 18:36:05,112 [Epoch: 116 Step: 00015400] Batch Recognition Loss:   0.000313 => Gls Tokens per Sec:     1526 || Batch Translation Loss:   0.306806 => Txt Tokens per Sec:     4250 || Lr: 0.000100
2024-02-11 18:36:06,742 Epoch 116: Total Training Recognition Loss 0.08  Total Training Translation Loss 39.94 
2024-02-11 18:36:06,743 EPOCH 117
2024-02-11 18:36:11,085 [Epoch: 117 Step: 00015500] Batch Recognition Loss:   0.001382 => Gls Tokens per Sec:     1327 || Batch Translation Loss:   0.293698 => Txt Tokens per Sec:     3775 || Lr: 0.000100
2024-02-11 18:36:14,596 Epoch 117: Total Training Recognition Loss 0.08  Total Training Translation Loss 34.12 
2024-02-11 18:36:14,596 EPOCH 118
2024-02-11 18:36:16,701 [Epoch: 118 Step: 00015600] Batch Recognition Loss:   0.000158 => Gls Tokens per Sec:     1483 || Batch Translation Loss:   0.616269 => Txt Tokens per Sec:     4140 || Lr: 0.000100
2024-02-11 18:36:21,954 Epoch 118: Total Training Recognition Loss 0.08  Total Training Translation Loss 42.23 
2024-02-11 18:36:21,955 EPOCH 119
2024-02-11 18:36:22,348 [Epoch: 119 Step: 00015700] Batch Recognition Loss:   0.000327 => Gls Tokens per Sec:     1228 || Batch Translation Loss:   0.583117 => Txt Tokens per Sec:     3739 || Lr: 0.000100
2024-02-11 18:36:27,775 [Epoch: 119 Step: 00015800] Batch Recognition Loss:   0.001061 => Gls Tokens per Sec:     1472 || Batch Translation Loss:   0.579748 => Txt Tokens per Sec:     4048 || Lr: 0.000100
2024-02-11 18:36:29,434 Epoch 119: Total Training Recognition Loss 0.09  Total Training Translation Loss 46.38 
2024-02-11 18:36:29,434 EPOCH 120
2024-02-11 18:36:33,795 [Epoch: 120 Step: 00015900] Batch Recognition Loss:   0.000262 => Gls Tokens per Sec:     1339 || Batch Translation Loss:   0.232696 => Txt Tokens per Sec:     3776 || Lr: 0.000100
2024-02-11 18:36:37,202 Epoch 120: Total Training Recognition Loss 0.09  Total Training Translation Loss 41.25 
2024-02-11 18:36:37,202 EPOCH 121
2024-02-11 18:36:39,268 [Epoch: 121 Step: 00016000] Batch Recognition Loss:   0.000375 => Gls Tokens per Sec:     1550 || Batch Translation Loss:   0.324491 => Txt Tokens per Sec:     4289 || Lr: 0.000100
2024-02-11 18:36:50,197 Validation result at epoch 121, step    16000: duration: 10.9280s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00503	Translation Loss: 90753.96094	PPL: 9558.78027
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.58	(BLEU-1: 10.68,	BLEU-2: 3.39,	BLEU-3: 1.25,	BLEU-4: 0.58)
	CHRF 16.55	ROUGE 9.01
2024-02-11 18:36:50,198 Logging Recognition and Translation Outputs
2024-02-11 18:36:50,198 ========================================================================================================================
2024-02-11 18:36:50,198 Logging Sequence: 70_65.00
2024-02-11 18:36:50,199 	Gloss Reference :	A B+C+D+E
2024-02-11 18:36:50,199 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:36:50,199 	Gloss Alignment :	         
2024-02-11 18:36:50,199 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:36:50,200 	Text Reference  :	during the press conference a table was  placed in   front of the **** media  
2024-02-11 18:36:50,200 	Text Hypothesis :	and    the ***** ********** * first time that   euro 2020  is the beer company
2024-02-11 18:36:50,200 	Text Alignment  :	S          D     D          D S     S    S      S    S     S      I    S      
2024-02-11 18:36:50,201 ========================================================================================================================
2024-02-11 18:36:50,201 Logging Sequence: 96_110.00
2024-02-11 18:36:50,201 	Gloss Reference :	A B+C+D+E
2024-02-11 18:36:50,201 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:36:50,201 	Gloss Alignment :	         
2024-02-11 18:36:50,201 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:36:50,202 	Text Reference  :	***** *** ***** viewers were ******* *** extremely distressed
2024-02-11 18:36:50,202 	Text Hypothesis :	after the match india   were waiting for india's   loss      
2024-02-11 18:36:50,202 	Text Alignment  :	I     I   I     S            I       I   S         S         
2024-02-11 18:36:50,202 ========================================================================================================================
2024-02-11 18:36:50,202 Logging Sequence: 160_92.00
2024-02-11 18:36:50,202 	Gloss Reference :	A B+C+D+E
2024-02-11 18:36:50,203 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:36:50,203 	Gloss Alignment :	         
2024-02-11 18:36:50,203 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:36:50,204 	Text Reference  :	you are all aware that india south      africa     are     playing a   test odi  series on 26  december
2024-02-11 18:36:50,204 	Text Hypothesis :	*** *** *** ***** **** now   australian cricketers matthew wade    and now  only bronze at the 2000    
2024-02-11 18:36:50,204 	Text Alignment  :	D   D   D   D     D    S     S          S          S       S       S   S    S    S      S  S   S       
2024-02-11 18:36:50,204 ========================================================================================================================
2024-02-11 18:36:50,205 Logging Sequence: 145_222.00
2024-02-11 18:36:50,205 	Gloss Reference :	A B+C+D+E
2024-02-11 18:36:50,205 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:36:50,205 	Gloss Alignment :	         
2024-02-11 18:36:50,205 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:36:50,206 	Text Reference  :	***** ********* *** the **** mp     also  gave an   assurance to     sponsor   barwin' trip 
2024-02-11 18:36:50,206 	Text Hypothesis :	since pathirana was the only female dhoni to   bowl the       sports authority of      india
2024-02-11 18:36:50,206 	Text Alignment  :	I     I         I       I    S      S     S    S    S         S      S         S       S    
2024-02-11 18:36:50,206 ========================================================================================================================
2024-02-11 18:36:50,206 Logging Sequence: 152_167.00
2024-02-11 18:36:50,207 	Gloss Reference :	A B+C+D+E
2024-02-11 18:36:50,207 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:36:50,207 	Gloss Alignment :	         
2024-02-11 18:36:50,207 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:36:50,208 	Text Reference  :	after the match virat showed his sportsmanship by      shaking hands      with     batsmen mohammad rizwan babar azam 
2024-02-11 18:36:50,208 	Text Hypothesis :	***** *** ***** ***** ****** *** ************* however zabka   gracefully declined to      wear     a      gold  medal
2024-02-11 18:36:50,208 	Text Alignment  :	D     D   D     D     D      D   D             S       S       S          S        S       S        S      S     S    
2024-02-11 18:36:50,208 ========================================================================================================================
2024-02-11 18:36:55,573 Epoch 121: Total Training Recognition Loss 0.11  Total Training Translation Loss 40.98 
2024-02-11 18:36:55,574 EPOCH 122
2024-02-11 18:36:55,935 [Epoch: 122 Step: 00016100] Batch Recognition Loss:   0.000433 => Gls Tokens per Sec:     1551 || Batch Translation Loss:   0.610345 => Txt Tokens per Sec:     4238 || Lr: 0.000100
2024-02-11 18:37:01,497 [Epoch: 122 Step: 00016200] Batch Recognition Loss:   0.000618 => Gls Tokens per Sec:     1439 || Batch Translation Loss:   0.200683 => Txt Tokens per Sec:     4027 || Lr: 0.000100
2024-02-11 18:37:03,040 Epoch 122: Total Training Recognition Loss 0.11  Total Training Translation Loss 46.53 
2024-02-11 18:37:03,040 EPOCH 123
2024-02-11 18:37:07,315 [Epoch: 123 Step: 00016300] Batch Recognition Loss:   0.000456 => Gls Tokens per Sec:     1385 || Batch Translation Loss:   0.694560 => Txt Tokens per Sec:     3865 || Lr: 0.000100
2024-02-11 18:37:10,484 Epoch 123: Total Training Recognition Loss 0.09  Total Training Translation Loss 44.61 
2024-02-11 18:37:10,484 EPOCH 124
2024-02-11 18:37:12,973 [Epoch: 124 Step: 00016400] Batch Recognition Loss:   0.000366 => Gls Tokens per Sec:     1319 || Batch Translation Loss:   0.207966 => Txt Tokens per Sec:     3680 || Lr: 0.000100
2024-02-11 18:37:18,418 Epoch 124: Total Training Recognition Loss 0.09  Total Training Translation Loss 41.94 
2024-02-11 18:37:18,419 EPOCH 125
2024-02-11 18:37:18,993 [Epoch: 125 Step: 00016500] Batch Recognition Loss:   0.000264 => Gls Tokens per Sec:     1115 || Batch Translation Loss:   0.175655 => Txt Tokens per Sec:     3653 || Lr: 0.000100
2024-02-11 18:37:24,047 [Epoch: 125 Step: 00016600] Batch Recognition Loss:   0.000467 => Gls Tokens per Sec:     1581 || Batch Translation Loss:   0.712628 => Txt Tokens per Sec:     4301 || Lr: 0.000100
2024-02-11 18:37:25,570 Epoch 125: Total Training Recognition Loss 0.08  Total Training Translation Loss 36.80 
2024-02-11 18:37:25,570 EPOCH 126
2024-02-11 18:37:30,108 [Epoch: 126 Step: 00016700] Batch Recognition Loss:   0.002253 => Gls Tokens per Sec:     1323 || Batch Translation Loss:   0.101496 => Txt Tokens per Sec:     3689 || Lr: 0.000100
2024-02-11 18:37:33,562 Epoch 126: Total Training Recognition Loss 0.08  Total Training Translation Loss 33.54 
2024-02-11 18:37:33,563 EPOCH 127
2024-02-11 18:37:35,661 [Epoch: 127 Step: 00016800] Batch Recognition Loss:   0.001387 => Gls Tokens per Sec:     1602 || Batch Translation Loss:   0.209009 => Txt Tokens per Sec:     4408 || Lr: 0.000100
2024-02-11 18:37:40,763 Epoch 127: Total Training Recognition Loss 0.09  Total Training Translation Loss 39.51 
2024-02-11 18:37:40,764 EPOCH 128
2024-02-11 18:37:41,269 [Epoch: 128 Step: 00016900] Batch Recognition Loss:   0.000602 => Gls Tokens per Sec:     1429 || Batch Translation Loss:   0.227887 => Txt Tokens per Sec:     3768 || Lr: 0.000100
2024-02-11 18:37:46,938 [Epoch: 128 Step: 00017000] Batch Recognition Loss:   0.000627 => Gls Tokens per Sec:     1411 || Batch Translation Loss:   0.669622 => Txt Tokens per Sec:     3953 || Lr: 0.000100
2024-02-11 18:37:48,334 Epoch 128: Total Training Recognition Loss 0.09  Total Training Translation Loss 41.39 
2024-02-11 18:37:48,334 EPOCH 129
2024-02-11 18:37:52,524 [Epoch: 129 Step: 00017100] Batch Recognition Loss:   0.000429 => Gls Tokens per Sec:     1451 || Batch Translation Loss:   0.165199 => Txt Tokens per Sec:     3969 || Lr: 0.000100
2024-02-11 18:37:55,951 Epoch 129: Total Training Recognition Loss 0.10  Total Training Translation Loss 42.73 
2024-02-11 18:37:55,951 EPOCH 130
2024-02-11 18:37:58,373 [Epoch: 130 Step: 00017200] Batch Recognition Loss:   0.000658 => Gls Tokens per Sec:     1421 || Batch Translation Loss:   0.266132 => Txt Tokens per Sec:     4050 || Lr: 0.000100
2024-02-11 18:38:02,956 Epoch 130: Total Training Recognition Loss 0.11  Total Training Translation Loss 35.71 
2024-02-11 18:38:02,957 EPOCH 131
2024-02-11 18:38:03,568 [Epoch: 131 Step: 00017300] Batch Recognition Loss:   0.000660 => Gls Tokens per Sec:     1311 || Batch Translation Loss:   0.538894 => Txt Tokens per Sec:     3766 || Lr: 0.000100
2024-02-11 18:38:09,242 [Epoch: 131 Step: 00017400] Batch Recognition Loss:   0.000641 => Gls Tokens per Sec:     1410 || Batch Translation Loss:   0.182281 => Txt Tokens per Sec:     3895 || Lr: 0.000100
2024-02-11 18:38:10,643 Epoch 131: Total Training Recognition Loss 0.09  Total Training Translation Loss 35.45 
2024-02-11 18:38:10,643 EPOCH 132
2024-02-11 18:38:14,859 [Epoch: 132 Step: 00017500] Batch Recognition Loss:   0.000920 => Gls Tokens per Sec:     1462 || Batch Translation Loss:   0.446421 => Txt Tokens per Sec:     4061 || Lr: 0.000100
2024-02-11 18:38:18,204 Epoch 132: Total Training Recognition Loss 0.10  Total Training Translation Loss 35.52 
2024-02-11 18:38:18,205 EPOCH 133
2024-02-11 18:38:20,542 [Epoch: 133 Step: 00017600] Batch Recognition Loss:   0.000702 => Gls Tokens per Sec:     1507 || Batch Translation Loss:   0.357740 => Txt Tokens per Sec:     4177 || Lr: 0.000100
2024-02-11 18:38:25,901 Epoch 133: Total Training Recognition Loss 0.11  Total Training Translation Loss 34.50 
2024-02-11 18:38:25,901 EPOCH 134
2024-02-11 18:38:26,564 [Epoch: 134 Step: 00017700] Batch Recognition Loss:   0.000527 => Gls Tokens per Sec:     1329 || Batch Translation Loss:   0.433614 => Txt Tokens per Sec:     3846 || Lr: 0.000100
2024-02-11 18:38:32,199 [Epoch: 134 Step: 00017800] Batch Recognition Loss:   0.000517 => Gls Tokens per Sec:     1420 || Batch Translation Loss:   0.144517 => Txt Tokens per Sec:     3923 || Lr: 0.000100
2024-02-11 18:38:33,581 Epoch 134: Total Training Recognition Loss 0.09  Total Training Translation Loss 38.15 
2024-02-11 18:38:33,582 EPOCH 135
2024-02-11 18:38:38,236 [Epoch: 135 Step: 00017900] Batch Recognition Loss:   0.000192 => Gls Tokens per Sec:     1341 || Batch Translation Loss:   0.117670 => Txt Tokens per Sec:     3765 || Lr: 0.000100
2024-02-11 18:38:41,481 Epoch 135: Total Training Recognition Loss 0.07  Total Training Translation Loss 35.03 
2024-02-11 18:38:41,482 EPOCH 136
2024-02-11 18:38:43,661 [Epoch: 136 Step: 00018000] Batch Recognition Loss:   0.001730 => Gls Tokens per Sec:     1652 || Batch Translation Loss:   0.579890 => Txt Tokens per Sec:     4411 || Lr: 0.000100
2024-02-11 18:38:54,544 Validation result at epoch 136, step    18000: duration: 10.8820s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00393	Translation Loss: 91072.44531	PPL: 9871.22363
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.52	(BLEU-1: 11.63,	BLEU-2: 3.91,	BLEU-3: 1.37,	BLEU-4: 0.52)
	CHRF 16.85	ROUGE 10.09
2024-02-11 18:38:54,545 Logging Recognition and Translation Outputs
2024-02-11 18:38:54,545 ========================================================================================================================
2024-02-11 18:38:54,545 Logging Sequence: 176_102.00
2024-02-11 18:38:54,546 	Gloss Reference :	A B+C+D+E
2024-02-11 18:38:54,546 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:38:54,546 	Gloss Alignment :	         
2024-02-11 18:38:54,546 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:38:54,547 	Text Reference  :	he is the 2nd indian wrestler to   qualify for      the   finals in    the olympics after sushil kumar  
2024-02-11 18:38:54,547 	Text Hypothesis :	** ** *** *** did    you      know that    everyone loves lifted video and pakistan win   new    zealand
2024-02-11 18:38:54,548 	Text Alignment  :	D  D  D   D   S      S        S    S       S        S     S      S     S   S        S     S      S      
2024-02-11 18:38:54,548 ========================================================================================================================
2024-02-11 18:38:54,548 Logging Sequence: 176_2.00
2024-02-11 18:38:54,548 	Gloss Reference :	A B+C+D+E
2024-02-11 18:38:54,548 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:38:54,548 	Gloss Alignment :	         
2024-02-11 18:38:54,549 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:38:54,549 	Text Reference  :	the 2020 olympics are currently being held in  tokyo   japan
2024-02-11 18:38:54,549 	Text Hypothesis :	*** **** but      why too       is    how  how earlier well 
2024-02-11 18:38:54,549 	Text Alignment  :	D   D    S        S   S         S     S    S   S       S    
2024-02-11 18:38:54,550 ========================================================================================================================
2024-02-11 18:38:54,550 Logging Sequence: 60_46.00
2024-02-11 18:38:54,550 	Gloss Reference :	A B+C+D+E
2024-02-11 18:38:54,550 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:38:54,550 	Gloss Alignment :	         
2024-02-11 18:38:54,550 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:38:54,552 	Text Reference  :	a close friend andrew neophitou and warne' manager james erskine kept    calling warne but    there  was no       response from him     
2024-02-11 18:38:54,552 	Text Hypothesis :	* ***** ****** ****** ********* *** but    then    the   ball    leaving skipper rohit sharma taylor had revealed with     the  upcoming
2024-02-11 18:38:54,552 	Text Alignment  :	D D     D      D      D         D   S      S       S     S       S       S       S     S      S      S   S        S        S    S       
2024-02-11 18:38:54,552 ========================================================================================================================
2024-02-11 18:38:54,552 Logging Sequence: 143_138.00
2024-02-11 18:38:54,553 	Gloss Reference :	A B+C+D+E
2024-02-11 18:38:54,553 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:38:54,553 	Gloss Alignment :	         
2024-02-11 18:38:54,553 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:38:54,554 	Text Reference  :	ronaldo has   admitted that  his conduct   after the 9th april match was improper and agrees to         pay    the      fine
2024-02-11 18:38:54,554 	Text Hypothesis :	******* since stokes   broke his behaviour left  the *** ***** ***** *** ******** *** ****** manchester united football team
2024-02-11 18:38:54,554 	Text Alignment  :	D       S     S        S         S         S         D   D     D     D   D        D   D      S          S      S        S   
2024-02-11 18:38:54,555 ========================================================================================================================
2024-02-11 18:38:54,555 Logging Sequence: 148_49.00
2024-02-11 18:38:54,555 	Gloss Reference :	A B+C+D+E
2024-02-11 18:38:54,555 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:38:54,555 	Gloss Alignment :	         
2024-02-11 18:38:54,555 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:38:54,556 	Text Reference  :	then the 4th  over  was bowled by    mohammed siraj
2024-02-11 18:38:54,556 	Text Hypothesis :	in   the same overs is  all    posts as       well 
2024-02-11 18:38:54,556 	Text Alignment  :	S        S    S     S   S      S     S        S    
2024-02-11 18:38:54,556 ========================================================================================================================
2024-02-11 18:39:00,036 Epoch 136: Total Training Recognition Loss 0.08  Total Training Translation Loss 35.12 
2024-02-11 18:39:00,037 EPOCH 137
2024-02-11 18:39:00,779 [Epoch: 137 Step: 00018100] Batch Recognition Loss:   0.000288 => Gls Tokens per Sec:     1296 || Batch Translation Loss:   0.149604 => Txt Tokens per Sec:     3511 || Lr: 0.000100
2024-02-11 18:39:06,526 [Epoch: 137 Step: 00018200] Batch Recognition Loss:   0.000558 => Gls Tokens per Sec:     1390 || Batch Translation Loss:   0.258390 => Txt Tokens per Sec:     3884 || Lr: 0.000100
2024-02-11 18:39:07,735 Epoch 137: Total Training Recognition Loss 0.06  Total Training Translation Loss 30.42 
2024-02-11 18:39:07,735 EPOCH 138
2024-02-11 18:39:12,394 [Epoch: 138 Step: 00018300] Batch Recognition Loss:   0.000284 => Gls Tokens per Sec:     1357 || Batch Translation Loss:   0.208948 => Txt Tokens per Sec:     3788 || Lr: 0.000100
2024-02-11 18:39:15,410 Epoch 138: Total Training Recognition Loss 0.07  Total Training Translation Loss 26.86 
2024-02-11 18:39:15,411 EPOCH 139
2024-02-11 18:39:18,232 [Epoch: 139 Step: 00018400] Batch Recognition Loss:   0.000734 => Gls Tokens per Sec:     1305 || Batch Translation Loss:   0.090948 => Txt Tokens per Sec:     3698 || Lr: 0.000100
2024-02-11 18:39:23,032 Epoch 139: Total Training Recognition Loss 0.09  Total Training Translation Loss 37.35 
2024-02-11 18:39:23,032 EPOCH 140
2024-02-11 18:39:23,915 [Epoch: 140 Step: 00018500] Batch Recognition Loss:   0.000437 => Gls Tokens per Sec:     1180 || Batch Translation Loss:   0.073637 => Txt Tokens per Sec:     3070 || Lr: 0.000100
2024-02-11 18:39:29,951 [Epoch: 140 Step: 00018600] Batch Recognition Loss:   0.000298 => Gls Tokens per Sec:     1324 || Batch Translation Loss:   0.309478 => Txt Tokens per Sec:     3733 || Lr: 0.000100
2024-02-11 18:39:31,031 Epoch 140: Total Training Recognition Loss 0.08  Total Training Translation Loss 32.90 
2024-02-11 18:39:31,032 EPOCH 141
2024-02-11 18:39:35,933 [Epoch: 141 Step: 00018700] Batch Recognition Loss:   0.000527 => Gls Tokens per Sec:     1306 || Batch Translation Loss:   0.289012 => Txt Tokens per Sec:     3584 || Lr: 0.000100
2024-02-11 18:39:38,995 Epoch 141: Total Training Recognition Loss 0.12  Total Training Translation Loss 38.62 
2024-02-11 18:39:38,995 EPOCH 142
2024-02-11 18:39:41,579 [Epoch: 142 Step: 00018800] Batch Recognition Loss:   0.000218 => Gls Tokens per Sec:     1456 || Batch Translation Loss:   0.195064 => Txt Tokens per Sec:     4347 || Lr: 0.000100
2024-02-11 18:39:45,916 Epoch 142: Total Training Recognition Loss 0.13  Total Training Translation Loss 36.53 
2024-02-11 18:39:45,917 EPOCH 143
2024-02-11 18:39:46,724 [Epoch: 143 Step: 00018900] Batch Recognition Loss:   0.001122 => Gls Tokens per Sec:     1391 || Batch Translation Loss:   0.141647 => Txt Tokens per Sec:     3815 || Lr: 0.000100
2024-02-11 18:39:52,562 [Epoch: 143 Step: 00019000] Batch Recognition Loss:   0.000792 => Gls Tokens per Sec:     1370 || Batch Translation Loss:   0.162035 => Txt Tokens per Sec:     3822 || Lr: 0.000100
2024-02-11 18:39:53,621 Epoch 143: Total Training Recognition Loss 0.09  Total Training Translation Loss 31.11 
2024-02-11 18:39:53,621 EPOCH 144
2024-02-11 18:39:57,871 [Epoch: 144 Step: 00019100] Batch Recognition Loss:   0.000261 => Gls Tokens per Sec:     1525 || Batch Translation Loss:   0.101071 => Txt Tokens per Sec:     4201 || Lr: 0.000100
2024-02-11 18:40:00,952 Epoch 144: Total Training Recognition Loss 0.10  Total Training Translation Loss 33.56 
2024-02-11 18:40:00,953 EPOCH 145
2024-02-11 18:40:03,441 [Epoch: 145 Step: 00019200] Batch Recognition Loss:   0.000606 => Gls Tokens per Sec:     1544 || Batch Translation Loss:   0.288305 => Txt Tokens per Sec:     4147 || Lr: 0.000100
2024-02-11 18:40:08,369 Epoch 145: Total Training Recognition Loss 0.08  Total Training Translation Loss 39.46 
2024-02-11 18:40:08,369 EPOCH 146
2024-02-11 18:40:09,169 [Epoch: 146 Step: 00019300] Batch Recognition Loss:   0.000753 => Gls Tokens per Sec:     1502 || Batch Translation Loss:   0.195321 => Txt Tokens per Sec:     3941 || Lr: 0.000100
2024-02-11 18:40:14,754 [Epoch: 146 Step: 00019400] Batch Recognition Loss:   0.000191 => Gls Tokens per Sec:     1431 || Batch Translation Loss:   0.097091 => Txt Tokens per Sec:     4035 || Lr: 0.000100
2024-02-11 18:40:15,785 Epoch 146: Total Training Recognition Loss 0.09  Total Training Translation Loss 30.35 
2024-02-11 18:40:15,786 EPOCH 147
2024-02-11 18:40:20,388 [Epoch: 147 Step: 00019500] Batch Recognition Loss:   0.000355 => Gls Tokens per Sec:     1425 || Batch Translation Loss:   0.051447 => Txt Tokens per Sec:     3981 || Lr: 0.000100
2024-02-11 18:40:23,290 Epoch 147: Total Training Recognition Loss 0.10  Total Training Translation Loss 38.65 
2024-02-11 18:40:23,290 EPOCH 148
2024-02-11 18:40:25,898 [Epoch: 148 Step: 00019600] Batch Recognition Loss:   0.000240 => Gls Tokens per Sec:     1504 || Batch Translation Loss:   0.250354 => Txt Tokens per Sec:     4202 || Lr: 0.000100
2024-02-11 18:40:30,708 Epoch 148: Total Training Recognition Loss 0.08  Total Training Translation Loss 38.64 
2024-02-11 18:40:30,708 EPOCH 149
2024-02-11 18:40:31,573 [Epoch: 149 Step: 00019700] Batch Recognition Loss:   0.000370 => Gls Tokens per Sec:     1483 || Batch Translation Loss:   0.138761 => Txt Tokens per Sec:     4032 || Lr: 0.000100
2024-02-11 18:40:37,090 [Epoch: 149 Step: 00019800] Batch Recognition Loss:   0.000183 => Gls Tokens per Sec:     1450 || Batch Translation Loss:   0.186921 => Txt Tokens per Sec:     4055 || Lr: 0.000100
2024-02-11 18:40:38,239 Epoch 149: Total Training Recognition Loss 0.11  Total Training Translation Loss 33.25 
2024-02-11 18:40:38,240 EPOCH 150
2024-02-11 18:40:43,235 [Epoch: 150 Step: 00019900] Batch Recognition Loss:   0.000565 => Gls Tokens per Sec:     1330 || Batch Translation Loss:   0.342783 => Txt Tokens per Sec:     3731 || Lr: 0.000100
2024-02-11 18:40:46,287 Epoch 150: Total Training Recognition Loss 0.09  Total Training Translation Loss 31.06 
2024-02-11 18:40:46,288 EPOCH 151
2024-02-11 18:40:49,149 [Epoch: 151 Step: 00020000] Batch Recognition Loss:   0.000239 => Gls Tokens per Sec:     1398 || Batch Translation Loss:   0.124814 => Txt Tokens per Sec:     3846 || Lr: 0.000100
2024-02-11 18:40:59,949 Validation result at epoch 151, step    20000: duration: 10.7980s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00346	Translation Loss: 91549.89062	PPL: 10358.84473
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.69	(BLEU-1: 10.85,	BLEU-2: 3.63,	BLEU-3: 1.46,	BLEU-4: 0.69)
	CHRF 17.46	ROUGE 9.36
2024-02-11 18:40:59,950 Logging Recognition and Translation Outputs
2024-02-11 18:40:59,950 ========================================================================================================================
2024-02-11 18:40:59,950 Logging Sequence: 106_94.00
2024-02-11 18:40:59,950 	Gloss Reference :	A B+C+D+E
2024-02-11 18:40:59,950 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:40:59,950 	Gloss Alignment :	         
2024-02-11 18:40:59,950 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:40:59,952 	Text Reference  :	smriti mandhana's extremely good     batting   helped    india     win she scored 51    runs off  just 25     balls
2024-02-11 18:40:59,952 	Text Hypothesis :	****** the        sports    ministry suspended assistant secretary of  wfi vinod  tomar and  made the  indian goals
2024-02-11 18:40:59,952 	Text Alignment  :	D      S          S         S        S         S         S         S   S   S      S     S    S    S    S      S    
2024-02-11 18:40:59,952 ========================================================================================================================
2024-02-11 18:40:59,952 Logging Sequence: 164_435.00
2024-02-11 18:40:59,953 	Gloss Reference :	A B+C+D+E
2024-02-11 18:40:59,953 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:40:59,953 	Gloss Alignment :	         
2024-02-11 18:40:59,953 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:40:59,954 	Text Reference  :	one of the followers said     bcci   will        even        earn  more  than this
2024-02-11 18:40:59,954 	Text Hypothesis :	*** ** *** and       shocking famous grandmaster viswanathan anand comes just 50  
2024-02-11 18:40:59,954 	Text Alignment  :	D   D  D   S         S        S      S           S           S     S     S    S   
2024-02-11 18:40:59,954 ========================================================================================================================
2024-02-11 18:40:59,954 Logging Sequence: 86_45.00
2024-02-11 18:40:59,954 	Gloss Reference :	A B+C+D+E
2024-02-11 18:40:59,954 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:40:59,955 	Gloss Alignment :	         
2024-02-11 18:40:59,955 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:40:59,956 	Text Reference  :	yashpal first  became  famous when       in        1972  in the punjab schools
2024-02-11 18:40:59,956 	Text Hypothesis :	former  indian captain dilip  vengsarkar expressed shock at the ****** finals 
2024-02-11 18:40:59,956 	Text Alignment  :	S       S      S       S      S          S         S     S      D      S      
2024-02-11 18:40:59,956 ========================================================================================================================
2024-02-11 18:40:59,956 Logging Sequence: 113_196.00
2024-02-11 18:40:59,956 	Gloss Reference :	A B+C+D+E
2024-02-11 18:40:59,956 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:40:59,956 	Gloss Alignment :	         
2024-02-11 18:40:59,957 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:40:59,957 	Text Reference  :	infidelity being the reason        for separation is         just a  rumour sad     
2024-02-11 18:40:59,957 	Text Hypothesis :	********** it    is  disappointing lot of         cricketers to   be played pakistan
2024-02-11 18:40:59,958 	Text Alignment  :	D          S     S   S             S   S          S          S    S  S      S       
2024-02-11 18:40:59,958 ========================================================================================================================
2024-02-11 18:40:59,958 Logging Sequence: 105_139.00
2024-02-11 18:40:59,958 	Gloss Reference :	A B+C+D+E
2024-02-11 18:40:59,958 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:40:59,958 	Gloss Alignment :	         
2024-02-11 18:40:59,958 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:40:59,959 	Text Reference  :	** ************* *** and now  he   has  finally achieved his    dream   by  defeating carlsen
2024-02-11 18:40:59,959 	Text Hypothesis :	he congratualted him and many good luck for     the      people believe and the       loss   
2024-02-11 18:40:59,960 	Text Alignment  :	I  I             I       S    S    S    S       S        S      S       S   S         S      
2024-02-11 18:40:59,960 ========================================================================================================================
2024-02-11 18:41:04,865 Epoch 151: Total Training Recognition Loss 0.08  Total Training Translation Loss 24.27 
2024-02-11 18:41:04,866 EPOCH 152
2024-02-11 18:41:05,843 [Epoch: 152 Step: 00020100] Batch Recognition Loss:   0.000813 => Gls Tokens per Sec:     1393 || Batch Translation Loss:   0.109235 => Txt Tokens per Sec:     3879 || Lr: 0.000100
2024-02-11 18:41:11,622 [Epoch: 152 Step: 00020200] Batch Recognition Loss:   0.000812 => Gls Tokens per Sec:     1383 || Batch Translation Loss:   0.307282 => Txt Tokens per Sec:     3843 || Lr: 0.000100
2024-02-11 18:41:12,589 Epoch 152: Total Training Recognition Loss 0.07  Total Training Translation Loss 26.64 
2024-02-11 18:41:12,589 EPOCH 153
2024-02-11 18:41:17,674 [Epoch: 153 Step: 00020300] Batch Recognition Loss:   0.000339 => Gls Tokens per Sec:     1322 || Batch Translation Loss:   0.118490 => Txt Tokens per Sec:     3719 || Lr: 0.000100
2024-02-11 18:41:20,542 Epoch 153: Total Training Recognition Loss 0.07  Total Training Translation Loss 27.03 
2024-02-11 18:41:20,542 EPOCH 154
2024-02-11 18:41:23,300 [Epoch: 154 Step: 00020400] Batch Recognition Loss:   0.000232 => Gls Tokens per Sec:     1480 || Batch Translation Loss:   0.131506 => Txt Tokens per Sec:     3880 || Lr: 0.000100
2024-02-11 18:41:28,202 Epoch 154: Total Training Recognition Loss 0.08  Total Training Translation Loss 37.19 
2024-02-11 18:41:28,203 EPOCH 155
2024-02-11 18:41:29,230 [Epoch: 155 Step: 00020500] Batch Recognition Loss:   0.000225 => Gls Tokens per Sec:     1403 || Batch Translation Loss:   0.067695 => Txt Tokens per Sec:     3995 || Lr: 0.000100
2024-02-11 18:41:34,995 [Epoch: 155 Step: 00020600] Batch Recognition Loss:   0.000442 => Gls Tokens per Sec:     1388 || Batch Translation Loss:   0.335751 => Txt Tokens per Sec:     3844 || Lr: 0.000100
2024-02-11 18:41:35,945 Epoch 155: Total Training Recognition Loss 0.10  Total Training Translation Loss 38.54 
2024-02-11 18:41:35,945 EPOCH 156
2024-02-11 18:41:40,939 [Epoch: 156 Step: 00020700] Batch Recognition Loss:   0.000985 => Gls Tokens per Sec:     1362 || Batch Translation Loss:   0.547890 => Txt Tokens per Sec:     3799 || Lr: 0.000100
2024-02-11 18:41:43,573 Epoch 156: Total Training Recognition Loss 0.08  Total Training Translation Loss 38.91 
2024-02-11 18:41:43,573 EPOCH 157
2024-02-11 18:41:46,324 [Epoch: 157 Step: 00020800] Batch Recognition Loss:   0.000596 => Gls Tokens per Sec:     1513 || Batch Translation Loss:   0.160721 => Txt Tokens per Sec:     4208 || Lr: 0.000100
2024-02-11 18:41:50,908 Epoch 157: Total Training Recognition Loss 0.09  Total Training Translation Loss 33.41 
2024-02-11 18:41:50,909 EPOCH 158
2024-02-11 18:41:52,150 [Epoch: 158 Step: 00020900] Batch Recognition Loss:   0.000296 => Gls Tokens per Sec:     1227 || Batch Translation Loss:   0.208470 => Txt Tokens per Sec:     3319 || Lr: 0.000100
2024-02-11 18:41:57,850 [Epoch: 158 Step: 00021000] Batch Recognition Loss:   0.000719 => Gls Tokens per Sec:     1404 || Batch Translation Loss:   0.276347 => Txt Tokens per Sec:     3952 || Lr: 0.000100
2024-02-11 18:41:58,748 Epoch 158: Total Training Recognition Loss 0.12  Total Training Translation Loss 33.54 
2024-02-11 18:41:58,748 EPOCH 159
2024-02-11 18:42:03,403 [Epoch: 159 Step: 00021100] Batch Recognition Loss:   0.000640 => Gls Tokens per Sec:     1478 || Batch Translation Loss:   0.144885 => Txt Tokens per Sec:     4112 || Lr: 0.000100
2024-02-11 18:42:06,199 Epoch 159: Total Training Recognition Loss 0.08  Total Training Translation Loss 29.90 
2024-02-11 18:42:06,200 EPOCH 160
2024-02-11 18:42:09,036 [Epoch: 160 Step: 00021200] Batch Recognition Loss:   0.000264 => Gls Tokens per Sec:     1496 || Batch Translation Loss:   0.070072 => Txt Tokens per Sec:     4026 || Lr: 0.000100
2024-02-11 18:42:13,432 Epoch 160: Total Training Recognition Loss 0.08  Total Training Translation Loss 22.38 
2024-02-11 18:42:13,432 EPOCH 161
2024-02-11 18:42:14,491 [Epoch: 161 Step: 00021300] Batch Recognition Loss:   0.000244 => Gls Tokens per Sec:     1512 || Batch Translation Loss:   0.126556 => Txt Tokens per Sec:     4110 || Lr: 0.000100
2024-02-11 18:42:20,193 [Epoch: 161 Step: 00021400] Batch Recognition Loss:   0.000192 => Gls Tokens per Sec:     1401 || Batch Translation Loss:   0.122048 => Txt Tokens per Sec:     3867 || Lr: 0.000100
2024-02-11 18:42:20,961 Epoch 161: Total Training Recognition Loss 0.07  Total Training Translation Loss 29.28 
2024-02-11 18:42:20,961 EPOCH 162
2024-02-11 18:42:25,734 [Epoch: 162 Step: 00021500] Batch Recognition Loss:   0.000195 => Gls Tokens per Sec:     1459 || Batch Translation Loss:   0.132485 => Txt Tokens per Sec:     4067 || Lr: 0.000100
2024-02-11 18:42:28,386 Epoch 162: Total Training Recognition Loss 0.06  Total Training Translation Loss 27.49 
2024-02-11 18:42:28,387 EPOCH 163
2024-02-11 18:42:31,315 [Epoch: 163 Step: 00021600] Batch Recognition Loss:   0.000460 => Gls Tokens per Sec:     1476 || Batch Translation Loss:   0.141586 => Txt Tokens per Sec:     4135 || Lr: 0.000100
2024-02-11 18:42:35,819 Epoch 163: Total Training Recognition Loss 0.06  Total Training Translation Loss 26.22 
2024-02-11 18:42:35,820 EPOCH 164
2024-02-11 18:42:36,945 [Epoch: 164 Step: 00021700] Batch Recognition Loss:   0.000549 => Gls Tokens per Sec:     1495 || Batch Translation Loss:   0.318867 => Txt Tokens per Sec:     4194 || Lr: 0.000100
2024-02-11 18:42:42,743 [Epoch: 164 Step: 00021800] Batch Recognition Loss:   0.000213 => Gls Tokens per Sec:     1380 || Batch Translation Loss:   0.089656 => Txt Tokens per Sec:     3796 || Lr: 0.000100
2024-02-11 18:42:43,575 Epoch 164: Total Training Recognition Loss 0.07  Total Training Translation Loss 30.56 
2024-02-11 18:42:43,575 EPOCH 165
2024-02-11 18:42:48,469 [Epoch: 165 Step: 00021900] Batch Recognition Loss:   0.000274 => Gls Tokens per Sec:     1439 || Batch Translation Loss:   0.224526 => Txt Tokens per Sec:     3954 || Lr: 0.000100
2024-02-11 18:42:50,893 Epoch 165: Total Training Recognition Loss 0.08  Total Training Translation Loss 31.90 
2024-02-11 18:42:50,893 EPOCH 166
2024-02-11 18:42:53,846 [Epoch: 166 Step: 00022000] Batch Recognition Loss:   0.000244 => Gls Tokens per Sec:     1491 || Batch Translation Loss:   0.501824 => Txt Tokens per Sec:     4140 || Lr: 0.000100
2024-02-11 18:43:04,832 Validation result at epoch 166, step    22000: duration: 10.9853s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00275	Translation Loss: 94931.70312	PPL: 14575.90137
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.67	(BLEU-1: 10.73,	BLEU-2: 3.65,	BLEU-3: 1.40,	BLEU-4: 0.67)
	CHRF 16.77	ROUGE 9.22
2024-02-11 18:43:04,834 Logging Recognition and Translation Outputs
2024-02-11 18:43:04,834 ========================================================================================================================
2024-02-11 18:43:04,834 Logging Sequence: 123_18.00
2024-02-11 18:43:04,834 	Gloss Reference :	A B+C+D+E
2024-02-11 18:43:04,834 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:43:04,834 	Gloss Alignment :	         
2024-02-11 18:43:04,834 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:43:04,835 	Text Reference  :	*** *** ***** dhoni  is *** ***** **** *** ******* **** *** *** ********** a  once-in-a-generation player
2024-02-11 18:43:04,835 	Text Hypothesis :	now you might wonder is the first time and started from all the collection of their                umpire
2024-02-11 18:43:04,835 	Text Alignment  :	I   I   I     S         I   I     I    I   I       I    I   I   I          S  S                    S     
2024-02-11 18:43:04,835 ========================================================================================================================
2024-02-11 18:43:04,836 Logging Sequence: 128_212.00
2024-02-11 18:43:04,836 	Gloss Reference :	A B+C+D+E
2024-02-11 18:43:04,836 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:43:04,836 	Gloss Alignment :	         
2024-02-11 18:43:04,836 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:43:04,837 	Text Reference  :	*** *** ** *** * **** people trolled him  for being absentminded
2024-02-11 18:43:04,837 	Text Hypothesis :	the all of the 4 test team   was     held in  all   out         
2024-02-11 18:43:04,837 	Text Alignment  :	I   I   I  I   I I    S      S       S    S   S     S           
2024-02-11 18:43:04,837 ========================================================================================================================
2024-02-11 18:43:04,837 Logging Sequence: 154_124.00
2024-02-11 18:43:04,837 	Gloss Reference :	A B+C+D+E
2024-02-11 18:43:04,838 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:43:04,838 	Gloss Alignment :	         
2024-02-11 18:43:04,838 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:43:04,839 	Text Reference  :	another problem is that because of ipl and t20 world cup many   players   may   not  be available for the matches
2024-02-11 18:43:04,839 	Text Hypothesis :	******* ******* ** the  case    of *** *** *** ***** *** junior wimbledon which will be held      in  an  earlier
2024-02-11 18:43:04,839 	Text Alignment  :	D       D       D  S    S          D   D   D   D     D   S      S         S     S       S         S   S   S      
2024-02-11 18:43:04,840 ========================================================================================================================
2024-02-11 18:43:04,840 Logging Sequence: 153_189.00
2024-02-11 18:43:04,840 	Gloss Reference :	A B+C+D+E
2024-02-11 18:43:04,840 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:43:04,840 	Gloss Alignment :	         
2024-02-11 18:43:04,840 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:43:04,842 	Text Reference  :	in 1992 the   final  was **** played between pakistan england and now in 2022 the same  has happened
2024-02-11 18:43:04,842 	Text Hypothesis :	** **** later rooney was just proud  of      her      career  and *** ** **** *** ended its innings 
2024-02-11 18:43:04,842 	Text Alignment  :	D  D    S     S          I    S      S       S        S           D   D  D    D   S     S   S       
2024-02-11 18:43:04,842 ========================================================================================================================
2024-02-11 18:43:04,842 Logging Sequence: 148_250.00
2024-02-11 18:43:04,842 	Gloss Reference :	A B+C+D+E
2024-02-11 18:43:04,842 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:43:04,842 	Gloss Alignment :	         
2024-02-11 18:43:04,843 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:43:04,844 	Text Reference  :	like there is   bcci there  is     asian cricket council acc      whose chief is jay shah
2024-02-11 18:43:04,844 	Text Hypothesis :	**** later with a    single person to    have    lifted  everyone was   out   of rs  250 
2024-02-11 18:43:04,844 	Text Alignment  :	D    S     S    S    S      S      S     S       S       S        S     S     S  S   S   
2024-02-11 18:43:04,844 ========================================================================================================================
2024-02-11 18:43:09,267 Epoch 166: Total Training Recognition Loss 0.07  Total Training Translation Loss 30.84 
2024-02-11 18:43:09,268 EPOCH 167
2024-02-11 18:43:10,562 [Epoch: 167 Step: 00022100] Batch Recognition Loss:   0.000227 => Gls Tokens per Sec:     1361 || Batch Translation Loss:   0.060220 => Txt Tokens per Sec:     3834 || Lr: 0.000100
2024-02-11 18:43:16,353 [Epoch: 167 Step: 00022200] Batch Recognition Loss:   0.000420 => Gls Tokens per Sec:     1380 || Batch Translation Loss:   0.182277 => Txt Tokens per Sec:     3819 || Lr: 0.000100
2024-02-11 18:43:17,013 Epoch 167: Total Training Recognition Loss 0.06  Total Training Translation Loss 29.07 
2024-02-11 18:43:17,014 EPOCH 168
2024-02-11 18:43:21,680 [Epoch: 168 Step: 00022300] Batch Recognition Loss:   0.000334 => Gls Tokens per Sec:     1526 || Batch Translation Loss:   0.330601 => Txt Tokens per Sec:     4166 || Lr: 0.000100
2024-02-11 18:43:24,400 Epoch 168: Total Training Recognition Loss 0.06  Total Training Translation Loss 31.55 
2024-02-11 18:43:24,400 EPOCH 169
2024-02-11 18:43:27,629 [Epoch: 169 Step: 00022400] Batch Recognition Loss:   0.000444 => Gls Tokens per Sec:     1388 || Batch Translation Loss:   0.067271 => Txt Tokens per Sec:     3976 || Lr: 0.000100
2024-02-11 18:43:32,071 Epoch 169: Total Training Recognition Loss 0.08  Total Training Translation Loss 28.21 
2024-02-11 18:43:32,072 EPOCH 170
2024-02-11 18:43:33,346 [Epoch: 170 Step: 00022500] Batch Recognition Loss:   0.000344 => Gls Tokens per Sec:     1447 || Batch Translation Loss:   0.546947 => Txt Tokens per Sec:     3988 || Lr: 0.000100
2024-02-11 18:43:39,477 [Epoch: 170 Step: 00022600] Batch Recognition Loss:   0.000289 => Gls Tokens per Sec:     1303 || Batch Translation Loss:   0.514351 => Txt Tokens per Sec:     3629 || Lr: 0.000100
2024-02-11 18:43:39,991 Epoch 170: Total Training Recognition Loss 0.07  Total Training Translation Loss 31.67 
2024-02-11 18:43:39,992 EPOCH 171
2024-02-11 18:43:45,149 [Epoch: 171 Step: 00022700] Batch Recognition Loss:   0.000490 => Gls Tokens per Sec:     1397 || Batch Translation Loss:   0.199159 => Txt Tokens per Sec:     3846 || Lr: 0.000100
2024-02-11 18:43:47,617 Epoch 171: Total Training Recognition Loss 0.07  Total Training Translation Loss 29.39 
2024-02-11 18:43:47,618 EPOCH 172
2024-02-11 18:43:50,753 [Epoch: 172 Step: 00022800] Batch Recognition Loss:   0.000129 => Gls Tokens per Sec:     1455 || Batch Translation Loss:   0.080940 => Txt Tokens per Sec:     3942 || Lr: 0.000100
2024-02-11 18:43:55,260 Epoch 172: Total Training Recognition Loss 0.08  Total Training Translation Loss 32.56 
2024-02-11 18:43:55,260 EPOCH 173
2024-02-11 18:43:56,715 [Epoch: 173 Step: 00022900] Batch Recognition Loss:   0.001137 => Gls Tokens per Sec:     1320 || Batch Translation Loss:   0.211997 => Txt Tokens per Sec:     3688 || Lr: 0.000100
2024-02-11 18:44:02,742 [Epoch: 173 Step: 00023000] Batch Recognition Loss:   0.000398 => Gls Tokens per Sec:     1326 || Batch Translation Loss:   0.256473 => Txt Tokens per Sec:     3634 || Lr: 0.000100
2024-02-11 18:44:03,308 Epoch 173: Total Training Recognition Loss 0.06  Total Training Translation Loss 24.89 
2024-02-11 18:44:03,309 EPOCH 174
2024-02-11 18:44:08,205 [Epoch: 174 Step: 00023100] Batch Recognition Loss:   0.002056 => Gls Tokens per Sec:     1487 || Batch Translation Loss:   0.179402 => Txt Tokens per Sec:     4099 || Lr: 0.000100
2024-02-11 18:44:10,598 Epoch 174: Total Training Recognition Loss 0.09  Total Training Translation Loss 26.92 
2024-02-11 18:44:10,598 EPOCH 175
2024-02-11 18:44:14,123 [Epoch: 175 Step: 00023200] Batch Recognition Loss:   0.000298 => Gls Tokens per Sec:     1317 || Batch Translation Loss:   0.182093 => Txt Tokens per Sec:     3697 || Lr: 0.000100
2024-02-11 18:44:18,197 Epoch 175: Total Training Recognition Loss 0.07  Total Training Translation Loss 24.84 
2024-02-11 18:44:18,197 EPOCH 176
2024-02-11 18:44:19,461 [Epoch: 176 Step: 00023300] Batch Recognition Loss:   0.000348 => Gls Tokens per Sec:     1584 || Batch Translation Loss:   0.102732 => Txt Tokens per Sec:     4264 || Lr: 0.000100
2024-02-11 18:44:25,086 [Epoch: 176 Step: 00023400] Batch Recognition Loss:   0.000395 => Gls Tokens per Sec:     1420 || Batch Translation Loss:   0.141509 => Txt Tokens per Sec:     4006 || Lr: 0.000100
2024-02-11 18:44:25,436 Epoch 176: Total Training Recognition Loss 0.08  Total Training Translation Loss 22.48 
2024-02-11 18:44:25,436 EPOCH 177
2024-02-11 18:44:30,131 [Epoch: 177 Step: 00023500] Batch Recognition Loss:   0.000569 => Gls Tokens per Sec:     1568 || Batch Translation Loss:   0.052240 => Txt Tokens per Sec:     4343 || Lr: 0.000100
2024-02-11 18:44:32,515 Epoch 177: Total Training Recognition Loss 0.07  Total Training Translation Loss 28.47 
2024-02-11 18:44:32,516 EPOCH 178
2024-02-11 18:44:35,969 [Epoch: 178 Step: 00023600] Batch Recognition Loss:   0.000649 => Gls Tokens per Sec:     1367 || Batch Translation Loss:   0.221625 => Txt Tokens per Sec:     3826 || Lr: 0.000100
2024-02-11 18:44:40,005 Epoch 178: Total Training Recognition Loss 0.08  Total Training Translation Loss 30.62 
2024-02-11 18:44:40,005 EPOCH 179
2024-02-11 18:44:41,228 [Epoch: 179 Step: 00023700] Batch Recognition Loss:   0.000551 => Gls Tokens per Sec:     1704 || Batch Translation Loss:   0.108050 => Txt Tokens per Sec:     4644 || Lr: 0.000100
2024-02-11 18:44:47,123 [Epoch: 179 Step: 00023800] Batch Recognition Loss:   0.000329 => Gls Tokens per Sec:     1356 || Batch Translation Loss:   0.028005 => Txt Tokens per Sec:     3777 || Lr: 0.000100
2024-02-11 18:44:47,506 Epoch 179: Total Training Recognition Loss 0.09  Total Training Translation Loss 32.23 
2024-02-11 18:44:47,506 EPOCH 180
2024-02-11 18:44:52,856 [Epoch: 180 Step: 00023900] Batch Recognition Loss:   0.000765 => Gls Tokens per Sec:     1391 || Batch Translation Loss:   0.513018 => Txt Tokens per Sec:     3895 || Lr: 0.000100
2024-02-11 18:44:55,042 Epoch 180: Total Training Recognition Loss 0.08  Total Training Translation Loss 33.07 
2024-02-11 18:44:55,042 EPOCH 181
2024-02-11 18:44:58,032 [Epoch: 181 Step: 00024000] Batch Recognition Loss:   0.000603 => Gls Tokens per Sec:     1606 || Batch Translation Loss:   0.068114 => Txt Tokens per Sec:     4461 || Lr: 0.000100
2024-02-11 18:45:09,258 Validation result at epoch 181, step    24000: duration: 11.2255s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00269	Translation Loss: 94119.53906	PPL: 13428.08984
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.62	(BLEU-1: 11.29,	BLEU-2: 3.58,	BLEU-3: 1.36,	BLEU-4: 0.62)
	CHRF 17.06	ROUGE 9.92
2024-02-11 18:45:09,259 Logging Recognition and Translation Outputs
2024-02-11 18:45:09,259 ========================================================================================================================
2024-02-11 18:45:09,260 Logging Sequence: 113_27.00
2024-02-11 18:45:09,260 	Gloss Reference :	A B+C+D+E
2024-02-11 18:45:09,260 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:45:09,260 	Gloss Alignment :	         
2024-02-11 18:45:09,260 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:45:09,262 	Text Reference  :	***** **** **** the couple **** met   in      australia for  the  first time   and ******** *************** ** *** fell   in love
2024-02-11 18:45:09,262 	Text Hypothesis :	after this post the couple were going batting where     they will be    played and pakistan congratulations on 4th living in 2022
2024-02-11 18:45:09,262 	Text Alignment  :	I     I    I               I    S     S       S         S    S    S     S          I        I               I  I   S         S   
2024-02-11 18:45:09,262 ========================================================================================================================
2024-02-11 18:45:09,262 Logging Sequence: 84_98.00
2024-02-11 18:45:09,262 	Gloss Reference :	A B+C+D+E
2024-02-11 18:45:09,263 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:45:09,263 	Gloss Alignment :	         
2024-02-11 18:45:09,263 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:45:09,263 	Text Reference  :	everyone is expected to ***** follow    the   law
2024-02-11 18:45:09,263 	Text Hypothesis :	******** ** ******** to raise awareness about it 
2024-02-11 18:45:09,263 	Text Alignment  :	D        D  D           I     S         S     S  
2024-02-11 18:45:09,264 ========================================================================================================================
2024-02-11 18:45:09,264 Logging Sequence: 125_82.00
2024-02-11 18:45:09,264 	Gloss Reference :	A B+C+D+E
2024-02-11 18:45:09,264 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:45:09,264 	Gloss Alignment :	         
2024-02-11 18:45:09,264 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:45:09,265 	Text Reference  :	**** **** ** *********** many such comments went viral
2024-02-11 18:45:09,265 	Text Hypothesis :	they will be quarantined for  days after    the  media
2024-02-11 18:45:09,265 	Text Alignment  :	I    I    I  I           S    S    S        S    S    
2024-02-11 18:45:09,265 ========================================================================================================================
2024-02-11 18:45:09,265 Logging Sequence: 60_124.00
2024-02-11 18:45:09,266 	Gloss Reference :	A B+C+D+E
2024-02-11 18:45:09,266 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:45:09,266 	Gloss Alignment :	         
2024-02-11 18:45:09,266 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:45:09,267 	Text Reference  :	in ******** *** **** ******* ** *** *** spin -   there are  2  types of  spin  
2024-02-11 18:45:09,267 	Text Hypothesis :	in leg-spin the ball bounces on the leg side and then  goes on to    the stumps
2024-02-11 18:45:09,267 	Text Alignment  :	   I        I   I    I       I  I   I   S    S   S     S    S  S     S   S     
2024-02-11 18:45:09,267 ========================================================================================================================
2024-02-11 18:45:09,267 Logging Sequence: 149_157.00
2024-02-11 18:45:09,268 	Gloss Reference :	A B+C+D+E
2024-02-11 18:45:09,268 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:45:09,268 	Gloss Alignment :	         
2024-02-11 18:45:09,268 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:45:09,269 	Text Reference  :	like india has     bcci sri lanka     cricket slc is      sri lanka's board which issued a statement confirming that danushka has been arrested
2024-02-11 18:45:09,269 	Text Hypothesis :	**** ***** despite the  r   premadasa games   in  colombo sri ******* ***** ***** ****** * ********* ********** **** ******** *** **** lanka   
2024-02-11 18:45:09,269 	Text Alignment  :	D    D     S       S    S   S         S       S   S           D       D     D     D      D D         D          D    D        D   D    S       
2024-02-11 18:45:09,270 ========================================================================================================================
2024-02-11 18:45:13,603 Epoch 181: Total Training Recognition Loss 0.07  Total Training Translation Loss 23.00 
2024-02-11 18:45:13,603 EPOCH 182
2024-02-11 18:45:15,180 [Epoch: 182 Step: 00024100] Batch Recognition Loss:   0.000286 => Gls Tokens per Sec:     1371 || Batch Translation Loss:   0.136571 => Txt Tokens per Sec:     3854 || Lr: 0.000100
2024-02-11 18:45:20,940 [Epoch: 182 Step: 00024200] Batch Recognition Loss:   0.000436 => Gls Tokens per Sec:     1387 || Batch Translation Loss:   0.078130 => Txt Tokens per Sec:     3862 || Lr: 0.000100
2024-02-11 18:45:21,241 Epoch 182: Total Training Recognition Loss 0.08  Total Training Translation Loss 22.57 
2024-02-11 18:45:21,242 EPOCH 183
2024-02-11 18:45:26,435 [Epoch: 183 Step: 00024300] Batch Recognition Loss:   0.000359 => Gls Tokens per Sec:     1448 || Batch Translation Loss:   0.307786 => Txt Tokens per Sec:     4013 || Lr: 0.000100
2024-02-11 18:45:28,731 Epoch 183: Total Training Recognition Loss 0.08  Total Training Translation Loss 27.21 
2024-02-11 18:45:28,732 EPOCH 184
2024-02-11 18:45:31,962 [Epoch: 184 Step: 00024400] Batch Recognition Loss:   0.002640 => Gls Tokens per Sec:     1511 || Batch Translation Loss:   0.123514 => Txt Tokens per Sec:     4132 || Lr: 0.000100
2024-02-11 18:45:35,877 Epoch 184: Total Training Recognition Loss 0.06  Total Training Translation Loss 26.83 
2024-02-11 18:45:35,877 EPOCH 185
2024-02-11 18:45:37,584 [Epoch: 185 Step: 00024500] Batch Recognition Loss:   0.000451 => Gls Tokens per Sec:     1313 || Batch Translation Loss:   0.152567 => Txt Tokens per Sec:     3844 || Lr: 0.000100
2024-02-11 18:45:43,397 [Epoch: 185 Step: 00024600] Batch Recognition Loss:   0.003505 => Gls Tokens per Sec:     1375 || Batch Translation Loss:   0.060588 => Txt Tokens per Sec:     3767 || Lr: 0.000100
2024-02-11 18:45:43,656 Epoch 185: Total Training Recognition Loss 0.09  Total Training Translation Loss 33.14 
2024-02-11 18:45:43,656 EPOCH 186
2024-02-11 18:45:48,627 [Epoch: 186 Step: 00024700] Batch Recognition Loss:   0.000273 => Gls Tokens per Sec:     1529 || Batch Translation Loss:   0.094594 => Txt Tokens per Sec:     4268 || Lr: 0.000100
2024-02-11 18:45:50,619 Epoch 186: Total Training Recognition Loss 0.09  Total Training Translation Loss 23.53 
2024-02-11 18:45:50,620 EPOCH 187
2024-02-11 18:45:54,277 [Epoch: 187 Step: 00024800] Batch Recognition Loss:   0.000618 => Gls Tokens per Sec:     1357 || Batch Translation Loss:   0.182623 => Txt Tokens per Sec:     3752 || Lr: 0.000100
2024-02-11 18:45:58,270 Epoch 187: Total Training Recognition Loss 0.08  Total Training Translation Loss 23.43 
2024-02-11 18:45:58,270 EPOCH 188
2024-02-11 18:45:59,780 [Epoch: 188 Step: 00024900] Batch Recognition Loss:   0.000429 => Gls Tokens per Sec:     1537 || Batch Translation Loss:   0.199277 => Txt Tokens per Sec:     4123 || Lr: 0.000100
2024-02-11 18:46:05,218 [Epoch: 188 Step: 00025000] Batch Recognition Loss:   0.000195 => Gls Tokens per Sec:     1471 || Batch Translation Loss:   0.225515 => Txt Tokens per Sec:     4127 || Lr: 0.000100
2024-02-11 18:46:05,495 Epoch 188: Total Training Recognition Loss 0.07  Total Training Translation Loss 23.00 
2024-02-11 18:46:05,495 EPOCH 189
2024-02-11 18:46:11,110 [Epoch: 189 Step: 00025100] Batch Recognition Loss:   0.000171 => Gls Tokens per Sec:     1368 || Batch Translation Loss:   0.059555 => Txt Tokens per Sec:     3800 || Lr: 0.000100
2024-02-11 18:46:13,116 Epoch 189: Total Training Recognition Loss 0.07  Total Training Translation Loss 25.45 
2024-02-11 18:46:13,116 EPOCH 190
2024-02-11 18:46:16,366 [Epoch: 190 Step: 00025200] Batch Recognition Loss:   0.000335 => Gls Tokens per Sec:     1552 || Batch Translation Loss:   0.188424 => Txt Tokens per Sec:     4290 || Lr: 0.000100
2024-02-11 18:46:20,594 Epoch 190: Total Training Recognition Loss 0.08  Total Training Translation Loss 26.68 
2024-02-11 18:46:20,594 EPOCH 191
2024-02-11 18:46:22,304 [Epoch: 191 Step: 00025300] Batch Recognition Loss:   0.000368 => Gls Tokens per Sec:     1405 || Batch Translation Loss:   0.447538 => Txt Tokens per Sec:     3940 || Lr: 0.000100
2024-02-11 18:46:28,202 [Epoch: 191 Step: 00025400] Batch Recognition Loss:   0.000359 => Gls Tokens per Sec:     1355 || Batch Translation Loss:   0.096666 => Txt Tokens per Sec:     3741 || Lr: 0.000100
2024-02-11 18:46:28,395 Epoch 191: Total Training Recognition Loss 0.09  Total Training Translation Loss 27.89 
2024-02-11 18:46:28,395 EPOCH 192
2024-02-11 18:46:34,069 [Epoch: 192 Step: 00025500] Batch Recognition Loss:   0.000459 => Gls Tokens per Sec:     1368 || Batch Translation Loss:   0.076121 => Txt Tokens per Sec:     3786 || Lr: 0.000100
2024-02-11 18:46:36,180 Epoch 192: Total Training Recognition Loss 0.06  Total Training Translation Loss 21.87 
2024-02-11 18:46:36,181 EPOCH 193
2024-02-11 18:46:39,802 [Epoch: 193 Step: 00025600] Batch Recognition Loss:   0.000261 => Gls Tokens per Sec:     1414 || Batch Translation Loss:   0.057100 => Txt Tokens per Sec:     3948 || Lr: 0.000100
2024-02-11 18:46:43,848 Epoch 193: Total Training Recognition Loss 0.06  Total Training Translation Loss 17.98 
2024-02-11 18:46:43,849 EPOCH 194
2024-02-11 18:46:45,618 [Epoch: 194 Step: 00025700] Batch Recognition Loss:   0.000576 => Gls Tokens per Sec:     1403 || Batch Translation Loss:   0.089110 => Txt Tokens per Sec:     4162 || Lr: 0.000100
2024-02-11 18:46:51,508 [Epoch: 194 Step: 00025800] Batch Recognition Loss:   0.000402 => Gls Tokens per Sec:     1357 || Batch Translation Loss:   0.272608 => Txt Tokens per Sec:     3683 || Lr: 0.000100
2024-02-11 18:46:51,629 Epoch 194: Total Training Recognition Loss 0.07  Total Training Translation Loss 23.10 
2024-02-11 18:46:51,630 EPOCH 195
2024-02-11 18:46:57,281 [Epoch: 195 Step: 00025900] Batch Recognition Loss:   0.001499 => Gls Tokens per Sec:     1388 || Batch Translation Loss:   0.185679 => Txt Tokens per Sec:     3901 || Lr: 0.000100
2024-02-11 18:46:59,298 Epoch 195: Total Training Recognition Loss 0.08  Total Training Translation Loss 35.55 
2024-02-11 18:46:59,298 EPOCH 196
2024-02-11 18:47:02,822 [Epoch: 196 Step: 00026000] Batch Recognition Loss:   0.000097 => Gls Tokens per Sec:     1476 || Batch Translation Loss:   0.203426 => Txt Tokens per Sec:     4140 || Lr: 0.000100
2024-02-11 18:47:13,794 Validation result at epoch 196, step    26000: duration: 10.9725s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00190	Translation Loss: 93355.56250	PPL: 12431.02734
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.52	(BLEU-1: 11.02,	BLEU-2: 3.25,	BLEU-3: 1.18,	BLEU-4: 0.52)
	CHRF 16.91	ROUGE 9.35
2024-02-11 18:47:13,795 Logging Recognition and Translation Outputs
2024-02-11 18:47:13,795 ========================================================================================================================
2024-02-11 18:47:13,796 Logging Sequence: 63_81.00
2024-02-11 18:47:13,796 	Gloss Reference :	A B+C+D+E
2024-02-11 18:47:13,796 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:47:13,796 	Gloss Alignment :	         
2024-02-11 18:47:13,796 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:47:13,798 	Text Reference  :	for example if 8 tenders are submitted 3 may be rejected and remaining 5    will     bid  for the 2 teams in  dubai on  25th      october
2024-02-11 18:47:13,798 	Text Hypothesis :	*** ******* ** * ******* *** ********* * *** ** however  the batters   were stepping down as  the * ***** end of    the 90-minute match  
2024-02-11 18:47:13,798 	Text Alignment  :	D   D       D  D D       D   D         D D   D  S        S   S         S    S        S    S       D D     S   S     S   S         S      
2024-02-11 18:47:13,798 ========================================================================================================================
2024-02-11 18:47:13,799 Logging Sequence: 158_56.00
2024-02-11 18:47:13,799 	Gloss Reference :	A B+C+D+E
2024-02-11 18:47:13,799 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:47:13,799 	Gloss Alignment :	         
2024-02-11 18:47:13,799 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:47:13,800 	Text Reference  :	********* ***** ********* lucknow's krunal pandya was dismissed by a     catch taken  by   virat kohli
2024-02-11 18:47:13,800 	Text Hypothesis :	germany's kohli recreated the       game   in     the ahmedabad we would be    sacked from the   team 
2024-02-11 18:47:13,800 	Text Alignment  :	I         I     I         S         S      S      S   S         S  S     S     S      S    S     S    
2024-02-11 18:47:13,801 ========================================================================================================================
2024-02-11 18:47:13,801 Logging Sequence: 117_123.00
2024-02-11 18:47:13,801 	Gloss Reference :	A B+C+D+E
2024-02-11 18:47:13,801 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:47:13,801 	Gloss Alignment :	         
2024-02-11 18:47:13,801 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:47:13,802 	Text Reference  :	krunal scored a total of 58  runs   off  31  balls   laced  with   seven 4's    and two 6's
2024-02-11 18:47:13,803 	Text Hypothesis :	****** ****** * ***** ** and walked away his brother hardik pandya then  hugged and put up 
2024-02-11 18:47:13,803 	Text Alignment  :	D      D      D D     D  S   S      S    S   S       S      S      S     S          S   S  
2024-02-11 18:47:13,803 ========================================================================================================================
2024-02-11 18:47:13,803 Logging Sequence: 123_14.00
2024-02-11 18:47:13,803 	Gloss Reference :	A B+C+D+E
2024-02-11 18:47:13,803 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:47:13,803 	Gloss Alignment :	         
2024-02-11 18:47:13,803 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:47:13,804 	Text Reference  :	he is very down to earth and amazing leader as   well 
2024-02-11 18:47:13,804 	Text Hypothesis :	** ** **** **** ** ***** and i       am     very proud
2024-02-11 18:47:13,804 	Text Alignment  :	D  D  D    D    D  D         S       S      S    S    
2024-02-11 18:47:13,805 ========================================================================================================================
2024-02-11 18:47:13,805 Logging Sequence: 61_181.00
2024-02-11 18:47:13,805 	Gloss Reference :	A B+C+D+E
2024-02-11 18:47:13,805 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:47:13,805 	Gloss Alignment :	         
2024-02-11 18:47:13,805 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:47:13,807 	Text Reference  :	one   other fan said  it   is babar's personal chat we      should focuc on   this cricketing career and not       his     personal life   
2024-02-11 18:47:13,807 	Text Hypothesis :	where do    you think this is ******* ******** a    reserve day    kohli will be   held       at     the ahemdabad stadium in       gujarat
2024-02-11 18:47:13,808 	Text Alignment  :	S     S     S   S     S       D       D        S    S       S      S     S    S    S          S      S   S         S       S        S      
2024-02-11 18:47:13,808 ========================================================================================================================
2024-02-11 18:47:17,795 Epoch 196: Total Training Recognition Loss 0.08  Total Training Translation Loss 31.54 
2024-02-11 18:47:17,796 EPOCH 197
2024-02-11 18:47:19,848 [Epoch: 197 Step: 00026100] Batch Recognition Loss:   0.000456 => Gls Tokens per Sec:     1248 || Batch Translation Loss:   0.123745 => Txt Tokens per Sec:     3760 || Lr: 0.000100
2024-02-11 18:47:25,668 [Epoch: 197 Step: 00026200] Batch Recognition Loss:   0.000397 => Gls Tokens per Sec:     1373 || Batch Translation Loss:   0.122274 => Txt Tokens per Sec:     3708 || Lr: 0.000100
2024-02-11 18:47:25,718 Epoch 197: Total Training Recognition Loss 0.08  Total Training Translation Loss 20.86 
2024-02-11 18:47:25,719 EPOCH 198
2024-02-11 18:47:30,853 [Epoch: 198 Step: 00026300] Batch Recognition Loss:   0.000532 => Gls Tokens per Sec:     1543 || Batch Translation Loss:   0.132143 => Txt Tokens per Sec:     4278 || Lr: 0.000100
2024-02-11 18:47:33,003 Epoch 198: Total Training Recognition Loss 0.05  Total Training Translation Loss 20.23 
2024-02-11 18:47:33,003 EPOCH 199
2024-02-11 18:47:36,796 [Epoch: 199 Step: 00026400] Batch Recognition Loss:   0.000184 => Gls Tokens per Sec:     1393 || Batch Translation Loss:   0.042763 => Txt Tokens per Sec:     3954 || Lr: 0.000100
2024-02-11 18:47:40,653 Epoch 199: Total Training Recognition Loss 0.06  Total Training Translation Loss 25.29 
2024-02-11 18:47:40,653 EPOCH 200
2024-02-11 18:47:42,441 [Epoch: 200 Step: 00026500] Batch Recognition Loss:   0.000324 => Gls Tokens per Sec:     1477 || Batch Translation Loss:   0.061773 => Txt Tokens per Sec:     4157 || Lr: 0.000100
2024-02-11 18:47:48,290 [Epoch: 200 Step: 00026600] Batch Recognition Loss:   0.000901 => Gls Tokens per Sec:     1366 || Batch Translation Loss:   0.096818 => Txt Tokens per Sec:     3779 || Lr: 0.000100
2024-02-11 18:47:48,290 Epoch 200: Total Training Recognition Loss 0.06  Total Training Translation Loss 20.22 
2024-02-11 18:47:48,290 EPOCH 201
2024-02-11 18:47:53,803 [Epoch: 201 Step: 00026700] Batch Recognition Loss:   0.000341 => Gls Tokens per Sec:     1451 || Batch Translation Loss:   0.187726 => Txt Tokens per Sec:     4017 || Lr: 0.000100
2024-02-11 18:47:55,578 Epoch 201: Total Training Recognition Loss 0.07  Total Training Translation Loss 19.70 
2024-02-11 18:47:55,579 EPOCH 202
2024-02-11 18:47:59,173 [Epoch: 202 Step: 00026800] Batch Recognition Loss:   0.000264 => Gls Tokens per Sec:     1491 || Batch Translation Loss:   0.340907 => Txt Tokens per Sec:     4224 || Lr: 0.000100
2024-02-11 18:48:03,085 Epoch 202: Total Training Recognition Loss 0.07  Total Training Translation Loss 22.82 
2024-02-11 18:48:03,086 EPOCH 203
2024-02-11 18:48:04,954 [Epoch: 203 Step: 00026900] Batch Recognition Loss:   0.000554 => Gls Tokens per Sec:     1457 || Batch Translation Loss:   1.015023 => Txt Tokens per Sec:     4028 || Lr: 0.000100
2024-02-11 18:48:10,719 Epoch 203: Total Training Recognition Loss 0.07  Total Training Translation Loss 29.19 
2024-02-11 18:48:10,719 EPOCH 204
2024-02-11 18:48:10,779 [Epoch: 204 Step: 00027000] Batch Recognition Loss:   0.000246 => Gls Tokens per Sec:     1356 || Batch Translation Loss:   0.140701 => Txt Tokens per Sec:     4864 || Lr: 0.000100
2024-02-11 18:48:16,259 [Epoch: 204 Step: 00027100] Batch Recognition Loss:   0.000412 => Gls Tokens per Sec:     1458 || Batch Translation Loss:   0.190667 => Txt Tokens per Sec:     4044 || Lr: 0.000100
2024-02-11 18:48:18,164 Epoch 204: Total Training Recognition Loss 0.08  Total Training Translation Loss 25.47 
2024-02-11 18:48:18,165 EPOCH 205
2024-02-11 18:48:21,697 [Epoch: 205 Step: 00027200] Batch Recognition Loss:   0.000461 => Gls Tokens per Sec:     1541 || Batch Translation Loss:   0.136867 => Txt Tokens per Sec:     4260 || Lr: 0.000100
2024-02-11 18:48:25,675 Epoch 205: Total Training Recognition Loss 0.05  Total Training Translation Loss 23.19 
2024-02-11 18:48:25,675 EPOCH 206
2024-02-11 18:48:27,578 [Epoch: 206 Step: 00027300] Batch Recognition Loss:   0.000983 => Gls Tokens per Sec:     1471 || Batch Translation Loss:   0.102848 => Txt Tokens per Sec:     4079 || Lr: 0.000100
2024-02-11 18:48:33,678 Epoch 206: Total Training Recognition Loss 0.06  Total Training Translation Loss 23.94 
2024-02-11 18:48:33,679 EPOCH 207
2024-02-11 18:48:33,766 [Epoch: 207 Step: 00027400] Batch Recognition Loss:   0.000407 => Gls Tokens per Sec:     1860 || Batch Translation Loss:   0.067528 => Txt Tokens per Sec:     3640 || Lr: 0.000100
2024-02-11 18:48:38,988 [Epoch: 207 Step: 00027500] Batch Recognition Loss:   0.000490 => Gls Tokens per Sec:     1532 || Batch Translation Loss:   0.697664 => Txt Tokens per Sec:     4285 || Lr: 0.000100
2024-02-11 18:48:40,789 Epoch 207: Total Training Recognition Loss 0.09  Total Training Translation Loss 28.10 
2024-02-11 18:48:40,789 EPOCH 208
2024-02-11 18:48:45,144 [Epoch: 208 Step: 00027600] Batch Recognition Loss:   0.000159 => Gls Tokens per Sec:     1268 || Batch Translation Loss:   0.175471 => Txt Tokens per Sec:     3596 || Lr: 0.000100
2024-02-11 18:48:48,650 Epoch 208: Total Training Recognition Loss 0.07  Total Training Translation Loss 26.43 
2024-02-11 18:48:48,651 EPOCH 209
2024-02-11 18:48:50,573 [Epoch: 209 Step: 00027700] Batch Recognition Loss:   0.000176 => Gls Tokens per Sec:     1498 || Batch Translation Loss:   0.097742 => Txt Tokens per Sec:     4092 || Lr: 0.000100
2024-02-11 18:48:56,283 Epoch 209: Total Training Recognition Loss 0.06  Total Training Translation Loss 23.46 
2024-02-11 18:48:56,284 EPOCH 210
2024-02-11 18:48:56,421 [Epoch: 210 Step: 00027800] Batch Recognition Loss:   0.000284 => Gls Tokens per Sec:     1765 || Batch Translation Loss:   0.118963 => Txt Tokens per Sec:     3809 || Lr: 0.000100
2024-02-11 18:49:01,925 [Epoch: 210 Step: 00027900] Batch Recognition Loss:   0.000199 => Gls Tokens per Sec:     1453 || Batch Translation Loss:   0.473618 => Txt Tokens per Sec:     4087 || Lr: 0.000100
2024-02-11 18:49:03,562 Epoch 210: Total Training Recognition Loss 0.07  Total Training Translation Loss 25.96 
2024-02-11 18:49:03,562 EPOCH 211
2024-02-11 18:49:07,191 [Epoch: 211 Step: 00028000] Batch Recognition Loss:   0.000174 => Gls Tokens per Sec:     1544 || Batch Translation Loss:   0.163650 => Txt Tokens per Sec:     4367 || Lr: 0.000100
2024-02-11 18:49:18,821 Validation result at epoch 211, step    28000: duration: 11.6283s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00254	Translation Loss: 92574.52344	PPL: 11488.17285
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.28	(BLEU-1: 10.36,	BLEU-2: 2.64,	BLEU-3: 0.77,	BLEU-4: 0.28)
	CHRF 17.17	ROUGE 8.27
2024-02-11 18:49:18,822 Logging Recognition and Translation Outputs
2024-02-11 18:49:18,822 ========================================================================================================================
2024-02-11 18:49:18,822 Logging Sequence: 164_197.00
2024-02-11 18:49:18,822 	Gloss Reference :	A B+C+D+E
2024-02-11 18:49:18,823 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:49:18,823 	Gloss Alignment :	         
2024-02-11 18:49:18,823 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:49:18,824 	Text Reference  :	viacom18    already had   the ***** digital rights of    telecasting ipl matches
2024-02-11 18:49:18,824 	Text Hypothesis :	calculating will    bring the total to      98400  balls in          5   years  
2024-02-11 18:49:18,824 	Text Alignment  :	S           S       S         I     S       S      S     S           S   S      
2024-02-11 18:49:18,824 ========================================================================================================================
2024-02-11 18:49:18,824 Logging Sequence: 121_124.00
2024-02-11 18:49:18,824 	Gloss Reference :	A B+C+D+E
2024-02-11 18:49:18,824 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:49:18,825 	Gloss Alignment :	         
2024-02-11 18:49:18,825 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:49:18,825 	Text Reference  :	but  some suspicious drugs were found in    her sample 
2024-02-11 18:49:18,825 	Text Hypothesis :	then the  gold       medal will be    given to  mirabai
2024-02-11 18:49:18,826 	Text Alignment  :	S    S    S          S     S    S     S     S   S      
2024-02-11 18:49:18,826 ========================================================================================================================
2024-02-11 18:49:18,826 Logging Sequence: 127_197.00
2024-02-11 18:49:18,826 	Gloss Reference :	A B+C+D+E
2024-02-11 18:49:18,826 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:49:18,826 	Gloss Alignment :	         
2024-02-11 18:49:18,826 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:49:18,828 	Text Reference  :	at wac indian men's 4x400m relay team    reached the     finals  however they   stood  5th in       the  finals and couldn't win a   medal
2024-02-11 18:49:18,829 	Text Hypothesis :	** *** ****** ***** ****** ***** however an      amazing bowling and     hardik pandya who recently when she    was excited  to  buy him  
2024-02-11 18:49:18,829 	Text Alignment  :	D  D   D      D     D      D     S       S       S       S       S       S      S      S   S        S    S      S   S        S   S   S    
2024-02-11 18:49:18,829 ========================================================================================================================
2024-02-11 18:49:18,829 Logging Sequence: 72_257.00
2024-02-11 18:49:18,829 	Gloss Reference :	A B+C+D+E
2024-02-11 18:49:18,829 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:49:18,829 	Gloss Alignment :	         
2024-02-11 18:49:18,829 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:49:18,831 	Text Reference  :	***** ***** *** ********** *** **** in *** ****** *** *** **** * ***** **** 2019    shaw   was suspended for a doping violation
2024-02-11 18:49:18,831 	Text Hypothesis :	later mirza was witnessing bad form in her career and she took a break from playing tennis in  2017      for a ****** surgery  
2024-02-11 18:49:18,831 	Text Alignment  :	I     I     I   I          I   I       I   I      I   I   I    I I     I    S       S      S   S               D      S        
2024-02-11 18:49:18,831 ========================================================================================================================
2024-02-11 18:49:18,831 Logging Sequence: 73_116.00
2024-02-11 18:49:18,831 	Gloss Reference :	A B+C+D+E
2024-02-11 18:49:18,831 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:49:18,832 	Gloss Alignment :	         
2024-02-11 18:49:18,832 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:49:18,833 	Text Reference  :	***** the       cheapest dish on   the **** ****** menu is    raita whereas the ******** ******* costliest dish  is   
2024-02-11 18:49:18,833 	Text Hypothesis :	raina resturant has      food from the rich spices of   north india to      the aromatic curries of        south india
2024-02-11 18:49:18,833 	Text Alignment  :	I     S         S        S    S        I    I      S    S     S     S           I        I       S         S     S    
2024-02-11 18:49:18,833 ========================================================================================================================
2024-02-11 18:49:22,476 Epoch 211: Total Training Recognition Loss 0.07  Total Training Translation Loss 28.98 
2024-02-11 18:49:22,476 EPOCH 212
2024-02-11 18:49:24,662 [Epoch: 212 Step: 00028100] Batch Recognition Loss:   0.000212 => Gls Tokens per Sec:     1355 || Batch Translation Loss:   0.086929 => Txt Tokens per Sec:     3822 || Lr: 0.000100
2024-02-11 18:49:30,370 Epoch 212: Total Training Recognition Loss 0.08  Total Training Translation Loss 30.35 
2024-02-11 18:49:30,371 EPOCH 213
2024-02-11 18:49:30,603 [Epoch: 213 Step: 00028200] Batch Recognition Loss:   0.000299 => Gls Tokens per Sec:     1391 || Batch Translation Loss:   0.484921 => Txt Tokens per Sec:     3874 || Lr: 0.000100
2024-02-11 18:49:36,231 [Epoch: 213 Step: 00028300] Batch Recognition Loss:   0.000281 => Gls Tokens per Sec:     1421 || Batch Translation Loss:   0.070464 => Txt Tokens per Sec:     3939 || Lr: 0.000100
2024-02-11 18:49:37,844 Epoch 213: Total Training Recognition Loss 0.08  Total Training Translation Loss 24.72 
2024-02-11 18:49:37,845 EPOCH 214
2024-02-11 18:49:41,952 [Epoch: 214 Step: 00028400] Batch Recognition Loss:   0.000368 => Gls Tokens per Sec:     1383 || Batch Translation Loss:   0.115069 => Txt Tokens per Sec:     3845 || Lr: 0.000100
2024-02-11 18:49:45,636 Epoch 214: Total Training Recognition Loss 0.07  Total Training Translation Loss 17.14 
2024-02-11 18:49:45,636 EPOCH 215
2024-02-11 18:49:47,790 [Epoch: 215 Step: 00028500] Batch Recognition Loss:   0.000507 => Gls Tokens per Sec:     1413 || Batch Translation Loss:   0.080286 => Txt Tokens per Sec:     3978 || Lr: 0.000100
2024-02-11 18:49:53,257 Epoch 215: Total Training Recognition Loss 0.07  Total Training Translation Loss 17.51 
2024-02-11 18:49:53,258 EPOCH 216
2024-02-11 18:49:53,509 [Epoch: 216 Step: 00028600] Batch Recognition Loss:   0.000545 => Gls Tokens per Sec:     1600 || Batch Translation Loss:   0.040739 => Txt Tokens per Sec:     4284 || Lr: 0.000100
2024-02-11 18:49:59,269 [Epoch: 216 Step: 00028700] Batch Recognition Loss:   0.000400 => Gls Tokens per Sec:     1389 || Batch Translation Loss:   0.062385 => Txt Tokens per Sec:     3901 || Lr: 0.000100
2024-02-11 18:50:00,805 Epoch 216: Total Training Recognition Loss 0.06  Total Training Translation Loss 21.77 
2024-02-11 18:50:00,805 EPOCH 217
2024-02-11 18:50:04,796 [Epoch: 217 Step: 00028800] Batch Recognition Loss:   0.000279 => Gls Tokens per Sec:     1444 || Batch Translation Loss:   0.041711 => Txt Tokens per Sec:     3972 || Lr: 0.000100
2024-02-11 18:50:08,420 Epoch 217: Total Training Recognition Loss 0.09  Total Training Translation Loss 19.24 
2024-02-11 18:50:08,421 EPOCH 218
2024-02-11 18:50:10,656 [Epoch: 218 Step: 00028900] Batch Recognition Loss:   0.000302 => Gls Tokens per Sec:     1397 || Batch Translation Loss:   0.183068 => Txt Tokens per Sec:     3844 || Lr: 0.000100
2024-02-11 18:50:15,997 Epoch 218: Total Training Recognition Loss 0.07  Total Training Translation Loss 23.51 
2024-02-11 18:50:15,997 EPOCH 219
2024-02-11 18:50:16,415 [Epoch: 219 Step: 00029000] Batch Recognition Loss:   0.000503 => Gls Tokens per Sec:     1154 || Batch Translation Loss:   0.148880 => Txt Tokens per Sec:     3252 || Lr: 0.000100
2024-02-11 18:50:22,195 [Epoch: 219 Step: 00029100] Batch Recognition Loss:   0.000298 => Gls Tokens per Sec:     1384 || Batch Translation Loss:   0.504446 => Txt Tokens per Sec:     3858 || Lr: 0.000100
2024-02-11 18:50:23,813 Epoch 219: Total Training Recognition Loss 0.06  Total Training Translation Loss 17.79 
2024-02-11 18:50:23,814 EPOCH 220
2024-02-11 18:50:27,752 [Epoch: 220 Step: 00029200] Batch Recognition Loss:   0.000307 => Gls Tokens per Sec:     1483 || Batch Translation Loss:   0.067972 => Txt Tokens per Sec:     4133 || Lr: 0.000100
2024-02-11 18:50:31,192 Epoch 220: Total Training Recognition Loss 0.05  Total Training Translation Loss 19.29 
2024-02-11 18:50:31,193 EPOCH 221
2024-02-11 18:50:33,699 [Epoch: 221 Step: 00029300] Batch Recognition Loss:   0.000386 => Gls Tokens per Sec:     1277 || Batch Translation Loss:   0.089220 => Txt Tokens per Sec:     3455 || Lr: 0.000100
2024-02-11 18:50:39,075 Epoch 221: Total Training Recognition Loss 0.06  Total Training Translation Loss 29.16 
2024-02-11 18:50:39,076 EPOCH 222
2024-02-11 18:50:39,575 [Epoch: 222 Step: 00029400] Batch Recognition Loss:   0.000467 => Gls Tokens per Sec:     1125 || Batch Translation Loss:   0.208137 => Txt Tokens per Sec:     3207 || Lr: 0.000100
2024-02-11 18:50:45,247 [Epoch: 222 Step: 00029500] Batch Recognition Loss:   0.000327 => Gls Tokens per Sec:     1411 || Batch Translation Loss:   0.228388 => Txt Tokens per Sec:     3887 || Lr: 0.000100
2024-02-11 18:50:46,846 Epoch 222: Total Training Recognition Loss 0.08  Total Training Translation Loss 32.14 
2024-02-11 18:50:46,846 EPOCH 223
2024-02-11 18:50:50,990 [Epoch: 223 Step: 00029600] Batch Recognition Loss:   0.000457 => Gls Tokens per Sec:     1429 || Batch Translation Loss:   0.061117 => Txt Tokens per Sec:     3904 || Lr: 0.000100
2024-02-11 18:50:54,150 Epoch 223: Total Training Recognition Loss 0.07  Total Training Translation Loss 24.30 
2024-02-11 18:50:54,150 EPOCH 224
2024-02-11 18:50:56,703 [Epoch: 224 Step: 00029700] Batch Recognition Loss:   0.000158 => Gls Tokens per Sec:     1285 || Batch Translation Loss:   0.033334 => Txt Tokens per Sec:     3562 || Lr: 0.000100
2024-02-11 18:51:02,161 Epoch 224: Total Training Recognition Loss 0.06  Total Training Translation Loss 16.80 
2024-02-11 18:51:02,162 EPOCH 225
2024-02-11 18:51:02,664 [Epoch: 225 Step: 00029800] Batch Recognition Loss:   0.000231 => Gls Tokens per Sec:     1280 || Batch Translation Loss:   0.338144 => Txt Tokens per Sec:     3282 || Lr: 0.000100
2024-02-11 18:51:08,553 [Epoch: 225 Step: 00029900] Batch Recognition Loss:   0.000641 => Gls Tokens per Sec:     1357 || Batch Translation Loss:   0.328355 => Txt Tokens per Sec:     3793 || Lr: 0.000100
2024-02-11 18:51:09,888 Epoch 225: Total Training Recognition Loss 0.05  Total Training Translation Loss 19.03 
2024-02-11 18:51:09,888 EPOCH 226
2024-02-11 18:51:13,950 [Epoch: 226 Step: 00030000] Batch Recognition Loss:   0.000381 => Gls Tokens per Sec:     1478 || Batch Translation Loss:   0.034995 => Txt Tokens per Sec:     4108 || Lr: 0.000100
2024-02-11 18:51:24,670 Validation result at epoch 226, step    30000: duration: 10.7202s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00318	Translation Loss: 95272.77344	PPL: 15086.69824
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.55	(BLEU-1: 9.78,	BLEU-2: 3.13,	BLEU-3: 1.15,	BLEU-4: 0.55)
	CHRF 16.45	ROUGE 8.65
2024-02-11 18:51:24,672 Logging Recognition and Translation Outputs
2024-02-11 18:51:24,672 ========================================================================================================================
2024-02-11 18:51:24,672 Logging Sequence: 58_52.00
2024-02-11 18:51:24,673 	Gloss Reference :	A B+C+D+E
2024-02-11 18:51:24,673 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:51:24,673 	Gloss Alignment :	         
2024-02-11 18:51:24,673 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:51:24,674 	Text Reference  :	the ****** ******** **** ******* games  are currently being held in    
2024-02-11 18:51:24,674 	Text Hypothesis :	the indian athletes have already bagged a   total     of    28   medals
2024-02-11 18:51:24,674 	Text Alignment  :	    I      I        I    I       S      S   S         S     S    S     
2024-02-11 18:51:24,674 ========================================================================================================================
2024-02-11 18:51:24,675 Logging Sequence: 148_49.00
2024-02-11 18:51:24,675 	Gloss Reference :	A B+C+D+E
2024-02-11 18:51:24,675 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:51:24,675 	Gloss Alignment :	         
2024-02-11 18:51:24,675 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:51:24,676 	Text Reference  :	then the 4th over  was    bowled by      mohammed siraj
2024-02-11 18:51:24,676 	Text Hypothesis :	**** the *** first person have   already made     india
2024-02-11 18:51:24,676 	Text Alignment  :	D        D   S     S      S      S       S        S    
2024-02-11 18:51:24,676 ========================================================================================================================
2024-02-11 18:51:24,676 Logging Sequence: 169_115.00
2024-02-11 18:51:24,677 	Gloss Reference :	A B+C+D+E
2024-02-11 18:51:24,677 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:51:24,677 	Gloss Alignment :	         
2024-02-11 18:51:24,677 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:51:24,678 	Text Reference  :	a   user      replaced the words   india   with         khalistan at      several locations on arshdeep's profile
2024-02-11 18:51:24,678 	Text Hypothesis :	the remaining 75       is  through selling broadcasting the       opening game    for       rs 250        crore  
2024-02-11 18:51:24,678 	Text Alignment  :	S   S         S        S   S       S       S            S         S       S       S         S  S          S      
2024-02-11 18:51:24,679 ========================================================================================================================
2024-02-11 18:51:24,679 Logging Sequence: 84_2.00
2024-02-11 18:51:24,679 	Gloss Reference :	A B+C+D+E
2024-02-11 18:51:24,679 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:51:24,679 	Gloss Alignment :	         
2024-02-11 18:51:24,679 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:51:24,681 	Text Reference  :	the 2022 fifa football world cup is      going on   in qatar from 20th november 2022 to     18th december 2022
2024-02-11 18:51:24,681 	Text Hypothesis :	the **** **** ******** same  sex couples are   held in qatar **** **** ******** for  around the  world    cup 
2024-02-11 18:51:24,681 	Text Alignment  :	    D    D    D        S     S   S       S     S             D    D    D        S    S      S    S        S   
2024-02-11 18:51:24,681 ========================================================================================================================
2024-02-11 18:51:24,681 Logging Sequence: 101_234.00
2024-02-11 18:51:24,682 	Gloss Reference :	A B+C+D+E
2024-02-11 18:51:24,682 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:51:24,682 	Gloss Alignment :	         
2024-02-11 18:51:24,682 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:51:24,683 	Text Reference  :	now similarly in u-19 world cup  the ***** winning   runs were     scored by hitting a  six  
2024-02-11 18:51:24,683 	Text Hypothesis :	*** later     on 16   july  2021 the aiscd announced his  minsters is     on 15th    of india
2024-02-11 18:51:24,684 	Text Alignment  :	D   S         S  S    S     S        I     S         S    S        S      S  S       S  S    
2024-02-11 18:51:24,684 ========================================================================================================================
2024-02-11 18:51:28,116 Epoch 226: Total Training Recognition Loss 0.06  Total Training Translation Loss 18.35 
2024-02-11 18:51:28,116 EPOCH 227
2024-02-11 18:51:30,487 [Epoch: 227 Step: 00030100] Batch Recognition Loss:   0.000559 => Gls Tokens per Sec:     1418 || Batch Translation Loss:   0.034828 => Txt Tokens per Sec:     3905 || Lr: 0.000100
2024-02-11 18:51:35,900 Epoch 227: Total Training Recognition Loss 0.06  Total Training Translation Loss 18.99 
2024-02-11 18:51:35,900 EPOCH 228
2024-02-11 18:51:36,390 [Epoch: 228 Step: 00030200] Batch Recognition Loss:   0.000173 => Gls Tokens per Sec:     1475 || Batch Translation Loss:   0.157420 => Txt Tokens per Sec:     4139 || Lr: 0.000100
2024-02-11 18:51:42,122 [Epoch: 228 Step: 00030300] Batch Recognition Loss:   0.000531 => Gls Tokens per Sec:     1394 || Batch Translation Loss:   0.096161 => Txt Tokens per Sec:     3898 || Lr: 0.000100
2024-02-11 18:51:43,373 Epoch 228: Total Training Recognition Loss 0.06  Total Training Translation Loss 17.74 
2024-02-11 18:51:43,373 EPOCH 229
2024-02-11 18:51:47,504 [Epoch: 229 Step: 00030400] Batch Recognition Loss:   0.000346 => Gls Tokens per Sec:     1472 || Batch Translation Loss:   0.124967 => Txt Tokens per Sec:     4087 || Lr: 0.000100
2024-02-11 18:51:51,002 Epoch 229: Total Training Recognition Loss 0.06  Total Training Translation Loss 18.38 
2024-02-11 18:51:51,003 EPOCH 230
2024-02-11 18:51:53,434 [Epoch: 230 Step: 00030500] Batch Recognition Loss:   0.000318 => Gls Tokens per Sec:     1416 || Batch Translation Loss:   0.260573 => Txt Tokens per Sec:     3874 || Lr: 0.000100
2024-02-11 18:51:58,565 Epoch 230: Total Training Recognition Loss 0.06  Total Training Translation Loss 22.90 
2024-02-11 18:51:58,565 EPOCH 231
2024-02-11 18:51:59,040 [Epoch: 231 Step: 00030600] Batch Recognition Loss:   0.000204 => Gls Tokens per Sec:     1688 || Batch Translation Loss:   0.077491 => Txt Tokens per Sec:     4810 || Lr: 0.000100
2024-02-11 18:52:04,618 [Epoch: 231 Step: 00030700] Batch Recognition Loss:   0.001681 => Gls Tokens per Sec:     1434 || Batch Translation Loss:   0.029497 => Txt Tokens per Sec:     3988 || Lr: 0.000100
2024-02-11 18:52:05,943 Epoch 231: Total Training Recognition Loss 0.06  Total Training Translation Loss 22.92 
2024-02-11 18:52:05,943 EPOCH 232
2024-02-11 18:52:10,206 [Epoch: 232 Step: 00030800] Batch Recognition Loss:   0.000286 => Gls Tokens per Sec:     1446 || Batch Translation Loss:   0.036212 => Txt Tokens per Sec:     4067 || Lr: 0.000100
2024-02-11 18:52:13,454 Epoch 232: Total Training Recognition Loss 0.07  Total Training Translation Loss 22.60 
2024-02-11 18:52:13,454 EPOCH 233
2024-02-11 18:52:15,809 [Epoch: 233 Step: 00030900] Batch Recognition Loss:   0.000310 => Gls Tokens per Sec:     1495 || Batch Translation Loss:   0.124795 => Txt Tokens per Sec:     4202 || Lr: 0.000100
2024-02-11 18:52:21,160 Epoch 233: Total Training Recognition Loss 0.07  Total Training Translation Loss 17.57 
2024-02-11 18:52:21,160 EPOCH 234
2024-02-11 18:52:21,715 [Epoch: 234 Step: 00031000] Batch Recognition Loss:   0.000222 => Gls Tokens per Sec:     1588 || Batch Translation Loss:   0.036168 => Txt Tokens per Sec:     4278 || Lr: 0.000100
2024-02-11 18:52:27,004 [Epoch: 234 Step: 00031100] Batch Recognition Loss:   0.002864 => Gls Tokens per Sec:     1513 || Batch Translation Loss:   0.215421 => Txt Tokens per Sec:     4206 || Lr: 0.000100
2024-02-11 18:52:28,251 Epoch 234: Total Training Recognition Loss 0.05  Total Training Translation Loss 16.95 
2024-02-11 18:52:28,251 EPOCH 235
2024-02-11 18:52:33,117 [Epoch: 235 Step: 00031200] Batch Recognition Loss:   0.000477 => Gls Tokens per Sec:     1283 || Batch Translation Loss:   0.044718 => Txt Tokens per Sec:     3660 || Lr: 0.000100
2024-02-11 18:52:36,076 Epoch 235: Total Training Recognition Loss 0.05  Total Training Translation Loss 16.50 
2024-02-11 18:52:36,076 EPOCH 236
2024-02-11 18:52:38,541 [Epoch: 236 Step: 00031300] Batch Recognition Loss:   0.000186 => Gls Tokens per Sec:     1461 || Batch Translation Loss:   0.209989 => Txt Tokens per Sec:     4200 || Lr: 0.000100
2024-02-11 18:52:43,401 Epoch 236: Total Training Recognition Loss 0.09  Total Training Translation Loss 25.80 
2024-02-11 18:52:43,402 EPOCH 237
2024-02-11 18:52:44,051 [Epoch: 237 Step: 00031400] Batch Recognition Loss:   0.000470 => Gls Tokens per Sec:     1481 || Batch Translation Loss:   0.069821 => Txt Tokens per Sec:     4293 || Lr: 0.000100
2024-02-11 18:52:50,108 [Epoch: 237 Step: 00031500] Batch Recognition Loss:   0.000430 => Gls Tokens per Sec:     1320 || Batch Translation Loss:   0.288380 => Txt Tokens per Sec:     3686 || Lr: 0.000100
2024-02-11 18:52:51,207 Epoch 237: Total Training Recognition Loss 0.07  Total Training Translation Loss 30.65 
2024-02-11 18:52:51,207 EPOCH 238
2024-02-11 18:52:55,750 [Epoch: 238 Step: 00031600] Batch Recognition Loss:   0.000418 => Gls Tokens per Sec:     1391 || Batch Translation Loss:   0.071004 => Txt Tokens per Sec:     3880 || Lr: 0.000100
2024-02-11 18:52:59,219 Epoch 238: Total Training Recognition Loss 0.06  Total Training Translation Loss 19.53 
2024-02-11 18:52:59,220 EPOCH 239
2024-02-11 18:53:01,932 [Epoch: 239 Step: 00031700] Batch Recognition Loss:   0.000160 => Gls Tokens per Sec:     1357 || Batch Translation Loss:   0.616702 => Txt Tokens per Sec:     3769 || Lr: 0.000100
2024-02-11 18:53:06,808 Epoch 239: Total Training Recognition Loss 0.06  Total Training Translation Loss 18.91 
2024-02-11 18:53:06,809 EPOCH 240
2024-02-11 18:53:07,642 [Epoch: 240 Step: 00031800] Batch Recognition Loss:   0.000486 => Gls Tokens per Sec:     1252 || Batch Translation Loss:   0.141085 => Txt Tokens per Sec:     3919 || Lr: 0.000100
2024-02-11 18:53:13,421 [Epoch: 240 Step: 00031900] Batch Recognition Loss:   0.000339 => Gls Tokens per Sec:     1384 || Batch Translation Loss:   0.033113 => Txt Tokens per Sec:     3765 || Lr: 0.000100
2024-02-11 18:53:14,635 Epoch 240: Total Training Recognition Loss 0.07  Total Training Translation Loss 21.89 
2024-02-11 18:53:14,635 EPOCH 241
2024-02-11 18:53:19,089 [Epoch: 241 Step: 00032000] Batch Recognition Loss:   0.000599 => Gls Tokens per Sec:     1437 || Batch Translation Loss:   0.140803 => Txt Tokens per Sec:     3996 || Lr: 0.000100
2024-02-11 18:53:29,736 Validation result at epoch 241, step    32000: duration: 10.6460s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00384	Translation Loss: 94804.61719	PPL: 14390.02148
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.64	(BLEU-1: 10.48,	BLEU-2: 3.11,	BLEU-3: 1.26,	BLEU-4: 0.64)
	CHRF 16.80	ROUGE 8.86
2024-02-11 18:53:29,737 Logging Recognition and Translation Outputs
2024-02-11 18:53:29,737 ========================================================================================================================
2024-02-11 18:53:29,737 Logging Sequence: 69_164.00
2024-02-11 18:53:29,738 	Gloss Reference :	A B+C+D+E
2024-02-11 18:53:29,738 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:53:29,738 	Gloss Alignment :	         
2024-02-11 18:53:29,738 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:53:29,739 	Text Reference  :	***** however i   know  that my  fans dearly   love  me 
2024-02-11 18:53:29,739 	Text Hypothesis :	after playing the final of   the icc  under-19 world cup
2024-02-11 18:53:29,739 	Text Alignment  :	I     S       S   S     S    S   S    S        S     S  
2024-02-11 18:53:29,739 ========================================================================================================================
2024-02-11 18:53:29,739 Logging Sequence: 58_52.00
2024-02-11 18:53:29,739 	Gloss Reference :	A B+C+D+E
2024-02-11 18:53:29,740 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:53:29,740 	Gloss Alignment :	         
2024-02-11 18:53:29,740 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:53:29,740 	Text Reference  :	the ******* *** ****** games are currently being held in    
2024-02-11 18:53:29,740 	Text Hypothesis :	the talents and skills of    our athletes  knows no   bounds
2024-02-11 18:53:29,741 	Text Alignment  :	    I       I   I      S     S   S         S     S    S     
2024-02-11 18:53:29,741 ========================================================================================================================
2024-02-11 18:53:29,741 Logging Sequence: 168_93.00
2024-02-11 18:53:29,741 	Gloss Reference :	A B+C+D+E
2024-02-11 18:53:29,741 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:53:29,741 	Gloss Alignment :	         
2024-02-11 18:53:29,741 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:53:29,742 	Text Reference  :	******* ** *** many   have said the   couple are over-protective
2024-02-11 18:53:29,742 	Text Hypothesis :	however he had severe pain and  never met    his batting        
2024-02-11 18:53:29,742 	Text Alignment  :	I       I  I   S      S    S    S     S      S   S              
2024-02-11 18:53:29,742 ========================================================================================================================
2024-02-11 18:53:29,742 Logging Sequence: 143_147.00
2024-02-11 18:53:29,743 	Gloss Reference :	A B+C+D+E
2024-02-11 18:53:29,743 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:53:29,743 	Gloss Alignment :	         
2024-02-11 18:53:29,743 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:53:29,745 	Text Reference  :	about the ban ronaldo requested  he     be       allowed     to explain the reason     of     his    behaviour to  the fa  and merseyside police     
2024-02-11 18:53:29,745 	Text Hypothesis :	***** *** *** ******* manchester united football association fa and     the merseyside police banned ronaldo   for two men and ********** afghanistan
2024-02-11 18:53:29,745 	Text Alignment  :	D     D   D   D       S          S      S        S           S  S           S          S      S      S         S   S   S       D          S          
2024-02-11 18:53:29,745 ========================================================================================================================
2024-02-11 18:53:29,746 Logging Sequence: 52_221.00
2024-02-11 18:53:29,746 	Gloss Reference :	A B+C+D+E
2024-02-11 18:53:29,746 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:53:29,746 	Gloss Alignment :	         
2024-02-11 18:53:29,746 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:53:29,747 	Text Reference  :	this was revealed after the gaming company thanked dhoni via a tweet   when   people saw        dhoni people started downloading the   game 
2024-02-11 18:53:29,748 	Text Hypothesis :	**** *** ******** ***** *** ****** ******* ******* ***** *** a similar reward of     cricketers to    become the     official    candy crush
2024-02-11 18:53:29,748 	Text Alignment  :	D    D   D        D     D   D      D       D       D     D     S       S      S      S          S     S      S       S           S     S    
2024-02-11 18:53:29,748 ========================================================================================================================
2024-02-11 18:53:33,098 Epoch 241: Total Training Recognition Loss 0.07  Total Training Translation Loss 19.48 
2024-02-11 18:53:33,098 EPOCH 242
2024-02-11 18:53:35,991 [Epoch: 242 Step: 00032100] Batch Recognition Loss:   0.000427 => Gls Tokens per Sec:     1300 || Batch Translation Loss:   0.100409 => Txt Tokens per Sec:     3688 || Lr: 0.000100
2024-02-11 18:53:40,901 Epoch 242: Total Training Recognition Loss 0.04  Total Training Translation Loss 21.79 
2024-02-11 18:53:40,902 EPOCH 243
2024-02-11 18:53:41,547 [Epoch: 243 Step: 00032200] Batch Recognition Loss:   0.000219 => Gls Tokens per Sec:     1736 || Batch Translation Loss:   0.049612 => Txt Tokens per Sec:     4667 || Lr: 0.000100
2024-02-11 18:53:46,874 [Epoch: 243 Step: 00032300] Batch Recognition Loss:   0.000302 => Gls Tokens per Sec:     1502 || Batch Translation Loss:   0.045257 => Txt Tokens per Sec:     4203 || Lr: 0.000100
2024-02-11 18:53:47,959 Epoch 243: Total Training Recognition Loss 0.06  Total Training Translation Loss 22.30 
2024-02-11 18:53:47,959 EPOCH 244
2024-02-11 18:53:52,676 [Epoch: 244 Step: 00032400] Batch Recognition Loss:   0.000521 => Gls Tokens per Sec:     1374 || Batch Translation Loss:   0.097969 => Txt Tokens per Sec:     3823 || Lr: 0.000100
2024-02-11 18:53:55,848 Epoch 244: Total Training Recognition Loss 0.08  Total Training Translation Loss 24.88 
2024-02-11 18:53:55,849 EPOCH 245
2024-02-11 18:53:58,423 [Epoch: 245 Step: 00032500] Batch Recognition Loss:   0.000559 => Gls Tokens per Sec:     1492 || Batch Translation Loss:   0.030056 => Txt Tokens per Sec:     4122 || Lr: 0.000100
2024-02-11 18:54:03,134 Epoch 245: Total Training Recognition Loss 0.06  Total Training Translation Loss 24.06 
2024-02-11 18:54:03,135 EPOCH 246
2024-02-11 18:54:03,916 [Epoch: 246 Step: 00032600] Batch Recognition Loss:   0.000242 => Gls Tokens per Sec:     1538 || Batch Translation Loss:   0.134398 => Txt Tokens per Sec:     3997 || Lr: 0.000100
2024-02-11 18:54:09,625 [Epoch: 246 Step: 00032700] Batch Recognition Loss:   0.000193 => Gls Tokens per Sec:     1401 || Batch Translation Loss:   0.252233 => Txt Tokens per Sec:     3974 || Lr: 0.000100
2024-02-11 18:54:10,646 Epoch 246: Total Training Recognition Loss 0.07  Total Training Translation Loss 26.18 
2024-02-11 18:54:10,646 EPOCH 247
2024-02-11 18:54:15,242 [Epoch: 247 Step: 00032800] Batch Recognition Loss:   0.000576 => Gls Tokens per Sec:     1428 || Batch Translation Loss:   0.216504 => Txt Tokens per Sec:     3968 || Lr: 0.000100
2024-02-11 18:54:18,252 Epoch 247: Total Training Recognition Loss 0.05  Total Training Translation Loss 19.89 
2024-02-11 18:54:18,252 EPOCH 248
2024-02-11 18:54:20,886 [Epoch: 248 Step: 00032900] Batch Recognition Loss:   0.000682 => Gls Tokens per Sec:     1489 || Batch Translation Loss:   0.051263 => Txt Tokens per Sec:     4035 || Lr: 0.000100
2024-02-11 18:54:25,628 Epoch 248: Total Training Recognition Loss 0.06  Total Training Translation Loss 15.97 
2024-02-11 18:54:25,628 EPOCH 249
2024-02-11 18:54:26,438 [Epoch: 249 Step: 00033000] Batch Recognition Loss:   0.000323 => Gls Tokens per Sec:     1582 || Batch Translation Loss:   0.125921 => Txt Tokens per Sec:     4582 || Lr: 0.000100
2024-02-11 18:54:32,104 [Epoch: 249 Step: 00033100] Batch Recognition Loss:   0.000403 => Gls Tokens per Sec:     1412 || Batch Translation Loss:   0.816172 => Txt Tokens per Sec:     3898 || Lr: 0.000100
2024-02-11 18:54:33,156 Epoch 249: Total Training Recognition Loss 0.07  Total Training Translation Loss 20.99 
2024-02-11 18:54:33,156 EPOCH 250
2024-02-11 18:54:37,922 [Epoch: 250 Step: 00033200] Batch Recognition Loss:   0.000445 => Gls Tokens per Sec:     1393 || Batch Translation Loss:   0.099871 => Txt Tokens per Sec:     3840 || Lr: 0.000100
2024-02-11 18:54:40,960 Epoch 250: Total Training Recognition Loss 0.06  Total Training Translation Loss 23.68 
2024-02-11 18:54:40,960 EPOCH 251
2024-02-11 18:54:43,954 [Epoch: 251 Step: 00033300] Batch Recognition Loss:   0.000381 => Gls Tokens per Sec:     1337 || Batch Translation Loss:   0.055051 => Txt Tokens per Sec:     3689 || Lr: 0.000100
2024-02-11 18:54:48,466 Epoch 251: Total Training Recognition Loss 0.06  Total Training Translation Loss 21.90 
2024-02-11 18:54:48,467 EPOCH 252
2024-02-11 18:54:49,464 [Epoch: 252 Step: 00033400] Batch Recognition Loss:   0.000497 => Gls Tokens per Sec:     1365 || Batch Translation Loss:   0.087512 => Txt Tokens per Sec:     3542 || Lr: 0.000100
2024-02-11 18:54:55,404 [Epoch: 252 Step: 00033500] Batch Recognition Loss:   0.000852 => Gls Tokens per Sec:     1345 || Batch Translation Loss:   0.079801 => Txt Tokens per Sec:     3758 || Lr: 0.000100
2024-02-11 18:54:56,341 Epoch 252: Total Training Recognition Loss 0.07  Total Training Translation Loss 22.29 
2024-02-11 18:54:56,342 EPOCH 253
2024-02-11 18:55:01,046 [Epoch: 253 Step: 00033600] Batch Recognition Loss:   0.000225 => Gls Tokens per Sec:     1429 || Batch Translation Loss:   0.072971 => Txt Tokens per Sec:     3934 || Lr: 0.000100
2024-02-11 18:55:03,879 Epoch 253: Total Training Recognition Loss 0.05  Total Training Translation Loss 16.03 
2024-02-11 18:55:03,880 EPOCH 254
2024-02-11 18:55:07,001 [Epoch: 254 Step: 00033700] Batch Recognition Loss:   0.000113 => Gls Tokens per Sec:     1308 || Batch Translation Loss:   0.028257 => Txt Tokens per Sec:     3734 || Lr: 0.000100
2024-02-11 18:55:11,463 Epoch 254: Total Training Recognition Loss 0.07  Total Training Translation Loss 16.29 
2024-02-11 18:55:11,463 EPOCH 255
2024-02-11 18:55:12,282 [Epoch: 255 Step: 00033800] Batch Recognition Loss:   0.000280 => Gls Tokens per Sec:     1760 || Batch Translation Loss:   0.336687 => Txt Tokens per Sec:     4259 || Lr: 0.000100
2024-02-11 18:55:18,302 [Epoch: 255 Step: 00033900] Batch Recognition Loss:   0.000351 => Gls Tokens per Sec:     1327 || Batch Translation Loss:   1.137439 => Txt Tokens per Sec:     3778 || Lr: 0.000100
2024-02-11 18:55:19,114 Epoch 255: Total Training Recognition Loss 0.06  Total Training Translation Loss 19.46 
2024-02-11 18:55:19,114 EPOCH 256
2024-02-11 18:55:23,830 [Epoch: 256 Step: 00034000] Batch Recognition Loss:   0.000272 => Gls Tokens per Sec:     1442 || Batch Translation Loss:   0.211165 => Txt Tokens per Sec:     3977 || Lr: 0.000100
2024-02-11 18:55:34,910 Validation result at epoch 256, step    34000: duration: 11.0795s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00268	Translation Loss: 92552.99219	PPL: 11463.22168
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.58	(BLEU-1: 10.45,	BLEU-2: 3.14,	BLEU-3: 1.25,	BLEU-4: 0.58)
	CHRF 16.60	ROUGE 8.80
2024-02-11 18:55:34,911 Logging Recognition and Translation Outputs
2024-02-11 18:55:34,911 ========================================================================================================================
2024-02-11 18:55:34,911 Logging Sequence: 81_20.00
2024-02-11 18:55:34,911 	Gloss Reference :	A B+C+D+E
2024-02-11 18:55:34,912 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:55:34,912 	Gloss Alignment :	         
2024-02-11 18:55:34,912 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:55:34,915 	Text Reference  :	people purchased homes from the ******** ****** builder they   have  said the builder took money from    them *** **** but never completed the ******** housing projects
2024-02-11 18:55:34,915 	Text Hypothesis :	who    purchased flats in   the building posted on      social media that the builder **** had   scammed them and they did not   complete  the promised housing projects
2024-02-11 18:55:34,915 	Text Alignment  :	S                S     S        I        I      S       S      S     S                D    S     S            I   I    S   S     S             I                        
2024-02-11 18:55:34,915 ========================================================================================================================
2024-02-11 18:55:34,915 Logging Sequence: 123_68.00
2024-02-11 18:55:34,915 	Gloss Reference :	A B+C+D+E
2024-02-11 18:55:34,915 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:55:34,916 	Gloss Alignment :	         
2024-02-11 18:55:34,916 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:55:34,917 	Text Reference  :	now i am sure you must be    eager to   know about    the        other vehicles that he owns and their price tags
2024-02-11 18:55:34,917 	Text Hypothesis :	*** * ** **** *** **** dhoni was   owns his  stunning collection of    vehicles **** ** **** *** ***** ***** ****
2024-02-11 18:55:34,917 	Text Alignment  :	D   D D  D    D   D    S     S     S    S    S        S          S              D    D  D    D   D     D     D   
2024-02-11 18:55:34,917 ========================================================================================================================
2024-02-11 18:55:34,917 Logging Sequence: 141_143.00
2024-02-11 18:55:34,917 	Gloss Reference :	A B+C+D+E
2024-02-11 18:55:34,918 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:55:34,918 	Gloss Alignment :	         
2024-02-11 18:55:34,918 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:55:34,919 	Text Reference  :	he has learnt the art  of       chasing from    the    great mahendra singh dhoni    
2024-02-11 18:55:34,919 	Text Hypothesis :	** *** ****** *** even athletes have    already bagged a     pizza    of    gratitude
2024-02-11 18:55:34,919 	Text Alignment  :	D  D   D      D   S    S        S       S       S      S     S        S     S        
2024-02-11 18:55:34,919 ========================================================================================================================
2024-02-11 18:55:34,919 Logging Sequence: 166_135.00
2024-02-11 18:55:34,919 	Gloss Reference :	A B+C+D+E
2024-02-11 18:55:34,919 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:55:34,920 	Gloss Alignment :	         
2024-02-11 18:55:34,920 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:55:34,921 	Text Reference  :	out of the remaining nine teams india must play * series ** **** * **** with   6 teams
2024-02-11 18:55:34,921 	Text Hypothesis :	*** ** *** which     is   why   they  will play 3 series at home - miny needed 2 teams
2024-02-11 18:55:34,921 	Text Alignment  :	D   D  D   S         S    S     S     S         I        I  I    I I    S      S      
2024-02-11 18:55:34,921 ========================================================================================================================
2024-02-11 18:55:34,921 Logging Sequence: 62_249.00
2024-02-11 18:55:34,922 	Gloss Reference :	A B+C+D+E
2024-02-11 18:55:34,922 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:55:34,922 	Gloss Alignment :	         
2024-02-11 18:55:34,922 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:55:34,924 	Text Reference  :	women's team was run by the women' cricket association of   india and they had to pay from their pockets for traveling for international matches   
2024-02-11 18:55:34,924 	Text Hypothesis :	******* **** *** *** in the men'   world   cup         2022 india *** is   set to *** **** ***** play    sri lanka     in  the           semi-final
2024-02-11 18:55:34,924 	Text Alignment  :	D       D    D   D   S      S      S       S           S          D   S    S      D   D    D     S       S   S         S   S             S         
2024-02-11 18:55:34,924 ========================================================================================================================
2024-02-11 18:55:37,884 Epoch 256: Total Training Recognition Loss 0.05  Total Training Translation Loss 23.44 
2024-02-11 18:55:37,884 EPOCH 257
2024-02-11 18:55:40,907 [Epoch: 257 Step: 00034100] Batch Recognition Loss:   0.000677 => Gls Tokens per Sec:     1377 || Batch Translation Loss:   0.048322 => Txt Tokens per Sec:     3816 || Lr: 0.000100
2024-02-11 18:55:45,671 Epoch 257: Total Training Recognition Loss 0.06  Total Training Translation Loss 19.46 
2024-02-11 18:55:45,671 EPOCH 258
2024-02-11 18:55:46,733 [Epoch: 258 Step: 00034200] Batch Recognition Loss:   0.000671 => Gls Tokens per Sec:     1433 || Batch Translation Loss:   0.023968 => Txt Tokens per Sec:     3926 || Lr: 0.000100
2024-02-11 18:55:51,933 [Epoch: 258 Step: 00034300] Batch Recognition Loss:   0.000227 => Gls Tokens per Sec:     1539 || Batch Translation Loss:   0.331381 => Txt Tokens per Sec:     4293 || Lr: 0.000100
2024-02-11 18:55:52,845 Epoch 258: Total Training Recognition Loss 0.05  Total Training Translation Loss 21.33 
2024-02-11 18:55:52,846 EPOCH 259
2024-02-11 18:55:57,853 [Epoch: 259 Step: 00034400] Batch Recognition Loss:   0.000373 => Gls Tokens per Sec:     1374 || Batch Translation Loss:   0.094826 => Txt Tokens per Sec:     3830 || Lr: 0.000100
2024-02-11 18:56:00,485 Epoch 259: Total Training Recognition Loss 0.05  Total Training Translation Loss 23.54 
2024-02-11 18:56:00,485 EPOCH 260
2024-02-11 18:56:03,756 [Epoch: 260 Step: 00034500] Batch Recognition Loss:   0.000933 => Gls Tokens per Sec:     1297 || Batch Translation Loss:   0.203209 => Txt Tokens per Sec:     3638 || Lr: 0.000100
2024-02-11 18:56:08,392 Epoch 260: Total Training Recognition Loss 0.07  Total Training Translation Loss 19.57 
2024-02-11 18:56:08,393 EPOCH 261
2024-02-11 18:56:09,536 [Epoch: 261 Step: 00034600] Batch Recognition Loss:   0.000255 => Gls Tokens per Sec:     1400 || Batch Translation Loss:   0.088266 => Txt Tokens per Sec:     3892 || Lr: 0.000100
2024-02-11 18:56:14,919 [Epoch: 261 Step: 00034700] Batch Recognition Loss:   0.000244 => Gls Tokens per Sec:     1484 || Batch Translation Loss:   0.115946 => Txt Tokens per Sec:     4099 || Lr: 0.000100
2024-02-11 18:56:15,653 Epoch 261: Total Training Recognition Loss 0.06  Total Training Translation Loss 14.90 
2024-02-11 18:56:15,654 EPOCH 262
2024-02-11 18:56:20,562 [Epoch: 262 Step: 00034800] Batch Recognition Loss:   0.000476 => Gls Tokens per Sec:     1418 || Batch Translation Loss:   0.040992 => Txt Tokens per Sec:     3912 || Lr: 0.000100
2024-02-11 18:56:23,282 Epoch 262: Total Training Recognition Loss 0.06  Total Training Translation Loss 23.26 
2024-02-11 18:56:23,282 EPOCH 263
2024-02-11 18:56:26,285 [Epoch: 263 Step: 00034900] Batch Recognition Loss:   0.000341 => Gls Tokens per Sec:     1439 || Batch Translation Loss:   0.405584 => Txt Tokens per Sec:     4114 || Lr: 0.000100
2024-02-11 18:56:30,374 Epoch 263: Total Training Recognition Loss 0.06  Total Training Translation Loss 22.35 
2024-02-11 18:56:30,374 EPOCH 264
2024-02-11 18:56:31,556 [Epoch: 264 Step: 00035000] Batch Recognition Loss:   0.000673 => Gls Tokens per Sec:     1423 || Batch Translation Loss:   0.135422 => Txt Tokens per Sec:     3769 || Lr: 0.000100
2024-02-11 18:56:37,260 [Epoch: 264 Step: 00035100] Batch Recognition Loss:   0.000302 => Gls Tokens per Sec:     1403 || Batch Translation Loss:   0.021547 => Txt Tokens per Sec:     3915 || Lr: 0.000100
2024-02-11 18:56:38,080 Epoch 264: Total Training Recognition Loss 0.07  Total Training Translation Loss 15.49 
2024-02-11 18:56:38,080 EPOCH 265
2024-02-11 18:56:42,693 [Epoch: 265 Step: 00035200] Batch Recognition Loss:   0.001489 => Gls Tokens per Sec:     1526 || Batch Translation Loss:   0.136734 => Txt Tokens per Sec:     4197 || Lr: 0.000100
2024-02-11 18:56:45,486 Epoch 265: Total Training Recognition Loss 0.06  Total Training Translation Loss 16.89 
2024-02-11 18:56:45,486 EPOCH 266
2024-02-11 18:56:48,626 [Epoch: 266 Step: 00035300] Batch Recognition Loss:   0.000249 => Gls Tokens per Sec:     1402 || Batch Translation Loss:   0.109162 => Txt Tokens per Sec:     3903 || Lr: 0.000100
2024-02-11 18:56:52,936 Epoch 266: Total Training Recognition Loss 0.05  Total Training Translation Loss 18.68 
2024-02-11 18:56:52,936 EPOCH 267
2024-02-11 18:56:54,048 [Epoch: 267 Step: 00035400] Batch Recognition Loss:   0.000217 => Gls Tokens per Sec:     1584 || Batch Translation Loss:   0.075312 => Txt Tokens per Sec:     4175 || Lr: 0.000100
2024-02-11 18:56:59,461 [Epoch: 267 Step: 00035500] Batch Recognition Loss:   0.000356 => Gls Tokens per Sec:     1476 || Batch Translation Loss:   0.255881 => Txt Tokens per Sec:     4162 || Lr: 0.000100
2024-02-11 18:57:00,063 Epoch 267: Total Training Recognition Loss 0.05  Total Training Translation Loss 17.67 
2024-02-11 18:57:00,063 EPOCH 268
2024-02-11 18:57:05,019 [Epoch: 268 Step: 00035600] Batch Recognition Loss:   0.000548 => Gls Tokens per Sec:     1437 || Batch Translation Loss:   0.030413 => Txt Tokens per Sec:     3944 || Lr: 0.000100
2024-02-11 18:57:07,694 Epoch 268: Total Training Recognition Loss 0.06  Total Training Translation Loss 23.22 
2024-02-11 18:57:07,694 EPOCH 269
2024-02-11 18:57:10,874 [Epoch: 269 Step: 00035700] Batch Recognition Loss:   0.000289 => Gls Tokens per Sec:     1410 || Batch Translation Loss:   0.067606 => Txt Tokens per Sec:     3887 || Lr: 0.000100
2024-02-11 18:57:15,433 Epoch 269: Total Training Recognition Loss 0.07  Total Training Translation Loss 21.46 
2024-02-11 18:57:15,433 EPOCH 270
2024-02-11 18:57:16,697 [Epoch: 270 Step: 00035800] Batch Recognition Loss:   0.000525 => Gls Tokens per Sec:     1457 || Batch Translation Loss:   0.075421 => Txt Tokens per Sec:     4069 || Lr: 0.000100
2024-02-11 18:57:21,922 [Epoch: 270 Step: 00035900] Batch Recognition Loss:   0.000592 => Gls Tokens per Sec:     1529 || Batch Translation Loss:   0.057512 => Txt Tokens per Sec:     4268 || Lr: 0.000100
2024-02-11 18:57:22,557 Epoch 270: Total Training Recognition Loss 0.08  Total Training Translation Loss 23.07 
2024-02-11 18:57:22,558 EPOCH 271
2024-02-11 18:57:27,741 [Epoch: 271 Step: 00036000] Batch Recognition Loss:   0.000525 => Gls Tokens per Sec:     1389 || Batch Translation Loss:   0.077230 => Txt Tokens per Sec:     3821 || Lr: 0.000100
2024-02-11 18:57:38,647 Validation result at epoch 271, step    36000: duration: 10.9053s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00348	Translation Loss: 93324.55469	PPL: 12392.15625
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.62	(BLEU-1: 10.70,	BLEU-2: 3.32,	BLEU-3: 1.26,	BLEU-4: 0.62)
	CHRF 16.43	ROUGE 9.01
2024-02-11 18:57:38,648 Logging Recognition and Translation Outputs
2024-02-11 18:57:38,648 ========================================================================================================================
2024-02-11 18:57:38,648 Logging Sequence: 162_2.00
2024-02-11 18:57:38,648 	Gloss Reference :	A B+C+D+E
2024-02-11 18:57:38,649 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:57:38,649 	Gloss Alignment :	         
2024-02-11 18:57:38,649 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:57:38,650 	Text Reference  :	***** *** india  had  lost the **** *** t20 world cup        match against pakistan
2024-02-11 18:57:38,650 	Text Hypothesis :	kohli has always been made the same but his guru' photograph in    his     child   
2024-02-11 18:57:38,650 	Text Alignment  :	I     I   S      S    S        I    I   S   S     S          S     S       S       
2024-02-11 18:57:38,650 ========================================================================================================================
2024-02-11 18:57:38,650 Logging Sequence: 167_92.00
2024-02-11 18:57:38,650 	Gloss Reference :	A B+C+D+E
2024-02-11 18:57:38,651 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:57:38,651 	Gloss Alignment :	         
2024-02-11 18:57:38,651 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:57:38,651 	Text Reference  :	**** ** *** so   people have been warned to   be  careful
2024-02-11 18:57:38,652 	Text Hypothesis :	this is why they will   have **** their  wait for life   
2024-02-11 18:57:38,652 	Text Alignment  :	I    I  I   S    S           D    S      S    S   S      
2024-02-11 18:57:38,652 ========================================================================================================================
2024-02-11 18:57:38,652 Logging Sequence: 104_2.00
2024-02-11 18:57:38,652 	Gloss Reference :	A B+C+D+E
2024-02-11 18:57:38,652 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:57:38,652 	Gloss Alignment :	         
2024-02-11 18:57:38,653 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:57:38,654 	Text Reference  :	*** ****** ******* ****** **** super talented chess  player r  praggnanandhaa from   chennai       tamil nadu has made    india   proud 
2024-02-11 18:57:38,654 	Text Hypothesis :	the indian premier league 2021 at    the      rising season of young          ramesh congratulated him   for  his amazing batting skills
2024-02-11 18:57:38,654 	Text Alignment  :	I   I      I       I      I    S     S        S      S      S  S              S      S             S     S    S   S       S       S     
2024-02-11 18:57:38,654 ========================================================================================================================
2024-02-11 18:57:38,655 Logging Sequence: 79_185.00
2024-02-11 18:57:38,655 	Gloss Reference :	A B+C+D+E
2024-02-11 18:57:38,655 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:57:38,655 	Gloss Alignment :	         
2024-02-11 18:57:38,655 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:57:38,656 	Text Reference  :	then the  match    will be     cancelled and **** *** **** **** ** postponed
2024-02-11 18:57:38,656 	Text Hypothesis :	they were supposed to   repeat itself    and want the bcci paid rs worried  
2024-02-11 18:57:38,656 	Text Alignment  :	S    S    S        S    S      S             I    I   I    I    I  S        
2024-02-11 18:57:38,656 ========================================================================================================================
2024-02-11 18:57:38,656 Logging Sequence: 175_138.00
2024-02-11 18:57:38,657 	Gloss Reference :	A B+C+D+E
2024-02-11 18:57:38,657 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:57:38,657 	Gloss Alignment :	         
2024-02-11 18:57:38,657 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:57:38,658 	Text Reference  :	***** *** ***** what a proud moment unlike earlier indians are  making a   name in sports congratulations
2024-02-11 18:57:38,658 	Text Hypothesis :	let's see there is   a lot   of     my     father  and     want to     see if   i  am     married        
2024-02-11 18:57:38,659 	Text Alignment  :	I     I   I     S      S     S      S      S       S       S    S      S   S    S  S      S              
2024-02-11 18:57:38,659 ========================================================================================================================
2024-02-11 18:57:41,288 Epoch 271: Total Training Recognition Loss 0.07  Total Training Translation Loss 17.76 
2024-02-11 18:57:41,288 EPOCH 272
2024-02-11 18:57:44,476 [Epoch: 272 Step: 00036100] Batch Recognition Loss:   0.000763 => Gls Tokens per Sec:     1431 || Batch Translation Loss:   0.040587 => Txt Tokens per Sec:     3900 || Lr: 0.000050
2024-02-11 18:57:49,051 Epoch 272: Total Training Recognition Loss 0.05  Total Training Translation Loss 12.00 
2024-02-11 18:57:49,051 EPOCH 273
2024-02-11 18:57:50,388 [Epoch: 273 Step: 00036200] Batch Recognition Loss:   0.000230 => Gls Tokens per Sec:     1437 || Batch Translation Loss:   0.030714 => Txt Tokens per Sec:     4129 || Lr: 0.000050
2024-02-11 18:57:56,322 [Epoch: 273 Step: 00036300] Batch Recognition Loss:   0.000725 => Gls Tokens per Sec:     1347 || Batch Translation Loss:   0.106468 => Txt Tokens per Sec:     3732 || Lr: 0.000050
2024-02-11 18:57:56,804 Epoch 273: Total Training Recognition Loss 0.05  Total Training Translation Loss 5.14 
2024-02-11 18:57:56,804 EPOCH 274
2024-02-11 18:58:02,059 [Epoch: 274 Step: 00036400] Batch Recognition Loss:   0.000345 => Gls Tokens per Sec:     1386 || Batch Translation Loss:   0.021995 => Txt Tokens per Sec:     3843 || Lr: 0.000050
2024-02-11 18:58:04,588 Epoch 274: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.60 
2024-02-11 18:58:04,589 EPOCH 275
2024-02-11 18:58:07,991 [Epoch: 275 Step: 00036500] Batch Recognition Loss:   0.000256 => Gls Tokens per Sec:     1364 || Batch Translation Loss:   0.015120 => Txt Tokens per Sec:     3818 || Lr: 0.000050
2024-02-11 18:58:12,255 Epoch 275: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.38 
2024-02-11 18:58:12,256 EPOCH 276
2024-02-11 18:58:13,599 [Epoch: 276 Step: 00036600] Batch Recognition Loss:   0.000662 => Gls Tokens per Sec:     1489 || Batch Translation Loss:   0.059977 => Txt Tokens per Sec:     4223 || Lr: 0.000050
2024-02-11 18:58:19,254 [Epoch: 276 Step: 00036700] Batch Recognition Loss:   0.000272 => Gls Tokens per Sec:     1413 || Batch Translation Loss:   0.028735 => Txt Tokens per Sec:     3936 || Lr: 0.000050
2024-02-11 18:58:19,708 Epoch 276: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.12 
2024-02-11 18:58:19,709 EPOCH 277
2024-02-11 18:58:25,305 [Epoch: 277 Step: 00036800] Batch Recognition Loss:   0.000223 => Gls Tokens per Sec:     1316 || Batch Translation Loss:   0.015066 => Txt Tokens per Sec:     3647 || Lr: 0.000050
2024-02-11 18:58:27,782 Epoch 277: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.26 
2024-02-11 18:58:27,782 EPOCH 278
2024-02-11 18:58:31,261 [Epoch: 278 Step: 00036900] Batch Recognition Loss:   0.000167 => Gls Tokens per Sec:     1358 || Batch Translation Loss:   0.011508 => Txt Tokens per Sec:     3888 || Lr: 0.000050
2024-02-11 18:58:35,388 Epoch 278: Total Training Recognition Loss 0.04  Total Training Translation Loss 4.39 
2024-02-11 18:58:35,388 EPOCH 279
2024-02-11 18:58:36,803 [Epoch: 279 Step: 00037000] Batch Recognition Loss:   0.000334 => Gls Tokens per Sec:     1471 || Batch Translation Loss:   0.019195 => Txt Tokens per Sec:     3991 || Lr: 0.000050
2024-02-11 18:58:42,768 [Epoch: 279 Step: 00037100] Batch Recognition Loss:   0.002091 => Gls Tokens per Sec:     1339 || Batch Translation Loss:   0.050301 => Txt Tokens per Sec:     3733 || Lr: 0.000050
2024-02-11 18:58:43,229 Epoch 279: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.39 
2024-02-11 18:58:43,229 EPOCH 280
2024-02-11 18:58:48,674 [Epoch: 280 Step: 00037200] Batch Recognition Loss:   0.000183 => Gls Tokens per Sec:     1367 || Batch Translation Loss:   0.016933 => Txt Tokens per Sec:     3837 || Lr: 0.000050
2024-02-11 18:58:50,994 Epoch 280: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.23 
2024-02-11 18:58:50,994 EPOCH 281
2024-02-11 18:58:54,268 [Epoch: 281 Step: 00037300] Batch Recognition Loss:   0.000150 => Gls Tokens per Sec:     1467 || Batch Translation Loss:   0.012873 => Txt Tokens per Sec:     4002 || Lr: 0.000050
2024-02-11 18:58:58,653 Epoch 281: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.17 
2024-02-11 18:58:58,654 EPOCH 282
2024-02-11 18:59:00,172 [Epoch: 282 Step: 00037400] Batch Recognition Loss:   0.000143 => Gls Tokens per Sec:     1424 || Batch Translation Loss:   0.024913 => Txt Tokens per Sec:     3859 || Lr: 0.000050
2024-02-11 18:59:05,877 [Epoch: 282 Step: 00037500] Batch Recognition Loss:   0.000383 => Gls Tokens per Sec:     1402 || Batch Translation Loss:   0.025892 => Txt Tokens per Sec:     3920 || Lr: 0.000050
2024-02-11 18:59:06,287 Epoch 282: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.40 
2024-02-11 18:59:06,287 EPOCH 283
2024-02-11 18:59:11,532 [Epoch: 283 Step: 00037600] Batch Recognition Loss:   0.000157 => Gls Tokens per Sec:     1434 || Batch Translation Loss:   0.018265 => Txt Tokens per Sec:     3959 || Lr: 0.000050
2024-02-11 18:59:13,777 Epoch 283: Total Training Recognition Loss 0.04  Total Training Translation Loss 7.17 
2024-02-11 18:59:13,777 EPOCH 284
2024-02-11 18:59:16,972 [Epoch: 284 Step: 00037700] Batch Recognition Loss:   0.000175 => Gls Tokens per Sec:     1528 || Batch Translation Loss:   0.019220 => Txt Tokens per Sec:     4151 || Lr: 0.000050
2024-02-11 18:59:21,007 Epoch 284: Total Training Recognition Loss 0.04  Total Training Translation Loss 7.55 
2024-02-11 18:59:21,007 EPOCH 285
2024-02-11 18:59:22,645 [Epoch: 285 Step: 00037800] Batch Recognition Loss:   0.000428 => Gls Tokens per Sec:     1368 || Batch Translation Loss:   0.026534 => Txt Tokens per Sec:     3742 || Lr: 0.000050
2024-02-11 18:59:28,514 [Epoch: 285 Step: 00037900] Batch Recognition Loss:   0.000200 => Gls Tokens per Sec:     1362 || Batch Translation Loss:   0.017400 => Txt Tokens per Sec:     3804 || Lr: 0.000050
2024-02-11 18:59:28,763 Epoch 285: Total Training Recognition Loss 0.04  Total Training Translation Loss 7.99 
2024-02-11 18:59:28,763 EPOCH 286
2024-02-11 18:59:33,974 [Epoch: 286 Step: 00038000] Batch Recognition Loss:   0.000129 => Gls Tokens per Sec:     1459 || Batch Translation Loss:   0.031442 => Txt Tokens per Sec:     4046 || Lr: 0.000050
2024-02-11 18:59:44,883 Validation result at epoch 286, step    38000: duration: 10.9094s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00171	Translation Loss: 94123.15625	PPL: 13432.99609
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.50	(BLEU-1: 10.68,	BLEU-2: 3.22,	BLEU-3: 1.14,	BLEU-4: 0.50)
	CHRF 16.61	ROUGE 8.97
2024-02-11 18:59:44,884 Logging Recognition and Translation Outputs
2024-02-11 18:59:44,884 ========================================================================================================================
2024-02-11 18:59:44,885 Logging Sequence: 122_184.00
2024-02-11 18:59:44,885 	Gloss Reference :	A B+C+D+E
2024-02-11 18:59:44,885 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:59:44,885 	Gloss Alignment :	         
2024-02-11 18:59:44,885 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:59:44,886 	Text Reference  :	are playing exceptionally well and keeping hopes of   further olympic medals   alive       
2024-02-11 18:59:44,886 	Text Hypothesis :	*** ******* ************* **** *** ******* euro  2020 is      a       football championship
2024-02-11 18:59:44,886 	Text Alignment  :	D   D       D             D    D   D       S     S    S       S       S        S           
2024-02-11 18:59:44,886 ========================================================================================================================
2024-02-11 18:59:44,886 Logging Sequence: 70_243.00
2024-02-11 18:59:44,887 	Gloss Reference :	A B+C+D+E
2024-02-11 18:59:44,887 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:59:44,887 	Gloss Alignment :	         
2024-02-11 18:59:44,887 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:59:44,888 	Text Reference  :	** *** he  then  noticed a    beer            bottle near the    mic  
2024-02-11 18:59:44,888 	Text Hypothesis :	to fix you could not     have congratulations for    a    silver medal
2024-02-11 18:59:44,888 	Text Alignment  :	I  I   S   S     S       S    S               S      S    S      S    
2024-02-11 18:59:44,888 ========================================================================================================================
2024-02-11 18:59:44,888 Logging Sequence: 101_30.00
2024-02-11 18:59:44,888 	Gloss Reference :	A B+C+D+E
2024-02-11 18:59:44,888 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:59:44,889 	Gloss Alignment :	         
2024-02-11 18:59:44,889 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:59:44,890 	Text Reference  :	the national under-19 teams of    countries like australia england india    etc contested in  the   tournament
2024-02-11 18:59:44,890 	Text Hypothesis :	*** ******** ******** ***** along with      the  finals    were    infected by  the       odi world cup       
2024-02-11 18:59:44,890 	Text Alignment  :	D   D        D        D     S     S         S    S         S       S        S   S         S   S     S         
2024-02-11 18:59:44,890 ========================================================================================================================
2024-02-11 18:59:44,890 Logging Sequence: 144_94.00
2024-02-11 18:59:44,890 	Gloss Reference :	A B+C+D+E
2024-02-11 18:59:44,890 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:59:44,891 	Gloss Alignment :	         
2024-02-11 18:59:44,891 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:59:44,892 	Text Reference  :	*** fortunately bjp  rajasthan state president  satish    punia  was      deeply  moved and offered to help her   
2024-02-11 18:59:44,892 	Text Hypothesis :	let me          tell you       how   australia' legendary bowler watching cricket final and ******* ** **** became
2024-02-11 18:59:44,892 	Text Alignment  :	I   S           S    S         S     S          S         S      S        S       S         D       D  D    S     
2024-02-11 18:59:44,892 ========================================================================================================================
2024-02-11 18:59:44,892 Logging Sequence: 87_45.00
2024-02-11 18:59:44,893 	Gloss Reference :	A B+C+D+E
2024-02-11 18:59:44,893 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 18:59:44,893 	Gloss Alignment :	         
2024-02-11 18:59:44,893 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 18:59:44,894 	Text Reference  :	****** *** ***** ******* ***** *** at    31 runs india      had lost  2  wickets     
2024-02-11 18:59:44,894 	Text Hypothesis :	before the match started after the viral it is   completing 75  years of independence
2024-02-11 18:59:44,894 	Text Alignment  :	I      I   I     I       I     I   S     S  S    S          S   S     S  S           
2024-02-11 18:59:44,894 ========================================================================================================================
2024-02-11 18:59:47,036 Epoch 286: Total Training Recognition Loss 0.04  Total Training Translation Loss 5.09 
2024-02-11 18:59:47,036 EPOCH 287
2024-02-11 18:59:50,748 [Epoch: 287 Step: 00038100] Batch Recognition Loss:   0.000112 => Gls Tokens per Sec:     1337 || Batch Translation Loss:   0.018734 => Txt Tokens per Sec:     3766 || Lr: 0.000050
2024-02-11 18:59:54,673 Epoch 287: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.16 
2024-02-11 18:59:54,673 EPOCH 288
2024-02-11 18:59:56,382 [Epoch: 288 Step: 00038200] Batch Recognition Loss:   0.000170 => Gls Tokens per Sec:     1358 || Batch Translation Loss:   0.026731 => Txt Tokens per Sec:     3796 || Lr: 0.000050
2024-02-11 19:00:02,162 [Epoch: 288 Step: 00038300] Batch Recognition Loss:   0.000199 => Gls Tokens per Sec:     1383 || Batch Translation Loss:   0.023686 => Txt Tokens per Sec:     3829 || Lr: 0.000050
2024-02-11 19:00:02,387 Epoch 288: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.69 
2024-02-11 19:00:02,387 EPOCH 289
2024-02-11 19:00:08,016 [Epoch: 289 Step: 00038400] Batch Recognition Loss:   0.000155 => Gls Tokens per Sec:     1365 || Batch Translation Loss:   0.014151 => Txt Tokens per Sec:     3816 || Lr: 0.000050
2024-02-11 19:00:10,268 Epoch 289: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.49 
2024-02-11 19:00:10,268 EPOCH 290
2024-02-11 19:00:13,810 [Epoch: 290 Step: 00038500] Batch Recognition Loss:   0.000147 => Gls Tokens per Sec:     1423 || Batch Translation Loss:   0.013247 => Txt Tokens per Sec:     3971 || Lr: 0.000050
2024-02-11 19:00:17,881 Epoch 290: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.77 
2024-02-11 19:00:17,882 EPOCH 291
2024-02-11 19:00:19,540 [Epoch: 291 Step: 00038600] Batch Recognition Loss:   0.000438 => Gls Tokens per Sec:     1448 || Batch Translation Loss:   0.008935 => Txt Tokens per Sec:     4248 || Lr: 0.000050
2024-02-11 19:00:25,374 [Epoch: 291 Step: 00038700] Batch Recognition Loss:   0.000128 => Gls Tokens per Sec:     1370 || Batch Translation Loss:   0.018308 => Txt Tokens per Sec:     3753 || Lr: 0.000050
2024-02-11 19:00:25,621 Epoch 291: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.93 
2024-02-11 19:00:25,621 EPOCH 292
2024-02-11 19:00:31,173 [Epoch: 292 Step: 00038800] Batch Recognition Loss:   0.000153 => Gls Tokens per Sec:     1398 || Batch Translation Loss:   0.032058 => Txt Tokens per Sec:     3894 || Lr: 0.000050
2024-02-11 19:00:33,246 Epoch 292: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.91 
2024-02-11 19:00:33,246 EPOCH 293
2024-02-11 19:00:36,611 [Epoch: 293 Step: 00038900] Batch Recognition Loss:   0.000145 => Gls Tokens per Sec:     1522 || Batch Translation Loss:   0.024161 => Txt Tokens per Sec:     4293 || Lr: 0.000050
2024-02-11 19:00:40,407 Epoch 293: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.52 
2024-02-11 19:00:40,407 EPOCH 294
2024-02-11 19:00:42,011 [Epoch: 294 Step: 00039000] Batch Recognition Loss:   0.000108 => Gls Tokens per Sec:     1548 || Batch Translation Loss:   0.011031 => Txt Tokens per Sec:     4011 || Lr: 0.000050
2024-02-11 19:00:48,066 [Epoch: 294 Step: 00039100] Batch Recognition Loss:   0.000289 => Gls Tokens per Sec:     1320 || Batch Translation Loss:   0.009833 => Txt Tokens per Sec:     3730 || Lr: 0.000050
2024-02-11 19:00:48,207 Epoch 294: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.10 
2024-02-11 19:00:48,208 EPOCH 295
2024-02-11 19:00:53,782 [Epoch: 295 Step: 00039200] Batch Recognition Loss:   0.000265 => Gls Tokens per Sec:     1407 || Batch Translation Loss:   0.022796 => Txt Tokens per Sec:     3904 || Lr: 0.000050
2024-02-11 19:00:55,751 Epoch 295: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.02 
2024-02-11 19:00:55,751 EPOCH 296
2024-02-11 19:00:59,016 [Epoch: 296 Step: 00039300] Batch Recognition Loss:   0.000153 => Gls Tokens per Sec:     1593 || Batch Translation Loss:   0.028808 => Txt Tokens per Sec:     4335 || Lr: 0.000050
2024-02-11 19:01:02,813 Epoch 296: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.05 
2024-02-11 19:01:02,813 EPOCH 297
2024-02-11 19:01:04,831 [Epoch: 297 Step: 00039400] Batch Recognition Loss:   0.000269 => Gls Tokens per Sec:     1269 || Batch Translation Loss:   0.022542 => Txt Tokens per Sec:     3620 || Lr: 0.000050
2024-02-11 19:01:10,551 [Epoch: 297 Step: 00039500] Batch Recognition Loss:   0.000425 => Gls Tokens per Sec:     1397 || Batch Translation Loss:   0.029121 => Txt Tokens per Sec:     3853 || Lr: 0.000050
2024-02-11 19:01:10,591 Epoch 297: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.73 
2024-02-11 19:01:10,592 EPOCH 298
2024-02-11 19:01:16,138 [Epoch: 298 Step: 00039600] Batch Recognition Loss:   0.000112 => Gls Tokens per Sec:     1428 || Batch Translation Loss:   0.031173 => Txt Tokens per Sec:     3949 || Lr: 0.000050
2024-02-11 19:01:17,882 Epoch 298: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.29 
2024-02-11 19:01:17,882 EPOCH 299
2024-02-11 19:01:21,203 [Epoch: 299 Step: 00039700] Batch Recognition Loss:   0.000349 => Gls Tokens per Sec:     1590 || Batch Translation Loss:   0.277371 => Txt Tokens per Sec:     4537 || Lr: 0.000050
2024-02-11 19:01:24,836 Epoch 299: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.96 
2024-02-11 19:01:24,837 EPOCH 300
2024-02-11 19:01:26,739 [Epoch: 300 Step: 00039800] Batch Recognition Loss:   0.000237 => Gls Tokens per Sec:     1389 || Batch Translation Loss:   0.060761 => Txt Tokens per Sec:     3946 || Lr: 0.000050
2024-02-11 19:01:32,624 [Epoch: 300 Step: 00039900] Batch Recognition Loss:   0.000176 => Gls Tokens per Sec:     1358 || Batch Translation Loss:   0.042343 => Txt Tokens per Sec:     3745 || Lr: 0.000050
2024-02-11 19:01:32,625 Epoch 300: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.48 
2024-02-11 19:01:32,625 EPOCH 301
2024-02-11 19:01:38,014 [Epoch: 301 Step: 00040000] Batch Recognition Loss:   0.000153 => Gls Tokens per Sec:     1485 || Batch Translation Loss:   0.034868 => Txt Tokens per Sec:     4116 || Lr: 0.000050
2024-02-11 19:01:49,690 Validation result at epoch 301, step    40000: duration: 11.6752s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00169	Translation Loss: 92740.65625	PPL: 11682.54883
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.50	(BLEU-1: 11.43,	BLEU-2: 3.66,	BLEU-3: 1.31,	BLEU-4: 0.50)
	CHRF 17.07	ROUGE 9.52
2024-02-11 19:01:49,691 Logging Recognition and Translation Outputs
2024-02-11 19:01:49,691 ========================================================================================================================
2024-02-11 19:01:49,692 Logging Sequence: 145_16.00
2024-02-11 19:01:49,692 	Gloss Reference :	A B+C+D+E
2024-02-11 19:01:49,692 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 19:01:49,692 	Gloss Alignment :	         
2024-02-11 19:01:49,692 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 19:01:49,694 	Text Reference  :	*** sameeha barwin an 18 year old deaf girl    from kanyakumari district in  tamil    nadu  
2024-02-11 19:01:49,694 	Text Hypothesis :	the athlete who    is no one  of  the  sameeha who  is          through  the talented player
2024-02-11 19:01:49,694 	Text Alignment  :	I   S       S      S  S  S    S   S    S       S    S           S        S   S        S     
2024-02-11 19:01:49,694 ========================================================================================================================
2024-02-11 19:01:49,694 Logging Sequence: 67_80.00
2024-02-11 19:01:49,694 	Gloss Reference :	A B+C+D+E
2024-02-11 19:01:49,694 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 19:01:49,695 	Gloss Alignment :	         
2024-02-11 19:01:49,695 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 19:01:49,695 	Text Reference  :	india has always been    like    a    second home for me   
2024-02-11 19:01:49,695 	Text Hypothesis :	***** *** ****** against rishabh pant and    led  the hotel
2024-02-11 19:01:49,695 	Text Alignment  :	D     D   D      S       S       S    S      S    S   S    
2024-02-11 19:01:49,696 ========================================================================================================================
2024-02-11 19:01:49,696 Logging Sequence: 169_177.00
2024-02-11 19:01:49,696 	Gloss Reference :	A B+C+D+E
2024-02-11 19:01:49,696 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 19:01:49,696 	Gloss Alignment :	         
2024-02-11 19:01:49,696 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 19:01:49,698 	Text Reference  :	the ****** ** government found     this   unacceptable this     could have put  arshdeep's family in   danger harrassment
2024-02-11 19:01:49,698 	Text Hypothesis :	the series is from       rajasthan royals and          welcomed him   to   come back       to     play the    team       
2024-02-11 19:01:49,698 	Text Alignment  :	    I      I  S          S         S      S            S        S     S    S    S          S      S    S      S          
2024-02-11 19:01:49,698 ========================================================================================================================
2024-02-11 19:01:49,698 Logging Sequence: 96_203.00
2024-02-11 19:01:49,698 	Gloss Reference :	A B+C+D+E
2024-02-11 19:01:49,699 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 19:01:49,699 	Gloss Alignment :	         
2024-02-11 19:01:49,699 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 19:01:49,700 	Text Reference  :	hardik pandya who scored 33 runs in just 17 balls was given the man of   the     match
2024-02-11 19:01:49,700 	Text Hypothesis :	****** the    ipl will   be held in **** ** ***** uae from  csk and 13th october 2022 
2024-02-11 19:01:49,700 	Text Alignment  :	D      S      S   S      S  S       D    D  D     S   S     S   S   S    S       S    
2024-02-11 19:01:49,701 ========================================================================================================================
2024-02-11 19:01:49,701 Logging Sequence: 65_134.00
2024-02-11 19:01:49,701 	Gloss Reference :	A B+C+D+E
2024-02-11 19:01:49,701 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 19:01:49,701 	Gloss Alignment :	         
2024-02-11 19:01:49,701 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 19:01:49,703 	Text Reference  :	********* ********** ** in      a   commercial flight each    ticket would have costed '‚¹2 lakhs totalling to     about '‚¹2 crore
2024-02-11 19:01:49,703 	Text Hypothesis :	wrestling federation of india's wfi is         brij   bhushan sharan singh who  were   also a     huge      amount for   the  match
2024-02-11 19:01:49,703 	Text Alignment  :	I         I          I  S       S   S          S      S       S      S     S    S      S    S     S         S      S     S    S    
2024-02-11 19:01:49,703 ========================================================================================================================
2024-02-11 19:01:51,547 Epoch 301: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.40 
2024-02-11 19:01:51,548 EPOCH 302
2024-02-11 19:01:55,448 [Epoch: 302 Step: 00040100] Batch Recognition Loss:   0.000212 => Gls Tokens per Sec:     1374 || Batch Translation Loss:   0.006350 => Txt Tokens per Sec:     3852 || Lr: 0.000050
2024-02-11 19:01:59,209 Epoch 302: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.58 
2024-02-11 19:01:59,209 EPOCH 303
2024-02-11 19:02:01,145 [Epoch: 303 Step: 00040200] Batch Recognition Loss:   0.000278 => Gls Tokens per Sec:     1406 || Batch Translation Loss:   0.031211 => Txt Tokens per Sec:     4021 || Lr: 0.000050
2024-02-11 19:02:07,364 Epoch 303: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.32 
2024-02-11 19:02:07,365 EPOCH 304
2024-02-11 19:02:07,430 [Epoch: 304 Step: 00040300] Batch Recognition Loss:   0.000201 => Gls Tokens per Sec:     1255 || Batch Translation Loss:   0.018324 => Txt Tokens per Sec:     3293 || Lr: 0.000050
2024-02-11 19:02:13,286 [Epoch: 304 Step: 00040400] Batch Recognition Loss:   0.000224 => Gls Tokens per Sec:     1366 || Batch Translation Loss:   0.013499 => Txt Tokens per Sec:     3754 || Lr: 0.000050
2024-02-11 19:02:15,221 Epoch 304: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.71 
2024-02-11 19:02:15,221 EPOCH 305
2024-02-11 19:02:18,756 [Epoch: 305 Step: 00040500] Batch Recognition Loss:   0.000208 => Gls Tokens per Sec:     1539 || Batch Translation Loss:   0.013735 => Txt Tokens per Sec:     4201 || Lr: 0.000050
2024-02-11 19:02:22,352 Epoch 305: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.44 
2024-02-11 19:02:22,354 EPOCH 306
2024-02-11 19:02:24,430 [Epoch: 306 Step: 00040600] Batch Recognition Loss:   0.000142 => Gls Tokens per Sec:     1350 || Batch Translation Loss:   0.024801 => Txt Tokens per Sec:     3604 || Lr: 0.000050
2024-02-11 19:02:30,389 Epoch 306: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.30 
2024-02-11 19:02:30,389 EPOCH 307
2024-02-11 19:02:30,512 [Epoch: 307 Step: 00040700] Batch Recognition Loss:   0.000246 => Gls Tokens per Sec:     1309 || Batch Translation Loss:   0.035532 => Txt Tokens per Sec:     3853 || Lr: 0.000050
2024-02-11 19:02:36,216 [Epoch: 307 Step: 00040800] Batch Recognition Loss:   0.000177 => Gls Tokens per Sec:     1403 || Batch Translation Loss:   0.034327 => Txt Tokens per Sec:     3909 || Lr: 0.000050
2024-02-11 19:02:37,983 Epoch 307: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.59 
2024-02-11 19:02:37,984 EPOCH 308
2024-02-11 19:02:41,680 [Epoch: 308 Step: 00040900] Batch Recognition Loss:   0.000334 => Gls Tokens per Sec:     1494 || Batch Translation Loss:   0.151857 => Txt Tokens per Sec:     4127 || Lr: 0.000050
2024-02-11 19:02:45,073 Epoch 308: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.53 
2024-02-11 19:02:45,074 EPOCH 309
2024-02-11 19:02:46,973 [Epoch: 309 Step: 00041000] Batch Recognition Loss:   0.000125 => Gls Tokens per Sec:     1517 || Batch Translation Loss:   0.013192 => Txt Tokens per Sec:     4127 || Lr: 0.000050
2024-02-11 19:02:52,746 Epoch 309: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.96 
2024-02-11 19:02:52,747 EPOCH 310
2024-02-11 19:02:52,885 [Epoch: 310 Step: 00041100] Batch Recognition Loss:   0.000340 => Gls Tokens per Sec:     1743 || Batch Translation Loss:   0.012633 => Txt Tokens per Sec:     4313 || Lr: 0.000050
2024-02-11 19:02:58,076 [Epoch: 310 Step: 00041200] Batch Recognition Loss:   0.000222 => Gls Tokens per Sec:     1539 || Batch Translation Loss:   0.018585 => Txt Tokens per Sec:     4307 || Lr: 0.000050
2024-02-11 19:02:59,539 Epoch 310: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.73 
2024-02-11 19:02:59,539 EPOCH 311
2024-02-11 19:03:03,788 [Epoch: 311 Step: 00041300] Batch Recognition Loss:   0.000334 => Gls Tokens per Sec:     1318 || Batch Translation Loss:   0.013072 => Txt Tokens per Sec:     3742 || Lr: 0.000050
2024-02-11 19:03:07,281 Epoch 311: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.75 
2024-02-11 19:03:07,281 EPOCH 312
2024-02-11 19:03:09,304 [Epoch: 312 Step: 00041400] Batch Recognition Loss:   0.000185 => Gls Tokens per Sec:     1464 || Batch Translation Loss:   0.027422 => Txt Tokens per Sec:     4066 || Lr: 0.000050
2024-02-11 19:03:14,337 Epoch 312: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.74 
2024-02-11 19:03:14,338 EPOCH 313
2024-02-11 19:03:14,589 [Epoch: 313 Step: 00041500] Batch Recognition Loss:   0.000164 => Gls Tokens per Sec:     1280 || Batch Translation Loss:   0.017654 => Txt Tokens per Sec:     3493 || Lr: 0.000050
2024-02-11 19:03:20,322 [Epoch: 313 Step: 00041600] Batch Recognition Loss:   0.000161 => Gls Tokens per Sec:     1395 || Batch Translation Loss:   0.007447 => Txt Tokens per Sec:     3867 || Lr: 0.000050
2024-02-11 19:03:21,927 Epoch 313: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.17 
2024-02-11 19:03:21,928 EPOCH 314
2024-02-11 19:03:25,397 [Epoch: 314 Step: 00041700] Batch Recognition Loss:   0.000280 => Gls Tokens per Sec:     1638 || Batch Translation Loss:   0.012453 => Txt Tokens per Sec:     4477 || Lr: 0.000050
2024-02-11 19:03:28,425 Epoch 314: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.90 
2024-02-11 19:03:28,425 EPOCH 315
2024-02-11 19:03:30,466 [Epoch: 315 Step: 00041800] Batch Recognition Loss:   0.000408 => Gls Tokens per Sec:     1490 || Batch Translation Loss:   0.027766 => Txt Tokens per Sec:     4061 || Lr: 0.000050
2024-02-11 19:03:37,641 Epoch 315: Total Training Recognition Loss 0.04  Total Training Translation Loss 6.50 
2024-02-11 19:03:37,641 EPOCH 316
2024-02-11 19:03:37,968 [Epoch: 316 Step: 00041900] Batch Recognition Loss:   0.000182 => Gls Tokens per Sec:     1231 || Batch Translation Loss:   0.013411 => Txt Tokens per Sec:     3253 || Lr: 0.000050
2024-02-11 19:03:45,641 [Epoch: 316 Step: 00042000] Batch Recognition Loss:   0.000125 => Gls Tokens per Sec:     1043 || Batch Translation Loss:   0.031769 => Txt Tokens per Sec:     2932 || Lr: 0.000050
2024-02-11 19:04:00,200 Validation result at epoch 316, step    42000: duration: 14.5582s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00251	Translation Loss: 91253.91406	PPL: 10053.79883
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.49	(BLEU-1: 9.63,	BLEU-2: 2.82,	BLEU-3: 1.01,	BLEU-4: 0.49)
	CHRF 16.99	ROUGE 8.02
2024-02-11 19:04:00,202 Logging Recognition and Translation Outputs
2024-02-11 19:04:00,202 ========================================================================================================================
2024-02-11 19:04:00,202 Logging Sequence: 66_47.00
2024-02-11 19:04:00,203 	Gloss Reference :	A B+C+D+E
2024-02-11 19:04:00,203 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 19:04:00,203 	Gloss Alignment :	         
2024-02-11 19:04:00,203 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 19:04:00,204 	Text Reference  :	and he also provides his valued support and  inputs off the   field  
2024-02-11 19:04:00,204 	Text Hypothesis :	*** ** **** ******** *** i      am      very well   by  these captain
2024-02-11 19:04:00,204 	Text Alignment  :	D   D  D    D        D   S      S       S    S      S   S     S      
2024-02-11 19:04:00,204 ========================================================================================================================
2024-02-11 19:04:00,204 Logging Sequence: 126_202.00
2024-02-11 19:04:00,204 	Gloss Reference :	A B+C+D+E
2024-02-11 19:04:00,205 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 19:04:00,205 	Gloss Alignment :	         
2024-02-11 19:04:00,205 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 19:04:00,206 	Text Reference  :	** haryana government has  announced a  rs 6 crore cash prize 
2024-02-11 19:04:00,206 	Text Hypothesis :	he took    a          cash prize     of rs 1 crore for  neeraj
2024-02-11 19:04:00,206 	Text Alignment  :	I  S       S          S    S         S     S       S    S     
2024-02-11 19:04:00,206 ========================================================================================================================
2024-02-11 19:04:00,206 Logging Sequence: 144_94.00
2024-02-11 19:04:00,206 	Gloss Reference :	A B+C+D+E
2024-02-11 19:04:00,207 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 19:04:00,207 	Gloss Alignment :	         
2024-02-11 19:04:00,207 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 19:04:00,209 	Text Reference  :	fortunately bjp     rajasthan state president satish punia was    deeply moved    and offered   to help her       
2024-02-11 19:04:00,209 	Text Hypothesis :	*********** however there     was   a         lot    of    mental stress everyone was overjoyed by the  tournament
2024-02-11 19:04:00,210 	Text Alignment  :	D           S       S         S     S         S      S     S      S      S        S   S         S  S    S         
2024-02-11 19:04:00,210 ========================================================================================================================
2024-02-11 19:04:00,210 Logging Sequence: 102_56.00
2024-02-11 19:04:00,210 	Gloss Reference :	A B+C+D+E
2024-02-11 19:04:00,210 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 19:04:00,211 	Gloss Alignment :	         
2024-02-11 19:04:00,211 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 19:04:00,212 	Text Reference  :	here are the 3 players who  won  the gold   medal  
2024-02-11 19:04:00,212 	Text Hypothesis :	**** *** *** * there   were many in  azam's support
2024-02-11 19:04:00,212 	Text Alignment  :	D    D   D   D S       S    S    S   S      S      
2024-02-11 19:04:00,212 ========================================================================================================================
2024-02-11 19:04:00,212 Logging Sequence: 64_168.00
2024-02-11 19:04:00,212 	Gloss Reference :	A B+C+D+E
2024-02-11 19:04:00,213 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 19:04:00,213 	Gloss Alignment :	         
2024-02-11 19:04:00,213 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 19:04:00,215 	Text Reference  :	the bcci can not     move ipl to   uae   as the country has banned all indian flights
2024-02-11 19:04:00,215 	Text Hypothesis :	*** who  was working as   a   huge shock as *** there   was sure   for the    ipl    
2024-02-11 19:04:00,215 	Text Alignment  :	D   S    S   S       S    S   S    S        D   S       S   S      S   S      S      
2024-02-11 19:04:00,215 ========================================================================================================================
2024-02-11 19:04:02,357 Epoch 316: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.32 
2024-02-11 19:04:02,357 EPOCH 317
2024-02-11 19:04:07,740 [Epoch: 317 Step: 00042100] Batch Recognition Loss:   0.000144 => Gls Tokens per Sec:     1070 || Batch Translation Loss:   0.027557 => Txt Tokens per Sec:     2985 || Lr: 0.000050
2024-02-11 19:04:11,976 Epoch 317: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.19 
2024-02-11 19:04:11,977 EPOCH 318
2024-02-11 19:04:14,924 [Epoch: 318 Step: 00042200] Batch Recognition Loss:   0.000293 => Gls Tokens per Sec:     1059 || Batch Translation Loss:   0.023592 => Txt Tokens per Sec:     3060 || Lr: 0.000050
2024-02-11 19:04:22,125 Epoch 318: Total Training Recognition Loss 0.04  Total Training Translation Loss 6.74 
2024-02-11 19:04:22,126 EPOCH 319
2024-02-11 19:04:22,513 [Epoch: 319 Step: 00042300] Batch Recognition Loss:   0.000217 => Gls Tokens per Sec:     1245 || Batch Translation Loss:   0.050683 => Txt Tokens per Sec:     3615 || Lr: 0.000050
2024-02-11 19:04:29,724 [Epoch: 319 Step: 00042400] Batch Recognition Loss:   0.000376 => Gls Tokens per Sec:     1109 || Batch Translation Loss:   0.011442 => Txt Tokens per Sec:     3050 || Lr: 0.000050
2024-02-11 19:04:31,925 Epoch 319: Total Training Recognition Loss 0.04  Total Training Translation Loss 4.73 
2024-02-11 19:04:31,925 EPOCH 320
2024-02-11 19:04:37,318 [Epoch: 320 Step: 00042500] Batch Recognition Loss:   0.000151 => Gls Tokens per Sec:     1083 || Batch Translation Loss:   0.010333 => Txt Tokens per Sec:     3014 || Lr: 0.000050
2024-02-11 19:04:41,847 Epoch 320: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.07 
2024-02-11 19:04:41,847 EPOCH 321
2024-02-11 19:04:45,052 [Epoch: 321 Step: 00042600] Batch Recognition Loss:   0.000169 => Gls Tokens per Sec:      999 || Batch Translation Loss:   0.032043 => Txt Tokens per Sec:     2757 || Lr: 0.000050
2024-02-11 19:04:51,710 Epoch 321: Total Training Recognition Loss 0.04  Total Training Translation Loss 4.75 
2024-02-11 19:04:51,711 EPOCH 322
2024-02-11 19:04:52,116 [Epoch: 322 Step: 00042700] Batch Recognition Loss:   0.000450 => Gls Tokens per Sec:     1390 || Batch Translation Loss:   0.027285 => Txt Tokens per Sec:     3777 || Lr: 0.000050
2024-02-11 19:04:59,924 [Epoch: 322 Step: 00042800] Batch Recognition Loss:   0.000177 => Gls Tokens per Sec:     1025 || Batch Translation Loss:   0.031242 => Txt Tokens per Sec:     2854 || Lr: 0.000050
2024-02-11 19:05:01,979 Epoch 322: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.92 
2024-02-11 19:05:01,980 EPOCH 323
2024-02-11 19:05:07,654 [Epoch: 323 Step: 00042900] Batch Recognition Loss:   0.000236 => Gls Tokens per Sec:     1044 || Batch Translation Loss:   0.031661 => Txt Tokens per Sec:     2943 || Lr: 0.000050
2024-02-11 19:05:12,206 Epoch 323: Total Training Recognition Loss 0.04  Total Training Translation Loss 5.53 
2024-02-11 19:05:12,207 EPOCH 324
2024-02-11 19:05:15,511 [Epoch: 324 Step: 00043000] Batch Recognition Loss:   0.000215 => Gls Tokens per Sec:      994 || Batch Translation Loss:   0.022726 => Txt Tokens per Sec:     2808 || Lr: 0.000050
2024-02-11 19:05:22,376 Epoch 324: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.81 
2024-02-11 19:05:22,376 EPOCH 325
2024-02-11 19:05:22,931 [Epoch: 325 Step: 00043100] Batch Recognition Loss:   0.000206 => Gls Tokens per Sec:     1155 || Batch Translation Loss:   0.038148 => Txt Tokens per Sec:     3037 || Lr: 0.000050
2024-02-11 19:05:30,215 [Epoch: 325 Step: 00043200] Batch Recognition Loss:   0.000095 => Gls Tokens per Sec:     1098 || Batch Translation Loss:   0.023902 => Txt Tokens per Sec:     3055 || Lr: 0.000050
2024-02-11 19:05:32,226 Epoch 325: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.88 
2024-02-11 19:05:32,226 EPOCH 326
2024-02-11 19:05:37,295 [Epoch: 326 Step: 00043300] Batch Recognition Loss:   0.000225 => Gls Tokens per Sec:     1184 || Batch Translation Loss:   0.044526 => Txt Tokens per Sec:     3231 || Lr: 0.000050
2024-02-11 19:05:42,146 Epoch 326: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.74 
2024-02-11 19:05:42,147 EPOCH 327
2024-02-11 19:05:45,334 [Epoch: 327 Step: 00043400] Batch Recognition Loss:   0.000279 => Gls Tokens per Sec:     1055 || Batch Translation Loss:   0.026630 => Txt Tokens per Sec:     2946 || Lr: 0.000050
2024-02-11 19:05:52,226 Epoch 327: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.27 
2024-02-11 19:05:52,227 EPOCH 328
2024-02-11 19:05:52,988 [Epoch: 328 Step: 00043500] Batch Recognition Loss:   0.000226 => Gls Tokens per Sec:      948 || Batch Translation Loss:   0.025195 => Txt Tokens per Sec:     2323 || Lr: 0.000050
2024-02-11 19:06:00,508 [Epoch: 328 Step: 00043600] Batch Recognition Loss:   0.000353 => Gls Tokens per Sec:     1064 || Batch Translation Loss:   0.007898 => Txt Tokens per Sec:     2954 || Lr: 0.000050
2024-02-11 19:06:02,474 Epoch 328: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.37 
2024-02-11 19:06:02,474 EPOCH 329
2024-02-11 19:06:08,211 [Epoch: 329 Step: 00043700] Batch Recognition Loss:   0.000160 => Gls Tokens per Sec:     1060 || Batch Translation Loss:   0.018294 => Txt Tokens per Sec:     2925 || Lr: 0.000050
2024-02-11 19:06:12,400 Epoch 329: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.95 
2024-02-11 19:06:12,400 EPOCH 330
2024-02-11 19:06:15,622 [Epoch: 330 Step: 00043800] Batch Recognition Loss:   0.000264 => Gls Tokens per Sec:     1068 || Batch Translation Loss:   0.028104 => Txt Tokens per Sec:     3040 || Lr: 0.000050
2024-02-11 19:06:22,717 Epoch 330: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.75 
2024-02-11 19:06:22,718 EPOCH 331
2024-02-11 19:06:23,379 [Epoch: 331 Step: 00043900] Batch Recognition Loss:   0.000154 => Gls Tokens per Sec:     1212 || Batch Translation Loss:   0.008192 => Txt Tokens per Sec:     3349 || Lr: 0.000050
2024-02-11 19:06:30,592 [Epoch: 331 Step: 00044000] Batch Recognition Loss:   0.000251 => Gls Tokens per Sec:     1109 || Batch Translation Loss:   0.018597 => Txt Tokens per Sec:     3111 || Lr: 0.000050
2024-02-11 19:06:45,233 Validation result at epoch 331, step    44000: duration: 14.6416s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00199	Translation Loss: 90891.32812	PPL: 9692.31543
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.46	(BLEU-1: 10.01,	BLEU-2: 2.92,	BLEU-3: 1.16,	BLEU-4: 0.46)
	CHRF 16.75	ROUGE 8.33
2024-02-11 19:06:45,235 Logging Recognition and Translation Outputs
2024-02-11 19:06:45,235 ========================================================================================================================
2024-02-11 19:06:45,235 Logging Sequence: 59_2.00
2024-02-11 19:06:45,235 	Gloss Reference :	A B+C+D+E
2024-02-11 19:06:45,235 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 19:06:45,235 	Gloss Alignment :	         
2024-02-11 19:06:45,236 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 19:06:45,236 	Text Reference  :	at the 2020 tokyo olympics in    japan          
2024-02-11 19:06:45,236 	Text Hypothesis :	** *** **** ***** once     again congratulations
2024-02-11 19:06:45,236 	Text Alignment  :	D  D   D    D     S        S     S              
2024-02-11 19:06:45,236 ========================================================================================================================
2024-02-11 19:06:45,237 Logging Sequence: 169_165.00
2024-02-11 19:06:45,237 	Gloss Reference :	A B+C+D+E
2024-02-11 19:06:45,237 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 19:06:45,237 	Gloss Alignment :	         
2024-02-11 19:06:45,237 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 19:06:45,239 	Text Reference  :	the indian government was outraged by     the   incident and these changes were undone by    wikipedia
2024-02-11 19:06:45,239 	Text Hypothesis :	the ****** teams      can be       edited false info     can be    removed and  the    right time     
2024-02-11 19:06:45,240 	Text Alignment  :	    D      S          S   S        S      S     S        S   S     S       S    S      S     S        
2024-02-11 19:06:45,240 ========================================================================================================================
2024-02-11 19:06:45,240 Logging Sequence: 100_103.00
2024-02-11 19:06:45,240 	Gloss Reference :	A B+C+D+E
2024-02-11 19:06:45,240 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 19:06:45,240 	Gloss Alignment :	         
2024-02-11 19:06:45,240 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 19:06:45,241 	Text Reference  :	which is why two separate teams were    sent  
2024-02-11 19:06:45,241 	Text Hypothesis :	he    is *** *** ******** an    amazing player
2024-02-11 19:06:45,241 	Text Alignment  :	S        D   D   D        S     S       S     
2024-02-11 19:06:45,241 ========================================================================================================================
2024-02-11 19:06:45,242 Logging Sequence: 157_83.00
2024-02-11 19:06:45,242 	Gloss Reference :	A B+C+D+E
2024-02-11 19:06:45,242 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 19:06:45,242 	Gloss Alignment :	         
2024-02-11 19:06:45,242 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 19:06:45,244 	Text Reference  :	also when you eat sandwich at a streetside hawker or stall the sandwich maker will   first     apply butter with a    knife  
2024-02-11 19:06:45,244 	Text Hypothesis :	**** **** *** *** ******** ** * ********** ****** ** ***** the ******** ***** sports authority of    india  sai  then tweeted
2024-02-11 19:06:45,244 	Text Alignment  :	D    D    D   D   D        D  D D          D      D  D         D        D     S      S         S     S      S    S    S      
2024-02-11 19:06:45,244 ========================================================================================================================
2024-02-11 19:06:45,245 Logging Sequence: 73_116.00
2024-02-11 19:06:45,245 	Gloss Reference :	A B+C+D+E
2024-02-11 19:06:45,245 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 19:06:45,245 	Gloss Alignment :	         
2024-02-11 19:06:45,245 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 19:06:45,247 	Text Reference  :	the  cheapest dish    on     the   menu is raita   whereas the costliest dish is     
2024-02-11 19:06:45,247 	Text Hypothesis :	this is       because sushil kumar will be playing in      the ********* **** threats
2024-02-11 19:06:45,247 	Text Alignment  :	S    S        S       S      S     S    S  S       S           D         D    S      
2024-02-11 19:06:45,247 ========================================================================================================================
2024-02-11 19:06:46,899 Epoch 331: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.16 
2024-02-11 19:06:46,899 EPOCH 332
2024-02-11 19:06:52,842 [Epoch: 332 Step: 00044100] Batch Recognition Loss:   0.000164 => Gls Tokens per Sec:     1037 || Batch Translation Loss:   0.015448 => Txt Tokens per Sec:     2875 || Lr: 0.000050
2024-02-11 19:06:57,073 Epoch 332: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.92 
2024-02-11 19:06:57,073 EPOCH 333
2024-02-11 19:07:00,486 [Epoch: 333 Step: 00044200] Batch Recognition Loss:   0.000168 => Gls Tokens per Sec:     1032 || Batch Translation Loss:   0.020010 => Txt Tokens per Sec:     2937 || Lr: 0.000050
2024-02-11 19:07:07,493 Epoch 333: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.97 
2024-02-11 19:07:07,494 EPOCH 334
2024-02-11 19:07:08,181 [Epoch: 334 Step: 00044300] Batch Recognition Loss:   0.000126 => Gls Tokens per Sec:     1282 || Batch Translation Loss:   0.016479 => Txt Tokens per Sec:     3409 || Lr: 0.000050
2024-02-11 19:07:15,936 [Epoch: 334 Step: 00044400] Batch Recognition Loss:   0.000217 => Gls Tokens per Sec:     1032 || Batch Translation Loss:   0.004866 => Txt Tokens per Sec:     2873 || Lr: 0.000050
2024-02-11 19:07:17,849 Epoch 334: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.37 
2024-02-11 19:07:17,849 EPOCH 335
2024-02-11 19:07:24,314 [Epoch: 335 Step: 00044500] Batch Recognition Loss:   0.000139 => Gls Tokens per Sec:      966 || Batch Translation Loss:   0.017910 => Txt Tokens per Sec:     2687 || Lr: 0.000050
2024-02-11 19:07:28,025 Epoch 335: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.71 
2024-02-11 19:07:28,025 EPOCH 336
2024-02-11 19:07:30,472 [Epoch: 336 Step: 00044600] Batch Recognition Loss:   0.000145 => Gls Tokens per Sec:     1471 || Batch Translation Loss:   0.029606 => Txt Tokens per Sec:     4203 || Lr: 0.000050
2024-02-11 19:07:35,666 Epoch 336: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.51 
2024-02-11 19:07:35,666 EPOCH 337
2024-02-11 19:07:36,496 [Epoch: 337 Step: 00044700] Batch Recognition Loss:   0.000283 => Gls Tokens per Sec:     1161 || Batch Translation Loss:   0.064810 => Txt Tokens per Sec:     3446 || Lr: 0.000050
2024-02-11 19:07:42,368 [Epoch: 337 Step: 00044800] Batch Recognition Loss:   0.000207 => Gls Tokens per Sec:     1362 || Batch Translation Loss:   0.053380 => Txt Tokens per Sec:     3713 || Lr: 0.000050
2024-02-11 19:07:43,754 Epoch 337: Total Training Recognition Loss 0.03  Total Training Translation Loss 12.15 
2024-02-11 19:07:43,754 EPOCH 338
2024-02-11 19:07:47,977 [Epoch: 338 Step: 00044900] Batch Recognition Loss:   0.000437 => Gls Tokens per Sec:     1497 || Batch Translation Loss:   0.024218 => Txt Tokens per Sec:     4176 || Lr: 0.000050
2024-02-11 19:07:51,055 Epoch 338: Total Training Recognition Loss 0.04  Total Training Translation Loss 12.80 
2024-02-11 19:07:51,056 EPOCH 339
2024-02-11 19:07:53,928 [Epoch: 339 Step: 00045000] Batch Recognition Loss:   0.000267 => Gls Tokens per Sec:     1282 || Batch Translation Loss:   0.024154 => Txt Tokens per Sec:     3636 || Lr: 0.000050
2024-02-11 19:07:59,033 Epoch 339: Total Training Recognition Loss 0.04  Total Training Translation Loss 6.82 
2024-02-11 19:07:59,034 EPOCH 340
2024-02-11 19:07:59,797 [Epoch: 340 Step: 00045100] Batch Recognition Loss:   0.000456 => Gls Tokens per Sec:     1365 || Batch Translation Loss:   0.040227 => Txt Tokens per Sec:     4029 || Lr: 0.000050
2024-02-11 19:08:05,650 [Epoch: 340 Step: 00045200] Batch Recognition Loss:   0.000123 => Gls Tokens per Sec:     1367 || Batch Translation Loss:   0.015341 => Txt Tokens per Sec:     3740 || Lr: 0.000050
2024-02-11 19:08:07,020 Epoch 340: Total Training Recognition Loss 0.04  Total Training Translation Loss 4.73 
2024-02-11 19:08:07,020 EPOCH 341
2024-02-11 19:08:11,752 [Epoch: 341 Step: 00045300] Batch Recognition Loss:   0.000195 => Gls Tokens per Sec:     1353 || Batch Translation Loss:   0.011969 => Txt Tokens per Sec:     3708 || Lr: 0.000050
2024-02-11 19:08:14,875 Epoch 341: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.82 
2024-02-11 19:08:14,876 EPOCH 342
2024-02-11 19:08:17,822 [Epoch: 342 Step: 00045400] Batch Recognition Loss:   0.000201 => Gls Tokens per Sec:     1277 || Batch Translation Loss:   0.033058 => Txt Tokens per Sec:     3540 || Lr: 0.000050
2024-02-11 19:08:22,749 Epoch 342: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.45 
2024-02-11 19:08:22,749 EPOCH 343
2024-02-11 19:08:23,527 [Epoch: 343 Step: 00045500] Batch Recognition Loss:   0.000182 => Gls Tokens per Sec:     1442 || Batch Translation Loss:   0.009727 => Txt Tokens per Sec:     3873 || Lr: 0.000050
2024-02-11 19:08:29,414 [Epoch: 343 Step: 00045600] Batch Recognition Loss:   0.000204 => Gls Tokens per Sec:     1359 || Batch Translation Loss:   0.029482 => Txt Tokens per Sec:     3777 || Lr: 0.000050
2024-02-11 19:08:30,569 Epoch 343: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.20 
2024-02-11 19:08:30,569 EPOCH 344
2024-02-11 19:08:35,450 [Epoch: 344 Step: 00045700] Batch Recognition Loss:   0.000229 => Gls Tokens per Sec:     1328 || Batch Translation Loss:   0.099690 => Txt Tokens per Sec:     3672 || Lr: 0.000050
2024-02-11 19:08:38,408 Epoch 344: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.15 
2024-02-11 19:08:38,408 EPOCH 345
2024-02-11 19:08:41,089 [Epoch: 345 Step: 00045800] Batch Recognition Loss:   0.000248 => Gls Tokens per Sec:     1433 || Batch Translation Loss:   0.056945 => Txt Tokens per Sec:     3988 || Lr: 0.000050
2024-02-11 19:08:46,182 Epoch 345: Total Training Recognition Loss 0.04  Total Training Translation Loss 6.50 
2024-02-11 19:08:46,182 EPOCH 346
2024-02-11 19:08:47,091 [Epoch: 346 Step: 00045900] Batch Recognition Loss:   0.000156 => Gls Tokens per Sec:     1322 || Batch Translation Loss:   0.019069 => Txt Tokens per Sec:     4060 || Lr: 0.000050
2024-02-11 19:08:53,053 [Epoch: 346 Step: 00046000] Batch Recognition Loss:   0.000124 => Gls Tokens per Sec:     1340 || Batch Translation Loss:   0.033228 => Txt Tokens per Sec:     3636 || Lr: 0.000050
2024-02-11 19:09:04,213 Validation result at epoch 346, step    46000: duration: 11.1600s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00212	Translation Loss: 91434.19531	PPL: 10238.52051
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.73	(BLEU-1: 10.77,	BLEU-2: 3.44,	BLEU-3: 1.48,	BLEU-4: 0.73)
	CHRF 17.29	ROUGE 8.91
2024-02-11 19:09:04,214 Logging Recognition and Translation Outputs
2024-02-11 19:09:04,214 ========================================================================================================================
2024-02-11 19:09:04,214 Logging Sequence: 61_280.00
2024-02-11 19:09:04,214 	Gloss Reference :	A B+C+D+E
2024-02-11 19:09:04,215 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 19:09:04,215 	Gloss Alignment :	         
2024-02-11 19:09:04,215 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 19:09:04,215 	Text Reference  :	************ * ***** ********* *** babar kept me      in rented houses and  
2024-02-11 19:09:04,216 	Text Hypothesis :	surprisingly 5 years argentina won the   same however he is     also   watch
2024-02-11 19:09:04,216 	Text Alignment  :	I            I I     I         I   S     S    S       S  S      S      S    
2024-02-11 19:09:04,216 ========================================================================================================================
2024-02-11 19:09:04,216 Logging Sequence: 144_84.00
2024-02-11 19:09:04,216 	Gloss Reference :	A B+C+D+E
2024-02-11 19:09:04,216 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 19:09:04,216 	Gloss Alignment :	         
2024-02-11 19:09:04,217 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 19:09:04,218 	Text Reference  :	mumal shared that since her   family is  very   poor      they   were unable to      afford a   cricket kit     
2024-02-11 19:09:04,218 	Text Hypothesis :	since there  was  an    image of     the indian cricketer wishes of   her    hitting fours  and wished  happened
2024-02-11 19:09:04,218 	Text Alignment  :	S     S      S    S     S     S      S   S      S         S      S    S      S       S      S   S       S       
2024-02-11 19:09:04,219 ========================================================================================================================
2024-02-11 19:09:04,219 Logging Sequence: 98_175.00
2024-02-11 19:09:04,219 	Gloss Reference :	A B+C+D+E
2024-02-11 19:09:04,219 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 19:09:04,219 	Gloss Alignment :	         
2024-02-11 19:09:04,219 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 19:09:04,220 	Text Reference  :	yuvraj singh shared a   video     on     instagram
2024-02-11 19:09:04,220 	Text Hypothesis :	she    is    happy  and applauded bcci's decision 
2024-02-11 19:09:04,220 	Text Alignment  :	S      S     S      S   S         S      S        
2024-02-11 19:09:04,220 ========================================================================================================================
2024-02-11 19:09:04,220 Logging Sequence: 119_20.00
2024-02-11 19:09:04,220 	Gloss Reference :	A B+C+D+E
2024-02-11 19:09:04,220 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 19:09:04,221 	Gloss Alignment :	         
2024-02-11 19:09:04,221 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 19:09:04,222 	Text Reference  :	**** messi intended to  gift something to     all the players and the staff to special to celebrate the moment
2024-02-11 19:09:04,222 	Text Hypothesis :	they said  it       was a    proud     moment for the camera  and *** ***** ** ******* ** ********* the camera
2024-02-11 19:09:04,222 	Text Alignment  :	I    S     S        S   S    S         S      S       S           D   D     D  D       D  D             S     
2024-02-11 19:09:04,222 ========================================================================================================================
2024-02-11 19:09:04,223 Logging Sequence: 97_221.00
2024-02-11 19:09:04,223 	Gloss Reference :	A B+C+D+E
2024-02-11 19:09:04,223 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 19:09:04,223 	Gloss Alignment :	         
2024-02-11 19:09:04,223 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 19:09:04,225 	Text Reference  :	he said 'people must learn 5 words in sign language per  day       it  will be  beneficial to   communicate with    the deaf    community'
2024-02-11 19:09:04,225 	Text Hypothesis :	he **** ******* **** ***** * ***** ** also thanked  bcci secretary jay shah for his        care and         concern he  thanked everyone  
2024-02-11 19:09:04,225 	Text Alignment  :	   D    D       D    D     D D     D  S    S        S    S         S   S    S   S          S    S           S       S   S       S         
2024-02-11 19:09:04,225 ========================================================================================================================
2024-02-11 19:09:05,295 Epoch 346: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.05 
2024-02-11 19:09:05,295 EPOCH 347
2024-02-11 19:09:10,363 [Epoch: 347 Step: 00046100] Batch Recognition Loss:   0.000149 => Gls Tokens per Sec:     1295 || Batch Translation Loss:   0.020344 => Txt Tokens per Sec:     3607 || Lr: 0.000050
2024-02-11 19:09:13,361 Epoch 347: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.20 
2024-02-11 19:09:13,361 EPOCH 348
2024-02-11 19:09:16,127 [Epoch: 348 Step: 00046200] Batch Recognition Loss:   0.000315 => Gls Tokens per Sec:     1418 || Batch Translation Loss:   0.066082 => Txt Tokens per Sec:     3962 || Lr: 0.000050
2024-02-11 19:09:21,199 Epoch 348: Total Training Recognition Loss 0.03  Total Training Translation Loss 10.00 
2024-02-11 19:09:21,200 EPOCH 349
2024-02-11 19:09:22,093 [Epoch: 349 Step: 00046300] Batch Recognition Loss:   0.000396 => Gls Tokens per Sec:     1434 || Batch Translation Loss:   0.032922 => Txt Tokens per Sec:     3986 || Lr: 0.000050
2024-02-11 19:09:28,039 [Epoch: 349 Step: 00046400] Batch Recognition Loss:   0.000236 => Gls Tokens per Sec:     1344 || Batch Translation Loss:   0.030061 => Txt Tokens per Sec:     3731 || Lr: 0.000050
2024-02-11 19:09:29,058 Epoch 349: Total Training Recognition Loss 0.04  Total Training Translation Loss 7.28 
2024-02-11 19:09:29,058 EPOCH 350
2024-02-11 19:09:33,972 [Epoch: 350 Step: 00046500] Batch Recognition Loss:   0.000213 => Gls Tokens per Sec:     1352 || Batch Translation Loss:   0.029279 => Txt Tokens per Sec:     3730 || Lr: 0.000050
2024-02-11 19:09:36,993 Epoch 350: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.29 
2024-02-11 19:09:36,994 EPOCH 351
2024-02-11 19:09:39,818 [Epoch: 351 Step: 00046600] Batch Recognition Loss:   0.000180 => Gls Tokens per Sec:     1416 || Batch Translation Loss:   0.040151 => Txt Tokens per Sec:     3955 || Lr: 0.000050
2024-02-11 19:09:44,832 Epoch 351: Total Training Recognition Loss 0.04  Total Training Translation Loss 5.47 
2024-02-11 19:09:44,833 EPOCH 352
2024-02-11 19:09:45,737 [Epoch: 352 Step: 00046700] Batch Recognition Loss:   0.000138 => Gls Tokens per Sec:     1505 || Batch Translation Loss:   0.028780 => Txt Tokens per Sec:     4069 || Lr: 0.000050
2024-02-11 19:09:51,847 [Epoch: 352 Step: 00046800] Batch Recognition Loss:   0.000194 => Gls Tokens per Sec:     1308 || Batch Translation Loss:   0.105342 => Txt Tokens per Sec:     3642 || Lr: 0.000050
2024-02-11 19:09:52,776 Epoch 352: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.09 
2024-02-11 19:09:52,776 EPOCH 353
2024-02-11 19:09:57,799 [Epoch: 353 Step: 00046900] Batch Recognition Loss:   0.000203 => Gls Tokens per Sec:     1338 || Batch Translation Loss:   0.022876 => Txt Tokens per Sec:     3753 || Lr: 0.000050
2024-02-11 19:10:00,817 Epoch 353: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.44 
2024-02-11 19:10:00,818 EPOCH 354
2024-02-11 19:10:03,486 [Epoch: 354 Step: 00047000] Batch Recognition Loss:   0.000275 => Gls Tokens per Sec:     1529 || Batch Translation Loss:   0.026414 => Txt Tokens per Sec:     4347 || Lr: 0.000050
2024-02-11 19:10:08,552 Epoch 354: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.32 
2024-02-11 19:10:08,553 EPOCH 355
2024-02-11 19:10:09,576 [Epoch: 355 Step: 00047100] Batch Recognition Loss:   0.000176 => Gls Tokens per Sec:     1410 || Batch Translation Loss:   0.019341 => Txt Tokens per Sec:     3920 || Lr: 0.000050
2024-02-11 19:10:15,658 [Epoch: 355 Step: 00047200] Batch Recognition Loss:   0.000290 => Gls Tokens per Sec:     1315 || Batch Translation Loss:   0.035389 => Txt Tokens per Sec:     3651 || Lr: 0.000050
2024-02-11 19:10:16,598 Epoch 355: Total Training Recognition Loss 0.04  Total Training Translation Loss 4.85 
2024-02-11 19:10:16,598 EPOCH 356
2024-02-11 19:10:21,842 [Epoch: 356 Step: 00047300] Batch Recognition Loss:   0.000140 => Gls Tokens per Sec:     1297 || Batch Translation Loss:   0.019071 => Txt Tokens per Sec:     3649 || Lr: 0.000050
2024-02-11 19:10:24,665 Epoch 356: Total Training Recognition Loss 0.04  Total Training Translation Loss 5.91 
2024-02-11 19:10:24,665 EPOCH 357
2024-02-11 19:10:27,792 [Epoch: 357 Step: 00047400] Batch Recognition Loss:   0.000145 => Gls Tokens per Sec:     1331 || Batch Translation Loss:   0.014999 => Txt Tokens per Sec:     3812 || Lr: 0.000050
2024-02-11 19:10:32,561 Epoch 357: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.49 
2024-02-11 19:10:32,561 EPOCH 358
2024-02-11 19:10:33,821 [Epoch: 358 Step: 00047500] Batch Recognition Loss:   0.000262 => Gls Tokens per Sec:     1207 || Batch Translation Loss:   0.032442 => Txt Tokens per Sec:     3486 || Lr: 0.000050
2024-02-11 19:10:39,595 [Epoch: 358 Step: 00047600] Batch Recognition Loss:   0.000252 => Gls Tokens per Sec:     1384 || Batch Translation Loss:   0.651317 => Txt Tokens per Sec:     3841 || Lr: 0.000050
2024-02-11 19:10:40,393 Epoch 358: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.04 
2024-02-11 19:10:40,393 EPOCH 359
2024-02-11 19:10:45,038 [Epoch: 359 Step: 00047700] Batch Recognition Loss:   0.000290 => Gls Tokens per Sec:     1482 || Batch Translation Loss:   0.173408 => Txt Tokens per Sec:     4073 || Lr: 0.000050
2024-02-11 19:10:47,872 Epoch 359: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.23 
2024-02-11 19:10:47,873 EPOCH 360
2024-02-11 19:10:51,106 [Epoch: 360 Step: 00047800] Batch Recognition Loss:   0.000303 => Gls Tokens per Sec:     1312 || Batch Translation Loss:   0.023257 => Txt Tokens per Sec:     3629 || Lr: 0.000050
2024-02-11 19:10:56,033 Epoch 360: Total Training Recognition Loss 0.04  Total Training Translation Loss 5.28 
2024-02-11 19:10:56,033 EPOCH 361
2024-02-11 19:10:57,076 [Epoch: 361 Step: 00047900] Batch Recognition Loss:   0.000340 => Gls Tokens per Sec:     1537 || Batch Translation Loss:   0.040772 => Txt Tokens per Sec:     4496 || Lr: 0.000050
2024-02-11 19:11:02,521 [Epoch: 361 Step: 00048000] Batch Recognition Loss:   0.000150 => Gls Tokens per Sec:     1467 || Batch Translation Loss:   0.037905 => Txt Tokens per Sec:     4016 || Lr: 0.000050
2024-02-11 19:11:13,774 Validation result at epoch 361, step    48000: duration: 11.2522s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00231	Translation Loss: 90797.26562	PPL: 9600.67773
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.63	(BLEU-1: 10.21,	BLEU-2: 3.22,	BLEU-3: 1.29,	BLEU-4: 0.63)
	CHRF 16.66	ROUGE 8.83
2024-02-11 19:11:13,776 Logging Recognition and Translation Outputs
2024-02-11 19:11:13,776 ========================================================================================================================
2024-02-11 19:11:13,776 Logging Sequence: 125_105.00
2024-02-11 19:11:13,776 	Gloss Reference :	A B+C+D+E
2024-02-11 19:11:13,777 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 19:11:13,777 	Gloss Alignment :	         
2024-02-11 19:11:13,777 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 19:11:13,778 	Text Reference  :	people must    not target nadeem let      me    tell you   what happened
2024-02-11 19:11:13,778 	Text Hypothesis :	****** because of  the    recent covid-19 spike in   india the  hotel   
2024-02-11 19:11:13,778 	Text Alignment  :	D      S       S   S      S      S        S     S    S     S    S       
2024-02-11 19:11:13,778 ========================================================================================================================
2024-02-11 19:11:13,778 Logging Sequence: 79_185.00
2024-02-11 19:11:13,778 	Gloss Reference :	A B+C+D+E
2024-02-11 19:11:13,778 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 19:11:13,779 	Gloss Alignment :	         
2024-02-11 19:11:13,779 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 19:11:13,779 	Text Reference  :	then the match will be        cancelled and postponed
2024-02-11 19:11:13,779 	Text Hypothesis :	**** *** i     have qualified for       the finals   
2024-02-11 19:11:13,779 	Text Alignment  :	D    D   S     S    S         S         S   S        
2024-02-11 19:11:13,779 ========================================================================================================================
2024-02-11 19:11:13,780 Logging Sequence: 142_73.00
2024-02-11 19:11:13,780 	Gloss Reference :	A B+C+D+E
2024-02-11 19:11:13,780 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 19:11:13,780 	Gloss Alignment :	         
2024-02-11 19:11:13,780 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 19:11:13,781 	Text Reference  :	he was **** detained as       well     strange
2024-02-11 19:11:13,781 	Text Hypothesis :	it was held in       mumbai's wankhede stadium
2024-02-11 19:11:13,781 	Text Alignment  :	S      I    S        S        S        S      
2024-02-11 19:11:13,781 ========================================================================================================================
2024-02-11 19:11:13,781 Logging Sequence: 128_77.00
2024-02-11 19:11:13,781 	Gloss Reference :	A B+C+D+E
2024-02-11 19:11:13,781 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 19:11:13,781 	Gloss Alignment :	         
2024-02-11 19:11:13,782 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 19:11:13,782 	Text Reference  :	in the first innings new    zealand had   already scored 249  runs
2024-02-11 19:11:13,782 	Text Hypothesis :	** the first time    hardik met     india and     was    very well
2024-02-11 19:11:13,783 	Text Alignment  :	D            S       S      S       S     S       S      S    S   
2024-02-11 19:11:13,783 ========================================================================================================================
2024-02-11 19:11:13,783 Logging Sequence: 122_81.00
2024-02-11 19:11:13,783 	Gloss Reference :	A B+C+D+E
2024-02-11 19:11:13,783 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 19:11:13,783 	Gloss Alignment :	         
2024-02-11 19:11:13,783 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 19:11:13,784 	Text Reference  :	and has also become the 1st weightlifter win    a silver medal *** *************
2024-02-11 19:11:13,784 	Text Hypothesis :	*** *** **** ****** *** *** she          secure a silver medal for weightlifting
2024-02-11 19:11:13,784 	Text Alignment  :	D   D   D    D      D   D   S            S                     I   I            
2024-02-11 19:11:13,784 ========================================================================================================================
2024-02-11 19:11:14,607 Epoch 361: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.77 
2024-02-11 19:11:14,608 EPOCH 362
2024-02-11 19:11:19,316 [Epoch: 362 Step: 00048100] Batch Recognition Loss:   0.000172 => Gls Tokens per Sec:     1478 || Batch Translation Loss:   0.023602 => Txt Tokens per Sec:     4096 || Lr: 0.000050
2024-02-11 19:11:21,908 Epoch 362: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.18 
2024-02-11 19:11:21,908 EPOCH 363
2024-02-11 19:11:24,340 [Epoch: 363 Step: 00048200] Batch Recognition Loss:   0.000249 => Gls Tokens per Sec:     1778 || Batch Translation Loss:   0.022911 => Txt Tokens per Sec:     4834 || Lr: 0.000050
2024-02-11 19:11:28,669 Epoch 363: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.08 
2024-02-11 19:11:28,670 EPOCH 364
2024-02-11 19:11:29,918 [Epoch: 364 Step: 00048300] Batch Recognition Loss:   0.000255 => Gls Tokens per Sec:     1348 || Batch Translation Loss:   0.050565 => Txt Tokens per Sec:     3728 || Lr: 0.000050
2024-02-11 19:11:35,295 [Epoch: 364 Step: 00048400] Batch Recognition Loss:   0.000874 => Gls Tokens per Sec:     1488 || Batch Translation Loss:   0.020558 => Txt Tokens per Sec:     4130 || Lr: 0.000050
2024-02-11 19:11:36,016 Epoch 364: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.74 
2024-02-11 19:11:36,016 EPOCH 365
2024-02-11 19:11:40,426 [Epoch: 365 Step: 00048500] Batch Recognition Loss:   0.000161 => Gls Tokens per Sec:     1597 || Batch Translation Loss:   0.037730 => Txt Tokens per Sec:     4413 || Lr: 0.000050
2024-02-11 19:11:43,176 Epoch 365: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.49 
2024-02-11 19:11:43,177 EPOCH 366
2024-02-11 19:11:46,016 [Epoch: 366 Step: 00048600] Batch Recognition Loss:   0.000196 => Gls Tokens per Sec:     1550 || Batch Translation Loss:   0.028195 => Txt Tokens per Sec:     4169 || Lr: 0.000050
2024-02-11 19:11:50,281 Epoch 366: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.91 
2024-02-11 19:11:50,281 EPOCH 367
2024-02-11 19:11:51,420 [Epoch: 367 Step: 00048700] Batch Recognition Loss:   0.000264 => Gls Tokens per Sec:     1546 || Batch Translation Loss:   0.019761 => Txt Tokens per Sec:     4410 || Lr: 0.000050
2024-02-11 19:11:57,073 [Epoch: 367 Step: 00048800] Batch Recognition Loss:   0.000196 => Gls Tokens per Sec:     1414 || Batch Translation Loss:   0.042176 => Txt Tokens per Sec:     3899 || Lr: 0.000050
2024-02-11 19:11:57,741 Epoch 367: Total Training Recognition Loss 0.05  Total Training Translation Loss 11.39 
2024-02-11 19:11:57,741 EPOCH 368
2024-02-11 19:12:02,427 [Epoch: 368 Step: 00048900] Batch Recognition Loss:   0.000306 => Gls Tokens per Sec:     1520 || Batch Translation Loss:   0.075739 => Txt Tokens per Sec:     4177 || Lr: 0.000050
2024-02-11 19:12:05,053 Epoch 368: Total Training Recognition Loss 0.04  Total Training Translation Loss 5.98 
2024-02-11 19:12:05,053 EPOCH 369
2024-02-11 19:12:08,371 [Epoch: 369 Step: 00049000] Batch Recognition Loss:   0.000264 => Gls Tokens per Sec:     1351 || Batch Translation Loss:   0.145809 => Txt Tokens per Sec:     3787 || Lr: 0.000050
2024-02-11 19:12:13,183 Epoch 369: Total Training Recognition Loss 0.04  Total Training Translation Loss 6.63 
2024-02-11 19:12:13,184 EPOCH 370
2024-02-11 19:12:14,501 [Epoch: 370 Step: 00049100] Batch Recognition Loss:   0.000323 => Gls Tokens per Sec:     1398 || Batch Translation Loss:   0.015273 => Txt Tokens per Sec:     3954 || Lr: 0.000050
2024-02-11 19:12:20,544 [Epoch: 370 Step: 00049200] Batch Recognition Loss:   0.000128 => Gls Tokens per Sec:     1324 || Batch Translation Loss:   0.019560 => Txt Tokens per Sec:     3680 || Lr: 0.000050
2024-02-11 19:12:21,169 Epoch 370: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.94 
2024-02-11 19:12:21,169 EPOCH 371
2024-02-11 19:12:26,470 [Epoch: 371 Step: 00049300] Batch Recognition Loss:   0.000284 => Gls Tokens per Sec:     1358 || Batch Translation Loss:   0.031532 => Txt Tokens per Sec:     3776 || Lr: 0.000050
2024-02-11 19:12:28,910 Epoch 371: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.66 
2024-02-11 19:12:28,911 EPOCH 372
2024-02-11 19:12:32,219 [Epoch: 372 Step: 00049400] Batch Recognition Loss:   0.000162 => Gls Tokens per Sec:     1379 || Batch Translation Loss:   0.046086 => Txt Tokens per Sec:     3720 || Lr: 0.000050
2024-02-11 19:12:37,035 Epoch 372: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.01 
2024-02-11 19:12:37,035 EPOCH 373
2024-02-11 19:12:38,463 [Epoch: 373 Step: 00049500] Batch Recognition Loss:   0.000170 => Gls Tokens per Sec:     1346 || Batch Translation Loss:   0.028573 => Txt Tokens per Sec:     3878 || Lr: 0.000050
2024-02-11 19:12:43,910 [Epoch: 373 Step: 00049600] Batch Recognition Loss:   0.000187 => Gls Tokens per Sec:     1469 || Batch Translation Loss:   0.056602 => Txt Tokens per Sec:     4055 || Lr: 0.000050
2024-02-11 19:12:44,486 Epoch 373: Total Training Recognition Loss 0.03  Total Training Translation Loss 10.48 
2024-02-11 19:12:44,486 EPOCH 374
2024-02-11 19:12:49,488 [Epoch: 374 Step: 00049700] Batch Recognition Loss:   0.000263 => Gls Tokens per Sec:     1456 || Batch Translation Loss:   0.032342 => Txt Tokens per Sec:     4076 || Lr: 0.000050
2024-02-11 19:12:52,028 Epoch 374: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.94 
2024-02-11 19:12:52,028 EPOCH 375
2024-02-11 19:12:55,480 [Epoch: 375 Step: 00049800] Batch Recognition Loss:   0.000172 => Gls Tokens per Sec:     1344 || Batch Translation Loss:   0.031198 => Txt Tokens per Sec:     3661 || Lr: 0.000050
2024-02-11 19:13:00,228 Epoch 375: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.63 
2024-02-11 19:13:00,228 EPOCH 376
2024-02-11 19:13:01,814 [Epoch: 376 Step: 00049900] Batch Recognition Loss:   0.000172 => Gls Tokens per Sec:     1262 || Batch Translation Loss:   0.028333 => Txt Tokens per Sec:     3487 || Lr: 0.000050
2024-02-11 19:13:07,616 [Epoch: 376 Step: 00050000] Batch Recognition Loss:   0.000153 => Gls Tokens per Sec:     1378 || Batch Translation Loss:   0.050351 => Txt Tokens per Sec:     3808 || Lr: 0.000050
2024-02-11 19:13:18,690 Validation result at epoch 376, step    50000: duration: 11.0728s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00235	Translation Loss: 89950.97656	PPL: 8814.22949
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.71	(BLEU-1: 10.62,	BLEU-2: 3.72,	BLEU-3: 1.53,	BLEU-4: 0.71)
	CHRF 16.68	ROUGE 9.25
2024-02-11 19:13:18,691 Logging Recognition and Translation Outputs
2024-02-11 19:13:18,691 ========================================================================================================================
2024-02-11 19:13:18,691 Logging Sequence: 153_49.00
2024-02-11 19:13:18,691 	Gloss Reference :	A B+C+D+E
2024-02-11 19:13:18,691 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 19:13:18,692 	Gloss Alignment :	         
2024-02-11 19:13:18,692 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 19:13:18,692 	Text Reference  :	*** when it   was england's turn to   bat  
2024-02-11 19:13:18,693 	Text Hypothesis :	let me   tell you about     the  full story
2024-02-11 19:13:18,693 	Text Alignment  :	I   S    S    S   S         S    S    S    
2024-02-11 19:13:18,693 ========================================================================================================================
2024-02-11 19:13:18,693 Logging Sequence: 125_105.00
2024-02-11 19:13:18,693 	Gloss Reference :	A B+C+D+E
2024-02-11 19:13:18,693 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 19:13:18,693 	Gloss Alignment :	         
2024-02-11 19:13:18,693 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 19:13:18,694 	Text Reference  :	people must not target nadeem let   me  tell   you what happened
2024-02-11 19:13:18,694 	Text Hypothesis :	****** **** *** since  stokes broke his family at  the  hotel   
2024-02-11 19:13:18,694 	Text Alignment  :	D      D    D   S      S      S     S   S      S   S    S       
2024-02-11 19:13:18,695 ========================================================================================================================
2024-02-11 19:13:18,695 Logging Sequence: 169_115.00
2024-02-11 19:13:18,695 	Gloss Reference :	A B+C+D+E
2024-02-11 19:13:18,695 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 19:13:18,695 	Gloss Alignment :	         
2024-02-11 19:13:18,695 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 19:13:18,696 	Text Reference  :	a     user replaced the  words  india with       khalistan at  several locations on  arshdeep's profile
2024-02-11 19:13:18,697 	Text Hypothesis :	kohli had  revealed that before the   tournament he        did not     touch     the women's    team   
2024-02-11 19:13:18,697 	Text Alignment  :	S     S    S        S    S      S     S          S         S   S       S         S   S          S      
2024-02-11 19:13:18,697 ========================================================================================================================
2024-02-11 19:13:18,697 Logging Sequence: 72_194.00
2024-02-11 19:13:18,697 	Gloss Reference :	A B+C+D+E
2024-02-11 19:13:18,697 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 19:13:18,697 	Gloss Alignment :	         
2024-02-11 19:13:18,697 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 19:13:18,699 	Text Reference  :	shah told her  to    do    what she  wants and filed a    police complaint against her     
2024-02-11 19:13:18,699 	Text Hypothesis :	**** when they would share the  same medal at  the   same colony has       an      argument
2024-02-11 19:13:18,699 	Text Alignment  :	D    S    S    S     S     S    S    S     S   S     S    S      S         S       S       
2024-02-11 19:13:18,699 ========================================================================================================================
2024-02-11 19:13:18,699 Logging Sequence: 88_148.00
2024-02-11 19:13:18,700 	Gloss Reference :	A B+C+D+E
2024-02-11 19:13:18,700 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 19:13:18,700 	Gloss Alignment :	         
2024-02-11 19:13:18,700 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 19:13:18,701 	Text Reference  :	now too with the threat to messi it   has   been proved    that the police is useless 
2024-02-11 19:13:18,701 	Text Hypothesis :	*** *** **** *** this   is why   they could have commented on   the ****** ** incident
2024-02-11 19:13:18,702 	Text Alignment  :	D   D   D    D   S      S  S     S    S     S    S         S        D      D  S       
2024-02-11 19:13:18,702 ========================================================================================================================
2024-02-11 19:13:19,258 Epoch 376: Total Training Recognition Loss 0.03  Total Training Translation Loss 10.00 
2024-02-11 19:13:19,258 EPOCH 377
2024-02-11 19:13:24,590 [Epoch: 377 Step: 00050100] Batch Recognition Loss:   0.000139 => Gls Tokens per Sec:     1381 || Batch Translation Loss:   0.021200 => Txt Tokens per Sec:     3858 || Lr: 0.000050
2024-02-11 19:13:27,063 Epoch 377: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.49 
2024-02-11 19:13:27,064 EPOCH 378
2024-02-11 19:13:30,647 [Epoch: 378 Step: 00050200] Batch Recognition Loss:   0.000143 => Gls Tokens per Sec:     1318 || Batch Translation Loss:   0.008190 => Txt Tokens per Sec:     3656 || Lr: 0.000050
2024-02-11 19:13:35,133 Epoch 378: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.52 
2024-02-11 19:13:35,133 EPOCH 379
2024-02-11 19:13:36,590 [Epoch: 379 Step: 00050300] Batch Recognition Loss:   0.000345 => Gls Tokens per Sec:     1427 || Batch Translation Loss:   0.018389 => Txt Tokens per Sec:     4206 || Lr: 0.000050
2024-02-11 19:13:42,417 [Epoch: 379 Step: 00050400] Batch Recognition Loss:   0.000220 => Gls Tokens per Sec:     1371 || Batch Translation Loss:   0.038770 => Txt Tokens per Sec:     3748 || Lr: 0.000050
2024-02-11 19:13:42,854 Epoch 379: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.64 
2024-02-11 19:13:42,854 EPOCH 380
2024-02-11 19:13:48,172 [Epoch: 380 Step: 00050500] Batch Recognition Loss:   0.000172 => Gls Tokens per Sec:     1400 || Batch Translation Loss:   0.039553 => Txt Tokens per Sec:     3850 || Lr: 0.000050
2024-02-11 19:13:50,692 Epoch 380: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.12 
2024-02-11 19:13:50,693 EPOCH 381
2024-02-11 19:13:54,356 [Epoch: 381 Step: 00050600] Batch Recognition Loss:   0.000354 => Gls Tokens per Sec:     1311 || Batch Translation Loss:   0.012210 => Txt Tokens per Sec:     3639 || Lr: 0.000050
2024-02-11 19:13:58,821 Epoch 381: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.26 
2024-02-11 19:13:58,821 EPOCH 382
2024-02-11 19:14:00,239 [Epoch: 382 Step: 00050700] Batch Recognition Loss:   0.000222 => Gls Tokens per Sec:     1525 || Batch Translation Loss:   0.039223 => Txt Tokens per Sec:     3997 || Lr: 0.000050
2024-02-11 19:14:06,349 [Epoch: 382 Step: 00050800] Batch Recognition Loss:   0.000553 => Gls Tokens per Sec:     1308 || Batch Translation Loss:   0.008767 => Txt Tokens per Sec:     3701 || Lr: 0.000050
2024-02-11 19:14:06,693 Epoch 382: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.95 
2024-02-11 19:14:06,694 EPOCH 383
2024-02-11 19:14:12,179 [Epoch: 383 Step: 00050900] Batch Recognition Loss:   0.000237 => Gls Tokens per Sec:     1371 || Batch Translation Loss:   0.031708 => Txt Tokens per Sec:     3804 || Lr: 0.000050
2024-02-11 19:14:14,675 Epoch 383: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.28 
2024-02-11 19:14:14,676 EPOCH 384
2024-02-11 19:14:18,393 [Epoch: 384 Step: 00051000] Batch Recognition Loss:   0.000161 => Gls Tokens per Sec:     1313 || Batch Translation Loss:   0.075735 => Txt Tokens per Sec:     3657 || Lr: 0.000050
2024-02-11 19:14:22,676 Epoch 384: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.90 
2024-02-11 19:14:22,676 EPOCH 385
2024-02-11 19:14:24,120 [Epoch: 385 Step: 00051100] Batch Recognition Loss:   0.000123 => Gls Tokens per Sec:     1552 || Batch Translation Loss:   0.018904 => Txt Tokens per Sec:     4402 || Lr: 0.000050
2024-02-11 19:14:30,090 [Epoch: 385 Step: 00051200] Batch Recognition Loss:   0.000211 => Gls Tokens per Sec:     1339 || Batch Translation Loss:   0.027782 => Txt Tokens per Sec:     3701 || Lr: 0.000050
2024-02-11 19:14:30,373 Epoch 385: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.51 
2024-02-11 19:14:30,373 EPOCH 386
2024-02-11 19:14:36,120 [Epoch: 386 Step: 00051300] Batch Recognition Loss:   0.000184 => Gls Tokens per Sec:     1323 || Batch Translation Loss:   0.021164 => Txt Tokens per Sec:     3648 || Lr: 0.000050
2024-02-11 19:14:38,289 Epoch 386: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.76 
2024-02-11 19:14:38,289 EPOCH 387
2024-02-11 19:14:41,382 [Epoch: 387 Step: 00051400] Batch Recognition Loss:   0.000306 => Gls Tokens per Sec:     1604 || Batch Translation Loss:   0.035530 => Txt Tokens per Sec:     4523 || Lr: 0.000050
2024-02-11 19:14:45,395 Epoch 387: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.20 
2024-02-11 19:14:45,395 EPOCH 388
2024-02-11 19:14:47,029 [Epoch: 388 Step: 00051500] Batch Recognition Loss:   0.000229 => Gls Tokens per Sec:     1421 || Batch Translation Loss:   0.025356 => Txt Tokens per Sec:     4027 || Lr: 0.000050
2024-02-11 19:14:52,707 [Epoch: 388 Step: 00051600] Batch Recognition Loss:   0.000147 => Gls Tokens per Sec:     1407 || Batch Translation Loss:   0.035549 => Txt Tokens per Sec:     3893 || Lr: 0.000050
2024-02-11 19:14:52,914 Epoch 388: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.97 
2024-02-11 19:14:52,916 EPOCH 389
2024-02-11 19:14:58,397 [Epoch: 389 Step: 00051700] Batch Recognition Loss:   0.000164 => Gls Tokens per Sec:     1401 || Batch Translation Loss:   0.085997 => Txt Tokens per Sec:     3911 || Lr: 0.000050
2024-02-11 19:15:00,608 Epoch 389: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.44 
2024-02-11 19:15:00,609 EPOCH 390
2024-02-11 19:15:04,161 [Epoch: 390 Step: 00051800] Batch Recognition Loss:   0.000252 => Gls Tokens per Sec:     1420 || Batch Translation Loss:   0.044070 => Txt Tokens per Sec:     3928 || Lr: 0.000050
2024-02-11 19:15:08,296 Epoch 390: Total Training Recognition Loss 0.04  Total Training Translation Loss 12.16 
2024-02-11 19:15:08,296 EPOCH 391
2024-02-11 19:15:10,032 [Epoch: 391 Step: 00051900] Batch Recognition Loss:   0.000235 => Gls Tokens per Sec:     1384 || Batch Translation Loss:   0.023360 => Txt Tokens per Sec:     3670 || Lr: 0.000050
2024-02-11 19:15:15,560 [Epoch: 391 Step: 00052000] Batch Recognition Loss:   0.000418 => Gls Tokens per Sec:     1447 || Batch Translation Loss:   0.056778 => Txt Tokens per Sec:     4064 || Lr: 0.000050
2024-02-11 19:15:26,674 Validation result at epoch 391, step    52000: duration: 11.1126s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00209	Translation Loss: 89839.92188	PPL: 8715.91992
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.63	(BLEU-1: 10.50,	BLEU-2: 3.51,	BLEU-3: 1.39,	BLEU-4: 0.63)
	CHRF 16.99	ROUGE 9.08
2024-02-11 19:15:26,675 Logging Recognition and Translation Outputs
2024-02-11 19:15:26,675 ========================================================================================================================
2024-02-11 19:15:26,675 Logging Sequence: 145_207.00
2024-02-11 19:15:26,676 	Gloss Reference :	A B+C+D+E
2024-02-11 19:15:26,676 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 19:15:26,676 	Gloss Alignment :	         
2024-02-11 19:15:26,676 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 19:15:26,677 	Text Reference  :	** *** ***** **** he   then    wrote  a    letter to union sports minister anurag thakur  
2024-02-11 19:15:26,677 	Text Hypothesis :	as per being kept that anushka sharma will have   to wait  and    ended    for    pictures
2024-02-11 19:15:26,677 	Text Alignment  :	I  I   I     I    S    S       S      S    S         S     S      S        S      S       
2024-02-11 19:15:26,678 ========================================================================================================================
2024-02-11 19:15:26,678 Logging Sequence: 164_494.00
2024-02-11 19:15:26,678 	Gloss Reference :	A B+C+D+E
2024-02-11 19:15:26,678 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 19:15:26,678 	Gloss Alignment :	         
2024-02-11 19:15:26,678 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 19:15:26,680 	Text Reference  :	according to bcci 50   is  used   by      bcci for expenses related to  governing indian cricket staff expenses etc   
2024-02-11 19:15:26,680 	Text Hypothesis :	********* ** **** with the mumbai indians team was shocked  with    him he        is     the     best  bowling  attack
2024-02-11 19:15:26,680 	Text Alignment  :	D         D  D    S    S   S      S       S    S   S        S       S   S         S      S       S     S        S     
2024-02-11 19:15:26,680 ========================================================================================================================
2024-02-11 19:15:26,680 Logging Sequence: 56_8.00
2024-02-11 19:15:26,680 	Gloss Reference :	A B+C+D+E
2024-02-11 19:15:26,681 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 19:15:26,681 	Gloss Alignment :	         
2024-02-11 19:15:26,681 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 19:15:26,682 	Text Reference  :	on 22 april 2023 there was a    match between mumbai indians and punjab kings
2024-02-11 19:15:26,682 	Text Hypothesis :	** ** ***** **** ***** he  took time  since   the    stump   in  the    ipl  
2024-02-11 19:15:26,682 	Text Alignment  :	D  D  D     D    D     S   S    S     S       S      S       S   S      S    
2024-02-11 19:15:26,682 ========================================================================================================================
2024-02-11 19:15:26,682 Logging Sequence: 171_30.00
2024-02-11 19:15:26,682 	Gloss Reference :	A B+C+D+E
2024-02-11 19:15:26,683 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 19:15:26,683 	Gloss Alignment :	         
2024-02-11 19:15:26,683 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 19:15:26,683 	Text Reference  :	everyone will be hooked to       their screens for      this match    
2024-02-11 19:15:26,683 	Text Hypothesis :	******** **** ** he     suffered a     video   recently were surprised
2024-02-11 19:15:26,684 	Text Alignment  :	D        D    D  S      S        S     S       S        S    S        
2024-02-11 19:15:26,684 ========================================================================================================================
2024-02-11 19:15:26,684 Logging Sequence: 147_193.00
2024-02-11 19:15:26,684 	Gloss Reference :	A B+C+D+E
2024-02-11 19:15:26,684 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 19:15:26,684 	Gloss Alignment :	         
2024-02-11 19:15:26,684 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 19:15:26,685 	Text Reference  :	*** **** **** ******* ** **** ** ***** people applauded biles for  her    achievements
2024-02-11 19:15:26,685 	Text Hypothesis :	she said that earlier we will be given a      thawb     on    very formal occasions   
2024-02-11 19:15:26,685 	Text Alignment  :	I   I    I    I       I  I    I  I     S      S         S     S    S      S           
2024-02-11 19:15:26,685 ========================================================================================================================
2024-02-11 19:15:26,917 Epoch 391: Total Training Recognition Loss 0.04  Total Training Translation Loss 10.19 
2024-02-11 19:15:26,917 EPOCH 392
2024-02-11 19:15:32,475 [Epoch: 392 Step: 00052100] Batch Recognition Loss:   0.000163 => Gls Tokens per Sec:     1397 || Batch Translation Loss:   0.076667 => Txt Tokens per Sec:     3886 || Lr: 0.000050
2024-02-11 19:15:34,797 Epoch 392: Total Training Recognition Loss 0.04  Total Training Translation Loss 6.12 
2024-02-11 19:15:34,798 EPOCH 393
2024-02-11 19:15:38,614 [Epoch: 393 Step: 00052200] Batch Recognition Loss:   0.000265 => Gls Tokens per Sec:     1342 || Batch Translation Loss:   0.039954 => Txt Tokens per Sec:     3737 || Lr: 0.000050
2024-02-11 19:15:42,465 Epoch 393: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.07 
2024-02-11 19:15:42,465 EPOCH 394
2024-02-11 19:15:44,028 [Epoch: 394 Step: 00052300] Batch Recognition Loss:   0.000402 => Gls Tokens per Sec:     1588 || Batch Translation Loss:   0.037821 => Txt Tokens per Sec:     4375 || Lr: 0.000050
2024-02-11 19:15:50,296 [Epoch: 394 Step: 00052400] Batch Recognition Loss:   0.000191 => Gls Tokens per Sec:     1275 || Batch Translation Loss:   0.024272 => Txt Tokens per Sec:     3550 || Lr: 0.000050
2024-02-11 19:15:50,411 Epoch 394: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.18 
2024-02-11 19:15:50,412 EPOCH 395
2024-02-11 19:15:56,026 [Epoch: 395 Step: 00052500] Batch Recognition Loss:   0.000199 => Gls Tokens per Sec:     1397 || Batch Translation Loss:   0.012601 => Txt Tokens per Sec:     3862 || Lr: 0.000050
2024-02-11 19:15:58,094 Epoch 395: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.54 
2024-02-11 19:15:58,094 EPOCH 396
2024-02-11 19:16:01,673 [Epoch: 396 Step: 00052600] Batch Recognition Loss:   0.000262 => Gls Tokens per Sec:     1454 || Batch Translation Loss:   0.022154 => Txt Tokens per Sec:     3937 || Lr: 0.000050
2024-02-11 19:16:06,029 Epoch 396: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.79 
2024-02-11 19:16:06,029 EPOCH 397
2024-02-11 19:16:07,676 [Epoch: 397 Step: 00052700] Batch Recognition Loss:   0.000158 => Gls Tokens per Sec:     1556 || Batch Translation Loss:   0.016391 => Txt Tokens per Sec:     4273 || Lr: 0.000050
2024-02-11 19:16:13,023 [Epoch: 397 Step: 00052800] Batch Recognition Loss:   0.000297 => Gls Tokens per Sec:     1495 || Batch Translation Loss:   0.060250 => Txt Tokens per Sec:     4179 || Lr: 0.000050
2024-02-11 19:16:13,066 Epoch 397: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.99 
2024-02-11 19:16:13,066 EPOCH 398
2024-02-11 19:16:19,236 [Epoch: 398 Step: 00052900] Batch Recognition Loss:   0.000158 => Gls Tokens per Sec:     1284 || Batch Translation Loss:   0.018589 => Txt Tokens per Sec:     3555 || Lr: 0.000050
2024-02-11 19:16:21,414 Epoch 398: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.05 
2024-02-11 19:16:21,414 EPOCH 399
2024-02-11 19:16:25,351 [Epoch: 399 Step: 00053000] Batch Recognition Loss:   0.000297 => Gls Tokens per Sec:     1341 || Batch Translation Loss:   0.073839 => Txt Tokens per Sec:     3757 || Lr: 0.000050
2024-02-11 19:16:29,167 Epoch 399: Total Training Recognition Loss 0.04  Total Training Translation Loss 14.48 
2024-02-11 19:16:29,168 EPOCH 400
2024-02-11 19:16:31,094 [Epoch: 400 Step: 00053100] Batch Recognition Loss:   0.000416 => Gls Tokens per Sec:     1372 || Batch Translation Loss:   0.040609 => Txt Tokens per Sec:     3888 || Lr: 0.000050
2024-02-11 19:16:37,035 [Epoch: 400 Step: 00053200] Batch Recognition Loss:   0.000202 => Gls Tokens per Sec:     1345 || Batch Translation Loss:   0.031935 => Txt Tokens per Sec:     3712 || Lr: 0.000050
2024-02-11 19:16:37,036 Epoch 400: Total Training Recognition Loss 0.04  Total Training Translation Loss 6.16 
2024-02-11 19:16:37,036 EPOCH 401
2024-02-11 19:16:43,220 [Epoch: 401 Step: 00053300] Batch Recognition Loss:   0.000285 => Gls Tokens per Sec:     1294 || Batch Translation Loss:   0.031712 => Txt Tokens per Sec:     3587 || Lr: 0.000050
2024-02-11 19:16:45,388 Epoch 401: Total Training Recognition Loss 0.04  Total Training Translation Loss 4.75 
2024-02-11 19:16:45,389 EPOCH 402
2024-02-11 19:16:48,742 [Epoch: 402 Step: 00053400] Batch Recognition Loss:   0.000253 => Gls Tokens per Sec:     1599 || Batch Translation Loss:   0.037769 => Txt Tokens per Sec:     4363 || Lr: 0.000050
2024-02-11 19:16:53,055 Epoch 402: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.26 
2024-02-11 19:16:53,055 EPOCH 403
2024-02-11 19:16:54,839 [Epoch: 403 Step: 00053500] Batch Recognition Loss:   0.000254 => Gls Tokens per Sec:     1527 || Batch Translation Loss:   0.048800 => Txt Tokens per Sec:     4037 || Lr: 0.000050
2024-02-11 19:17:00,542 Epoch 403: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.53 
2024-02-11 19:17:00,542 EPOCH 404
2024-02-11 19:17:00,589 [Epoch: 404 Step: 00053600] Batch Recognition Loss:   0.000292 => Gls Tokens per Sec:     1756 || Batch Translation Loss:   0.014759 => Txt Tokens per Sec:     4609 || Lr: 0.000050
2024-02-11 19:17:06,269 [Epoch: 404 Step: 00053700] Batch Recognition Loss:   0.000218 => Gls Tokens per Sec:     1408 || Batch Translation Loss:   0.014480 => Txt Tokens per Sec:     3907 || Lr: 0.000050
2024-02-11 19:17:08,143 Epoch 404: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.72 
2024-02-11 19:17:08,143 EPOCH 405
2024-02-11 19:17:11,996 [Epoch: 405 Step: 00053800] Batch Recognition Loss:   0.000177 => Gls Tokens per Sec:     1413 || Batch Translation Loss:   0.072193 => Txt Tokens per Sec:     3935 || Lr: 0.000050
2024-02-11 19:17:15,822 Epoch 405: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.36 
2024-02-11 19:17:15,822 EPOCH 406
2024-02-11 19:17:17,734 [Epoch: 406 Step: 00053900] Batch Recognition Loss:   0.000196 => Gls Tokens per Sec:     1466 || Batch Translation Loss:   0.318517 => Txt Tokens per Sec:     4064 || Lr: 0.000050
2024-02-11 19:17:23,254 Epoch 406: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.91 
2024-02-11 19:17:23,255 EPOCH 407
2024-02-11 19:17:23,353 [Epoch: 407 Step: 00054000] Batch Recognition Loss:   0.000153 => Gls Tokens per Sec:     1656 || Batch Translation Loss:   0.029261 => Txt Tokens per Sec:     4449 || Lr: 0.000050
2024-02-11 19:17:34,585 Validation result at epoch 407, step    54000: duration: 11.2313s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00278	Translation Loss: 89069.78906	PPL: 8063.72998
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.80	(BLEU-1: 10.67,	BLEU-2: 3.50,	BLEU-3: 1.58,	BLEU-4: 0.80)
	CHRF 16.84	ROUGE 9.26
2024-02-11 19:17:34,586 Logging Recognition and Translation Outputs
2024-02-11 19:17:34,586 ========================================================================================================================
2024-02-11 19:17:34,587 Logging Sequence: 75_247.00
2024-02-11 19:17:34,587 	Gloss Reference :	A B+C+D+E
2024-02-11 19:17:34,587 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 19:17:34,587 	Gloss Alignment :	         
2024-02-11 19:17:34,587 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 19:17:34,588 	Text Reference  :	******* *** **** we   request the   media     for privacy at  this  difficult time     
2024-02-11 19:17:34,588 	Text Hypothesis :	however our baby girl like    india australia won the     t20 world cup       countries
2024-02-11 19:17:34,588 	Text Alignment  :	I       I   I    S    S       S     S         S   S       S   S     S         S        
2024-02-11 19:17:34,588 ========================================================================================================================
2024-02-11 19:17:34,589 Logging Sequence: 151_110.00
2024-02-11 19:17:34,589 	Gloss Reference :	A B+C+D+E
2024-02-11 19:17:34,589 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 19:17:34,589 	Gloss Alignment :	         
2024-02-11 19:17:34,589 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 19:17:34,591 	Text Reference  :	raina said that he has   resigned from cricket doesn't mean that i will be          completely cut    off    from cricket
2024-02-11 19:17:34,591 	Text Hypothesis :	and   said **** it would love     to   cricket ******* **** **** * **** association and        rajeev shukla for  all    
2024-02-11 19:17:34,591 	Text Alignment  :	S          D    S  S     S        S            D       D    D    D D    S           S          S      S      S    S      
2024-02-11 19:17:34,591 ========================================================================================================================
2024-02-11 19:17:34,591 Logging Sequence: 121_176.00
2024-02-11 19:17:34,591 	Gloss Reference :	A B+C+D+E
2024-02-11 19:17:34,591 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 19:17:34,592 	Gloss Alignment :	         
2024-02-11 19:17:34,592 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 19:17:34,593 	Text Reference  :	he presented her with a shawl congratulated her and  told  her    india is proud of *** *** ***** *** her  victory
2024-02-11 19:17:34,593 	Text Hypothesis :	** ********* *** **** * ***** ************* now once again making india ** proud of the t20 world cup will help   
2024-02-11 19:17:34,593 	Text Alignment  :	D  D         D   D    D D     D             S   S    S     S            D           I   I   I     I   S    S      
2024-02-11 19:17:34,593 ========================================================================================================================
2024-02-11 19:17:34,593 Logging Sequence: 51_152.00
2024-02-11 19:17:34,594 	Gloss Reference :	A B+C+D+E
2024-02-11 19:17:34,594 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 19:17:34,594 	Gloss Alignment :	         
2024-02-11 19:17:34,594 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 19:17:34,595 	Text Reference  :	australian players did  this         to celebrate their *** *** victory in   their cultural way
2024-02-11 19:17:34,595 	Text Hypothesis :	people     were    very disappointed to see       their son and has     been more  than     me 
2024-02-11 19:17:34,595 	Text Alignment  :	S          S       S    S               S               I   I   S       S    S     S        S  
2024-02-11 19:17:34,595 ========================================================================================================================
2024-02-11 19:17:34,596 Logging Sequence: 62_205.00
2024-02-11 19:17:34,596 	Gloss Reference :	A B+C+D+E
2024-02-11 19:17:34,596 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 19:17:34,596 	Gloss Alignment :	         
2024-02-11 19:17:34,596 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 19:17:34,598 	Text Reference  :	******* ** with bcci's  annoucement about equal     pay india has become the second country to implement equal pay after new  zealand
2024-02-11 19:17:34,598 	Text Hypothesis :	however on 27th october 2022        bcci  announced his match fee for    the men's  teams   in the       men's and the   same venue  
2024-02-11 19:17:34,599 	Text Alignment  :	I       I  S    S       S           S     S         S   S     S   S          S      S       S  S         S     S   S     S    S      
2024-02-11 19:17:34,599 ========================================================================================================================
2024-02-11 19:17:40,483 [Epoch: 407 Step: 00054100] Batch Recognition Loss:   0.000180 => Gls Tokens per Sec:     1356 || Batch Translation Loss:   0.090292 => Txt Tokens per Sec:     3779 || Lr: 0.000050
2024-02-11 19:17:42,297 Epoch 407: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.66 
2024-02-11 19:17:42,297 EPOCH 408
2024-02-11 19:17:46,011 [Epoch: 408 Step: 00054200] Batch Recognition Loss:   0.000340 => Gls Tokens per Sec:     1487 || Batch Translation Loss:   0.065423 => Txt Tokens per Sec:     4130 || Lr: 0.000050
2024-02-11 19:17:49,256 Epoch 408: Total Training Recognition Loss 0.04  Total Training Translation Loss 7.66 
2024-02-11 19:17:49,256 EPOCH 409
2024-02-11 19:17:51,235 [Epoch: 409 Step: 00054300] Batch Recognition Loss:   0.000130 => Gls Tokens per Sec:     1456 || Batch Translation Loss:   0.022870 => Txt Tokens per Sec:     4086 || Lr: 0.000050
2024-02-11 19:17:56,964 Epoch 409: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.10 
2024-02-11 19:17:56,964 EPOCH 410
2024-02-11 19:17:57,135 [Epoch: 410 Step: 00054400] Batch Recognition Loss:   0.000681 => Gls Tokens per Sec:     1413 || Batch Translation Loss:   0.024837 => Txt Tokens per Sec:     4403 || Lr: 0.000050
2024-02-11 19:18:02,807 [Epoch: 410 Step: 00054500] Batch Recognition Loss:   0.000238 => Gls Tokens per Sec:     1410 || Batch Translation Loss:   0.049061 => Txt Tokens per Sec:     3942 || Lr: 0.000050
2024-02-11 19:18:04,789 Epoch 410: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.76 
2024-02-11 19:18:04,790 EPOCH 411
2024-02-11 19:18:08,741 [Epoch: 411 Step: 00054600] Batch Recognition Loss:   0.000237 => Gls Tokens per Sec:     1418 || Batch Translation Loss:   0.025830 => Txt Tokens per Sec:     3923 || Lr: 0.000050
2024-02-11 19:18:12,515 Epoch 411: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.07 
2024-02-11 19:18:12,515 EPOCH 412
2024-02-11 19:18:14,281 [Epoch: 412 Step: 00054700] Batch Recognition Loss:   0.000238 => Gls Tokens per Sec:     1677 || Batch Translation Loss:   0.057726 => Txt Tokens per Sec:     4453 || Lr: 0.000050
2024-02-11 19:18:20,352 Epoch 412: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.90 
2024-02-11 19:18:20,353 EPOCH 413
2024-02-11 19:18:20,583 [Epoch: 413 Step: 00054800] Batch Recognition Loss:   0.000152 => Gls Tokens per Sec:     1397 || Batch Translation Loss:   0.011661 => Txt Tokens per Sec:     3514 || Lr: 0.000050
2024-02-11 19:18:26,360 [Epoch: 413 Step: 00054900] Batch Recognition Loss:   0.000115 => Gls Tokens per Sec:     1385 || Batch Translation Loss:   0.011511 => Txt Tokens per Sec:     3838 || Lr: 0.000050
2024-02-11 19:18:27,928 Epoch 413: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.33 
2024-02-11 19:18:27,928 EPOCH 414
2024-02-11 19:18:32,059 [Epoch: 414 Step: 00055000] Batch Recognition Loss:   0.000132 => Gls Tokens per Sec:     1375 || Batch Translation Loss:   0.017137 => Txt Tokens per Sec:     3840 || Lr: 0.000050
2024-02-11 19:18:35,585 Epoch 414: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.73 
2024-02-11 19:18:35,585 EPOCH 415
2024-02-11 19:18:37,607 [Epoch: 415 Step: 00055100] Batch Recognition Loss:   0.000243 => Gls Tokens per Sec:     1504 || Batch Translation Loss:   0.036502 => Txt Tokens per Sec:     4200 || Lr: 0.000050
2024-02-11 19:18:43,176 Epoch 415: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.92 
2024-02-11 19:18:43,177 EPOCH 416
2024-02-11 19:18:43,504 [Epoch: 416 Step: 00055200] Batch Recognition Loss:   0.000314 => Gls Tokens per Sec:     1225 || Batch Translation Loss:   0.019354 => Txt Tokens per Sec:     3774 || Lr: 0.000050
2024-02-11 19:18:49,538 [Epoch: 416 Step: 00055300] Batch Recognition Loss:   0.000180 => Gls Tokens per Sec:     1325 || Batch Translation Loss:   0.033025 => Txt Tokens per Sec:     3668 || Lr: 0.000050
2024-02-11 19:18:51,053 Epoch 416: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.27 
2024-02-11 19:18:51,053 EPOCH 417
2024-02-11 19:18:54,758 [Epoch: 417 Step: 00055400] Batch Recognition Loss:   0.000120 => Gls Tokens per Sec:     1555 || Batch Translation Loss:   0.007501 => Txt Tokens per Sec:     4208 || Lr: 0.000050
2024-02-11 19:18:58,529 Epoch 417: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.03 
2024-02-11 19:18:58,530 EPOCH 418
2024-02-11 19:19:00,869 [Epoch: 418 Step: 00055500] Batch Recognition Loss:   0.000182 => Gls Tokens per Sec:     1334 || Batch Translation Loss:   0.022197 => Txt Tokens per Sec:     3584 || Lr: 0.000050
2024-02-11 19:19:06,491 Epoch 418: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.93 
2024-02-11 19:19:06,492 EPOCH 419
2024-02-11 19:19:06,760 [Epoch: 419 Step: 00055600] Batch Recognition Loss:   0.000254 => Gls Tokens per Sec:     1798 || Batch Translation Loss:   0.016151 => Txt Tokens per Sec:     4899 || Lr: 0.000050
2024-02-11 19:19:12,839 [Epoch: 419 Step: 00055700] Batch Recognition Loss:   0.000206 => Gls Tokens per Sec:     1315 || Batch Translation Loss:   0.038393 => Txt Tokens per Sec:     3632 || Lr: 0.000050
2024-02-11 19:19:14,420 Epoch 419: Total Training Recognition Loss 0.04  Total Training Translation Loss 9.14 
2024-02-11 19:19:14,420 EPOCH 420
2024-02-11 19:19:18,818 [Epoch: 420 Step: 00055800] Batch Recognition Loss:   0.000220 => Gls Tokens per Sec:     1328 || Batch Translation Loss:   0.032522 => Txt Tokens per Sec:     3741 || Lr: 0.000050
2024-02-11 19:19:22,449 Epoch 420: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.31 
2024-02-11 19:19:22,450 EPOCH 421
2024-02-11 19:19:24,751 [Epoch: 421 Step: 00055900] Batch Recognition Loss:   0.000344 => Gls Tokens per Sec:     1392 || Batch Translation Loss:   0.025302 => Txt Tokens per Sec:     3911 || Lr: 0.000050
2024-02-11 19:19:30,349 Epoch 421: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.31 
2024-02-11 19:19:30,349 EPOCH 422
2024-02-11 19:19:30,759 [Epoch: 422 Step: 00056000] Batch Recognition Loss:   0.000279 => Gls Tokens per Sec:     1367 || Batch Translation Loss:   0.025999 => Txt Tokens per Sec:     4143 || Lr: 0.000050
2024-02-11 19:19:41,934 Validation result at epoch 422, step    56000: duration: 11.1750s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00232	Translation Loss: 90104.70312	PPL: 8952.13672
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.74	(BLEU-1: 10.17,	BLEU-2: 3.38,	BLEU-3: 1.44,	BLEU-4: 0.74)
	CHRF 16.71	ROUGE 8.63
2024-02-11 19:19:41,935 Logging Recognition and Translation Outputs
2024-02-11 19:19:41,935 ========================================================================================================================
2024-02-11 19:19:41,936 Logging Sequence: 60_112.00
2024-02-11 19:19:41,936 	Gloss Reference :	A B+C+D+E
2024-02-11 19:19:41,936 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 19:19:41,936 	Gloss Alignment :	         
2024-02-11 19:19:41,936 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 19:19:41,939 	Text Reference  :	let me tell you    as  you   know in  fast bowling the   bowler  take a   long   run up and comes    running and          throws   the bowl    
2024-02-11 19:19:41,939 	Text Hypothesis :	*** ** fans across the globe see  him as   a       young version of   his father as  he has recently started professional training for football
2024-02-11 19:19:41,939 	Text Alignment  :	D   D  S    S      S   S     S    S   S    S       S     S       S    S   S      S   S  S   S        S       S            S        S   S       
2024-02-11 19:19:41,939 ========================================================================================================================
2024-02-11 19:19:41,940 Logging Sequence: 99_130.00
2024-02-11 19:19:41,940 	Gloss Reference :	A B+C+D+E
2024-02-11 19:19:41,940 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 19:19:41,940 	Gloss Alignment :	         
2024-02-11 19:19:41,940 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 19:19:41,941 	Text Reference  :	there was also a violent clash betweem the afghan pakistani fans    in the ********** **** **** stadium   
2024-02-11 19:19:41,941 	Text Hypothesis :	***** *** **** * ******* ***** ******* the ****** ********* umpires in the tournament were from bangalesh'
2024-02-11 19:19:41,941 	Text Alignment  :	D     D   D    D D       D     D           D      D         S              I          I    I    S         
2024-02-11 19:19:41,941 ========================================================================================================================
2024-02-11 19:19:41,941 Logging Sequence: 116_136.00
2024-02-11 19:19:41,942 	Gloss Reference :	A B+C+D+E
2024-02-11 19:19:41,942 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 19:19:41,942 	Gloss Alignment :	         
2024-02-11 19:19:41,942 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 19:19:41,943 	Text Reference  :	* ** ** ***** **** **** *** * kohli's wife  anushka sharma reposted his video on   her insta stories
2024-02-11 19:19:41,943 	Text Hypothesis :	i am so happy that they get a strong  world cup     but    they     had me    tell you about it     
2024-02-11 19:19:41,943 	Text Alignment  :	I I  I  I     I    I    I   I S       S     S       S      S        S   S     S    S   S     S      
2024-02-11 19:19:41,943 ========================================================================================================================
2024-02-11 19:19:41,944 Logging Sequence: 54_54.00
2024-02-11 19:19:41,944 	Gloss Reference :	A B+C+D+E
2024-02-11 19:19:41,944 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 19:19:41,944 	Gloss Alignment :	         
2024-02-11 19:19:41,944 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 19:19:41,946 	Text Reference  :	***** *** *** *** what let me  tell you the high room     rates   are because of   the     thrilling india-pakistan match
2024-02-11 19:19:41,946 	Text Hypothesis :	india had won the toss and led to   him a   huge argument however it  was     very excited to        the            match
2024-02-11 19:19:41,946 	Text Alignment  :	I     I   I   I   S    S   S   S    S   S   S    S        S       S   S       S    S       S         S                   
2024-02-11 19:19:41,946 ========================================================================================================================
2024-02-11 19:19:41,946 Logging Sequence: 64_168.00
2024-02-11 19:19:41,946 	Gloss Reference :	A B+C+D+E
2024-02-11 19:19:41,946 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 19:19:41,946 	Gloss Alignment :	         
2024-02-11 19:19:41,947 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 19:19:41,948 	Text Reference  :	the bcci can not  move ipl  to uae  as the  country has   banned all    indian flights   
2024-02-11 19:19:41,948 	Text Hypothesis :	the **** ipl will be   held in june as well against staff safe   during the    tournament
2024-02-11 19:19:41,948 	Text Alignment  :	    D    S   S    S    S    S  S       S    S       S     S      S      S      S         
2024-02-11 19:19:41,948 ========================================================================================================================
2024-02-11 19:19:47,955 [Epoch: 422 Step: 00056100] Batch Recognition Loss:   0.000494 => Gls Tokens per Sec:     1329 || Batch Translation Loss:   0.017130 => Txt Tokens per Sec:     3681 || Lr: 0.000050
2024-02-11 19:19:49,594 Epoch 422: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.74 
2024-02-11 19:19:49,594 EPOCH 423
2024-02-11 19:19:53,770 [Epoch: 423 Step: 00056200] Batch Recognition Loss:   0.000227 => Gls Tokens per Sec:     1418 || Batch Translation Loss:   0.015394 => Txt Tokens per Sec:     4013 || Lr: 0.000050
2024-02-11 19:19:56,861 Epoch 423: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.23 
2024-02-11 19:19:56,861 EPOCH 424
2024-02-11 19:19:59,209 [Epoch: 424 Step: 00056300] Batch Recognition Loss:   0.000226 => Gls Tokens per Sec:     1397 || Batch Translation Loss:   0.041184 => Txt Tokens per Sec:     3866 || Lr: 0.000050
2024-02-11 19:20:04,934 Epoch 424: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.68 
2024-02-11 19:20:04,934 EPOCH 425
2024-02-11 19:20:05,448 [Epoch: 425 Step: 00056400] Batch Recognition Loss:   0.000166 => Gls Tokens per Sec:     1247 || Batch Translation Loss:   0.035940 => Txt Tokens per Sec:     3435 || Lr: 0.000050
2024-02-11 19:20:10,743 [Epoch: 425 Step: 00056500] Batch Recognition Loss:   0.000407 => Gls Tokens per Sec:     1511 || Batch Translation Loss:   0.103801 => Txt Tokens per Sec:     4232 || Lr: 0.000050
2024-02-11 19:20:12,209 Epoch 425: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.86 
2024-02-11 19:20:12,210 EPOCH 426
2024-02-11 19:20:16,705 [Epoch: 426 Step: 00056600] Batch Recognition Loss:   0.000124 => Gls Tokens per Sec:     1335 || Batch Translation Loss:   0.010468 => Txt Tokens per Sec:     3683 || Lr: 0.000050
2024-02-11 19:20:20,423 Epoch 426: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.89 
2024-02-11 19:20:20,423 EPOCH 427
2024-02-11 19:20:22,683 [Epoch: 427 Step: 00056700] Batch Recognition Loss:   0.000292 => Gls Tokens per Sec:     1487 || Batch Translation Loss:   0.020243 => Txt Tokens per Sec:     4177 || Lr: 0.000050
2024-02-11 19:20:27,920 Epoch 427: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.91 
2024-02-11 19:20:27,921 EPOCH 428
2024-02-11 19:20:28,454 [Epoch: 428 Step: 00056800] Batch Recognition Loss:   0.000152 => Gls Tokens per Sec:     1355 || Batch Translation Loss:   0.016198 => Txt Tokens per Sec:     3882 || Lr: 0.000050
2024-02-11 19:20:34,162 [Epoch: 428 Step: 00056900] Batch Recognition Loss:   0.000193 => Gls Tokens per Sec:     1402 || Batch Translation Loss:   0.014884 => Txt Tokens per Sec:     3907 || Lr: 0.000050
2024-02-11 19:20:35,537 Epoch 428: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.02 
2024-02-11 19:20:35,537 EPOCH 429
2024-02-11 19:20:39,620 [Epoch: 429 Step: 00057000] Batch Recognition Loss:   0.000227 => Gls Tokens per Sec:     1490 || Batch Translation Loss:   0.015879 => Txt Tokens per Sec:     4206 || Lr: 0.000050
2024-02-11 19:20:43,076 Epoch 429: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.07 
2024-02-11 19:20:43,076 EPOCH 430
2024-02-11 19:20:45,540 [Epoch: 430 Step: 00057100] Batch Recognition Loss:   0.000196 => Gls Tokens per Sec:     1397 || Batch Translation Loss:   0.014031 => Txt Tokens per Sec:     4031 || Lr: 0.000050
2024-02-11 19:20:50,602 Epoch 430: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.92 
2024-02-11 19:20:50,603 EPOCH 431
2024-02-11 19:20:51,104 [Epoch: 431 Step: 00057200] Batch Recognition Loss:   0.000228 => Gls Tokens per Sec:     1601 || Batch Translation Loss:   0.015621 => Txt Tokens per Sec:     4444 || Lr: 0.000050
2024-02-11 19:20:57,084 [Epoch: 431 Step: 00057300] Batch Recognition Loss:   0.000126 => Gls Tokens per Sec:     1336 || Batch Translation Loss:   0.016475 => Txt Tokens per Sec:     3716 || Lr: 0.000050
2024-02-11 19:20:58,421 Epoch 431: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.73 
2024-02-11 19:20:58,421 EPOCH 432
2024-02-11 19:21:02,544 [Epoch: 432 Step: 00057400] Batch Recognition Loss:   0.000239 => Gls Tokens per Sec:     1494 || Batch Translation Loss:   0.062053 => Txt Tokens per Sec:     4143 || Lr: 0.000050
2024-02-11 19:21:05,876 Epoch 432: Total Training Recognition Loss 0.04  Total Training Translation Loss 5.73 
2024-02-11 19:21:05,877 EPOCH 433
2024-02-11 19:21:08,542 [Epoch: 433 Step: 00057500] Batch Recognition Loss:   0.000258 => Gls Tokens per Sec:     1321 || Batch Translation Loss:   0.031233 => Txt Tokens per Sec:     3658 || Lr: 0.000050
2024-02-11 19:21:14,047 Epoch 433: Total Training Recognition Loss 0.04  Total Training Translation Loss 6.52 
2024-02-11 19:21:14,047 EPOCH 434
2024-02-11 19:21:14,693 [Epoch: 434 Step: 00057600] Batch Recognition Loss:   0.000168 => Gls Tokens per Sec:     1364 || Batch Translation Loss:   0.018339 => Txt Tokens per Sec:     4056 || Lr: 0.000050
2024-02-11 19:21:20,098 [Epoch: 434 Step: 00057700] Batch Recognition Loss:   0.000264 => Gls Tokens per Sec:     1480 || Batch Translation Loss:   0.050121 => Txt Tokens per Sec:     4043 || Lr: 0.000050
2024-02-11 19:21:21,509 Epoch 434: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.39 
2024-02-11 19:21:21,509 EPOCH 435
2024-02-11 19:21:26,159 [Epoch: 435 Step: 00057800] Batch Recognition Loss:   0.000183 => Gls Tokens per Sec:     1342 || Batch Translation Loss:   0.040051 => Txt Tokens per Sec:     3728 || Lr: 0.000050
2024-02-11 19:21:29,404 Epoch 435: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.08 
2024-02-11 19:21:29,404 EPOCH 436
2024-02-11 19:21:31,808 [Epoch: 436 Step: 00057900] Batch Recognition Loss:   0.000142 => Gls Tokens per Sec:     1498 || Batch Translation Loss:   0.015847 => Txt Tokens per Sec:     4066 || Lr: 0.000050
2024-02-11 19:21:37,128 Epoch 436: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.95 
2024-02-11 19:21:37,129 EPOCH 437
2024-02-11 19:21:37,803 [Epoch: 437 Step: 00058000] Batch Recognition Loss:   0.000199 => Gls Tokens per Sec:     1425 || Batch Translation Loss:   0.027076 => Txt Tokens per Sec:     3790 || Lr: 0.000050
2024-02-11 19:21:49,082 Hooray! New best validation result [eval_metric]!
2024-02-11 19:21:49,083 Saving new checkpoint.
2024-02-11 19:21:49,372 Validation result at epoch 437, step    58000: duration: 11.5679s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00271	Translation Loss: 89501.89062	PPL: 8423.40723
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.92	(BLEU-1: 10.80,	BLEU-2: 3.85,	BLEU-3: 1.71,	BLEU-4: 0.92)
	CHRF 17.25	ROUGE 9.15
2024-02-11 19:21:49,373 Logging Recognition and Translation Outputs
2024-02-11 19:21:49,373 ========================================================================================================================
2024-02-11 19:21:49,373 Logging Sequence: 153_165.00
2024-02-11 19:21:49,373 	Gloss Reference :	A B+C+D+E
2024-02-11 19:21:49,374 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 19:21:49,374 	Gloss Alignment :	         
2024-02-11 19:21:49,374 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 19:21:49,375 	Text Reference  :	in 1992  pakistan played against new    zealand    in     the semi-finals and *** *** * **** **** ** ********* *** ** **** won
2024-02-11 19:21:49,376 	Text Hypothesis :	** later on       21st   of      august dhanashree posted a   picture     and put out a long note on instagram let me tell you
2024-02-11 19:21:49,376 	Text Alignment  :	D  S     S        S      S       S      S          S      S   S               I   I   I I    I    I  I         I   I  I    S  
2024-02-11 19:21:49,376 ========================================================================================================================
2024-02-11 19:21:49,376 Logging Sequence: 181_43.00
2024-02-11 19:21:49,376 	Gloss Reference :	A B+C+D+E
2024-02-11 19:21:49,376 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 19:21:49,376 	Gloss Alignment :	         
2024-02-11 19:21:49,377 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 19:21:49,377 	Text Reference  :	***** **** he  has been blessed with a  baby boy  
2024-02-11 19:21:49,377 	Text Hypothesis :	mumal also won the cup  against fans in the  match
2024-02-11 19:21:49,377 	Text Alignment  :	I     I    S   S   S    S       S    S  S    S    
2024-02-11 19:21:49,378 ========================================================================================================================
2024-02-11 19:21:49,378 Logging Sequence: 63_94.00
2024-02-11 19:21:49,378 	Gloss Reference :	A B+C+D+E
2024-02-11 19:21:49,378 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 19:21:49,378 	Gloss Alignment :	         
2024-02-11 19:21:49,378 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 19:21:49,380 	Text Reference  :	the auction will start  from rs  2000  crore   the    highest bidder will buy  the teams    
2024-02-11 19:21:49,380 	Text Hypothesis :	*** ******* ipl  2021's 3rd  may match between mumbai and     ipl    are  held in  bengaluru
2024-02-11 19:21:49,380 	Text Alignment  :	D   D       S    S      S    S   S     S       S      S       S      S    S    S   S        
2024-02-11 19:21:49,380 ========================================================================================================================
2024-02-11 19:21:49,380 Logging Sequence: 73_143.00
2024-02-11 19:21:49,380 	Gloss Reference :	A B+C+D+E
2024-02-11 19:21:49,380 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 19:21:49,381 	Gloss Alignment :	         
2024-02-11 19:21:49,381 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 19:21:49,382 	Text Reference  :	there are many expensive dishesif you are     craving to these you  can visit there  
2024-02-11 19:21:49,382 	Text Hypothesis :	***** *** he   was       born     and brought down    in 1     note on  his   parents
2024-02-11 19:21:49,382 	Text Alignment  :	D     D   S    S         S        S   S       S       S  S     S    S   S     S      
2024-02-11 19:21:49,382 ========================================================================================================================
2024-02-11 19:21:49,382 Logging Sequence: 86_106.00
2024-02-11 19:21:49,382 	Gloss Reference :	A B+C+D+E
2024-02-11 19:21:49,383 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 19:21:49,383 	Gloss Alignment :	         
2024-02-11 19:21:49,383 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 19:21:49,385 	Text Reference  :	he      was an umpire too and stood  in ****** a     few of      women' odis
2024-02-11 19:21:49,385 	Text Hypothesis :	yashpal was ** one    of  the heroes in india' world cup triumph in     1983
2024-02-11 19:21:49,385 	Text Alignment  :	S           D  S      S   S   S         I      S     S   S       S      S   
2024-02-11 19:21:49,385 ========================================================================================================================
2024-02-11 19:21:55,141 [Epoch: 437 Step: 00058100] Batch Recognition Loss:   0.000166 => Gls Tokens per Sec:     1387 || Batch Translation Loss:   0.022028 => Txt Tokens per Sec:     3859 || Lr: 0.000050
2024-02-11 19:21:56,382 Epoch 437: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.16 
2024-02-11 19:21:56,383 EPOCH 438
2024-02-11 19:22:01,274 [Epoch: 438 Step: 00058200] Batch Recognition Loss:   0.000197 => Gls Tokens per Sec:     1292 || Batch Translation Loss:   0.048774 => Txt Tokens per Sec:     3649 || Lr: 0.000050
2024-02-11 19:22:04,374 Epoch 438: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.72 
2024-02-11 19:22:04,375 EPOCH 439
2024-02-11 19:22:06,986 [Epoch: 439 Step: 00058300] Batch Recognition Loss:   0.000225 => Gls Tokens per Sec:     1410 || Batch Translation Loss:   0.203208 => Txt Tokens per Sec:     4036 || Lr: 0.000050
2024-02-11 19:22:12,203 Epoch 439: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.54 
2024-02-11 19:22:12,204 EPOCH 440
2024-02-11 19:22:12,935 [Epoch: 440 Step: 00058400] Batch Recognition Loss:   0.000200 => Gls Tokens per Sec:     1423 || Batch Translation Loss:   0.022129 => Txt Tokens per Sec:     3612 || Lr: 0.000050
2024-02-11 19:22:18,575 [Epoch: 440 Step: 00058500] Batch Recognition Loss:   0.000157 => Gls Tokens per Sec:     1419 || Batch Translation Loss:   0.020981 => Txt Tokens per Sec:     3988 || Lr: 0.000050
2024-02-11 19:22:19,697 Epoch 440: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.24 
2024-02-11 19:22:19,697 EPOCH 441
2024-02-11 19:22:24,253 [Epoch: 441 Step: 00058600] Batch Recognition Loss:   0.000145 => Gls Tokens per Sec:     1405 || Batch Translation Loss:   0.017538 => Txt Tokens per Sec:     3936 || Lr: 0.000050
2024-02-11 19:22:27,420 Epoch 441: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.66 
2024-02-11 19:22:27,420 EPOCH 442
2024-02-11 19:22:30,010 [Epoch: 442 Step: 00058700] Batch Recognition Loss:   0.000202 => Gls Tokens per Sec:     1453 || Batch Translation Loss:   0.044275 => Txt Tokens per Sec:     4205 || Lr: 0.000050
2024-02-11 19:22:34,728 Epoch 442: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.02 
2024-02-11 19:22:34,728 EPOCH 443
2024-02-11 19:22:35,406 [Epoch: 443 Step: 00058800] Batch Recognition Loss:   0.000238 => Gls Tokens per Sec:     1654 || Batch Translation Loss:   0.017878 => Txt Tokens per Sec:     4516 || Lr: 0.000050
2024-02-11 19:22:41,439 [Epoch: 443 Step: 00058900] Batch Recognition Loss:   0.000168 => Gls Tokens per Sec:     1324 || Batch Translation Loss:   0.017729 => Txt Tokens per Sec:     3665 || Lr: 0.000050
2024-02-11 19:22:42,501 Epoch 443: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.34 
2024-02-11 19:22:42,502 EPOCH 444
2024-02-11 19:22:46,896 [Epoch: 444 Step: 00059000] Batch Recognition Loss:   0.000160 => Gls Tokens per Sec:     1475 || Batch Translation Loss:   0.022326 => Txt Tokens per Sec:     4086 || Lr: 0.000050
2024-02-11 19:22:49,921 Epoch 444: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.88 
2024-02-11 19:22:49,921 EPOCH 445
2024-02-11 19:22:52,923 [Epoch: 445 Step: 00059100] Batch Recognition Loss:   0.000269 => Gls Tokens per Sec:     1280 || Batch Translation Loss:   0.022129 => Txt Tokens per Sec:     3545 || Lr: 0.000050
2024-02-11 19:22:57,975 Epoch 445: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.23 
2024-02-11 19:22:57,976 EPOCH 446
2024-02-11 19:22:58,752 [Epoch: 446 Step: 00059200] Batch Recognition Loss:   0.000256 => Gls Tokens per Sec:     1548 || Batch Translation Loss:   0.023053 => Txt Tokens per Sec:     4033 || Lr: 0.000050
2024-02-11 19:23:04,074 [Epoch: 446 Step: 00059300] Batch Recognition Loss:   0.000258 => Gls Tokens per Sec:     1501 || Batch Translation Loss:   0.124021 => Txt Tokens per Sec:     4199 || Lr: 0.000050
2024-02-11 19:23:05,143 Epoch 446: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.63 
2024-02-11 19:23:05,143 EPOCH 447
2024-02-11 19:23:09,998 [Epoch: 447 Step: 00059400] Batch Recognition Loss:   0.000234 => Gls Tokens per Sec:     1352 || Batch Translation Loss:   0.053413 => Txt Tokens per Sec:     3676 || Lr: 0.000050
2024-02-11 19:23:13,119 Epoch 447: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.24 
2024-02-11 19:23:13,119 EPOCH 448
2024-02-11 19:23:15,807 [Epoch: 448 Step: 00059500] Batch Recognition Loss:   0.000264 => Gls Tokens per Sec:     1459 || Batch Translation Loss:   0.044778 => Txt Tokens per Sec:     4097 || Lr: 0.000050
2024-02-11 19:23:20,651 Epoch 448: Total Training Recognition Loss 0.04  Total Training Translation Loss 8.95 
2024-02-11 19:23:20,652 EPOCH 449
2024-02-11 19:23:21,767 [Epoch: 449 Step: 00059600] Batch Recognition Loss:   0.000285 => Gls Tokens per Sec:     1149 || Batch Translation Loss:   0.049137 => Txt Tokens per Sec:     3351 || Lr: 0.000050
2024-02-11 19:23:27,130 [Epoch: 449 Step: 00059700] Batch Recognition Loss:   0.000192 => Gls Tokens per Sec:     1492 || Batch Translation Loss:   0.022376 => Txt Tokens per Sec:     4130 || Lr: 0.000050
2024-02-11 19:23:28,146 Epoch 449: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.74 
2024-02-11 19:23:28,146 EPOCH 450
2024-02-11 19:23:33,055 [Epoch: 450 Step: 00059800] Batch Recognition Loss:   0.000411 => Gls Tokens per Sec:     1353 || Batch Translation Loss:   0.016950 => Txt Tokens per Sec:     3764 || Lr: 0.000050
2024-02-11 19:23:36,139 Epoch 450: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.48 
2024-02-11 19:23:36,139 EPOCH 451
2024-02-11 19:23:38,937 [Epoch: 451 Step: 00059900] Batch Recognition Loss:   0.000283 => Gls Tokens per Sec:     1431 || Batch Translation Loss:   0.047324 => Txt Tokens per Sec:     4054 || Lr: 0.000050
2024-02-11 19:23:43,307 Epoch 451: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.63 
2024-02-11 19:23:43,307 EPOCH 452
2024-02-11 19:23:44,222 [Epoch: 452 Step: 00060000] Batch Recognition Loss:   0.000294 => Gls Tokens per Sec:     1488 || Batch Translation Loss:   0.023262 => Txt Tokens per Sec:     4156 || Lr: 0.000050
2024-02-11 19:23:55,612 Validation result at epoch 452, step    60000: duration: 11.3897s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00238	Translation Loss: 89397.53125	PPL: 8335.09082
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.84	(BLEU-1: 10.73,	BLEU-2: 3.65,	BLEU-3: 1.63,	BLEU-4: 0.84)
	CHRF 16.87	ROUGE 9.32
2024-02-11 19:23:55,613 Logging Recognition and Translation Outputs
2024-02-11 19:23:55,614 ========================================================================================================================
2024-02-11 19:23:55,614 Logging Sequence: 55_81.00
2024-02-11 19:23:55,614 	Gloss Reference :	A B+C+D+E
2024-02-11 19:23:55,614 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 19:23:55,614 	Gloss Alignment :	         
2024-02-11 19:23:55,614 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 19:23:55,615 	Text Reference  :	his **** **** ** mother was a teacher and his father was a  refrigerator specialist
2024-02-11 19:23:55,615 	Text Hypothesis :	his last game as he     was a ******* *** *** part   of  rs 6            lakh      
2024-02-11 19:23:55,616 	Text Alignment  :	    I    I    I  S            D       D   D   S      S   S  S            S         
2024-02-11 19:23:55,616 ========================================================================================================================
2024-02-11 19:23:55,616 Logging Sequence: 72_194.00
2024-02-11 19:23:55,616 	Gloss Reference :	A B+C+D+E
2024-02-11 19:23:55,616 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 19:23:55,616 	Gloss Alignment :	         
2024-02-11 19:23:55,616 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 19:23:55,617 	Text Reference  :	shah told her to do what she     wants and  filed  a   police complaint against her  
2024-02-11 19:23:55,618 	Text Hypothesis :	**** **** *** ** a  new  zealand was   just giving her or     by        the     deaf'
2024-02-11 19:23:55,618 	Text Alignment  :	D    D    D   D  S  S    S       S     S    S      S   S      S         S       S    
2024-02-11 19:23:55,618 ========================================================================================================================
2024-02-11 19:23:55,618 Logging Sequence: 108_59.00
2024-02-11 19:23:55,618 	Gloss Reference :	A B+C+D+E
2024-02-11 19:23:55,618 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 19:23:55,618 	Gloss Alignment :	         
2024-02-11 19:23:55,619 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 19:23:55,620 	Text Reference  :	ishan kishan remained the biggest buy of ipl as mumbai indians paid a   whopping rs         1525 crore   to keep him    
2024-02-11 19:23:55,620 	Text Hypothesis :	***** ****** ******** *** ******* *** ** *** ** a      mammoth over 120 lakh     spectators were present at the  auction
2024-02-11 19:23:55,620 	Text Alignment  :	D     D      D        D   D       D   D  D   D  S      S       S    S   S        S          S    S       S  S    S      
2024-02-11 19:23:55,620 ========================================================================================================================
2024-02-11 19:23:55,620 Logging Sequence: 168_104.00
2024-02-11 19:23:55,621 	Gloss Reference :	A B+C+D+E
2024-02-11 19:23:55,621 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 19:23:55,621 	Gloss Alignment :	         
2024-02-11 19:23:55,621 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 19:23:55,622 	Text Reference  :	however many debated against this saying the  couple know how to  care    for their child
2024-02-11 19:23:55,622 	Text Hypothesis :	he      had  a       medical team will   give you    to   the ipl because of  the   team 
2024-02-11 19:23:55,622 	Text Alignment  :	S       S    S       S       S    S      S    S      S    S   S   S       S   S     S    
2024-02-11 19:23:55,623 ========================================================================================================================
2024-02-11 19:23:55,623 Logging Sequence: 54_77.00
2024-02-11 19:23:55,623 	Gloss Reference :	A B+C+D+E
2024-02-11 19:23:55,623 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 19:23:55,623 	Gloss Alignment :	         
2024-02-11 19:23:55,623 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 19:23:55,624 	Text Reference  :	the room rent is about '‚¹          5700 per night these rates are  for 2nd and 3rd july 2023   
2024-02-11 19:23:55,625 	Text Hypothesis :	*** **** **** ** ***** surprisingly many but i     have  to    wait for *** *** *** **** updates
2024-02-11 19:23:55,625 	Text Alignment  :	D   D    D    D  D     S            S    S   S     S     S     S        D   D   D   D    S      
2024-02-11 19:23:55,625 ========================================================================================================================
2024-02-11 19:24:01,593 [Epoch: 452 Step: 00060100] Batch Recognition Loss:   0.000216 => Gls Tokens per Sec:     1337 || Batch Translation Loss:   0.018312 => Txt Tokens per Sec:     3691 || Lr: 0.000050
2024-02-11 19:24:02,581 Epoch 452: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.96 
2024-02-11 19:24:02,582 EPOCH 453
2024-02-11 19:24:07,471 [Epoch: 453 Step: 00060200] Batch Recognition Loss:   0.000256 => Gls Tokens per Sec:     1375 || Batch Translation Loss:   0.016140 => Txt Tokens per Sec:     3883 || Lr: 0.000050
2024-02-11 19:24:10,370 Epoch 453: Total Training Recognition Loss 0.04  Total Training Translation Loss 6.76 
2024-02-11 19:24:10,371 EPOCH 454
2024-02-11 19:24:13,162 [Epoch: 454 Step: 00060300] Batch Recognition Loss:   0.000181 => Gls Tokens per Sec:     1462 || Batch Translation Loss:   0.013024 => Txt Tokens per Sec:     4029 || Lr: 0.000050
2024-02-11 19:24:18,162 Epoch 454: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.55 
2024-02-11 19:24:18,163 EPOCH 455
2024-02-11 19:24:19,061 [Epoch: 455 Step: 00060400] Batch Recognition Loss:   0.000233 => Gls Tokens per Sec:     1605 || Batch Translation Loss:   0.031522 => Txt Tokens per Sec:     4331 || Lr: 0.000050
2024-02-11 19:24:24,596 [Epoch: 455 Step: 00060500] Batch Recognition Loss:   0.000253 => Gls Tokens per Sec:     1446 || Batch Translation Loss:   0.081868 => Txt Tokens per Sec:     3997 || Lr: 0.000050
2024-02-11 19:24:25,518 Epoch 455: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.51 
2024-02-11 19:24:25,518 EPOCH 456
2024-02-11 19:24:30,689 [Epoch: 456 Step: 00060600] Batch Recognition Loss:   0.000208 => Gls Tokens per Sec:     1315 || Batch Translation Loss:   0.028047 => Txt Tokens per Sec:     3663 || Lr: 0.000050
2024-02-11 19:24:33,614 Epoch 456: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.28 
2024-02-11 19:24:33,614 EPOCH 457
2024-02-11 19:24:36,614 [Epoch: 457 Step: 00060700] Batch Recognition Loss:   0.000194 => Gls Tokens per Sec:     1387 || Batch Translation Loss:   0.026071 => Txt Tokens per Sec:     4001 || Lr: 0.000050
2024-02-11 19:24:40,925 Epoch 457: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.56 
2024-02-11 19:24:40,926 EPOCH 458
2024-02-11 19:24:42,136 [Epoch: 458 Step: 00060800] Batch Recognition Loss:   0.000269 => Gls Tokens per Sec:     1257 || Batch Translation Loss:   0.031195 => Txt Tokens per Sec:     3740 || Lr: 0.000050
2024-02-11 19:24:47,844 [Epoch: 458 Step: 00060900] Batch Recognition Loss:   0.000121 => Gls Tokens per Sec:     1402 || Batch Translation Loss:   0.033301 => Txt Tokens per Sec:     3886 || Lr: 0.000050
2024-02-11 19:24:48,657 Epoch 458: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.48 
2024-02-11 19:24:48,657 EPOCH 459
2024-02-11 19:24:53,245 [Epoch: 459 Step: 00061000] Batch Recognition Loss:   0.000170 => Gls Tokens per Sec:     1500 || Batch Translation Loss:   0.027616 => Txt Tokens per Sec:     4148 || Lr: 0.000050
2024-02-11 19:24:56,225 Epoch 459: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.30 
2024-02-11 19:24:56,225 EPOCH 460
2024-02-11 19:24:59,253 [Epoch: 460 Step: 00061100] Batch Recognition Loss:   0.000149 => Gls Tokens per Sec:     1401 || Batch Translation Loss:   0.015805 => Txt Tokens per Sec:     3727 || Lr: 0.000050
2024-02-11 19:25:04,013 Epoch 460: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.96 
2024-02-11 19:25:04,013 EPOCH 461
2024-02-11 19:25:05,093 [Epoch: 461 Step: 00061200] Batch Recognition Loss:   0.000124 => Gls Tokens per Sec:     1483 || Batch Translation Loss:   0.017544 => Txt Tokens per Sec:     4037 || Lr: 0.000050
2024-02-11 19:25:10,879 [Epoch: 461 Step: 00061300] Batch Recognition Loss:   0.000177 => Gls Tokens per Sec:     1381 || Batch Translation Loss:   0.016239 => Txt Tokens per Sec:     3870 || Lr: 0.000050
2024-02-11 19:25:11,647 Epoch 461: Total Training Recognition Loss 0.05  Total Training Translation Loss 13.23 
2024-02-11 19:25:11,648 EPOCH 462
2024-02-11 19:25:16,923 [Epoch: 462 Step: 00061400] Batch Recognition Loss:   0.000249 => Gls Tokens per Sec:     1320 || Batch Translation Loss:   0.062666 => Txt Tokens per Sec:     3703 || Lr: 0.000050
2024-02-11 19:25:19,659 Epoch 462: Total Training Recognition Loss 0.04  Total Training Translation Loss 5.62 
2024-02-11 19:25:19,660 EPOCH 463
2024-02-11 19:25:22,966 [Epoch: 463 Step: 00061500] Batch Recognition Loss:   0.000204 => Gls Tokens per Sec:     1307 || Batch Translation Loss:   0.295735 => Txt Tokens per Sec:     3578 || Lr: 0.000050
2024-02-11 19:25:27,590 Epoch 463: Total Training Recognition Loss 0.04  Total Training Translation Loss 5.29 
2024-02-11 19:25:27,590 EPOCH 464
2024-02-11 19:25:28,772 [Epoch: 464 Step: 00061600] Batch Recognition Loss:   0.000135 => Gls Tokens per Sec:     1423 || Batch Translation Loss:   0.013143 => Txt Tokens per Sec:     4203 || Lr: 0.000050
2024-02-11 19:25:34,414 [Epoch: 464 Step: 00061700] Batch Recognition Loss:   0.000223 => Gls Tokens per Sec:     1416 || Batch Translation Loss:   0.029849 => Txt Tokens per Sec:     3889 || Lr: 0.000050
2024-02-11 19:25:35,122 Epoch 464: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.53 
2024-02-11 19:25:35,122 EPOCH 465
2024-02-11 19:25:40,828 [Epoch: 465 Step: 00061800] Batch Recognition Loss:   0.000200 => Gls Tokens per Sec:     1234 || Batch Translation Loss:   0.020644 => Txt Tokens per Sec:     3454 || Lr: 0.000050
2024-02-11 19:25:43,627 Epoch 465: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.68 
2024-02-11 19:25:43,627 EPOCH 466
2024-02-11 19:25:46,754 [Epoch: 466 Step: 00061900] Batch Recognition Loss:   0.000183 => Gls Tokens per Sec:     1408 || Batch Translation Loss:   0.012895 => Txt Tokens per Sec:     3930 || Lr: 0.000050
2024-02-11 19:25:50,947 Epoch 466: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.64 
2024-02-11 19:25:50,947 EPOCH 467
2024-02-11 19:25:52,388 [Epoch: 467 Step: 00062000] Batch Recognition Loss:   0.000204 => Gls Tokens per Sec:     1223 || Batch Translation Loss:   0.014383 => Txt Tokens per Sec:     3546 || Lr: 0.000050
2024-02-11 19:26:03,878 Validation result at epoch 467, step    62000: duration: 11.4897s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00223	Translation Loss: 89205.64844	PPL: 8175.13086
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.86	(BLEU-1: 11.13,	BLEU-2: 3.59,	BLEU-3: 1.61,	BLEU-4: 0.86)
	CHRF 17.36	ROUGE 9.46
2024-02-11 19:26:03,879 Logging Recognition and Translation Outputs
2024-02-11 19:26:03,879 ========================================================================================================================
2024-02-11 19:26:03,879 Logging Sequence: 173_43.00
2024-02-11 19:26:03,879 	Gloss Reference :	A B+C+D+E
2024-02-11 19:26:03,880 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 19:26:03,880 	Gloss Alignment :	         
2024-02-11 19:26:03,880 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 19:26:03,881 	Text Reference  :	****** reports say   that   virat realised his batting has   suffered on   account of     his captaincy
2024-02-11 19:26:03,881 	Text Hypothesis :	across the     world people were  glued    to  the     covid screens  with 6       medals and pakistan 
2024-02-11 19:26:03,881 	Text Alignment  :	I      S       S     S      S     S        S   S       S     S        S    S       S      S   S        
2024-02-11 19:26:03,882 ========================================================================================================================
2024-02-11 19:26:03,882 Logging Sequence: 122_205.00
2024-02-11 19:26:03,882 	Gloss Reference :	A B+C+D+E
2024-02-11 19:26:03,882 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 19:26:03,882 	Gloss Alignment :	         
2024-02-11 19:26:03,882 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 19:26:03,883 	Text Reference  :	******* * ** **** ***** **** sadly she  lost in   the **** match
2024-02-11 19:26:03,883 	Text Hypothesis :	package c is from first time i     want to   bowl the gold medal
2024-02-11 19:26:03,883 	Text Alignment  :	I       I I  I    I     I    S     S    S    S        I    S    
2024-02-11 19:26:03,883 ========================================================================================================================
2024-02-11 19:26:03,883 Logging Sequence: 149_65.00
2024-02-11 19:26:03,883 	Gloss Reference :	A B+C+D+E
2024-02-11 19:26:03,883 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 19:26:03,883 	Gloss Alignment :	         
2024-02-11 19:26:03,883 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 19:26:03,886 	Text Reference  :	at 6am   on  6th      november 2022 the police reached sri lankan team's hotel   in * sydney australia's central business district cbd  
2024-02-11 19:26:03,886 	Text Hypothesis :	** after her marriage she      took the ****** ******* *** ****** ****** playing in a test   match       between india    and      japan
2024-02-11 19:26:03,886 	Text Alignment  :	D  S     S   S        S        S        D      D       D   D      D      S          I S      S           S       S        S        S    
2024-02-11 19:26:03,886 ========================================================================================================================
2024-02-11 19:26:03,887 Logging Sequence: 81_46.00
2024-02-11 19:26:03,887 	Gloss Reference :	A B+C+D+E
2024-02-11 19:26:03,887 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 19:26:03,887 	Gloss Alignment :	         
2024-02-11 19:26:03,887 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 19:26:03,887 	Text Reference  :	**** **** ****** here is the    story
2024-02-11 19:26:03,888 	Text Hypothesis :	they have always had  a  strong bond 
2024-02-11 19:26:03,888 	Text Alignment  :	I    I    I      S    S  S      S    
2024-02-11 19:26:03,888 ========================================================================================================================
2024-02-11 19:26:03,888 Logging Sequence: 80_214.00
2024-02-11 19:26:03,888 	Gloss Reference :	A B+C+D+E
2024-02-11 19:26:03,888 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 19:26:03,888 	Gloss Alignment :	         
2024-02-11 19:26:03,889 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 19:26:03,889 	Text Reference  :	now let's wait for updates to know ****** about    their  relationship
2024-02-11 19:26:03,889 	Text Hypothesis :	*** ***** **** *** so      i  know indian wrestler sushil kumar       
2024-02-11 19:26:03,889 	Text Alignment  :	D   D     D    D   S       S       I      S        S      S           
2024-02-11 19:26:03,889 ========================================================================================================================
2024-02-11 19:26:09,866 [Epoch: 467 Step: 00062100] Batch Recognition Loss:   0.000237 => Gls Tokens per Sec:     1336 || Batch Translation Loss:   0.016663 => Txt Tokens per Sec:     3670 || Lr: 0.000050
2024-02-11 19:26:10,563 Epoch 467: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.19 
2024-02-11 19:26:10,563 EPOCH 468
2024-02-11 19:26:15,345 [Epoch: 468 Step: 00062200] Batch Recognition Loss:   0.000593 => Gls Tokens per Sec:     1489 || Batch Translation Loss:   0.021960 => Txt Tokens per Sec:     4085 || Lr: 0.000050
2024-02-11 19:26:17,800 Epoch 468: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.16 
2024-02-11 19:26:17,800 EPOCH 469
2024-02-11 19:26:20,996 [Epoch: 469 Step: 00062300] Batch Recognition Loss:   0.000246 => Gls Tokens per Sec:     1403 || Batch Translation Loss:   0.042890 => Txt Tokens per Sec:     4001 || Lr: 0.000050
2024-02-11 19:26:25,413 Epoch 469: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.97 
2024-02-11 19:26:25,414 EPOCH 470
2024-02-11 19:26:26,688 [Epoch: 470 Step: 00062400] Batch Recognition Loss:   0.000288 => Gls Tokens per Sec:     1445 || Batch Translation Loss:   0.034270 => Txt Tokens per Sec:     4217 || Lr: 0.000050
2024-02-11 19:26:32,239 [Epoch: 470 Step: 00062500] Batch Recognition Loss:   0.000243 => Gls Tokens per Sec:     1439 || Batch Translation Loss:   0.108901 => Txt Tokens per Sec:     3928 || Lr: 0.000050
2024-02-11 19:26:32,898 Epoch 470: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.23 
2024-02-11 19:26:32,898 EPOCH 471
2024-02-11 19:26:38,235 [Epoch: 471 Step: 00062600] Batch Recognition Loss:   0.000149 => Gls Tokens per Sec:     1349 || Batch Translation Loss:   0.040425 => Txt Tokens per Sec:     3771 || Lr: 0.000050
2024-02-11 19:26:40,698 Epoch 471: Total Training Recognition Loss 0.04  Total Training Translation Loss 9.70 
2024-02-11 19:26:40,698 EPOCH 472
2024-02-11 19:26:43,619 [Epoch: 472 Step: 00062700] Batch Recognition Loss:   0.000219 => Gls Tokens per Sec:     1562 || Batch Translation Loss:   0.031640 => Txt Tokens per Sec:     4355 || Lr: 0.000050
2024-02-11 19:26:48,152 Epoch 472: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.30 
2024-02-11 19:26:48,152 EPOCH 473
2024-02-11 19:26:49,444 [Epoch: 473 Step: 00062800] Batch Recognition Loss:   0.000153 => Gls Tokens per Sec:     1490 || Batch Translation Loss:   0.031605 => Txt Tokens per Sec:     4079 || Lr: 0.000050
2024-02-11 19:26:55,074 [Epoch: 473 Step: 00062900] Batch Recognition Loss:   0.000147 => Gls Tokens per Sec:     1421 || Batch Translation Loss:   0.062501 => Txt Tokens per Sec:     3969 || Lr: 0.000050
2024-02-11 19:26:55,644 Epoch 473: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.33 
2024-02-11 19:26:55,644 EPOCH 474
2024-02-11 19:27:01,049 [Epoch: 474 Step: 00063000] Batch Recognition Loss:   0.000134 => Gls Tokens per Sec:     1347 || Batch Translation Loss:   0.031125 => Txt Tokens per Sec:     3726 || Lr: 0.000050
2024-02-11 19:27:03,556 Epoch 474: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.76 
2024-02-11 19:27:03,556 EPOCH 475
2024-02-11 19:27:06,526 [Epoch: 475 Step: 00063100] Batch Recognition Loss:   0.000095 => Gls Tokens per Sec:     1563 || Batch Translation Loss:   0.196695 => Txt Tokens per Sec:     4342 || Lr: 0.000050
2024-02-11 19:27:10,745 Epoch 475: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.05 
2024-02-11 19:27:10,745 EPOCH 476
2024-02-11 19:27:12,383 [Epoch: 476 Step: 00063200] Batch Recognition Loss:   0.000217 => Gls Tokens per Sec:     1222 || Batch Translation Loss:   0.080733 => Txt Tokens per Sec:     3214 || Lr: 0.000050
2024-02-11 19:27:18,342 [Epoch: 476 Step: 00063300] Batch Recognition Loss:   0.000306 => Gls Tokens per Sec:     1341 || Batch Translation Loss:   0.027752 => Txt Tokens per Sec:     3789 || Lr: 0.000050
2024-02-11 19:27:18,848 Epoch 476: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.74 
2024-02-11 19:27:18,848 EPOCH 477
2024-02-11 19:27:24,441 [Epoch: 477 Step: 00063400] Batch Recognition Loss:   0.000224 => Gls Tokens per Sec:     1316 || Batch Translation Loss:   0.012477 => Txt Tokens per Sec:     3690 || Lr: 0.000050
2024-02-11 19:27:26,855 Epoch 477: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.05 
2024-02-11 19:27:26,855 EPOCH 478
2024-02-11 19:27:29,990 [Epoch: 478 Step: 00063500] Batch Recognition Loss:   0.000200 => Gls Tokens per Sec:     1506 || Batch Translation Loss:   0.011453 => Txt Tokens per Sec:     4171 || Lr: 0.000050
2024-02-11 19:27:34,521 Epoch 478: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.63 
2024-02-11 19:27:34,521 EPOCH 479
2024-02-11 19:27:35,983 [Epoch: 479 Step: 00063600] Batch Recognition Loss:   0.000269 => Gls Tokens per Sec:     1425 || Batch Translation Loss:   0.019209 => Txt Tokens per Sec:     3550 || Lr: 0.000050
2024-02-11 19:27:42,057 [Epoch: 479 Step: 00063700] Batch Recognition Loss:   0.000135 => Gls Tokens per Sec:     1315 || Batch Translation Loss:   0.095315 => Txt Tokens per Sec:     3773 || Lr: 0.000050
2024-02-11 19:27:42,434 Epoch 479: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.51 
2024-02-11 19:27:42,434 EPOCH 480
2024-02-11 19:27:48,288 [Epoch: 480 Step: 00063800] Batch Recognition Loss:   0.000199 => Gls Tokens per Sec:     1271 || Batch Translation Loss:   0.022746 => Txt Tokens per Sec:     3552 || Lr: 0.000050
2024-02-11 19:27:50,729 Epoch 480: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.77 
2024-02-11 19:27:50,730 EPOCH 481
2024-02-11 19:27:54,288 [Epoch: 481 Step: 00063900] Batch Recognition Loss:   0.000215 => Gls Tokens per Sec:     1350 || Batch Translation Loss:   0.025503 => Txt Tokens per Sec:     3825 || Lr: 0.000050
2024-02-11 19:27:58,859 Epoch 481: Total Training Recognition Loss 0.05  Total Training Translation Loss 7.11 
2024-02-11 19:27:58,860 EPOCH 482
2024-02-11 19:28:00,531 [Epoch: 482 Step: 00064000] Batch Recognition Loss:   0.000201 => Gls Tokens per Sec:     1295 || Batch Translation Loss:   0.031558 => Txt Tokens per Sec:     3700 || Lr: 0.000050
2024-02-11 19:28:12,433 Validation result at epoch 482, step    64000: duration: 11.9028s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00251	Translation Loss: 89434.46094	PPL: 8366.23828
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.71	(BLEU-1: 10.79,	BLEU-2: 3.48,	BLEU-3: 1.44,	BLEU-4: 0.71)
	CHRF 16.99	ROUGE 9.29
2024-02-11 19:28:12,434 Logging Recognition and Translation Outputs
2024-02-11 19:28:12,434 ========================================================================================================================
2024-02-11 19:28:12,434 Logging Sequence: 176_29.00
2024-02-11 19:28:12,435 	Gloss Reference :	A B+C+D+E
2024-02-11 19:28:12,435 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 19:28:12,435 	Gloss Alignment :	         
2024-02-11 19:28:12,435 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 19:28:12,436 	Text Reference  :	****** it    was    a    strong bout   and  sanayev was   in the *** ** **** lead   
2024-02-11 19:28:12,436 	Text Hypothesis :	people began saying that the    reason were against singh at the age of both ukraine
2024-02-11 19:28:12,436 	Text Alignment  :	I      S     S      S    S      S      S    S       S     S      I   I  I    S      
2024-02-11 19:28:12,437 ========================================================================================================================
2024-02-11 19:28:12,437 Logging Sequence: 72_194.00
2024-02-11 19:28:12,437 	Gloss Reference :	A B+C+D+E
2024-02-11 19:28:12,437 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 19:28:12,437 	Gloss Alignment :	         
2024-02-11 19:28:12,437 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 19:28:12,438 	Text Reference  :	***** shah told  her to do   what     she     wants and filed a police complaint against her   
2024-02-11 19:28:12,438 	Text Hypothesis :	let's see  there was an huge argument between sapna and ***** * ****** ********* ******* shobit
2024-02-11 19:28:12,439 	Text Alignment  :	I     S    S     S   S  S    S        S       S         D     D D      D         D       S     
2024-02-11 19:28:12,439 ========================================================================================================================
2024-02-11 19:28:12,439 Logging Sequence: 61_181.00
2024-02-11 19:28:12,439 	Gloss Reference :	A B+C+D+E
2024-02-11 19:28:12,439 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 19:28:12,439 	Gloss Alignment :	         
2024-02-11 19:28:12,439 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 19:28:12,442 	Text Reference  :	one other fan said it is   babar's personal chat we  should focuc  on    this    cricketing career and        not his    personal life
2024-02-11 19:28:12,442 	Text Hypothesis :	*** ***** she said ** 'the ability to       win  all the    league stage matches and        the    semi-final to  emerge as       well
2024-02-11 19:28:12,442 	Text Alignment  :	D   D     S        D  S    S       S        S    S   S      S      S     S       S          S      S          S   S      S        S   
2024-02-11 19:28:12,442 ========================================================================================================================
2024-02-11 19:28:12,442 Logging Sequence: 178_38.00
2024-02-11 19:28:12,442 	Gloss Reference :	A B+C+D+E
2024-02-11 19:28:12,442 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 19:28:12,442 	Gloss Alignment :	         
2024-02-11 19:28:12,443 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 19:28:12,444 	Text Reference  :	** *** ****** *** including 6       others as    the persons responsible for rana' death
2024-02-11 19:28:12,444 	Text Hypothesis :	in the second odi match     against punjab kings xi  punjab  into        a   long  time 
2024-02-11 19:28:12,444 	Text Alignment  :	I  I   I      I   S         S       S      S     S   S       S           S   S     S    
2024-02-11 19:28:12,444 ========================================================================================================================
2024-02-11 19:28:12,444 Logging Sequence: 179_309.00
2024-02-11 19:28:12,444 	Gloss Reference :	A B+C+D+E
2024-02-11 19:28:12,444 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 19:28:12,444 	Gloss Alignment :	         
2024-02-11 19:28:12,445 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 19:28:12,446 	Text Reference  :	before the ioa     could send the   notice wfi has asked phogat to explain her  indiscipline
2024-02-11 19:28:12,446 	Text Hypothesis :	this   is  because he    was  known as     he  has ***** ****** ** ******* been injured     
2024-02-11 19:28:12,446 	Text Alignment  :	S      S   S       S     S    S     S      S       D     D      D  D       S    S           
2024-02-11 19:28:12,446 ========================================================================================================================
2024-02-11 19:28:18,526 [Epoch: 482 Step: 00064100] Batch Recognition Loss:   0.000187 => Gls Tokens per Sec:     1312 || Batch Translation Loss:   0.053413 => Txt Tokens per Sec:     3600 || Lr: 0.000050
2024-02-11 19:28:18,887 Epoch 482: Total Training Recognition Loss 0.03  Total Training Translation Loss 11.92 
2024-02-11 19:28:18,887 EPOCH 483
2024-02-11 19:28:24,410 [Epoch: 483 Step: 00064200] Batch Recognition Loss:   0.000147 => Gls Tokens per Sec:     1362 || Batch Translation Loss:   0.060975 => Txt Tokens per Sec:     3852 || Lr: 0.000050
2024-02-11 19:28:26,796 Epoch 483: Total Training Recognition Loss 0.03  Total Training Translation Loss 10.61 
2024-02-11 19:28:26,796 EPOCH 484
2024-02-11 19:28:30,435 [Epoch: 484 Step: 00064300] Batch Recognition Loss:   0.000238 => Gls Tokens per Sec:     1341 || Batch Translation Loss:   0.050716 => Txt Tokens per Sec:     3769 || Lr: 0.000050
2024-02-11 19:28:34,602 Epoch 484: Total Training Recognition Loss 0.04  Total Training Translation Loss 7.41 
2024-02-11 19:28:34,602 EPOCH 485
2024-02-11 19:28:36,323 [Epoch: 485 Step: 00064400] Batch Recognition Loss:   0.000337 => Gls Tokens per Sec:     1302 || Batch Translation Loss:   0.025000 => Txt Tokens per Sec:     3466 || Lr: 0.000050
2024-02-11 19:28:42,609 [Epoch: 485 Step: 00064500] Batch Recognition Loss:   0.000184 => Gls Tokens per Sec:     1271 || Batch Translation Loss:   0.024487 => Txt Tokens per Sec:     3573 || Lr: 0.000050
2024-02-11 19:28:42,886 Epoch 485: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.99 
2024-02-11 19:28:42,886 EPOCH 486
2024-02-11 19:28:48,337 [Epoch: 486 Step: 00064600] Batch Recognition Loss:   0.000188 => Gls Tokens per Sec:     1394 || Batch Translation Loss:   0.023556 => Txt Tokens per Sec:     3894 || Lr: 0.000050
2024-02-11 19:28:50,503 Epoch 486: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.57 
2024-02-11 19:28:50,503 EPOCH 487
2024-02-11 19:28:54,122 [Epoch: 487 Step: 00064700] Batch Recognition Loss:   0.000202 => Gls Tokens per Sec:     1371 || Batch Translation Loss:   0.017793 => Txt Tokens per Sec:     3829 || Lr: 0.000050
2024-02-11 19:28:58,398 Epoch 487: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.74 
2024-02-11 19:28:58,399 EPOCH 488
2024-02-11 19:29:00,225 [Epoch: 488 Step: 00064800] Batch Recognition Loss:   0.000196 => Gls Tokens per Sec:     1271 || Batch Translation Loss:   0.295070 => Txt Tokens per Sec:     3800 || Lr: 0.000050
