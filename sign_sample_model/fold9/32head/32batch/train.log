2024-02-04 05:32:26,603 Hello! This is Joey-NMT.
2024-02-04 05:32:26,613 Total params: 25639944
2024-02-04 05:32:26,614 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'gloss_output_layer.bias', 'gloss_output_layer.weight', 'sgn_embed.ln.bias', 'sgn_embed.ln.weight', 'sgn_embed.norm.norm.bias', 'sgn_embed.norm.norm.weight', 'txt_embed.norm.norm.bias', 'txt_embed.norm.norm.weight']
2024-02-04 05:32:27,797 cfg.name                           : sign_experiment
2024-02-04 05:32:27,797 cfg.data.data_path                 : ./data/Sports_dataset/9/
2024-02-04 05:32:27,797 cfg.data.version                   : phoenix_2014_trans
2024-02-04 05:32:27,797 cfg.data.sgn                       : sign
2024-02-04 05:32:27,797 cfg.data.txt                       : text
2024-02-04 05:32:27,797 cfg.data.gls                       : gloss
2024-02-04 05:32:27,797 cfg.data.train                     : excel_data.train
2024-02-04 05:32:27,798 cfg.data.dev                       : excel_data.dev
2024-02-04 05:32:27,798 cfg.data.test                      : excel_data.test
2024-02-04 05:32:27,798 cfg.data.feature_size              : 2560
2024-02-04 05:32:27,798 cfg.data.level                     : word
2024-02-04 05:32:27,798 cfg.data.txt_lowercase             : True
2024-02-04 05:32:27,798 cfg.data.max_sent_length           : 500
2024-02-04 05:32:27,798 cfg.data.random_train_subset       : -1
2024-02-04 05:32:27,799 cfg.data.random_dev_subset         : -1
2024-02-04 05:32:27,799 cfg.testing.recognition_beam_sizes : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
2024-02-04 05:32:27,799 cfg.testing.translation_beam_sizes : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
2024-02-04 05:32:27,799 cfg.testing.translation_beam_alphas : [-1, 0, 1, 2, 3, 4, 5]
2024-02-04 05:32:27,799 cfg.training.reset_best_ckpt       : False
2024-02-04 05:32:27,799 cfg.training.reset_scheduler       : False
2024-02-04 05:32:27,799 cfg.training.reset_optimizer       : False
2024-02-04 05:32:27,799 cfg.training.random_seed           : 42
2024-02-04 05:32:27,800 cfg.training.model_dir             : ./sign_sample_model/fold9/32head/32batch
2024-02-04 05:32:27,800 cfg.training.recognition_loss_weight : 1.0
2024-02-04 05:32:27,800 cfg.training.translation_loss_weight : 1.0
2024-02-04 05:32:27,800 cfg.training.eval_metric           : bleu
2024-02-04 05:32:27,800 cfg.training.optimizer             : adam
2024-02-04 05:32:27,800 cfg.training.learning_rate         : 0.0001
2024-02-04 05:32:27,800 cfg.training.batch_size            : 32
2024-02-04 05:32:27,800 cfg.training.num_valid_log         : 5
2024-02-04 05:32:27,801 cfg.training.epochs                : 50000
2024-02-04 05:32:27,801 cfg.training.early_stopping_metric : eval_metric
2024-02-04 05:32:27,801 cfg.training.batch_type            : sentence
2024-02-04 05:32:27,801 cfg.training.translation_normalization : batch
2024-02-04 05:32:27,801 cfg.training.eval_recognition_beam_size : 1
2024-02-04 05:32:27,801 cfg.training.eval_translation_beam_size : 1
2024-02-04 05:32:27,801 cfg.training.eval_translation_beam_alpha : -1
2024-02-04 05:32:27,801 cfg.training.overwrite             : True
2024-02-04 05:32:27,801 cfg.training.shuffle               : True
2024-02-04 05:32:27,802 cfg.training.use_cuda              : True
2024-02-04 05:32:27,802 cfg.training.translation_max_output_length : 40
2024-02-04 05:32:27,802 cfg.training.keep_last_ckpts       : 1
2024-02-04 05:32:27,802 cfg.training.batch_multiplier      : 1
2024-02-04 05:32:27,802 cfg.training.logging_freq          : 100
2024-02-04 05:32:27,802 cfg.training.validation_freq       : 2000
2024-02-04 05:32:27,802 cfg.training.betas                 : [0.9, 0.998]
2024-02-04 05:32:27,802 cfg.training.scheduling            : plateau
2024-02-04 05:32:27,803 cfg.training.learning_rate_min     : 1e-08
2024-02-04 05:32:27,803 cfg.training.weight_decay          : 0.0001
2024-02-04 05:32:27,803 cfg.training.patience              : 12
2024-02-04 05:32:27,803 cfg.training.decrease_factor       : 0.5
2024-02-04 05:32:27,803 cfg.training.label_smoothing       : 0.0
2024-02-04 05:32:27,803 cfg.model.initializer              : xavier
2024-02-04 05:32:27,803 cfg.model.bias_initializer         : zeros
2024-02-04 05:32:27,803 cfg.model.init_gain                : 1.0
2024-02-04 05:32:27,803 cfg.model.embed_initializer        : xavier
2024-02-04 05:32:27,804 cfg.model.embed_init_gain          : 1.0
2024-02-04 05:32:27,804 cfg.model.tied_softmax             : True
2024-02-04 05:32:27,804 cfg.model.encoder.type             : transformer
2024-02-04 05:32:27,804 cfg.model.encoder.num_layers       : 3
2024-02-04 05:32:27,804 cfg.model.encoder.num_heads        : 32
2024-02-04 05:32:27,804 cfg.model.encoder.embeddings.embedding_dim : 512
2024-02-04 05:32:27,804 cfg.model.encoder.embeddings.scale : False
2024-02-04 05:32:27,804 cfg.model.encoder.embeddings.dropout : 0.1
2024-02-04 05:32:27,804 cfg.model.encoder.embeddings.norm_type : batch
2024-02-04 05:32:27,805 cfg.model.encoder.embeddings.activation_type : softsign
2024-02-04 05:32:27,805 cfg.model.encoder.hidden_size      : 512
2024-02-04 05:32:27,805 cfg.model.encoder.ff_size          : 2048
2024-02-04 05:32:27,805 cfg.model.encoder.dropout          : 0.1
2024-02-04 05:32:27,805 cfg.model.decoder.type             : transformer
2024-02-04 05:32:27,805 cfg.model.decoder.num_layers       : 3
2024-02-04 05:32:27,805 cfg.model.decoder.num_heads        : 32
2024-02-04 05:32:27,805 cfg.model.decoder.embeddings.embedding_dim : 512
2024-02-04 05:32:27,806 cfg.model.decoder.embeddings.scale : False
2024-02-04 05:32:27,806 cfg.model.decoder.embeddings.dropout : 0.1
2024-02-04 05:32:27,806 cfg.model.decoder.embeddings.norm_type : batch
2024-02-04 05:32:27,806 cfg.model.decoder.embeddings.activation_type : softsign
2024-02-04 05:32:27,806 cfg.model.decoder.hidden_size      : 512
2024-02-04 05:32:27,806 cfg.model.decoder.ff_size          : 2048
2024-02-04 05:32:27,806 cfg.model.decoder.dropout          : 0.1
2024-02-04 05:32:27,806 Data set sizes: 
	train 2126,
	valid 708,
	test 706
2024-02-04 05:32:27,806 First training example:
	[GLS] A B C D E
	[TXT] although new zealand was disappointed to faltered at the finals against australia they did well throughout the tournament
2024-02-04 05:32:27,807 First 10 words (gls): (0) <si> (1) <unk> (2) <pad> (3) A (4) B (5) C (6) D (7) E
2024-02-04 05:32:27,807 First 10 words (txt): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) the (5) and (6) to (7) a (8) in (9) of
2024-02-04 05:32:27,807 Number of unique glosses (types): 8
2024-02-04 05:32:27,807 Number of unique words (types): 4397
2024-02-04 05:32:27,807 SignModel(
	encoder=TransformerEncoder(num_layers=3, num_heads=32),
	decoder=TransformerDecoder(num_layers=3, num_heads=32),
	sgn_embed=SpatialEmbeddings(embedding_dim=512, input_size=2560),
	txt_embed=Embeddings(embedding_dim=512, vocab_size=4397))
2024-02-04 05:32:27,811 EPOCH 1
2024-02-04 05:32:33,676 Epoch   1: Total Training Recognition Loss 156.13  Total Training Translation Loss 6519.90 
2024-02-04 05:32:33,676 EPOCH 2
2024-02-04 05:32:36,241 [Epoch: 002 Step: 00000100] Batch Recognition Loss:   0.068378 => Gls Tokens per Sec:     2060 || Batch Translation Loss:  84.538765 => Txt Tokens per Sec:     5870 || Lr: 0.000100
2024-02-04 05:32:38,919 Epoch   2: Total Training Recognition Loss 6.80  Total Training Translation Loss 5945.03 
2024-02-04 05:32:38,919 EPOCH 3
2024-02-04 05:32:44,258 [Epoch: 003 Step: 00000200] Batch Recognition Loss:   0.007355 => Gls Tokens per Sec:     1961 || Batch Translation Loss:  85.197311 => Txt Tokens per Sec:     5453 || Lr: 0.000100
2024-02-04 05:32:44,312 Epoch   3: Total Training Recognition Loss 1.24  Total Training Translation Loss 5836.25 
2024-02-04 05:32:44,313 EPOCH 4
2024-02-04 05:32:49,974 Epoch   4: Total Training Recognition Loss 0.69  Total Training Translation Loss 5650.69 
2024-02-04 05:32:49,975 EPOCH 5
2024-02-04 05:32:52,143 [Epoch: 005 Step: 00000300] Batch Recognition Loss:   0.011016 => Gls Tokens per Sec:     2362 || Batch Translation Loss:  89.680824 => Txt Tokens per Sec:     6259 || Lr: 0.000100
2024-02-04 05:32:55,196 Epoch   5: Total Training Recognition Loss 0.56  Total Training Translation Loss 5413.92 
2024-02-04 05:32:55,196 EPOCH 6
2024-02-04 05:33:00,156 [Epoch: 006 Step: 00000400] Batch Recognition Loss:   0.005602 => Gls Tokens per Sec:     2079 || Batch Translation Loss:  73.186089 => Txt Tokens per Sec:     5758 || Lr: 0.000100
2024-02-04 05:33:00,346 Epoch   6: Total Training Recognition Loss 0.45  Total Training Translation Loss 5193.87 
2024-02-04 05:33:00,346 EPOCH 7
2024-02-04 05:33:05,858 Epoch   7: Total Training Recognition Loss 0.46  Total Training Translation Loss 4981.99 
2024-02-04 05:33:05,859 EPOCH 8
2024-02-04 05:33:08,171 [Epoch: 008 Step: 00000500] Batch Recognition Loss:   0.004510 => Gls Tokens per Sec:     2147 || Batch Translation Loss:  56.967659 => Txt Tokens per Sec:     6062 || Lr: 0.000100
2024-02-04 05:33:11,071 Epoch   8: Total Training Recognition Loss 0.49  Total Training Translation Loss 4773.05 
2024-02-04 05:33:11,071 EPOCH 9
2024-02-04 05:33:16,295 [Epoch: 009 Step: 00000600] Batch Recognition Loss:   0.004418 => Gls Tokens per Sec:     1944 || Batch Translation Loss:  85.164215 => Txt Tokens per Sec:     5379 || Lr: 0.000100
2024-02-04 05:33:16,521 Epoch   9: Total Training Recognition Loss 0.48  Total Training Translation Loss 4589.40 
2024-02-04 05:33:16,521 EPOCH 10
2024-02-04 05:33:21,862 Epoch  10: Total Training Recognition Loss 0.59  Total Training Translation Loss 4392.86 
2024-02-04 05:33:21,862 EPOCH 11
2024-02-04 05:33:24,026 [Epoch: 011 Step: 00000700] Batch Recognition Loss:   0.017090 => Gls Tokens per Sec:     2221 || Batch Translation Loss:  35.557781 => Txt Tokens per Sec:     6060 || Lr: 0.000100
2024-02-04 05:33:26,828 Epoch  11: Total Training Recognition Loss 0.48  Total Training Translation Loss 4205.92 
2024-02-04 05:33:26,828 EPOCH 12
2024-02-04 05:33:32,167 [Epoch: 012 Step: 00000800] Batch Recognition Loss:   0.010456 => Gls Tokens per Sec:     1871 || Batch Translation Loss:  77.286919 => Txt Tokens per Sec:     5198 || Lr: 0.000100
2024-02-04 05:33:32,467 Epoch  12: Total Training Recognition Loss 0.61  Total Training Translation Loss 4039.99 
2024-02-04 05:33:32,468 EPOCH 13
2024-02-04 05:33:37,571 Epoch  13: Total Training Recognition Loss 0.61  Total Training Translation Loss 3858.21 
2024-02-04 05:33:37,571 EPOCH 14
2024-02-04 05:33:40,172 [Epoch: 014 Step: 00000900] Batch Recognition Loss:   0.014015 => Gls Tokens per Sec:     1785 || Batch Translation Loss:  62.888702 => Txt Tokens per Sec:     5249 || Lr: 0.000100
2024-02-04 05:33:42,919 Epoch  14: Total Training Recognition Loss 0.73  Total Training Translation Loss 3704.99 
2024-02-04 05:33:42,919 EPOCH 15
2024-02-04 05:33:47,937 [Epoch: 015 Step: 00001000] Batch Recognition Loss:   0.010471 => Gls Tokens per Sec:     1959 || Batch Translation Loss:  57.831085 => Txt Tokens per Sec:     5426 || Lr: 0.000100
2024-02-04 05:33:48,333 Epoch  15: Total Training Recognition Loss 0.79  Total Training Translation Loss 3542.06 
2024-02-04 05:33:48,333 EPOCH 16
2024-02-04 05:33:53,545 Epoch  16: Total Training Recognition Loss 0.86  Total Training Translation Loss 3384.41 
2024-02-04 05:33:53,546 EPOCH 17
2024-02-04 05:33:55,997 [Epoch: 017 Step: 00001100] Batch Recognition Loss:   0.014944 => Gls Tokens per Sec:     1793 || Batch Translation Loss:  27.346943 => Txt Tokens per Sec:     5010 || Lr: 0.000100
2024-02-04 05:33:59,215 Epoch  17: Total Training Recognition Loss 0.95  Total Training Translation Loss 3236.05 
2024-02-04 05:33:59,216 EPOCH 18
2024-02-04 05:34:04,089 [Epoch: 018 Step: 00001200] Batch Recognition Loss:   0.015148 => Gls Tokens per Sec:     1984 || Batch Translation Loss:  49.439247 => Txt Tokens per Sec:     5447 || Lr: 0.000100
2024-02-04 05:34:04,636 Epoch  18: Total Training Recognition Loss 0.92  Total Training Translation Loss 3075.21 
2024-02-04 05:34:04,637 EPOCH 19
2024-02-04 05:34:09,544 Epoch  19: Total Training Recognition Loss 1.01  Total Training Translation Loss 2940.61 
2024-02-04 05:34:09,544 EPOCH 20
2024-02-04 05:34:11,692 [Epoch: 020 Step: 00001300] Batch Recognition Loss:   0.006821 => Gls Tokens per Sec:     2012 || Batch Translation Loss:  37.866283 => Txt Tokens per Sec:     5512 || Lr: 0.000100
2024-02-04 05:34:15,002 Epoch  20: Total Training Recognition Loss 0.98  Total Training Translation Loss 2768.82 
2024-02-04 05:34:15,002 EPOCH 21
2024-02-04 05:34:19,572 [Epoch: 021 Step: 00001400] Batch Recognition Loss:   0.010371 => Gls Tokens per Sec:     2082 || Batch Translation Loss:  36.026287 => Txt Tokens per Sec:     5715 || Lr: 0.000100
2024-02-04 05:34:20,260 Epoch  21: Total Training Recognition Loss 1.10  Total Training Translation Loss 2654.94 
2024-02-04 05:34:20,260 EPOCH 22
2024-02-04 05:34:25,980 Epoch  22: Total Training Recognition Loss 1.17  Total Training Translation Loss 2537.75 
2024-02-04 05:34:25,980 EPOCH 23
2024-02-04 05:34:28,040 [Epoch: 023 Step: 00001500] Batch Recognition Loss:   0.011304 => Gls Tokens per Sec:     2020 || Batch Translation Loss:  30.842436 => Txt Tokens per Sec:     5467 || Lr: 0.000100
2024-02-04 05:34:31,488 Epoch  23: Total Training Recognition Loss 1.07  Total Training Translation Loss 2380.60 
2024-02-04 05:34:31,489 EPOCH 24
2024-02-04 05:34:36,064 [Epoch: 024 Step: 00001600] Batch Recognition Loss:   0.016121 => Gls Tokens per Sec:     2045 || Batch Translation Loss:  43.913242 => Txt Tokens per Sec:     5687 || Lr: 0.000100
2024-02-04 05:34:36,904 Epoch  24: Total Training Recognition Loss 1.07  Total Training Translation Loss 2232.37 
2024-02-04 05:34:36,904 EPOCH 25
2024-02-04 05:34:42,452 Epoch  25: Total Training Recognition Loss 1.13  Total Training Translation Loss 2100.75 
2024-02-04 05:34:42,452 EPOCH 26
2024-02-04 05:34:44,354 [Epoch: 026 Step: 00001700] Batch Recognition Loss:   0.017810 => Gls Tokens per Sec:     2104 || Batch Translation Loss:  42.242226 => Txt Tokens per Sec:     5884 || Lr: 0.000100
2024-02-04 05:34:47,670 Epoch  26: Total Training Recognition Loss 1.06  Total Training Translation Loss 1969.47 
2024-02-04 05:34:47,670 EPOCH 27
2024-02-04 05:34:52,226 [Epoch: 027 Step: 00001800] Batch Recognition Loss:   0.018736 => Gls Tokens per Sec:     2017 || Batch Translation Loss:  35.857605 => Txt Tokens per Sec:     5609 || Lr: 0.000100
2024-02-04 05:34:52,889 Epoch  27: Total Training Recognition Loss 1.10  Total Training Translation Loss 1853.85 
2024-02-04 05:34:52,889 EPOCH 28
2024-02-04 05:34:58,309 Epoch  28: Total Training Recognition Loss 1.11  Total Training Translation Loss 1733.50 
2024-02-04 05:34:58,309 EPOCH 29
2024-02-04 05:35:00,056 [Epoch: 029 Step: 00001900] Batch Recognition Loss:   0.013843 => Gls Tokens per Sec:     2199 || Batch Translation Loss:  12.938041 => Txt Tokens per Sec:     6234 || Lr: 0.000100
2024-02-04 05:35:03,209 Epoch  29: Total Training Recognition Loss 1.06  Total Training Translation Loss 1616.86 
2024-02-04 05:35:03,209 EPOCH 30
2024-02-04 05:35:07,616 [Epoch: 030 Step: 00002000] Batch Recognition Loss:   0.015733 => Gls Tokens per Sec:     2049 || Batch Translation Loss:  16.055706 => Txt Tokens per Sec:     5620 || Lr: 0.000100
2024-02-04 05:35:16,643 Hooray! New best validation result [eval_metric]!
2024-02-04 05:35:16,644 Saving new checkpoint.
2024-02-04 05:35:16,886 Validation result at epoch  30, step     2000: duration: 9.2692s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.10733	Translation Loss: 63108.16406	PPL: 552.93311
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.82	(BLEU-1: 13.04,	BLEU-2: 4.31,	BLEU-3: 1.76,	BLEU-4: 0.82)
	CHRF 16.63	ROUGE 11.39
2024-02-04 05:35:16,887 Logging Recognition and Translation Outputs
2024-02-04 05:35:16,887 ========================================================================================================================
2024-02-04 05:35:16,887 Logging Sequence: 182_115.00
2024-02-04 05:35:16,888 	Gloss Reference :	A B+C+D+E
2024-02-04 05:35:16,888 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 05:35:16,888 	Gloss Alignment :	         
2024-02-04 05:35:16,888 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 05:35:16,890 	Text Reference  :	fans are unclear whether yuvraj will be    returning to    play    test match odi  or   in   t20 leagues from february 2022
2024-02-04 05:35:16,890 	Text Hypothesis :	**** *** ******* kohli   was    very proud by        these matches as   they  were very well and now     on   the      game
2024-02-04 05:35:16,890 	Text Alignment  :	D    D   D       S       S      S    S     S         S     S       S    S     S    S    S    S   S       S    S        S   
2024-02-04 05:35:16,890 ========================================================================================================================
2024-02-04 05:35:16,890 Logging Sequence: 140_120.00
2024-02-04 05:35:16,891 	Gloss Reference :	A B+C+D+E
2024-02-04 05:35:16,891 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 05:35:16,891 	Gloss Alignment :	         
2024-02-04 05:35:16,891 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 05:35:16,893 	Text Reference  :	but why so it is because pant is     a talented player and     it      will help  encouraging the youth of uttarakhand toward sports
2024-02-04 05:35:16,893 	Text Hypothesis :	*** *** ** he is because he   played a test     series against england and  india won         the ***** ** *********** world  cup   
2024-02-04 05:35:16,893 	Text Alignment  :	D   D   D  S             S    S        S        S      S       S       S    S     S               D     D  D           S      S     
2024-02-04 05:35:16,893 ========================================================================================================================
2024-02-04 05:35:16,893 Logging Sequence: 85_36.00
2024-02-04 05:35:16,894 	Gloss Reference :	A B+C+D+E
2024-02-04 05:35:16,894 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 05:35:16,894 	Gloss Alignment :	         
2024-02-04 05:35:16,894 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 05:35:16,895 	Text Reference  :	**** symonds has scored 2 centuries in 26 tests that   he played  for his    country
2024-02-04 05:35:16,895 	Text Hypothesis :	when he      has ****** * ********* ** ** ***** played 8  wickets and scored 3175   
2024-02-04 05:35:16,895 	Text Alignment  :	I    S           D      D D         D  D  D     S      S  S       S   S      S      
2024-02-04 05:35:16,895 ========================================================================================================================
2024-02-04 05:35:16,895 Logging Sequence: 164_100.00
2024-02-04 05:35:16,895 	Gloss Reference :	A B+C+D+E
2024-02-04 05:35:16,896 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 05:35:16,896 	Gloss Alignment :	         
2024-02-04 05:35:16,896 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 05:35:16,898 	Text Reference  :	the  tv rights  for broadcasting ipl matches in  india for the next 5 years went to star   india for          rs  23575 crore
2024-02-04 05:35:16,898 	Text Hypothesis :	this is because of  the          ipl ******* and now   won the **** * ***** **** ** rights of    broadcasting the world cup  
2024-02-04 05:35:16,898 	Text Alignment  :	S    S  S       S   S                D       S   S     S       D    D D     D    D  S      S     S            S   S     S    
2024-02-04 05:35:16,898 ========================================================================================================================
2024-02-04 05:35:16,898 Logging Sequence: 76_79.00
2024-02-04 05:35:16,898 	Gloss Reference :	A B+C+D+E
2024-02-04 05:35:16,899 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 05:35:16,899 	Gloss Alignment :	         
2024-02-04 05:35:16,899 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 05:35:16,900 	Text Reference  :	*** *** **** ** ****** ******* ****** *** ****** ******* ******* speaking to      ani     csk     ceo  kasi viswanathan said 
2024-02-04 05:35:16,900 	Text Hypothesis :	the ipl will be played between mumbai and mumbai indians indians indians  indians indians indians were held in          dubai
2024-02-04 05:35:16,900 	Text Alignment  :	I   I   I    I  I      I       I      I   I      I       I       S        S       S       S       S    S    S           S    
2024-02-04 05:35:16,900 ========================================================================================================================
2024-02-04 05:35:17,965 Epoch  30: Total Training Recognition Loss 1.06  Total Training Translation Loss 1521.35 
2024-02-04 05:35:17,965 EPOCH 31
2024-02-04 05:35:23,545 Epoch  31: Total Training Recognition Loss 1.08  Total Training Translation Loss 1416.65 
2024-02-04 05:35:23,546 EPOCH 32
2024-02-04 05:35:25,330 [Epoch: 032 Step: 00002100] Batch Recognition Loss:   0.012512 => Gls Tokens per Sec:     2063 || Batch Translation Loss:  12.424413 => Txt Tokens per Sec:     5745 || Lr: 0.000100
2024-02-04 05:35:28,590 Epoch  32: Total Training Recognition Loss 0.99  Total Training Translation Loss 1292.76 
2024-02-04 05:35:28,590 EPOCH 33
2024-02-04 05:35:33,212 [Epoch: 033 Step: 00002200] Batch Recognition Loss:   0.013944 => Gls Tokens per Sec:     1939 || Batch Translation Loss:  16.506128 => Txt Tokens per Sec:     5469 || Lr: 0.000100
2024-02-04 05:35:34,070 Epoch  33: Total Training Recognition Loss 0.97  Total Training Translation Loss 1209.46 
2024-02-04 05:35:34,070 EPOCH 34
2024-02-04 05:35:39,677 Epoch  34: Total Training Recognition Loss 0.97  Total Training Translation Loss 1114.27 
2024-02-04 05:35:39,678 EPOCH 35
2024-02-04 05:35:41,588 [Epoch: 035 Step: 00002300] Batch Recognition Loss:   0.017814 => Gls Tokens per Sec:     1797 || Batch Translation Loss:  19.528605 => Txt Tokens per Sec:     4955 || Lr: 0.000100
2024-02-04 05:35:45,275 Epoch  35: Total Training Recognition Loss 0.95  Total Training Translation Loss 1023.61 
2024-02-04 05:35:45,275 EPOCH 36
2024-02-04 05:35:49,719 [Epoch: 036 Step: 00002400] Batch Recognition Loss:   0.010742 => Gls Tokens per Sec:     1960 || Batch Translation Loss:   5.497292 => Txt Tokens per Sec:     5377 || Lr: 0.000100
2024-02-04 05:35:50,904 Epoch  36: Total Training Recognition Loss 0.85  Total Training Translation Loss 944.11 
2024-02-04 05:35:50,905 EPOCH 37
2024-02-04 05:35:55,932 Epoch  37: Total Training Recognition Loss 0.87  Total Training Translation Loss 852.16 
2024-02-04 05:35:55,932 EPOCH 38
2024-02-04 05:35:57,833 [Epoch: 038 Step: 00002500] Batch Recognition Loss:   0.006773 => Gls Tokens per Sec:     1768 || Batch Translation Loss:  10.575414 => Txt Tokens per Sec:     5210 || Lr: 0.000100
2024-02-04 05:36:01,486 Epoch  38: Total Training Recognition Loss 0.78  Total Training Translation Loss 774.47 
2024-02-04 05:36:01,486 EPOCH 39
2024-02-04 05:36:05,731 [Epoch: 039 Step: 00002600] Batch Recognition Loss:   0.009040 => Gls Tokens per Sec:     2015 || Batch Translation Loss:   8.785995 => Txt Tokens per Sec:     5742 || Lr: 0.000100
2024-02-04 05:36:06,484 Epoch  39: Total Training Recognition Loss 0.79  Total Training Translation Loss 707.48 
2024-02-04 05:36:06,485 EPOCH 40
2024-02-04 05:36:11,939 Epoch  40: Total Training Recognition Loss 0.70  Total Training Translation Loss 652.62 
2024-02-04 05:36:11,940 EPOCH 41
2024-02-04 05:36:13,303 [Epoch: 041 Step: 00002700] Batch Recognition Loss:   0.008793 => Gls Tokens per Sec:     2350 || Batch Translation Loss:  10.972318 => Txt Tokens per Sec:     6264 || Lr: 0.000100
2024-02-04 05:36:17,212 Epoch  41: Total Training Recognition Loss 0.70  Total Training Translation Loss 600.80 
2024-02-04 05:36:17,213 EPOCH 42
2024-02-04 05:36:21,580 [Epoch: 042 Step: 00002800] Batch Recognition Loss:   0.007025 => Gls Tokens per Sec:     1942 || Batch Translation Loss:   8.818874 => Txt Tokens per Sec:     5438 || Lr: 0.000100
2024-02-04 05:36:22,675 Epoch  42: Total Training Recognition Loss 0.66  Total Training Translation Loss 540.48 
2024-02-04 05:36:22,675 EPOCH 43
2024-02-04 05:36:28,174 Epoch  43: Total Training Recognition Loss 0.60  Total Training Translation Loss 485.23 
2024-02-04 05:36:28,174 EPOCH 44
2024-02-04 05:36:29,693 [Epoch: 044 Step: 00002900] Batch Recognition Loss:   0.005390 => Gls Tokens per Sec:     1943 || Batch Translation Loss:   3.065128 => Txt Tokens per Sec:     5377 || Lr: 0.000100
2024-02-04 05:36:33,398 Epoch  44: Total Training Recognition Loss 0.61  Total Training Translation Loss 436.62 
2024-02-04 05:36:33,399 EPOCH 45
2024-02-04 05:36:37,497 [Epoch: 045 Step: 00003000] Batch Recognition Loss:   0.006274 => Gls Tokens per Sec:     2009 || Batch Translation Loss:   7.073267 => Txt Tokens per Sec:     5435 || Lr: 0.000100
2024-02-04 05:36:38,841 Epoch  45: Total Training Recognition Loss 0.52  Total Training Translation Loss 397.28 
2024-02-04 05:36:38,842 EPOCH 46
2024-02-04 05:36:44,254 Epoch  46: Total Training Recognition Loss 0.50  Total Training Translation Loss 367.79 
2024-02-04 05:36:44,254 EPOCH 47
2024-02-04 05:36:45,404 [Epoch: 047 Step: 00003100] Batch Recognition Loss:   0.006830 => Gls Tokens per Sec:     2506 || Batch Translation Loss:   4.357892 => Txt Tokens per Sec:     6128 || Lr: 0.000100
2024-02-04 05:36:49,693 Epoch  47: Total Training Recognition Loss 0.51  Total Training Translation Loss 342.59 
2024-02-04 05:36:49,694 EPOCH 48
2024-02-04 05:36:53,874 [Epoch: 048 Step: 00003200] Batch Recognition Loss:   0.006620 => Gls Tokens per Sec:     1953 || Batch Translation Loss:   4.145802 => Txt Tokens per Sec:     5361 || Lr: 0.000100
2024-02-04 05:36:55,213 Epoch  48: Total Training Recognition Loss 0.43  Total Training Translation Loss 307.46 
2024-02-04 05:36:55,213 EPOCH 49
2024-02-04 05:37:00,366 Epoch  49: Total Training Recognition Loss 0.41  Total Training Translation Loss 276.34 
2024-02-04 05:37:00,367 EPOCH 50
2024-02-04 05:37:01,589 [Epoch: 050 Step: 00003300] Batch Recognition Loss:   0.004649 => Gls Tokens per Sec:     2228 || Batch Translation Loss:   4.814589 => Txt Tokens per Sec:     6190 || Lr: 0.000100
2024-02-04 05:37:05,544 Epoch  50: Total Training Recognition Loss 0.39  Total Training Translation Loss 257.12 
2024-02-04 05:37:05,545 EPOCH 51
2024-02-04 05:37:09,805 [Epoch: 051 Step: 00003400] Batch Recognition Loss:   0.005110 => Gls Tokens per Sec:     1858 || Batch Translation Loss:   4.113646 => Txt Tokens per Sec:     5252 || Lr: 0.000100
2024-02-04 05:37:11,148 Epoch  51: Total Training Recognition Loss 0.38  Total Training Translation Loss 236.06 
2024-02-04 05:37:11,148 EPOCH 52
2024-02-04 05:37:16,523 Epoch  52: Total Training Recognition Loss 0.36  Total Training Translation Loss 215.15 
2024-02-04 05:37:16,524 EPOCH 53
2024-02-04 05:37:17,813 [Epoch: 053 Step: 00003500] Batch Recognition Loss:   0.004698 => Gls Tokens per Sec:     1988 || Batch Translation Loss:   2.442250 => Txt Tokens per Sec:     5690 || Lr: 0.000100
2024-02-04 05:37:21,747 Epoch  53: Total Training Recognition Loss 0.31  Total Training Translation Loss 199.20 
2024-02-04 05:37:21,747 EPOCH 54
2024-02-04 05:37:25,239 [Epoch: 054 Step: 00003600] Batch Recognition Loss:   0.005418 => Gls Tokens per Sec:     2247 || Batch Translation Loss:   2.762872 => Txt Tokens per Sec:     6156 || Lr: 0.000100
2024-02-04 05:37:26,825 Epoch  54: Total Training Recognition Loss 0.31  Total Training Translation Loss 181.46 
2024-02-04 05:37:26,825 EPOCH 55
2024-02-04 05:37:32,119 Epoch  55: Total Training Recognition Loss 0.30  Total Training Translation Loss 169.90 
2024-02-04 05:37:32,119 EPOCH 56
2024-02-04 05:37:33,617 [Epoch: 056 Step: 00003700] Batch Recognition Loss:   0.002555 => Gls Tokens per Sec:     1544 || Batch Translation Loss:   1.862972 => Txt Tokens per Sec:     4594 || Lr: 0.000100
2024-02-04 05:37:37,697 Epoch  56: Total Training Recognition Loss 0.28  Total Training Translation Loss 160.03 
2024-02-04 05:37:37,697 EPOCH 57
2024-02-04 05:37:41,671 [Epoch: 057 Step: 00003800] Batch Recognition Loss:   0.003980 => Gls Tokens per Sec:     1934 || Batch Translation Loss:   3.076225 => Txt Tokens per Sec:     5370 || Lr: 0.000100
2024-02-04 05:37:43,144 Epoch  57: Total Training Recognition Loss 0.26  Total Training Translation Loss 150.00 
2024-02-04 05:37:43,144 EPOCH 58
2024-02-04 05:37:48,374 Epoch  58: Total Training Recognition Loss 0.24  Total Training Translation Loss 138.02 
2024-02-04 05:37:48,375 EPOCH 59
2024-02-04 05:37:49,471 [Epoch: 059 Step: 00003900] Batch Recognition Loss:   0.004688 => Gls Tokens per Sec:     2047 || Batch Translation Loss:   2.654354 => Txt Tokens per Sec:     5615 || Lr: 0.000100
2024-02-04 05:37:53,790 Epoch  59: Total Training Recognition Loss 0.23  Total Training Translation Loss 132.18 
2024-02-04 05:37:53,791 EPOCH 60
2024-02-04 05:37:57,347 [Epoch: 060 Step: 00004000] Batch Recognition Loss:   0.002119 => Gls Tokens per Sec:     2090 || Batch Translation Loss:   1.915775 => Txt Tokens per Sec:     5791 || Lr: 0.000100
2024-02-04 05:38:05,653 Validation result at epoch  60, step     4000: duration: 8.3056s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.01232	Translation Loss: 73582.21094	PPL: 1577.15149
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.58	(BLEU-1: 12.03,	BLEU-2: 3.79,	BLEU-3: 1.37,	BLEU-4: 0.58)
	CHRF 16.80	ROUGE 10.37
2024-02-04 05:38:05,654 Logging Recognition and Translation Outputs
2024-02-04 05:38:05,654 ========================================================================================================================
2024-02-04 05:38:05,654 Logging Sequence: 133_173.00
2024-02-04 05:38:05,654 	Gloss Reference :	A B+C+D+E
2024-02-04 05:38:05,654 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 05:38:05,654 	Gloss Alignment :	         
2024-02-04 05:38:05,655 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 05:38:05,655 	Text Reference  :	according to sources the leaders of the two countries are set to join the commentary panel   as  well   
2024-02-04 05:38:05,655 	Text Hypothesis :	********* ** ******* *** ******* ** *** *** ********* *** *** ** **** pm  modi       tweeted his arrival
2024-02-04 05:38:05,655 	Text Alignment  :	D         D  D       D   D       D  D   D   D         D   D   D  D    S   S          S       S   S      
2024-02-04 05:38:05,656 ========================================================================================================================
2024-02-04 05:38:05,656 Logging Sequence: 83_33.00
2024-02-04 05:38:05,656 	Gloss Reference :	A B+C+D+E
2024-02-04 05:38:05,656 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 05:38:05,656 	Gloss Alignment :	         
2024-02-04 05:38:05,656 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 05:38:05,657 	Text Reference  :	*** ****** ******* a   football **** ** ***** match lasts for two   equal halves of  45      minutes
2024-02-04 05:38:05,658 	Text Hypothesis :	the camera records the football team is awake and   he    is  awake and   at     the denmark team   
2024-02-04 05:38:05,658 	Text Alignment  :	I   I      I       S            I    I  I     S     S     S   S     S     S      S   S       S      
2024-02-04 05:38:05,658 ========================================================================================================================
2024-02-04 05:38:05,658 Logging Sequence: 68_147.00
2024-02-04 05:38:05,658 	Gloss Reference :	A B+C+D+E
2024-02-04 05:38:05,658 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 05:38:05,658 	Gloss Alignment :	         
2024-02-04 05:38:05,658 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 05:38:05,660 	Text Reference  :	remember the    2007 t20 world cup  amid   a     lot  of     sledging by   english players
2024-02-04 05:38:05,660 	Text Hypothesis :	bumrah   scored runs in  an    over stuart broad away stuart broad    gave away    over   
2024-02-04 05:38:05,660 	Text Alignment  :	S        S      S    S   S     S    S      S     S    S      S        S    S       S      
2024-02-04 05:38:05,660 ========================================================================================================================
2024-02-04 05:38:05,660 Logging Sequence: 165_8.00
2024-02-04 05:38:05,660 	Gloss Reference :	A B+C+D+E
2024-02-04 05:38:05,660 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 05:38:05,660 	Gloss Alignment :	         
2024-02-04 05:38:05,661 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 05:38:05,661 	Text Reference  :	however many don't believe in it  it   varies among   people
2024-02-04 05:38:05,661 	Text Hypothesis :	******* **** ***** ******* he has been an     amazing player
2024-02-04 05:38:05,661 	Text Alignment  :	D       D    D     D       S  S   S    S      S       S     
2024-02-04 05:38:05,661 ========================================================================================================================
2024-02-04 05:38:05,662 Logging Sequence: 119_71.00
2024-02-04 05:38:05,662 	Gloss Reference :	A B+C+D+E
2024-02-04 05:38:05,662 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 05:38:05,662 	Gloss Alignment :	         
2024-02-04 05:38:05,662 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 05:38:05,663 	Text Reference  :	the special gold devices have each player' names and    jersey numbers next to   the camera
2024-02-04 05:38:05,663 	Text Hypothesis :	*** he      is   also    a    gift that    the   team's phones to      be   held in  qartar
2024-02-04 05:38:05,664 	Text Alignment  :	D   S       S    S       S    S    S       S     S      S      S       S    S    S   S     
2024-02-04 05:38:05,664 ========================================================================================================================
2024-02-04 05:38:07,459 Epoch  60: Total Training Recognition Loss 0.23  Total Training Translation Loss 127.12 
2024-02-04 05:38:07,460 EPOCH 61
2024-02-04 05:38:13,080 Epoch  61: Total Training Recognition Loss 0.24  Total Training Translation Loss 116.66 
2024-02-04 05:38:13,080 EPOCH 62
2024-02-04 05:38:14,016 [Epoch: 062 Step: 00004100] Batch Recognition Loss:   0.002819 => Gls Tokens per Sec:     2225 || Batch Translation Loss:   2.198868 => Txt Tokens per Sec:     6079 || Lr: 0.000100
2024-02-04 05:38:18,284 Epoch  62: Total Training Recognition Loss 0.22  Total Training Translation Loss 111.44 
2024-02-04 05:38:18,285 EPOCH 63
2024-02-04 05:38:21,640 [Epoch: 063 Step: 00004200] Batch Recognition Loss:   0.002143 => Gls Tokens per Sec:     2195 || Batch Translation Loss:   1.204916 => Txt Tokens per Sec:     5997 || Lr: 0.000100
2024-02-04 05:38:23,467 Epoch  63: Total Training Recognition Loss 0.20  Total Training Translation Loss 103.00 
2024-02-04 05:38:23,467 EPOCH 64
2024-02-04 05:38:28,905 Epoch  64: Total Training Recognition Loss 0.19  Total Training Translation Loss 96.59 
2024-02-04 05:38:28,906 EPOCH 65
2024-02-04 05:38:29,616 [Epoch: 065 Step: 00004300] Batch Recognition Loss:   0.002497 => Gls Tokens per Sec:     2708 || Batch Translation Loss:   1.050015 => Txt Tokens per Sec:     6728 || Lr: 0.000100
2024-02-04 05:38:34,327 Epoch  65: Total Training Recognition Loss 0.19  Total Training Translation Loss 99.90 
2024-02-04 05:38:34,327 EPOCH 66
2024-02-04 05:38:38,173 [Epoch: 066 Step: 00004400] Batch Recognition Loss:   0.002459 => Gls Tokens per Sec:     1850 || Batch Translation Loss:   1.550283 => Txt Tokens per Sec:     5264 || Lr: 0.000100
2024-02-04 05:38:39,627 Epoch  66: Total Training Recognition Loss 0.19  Total Training Translation Loss 94.98 
2024-02-04 05:38:39,627 EPOCH 67
2024-02-04 05:38:45,105 Epoch  67: Total Training Recognition Loss 0.17  Total Training Translation Loss 88.63 
2024-02-04 05:38:45,106 EPOCH 68
2024-02-04 05:38:45,888 [Epoch: 068 Step: 00004500] Batch Recognition Loss:   0.003682 => Gls Tokens per Sec:     2138 || Batch Translation Loss:   1.079048 => Txt Tokens per Sec:     5841 || Lr: 0.000100
2024-02-04 05:38:50,258 Epoch  68: Total Training Recognition Loss 0.19  Total Training Translation Loss 90.96 
2024-02-04 05:38:50,259 EPOCH 69
2024-02-04 05:38:53,861 [Epoch: 069 Step: 00004600] Batch Recognition Loss:   0.001711 => Gls Tokens per Sec:     1930 || Batch Translation Loss:   1.015203 => Txt Tokens per Sec:     5307 || Lr: 0.000100
2024-02-04 05:38:55,901 Epoch  69: Total Training Recognition Loss 0.17  Total Training Translation Loss 81.50 
2024-02-04 05:38:55,901 EPOCH 70
2024-02-04 05:39:01,274 Epoch  70: Total Training Recognition Loss 0.16  Total Training Translation Loss 79.16 
2024-02-04 05:39:01,275 EPOCH 71
2024-02-04 05:39:02,054 [Epoch: 071 Step: 00004700] Batch Recognition Loss:   0.002125 => Gls Tokens per Sec:     2057 || Batch Translation Loss:   1.198389 => Txt Tokens per Sec:     5192 || Lr: 0.000100
2024-02-04 05:39:06,310 Epoch  71: Total Training Recognition Loss 0.17  Total Training Translation Loss 73.42 
2024-02-04 05:39:06,311 EPOCH 72
2024-02-04 05:39:10,207 [Epoch: 072 Step: 00004800] Batch Recognition Loss:   0.003334 => Gls Tokens per Sec:     1743 || Batch Translation Loss:   1.623815 => Txt Tokens per Sec:     4957 || Lr: 0.000100
2024-02-04 05:39:11,954 Epoch  72: Total Training Recognition Loss 0.15  Total Training Translation Loss 70.74 
2024-02-04 05:39:11,955 EPOCH 73
2024-02-04 05:39:17,298 Epoch  73: Total Training Recognition Loss 0.15  Total Training Translation Loss 62.45 
2024-02-04 05:39:17,298 EPOCH 74
2024-02-04 05:39:18,165 [Epoch: 074 Step: 00004900] Batch Recognition Loss:   0.003875 => Gls Tokens per Sec:     1664 || Batch Translation Loss:   1.419467 => Txt Tokens per Sec:     4892 || Lr: 0.000100
2024-02-04 05:39:22,585 Epoch  74: Total Training Recognition Loss 0.13  Total Training Translation Loss 59.04 
2024-02-04 05:39:22,586 EPOCH 75
2024-02-04 05:39:25,995 [Epoch: 075 Step: 00005000] Batch Recognition Loss:   0.001504 => Gls Tokens per Sec:     1971 || Batch Translation Loss:   0.820179 => Txt Tokens per Sec:     5346 || Lr: 0.000100
2024-02-04 05:39:28,146 Epoch  75: Total Training Recognition Loss 0.12  Total Training Translation Loss 59.29 
2024-02-04 05:39:28,146 EPOCH 76
2024-02-04 05:39:32,873 Epoch  76: Total Training Recognition Loss 0.12  Total Training Translation Loss 57.58 
2024-02-04 05:39:32,874 EPOCH 77
2024-02-04 05:39:33,565 [Epoch: 077 Step: 00005100] Batch Recognition Loss:   0.001947 => Gls Tokens per Sec:     1722 || Batch Translation Loss:   1.346205 => Txt Tokens per Sec:     4939 || Lr: 0.000100
2024-02-04 05:39:37,664 Epoch  77: Total Training Recognition Loss 0.13  Total Training Translation Loss 62.44 
2024-02-04 05:39:37,665 EPOCH 78
2024-02-04 05:39:41,026 [Epoch: 078 Step: 00005200] Batch Recognition Loss:   0.000690 => Gls Tokens per Sec:     1926 || Batch Translation Loss:   0.897808 => Txt Tokens per Sec:     5162 || Lr: 0.000100
2024-02-04 05:39:43,237 Epoch  78: Total Training Recognition Loss 0.10  Total Training Translation Loss 60.61 
2024-02-04 05:39:43,237 EPOCH 79
2024-02-04 05:39:48,469 Epoch  79: Total Training Recognition Loss 0.12  Total Training Translation Loss 59.37 
2024-02-04 05:39:48,469 EPOCH 80
2024-02-04 05:39:49,052 [Epoch: 080 Step: 00005300] Batch Recognition Loss:   0.001717 => Gls Tokens per Sec:     1927 || Batch Translation Loss:   1.323673 => Txt Tokens per Sec:     5232 || Lr: 0.000100
2024-02-04 05:39:53,734 Epoch  80: Total Training Recognition Loss 0.12  Total Training Translation Loss 57.59 
2024-02-04 05:39:53,735 EPOCH 81
2024-02-04 05:39:56,884 [Epoch: 081 Step: 00005400] Batch Recognition Loss:   0.001578 => Gls Tokens per Sec:     2033 || Batch Translation Loss:   0.775564 => Txt Tokens per Sec:     5645 || Lr: 0.000100
2024-02-04 05:39:59,096 Epoch  81: Total Training Recognition Loss 0.11  Total Training Translation Loss 52.75 
2024-02-04 05:39:59,096 EPOCH 82
2024-02-04 05:40:04,704 Epoch  82: Total Training Recognition Loss 0.11  Total Training Translation Loss 56.07 
2024-02-04 05:40:04,705 EPOCH 83
2024-02-04 05:40:05,191 [Epoch: 083 Step: 00005500] Batch Recognition Loss:   0.001965 => Gls Tokens per Sec:     1979 || Batch Translation Loss:   0.897299 => Txt Tokens per Sec:     5769 || Lr: 0.000100
2024-02-04 05:40:10,061 Epoch  83: Total Training Recognition Loss 0.10  Total Training Translation Loss 52.57 
2024-02-04 05:40:10,061 EPOCH 84
2024-02-04 05:40:13,460 [Epoch: 084 Step: 00005600] Batch Recognition Loss:   0.001008 => Gls Tokens per Sec:     1810 || Batch Translation Loss:   0.524964 => Txt Tokens per Sec:     5264 || Lr: 0.000100
2024-02-04 05:40:15,490 Epoch  84: Total Training Recognition Loss 0.10  Total Training Translation Loss 49.95 
2024-02-04 05:40:15,490 EPOCH 85
2024-02-04 05:40:20,342 Epoch  85: Total Training Recognition Loss 0.12  Total Training Translation Loss 51.18 
2024-02-04 05:40:20,342 EPOCH 86
2024-02-04 05:40:20,636 [Epoch: 086 Step: 00005700] Batch Recognition Loss:   0.001303 => Gls Tokens per Sec:     2730 || Batch Translation Loss:   0.877711 => Txt Tokens per Sec:     7386 || Lr: 0.000100
2024-02-04 05:40:25,898 Epoch  86: Total Training Recognition Loss 0.09  Total Training Translation Loss 48.35 
2024-02-04 05:40:25,899 EPOCH 87
2024-02-04 05:40:28,646 [Epoch: 087 Step: 00005800] Batch Recognition Loss:   0.002063 => Gls Tokens per Sec:     2182 || Batch Translation Loss:   0.497431 => Txt Tokens per Sec:     6040 || Lr: 0.000100
2024-02-04 05:40:31,177 Epoch  87: Total Training Recognition Loss 0.10  Total Training Translation Loss 41.43 
2024-02-04 05:40:31,178 EPOCH 88
2024-02-04 05:40:36,397 Epoch  88: Total Training Recognition Loss 0.09  Total Training Translation Loss 42.75 
2024-02-04 05:40:36,397 EPOCH 89
2024-02-04 05:40:36,650 [Epoch: 089 Step: 00005900] Batch Recognition Loss:   0.002038 => Gls Tokens per Sec:     2540 || Batch Translation Loss:   0.370352 => Txt Tokens per Sec:     7040 || Lr: 0.000100
2024-02-04 05:40:41,635 Epoch  89: Total Training Recognition Loss 0.09  Total Training Translation Loss 40.96 
2024-02-04 05:40:41,635 EPOCH 90
2024-02-04 05:40:44,363 [Epoch: 090 Step: 00006000] Batch Recognition Loss:   0.001244 => Gls Tokens per Sec:     2138 || Batch Translation Loss:   0.504546 => Txt Tokens per Sec:     5838 || Lr: 0.000100
2024-02-04 05:40:52,648 Validation result at epoch  90, step     6000: duration: 8.2847s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00354	Translation Loss: 81867.34375	PPL: 3613.63525
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.58	(BLEU-1: 10.41,	BLEU-2: 3.42,	BLEU-3: 1.25,	BLEU-4: 0.58)
	CHRF 16.42	ROUGE 9.41
2024-02-04 05:40:52,648 Logging Recognition and Translation Outputs
2024-02-04 05:40:52,648 ========================================================================================================================
2024-02-04 05:40:52,649 Logging Sequence: 89_111.00
2024-02-04 05:40:52,649 	Gloss Reference :	A B+C+D+E
2024-02-04 05:40:52,649 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 05:40:52,649 	Gloss Alignment :	         
2024-02-04 05:40:52,649 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 05:40:52,650 	Text Reference  :	*** ******* ******** * however  selectors never    selected me   for the    team 
2024-02-04 05:40:52,650 	Text Hypothesis :	the spinner received a plethora of        messages and      went on  social media
2024-02-04 05:40:52,650 	Text Alignment  :	I   I       I        I S        S         S        S        S    S   S      S    
2024-02-04 05:40:52,650 ========================================================================================================================
2024-02-04 05:40:52,650 Logging Sequence: 137_23.00
2024-02-04 05:40:52,651 	Gloss Reference :	A B+C+D+E
2024-02-04 05:40:52,651 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 05:40:52,651 	Gloss Alignment :	         
2024-02-04 05:40:52,651 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 05:40:52,652 	Text Reference  :	fan from around  the ***** world are in      qatar for the **** **** fifa    world cup 
2024-02-04 05:40:52,652 	Text Hypothesis :	*** **** however the match could not allowed to    all the fans were shocked by    this
2024-02-04 05:40:52,652 	Text Alignment  :	D   D    S           I     S     S   S       S     S       I    I    S       S     S   
2024-02-04 05:40:52,652 ========================================================================================================================
2024-02-04 05:40:52,652 Logging Sequence: 128_145.00
2024-02-04 05:40:52,653 	Gloss Reference :	A B+C+D+E
2024-02-04 05:40:52,653 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 05:40:52,653 	Gloss Alignment :	         
2024-02-04 05:40:52,653 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 05:40:52,654 	Text Reference  :	**** ** *** ** icc also uploaded a   video of   the same
2024-02-04 05:40:52,654 	Text Hypothesis :	this is why he did not  get      her and   then hit him 
2024-02-04 05:40:52,654 	Text Alignment  :	I    I  I   I  S   S    S        S   S     S    S   S   
2024-02-04 05:40:52,654 ========================================================================================================================
2024-02-04 05:40:52,654 Logging Sequence: 165_192.00
2024-02-04 05:40:52,654 	Gloss Reference :	A B+C+D+E
2024-02-04 05:40:52,654 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 05:40:52,655 	Gloss Alignment :	         
2024-02-04 05:40:52,655 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 05:40:52,655 	Text Reference  :	******* 3  ravichandran ashwin believes  that his bag ** *** is     lucky   
2024-02-04 05:40:52,656 	Text Hypothesis :	however he had          been   postponed and  his bag to the indian athletes
2024-02-04 05:40:52,656 	Text Alignment  :	I       S  S            S      S         S            I  I   S      S       
2024-02-04 05:40:52,656 ========================================================================================================================
2024-02-04 05:40:52,656 Logging Sequence: 180_494.00
2024-02-04 05:40:52,656 	Gloss Reference :	A B+C+D+E
2024-02-04 05:40:52,656 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 05:40:52,656 	Gloss Alignment :	         
2024-02-04 05:40:52,656 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 05:40:52,658 	Text Reference  :	the women wrestlers spoke angrily against the police and     the  controversy in  front  of       the    media 
2024-02-04 05:40:52,658 	Text Hypothesis :	*** ***** ********* ***** ******* however a   police officer said that        the sports minister anurag thakur
2024-02-04 05:40:52,658 	Text Alignment  :	D   D     D         D     D       S       S          S       S    S           S   S      S        S      S     
2024-02-04 05:40:52,658 ========================================================================================================================
2024-02-04 05:40:55,253 Epoch  90: Total Training Recognition Loss 0.07  Total Training Translation Loss 38.22 
2024-02-04 05:40:55,253 EPOCH 91
2024-02-04 05:41:01,082 Epoch  91: Total Training Recognition Loss 0.08  Total Training Translation Loss 39.62 
2024-02-04 05:41:01,083 EPOCH 92
2024-02-04 05:41:01,326 [Epoch: 092 Step: 00006100] Batch Recognition Loss:   0.001672 => Gls Tokens per Sec:     1983 || Batch Translation Loss:   0.499406 => Txt Tokens per Sec:     5845 || Lr: 0.000100
2024-02-04 05:41:06,767 Epoch  92: Total Training Recognition Loss 0.09  Total Training Translation Loss 39.94 
2024-02-04 05:41:06,767 EPOCH 93
2024-02-04 05:41:09,796 [Epoch: 093 Step: 00006200] Batch Recognition Loss:   0.000956 => Gls Tokens per Sec:     1903 || Batch Translation Loss:   0.529095 => Txt Tokens per Sec:     5366 || Lr: 0.000100
2024-02-04 05:41:12,278 Epoch  93: Total Training Recognition Loss 0.08  Total Training Translation Loss 38.56 
2024-02-04 05:41:12,278 EPOCH 94
2024-02-04 05:41:17,878 Epoch  94: Total Training Recognition Loss 0.08  Total Training Translation Loss 34.29 
2024-02-04 05:41:17,878 EPOCH 95
2024-02-04 05:41:17,986 [Epoch: 095 Step: 00006300] Batch Recognition Loss:   0.000863 => Gls Tokens per Sec:     3065 || Batch Translation Loss:   0.306947 => Txt Tokens per Sec:     6600 || Lr: 0.000100
2024-02-04 05:41:23,377 Epoch  95: Total Training Recognition Loss 0.09  Total Training Translation Loss 32.40 
2024-02-04 05:41:23,377 EPOCH 96
2024-02-04 05:41:26,246 [Epoch: 096 Step: 00006400] Batch Recognition Loss:   0.000858 => Gls Tokens per Sec:     1921 || Batch Translation Loss:   0.403916 => Txt Tokens per Sec:     5437 || Lr: 0.000100
2024-02-04 05:41:28,604 Epoch  96: Total Training Recognition Loss 0.07  Total Training Translation Loss 33.56 
2024-02-04 05:41:28,605 EPOCH 97
2024-02-04 05:41:34,167 Epoch  97: Total Training Recognition Loss 0.07  Total Training Translation Loss 33.78 
2024-02-04 05:41:34,167 EPOCH 98
2024-02-04 05:41:34,236 [Epoch: 098 Step: 00006500] Batch Recognition Loss:   0.000800 => Gls Tokens per Sec:     2353 || Batch Translation Loss:   0.324441 => Txt Tokens per Sec:     5191 || Lr: 0.000100
2024-02-04 05:41:39,156 Epoch  98: Total Training Recognition Loss 0.07  Total Training Translation Loss 36.45 
2024-02-04 05:41:39,156 EPOCH 99
2024-02-04 05:41:42,075 [Epoch: 099 Step: 00006600] Batch Recognition Loss:   0.000654 => Gls Tokens per Sec:     1865 || Batch Translation Loss:   1.096990 => Txt Tokens per Sec:     5101 || Lr: 0.000100
2024-02-04 05:41:44,688 Epoch  99: Total Training Recognition Loss 0.07  Total Training Translation Loss 35.87 
2024-02-04 05:41:44,688 EPOCH 100
2024-02-04 05:41:49,943 [Epoch: 100 Step: 00006700] Batch Recognition Loss:   0.000787 => Gls Tokens per Sec:     2024 || Batch Translation Loss:   1.180939 => Txt Tokens per Sec:     5618 || Lr: 0.000100
2024-02-04 05:41:49,943 Epoch 100: Total Training Recognition Loss 0.08  Total Training Translation Loss 39.18 
2024-02-04 05:41:49,943 EPOCH 101
2024-02-04 05:41:55,202 Epoch 101: Total Training Recognition Loss 0.07  Total Training Translation Loss 38.09 
2024-02-04 05:41:55,202 EPOCH 102
2024-02-04 05:41:57,483 [Epoch: 102 Step: 00006800] Batch Recognition Loss:   0.000707 => Gls Tokens per Sec:     2316 || Batch Translation Loss:   0.590640 => Txt Tokens per Sec:     6422 || Lr: 0.000100
2024-02-04 05:42:00,436 Epoch 102: Total Training Recognition Loss 0.10  Total Training Translation Loss 33.65 
2024-02-04 05:42:00,437 EPOCH 103
2024-02-04 05:42:05,460 [Epoch: 103 Step: 00006900] Batch Recognition Loss:   0.001334 => Gls Tokens per Sec:     2085 || Batch Translation Loss:   0.377379 => Txt Tokens per Sec:     5797 || Lr: 0.000100
2024-02-04 05:42:05,516 Epoch 103: Total Training Recognition Loss 0.06  Total Training Translation Loss 29.61 
2024-02-04 05:42:05,517 EPOCH 104
2024-02-04 05:42:11,171 Epoch 104: Total Training Recognition Loss 0.06  Total Training Translation Loss 30.27 
2024-02-04 05:42:11,172 EPOCH 105
2024-02-04 05:42:13,627 [Epoch: 105 Step: 00007000] Batch Recognition Loss:   0.000755 => Gls Tokens per Sec:     2050 || Batch Translation Loss:   0.497617 => Txt Tokens per Sec:     5839 || Lr: 0.000100
2024-02-04 05:42:16,407 Epoch 105: Total Training Recognition Loss 0.06  Total Training Translation Loss 29.51 
2024-02-04 05:42:16,407 EPOCH 106
2024-02-04 05:42:21,504 [Epoch: 106 Step: 00007100] Batch Recognition Loss:   0.000598 => Gls Tokens per Sec:     2023 || Batch Translation Loss:   0.256547 => Txt Tokens per Sec:     5605 || Lr: 0.000100
2024-02-04 05:42:21,669 Epoch 106: Total Training Recognition Loss 0.07  Total Training Translation Loss 26.92 
2024-02-04 05:42:21,669 EPOCH 107
2024-02-04 05:42:27,230 Epoch 107: Total Training Recognition Loss 0.05  Total Training Translation Loss 25.87 
2024-02-04 05:42:27,231 EPOCH 108
2024-02-04 05:42:29,298 [Epoch: 108 Step: 00007200] Batch Recognition Loss:   0.000730 => Gls Tokens per Sec:     2399 || Batch Translation Loss:   0.416116 => Txt Tokens per Sec:     6361 || Lr: 0.000100
2024-02-04 05:42:32,189 Epoch 108: Total Training Recognition Loss 0.06  Total Training Translation Loss 26.87 
2024-02-04 05:42:32,189 EPOCH 109
2024-02-04 05:42:37,570 [Epoch: 109 Step: 00007300] Batch Recognition Loss:   0.001418 => Gls Tokens per Sec:     1887 || Batch Translation Loss:   0.215946 => Txt Tokens per Sec:     5264 || Lr: 0.000100
2024-02-04 05:42:37,777 Epoch 109: Total Training Recognition Loss 0.06  Total Training Translation Loss 26.74 
2024-02-04 05:42:37,777 EPOCH 110
2024-02-04 05:42:42,896 Epoch 110: Total Training Recognition Loss 0.06  Total Training Translation Loss 30.31 
2024-02-04 05:42:42,896 EPOCH 111
2024-02-04 05:42:45,494 [Epoch: 111 Step: 00007400] Batch Recognition Loss:   0.000406 => Gls Tokens per Sec:     1814 || Batch Translation Loss:   0.278599 => Txt Tokens per Sec:     5265 || Lr: 0.000100
2024-02-04 05:42:48,211 Epoch 111: Total Training Recognition Loss 0.06  Total Training Translation Loss 28.25 
2024-02-04 05:42:48,211 EPOCH 112
2024-02-04 05:42:53,465 [Epoch: 112 Step: 00007500] Batch Recognition Loss:   0.000823 => Gls Tokens per Sec:     1902 || Batch Translation Loss:   0.874455 => Txt Tokens per Sec:     5352 || Lr: 0.000100
2024-02-04 05:42:53,710 Epoch 112: Total Training Recognition Loss 0.06  Total Training Translation Loss 27.22 
2024-02-04 05:42:53,710 EPOCH 113
2024-02-04 05:42:59,449 Epoch 113: Total Training Recognition Loss 0.05  Total Training Translation Loss 26.02 
2024-02-04 05:42:59,449 EPOCH 114
2024-02-04 05:43:01,873 [Epoch: 114 Step: 00007600] Batch Recognition Loss:   0.000614 => Gls Tokens per Sec:     1915 || Batch Translation Loss:   0.653700 => Txt Tokens per Sec:     5208 || Lr: 0.000100
2024-02-04 05:43:05,075 Epoch 114: Total Training Recognition Loss 0.05  Total Training Translation Loss 29.35 
2024-02-04 05:43:05,076 EPOCH 115
2024-02-04 05:43:09,978 [Epoch: 115 Step: 00007700] Batch Recognition Loss:   0.000518 => Gls Tokens per Sec:     2006 || Batch Translation Loss:   0.523230 => Txt Tokens per Sec:     5524 || Lr: 0.000100
2024-02-04 05:43:10,472 Epoch 115: Total Training Recognition Loss 0.05  Total Training Translation Loss 28.70 
2024-02-04 05:43:10,473 EPOCH 116
2024-02-04 05:43:15,971 Epoch 116: Total Training Recognition Loss 0.06  Total Training Translation Loss 26.34 
2024-02-04 05:43:15,972 EPOCH 117
2024-02-04 05:43:18,004 [Epoch: 117 Step: 00007800] Batch Recognition Loss:   0.000733 => Gls Tokens per Sec:     2162 || Batch Translation Loss:   0.306692 => Txt Tokens per Sec:     6050 || Lr: 0.000100
2024-02-04 05:43:21,117 Epoch 117: Total Training Recognition Loss 0.05  Total Training Translation Loss 21.69 
2024-02-04 05:43:21,118 EPOCH 118
2024-02-04 05:43:26,089 [Epoch: 118 Step: 00007900] Batch Recognition Loss:   0.000277 => Gls Tokens per Sec:     1945 || Batch Translation Loss:   0.245410 => Txt Tokens per Sec:     5403 || Lr: 0.000100
2024-02-04 05:43:26,545 Epoch 118: Total Training Recognition Loss 0.04  Total Training Translation Loss 19.48 
2024-02-04 05:43:26,545 EPOCH 119
2024-02-04 05:43:31,880 Epoch 119: Total Training Recognition Loss 0.05  Total Training Translation Loss 20.73 
2024-02-04 05:43:31,880 EPOCH 120
2024-02-04 05:43:33,574 [Epoch: 120 Step: 00008000] Batch Recognition Loss:   0.000355 => Gls Tokens per Sec:     2498 || Batch Translation Loss:   0.282195 => Txt Tokens per Sec:     6445 || Lr: 0.000100
2024-02-04 05:43:41,825 Validation result at epoch 120, step     8000: duration: 8.2488s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00203	Translation Loss: 88007.92969	PPL: 6680.56738
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.36	(BLEU-1: 11.28,	BLEU-2: 3.12,	BLEU-3: 1.04,	BLEU-4: 0.36)
	CHRF 16.80	ROUGE 9.61
2024-02-04 05:43:41,826 Logging Recognition and Translation Outputs
2024-02-04 05:43:41,826 ========================================================================================================================
2024-02-04 05:43:41,826 Logging Sequence: 88_57.00
2024-02-04 05:43:41,826 	Gloss Reference :	A B+C+D+E
2024-02-04 05:43:41,826 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 05:43:41,826 	Gloss Alignment :	         
2024-02-04 05:43:41,826 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 05:43:41,828 	Text Reference  :	which stated messi  we're waiting    for you    to      come  here   you      will      be     finished    when you   come   
2024-02-04 05:43:41,828 	Text Hypothesis :	***** people failed to    understand the reason however mayor javkin attacked argentina police authorities over their failure
2024-02-04 05:43:41,828 	Text Alignment  :	D     S      S      S     S          S   S      S       S     S      S        S         S      S           S    S     S      
2024-02-04 05:43:41,828 ========================================================================================================================
2024-02-04 05:43:41,829 Logging Sequence: 171_142.00
2024-02-04 05:43:41,829 	Gloss Reference :	A B+C+D+E
2024-02-04 05:43:41,829 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 05:43:41,829 	Gloss Alignment :	         
2024-02-04 05:43:41,829 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 05:43:41,830 	Text Reference  :	******* this decision  on      dhoni    made a ***** **** significant impact as   pathirana claimed  two  tough wickets
2024-02-04 05:43:41,831 	Text Hypothesis :	however the  wrestlers started speaking with a great time that        he     will be        refunded with 5     years  
2024-02-04 05:43:41,831 	Text Alignment  :	I       S    S         S       S        S      I     I    S           S      S    S         S        S    S     S      
2024-02-04 05:43:41,831 ========================================================================================================================
2024-02-04 05:43:41,831 Logging Sequence: 125_207.00
2024-02-04 05:43:41,831 	Gloss Reference :	A B+C+D+E
2024-02-04 05:43:41,831 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 05:43:41,831 	Gloss Alignment :	         
2024-02-04 05:43:41,832 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 05:43:41,833 	Text Reference  :	he had not practised since       he   returned and      he      had also      fallen sick 
2024-02-04 05:43:41,833 	Text Hypothesis :	he was not ********* comfortable with the      comments section is  currently in     india
2024-02-04 05:43:41,833 	Text Alignment  :	   S       D         S           S    S        S        S       S   S         S      S    
2024-02-04 05:43:41,833 ========================================================================================================================
2024-02-04 05:43:41,833 Logging Sequence: 68_230.00
2024-02-04 05:43:41,833 	Gloss Reference :	A B+C+D+E
2024-02-04 05:43:41,833 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 05:43:41,833 	Gloss Alignment :	         
2024-02-04 05:43:41,834 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 05:43:41,834 	Text Reference  :	let us know what you  think  in   the ****** comments below 
2024-02-04 05:43:41,834 	Text Hypothesis :	*** ** **** but  some walked with the bowler deepak   chahar
2024-02-04 05:43:41,834 	Text Alignment  :	D   D  D    S    S    S      S        I      S        S     
2024-02-04 05:43:41,834 ========================================================================================================================
2024-02-04 05:43:41,835 Logging Sequence: 126_82.00
2024-02-04 05:43:41,835 	Gloss Reference :	A B+C+D+E
2024-02-04 05:43:41,835 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 05:43:41,835 	Gloss Alignment :	         
2024-02-04 05:43:41,835 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 05:43:41,837 	Text Reference  :	** ******* neeraj also    dedicated his gold medal to  former indian olympians who came close to  winning medals
2024-02-04 05:43:41,837 	Text Hypothesis :	he stepped away   because he        did not  stay  and then   the    players   who **** is    the second  team  
2024-02-04 05:43:41,837 	Text Alignment  :	I  I       S      S       S         S   S    S     S   S      S      S             D    S     S   S       S     
2024-02-04 05:43:41,837 ========================================================================================================================
2024-02-04 05:43:45,309 Epoch 120: Total Training Recognition Loss 0.04  Total Training Translation Loss 21.96 
2024-02-04 05:43:45,309 EPOCH 121
2024-02-04 05:43:50,226 [Epoch: 121 Step: 00008100] Batch Recognition Loss:   0.000481 => Gls Tokens per Sec:     1935 || Batch Translation Loss:   0.474048 => Txt Tokens per Sec:     5267 || Lr: 0.000100
2024-02-04 05:43:50,912 Epoch 121: Total Training Recognition Loss 0.06  Total Training Translation Loss 22.82 
2024-02-04 05:43:50,912 EPOCH 122
2024-02-04 05:43:56,097 Epoch 122: Total Training Recognition Loss 0.07  Total Training Translation Loss 24.01 
2024-02-04 05:43:56,098 EPOCH 123
2024-02-04 05:43:58,238 [Epoch: 123 Step: 00008200] Batch Recognition Loss:   0.000413 => Gls Tokens per Sec:     1945 || Batch Translation Loss:   0.221369 => Txt Tokens per Sec:     5524 || Lr: 0.000100
2024-02-04 05:44:01,401 Epoch 123: Total Training Recognition Loss 0.06  Total Training Translation Loss 25.13 
2024-02-04 05:44:01,402 EPOCH 124
2024-02-04 05:44:05,994 [Epoch: 124 Step: 00008300] Batch Recognition Loss:   0.001161 => Gls Tokens per Sec:     2036 || Batch Translation Loss:   0.203920 => Txt Tokens per Sec:     5750 || Lr: 0.000100
2024-02-04 05:44:06,529 Epoch 124: Total Training Recognition Loss 0.07  Total Training Translation Loss 23.50 
2024-02-04 05:44:06,529 EPOCH 125
2024-02-04 05:44:11,759 Epoch 125: Total Training Recognition Loss 0.05  Total Training Translation Loss 20.00 
2024-02-04 05:44:11,759 EPOCH 126
2024-02-04 05:44:14,029 [Epoch: 126 Step: 00008400] Batch Recognition Loss:   0.000844 => Gls Tokens per Sec:     1764 || Batch Translation Loss:   0.202149 => Txt Tokens per Sec:     5147 || Lr: 0.000100
2024-02-04 05:44:17,390 Epoch 126: Total Training Recognition Loss 0.05  Total Training Translation Loss 18.01 
2024-02-04 05:44:17,391 EPOCH 127
2024-02-04 05:44:21,904 [Epoch: 127 Step: 00008500] Batch Recognition Loss:   0.000646 => Gls Tokens per Sec:     2037 || Batch Translation Loss:   0.259363 => Txt Tokens per Sec:     5700 || Lr: 0.000100
2024-02-04 05:44:22,576 Epoch 127: Total Training Recognition Loss 0.05  Total Training Translation Loss 19.35 
2024-02-04 05:44:22,577 EPOCH 128
2024-02-04 05:44:27,893 Epoch 128: Total Training Recognition Loss 0.05  Total Training Translation Loss 20.43 
2024-02-04 05:44:27,893 EPOCH 129
2024-02-04 05:44:29,588 [Epoch: 129 Step: 00008600] Batch Recognition Loss:   0.000638 => Gls Tokens per Sec:     2267 || Batch Translation Loss:   0.297057 => Txt Tokens per Sec:     6340 || Lr: 0.000100
2024-02-04 05:44:33,216 Epoch 129: Total Training Recognition Loss 0.06  Total Training Translation Loss 21.91 
2024-02-04 05:44:33,217 EPOCH 130
2024-02-04 05:44:37,383 [Epoch: 130 Step: 00008700] Batch Recognition Loss:   0.000250 => Gls Tokens per Sec:     2168 || Batch Translation Loss:   0.245142 => Txt Tokens per Sec:     6035 || Lr: 0.000100
2024-02-04 05:44:38,344 Epoch 130: Total Training Recognition Loss 0.05  Total Training Translation Loss 21.15 
2024-02-04 05:44:38,345 EPOCH 131
2024-02-04 05:44:43,807 Epoch 131: Total Training Recognition Loss 0.05  Total Training Translation Loss 19.87 
2024-02-04 05:44:43,808 EPOCH 132
2024-02-04 05:44:45,486 [Epoch: 132 Step: 00008800] Batch Recognition Loss:   0.000691 => Gls Tokens per Sec:     2139 || Batch Translation Loss:   0.512159 => Txt Tokens per Sec:     5977 || Lr: 0.000100
2024-02-04 05:44:49,022 Epoch 132: Total Training Recognition Loss 0.06  Total Training Translation Loss 20.32 
2024-02-04 05:44:49,023 EPOCH 133
2024-02-04 05:44:53,291 [Epoch: 133 Step: 00008900] Batch Recognition Loss:   0.000502 => Gls Tokens per Sec:     2078 || Batch Translation Loss:   0.546167 => Txt Tokens per Sec:     5794 || Lr: 0.000100
2024-02-04 05:44:54,042 Epoch 133: Total Training Recognition Loss 0.04  Total Training Translation Loss 20.74 
2024-02-04 05:44:54,042 EPOCH 134
2024-02-04 05:44:59,681 Epoch 134: Total Training Recognition Loss 0.05  Total Training Translation Loss 22.42 
2024-02-04 05:44:59,682 EPOCH 135
2024-02-04 05:45:01,456 [Epoch: 135 Step: 00009000] Batch Recognition Loss:   0.000549 => Gls Tokens per Sec:     1933 || Batch Translation Loss:   0.293315 => Txt Tokens per Sec:     5453 || Lr: 0.000100
2024-02-04 05:45:05,047 Epoch 135: Total Training Recognition Loss 0.04  Total Training Translation Loss 20.56 
2024-02-04 05:45:05,048 EPOCH 136
2024-02-04 05:45:09,333 [Epoch: 136 Step: 00009100] Batch Recognition Loss:   0.000499 => Gls Tokens per Sec:     2033 || Batch Translation Loss:   0.350573 => Txt Tokens per Sec:     5536 || Lr: 0.000100
2024-02-04 05:45:10,498 Epoch 136: Total Training Recognition Loss 0.06  Total Training Translation Loss 24.33 
2024-02-04 05:45:10,498 EPOCH 137
2024-02-04 05:45:15,686 Epoch 137: Total Training Recognition Loss 0.04  Total Training Translation Loss 21.15 
2024-02-04 05:45:15,687 EPOCH 138
2024-02-04 05:45:17,418 [Epoch: 138 Step: 00009200] Batch Recognition Loss:   0.000596 => Gls Tokens per Sec:     1941 || Batch Translation Loss:   0.588839 => Txt Tokens per Sec:     5198 || Lr: 0.000100
2024-02-04 05:45:21,282 Epoch 138: Total Training Recognition Loss 0.06  Total Training Translation Loss 18.00 
2024-02-04 05:45:21,283 EPOCH 139
2024-02-04 05:45:25,683 [Epoch: 139 Step: 00009300] Batch Recognition Loss:   0.000281 => Gls Tokens per Sec:     1944 || Batch Translation Loss:   0.257559 => Txt Tokens per Sec:     5397 || Lr: 0.000100
2024-02-04 05:45:26,761 Epoch 139: Total Training Recognition Loss 0.04  Total Training Translation Loss 17.19 
2024-02-04 05:45:26,761 EPOCH 140
2024-02-04 05:45:32,297 Epoch 140: Total Training Recognition Loss 0.04  Total Training Translation Loss 18.48 
2024-02-04 05:45:32,297 EPOCH 141
2024-02-04 05:45:34,103 [Epoch: 141 Step: 00009400] Batch Recognition Loss:   0.000529 => Gls Tokens per Sec:     1773 || Batch Translation Loss:   0.240838 => Txt Tokens per Sec:     5230 || Lr: 0.000100
2024-02-04 05:45:37,772 Epoch 141: Total Training Recognition Loss 0.04  Total Training Translation Loss 17.53 
2024-02-04 05:45:37,773 EPOCH 142
2024-02-04 05:45:42,132 [Epoch: 142 Step: 00009500] Batch Recognition Loss:   0.000324 => Gls Tokens per Sec:     1925 || Batch Translation Loss:   0.281397 => Txt Tokens per Sec:     5396 || Lr: 0.000100
2024-02-04 05:45:43,250 Epoch 142: Total Training Recognition Loss 0.04  Total Training Translation Loss 18.91 
2024-02-04 05:45:43,250 EPOCH 143
2024-02-04 05:45:48,484 Epoch 143: Total Training Recognition Loss 0.04  Total Training Translation Loss 18.40 
2024-02-04 05:45:48,485 EPOCH 144
2024-02-04 05:45:50,137 [Epoch: 144 Step: 00009600] Batch Recognition Loss:   0.000534 => Gls Tokens per Sec:     1787 || Batch Translation Loss:   0.152016 => Txt Tokens per Sec:     5296 || Lr: 0.000100
2024-02-04 05:45:53,672 Epoch 144: Total Training Recognition Loss 0.04  Total Training Translation Loss 19.95 
2024-02-04 05:45:53,673 EPOCH 145
2024-02-04 05:45:58,078 [Epoch: 145 Step: 00009700] Batch Recognition Loss:   0.000450 => Gls Tokens per Sec:     1869 || Batch Translation Loss:   0.205833 => Txt Tokens per Sec:     5204 || Lr: 0.000100
2024-02-04 05:45:59,126 Epoch 145: Total Training Recognition Loss 0.05  Total Training Translation Loss 18.79 
2024-02-04 05:45:59,126 EPOCH 146
2024-02-04 05:46:04,609 Epoch 146: Total Training Recognition Loss 0.04  Total Training Translation Loss 16.34 
2024-02-04 05:46:04,610 EPOCH 147
2024-02-04 05:46:05,844 [Epoch: 147 Step: 00009800] Batch Recognition Loss:   0.000499 => Gls Tokens per Sec:     2337 || Batch Translation Loss:   0.278385 => Txt Tokens per Sec:     6264 || Lr: 0.000100
2024-02-04 05:46:09,373 Epoch 147: Total Training Recognition Loss 0.03  Total Training Translation Loss 15.48 
2024-02-04 05:46:09,373 EPOCH 148
2024-02-04 05:46:12,990 [Epoch: 148 Step: 00009900] Batch Recognition Loss:   0.000462 => Gls Tokens per Sec:     2257 || Batch Translation Loss:   0.227690 => Txt Tokens per Sec:     6171 || Lr: 0.000100
2024-02-04 05:46:14,566 Epoch 148: Total Training Recognition Loss 0.04  Total Training Translation Loss 15.21 
2024-02-04 05:46:14,567 EPOCH 149
2024-02-04 05:46:19,994 Epoch 149: Total Training Recognition Loss 0.04  Total Training Translation Loss 13.78 
2024-02-04 05:46:19,995 EPOCH 150
2024-02-04 05:46:21,505 [Epoch: 150 Step: 00010000] Batch Recognition Loss:   0.000185 => Gls Tokens per Sec:     1803 || Batch Translation Loss:   0.064468 => Txt Tokens per Sec:     4981 || Lr: 0.000100
2024-02-04 05:46:30,186 Hooray! New best validation result [eval_metric]!
2024-02-04 05:46:30,187 Saving new checkpoint.
2024-02-04 05:46:30,469 Validation result at epoch 150, step    10000: duration: 8.9640s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00184	Translation Loss: 89947.78906	PPL: 8111.83643
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 1.02	(BLEU-1: 11.89,	BLEU-2: 4.00,	BLEU-3: 1.85,	BLEU-4: 1.02)
	CHRF 17.47	ROUGE 10.05
2024-02-04 05:46:30,470 Logging Recognition and Translation Outputs
2024-02-04 05:46:30,470 ========================================================================================================================
2024-02-04 05:46:30,471 Logging Sequence: 159_139.00
2024-02-04 05:46:30,471 	Gloss Reference :	A B+C+D+E
2024-02-04 05:46:30,471 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 05:46:30,471 	Gloss Alignment :	         
2024-02-04 05:46:30,471 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 05:46:30,472 	Text Reference  :	he took time    and  finally was ready for the asia cup where he       scored the century
2024-02-04 05:46:30,472 	Text Hypothesis :	** **** earlier when he      was ***** *** *** **** *** ***** stepping down   as  well   
2024-02-04 05:46:30,473 	Text Alignment  :	D  D    S       S    S           D     D   D   D    D   D     S        S      S   S      
2024-02-04 05:46:30,473 ========================================================================================================================
2024-02-04 05:46:30,473 Logging Sequence: 159_159.00
2024-02-04 05:46:30,473 	Gloss Reference :	A B+C+D+E
2024-02-04 05:46:30,473 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 05:46:30,473 	Gloss Alignment :	         
2024-02-04 05:46:30,473 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 05:46:30,476 	Text Reference  :	he   said it     wasn't   easy    the  mind has   to  be    focussed and he is glad that he is back in  form  with the asia    cup          century
2024-02-04 05:46:30,476 	Text Hypothesis :	well my   crowds everyone thought that both match the first time     and ** ** **** **** ** ** **** the score was  not playing professional cricket
2024-02-04 05:46:30,476 	Text Alignment  :	S    S    S      S        S       S    S    S     S   S     S            D  D  D    D    D  D  D    S   S     S    S   S       S            S      
2024-02-04 05:46:30,476 ========================================================================================================================
2024-02-04 05:46:30,477 Logging Sequence: 103_8.00
2024-02-04 05:46:30,477 	Gloss Reference :	A B+C+D+E
2024-02-04 05:46:30,477 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 05:46:30,477 	Gloss Alignment :	         
2024-02-04 05:46:30,477 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 05:46:30,478 	Text Reference  :	were going on           in    birmingham england      from 28th july to  8th     august 2022 
2024-02-04 05:46:30,478 	Text Hypothesis :	**** the   commonwealth games encourage  independence from **** **** the british empire games
2024-02-04 05:46:30,478 	Text Alignment  :	D    S     S            S     S          S                 D    D    S   S       S      S    
2024-02-04 05:46:30,478 ========================================================================================================================
2024-02-04 05:46:30,478 Logging Sequence: 164_546.00
2024-02-04 05:46:30,479 	Gloss Reference :	A B+C+D+E
2024-02-04 05:46:30,479 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 05:46:30,479 	Gloss Alignment :	         
2024-02-04 05:46:30,479 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 05:46:30,480 	Text Reference  :	reliance has turned out   to    be  the    strongest company
2024-02-04 05:46:30,480 	Text Hypothesis :	after    a   tough  match india has beaten them      19     
2024-02-04 05:46:30,480 	Text Alignment  :	S        S   S      S     S     S   S      S         S      
2024-02-04 05:46:30,480 ========================================================================================================================
2024-02-04 05:46:30,480 Logging Sequence: 132_173.00
2024-02-04 05:46:30,480 	Gloss Reference :	A B+C+D+E
2024-02-04 05:46:30,480 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 05:46:30,481 	Gloss Alignment :	         
2024-02-04 05:46:30,481 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 05:46:30,482 	Text Reference  :	**** *** ****** ******** *** ******** ** ** ******** **** ** ****** **** * usman is   australia' first muslim player
2024-02-04 05:46:30,482 	Text Hypothesis :	bcci had tested positive for covid-19 on 26 december 2021 he always used a break from playing    at    1      year  
2024-02-04 05:46:30,482 	Text Alignment  :	I    I   I      I        I   I        I  I  I        I    I  I      I    I S     S    S          S     S      S     
2024-02-04 05:46:30,482 ========================================================================================================================
2024-02-04 05:46:34,482 Epoch 150: Total Training Recognition Loss 0.03  Total Training Translation Loss 11.62 
2024-02-04 05:46:34,482 EPOCH 151
2024-02-04 05:46:38,353 [Epoch: 151 Step: 00010100] Batch Recognition Loss:   0.000470 => Gls Tokens per Sec:     2044 || Batch Translation Loss:   0.202198 => Txt Tokens per Sec:     5699 || Lr: 0.000100
2024-02-04 05:46:39,765 Epoch 151: Total Training Recognition Loss 0.03  Total Training Translation Loss 16.40 
2024-02-04 05:46:39,766 EPOCH 152
2024-02-04 05:46:45,131 Epoch 152: Total Training Recognition Loss 0.04  Total Training Translation Loss 13.15 
2024-02-04 05:46:45,131 EPOCH 153
2024-02-04 05:46:46,433 [Epoch: 153 Step: 00010200] Batch Recognition Loss:   0.000361 => Gls Tokens per Sec:     1969 || Batch Translation Loss:   0.162987 => Txt Tokens per Sec:     5329 || Lr: 0.000100
2024-02-04 05:46:50,631 Epoch 153: Total Training Recognition Loss 0.04  Total Training Translation Loss 15.89 
2024-02-04 05:46:50,631 EPOCH 154
2024-02-04 05:46:54,266 [Epoch: 154 Step: 00010300] Batch Recognition Loss:   0.000479 => Gls Tokens per Sec:     2132 || Batch Translation Loss:   0.127608 => Txt Tokens per Sec:     6006 || Lr: 0.000100
2024-02-04 05:46:55,589 Epoch 154: Total Training Recognition Loss 0.05  Total Training Translation Loss 16.59 
2024-02-04 05:46:55,589 EPOCH 155
2024-02-04 05:47:01,083 Epoch 155: Total Training Recognition Loss 0.04  Total Training Translation Loss 17.67 
2024-02-04 05:47:01,084 EPOCH 156
2024-02-04 05:47:02,280 [Epoch: 156 Step: 00010400] Batch Recognition Loss:   0.000917 => Gls Tokens per Sec:     2009 || Batch Translation Loss:   0.456122 => Txt Tokens per Sec:     5960 || Lr: 0.000100
2024-02-04 05:47:06,304 Epoch 156: Total Training Recognition Loss 0.05  Total Training Translation Loss 24.15 
2024-02-04 05:47:06,305 EPOCH 157
2024-02-04 05:47:10,282 [Epoch: 157 Step: 00010500] Batch Recognition Loss:   0.000773 => Gls Tokens per Sec:     1909 || Batch Translation Loss:   0.623731 => Txt Tokens per Sec:     5286 || Lr: 0.000100
2024-02-04 05:47:11,799 Epoch 157: Total Training Recognition Loss 0.05  Total Training Translation Loss 19.03 
2024-02-04 05:47:11,799 EPOCH 158
2024-02-04 05:47:17,073 Epoch 158: Total Training Recognition Loss 0.06  Total Training Translation Loss 16.07 
2024-02-04 05:47:17,074 EPOCH 159
2024-02-04 05:47:18,000 [Epoch: 159 Step: 00010600] Batch Recognition Loss:   0.000657 => Gls Tokens per Sec:     2423 || Batch Translation Loss:   0.175865 => Txt Tokens per Sec:     6573 || Lr: 0.000100
2024-02-04 05:47:22,247 Epoch 159: Total Training Recognition Loss 0.05  Total Training Translation Loss 19.28 
2024-02-04 05:47:22,248 EPOCH 160
2024-02-04 05:47:25,889 [Epoch: 160 Step: 00010700] Batch Recognition Loss:   0.000477 => Gls Tokens per Sec:     2065 || Batch Translation Loss:   0.320791 => Txt Tokens per Sec:     5674 || Lr: 0.000100
2024-02-04 05:47:27,706 Epoch 160: Total Training Recognition Loss 0.04  Total Training Translation Loss 18.49 
2024-02-04 05:47:27,706 EPOCH 161
2024-02-04 05:47:33,104 Epoch 161: Total Training Recognition Loss 0.05  Total Training Translation Loss 18.32 
2024-02-04 05:47:33,105 EPOCH 162
2024-02-04 05:47:34,357 [Epoch: 162 Step: 00010800] Batch Recognition Loss:   0.000419 => Gls Tokens per Sec:     1592 || Batch Translation Loss:   0.190623 => Txt Tokens per Sec:     4601 || Lr: 0.000100
2024-02-04 05:47:38,630 Epoch 162: Total Training Recognition Loss 0.05  Total Training Translation Loss 21.00 
2024-02-04 05:47:38,631 EPOCH 163
2024-02-04 05:47:42,146 [Epoch: 163 Step: 00010900] Batch Recognition Loss:   0.000543 => Gls Tokens per Sec:     2069 || Batch Translation Loss:   0.227671 => Txt Tokens per Sec:     5743 || Lr: 0.000100
2024-02-04 05:47:43,895 Epoch 163: Total Training Recognition Loss 0.04  Total Training Translation Loss 17.60 
2024-02-04 05:47:43,896 EPOCH 164
2024-02-04 05:47:49,036 Epoch 164: Total Training Recognition Loss 0.04  Total Training Translation Loss 14.46 
2024-02-04 05:47:49,037 EPOCH 165
2024-02-04 05:47:50,024 [Epoch: 165 Step: 00011000] Batch Recognition Loss:   0.000412 => Gls Tokens per Sec:     1945 || Batch Translation Loss:   0.316343 => Txt Tokens per Sec:     5396 || Lr: 0.000100
2024-02-04 05:47:54,354 Epoch 165: Total Training Recognition Loss 0.03  Total Training Translation Loss 13.08 
2024-02-04 05:47:54,355 EPOCH 166
2024-02-04 05:47:57,905 [Epoch: 166 Step: 00011100] Batch Recognition Loss:   0.000331 => Gls Tokens per Sec:     2028 || Batch Translation Loss:   0.228150 => Txt Tokens per Sec:     5554 || Lr: 0.000100
2024-02-04 05:47:59,935 Epoch 166: Total Training Recognition Loss 0.04  Total Training Translation Loss 13.97 
2024-02-04 05:47:59,936 EPOCH 167
2024-02-04 05:48:04,781 Epoch 167: Total Training Recognition Loss 0.03  Total Training Translation Loss 13.38 
2024-02-04 05:48:04,781 EPOCH 168
2024-02-04 05:48:05,397 [Epoch: 168 Step: 00011200] Batch Recognition Loss:   0.000261 => Gls Tokens per Sec:     2862 || Batch Translation Loss:   0.142082 => Txt Tokens per Sec:     7254 || Lr: 0.000100
2024-02-04 05:48:10,099 Epoch 168: Total Training Recognition Loss 0.03  Total Training Translation Loss 12.02 
2024-02-04 05:48:10,100 EPOCH 169
2024-02-04 05:48:13,418 [Epoch: 169 Step: 00011300] Batch Recognition Loss:   0.000351 => Gls Tokens per Sec:     2095 || Batch Translation Loss:   0.075558 => Txt Tokens per Sec:     5758 || Lr: 0.000100
2024-02-04 05:48:15,087 Epoch 169: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.47 
2024-02-04 05:48:15,087 EPOCH 170
2024-02-04 05:48:20,717 Epoch 170: Total Training Recognition Loss 0.03  Total Training Translation Loss 13.88 
2024-02-04 05:48:20,718 EPOCH 171
2024-02-04 05:48:21,492 [Epoch: 171 Step: 00011400] Batch Recognition Loss:   0.001871 => Gls Tokens per Sec:     2070 || Batch Translation Loss:   0.142272 => Txt Tokens per Sec:     5607 || Lr: 0.000100
2024-02-04 05:48:25,859 Epoch 171: Total Training Recognition Loss 0.03  Total Training Translation Loss 14.49 
2024-02-04 05:48:25,859 EPOCH 172
2024-02-04 05:48:29,463 [Epoch: 172 Step: 00011500] Batch Recognition Loss:   0.000263 => Gls Tokens per Sec:     1885 || Batch Translation Loss:   0.192948 => Txt Tokens per Sec:     5198 || Lr: 0.000100
2024-02-04 05:48:31,278 Epoch 172: Total Training Recognition Loss 0.03  Total Training Translation Loss 14.03 
2024-02-04 05:48:31,278 EPOCH 173
2024-02-04 05:48:36,471 Epoch 173: Total Training Recognition Loss 0.04  Total Training Translation Loss 14.79 
2024-02-04 05:48:36,471 EPOCH 174
2024-02-04 05:48:37,281 [Epoch: 174 Step: 00011600] Batch Recognition Loss:   0.000574 => Gls Tokens per Sec:     1779 || Batch Translation Loss:   0.100335 => Txt Tokens per Sec:     5155 || Lr: 0.000100
2024-02-04 05:48:41,943 Epoch 174: Total Training Recognition Loss 0.03  Total Training Translation Loss 15.43 
2024-02-04 05:48:41,944 EPOCH 175
2024-02-04 05:48:45,330 [Epoch: 175 Step: 00011700] Batch Recognition Loss:   0.000418 => Gls Tokens per Sec:     1958 || Batch Translation Loss:   0.135173 => Txt Tokens per Sec:     5532 || Lr: 0.000100
2024-02-04 05:48:47,249 Epoch 175: Total Training Recognition Loss 0.04  Total Training Translation Loss 13.87 
2024-02-04 05:48:47,249 EPOCH 176
2024-02-04 05:48:52,380 Epoch 176: Total Training Recognition Loss 0.04  Total Training Translation Loss 11.82 
2024-02-04 05:48:52,380 EPOCH 177
2024-02-04 05:48:53,255 [Epoch: 177 Step: 00011800] Batch Recognition Loss:   0.000334 => Gls Tokens per Sec:     1464 || Batch Translation Loss:   0.159641 => Txt Tokens per Sec:     4418 || Lr: 0.000100
2024-02-04 05:48:57,910 Epoch 177: Total Training Recognition Loss 0.03  Total Training Translation Loss 11.36 
2024-02-04 05:48:57,910 EPOCH 178
2024-02-04 05:49:01,141 [Epoch: 178 Step: 00011900] Batch Recognition Loss:   0.000524 => Gls Tokens per Sec:     2003 || Batch Translation Loss:   1.126117 => Txt Tokens per Sec:     5563 || Lr: 0.000100
2024-02-04 05:49:03,212 Epoch 178: Total Training Recognition Loss 0.04  Total Training Translation Loss 13.70 
2024-02-04 05:49:03,212 EPOCH 179
2024-02-04 05:49:08,463 Epoch 179: Total Training Recognition Loss 0.03  Total Training Translation Loss 14.86 
2024-02-04 05:49:08,464 EPOCH 180
2024-02-04 05:49:09,134 [Epoch: 180 Step: 00012000] Batch Recognition Loss:   0.000432 => Gls Tokens per Sec:     1674 || Batch Translation Loss:   0.122855 => Txt Tokens per Sec:     4922 || Lr: 0.000100
2024-02-04 05:49:17,711 Validation result at epoch 180, step    12000: duration: 8.5771s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00296	Translation Loss: 90178.62500	PPL: 8301.40039
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.86	(BLEU-1: 11.69,	BLEU-2: 3.73,	BLEU-3: 1.61,	BLEU-4: 0.86)
	CHRF 17.99	ROUGE 9.72
2024-02-04 05:49:17,713 Logging Recognition and Translation Outputs
2024-02-04 05:49:17,713 ========================================================================================================================
2024-02-04 05:49:17,713 Logging Sequence: 177_50.00
2024-02-04 05:49:17,714 	Gloss Reference :	A B+C+D+E
2024-02-04 05:49:17,714 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 05:49:17,714 	Gloss Alignment :	         
2024-02-04 05:49:17,714 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 05:49:17,716 	Text Reference  :	a     similar reward    of rs 50000 was  announced for information ******* ** against his    associate ajay   kumar
2024-02-04 05:49:17,716 	Text Hypothesis :	delhi police  announced a  rs 1     lakh reward    for information leading to the     arrest of        sushil kumar
2024-02-04 05:49:17,716 	Text Alignment  :	S     S       S         S     S     S    S                         I       I  S       S      S         S           
2024-02-04 05:49:17,716 ========================================================================================================================
2024-02-04 05:49:17,716 Logging Sequence: 122_86.00
2024-02-04 05:49:17,716 	Gloss Reference :	A B+C+D+E
2024-02-04 05:49:17,716 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 05:49:17,717 	Gloss Alignment :	         
2024-02-04 05:49:17,717 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 05:49:17,717 	Text Reference  :	after winning chanu spoke    to    the media and   said      
2024-02-04 05:49:17,717 	Text Hypothesis :	as    per     the   olympics rules if  a     press conference
2024-02-04 05:49:17,717 	Text Alignment  :	S     S       S     S        S     S   S     S     S         
2024-02-04 05:49:17,718 ========================================================================================================================
2024-02-04 05:49:17,718 Logging Sequence: 165_27.00
2024-02-04 05:49:17,718 	Gloss Reference :	A B+C+D+E
2024-02-04 05:49:17,718 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 05:49:17,718 	Gloss Alignment :	         
2024-02-04 05:49:17,718 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 05:49:17,720 	Text Reference  :	so  then ****** **** *** they      change their    routes some people believe in  this      while some don't
2024-02-04 05:49:17,720 	Text Hypothesis :	and then handed over the captaincy to     ravindra jadeja who  is     watch   the captaincy to    give it   
2024-02-04 05:49:17,720 	Text Alignment  :	S        I      I    I   S         S      S        S      S    S      S       S   S         S     S    S    
2024-02-04 05:49:17,720 ========================================================================================================================
2024-02-04 05:49:17,720 Logging Sequence: 70_65.00
2024-02-04 05:49:17,720 	Gloss Reference :	A B+C+D+E
2024-02-04 05:49:17,721 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 05:49:17,721 	Gloss Alignment :	         
2024-02-04 05:49:17,721 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 05:49:17,722 	Text Reference  :	during the press conference a        table was placed   in front of         the media
2024-02-04 05:49:17,722 	Text Hypothesis :	****** the ***** ********** olympics will  be  creating a  press conference for 170  
2024-02-04 05:49:17,722 	Text Alignment  :	D          D     D          S        S     S   S        S  S     S          S   S    
2024-02-04 05:49:17,722 ========================================================================================================================
2024-02-04 05:49:17,722 Logging Sequence: 149_65.00
2024-02-04 05:49:17,722 	Gloss Reference :	A B+C+D+E
2024-02-04 05:49:17,722 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 05:49:17,723 	Gloss Alignment :	         
2024-02-04 05:49:17,723 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 05:49:17,725 	Text Reference  :	*** at  6am   on      6th  november 2022 the  police reached sri lankan team's  hotel in   sydney australia's central business district cbd 
2024-02-04 05:49:17,725 	Text Hypothesis :	now the woman alleged that if       i    will be     eager   to  host   matches are   more than   talking     with    2nd      november 2022
2024-02-04 05:49:17,725 	Text Alignment  :	I   S   S     S       S    S        S    S    S      S       S   S      S       S     S    S      S           S       S        S        S   
2024-02-04 05:49:17,725 ========================================================================================================================
2024-02-04 05:49:22,533 Epoch 180: Total Training Recognition Loss 0.04  Total Training Translation Loss 17.04 
2024-02-04 05:49:22,534 EPOCH 181
2024-02-04 05:49:25,271 [Epoch: 181 Step: 00012100] Batch Recognition Loss:   0.000387 => Gls Tokens per Sec:     2339 || Batch Translation Loss:   0.075937 => Txt Tokens per Sec:     6472 || Lr: 0.000100
2024-02-04 05:49:27,327 Epoch 181: Total Training Recognition Loss 0.04  Total Training Translation Loss 16.52 
2024-02-04 05:49:27,327 EPOCH 182
2024-02-04 05:49:33,031 Epoch 182: Total Training Recognition Loss 0.05  Total Training Translation Loss 15.08 
2024-02-04 05:49:33,032 EPOCH 183
2024-02-04 05:49:33,680 [Epoch: 183 Step: 00012200] Batch Recognition Loss:   0.000525 => Gls Tokens per Sec:     1346 || Batch Translation Loss:   2.123478 => Txt Tokens per Sec:     4023 || Lr: 0.000100
2024-02-04 05:49:38,531 Epoch 183: Total Training Recognition Loss 0.04  Total Training Translation Loss 22.68 
2024-02-04 05:49:38,531 EPOCH 184
2024-02-04 05:49:41,701 [Epoch: 184 Step: 00012300] Batch Recognition Loss:   0.001613 => Gls Tokens per Sec:     1942 || Batch Translation Loss:   0.156872 => Txt Tokens per Sec:     5542 || Lr: 0.000100
2024-02-04 05:49:43,438 Epoch 184: Total Training Recognition Loss 0.07  Total Training Translation Loss 17.70 
2024-02-04 05:49:43,438 EPOCH 185
2024-02-04 05:49:48,934 Epoch 185: Total Training Recognition Loss 0.04  Total Training Translation Loss 12.62 
2024-02-04 05:49:48,934 EPOCH 186
2024-02-04 05:49:49,317 [Epoch: 186 Step: 00012400] Batch Recognition Loss:   0.000298 => Gls Tokens per Sec:     2096 || Batch Translation Loss:   0.254736 => Txt Tokens per Sec:     6109 || Lr: 0.000100
2024-02-04 05:49:54,344 Epoch 186: Total Training Recognition Loss 0.04  Total Training Translation Loss 12.37 
2024-02-04 05:49:54,345 EPOCH 187
2024-02-04 05:49:57,379 [Epoch: 187 Step: 00012500] Batch Recognition Loss:   0.000421 => Gls Tokens per Sec:     1975 || Batch Translation Loss:   0.109592 => Txt Tokens per Sec:     5338 || Lr: 0.000100
2024-02-04 05:49:59,579 Epoch 187: Total Training Recognition Loss 0.05  Total Training Translation Loss 12.02 
2024-02-04 05:49:59,580 EPOCH 188
2024-02-04 05:50:05,050 Epoch 188: Total Training Recognition Loss 0.04  Total Training Translation Loss 9.36 
2024-02-04 05:50:05,051 EPOCH 189
2024-02-04 05:50:05,321 [Epoch: 189 Step: 00012600] Batch Recognition Loss:   0.000159 => Gls Tokens per Sec:     2370 || Batch Translation Loss:   0.108974 => Txt Tokens per Sec:     6663 || Lr: 0.000100
2024-02-04 05:50:10,270 Epoch 189: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.35 
2024-02-04 05:50:10,271 EPOCH 190
2024-02-04 05:50:13,262 [Epoch: 190 Step: 00012700] Batch Recognition Loss:   0.000424 => Gls Tokens per Sec:     1980 || Batch Translation Loss:   0.082082 => Txt Tokens per Sec:     5453 || Lr: 0.000100
2024-02-04 05:50:15,922 Epoch 190: Total Training Recognition Loss 0.04  Total Training Translation Loss 9.88 
2024-02-04 05:50:15,922 EPOCH 191
2024-02-04 05:50:21,469 Epoch 191: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.67 
2024-02-04 05:50:21,469 EPOCH 192
2024-02-04 05:50:21,905 [Epoch: 192 Step: 00012800] Batch Recognition Loss:   0.000397 => Gls Tokens per Sec:     1105 || Batch Translation Loss:   0.139151 => Txt Tokens per Sec:     3676 || Lr: 0.000100
2024-02-04 05:50:27,112 Epoch 192: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.80 
2024-02-04 05:50:27,113 EPOCH 193
2024-02-04 05:50:29,820 [Epoch: 193 Step: 00012900] Batch Recognition Loss:   0.000396 => Gls Tokens per Sec:     2128 || Batch Translation Loss:   0.174885 => Txt Tokens per Sec:     5690 || Lr: 0.000100
2024-02-04 05:50:32,544 Epoch 193: Total Training Recognition Loss 0.03  Total Training Translation Loss 10.39 
2024-02-04 05:50:32,545 EPOCH 194
2024-02-04 05:50:37,851 Epoch 194: Total Training Recognition Loss 0.04  Total Training Translation Loss 12.82 
2024-02-04 05:50:37,852 EPOCH 195
2024-02-04 05:50:38,054 [Epoch: 195 Step: 00013000] Batch Recognition Loss:   0.000357 => Gls Tokens per Sec:     1592 || Batch Translation Loss:   0.132561 => Txt Tokens per Sec:     5035 || Lr: 0.000100
2024-02-04 05:50:43,210 Epoch 195: Total Training Recognition Loss 0.03  Total Training Translation Loss 15.32 
2024-02-04 05:50:43,211 EPOCH 196
2024-02-04 05:50:46,102 [Epoch: 196 Step: 00013100] Batch Recognition Loss:   0.000285 => Gls Tokens per Sec:     1906 || Batch Translation Loss:   0.236120 => Txt Tokens per Sec:     5224 || Lr: 0.000100
2024-02-04 05:50:48,793 Epoch 196: Total Training Recognition Loss 0.03  Total Training Translation Loss 13.77 
2024-02-04 05:50:48,793 EPOCH 197
2024-02-04 05:50:54,318 Epoch 197: Total Training Recognition Loss 0.03  Total Training Translation Loss 12.92 
2024-02-04 05:50:54,318 EPOCH 198
2024-02-04 05:50:54,379 [Epoch: 198 Step: 00013200] Batch Recognition Loss:   0.000253 => Gls Tokens per Sec:     2633 || Batch Translation Loss:   0.145470 => Txt Tokens per Sec:     5891 || Lr: 0.000100
2024-02-04 05:50:59,401 Epoch 198: Total Training Recognition Loss 0.04  Total Training Translation Loss 17.21 
2024-02-04 05:50:59,401 EPOCH 199
2024-02-04 05:51:02,250 [Epoch: 199 Step: 00013300] Batch Recognition Loss:   0.000529 => Gls Tokens per Sec:     1912 || Batch Translation Loss:   0.301362 => Txt Tokens per Sec:     5267 || Lr: 0.000100
2024-02-04 05:51:05,042 Epoch 199: Total Training Recognition Loss 0.04  Total Training Translation Loss 19.23 
2024-02-04 05:51:05,042 EPOCH 200
2024-02-04 05:51:10,627 [Epoch: 200 Step: 00013400] Batch Recognition Loss:   0.000795 => Gls Tokens per Sec:     1904 || Batch Translation Loss:   0.152964 => Txt Tokens per Sec:     5284 || Lr: 0.000100
2024-02-04 05:51:10,628 Epoch 200: Total Training Recognition Loss 0.05  Total Training Translation Loss 25.59 
2024-02-04 05:51:10,628 EPOCH 201
2024-02-04 05:51:16,077 Epoch 201: Total Training Recognition Loss 0.05  Total Training Translation Loss 16.78 
2024-02-04 05:51:16,078 EPOCH 202
2024-02-04 05:51:18,443 [Epoch: 202 Step: 00013500] Batch Recognition Loss:   0.000314 => Gls Tokens per Sec:     2196 || Batch Translation Loss:   0.153140 => Txt Tokens per Sec:     6206 || Lr: 0.000100
2024-02-04 05:51:21,054 Epoch 202: Total Training Recognition Loss 0.04  Total Training Translation Loss 15.06 
2024-02-04 05:51:21,055 EPOCH 203
2024-02-04 05:51:26,345 [Epoch: 203 Step: 00013600] Batch Recognition Loss:   0.000272 => Gls Tokens per Sec:     1979 || Batch Translation Loss:   0.233083 => Txt Tokens per Sec:     5472 || Lr: 0.000100
2024-02-04 05:51:26,487 Epoch 203: Total Training Recognition Loss 0.03  Total Training Translation Loss 12.57 
2024-02-04 05:51:26,488 EPOCH 204
2024-02-04 05:51:31,791 Epoch 204: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.49 
2024-02-04 05:51:31,791 EPOCH 205
2024-02-04 05:51:34,143 [Epoch: 205 Step: 00013700] Batch Recognition Loss:   0.000475 => Gls Tokens per Sec:     2178 || Batch Translation Loss:   0.116873 => Txt Tokens per Sec:     5965 || Lr: 0.000100
2024-02-04 05:51:36,927 Epoch 205: Total Training Recognition Loss 0.03  Total Training Translation Loss 10.27 
2024-02-04 05:51:36,927 EPOCH 206
2024-02-04 05:51:42,216 [Epoch: 206 Step: 00013800] Batch Recognition Loss:   0.000515 => Gls Tokens per Sec:     1951 || Batch Translation Loss:   0.072234 => Txt Tokens per Sec:     5429 || Lr: 0.000100
2024-02-04 05:51:42,333 Epoch 206: Total Training Recognition Loss 0.03  Total Training Translation Loss 13.22 
2024-02-04 05:51:42,333 EPOCH 207
2024-02-04 05:51:47,511 Epoch 207: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.37 
2024-02-04 05:51:47,511 EPOCH 208
2024-02-04 05:51:50,052 [Epoch: 208 Step: 00013900] Batch Recognition Loss:   0.000389 => Gls Tokens per Sec:     1917 || Batch Translation Loss:   0.225587 => Txt Tokens per Sec:     5440 || Lr: 0.000100
2024-02-04 05:51:52,799 Epoch 208: Total Training Recognition Loss 0.04  Total Training Translation Loss 8.64 
2024-02-04 05:51:52,800 EPOCH 209
2024-02-04 05:51:58,163 [Epoch: 209 Step: 00014000] Batch Recognition Loss:   0.000499 => Gls Tokens per Sec:     1893 || Batch Translation Loss:   0.130231 => Txt Tokens per Sec:     5277 || Lr: 0.000100
2024-02-04 05:52:06,463 Validation result at epoch 209, step    14000: duration: 8.2989s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00125	Translation Loss: 92781.15625	PPL: 10771.02051
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.93	(BLEU-1: 9.98,	BLEU-2: 3.38,	BLEU-3: 1.62,	BLEU-4: 0.93)
	CHRF 16.52	ROUGE 8.86
2024-02-04 05:52:06,464 Logging Recognition and Translation Outputs
2024-02-04 05:52:06,464 ========================================================================================================================
2024-02-04 05:52:06,464 Logging Sequence: 141_40.00
2024-02-04 05:52:06,464 	Gloss Reference :	A B+C+D+E
2024-02-04 05:52:06,464 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 05:52:06,465 	Gloss Alignment :	         
2024-02-04 05:52:06,465 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 05:52:06,466 	Text Reference  :	got infected with covid-19 he was quarantined and could not take part  in   the   warmup match 
2024-02-04 05:52:06,466 	Text Hypothesis :	*** ******** **** ******** ** *** *********** *** ***** you are  aware that india south  africa
2024-02-04 05:52:06,466 	Text Alignment  :	D   D        D    D        D  D   D           D   D     S   S    S     S    S     S      S     
2024-02-04 05:52:06,466 ========================================================================================================================
2024-02-04 05:52:06,466 Logging Sequence: 117_37.00
2024-02-04 05:52:06,466 	Gloss Reference :	A B+C+D+E
2024-02-04 05:52:06,466 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 05:52:06,466 	Gloss Alignment :	         
2024-02-04 05:52:06,467 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 05:52:06,467 	Text Reference  :	* ** **** shikhar dhawan put  up   a    wonderful performance scoring 98  runs
2024-02-04 05:52:06,468 	Text Hypothesis :	i am sure you     all    must play very well      and         scored  159 runs
2024-02-04 05:52:06,468 	Text Alignment  :	I I  I    S       S      S    S    S    S         S           S       S       
2024-02-04 05:52:06,468 ========================================================================================================================
2024-02-04 05:52:06,468 Logging Sequence: 64_13.00
2024-02-04 05:52:06,468 	Gloss Reference :	A B+C+D+E
2024-02-04 05:52:06,468 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 05:52:06,468 	Gloss Alignment :	         
2024-02-04 05:52:06,468 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 05:52:06,469 	Text Reference  :	*** arrangements were made     to move all the ipl matches to      the  wankhede stadium in  mumbai
2024-02-04 05:52:06,470 	Text Hypothesis :	ipl that         was  supposed to **** *** *** *** ******* promote safe sex      and     all out   
2024-02-04 05:52:06,470 	Text Alignment  :	I   S            S    S           D    D   D   D   D       S       S    S        S       S   S     
2024-02-04 05:52:06,470 ========================================================================================================================
2024-02-04 05:52:06,470 Logging Sequence: 98_121.00
2024-02-04 05:52:06,470 	Gloss Reference :	A B+C+D+E
2024-02-04 05:52:06,470 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 05:52:06,470 	Gloss Alignment :	         
2024-02-04 05:52:06,470 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 05:52:06,471 	Text Reference  :	so then england legends and bangladesh legends were added to the  tournament
2024-02-04 05:52:06,471 	Text Hypothesis :	** **** ******* ******* *** she        did     not  want  to risk this      
2024-02-04 05:52:06,471 	Text Alignment  :	D  D    D       D       D   S          S       S    S        S    S         
2024-02-04 05:52:06,471 ========================================================================================================================
2024-02-04 05:52:06,472 Logging Sequence: 179_414.00
2024-02-04 05:52:06,472 	Gloss Reference :	A B+C+D+E
2024-02-04 05:52:06,472 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 05:52:06,472 	Gloss Alignment :	         
2024-02-04 05:52:06,472 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 05:52:06,473 	Text Reference  :	we could  not  travel to   delhi as  there was  a lockdown in our home town haryana 
2024-02-04 05:52:06,473 	Text Hypothesis :	a  source said vinesh told you   can see   what a ******** ** *** **** sai  decision
2024-02-04 05:52:06,474 	Text Alignment  :	S  S      S    S      S    S     S   S     S      D        D  D   D    S    S       
2024-02-04 05:52:06,474 ========================================================================================================================
2024-02-04 05:52:06,663 Epoch 209: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.96 
2024-02-04 05:52:06,664 EPOCH 210
2024-02-04 05:52:12,283 Epoch 210: Total Training Recognition Loss 0.03  Total Training Translation Loss 11.78 
2024-02-04 05:52:12,283 EPOCH 211
2024-02-04 05:52:14,825 [Epoch: 211 Step: 00014100] Batch Recognition Loss:   0.000365 => Gls Tokens per Sec:     1853 || Batch Translation Loss:   0.156504 => Txt Tokens per Sec:     5267 || Lr: 0.000100
2024-02-04 05:52:17,742 Epoch 211: Total Training Recognition Loss 0.04  Total Training Translation Loss 13.06 
2024-02-04 05:52:17,743 EPOCH 212
2024-02-04 05:52:22,953 [Epoch: 212 Step: 00014200] Batch Recognition Loss:   0.000295 => Gls Tokens per Sec:     1918 || Batch Translation Loss:   0.174237 => Txt Tokens per Sec:     5326 || Lr: 0.000100
2024-02-04 05:52:23,230 Epoch 212: Total Training Recognition Loss 0.03  Total Training Translation Loss 15.57 
2024-02-04 05:52:23,231 EPOCH 213
2024-02-04 05:52:28,895 Epoch 213: Total Training Recognition Loss 0.04  Total Training Translation Loss 12.67 
2024-02-04 05:52:28,896 EPOCH 214
2024-02-04 05:52:31,032 [Epoch: 214 Step: 00014300] Batch Recognition Loss:   0.000325 => Gls Tokens per Sec:     2173 || Batch Translation Loss:   0.142808 => Txt Tokens per Sec:     5891 || Lr: 0.000100
2024-02-04 05:52:34,398 Epoch 214: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.57 
2024-02-04 05:52:34,398 EPOCH 215
2024-02-04 05:52:39,507 [Epoch: 215 Step: 00014400] Batch Recognition Loss:   0.000821 => Gls Tokens per Sec:     1925 || Batch Translation Loss:   0.029238 => Txt Tokens per Sec:     5368 || Lr: 0.000100
2024-02-04 05:52:39,904 Epoch 215: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.44 
2024-02-04 05:52:39,904 EPOCH 216
2024-02-04 05:52:45,428 Epoch 216: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.08 
2024-02-04 05:52:45,429 EPOCH 217
2024-02-04 05:52:47,668 [Epoch: 217 Step: 00014500] Batch Recognition Loss:   0.000676 => Gls Tokens per Sec:     1960 || Batch Translation Loss:   0.050404 => Txt Tokens per Sec:     5293 || Lr: 0.000100
2024-02-04 05:52:50,943 Epoch 217: Total Training Recognition Loss 0.04  Total Training Translation Loss 11.09 
2024-02-04 05:52:50,944 EPOCH 218
2024-02-04 05:52:56,196 [Epoch: 218 Step: 00014600] Batch Recognition Loss:   0.000281 => Gls Tokens per Sec:     1842 || Batch Translation Loss:   0.109797 => Txt Tokens per Sec:     5130 || Lr: 0.000100
2024-02-04 05:52:56,695 Epoch 218: Total Training Recognition Loss 0.04  Total Training Translation Loss 9.40 
2024-02-04 05:52:56,695 EPOCH 219
2024-02-04 05:53:02,448 Epoch 219: Total Training Recognition Loss 0.02  Total Training Translation Loss 8.69 
2024-02-04 05:53:02,449 EPOCH 220
2024-02-04 05:53:04,436 [Epoch: 220 Step: 00014700] Batch Recognition Loss:   0.000237 => Gls Tokens per Sec:     2129 || Batch Translation Loss:   0.050007 => Txt Tokens per Sec:     5737 || Lr: 0.000100
2024-02-04 05:53:07,739 Epoch 220: Total Training Recognition Loss 0.02  Total Training Translation Loss 9.60 
2024-02-04 05:53:07,740 EPOCH 221
2024-02-04 05:53:12,735 [Epoch: 221 Step: 00014800] Batch Recognition Loss:   0.000371 => Gls Tokens per Sec:     1905 || Batch Translation Loss:   0.059218 => Txt Tokens per Sec:     5294 || Lr: 0.000100
2024-02-04 05:53:13,281 Epoch 221: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.59 
2024-02-04 05:53:13,282 EPOCH 222
2024-02-04 05:53:18,742 Epoch 222: Total Training Recognition Loss 0.03  Total Training Translation Loss 12.78 
2024-02-04 05:53:18,743 EPOCH 223
2024-02-04 05:53:20,700 [Epoch: 223 Step: 00014900] Batch Recognition Loss:   0.000488 => Gls Tokens per Sec:     2081 || Batch Translation Loss:   0.413533 => Txt Tokens per Sec:     5287 || Lr: 0.000100
2024-02-04 05:53:24,303 Epoch 223: Total Training Recognition Loss 0.04  Total Training Translation Loss 12.95 
2024-02-04 05:53:24,304 EPOCH 224
2024-02-04 05:53:29,121 [Epoch: 224 Step: 00015000] Batch Recognition Loss:   0.000534 => Gls Tokens per Sec:     1941 || Batch Translation Loss:   0.220927 => Txt Tokens per Sec:     5420 || Lr: 0.000100
2024-02-04 05:53:29,791 Epoch 224: Total Training Recognition Loss 0.02  Total Training Translation Loss 13.33 
2024-02-04 05:53:29,791 EPOCH 225
2024-02-04 05:53:35,108 Epoch 225: Total Training Recognition Loss 0.04  Total Training Translation Loss 12.19 
2024-02-04 05:53:35,108 EPOCH 226
2024-02-04 05:53:37,082 [Epoch: 226 Step: 00015100] Batch Recognition Loss:   0.000295 => Gls Tokens per Sec:     2027 || Batch Translation Loss:   0.072437 => Txt Tokens per Sec:     5469 || Lr: 0.000100
2024-02-04 05:53:40,686 Epoch 226: Total Training Recognition Loss 0.03  Total Training Translation Loss 13.31 
2024-02-04 05:53:40,686 EPOCH 227
2024-02-04 05:53:45,402 [Epoch: 227 Step: 00015200] Batch Recognition Loss:   0.000572 => Gls Tokens per Sec:     1949 || Batch Translation Loss:   0.117165 => Txt Tokens per Sec:     5386 || Lr: 0.000100
2024-02-04 05:53:46,173 Epoch 227: Total Training Recognition Loss 0.03  Total Training Translation Loss 13.10 
2024-02-04 05:53:46,173 EPOCH 228
2024-02-04 05:53:51,677 Epoch 228: Total Training Recognition Loss 0.04  Total Training Translation Loss 14.81 
2024-02-04 05:53:51,677 EPOCH 229
2024-02-04 05:53:53,504 [Epoch: 229 Step: 00015300] Batch Recognition Loss:   0.000457 => Gls Tokens per Sec:     2104 || Batch Translation Loss:   0.127695 => Txt Tokens per Sec:     5760 || Lr: 0.000100
2024-02-04 05:53:57,092 Epoch 229: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.86 
2024-02-04 05:53:57,092 EPOCH 230
2024-02-04 05:54:01,342 [Epoch: 230 Step: 00015400] Batch Recognition Loss:   0.000526 => Gls Tokens per Sec:     2125 || Batch Translation Loss:   0.068728 => Txt Tokens per Sec:     5958 || Lr: 0.000100
2024-02-04 05:54:01,939 Epoch 230: Total Training Recognition Loss 0.04  Total Training Translation Loss 10.42 
2024-02-04 05:54:01,940 EPOCH 231
2024-02-04 05:54:07,506 Epoch 231: Total Training Recognition Loss 0.04  Total Training Translation Loss 9.91 
2024-02-04 05:54:07,507 EPOCH 232
2024-02-04 05:54:09,120 [Epoch: 232 Step: 00015500] Batch Recognition Loss:   0.000461 => Gls Tokens per Sec:     2282 || Batch Translation Loss:   0.084399 => Txt Tokens per Sec:     6163 || Lr: 0.000100
2024-02-04 05:54:12,799 Epoch 232: Total Training Recognition Loss 0.03  Total Training Translation Loss 11.28 
2024-02-04 05:54:12,800 EPOCH 233
2024-02-04 05:54:17,062 [Epoch: 233 Step: 00015600] Batch Recognition Loss:   0.000350 => Gls Tokens per Sec:     2082 || Batch Translation Loss:   0.124577 => Txt Tokens per Sec:     5790 || Lr: 0.000100
2024-02-04 05:54:17,846 Epoch 233: Total Training Recognition Loss 0.02  Total Training Translation Loss 10.44 
2024-02-04 05:54:17,846 EPOCH 234
2024-02-04 05:54:22,994 Epoch 234: Total Training Recognition Loss 0.03  Total Training Translation Loss 12.13 
2024-02-04 05:54:22,995 EPOCH 235
2024-02-04 05:54:24,558 [Epoch: 235 Step: 00015700] Batch Recognition Loss:   0.000356 => Gls Tokens per Sec:     2254 || Batch Translation Loss:   0.236447 => Txt Tokens per Sec:     6104 || Lr: 0.000100
2024-02-04 05:54:28,331 Epoch 235: Total Training Recognition Loss 0.03  Total Training Translation Loss 12.43 
2024-02-04 05:54:28,331 EPOCH 236
2024-02-04 05:54:32,775 [Epoch: 236 Step: 00015800] Batch Recognition Loss:   0.000399 => Gls Tokens per Sec:     1961 || Batch Translation Loss:   0.330331 => Txt Tokens per Sec:     5369 || Lr: 0.000100
2024-02-04 05:54:33,858 Epoch 236: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.03 
2024-02-04 05:54:33,859 EPOCH 237
2024-02-04 05:54:38,971 Epoch 237: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.24 
2024-02-04 05:54:38,972 EPOCH 238
2024-02-04 05:54:40,683 [Epoch: 238 Step: 00015900] Batch Recognition Loss:   0.000301 => Gls Tokens per Sec:     1913 || Batch Translation Loss:   0.073930 => Txt Tokens per Sec:     5517 || Lr: 0.000100
2024-02-04 05:54:44,418 Epoch 238: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.57 
2024-02-04 05:54:44,418 EPOCH 239
2024-02-04 05:54:48,817 [Epoch: 239 Step: 00016000] Batch Recognition Loss:   0.000520 => Gls Tokens per Sec:     1964 || Batch Translation Loss:   0.043159 => Txt Tokens per Sec:     5475 || Lr: 0.000100
2024-02-04 05:54:57,111 Validation result at epoch 239, step    16000: duration: 8.2930s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00202	Translation Loss: 94566.32812	PPL: 12877.75781
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.56	(BLEU-1: 11.45,	BLEU-2: 3.55,	BLEU-3: 1.21,	BLEU-4: 0.56)
	CHRF 17.28	ROUGE 9.72
2024-02-04 05:54:57,112 Logging Recognition and Translation Outputs
2024-02-04 05:54:57,113 ========================================================================================================================
2024-02-04 05:54:57,113 Logging Sequence: 147_132.00
2024-02-04 05:54:57,113 	Gloss Reference :	A B+C+D+E
2024-02-04 05:54:57,113 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 05:54:57,113 	Gloss Alignment :	         
2024-02-04 05:54:57,113 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 05:54:57,114 	Text Reference  :	*** **** i      can not     earlier    i   used to  have fun in  gymnastics
2024-02-04 05:54:57,114 	Text Hypothesis :	the next wanted to  promote positivity and they had sent her win this      
2024-02-04 05:54:57,114 	Text Alignment  :	I   I    S      S   S       S          S   S    S   S    S   S   S         
2024-02-04 05:54:57,115 ========================================================================================================================
2024-02-04 05:54:57,115 Logging Sequence: 116_162.00
2024-02-04 05:54:57,115 	Gloss Reference :	A B+C+D+E
2024-02-04 05:54:57,115 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 05:54:57,115 	Gloss Alignment :	         
2024-02-04 05:54:57,115 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 05:54:57,117 	Text Reference  :	***** ******* **** turned       out the **** video was  shared on social media by a    staff    at   the  hotel   
2024-02-04 05:54:57,117 	Text Hypothesis :	seven players also participated in  the same room  with going  on ****** ***** ** 12th november 2022 this happened
2024-02-04 05:54:57,117 	Text Alignment  :	I     I       I    S            S       I    S     S    S         D      D     D  S    S        S    S    S       
2024-02-04 05:54:57,117 ========================================================================================================================
2024-02-04 05:54:57,117 Logging Sequence: 73_79.00
2024-02-04 05:54:57,117 	Gloss Reference :	A B+C+D+E
2024-02-04 05:54:57,118 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 05:54:57,118 	Gloss Alignment :	         
2024-02-04 05:54:57,118 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 05:54:57,119 	Text Reference  :	raina resturant has food from the rich spices of north india to  the aromatic curries of     south  india      
2024-02-04 05:54:57,119 	Text Hypothesis :	***** ********* *** **** **** the **** ****** ** match has   now 8   things   as      'raina indian restaurant'
2024-02-04 05:54:57,119 	Text Alignment  :	D     D         D   D    D        D    D      D  S     S     S   S   S        S       S      S      S          
2024-02-04 05:54:57,119 ========================================================================================================================
2024-02-04 05:54:57,119 Logging Sequence: 165_523.00
2024-02-04 05:54:57,120 	Gloss Reference :	A B+C+D+E
2024-02-04 05:54:57,120 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 05:54:57,120 	Gloss Alignment :	         
2024-02-04 05:54:57,120 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 05:54:57,121 	Text Reference  :	as he believed that his team might lose if   he      takes off      his batting pads 
2024-02-04 05:54:57,121 	Text Hypothesis :	** ** ******** **** *** but  we    have been sharing their emotions on  social  media
2024-02-04 05:54:57,121 	Text Alignment  :	D  D  D        D    D   S    S     S    S    S       S     S        S   S       S    
2024-02-04 05:54:57,121 ========================================================================================================================
2024-02-04 05:54:57,121 Logging Sequence: 125_72.00
2024-02-04 05:54:57,122 	Gloss Reference :	A B+C+D+E
2024-02-04 05:54:57,122 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 05:54:57,122 	Gloss Alignment :	         
2024-02-04 05:54:57,122 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 05:54:57,123 	Text Reference  :	some said the pakistani javelineer had  milicious intentions of   tampering with the javelin out of jealousy   
2024-02-04 05:54:57,123 	Text Hypothesis :	**** **** *** new       zealand    lost by        372        runs and       euro the ******* *** ** semi-finals
2024-02-04 05:54:57,123 	Text Alignment  :	D    D    D   S         S          S    S         S          S    S         S        D       D   D  S          
2024-02-04 05:54:57,123 ========================================================================================================================
2024-02-04 05:54:58,133 Epoch 239: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.23 
2024-02-04 05:54:58,133 EPOCH 240
2024-02-04 05:55:03,842 Epoch 240: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.21 
2024-02-04 05:55:03,843 EPOCH 241
2024-02-04 05:55:05,466 [Epoch: 241 Step: 00016100] Batch Recognition Loss:   0.000440 => Gls Tokens per Sec:     1974 || Batch Translation Loss:   0.116253 => Txt Tokens per Sec:     5181 || Lr: 0.000100
2024-02-04 05:55:09,412 Epoch 241: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.78 
2024-02-04 05:55:09,413 EPOCH 242
2024-02-04 05:55:13,407 [Epoch: 242 Step: 00016200] Batch Recognition Loss:   0.000509 => Gls Tokens per Sec:     2102 || Batch Translation Loss:   0.093814 => Txt Tokens per Sec:     5942 || Lr: 0.000100
2024-02-04 05:55:14,284 Epoch 242: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.28 
2024-02-04 05:55:14,284 EPOCH 243
2024-02-04 05:55:19,794 Epoch 243: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.98 
2024-02-04 05:55:19,795 EPOCH 244
2024-02-04 05:55:21,192 [Epoch: 244 Step: 00016300] Batch Recognition Loss:   0.000343 => Gls Tokens per Sec:     2178 || Batch Translation Loss:   0.089489 => Txt Tokens per Sec:     5965 || Lr: 0.000100
2024-02-04 05:55:25,004 Epoch 244: Total Training Recognition Loss 0.02  Total Training Translation Loss 10.73 
2024-02-04 05:55:25,005 EPOCH 245
2024-02-04 05:55:29,332 [Epoch: 245 Step: 00016400] Batch Recognition Loss:   0.000409 => Gls Tokens per Sec:     1903 || Batch Translation Loss:   0.405465 => Txt Tokens per Sec:     5343 || Lr: 0.000100
2024-02-04 05:55:30,349 Epoch 245: Total Training Recognition Loss 0.03  Total Training Translation Loss 11.39 
2024-02-04 05:55:30,349 EPOCH 246
2024-02-04 05:55:35,592 Epoch 246: Total Training Recognition Loss 0.04  Total Training Translation Loss 12.30 
2024-02-04 05:55:35,592 EPOCH 247
2024-02-04 05:55:36,822 [Epoch: 247 Step: 00016500] Batch Recognition Loss:   0.000412 => Gls Tokens per Sec:     2345 || Batch Translation Loss:   0.173547 => Txt Tokens per Sec:     6237 || Lr: 0.000100
2024-02-04 05:55:40,644 Epoch 247: Total Training Recognition Loss 0.03  Total Training Translation Loss 12.44 
2024-02-04 05:55:40,644 EPOCH 248
2024-02-04 05:55:44,124 [Epoch: 248 Step: 00016600] Batch Recognition Loss:   0.000557 => Gls Tokens per Sec:     2320 || Batch Translation Loss:   0.412658 => Txt Tokens per Sec:     6236 || Lr: 0.000100
2024-02-04 05:55:45,580 Epoch 248: Total Training Recognition Loss 0.03  Total Training Translation Loss 11.36 
2024-02-04 05:55:45,580 EPOCH 249
2024-02-04 05:55:50,721 Epoch 249: Total Training Recognition Loss 0.02  Total Training Translation Loss 11.89 
2024-02-04 05:55:50,722 EPOCH 250
2024-02-04 05:55:52,098 [Epoch: 250 Step: 00016700] Batch Recognition Loss:   0.000570 => Gls Tokens per Sec:     1912 || Batch Translation Loss:   0.132740 => Txt Tokens per Sec:     5230 || Lr: 0.000100
2024-02-04 05:55:55,950 Epoch 250: Total Training Recognition Loss 0.03  Total Training Translation Loss 10.96 
2024-02-04 05:55:55,950 EPOCH 251
2024-02-04 05:56:00,055 [Epoch: 251 Step: 00016800] Batch Recognition Loss:   0.000317 => Gls Tokens per Sec:     1928 || Batch Translation Loss:   0.131842 => Txt Tokens per Sec:     5386 || Lr: 0.000100
2024-02-04 05:56:01,370 Epoch 251: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.49 
2024-02-04 05:56:01,370 EPOCH 252
2024-02-04 05:56:06,692 Epoch 252: Total Training Recognition Loss 0.03  Total Training Translation Loss 11.29 
2024-02-04 05:56:06,693 EPOCH 253
2024-02-04 05:56:07,951 [Epoch: 253 Step: 00016900] Batch Recognition Loss:   0.000329 => Gls Tokens per Sec:     2038 || Batch Translation Loss:   0.256906 => Txt Tokens per Sec:     5490 || Lr: 0.000100
2024-02-04 05:56:12,144 Epoch 253: Total Training Recognition Loss 0.03  Total Training Translation Loss 12.03 
2024-02-04 05:56:12,144 EPOCH 254
2024-02-04 05:56:15,565 [Epoch: 254 Step: 00017000] Batch Recognition Loss:   0.000604 => Gls Tokens per Sec:     2267 || Batch Translation Loss:   0.154867 => Txt Tokens per Sec:     6009 || Lr: 0.000100
2024-02-04 05:56:17,377 Epoch 254: Total Training Recognition Loss 0.04  Total Training Translation Loss 14.47 
2024-02-04 05:56:17,377 EPOCH 255
2024-02-04 05:56:22,568 Epoch 255: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.13 
2024-02-04 05:56:22,568 EPOCH 256
2024-02-04 05:56:23,974 [Epoch: 256 Step: 00017100] Batch Recognition Loss:   0.000188 => Gls Tokens per Sec:     1709 || Batch Translation Loss:   0.093770 => Txt Tokens per Sec:     4817 || Lr: 0.000100
2024-02-04 05:56:28,033 Epoch 256: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.68 
2024-02-04 05:56:28,034 EPOCH 257
2024-02-04 05:56:31,565 [Epoch: 257 Step: 00017200] Batch Recognition Loss:   0.000189 => Gls Tokens per Sec:     2175 || Batch Translation Loss:   0.076349 => Txt Tokens per Sec:     6008 || Lr: 0.000100
2024-02-04 05:56:33,203 Epoch 257: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.87 
2024-02-04 05:56:33,203 EPOCH 258
2024-02-04 05:56:38,540 Epoch 258: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.08 
2024-02-04 05:56:38,540 EPOCH 259
2024-02-04 05:56:39,677 [Epoch: 259 Step: 00017300] Batch Recognition Loss:   0.000220 => Gls Tokens per Sec:     1973 || Batch Translation Loss:   0.164139 => Txt Tokens per Sec:     5554 || Lr: 0.000100
2024-02-04 05:56:43,936 Epoch 259: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.29 
2024-02-04 05:56:43,937 EPOCH 260
2024-02-04 05:56:47,523 [Epoch: 260 Step: 00017400] Batch Recognition Loss:   0.000166 => Gls Tokens per Sec:     2072 || Batch Translation Loss:   0.239865 => Txt Tokens per Sec:     5802 || Lr: 0.000100
2024-02-04 05:56:49,115 Epoch 260: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.05 
2024-02-04 05:56:49,116 EPOCH 261
2024-02-04 05:56:54,423 Epoch 261: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.34 
2024-02-04 05:56:54,424 EPOCH 262
2024-02-04 05:56:55,386 [Epoch: 262 Step: 00017500] Batch Recognition Loss:   0.000965 => Gls Tokens per Sec:     2162 || Batch Translation Loss:   0.037492 => Txt Tokens per Sec:     6464 || Lr: 0.000100
2024-02-04 05:56:59,763 Epoch 262: Total Training Recognition Loss 0.03  Total Training Translation Loss 10.39 
2024-02-04 05:56:59,763 EPOCH 263
2024-02-04 05:57:03,348 [Epoch: 263 Step: 00017600] Batch Recognition Loss:   0.000287 => Gls Tokens per Sec:     2054 || Batch Translation Loss:   0.254851 => Txt Tokens per Sec:     5921 || Lr: 0.000100
2024-02-04 05:57:04,831 Epoch 263: Total Training Recognition Loss 0.03  Total Training Translation Loss 13.15 
2024-02-04 05:57:04,832 EPOCH 264
2024-02-04 05:57:10,270 Epoch 264: Total Training Recognition Loss 0.03  Total Training Translation Loss 10.13 
2024-02-04 05:57:10,270 EPOCH 265
2024-02-04 05:57:11,188 [Epoch: 265 Step: 00017700] Batch Recognition Loss:   0.000779 => Gls Tokens per Sec:     2093 || Batch Translation Loss:   0.209223 => Txt Tokens per Sec:     5619 || Lr: 0.000100
2024-02-04 05:57:15,679 Epoch 265: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.82 
2024-02-04 05:57:15,679 EPOCH 266
2024-02-04 05:57:19,114 [Epoch: 266 Step: 00017800] Batch Recognition Loss:   0.000224 => Gls Tokens per Sec:     2070 || Batch Translation Loss:   0.060005 => Txt Tokens per Sec:     5727 || Lr: 0.000100
2024-02-04 05:57:20,809 Epoch 266: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.77 
2024-02-04 05:57:20,809 EPOCH 267
2024-02-04 05:57:26,456 Epoch 267: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.33 
2024-02-04 05:57:26,456 EPOCH 268
2024-02-04 05:57:27,226 [Epoch: 268 Step: 00017900] Batch Recognition Loss:   0.000509 => Gls Tokens per Sec:     2291 || Batch Translation Loss:   0.075562 => Txt Tokens per Sec:     6427 || Lr: 0.000100
2024-02-04 05:57:31,633 Epoch 268: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.46 
2024-02-04 05:57:31,634 EPOCH 269
2024-02-04 05:57:35,162 [Epoch: 269 Step: 00018000] Batch Recognition Loss:   0.000476 => Gls Tokens per Sec:     1970 || Batch Translation Loss:   0.152828 => Txt Tokens per Sec:     5464 || Lr: 0.000100
2024-02-04 05:57:43,642 Validation result at epoch 269, step    18000: duration: 8.4800s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00130	Translation Loss: 94472.32031	PPL: 12757.18457
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.75	(BLEU-1: 10.70,	BLEU-2: 3.39,	BLEU-3: 1.46,	BLEU-4: 0.75)
	CHRF 16.78	ROUGE 9.48
2024-02-04 05:57:43,643 Logging Recognition and Translation Outputs
2024-02-04 05:57:43,643 ========================================================================================================================
2024-02-04 05:57:43,643 Logging Sequence: 155_119.00
2024-02-04 05:57:43,644 	Gloss Reference :	A B+C+D+E
2024-02-04 05:57:43,644 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 05:57:43,644 	Gloss Alignment :	         
2024-02-04 05:57:43,644 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 05:57:43,645 	Text Reference  :	a report said that the  taliban wanted icc   to replace the afghan flag with its own       
2024-02-04 05:57:43,645 	Text Hypothesis :	* ****** and  was  born on      social media to ******* *** win    they lost the tournament
2024-02-04 05:57:43,645 	Text Alignment  :	D D      S    S    S    S       S      S        D       D   S      S    S    S   S         
2024-02-04 05:57:43,645 ========================================================================================================================
2024-02-04 05:57:43,646 Logging Sequence: 153_43.00
2024-02-04 05:57:43,646 	Gloss Reference :	A B+C+D+E
2024-02-04 05:57:43,646 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 05:57:43,646 	Gloss Alignment :	         
2024-02-04 05:57:43,646 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 05:57:43,647 	Text Reference  :	***** ** ******* ***** *** these runs were all  because of hardik pandya   and virat kohli  
2024-02-04 05:57:43,647 	Text Hypothesis :	india is because dhoni was not   in   the  same ritual  of the    security of  the   british
2024-02-04 05:57:43,647 	Text Alignment  :	I     I  I       I     I   S     S    S    S    S          S      S        S   S     S      
2024-02-04 05:57:43,648 ========================================================================================================================
2024-02-04 05:57:43,648 Logging Sequence: 150_35.00
2024-02-04 05:57:43,648 	Gloss Reference :	A B+C+D+E
2024-02-04 05:57:43,648 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 05:57:43,648 	Gloss Alignment :	         
2024-02-04 05:57:43,648 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 05:57:43,649 	Text Reference  :	*** wow  india football team    is     really     strong   
2024-02-04 05:57:43,649 	Text Hypothesis :	one must not   venture  outside unless absolutely necessary
2024-02-04 05:57:43,649 	Text Alignment  :	I   S    S     S        S       S      S          S        
2024-02-04 05:57:43,649 ========================================================================================================================
2024-02-04 05:57:43,649 Logging Sequence: 146_154.00
2024-02-04 05:57:43,649 	Gloss Reference :	A B+C+D+E
2024-02-04 05:57:43,649 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 05:57:43,650 	Gloss Alignment :	         
2024-02-04 05:57:43,650 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 05:57:43,651 	Text Reference  :	*** bwf    said that testing protocols have   been implemented to      ensure the  health and safety of all participants
2024-02-04 05:57:43,651 	Text Hypothesis :	the second time that ******* ********* taylor came in          england and    held every  4   years  of the world       
2024-02-04 05:57:43,652 	Text Alignment  :	I   S      S         D       D         S      S    S           S       S      S    S      S   S         S   S           
2024-02-04 05:57:43,652 ========================================================================================================================
2024-02-04 05:57:43,652 Logging Sequence: 76_79.00
2024-02-04 05:57:43,652 	Gloss Reference :	A B+C+D+E
2024-02-04 05:57:43,652 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 05:57:43,652 	Gloss Alignment :	         
2024-02-04 05:57:43,652 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 05:57:43,653 	Text Reference  :	** speaking to        ani csk       ceo kasi   viswanathan said 
2024-02-04 05:57:43,653 	Text Hypothesis :	on 23rd     september the president of  mumbai the         match
2024-02-04 05:57:43,653 	Text Alignment  :	I  S        S         S   S         S   S      S           S    
2024-02-04 05:57:43,653 ========================================================================================================================
2024-02-04 05:57:45,537 Epoch 269: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.00 
2024-02-04 05:57:45,537 EPOCH 270
2024-02-04 05:57:51,245 Epoch 270: Total Training Recognition Loss 0.02  Total Training Translation Loss 13.63 
2024-02-04 05:57:51,245 EPOCH 271
2024-02-04 05:57:51,891 [Epoch: 271 Step: 00018100] Batch Recognition Loss:   0.001237 => Gls Tokens per Sec:     2340 || Batch Translation Loss:   0.148006 => Txt Tokens per Sec:     5782 || Lr: 0.000100
2024-02-04 05:57:56,481 Epoch 271: Total Training Recognition Loss 0.04  Total Training Translation Loss 11.28 
2024-02-04 05:57:56,482 EPOCH 272
2024-02-04 05:57:59,941 [Epoch: 272 Step: 00018200] Batch Recognition Loss:   0.000424 => Gls Tokens per Sec:     1964 || Batch Translation Loss:   0.095735 => Txt Tokens per Sec:     5297 || Lr: 0.000100
2024-02-04 05:58:02,026 Epoch 272: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.93 
2024-02-04 05:58:02,026 EPOCH 273
2024-02-04 05:58:07,483 Epoch 273: Total Training Recognition Loss 0.02  Total Training Translation Loss 9.31 
2024-02-04 05:58:07,484 EPOCH 274
2024-02-04 05:58:08,155 [Epoch: 274 Step: 00018300] Batch Recognition Loss:   0.000543 => Gls Tokens per Sec:     2149 || Batch Translation Loss:   0.093934 => Txt Tokens per Sec:     5833 || Lr: 0.000100
2024-02-04 05:58:12,957 Epoch 274: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.09 
2024-02-04 05:58:12,958 EPOCH 275
2024-02-04 05:58:16,336 [Epoch: 275 Step: 00018400] Batch Recognition Loss:   0.000186 => Gls Tokens per Sec:     1964 || Batch Translation Loss:   0.057357 => Txt Tokens per Sec:     5372 || Lr: 0.000100
2024-02-04 05:58:18,491 Epoch 275: Total Training Recognition Loss 0.02  Total Training Translation Loss 8.15 
2024-02-04 05:58:18,491 EPOCH 276
2024-02-04 05:58:23,508 Epoch 276: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.93 
2024-02-04 05:58:23,509 EPOCH 277
2024-02-04 05:58:24,214 [Epoch: 277 Step: 00018500] Batch Recognition Loss:   0.000429 => Gls Tokens per Sec:     1820 || Batch Translation Loss:   0.048512 => Txt Tokens per Sec:     5278 || Lr: 0.000100
2024-02-04 05:58:28,790 Epoch 277: Total Training Recognition Loss 0.02  Total Training Translation Loss 8.03 
2024-02-04 05:58:28,790 EPOCH 278
2024-02-04 05:58:31,863 [Epoch: 278 Step: 00018600] Batch Recognition Loss:   0.000383 => Gls Tokens per Sec:     2107 || Batch Translation Loss:   0.188316 => Txt Tokens per Sec:     5670 || Lr: 0.000100
2024-02-04 05:58:34,094 Epoch 278: Total Training Recognition Loss 0.06  Total Training Translation Loss 9.64 
2024-02-04 05:58:34,094 EPOCH 279
2024-02-04 05:58:39,754 Epoch 279: Total Training Recognition Loss 0.02  Total Training Translation Loss 8.02 
2024-02-04 05:58:39,755 EPOCH 280
2024-02-04 05:58:40,410 [Epoch: 280 Step: 00018700] Batch Recognition Loss:   0.000446 => Gls Tokens per Sec:     1713 || Batch Translation Loss:   0.420716 => Txt Tokens per Sec:     5364 || Lr: 0.000100
2024-02-04 05:58:45,155 Epoch 280: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.43 
2024-02-04 05:58:45,155 EPOCH 281
2024-02-04 05:58:48,293 [Epoch: 281 Step: 00018800] Batch Recognition Loss:   0.000347 => Gls Tokens per Sec:     2040 || Batch Translation Loss:   0.087337 => Txt Tokens per Sec:     5707 || Lr: 0.000100
2024-02-04 05:58:50,631 Epoch 281: Total Training Recognition Loss 0.02  Total Training Translation Loss 10.55 
2024-02-04 05:58:50,631 EPOCH 282
2024-02-04 05:58:56,200 Epoch 282: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.71 
2024-02-04 05:58:56,201 EPOCH 283
2024-02-04 05:58:56,796 [Epoch: 283 Step: 00018900] Batch Recognition Loss:   0.000325 => Gls Tokens per Sec:     1615 || Batch Translation Loss:   0.124575 => Txt Tokens per Sec:     4787 || Lr: 0.000100
2024-02-04 05:59:01,632 Epoch 283: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.75 
2024-02-04 05:59:01,633 EPOCH 284
2024-02-04 05:59:04,698 [Epoch: 284 Step: 00019000] Batch Recognition Loss:   0.000348 => Gls Tokens per Sec:     2007 || Batch Translation Loss:   0.300879 => Txt Tokens per Sec:     5361 || Lr: 0.000100
2024-02-04 05:59:07,068 Epoch 284: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.91 
2024-02-04 05:59:07,068 EPOCH 285
2024-02-04 05:59:12,284 Epoch 285: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.59 
2024-02-04 05:59:12,285 EPOCH 286
2024-02-04 05:59:12,720 [Epoch: 286 Step: 00019100] Batch Recognition Loss:   0.000182 => Gls Tokens per Sec:     1843 || Batch Translation Loss:   0.276311 => Txt Tokens per Sec:     4903 || Lr: 0.000100
2024-02-04 05:59:17,712 Epoch 286: Total Training Recognition Loss 0.03  Total Training Translation Loss 12.66 
2024-02-04 05:59:17,712 EPOCH 287
2024-02-04 05:59:20,514 [Epoch: 287 Step: 00019200] Batch Recognition Loss:   0.000251 => Gls Tokens per Sec:     2139 || Batch Translation Loss:   0.032913 => Txt Tokens per Sec:     6155 || Lr: 0.000100
2024-02-04 05:59:22,820 Epoch 287: Total Training Recognition Loss 0.04  Total Training Translation Loss 11.95 
2024-02-04 05:59:22,820 EPOCH 288
2024-02-04 05:59:28,179 Epoch 288: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.73 
2024-02-04 05:59:28,179 EPOCH 289
2024-02-04 05:59:28,404 [Epoch: 289 Step: 00019300] Batch Recognition Loss:   0.000187 => Gls Tokens per Sec:     2857 || Batch Translation Loss:   0.107137 => Txt Tokens per Sec:     7429 || Lr: 0.000100
2024-02-04 05:59:33,594 Epoch 289: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.66 
2024-02-04 05:59:33,595 EPOCH 290
2024-02-04 05:59:36,004 [Epoch: 290 Step: 00019400] Batch Recognition Loss:   0.000297 => Gls Tokens per Sec:     2420 || Batch Translation Loss:   0.210426 => Txt Tokens per Sec:     6374 || Lr: 0.000100
2024-02-04 05:59:38,540 Epoch 290: Total Training Recognition Loss 0.02  Total Training Translation Loss 8.41 
2024-02-04 05:59:38,541 EPOCH 291
2024-02-04 05:59:44,099 Epoch 291: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.21 
2024-02-04 05:59:44,099 EPOCH 292
2024-02-04 05:59:44,327 [Epoch: 292 Step: 00019500] Batch Recognition Loss:   0.000291 => Gls Tokens per Sec:     2115 || Batch Translation Loss:   0.045423 => Txt Tokens per Sec:     5965 || Lr: 0.000100
2024-02-04 05:59:49,456 Epoch 292: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.37 
2024-02-04 05:59:49,457 EPOCH 293
2024-02-04 05:59:52,075 [Epoch: 293 Step: 00019600] Batch Recognition Loss:   0.000347 => Gls Tokens per Sec:     2202 || Batch Translation Loss:   0.346467 => Txt Tokens per Sec:     6018 || Lr: 0.000100
2024-02-04 05:59:54,597 Epoch 293: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.29 
2024-02-04 05:59:54,597 EPOCH 294
2024-02-04 06:00:00,339 Epoch 294: Total Training Recognition Loss 0.02  Total Training Translation Loss 10.34 
2024-02-04 06:00:00,339 EPOCH 295
2024-02-04 06:00:00,503 [Epoch: 295 Step: 00019700] Batch Recognition Loss:   0.000223 => Gls Tokens per Sec:     1975 || Batch Translation Loss:   0.307749 => Txt Tokens per Sec:     6309 || Lr: 0.000100
2024-02-04 06:00:05,649 Epoch 295: Total Training Recognition Loss 0.03  Total Training Translation Loss 11.02 
2024-02-04 06:00:05,649 EPOCH 296
2024-02-04 06:00:08,358 [Epoch: 296 Step: 00019800] Batch Recognition Loss:   0.000300 => Gls Tokens per Sec:     2068 || Batch Translation Loss:   0.166933 => Txt Tokens per Sec:     5712 || Lr: 0.000100
2024-02-04 06:00:11,117 Epoch 296: Total Training Recognition Loss 0.03  Total Training Translation Loss 10.55 
2024-02-04 06:00:11,118 EPOCH 297
2024-02-04 06:00:16,534 Epoch 297: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.56 
2024-02-04 06:00:16,534 EPOCH 298
2024-02-04 06:00:16,588 [Epoch: 298 Step: 00019900] Batch Recognition Loss:   0.000352 => Gls Tokens per Sec:     3019 || Batch Translation Loss:   0.093851 => Txt Tokens per Sec:     6717 || Lr: 0.000100
2024-02-04 06:00:22,121 Epoch 298: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.16 
2024-02-04 06:00:22,122 EPOCH 299
2024-02-04 06:00:24,833 [Epoch: 299 Step: 00020000] Batch Recognition Loss:   0.000593 => Gls Tokens per Sec:     2007 || Batch Translation Loss:   0.037426 => Txt Tokens per Sec:     5622 || Lr: 0.000100
2024-02-04 06:00:33,943 Validation result at epoch 299, step    20000: duration: 9.1098s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00196	Translation Loss: 95074.60938	PPL: 13549.71875
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.66	(BLEU-1: 10.55,	BLEU-2: 3.62,	BLEU-3: 1.47,	BLEU-4: 0.66)
	CHRF 17.13	ROUGE 9.27
2024-02-04 06:00:33,945 Logging Recognition and Translation Outputs
2024-02-04 06:00:33,945 ========================================================================================================================
2024-02-04 06:00:33,946 Logging Sequence: 174_121.00
2024-02-04 06:00:33,946 	Gloss Reference :	A B+C+D+E
2024-02-04 06:00:33,947 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:00:33,947 	Gloss Alignment :	         
2024-02-04 06:00:33,947 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:00:33,949 	Text Reference  :	*** ****** ********* there was   a   strong  competition     and  a     difficult auction for the 5  franchise owners
2024-02-04 06:00:33,950 	Text Hypothesis :	the sports authority of    india sai tweeted congratulations team india was       given   a   lot of 2         crore 
2024-02-04 06:00:33,950 	Text Alignment  :	I   I      I         S     S     S   S       S               S    S     S         S       S   S   S  S         S     
2024-02-04 06:00:33,950 ========================================================================================================================
2024-02-04 06:00:33,950 Logging Sequence: 170_24.00
2024-02-04 06:00:33,951 	Gloss Reference :	A B+C+D+E
2024-02-04 06:00:33,951 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:00:33,951 	Gloss Alignment :	         
2024-02-04 06:00:33,951 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:00:33,952 	Text Reference  :	******* **** ****** ****** let  me    tell you   about it     
2024-02-04 06:00:33,952 	Text Hypothesis :	earlier when wooden stumps were glued to   their tv    screens
2024-02-04 06:00:33,952 	Text Alignment  :	I       I    I      I      S    S     S    S     S     S      
2024-02-04 06:00:33,952 ========================================================================================================================
2024-02-04 06:00:33,953 Logging Sequence: 73_79.00
2024-02-04 06:00:33,953 	Gloss Reference :	A B+C+D+E
2024-02-04 06:00:33,953 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:00:33,953 	Gloss Alignment :	         
2024-02-04 06:00:33,954 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:00:33,956 	Text Reference  :	raina resturant has food from the  rich  spices of north india to  the aromatic        curries of south india 
2024-02-04 06:00:33,956 	Text Hypothesis :	***** ********* pm  modi took this match as     he was   taken for the ahmedabad-based dish    on the   umpire
2024-02-04 06:00:33,956 	Text Alignment  :	D     D         S   S    S    S    S     S      S  S     S     S       S               S       S  S     S     
2024-02-04 06:00:33,957 ========================================================================================================================
2024-02-04 06:00:33,957 Logging Sequence: 140_2.00
2024-02-04 06:00:33,957 	Gloss Reference :	A B+C+D+E
2024-02-04 06:00:33,957 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:00:33,957 	Gloss Alignment :	         
2024-02-04 06:00:33,958 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:00:33,959 	Text Reference  :	**** indian batsman-wicket keeper rishabh pant has    outstanding skills in cricket
2024-02-04 06:00:33,959 	Text Hypothesis :	felt sachin was            lucky  so      he   always gave        him    to india  
2024-02-04 06:00:33,959 	Text Alignment  :	I    S      S              S      S       S    S      S           S      S  S      
2024-02-04 06:00:33,959 ========================================================================================================================
2024-02-04 06:00:33,959 Logging Sequence: 81_470.00
2024-02-04 06:00:33,960 	Gloss Reference :	A B+C+D+E
2024-02-04 06:00:33,960 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:00:33,960 	Gloss Alignment :	         
2024-02-04 06:00:33,960 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:00:33,962 	Text Reference  :	or you don't know if you do  let us  know      in the comments
2024-02-04 06:00:33,962 	Text Hypothesis :	** *** ***** **** ** the two men has completed as 6   balls   
2024-02-04 06:00:33,962 	Text Alignment  :	D  D   D     D    D  S   S   S   S   S         S  S   S       
2024-02-04 06:00:33,962 ========================================================================================================================
2024-02-04 06:00:37,159 Epoch 299: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.97 
2024-02-04 06:00:37,160 EPOCH 300
2024-02-04 06:00:42,628 [Epoch: 300 Step: 00020100] Batch Recognition Loss:   0.000299 => Gls Tokens per Sec:     1945 || Batch Translation Loss:   0.065317 => Txt Tokens per Sec:     5398 || Lr: 0.000100
2024-02-04 06:00:42,628 Epoch 300: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.87 
2024-02-04 06:00:42,629 EPOCH 301
2024-02-04 06:00:48,175 Epoch 301: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.82 
2024-02-04 06:00:48,175 EPOCH 302
2024-02-04 06:00:51,092 [Epoch: 302 Step: 00020200] Batch Recognition Loss:   0.000746 => Gls Tokens per Sec:     1780 || Batch Translation Loss:   0.608963 => Txt Tokens per Sec:     5252 || Lr: 0.000100
2024-02-04 06:00:53,843 Epoch 302: Total Training Recognition Loss 0.03  Total Training Translation Loss 10.13 
2024-02-04 06:00:53,844 EPOCH 303
2024-02-04 06:00:59,017 [Epoch: 303 Step: 00020300] Batch Recognition Loss:   0.000432 => Gls Tokens per Sec:     2024 || Batch Translation Loss:   0.200373 => Txt Tokens per Sec:     5620 || Lr: 0.000100
2024-02-04 06:00:59,065 Epoch 303: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.65 
2024-02-04 06:00:59,066 EPOCH 304
2024-02-04 06:01:04,486 Epoch 304: Total Training Recognition Loss 0.03  Total Training Translation Loss 15.42 
2024-02-04 06:01:04,487 EPOCH 305
2024-02-04 06:01:06,863 [Epoch: 305 Step: 00020400] Batch Recognition Loss:   0.000446 => Gls Tokens per Sec:     2157 || Batch Translation Loss:   0.252773 => Txt Tokens per Sec:     5924 || Lr: 0.000100
2024-02-04 06:01:09,602 Epoch 305: Total Training Recognition Loss 0.03  Total Training Translation Loss 15.46 
2024-02-04 06:01:09,602 EPOCH 306
2024-02-04 06:01:14,669 [Epoch: 306 Step: 00020500] Batch Recognition Loss:   0.000484 => Gls Tokens per Sec:     2036 || Batch Translation Loss:   0.316393 => Txt Tokens per Sec:     5634 || Lr: 0.000100
2024-02-04 06:01:14,800 Epoch 306: Total Training Recognition Loss 0.03  Total Training Translation Loss 12.30 
2024-02-04 06:01:14,801 EPOCH 307
2024-02-04 06:01:19,934 Epoch 307: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.43 
2024-02-04 06:01:19,934 EPOCH 308
2024-02-04 06:01:22,571 [Epoch: 308 Step: 00020600] Batch Recognition Loss:   0.000203 => Gls Tokens per Sec:     1882 || Batch Translation Loss:   0.067129 => Txt Tokens per Sec:     5354 || Lr: 0.000100
2024-02-04 06:01:25,463 Epoch 308: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.53 
2024-02-04 06:01:25,464 EPOCH 309
2024-02-04 06:01:30,586 [Epoch: 309 Step: 00020700] Batch Recognition Loss:   0.000298 => Gls Tokens per Sec:     1982 || Batch Translation Loss:   0.041018 => Txt Tokens per Sec:     5515 || Lr: 0.000100
2024-02-04 06:01:30,840 Epoch 309: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.32 
2024-02-04 06:01:30,840 EPOCH 310
2024-02-04 06:01:36,096 Epoch 310: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.27 
2024-02-04 06:01:36,096 EPOCH 311
2024-02-04 06:01:38,665 [Epoch: 311 Step: 00020800] Batch Recognition Loss:   0.000289 => Gls Tokens per Sec:     1870 || Batch Translation Loss:   0.071636 => Txt Tokens per Sec:     5261 || Lr: 0.000100
2024-02-04 06:01:41,696 Epoch 311: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.98 
2024-02-04 06:01:41,696 EPOCH 312
2024-02-04 06:01:46,783 [Epoch: 312 Step: 00020900] Batch Recognition Loss:   0.000226 => Gls Tokens per Sec:     1964 || Batch Translation Loss:   0.040756 => Txt Tokens per Sec:     5473 || Lr: 0.000100
2024-02-04 06:01:47,004 Epoch 312: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.86 
2024-02-04 06:01:47,004 EPOCH 313
2024-02-04 06:01:52,490 Epoch 313: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.01 
2024-02-04 06:01:52,490 EPOCH 314
2024-02-04 06:01:54,756 [Epoch: 314 Step: 00021000] Batch Recognition Loss:   0.000229 => Gls Tokens per Sec:     2049 || Batch Translation Loss:   0.044661 => Txt Tokens per Sec:     5629 || Lr: 0.000100
2024-02-04 06:01:57,768 Epoch 314: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.48 
2024-02-04 06:01:57,769 EPOCH 315
2024-02-04 06:02:02,798 [Epoch: 315 Step: 00021100] Batch Recognition Loss:   0.000728 => Gls Tokens per Sec:     1955 || Batch Translation Loss:   0.101296 => Txt Tokens per Sec:     5473 || Lr: 0.000100
2024-02-04 06:02:03,097 Epoch 315: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.82 
2024-02-04 06:02:03,097 EPOCH 316
2024-02-04 06:02:08,399 Epoch 316: Total Training Recognition Loss 0.02  Total Training Translation Loss 10.32 
2024-02-04 06:02:08,400 EPOCH 317
2024-02-04 06:02:10,532 [Epoch: 317 Step: 00021200] Batch Recognition Loss:   0.000432 => Gls Tokens per Sec:     2059 || Batch Translation Loss:   0.209921 => Txt Tokens per Sec:     5864 || Lr: 0.000100
2024-02-04 06:02:13,654 Epoch 317: Total Training Recognition Loss 0.02  Total Training Translation Loss 9.73 
2024-02-04 06:02:13,654 EPOCH 318
2024-02-04 06:02:18,851 [Epoch: 318 Step: 00021300] Batch Recognition Loss:   0.001223 => Gls Tokens per Sec:     1862 || Batch Translation Loss:   0.037213 => Txt Tokens per Sec:     5237 || Lr: 0.000100
2024-02-04 06:02:19,247 Epoch 318: Total Training Recognition Loss 0.03  Total Training Translation Loss 11.07 
2024-02-04 06:02:19,248 EPOCH 319
2024-02-04 06:02:24,538 Epoch 319: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.37 
2024-02-04 06:02:24,538 EPOCH 320
2024-02-04 06:02:26,741 [Epoch: 320 Step: 00021400] Batch Recognition Loss:   0.000315 => Gls Tokens per Sec:     1921 || Batch Translation Loss:   0.083148 => Txt Tokens per Sec:     5322 || Lr: 0.000100
2024-02-04 06:02:29,771 Epoch 320: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.23 
2024-02-04 06:02:29,771 EPOCH 321
2024-02-04 06:02:34,709 [Epoch: 321 Step: 00021500] Batch Recognition Loss:   0.000298 => Gls Tokens per Sec:     1927 || Batch Translation Loss:   0.057418 => Txt Tokens per Sec:     5383 || Lr: 0.000100
2024-02-04 06:02:35,210 Epoch 321: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.14 
2024-02-04 06:02:35,210 EPOCH 322
2024-02-04 06:02:40,574 Epoch 322: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.93 
2024-02-04 06:02:40,575 EPOCH 323
2024-02-04 06:02:42,550 [Epoch: 323 Step: 00021600] Batch Recognition Loss:   0.000280 => Gls Tokens per Sec:     2107 || Batch Translation Loss:   0.032377 => Txt Tokens per Sec:     5905 || Lr: 0.000100
2024-02-04 06:02:45,685 Epoch 323: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.49 
2024-02-04 06:02:45,685 EPOCH 324
2024-02-04 06:02:49,817 [Epoch: 324 Step: 00021700] Batch Recognition Loss:   0.000263 => Gls Tokens per Sec:     2263 || Batch Translation Loss:   0.039420 => Txt Tokens per Sec:     6314 || Lr: 0.000100
2024-02-04 06:02:50,298 Epoch 324: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.37 
2024-02-04 06:02:50,298 EPOCH 325
2024-02-04 06:02:55,016 Epoch 325: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.18 
2024-02-04 06:02:55,016 EPOCH 326
2024-02-04 06:02:57,224 [Epoch: 326 Step: 00021800] Batch Recognition Loss:   0.000153 => Gls Tokens per Sec:     1813 || Batch Translation Loss:   0.041521 => Txt Tokens per Sec:     5121 || Lr: 0.000100
2024-02-04 06:03:00,587 Epoch 326: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.67 
2024-02-04 06:03:00,587 EPOCH 327
2024-02-04 06:03:05,342 [Epoch: 327 Step: 00021900] Batch Recognition Loss:   0.000291 => Gls Tokens per Sec:     1933 || Batch Translation Loss:   0.060100 => Txt Tokens per Sec:     5389 || Lr: 0.000100
2024-02-04 06:03:06,046 Epoch 327: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.11 
2024-02-04 06:03:06,046 EPOCH 328
2024-02-04 06:03:11,116 Epoch 328: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.76 
2024-02-04 06:03:11,116 EPOCH 329
2024-02-04 06:03:12,733 [Epoch: 329 Step: 00022000] Batch Recognition Loss:   0.000306 => Gls Tokens per Sec:     2321 || Batch Translation Loss:   0.034656 => Txt Tokens per Sec:     6352 || Lr: 0.000100
2024-02-04 06:03:21,620 Validation result at epoch 329, step    22000: duration: 8.8864s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00146	Translation Loss: 93459.41406	PPL: 11527.47461
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.68	(BLEU-1: 10.44,	BLEU-2: 3.46,	BLEU-3: 1.45,	BLEU-4: 0.68)
	CHRF 17.06	ROUGE 8.98
2024-02-04 06:03:21,621 Logging Recognition and Translation Outputs
2024-02-04 06:03:21,621 ========================================================================================================================
2024-02-04 06:03:21,621 Logging Sequence: 146_56.00
2024-02-04 06:03:21,621 	Gloss Reference :	A B+C+D+E
2024-02-04 06:03:21,621 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:03:21,621 	Gloss Alignment :	         
2024-02-04 06:03:21,621 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:03:21,623 	Text Reference  :	when the players go back to the hotel  as   per rules     all of   them have     to      undergo rtpcr test  for covid-19 everyday
2024-02-04 06:03:21,624 	Text Hypothesis :	**** *** ******* ** **** ** the indian team is  extremely fit when a    champion players for     the   death of  fans     everyday
2024-02-04 06:03:21,624 	Text Alignment  :	D    D   D       D  D    D      S      S    S   S         S   S    S    S        S       S       S     S     S   S                
2024-02-04 06:03:21,624 ========================================================================================================================
2024-02-04 06:03:21,624 Logging Sequence: 118_338.00
2024-02-04 06:03:21,624 	Gloss Reference :	A B+C+D+E
2024-02-04 06:03:21,624 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:03:21,624 	Gloss Alignment :	         
2024-02-04 06:03:21,624 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:03:21,625 	Text Reference  :	** *** ****** this   is   why   even        messi wore it    
2024-02-04 06:03:21,625 	Text Hypothesis :	as the entire nation were still speculating on    the  matter
2024-02-04 06:03:21,625 	Text Alignment  :	I  I   I      S      S    S     S           S     S    S     
2024-02-04 06:03:21,625 ========================================================================================================================
2024-02-04 06:03:21,625 Logging Sequence: 66_61.00
2024-02-04 06:03:21,626 	Gloss Reference :	A B+C+D+E
2024-02-04 06:03:21,626 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:03:21,626 	Gloss Alignment :	         
2024-02-04 06:03:21,626 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:03:21,627 	Text Reference  :	instead   of     returning back to  his        homeland because of his injury
2024-02-04 06:03:21,627 	Text Hypothesis :	rajasthan royals ben       as   the government did      not     us to  him   
2024-02-04 06:03:21,627 	Text Alignment  :	S         S      S         S    S   S          S        S       S  S   S     
2024-02-04 06:03:21,627 ========================================================================================================================
2024-02-04 06:03:21,627 Logging Sequence: 81_278.00
2024-02-04 06:03:21,627 	Gloss Reference :	A B+C+D+E
2024-02-04 06:03:21,628 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:03:21,628 	Gloss Alignment :	         
2024-02-04 06:03:21,628 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:03:21,630 	Text Reference  :	*** ******* ***** of this amrapali group paid rs 3570 crore the remaining rs 652 crore was paid by amrapali sapphire developers a ******** subsidiary of amrapali group   
2024-02-04 06:03:21,630 	Text Hypothesis :	the supreme court of **** amrapali group **** ** **** ***** *** ********* ** *** ***** *** paid ** ******** ******** ********** a forensic audit      of amrapali builders
2024-02-04 06:03:21,630 	Text Alignment  :	I   I       I        D                   D    D  D    D     D   D         D  D   D     D        D  D        D        D            I        S                      S       
2024-02-04 06:03:21,630 ========================================================================================================================
2024-02-04 06:03:21,630 Logging Sequence: 162_125.00
2024-02-04 06:03:21,630 	Gloss Reference :	A B+C+D+E
2024-02-04 06:03:21,630 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:03:21,630 	Gloss Alignment :	         
2024-02-04 06:03:21,631 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:03:21,632 	Text Reference  :	******* *** **** ****** *** ******* in        response to  this       kohli received many    hate     comments on social media
2024-02-04 06:03:21,632 	Text Hypothesis :	ronaldo has also become the penalty shoot-out a        sex trafficker so    he       praised neeraj's throw    at the    car  
2024-02-04 06:03:21,632 	Text Alignment  :	I       I   I    I      I   I       S         S        S   S          S     S        S       S        S        S  S      S    
2024-02-04 06:03:21,632 ========================================================================================================================
2024-02-04 06:03:25,243 Epoch 329: Total Training Recognition Loss 0.02  Total Training Translation Loss 8.81 
2024-02-04 06:03:25,244 EPOCH 330
2024-02-04 06:03:30,141 [Epoch: 330 Step: 00022100] Batch Recognition Loss:   0.000324 => Gls Tokens per Sec:     1845 || Batch Translation Loss:   0.097623 => Txt Tokens per Sec:     5126 || Lr: 0.000100
2024-02-04 06:03:30,854 Epoch 330: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.57 
2024-02-04 06:03:30,855 EPOCH 331
2024-02-04 06:03:36,146 Epoch 331: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.24 
2024-02-04 06:03:36,147 EPOCH 332
2024-02-04 06:03:37,784 [Epoch: 332 Step: 00022200] Batch Recognition Loss:   0.000505 => Gls Tokens per Sec:     2248 || Batch Translation Loss:   0.079901 => Txt Tokens per Sec:     6071 || Lr: 0.000100
2024-02-04 06:03:41,319 Epoch 332: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.54 
2024-02-04 06:03:41,319 EPOCH 333
2024-02-04 06:03:46,180 [Epoch: 333 Step: 00022300] Batch Recognition Loss:   0.000319 => Gls Tokens per Sec:     1825 || Batch Translation Loss:   0.090670 => Txt Tokens per Sec:     5185 || Lr: 0.000100
2024-02-04 06:03:46,894 Epoch 333: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.49 
2024-02-04 06:03:46,895 EPOCH 334
2024-02-04 06:03:52,232 Epoch 334: Total Training Recognition Loss 0.03  Total Training Translation Loss 12.80 
2024-02-04 06:03:52,232 EPOCH 335
2024-02-04 06:03:54,206 [Epoch: 335 Step: 00022400] Batch Recognition Loss:   0.000217 => Gls Tokens per Sec:     1738 || Batch Translation Loss:   0.075777 => Txt Tokens per Sec:     4632 || Lr: 0.000100
2024-02-04 06:03:57,862 Epoch 335: Total Training Recognition Loss 0.02  Total Training Translation Loss 8.25 
2024-02-04 06:03:57,862 EPOCH 336
2024-02-04 06:04:02,443 [Epoch: 336 Step: 00022500] Batch Recognition Loss:   0.000529 => Gls Tokens per Sec:     1901 || Batch Translation Loss:   0.039282 => Txt Tokens per Sec:     5298 || Lr: 0.000100
2024-02-04 06:04:03,411 Epoch 336: Total Training Recognition Loss 0.04  Total Training Translation Loss 5.15 
2024-02-04 06:04:03,412 EPOCH 337
2024-02-04 06:04:08,670 Epoch 337: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.61 
2024-02-04 06:04:08,670 EPOCH 338
2024-02-04 06:04:10,246 [Epoch: 338 Step: 00022600] Batch Recognition Loss:   0.000279 => Gls Tokens per Sec:     2134 || Batch Translation Loss:   0.051734 => Txt Tokens per Sec:     6318 || Lr: 0.000100
2024-02-04 06:04:13,751 Epoch 338: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.60 
2024-02-04 06:04:13,752 EPOCH 339
2024-02-04 06:04:18,149 [Epoch: 339 Step: 00022700] Batch Recognition Loss:   0.000140 => Gls Tokens per Sec:     1965 || Batch Translation Loss:   0.042010 => Txt Tokens per Sec:     5496 || Lr: 0.000100
2024-02-04 06:04:19,143 Epoch 339: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.00 
2024-02-04 06:04:19,144 EPOCH 340
2024-02-04 06:04:24,651 Epoch 340: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.77 
2024-02-04 06:04:24,651 EPOCH 341
2024-02-04 06:04:26,313 [Epoch: 341 Step: 00022800] Batch Recognition Loss:   0.000428 => Gls Tokens per Sec:     1926 || Batch Translation Loss:   0.063805 => Txt Tokens per Sec:     5800 || Lr: 0.000100
2024-02-04 06:04:29,618 Epoch 341: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.90 
2024-02-04 06:04:29,618 EPOCH 342
2024-02-04 06:04:34,046 [Epoch: 342 Step: 00022900] Batch Recognition Loss:   0.000225 => Gls Tokens per Sec:     1896 || Batch Translation Loss:   0.040362 => Txt Tokens per Sec:     5243 || Lr: 0.000100
2024-02-04 06:04:35,131 Epoch 342: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.54 
2024-02-04 06:04:35,131 EPOCH 343
2024-02-04 06:04:40,312 Epoch 343: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.57 
2024-02-04 06:04:40,313 EPOCH 344
2024-02-04 06:04:41,838 [Epoch: 344 Step: 00023000] Batch Recognition Loss:   0.000366 => Gls Tokens per Sec:     1934 || Batch Translation Loss:   0.186269 => Txt Tokens per Sec:     5188 || Lr: 0.000100
2024-02-04 06:04:45,746 Epoch 344: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.29 
2024-02-04 06:04:45,746 EPOCH 345
2024-02-04 06:04:49,967 [Epoch: 345 Step: 00023100] Batch Recognition Loss:   0.000923 => Gls Tokens per Sec:     1951 || Batch Translation Loss:   0.081743 => Txt Tokens per Sec:     5359 || Lr: 0.000100
2024-02-04 06:04:51,158 Epoch 345: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.00 
2024-02-04 06:04:51,159 EPOCH 346
2024-02-04 06:04:56,006 Epoch 346: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.52 
2024-02-04 06:04:56,006 EPOCH 347
2024-02-04 06:04:57,601 [Epoch: 347 Step: 00023200] Batch Recognition Loss:   0.000523 => Gls Tokens per Sec:     1808 || Batch Translation Loss:   0.608555 => Txt Tokens per Sec:     5105 || Lr: 0.000100
2024-02-04 06:05:01,535 Epoch 347: Total Training Recognition Loss 0.02  Total Training Translation Loss 9.05 
2024-02-04 06:05:01,535 EPOCH 348
2024-02-04 06:05:05,623 [Epoch: 348 Step: 00023300] Batch Recognition Loss:   0.000394 => Gls Tokens per Sec:     1975 || Batch Translation Loss:   0.109943 => Txt Tokens per Sec:     5537 || Lr: 0.000100
2024-02-04 06:05:06,871 Epoch 348: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.97 
2024-02-04 06:05:06,872 EPOCH 349
2024-02-04 06:05:12,218 Epoch 349: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.63 
2024-02-04 06:05:12,218 EPOCH 350
2024-02-04 06:05:13,424 [Epoch: 350 Step: 00023400] Batch Recognition Loss:   0.000490 => Gls Tokens per Sec:     2258 || Batch Translation Loss:   0.023001 => Txt Tokens per Sec:     5995 || Lr: 0.000100
2024-02-04 06:05:17,542 Epoch 350: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.51 
2024-02-04 06:05:17,543 EPOCH 351
2024-02-04 06:05:21,106 [Epoch: 351 Step: 00023500] Batch Recognition Loss:   0.000339 => Gls Tokens per Sec:     2246 || Batch Translation Loss:   0.152642 => Txt Tokens per Sec:     6171 || Lr: 0.000100
2024-02-04 06:05:22,821 Epoch 351: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.14 
2024-02-04 06:05:22,821 EPOCH 352
2024-02-04 06:05:28,133 Epoch 352: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.07 
2024-02-04 06:05:28,133 EPOCH 353
2024-02-04 06:05:29,185 [Epoch: 353 Step: 00023600] Batch Recognition Loss:   0.000266 => Gls Tokens per Sec:     2436 || Batch Translation Loss:   0.073136 => Txt Tokens per Sec:     6788 || Lr: 0.000100
2024-02-04 06:05:33,556 Epoch 353: Total Training Recognition Loss 0.02  Total Training Translation Loss 9.27 
2024-02-04 06:05:33,557 EPOCH 354
2024-02-04 06:05:37,447 [Epoch: 354 Step: 00023700] Batch Recognition Loss:   0.000159 => Gls Tokens per Sec:     2016 || Batch Translation Loss:   0.076229 => Txt Tokens per Sec:     5600 || Lr: 0.000100
2024-02-04 06:05:38,845 Epoch 354: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.90 
2024-02-04 06:05:38,845 EPOCH 355
2024-02-04 06:05:44,162 Epoch 355: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.25 
2024-02-04 06:05:44,162 EPOCH 356
2024-02-04 06:05:45,186 [Epoch: 356 Step: 00023800] Batch Recognition Loss:   0.000369 => Gls Tokens per Sec:     2345 || Batch Translation Loss:   0.092213 => Txt Tokens per Sec:     6768 || Lr: 0.000100
2024-02-04 06:05:49,344 Epoch 356: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.60 
2024-02-04 06:05:49,344 EPOCH 357
2024-02-04 06:05:53,101 [Epoch: 357 Step: 00023900] Batch Recognition Loss:   0.000264 => Gls Tokens per Sec:     2021 || Batch Translation Loss:   0.041411 => Txt Tokens per Sec:     5590 || Lr: 0.000100
2024-02-04 06:05:54,569 Epoch 357: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.56 
2024-02-04 06:05:54,569 EPOCH 358
2024-02-04 06:06:00,140 Epoch 358: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.38 
2024-02-04 06:06:00,140 EPOCH 359
2024-02-04 06:06:01,039 [Epoch: 359 Step: 00024000] Batch Recognition Loss:   0.000185 => Gls Tokens per Sec:     2494 || Batch Translation Loss:   0.062867 => Txt Tokens per Sec:     6618 || Lr: 0.000100
2024-02-04 06:06:09,409 Validation result at epoch 359, step    24000: duration: 8.3701s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00164	Translation Loss: 94482.25000	PPL: 12769.86719
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.91	(BLEU-1: 9.74,	BLEU-2: 3.38,	BLEU-3: 1.64,	BLEU-4: 0.91)
	CHRF 16.61	ROUGE 8.79
2024-02-04 06:06:09,410 Logging Recognition and Translation Outputs
2024-02-04 06:06:09,410 ========================================================================================================================
2024-02-04 06:06:09,410 Logging Sequence: 169_165.00
2024-02-04 06:06:09,410 	Gloss Reference :	A B+C+D+E
2024-02-04 06:06:09,411 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:06:09,411 	Gloss Alignment :	         
2024-02-04 06:06:09,411 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:06:09,412 	Text Reference  :	the indian government was outraged by      the incident and these  changes were undone  by  wikipedia
2024-02-04 06:06:09,412 	Text Hypothesis :	*** ****** he         was badly    trolled and mocked   on  social media   for  missing the catch    
2024-02-04 06:06:09,412 	Text Alignment  :	D   D      S              S        S       S   S        S   S      S       S    S       S   S        
2024-02-04 06:06:09,412 ========================================================================================================================
2024-02-04 06:06:09,412 Logging Sequence: 175_60.00
2024-02-04 06:06:09,413 	Gloss Reference :	A B+C+D+E
2024-02-04 06:06:09,413 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:06:09,413 	Gloss Alignment :	         
2024-02-04 06:06:09,413 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:06:09,414 	Text Reference  :	******* **** **** ******* ** *** *** that is how   india bagged 9  medals in  the  youth tournament
2024-02-04 06:06:09,414 	Text Hypothesis :	indians were very excited to see him win  a  world cup   match  as he     has been a     viral     
2024-02-04 06:06:09,414 	Text Alignment  :	I       I    I    I       I  I   I   S    S  S     S     S      S  S      S   S    S     S         
2024-02-04 06:06:09,415 ========================================================================================================================
2024-02-04 06:06:09,415 Logging Sequence: 61_255.00
2024-02-04 06:06:09,415 	Gloss Reference :	A B+C+D+E
2024-02-04 06:06:09,415 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:06:09,415 	Gloss Alignment :	         
2024-02-04 06:06:09,415 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:06:09,416 	Text Reference  :	** ** *** in    2011 we   decided to   marry and  informed our families
2024-02-04 06:06:09,416 	Text Hypothesis :	it is not known how  much longer  have a     part of       the country 
2024-02-04 06:06:09,416 	Text Alignment  :	I  I  I   S     S    S    S       S    S     S    S        S   S       
2024-02-04 06:06:09,416 ========================================================================================================================
2024-02-04 06:06:09,417 Logging Sequence: 173_39.00
2024-02-04 06:06:09,417 	Gloss Reference :	A B+C+D+E
2024-02-04 06:06:09,417 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:06:09,417 	Gloss Alignment :	         
2024-02-04 06:06:09,417 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:06:09,418 	Text Reference  :	*** **** **** kohli will    step down as  india' captain  
2024-02-04 06:06:09,418 	Text Hypothesis :	csk were just two   tickets to   play the other  celebrity
2024-02-04 06:06:09,418 	Text Alignment  :	I   I    I    S     S       S    S    S   S      S        
2024-02-04 06:06:09,418 ========================================================================================================================
2024-02-04 06:06:09,418 Logging Sequence: 172_82.00
2024-02-04 06:06:09,418 	Gloss Reference :	A B+C+D+E
2024-02-04 06:06:09,418 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:06:09,419 	Gloss Alignment :	         
2024-02-04 06:06:09,419 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:06:09,421 	Text Reference  :	you all know that    the toss was about to *** start   at  700 pm    but        it started raining     at       around 630    pm  
2024-02-04 06:06:09,421 	Text Hypothesis :	*** *** **** however icc did  not agree to the demands the two sides eventually on their   differences allowing the    afghan ball
2024-02-04 06:06:09,421 	Text Alignment  :	D   D   D    S       S   S    S   S        I   S       S   S   S     S          S  S       S           S        S      S      S   
2024-02-04 06:06:09,421 ========================================================================================================================
2024-02-04 06:06:13,904 Epoch 359: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.23 
2024-02-04 06:06:13,904 EPOCH 360
2024-02-04 06:06:17,688 [Epoch: 360 Step: 00024100] Batch Recognition Loss:   0.000530 => Gls Tokens per Sec:     1965 || Batch Translation Loss:   0.106605 => Txt Tokens per Sec:     5329 || Lr: 0.000100
2024-02-04 06:06:19,365 Epoch 360: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.50 
2024-02-04 06:06:19,365 EPOCH 361
2024-02-04 06:06:24,621 Epoch 361: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.61 
2024-02-04 06:06:24,621 EPOCH 362
2024-02-04 06:06:25,496 [Epoch: 362 Step: 00024200] Batch Recognition Loss:   0.000213 => Gls Tokens per Sec:     2381 || Batch Translation Loss:   0.155021 => Txt Tokens per Sec:     6182 || Lr: 0.000100
2024-02-04 06:06:29,825 Epoch 362: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.99 
2024-02-04 06:06:29,825 EPOCH 363
2024-02-04 06:06:33,835 [Epoch: 363 Step: 00024300] Batch Recognition Loss:   0.000234 => Gls Tokens per Sec:     1814 || Batch Translation Loss:   0.199070 => Txt Tokens per Sec:     5155 || Lr: 0.000100
2024-02-04 06:06:35,379 Epoch 363: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.90 
2024-02-04 06:06:35,380 EPOCH 364
2024-02-04 06:06:40,125 Epoch 364: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.80 
2024-02-04 06:06:40,125 EPOCH 365
2024-02-04 06:06:41,102 [Epoch: 365 Step: 00024400] Batch Recognition Loss:   0.000319 => Gls Tokens per Sec:     1970 || Batch Translation Loss:   0.116985 => Txt Tokens per Sec:     5730 || Lr: 0.000100
2024-02-04 06:06:45,713 Epoch 365: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.05 
2024-02-04 06:06:45,714 EPOCH 366
2024-02-04 06:06:49,264 [Epoch: 366 Step: 00024500] Batch Recognition Loss:   0.000427 => Gls Tokens per Sec:     2003 || Batch Translation Loss:   0.283204 => Txt Tokens per Sec:     5668 || Lr: 0.000100
2024-02-04 06:06:50,964 Epoch 366: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.53 
2024-02-04 06:06:50,965 EPOCH 367
2024-02-04 06:06:56,366 Epoch 367: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.12 
2024-02-04 06:06:56,367 EPOCH 368
2024-02-04 06:06:57,177 [Epoch: 368 Step: 00024600] Batch Recognition Loss:   0.000797 => Gls Tokens per Sec:     2066 || Batch Translation Loss:   0.095769 => Txt Tokens per Sec:     5645 || Lr: 0.000100
2024-02-04 06:07:01,786 Epoch 368: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.53 
2024-02-04 06:07:01,787 EPOCH 369
2024-02-04 06:07:05,423 [Epoch: 369 Step: 00024700] Batch Recognition Loss:   0.000211 => Gls Tokens per Sec:     1912 || Batch Translation Loss:   0.052615 => Txt Tokens per Sec:     5362 || Lr: 0.000100
2024-02-04 06:07:07,332 Epoch 369: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.85 
2024-02-04 06:07:07,333 EPOCH 370
2024-02-04 06:07:12,765 Epoch 370: Total Training Recognition Loss 0.03  Total Training Translation Loss 14.48 
2024-02-04 06:07:12,766 EPOCH 371
2024-02-04 06:07:13,632 [Epoch: 371 Step: 00024800] Batch Recognition Loss:   0.000429 => Gls Tokens per Sec:     1849 || Batch Translation Loss:   0.232463 => Txt Tokens per Sec:     5551 || Lr: 0.000100
2024-02-04 06:07:18,241 Epoch 371: Total Training Recognition Loss 0.04  Total Training Translation Loss 15.30 
2024-02-04 06:07:18,241 EPOCH 372
2024-02-04 06:07:21,571 [Epoch: 372 Step: 00024900] Batch Recognition Loss:   0.000255 => Gls Tokens per Sec:     2040 || Batch Translation Loss:   0.050179 => Txt Tokens per Sec:     5760 || Lr: 0.000100
2024-02-04 06:07:23,478 Epoch 372: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.65 
2024-02-04 06:07:23,478 EPOCH 373
2024-02-04 06:07:29,040 Epoch 373: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.16 
2024-02-04 06:07:29,040 EPOCH 374
2024-02-04 06:07:29,694 [Epoch: 374 Step: 00025000] Batch Recognition Loss:   0.000191 => Gls Tokens per Sec:     2204 || Batch Translation Loss:   0.130840 => Txt Tokens per Sec:     6225 || Lr: 0.000100
2024-02-04 06:07:33,939 Epoch 374: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.57 
2024-02-04 06:07:33,939 EPOCH 375
2024-02-04 06:07:37,446 [Epoch: 375 Step: 00025100] Batch Recognition Loss:   0.000169 => Gls Tokens per Sec:     1891 || Batch Translation Loss:   0.065394 => Txt Tokens per Sec:     5302 || Lr: 0.000100
2024-02-04 06:07:39,529 Epoch 375: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.92 
2024-02-04 06:07:39,530 EPOCH 376
2024-02-04 06:07:44,251 Epoch 376: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.13 
2024-02-04 06:07:44,251 EPOCH 377
2024-02-04 06:07:44,775 [Epoch: 377 Step: 00025200] Batch Recognition Loss:   0.000214 => Gls Tokens per Sec:     2449 || Batch Translation Loss:   0.041469 => Txt Tokens per Sec:     6364 || Lr: 0.000100
2024-02-04 06:07:49,723 Epoch 377: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.26 
2024-02-04 06:07:49,724 EPOCH 378
2024-02-04 06:07:52,661 [Epoch: 378 Step: 00025300] Batch Recognition Loss:   0.000227 => Gls Tokens per Sec:     2235 || Batch Translation Loss:   0.047839 => Txt Tokens per Sec:     6113 || Lr: 0.000100
2024-02-04 06:07:54,708 Epoch 378: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.09 
2024-02-04 06:07:54,708 EPOCH 379
2024-02-04 06:08:00,160 Epoch 379: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.22 
2024-02-04 06:08:00,160 EPOCH 380
2024-02-04 06:08:00,705 [Epoch: 380 Step: 00025400] Batch Recognition Loss:   0.000297 => Gls Tokens per Sec:     2059 || Batch Translation Loss:   0.031708 => Txt Tokens per Sec:     6075 || Lr: 0.000100
2024-02-04 06:08:05,525 Epoch 380: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.95 
2024-02-04 06:08:05,526 EPOCH 381
2024-02-04 06:08:08,846 [Epoch: 381 Step: 00025500] Batch Recognition Loss:   0.000360 => Gls Tokens per Sec:     1902 || Batch Translation Loss:   0.099691 => Txt Tokens per Sec:     5290 || Lr: 0.000100
2024-02-04 06:08:10,793 Epoch 381: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.79 
2024-02-04 06:08:10,793 EPOCH 382
2024-02-04 06:08:16,048 Epoch 382: Total Training Recognition Loss 0.02  Total Training Translation Loss 8.24 
2024-02-04 06:08:16,048 EPOCH 383
2024-02-04 06:08:16,486 [Epoch: 383 Step: 00025600] Batch Recognition Loss:   0.000266 => Gls Tokens per Sec:     2197 || Batch Translation Loss:   0.699245 => Txt Tokens per Sec:     5650 || Lr: 0.000100
2024-02-04 06:08:21,339 Epoch 383: Total Training Recognition Loss 0.03  Total Training Translation Loss 12.78 
2024-02-04 06:08:21,340 EPOCH 384
2024-02-04 06:08:24,588 [Epoch: 384 Step: 00025700] Batch Recognition Loss:   0.000512 => Gls Tokens per Sec:     1922 || Batch Translation Loss:   0.411978 => Txt Tokens per Sec:     5424 || Lr: 0.000100
2024-02-04 06:08:26,773 Epoch 384: Total Training Recognition Loss 0.02  Total Training Translation Loss 8.10 
2024-02-04 06:08:26,774 EPOCH 385
2024-02-04 06:08:32,038 Epoch 385: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.39 
2024-02-04 06:08:32,039 EPOCH 386
2024-02-04 06:08:32,518 [Epoch: 386 Step: 00025800] Batch Recognition Loss:   0.000498 => Gls Tokens per Sec:     1674 || Batch Translation Loss:   0.074739 => Txt Tokens per Sec:     4411 || Lr: 0.000100
2024-02-04 06:08:37,454 Epoch 386: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.24 
2024-02-04 06:08:37,454 EPOCH 387
2024-02-04 06:08:40,305 [Epoch: 387 Step: 00025900] Batch Recognition Loss:   0.000762 => Gls Tokens per Sec:     2101 || Batch Translation Loss:   0.065077 => Txt Tokens per Sec:     6023 || Lr: 0.000100
2024-02-04 06:08:42,655 Epoch 387: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.96 
2024-02-04 06:08:42,655 EPOCH 388
2024-02-04 06:08:47,886 Epoch 388: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.55 
2024-02-04 06:08:47,887 EPOCH 389
2024-02-04 06:08:48,277 [Epoch: 389 Step: 00026000] Batch Recognition Loss:   0.000847 => Gls Tokens per Sec:     1641 || Batch Translation Loss:   0.020026 => Txt Tokens per Sec:     4805 || Lr: 0.000100
2024-02-04 06:08:56,774 Validation result at epoch 389, step    26000: duration: 8.4966s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00145	Translation Loss: 95236.20312	PPL: 13770.60449
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.83	(BLEU-1: 10.03,	BLEU-2: 3.44,	BLEU-3: 1.60,	BLEU-4: 0.83)
	CHRF 16.72	ROUGE 8.70
2024-02-04 06:08:56,776 Logging Recognition and Translation Outputs
2024-02-04 06:08:56,776 ========================================================================================================================
2024-02-04 06:08:56,776 Logging Sequence: 130_139.00
2024-02-04 06:08:56,777 	Gloss Reference :	A B+C+D+E
2024-02-04 06:08:56,777 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:08:56,777 	Gloss Alignment :	         
2024-02-04 06:08:56,777 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:08:56,780 	Text Reference  :	he shared a picture of  a little pouch he knit for his olympic gold   medal with uk flag on  one  side   and **** japanese flag on      the     other
2024-02-04 06:08:56,780 	Text Hypothesis :	he ****** * ******* won a ****** ***** ** **** *** *** ******* bronze medal **** ** at   the 2012 london and 2016 rio      de   janeiro olympic games
2024-02-04 06:08:56,780 	Text Alignment  :	   D      D D       S     D      D     D  D    D   D   D       S            D    D  S    S   S    S          I    S        S    S       S       S    
2024-02-04 06:08:56,780 ========================================================================================================================
2024-02-04 06:08:56,780 Logging Sequence: 72_194.00
2024-02-04 06:08:56,781 	Gloss Reference :	A B+C+D+E
2024-02-04 06:08:56,781 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:08:56,781 	Gloss Alignment :	         
2024-02-04 06:08:56,781 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:08:56,782 	Text Reference  :	shah told her to do what she wants and filed a  police complaint against her  
2024-02-04 06:08:56,782 	Text Hypothesis :	**** **** *** ** ** **** one of    the video of people are       going   viral
2024-02-04 06:08:56,782 	Text Alignment  :	D    D    D   D  D  D    S   S     S   S     S  S      S         S       S    
2024-02-04 06:08:56,782 ========================================================================================================================
2024-02-04 06:08:56,783 Logging Sequence: 69_177.00
2024-02-04 06:08:56,783 	Gloss Reference :	A B+C+D+E
2024-02-04 06:08:56,783 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:08:56,783 	Gloss Alignment :	         
2024-02-04 06:08:56,783 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:08:56,786 	Text Reference  :	he   said 'i   will    continue playing i   know it's about time i retire i      also have   a    knee condition
2024-02-04 06:08:56,786 	Text Hypothesis :	when csk  were waiting to       see     him in   the  first time * ****** people were elated with the  win      
2024-02-04 06:08:56,786 	Text Alignment  :	S    S    S    S       S        S       S   S    S    S          D D      S      S    S      S    S    S        
2024-02-04 06:08:56,786 ========================================================================================================================
2024-02-04 06:08:56,786 Logging Sequence: 95_118.00
2024-02-04 06:08:56,786 	Gloss Reference :	A B+C+D+E
2024-02-04 06:08:56,787 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:08:56,787 	Gloss Alignment :	         
2024-02-04 06:08:56,787 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:08:56,788 	Text Reference  :	**** **** **** ***** *** ****** the game  was stopped strangely due to ** ******* excessive sunlight
2024-02-04 06:08:56,788 	Text Hypothesis :	they were tire marks all around the pitch and the     play      had to be stopped how       crazy   
2024-02-04 06:08:56,788 	Text Alignment  :	I    I    I    I     I   I          S     S   S       S         S      I  I       S         S       
2024-02-04 06:08:56,789 ========================================================================================================================
2024-02-04 06:08:56,789 Logging Sequence: 112_8.00
2024-02-04 06:08:56,789 	Gloss Reference :	A B+C+D+E
2024-02-04 06:08:56,789 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:08:56,789 	Gloss Alignment :	         
2024-02-04 06:08:56,789 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:08:56,792 	Text Reference  :	before there were 8 teams such as    mumbai indians delhi capitals punjab kings etc     and now   there will be  10 teams in 2022
2024-02-04 06:08:56,792 	Text Hypothesis :	****** ***** **** * you   can  check out    of      1     month    yes    ipl   matches has never been  made the 6  teams ** ****
2024-02-04 06:08:56,792 	Text Alignment  :	D      D     D    D S     S    S     S      S       S     S        S      S     S       S   S     S     S    S   S        D  D   
2024-02-04 06:08:56,793 ========================================================================================================================
2024-02-04 06:09:01,866 Epoch 389: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.26 
2024-02-04 06:09:01,867 EPOCH 390
2024-02-04 06:09:04,695 [Epoch: 390 Step: 00026100] Batch Recognition Loss:   0.000212 => Gls Tokens per Sec:     2094 || Batch Translation Loss:   0.173120 => Txt Tokens per Sec:     6005 || Lr: 0.000100
2024-02-04 06:09:07,173 Epoch 390: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.51 
2024-02-04 06:09:07,174 EPOCH 391
2024-02-04 06:09:12,617 Epoch 391: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.33 
2024-02-04 06:09:12,618 EPOCH 392
2024-02-04 06:09:12,782 [Epoch: 392 Step: 00026200] Batch Recognition Loss:   0.000187 => Gls Tokens per Sec:     2927 || Batch Translation Loss:   0.020227 => Txt Tokens per Sec:     7299 || Lr: 0.000100
2024-02-04 06:09:17,477 Epoch 392: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.94 
2024-02-04 06:09:17,478 EPOCH 393
2024-02-04 06:09:20,585 [Epoch: 393 Step: 00026300] Batch Recognition Loss:   0.000286 => Gls Tokens per Sec:     1855 || Batch Translation Loss:   0.051734 => Txt Tokens per Sec:     5198 || Lr: 0.000100
2024-02-04 06:09:23,006 Epoch 393: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.86 
2024-02-04 06:09:23,006 EPOCH 394
2024-02-04 06:09:28,211 Epoch 394: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.18 
2024-02-04 06:09:28,212 EPOCH 395
2024-02-04 06:09:28,395 [Epoch: 395 Step: 00026400] Batch Recognition Loss:   0.000155 => Gls Tokens per Sec:     1758 || Batch Translation Loss:   0.030941 => Txt Tokens per Sec:     5577 || Lr: 0.000100
2024-02-04 06:09:33,298 Epoch 395: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.11 
2024-02-04 06:09:33,298 EPOCH 396
2024-02-04 06:09:36,200 [Epoch: 396 Step: 00026500] Batch Recognition Loss:   0.000306 => Gls Tokens per Sec:     1930 || Batch Translation Loss:   0.158217 => Txt Tokens per Sec:     5425 || Lr: 0.000100
2024-02-04 06:09:38,767 Epoch 396: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.13 
2024-02-04 06:09:38,767 EPOCH 397
2024-02-04 06:09:43,995 Epoch 397: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.98 
2024-02-04 06:09:43,996 EPOCH 398
2024-02-04 06:09:44,070 [Epoch: 398 Step: 00026600] Batch Recognition Loss:   0.000198 => Gls Tokens per Sec:     2192 || Batch Translation Loss:   0.121972 => Txt Tokens per Sec:     6068 || Lr: 0.000100
2024-02-04 06:09:49,134 Epoch 398: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.47 
2024-02-04 06:09:49,134 EPOCH 399
2024-02-04 06:09:51,342 [Epoch: 399 Step: 00026700] Batch Recognition Loss:   0.000286 => Gls Tokens per Sec:     2465 || Batch Translation Loss:   0.045800 => Txt Tokens per Sec:     6753 || Lr: 0.000100
2024-02-04 06:09:53,636 Epoch 399: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.02 
2024-02-04 06:09:53,636 EPOCH 400
2024-02-04 06:09:58,884 [Epoch: 400 Step: 00026800] Batch Recognition Loss:   0.000206 => Gls Tokens per Sec:     2026 || Batch Translation Loss:   0.054804 => Txt Tokens per Sec:     5624 || Lr: 0.000100
2024-02-04 06:09:58,885 Epoch 400: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.60 
2024-02-04 06:09:58,885 EPOCH 401
2024-02-04 06:10:04,095 Epoch 401: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.01 
2024-02-04 06:10:04,096 EPOCH 402
2024-02-04 06:10:06,516 [Epoch: 402 Step: 00026900] Batch Recognition Loss:   0.000359 => Gls Tokens per Sec:     2145 || Batch Translation Loss:   0.056309 => Txt Tokens per Sec:     5832 || Lr: 0.000100
2024-02-04 06:10:09,357 Epoch 402: Total Training Recognition Loss 0.02  Total Training Translation Loss 9.54 
2024-02-04 06:10:09,357 EPOCH 403
2024-02-04 06:10:14,305 [Epoch: 403 Step: 00027000] Batch Recognition Loss:   0.000305 => Gls Tokens per Sec:     2117 || Batch Translation Loss:   0.050984 => Txt Tokens per Sec:     5911 || Lr: 0.000100
2024-02-04 06:10:14,346 Epoch 403: Total Training Recognition Loss 0.03  Total Training Translation Loss 11.09 
2024-02-04 06:10:14,347 EPOCH 404
2024-02-04 06:10:18,935 Epoch 404: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.50 
2024-02-04 06:10:18,935 EPOCH 405
2024-02-04 06:10:21,630 [Epoch: 405 Step: 00027100] Batch Recognition Loss:   0.000535 => Gls Tokens per Sec:     1867 || Batch Translation Loss:   0.015723 => Txt Tokens per Sec:     5020 || Lr: 0.000100
2024-02-04 06:10:24,580 Epoch 405: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.73 
2024-02-04 06:10:24,580 EPOCH 406
2024-02-04 06:10:29,714 [Epoch: 406 Step: 00027200] Batch Recognition Loss:   0.000391 => Gls Tokens per Sec:     2026 || Batch Translation Loss:   0.040312 => Txt Tokens per Sec:     5662 || Lr: 0.000100
2024-02-04 06:10:29,900 Epoch 406: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.72 
2024-02-04 06:10:29,900 EPOCH 407
2024-02-04 06:10:35,088 Epoch 407: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.36 
2024-02-04 06:10:35,088 EPOCH 408
2024-02-04 06:10:37,372 [Epoch: 408 Step: 00027300] Batch Recognition Loss:   0.000293 => Gls Tokens per Sec:     2133 || Batch Translation Loss:   0.032096 => Txt Tokens per Sec:     5817 || Lr: 0.000100
2024-02-04 06:10:40,479 Epoch 408: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.45 
2024-02-04 06:10:40,479 EPOCH 409
2024-02-04 06:10:45,392 [Epoch: 409 Step: 00027400] Batch Recognition Loss:   0.000333 => Gls Tokens per Sec:     2067 || Batch Translation Loss:   0.051978 => Txt Tokens per Sec:     5742 || Lr: 0.000100
2024-02-04 06:10:45,659 Epoch 409: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.86 
2024-02-04 06:10:45,659 EPOCH 410
2024-02-04 06:10:51,410 Epoch 410: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.69 
2024-02-04 06:10:51,411 EPOCH 411
2024-02-04 06:10:53,520 [Epoch: 411 Step: 00027500] Batch Recognition Loss:   0.000333 => Gls Tokens per Sec:     2234 || Batch Translation Loss:   0.054618 => Txt Tokens per Sec:     6235 || Lr: 0.000100
2024-02-04 06:10:56,577 Epoch 411: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.39 
2024-02-04 06:10:56,578 EPOCH 412
2024-02-04 06:11:01,338 [Epoch: 412 Step: 00027600] Batch Recognition Loss:   0.000188 => Gls Tokens per Sec:     2118 || Batch Translation Loss:   0.030779 => Txt Tokens per Sec:     5824 || Lr: 0.000100
2024-02-04 06:11:01,874 Epoch 412: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.29 
2024-02-04 06:11:01,875 EPOCH 413
2024-02-04 06:11:07,444 Epoch 413: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.39 
2024-02-04 06:11:07,445 EPOCH 414
2024-02-04 06:11:09,421 [Epoch: 414 Step: 00027700] Batch Recognition Loss:   0.000235 => Gls Tokens per Sec:     2351 || Batch Translation Loss:   0.158446 => Txt Tokens per Sec:     6320 || Lr: 0.000100
2024-02-04 06:11:12,769 Epoch 414: Total Training Recognition Loss 0.03  Total Training Translation Loss 10.07 
2024-02-04 06:11:12,770 EPOCH 415
2024-02-04 06:11:17,996 [Epoch: 415 Step: 00027800] Batch Recognition Loss:   0.000400 => Gls Tokens per Sec:     1881 || Batch Translation Loss:   0.087588 => Txt Tokens per Sec:     5236 || Lr: 0.000100
2024-02-04 06:11:18,470 Epoch 415: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.63 
2024-02-04 06:11:18,471 EPOCH 416
2024-02-04 06:11:23,534 Epoch 416: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.71 
2024-02-04 06:11:23,535 EPOCH 417
2024-02-04 06:11:25,906 [Epoch: 417 Step: 00027900] Batch Recognition Loss:   0.000287 => Gls Tokens per Sec:     1890 || Batch Translation Loss:   0.121889 => Txt Tokens per Sec:     5187 || Lr: 0.000100
2024-02-04 06:11:29,053 Epoch 417: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.41 
2024-02-04 06:11:29,054 EPOCH 418
2024-02-04 06:11:34,226 [Epoch: 418 Step: 00028000] Batch Recognition Loss:   0.000523 => Gls Tokens per Sec:     1870 || Batch Translation Loss:   0.074568 => Txt Tokens per Sec:     5178 || Lr: 0.000100
2024-02-04 06:11:42,681 Validation result at epoch 418, step    28000: duration: 8.4539s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00173	Translation Loss: 94105.71875	PPL: 12297.65527
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.41	(BLEU-1: 10.80,	BLEU-2: 3.10,	BLEU-3: 1.00,	BLEU-4: 0.41)
	CHRF 16.95	ROUGE 9.54
2024-02-04 06:11:42,682 Logging Recognition and Translation Outputs
2024-02-04 06:11:42,682 ========================================================================================================================
2024-02-04 06:11:42,682 Logging Sequence: 67_98.00
2024-02-04 06:11:42,682 	Gloss Reference :	A B+C+D+E
2024-02-04 06:11:42,683 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:11:42,683 	Gloss Alignment :	         
2024-02-04 06:11:42,683 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:11:42,684 	Text Reference  :	***** *** *** it   saddens me      to **** ***** *** see people suffering and      dying due  to    lack of oxygen
2024-02-04 06:11:42,685 	Text Hypothesis :	india won the toss and     decided to ball first but in  the    tokyo     olympics that  ever since 2012 to win   
2024-02-04 06:11:42,685 	Text Alignment  :	I     I   I   S    S       S          I    I     I   S   S      S         S        S     S    S     S    S  S     
2024-02-04 06:11:42,685 ========================================================================================================================
2024-02-04 06:11:42,685 Logging Sequence: 157_83.00
2024-02-04 06:11:42,685 	Gloss Reference :	A B+C+D+E
2024-02-04 06:11:42,685 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:11:42,685 	Gloss Alignment :	         
2024-02-04 06:11:42,685 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:11:42,688 	Text Reference  :	also when    you     eat     sandwich at a streetside hawker  or  stall the sandwich maker      will first apply butter with  a   knife
2024-02-04 06:11:42,688 	Text Hypothesis :	**** cricket council noticed that     at * the        stadium was sent  to  her      spectators to   had   come  to     watch the match
2024-02-04 06:11:42,688 	Text Alignment  :	D    S       S       S       S           D S          S       S   S     S   S        S          S    S     S     S      S     S   S    
2024-02-04 06:11:42,688 ========================================================================================================================
2024-02-04 06:11:42,688 Logging Sequence: 76_35.00
2024-02-04 06:11:42,688 	Gloss Reference :	A B+C+D+E
2024-02-04 06:11:42,688 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:11:42,689 	Gloss Alignment :	         
2024-02-04 06:11:42,689 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:11:42,689 	Text Reference  :	bcci president sourav ganguly along with board secretary jay  shah        
2024-02-04 06:11:42,689 	Text Hypothesis :	**** in        the    match   kkr   had  won   the       saff championship
2024-02-04 06:11:42,690 	Text Alignment  :	D    S         S      S       S     S    S     S         S    S           
2024-02-04 06:11:42,690 ========================================================================================================================
2024-02-04 06:11:42,690 Logging Sequence: 139_180.00
2024-02-04 06:11:42,690 	Gloss Reference :	A B+C+D+E
2024-02-04 06:11:42,690 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:11:42,690 	Gloss Alignment :	         
2024-02-04 06:11:42,690 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:11:42,691 	Text Reference  :	netherlands also  faced similar riots
2024-02-04 06:11:42,691 	Text Hypothesis :	the         video has   not     viral
2024-02-04 06:11:42,691 	Text Alignment  :	S           S     S     S       S    
2024-02-04 06:11:42,691 ========================================================================================================================
2024-02-04 06:11:42,691 Logging Sequence: 98_87.00
2024-02-04 06:11:42,691 	Gloss Reference :	A B+C+D+E
2024-02-04 06:11:42,691 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:11:42,692 	Gloss Alignment :	         
2024-02-04 06:11:42,692 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:11:42,693 	Text Reference  :	instead of starting afresh     in ********* ***** 2021  the organizers opted   to resume with the previous edition
2024-02-04 06:11:42,693 	Text Hypothesis :	he      is least    interested in answering phone calls and was        allowed to focus  on   the ******** field  
2024-02-04 06:11:42,693 	Text Alignment  :	S       S  S        S             I         I     S     S   S          S          S      S        D        S      
2024-02-04 06:11:42,693 ========================================================================================================================
2024-02-04 06:11:43,209 Epoch 418: Total Training Recognition Loss 0.02  Total Training Translation Loss 8.53 
2024-02-04 06:11:43,209 EPOCH 419
2024-02-04 06:11:49,037 Epoch 419: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.72 
2024-02-04 06:11:49,037 EPOCH 420
2024-02-04 06:11:51,289 [Epoch: 420 Step: 00028100] Batch Recognition Loss:   0.000166 => Gls Tokens per Sec:     1880 || Batch Translation Loss:   0.123225 => Txt Tokens per Sec:     5356 || Lr: 0.000100
2024-02-04 06:11:54,516 Epoch 420: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.69 
2024-02-04 06:11:54,517 EPOCH 421
2024-02-04 06:11:59,333 [Epoch: 421 Step: 00028200] Batch Recognition Loss:   0.000298 => Gls Tokens per Sec:     1975 || Batch Translation Loss:   0.033062 => Txt Tokens per Sec:     5470 || Lr: 0.000100
2024-02-04 06:11:59,858 Epoch 421: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.38 
2024-02-04 06:11:59,858 EPOCH 422
2024-02-04 06:12:05,376 Epoch 422: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.61 
2024-02-04 06:12:05,376 EPOCH 423
2024-02-04 06:12:07,187 [Epoch: 423 Step: 00028300] Batch Recognition Loss:   0.000354 => Gls Tokens per Sec:     2248 || Batch Translation Loss:   0.209746 => Txt Tokens per Sec:     6169 || Lr: 0.000100
2024-02-04 06:12:10,555 Epoch 423: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.16 
2024-02-04 06:12:10,556 EPOCH 424
2024-02-04 06:12:15,223 [Epoch: 424 Step: 00028400] Batch Recognition Loss:   0.000450 => Gls Tokens per Sec:     2003 || Batch Translation Loss:   0.017073 => Txt Tokens per Sec:     5612 || Lr: 0.000100
2024-02-04 06:12:15,722 Epoch 424: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.65 
2024-02-04 06:12:15,722 EPOCH 425
2024-02-04 06:12:21,122 Epoch 425: Total Training Recognition Loss 0.02  Total Training Translation Loss 8.01 
2024-02-04 06:12:21,122 EPOCH 426
2024-02-04 06:12:22,827 [Epoch: 426 Step: 00028500] Batch Recognition Loss:   0.000186 => Gls Tokens per Sec:     2347 || Batch Translation Loss:   0.034457 => Txt Tokens per Sec:     6467 || Lr: 0.000100
2024-02-04 06:12:26,387 Epoch 426: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.14 
2024-02-04 06:12:26,387 EPOCH 427
2024-02-04 06:12:31,182 [Epoch: 427 Step: 00028600] Batch Recognition Loss:   0.000217 => Gls Tokens per Sec:     1936 || Batch Translation Loss:   0.055601 => Txt Tokens per Sec:     5466 || Lr: 0.000100
2024-02-04 06:12:31,766 Epoch 427: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.76 
2024-02-04 06:12:31,766 EPOCH 428
2024-02-04 06:12:37,078 Epoch 428: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.37 
2024-02-04 06:12:37,079 EPOCH 429
2024-02-04 06:12:38,845 [Epoch: 429 Step: 00028700] Batch Recognition Loss:   0.000255 => Gls Tokens per Sec:     2125 || Batch Translation Loss:   0.063486 => Txt Tokens per Sec:     5918 || Lr: 0.000100
2024-02-04 06:12:42,135 Epoch 429: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.03 
2024-02-04 06:12:42,135 EPOCH 430
2024-02-04 06:12:46,790 [Epoch: 430 Step: 00028800] Batch Recognition Loss:   0.000258 => Gls Tokens per Sec:     1940 || Batch Translation Loss:   0.103556 => Txt Tokens per Sec:     5329 || Lr: 0.000100
2024-02-04 06:12:47,555 Epoch 430: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.36 
2024-02-04 06:12:47,555 EPOCH 431
2024-02-04 06:12:52,823 Epoch 431: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.11 
2024-02-04 06:12:52,824 EPOCH 432
2024-02-04 06:12:54,434 [Epoch: 432 Step: 00028900] Batch Recognition Loss:   0.000321 => Gls Tokens per Sec:     2287 || Batch Translation Loss:   0.010762 => Txt Tokens per Sec:     6203 || Lr: 0.000100
2024-02-04 06:12:58,338 Epoch 432: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.84 
2024-02-04 06:12:58,338 EPOCH 433
2024-02-04 06:13:03,085 [Epoch: 433 Step: 00029000] Batch Recognition Loss:   0.000177 => Gls Tokens per Sec:     1888 || Batch Translation Loss:   0.025008 => Txt Tokens per Sec:     5292 || Lr: 0.000100
2024-02-04 06:13:03,943 Epoch 433: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.95 
2024-02-04 06:13:03,943 EPOCH 434
2024-02-04 06:13:09,104 Epoch 434: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.95 
2024-02-04 06:13:09,105 EPOCH 435
2024-02-04 06:13:10,838 [Epoch: 435 Step: 00029100] Batch Recognition Loss:   0.000418 => Gls Tokens per Sec:     2033 || Batch Translation Loss:   0.026377 => Txt Tokens per Sec:     5481 || Lr: 0.000100
2024-02-04 06:13:14,328 Epoch 435: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.07 
2024-02-04 06:13:14,329 EPOCH 436
2024-02-04 06:13:18,224 [Epoch: 436 Step: 00029200] Batch Recognition Loss:   0.000390 => Gls Tokens per Sec:     2237 || Batch Translation Loss:   0.185744 => Txt Tokens per Sec:     6175 || Lr: 0.000100
2024-02-04 06:13:19,369 Epoch 436: Total Training Recognition Loss 0.02  Total Training Translation Loss 10.96 
2024-02-04 06:13:19,370 EPOCH 437
2024-02-04 06:13:24,686 Epoch 437: Total Training Recognition Loss 0.04  Total Training Translation Loss 12.12 
2024-02-04 06:13:24,686 EPOCH 438
2024-02-04 06:13:26,369 [Epoch: 438 Step: 00029300] Batch Recognition Loss:   0.000521 => Gls Tokens per Sec:     1944 || Batch Translation Loss:   0.101167 => Txt Tokens per Sec:     5730 || Lr: 0.000100
2024-02-04 06:13:29,905 Epoch 438: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.15 
2024-02-04 06:13:29,905 EPOCH 439
2024-02-04 06:13:34,492 [Epoch: 439 Step: 00029400] Batch Recognition Loss:   0.000547 => Gls Tokens per Sec:     1864 || Batch Translation Loss:   0.473954 => Txt Tokens per Sec:     5183 || Lr: 0.000100
2024-02-04 06:13:35,554 Epoch 439: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.35 
2024-02-04 06:13:35,554 EPOCH 440
2024-02-04 06:13:40,925 Epoch 440: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.95 
2024-02-04 06:13:40,925 EPOCH 441
2024-02-04 06:13:42,556 [Epoch: 441 Step: 00029500] Batch Recognition Loss:   0.000371 => Gls Tokens per Sec:     1906 || Batch Translation Loss:   0.066990 => Txt Tokens per Sec:     5594 || Lr: 0.000100
2024-02-04 06:13:46,205 Epoch 441: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.69 
2024-02-04 06:13:46,205 EPOCH 442
2024-02-04 06:13:50,234 [Epoch: 442 Step: 00029600] Batch Recognition Loss:   0.000293 => Gls Tokens per Sec:     2083 || Batch Translation Loss:   0.029014 => Txt Tokens per Sec:     5803 || Lr: 0.000100
2024-02-04 06:13:51,409 Epoch 442: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.32 
2024-02-04 06:13:51,410 EPOCH 443
2024-02-04 06:13:57,022 Epoch 443: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.46 
2024-02-04 06:13:57,023 EPOCH 444
2024-02-04 06:13:58,483 [Epoch: 444 Step: 00029700] Batch Recognition Loss:   0.000269 => Gls Tokens per Sec:     2083 || Batch Translation Loss:   0.034565 => Txt Tokens per Sec:     6127 || Lr: 0.000100
2024-02-04 06:14:01,974 Epoch 444: Total Training Recognition Loss 0.02  Total Training Translation Loss 8.18 
2024-02-04 06:14:01,975 EPOCH 445
2024-02-04 06:14:06,306 [Epoch: 445 Step: 00029800] Batch Recognition Loss:   0.000389 => Gls Tokens per Sec:     1901 || Batch Translation Loss:   0.079493 => Txt Tokens per Sec:     5316 || Lr: 0.000100
2024-02-04 06:14:07,483 Epoch 445: Total Training Recognition Loss 0.03  Total Training Translation Loss 10.73 
2024-02-04 06:14:07,483 EPOCH 446
2024-02-04 06:14:12,732 Epoch 446: Total Training Recognition Loss 0.03  Total Training Translation Loss 11.09 
2024-02-04 06:14:12,733 EPOCH 447
2024-02-04 06:14:13,960 [Epoch: 447 Step: 00029900] Batch Recognition Loss:   0.000205 => Gls Tokens per Sec:     2348 || Batch Translation Loss:   0.049516 => Txt Tokens per Sec:     6314 || Lr: 0.000100
2024-02-04 06:14:17,790 Epoch 447: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.48 
2024-02-04 06:14:17,790 EPOCH 448
2024-02-04 06:14:22,103 [Epoch: 448 Step: 00030000] Batch Recognition Loss:   0.000325 => Gls Tokens per Sec:     1872 || Batch Translation Loss:   0.058251 => Txt Tokens per Sec:     5324 || Lr: 0.000100
2024-02-04 06:14:30,277 Validation result at epoch 448, step    30000: duration: 8.1730s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00156	Translation Loss: 94343.07031	PPL: 12593.24805
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.30	(BLEU-1: 10.10,	BLEU-2: 2.75,	BLEU-3: 0.84,	BLEU-4: 0.30)
	CHRF 16.19	ROUGE 8.72
2024-02-04 06:14:30,278 Logging Recognition and Translation Outputs
2024-02-04 06:14:30,278 ========================================================================================================================
2024-02-04 06:14:30,278 Logging Sequence: 165_502.00
2024-02-04 06:14:30,279 	Gloss Reference :	A B+C+D+E
2024-02-04 06:14:30,279 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:14:30,279 	Gloss Alignment :	         
2024-02-04 06:14:30,279 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:14:30,281 	Text Reference  :	tendulkar would sit   in       the **** pavilion wearing both      his     batting pads even after   he   got   out
2024-02-04 06:14:30,281 	Text Hypothesis :	********* this  group includes the many teams    like    australia england india   must play against each other out
2024-02-04 06:14:30,281 	Text Alignment  :	D         S     S     S            I    S        S       S         S       S       S    S    S       S    S        
2024-02-04 06:14:30,281 ========================================================================================================================
2024-02-04 06:14:30,281 Logging Sequence: 127_57.00
2024-02-04 06:14:30,281 	Gloss Reference :	A B+C+D+E
2024-02-04 06:14:30,281 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:14:30,281 	Gloss Alignment :	         
2024-02-04 06:14:30,282 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:14:30,283 	Text Reference  :	till date india had won only 2 medals at the championships which  like the olympics is   the             highest level championship
2024-02-04 06:14:30,283 	Text Hypothesis :	**** **** ***** *** *** **** * ****** ** the bidding       amount for  the same     time congratulations to      west  indies      
2024-02-04 06:14:30,283 	Text Alignment  :	D    D    D     D   D   D    D D      D      S             S      S        S        S    S               S       S     S           
2024-02-04 06:14:30,283 ========================================================================================================================
2024-02-04 06:14:30,283 Logging Sequence: 169_10.00
2024-02-04 06:14:30,284 	Gloss Reference :	A B+C+D+E
2024-02-04 06:14:30,284 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:14:30,284 	Gloss Alignment :	         
2024-02-04 06:14:30,284 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:14:30,285 	Text Reference  :	the 18th over was bowled by     ravi bishnoi with khushdil shah  and     asif ali     on  the crease 
2024-02-04 06:14:30,285 	Text Hypothesis :	*** **** **** a   first  person in   the     asia cup      final against each captain for a   victory
2024-02-04 06:14:30,286 	Text Alignment  :	D   D    D    S   S      S      S    S       S    S        S     S       S    S       S   S   S      
2024-02-04 06:14:30,286 ========================================================================================================================
2024-02-04 06:14:30,286 Logging Sequence: 64_89.00
2024-02-04 06:14:30,286 	Gloss Reference :	A B+C+D+E
2024-02-04 06:14:30,286 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:14:30,286 	Gloss Alignment :	         
2024-02-04 06:14:30,286 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:14:30,287 	Text Reference  :	but this can  not go on amidst the rising cases human lives    need to   be safeguarded
2024-02-04 06:14:30,288 	Text Hypothesis :	*** ipl  will not ** ** ****** *** ****** ***** be    possible in   june as well       
2024-02-04 06:14:30,288 	Text Alignment  :	D   S    S        D  D  D      D   D      D     S     S        S    S    S  S          
2024-02-04 06:14:30,288 ========================================================================================================================
2024-02-04 06:14:30,288 Logging Sequence: 166_261.00
2024-02-04 06:14:30,288 	Gloss Reference :	A B+C+D+E
2024-02-04 06:14:30,288 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:14:30,288 	Gloss Alignment :	         
2024-02-04 06:14:30,288 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:14:30,289 	Text Reference  :	***** for all organizational matters and the schedule   
2024-02-04 06:14:30,289 	Text Hypothesis :	babar and the ball           leaving for the coronavirus
2024-02-04 06:14:30,289 	Text Alignment  :	I     S   S   S              S       S       S          
2024-02-04 06:14:30,289 ========================================================================================================================
2024-02-04 06:14:31,484 Epoch 448: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.95 
2024-02-04 06:14:31,484 EPOCH 449
2024-02-04 06:14:37,016 Epoch 449: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.68 
2024-02-04 06:14:37,017 EPOCH 450
2024-02-04 06:14:38,344 [Epoch: 450 Step: 00030100] Batch Recognition Loss:   0.000574 => Gls Tokens per Sec:     1983 || Batch Translation Loss:   0.077038 => Txt Tokens per Sec:     5278 || Lr: 0.000100
2024-02-04 06:14:42,364 Epoch 450: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.14 
2024-02-04 06:14:42,365 EPOCH 451
2024-02-04 06:14:46,127 [Epoch: 451 Step: 00030200] Batch Recognition Loss:   0.000408 => Gls Tokens per Sec:     2103 || Batch Translation Loss:   0.102783 => Txt Tokens per Sec:     5741 || Lr: 0.000100
2024-02-04 06:14:47,660 Epoch 451: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.02 
2024-02-04 06:14:47,660 EPOCH 452
2024-02-04 06:14:52,813 Epoch 452: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.52 
2024-02-04 06:14:52,813 EPOCH 453
2024-02-04 06:14:54,143 [Epoch: 453 Step: 00030300] Batch Recognition Loss:   0.000651 => Gls Tokens per Sec:     1926 || Batch Translation Loss:   0.035915 => Txt Tokens per Sec:     5507 || Lr: 0.000100
2024-02-04 06:14:58,374 Epoch 453: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.37 
2024-02-04 06:14:58,374 EPOCH 454
2024-02-04 06:15:02,227 [Epoch: 454 Step: 00030400] Batch Recognition Loss:   0.000322 => Gls Tokens per Sec:     2012 || Batch Translation Loss:   0.035644 => Txt Tokens per Sec:     5649 || Lr: 0.000100
2024-02-04 06:15:03,572 Epoch 454: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.09 
2024-02-04 06:15:03,572 EPOCH 455
2024-02-04 06:15:08,815 Epoch 455: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.97 
2024-02-04 06:15:08,815 EPOCH 456
2024-02-04 06:15:09,858 [Epoch: 456 Step: 00030500] Batch Recognition Loss:   0.000138 => Gls Tokens per Sec:     2307 || Batch Translation Loss:   0.019941 => Txt Tokens per Sec:     6252 || Lr: 0.000100
2024-02-04 06:15:14,245 Epoch 456: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.88 
2024-02-04 06:15:14,246 EPOCH 457
2024-02-04 06:15:17,971 [Epoch: 457 Step: 00030600] Batch Recognition Loss:   0.000364 => Gls Tokens per Sec:     2038 || Batch Translation Loss:   0.047418 => Txt Tokens per Sec:     5696 || Lr: 0.000100
2024-02-04 06:15:19,355 Epoch 457: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.79 
2024-02-04 06:15:19,355 EPOCH 458
2024-02-04 06:15:25,056 Epoch 458: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.35 
2024-02-04 06:15:25,057 EPOCH 459
2024-02-04 06:15:26,056 [Epoch: 459 Step: 00030700] Batch Recognition Loss:   0.000286 => Gls Tokens per Sec:     2244 || Batch Translation Loss:   0.045051 => Txt Tokens per Sec:     6481 || Lr: 0.000100
2024-02-04 06:15:30,223 Epoch 459: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.89 
2024-02-04 06:15:30,224 EPOCH 460
2024-02-04 06:15:33,693 [Epoch: 460 Step: 00030800] Batch Recognition Loss:   0.000202 => Gls Tokens per Sec:     2143 || Batch Translation Loss:   0.040729 => Txt Tokens per Sec:     5825 || Lr: 0.000100
2024-02-04 06:15:35,335 Epoch 460: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.01 
2024-02-04 06:15:35,335 EPOCH 461
2024-02-04 06:15:40,826 Epoch 461: Total Training Recognition Loss 0.02  Total Training Translation Loss 9.97 
2024-02-04 06:15:40,826 EPOCH 462
2024-02-04 06:15:41,659 [Epoch: 462 Step: 00030900] Batch Recognition Loss:   0.000395 => Gls Tokens per Sec:     2498 || Batch Translation Loss:   0.091936 => Txt Tokens per Sec:     6695 || Lr: 0.000100
2024-02-04 06:15:45,734 Epoch 462: Total Training Recognition Loss 0.04  Total Training Translation Loss 11.32 
2024-02-04 06:15:45,734 EPOCH 463
2024-02-04 06:15:49,179 [Epoch: 463 Step: 00031000] Batch Recognition Loss:   0.000556 => Gls Tokens per Sec:     2110 || Batch Translation Loss:   0.155593 => Txt Tokens per Sec:     5906 || Lr: 0.000100
2024-02-04 06:15:50,432 Epoch 463: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.25 
2024-02-04 06:15:50,433 EPOCH 464
2024-02-04 06:15:55,921 Epoch 464: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.25 
2024-02-04 06:15:55,922 EPOCH 465
2024-02-04 06:15:56,597 [Epoch: 465 Step: 00031100] Batch Recognition Loss:   0.000218 => Gls Tokens per Sec:     2847 || Batch Translation Loss:   0.065139 => Txt Tokens per Sec:     7266 || Lr: 0.000100
2024-02-04 06:16:01,047 Epoch 465: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.46 
2024-02-04 06:16:01,048 EPOCH 466
2024-02-04 06:16:04,860 [Epoch: 466 Step: 00031200] Batch Recognition Loss:   0.000358 => Gls Tokens per Sec:     1865 || Batch Translation Loss:   0.092953 => Txt Tokens per Sec:     5331 || Lr: 0.000100
2024-02-04 06:16:06,431 Epoch 466: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.18 
2024-02-04 06:16:06,431 EPOCH 467
2024-02-04 06:16:11,806 Epoch 467: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.18 
2024-02-04 06:16:11,807 EPOCH 468
2024-02-04 06:16:12,603 [Epoch: 468 Step: 00031300] Batch Recognition Loss:   0.000326 => Gls Tokens per Sec:     2214 || Batch Translation Loss:   0.034070 => Txt Tokens per Sec:     6308 || Lr: 0.000100
2024-02-04 06:16:16,883 Epoch 468: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.52 
2024-02-04 06:16:16,883 EPOCH 469
2024-02-04 06:16:20,613 [Epoch: 469 Step: 00031400] Batch Recognition Loss:   0.000215 => Gls Tokens per Sec:     1864 || Batch Translation Loss:   0.081730 => Txt Tokens per Sec:     5044 || Lr: 0.000100
2024-02-04 06:16:22,531 Epoch 469: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.24 
2024-02-04 06:16:22,532 EPOCH 470
2024-02-04 06:16:27,796 Epoch 470: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.04 
2024-02-04 06:16:27,797 EPOCH 471
2024-02-04 06:16:28,653 [Epoch: 471 Step: 00031500] Batch Recognition Loss:   0.000290 => Gls Tokens per Sec:     1870 || Batch Translation Loss:   0.070090 => Txt Tokens per Sec:     5435 || Lr: 0.000100
2024-02-04 06:16:33,337 Epoch 471: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.96 
2024-02-04 06:16:33,337 EPOCH 472
2024-02-04 06:16:36,302 [Epoch: 472 Step: 00031600] Batch Recognition Loss:   0.000229 => Gls Tokens per Sec:     2291 || Batch Translation Loss:   0.036931 => Txt Tokens per Sec:     6207 || Lr: 0.000100
2024-02-04 06:16:38,525 Epoch 472: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.78 
2024-02-04 06:16:38,526 EPOCH 473
2024-02-04 06:16:43,686 Epoch 473: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.04 
2024-02-04 06:16:43,686 EPOCH 474
2024-02-04 06:16:44,503 [Epoch: 474 Step: 00031700] Batch Recognition Loss:   0.000149 => Gls Tokens per Sec:     1766 || Batch Translation Loss:   0.137432 => Txt Tokens per Sec:     5393 || Lr: 0.000100
2024-02-04 06:16:49,227 Epoch 474: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.40 
2024-02-04 06:16:49,227 EPOCH 475
2024-02-04 06:16:52,160 [Epoch: 475 Step: 00031800] Batch Recognition Loss:   0.000292 => Gls Tokens per Sec:     2292 || Batch Translation Loss:   0.046150 => Txt Tokens per Sec:     6420 || Lr: 0.000100
2024-02-04 06:16:54,488 Epoch 475: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.25 
2024-02-04 06:16:54,488 EPOCH 476
2024-02-04 06:16:59,960 Epoch 476: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.66 
2024-02-04 06:16:59,960 EPOCH 477
2024-02-04 06:17:00,571 [Epoch: 477 Step: 00031900] Batch Recognition Loss:   0.000469 => Gls Tokens per Sec:     2096 || Batch Translation Loss:   0.266912 => Txt Tokens per Sec:     6092 || Lr: 0.000100
2024-02-04 06:17:05,374 Epoch 477: Total Training Recognition Loss 0.02  Total Training Translation Loss 8.45 
2024-02-04 06:17:05,375 EPOCH 478
2024-02-04 06:17:08,344 [Epoch: 478 Step: 00032000] Batch Recognition Loss:   0.000234 => Gls Tokens per Sec:     2180 || Batch Translation Loss:   0.140852 => Txt Tokens per Sec:     5970 || Lr: 0.000100
2024-02-04 06:17:17,246 Validation result at epoch 478, step    32000: duration: 8.9015s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00142	Translation Loss: 93260.83594	PPL: 11300.66406
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.69	(BLEU-1: 10.93,	BLEU-2: 3.45,	BLEU-3: 1.47,	BLEU-4: 0.69)
	CHRF 17.16	ROUGE 9.09
2024-02-04 06:17:17,247 Logging Recognition and Translation Outputs
2024-02-04 06:17:17,248 ========================================================================================================================
2024-02-04 06:17:17,248 Logging Sequence: 86_11.00
2024-02-04 06:17:17,248 	Gloss Reference :	A B+C+D+E
2024-02-04 06:17:17,248 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:17:17,248 	Gloss Alignment :	         
2024-02-04 06:17:17,248 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:17:17,249 	Text Reference  :	he was **** ******* ** 66  years old    
2024-02-04 06:17:17,249 	Text Hypothesis :	he was very excited to see her   victory
2024-02-04 06:17:17,249 	Text Alignment  :	       I    I       I  S   S     S      
2024-02-04 06:17:17,249 ========================================================================================================================
2024-02-04 06:17:17,249 Logging Sequence: 67_16.00
2024-02-04 06:17:17,249 	Gloss Reference :	A B+C+D+E
2024-02-04 06:17:17,249 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:17:17,250 	Gloss Alignment :	         
2024-02-04 06:17:17,250 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:17:17,250 	Text Reference  :	* **** ********* **** to    help india's fight against the    covid-19 pandemic
2024-02-04 06:17:17,251 	Text Hypothesis :	i have dedicated this match as   a       month has     landed in       love    
2024-02-04 06:17:17,251 	Text Alignment  :	I I    I         I    S     S    S       S     S       S      S        S       
2024-02-04 06:17:17,251 ========================================================================================================================
2024-02-04 06:17:17,251 Logging Sequence: 69_177.00
2024-02-04 06:17:17,251 	Gloss Reference :	A B+C+D+E
2024-02-04 06:17:17,251 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:17:17,251 	Gloss Alignment :	         
2024-02-04 06:17:17,251 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:17:17,253 	Text Reference  :	he said 'i will continue playing i know it's about time  i   retire    i   also have a   knee    condition
2024-02-04 06:17:17,253 	Text Hypothesis :	** **** ** **** ******** ******* * when both the   score was presented the end  of   the lucknow season   
2024-02-04 06:17:17,253 	Text Alignment  :	D  D    D  D    D        D       D S    S    S     S     S   S         S   S    S    S   S       S        
2024-02-04 06:17:17,253 ========================================================================================================================
2024-02-04 06:17:17,253 Logging Sequence: 165_615.00
2024-02-04 06:17:17,253 	Gloss Reference :	A B+C+D+E
2024-02-04 06:17:17,254 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:17:17,254 	Gloss Alignment :	         
2024-02-04 06:17:17,254 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:17:17,254 	Text Reference  :	**** ** *** ** *** **** ********* *** we defeated pakistan too     
2024-02-04 06:17:17,254 	Text Hypothesis :	this is why it has been postponed due to the      covid    pandemic
2024-02-04 06:17:17,254 	Text Alignment  :	I    I  I   I  I   I    I         I   S  S        S        S       
2024-02-04 06:17:17,255 ========================================================================================================================
2024-02-04 06:17:17,255 Logging Sequence: 61_5.00
2024-02-04 06:17:17,255 	Gloss Reference :	A B+C+D+E
2024-02-04 06:17:17,255 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:17:17,255 	Gloss Alignment :	         
2024-02-04 06:17:17,255 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:17:17,256 	Text Reference  :	they rivalry is    seen   the  most during india pakistan cricket matches
2024-02-04 06:17:17,256 	Text Hypothesis :	**** ******* babar azam's chat and  told   him   call     became  viral  
2024-02-04 06:17:17,256 	Text Alignment  :	D    D       S     S      S    S    S      S     S        S       S      
2024-02-04 06:17:17,256 ========================================================================================================================
2024-02-04 06:17:19,453 Epoch 478: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.02 
2024-02-04 06:17:19,453 EPOCH 479
2024-02-04 06:17:24,629 Epoch 479: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.81 
2024-02-04 06:17:24,629 EPOCH 480
2024-02-04 06:17:25,053 [Epoch: 480 Step: 00032100] Batch Recognition Loss:   0.000170 => Gls Tokens per Sec:     2648 || Batch Translation Loss:   0.014244 => Txt Tokens per Sec:     5967 || Lr: 0.000100
2024-02-04 06:17:30,165 Epoch 480: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.74 
2024-02-04 06:17:30,165 EPOCH 481
2024-02-04 06:17:33,233 [Epoch: 481 Step: 00032200] Batch Recognition Loss:   0.000268 => Gls Tokens per Sec:     2086 || Batch Translation Loss:   0.095234 => Txt Tokens per Sec:     5767 || Lr: 0.000100
2024-02-04 06:17:35,418 Epoch 481: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.10 
2024-02-04 06:17:35,418 EPOCH 482
2024-02-04 06:17:40,944 Epoch 482: Total Training Recognition Loss 0.02  Total Training Translation Loss 10.02 
2024-02-04 06:17:40,945 EPOCH 483
2024-02-04 06:17:41,389 [Epoch: 483 Step: 00032300] Batch Recognition Loss:   0.000263 => Gls Tokens per Sec:     2167 || Batch Translation Loss:   0.254509 => Txt Tokens per Sec:     6124 || Lr: 0.000100
2024-02-04 06:17:46,424 Epoch 483: Total Training Recognition Loss 0.03  Total Training Translation Loss 12.41 
2024-02-04 06:17:46,425 EPOCH 484
2024-02-04 06:17:49,612 [Epoch: 484 Step: 00032400] Batch Recognition Loss:   0.000424 => Gls Tokens per Sec:     1959 || Batch Translation Loss:   0.052021 => Txt Tokens per Sec:     5441 || Lr: 0.000100
2024-02-04 06:17:51,953 Epoch 484: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.18 
2024-02-04 06:17:51,954 EPOCH 485
2024-02-04 06:17:57,452 Epoch 485: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.50 
2024-02-04 06:17:57,452 EPOCH 486
2024-02-04 06:17:57,859 [Epoch: 486 Step: 00032500] Batch Recognition Loss:   0.000298 => Gls Tokens per Sec:     1975 || Batch Translation Loss:   0.046117 => Txt Tokens per Sec:     5151 || Lr: 0.000100
2024-02-04 06:18:02,878 Epoch 486: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.45 
2024-02-04 06:18:02,878 EPOCH 487
2024-02-04 06:18:05,728 [Epoch: 487 Step: 00032600] Batch Recognition Loss:   0.000415 => Gls Tokens per Sec:     2135 || Batch Translation Loss:   0.031905 => Txt Tokens per Sec:     5725 || Lr: 0.000100
2024-02-04 06:18:08,381 Epoch 487: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.66 
2024-02-04 06:18:08,381 EPOCH 488
2024-02-04 06:18:13,828 Epoch 488: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.01 
2024-02-04 06:18:13,829 EPOCH 489
2024-02-04 06:18:14,156 [Epoch: 489 Step: 00032700] Batch Recognition Loss:   0.000262 => Gls Tokens per Sec:     1963 || Batch Translation Loss:   0.021046 => Txt Tokens per Sec:     5365 || Lr: 0.000100
2024-02-04 06:18:19,500 Epoch 489: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.69 
2024-02-04 06:18:19,500 EPOCH 490
2024-02-04 06:18:22,605 [Epoch: 490 Step: 00032800] Batch Recognition Loss:   0.000203 => Gls Tokens per Sec:     1878 || Batch Translation Loss:   0.074152 => Txt Tokens per Sec:     5228 || Lr: 0.000100
2024-02-04 06:18:25,075 Epoch 490: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.34 
2024-02-04 06:18:25,075 EPOCH 491
2024-02-04 06:18:30,431 Epoch 491: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.34 
2024-02-04 06:18:30,432 EPOCH 492
2024-02-04 06:18:30,650 [Epoch: 492 Step: 00032900] Batch Recognition Loss:   0.000438 => Gls Tokens per Sec:     2212 || Batch Translation Loss:   0.032832 => Txt Tokens per Sec:     5341 || Lr: 0.000100
2024-02-04 06:18:35,957 Epoch 492: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.08 
2024-02-04 06:18:35,958 EPOCH 493
2024-02-04 06:18:38,755 [Epoch: 493 Step: 00033000] Batch Recognition Loss:   0.000248 => Gls Tokens per Sec:     2028 || Batch Translation Loss:   0.090874 => Txt Tokens per Sec:     5638 || Lr: 0.000100
2024-02-04 06:18:41,132 Epoch 493: Total Training Recognition Loss 0.02  Total Training Translation Loss 8.09 
2024-02-04 06:18:41,132 EPOCH 494
2024-02-04 06:18:46,639 Epoch 494: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.69 
2024-02-04 06:18:46,640 EPOCH 495
2024-02-04 06:18:46,757 [Epoch: 495 Step: 00033100] Batch Recognition Loss:   0.000270 => Gls Tokens per Sec:     2759 || Batch Translation Loss:   0.034659 => Txt Tokens per Sec:     6146 || Lr: 0.000100
2024-02-04 06:18:51,793 Epoch 495: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.06 
2024-02-04 06:18:51,794 EPOCH 496
2024-02-04 06:18:54,584 [Epoch: 496 Step: 00033200] Batch Recognition Loss:   0.000528 => Gls Tokens per Sec:     1976 || Batch Translation Loss:   0.080137 => Txt Tokens per Sec:     5538 || Lr: 0.000100
2024-02-04 06:18:57,272 Epoch 496: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.25 
2024-02-04 06:18:57,273 EPOCH 497
2024-02-04 06:19:02,730 Epoch 497: Total Training Recognition Loss 0.04  Total Training Translation Loss 6.99 
2024-02-04 06:19:02,731 EPOCH 498
2024-02-04 06:19:02,801 [Epoch: 498 Step: 00033300] Batch Recognition Loss:   0.000200 => Gls Tokens per Sec:     2353 || Batch Translation Loss:   0.066659 => Txt Tokens per Sec:     5265 || Lr: 0.000100
2024-02-04 06:19:07,637 Epoch 498: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.00 
2024-02-04 06:19:07,637 EPOCH 499
2024-02-04 06:19:10,372 [Epoch: 499 Step: 00033400] Batch Recognition Loss:   0.000240 => Gls Tokens per Sec:     1989 || Batch Translation Loss:   0.076463 => Txt Tokens per Sec:     5489 || Lr: 0.000100
2024-02-04 06:19:13,090 Epoch 499: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.68 
2024-02-04 06:19:13,091 EPOCH 500
2024-02-04 06:19:18,567 [Epoch: 500 Step: 00033500] Batch Recognition Loss:   0.000436 => Gls Tokens per Sec:     1941 || Batch Translation Loss:   0.265320 => Txt Tokens per Sec:     5389 || Lr: 0.000100
2024-02-04 06:19:18,568 Epoch 500: Total Training Recognition Loss 0.04  Total Training Translation Loss 7.50 
2024-02-04 06:19:18,568 EPOCH 501
2024-02-04 06:19:23,956 Epoch 501: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.15 
2024-02-04 06:19:23,956 EPOCH 502
2024-02-04 06:19:26,952 [Epoch: 502 Step: 00033600] Batch Recognition Loss:   0.000212 => Gls Tokens per Sec:     1733 || Batch Translation Loss:   0.102087 => Txt Tokens per Sec:     4922 || Lr: 0.000100
2024-02-04 06:19:29,417 Epoch 502: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.15 
2024-02-04 06:19:29,418 EPOCH 503
2024-02-04 06:19:34,204 [Epoch: 503 Step: 00033700] Batch Recognition Loss:   0.000174 => Gls Tokens per Sec:     2188 || Batch Translation Loss:   0.039650 => Txt Tokens per Sec:     6076 || Lr: 0.000100
2024-02-04 06:19:34,279 Epoch 503: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.34 
2024-02-04 06:19:34,280 EPOCH 504
2024-02-04 06:19:39,771 Epoch 504: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.37 
2024-02-04 06:19:39,772 EPOCH 505
2024-02-04 06:19:42,103 [Epoch: 505 Step: 00033800] Batch Recognition Loss:   0.000210 => Gls Tokens per Sec:     2197 || Batch Translation Loss:   0.048196 => Txt Tokens per Sec:     6149 || Lr: 0.000100
2024-02-04 06:19:45,086 Epoch 505: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.76 
2024-02-04 06:19:45,086 EPOCH 506
2024-02-04 06:19:50,271 [Epoch: 506 Step: 00033900] Batch Recognition Loss:   0.000480 => Gls Tokens per Sec:     1989 || Batch Translation Loss:   0.306768 => Txt Tokens per Sec:     5496 || Lr: 0.000100
2024-02-04 06:19:50,452 Epoch 506: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.62 
2024-02-04 06:19:50,452 EPOCH 507
2024-02-04 06:19:55,964 Epoch 507: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.44 
2024-02-04 06:19:55,965 EPOCH 508
2024-02-04 06:19:58,018 [Epoch: 508 Step: 00034000] Batch Recognition Loss:   0.000239 => Gls Tokens per Sec:     2418 || Batch Translation Loss:   0.036463 => Txt Tokens per Sec:     6696 || Lr: 0.000100
2024-02-04 06:20:06,745 Validation result at epoch 508, step    34000: duration: 8.7269s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00176	Translation Loss: 92418.50000	PPL: 10387.14648
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.38	(BLEU-1: 10.11,	BLEU-2: 3.13,	BLEU-3: 1.15,	BLEU-4: 0.38)
	CHRF 17.11	ROUGE 8.39
2024-02-04 06:20:06,746 Logging Recognition and Translation Outputs
2024-02-04 06:20:06,746 ========================================================================================================================
2024-02-04 06:20:06,747 Logging Sequence: 92_199.00
2024-02-04 06:20:06,747 	Gloss Reference :	A B+C+D+E
2024-02-04 06:20:06,747 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:20:06,747 	Gloss Alignment :	         
2024-02-04 06:20:06,747 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:20:06,748 	Text Reference  :	*** people on     social media said that   
2024-02-04 06:20:06,748 	Text Hypothesis :	now he     became one    over  his  brother
2024-02-04 06:20:06,748 	Text Alignment  :	I   S      S      S      S     S    S      
2024-02-04 06:20:06,748 ========================================================================================================================
2024-02-04 06:20:06,748 Logging Sequence: 109_64.00
2024-02-04 06:20:06,748 	Gloss Reference :	A B+C+D+E
2024-02-04 06:20:06,749 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:20:06,749 	Gloss Alignment :	         
2024-02-04 06:20:06,749 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:20:06,750 	Text Reference  :	the     2 players as   well as    the ******** entire kkr team have     been  quarantined
2024-02-04 06:20:06,750 	Text Hypothesis :	however a medical team will check the contacts of     the two  positive cases everyday   
2024-02-04 06:20:06,750 	Text Alignment  :	S       S S       S    S    S         I        S      S   S    S        S     S          
2024-02-04 06:20:06,750 ========================================================================================================================
2024-02-04 06:20:06,750 Logging Sequence: 84_108.00
2024-02-04 06:20:06,751 	Gloss Reference :	A B+C+D+E
2024-02-04 06:20:06,751 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:20:06,751 	Gloss Alignment :	         
2024-02-04 06:20:06,751 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:20:06,752 	Text Reference  :	so in order to show their protest they     covered their mouth in    the   photos which then went   viral
2024-02-04 06:20:06,753 	Text Hypothesis :	** ** ***** ** **** isn't that    shocking 2022    you   are   still going to     focus on   social media
2024-02-04 06:20:06,753 	Text Alignment  :	D  D  D     D  D    S     S       S        S       S     S     S     S     S      S     S    S      S    
2024-02-04 06:20:06,753 ========================================================================================================================
2024-02-04 06:20:06,753 Logging Sequence: 115_24.00
2024-02-04 06:20:06,753 	Gloss Reference :	A B+C+D+E
2024-02-04 06:20:06,753 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:20:06,753 	Gloss Alignment :	         
2024-02-04 06:20:06,754 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:20:06,754 	Text Reference  :	bumrah also did not   participate in the 5        match t20 series  
2024-02-04 06:20:06,754 	Text Hypothesis :	****** **** the image was         of his marriage to    be  together
2024-02-04 06:20:06,755 	Text Alignment  :	D      D    S   S     S           S  S   S        S     S   S       
2024-02-04 06:20:06,755 ========================================================================================================================
2024-02-04 06:20:06,755 Logging Sequence: 96_129.00
2024-02-04 06:20:06,755 	Gloss Reference :	A B+C+D+E
2024-02-04 06:20:06,755 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:20:06,755 	Gloss Alignment :	         
2024-02-04 06:20:06,755 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:20:06,756 	Text Reference  :	***** *** ****** *** **** **** *** **** viewers were  very stressed
2024-02-04 06:20:06,756 	Text Hypothesis :	while the couple met with this win will be      happy to   win     
2024-02-04 06:20:06,756 	Text Alignment  :	I     I   I      I   I    I    I   I    S       S     S    S       
2024-02-04 06:20:06,756 ========================================================================================================================
2024-02-04 06:20:09,818 Epoch 508: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.13 
2024-02-04 06:20:09,818 EPOCH 509
2024-02-04 06:20:14,922 [Epoch: 509 Step: 00034100] Batch Recognition Loss:   0.000296 => Gls Tokens per Sec:     1989 || Batch Translation Loss:   0.061504 => Txt Tokens per Sec:     5532 || Lr: 0.000100
2024-02-04 06:20:15,112 Epoch 509: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.40 
2024-02-04 06:20:15,112 EPOCH 510
2024-02-04 06:20:20,412 Epoch 510: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.95 
2024-02-04 06:20:20,412 EPOCH 511
2024-02-04 06:20:22,608 [Epoch: 511 Step: 00034200] Batch Recognition Loss:   0.000290 => Gls Tokens per Sec:     2146 || Batch Translation Loss:   0.030098 => Txt Tokens per Sec:     5822 || Lr: 0.000100
2024-02-04 06:20:25,714 Epoch 511: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.77 
2024-02-04 06:20:25,715 EPOCH 512
2024-02-04 06:20:30,372 [Epoch: 512 Step: 00034300] Batch Recognition Loss:   0.000177 => Gls Tokens per Sec:     2146 || Batch Translation Loss:   0.064989 => Txt Tokens per Sec:     6011 || Lr: 0.000100
2024-02-04 06:20:30,592 Epoch 512: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.15 
2024-02-04 06:20:30,592 EPOCH 513
2024-02-04 06:20:36,046 Epoch 513: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.19 
2024-02-04 06:20:36,046 EPOCH 514
2024-02-04 06:20:38,375 [Epoch: 514 Step: 00034400] Batch Recognition Loss:   0.000242 => Gls Tokens per Sec:     1955 || Batch Translation Loss:   0.106364 => Txt Tokens per Sec:     5617 || Lr: 0.000100
2024-02-04 06:20:40,960 Epoch 514: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.26 
2024-02-04 06:20:40,960 EPOCH 515
2024-02-04 06:20:45,510 [Epoch: 515 Step: 00034500] Batch Recognition Loss:   0.000600 => Gls Tokens per Sec:     2161 || Batch Translation Loss:   0.044934 => Txt Tokens per Sec:     6056 || Lr: 0.000100
2024-02-04 06:20:45,868 Epoch 515: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.33 
2024-02-04 06:20:45,868 EPOCH 516
2024-02-04 06:20:51,524 Epoch 516: Total Training Recognition Loss 0.02  Total Training Translation Loss 9.62 
2024-02-04 06:20:51,524 EPOCH 517
2024-02-04 06:20:53,787 [Epoch: 517 Step: 00034600] Batch Recognition Loss:   0.000345 => Gls Tokens per Sec:     1981 || Batch Translation Loss:   0.090264 => Txt Tokens per Sec:     5520 || Lr: 0.000100
2024-02-04 06:20:56,929 Epoch 517: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.59 
2024-02-04 06:20:56,929 EPOCH 518
2024-02-04 06:21:01,805 [Epoch: 518 Step: 00034700] Batch Recognition Loss:   0.000234 => Gls Tokens per Sec:     1984 || Batch Translation Loss:   0.054383 => Txt Tokens per Sec:     5507 || Lr: 0.000100
2024-02-04 06:21:02,275 Epoch 518: Total Training Recognition Loss 0.02  Total Training Translation Loss 9.01 
2024-02-04 06:21:02,276 EPOCH 519
2024-02-04 06:21:07,778 Epoch 519: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.99 
2024-02-04 06:21:07,778 EPOCH 520
2024-02-04 06:21:09,890 [Epoch: 520 Step: 00034800] Batch Recognition Loss:   0.000358 => Gls Tokens per Sec:     2046 || Batch Translation Loss:   0.051643 => Txt Tokens per Sec:     5640 || Lr: 0.000100
2024-02-04 06:21:13,233 Epoch 520: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.19 
2024-02-04 06:21:13,233 EPOCH 521
2024-02-04 06:21:18,023 [Epoch: 521 Step: 00034900] Batch Recognition Loss:   0.000171 => Gls Tokens per Sec:     1986 || Batch Translation Loss:   0.029456 => Txt Tokens per Sec:     5494 || Lr: 0.000100
2024-02-04 06:21:18,618 Epoch 521: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.52 
2024-02-04 06:21:18,619 EPOCH 522
2024-02-04 06:21:24,086 Epoch 522: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.45 
2024-02-04 06:21:24,086 EPOCH 523
2024-02-04 06:21:26,169 [Epoch: 523 Step: 00035000] Batch Recognition Loss:   0.000462 => Gls Tokens per Sec:     1998 || Batch Translation Loss:   0.038568 => Txt Tokens per Sec:     5677 || Lr: 0.000100
2024-02-04 06:21:29,300 Epoch 523: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.19 
2024-02-04 06:21:29,300 EPOCH 524
2024-02-04 06:21:33,932 [Epoch: 524 Step: 00035100] Batch Recognition Loss:   0.000281 => Gls Tokens per Sec:     2019 || Batch Translation Loss:   0.033534 => Txt Tokens per Sec:     5550 || Lr: 0.000100
2024-02-04 06:21:34,704 Epoch 524: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.30 
2024-02-04 06:21:34,704 EPOCH 525
2024-02-04 06:21:39,705 Epoch 525: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.01 
2024-02-04 06:21:39,706 EPOCH 526
2024-02-04 06:21:41,789 [Epoch: 526 Step: 00035200] Batch Recognition Loss:   0.000227 => Gls Tokens per Sec:     1921 || Batch Translation Loss:   0.015195 => Txt Tokens per Sec:     5264 || Lr: 0.000100
2024-02-04 06:21:45,224 Epoch 526: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.18 
2024-02-04 06:21:45,224 EPOCH 527
2024-02-04 06:21:49,606 [Epoch: 527 Step: 00035300] Batch Recognition Loss:   0.000338 => Gls Tokens per Sec:     2098 || Batch Translation Loss:   0.041663 => Txt Tokens per Sec:     5713 || Lr: 0.000100
2024-02-04 06:21:50,491 Epoch 527: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.64 
2024-02-04 06:21:50,491 EPOCH 528
2024-02-04 06:21:55,654 Epoch 528: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.10 
2024-02-04 06:21:55,655 EPOCH 529
2024-02-04 06:21:57,627 [Epoch: 529 Step: 00035400] Batch Recognition Loss:   0.000244 => Gls Tokens per Sec:     1948 || Batch Translation Loss:   0.022954 => Txt Tokens per Sec:     5349 || Lr: 0.000100
2024-02-04 06:22:01,111 Epoch 529: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.00 
2024-02-04 06:22:01,111 EPOCH 530
2024-02-04 06:22:05,345 [Epoch: 530 Step: 00035500] Batch Recognition Loss:   0.000288 => Gls Tokens per Sec:     2134 || Batch Translation Loss:   0.198069 => Txt Tokens per Sec:     5908 || Lr: 0.000100
2024-02-04 06:22:06,231 Epoch 530: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.64 
2024-02-04 06:22:06,231 EPOCH 531
2024-02-04 06:22:11,500 Epoch 531: Total Training Recognition Loss 0.02  Total Training Translation Loss 8.54 
2024-02-04 06:22:11,500 EPOCH 532
2024-02-04 06:22:13,054 [Epoch: 532 Step: 00035600] Batch Recognition Loss:   0.000464 => Gls Tokens per Sec:     2370 || Batch Translation Loss:   0.088669 => Txt Tokens per Sec:     6228 || Lr: 0.000100
2024-02-04 06:22:16,977 Epoch 532: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.36 
2024-02-04 06:22:16,977 EPOCH 533
2024-02-04 06:22:21,179 [Epoch: 533 Step: 00035700] Batch Recognition Loss:   0.000286 => Gls Tokens per Sec:     2112 || Batch Translation Loss:   0.038243 => Txt Tokens per Sec:     5883 || Lr: 0.000100
2024-02-04 06:22:22,172 Epoch 533: Total Training Recognition Loss 0.02  Total Training Translation Loss 8.42 
2024-02-04 06:22:22,173 EPOCH 534
2024-02-04 06:22:27,447 Epoch 534: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.11 
2024-02-04 06:22:27,448 EPOCH 535
2024-02-04 06:22:28,808 [Epoch: 535 Step: 00035800] Batch Recognition Loss:   0.000246 => Gls Tokens per Sec:     2589 || Batch Translation Loss:   0.027608 => Txt Tokens per Sec:     6894 || Lr: 0.000100
2024-02-04 06:22:32,863 Epoch 535: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.88 
2024-02-04 06:22:32,864 EPOCH 536
2024-02-04 06:22:37,282 [Epoch: 536 Step: 00035900] Batch Recognition Loss:   0.000258 => Gls Tokens per Sec:     1972 || Batch Translation Loss:   0.143341 => Txt Tokens per Sec:     5527 || Lr: 0.000100
2024-02-04 06:22:38,152 Epoch 536: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.95 
2024-02-04 06:22:38,152 EPOCH 537
2024-02-04 06:22:43,484 Epoch 537: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.60 
2024-02-04 06:22:43,484 EPOCH 538
2024-02-04 06:22:44,948 [Epoch: 538 Step: 00036000] Batch Recognition Loss:   0.000476 => Gls Tokens per Sec:     2236 || Batch Translation Loss:   0.015080 => Txt Tokens per Sec:     6209 || Lr: 0.000100
2024-02-04 06:22:53,515 Validation result at epoch 538, step    36000: duration: 8.5653s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00120	Translation Loss: 92463.72656	PPL: 10434.25684
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.81	(BLEU-1: 10.30,	BLEU-2: 3.50,	BLEU-3: 1.56,	BLEU-4: 0.81)
	CHRF 17.12	ROUGE 8.85
2024-02-04 06:22:53,516 Logging Recognition and Translation Outputs
2024-02-04 06:22:53,516 ========================================================================================================================
2024-02-04 06:22:53,516 Logging Sequence: 78_198.00
2024-02-04 06:22:53,516 	Gloss Reference :	A B+C+D+E
2024-02-04 06:22:53,516 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:22:53,516 	Gloss Alignment :	         
2024-02-04 06:22:53,516 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:22:53,517 	Text Reference  :	******* **** ** ***** ** *** *** ****** they have been  flooded with congratulations comments  
2024-02-04 06:22:53,517 	Text Hypothesis :	england lost to italy in the ipl rights for  each other teams   of   the             semi-final
2024-02-04 06:22:53,517 	Text Alignment  :	I       I    I  I     I  I   I   I      S    S    S     S       S    S               S         
2024-02-04 06:22:53,518 ========================================================================================================================
2024-02-04 06:22:53,518 Logging Sequence: 145_216.00
2024-02-04 06:22:53,518 	Gloss Reference :	A B+C+D+E
2024-02-04 06:22:53,518 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:22:53,518 	Gloss Alignment :	         
2024-02-04 06:22:53,518 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:22:53,519 	Text Reference  :	asking him to include sameeha in  the  world championship as      she was a       talented athlete  
2024-02-04 06:22:53,519 	Text Hypothesis :	****** *** ** ******* he      has also taken it's         revenge by  his batting and      captaincy
2024-02-04 06:22:53,519 	Text Alignment  :	D      D   D  D       S       S   S    S     S            S       S   S   S       S        S        
2024-02-04 06:22:53,520 ========================================================================================================================
2024-02-04 06:22:53,520 Logging Sequence: 70_137.00
2024-02-04 06:22:53,520 	Gloss Reference :	A B+C+D+E
2024-02-04 06:22:53,520 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:22:53,520 	Gloss Alignment :	         
2024-02-04 06:22:53,520 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:22:53,522 	Text Reference  :	the ******** small gesture appeared    to  encourage people to  drink water instead    of aerated drinks  
2024-02-04 06:22:53,522 	Text Hypothesis :	the olympics still haven't apprehended the olympic   games  and she   took  spectators to various olympics
2024-02-04 06:22:53,522 	Text Alignment  :	    I        S     S       S           S   S         S      S   S     S     S          S  S       S       
2024-02-04 06:22:53,522 ========================================================================================================================
2024-02-04 06:22:53,522 Logging Sequence: 119_20.00
2024-02-04 06:22:53,522 	Gloss Reference :	A B+C+D+E
2024-02-04 06:22:53,522 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:22:53,522 	Gloss Alignment :	         
2024-02-04 06:22:53,523 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:22:53,524 	Text Reference  :	messi intended to  gift  something to      all the  players and the staff to special to     celebrate the     moment
2024-02-04 06:22:53,524 	Text Hypothesis :	as    per      the mayor of        rosario is  also termed  as  the ***** ** ******* reason for       idesign gold  
2024-02-04 06:22:53,524 	Text Alignment  :	S     S        S   S     S         S       S   S    S       S       D     D  D       S      S         S       S     
2024-02-04 06:22:53,524 ========================================================================================================================
2024-02-04 06:22:53,525 Logging Sequence: 106_15.00
2024-02-04 06:22:53,525 	Gloss Reference :	A B+C+D+E
2024-02-04 06:22:53,525 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:22:53,525 	Gloss Alignment :	         
2024-02-04 06:22:53,525 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:22:53,526 	Text Reference  :	but what about   women's cricket   earlier we      never spoke about it ** **** **
2024-02-04 06:22:53,526 	Text Hypothesis :	so  what amazing women   wrestlers are     stunned to    know  that  it is also up
2024-02-04 06:22:53,526 	Text Alignment  :	S        S       S       S         S       S       S     S     S        I  I    I 
2024-02-04 06:22:53,527 ========================================================================================================================
2024-02-04 06:22:57,473 Epoch 538: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.19 
2024-02-04 06:22:57,473 EPOCH 539
2024-02-04 06:23:01,837 [Epoch: 539 Step: 00036100] Batch Recognition Loss:   0.000201 => Gls Tokens per Sec:     1960 || Batch Translation Loss:   0.033515 => Txt Tokens per Sec:     5430 || Lr: 0.000050
2024-02-04 06:23:02,972 Epoch 539: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.70 
2024-02-04 06:23:02,973 EPOCH 540
2024-02-04 06:23:08,414 Epoch 540: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.53 
2024-02-04 06:23:08,414 EPOCH 541
2024-02-04 06:23:10,016 [Epoch: 541 Step: 00036200] Batch Recognition Loss:   0.000170 => Gls Tokens per Sec:     1943 || Batch Translation Loss:   0.041080 => Txt Tokens per Sec:     5254 || Lr: 0.000050
2024-02-04 06:23:13,927 Epoch 541: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.44 
2024-02-04 06:23:13,928 EPOCH 542
2024-02-04 06:23:18,196 [Epoch: 542 Step: 00036300] Batch Recognition Loss:   0.000271 => Gls Tokens per Sec:     1988 || Batch Translation Loss:   0.014268 => Txt Tokens per Sec:     5521 || Lr: 0.000050
2024-02-04 06:23:19,339 Epoch 542: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.33 
2024-02-04 06:23:19,339 EPOCH 543
2024-02-04 06:23:24,706 Epoch 543: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.29 
2024-02-04 06:23:24,706 EPOCH 544
2024-02-04 06:23:26,008 [Epoch: 544 Step: 00036400] Batch Recognition Loss:   0.000156 => Gls Tokens per Sec:     2338 || Batch Translation Loss:   0.016994 => Txt Tokens per Sec:     6444 || Lr: 0.000050
2024-02-04 06:23:30,032 Epoch 544: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.97 
2024-02-04 06:23:30,033 EPOCH 545
2024-02-04 06:23:34,264 [Epoch: 545 Step: 00036500] Batch Recognition Loss:   0.000141 => Gls Tokens per Sec:     1967 || Batch Translation Loss:   0.014634 => Txt Tokens per Sec:     5429 || Lr: 0.000050
2024-02-04 06:23:35,489 Epoch 545: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-04 06:23:35,490 EPOCH 546
2024-02-04 06:23:40,356 Epoch 546: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.91 
2024-02-04 06:23:40,356 EPOCH 547
2024-02-04 06:23:41,806 [Epoch: 547 Step: 00036600] Batch Recognition Loss:   0.000126 => Gls Tokens per Sec:     1988 || Batch Translation Loss:   0.016801 => Txt Tokens per Sec:     5446 || Lr: 0.000050
2024-02-04 06:23:45,843 Epoch 547: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.88 
2024-02-04 06:23:45,843 EPOCH 548
2024-02-04 06:23:49,768 [Epoch: 548 Step: 00036700] Batch Recognition Loss:   0.000189 => Gls Tokens per Sec:     2079 || Batch Translation Loss:   0.012359 => Txt Tokens per Sec:     5706 || Lr: 0.000050
2024-02-04 06:23:51,220 Epoch 548: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.85 
2024-02-04 06:23:51,220 EPOCH 549
2024-02-04 06:23:56,432 Epoch 549: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.01 
2024-02-04 06:23:56,433 EPOCH 550
2024-02-04 06:23:57,990 [Epoch: 550 Step: 00036800] Batch Recognition Loss:   0.000176 => Gls Tokens per Sec:     1748 || Batch Translation Loss:   0.014054 => Txt Tokens per Sec:     5092 || Lr: 0.000050
2024-02-04 06:24:02,000 Epoch 550: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.48 
2024-02-04 06:24:02,000 EPOCH 551
2024-02-04 06:24:05,754 [Epoch: 551 Step: 00036900] Batch Recognition Loss:   0.000107 => Gls Tokens per Sec:     2132 || Batch Translation Loss:   0.004854 => Txt Tokens per Sec:     5892 || Lr: 0.000050
2024-02-04 06:24:07,150 Epoch 551: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.00 
2024-02-04 06:24:07,150 EPOCH 552
2024-02-04 06:24:12,345 Epoch 552: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.57 
2024-02-04 06:24:12,346 EPOCH 553
2024-02-04 06:24:13,580 [Epoch: 553 Step: 00037000] Batch Recognition Loss:   0.000200 => Gls Tokens per Sec:     2076 || Batch Translation Loss:   0.010437 => Txt Tokens per Sec:     5733 || Lr: 0.000050
2024-02-04 06:24:17,815 Epoch 553: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.79 
2024-02-04 06:24:17,815 EPOCH 554
2024-02-04 06:24:21,714 [Epoch: 554 Step: 00037100] Batch Recognition Loss:   0.000173 => Gls Tokens per Sec:     1989 || Batch Translation Loss:   0.034328 => Txt Tokens per Sec:     5594 || Lr: 0.000050
2024-02-04 06:24:23,122 Epoch 554: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.24 
2024-02-04 06:24:23,122 EPOCH 555
2024-02-04 06:24:28,374 Epoch 555: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.30 
2024-02-04 06:24:28,374 EPOCH 556
2024-02-04 06:24:29,529 [Epoch: 556 Step: 00037200] Batch Recognition Loss:   0.000161 => Gls Tokens per Sec:     2002 || Batch Translation Loss:   0.015342 => Txt Tokens per Sec:     5414 || Lr: 0.000050
2024-02-04 06:24:33,712 Epoch 556: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.57 
2024-02-04 06:24:33,712 EPOCH 557
2024-02-04 06:24:37,906 [Epoch: 557 Step: 00037300] Batch Recognition Loss:   0.000191 => Gls Tokens per Sec:     1811 || Batch Translation Loss:   0.013197 => Txt Tokens per Sec:     5130 || Lr: 0.000050
2024-02-04 06:24:39,303 Epoch 557: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.29 
2024-02-04 06:24:39,304 EPOCH 558
2024-02-04 06:24:44,273 Epoch 558: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.15 
2024-02-04 06:24:44,274 EPOCH 559
2024-02-04 06:24:45,257 [Epoch: 559 Step: 00037400] Batch Recognition Loss:   0.000200 => Gls Tokens per Sec:     2279 || Batch Translation Loss:   0.012407 => Txt Tokens per Sec:     6575 || Lr: 0.000050
2024-02-04 06:24:49,618 Epoch 559: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.42 
2024-02-04 06:24:49,619 EPOCH 560
2024-02-04 06:24:53,220 [Epoch: 560 Step: 00037500] Batch Recognition Loss:   0.000196 => Gls Tokens per Sec:     2064 || Batch Translation Loss:   0.022326 => Txt Tokens per Sec:     5705 || Lr: 0.000050
2024-02-04 06:24:54,721 Epoch 560: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.03 
2024-02-04 06:24:54,722 EPOCH 561
2024-02-04 06:25:00,428 Epoch 561: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.79 
2024-02-04 06:25:00,428 EPOCH 562
2024-02-04 06:25:01,529 [Epoch: 562 Step: 00037600] Batch Recognition Loss:   0.000246 => Gls Tokens per Sec:     1891 || Batch Translation Loss:   0.021869 => Txt Tokens per Sec:     5395 || Lr: 0.000050
2024-02-04 06:25:05,826 Epoch 562: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.26 
2024-02-04 06:25:05,826 EPOCH 563
2024-02-04 06:25:09,014 [Epoch: 563 Step: 00037700] Batch Recognition Loss:   0.000307 => Gls Tokens per Sec:     2281 || Batch Translation Loss:   0.007301 => Txt Tokens per Sec:     6317 || Lr: 0.000050
2024-02-04 06:25:10,402 Epoch 563: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.15 
2024-02-04 06:25:10,402 EPOCH 564
2024-02-04 06:25:15,912 Epoch 564: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.18 
2024-02-04 06:25:15,912 EPOCH 565
2024-02-04 06:25:17,061 [Epoch: 565 Step: 00037800] Batch Recognition Loss:   0.000229 => Gls Tokens per Sec:     1594 || Batch Translation Loss:   0.028847 => Txt Tokens per Sec:     4698 || Lr: 0.000050
2024-02-04 06:25:20,842 Epoch 565: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.83 
2024-02-04 06:25:20,843 EPOCH 566
2024-02-04 06:25:24,446 [Epoch: 566 Step: 00037900] Batch Recognition Loss:   0.000248 => Gls Tokens per Sec:     1974 || Batch Translation Loss:   0.033525 => Txt Tokens per Sec:     5498 || Lr: 0.000050
2024-02-04 06:25:26,422 Epoch 566: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.68 
2024-02-04 06:25:26,423 EPOCH 567
2024-02-04 06:25:31,907 Epoch 567: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.08 
2024-02-04 06:25:31,907 EPOCH 568
2024-02-04 06:25:32,683 [Epoch: 568 Step: 00038000] Batch Recognition Loss:   0.000165 => Gls Tokens per Sec:     2277 || Batch Translation Loss:   0.009297 => Txt Tokens per Sec:     6254 || Lr: 0.000050
2024-02-04 06:25:41,187 Validation result at epoch 568, step    38000: duration: 8.5040s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00120	Translation Loss: 93236.91406	PPL: 11273.64648
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.74	(BLEU-1: 10.16,	BLEU-2: 3.29,	BLEU-3: 1.45,	BLEU-4: 0.74)
	CHRF 16.99	ROUGE 8.85
2024-02-04 06:25:41,188 Logging Recognition and Translation Outputs
2024-02-04 06:25:41,188 ========================================================================================================================
2024-02-04 06:25:41,188 Logging Sequence: 72_194.00
2024-02-04 06:25:41,188 	Gloss Reference :	A B+C+D+E
2024-02-04 06:25:41,189 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:25:41,189 	Gloss Alignment :	         
2024-02-04 06:25:41,189 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:25:41,190 	Text Reference  :	shah told her to do what she   wants and  filed  a    police complaint against her    
2024-02-04 06:25:41,190 	Text Hypothesis :	**** **** *** ** ** **** babar kept  such people were glued  to        their   victory
2024-02-04 06:25:41,190 	Text Alignment  :	D    D    D   D  D  D    S     S     S    S      S    S      S         S       S      
2024-02-04 06:25:41,190 ========================================================================================================================
2024-02-04 06:25:41,190 Logging Sequence: 108_59.00
2024-02-04 06:25:41,191 	Gloss Reference :	A B+C+D+E
2024-02-04 06:25:41,191 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:25:41,191 	Gloss Alignment :	         
2024-02-04 06:25:41,191 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:25:41,193 	Text Reference  :	ishan kishan remained the biggest buy of ipl   as mumbai indians paid     a         whopping rs     1525 crore to keep him   
2024-02-04 06:25:41,193 	Text Hypothesis :	***** ****** ******** *** ******* he  is known as the    most    followed cricketer with     people are  when  2  of   vamika
2024-02-04 06:25:41,193 	Text Alignment  :	D     D      D        D   D       S   S  S        S      S       S        S         S        S      S    S     S  S    S     
2024-02-04 06:25:41,193 ========================================================================================================================
2024-02-04 06:25:41,193 Logging Sequence: 109_10.00
2024-02-04 06:25:41,194 	Gloss Reference :	A B+C+D+E
2024-02-04 06:25:41,194 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:25:41,194 	Gloss Alignment :	         
2024-02-04 06:25:41,194 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:25:41,195 	Text Reference  :	was scheduled to    be  played at      the   narendra modi  stadium in  ahmedabad
2024-02-04 06:25:41,195 	Text Hypothesis :	*** ********* there are many   batsmen about 4        times if      any players  
2024-02-04 06:25:41,195 	Text Alignment  :	D   D         S     S   S      S       S     S        S     S       S   S        
2024-02-04 06:25:41,195 ========================================================================================================================
2024-02-04 06:25:41,195 Logging Sequence: 103_202.00
2024-02-04 06:25:41,196 	Gloss Reference :	A B+C+D+E
2024-02-04 06:25:41,196 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:25:41,196 	Gloss Alignment :	         
2024-02-04 06:25:41,196 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:25:41,197 	Text Reference  :	india in total has won 61           medals including 22           gold medals 16      silver    medals 23     bronze      medals
2024-02-04 06:25:41,197 	Text Hypothesis :	***** ** ***** *** *** commonwealth games  encourage independence from the    british democracy human  rights development etc   
2024-02-04 06:25:41,198 	Text Alignment  :	D     D  D     D   D   S            S      S         S            S    S      S       S         S      S      S           S     
2024-02-04 06:25:41,198 ========================================================================================================================
2024-02-04 06:25:41,198 Logging Sequence: 149_77.00
2024-02-04 06:25:41,198 	Gloss Reference :	A B+C+D+E
2024-02-04 06:25:41,198 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:25:41,198 	Gloss Alignment :	         
2024-02-04 06:25:41,198 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:25:41,200 	Text Reference  :	and arrested danushka for  alleged sexual assault  of a 29     year       old  woman whose name has not been disclosed
2024-02-04 06:25:41,200 	Text Hypothesis :	*** woman    from     rose bay     a      suburban of * sydney complained that she   was   not  get rs  1    crore    
2024-02-04 06:25:41,200 	Text Alignment  :	D   S        S        S    S       S      S           D S      S          S    S     S     S    S   S   S    S        
2024-02-04 06:25:41,201 ========================================================================================================================
2024-02-04 06:25:45,799 Epoch 568: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.05 
2024-02-04 06:25:45,799 EPOCH 569
2024-02-04 06:25:49,295 [Epoch: 569 Step: 00038100] Batch Recognition Loss:   0.000159 => Gls Tokens per Sec:     2014 || Batch Translation Loss:   0.020623 => Txt Tokens per Sec:     5559 || Lr: 0.000050
2024-02-04 06:25:51,333 Epoch 569: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.73 
2024-02-04 06:25:51,333 EPOCH 570
2024-02-04 06:25:56,529 Epoch 570: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.38 
2024-02-04 06:25:56,530 EPOCH 571
2024-02-04 06:25:57,373 [Epoch: 571 Step: 00038200] Batch Recognition Loss:   0.000143 => Gls Tokens per Sec:     1902 || Batch Translation Loss:   0.010057 => Txt Tokens per Sec:     5123 || Lr: 0.000050
2024-02-04 06:26:01,833 Epoch 571: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.91 
2024-02-04 06:26:01,833 EPOCH 572
2024-02-04 06:26:05,236 [Epoch: 572 Step: 00038300] Batch Recognition Loss:   0.000159 => Gls Tokens per Sec:     2023 || Batch Translation Loss:   0.006645 => Txt Tokens per Sec:     5481 || Lr: 0.000050
2024-02-04 06:26:07,333 Epoch 572: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.21 
2024-02-04 06:26:07,333 EPOCH 573
2024-02-04 06:26:12,394 Epoch 573: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.28 
2024-02-04 06:26:12,395 EPOCH 574
2024-02-04 06:26:13,056 [Epoch: 574 Step: 00038400] Batch Recognition Loss:   0.000148 => Gls Tokens per Sec:     2181 || Batch Translation Loss:   0.011850 => Txt Tokens per Sec:     5835 || Lr: 0.000050
2024-02-04 06:26:17,757 Epoch 574: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.11 
2024-02-04 06:26:17,758 EPOCH 575
2024-02-04 06:26:20,815 [Epoch: 575 Step: 00038500] Batch Recognition Loss:   0.000179 => Gls Tokens per Sec:     2168 || Batch Translation Loss:   0.015645 => Txt Tokens per Sec:     6010 || Lr: 0.000050
2024-02-04 06:26:22,828 Epoch 575: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.32 
2024-02-04 06:26:22,828 EPOCH 576
2024-02-04 06:26:28,179 Epoch 576: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.35 
2024-02-04 06:26:28,180 EPOCH 577
2024-02-04 06:26:28,799 [Epoch: 577 Step: 00038600] Batch Recognition Loss:   0.000157 => Gls Tokens per Sec:     2071 || Batch Translation Loss:   0.010787 => Txt Tokens per Sec:     5910 || Lr: 0.000050
2024-02-04 06:26:33,483 Epoch 577: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.41 
2024-02-04 06:26:33,483 EPOCH 578
2024-02-04 06:26:36,277 [Epoch: 578 Step: 00038700] Batch Recognition Loss:   0.000186 => Gls Tokens per Sec:     2349 || Batch Translation Loss:   0.015798 => Txt Tokens per Sec:     6534 || Lr: 0.000050
2024-02-04 06:26:38,515 Epoch 578: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.99 
2024-02-04 06:26:38,516 EPOCH 579
2024-02-04 06:26:43,980 Epoch 579: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.16 
2024-02-04 06:26:43,980 EPOCH 580
2024-02-04 06:26:44,467 [Epoch: 580 Step: 00038800] Batch Recognition Loss:   0.000166 => Gls Tokens per Sec:     2303 || Batch Translation Loss:   0.016106 => Txt Tokens per Sec:     6329 || Lr: 0.000050
2024-02-04 06:26:49,075 Epoch 580: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.22 
2024-02-04 06:26:49,076 EPOCH 581
2024-02-04 06:26:52,244 [Epoch: 581 Step: 00038900] Batch Recognition Loss:   0.000131 => Gls Tokens per Sec:     2021 || Batch Translation Loss:   0.015628 => Txt Tokens per Sec:     5748 || Lr: 0.000050
2024-02-04 06:26:54,384 Epoch 581: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.32 
2024-02-04 06:26:54,384 EPOCH 582
2024-02-04 06:26:59,749 Epoch 582: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.54 
2024-02-04 06:26:59,750 EPOCH 583
2024-02-04 06:27:00,140 [Epoch: 583 Step: 00039000] Batch Recognition Loss:   0.000217 => Gls Tokens per Sec:     2464 || Batch Translation Loss:   0.029178 => Txt Tokens per Sec:     6415 || Lr: 0.000050
2024-02-04 06:27:04,836 Epoch 583: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.22 
2024-02-04 06:27:04,837 EPOCH 584
2024-02-04 06:27:08,142 [Epoch: 584 Step: 00039100] Batch Recognition Loss:   0.000145 => Gls Tokens per Sec:     1861 || Batch Translation Loss:   0.044346 => Txt Tokens per Sec:     5368 || Lr: 0.000050
2024-02-04 06:27:10,371 Epoch 584: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.38 
2024-02-04 06:27:10,371 EPOCH 585
2024-02-04 06:27:15,563 Epoch 585: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.80 
2024-02-04 06:27:15,564 EPOCH 586
2024-02-04 06:27:16,014 [Epoch: 586 Step: 00039200] Batch Recognition Loss:   0.000342 => Gls Tokens per Sec:     1784 || Batch Translation Loss:   0.015027 => Txt Tokens per Sec:     5596 || Lr: 0.000050
2024-02-04 06:27:20,802 Epoch 586: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.33 
2024-02-04 06:27:20,802 EPOCH 587
2024-02-04 06:27:23,438 [Epoch: 587 Step: 00039300] Batch Recognition Loss:   0.000204 => Gls Tokens per Sec:     2273 || Batch Translation Loss:   0.116520 => Txt Tokens per Sec:     6216 || Lr: 0.000050
2024-02-04 06:27:25,900 Epoch 587: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.44 
2024-02-04 06:27:25,900 EPOCH 588
2024-02-04 06:27:31,398 Epoch 588: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.80 
2024-02-04 06:27:31,399 EPOCH 589
2024-02-04 06:27:31,689 [Epoch: 589 Step: 00039400] Batch Recognition Loss:   0.000169 => Gls Tokens per Sec:     2215 || Batch Translation Loss:   0.038942 => Txt Tokens per Sec:     6253 || Lr: 0.000050
2024-02-04 06:27:37,022 Epoch 589: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.23 
2024-02-04 06:27:37,023 EPOCH 590
2024-02-04 06:27:39,704 [Epoch: 590 Step: 00039500] Batch Recognition Loss:   0.000136 => Gls Tokens per Sec:     2176 || Batch Translation Loss:   0.024913 => Txt Tokens per Sec:     6160 || Lr: 0.000050
2024-02-04 06:27:42,041 Epoch 590: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-04 06:27:42,042 EPOCH 591
2024-02-04 06:27:47,266 Epoch 591: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.05 
2024-02-04 06:27:47,266 EPOCH 592
2024-02-04 06:27:47,482 [Epoch: 592 Step: 00039600] Batch Recognition Loss:   0.000158 => Gls Tokens per Sec:     2243 || Batch Translation Loss:   0.011586 => Txt Tokens per Sec:     6322 || Lr: 0.000050
2024-02-04 06:27:52,646 Epoch 592: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.93 
2024-02-04 06:27:52,646 EPOCH 593
2024-02-04 06:27:55,427 [Epoch: 593 Step: 00039700] Batch Recognition Loss:   0.000124 => Gls Tokens per Sec:     2072 || Batch Translation Loss:   0.012858 => Txt Tokens per Sec:     5921 || Lr: 0.000050
2024-02-04 06:27:58,025 Epoch 593: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.91 
2024-02-04 06:27:58,026 EPOCH 594
2024-02-04 06:28:03,492 Epoch 594: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.91 
2024-02-04 06:28:03,492 EPOCH 595
2024-02-04 06:28:03,585 [Epoch: 595 Step: 00039800] Batch Recognition Loss:   0.000156 => Gls Tokens per Sec:     3478 || Batch Translation Loss:   0.007782 => Txt Tokens per Sec:     8011 || Lr: 0.000050
2024-02-04 06:28:08,695 Epoch 595: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.13 
2024-02-04 06:28:08,695 EPOCH 596
2024-02-04 06:28:11,621 [Epoch: 596 Step: 00039900] Batch Recognition Loss:   0.000184 => Gls Tokens per Sec:     1884 || Batch Translation Loss:   0.011974 => Txt Tokens per Sec:     5478 || Lr: 0.000050
2024-02-04 06:28:13,952 Epoch 596: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.25 
2024-02-04 06:28:13,953 EPOCH 597
2024-02-04 06:28:19,531 Epoch 597: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.88 
2024-02-04 06:28:19,531 EPOCH 598
2024-02-04 06:28:19,619 [Epoch: 598 Step: 00040000] Batch Recognition Loss:   0.000173 => Gls Tokens per Sec:     1839 || Batch Translation Loss:   0.014964 => Txt Tokens per Sec:     6069 || Lr: 0.000050
2024-02-04 06:28:28,142 Validation result at epoch 598, step    40000: duration: 8.5237s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00089	Translation Loss: 91782.34375	PPL: 9746.50293
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.75	(BLEU-1: 10.52,	BLEU-2: 3.40,	BLEU-3: 1.48,	BLEU-4: 0.75)
	CHRF 17.21	ROUGE 9.11
2024-02-04 06:28:28,143 Logging Recognition and Translation Outputs
2024-02-04 06:28:28,143 ========================================================================================================================
2024-02-04 06:28:28,143 Logging Sequence: 123_104.00
2024-02-04 06:28:28,143 	Gloss Reference :	A B+C+D+E
2024-02-04 06:28:28,143 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:28:28,144 	Gloss Alignment :	         
2024-02-04 06:28:28,144 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:28:28,145 	Text Reference  :	***** *** *** the car  was presented to the former india cricketer from an unknown person  
2024-02-04 06:28:28,145 	Text Hypothesis :	india had won the toss and decided   to *** ****** ***** field     for  a  gold    medalist
2024-02-04 06:28:28,145 	Text Alignment  :	I     I   I       S    S   S            D   D      D     S         S    S  S       S       
2024-02-04 06:28:28,145 ========================================================================================================================
2024-02-04 06:28:28,145 Logging Sequence: 107_23.00
2024-02-04 06:28:28,145 	Gloss Reference :	A B+C+D+E
2024-02-04 06:28:28,146 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:28:28,146 	Gloss Alignment :	         
2024-02-04 06:28:28,146 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:28:28,146 	Text Reference  :	and viktor lilov  who is  also  from   the  usa 
2024-02-04 06:28:28,146 	Text Hypothesis :	*** ****** phogat won the delhi police made well
2024-02-04 06:28:28,147 	Text Alignment  :	D   D      S      S   S   S     S      S    S   
2024-02-04 06:28:28,147 ========================================================================================================================
2024-02-04 06:28:28,147 Logging Sequence: 134_212.00
2024-02-04 06:28:28,147 	Gloss Reference :	A B+C+D+E
2024-02-04 06:28:28,147 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:28:28,147 	Gloss Alignment :	         
2024-02-04 06:28:28,147 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:28:28,148 	Text Reference  :	** **** ******* ****** *** ******* dhanush said that         he practises little yoga 
2024-02-04 06:28:28,148 	Text Hypothesis :	so many hearing people are stunned at      your achievements i  am        so     proud
2024-02-04 06:28:28,148 	Text Alignment  :	I  I    I       I      I   I       S       S    S            S  S         S      S    
2024-02-04 06:28:28,148 ========================================================================================================================
2024-02-04 06:28:28,149 Logging Sequence: 165_577.00
2024-02-04 06:28:28,149 	Gloss Reference :	A B+C+D+E
2024-02-04 06:28:28,149 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:28:28,149 	Gloss Alignment :	         
2024-02-04 06:28:28,149 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:28:28,150 	Text Reference  :	then after 28 years india    won  the world cup again in   2011
2024-02-04 06:28:28,150 	Text Hypothesis :	**** it    is quite shocking that if  any   has been  many days
2024-02-04 06:28:28,150 	Text Alignment  :	D    S     S  S     S        S    S   S     S   S     S    S   
2024-02-04 06:28:28,150 ========================================================================================================================
2024-02-04 06:28:28,151 Logging Sequence: 88_142.00
2024-02-04 06:28:28,151 	Gloss Reference :	A B+C+D+E
2024-02-04 06:28:28,151 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:28:28,151 	Gloss Alignment :	         
2024-02-04 06:28:28,151 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:28:28,152 	Text Reference  :	*** this  is    because the police ***** does ******** ** ***** not     do  anything
2024-02-04 06:28:28,152 	Text Hypothesis :	the mayor added that    the police never does anything to catch however the ceremony
2024-02-04 06:28:28,152 	Text Alignment  :	I   S     S     S                  I          I        I  I     S       S   S       
2024-02-04 06:28:28,152 ========================================================================================================================
2024-02-04 06:28:33,795 Epoch 598: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.97 
2024-02-04 06:28:33,795 EPOCH 599
2024-02-04 06:28:36,680 [Epoch: 599 Step: 00040100] Batch Recognition Loss:   0.000269 => Gls Tokens per Sec:     1856 || Batch Translation Loss:   0.026501 => Txt Tokens per Sec:     5261 || Lr: 0.000050
2024-02-04 06:28:39,143 Epoch 599: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.58 
2024-02-04 06:28:39,143 EPOCH 600
2024-02-04 06:28:44,331 [Epoch: 600 Step: 00040200] Batch Recognition Loss:   0.000269 => Gls Tokens per Sec:     2050 || Batch Translation Loss:   0.023577 => Txt Tokens per Sec:     5690 || Lr: 0.000050
2024-02-04 06:28:44,331 Epoch 600: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.77 
2024-02-04 06:28:44,332 EPOCH 601
2024-02-04 06:28:49,562 Epoch 601: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.18 
2024-02-04 06:28:49,562 EPOCH 602
2024-02-04 06:28:52,202 [Epoch: 602 Step: 00040300] Batch Recognition Loss:   0.000365 => Gls Tokens per Sec:     2001 || Batch Translation Loss:   0.059833 => Txt Tokens per Sec:     5792 || Lr: 0.000050
2024-02-04 06:28:54,905 Epoch 602: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.84 
2024-02-04 06:28:54,905 EPOCH 603
2024-02-04 06:28:59,777 [Epoch: 603 Step: 00040400] Batch Recognition Loss:   0.000211 => Gls Tokens per Sec:     2149 || Batch Translation Loss:   0.069609 => Txt Tokens per Sec:     5938 || Lr: 0.000050
2024-02-04 06:28:59,895 Epoch 603: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.97 
2024-02-04 06:28:59,895 EPOCH 604
2024-02-04 06:29:04,486 Epoch 604: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.52 
2024-02-04 06:29:04,486 EPOCH 605
2024-02-04 06:29:07,064 [Epoch: 605 Step: 00040500] Batch Recognition Loss:   0.000346 => Gls Tokens per Sec:     1951 || Batch Translation Loss:   0.026658 => Txt Tokens per Sec:     5557 || Lr: 0.000050
2024-02-04 06:29:09,872 Epoch 605: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.66 
2024-02-04 06:29:09,872 EPOCH 606
2024-02-04 06:29:15,032 [Epoch: 606 Step: 00040600] Batch Recognition Loss:   0.000237 => Gls Tokens per Sec:     1999 || Batch Translation Loss:   0.010718 => Txt Tokens per Sec:     5518 || Lr: 0.000050
2024-02-04 06:29:15,252 Epoch 606: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.73 
2024-02-04 06:29:15,253 EPOCH 607
2024-02-04 06:29:20,626 Epoch 607: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.63 
2024-02-04 06:29:20,626 EPOCH 608
2024-02-04 06:29:22,854 [Epoch: 608 Step: 00040700] Batch Recognition Loss:   0.000260 => Gls Tokens per Sec:     2226 || Batch Translation Loss:   0.019576 => Txt Tokens per Sec:     6401 || Lr: 0.000050
2024-02-04 06:29:25,935 Epoch 608: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.17 
2024-02-04 06:29:25,936 EPOCH 609
2024-02-04 06:29:31,034 [Epoch: 609 Step: 00040800] Batch Recognition Loss:   0.000176 => Gls Tokens per Sec:     1991 || Batch Translation Loss:   0.011189 => Txt Tokens per Sec:     5513 || Lr: 0.000050
2024-02-04 06:29:31,338 Epoch 609: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.14 
2024-02-04 06:29:31,339 EPOCH 610
2024-02-04 06:29:36,741 Epoch 610: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.67 
2024-02-04 06:29:36,742 EPOCH 611
2024-02-04 06:29:38,944 [Epoch: 611 Step: 00040900] Batch Recognition Loss:   0.000214 => Gls Tokens per Sec:     2140 || Batch Translation Loss:   0.013916 => Txt Tokens per Sec:     6055 || Lr: 0.000050
2024-02-04 06:29:41,347 Epoch 611: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.62 
2024-02-04 06:29:41,347 EPOCH 612
2024-02-04 06:29:45,505 [Epoch: 612 Step: 00041000] Batch Recognition Loss:   0.000146 => Gls Tokens per Sec:     2403 || Batch Translation Loss:   0.011210 => Txt Tokens per Sec:     6592 || Lr: 0.000050
2024-02-04 06:29:45,939 Epoch 612: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.10 
2024-02-04 06:29:45,939 EPOCH 613
2024-02-04 06:29:51,468 Epoch 613: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.57 
2024-02-04 06:29:51,469 EPOCH 614
2024-02-04 06:29:53,717 [Epoch: 614 Step: 00041100] Batch Recognition Loss:   0.000178 => Gls Tokens per Sec:     2065 || Batch Translation Loss:   0.012134 => Txt Tokens per Sec:     5748 || Lr: 0.000050
2024-02-04 06:29:56,835 Epoch 614: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.12 
2024-02-04 06:29:56,835 EPOCH 615
2024-02-04 06:30:01,740 [Epoch: 615 Step: 00041200] Batch Recognition Loss:   0.000188 => Gls Tokens per Sec:     2005 || Batch Translation Loss:   0.019958 => Txt Tokens per Sec:     5599 || Lr: 0.000050
2024-02-04 06:30:02,185 Epoch 615: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.72 
2024-02-04 06:30:02,185 EPOCH 616
2024-02-04 06:30:07,682 Epoch 616: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.61 
2024-02-04 06:30:07,682 EPOCH 617
2024-02-04 06:30:09,800 [Epoch: 617 Step: 00041300] Batch Recognition Loss:   0.000161 => Gls Tokens per Sec:     2075 || Batch Translation Loss:   0.019526 => Txt Tokens per Sec:     5554 || Lr: 0.000050
2024-02-04 06:30:12,979 Epoch 617: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.10 
2024-02-04 06:30:12,980 EPOCH 618
2024-02-04 06:30:17,919 [Epoch: 618 Step: 00041400] Batch Recognition Loss:   0.000179 => Gls Tokens per Sec:     1958 || Batch Translation Loss:   0.026863 => Txt Tokens per Sec:     5394 || Lr: 0.000050
2024-02-04 06:30:18,500 Epoch 618: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.22 
2024-02-04 06:30:18,500 EPOCH 619
2024-02-04 06:30:23,716 Epoch 619: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.74 
2024-02-04 06:30:23,716 EPOCH 620
2024-02-04 06:30:25,907 [Epoch: 620 Step: 00041500] Batch Recognition Loss:   0.000198 => Gls Tokens per Sec:     1973 || Batch Translation Loss:   0.014130 => Txt Tokens per Sec:     5618 || Lr: 0.000050
2024-02-04 06:30:29,014 Epoch 620: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.02 
2024-02-04 06:30:29,014 EPOCH 621
2024-02-04 06:30:34,027 [Epoch: 621 Step: 00041600] Batch Recognition Loss:   0.000169 => Gls Tokens per Sec:     1897 || Batch Translation Loss:   0.056532 => Txt Tokens per Sec:     5313 || Lr: 0.000050
2024-02-04 06:30:34,490 Epoch 621: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.23 
2024-02-04 06:30:34,490 EPOCH 622
2024-02-04 06:30:39,505 Epoch 622: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.58 
2024-02-04 06:30:39,505 EPOCH 623
2024-02-04 06:30:41,567 [Epoch: 623 Step: 00041700] Batch Recognition Loss:   0.000227 => Gls Tokens per Sec:     2019 || Batch Translation Loss:   0.009310 => Txt Tokens per Sec:     5627 || Lr: 0.000050
2024-02-04 06:30:45,009 Epoch 623: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.17 
2024-02-04 06:30:45,009 EPOCH 624
2024-02-04 06:30:49,804 [Epoch: 624 Step: 00041800] Batch Recognition Loss:   0.000311 => Gls Tokens per Sec:     1969 || Batch Translation Loss:   0.022363 => Txt Tokens per Sec:     5465 || Lr: 0.000050
2024-02-04 06:30:50,469 Epoch 624: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.72 
2024-02-04 06:30:50,469 EPOCH 625
2024-02-04 06:30:55,555 Epoch 625: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.88 
2024-02-04 06:30:55,556 EPOCH 626
2024-02-04 06:30:57,552 [Epoch: 626 Step: 00041900] Batch Recognition Loss:   0.000216 => Gls Tokens per Sec:     2005 || Batch Translation Loss:   0.021814 => Txt Tokens per Sec:     5333 || Lr: 0.000050
2024-02-04 06:31:01,017 Epoch 626: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.95 
2024-02-04 06:31:01,017 EPOCH 627
2024-02-04 06:31:05,741 [Epoch: 627 Step: 00042000] Batch Recognition Loss:   0.000316 => Gls Tokens per Sec:     1946 || Batch Translation Loss:   0.012811 => Txt Tokens per Sec:     5473 || Lr: 0.000050
2024-02-04 06:31:14,117 Validation result at epoch 627, step    42000: duration: 8.3767s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00223	Translation Loss: 90963.48438	PPL: 8979.68652
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.51	(BLEU-1: 9.88,	BLEU-2: 2.78,	BLEU-3: 1.17,	BLEU-4: 0.51)
	CHRF 16.49	ROUGE 8.53
2024-02-04 06:31:14,118 Logging Recognition and Translation Outputs
2024-02-04 06:31:14,118 ========================================================================================================================
2024-02-04 06:31:14,119 Logging Sequence: 81_8.00
2024-02-04 06:31:14,119 	Gloss Reference :	A B+C+D+E
2024-02-04 06:31:14,119 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:31:14,119 	Gloss Alignment :	         
2024-02-04 06:31:14,119 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:31:14,121 	Text Reference  :	have   been    involved in  a huge controversy in connection to real estate developer amrapali group  since    last 7        years
2024-02-04 06:31:14,121 	Text Hypothesis :	yuvraj decided to       get a **** bar         in ********** ** the  uae    oman      the      bcci's decision of   amrapali mahi 
2024-02-04 06:31:14,121 	Text Alignment  :	S      S       S        S     D    S              D          D  S    S      S         S        S      S        S    S        S    
2024-02-04 06:31:14,122 ========================================================================================================================
2024-02-04 06:31:14,122 Logging Sequence: 148_239.00
2024-02-04 06:31:14,122 	Gloss Reference :	A B+C+D+E
2024-02-04 06:31:14,122 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:31:14,122 	Gloss Alignment :	         
2024-02-04 06:31:14,122 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:31:14,123 	Text Reference  :	the ground staff were    very happy and     thanked the bowler for his kind    gesture
2024-02-04 06:31:14,123 	Text Hypothesis :	*** yuvraj and   gambhir have been  flooded with    the ****** *** *** current season 
2024-02-04 06:31:14,123 	Text Alignment  :	D   S      S     S       S    S     S       S           D      D   D   S       S      
2024-02-04 06:31:14,124 ========================================================================================================================
2024-02-04 06:31:14,124 Logging Sequence: 165_8.00
2024-02-04 06:31:14,124 	Gloss Reference :	A B+C+D+E
2024-02-04 06:31:14,124 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:31:14,124 	Gloss Alignment :	         
2024-02-04 06:31:14,124 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:31:14,125 	Text Reference  :	however many don't believe in       it   it   varies  among people
2024-02-04 06:31:14,125 	Text Hypothesis :	******* **** ***** he      believed that this brought him   luck  
2024-02-04 06:31:14,125 	Text Alignment  :	D       D    D     S       S        S    S    S       S     S     
2024-02-04 06:31:14,125 ========================================================================================================================
2024-02-04 06:31:14,125 Logging Sequence: 93_93.00
2024-02-04 06:31:14,126 	Gloss Reference :	A B+C+D+E
2024-02-04 06:31:14,126 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:31:14,126 	Gloss Alignment :	         
2024-02-04 06:31:14,126 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:31:14,127 	Text Reference  :	** ****** ** ******* rooney was at     the club as  well 
2024-02-04 06:31:14,127 	Text Hypothesis :	he seemed to sloshed and    had passed out on   the chair
2024-02-04 06:31:14,127 	Text Alignment  :	I  I      I  I       S      S   S      S   S    S   S    
2024-02-04 06:31:14,127 ========================================================================================================================
2024-02-04 06:31:14,127 Logging Sequence: 96_129.00
2024-02-04 06:31:14,127 	Gloss Reference :	A B+C+D+E
2024-02-04 06:31:14,127 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:31:14,128 	Gloss Alignment :	         
2024-02-04 06:31:14,128 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:31:14,128 	Text Reference  :	***** *** viewers were very stressed
2024-02-04 06:31:14,128 	Text Hypothesis :	while the couple  were **** lost    
2024-02-04 06:31:14,128 	Text Alignment  :	I     I   S            D    S       
2024-02-04 06:31:14,128 ========================================================================================================================
2024-02-04 06:31:14,771 Epoch 627: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.74 
2024-02-04 06:31:14,771 EPOCH 628
2024-02-04 06:31:20,236 Epoch 628: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.09 
2024-02-04 06:31:20,237 EPOCH 629
2024-02-04 06:31:21,802 [Epoch: 629 Step: 00042100] Batch Recognition Loss:   0.000298 => Gls Tokens per Sec:     2454 || Batch Translation Loss:   0.017285 => Txt Tokens per Sec:     6424 || Lr: 0.000050
2024-02-04 06:31:25,655 Epoch 629: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.85 
2024-02-04 06:31:25,655 EPOCH 630
2024-02-04 06:31:29,776 [Epoch: 630 Step: 00042200] Batch Recognition Loss:   0.000210 => Gls Tokens per Sec:     2214 || Batch Translation Loss:   0.019599 => Txt Tokens per Sec:     6194 || Lr: 0.000050
2024-02-04 06:31:30,625 Epoch 630: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.34 
2024-02-04 06:31:30,625 EPOCH 631
2024-02-04 06:31:36,237 Epoch 631: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.41 
2024-02-04 06:31:36,238 EPOCH 632
2024-02-04 06:31:37,815 [Epoch: 632 Step: 00042300] Batch Recognition Loss:   0.000158 => Gls Tokens per Sec:     2278 || Batch Translation Loss:   0.018840 => Txt Tokens per Sec:     5995 || Lr: 0.000050
2024-02-04 06:31:41,627 Epoch 632: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.19 
2024-02-04 06:31:41,628 EPOCH 633
2024-02-04 06:31:45,783 [Epoch: 633 Step: 00042400] Batch Recognition Loss:   0.000197 => Gls Tokens per Sec:     2135 || Batch Translation Loss:   0.017202 => Txt Tokens per Sec:     5814 || Lr: 0.000050
2024-02-04 06:31:46,696 Epoch 633: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-04 06:31:46,696 EPOCH 634
2024-02-04 06:31:51,826 Epoch 634: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.21 
2024-02-04 06:31:51,827 EPOCH 635
2024-02-04 06:31:53,726 [Epoch: 635 Step: 00042500] Batch Recognition Loss:   0.000272 => Gls Tokens per Sec:     1807 || Batch Translation Loss:   0.019391 => Txt Tokens per Sec:     5037 || Lr: 0.000050
2024-02-04 06:31:57,434 Epoch 635: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.20 
2024-02-04 06:31:57,435 EPOCH 636
2024-02-04 06:32:01,822 [Epoch: 636 Step: 00042600] Batch Recognition Loss:   0.000161 => Gls Tokens per Sec:     1986 || Batch Translation Loss:   0.010361 => Txt Tokens per Sec:     5531 || Lr: 0.000050
2024-02-04 06:32:02,740 Epoch 636: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.00 
2024-02-04 06:32:02,740 EPOCH 637
2024-02-04 06:32:07,944 Epoch 637: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.99 
2024-02-04 06:32:07,944 EPOCH 638
2024-02-04 06:32:09,644 [Epoch: 638 Step: 00042700] Batch Recognition Loss:   0.000230 => Gls Tokens per Sec:     1977 || Batch Translation Loss:   0.015751 => Txt Tokens per Sec:     5409 || Lr: 0.000050
2024-02-04 06:32:13,456 Epoch 638: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.02 
2024-02-04 06:32:13,456 EPOCH 639
2024-02-04 06:32:17,265 [Epoch: 639 Step: 00042800] Batch Recognition Loss:   0.000151 => Gls Tokens per Sec:     2246 || Batch Translation Loss:   0.017148 => Txt Tokens per Sec:     6170 || Lr: 0.000050
2024-02-04 06:32:18,546 Epoch 639: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.91 
2024-02-04 06:32:18,547 EPOCH 640
2024-02-04 06:32:24,074 Epoch 640: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.98 
2024-02-04 06:32:24,075 EPOCH 641
2024-02-04 06:32:25,710 [Epoch: 641 Step: 00042900] Batch Recognition Loss:   0.000117 => Gls Tokens per Sec:     1904 || Batch Translation Loss:   0.093629 => Txt Tokens per Sec:     5232 || Lr: 0.000050
2024-02-04 06:32:29,540 Epoch 641: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.95 
2024-02-04 06:32:29,540 EPOCH 642
2024-02-04 06:32:34,036 [Epoch: 642 Step: 00043000] Batch Recognition Loss:   0.000421 => Gls Tokens per Sec:     1887 || Batch Translation Loss:   0.035397 => Txt Tokens per Sec:     5233 || Lr: 0.000050
2024-02-04 06:32:35,241 Epoch 642: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.91 
2024-02-04 06:32:35,241 EPOCH 643
2024-02-04 06:32:40,570 Epoch 643: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.06 
2024-02-04 06:32:40,570 EPOCH 644
2024-02-04 06:32:41,974 [Epoch: 644 Step: 00043100] Batch Recognition Loss:   0.000145 => Gls Tokens per Sec:     2166 || Batch Translation Loss:   0.015575 => Txt Tokens per Sec:     5908 || Lr: 0.000050
2024-02-04 06:32:45,944 Epoch 644: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.94 
2024-02-04 06:32:45,945 EPOCH 645
2024-02-04 06:32:50,056 [Epoch: 645 Step: 00043200] Batch Recognition Loss:   0.000213 => Gls Tokens per Sec:     2003 || Batch Translation Loss:   0.030048 => Txt Tokens per Sec:     5428 || Lr: 0.000050
2024-02-04 06:32:51,550 Epoch 645: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.72 
2024-02-04 06:32:51,550 EPOCH 646
2024-02-04 06:32:57,068 Epoch 646: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.33 
2024-02-04 06:32:57,069 EPOCH 647
2024-02-04 06:32:58,351 [Epoch: 647 Step: 00043300] Batch Recognition Loss:   0.000260 => Gls Tokens per Sec:     2179 || Batch Translation Loss:   0.028711 => Txt Tokens per Sec:     5648 || Lr: 0.000050
2024-02-04 06:33:02,576 Epoch 647: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.64 
2024-02-04 06:33:02,577 EPOCH 648
2024-02-04 06:33:06,634 [Epoch: 648 Step: 00043400] Batch Recognition Loss:   0.000167 => Gls Tokens per Sec:     1990 || Batch Translation Loss:   0.023327 => Txt Tokens per Sec:     5545 || Lr: 0.000050
2024-02-04 06:33:07,932 Epoch 648: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.56 
2024-02-04 06:33:07,933 EPOCH 649
2024-02-04 06:33:13,337 Epoch 649: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.95 
2024-02-04 06:33:13,337 EPOCH 650
2024-02-04 06:33:14,509 [Epoch: 650 Step: 00043500] Batch Recognition Loss:   0.000146 => Gls Tokens per Sec:     2247 || Batch Translation Loss:   0.012351 => Txt Tokens per Sec:     6107 || Lr: 0.000050
2024-02-04 06:33:18,524 Epoch 650: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.54 
2024-02-04 06:33:18,525 EPOCH 651
2024-02-04 06:33:22,671 [Epoch: 651 Step: 00043600] Batch Recognition Loss:   0.000198 => Gls Tokens per Sec:     1909 || Batch Translation Loss:   0.013563 => Txt Tokens per Sec:     5352 || Lr: 0.000050
2024-02-04 06:33:23,990 Epoch 651: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.36 
2024-02-04 06:33:23,991 EPOCH 652
2024-02-04 06:33:29,531 Epoch 652: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.26 
2024-02-04 06:33:29,531 EPOCH 653
2024-02-04 06:33:30,796 [Epoch: 653 Step: 00043700] Batch Recognition Loss:   0.000165 => Gls Tokens per Sec:     2027 || Batch Translation Loss:   0.021406 => Txt Tokens per Sec:     5661 || Lr: 0.000050
2024-02-04 06:33:34,665 Epoch 653: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.26 
2024-02-04 06:33:34,665 EPOCH 654
2024-02-04 06:33:38,584 [Epoch: 654 Step: 00043800] Batch Recognition Loss:   0.000217 => Gls Tokens per Sec:     2001 || Batch Translation Loss:   0.016408 => Txt Tokens per Sec:     5596 || Lr: 0.000050
2024-02-04 06:33:40,063 Epoch 654: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.04 
2024-02-04 06:33:40,063 EPOCH 655
2024-02-04 06:33:44,992 Epoch 655: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.22 
2024-02-04 06:33:44,992 EPOCH 656
2024-02-04 06:33:45,961 [Epoch: 656 Step: 00043900] Batch Recognition Loss:   0.000133 => Gls Tokens per Sec:     2386 || Batch Translation Loss:   0.045893 => Txt Tokens per Sec:     6273 || Lr: 0.000050
2024-02-04 06:33:50,515 Epoch 656: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.27 
2024-02-04 06:33:50,515 EPOCH 657
2024-02-04 06:33:54,516 [Epoch: 657 Step: 00044000] Batch Recognition Loss:   0.000347 => Gls Tokens per Sec:     1897 || Batch Translation Loss:   0.093709 => Txt Tokens per Sec:     5316 || Lr: 0.000050
2024-02-04 06:34:03,032 Validation result at epoch 657, step    44000: duration: 8.5160s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00209	Translation Loss: 90328.07031	PPL: 8426.48438
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.73	(BLEU-1: 11.41,	BLEU-2: 3.67,	BLEU-3: 1.58,	BLEU-4: 0.73)
	CHRF 16.91	ROUGE 9.73
2024-02-04 06:34:03,033 Logging Recognition and Translation Outputs
2024-02-04 06:34:03,033 ========================================================================================================================
2024-02-04 06:34:03,033 Logging Sequence: 117_29.00
2024-02-04 06:34:03,034 	Gloss Reference :	A B+C+D+E
2024-02-04 06:34:03,034 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:34:03,034 	Gloss Alignment :	         
2024-02-04 06:34:03,034 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:34:03,036 	Text Reference  :	however england was  unable to reach the target  they  were  all  out  lost   by      66    runs  
2024-02-04 06:34:03,036 	Text Hypothesis :	the     match   went on     to score -   chennai super kings runs were played between south africa
2024-02-04 06:34:03,036 	Text Alignment  :	S       S       S    S         S     S   S       S     S     S    S    S      S       S     S     
2024-02-04 06:34:03,036 ========================================================================================================================
2024-02-04 06:34:03,036 Logging Sequence: 84_176.00
2024-02-04 06:34:03,036 	Gloss Reference :	A B+C+D+E
2024-02-04 06:34:03,036 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:34:03,036 	Gloss Alignment :	         
2024-02-04 06:34:03,037 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:34:03,038 	Text Reference  :	*** **** *** **** ** germany's nancy faeser  who attended  the *** game in doha   against japan said   
2024-02-04 06:34:03,038 	Text Hypothesis :	and told him that he wanted    to    support the community the ban will be strong to      be    matches
2024-02-04 06:34:03,038 	Text Alignment  :	I   I    I   I    I  S         S     S       S   S             I   S    S  S      S       S     S      
2024-02-04 06:34:03,038 ========================================================================================================================
2024-02-04 06:34:03,038 Logging Sequence: 172_98.00
2024-02-04 06:34:03,038 	Gloss Reference :	A B+C+D+E
2024-02-04 06:34:03,039 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:34:03,039 	Gloss Alignment :	         
2024-02-04 06:34:03,039 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:34:03,040 	Text Reference  :	*** ***** ** *** since      700 pm     it      kept raining the intensity plunged around 915    pm   
2024-02-04 06:34:03,040 	Text Hypothesis :	the final of the tournament was played between csk  and     sri lanka     legends on     social media
2024-02-04 06:34:03,040 	Text Alignment  :	I   I     I  I   S          S   S      S       S    S       S   S         S       S      S      S    
2024-02-04 06:34:03,040 ========================================================================================================================
2024-02-04 06:34:03,040 Logging Sequence: 135_92.00
2024-02-04 06:34:03,041 	Gloss Reference :	A B+C+D+E
2024-02-04 06:34:03,041 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:34:03,041 	Gloss Alignment :	         
2024-02-04 06:34:03,041 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:34:03,043 	Text Reference  :	she wrote that **** ***** half    had      already been raised by  the ****** ***** ** ******* ***** ** ***** family's online fundraiser
2024-02-04 06:34:03,043 	Text Hypothesis :	*** after that they would restart training in      2019 and    won the silver medal in javelin throw in tokyo oylmpics in     2021      
2024-02-04 06:34:03,043 	Text Alignment  :	D   S          I    I     S       S        S       S    S      S       I      I     I  I       I     I  I     S        S      S         
2024-02-04 06:34:03,043 ========================================================================================================================
2024-02-04 06:34:03,043 Logging Sequence: 180_332.00
2024-02-04 06:34:03,043 	Gloss Reference :	A B+C+D+E
2024-02-04 06:34:03,043 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:34:03,044 	Gloss Alignment :	         
2024-02-04 06:34:03,044 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:34:03,045 	Text Reference  :	did i     eat     roti   made of   shilajit that **** **** ** i      got energy   to         assault so        many      girls 
2024-02-04 06:34:03,045 	Text Hypothesis :	*** after talking drinks and  told him      that they will be medals for sexually assaulting female  wrestlers including minors
2024-02-04 06:34:03,046 	Text Alignment  :	D   S     S       S      S    S    S             I    I    I  S      S   S        S          S       S         S         S     
2024-02-04 06:34:03,046 ========================================================================================================================
2024-02-04 06:34:04,417 Epoch 657: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.12 
2024-02-04 06:34:04,417 EPOCH 658
2024-02-04 06:34:10,096 Epoch 658: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.42 
2024-02-04 06:34:10,097 EPOCH 659
2024-02-04 06:34:11,249 [Epoch: 659 Step: 00044100] Batch Recognition Loss:   0.000368 => Gls Tokens per Sec:     1946 || Batch Translation Loss:   0.055412 => Txt Tokens per Sec:     5465 || Lr: 0.000050
2024-02-04 06:34:15,559 Epoch 659: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.07 
2024-02-04 06:34:15,559 EPOCH 660
2024-02-04 06:34:19,231 [Epoch: 660 Step: 00044200] Batch Recognition Loss:   0.000190 => Gls Tokens per Sec:     2049 || Batch Translation Loss:   0.028024 => Txt Tokens per Sec:     5648 || Lr: 0.000050
2024-02-04 06:34:20,775 Epoch 660: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.80 
2024-02-04 06:34:20,775 EPOCH 661
2024-02-04 06:34:25,820 Epoch 661: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.87 
2024-02-04 06:34:25,820 EPOCH 662
2024-02-04 06:34:26,784 [Epoch: 662 Step: 00044300] Batch Recognition Loss:   0.000162 => Gls Tokens per Sec:     2159 || Batch Translation Loss:   0.015088 => Txt Tokens per Sec:     5771 || Lr: 0.000050
2024-02-04 06:34:31,274 Epoch 662: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.38 
2024-02-04 06:34:31,275 EPOCH 663
2024-02-04 06:34:35,109 [Epoch: 663 Step: 00044400] Batch Recognition Loss:   0.000215 => Gls Tokens per Sec:     1897 || Batch Translation Loss:   0.091051 => Txt Tokens per Sec:     5306 || Lr: 0.000050
2024-02-04 06:34:36,694 Epoch 663: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.31 
2024-02-04 06:34:36,694 EPOCH 664
2024-02-04 06:34:41,803 Epoch 664: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.88 
2024-02-04 06:34:41,804 EPOCH 665
2024-02-04 06:34:43,040 [Epoch: 665 Step: 00044500] Batch Recognition Loss:   0.000236 => Gls Tokens per Sec:     1481 || Batch Translation Loss:   0.031787 => Txt Tokens per Sec:     4518 || Lr: 0.000050
2024-02-04 06:34:47,123 Epoch 665: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.16 
2024-02-04 06:34:47,123 EPOCH 666
2024-02-04 06:34:50,580 [Epoch: 666 Step: 00044600] Batch Recognition Loss:   0.000111 => Gls Tokens per Sec:     2057 || Batch Translation Loss:   0.012737 => Txt Tokens per Sec:     5696 || Lr: 0.000050
2024-02-04 06:34:52,494 Epoch 666: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.33 
2024-02-04 06:34:52,494 EPOCH 667
2024-02-04 06:34:57,662 Epoch 667: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.06 
2024-02-04 06:34:57,662 EPOCH 668
2024-02-04 06:34:58,402 [Epoch: 668 Step: 00044700] Batch Recognition Loss:   0.000272 => Gls Tokens per Sec:     2381 || Batch Translation Loss:   0.022132 => Txt Tokens per Sec:     6178 || Lr: 0.000050
2024-02-04 06:35:03,026 Epoch 668: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.68 
2024-02-04 06:35:03,027 EPOCH 669
2024-02-04 06:35:06,594 [Epoch: 669 Step: 00044800] Batch Recognition Loss:   0.000214 => Gls Tokens per Sec:     1975 || Batch Translation Loss:   0.269825 => Txt Tokens per Sec:     5604 || Lr: 0.000050
2024-02-04 06:35:08,340 Epoch 669: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.67 
2024-02-04 06:35:08,341 EPOCH 670
2024-02-04 06:35:13,716 Epoch 670: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.43 
2024-02-04 06:35:13,716 EPOCH 671
2024-02-04 06:35:14,375 [Epoch: 671 Step: 00044900] Batch Recognition Loss:   0.000246 => Gls Tokens per Sec:     2430 || Batch Translation Loss:   0.019844 => Txt Tokens per Sec:     6541 || Lr: 0.000050
2024-02-04 06:35:19,000 Epoch 671: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.28 
2024-02-04 06:35:19,001 EPOCH 672
2024-02-04 06:35:22,618 [Epoch: 672 Step: 00045000] Batch Recognition Loss:   0.000172 => Gls Tokens per Sec:     1878 || Batch Translation Loss:   0.015928 => Txt Tokens per Sec:     5273 || Lr: 0.000050
2024-02-04 06:35:24,440 Epoch 672: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.24 
2024-02-04 06:35:24,440 EPOCH 673
2024-02-04 06:35:29,971 Epoch 673: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.68 
2024-02-04 06:35:29,971 EPOCH 674
2024-02-04 06:35:30,659 [Epoch: 674 Step: 00045100] Batch Recognition Loss:   0.000201 => Gls Tokens per Sec:     1964 || Batch Translation Loss:   0.017869 => Txt Tokens per Sec:     5665 || Lr: 0.000050
2024-02-04 06:35:34,771 Epoch 674: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.12 
2024-02-04 06:35:34,771 EPOCH 675
2024-02-04 06:35:37,936 [Epoch: 675 Step: 00045200] Batch Recognition Loss:   0.000194 => Gls Tokens per Sec:     2124 || Batch Translation Loss:   0.014765 => Txt Tokens per Sec:     5808 || Lr: 0.000050
2024-02-04 06:35:40,182 Epoch 675: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.24 
2024-02-04 06:35:40,182 EPOCH 676
2024-02-04 06:35:45,435 Epoch 676: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.17 
2024-02-04 06:35:45,435 EPOCH 677
2024-02-04 06:35:46,046 [Epoch: 677 Step: 00045300] Batch Recognition Loss:   0.000148 => Gls Tokens per Sec:     2104 || Batch Translation Loss:   0.025687 => Txt Tokens per Sec:     5790 || Lr: 0.000050
2024-02-04 06:35:50,817 Epoch 677: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.29 
2024-02-04 06:35:50,817 EPOCH 678
2024-02-04 06:35:54,178 [Epoch: 678 Step: 00045400] Batch Recognition Loss:   0.000262 => Gls Tokens per Sec:     1925 || Batch Translation Loss:   0.807081 => Txt Tokens per Sec:     5473 || Lr: 0.000050
2024-02-04 06:35:56,021 Epoch 678: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.80 
2024-02-04 06:35:56,022 EPOCH 679
2024-02-04 06:36:01,089 Epoch 679: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.53 
2024-02-04 06:36:01,089 EPOCH 680
2024-02-04 06:36:01,722 [Epoch: 680 Step: 00045500] Batch Recognition Loss:   0.000164 => Gls Tokens per Sec:     1774 || Batch Translation Loss:   0.150144 => Txt Tokens per Sec:     5005 || Lr: 0.000050
2024-02-04 06:36:06,582 Epoch 680: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.82 
2024-02-04 06:36:06,583 EPOCH 681
2024-02-04 06:36:09,508 [Epoch: 681 Step: 00045600] Batch Recognition Loss:   0.000340 => Gls Tokens per Sec:     2158 || Batch Translation Loss:   0.046765 => Txt Tokens per Sec:     5870 || Lr: 0.000050
2024-02-04 06:36:11,938 Epoch 681: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.95 
2024-02-04 06:36:11,938 EPOCH 682
2024-02-04 06:36:17,075 Epoch 682: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.63 
2024-02-04 06:36:17,075 EPOCH 683
2024-02-04 06:36:17,522 [Epoch: 683 Step: 00045700] Batch Recognition Loss:   0.000253 => Gls Tokens per Sec:     2152 || Batch Translation Loss:   0.013724 => Txt Tokens per Sec:     5673 || Lr: 0.000050
2024-02-04 06:36:22,531 Epoch 683: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.51 
2024-02-04 06:36:22,532 EPOCH 684
2024-02-04 06:36:25,512 [Epoch: 684 Step: 00045800] Batch Recognition Loss:   0.000225 => Gls Tokens per Sec:     2094 || Batch Translation Loss:   0.022890 => Txt Tokens per Sec:     5881 || Lr: 0.000050
2024-02-04 06:36:27,553 Epoch 684: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.85 
2024-02-04 06:36:27,553 EPOCH 685
2024-02-04 06:36:33,082 Epoch 685: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.30 
2024-02-04 06:36:33,083 EPOCH 686
2024-02-04 06:36:33,454 [Epoch: 686 Step: 00045900] Batch Recognition Loss:   0.000234 => Gls Tokens per Sec:     2169 || Batch Translation Loss:   0.025158 => Txt Tokens per Sec:     6066 || Lr: 0.000050
2024-02-04 06:36:38,229 Epoch 686: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.65 
2024-02-04 06:36:38,230 EPOCH 687
2024-02-04 06:36:41,189 [Epoch: 687 Step: 00046000] Batch Recognition Loss:   0.000156 => Gls Tokens per Sec:     2025 || Batch Translation Loss:   0.018197 => Txt Tokens per Sec:     5598 || Lr: 0.000050
2024-02-04 06:36:49,568 Validation result at epoch 687, step    46000: duration: 8.3789s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00114	Translation Loss: 90451.80469	PPL: 8531.46680
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.60	(BLEU-1: 10.55,	BLEU-2: 3.02,	BLEU-3: 1.23,	BLEU-4: 0.60)
	CHRF 16.75	ROUGE 9.03
2024-02-04 06:36:49,569 Logging Recognition and Translation Outputs
2024-02-04 06:36:49,569 ========================================================================================================================
2024-02-04 06:36:49,569 Logging Sequence: 126_121.00
2024-02-04 06:36:49,569 	Gloss Reference :	A B+C+D+E
2024-02-04 06:36:49,570 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:36:49,570 	Gloss Alignment :	         
2024-02-04 06:36:49,570 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:36:49,571 	Text Reference  :	* *** everyone was       very happy by  his victory
2024-02-04 06:36:49,571 	Text Hypothesis :	1 day is       currently at   the   age of  india  
2024-02-04 06:36:49,571 	Text Alignment  :	I I   S        S         S    S     S   S   S      
2024-02-04 06:36:49,571 ========================================================================================================================
2024-02-04 06:36:49,571 Logging Sequence: 73_79.00
2024-02-04 06:36:49,571 	Gloss Reference :	A B+C+D+E
2024-02-04 06:36:49,571 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:36:49,571 	Gloss Alignment :	         
2024-02-04 06:36:49,572 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:36:49,573 	Text Reference  :	raina resturant has food from the ******* rich  spices of   north india  to the   aromatic curries of   south  india
2024-02-04 06:36:49,573 	Text Hypothesis :	***** ********* *** **** **** the company filed a      case on    behalf od dhoni as       they    also manage dhoni
2024-02-04 06:36:49,573 	Text Alignment  :	D     D         D   D    D        I       S     S      S    S     S      S  S     S        S       S    S      S    
2024-02-04 06:36:49,573 ========================================================================================================================
2024-02-04 06:36:49,574 Logging Sequence: 95_152.00
2024-02-04 06:36:49,574 	Gloss Reference :	A B+C+D+E
2024-02-04 06:36:49,574 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:36:49,574 	Gloss Alignment :	         
2024-02-04 06:36:49,574 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:36:49,575 	Text Reference  :	** ** * **** ****** how strange 
2024-02-04 06:36:49,575 	Text Hypothesis :	he is a very strong and pakistan
2024-02-04 06:36:49,575 	Text Alignment  :	I  I  I I    I      S   S       
2024-02-04 06:36:49,575 ========================================================================================================================
2024-02-04 06:36:49,575 Logging Sequence: 135_39.00
2024-02-04 06:36:49,575 	Gloss Reference :	A B+C+D+E
2024-02-04 06:36:49,576 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:36:49,576 	Gloss Alignment :	         
2024-02-04 06:36:49,576 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:36:49,577 	Text Reference  :	who needs to *** travel from     poland to stanford university in ******* california
2024-02-04 06:36:49,577 	Text Hypothesis :	*** flew  to the tokyo  olympics won    a  silver   medal      in javelin throw     
2024-02-04 06:36:49,577 	Text Alignment  :	D   S        I   S      S        S      S  S        S             I       S         
2024-02-04 06:36:49,577 ========================================================================================================================
2024-02-04 06:36:49,577 Logging Sequence: 87_2.00
2024-02-04 06:36:49,577 	Gloss Reference :	A B+C+D+E
2024-02-04 06:36:49,578 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:36:49,578 	Gloss Alignment :	         
2024-02-04 06:36:49,578 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:36:49,579 	Text Reference  :	cricketer gautam gambhir's jealousy against ms    dhoni and virat kohli has        been increasing day by day    
2024-02-04 06:36:49,580 	Text Hypothesis :	what      an     intense   bidding  for     india who   are now   any   spectators to   carry      on  in support
2024-02-04 06:36:49,580 	Text Alignment  :	S         S      S         S        S       S     S     S   S     S     S          S    S          S   S  S      
2024-02-04 06:36:49,580 ========================================================================================================================
2024-02-04 06:36:51,932 Epoch 687: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.27 
2024-02-04 06:36:51,932 EPOCH 688
2024-02-04 06:36:57,401 Epoch 688: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.51 
2024-02-04 06:36:57,402 EPOCH 689
2024-02-04 06:36:57,632 [Epoch: 689 Step: 00046100] Batch Recognition Loss:   0.000130 => Gls Tokens per Sec:     2795 || Batch Translation Loss:   0.013430 => Txt Tokens per Sec:     6825 || Lr: 0.000050
2024-02-04 06:37:02,581 Epoch 689: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.84 
2024-02-04 06:37:02,581 EPOCH 690
2024-02-04 06:37:05,865 [Epoch: 690 Step: 00046200] Batch Recognition Loss:   0.000152 => Gls Tokens per Sec:     1776 || Batch Translation Loss:   0.011797 => Txt Tokens per Sec:     5033 || Lr: 0.000050
2024-02-04 06:37:08,113 Epoch 690: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.78 
2024-02-04 06:37:08,114 EPOCH 691
2024-02-04 06:37:13,275 Epoch 691: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.37 
2024-02-04 06:37:13,275 EPOCH 692
2024-02-04 06:37:13,482 [Epoch: 692 Step: 00046300] Batch Recognition Loss:   0.000195 => Gls Tokens per Sec:     2337 || Batch Translation Loss:   0.021158 => Txt Tokens per Sec:     6281 || Lr: 0.000050
2024-02-04 06:37:18,490 Epoch 692: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.62 
2024-02-04 06:37:18,490 EPOCH 693
2024-02-04 06:37:21,262 [Epoch: 693 Step: 00046400] Batch Recognition Loss:   0.000171 => Gls Tokens per Sec:     2046 || Batch Translation Loss:   0.021983 => Txt Tokens per Sec:     5700 || Lr: 0.000050
2024-02-04 06:37:23,818 Epoch 693: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.94 
2024-02-04 06:37:23,819 EPOCH 694
2024-02-04 06:37:28,893 Epoch 694: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.68 
2024-02-04 06:37:28,893 EPOCH 695
2024-02-04 06:37:29,002 [Epoch: 695 Step: 00046500] Batch Recognition Loss:   0.000120 => Gls Tokens per Sec:     2963 || Batch Translation Loss:   0.013937 => Txt Tokens per Sec:     6676 || Lr: 0.000050
2024-02-04 06:37:34,380 Epoch 695: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.40 
2024-02-04 06:37:34,381 EPOCH 696
2024-02-04 06:37:36,912 [Epoch: 696 Step: 00046600] Batch Recognition Loss:   0.000134 => Gls Tokens per Sec:     2178 || Batch Translation Loss:   0.025012 => Txt Tokens per Sec:     5852 || Lr: 0.000050
2024-02-04 06:37:39,582 Epoch 696: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.54 
2024-02-04 06:37:39,583 EPOCH 697
2024-02-04 06:37:45,218 Epoch 697: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.72 
2024-02-04 06:37:45,219 EPOCH 698
2024-02-04 06:37:45,299 [Epoch: 698 Step: 00046700] Batch Recognition Loss:   0.000237 => Gls Tokens per Sec:     2025 || Batch Translation Loss:   0.029169 => Txt Tokens per Sec:     5937 || Lr: 0.000050
2024-02-04 06:37:50,414 Epoch 698: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.76 
2024-02-04 06:37:50,415 EPOCH 699
2024-02-04 06:37:53,110 [Epoch: 699 Step: 00046800] Batch Recognition Loss:   0.000185 => Gls Tokens per Sec:     1986 || Batch Translation Loss:   0.016991 => Txt Tokens per Sec:     5433 || Lr: 0.000050
2024-02-04 06:37:55,616 Epoch 699: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.61 
2024-02-04 06:37:55,616 EPOCH 700
2024-02-04 06:38:00,979 [Epoch: 700 Step: 00046900] Batch Recognition Loss:   0.000215 => Gls Tokens per Sec:     1983 || Batch Translation Loss:   0.016749 => Txt Tokens per Sec:     5504 || Lr: 0.000050
2024-02-04 06:38:00,979 Epoch 700: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.95 
2024-02-04 06:38:00,980 EPOCH 701
2024-02-04 06:38:06,138 Epoch 701: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.80 
2024-02-04 06:38:06,139 EPOCH 702
2024-02-04 06:38:08,831 [Epoch: 702 Step: 00047000] Batch Recognition Loss:   0.000285 => Gls Tokens per Sec:     1928 || Batch Translation Loss:   0.021183 => Txt Tokens per Sec:     5318 || Lr: 0.000050
2024-02-04 06:38:11,647 Epoch 702: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.81 
2024-02-04 06:38:11,647 EPOCH 703
2024-02-04 06:38:16,945 [Epoch: 703 Step: 00047100] Batch Recognition Loss:   0.000253 => Gls Tokens per Sec:     1976 || Batch Translation Loss:   0.036673 => Txt Tokens per Sec:     5481 || Lr: 0.000050
2024-02-04 06:38:17,019 Epoch 703: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.38 
2024-02-04 06:38:17,019 EPOCH 704
2024-02-04 06:38:22,236 Epoch 704: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.35 
2024-02-04 06:38:22,237 EPOCH 705
2024-02-04 06:38:24,600 [Epoch: 705 Step: 00047200] Batch Recognition Loss:   0.000125 => Gls Tokens per Sec:     2130 || Batch Translation Loss:   0.026121 => Txt Tokens per Sec:     5676 || Lr: 0.000050
2024-02-04 06:38:27,591 Epoch 705: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.36 
2024-02-04 06:38:27,591 EPOCH 706
2024-02-04 06:38:32,313 [Epoch: 706 Step: 00047300] Batch Recognition Loss:   0.000180 => Gls Tokens per Sec:     2203 || Batch Translation Loss:   0.017887 => Txt Tokens per Sec:     6092 || Lr: 0.000050
2024-02-04 06:38:32,570 Epoch 706: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.56 
2024-02-04 06:38:32,570 EPOCH 707
2024-02-04 06:38:38,037 Epoch 707: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.78 
2024-02-04 06:38:38,038 EPOCH 708
2024-02-04 06:38:40,272 [Epoch: 708 Step: 00047400] Batch Recognition Loss:   0.000236 => Gls Tokens per Sec:     2180 || Batch Translation Loss:   0.024842 => Txt Tokens per Sec:     5976 || Lr: 0.000050
2024-02-04 06:38:43,350 Epoch 708: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.79 
2024-02-04 06:38:43,350 EPOCH 709
2024-02-04 06:38:48,035 [Epoch: 709 Step: 00047500] Batch Recognition Loss:   0.000142 => Gls Tokens per Sec:     2167 || Batch Translation Loss:   0.021576 => Txt Tokens per Sec:     5963 || Lr: 0.000050
2024-02-04 06:38:48,286 Epoch 709: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.61 
2024-02-04 06:38:48,286 EPOCH 710
2024-02-04 06:38:53,865 Epoch 710: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.47 
2024-02-04 06:38:53,865 EPOCH 711
2024-02-04 06:38:55,936 [Epoch: 711 Step: 00047600] Batch Recognition Loss:   0.000140 => Gls Tokens per Sec:     2319 || Batch Translation Loss:   0.020552 => Txt Tokens per Sec:     6400 || Lr: 0.000050
2024-02-04 06:38:59,194 Epoch 711: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.45 
2024-02-04 06:38:59,195 EPOCH 712
2024-02-04 06:39:04,109 [Epoch: 712 Step: 00047700] Batch Recognition Loss:   0.000198 => Gls Tokens per Sec:     2034 || Batch Translation Loss:   0.021452 => Txt Tokens per Sec:     5617 || Lr: 0.000050
2024-02-04 06:39:04,412 Epoch 712: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.64 
2024-02-04 06:39:04,413 EPOCH 713
2024-02-04 06:39:09,836 Epoch 713: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.33 
2024-02-04 06:39:09,837 EPOCH 714
2024-02-04 06:39:11,827 [Epoch: 714 Step: 00047800] Batch Recognition Loss:   0.000140 => Gls Tokens per Sec:     2287 || Batch Translation Loss:   0.023960 => Txt Tokens per Sec:     6152 || Lr: 0.000050
2024-02-04 06:39:14,639 Epoch 714: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.25 
2024-02-04 06:39:14,639 EPOCH 715
2024-02-04 06:39:19,959 [Epoch: 715 Step: 00047900] Batch Recognition Loss:   0.000330 => Gls Tokens per Sec:     1848 || Batch Translation Loss:   0.117539 => Txt Tokens per Sec:     5103 || Lr: 0.000050
2024-02-04 06:39:20,392 Epoch 715: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.38 
2024-02-04 06:39:20,392 EPOCH 716
2024-02-04 06:39:25,789 Epoch 716: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.59 
2024-02-04 06:39:25,790 EPOCH 717
2024-02-04 06:39:27,946 [Epoch: 717 Step: 00048000] Batch Recognition Loss:   0.000173 => Gls Tokens per Sec:     2038 || Batch Translation Loss:   0.019919 => Txt Tokens per Sec:     5714 || Lr: 0.000050
2024-02-04 06:39:36,221 Validation result at epoch 717, step    48000: duration: 8.2744s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00117	Translation Loss: 91252.39062	PPL: 9243.08398
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.63	(BLEU-1: 10.55,	BLEU-2: 3.18,	BLEU-3: 1.22,	BLEU-4: 0.63)
	CHRF 16.76	ROUGE 8.99
2024-02-04 06:39:36,222 Logging Recognition and Translation Outputs
2024-02-04 06:39:36,222 ========================================================================================================================
2024-02-04 06:39:36,222 Logging Sequence: 88_159.00
2024-02-04 06:39:36,222 	Gloss Reference :	A B+C+D+E
2024-02-04 06:39:36,222 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:39:36,222 	Gloss Alignment :	         
2024-02-04 06:39:36,223 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:39:36,223 	Text Reference  :	however he  often comes to   the ****** ***** town to ** meet his relatives
2024-02-04 06:39:36,224 	Text Hypothesis :	******* the mayor added that the police never does to be held in  tokyo    
2024-02-04 06:39:36,224 	Text Alignment  :	D       S   S     S     S        I      I     S       I  S    S   S        
2024-02-04 06:39:36,224 ========================================================================================================================
2024-02-04 06:39:36,224 Logging Sequence: 180_53.00
2024-02-04 06:39:36,224 	Gloss Reference :	A B+C+D+E
2024-02-04 06:39:36,224 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:39:36,224 	Gloss Alignment :	         
2024-02-04 06:39:36,225 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:39:36,225 	Text Reference  :	*** the ****** ******* ** *** protest is       against singh     again
2024-02-04 06:39:36,225 	Text Hypothesis :	and the police decided to not sports  minister have    intensely that 
2024-02-04 06:39:36,225 	Text Alignment  :	I       I      I       I  I   S       S        S       S         S    
2024-02-04 06:39:36,225 ========================================================================================================================
2024-02-04 06:39:36,226 Logging Sequence: 163_30.00
2024-02-04 06:39:36,226 	Gloss Reference :	A B+C+D+E
2024-02-04 06:39:36,226 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:39:36,226 	Gloss Alignment :	         
2024-02-04 06:39:36,226 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:39:36,227 	Text Reference  :	** ** they never permitted anyone to      reveal her  face   
2024-02-04 06:39:36,227 	Text Hypothesis :	it is not  known if        the    meeting his    wife deepika
2024-02-04 06:39:36,227 	Text Alignment  :	I  I  S    S     S         S      S       S      S    S      
2024-02-04 06:39:36,227 ========================================================================================================================
2024-02-04 06:39:36,227 Logging Sequence: 51_110.00
2024-02-04 06:39:36,227 	Gloss Reference :	A B+C+D+E
2024-02-04 06:39:36,227 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:39:36,228 	Gloss Alignment :	         
2024-02-04 06:39:36,228 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:39:36,228 	Text Reference  :	the   aussies were very happy  with their victory
2024-02-04 06:39:36,228 	Text Hypothesis :	these are     the  same ritual of   their matches
2024-02-04 06:39:36,228 	Text Alignment  :	S     S       S    S    S      S          S      
2024-02-04 06:39:36,229 ========================================================================================================================
2024-02-04 06:39:36,229 Logging Sequence: 70_249.00
2024-02-04 06:39:36,229 	Gloss Reference :	A B+C+D+E
2024-02-04 06:39:36,229 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:39:36,229 	Gloss Alignment :	         
2024-02-04 06:39:36,229 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:39:36,230 	Text Reference  :	**** **** ********* **** **** **** ** **** have a   look at this video   
2024-02-04 06:39:36,230 	Text Hypothesis :	suga then announced that from 5610 to 5522 in   the end  of the  olympics
2024-02-04 06:39:36,230 	Text Alignment  :	I    I    I         I    I    I    I  I    S    S   S    S  S    S       
2024-02-04 06:39:36,230 ========================================================================================================================
2024-02-04 06:39:39,402 Epoch 717: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.96 
2024-02-04 06:39:39,402 EPOCH 718
2024-02-04 06:39:44,426 [Epoch: 718 Step: 00048100] Batch Recognition Loss:   0.000205 => Gls Tokens per Sec:     1925 || Batch Translation Loss:   0.036920 => Txt Tokens per Sec:     5394 || Lr: 0.000050
2024-02-04 06:39:44,852 Epoch 718: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.26 
2024-02-04 06:39:44,852 EPOCH 719
2024-02-04 06:39:50,388 Epoch 719: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.61 
2024-02-04 06:39:50,389 EPOCH 720
2024-02-04 06:39:53,045 [Epoch: 720 Step: 00048200] Batch Recognition Loss:   0.000267 => Gls Tokens per Sec:     1594 || Batch Translation Loss:   0.032263 => Txt Tokens per Sec:     4715 || Lr: 0.000050
2024-02-04 06:39:56,007 Epoch 720: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.78 
2024-02-04 06:39:56,007 EPOCH 721
2024-02-04 06:40:00,951 [Epoch: 721 Step: 00048300] Batch Recognition Loss:   0.000153 => Gls Tokens per Sec:     1924 || Batch Translation Loss:   0.015253 => Txt Tokens per Sec:     5354 || Lr: 0.000050
2024-02-04 06:40:01,481 Epoch 721: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.36 
2024-02-04 06:40:01,481 EPOCH 722
2024-02-04 06:40:06,868 Epoch 722: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.29 
2024-02-04 06:40:06,868 EPOCH 723
2024-02-04 06:40:08,734 [Epoch: 723 Step: 00048400] Batch Recognition Loss:   0.000217 => Gls Tokens per Sec:     2230 || Batch Translation Loss:   0.019713 => Txt Tokens per Sec:     6391 || Lr: 0.000050
2024-02-04 06:40:12,055 Epoch 723: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.92 
2024-02-04 06:40:12,055 EPOCH 724
2024-02-04 06:40:16,946 [Epoch: 724 Step: 00048500] Batch Recognition Loss:   0.000240 => Gls Tokens per Sec:     1913 || Batch Translation Loss:   0.023961 => Txt Tokens per Sec:     5383 || Lr: 0.000050
2024-02-04 06:40:17,541 Epoch 724: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.58 
2024-02-04 06:40:17,541 EPOCH 725
2024-02-04 06:40:23,151 Epoch 725: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.28 
2024-02-04 06:40:23,151 EPOCH 726
2024-02-04 06:40:25,369 [Epoch: 726 Step: 00048600] Batch Recognition Loss:   0.000208 => Gls Tokens per Sec:     1804 || Batch Translation Loss:   0.020695 => Txt Tokens per Sec:     5156 || Lr: 0.000050
2024-02-04 06:40:28,528 Epoch 726: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.67 
2024-02-04 06:40:28,528 EPOCH 727
2024-02-04 06:40:33,336 [Epoch: 727 Step: 00048700] Batch Recognition Loss:   0.000219 => Gls Tokens per Sec:     1912 || Batch Translation Loss:   0.022510 => Txt Tokens per Sec:     5359 || Lr: 0.000050
2024-02-04 06:40:33,951 Epoch 727: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.95 
2024-02-04 06:40:33,951 EPOCH 728
2024-02-04 06:40:39,431 Epoch 728: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.73 
2024-02-04 06:40:39,431 EPOCH 729
2024-02-04 06:40:41,148 [Epoch: 729 Step: 00048800] Batch Recognition Loss:   0.000257 => Gls Tokens per Sec:     2238 || Batch Translation Loss:   0.029407 => Txt Tokens per Sec:     6097 || Lr: 0.000050
2024-02-04 06:40:44,436 Epoch 729: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.75 
2024-02-04 06:40:44,436 EPOCH 730
2024-02-04 06:40:49,186 [Epoch: 730 Step: 00048900] Batch Recognition Loss:   0.000249 => Gls Tokens per Sec:     1901 || Batch Translation Loss:   0.024850 => Txt Tokens per Sec:     5251 || Lr: 0.000050
2024-02-04 06:40:49,929 Epoch 730: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.47 
2024-02-04 06:40:49,929 EPOCH 731
2024-02-04 06:40:55,433 Epoch 731: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.50 
2024-02-04 06:40:55,434 EPOCH 732
2024-02-04 06:40:57,276 [Epoch: 732 Step: 00049000] Batch Recognition Loss:   0.000173 => Gls Tokens per Sec:     1998 || Batch Translation Loss:   0.016556 => Txt Tokens per Sec:     5503 || Lr: 0.000050
2024-02-04 06:41:00,858 Epoch 732: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.42 
2024-02-04 06:41:00,858 EPOCH 733
2024-02-04 06:41:05,487 [Epoch: 733 Step: 00049100] Batch Recognition Loss:   0.000197 => Gls Tokens per Sec:     1917 || Batch Translation Loss:   0.012754 => Txt Tokens per Sec:     5343 || Lr: 0.000050
2024-02-04 06:41:06,280 Epoch 733: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.13 
2024-02-04 06:41:06,280 EPOCH 734
2024-02-04 06:41:11,515 Epoch 734: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.27 
2024-02-04 06:41:11,516 EPOCH 735
2024-02-04 06:41:13,540 [Epoch: 735 Step: 00049200] Batch Recognition Loss:   0.000223 => Gls Tokens per Sec:     1696 || Batch Translation Loss:   0.085230 => Txt Tokens per Sec:     4802 || Lr: 0.000050
2024-02-04 06:41:16,890 Epoch 735: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.41 
2024-02-04 06:41:16,890 EPOCH 736
2024-02-04 06:41:21,090 [Epoch: 736 Step: 00049300] Batch Recognition Loss:   0.000241 => Gls Tokens per Sec:     2074 || Batch Translation Loss:   0.027188 => Txt Tokens per Sec:     5665 || Lr: 0.000050
2024-02-04 06:41:22,215 Epoch 736: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.69 
2024-02-04 06:41:22,215 EPOCH 737
2024-02-04 06:41:27,701 Epoch 737: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.89 
2024-02-04 06:41:27,701 EPOCH 738
2024-02-04 06:41:29,634 [Epoch: 738 Step: 00049400] Batch Recognition Loss:   0.000395 => Gls Tokens per Sec:     1693 || Batch Translation Loss:   0.076129 => Txt Tokens per Sec:     4561 || Lr: 0.000050
2024-02-04 06:41:33,308 Epoch 738: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.53 
2024-02-04 06:41:33,308 EPOCH 739
2024-02-04 06:41:37,497 [Epoch: 739 Step: 00049500] Batch Recognition Loss:   0.000179 => Gls Tokens per Sec:     2041 || Batch Translation Loss:   0.022265 => Txt Tokens per Sec:     5588 || Lr: 0.000050
2024-02-04 06:41:38,658 Epoch 739: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.00 
2024-02-04 06:41:38,658 EPOCH 740
2024-02-04 06:41:44,065 Epoch 740: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.62 
2024-02-04 06:41:44,066 EPOCH 741
2024-02-04 06:41:45,820 [Epoch: 741 Step: 00049600] Batch Recognition Loss:   0.000267 => Gls Tokens per Sec:     1825 || Batch Translation Loss:   0.019577 => Txt Tokens per Sec:     5272 || Lr: 0.000050
2024-02-04 06:41:49,638 Epoch 741: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.71 
2024-02-04 06:41:49,639 EPOCH 742
2024-02-04 06:41:53,593 [Epoch: 742 Step: 00049700] Batch Recognition Loss:   0.000150 => Gls Tokens per Sec:     2145 || Batch Translation Loss:   0.013067 => Txt Tokens per Sec:     5989 || Lr: 0.000050
2024-02-04 06:41:54,660 Epoch 742: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.78 
2024-02-04 06:41:54,660 EPOCH 743
2024-02-04 06:42:00,244 Epoch 743: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.73 
2024-02-04 06:42:00,244 EPOCH 744
2024-02-04 06:42:01,852 [Epoch: 744 Step: 00049800] Batch Recognition Loss:   0.000281 => Gls Tokens per Sec:     1837 || Batch Translation Loss:   0.029983 => Txt Tokens per Sec:     5455 || Lr: 0.000050
2024-02-04 06:42:05,454 Epoch 744: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.15 
2024-02-04 06:42:05,455 EPOCH 745
2024-02-04 06:42:09,782 [Epoch: 745 Step: 00049900] Batch Recognition Loss:   0.000101 => Gls Tokens per Sec:     1902 || Batch Translation Loss:   0.009740 => Txt Tokens per Sec:     5365 || Lr: 0.000050
2024-02-04 06:42:10,774 Epoch 745: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.55 
2024-02-04 06:42:10,774 EPOCH 746
2024-02-04 06:42:16,154 Epoch 746: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.51 
2024-02-04 06:42:16,154 EPOCH 747
2024-02-04 06:42:17,589 [Epoch: 747 Step: 00050000] Batch Recognition Loss:   0.000200 => Gls Tokens per Sec:     2008 || Batch Translation Loss:   0.020791 => Txt Tokens per Sec:     5517 || Lr: 0.000050
2024-02-04 06:42:25,672 Validation result at epoch 747, step    50000: duration: 8.0833s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00135	Translation Loss: 90096.63281	PPL: 8233.57031
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.68	(BLEU-1: 10.30,	BLEU-2: 3.10,	BLEU-3: 1.30,	BLEU-4: 0.68)
	CHRF 16.77	ROUGE 8.66
2024-02-04 06:42:25,673 Logging Recognition and Translation Outputs
2024-02-04 06:42:25,673 ========================================================================================================================
2024-02-04 06:42:25,673 Logging Sequence: 59_58.00
2024-02-04 06:42:25,673 	Gloss Reference :	A B+C+D+E
2024-02-04 06:42:25,673 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:42:25,673 	Gloss Alignment :	         
2024-02-04 06:42:25,673 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:42:25,674 	Text Reference  :	*** to      fix   the damage they did not have a       lot     of time     
2024-02-04 06:42:25,674 	Text Hypothesis :	and talking about the ****** **** *** *** **** harmful effects of coca-cola
2024-02-04 06:42:25,674 	Text Alignment  :	I   S       S         D      D    D   D   D    S       S          S        
2024-02-04 06:42:25,674 ========================================================================================================================
2024-02-04 06:42:25,675 Logging Sequence: 165_2.00
2024-02-04 06:42:25,675 	Gloss Reference :	A B+C+D+E
2024-02-04 06:42:25,675 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:42:25,675 	Gloss Alignment :	         
2024-02-04 06:42:25,675 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:42:25,676 	Text Reference  :	many people believe in superstitions and think it       brings good  luck and bad   luck
2024-02-04 06:42:25,676 	Text Hypothesis :	**** ****** ******* ** ************* *** he    believed that   india won  the world cup 
2024-02-04 06:42:25,676 	Text Alignment  :	D    D      D       D  D             D   S     S        S      S     S    S   S     S   
2024-02-04 06:42:25,676 ========================================================================================================================
2024-02-04 06:42:25,676 Logging Sequence: 58_147.00
2024-02-04 06:42:25,677 	Gloss Reference :	A B+C+D+E
2024-02-04 06:42:25,677 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:42:25,677 	Gloss Alignment :	         
2024-02-04 06:42:25,677 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:42:25,678 	Text Reference  :	the women's cricket team grabbed gold by beating sri lanka in     the finals  what       a           historic win
2024-02-04 06:42:25,678 	Text Hypothesis :	*** ******* ******* **** ******* **** ** ******* *** many  former and current cricketers politicians fans     etc
2024-02-04 06:42:25,678 	Text Alignment  :	D   D       D       D    D       D    D  D       D   S     S      S   S       S          S           S        S  
2024-02-04 06:42:25,678 ========================================================================================================================
2024-02-04 06:42:25,679 Logging Sequence: 81_139.00
2024-02-04 06:42:25,679 	Gloss Reference :	A B+C+D+E
2024-02-04 06:42:25,679 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:42:25,679 	Gloss Alignment :	         
2024-02-04 06:42:25,679 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:42:25,681 	Text Reference  :	in 2017 the case was filed first in delhi high court by        rhiti sports     management on  behalf of ************* dhoni  
2024-02-04 06:42:25,681 	Text Hypothesis :	** **** *** **** *** ***** ***** in 2020  ms   dhoni announced his   retirement from       all forms  of international cricket
2024-02-04 06:42:25,681 	Text Alignment  :	D  D    D   D    D   D     D        S     S    S     S         S     S          S          S   S         I             S      
2024-02-04 06:42:25,681 ========================================================================================================================
2024-02-04 06:42:25,681 Logging Sequence: 125_72.00
2024-02-04 06:42:25,681 	Gloss Reference :	A B+C+D+E
2024-02-04 06:42:25,681 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:42:25,682 	Gloss Alignment :	         
2024-02-04 06:42:25,682 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:42:25,683 	Text Reference  :	some said the     pakistani javelineer had       milicious intentions of   tampering with  the javelin out of jealousy
2024-02-04 06:42:25,683 	Text Hypothesis :	**** **** shekhar filed     a          complaint against   the        team were      happy by  javelin *** ** throw   
2024-02-04 06:42:25,683 	Text Alignment  :	D    D    S       S         S          S         S         S          S    S         S     S           D   D  S       
2024-02-04 06:42:25,683 ========================================================================================================================
2024-02-04 06:42:29,384 Epoch 747: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.65 
2024-02-04 06:42:29,384 EPOCH 748
2024-02-04 06:42:33,667 [Epoch: 748 Step: 00050100] Batch Recognition Loss:   0.000148 => Gls Tokens per Sec:     1885 || Batch Translation Loss:   0.020617 => Txt Tokens per Sec:     5171 || Lr: 0.000050
2024-02-04 06:42:34,890 Epoch 748: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.92 
2024-02-04 06:42:34,890 EPOCH 749
2024-02-04 06:42:40,279 Epoch 749: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.67 
2024-02-04 06:42:40,280 EPOCH 750
2024-02-04 06:42:41,483 [Epoch: 750 Step: 00050200] Batch Recognition Loss:   0.000233 => Gls Tokens per Sec:     2262 || Batch Translation Loss:   0.071500 => Txt Tokens per Sec:     6091 || Lr: 0.000050
2024-02-04 06:42:45,778 Epoch 750: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.77 
2024-02-04 06:42:45,778 EPOCH 751
2024-02-04 06:42:49,880 [Epoch: 751 Step: 00050300] Batch Recognition Loss:   0.000253 => Gls Tokens per Sec:     1951 || Batch Translation Loss:   0.031658 => Txt Tokens per Sec:     5393 || Lr: 0.000050
2024-02-04 06:42:51,289 Epoch 751: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.22 
2024-02-04 06:42:51,289 EPOCH 752
2024-02-04 06:42:56,251 Epoch 752: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.33 
2024-02-04 06:42:56,252 EPOCH 753
2024-02-04 06:42:57,423 [Epoch: 753 Step: 00050400] Batch Recognition Loss:   0.000164 => Gls Tokens per Sec:     2188 || Batch Translation Loss:   0.028655 => Txt Tokens per Sec:     5902 || Lr: 0.000050
2024-02-04 06:43:01,648 Epoch 753: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.44 
2024-02-04 06:43:01,648 EPOCH 754
2024-02-04 06:43:05,738 [Epoch: 754 Step: 00050500] Batch Recognition Loss:   0.000344 => Gls Tokens per Sec:     1917 || Batch Translation Loss:   0.021961 => Txt Tokens per Sec:     5461 || Lr: 0.000050
2024-02-04 06:43:07,058 Epoch 754: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.99 
2024-02-04 06:43:07,058 EPOCH 755
2024-02-04 06:43:12,246 Epoch 755: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.75 
2024-02-04 06:43:12,247 EPOCH 756
2024-02-04 06:43:13,497 [Epoch: 756 Step: 00050600] Batch Recognition Loss:   0.000217 => Gls Tokens per Sec:     1922 || Batch Translation Loss:   0.021257 => Txt Tokens per Sec:     5342 || Lr: 0.000050
2024-02-04 06:43:17,941 Epoch 756: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.26 
2024-02-04 06:43:17,942 EPOCH 757
2024-02-04 06:43:21,457 [Epoch: 757 Step: 00050700] Batch Recognition Loss:   0.000143 => Gls Tokens per Sec:     2186 || Batch Translation Loss:   0.032815 => Txt Tokens per Sec:     6060 || Lr: 0.000050
2024-02-04 06:43:22,821 Epoch 757: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.91 
2024-02-04 06:43:22,821 EPOCH 758
2024-02-04 06:43:28,435 Epoch 758: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.89 
2024-02-04 06:43:28,435 EPOCH 759
2024-02-04 06:43:29,400 [Epoch: 759 Step: 00050800] Batch Recognition Loss:   0.000236 => Gls Tokens per Sec:     2324 || Batch Translation Loss:   0.019798 => Txt Tokens per Sec:     6477 || Lr: 0.000050
2024-02-04 06:43:33,577 Epoch 759: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.44 
2024-02-04 06:43:33,578 EPOCH 760
2024-02-04 06:43:37,255 [Epoch: 760 Step: 00050900] Batch Recognition Loss:   0.000167 => Gls Tokens per Sec:     2021 || Batch Translation Loss:   0.018000 => Txt Tokens per Sec:     5476 || Lr: 0.000050
2024-02-04 06:43:38,774 Epoch 760: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.41 
2024-02-04 06:43:38,775 EPOCH 761
2024-02-04 06:43:44,209 Epoch 761: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.13 
2024-02-04 06:43:44,209 EPOCH 762
2024-02-04 06:43:45,342 [Epoch: 762 Step: 00051000] Batch Recognition Loss:   0.000275 => Gls Tokens per Sec:     1838 || Batch Translation Loss:   0.019669 => Txt Tokens per Sec:     5397 || Lr: 0.000050
2024-02-04 06:43:49,296 Epoch 762: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.42 
2024-02-04 06:43:49,297 EPOCH 763
2024-02-04 06:43:53,069 [Epoch: 763 Step: 00051100] Batch Recognition Loss:   0.000200 => Gls Tokens per Sec:     1929 || Batch Translation Loss:   0.025332 => Txt Tokens per Sec:     5202 || Lr: 0.000050
2024-02-04 06:43:55,054 Epoch 763: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.73 
2024-02-04 06:43:55,054 EPOCH 764
2024-02-04 06:44:00,562 Epoch 764: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.72 
2024-02-04 06:44:00,563 EPOCH 765
2024-02-04 06:44:01,502 [Epoch: 765 Step: 00051200] Batch Recognition Loss:   0.000193 => Gls Tokens per Sec:     1951 || Batch Translation Loss:   0.021222 => Txt Tokens per Sec:     5919 || Lr: 0.000050
2024-02-04 06:44:05,466 Epoch 765: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.28 
2024-02-04 06:44:05,467 EPOCH 766
2024-02-04 06:44:09,056 [Epoch: 766 Step: 00051300] Batch Recognition Loss:   0.000131 => Gls Tokens per Sec:     2007 || Batch Translation Loss:   0.024998 => Txt Tokens per Sec:     5530 || Lr: 0.000050
2024-02-04 06:44:10,964 Epoch 766: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.97 
2024-02-04 06:44:10,965 EPOCH 767
2024-02-04 06:44:16,287 Epoch 767: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.91 
2024-02-04 06:44:16,287 EPOCH 768
2024-02-04 06:44:17,127 [Epoch: 768 Step: 00051400] Batch Recognition Loss:   0.000204 => Gls Tokens per Sec:     2097 || Batch Translation Loss:   0.019057 => Txt Tokens per Sec:     5565 || Lr: 0.000050
2024-02-04 06:44:21,492 Epoch 768: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.92 
2024-02-04 06:44:21,492 EPOCH 769
2024-02-04 06:44:25,105 [Epoch: 769 Step: 00051500] Batch Recognition Loss:   0.000165 => Gls Tokens per Sec:     1949 || Batch Translation Loss:   0.016838 => Txt Tokens per Sec:     5329 || Lr: 0.000050
2024-02-04 06:44:27,183 Epoch 769: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.29 
2024-02-04 06:44:27,184 EPOCH 770
2024-02-04 06:44:32,680 Epoch 770: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.56 
2024-02-04 06:44:32,680 EPOCH 771
2024-02-04 06:44:33,435 [Epoch: 771 Step: 00051600] Batch Recognition Loss:   0.000331 => Gls Tokens per Sec:     2127 || Batch Translation Loss:   0.145291 => Txt Tokens per Sec:     5882 || Lr: 0.000050
2024-02-04 06:44:38,060 Epoch 771: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.53 
2024-02-04 06:44:38,061 EPOCH 772
2024-02-04 06:44:41,742 [Epoch: 772 Step: 00051700] Batch Recognition Loss:   0.000141 => Gls Tokens per Sec:     1846 || Batch Translation Loss:   0.029454 => Txt Tokens per Sec:     5205 || Lr: 0.000050
2024-02-04 06:44:43,794 Epoch 772: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.01 
2024-02-04 06:44:43,794 EPOCH 773
2024-02-04 06:44:49,332 Epoch 773: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.65 
2024-02-04 06:44:49,332 EPOCH 774
2024-02-04 06:44:50,002 [Epoch: 774 Step: 00051800] Batch Recognition Loss:   0.000222 => Gls Tokens per Sec:     2156 || Batch Translation Loss:   0.020563 => Txt Tokens per Sec:     5817 || Lr: 0.000050
2024-02-04 06:44:54,321 Epoch 774: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.33 
2024-02-04 06:44:54,321 EPOCH 775
2024-02-04 06:44:57,771 [Epoch: 775 Step: 00051900] Batch Recognition Loss:   0.000237 => Gls Tokens per Sec:     1923 || Batch Translation Loss:   0.023662 => Txt Tokens per Sec:     5423 || Lr: 0.000050
2024-02-04 06:44:59,732 Epoch 775: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.11 
2024-02-04 06:44:59,732 EPOCH 776
2024-02-04 06:45:04,799 Epoch 776: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.64 
2024-02-04 06:45:04,800 EPOCH 777
2024-02-04 06:45:05,415 [Epoch: 777 Step: 00052000] Batch Recognition Loss:   0.000184 => Gls Tokens per Sec:     2081 || Batch Translation Loss:   0.036091 => Txt Tokens per Sec:     5904 || Lr: 0.000050
2024-02-04 06:45:13,938 Validation result at epoch 777, step    52000: duration: 8.5214s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00109	Translation Loss: 90238.28125	PPL: 8351.10742
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.68	(BLEU-1: 10.16,	BLEU-2: 3.12,	BLEU-3: 1.36,	BLEU-4: 0.68)
	CHRF 16.88	ROUGE 8.68
2024-02-04 06:45:13,939 Logging Recognition and Translation Outputs
2024-02-04 06:45:13,939 ========================================================================================================================
2024-02-04 06:45:13,940 Logging Sequence: 87_229.00
2024-02-04 06:45:13,940 	Gloss Reference :	A B+C+D+E
2024-02-04 06:45:13,940 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:45:13,940 	Gloss Alignment :	         
2024-02-04 06:45:13,940 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:45:13,941 	Text Reference  :	it   was not  against dhoni or   kohli
2024-02-04 06:45:13,941 	Text Hypothesis :	what a   good news    had   been fined
2024-02-04 06:45:13,941 	Text Alignment  :	S    S   S    S       S     S    S    
2024-02-04 06:45:13,941 ========================================================================================================================
2024-02-04 06:45:13,941 Logging Sequence: 134_153.00
2024-02-04 06:45:13,941 	Gloss Reference :	A B+C+D+E
2024-02-04 06:45:13,941 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:45:13,942 	Gloss Alignment :	         
2024-02-04 06:45:13,942 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:45:13,944 	Text Reference  :	pm    modi          in his interaction said that deaf    athletes must   fight for   their ***** goals      and *** **** never give up despite the losses     
2024-02-04 06:45:13,944 	Text Hypothesis :	after participating in his *********** **** joy  cricket athletes asking them  about their life' challenges and was just 83    over to win     the interaction
2024-02-04 06:45:13,944 	Text Alignment  :	S     S                    D           D    S    S                S      S     S           I     S              I   I    S     S    S  S           S          
2024-02-04 06:45:13,944 ========================================================================================================================
2024-02-04 06:45:13,945 Logging Sequence: 137_155.00
2024-02-04 06:45:13,945 	Gloss Reference :	A B+C+D+E
2024-02-04 06:45:13,945 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:45:13,945 	Gloss Alignment :	         
2024-02-04 06:45:13,945 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:45:13,946 	Text Reference  :	**** * an extremely high tax named  as          sin tax will be     applied
2024-02-04 06:45:13,946 	Text Hypothesis :	when i am overjoyed with the team's performance due to  his  sudden demise 
2024-02-04 06:45:13,946 	Text Alignment  :	I    I S  S         S    S   S      S           S   S   S    S      S      
2024-02-04 06:45:13,947 ========================================================================================================================
2024-02-04 06:45:13,947 Logging Sequence: 59_18.00
2024-02-04 06:45:13,947 	Gloss Reference :	A B+C+D+E
2024-02-04 06:45:13,947 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:45:13,947 	Gloss Alignment :	         
2024-02-04 06:45:13,947 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:45:13,948 	Text Reference  :	** 27-year-old jessica   fox  from australia won a  bronze  a   gold medal    in canoeing
2024-02-04 06:45:13,949 	Text Hypothesis :	he then        announced that he   does      not to qualify for the  olympics as it      
2024-02-04 06:45:13,949 	Text Alignment  :	I  S           S         S    S    S         S   S  S       S   S    S        S  S       
2024-02-04 06:45:13,949 ========================================================================================================================
2024-02-04 06:45:13,949 Logging Sequence: 173_103.00
2024-02-04 06:45:13,949 	Gloss Reference :	A B+C+D+E
2024-02-04 06:45:13,949 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:45:13,949 	Gloss Alignment :	         
2024-02-04 06:45:13,950 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:45:13,950 	Text Reference  :	***** *** ******** ****** ****** * ********* these rumours are      absolutely rubbish
2024-02-04 06:45:13,950 	Text Hypothesis :	after the incident yuvraj posted a statement on    his     official twitter    handle 
2024-02-04 06:45:13,950 	Text Alignment  :	I     I   I        I      I      I I         S     S       S        S          S      
2024-02-04 06:45:13,950 ========================================================================================================================
2024-02-04 06:45:18,699 Epoch 777: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.91 
2024-02-04 06:45:18,700 EPOCH 778
2024-02-04 06:45:21,907 [Epoch: 778 Step: 00052100] Batch Recognition Loss:   0.000245 => Gls Tokens per Sec:     2046 || Batch Translation Loss:   0.038978 => Txt Tokens per Sec:     5684 || Lr: 0.000050
2024-02-04 06:45:24,318 Epoch 778: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.74 
2024-02-04 06:45:24,318 EPOCH 779
2024-02-04 06:45:29,729 Epoch 779: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.78 
2024-02-04 06:45:29,730 EPOCH 780
2024-02-04 06:45:30,253 [Epoch: 780 Step: 00052200] Batch Recognition Loss:   0.000532 => Gls Tokens per Sec:     2144 || Batch Translation Loss:   0.116384 => Txt Tokens per Sec:     6132 || Lr: 0.000050
2024-02-04 06:45:34,858 Epoch 780: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.19 
2024-02-04 06:45:34,858 EPOCH 781
2024-02-04 06:45:38,055 [Epoch: 781 Step: 00052300] Batch Recognition Loss:   0.000186 => Gls Tokens per Sec:     2003 || Batch Translation Loss:   0.024475 => Txt Tokens per Sec:     5384 || Lr: 0.000050
2024-02-04 06:45:40,394 Epoch 781: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.51 
2024-02-04 06:45:40,394 EPOCH 782
2024-02-04 06:45:45,753 Epoch 782: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.91 
2024-02-04 06:45:45,754 EPOCH 783
2024-02-04 06:45:46,148 [Epoch: 783 Step: 00052400] Batch Recognition Loss:   0.000164 => Gls Tokens per Sec:     2443 || Batch Translation Loss:   0.018983 => Txt Tokens per Sec:     6646 || Lr: 0.000050
2024-02-04 06:45:50,833 Epoch 783: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.08 
2024-02-04 06:45:50,834 EPOCH 784
2024-02-04 06:45:53,539 [Epoch: 784 Step: 00052500] Batch Recognition Loss:   0.000187 => Gls Tokens per Sec:     2308 || Batch Translation Loss:   0.021423 => Txt Tokens per Sec:     6617 || Lr: 0.000050
2024-02-04 06:45:55,477 Epoch 784: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.68 
2024-02-04 06:45:55,478 EPOCH 785
2024-02-04 06:46:01,140 Epoch 785: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.61 
2024-02-04 06:46:01,141 EPOCH 786
2024-02-04 06:46:01,453 [Epoch: 786 Step: 00052600] Batch Recognition Loss:   0.000126 => Gls Tokens per Sec:     2572 || Batch Translation Loss:   0.015330 => Txt Tokens per Sec:     6711 || Lr: 0.000050
2024-02-04 06:46:06,521 Epoch 786: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.57 
2024-02-04 06:46:06,521 EPOCH 787
2024-02-04 06:46:09,548 [Epoch: 787 Step: 00052700] Batch Recognition Loss:   0.000188 => Gls Tokens per Sec:     2009 || Batch Translation Loss:   0.087103 => Txt Tokens per Sec:     5732 || Lr: 0.000050
2024-02-04 06:46:11,589 Epoch 787: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.85 
2024-02-04 06:46:11,589 EPOCH 788
2024-02-04 06:46:16,206 Epoch 788: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.14 
2024-02-04 06:46:16,206 EPOCH 789
2024-02-04 06:46:16,471 [Epoch: 789 Step: 00052800] Batch Recognition Loss:   0.000133 => Gls Tokens per Sec:     2083 || Batch Translation Loss:   0.050426 => Txt Tokens per Sec:     4712 || Lr: 0.000050
2024-02-04 06:46:21,444 Epoch 789: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.62 
2024-02-04 06:46:21,444 EPOCH 790
2024-02-04 06:46:24,366 [Epoch: 790 Step: 00052900] Batch Recognition Loss:   0.000266 => Gls Tokens per Sec:     1997 || Batch Translation Loss:   0.525852 => Txt Tokens per Sec:     5477 || Lr: 0.000050
2024-02-04 06:46:26,728 Epoch 790: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.82 
2024-02-04 06:46:26,728 EPOCH 791
2024-02-04 06:46:32,224 Epoch 791: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.58 
2024-02-04 06:46:32,224 EPOCH 792
2024-02-04 06:46:32,566 [Epoch: 792 Step: 00053000] Batch Recognition Loss:   0.000143 => Gls Tokens per Sec:     1408 || Batch Translation Loss:   0.014889 => Txt Tokens per Sec:     4487 || Lr: 0.000050
2024-02-04 06:46:37,372 Epoch 792: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.83 
2024-02-04 06:46:37,373 EPOCH 793
2024-02-04 06:46:40,315 [Epoch: 793 Step: 00053100] Batch Recognition Loss:   0.000110 => Gls Tokens per Sec:     1959 || Batch Translation Loss:   0.016357 => Txt Tokens per Sec:     5447 || Lr: 0.000050
2024-02-04 06:46:42,781 Epoch 793: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.62 
2024-02-04 06:46:42,781 EPOCH 794
2024-02-04 06:46:48,259 Epoch 794: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.45 
2024-02-04 06:46:48,259 EPOCH 795
2024-02-04 06:46:48,430 [Epoch: 795 Step: 00053200] Batch Recognition Loss:   0.000404 => Gls Tokens per Sec:     1878 || Batch Translation Loss:   0.036159 => Txt Tokens per Sec:     5528 || Lr: 0.000050
2024-02-04 06:46:53,465 Epoch 795: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.42 
2024-02-04 06:46:53,466 EPOCH 796
2024-02-04 06:46:56,337 [Epoch: 796 Step: 00053300] Batch Recognition Loss:   0.000190 => Gls Tokens per Sec:     1920 || Batch Translation Loss:   0.033947 => Txt Tokens per Sec:     5467 || Lr: 0.000050
2024-02-04 06:46:58,722 Epoch 796: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.22 
2024-02-04 06:46:58,722 EPOCH 797
2024-02-04 06:47:03,987 Epoch 797: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.85 
2024-02-04 06:47:03,988 EPOCH 798
2024-02-04 06:47:04,118 [Epoch: 798 Step: 00053400] Batch Recognition Loss:   0.000235 => Gls Tokens per Sec:     1240 || Batch Translation Loss:   0.029184 => Txt Tokens per Sec:     4403 || Lr: 0.000050
2024-02-04 06:47:09,479 Epoch 798: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.69 
2024-02-04 06:47:09,479 EPOCH 799
2024-02-04 06:47:12,193 [Epoch: 799 Step: 00053500] Batch Recognition Loss:   0.000132 => Gls Tokens per Sec:     2005 || Batch Translation Loss:   0.013456 => Txt Tokens per Sec:     5514 || Lr: 0.000050
2024-02-04 06:47:15,055 Epoch 799: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.88 
2024-02-04 06:47:15,055 EPOCH 800
2024-02-04 06:47:20,487 [Epoch: 800 Step: 00053600] Batch Recognition Loss:   0.000137 => Gls Tokens per Sec:     1957 || Batch Translation Loss:   0.014174 => Txt Tokens per Sec:     5433 || Lr: 0.000050
2024-02-04 06:47:20,488 Epoch 800: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.51 
2024-02-04 06:47:20,488 EPOCH 801
2024-02-04 06:47:25,428 Epoch 801: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.78 
2024-02-04 06:47:25,428 EPOCH 802
2024-02-04 06:47:28,300 [Epoch: 802 Step: 00053700] Batch Recognition Loss:   0.000192 => Gls Tokens per Sec:     1808 || Batch Translation Loss:   0.045847 => Txt Tokens per Sec:     5073 || Lr: 0.000050
2024-02-04 06:47:30,942 Epoch 802: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.77 
2024-02-04 06:47:30,942 EPOCH 803
2024-02-04 06:47:36,170 [Epoch: 803 Step: 00053800] Batch Recognition Loss:   0.000124 => Gls Tokens per Sec:     2003 || Batch Translation Loss:   0.020064 => Txt Tokens per Sec:     5566 || Lr: 0.000050
2024-02-04 06:47:36,237 Epoch 803: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.78 
2024-02-04 06:47:36,237 EPOCH 804
2024-02-04 06:47:41,619 Epoch 804: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.62 
2024-02-04 06:47:41,619 EPOCH 805
2024-02-04 06:47:44,513 [Epoch: 805 Step: 00053900] Batch Recognition Loss:   0.000198 => Gls Tokens per Sec:     1739 || Batch Translation Loss:   0.026434 => Txt Tokens per Sec:     4979 || Lr: 0.000050
2024-02-04 06:47:46,974 Epoch 805: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.22 
2024-02-04 06:47:46,975 EPOCH 806
2024-02-04 06:47:52,036 [Epoch: 806 Step: 00054000] Batch Recognition Loss:   0.000124 => Gls Tokens per Sec:     2037 || Batch Translation Loss:   0.016492 => Txt Tokens per Sec:     5678 || Lr: 0.000050
2024-02-04 06:48:00,478 Validation result at epoch 806, step    54000: duration: 8.4421s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00098	Translation Loss: 91795.75000	PPL: 9759.58984
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.76	(BLEU-1: 10.59,	BLEU-2: 3.22,	BLEU-3: 1.39,	BLEU-4: 0.76)
	CHRF 16.81	ROUGE 9.24
2024-02-04 06:48:00,480 Logging Recognition and Translation Outputs
2024-02-04 06:48:00,480 ========================================================================================================================
2024-02-04 06:48:00,480 Logging Sequence: 130_139.00
2024-02-04 06:48:00,480 	Gloss Reference :	A B+C+D+E
2024-02-04 06:48:00,480 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:48:00,480 	Gloss Alignment :	         
2024-02-04 06:48:00,481 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:48:00,483 	Text Reference  :	he shared a picture of  a little pouch he knit for his olympic gold   medal with uk flag on  one  side   and **** japanese flag on      the     other
2024-02-04 06:48:00,483 	Text Hypothesis :	he ****** * ******* won a ****** ***** ** **** *** *** ******* bronze medal **** ** at   the 2012 london and 2016 rio      de   janeiro olympic games
2024-02-04 06:48:00,483 	Text Alignment  :	   D      D D       S     D      D     D  D    D   D   D       S            D    D  S    S   S    S          I    S        S    S       S       S    
2024-02-04 06:48:00,483 ========================================================================================================================
2024-02-04 06:48:00,484 Logging Sequence: 148_155.00
2024-02-04 06:48:00,484 	Gloss Reference :	A B+C+D+E
2024-02-04 06:48:00,484 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:48:00,484 	Gloss Alignment :	         
2024-02-04 06:48:00,484 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:48:00,485 	Text Reference  :	india won the match with 263   balls remaining and  without losing any wicket
2024-02-04 06:48:00,485 	Text Hypothesis :	india won the ***** **** world cup   2023      will be      held   in  qatar 
2024-02-04 06:48:00,485 	Text Alignment  :	              D     D    S     S     S         S    S       S      S   S     
2024-02-04 06:48:00,486 ========================================================================================================================
2024-02-04 06:48:00,486 Logging Sequence: 126_99.00
2024-02-04 06:48:00,486 	Gloss Reference :	A B+C+D+E
2024-02-04 06:48:00,486 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:48:00,486 	Gloss Alignment :	         
2024-02-04 06:48:00,487 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:48:00,487 	Text Reference  :	he dedicated the     medal to *** ** sprinter   milkha singh
2024-02-04 06:48:00,487 	Text Hypothesis :	he got       another year  to win an individual gold   medal
2024-02-04 06:48:00,487 	Text Alignment  :	   S         S       S        I   I  S          S      S    
2024-02-04 06:48:00,487 ========================================================================================================================
2024-02-04 06:48:00,488 Logging Sequence: 149_77.00
2024-02-04 06:48:00,488 	Gloss Reference :	A B+C+D+E
2024-02-04 06:48:00,488 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:48:00,488 	Gloss Alignment :	         
2024-02-04 06:48:00,488 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:48:00,490 	Text Reference  :	and arrested danushka for alleged sexual assault of  a 29       year old    woman      whose name has not *** been    disclosed
2024-02-04 06:48:00,490 	Text Hypothesis :	*** ******** ******** *** woman   from   rose    bay a suburban of   sydney complained that  she  was not get married 2022     
2024-02-04 06:48:00,490 	Text Alignment  :	D   D        D        D   S       S      S       S     S        S    S      S          S     S    S       I   S       S        
2024-02-04 06:48:00,490 ========================================================================================================================
2024-02-04 06:48:00,491 Logging Sequence: 168_15.00
2024-02-04 06:48:00,491 	Gloss Reference :	A B+C+D+E
2024-02-04 06:48:00,491 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:48:00,491 	Gloss Alignment :	         
2024-02-04 06:48:00,491 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:48:00,492 	Text Reference  :	when in       public the couple are always approached for photographys and   autographs
2024-02-04 06:48:00,492 	Text Hypothesis :	and  covering up     the ****** *** little one        to  a            world cup       
2024-02-04 06:48:00,492 	Text Alignment  :	S    S        S          D      D   S      S          S   S            S     S         
2024-02-04 06:48:00,493 ========================================================================================================================
2024-02-04 06:48:00,655 Epoch 806: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.34 
2024-02-04 06:48:00,655 EPOCH 807
2024-02-04 06:48:06,250 Epoch 807: Total Training Recognition Loss 0.02  Total Training Translation Loss 8.20 
2024-02-04 06:48:06,250 EPOCH 808
2024-02-04 06:48:08,542 [Epoch: 808 Step: 00054100] Batch Recognition Loss:   0.000529 => Gls Tokens per Sec:     2165 || Batch Translation Loss:   0.123615 => Txt Tokens per Sec:     6124 || Lr: 0.000050
2024-02-04 06:48:11,479 Epoch 808: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.40 
2024-02-04 06:48:11,479 EPOCH 809
2024-02-04 06:48:16,399 [Epoch: 809 Step: 00054200] Batch Recognition Loss:   0.000377 => Gls Tokens per Sec:     2064 || Batch Translation Loss:   0.029416 => Txt Tokens per Sec:     5735 || Lr: 0.000050
2024-02-04 06:48:16,610 Epoch 809: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.90 
2024-02-04 06:48:16,610 EPOCH 810
2024-02-04 06:48:22,133 Epoch 810: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.67 
2024-02-04 06:48:22,134 EPOCH 811
2024-02-04 06:48:24,287 [Epoch: 811 Step: 00054300] Batch Recognition Loss:   0.000203 => Gls Tokens per Sec:     2187 || Batch Translation Loss:   0.021960 => Txt Tokens per Sec:     6096 || Lr: 0.000050
2024-02-04 06:48:27,502 Epoch 811: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.98 
2024-02-04 06:48:27,503 EPOCH 812
2024-02-04 06:48:32,722 [Epoch: 812 Step: 00054400] Batch Recognition Loss:   0.000188 => Gls Tokens per Sec:     1915 || Batch Translation Loss:   0.020764 => Txt Tokens per Sec:     5321 || Lr: 0.000050
2024-02-04 06:48:33,055 Epoch 812: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.59 
2024-02-04 06:48:33,056 EPOCH 813
2024-02-04 06:48:38,507 Epoch 813: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.89 
2024-02-04 06:48:38,508 EPOCH 814
2024-02-04 06:48:41,028 [Epoch: 814 Step: 00054500] Batch Recognition Loss:   0.000185 => Gls Tokens per Sec:     1842 || Batch Translation Loss:   0.018934 => Txt Tokens per Sec:     5267 || Lr: 0.000050
2024-02-04 06:48:44,047 Epoch 814: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.90 
2024-02-04 06:48:44,047 EPOCH 815
2024-02-04 06:48:49,017 [Epoch: 815 Step: 00054600] Batch Recognition Loss:   0.000294 => Gls Tokens per Sec:     1978 || Batch Translation Loss:   0.042524 => Txt Tokens per Sec:     5461 || Lr: 0.000050
2024-02-04 06:48:49,480 Epoch 815: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.09 
2024-02-04 06:48:49,480 EPOCH 816
2024-02-04 06:48:54,983 Epoch 816: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.64 
2024-02-04 06:48:54,984 EPOCH 817
2024-02-04 06:48:57,309 [Epoch: 817 Step: 00054700] Batch Recognition Loss:   0.000381 => Gls Tokens per Sec:     1927 || Batch Translation Loss:   0.024214 => Txt Tokens per Sec:     5407 || Lr: 0.000050
2024-02-04 06:49:00,543 Epoch 817: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.24 
2024-02-04 06:49:00,543 EPOCH 818
2024-02-04 06:49:05,353 [Epoch: 818 Step: 00054800] Batch Recognition Loss:   0.000162 => Gls Tokens per Sec:     2011 || Batch Translation Loss:   0.023727 => Txt Tokens per Sec:     5637 || Lr: 0.000050
2024-02-04 06:49:05,731 Epoch 818: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.69 
2024-02-04 06:49:05,732 EPOCH 819
2024-02-04 06:49:11,055 Epoch 819: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.46 
2024-02-04 06:49:11,056 EPOCH 820
2024-02-04 06:49:13,095 [Epoch: 820 Step: 00054900] Batch Recognition Loss:   0.000188 => Gls Tokens per Sec:     2075 || Batch Translation Loss:   0.025384 => Txt Tokens per Sec:     5566 || Lr: 0.000050
2024-02-04 06:49:16,322 Epoch 820: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.24 
2024-02-04 06:49:16,322 EPOCH 821
2024-02-04 06:49:21,356 [Epoch: 821 Step: 00055000] Batch Recognition Loss:   0.000251 => Gls Tokens per Sec:     1890 || Batch Translation Loss:   0.019319 => Txt Tokens per Sec:     5239 || Lr: 0.000050
2024-02-04 06:49:21,888 Epoch 821: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.35 
2024-02-04 06:49:21,888 EPOCH 822
2024-02-04 06:49:26,992 Epoch 822: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.44 
2024-02-04 06:49:26,993 EPOCH 823
2024-02-04 06:49:29,007 [Epoch: 823 Step: 00055100] Batch Recognition Loss:   0.000173 => Gls Tokens per Sec:     2067 || Batch Translation Loss:   0.020795 => Txt Tokens per Sec:     5916 || Lr: 0.000050
2024-02-04 06:49:32,244 Epoch 823: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.10 
2024-02-04 06:49:32,244 EPOCH 824
2024-02-04 06:49:37,221 [Epoch: 824 Step: 00055200] Batch Recognition Loss:   0.000316 => Gls Tokens per Sec:     1879 || Batch Translation Loss:   0.468593 => Txt Tokens per Sec:     5215 || Lr: 0.000050
2024-02-04 06:49:37,813 Epoch 824: Total Training Recognition Loss 0.02  Total Training Translation Loss 9.73 
2024-02-04 06:49:37,814 EPOCH 825
2024-02-04 06:49:43,062 Epoch 825: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.35 
2024-02-04 06:49:43,062 EPOCH 826
2024-02-04 06:49:45,053 [Epoch: 826 Step: 00055300] Batch Recognition Loss:   0.000220 => Gls Tokens per Sec:     2010 || Batch Translation Loss:   0.022477 => Txt Tokens per Sec:     5530 || Lr: 0.000050
2024-02-04 06:49:48,388 Epoch 826: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.90 
2024-02-04 06:49:48,389 EPOCH 827
2024-02-04 06:49:52,593 [Epoch: 827 Step: 00055400] Batch Recognition Loss:   0.000191 => Gls Tokens per Sec:     2186 || Batch Translation Loss:   0.024654 => Txt Tokens per Sec:     6026 || Lr: 0.000050
2024-02-04 06:49:53,361 Epoch 827: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.60 
2024-02-04 06:49:53,362 EPOCH 828
2024-02-04 06:49:58,779 Epoch 828: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.62 
2024-02-04 06:49:58,779 EPOCH 829
2024-02-04 06:50:00,619 [Epoch: 829 Step: 00055500] Batch Recognition Loss:   0.000337 => Gls Tokens per Sec:     2088 || Batch Translation Loss:   0.029104 => Txt Tokens per Sec:     5920 || Lr: 0.000050
2024-02-04 06:50:04,270 Epoch 829: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.61 
2024-02-04 06:50:04,270 EPOCH 830
2024-02-04 06:50:08,553 [Epoch: 830 Step: 00055600] Batch Recognition Loss:   0.000260 => Gls Tokens per Sec:     2109 || Batch Translation Loss:   0.026612 => Txt Tokens per Sec:     5885 || Lr: 0.000050
2024-02-04 06:50:09,263 Epoch 830: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.69 
2024-02-04 06:50:09,264 EPOCH 831
2024-02-04 06:50:14,755 Epoch 831: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.52 
2024-02-04 06:50:14,756 EPOCH 832
2024-02-04 06:50:16,202 [Epoch: 832 Step: 00055700] Batch Recognition Loss:   0.000116 => Gls Tokens per Sec:     2548 || Batch Translation Loss:   0.010137 => Txt Tokens per Sec:     6964 || Lr: 0.000050
2024-02-04 06:50:19,801 Epoch 832: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.01 
2024-02-04 06:50:19,801 EPOCH 833
2024-02-04 06:50:24,601 [Epoch: 833 Step: 00055800] Batch Recognition Loss:   0.000133 => Gls Tokens per Sec:     1848 || Batch Translation Loss:   0.018633 => Txt Tokens per Sec:     5091 || Lr: 0.000050
2024-02-04 06:50:25,580 Epoch 833: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.64 
2024-02-04 06:50:25,580 EPOCH 834
2024-02-04 06:50:30,881 Epoch 834: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.35 
2024-02-04 06:50:30,881 EPOCH 835
2024-02-04 06:50:32,594 [Epoch: 835 Step: 00055900] Batch Recognition Loss:   0.000254 => Gls Tokens per Sec:     2057 || Batch Translation Loss:   0.025379 => Txt Tokens per Sec:     5744 || Lr: 0.000050
2024-02-04 06:50:36,118 Epoch 835: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.17 
2024-02-04 06:50:36,118 EPOCH 836
2024-02-04 06:50:40,610 [Epoch: 836 Step: 00056000] Batch Recognition Loss:   0.000198 => Gls Tokens per Sec:     1940 || Batch Translation Loss:   0.017494 => Txt Tokens per Sec:     5449 || Lr: 0.000050
2024-02-04 06:50:48,596 Validation result at epoch 836, step    56000: duration: 7.9845s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00123	Translation Loss: 91977.75000	PPL: 9938.96289
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.67	(BLEU-1: 9.77,	BLEU-2: 3.06,	BLEU-3: 1.32,	BLEU-4: 0.67)
	CHRF 16.40	ROUGE 8.95
2024-02-04 06:50:48,597 Logging Recognition and Translation Outputs
2024-02-04 06:50:48,598 ========================================================================================================================
2024-02-04 06:50:48,598 Logging Sequence: 122_110.00
2024-02-04 06:50:48,598 	Gloss Reference :	A B+C+D+E
2024-02-04 06:50:48,598 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:50:48,598 	Gloss Alignment :	         
2024-02-04 06:50:48,598 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:50:48,599 	Text Reference  :	** now that i    achieved my dream   and   secured a    silver medal 
2024-02-04 06:50:48,599 	Text Hypothesis :	if her goal then used     to olympic games so      many 9      medals
2024-02-04 06:50:48,600 	Text Alignment  :	I  S   S    S    S        S  S       S     S       S    S      S     
2024-02-04 06:50:48,600 ========================================================================================================================
2024-02-04 06:50:48,600 Logging Sequence: 161_111.00
2024-02-04 06:50:48,600 	Gloss Reference :	A B+C+D+E
2024-02-04 06:50:48,600 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:50:48,600 	Gloss Alignment :	         
2024-02-04 06:50:48,600 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:50:48,602 	Text Reference  :	*** ********* ****** ***** his   last game as  captain  was  the     cape      town    test in   south africa in   jan     2022
2024-02-04 06:50:48,602 	Text Hypothesis :	the rajasthan royals owner whose name is   not revealed that shocked teammates january 2022 that is    on     12th january 2022
2024-02-04 06:50:48,602 	Text Alignment  :	I   I         I      I     S     S    S    S   S        S    S       S         S       S    S    S     S      S    S           
2024-02-04 06:50:48,603 ========================================================================================================================
2024-02-04 06:50:48,603 Logging Sequence: 136_79.00
2024-02-04 06:50:48,603 	Gloss Reference :	A B+C+D+E
2024-02-04 06:50:48,603 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:50:48,603 	Gloss Alignment :	         
2024-02-04 06:50:48,603 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:50:48,604 	Text Reference  :	with this win   sindhu became the first indian woman to  win two         individual olympic medals
2024-02-04 06:50:48,605 	Text Hypothesis :	**** **** sadly sindhu lost   the ***** match  and   she was heartbroken over       the     loss  
2024-02-04 06:50:48,605 	Text Alignment  :	D    D    S            S          D     S      S     S   S   S           S          S       S     
2024-02-04 06:50:48,605 ========================================================================================================================
2024-02-04 06:50:48,605 Logging Sequence: 166_335.00
2024-02-04 06:50:48,605 	Gloss Reference :	A B+C+D+E
2024-02-04 06:50:48,605 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:50:48,605 	Gloss Alignment :	         
2024-02-04 06:50:48,606 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:50:48,607 	Text Reference  :	******* ** ********** the ******** **** ** second   world test       championship is scheduled from june 2021  to 30 april 2023 
2024-02-04 06:50:48,607 	Text Hypothesis :	instead of respecting the couple's wish to maintain their daughter's privacy      as he        is   very close to ** go    waste
2024-02-04 06:50:48,607 	Text Alignment  :	I       I  I              I        I    I  S        S     S          S            S  S         S    S    S        D  S     S    
2024-02-04 06:50:48,607 ========================================================================================================================
2024-02-04 06:50:48,608 Logging Sequence: 95_152.00
2024-02-04 06:50:48,608 	Gloss Reference :	A B+C+D+E
2024-02-04 06:50:48,608 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:50:48,608 	Gloss Alignment :	         
2024-02-04 06:50:48,608 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:50:48,608 	Text Reference  :	**** *** ****** how    strange
2024-02-04 06:50:48,609 	Text Hypothesis :	they are caught indian cricket
2024-02-04 06:50:48,609 	Text Alignment  :	I    I   I      S      S      
2024-02-04 06:50:48,609 ========================================================================================================================
2024-02-04 06:50:49,519 Epoch 836: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.10 
2024-02-04 06:50:49,519 EPOCH 837
2024-02-04 06:50:54,773 Epoch 837: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.28 
2024-02-04 06:50:54,773 EPOCH 838
2024-02-04 06:50:56,282 [Epoch: 838 Step: 00056100] Batch Recognition Loss:   0.000180 => Gls Tokens per Sec:     2227 || Batch Translation Loss:   0.026820 => Txt Tokens per Sec:     6038 || Lr: 0.000050
2024-02-04 06:51:00,250 Epoch 838: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.92 
2024-02-04 06:51:00,251 EPOCH 839
2024-02-04 06:51:04,237 [Epoch: 839 Step: 00056200] Batch Recognition Loss:   0.000144 => Gls Tokens per Sec:     2145 || Batch Translation Loss:   0.024462 => Txt Tokens per Sec:     5974 || Lr: 0.000050
2024-02-04 06:51:05,198 Epoch 839: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.88 
2024-02-04 06:51:05,198 EPOCH 840
2024-02-04 06:51:10,675 Epoch 840: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.59 
2024-02-04 06:51:10,675 EPOCH 841
2024-02-04 06:51:12,312 [Epoch: 841 Step: 00056300] Batch Recognition Loss:   0.000284 => Gls Tokens per Sec:     1903 || Batch Translation Loss:   0.023857 => Txt Tokens per Sec:     5372 || Lr: 0.000050
2024-02-04 06:51:15,993 Epoch 841: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.29 
2024-02-04 06:51:15,994 EPOCH 842
2024-02-04 06:51:20,338 [Epoch: 842 Step: 00056400] Batch Recognition Loss:   0.000135 => Gls Tokens per Sec:     1932 || Batch Translation Loss:   0.011697 => Txt Tokens per Sec:     5390 || Lr: 0.000050
2024-02-04 06:51:21,399 Epoch 842: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.29 
2024-02-04 06:51:21,400 EPOCH 843
2024-02-04 06:51:27,057 Epoch 843: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.54 
2024-02-04 06:51:27,057 EPOCH 844
2024-02-04 06:51:28,424 [Epoch: 844 Step: 00056500] Batch Recognition Loss:   0.000127 => Gls Tokens per Sec:     2225 || Batch Translation Loss:   0.033219 => Txt Tokens per Sec:     6217 || Lr: 0.000050
2024-02-04 06:51:31,799 Epoch 844: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.93 
2024-02-04 06:51:31,799 EPOCH 845
2024-02-04 06:51:36,133 [Epoch: 845 Step: 00056600] Batch Recognition Loss:   0.000303 => Gls Tokens per Sec:     1899 || Batch Translation Loss:   0.064085 => Txt Tokens per Sec:     5409 || Lr: 0.000050
2024-02-04 06:51:37,146 Epoch 845: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.34 
2024-02-04 06:51:37,146 EPOCH 846
2024-02-04 06:51:42,552 Epoch 846: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.01 
2024-02-04 06:51:42,553 EPOCH 847
2024-02-04 06:51:43,980 [Epoch: 847 Step: 00056700] Batch Recognition Loss:   0.000190 => Gls Tokens per Sec:     1957 || Batch Translation Loss:   0.031157 => Txt Tokens per Sec:     5588 || Lr: 0.000050
2024-02-04 06:51:47,868 Epoch 847: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.47 
2024-02-04 06:51:47,868 EPOCH 848
2024-02-04 06:51:51,832 [Epoch: 848 Step: 00056800] Batch Recognition Loss:   0.000158 => Gls Tokens per Sec:     2036 || Batch Translation Loss:   0.017154 => Txt Tokens per Sec:     5574 || Lr: 0.000050
2024-02-04 06:51:53,156 Epoch 848: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.97 
2024-02-04 06:51:53,157 EPOCH 849
2024-02-04 06:51:58,155 Epoch 849: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.99 
2024-02-04 06:51:58,155 EPOCH 850
2024-02-04 06:51:59,539 [Epoch: 850 Step: 00056900] Batch Recognition Loss:   0.000593 => Gls Tokens per Sec:     1902 || Batch Translation Loss:   0.047511 => Txt Tokens per Sec:     5126 || Lr: 0.000050
2024-02-04 06:52:03,707 Epoch 850: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.44 
2024-02-04 06:52:03,708 EPOCH 851
2024-02-04 06:52:07,700 [Epoch: 851 Step: 00057000] Batch Recognition Loss:   0.000181 => Gls Tokens per Sec:     1981 || Batch Translation Loss:   0.017342 => Txt Tokens per Sec:     5607 || Lr: 0.000050
2024-02-04 06:52:08,930 Epoch 851: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.46 
2024-02-04 06:52:08,930 EPOCH 852
2024-02-04 06:52:14,335 Epoch 852: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.63 
2024-02-04 06:52:14,335 EPOCH 853
2024-02-04 06:52:15,868 [Epoch: 853 Step: 00057100] Batch Recognition Loss:   0.000246 => Gls Tokens per Sec:     1672 || Batch Translation Loss:   0.029003 => Txt Tokens per Sec:     5098 || Lr: 0.000050
2024-02-04 06:52:19,712 Epoch 853: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.71 
2024-02-04 06:52:19,712 EPOCH 854
2024-02-04 06:52:23,908 [Epoch: 854 Step: 00057200] Batch Recognition Loss:   0.000195 => Gls Tokens per Sec:     1848 || Batch Translation Loss:   0.021844 => Txt Tokens per Sec:     5222 || Lr: 0.000050
2024-02-04 06:52:25,285 Epoch 854: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.70 
2024-02-04 06:52:25,286 EPOCH 855
2024-02-04 06:52:30,627 Epoch 855: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.39 
2024-02-04 06:52:30,627 EPOCH 856
2024-02-04 06:52:31,594 [Epoch: 856 Step: 00057300] Batch Recognition Loss:   0.000155 => Gls Tokens per Sec:     2486 || Batch Translation Loss:   0.017245 => Txt Tokens per Sec:     6789 || Lr: 0.000050
2024-02-04 06:52:36,076 Epoch 856: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.52 
2024-02-04 06:52:36,077 EPOCH 857
2024-02-04 06:52:39,461 [Epoch: 857 Step: 00057400] Batch Recognition Loss:   0.000264 => Gls Tokens per Sec:     2271 || Batch Translation Loss:   0.012027 => Txt Tokens per Sec:     6247 || Lr: 0.000050
2024-02-04 06:52:41,253 Epoch 857: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.46 
2024-02-04 06:52:41,254 EPOCH 858
2024-02-04 06:52:46,632 Epoch 858: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.56 
2024-02-04 06:52:46,632 EPOCH 859
2024-02-04 06:52:47,573 [Epoch: 859 Step: 00057500] Batch Recognition Loss:   0.000145 => Gls Tokens per Sec:     2382 || Batch Translation Loss:   0.014261 => Txt Tokens per Sec:     6378 || Lr: 0.000050
2024-02-04 06:52:51,478 Epoch 859: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.86 
2024-02-04 06:52:51,478 EPOCH 860
2024-02-04 06:52:55,225 [Epoch: 860 Step: 00057600] Batch Recognition Loss:   0.000186 => Gls Tokens per Sec:     1984 || Batch Translation Loss:   0.027567 => Txt Tokens per Sec:     5414 || Lr: 0.000050
2024-02-04 06:52:56,966 Epoch 860: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.85 
2024-02-04 06:52:56,967 EPOCH 861
2024-02-04 06:53:01,938 Epoch 861: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.89 
2024-02-04 06:53:01,938 EPOCH 862
2024-02-04 06:53:03,153 [Epoch: 862 Step: 00057700] Batch Recognition Loss:   0.000238 => Gls Tokens per Sec:     1639 || Batch Translation Loss:   0.019860 => Txt Tokens per Sec:     4895 || Lr: 0.000050
2024-02-04 06:53:07,188 Epoch 862: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.74 
2024-02-04 06:53:07,188 EPOCH 863
2024-02-04 06:53:10,752 [Epoch: 863 Step: 00057800] Batch Recognition Loss:   0.000144 => Gls Tokens per Sec:     2066 || Batch Translation Loss:   0.045527 => Txt Tokens per Sec:     5703 || Lr: 0.000050
2024-02-04 06:53:12,559 Epoch 863: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.56 
2024-02-04 06:53:12,559 EPOCH 864
2024-02-04 06:53:17,799 Epoch 864: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.70 
2024-02-04 06:53:17,799 EPOCH 865
2024-02-04 06:53:18,818 [Epoch: 865 Step: 00057900] Batch Recognition Loss:   0.000205 => Gls Tokens per Sec:     1886 || Batch Translation Loss:   0.018723 => Txt Tokens per Sec:     5229 || Lr: 0.000050
2024-02-04 06:53:23,382 Epoch 865: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.62 
2024-02-04 06:53:23,382 EPOCH 866
2024-02-04 06:53:26,689 [Epoch: 866 Step: 00058000] Batch Recognition Loss:   0.000150 => Gls Tokens per Sec:     2150 || Batch Translation Loss:   0.013017 => Txt Tokens per Sec:     5815 || Lr: 0.000050
2024-02-04 06:53:35,059 Validation result at epoch 866, step    58000: duration: 8.3686s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00143	Translation Loss: 91837.87500	PPL: 9800.81445
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.81	(BLEU-1: 10.65,	BLEU-2: 3.44,	BLEU-3: 1.56,	BLEU-4: 0.81)
	CHRF 16.69	ROUGE 9.11
2024-02-04 06:53:35,061 Logging Recognition and Translation Outputs
2024-02-04 06:53:35,061 ========================================================================================================================
2024-02-04 06:53:35,061 Logging Sequence: 180_138.00
2024-02-04 06:53:35,062 	Gloss Reference :	A B+C+D+E
2024-02-04 06:53:35,062 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:53:35,062 	Gloss Alignment :	         
2024-02-04 06:53:35,062 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:53:35,064 	Text Reference  :	ioa president p t usha constituted a    seven-member panel which included world    champions from     various sports to    inquire into the     allegations
2024-02-04 06:53:35,064 	Text Hypothesis :	*** ********* * * **** not         only this         match they  blamed   mohammed shami     fielding with    a      towel singh   for  india's loss       
2024-02-04 06:53:35,064 	Text Alignment  :	D   D         D D D    S           S    S            S     S     S        S        S         S        S       S      S     S       S    S       S          
2024-02-04 06:53:35,064 ========================================================================================================================
2024-02-04 06:53:35,064 Logging Sequence: 128_189.00
2024-02-04 06:53:35,065 	Gloss Reference :	A B+C+D+E
2024-02-04 06:53:35,065 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:53:35,065 	Gloss Alignment :	         
2024-02-04 06:53:35,065 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:53:35,066 	Text Reference  :	meanwhile some funny    incidents happened during the    match
2024-02-04 06:53:35,066 	Text Hypothesis :	for       the  olympics are       aware    that   sushil kumar
2024-02-04 06:53:35,066 	Text Alignment  :	S         S    S        S         S        S      S      S    
2024-02-04 06:53:35,066 ========================================================================================================================
2024-02-04 06:53:35,066 Logging Sequence: 165_523.00
2024-02-04 06:53:35,066 	Gloss Reference :	A B+C+D+E
2024-02-04 06:53:35,066 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:53:35,067 	Gloss Alignment :	         
2024-02-04 06:53:35,067 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:53:35,068 	Text Reference  :	as he believed that his team might lose if  he    takes  off     his batting pads
2024-02-04 06:53:35,068 	Text Hypothesis :	** ** ******** **** *** when india lost the match ticket booking the indian  team
2024-02-04 06:53:35,068 	Text Alignment  :	D  D  D        D    D   S    S     S    S   S     S      S       S   S       S   
2024-02-04 06:53:35,068 ========================================================================================================================
2024-02-04 06:53:35,068 Logging Sequence: 145_168.00
2024-02-04 06:53:35,068 	Gloss Reference :	A B+C+D+E
2024-02-04 06:53:35,069 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:53:35,069 	Gloss Alignment :	         
2024-02-04 06:53:35,069 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:53:35,070 	Text Reference  :	******* ** the decision    has     devastated sameeha and  her     parents
2024-02-04 06:53:35,070 	Text Hypothesis :	sameeha is an  interesting history about      indian  deaf women's team   
2024-02-04 06:53:35,070 	Text Alignment  :	I       I  S   S           S       S          S       S    S       S      
2024-02-04 06:53:35,070 ========================================================================================================================
2024-02-04 06:53:35,070 Logging Sequence: 92_123.00
2024-02-04 06:53:35,070 	Gloss Reference :	A B+C+D+E
2024-02-04 06:53:35,070 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:53:35,070 	Gloss Alignment :	         
2024-02-04 06:53:35,071 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:53:35,071 	Text Reference  :	a heated argument also took place between members of the    family    and     the two men   
2024-02-04 06:53:35,071 	Text Hypothesis :	* ****** ******** **** **** ***** shekhar filed   a  police complaint against the *** indian
2024-02-04 06:53:35,072 	Text Alignment  :	D D      D        D    D    D     S       S       S  S      S         S           D   S     
2024-02-04 06:53:35,072 ========================================================================================================================
2024-02-04 06:53:36,942 Epoch 866: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.55 
2024-02-04 06:53:36,942 EPOCH 867
2024-02-04 06:53:42,166 Epoch 867: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.45 
2024-02-04 06:53:42,166 EPOCH 868
2024-02-04 06:53:42,977 [Epoch: 868 Step: 00058100] Batch Recognition Loss:   0.000184 => Gls Tokens per Sec:     2173 || Batch Translation Loss:   0.016750 => Txt Tokens per Sec:     6059 || Lr: 0.000050
2024-02-04 06:53:47,077 Epoch 868: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.41 
2024-02-04 06:53:47,077 EPOCH 869
2024-02-04 06:53:50,452 [Epoch: 869 Step: 00058200] Batch Recognition Loss:   0.000154 => Gls Tokens per Sec:     2087 || Batch Translation Loss:   0.053372 => Txt Tokens per Sec:     5832 || Lr: 0.000050
2024-02-04 06:53:52,416 Epoch 869: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.69 
2024-02-04 06:53:52,417 EPOCH 870
2024-02-04 06:53:57,393 Epoch 870: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.28 
2024-02-04 06:53:57,394 EPOCH 871
2024-02-04 06:53:58,293 [Epoch: 871 Step: 00058300] Batch Recognition Loss:   0.000525 => Gls Tokens per Sec:     1784 || Batch Translation Loss:   0.198248 => Txt Tokens per Sec:     4887 || Lr: 0.000050
2024-02-04 06:54:02,946 Epoch 871: Total Training Recognition Loss 0.03  Total Training Translation Loss 10.48 
2024-02-04 06:54:02,946 EPOCH 872
2024-02-04 06:54:06,170 [Epoch: 872 Step: 00058400] Batch Recognition Loss:   0.000359 => Gls Tokens per Sec:     2108 || Batch Translation Loss:   0.029487 => Txt Tokens per Sec:     5999 || Lr: 0.000050
2024-02-04 06:54:08,235 Epoch 872: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.89 
2024-02-04 06:54:08,235 EPOCH 873
2024-02-04 06:54:13,417 Epoch 873: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.41 
2024-02-04 06:54:13,417 EPOCH 874
2024-02-04 06:54:14,182 [Epoch: 874 Step: 00058500] Batch Recognition Loss:   0.000270 => Gls Tokens per Sec:     1885 || Batch Translation Loss:   0.030974 => Txt Tokens per Sec:     5575 || Lr: 0.000050
2024-02-04 06:54:18,727 Epoch 874: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.77 
2024-02-04 06:54:18,727 EPOCH 875
2024-02-04 06:54:22,068 [Epoch: 875 Step: 00058600] Batch Recognition Loss:   0.000240 => Gls Tokens per Sec:     1985 || Batch Translation Loss:   0.025738 => Txt Tokens per Sec:     5639 || Lr: 0.000050
2024-02-04 06:54:23,976 Epoch 875: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.65 
2024-02-04 06:54:23,977 EPOCH 876
2024-02-04 06:54:29,410 Epoch 876: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.32 
2024-02-04 06:54:29,411 EPOCH 877
2024-02-04 06:54:29,892 [Epoch: 877 Step: 00058700] Batch Recognition Loss:   0.000155 => Gls Tokens per Sec:     2661 || Batch Translation Loss:   0.010809 => Txt Tokens per Sec:     7329 || Lr: 0.000050
2024-02-04 06:54:34,695 Epoch 877: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.39 
2024-02-04 06:54:34,695 EPOCH 878
2024-02-04 06:54:37,658 [Epoch: 878 Step: 00058800] Batch Recognition Loss:   0.000149 => Gls Tokens per Sec:     2215 || Batch Translation Loss:   0.022524 => Txt Tokens per Sec:     5898 || Lr: 0.000050
2024-02-04 06:54:39,909 Epoch 878: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.87 
2024-02-04 06:54:39,910 EPOCH 879
2024-02-04 06:54:45,357 Epoch 879: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.10 
2024-02-04 06:54:45,358 EPOCH 880
2024-02-04 06:54:45,855 [Epoch: 880 Step: 00058900] Batch Recognition Loss:   0.000184 => Gls Tokens per Sec:     2254 || Batch Translation Loss:   0.062688 => Txt Tokens per Sec:     6322 || Lr: 0.000050
2024-02-04 06:54:50,457 Epoch 880: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.09 
2024-02-04 06:54:50,458 EPOCH 881
2024-02-04 06:54:53,855 [Epoch: 881 Step: 00059000] Batch Recognition Loss:   0.000112 => Gls Tokens per Sec:     1858 || Batch Translation Loss:   0.030483 => Txt Tokens per Sec:     5162 || Lr: 0.000050
2024-02-04 06:54:55,849 Epoch 881: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.82 
2024-02-04 06:54:55,849 EPOCH 882
2024-02-04 06:55:00,837 Epoch 882: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.01 
2024-02-04 06:55:00,837 EPOCH 883
2024-02-04 06:55:01,275 [Epoch: 883 Step: 00059100] Batch Recognition Loss:   0.000219 => Gls Tokens per Sec:     2197 || Batch Translation Loss:   0.011028 => Txt Tokens per Sec:     5169 || Lr: 0.000050
2024-02-04 06:55:06,126 Epoch 883: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.16 
2024-02-04 06:55:06,127 EPOCH 884
2024-02-04 06:55:08,776 [Epoch: 884 Step: 00059200] Batch Recognition Loss:   0.000196 => Gls Tokens per Sec:     2321 || Batch Translation Loss:   0.015748 => Txt Tokens per Sec:     6359 || Lr: 0.000050
2024-02-04 06:55:11,294 Epoch 884: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.60 
2024-02-04 06:55:11,295 EPOCH 885
2024-02-04 06:55:16,616 Epoch 885: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.17 
2024-02-04 06:55:16,617 EPOCH 886
2024-02-04 06:55:16,950 [Epoch: 886 Step: 00059300] Batch Recognition Loss:   0.000174 => Gls Tokens per Sec:     2402 || Batch Translation Loss:   0.006753 => Txt Tokens per Sec:     5931 || Lr: 0.000050
2024-02-04 06:55:22,216 Epoch 886: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.55 
2024-02-04 06:55:22,217 EPOCH 887
2024-02-04 06:55:25,020 [Epoch: 887 Step: 00059400] Batch Recognition Loss:   0.000137 => Gls Tokens per Sec:     2170 || Batch Translation Loss:   0.018391 => Txt Tokens per Sec:     5979 || Lr: 0.000050
2024-02-04 06:55:27,141 Epoch 887: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.49 
2024-02-04 06:55:27,141 EPOCH 888
2024-02-04 06:55:32,641 Epoch 888: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.84 
2024-02-04 06:55:32,641 EPOCH 889
2024-02-04 06:55:32,906 [Epoch: 889 Step: 00059500] Batch Recognition Loss:   0.000135 => Gls Tokens per Sec:     2424 || Batch Translation Loss:   0.024899 => Txt Tokens per Sec:     7076 || Lr: 0.000050
2024-02-04 06:55:37,695 Epoch 889: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.16 
2024-02-04 06:55:37,695 EPOCH 890
2024-02-04 06:55:41,117 [Epoch: 890 Step: 00059600] Batch Recognition Loss:   0.000218 => Gls Tokens per Sec:     1730 || Batch Translation Loss:   0.021684 => Txt Tokens per Sec:     4946 || Lr: 0.000050
2024-02-04 06:55:43,479 Epoch 890: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.82 
2024-02-04 06:55:43,479 EPOCH 891
2024-02-04 06:55:48,891 Epoch 891: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.97 
2024-02-04 06:55:48,891 EPOCH 892
2024-02-04 06:55:49,060 [Epoch: 892 Step: 00059700] Batch Recognition Loss:   0.000202 => Gls Tokens per Sec:     2874 || Batch Translation Loss:   0.057619 => Txt Tokens per Sec:     7114 || Lr: 0.000050
2024-02-04 06:55:53,803 Epoch 892: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.21 
2024-02-04 06:55:53,803 EPOCH 893
2024-02-04 06:55:56,873 [Epoch: 893 Step: 00059800] Batch Recognition Loss:   0.000262 => Gls Tokens per Sec:     1877 || Batch Translation Loss:   0.036813 => Txt Tokens per Sec:     5358 || Lr: 0.000050
2024-02-04 06:55:59,251 Epoch 893: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.74 
2024-02-04 06:55:59,251 EPOCH 894
2024-02-04 06:56:04,557 Epoch 894: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.63 
2024-02-04 06:56:04,558 EPOCH 895
2024-02-04 06:56:04,690 [Epoch: 895 Step: 00059900] Batch Recognition Loss:   0.000288 => Gls Tokens per Sec:     2443 || Batch Translation Loss:   0.017583 => Txt Tokens per Sec:     6588 || Lr: 0.000050
2024-02-04 06:56:09,708 Epoch 895: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.30 
2024-02-04 06:56:09,709 EPOCH 896
2024-02-04 06:56:12,847 [Epoch: 896 Step: 00060000] Batch Recognition Loss:   0.000269 => Gls Tokens per Sec:     1757 || Batch Translation Loss:   0.027502 => Txt Tokens per Sec:     4989 || Lr: 0.000050
2024-02-04 06:56:21,216 Validation result at epoch 896, step    60000: duration: 8.3686s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00124	Translation Loss: 91836.27344	PPL: 9799.24512
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.87	(BLEU-1: 10.79,	BLEU-2: 3.52,	BLEU-3: 1.60,	BLEU-4: 0.87)
	CHRF 17.06	ROUGE 9.28
2024-02-04 06:56:21,217 Logging Recognition and Translation Outputs
2024-02-04 06:56:21,218 ========================================================================================================================
2024-02-04 06:56:21,218 Logging Sequence: 179_269.00
2024-02-04 06:56:21,218 	Gloss Reference :	A B+C+D+E
2024-02-04 06:56:21,218 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:56:21,218 	Gloss Alignment :	         
2024-02-04 06:56:21,218 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:56:21,219 	Text Reference  :	the ban would mean she can't compete in any   national or other      domestic events  
2024-02-04 06:56:21,219 	Text Hypothesis :	*** *** ***** **** *** ***** ******* ** after her      no disrespect was      intended
2024-02-04 06:56:21,219 	Text Alignment  :	D   D   D     D    D   D     D       D  S     S        S  S          S        S       
2024-02-04 06:56:21,220 ========================================================================================================================
2024-02-04 06:56:21,220 Logging Sequence: 94_253.00
2024-02-04 06:56:21,220 	Gloss Reference :	A B+C+D+E
2024-02-04 06:56:21,220 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:56:21,220 	Gloss Alignment :	         
2024-02-04 06:56:21,220 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:56:21,223 	Text Reference  :	however      some  tickets will be       kept aside    for     physical sale       at the stadiums a    few    days prior to   the  match  
2024-02-04 06:56:21,223 	Text Hypothesis :	surprisingly there were    many negative and  pakistan through their    daughter's by the ******** same people can  have  been many wickets
2024-02-04 06:56:21,223 	Text Alignment  :	S            S     S       S    S        S    S        S       S        S          S      D        S    S      S    S     S    S    S      
2024-02-04 06:56:21,223 ========================================================================================================================
2024-02-04 06:56:21,224 Logging Sequence: 114_201.00
2024-02-04 06:56:21,224 	Gloss Reference :	A B+C+D+E
2024-02-04 06:56:21,224 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:56:21,224 	Gloss Alignment :	         
2024-02-04 06:56:21,224 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:56:21,225 	Text Reference  :	*** ****** ****** this is      his first time winning the copa 
2024-02-04 06:56:21,225 	Text Hypothesis :	the couple posted a    picture of  pant  and  made    the final
2024-02-04 06:56:21,225 	Text Alignment  :	I   I      I      S    S       S   S     S    S           S    
2024-02-04 06:56:21,225 ========================================================================================================================
2024-02-04 06:56:21,225 Logging Sequence: 118_104.00
2024-02-04 06:56:21,226 	Gloss Reference :	A B+C+D+E
2024-02-04 06:56:21,226 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:56:21,226 	Gloss Alignment :	         
2024-02-04 06:56:21,226 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:56:21,227 	Text Reference  :	** *** ***** kylian mbappã strong performance in the **** match was greatly appreciated
2024-02-04 06:56:21,227 	Text Hypothesis :	in the extra time   also   said   that        at the goal from  the penalty shootout   
2024-02-04 06:56:21,227 	Text Alignment  :	I  I   I     S      S      S      S           S      I    S     S   S       S          
2024-02-04 06:56:21,228 ========================================================================================================================
2024-02-04 06:56:21,228 Logging Sequence: 144_74.00
2024-02-04 06:56:21,228 	Gloss Reference :	A B+C+D+E
2024-02-04 06:56:21,228 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:56:21,228 	Gloss Alignment :	         
2024-02-04 06:56:21,228 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:56:21,229 	Text Reference  :	** *** isn't that  amazing
2024-02-04 06:56:21,229 	Text Hypothesis :	it was a     tough auction
2024-02-04 06:56:21,229 	Text Alignment  :	I  I   S     S     S      
2024-02-04 06:56:21,229 ========================================================================================================================
2024-02-04 06:56:23,646 Epoch 896: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.18 
2024-02-04 06:56:23,646 EPOCH 897
2024-02-04 06:56:28,972 Epoch 897: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.43 
2024-02-04 06:56:28,973 EPOCH 898
2024-02-04 06:56:29,069 [Epoch: 898 Step: 00060100] Batch Recognition Loss:   0.000194 => Gls Tokens per Sec:     1684 || Batch Translation Loss:   0.022388 => Txt Tokens per Sec:     5673 || Lr: 0.000050
2024-02-04 06:56:34,228 Epoch 898: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.77 
2024-02-04 06:56:34,229 EPOCH 899
2024-02-04 06:56:36,409 [Epoch: 899 Step: 00060200] Batch Recognition Loss:   0.000186 => Gls Tokens per Sec:     2495 || Batch Translation Loss:   0.012286 => Txt Tokens per Sec:     6457 || Lr: 0.000050
2024-02-04 06:56:38,945 Epoch 899: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.28 
2024-02-04 06:56:38,945 EPOCH 900
2024-02-04 06:56:44,491 [Epoch: 900 Step: 00060300] Batch Recognition Loss:   0.000153 => Gls Tokens per Sec:     1917 || Batch Translation Loss:   0.028661 => Txt Tokens per Sec:     5323 || Lr: 0.000050
2024-02-04 06:56:44,491 Epoch 900: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.35 
2024-02-04 06:56:44,491 EPOCH 901
2024-02-04 06:56:49,930 Epoch 901: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.68 
2024-02-04 06:56:49,931 EPOCH 902
2024-02-04 06:56:52,348 [Epoch: 902 Step: 00060400] Batch Recognition Loss:   0.000254 => Gls Tokens per Sec:     2148 || Batch Translation Loss:   0.011169 => Txt Tokens per Sec:     5675 || Lr: 0.000050
2024-02-04 06:56:55,165 Epoch 902: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.31 
2024-02-04 06:56:55,165 EPOCH 903
2024-02-04 06:57:00,413 [Epoch: 903 Step: 00060500] Batch Recognition Loss:   0.000149 => Gls Tokens per Sec:     1995 || Batch Translation Loss:   0.016708 => Txt Tokens per Sec:     5541 || Lr: 0.000050
2024-02-04 06:57:00,482 Epoch 903: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.07 
2024-02-04 06:57:00,482 EPOCH 904
2024-02-04 06:57:05,769 Epoch 904: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.01 
2024-02-04 06:57:05,769 EPOCH 905
2024-02-04 06:57:08,184 [Epoch: 905 Step: 00060600] Batch Recognition Loss:   0.000156 => Gls Tokens per Sec:     2122 || Batch Translation Loss:   0.016220 => Txt Tokens per Sec:     5697 || Lr: 0.000050
2024-02-04 06:57:11,070 Epoch 905: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.13 
2024-02-04 06:57:11,070 EPOCH 906
2024-02-04 06:57:15,663 [Epoch: 906 Step: 00060700] Batch Recognition Loss:   0.000257 => Gls Tokens per Sec:     2246 || Batch Translation Loss:   0.044531 => Txt Tokens per Sec:     6300 || Lr: 0.000050
2024-02-04 06:57:15,758 Epoch 906: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.27 
2024-02-04 06:57:15,758 EPOCH 907
2024-02-04 06:57:21,219 Epoch 907: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.41 
2024-02-04 06:57:21,220 EPOCH 908
2024-02-04 06:57:23,305 [Epoch: 908 Step: 00060800] Batch Recognition Loss:   0.000229 => Gls Tokens per Sec:     2380 || Batch Translation Loss:   0.024303 => Txt Tokens per Sec:     6348 || Lr: 0.000050
2024-02-04 06:57:26,472 Epoch 908: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.00 
2024-02-04 06:57:26,473 EPOCH 909
2024-02-04 06:57:31,900 [Epoch: 909 Step: 00060900] Batch Recognition Loss:   0.000156 => Gls Tokens per Sec:     1871 || Batch Translation Loss:   0.131689 => Txt Tokens per Sec:     5182 || Lr: 0.000050
2024-02-04 06:57:32,115 Epoch 909: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.60 
2024-02-04 06:57:32,115 EPOCH 910
2024-02-04 06:57:37,111 Epoch 910: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.94 
2024-02-04 06:57:37,111 EPOCH 911
2024-02-04 06:57:39,637 [Epoch: 911 Step: 00061000] Batch Recognition Loss:   0.000128 => Gls Tokens per Sec:     1902 || Batch Translation Loss:   0.012816 => Txt Tokens per Sec:     5403 || Lr: 0.000050
2024-02-04 06:57:42,474 Epoch 911: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.29 
2024-02-04 06:57:42,475 EPOCH 912
2024-02-04 06:57:47,363 [Epoch: 912 Step: 00061100] Batch Recognition Loss:   0.000196 => Gls Tokens per Sec:     2044 || Batch Translation Loss:   0.029463 => Txt Tokens per Sec:     5648 || Lr: 0.000050
2024-02-04 06:57:47,775 Epoch 912: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.71 
2024-02-04 06:57:47,775 EPOCH 913
2024-02-04 06:57:53,077 Epoch 913: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.74 
2024-02-04 06:57:53,077 EPOCH 914
2024-02-04 06:57:55,211 [Epoch: 914 Step: 00061200] Batch Recognition Loss:   0.000323 => Gls Tokens per Sec:     2134 || Batch Translation Loss:   0.027026 => Txt Tokens per Sec:     6025 || Lr: 0.000050
2024-02-04 06:57:58,092 Epoch 914: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.26 
2024-02-04 06:57:58,092 EPOCH 915
2024-02-04 06:58:03,022 [Epoch: 915 Step: 00061300] Batch Recognition Loss:   0.000481 => Gls Tokens per Sec:     1995 || Batch Translation Loss:   0.047769 => Txt Tokens per Sec:     5569 || Lr: 0.000050
2024-02-04 06:58:03,374 Epoch 915: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.12 
2024-02-04 06:58:03,374 EPOCH 916
2024-02-04 06:58:08,794 Epoch 916: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.93 
2024-02-04 06:58:08,795 EPOCH 917
2024-02-04 06:58:10,975 [Epoch: 917 Step: 00061400] Batch Recognition Loss:   0.000184 => Gls Tokens per Sec:     2015 || Batch Translation Loss:   0.104926 => Txt Tokens per Sec:     5524 || Lr: 0.000050
2024-02-04 06:58:14,158 Epoch 917: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.13 
2024-02-04 06:58:14,158 EPOCH 918
2024-02-04 06:58:19,261 [Epoch: 918 Step: 00061500] Batch Recognition Loss:   0.000192 => Gls Tokens per Sec:     1895 || Batch Translation Loss:   0.013682 => Txt Tokens per Sec:     5312 || Lr: 0.000050
2024-02-04 06:58:19,603 Epoch 918: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.27 
2024-02-04 06:58:19,603 EPOCH 919
2024-02-04 06:58:25,037 Epoch 919: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.14 
2024-02-04 06:58:25,038 EPOCH 920
2024-02-04 06:58:27,106 [Epoch: 920 Step: 00061600] Batch Recognition Loss:   0.000305 => Gls Tokens per Sec:     2046 || Batch Translation Loss:   0.050692 => Txt Tokens per Sec:     5473 || Lr: 0.000050
2024-02-04 06:58:30,199 Epoch 920: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.35 
2024-02-04 06:58:30,199 EPOCH 921
2024-02-04 06:58:35,051 [Epoch: 921 Step: 00061700] Batch Recognition Loss:   0.000156 => Gls Tokens per Sec:     1961 || Batch Translation Loss:   0.015629 => Txt Tokens per Sec:     5405 || Lr: 0.000050
2024-02-04 06:58:35,671 Epoch 921: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.02 
2024-02-04 06:58:35,672 EPOCH 922
2024-02-04 06:58:40,877 Epoch 922: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.70 
2024-02-04 06:58:40,877 EPOCH 923
2024-02-04 06:58:42,812 [Epoch: 923 Step: 00061800] Batch Recognition Loss:   0.000174 => Gls Tokens per Sec:     2104 || Batch Translation Loss:   0.036124 => Txt Tokens per Sec:     5787 || Lr: 0.000050
2024-02-04 06:58:46,210 Epoch 923: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.32 
2024-02-04 06:58:46,210 EPOCH 924
2024-02-04 06:58:51,014 [Epoch: 924 Step: 00061900] Batch Recognition Loss:   0.000115 => Gls Tokens per Sec:     1946 || Batch Translation Loss:   0.007960 => Txt Tokens per Sec:     5388 || Lr: 0.000050
2024-02-04 06:58:51,643 Epoch 924: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.00 
2024-02-04 06:58:51,643 EPOCH 925
2024-02-04 06:58:56,672 Epoch 925: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.48 
2024-02-04 06:58:56,673 EPOCH 926
2024-02-04 06:58:58,635 [Epoch: 926 Step: 00062000] Batch Recognition Loss:   0.000136 => Gls Tokens per Sec:     2039 || Batch Translation Loss:   0.021666 => Txt Tokens per Sec:     5516 || Lr: 0.000050
2024-02-04 06:59:06,907 Validation result at epoch 926, step    62000: duration: 8.2716s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00124	Translation Loss: 92450.71875	PPL: 10420.68262
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.66	(BLEU-1: 10.33,	BLEU-2: 3.07,	BLEU-3: 1.30,	BLEU-4: 0.66)
	CHRF 16.36	ROUGE 8.99
2024-02-04 06:59:06,908 Logging Recognition and Translation Outputs
2024-02-04 06:59:06,908 ========================================================================================================================
2024-02-04 06:59:06,908 Logging Sequence: 87_52.00
2024-02-04 06:59:06,909 	Gloss Reference :	A B+C+D+E
2024-02-04 06:59:06,909 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:59:06,909 	Gloss Alignment :	         
2024-02-04 06:59:06,909 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:59:06,910 	Text Reference  :	that is when gambhir walked into bat and rescued india with his   brilliant 97 runs 
2024-02-04 06:59:06,911 	Text Hypothesis :	**** ** even though  india  has  won the from    the   4th  world cup       in qatar
2024-02-04 06:59:06,911 	Text Alignment  :	D    D  S    S       S      S    S   S   S       S     S    S     S         S  S    
2024-02-04 06:59:06,911 ========================================================================================================================
2024-02-04 06:59:06,911 Logging Sequence: 85_2.00
2024-02-04 06:59:06,911 	Gloss Reference :	A B+C+D+E
2024-02-04 06:59:06,911 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:59:06,911 	Gloss Alignment :	         
2024-02-04 06:59:06,912 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:59:06,912 	Text Reference  :	andrew symonds is one of the finest all rounders in   the    history of  australian cricket       
2024-02-04 06:59:06,912 	Text Hypothesis :	****** ******* ** *** ** *** ****** he  has      also played 198     one day        internationals
2024-02-04 06:59:06,913 	Text Alignment  :	D      D       D  D   D  D   D      S   S        S    S      S       S   S          S             
2024-02-04 06:59:06,913 ========================================================================================================================
2024-02-04 06:59:06,913 Logging Sequence: 51_110.00
2024-02-04 06:59:06,913 	Gloss Reference :	A B+C+D+E
2024-02-04 06:59:06,913 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:59:06,913 	Gloss Alignment :	         
2024-02-04 06:59:06,913 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:59:06,914 	Text Reference  :	the aussies were very happy with their victory
2024-02-04 06:59:06,914 	Text Hypothesis :	*** ******* now  they have  won  the   toss   
2024-02-04 06:59:06,914 	Text Alignment  :	D   D       S    S    S     S    S     S      
2024-02-04 06:59:06,914 ========================================================================================================================
2024-02-04 06:59:06,914 Logging Sequence: 72_59.00
2024-02-04 06:59:06,915 	Gloss Reference :	A B+C+D+E
2024-02-04 06:59:06,915 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:59:06,915 	Gloss Alignment :	         
2024-02-04 06:59:06,915 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:59:06,916 	Text Reference  :	**** *** after that sapna and    shobit started arguing and ******* misbehaving with the       cricketer  
2024-02-04 06:59:06,916 	Text Hypothesis :	this you can   see  these reason for    my      victory and gambhir was         a    brilliant performance
2024-02-04 06:59:06,916 	Text Alignment  :	I    I   S     S    S     S      S      S       S           I       S           S    S         S          
2024-02-04 06:59:06,917 ========================================================================================================================
2024-02-04 06:59:06,917 Logging Sequence: 122_184.00
2024-02-04 06:59:06,917 	Gloss Reference :	A B+C+D+E
2024-02-04 06:59:06,917 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 06:59:06,917 	Gloss Alignment :	         
2024-02-04 06:59:06,917 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 06:59:06,918 	Text Reference  :	are playing exceptionally well    and keeping hopes of     further olympic medals alive
2024-02-04 06:59:06,918 	Text Hypothesis :	*** this    is            because gt  topped  the   league stage   with    the    mlc  
2024-02-04 06:59:06,918 	Text Alignment  :	D   S       S             S       S   S       S     S      S       S       S      S    
2024-02-04 06:59:06,918 ========================================================================================================================
2024-02-04 06:59:06,922 Training ended since there were no improvements inthe last learning rate step: 0.000050
2024-02-04 06:59:06,923 Best validation result at step    10000:   1.02 eval_metric.
2024-02-04 06:59:36,719 ------------------------------------------------------------
2024-02-04 06:59:36,720 [DEV] partition [RECOGNITION] experiment [BW]: 1
2024-02-04 06:59:45,394 finished in 8.6738s 
2024-02-04 06:59:45,395 ************************************************************
2024-02-04 06:59:45,395 [DEV] partition [RECOGNITION] results:
	New Best CTC Decode Beam Size: 1
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
2024-02-04 06:59:45,395 ************************************************************
2024-02-04 06:59:45,395 ------------------------------------------------------------
2024-02-04 06:59:45,396 [DEV] partition [RECOGNITION] experiment [BW]: 2
2024-02-04 06:59:53,590 finished in 8.1942s 
2024-02-04 06:59:53,590 ------------------------------------------------------------
2024-02-04 06:59:53,591 [DEV] partition [RECOGNITION] experiment [BW]: 3
2024-02-04 07:00:01,829 finished in 8.2383s 
2024-02-04 07:00:01,830 ------------------------------------------------------------
2024-02-04 07:00:01,830 [DEV] partition [RECOGNITION] experiment [BW]: 4
2024-02-04 07:00:10,041 finished in 8.2106s 
2024-02-04 07:00:10,041 ------------------------------------------------------------
2024-02-04 07:00:10,041 [DEV] partition [RECOGNITION] experiment [BW]: 5
2024-02-04 07:00:18,222 finished in 8.1817s 
2024-02-04 07:00:18,223 ------------------------------------------------------------
2024-02-04 07:00:18,223 [DEV] partition [RECOGNITION] experiment [BW]: 6
2024-02-04 07:00:26,367 finished in 8.1440s 
2024-02-04 07:00:26,368 ------------------------------------------------------------
2024-02-04 07:00:26,368 [DEV] partition [RECOGNITION] experiment [BW]: 7
2024-02-04 07:00:34,478 finished in 8.1095s 
2024-02-04 07:00:34,478 ------------------------------------------------------------
2024-02-04 07:00:34,478 [DEV] partition [RECOGNITION] experiment [BW]: 8
2024-02-04 07:00:42,655 finished in 8.1756s 
2024-02-04 07:00:42,655 ------------------------------------------------------------
2024-02-04 07:00:42,655 [DEV] partition [RECOGNITION] experiment [BW]: 9
2024-02-04 07:00:51,027 finished in 8.3726s 
2024-02-04 07:00:51,027 ------------------------------------------------------------
2024-02-04 07:00:51,027 [DEV] partition [RECOGNITION] experiment [BW]: 10
2024-02-04 07:00:59,434 finished in 8.4072s 
2024-02-04 07:00:59,435 ============================================================
2024-02-04 07:01:07,814 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 1 and Alpha: -1
	BLEU-4 1.02	(BLEU-1: 11.89,	BLEU-2: 4.00,	BLEU-3: 1.85,	BLEU-4: 1.02)
	CHRF 17.47	ROUGE 10.05
2024-02-04 07:01:07,815 ------------------------------------------------------------
2024-02-04 07:03:15,161 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 3 and Alpha: -1
	BLEU-4 1.02	(BLEU-1: 10.49,	BLEU-2: 3.73,	BLEU-3: 1.79,	BLEU-4: 1.02)
	CHRF 16.71	ROUGE 9.48
2024-02-04 07:03:15,162 ------------------------------------------------------------
2024-02-04 07:03:36,650 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 3 and Alpha: 1
	BLEU-4 1.14	(BLEU-1: 11.20,	BLEU-2: 3.95,	BLEU-3: 1.95,	BLEU-4: 1.14)
	CHRF 17.02	ROUGE 9.80
2024-02-04 07:03:36,651 ------------------------------------------------------------
2024-02-04 07:03:47,172 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 3 and Alpha: 2
	BLEU-4 1.15	(BLEU-1: 11.55,	BLEU-2: 4.06,	BLEU-3: 1.98,	BLEU-4: 1.15)
	CHRF 17.19	ROUGE 9.89
2024-02-04 07:03:47,173 ------------------------------------------------------------
2024-02-04 07:04:08,502 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 3 and Alpha: 4
	BLEU-4 1.15	(BLEU-1: 11.65,	BLEU-2: 4.07,	BLEU-3: 1.99,	BLEU-4: 1.15)
	CHRF 17.21	ROUGE 9.93
2024-02-04 07:04:08,503 ------------------------------------------------------------
2024-02-04 07:04:53,955 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 4 and Alpha: 1
	BLEU-4 1.16	(BLEU-1: 10.83,	BLEU-2: 3.96,	BLEU-3: 1.99,	BLEU-4: 1.16)
	CHRF 16.89	ROUGE 9.60
2024-02-04 07:04:53,956 ------------------------------------------------------------
2024-02-04 07:05:05,834 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 4 and Alpha: 2
	BLEU-4 1.17	(BLEU-1: 11.31,	BLEU-2: 4.07,	BLEU-3: 2.02,	BLEU-4: 1.17)
	CHRF 17.19	ROUGE 9.73
2024-02-04 07:05:05,835 ------------------------------------------------------------
2024-02-04 07:05:17,635 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 4 and Alpha: 3
	BLEU-4 1.17	(BLEU-1: 11.38,	BLEU-2: 4.09,	BLEU-3: 2.03,	BLEU-4: 1.17)
	CHRF 17.20	ROUGE 9.73
2024-02-04 07:05:17,636 ------------------------------------------------------------
2024-02-04 07:06:31,350 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 5 and Alpha: 2
	BLEU-4 1.18	(BLEU-1: 11.11,	BLEU-2: 4.09,	BLEU-3: 2.05,	BLEU-4: 1.18)
	CHRF 17.11	ROUGE 9.66
2024-02-04 07:06:31,350 ------------------------------------------------------------
2024-02-04 07:07:49,754 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 6 and Alpha: 1
	BLEU-4 1.19	(BLEU-1: 10.52,	BLEU-2: 3.97,	BLEU-3: 2.01,	BLEU-4: 1.19)
	CHRF 16.79	ROUGE 9.41
2024-02-04 07:07:49,755 ------------------------------------------------------------
2024-02-04 07:08:03,336 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 6 and Alpha: 2
	BLEU-4 1.22	(BLEU-1: 10.99,	BLEU-2: 4.13,	BLEU-3: 2.09,	BLEU-4: 1.22)
	CHRF 17.03	ROUGE 9.60
2024-02-04 07:08:03,337 ------------------------------------------------------------
2024-02-04 07:08:17,037 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 6 and Alpha: 3
	BLEU-4 1.23	(BLEU-1: 11.09,	BLEU-2: 4.20,	BLEU-3: 2.11,	BLEU-4: 1.23)
	CHRF 17.07	ROUGE 9.61
2024-02-04 07:08:17,037 ------------------------------------------------------------
2024-02-04 07:13:24,055 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 9 and Alpha: 2
	BLEU-4 1.23	(BLEU-1: 10.86,	BLEU-2: 4.06,	BLEU-3: 2.08,	BLEU-4: 1.23)
	CHRF 16.99	ROUGE 9.38
2024-02-04 07:13:24,055 ------------------------------------------------------------
2024-02-04 07:15:25,711 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 10 and Alpha: 2
	BLEU-4 1.32	(BLEU-1: 10.89,	BLEU-2: 4.17,	BLEU-3: 2.18,	BLEU-4: 1.32)
	CHRF 17.04	ROUGE 9.39
2024-02-04 07:15:25,712 ------------------------------------------------------------
2024-02-04 07:16:19,551 ************************************************************
2024-02-04 07:16:19,552 [DEV] partition [Recognition & Translation] results:
	Best CTC Decode Beam Size: 1
	Best Translation Beam Size: 10 and Alpha: 2
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 1.32	(BLEU-1: 10.89,	BLEU-2: 4.17,	BLEU-3: 2.18,	BLEU-4: 1.32)
	CHRF 17.04	ROUGE 9.39
2024-02-04 07:16:19,552 ************************************************************
2024-02-04 07:16:37,275 [TEST] partition [Recognition & Translation] results:
	Best CTC Decode Beam Size: 1
	Best Translation Beam Size: 10 and Alpha: 2
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.93	(BLEU-1: 10.25,	BLEU-2: 3.51,	BLEU-3: 1.67,	BLEU-4: 0.93)
	CHRF 16.63	ROUGE 8.74
2024-02-04 07:16:37,276 ************************************************************
