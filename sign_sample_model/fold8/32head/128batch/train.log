2024-02-06 02:19:08,408 Hello! This is Joey-NMT.
2024-02-06 02:19:08,415 Total params: 25642504
2024-02-06 02:19:08,416 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'gloss_output_layer.bias', 'gloss_output_layer.weight', 'sgn_embed.ln.bias', 'sgn_embed.ln.weight', 'sgn_embed.norm.norm.bias', 'sgn_embed.norm.norm.weight', 'txt_embed.norm.norm.bias', 'txt_embed.norm.norm.weight']
2024-02-06 02:19:09,527 cfg.name                           : sign_experiment
2024-02-06 02:19:09,527 cfg.data.data_path                 : ./data/Sports_dataset/8/
2024-02-06 02:19:09,527 cfg.data.version                   : phoenix_2014_trans
2024-02-06 02:19:09,527 cfg.data.sgn                       : sign
2024-02-06 02:19:09,527 cfg.data.txt                       : text
2024-02-06 02:19:09,528 cfg.data.gls                       : gloss
2024-02-06 02:19:09,528 cfg.data.train                     : excel_data.train
2024-02-06 02:19:09,528 cfg.data.dev                       : excel_data.dev
2024-02-06 02:19:09,528 cfg.data.test                      : excel_data.test
2024-02-06 02:19:09,528 cfg.data.feature_size              : 2560
2024-02-06 02:19:09,528 cfg.data.level                     : word
2024-02-06 02:19:09,528 cfg.data.txt_lowercase             : True
2024-02-06 02:19:09,528 cfg.data.max_sent_length           : 500
2024-02-06 02:19:09,529 cfg.data.random_train_subset       : -1
2024-02-06 02:19:09,529 cfg.data.random_dev_subset         : -1
2024-02-06 02:19:09,529 cfg.testing.recognition_beam_sizes : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
2024-02-06 02:19:09,529 cfg.testing.translation_beam_sizes : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
2024-02-06 02:19:09,529 cfg.testing.translation_beam_alphas : [-1, 0, 1, 2, 3, 4, 5]
2024-02-06 02:19:09,529 cfg.training.reset_best_ckpt       : False
2024-02-06 02:19:09,530 cfg.training.reset_scheduler       : False
2024-02-06 02:19:09,530 cfg.training.reset_optimizer       : False
2024-02-06 02:19:09,530 cfg.training.random_seed           : 42
2024-02-06 02:19:09,530 cfg.training.model_dir             : ./sign_sample_model/fold8/32head/128batch
2024-02-06 02:19:09,530 cfg.training.recognition_loss_weight : 1.0
2024-02-06 02:19:09,530 cfg.training.translation_loss_weight : 1.0
2024-02-06 02:19:09,530 cfg.training.eval_metric           : bleu
2024-02-06 02:19:09,530 cfg.training.optimizer             : adam
2024-02-06 02:19:09,531 cfg.training.learning_rate         : 0.0001
2024-02-06 02:19:09,531 cfg.training.batch_size            : 128
2024-02-06 02:19:09,531 cfg.training.num_valid_log         : 5
2024-02-06 02:19:09,531 cfg.training.epochs                : 50000
2024-02-06 02:19:09,531 cfg.training.early_stopping_metric : eval_metric
2024-02-06 02:19:09,531 cfg.training.batch_type            : sentence
2024-02-06 02:19:09,531 cfg.training.translation_normalization : batch
2024-02-06 02:19:09,532 cfg.training.eval_recognition_beam_size : 1
2024-02-06 02:19:09,532 cfg.training.eval_translation_beam_size : 1
2024-02-06 02:19:09,532 cfg.training.eval_translation_beam_alpha : -1
2024-02-06 02:19:09,532 cfg.training.overwrite             : True
2024-02-06 02:19:09,532 cfg.training.shuffle               : True
2024-02-06 02:19:09,532 cfg.training.use_cuda              : True
2024-02-06 02:19:09,533 cfg.training.translation_max_output_length : 40
2024-02-06 02:19:09,533 cfg.training.keep_last_ckpts       : 1
2024-02-06 02:19:09,533 cfg.training.batch_multiplier      : 1
2024-02-06 02:19:09,533 cfg.training.logging_freq          : 100
2024-02-06 02:19:09,533 cfg.training.validation_freq       : 2000
2024-02-06 02:19:09,533 cfg.training.betas                 : [0.9, 0.998]
2024-02-06 02:19:09,533 cfg.training.scheduling            : plateau
2024-02-06 02:19:09,534 cfg.training.learning_rate_min     : 1e-08
2024-02-06 02:19:09,534 cfg.training.weight_decay          : 0.0001
2024-02-06 02:19:09,534 cfg.training.patience              : 12
2024-02-06 02:19:09,534 cfg.training.decrease_factor       : 0.5
2024-02-06 02:19:09,534 cfg.training.label_smoothing       : 0.0
2024-02-06 02:19:09,534 cfg.model.initializer              : xavier
2024-02-06 02:19:09,534 cfg.model.bias_initializer         : zeros
2024-02-06 02:19:09,535 cfg.model.init_gain                : 1.0
2024-02-06 02:19:09,535 cfg.model.embed_initializer        : xavier
2024-02-06 02:19:09,535 cfg.model.embed_init_gain          : 1.0
2024-02-06 02:19:09,535 cfg.model.tied_softmax             : True
2024-02-06 02:19:09,535 cfg.model.encoder.type             : transformer
2024-02-06 02:19:09,535 cfg.model.encoder.num_layers       : 3
2024-02-06 02:19:09,535 cfg.model.encoder.num_heads        : 32
2024-02-06 02:19:09,536 cfg.model.encoder.embeddings.embedding_dim : 512
2024-02-06 02:19:09,536 cfg.model.encoder.embeddings.scale : False
2024-02-06 02:19:09,536 cfg.model.encoder.embeddings.dropout : 0.1
2024-02-06 02:19:09,536 cfg.model.encoder.embeddings.norm_type : batch
2024-02-06 02:19:09,536 cfg.model.encoder.embeddings.activation_type : softsign
2024-02-06 02:19:09,536 cfg.model.encoder.hidden_size      : 512
2024-02-06 02:19:09,537 cfg.model.encoder.ff_size          : 2048
2024-02-06 02:19:09,537 cfg.model.encoder.dropout          : 0.1
2024-02-06 02:19:09,537 cfg.model.decoder.type             : transformer
2024-02-06 02:19:09,537 cfg.model.decoder.num_layers       : 3
2024-02-06 02:19:09,537 cfg.model.decoder.num_heads        : 32
2024-02-06 02:19:09,537 cfg.model.decoder.embeddings.embedding_dim : 512
2024-02-06 02:19:09,537 cfg.model.decoder.embeddings.scale : False
2024-02-06 02:19:09,538 cfg.model.decoder.embeddings.dropout : 0.1
2024-02-06 02:19:09,538 cfg.model.decoder.embeddings.norm_type : batch
2024-02-06 02:19:09,538 cfg.model.decoder.embeddings.activation_type : softsign
2024-02-06 02:19:09,538 cfg.model.decoder.hidden_size      : 512
2024-02-06 02:19:09,538 cfg.model.decoder.ff_size          : 2048
2024-02-06 02:19:09,538 cfg.model.decoder.dropout          : 0.1
2024-02-06 02:19:09,539 Data set sizes: 
	train 2124,
	valid 708,
	test 708
2024-02-06 02:19:09,539 First training example:
	[GLS] A B C D E
	[TXT] how did she become a champion
2024-02-06 02:19:09,539 First 10 words (gls): (0) <si> (1) <unk> (2) <pad> (3) A (4) B (5) C (6) D (7) E
2024-02-06 02:19:09,539 First 10 words (txt): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) the (5) and (6) to (7) in (8) a (9) of
2024-02-06 02:19:09,539 Number of unique glosses (types): 8
2024-02-06 02:19:09,539 Number of unique words (types): 4402
2024-02-06 02:19:09,540 SignModel(
	encoder=TransformerEncoder(num_layers=3, num_heads=32),
	decoder=TransformerDecoder(num_layers=3, num_heads=32),
	sgn_embed=SpatialEmbeddings(embedding_dim=512, input_size=2560),
	txt_embed=Embeddings(embedding_dim=512, vocab_size=4402))
2024-02-06 02:19:09,544 EPOCH 1
2024-02-06 02:19:26,424 Epoch   1: Total Training Recognition Loss 128.04  Total Training Translation Loss 1791.33 
2024-02-06 02:19:26,425 EPOCH 2
2024-02-06 02:19:37,258 Epoch   2: Total Training Recognition Loss 54.18  Total Training Translation Loss 1633.16 
2024-02-06 02:19:37,259 EPOCH 3
2024-02-06 02:19:47,970 Epoch   3: Total Training Recognition Loss 32.20  Total Training Translation Loss 1554.05 
2024-02-06 02:19:47,970 EPOCH 4
2024-02-06 02:19:58,899 Epoch   4: Total Training Recognition Loss 24.27  Total Training Translation Loss 1517.32 
2024-02-06 02:19:58,900 EPOCH 5
2024-02-06 02:20:09,908 Epoch   5: Total Training Recognition Loss 19.52  Total Training Translation Loss 1501.90 
2024-02-06 02:20:09,909 EPOCH 6
2024-02-06 02:20:20,275 [Epoch: 006 Step: 00000100] Batch Recognition Loss:   0.976506 => Gls Tokens per Sec:      901 || Batch Translation Loss:  44.967590 => Txt Tokens per Sec:     2516 || Lr: 0.000100
2024-02-06 02:20:20,686 Epoch   6: Total Training Recognition Loss 16.28  Total Training Translation Loss 1492.68 
2024-02-06 02:20:20,686 EPOCH 7
2024-02-06 02:20:31,451 Epoch   7: Total Training Recognition Loss 14.83  Total Training Translation Loss 1482.00 
2024-02-06 02:20:31,452 EPOCH 8
2024-02-06 02:20:42,498 Epoch   8: Total Training Recognition Loss 13.52  Total Training Translation Loss 1463.30 
2024-02-06 02:20:42,499 EPOCH 9
2024-02-06 02:20:53,345 Epoch   9: Total Training Recognition Loss 12.20  Total Training Translation Loss 1440.38 
2024-02-06 02:20:53,345 EPOCH 10
2024-02-06 02:21:04,120 Epoch  10: Total Training Recognition Loss 20.95  Total Training Translation Loss 1414.55 
2024-02-06 02:21:04,121 EPOCH 11
2024-02-06 02:21:14,695 Epoch  11: Total Training Recognition Loss 17.87  Total Training Translation Loss 1386.50 
2024-02-06 02:21:14,696 EPOCH 12
2024-02-06 02:21:20,574 [Epoch: 012 Step: 00000200] Batch Recognition Loss:   0.527052 => Gls Tokens per Sec:     1416 || Batch Translation Loss:  81.015686 => Txt Tokens per Sec:     3945 || Lr: 0.000100
2024-02-06 02:21:25,463 Epoch  12: Total Training Recognition Loss 18.47  Total Training Translation Loss 1359.20 
2024-02-06 02:21:25,463 EPOCH 13
2024-02-06 02:21:36,434 Epoch  13: Total Training Recognition Loss 36.99  Total Training Translation Loss 1332.63 
2024-02-06 02:21:36,434 EPOCH 14
2024-02-06 02:21:47,431 Epoch  14: Total Training Recognition Loss 20.67  Total Training Translation Loss 1303.79 
2024-02-06 02:21:47,432 EPOCH 15
2024-02-06 02:21:58,464 Epoch  15: Total Training Recognition Loss 12.36  Total Training Translation Loss 1273.04 
2024-02-06 02:21:58,464 EPOCH 16
2024-02-06 02:22:09,275 Epoch  16: Total Training Recognition Loss 10.32  Total Training Translation Loss 1247.23 
2024-02-06 02:22:09,275 EPOCH 17
2024-02-06 02:22:20,034 Epoch  17: Total Training Recognition Loss 8.58  Total Training Translation Loss 1219.95 
2024-02-06 02:22:20,034 EPOCH 18
2024-02-06 02:22:26,856 [Epoch: 018 Step: 00000300] Batch Recognition Loss:   0.232222 => Gls Tokens per Sec:      994 || Batch Translation Loss:  69.765289 => Txt Tokens per Sec:     2841 || Lr: 0.000100
2024-02-06 02:22:31,038 Epoch  18: Total Training Recognition Loss 7.65  Total Training Translation Loss 1194.55 
2024-02-06 02:22:31,039 EPOCH 19
2024-02-06 02:22:41,785 Epoch  19: Total Training Recognition Loss 7.34  Total Training Translation Loss 1182.65 
2024-02-06 02:22:41,785 EPOCH 20
2024-02-06 02:22:52,584 Epoch  20: Total Training Recognition Loss 6.71  Total Training Translation Loss 1150.37 
2024-02-06 02:22:52,585 EPOCH 21
2024-02-06 02:23:03,428 Epoch  21: Total Training Recognition Loss 6.19  Total Training Translation Loss 1121.88 
2024-02-06 02:23:03,428 EPOCH 22
2024-02-06 02:23:14,010 Epoch  22: Total Training Recognition Loss 5.64  Total Training Translation Loss 1094.96 
2024-02-06 02:23:14,011 EPOCH 23
2024-02-06 02:23:24,890 Epoch  23: Total Training Recognition Loss 4.97  Total Training Translation Loss 1076.65 
2024-02-06 02:23:24,890 EPOCH 24
2024-02-06 02:23:33,777 [Epoch: 024 Step: 00000400] Batch Recognition Loss:   0.265214 => Gls Tokens per Sec:      619 || Batch Translation Loss:  70.726700 => Txt Tokens per Sec:     1738 || Lr: 0.000100
2024-02-06 02:23:35,603 Epoch  24: Total Training Recognition Loss 4.85  Total Training Translation Loss 1066.68 
2024-02-06 02:23:35,604 EPOCH 25
2024-02-06 02:23:46,065 Epoch  25: Total Training Recognition Loss 4.67  Total Training Translation Loss 1041.97 
2024-02-06 02:23:46,066 EPOCH 26
2024-02-06 02:23:56,797 Epoch  26: Total Training Recognition Loss 4.47  Total Training Translation Loss 1026.85 
2024-02-06 02:23:56,797 EPOCH 27
2024-02-06 02:24:07,300 Epoch  27: Total Training Recognition Loss 4.16  Total Training Translation Loss 1001.60 
2024-02-06 02:24:07,301 EPOCH 28
2024-02-06 02:24:18,595 Epoch  28: Total Training Recognition Loss 4.03  Total Training Translation Loss 979.50 
2024-02-06 02:24:18,595 EPOCH 29
2024-02-06 02:24:29,323 Epoch  29: Total Training Recognition Loss 3.68  Total Training Translation Loss 953.94 
2024-02-06 02:24:29,323 EPOCH 30
2024-02-06 02:24:34,148 [Epoch: 030 Step: 00000500] Batch Recognition Loss:   0.508456 => Gls Tokens per Sec:      929 || Batch Translation Loss:  72.281548 => Txt Tokens per Sec:     2534 || Lr: 0.000100
2024-02-06 02:24:40,239 Epoch  30: Total Training Recognition Loss 3.41  Total Training Translation Loss 935.38 
2024-02-06 02:24:40,239 EPOCH 31
2024-02-06 02:24:51,111 Epoch  31: Total Training Recognition Loss 3.33  Total Training Translation Loss 917.75 
2024-02-06 02:24:51,111 EPOCH 32
2024-02-06 02:25:02,124 Epoch  32: Total Training Recognition Loss 3.14  Total Training Translation Loss 899.72 
2024-02-06 02:25:02,125 EPOCH 33
2024-02-06 02:25:12,678 Epoch  33: Total Training Recognition Loss 3.01  Total Training Translation Loss 877.50 
2024-02-06 02:25:12,678 EPOCH 34
2024-02-06 02:25:23,526 Epoch  34: Total Training Recognition Loss 2.93  Total Training Translation Loss 875.53 
2024-02-06 02:25:23,526 EPOCH 35
2024-02-06 02:25:34,454 Epoch  35: Total Training Recognition Loss 2.76  Total Training Translation Loss 860.66 
2024-02-06 02:25:34,454 EPOCH 36
2024-02-06 02:25:37,500 [Epoch: 036 Step: 00000600] Batch Recognition Loss:   0.396364 => Gls Tokens per Sec:      965 || Batch Translation Loss:  55.951893 => Txt Tokens per Sec:     2409 || Lr: 0.000100
2024-02-06 02:25:45,065 Epoch  36: Total Training Recognition Loss 2.80  Total Training Translation Loss 842.58 
2024-02-06 02:25:45,065 EPOCH 37
2024-02-06 02:25:55,966 Epoch  37: Total Training Recognition Loss 2.59  Total Training Translation Loss 819.05 
2024-02-06 02:25:55,966 EPOCH 38
2024-02-06 02:26:06,379 Epoch  38: Total Training Recognition Loss 2.58  Total Training Translation Loss 798.37 
2024-02-06 02:26:06,379 EPOCH 39
2024-02-06 02:26:17,022 Epoch  39: Total Training Recognition Loss 2.52  Total Training Translation Loss 782.46 
2024-02-06 02:26:17,022 EPOCH 40
2024-02-06 02:26:27,747 Epoch  40: Total Training Recognition Loss 2.36  Total Training Translation Loss 765.53 
2024-02-06 02:26:27,747 EPOCH 41
2024-02-06 02:26:38,632 Epoch  41: Total Training Recognition Loss 2.37  Total Training Translation Loss 752.80 
2024-02-06 02:26:38,632 EPOCH 42
2024-02-06 02:26:40,567 [Epoch: 042 Step: 00000700] Batch Recognition Loss:   0.171886 => Gls Tokens per Sec:      993 || Batch Translation Loss:  56.693783 => Txt Tokens per Sec:     2747 || Lr: 0.000100
2024-02-06 02:26:49,292 Epoch  42: Total Training Recognition Loss 2.36  Total Training Translation Loss 729.57 
2024-02-06 02:26:49,293 EPOCH 43
2024-02-06 02:27:00,170 Epoch  43: Total Training Recognition Loss 2.34  Total Training Translation Loss 715.08 
2024-02-06 02:27:00,171 EPOCH 44
2024-02-06 02:27:11,120 Epoch  44: Total Training Recognition Loss 2.27  Total Training Translation Loss 699.41 
2024-02-06 02:27:11,120 EPOCH 45
2024-02-06 02:27:22,115 Epoch  45: Total Training Recognition Loss 2.32  Total Training Translation Loss 682.07 
2024-02-06 02:27:22,116 EPOCH 46
2024-02-06 02:27:32,886 Epoch  46: Total Training Recognition Loss 2.38  Total Training Translation Loss 664.12 
2024-02-06 02:27:32,886 EPOCH 47
2024-02-06 02:27:43,513 Epoch  47: Total Training Recognition Loss 1.99  Total Training Translation Loss 656.77 
2024-02-06 02:27:43,513 EPOCH 48
2024-02-06 02:27:43,645 [Epoch: 048 Step: 00000800] Batch Recognition Loss:   0.101157 => Gls Tokens per Sec:     4886 || Batch Translation Loss:  28.995609 => Txt Tokens per Sec:    10450 || Lr: 0.000100
2024-02-06 02:27:54,377 Epoch  48: Total Training Recognition Loss 2.13  Total Training Translation Loss 633.83 
2024-02-06 02:27:54,378 EPOCH 49
2024-02-06 02:28:05,222 Epoch  49: Total Training Recognition Loss 1.96  Total Training Translation Loss 619.68 
2024-02-06 02:28:05,223 EPOCH 50
2024-02-06 02:28:16,191 Epoch  50: Total Training Recognition Loss 1.92  Total Training Translation Loss 614.49 
2024-02-06 02:28:16,191 EPOCH 51
2024-02-06 02:28:26,814 Epoch  51: Total Training Recognition Loss 1.90  Total Training Translation Loss 597.08 
2024-02-06 02:28:26,814 EPOCH 52
2024-02-06 02:28:37,456 Epoch  52: Total Training Recognition Loss 1.90  Total Training Translation Loss 580.55 
2024-02-06 02:28:37,456 EPOCH 53
2024-02-06 02:28:48,520 [Epoch: 053 Step: 00000900] Batch Recognition Loss:   0.099018 => Gls Tokens per Sec:      902 || Batch Translation Loss:  22.809330 => Txt Tokens per Sec:     2565 || Lr: 0.000100
2024-02-06 02:28:48,665 Epoch  53: Total Training Recognition Loss 1.80  Total Training Translation Loss 562.64 
2024-02-06 02:28:48,666 EPOCH 54
2024-02-06 02:28:59,596 Epoch  54: Total Training Recognition Loss 1.77  Total Training Translation Loss 563.77 
2024-02-06 02:28:59,597 EPOCH 55
2024-02-06 02:29:10,436 Epoch  55: Total Training Recognition Loss 1.78  Total Training Translation Loss 539.79 
2024-02-06 02:29:10,436 EPOCH 56
2024-02-06 02:29:21,613 Epoch  56: Total Training Recognition Loss 1.73  Total Training Translation Loss 532.72 
2024-02-06 02:29:21,613 EPOCH 57
2024-02-06 02:29:32,683 Epoch  57: Total Training Recognition Loss 1.82  Total Training Translation Loss 507.33 
2024-02-06 02:29:32,683 EPOCH 58
2024-02-06 02:29:43,661 Epoch  58: Total Training Recognition Loss 1.72  Total Training Translation Loss 495.33 
2024-02-06 02:29:43,661 EPOCH 59
2024-02-06 02:29:53,942 [Epoch: 059 Step: 00001000] Batch Recognition Loss:   0.050578 => Gls Tokens per Sec:      846 || Batch Translation Loss:  24.988049 => Txt Tokens per Sec:     2362 || Lr: 0.000100
2024-02-06 02:29:54,625 Epoch  59: Total Training Recognition Loss 1.76  Total Training Translation Loss 479.86 
2024-02-06 02:29:54,625 EPOCH 60
2024-02-06 02:30:05,464 Epoch  60: Total Training Recognition Loss 1.72  Total Training Translation Loss 464.95 
2024-02-06 02:30:05,465 EPOCH 61
2024-02-06 02:30:17,004 Epoch  61: Total Training Recognition Loss 1.69  Total Training Translation Loss 447.91 
2024-02-06 02:30:17,005 EPOCH 62
2024-02-06 02:30:27,897 Epoch  62: Total Training Recognition Loss 1.57  Total Training Translation Loss 435.53 
2024-02-06 02:30:27,898 EPOCH 63
2024-02-06 02:30:38,917 Epoch  63: Total Training Recognition Loss 1.58  Total Training Translation Loss 422.47 
2024-02-06 02:30:38,917 EPOCH 64
2024-02-06 02:30:49,753 Epoch  64: Total Training Recognition Loss 1.69  Total Training Translation Loss 416.96 
2024-02-06 02:30:49,753 EPOCH 65
2024-02-06 02:30:56,174 [Epoch: 065 Step: 00001100] Batch Recognition Loss:   0.040921 => Gls Tokens per Sec:     1156 || Batch Translation Loss:  22.917747 => Txt Tokens per Sec:     3168 || Lr: 0.000100
2024-02-06 02:31:00,546 Epoch  65: Total Training Recognition Loss 1.65  Total Training Translation Loss 412.93 
2024-02-06 02:31:00,546 EPOCH 66
2024-02-06 02:31:11,400 Epoch  66: Total Training Recognition Loss 1.60  Total Training Translation Loss 391.90 
2024-02-06 02:31:11,401 EPOCH 67
2024-02-06 02:31:22,383 Epoch  67: Total Training Recognition Loss 1.56  Total Training Translation Loss 378.32 
2024-02-06 02:31:22,383 EPOCH 68
2024-02-06 02:31:33,128 Epoch  68: Total Training Recognition Loss 1.47  Total Training Translation Loss 367.11 
2024-02-06 02:31:33,129 EPOCH 69
2024-02-06 02:31:43,668 Epoch  69: Total Training Recognition Loss 1.44  Total Training Translation Loss 353.96 
2024-02-06 02:31:43,668 EPOCH 70
2024-02-06 02:31:54,327 Epoch  70: Total Training Recognition Loss 1.46  Total Training Translation Loss 347.87 
2024-02-06 02:31:54,328 EPOCH 71
2024-02-06 02:31:59,669 [Epoch: 071 Step: 00001200] Batch Recognition Loss:   0.092804 => Gls Tokens per Sec:     1199 || Batch Translation Loss:  13.493877 => Txt Tokens per Sec:     3171 || Lr: 0.000100
2024-02-06 02:32:05,037 Epoch  71: Total Training Recognition Loss 1.64  Total Training Translation Loss 332.92 
2024-02-06 02:32:05,038 EPOCH 72
2024-02-06 02:32:16,172 Epoch  72: Total Training Recognition Loss 1.60  Total Training Translation Loss 329.00 
2024-02-06 02:32:16,173 EPOCH 73
2024-02-06 02:32:26,984 Epoch  73: Total Training Recognition Loss 1.50  Total Training Translation Loss 312.33 
2024-02-06 02:32:26,985 EPOCH 74
2024-02-06 02:32:37,752 Epoch  74: Total Training Recognition Loss 1.40  Total Training Translation Loss 304.62 
2024-02-06 02:32:37,752 EPOCH 75
2024-02-06 02:32:48,580 Epoch  75: Total Training Recognition Loss 1.39  Total Training Translation Loss 294.42 
2024-02-06 02:32:48,580 EPOCH 76
2024-02-06 02:32:59,414 Epoch  76: Total Training Recognition Loss 1.37  Total Training Translation Loss 278.78 
2024-02-06 02:32:59,414 EPOCH 77
2024-02-06 02:33:02,624 [Epoch: 077 Step: 00001300] Batch Recognition Loss:   0.036468 => Gls Tokens per Sec:     1596 || Batch Translation Loss:  15.212393 => Txt Tokens per Sec:     4093 || Lr: 0.000100
2024-02-06 02:33:10,309 Epoch  77: Total Training Recognition Loss 1.38  Total Training Translation Loss 270.85 
2024-02-06 02:33:10,310 EPOCH 78
2024-02-06 02:33:21,239 Epoch  78: Total Training Recognition Loss 1.42  Total Training Translation Loss 266.97 
2024-02-06 02:33:21,239 EPOCH 79
2024-02-06 02:33:32,302 Epoch  79: Total Training Recognition Loss 1.43  Total Training Translation Loss 255.49 
2024-02-06 02:33:32,302 EPOCH 80
2024-02-06 02:33:43,192 Epoch  80: Total Training Recognition Loss 1.32  Total Training Translation Loss 243.15 
2024-02-06 02:33:43,193 EPOCH 81
2024-02-06 02:33:54,018 Epoch  81: Total Training Recognition Loss 1.32  Total Training Translation Loss 233.86 
2024-02-06 02:33:54,018 EPOCH 82
2024-02-06 02:34:04,877 Epoch  82: Total Training Recognition Loss 1.35  Total Training Translation Loss 225.98 
2024-02-06 02:34:04,878 EPOCH 83
2024-02-06 02:34:07,915 [Epoch: 083 Step: 00001400] Batch Recognition Loss:   0.088154 => Gls Tokens per Sec:     1265 || Batch Translation Loss:  17.645266 => Txt Tokens per Sec:     3703 || Lr: 0.000100
2024-02-06 02:34:15,777 Epoch  83: Total Training Recognition Loss 1.31  Total Training Translation Loss 225.09 
2024-02-06 02:34:15,777 EPOCH 84
2024-02-06 02:34:26,291 Epoch  84: Total Training Recognition Loss 1.32  Total Training Translation Loss 212.89 
2024-02-06 02:34:26,292 EPOCH 85
2024-02-06 02:34:37,165 Epoch  85: Total Training Recognition Loss 1.28  Total Training Translation Loss 200.47 
2024-02-06 02:34:37,166 EPOCH 86
2024-02-06 02:34:47,957 Epoch  86: Total Training Recognition Loss 1.23  Total Training Translation Loss 193.42 
2024-02-06 02:34:47,957 EPOCH 87
2024-02-06 02:34:58,927 Epoch  87: Total Training Recognition Loss 1.19  Total Training Translation Loss 183.20 
2024-02-06 02:34:58,928 EPOCH 88
2024-02-06 02:35:09,731 Epoch  88: Total Training Recognition Loss 1.19  Total Training Translation Loss 176.25 
2024-02-06 02:35:09,731 EPOCH 89
2024-02-06 02:35:12,811 [Epoch: 089 Step: 00001500] Batch Recognition Loss:   0.095236 => Gls Tokens per Sec:      747 || Batch Translation Loss:   4.295588 => Txt Tokens per Sec:     1868 || Lr: 0.000100
2024-02-06 02:35:20,615 Epoch  89: Total Training Recognition Loss 1.20  Total Training Translation Loss 171.32 
2024-02-06 02:35:20,615 EPOCH 90
2024-02-06 02:35:31,403 Epoch  90: Total Training Recognition Loss 1.19  Total Training Translation Loss 161.92 
2024-02-06 02:35:31,404 EPOCH 91
2024-02-06 02:35:41,932 Epoch  91: Total Training Recognition Loss 1.08  Total Training Translation Loss 154.37 
2024-02-06 02:35:41,933 EPOCH 92
2024-02-06 02:35:52,743 Epoch  92: Total Training Recognition Loss 1.09  Total Training Translation Loss 149.51 
2024-02-06 02:35:52,744 EPOCH 93
2024-02-06 02:36:03,640 Epoch  93: Total Training Recognition Loss 1.08  Total Training Translation Loss 144.34 
2024-02-06 02:36:03,640 EPOCH 94
2024-02-06 02:36:14,481 Epoch  94: Total Training Recognition Loss 1.08  Total Training Translation Loss 140.35 
2024-02-06 02:36:14,481 EPOCH 95
2024-02-06 02:36:14,907 [Epoch: 095 Step: 00001600] Batch Recognition Loss:   0.042553 => Gls Tokens per Sec:     3006 || Batch Translation Loss:   8.498411 => Txt Tokens per Sec:     8635 || Lr: 0.000100
2024-02-06 02:36:25,220 Epoch  95: Total Training Recognition Loss 1.05  Total Training Translation Loss 132.26 
2024-02-06 02:36:25,221 EPOCH 96
2024-02-06 02:36:35,726 Epoch  96: Total Training Recognition Loss 1.06  Total Training Translation Loss 124.69 
2024-02-06 02:36:35,726 EPOCH 97
2024-02-06 02:36:46,510 Epoch  97: Total Training Recognition Loss 1.10  Total Training Translation Loss 120.88 
2024-02-06 02:36:46,511 EPOCH 98
2024-02-06 02:36:57,489 Epoch  98: Total Training Recognition Loss 1.10  Total Training Translation Loss 116.44 
2024-02-06 02:36:57,490 EPOCH 99
2024-02-06 02:37:08,726 Epoch  99: Total Training Recognition Loss 1.01  Total Training Translation Loss 112.72 
2024-02-06 02:37:08,727 EPOCH 100
2024-02-06 02:37:19,762 [Epoch: 100 Step: 00001700] Batch Recognition Loss:   0.062749 => Gls Tokens per Sec:      963 || Batch Translation Loss:   7.902446 => Txt Tokens per Sec:     2663 || Lr: 0.000100
2024-02-06 02:37:19,762 Epoch 100: Total Training Recognition Loss 0.98  Total Training Translation Loss 107.68 
2024-02-06 02:37:19,762 EPOCH 101
2024-02-06 02:37:30,800 Epoch 101: Total Training Recognition Loss 0.99  Total Training Translation Loss 102.56 
2024-02-06 02:37:30,801 EPOCH 102
2024-02-06 02:37:42,165 Epoch 102: Total Training Recognition Loss 0.95  Total Training Translation Loss 97.33 
2024-02-06 02:37:42,166 EPOCH 103
2024-02-06 02:37:53,080 Epoch 103: Total Training Recognition Loss 0.95  Total Training Translation Loss 92.55 
2024-02-06 02:37:53,081 EPOCH 104
2024-02-06 02:38:03,724 Epoch 104: Total Training Recognition Loss 0.93  Total Training Translation Loss 89.00 
2024-02-06 02:38:03,724 EPOCH 105
2024-02-06 02:38:14,475 Epoch 105: Total Training Recognition Loss 0.94  Total Training Translation Loss 84.21 
2024-02-06 02:38:14,475 EPOCH 106
2024-02-06 02:38:25,196 [Epoch: 106 Step: 00001800] Batch Recognition Loss:   0.032487 => Gls Tokens per Sec:      871 || Batch Translation Loss:   3.980853 => Txt Tokens per Sec:     2421 || Lr: 0.000100
2024-02-06 02:38:25,670 Epoch 106: Total Training Recognition Loss 0.82  Total Training Translation Loss 82.72 
2024-02-06 02:38:25,670 EPOCH 107
2024-02-06 02:38:37,084 Epoch 107: Total Training Recognition Loss 0.88  Total Training Translation Loss 77.76 
2024-02-06 02:38:37,085 EPOCH 108
2024-02-06 02:38:48,420 Epoch 108: Total Training Recognition Loss 0.84  Total Training Translation Loss 74.66 
2024-02-06 02:38:48,421 EPOCH 109
2024-02-06 02:38:59,527 Epoch 109: Total Training Recognition Loss 0.77  Total Training Translation Loss 71.02 
2024-02-06 02:38:59,528 EPOCH 110
2024-02-06 02:39:10,623 Epoch 110: Total Training Recognition Loss 0.84  Total Training Translation Loss 69.11 
2024-02-06 02:39:10,624 EPOCH 111
2024-02-06 02:39:21,819 Epoch 111: Total Training Recognition Loss 0.82  Total Training Translation Loss 66.55 
2024-02-06 02:39:21,819 EPOCH 112
2024-02-06 02:39:32,014 [Epoch: 112 Step: 00001900] Batch Recognition Loss:   0.069450 => Gls Tokens per Sec:      791 || Batch Translation Loss:   2.746231 => Txt Tokens per Sec:     2202 || Lr: 0.000100
2024-02-06 02:39:32,982 Epoch 112: Total Training Recognition Loss 0.75  Total Training Translation Loss 63.81 
2024-02-06 02:39:32,982 EPOCH 113
2024-02-06 02:39:44,104 Epoch 113: Total Training Recognition Loss 0.78  Total Training Translation Loss 61.61 
2024-02-06 02:39:44,104 EPOCH 114
2024-02-06 02:39:55,722 Epoch 114: Total Training Recognition Loss 0.81  Total Training Translation Loss 59.91 
2024-02-06 02:39:55,723 EPOCH 115
2024-02-06 02:40:07,432 Epoch 115: Total Training Recognition Loss 0.72  Total Training Translation Loss 57.33 
2024-02-06 02:40:07,433 EPOCH 116
2024-02-06 02:40:18,875 Epoch 116: Total Training Recognition Loss 0.68  Total Training Translation Loss 56.47 
2024-02-06 02:40:18,876 EPOCH 117
2024-02-06 02:40:30,072 Epoch 117: Total Training Recognition Loss 0.66  Total Training Translation Loss 53.08 
2024-02-06 02:40:30,073 EPOCH 118
2024-02-06 02:40:38,344 [Epoch: 118 Step: 00002000] Batch Recognition Loss:   0.024295 => Gls Tokens per Sec:      820 || Batch Translation Loss:   3.424243 => Txt Tokens per Sec:     2256 || Lr: 0.000100
2024-02-06 02:41:49,984 Hooray! New best validation result [eval_metric]!
2024-02-06 02:41:49,986 Saving new checkpoint.
2024-02-06 02:41:50,280 Validation result at epoch 118, step     2000: duration: 71.9356s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.95281	Translation Loss: 72664.01562	PPL: 1418.97827
	Eval Metric: BLEU
	WER 7.77	(DEL: 0.00,	INS: 0.00,	SUB: 7.77)
	BLEU-4 0.83	(BLEU-1: 12.21,	BLEU-2: 4.29,	BLEU-3: 1.76,	BLEU-4: 0.83)
	CHRF 17.22	ROUGE 10.22
2024-02-06 02:41:50,282 Logging Recognition and Translation Outputs
2024-02-06 02:41:50,282 ========================================================================================================================
2024-02-06 02:41:50,282 Logging Sequence: 165_414.00
2024-02-06 02:41:50,283 	Gloss Reference :	A B+C+D+E
2024-02-06 02:41:50,283 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 02:41:50,283 	Gloss Alignment :	         
2024-02-06 02:41:50,283 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 02:41:50,285 	Text Reference  :	he felt   sachin  was    lucky so he            always  gave his sweater   to   give it   to the      umpire
2024-02-06 02:41:50,285 	Text Hypothesis :	** bowler jasprit bumrah gave  an extraordinary batsman who  is  currently luck in   such a  talented luck  
2024-02-06 02:41:50,285 	Text Alignment  :	D  S      S       S      S     S  S             S       S    S   S         S    S    S    S  S        S     
2024-02-06 02:41:50,285 ========================================================================================================================
2024-02-06 02:41:50,285 Logging Sequence: 169_268.00
2024-02-06 02:41:50,286 	Gloss Reference :	A B+C+D+E
2024-02-06 02:41:50,286 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 02:41:50,286 	Gloss Alignment :	         
2024-02-06 02:41:50,286 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 02:41:50,287 	Text Reference  :	shami supports arshdeep and  many fans    supported him   as  well      
2024-02-06 02:41:50,287 	Text Hypothesis :	***** a        panel    will be   created to        solve the miscreants
2024-02-06 02:41:50,287 	Text Alignment  :	D     S        S        S    S    S       S         S     S   S         
2024-02-06 02:41:50,287 ========================================================================================================================
2024-02-06 02:41:50,287 Logging Sequence: 172_15.00
2024-02-06 02:41:50,288 	Gloss Reference :	A B+C+D+E
2024-02-06 02:41:50,288 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 02:41:50,288 	Gloss Alignment :	         
2024-02-06 02:41:50,288 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 02:41:50,290 	Text Reference  :	now in the final match on       28 may 2023  the   two teams        were  up against each   other at    the same venue
2024-02-06 02:41:50,290 	Text Hypothesis :	*** ** *** ***** ***** whenever it is  being known as  commonwealth games as they    remove the   world cup as   well 
2024-02-06 02:41:50,290 	Text Alignment  :	D   D  D   D     D     S        S  S   S     S     S   S            S     S  S       S      S     S     S   S    S    
2024-02-06 02:41:50,290 ========================================================================================================================
2024-02-06 02:41:50,291 Logging Sequence: 96_158.00
2024-02-06 02:41:50,291 	Gloss Reference :	A B+C+D+E
2024-02-06 02:41:50,291 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 02:41:50,291 	Gloss Alignment :	         
2024-02-06 02:41:50,291 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 02:41:50,292 	Text Reference  :	****** **** ******* after this pandya fell on   his knees in   disappointment
2024-02-06 02:41:50,292 	Text Hypothesis :	people were shocked to    see  the    next ball for the   next wicket        
2024-02-06 02:41:50,292 	Text Alignment  :	I      I    I       S     S    S      S    S    S   S     S    S             
2024-02-06 02:41:50,292 ========================================================================================================================
2024-02-06 02:41:50,292 Logging Sequence: 152_73.00
2024-02-06 02:41:50,293 	Gloss Reference :	A B+C+D+E
2024-02-06 02:41:50,293 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 02:41:50,293 	Gloss Alignment :	         
2024-02-06 02:41:50,293 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 02:41:50,293 	Text Reference  :	eventually he    too  got      out by    shaheen afridi   
2024-02-06 02:41:50,293 	Text Hypothesis :	********** virat will continue in  india for     captaincy
2024-02-06 02:41:50,294 	Text Alignment  :	D          S     S    S        S   S     S       S        
2024-02-06 02:41:50,294 ========================================================================================================================
2024-02-06 02:41:53,525 Epoch 118: Total Training Recognition Loss 0.66  Total Training Translation Loss 52.15 
2024-02-06 02:41:53,526 EPOCH 119
2024-02-06 02:42:04,389 Epoch 119: Total Training Recognition Loss 0.65  Total Training Translation Loss 50.32 
2024-02-06 02:42:04,389 EPOCH 120
2024-02-06 02:42:15,171 Epoch 120: Total Training Recognition Loss 0.63  Total Training Translation Loss 49.05 
2024-02-06 02:42:15,172 EPOCH 121
2024-02-06 02:42:25,865 Epoch 121: Total Training Recognition Loss 0.68  Total Training Translation Loss 47.18 
2024-02-06 02:42:25,866 EPOCH 122
2024-02-06 02:42:36,725 Epoch 122: Total Training Recognition Loss 0.63  Total Training Translation Loss 46.44 
2024-02-06 02:42:36,726 EPOCH 123
2024-02-06 02:42:47,421 Epoch 123: Total Training Recognition Loss 0.61  Total Training Translation Loss 45.83 
2024-02-06 02:42:47,422 EPOCH 124
2024-02-06 02:42:53,000 [Epoch: 124 Step: 00002100] Batch Recognition Loss:   0.032356 => Gls Tokens per Sec:      986 || Batch Translation Loss:   3.405052 => Txt Tokens per Sec:     2608 || Lr: 0.000100
2024-02-06 02:42:58,258 Epoch 124: Total Training Recognition Loss 0.61  Total Training Translation Loss 44.21 
2024-02-06 02:42:58,258 EPOCH 125
2024-02-06 02:43:09,050 Epoch 125: Total Training Recognition Loss 0.58  Total Training Translation Loss 41.62 
2024-02-06 02:43:09,050 EPOCH 126
2024-02-06 02:43:20,075 Epoch 126: Total Training Recognition Loss 0.61  Total Training Translation Loss 38.92 
2024-02-06 02:43:20,075 EPOCH 127
2024-02-06 02:43:31,052 Epoch 127: Total Training Recognition Loss 0.57  Total Training Translation Loss 37.75 
2024-02-06 02:43:31,052 EPOCH 128
2024-02-06 02:43:42,111 Epoch 128: Total Training Recognition Loss 0.55  Total Training Translation Loss 36.98 
2024-02-06 02:43:42,111 EPOCH 129
2024-02-06 02:43:52,844 Epoch 129: Total Training Recognition Loss 0.59  Total Training Translation Loss 35.32 
2024-02-06 02:43:52,845 EPOCH 130
2024-02-06 02:43:56,029 [Epoch: 130 Step: 00002200] Batch Recognition Loss:   0.018128 => Gls Tokens per Sec:     1407 || Batch Translation Loss:   1.823715 => Txt Tokens per Sec:     3735 || Lr: 0.000100
2024-02-06 02:44:03,537 Epoch 130: Total Training Recognition Loss 0.54  Total Training Translation Loss 34.94 
2024-02-06 02:44:03,538 EPOCH 131
2024-02-06 02:44:14,068 Epoch 131: Total Training Recognition Loss 0.52  Total Training Translation Loss 34.19 
2024-02-06 02:44:14,069 EPOCH 132
2024-02-06 02:44:24,699 Epoch 132: Total Training Recognition Loss 0.53  Total Training Translation Loss 34.60 
2024-02-06 02:44:24,700 EPOCH 133
2024-02-06 02:44:35,520 Epoch 133: Total Training Recognition Loss 0.52  Total Training Translation Loss 34.78 
2024-02-06 02:44:35,520 EPOCH 134
2024-02-06 02:44:46,267 Epoch 134: Total Training Recognition Loss 0.51  Total Training Translation Loss 34.97 
2024-02-06 02:44:46,267 EPOCH 135
2024-02-06 02:44:56,526 Epoch 135: Total Training Recognition Loss 0.50  Total Training Translation Loss 32.16 
2024-02-06 02:44:56,526 EPOCH 136
2024-02-06 02:44:57,386 [Epoch: 136 Step: 00002300] Batch Recognition Loss:   0.019672 => Gls Tokens per Sec:     3728 || Batch Translation Loss:   2.161949 => Txt Tokens per Sec:     8428 || Lr: 0.000100
2024-02-06 02:45:07,384 Epoch 136: Total Training Recognition Loss 0.49  Total Training Translation Loss 30.99 
2024-02-06 02:45:07,385 EPOCH 137
2024-02-06 02:45:18,032 Epoch 137: Total Training Recognition Loss 0.52  Total Training Translation Loss 29.06 
2024-02-06 02:45:18,033 EPOCH 138
2024-02-06 02:45:28,937 Epoch 138: Total Training Recognition Loss 0.51  Total Training Translation Loss 28.01 
2024-02-06 02:45:28,938 EPOCH 139
2024-02-06 02:45:39,755 Epoch 139: Total Training Recognition Loss 0.47  Total Training Translation Loss 26.86 
2024-02-06 02:45:39,756 EPOCH 140
2024-02-06 02:45:50,332 Epoch 140: Total Training Recognition Loss 0.43  Total Training Translation Loss 25.81 
2024-02-06 02:45:50,333 EPOCH 141
2024-02-06 02:46:00,860 Epoch 141: Total Training Recognition Loss 0.45  Total Training Translation Loss 25.56 
2024-02-06 02:46:00,860 EPOCH 142
2024-02-06 02:46:01,457 [Epoch: 142 Step: 00002400] Batch Recognition Loss:   0.020266 => Gls Tokens per Sec:     3225 || Batch Translation Loss:   1.617841 => Txt Tokens per Sec:     8838 || Lr: 0.000100
2024-02-06 02:46:11,628 Epoch 142: Total Training Recognition Loss 0.46  Total Training Translation Loss 24.86 
2024-02-06 02:46:11,629 EPOCH 143
2024-02-06 02:46:22,502 Epoch 143: Total Training Recognition Loss 0.41  Total Training Translation Loss 23.75 
2024-02-06 02:46:22,503 EPOCH 144
2024-02-06 02:46:33,445 Epoch 144: Total Training Recognition Loss 0.43  Total Training Translation Loss 23.00 
2024-02-06 02:46:33,445 EPOCH 145
2024-02-06 02:46:44,285 Epoch 145: Total Training Recognition Loss 0.43  Total Training Translation Loss 22.72 
2024-02-06 02:46:44,286 EPOCH 146
2024-02-06 02:46:54,923 Epoch 146: Total Training Recognition Loss 0.42  Total Training Translation Loss 22.14 
2024-02-06 02:46:54,924 EPOCH 147
2024-02-06 02:47:05,841 Epoch 147: Total Training Recognition Loss 0.41  Total Training Translation Loss 22.62 
2024-02-06 02:47:05,841 EPOCH 148
2024-02-06 02:47:06,071 [Epoch: 148 Step: 00002500] Batch Recognition Loss:   0.020597 => Gls Tokens per Sec:     2807 || Batch Translation Loss:   1.320155 => Txt Tokens per Sec:     7965 || Lr: 0.000100
2024-02-06 02:47:16,433 Epoch 148: Total Training Recognition Loss 0.38  Total Training Translation Loss 22.18 
2024-02-06 02:47:16,434 EPOCH 149
2024-02-06 02:47:27,332 Epoch 149: Total Training Recognition Loss 0.39  Total Training Translation Loss 20.82 
2024-02-06 02:47:27,332 EPOCH 150
2024-02-06 02:47:38,244 Epoch 150: Total Training Recognition Loss 0.37  Total Training Translation Loss 20.46 
2024-02-06 02:47:38,244 EPOCH 151
2024-02-06 02:47:49,070 Epoch 151: Total Training Recognition Loss 0.36  Total Training Translation Loss 19.74 
2024-02-06 02:47:49,071 EPOCH 152
2024-02-06 02:47:59,817 Epoch 152: Total Training Recognition Loss 0.36  Total Training Translation Loss 19.38 
2024-02-06 02:47:59,818 EPOCH 153
2024-02-06 02:48:10,435 [Epoch: 153 Step: 00002600] Batch Recognition Loss:   0.015170 => Gls Tokens per Sec:      940 || Batch Translation Loss:   1.102311 => Txt Tokens per Sec:     2590 || Lr: 0.000100
2024-02-06 02:48:10,685 Epoch 153: Total Training Recognition Loss 0.36  Total Training Translation Loss 18.57 
2024-02-06 02:48:10,685 EPOCH 154
2024-02-06 02:48:21,527 Epoch 154: Total Training Recognition Loss 0.31  Total Training Translation Loss 18.12 
2024-02-06 02:48:21,528 EPOCH 155
2024-02-06 02:48:32,388 Epoch 155: Total Training Recognition Loss 0.35  Total Training Translation Loss 17.33 
2024-02-06 02:48:32,389 EPOCH 156
2024-02-06 02:48:43,295 Epoch 156: Total Training Recognition Loss 0.32  Total Training Translation Loss 17.14 
2024-02-06 02:48:43,295 EPOCH 157
2024-02-06 02:48:53,980 Epoch 157: Total Training Recognition Loss 0.37  Total Training Translation Loss 17.12 
2024-02-06 02:48:53,980 EPOCH 158
2024-02-06 02:49:04,765 Epoch 158: Total Training Recognition Loss 0.33  Total Training Translation Loss 16.58 
2024-02-06 02:49:04,766 EPOCH 159
2024-02-06 02:49:13,206 [Epoch: 159 Step: 00002700] Batch Recognition Loss:   0.017439 => Gls Tokens per Sec:     1031 || Batch Translation Loss:   1.289833 => Txt Tokens per Sec:     2818 || Lr: 0.000100
2024-02-06 02:49:15,440 Epoch 159: Total Training Recognition Loss 0.33  Total Training Translation Loss 16.14 
2024-02-06 02:49:15,440 EPOCH 160
2024-02-06 02:49:26,180 Epoch 160: Total Training Recognition Loss 0.37  Total Training Translation Loss 15.79 
2024-02-06 02:49:26,180 EPOCH 161
2024-02-06 02:49:36,907 Epoch 161: Total Training Recognition Loss 0.33  Total Training Translation Loss 16.04 
2024-02-06 02:49:36,907 EPOCH 162
2024-02-06 02:49:47,800 Epoch 162: Total Training Recognition Loss 0.31  Total Training Translation Loss 16.27 
2024-02-06 02:49:47,801 EPOCH 163
2024-02-06 02:49:58,455 Epoch 163: Total Training Recognition Loss 0.31  Total Training Translation Loss 15.38 
2024-02-06 02:49:58,456 EPOCH 164
2024-02-06 02:50:09,136 Epoch 164: Total Training Recognition Loss 0.29  Total Training Translation Loss 15.35 
2024-02-06 02:50:09,136 EPOCH 165
2024-02-06 02:50:17,141 [Epoch: 165 Step: 00002800] Batch Recognition Loss:   0.013821 => Gls Tokens per Sec:      927 || Batch Translation Loss:   0.946653 => Txt Tokens per Sec:     2558 || Lr: 0.000100
2024-02-06 02:50:20,240 Epoch 165: Total Training Recognition Loss 0.30  Total Training Translation Loss 14.70 
2024-02-06 02:50:20,241 EPOCH 166
2024-02-06 02:50:31,326 Epoch 166: Total Training Recognition Loss 0.31  Total Training Translation Loss 14.38 
2024-02-06 02:50:31,327 EPOCH 167
2024-02-06 02:50:42,350 Epoch 167: Total Training Recognition Loss 0.28  Total Training Translation Loss 13.63 
2024-02-06 02:50:42,350 EPOCH 168
2024-02-06 02:50:52,829 Epoch 168: Total Training Recognition Loss 0.28  Total Training Translation Loss 13.59 
2024-02-06 02:50:52,830 EPOCH 169
2024-02-06 02:51:03,708 Epoch 169: Total Training Recognition Loss 0.31  Total Training Translation Loss 13.11 
2024-02-06 02:51:03,709 EPOCH 170
2024-02-06 02:51:14,512 Epoch 170: Total Training Recognition Loss 0.27  Total Training Translation Loss 12.83 
2024-02-06 02:51:14,513 EPOCH 171
2024-02-06 02:51:18,088 [Epoch: 171 Step: 00002900] Batch Recognition Loss:   0.018920 => Gls Tokens per Sec:     1791 || Batch Translation Loss:   0.470193 => Txt Tokens per Sec:     4690 || Lr: 0.000100
2024-02-06 02:51:25,031 Epoch 171: Total Training Recognition Loss 0.29  Total Training Translation Loss 13.06 
2024-02-06 02:51:25,031 EPOCH 172
2024-02-06 02:51:35,286 Epoch 172: Total Training Recognition Loss 0.32  Total Training Translation Loss 12.98 
2024-02-06 02:51:35,287 EPOCH 173
2024-02-06 02:51:46,504 Epoch 173: Total Training Recognition Loss 0.31  Total Training Translation Loss 12.95 
2024-02-06 02:51:46,504 EPOCH 174
2024-02-06 02:51:57,086 Epoch 174: Total Training Recognition Loss 0.26  Total Training Translation Loss 11.90 
2024-02-06 02:51:57,087 EPOCH 175
2024-02-06 02:52:07,855 Epoch 175: Total Training Recognition Loss 0.25  Total Training Translation Loss 11.45 
2024-02-06 02:52:07,856 EPOCH 176
2024-02-06 02:52:18,380 Epoch 176: Total Training Recognition Loss 0.26  Total Training Translation Loss 11.07 
2024-02-06 02:52:18,381 EPOCH 177
2024-02-06 02:52:27,353 [Epoch: 177 Step: 00003000] Batch Recognition Loss:   0.014843 => Gls Tokens per Sec:      542 || Batch Translation Loss:   0.803281 => Txt Tokens per Sec:     1734 || Lr: 0.000100
2024-02-06 02:52:29,036 Epoch 177: Total Training Recognition Loss 0.23  Total Training Translation Loss 11.10 
2024-02-06 02:52:29,036 EPOCH 178
2024-02-06 02:52:39,665 Epoch 178: Total Training Recognition Loss 0.21  Total Training Translation Loss 11.07 
2024-02-06 02:52:39,665 EPOCH 179
2024-02-06 02:52:50,450 Epoch 179: Total Training Recognition Loss 0.25  Total Training Translation Loss 10.36 
2024-02-06 02:52:50,450 EPOCH 180
2024-02-06 02:53:01,330 Epoch 180: Total Training Recognition Loss 0.26  Total Training Translation Loss 10.22 
2024-02-06 02:53:01,330 EPOCH 181
2024-02-06 02:53:12,381 Epoch 181: Total Training Recognition Loss 0.22  Total Training Translation Loss 10.45 
2024-02-06 02:53:12,381 EPOCH 182
2024-02-06 02:53:23,106 Epoch 182: Total Training Recognition Loss 0.21  Total Training Translation Loss 10.27 
2024-02-06 02:53:23,106 EPOCH 183
2024-02-06 02:53:25,998 [Epoch: 183 Step: 00003100] Batch Recognition Loss:   0.008844 => Gls Tokens per Sec:     1329 || Batch Translation Loss:   0.693063 => Txt Tokens per Sec:     3542 || Lr: 0.000100
2024-02-06 02:53:33,762 Epoch 183: Total Training Recognition Loss 0.19  Total Training Translation Loss 10.06 
2024-02-06 02:53:33,762 EPOCH 184
2024-02-06 02:53:44,447 Epoch 184: Total Training Recognition Loss 0.21  Total Training Translation Loss 9.66 
2024-02-06 02:53:44,448 EPOCH 185
2024-02-06 02:53:55,262 Epoch 185: Total Training Recognition Loss 0.20  Total Training Translation Loss 10.04 
2024-02-06 02:53:55,262 EPOCH 186
2024-02-06 02:54:06,167 Epoch 186: Total Training Recognition Loss 0.20  Total Training Translation Loss 10.24 
2024-02-06 02:54:06,169 EPOCH 187
2024-02-06 02:54:16,998 Epoch 187: Total Training Recognition Loss 0.21  Total Training Translation Loss 10.18 
2024-02-06 02:54:16,998 EPOCH 188
2024-02-06 02:54:27,765 Epoch 188: Total Training Recognition Loss 0.19  Total Training Translation Loss 9.75 
2024-02-06 02:54:27,766 EPOCH 189
2024-02-06 02:54:30,854 [Epoch: 189 Step: 00003200] Batch Recognition Loss:   0.007159 => Gls Tokens per Sec:      745 || Batch Translation Loss:   0.604762 => Txt Tokens per Sec:     2027 || Lr: 0.000100
2024-02-06 02:54:38,600 Epoch 189: Total Training Recognition Loss 0.18  Total Training Translation Loss 9.60 
2024-02-06 02:54:38,601 EPOCH 190
2024-02-06 02:54:49,566 Epoch 190: Total Training Recognition Loss 0.22  Total Training Translation Loss 9.39 
2024-02-06 02:54:49,567 EPOCH 191
2024-02-06 02:55:00,394 Epoch 191: Total Training Recognition Loss 0.18  Total Training Translation Loss 9.31 
2024-02-06 02:55:00,395 EPOCH 192
2024-02-06 02:55:11,249 Epoch 192: Total Training Recognition Loss 0.19  Total Training Translation Loss 9.30 
2024-02-06 02:55:11,250 EPOCH 193
2024-02-06 02:55:22,207 Epoch 193: Total Training Recognition Loss 0.20  Total Training Translation Loss 9.66 
2024-02-06 02:55:22,207 EPOCH 194
2024-02-06 02:55:32,812 Epoch 194: Total Training Recognition Loss 0.20  Total Training Translation Loss 9.28 
2024-02-06 02:55:32,813 EPOCH 195
2024-02-06 02:55:34,659 [Epoch: 195 Step: 00003300] Batch Recognition Loss:   0.010328 => Gls Tokens per Sec:      694 || Batch Translation Loss:   0.504697 => Txt Tokens per Sec:     2143 || Lr: 0.000100
2024-02-06 02:55:43,618 Epoch 195: Total Training Recognition Loss 0.19  Total Training Translation Loss 9.56 
2024-02-06 02:55:43,618 EPOCH 196
2024-02-06 02:55:54,287 Epoch 196: Total Training Recognition Loss 0.20  Total Training Translation Loss 9.12 
2024-02-06 02:55:54,287 EPOCH 197
2024-02-06 02:56:05,125 Epoch 197: Total Training Recognition Loss 0.20  Total Training Translation Loss 8.75 
2024-02-06 02:56:05,126 EPOCH 198
2024-02-06 02:56:15,648 Epoch 198: Total Training Recognition Loss 0.23  Total Training Translation Loss 8.67 
2024-02-06 02:56:15,649 EPOCH 199
2024-02-06 02:56:26,307 Epoch 199: Total Training Recognition Loss 0.19  Total Training Translation Loss 8.37 
2024-02-06 02:56:26,308 EPOCH 200
2024-02-06 02:56:37,080 [Epoch: 200 Step: 00003400] Batch Recognition Loss:   0.006579 => Gls Tokens per Sec:      986 || Batch Translation Loss:   0.479568 => Txt Tokens per Sec:     2728 || Lr: 0.000100
2024-02-06 02:56:37,080 Epoch 200: Total Training Recognition Loss 0.15  Total Training Translation Loss 8.61 
2024-02-06 02:56:37,081 EPOCH 201
2024-02-06 02:56:48,111 Epoch 201: Total Training Recognition Loss 0.20  Total Training Translation Loss 8.50 
2024-02-06 02:56:48,112 EPOCH 202
2024-02-06 02:56:58,691 Epoch 202: Total Training Recognition Loss 0.18  Total Training Translation Loss 8.83 
2024-02-06 02:56:58,692 EPOCH 203
2024-02-06 02:57:09,362 Epoch 203: Total Training Recognition Loss 0.18  Total Training Translation Loss 8.15 
2024-02-06 02:57:09,363 EPOCH 204
2024-02-06 02:57:20,168 Epoch 204: Total Training Recognition Loss 0.17  Total Training Translation Loss 7.86 
2024-02-06 02:57:20,169 EPOCH 205
2024-02-06 02:57:30,921 Epoch 205: Total Training Recognition Loss 0.18  Total Training Translation Loss 7.59 
2024-02-06 02:57:30,921 EPOCH 206
2024-02-06 02:57:41,286 [Epoch: 206 Step: 00003500] Batch Recognition Loss:   0.011088 => Gls Tokens per Sec:      901 || Batch Translation Loss:   0.548032 => Txt Tokens per Sec:     2502 || Lr: 0.000100
2024-02-06 02:57:41,763 Epoch 206: Total Training Recognition Loss 0.22  Total Training Translation Loss 7.35 
2024-02-06 02:57:41,763 EPOCH 207
2024-02-06 02:57:52,449 Epoch 207: Total Training Recognition Loss 0.20  Total Training Translation Loss 7.23 
2024-02-06 02:57:52,449 EPOCH 208
2024-02-06 02:58:02,987 Epoch 208: Total Training Recognition Loss 0.16  Total Training Translation Loss 6.71 
2024-02-06 02:58:02,988 EPOCH 209
2024-02-06 02:58:13,756 Epoch 209: Total Training Recognition Loss 0.15  Total Training Translation Loss 6.54 
2024-02-06 02:58:13,757 EPOCH 210
2024-02-06 02:58:24,466 Epoch 210: Total Training Recognition Loss 0.17  Total Training Translation Loss 6.69 
2024-02-06 02:58:24,466 EPOCH 211
2024-02-06 02:58:35,000 Epoch 211: Total Training Recognition Loss 0.17  Total Training Translation Loss 6.71 
2024-02-06 02:58:35,001 EPOCH 212
2024-02-06 02:58:44,962 [Epoch: 212 Step: 00003600] Batch Recognition Loss:   0.020047 => Gls Tokens per Sec:      809 || Batch Translation Loss:   0.539104 => Txt Tokens per Sec:     2251 || Lr: 0.000100
2024-02-06 02:58:45,920 Epoch 212: Total Training Recognition Loss 0.18  Total Training Translation Loss 6.53 
2024-02-06 02:58:45,920 EPOCH 213
2024-02-06 02:58:56,598 Epoch 213: Total Training Recognition Loss 0.16  Total Training Translation Loss 6.15 
2024-02-06 02:58:56,599 EPOCH 214
2024-02-06 02:59:07,299 Epoch 214: Total Training Recognition Loss 0.13  Total Training Translation Loss 6.25 
2024-02-06 02:59:07,299 EPOCH 215
2024-02-06 02:59:17,966 Epoch 215: Total Training Recognition Loss 0.15  Total Training Translation Loss 5.84 
2024-02-06 02:59:17,967 EPOCH 216
2024-02-06 02:59:28,780 Epoch 216: Total Training Recognition Loss 0.14  Total Training Translation Loss 5.77 
2024-02-06 02:59:28,781 EPOCH 217
2024-02-06 02:59:39,448 Epoch 217: Total Training Recognition Loss 0.14  Total Training Translation Loss 5.82 
2024-02-06 02:59:39,449 EPOCH 218
2024-02-06 02:59:46,770 [Epoch: 218 Step: 00003700] Batch Recognition Loss:   0.013974 => Gls Tokens per Sec:      926 || Batch Translation Loss:   0.260702 => Txt Tokens per Sec:     2448 || Lr: 0.000100
2024-02-06 02:59:50,082 Epoch 218: Total Training Recognition Loss 0.13  Total Training Translation Loss 5.70 
2024-02-06 02:59:50,082 EPOCH 219
2024-02-06 03:00:00,914 Epoch 219: Total Training Recognition Loss 0.16  Total Training Translation Loss 5.81 
2024-02-06 03:00:00,914 EPOCH 220
2024-02-06 03:00:11,746 Epoch 220: Total Training Recognition Loss 0.13  Total Training Translation Loss 5.47 
2024-02-06 03:00:11,746 EPOCH 221
2024-02-06 03:00:22,519 Epoch 221: Total Training Recognition Loss 0.13  Total Training Translation Loss 5.41 
2024-02-06 03:00:22,519 EPOCH 222
2024-02-06 03:00:33,316 Epoch 222: Total Training Recognition Loss 0.13  Total Training Translation Loss 5.54 
2024-02-06 03:00:33,317 EPOCH 223
2024-02-06 03:00:44,309 Epoch 223: Total Training Recognition Loss 0.16  Total Training Translation Loss 6.01 
2024-02-06 03:00:44,309 EPOCH 224
2024-02-06 03:00:52,023 [Epoch: 224 Step: 00003800] Batch Recognition Loss:   0.012053 => Gls Tokens per Sec:      713 || Batch Translation Loss:   0.445773 => Txt Tokens per Sec:     2082 || Lr: 0.000100
2024-02-06 03:00:55,256 Epoch 224: Total Training Recognition Loss 0.12  Total Training Translation Loss 5.92 
2024-02-06 03:00:55,257 EPOCH 225
2024-02-06 03:01:05,871 Epoch 225: Total Training Recognition Loss 0.15  Total Training Translation Loss 6.24 
2024-02-06 03:01:05,872 EPOCH 226
2024-02-06 03:01:16,591 Epoch 226: Total Training Recognition Loss 0.13  Total Training Translation Loss 6.52 
2024-02-06 03:01:16,592 EPOCH 227
2024-02-06 03:01:27,120 Epoch 227: Total Training Recognition Loss 0.16  Total Training Translation Loss 7.82 
2024-02-06 03:01:27,120 EPOCH 228
2024-02-06 03:01:37,837 Epoch 228: Total Training Recognition Loss 0.14  Total Training Translation Loss 8.22 
2024-02-06 03:01:37,837 EPOCH 229
2024-02-06 03:01:48,654 Epoch 229: Total Training Recognition Loss 0.13  Total Training Translation Loss 7.34 
2024-02-06 03:01:48,655 EPOCH 230
2024-02-06 03:01:53,035 [Epoch: 230 Step: 00003900] Batch Recognition Loss:   0.003539 => Gls Tokens per Sec:     1023 || Batch Translation Loss:   0.364005 => Txt Tokens per Sec:     2791 || Lr: 0.000100
2024-02-06 03:01:59,436 Epoch 230: Total Training Recognition Loss 0.14  Total Training Translation Loss 6.60 
2024-02-06 03:01:59,436 EPOCH 231
2024-02-06 03:02:10,369 Epoch 231: Total Training Recognition Loss 0.13  Total Training Translation Loss 6.33 
2024-02-06 03:02:10,370 EPOCH 232
2024-02-06 03:02:21,175 Epoch 232: Total Training Recognition Loss 0.14  Total Training Translation Loss 6.35 
2024-02-06 03:02:21,175 EPOCH 233
2024-02-06 03:02:31,719 Epoch 233: Total Training Recognition Loss 0.15  Total Training Translation Loss 6.09 
2024-02-06 03:02:31,720 EPOCH 234
2024-02-06 03:02:42,312 Epoch 234: Total Training Recognition Loss 0.15  Total Training Translation Loss 5.55 
2024-02-06 03:02:42,313 EPOCH 235
2024-02-06 03:02:53,071 Epoch 235: Total Training Recognition Loss 0.13  Total Training Translation Loss 5.23 
2024-02-06 03:02:53,071 EPOCH 236
2024-02-06 03:02:57,794 [Epoch: 236 Step: 00004000] Batch Recognition Loss:   0.008531 => Gls Tokens per Sec:      623 || Batch Translation Loss:   0.318206 => Txt Tokens per Sec:     1738 || Lr: 0.000100
2024-02-06 03:03:38,551 Validation result at epoch 236, step     4000: duration: 40.7543s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.03647	Translation Loss: 81619.95312	PPL: 3471.04932
	Eval Metric: BLEU
	WER 5.44	(DEL: 0.00,	INS: 0.00,	SUB: 5.44)
	BLEU-4 0.78	(BLEU-1: 12.32,	BLEU-2: 4.14,	BLEU-3: 1.64,	BLEU-4: 0.78)
	CHRF 17.50	ROUGE 10.29
2024-02-06 03:03:38,553 Logging Recognition and Translation Outputs
2024-02-06 03:03:38,553 ========================================================================================================================
2024-02-06 03:03:38,553 Logging Sequence: 112_165.00
2024-02-06 03:03:38,553 	Gloss Reference :	A B+C+D+E
2024-02-06 03:03:38,556 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 03:03:38,556 	Gloss Alignment :	         
2024-02-06 03:03:38,556 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 03:03:38,558 	Text Reference  :	** *** **** **** the       narendra modi        stadium will   be   the   home   for the    ahmedabad-based franchise
2024-02-06 03:03:38,558 	Text Hypothesis :	do you know that wikipedia provides information on      celebs like their height age family background      etc      
2024-02-06 03:03:38,558 	Text Alignment  :	I  I   I    I    S         S        S           S       S      S    S     S      S   S      S               S        
2024-02-06 03:03:38,558 ========================================================================================================================
2024-02-06 03:03:38,558 Logging Sequence: 176_154.00
2024-02-06 03:03:38,558 	Gloss Reference :	A B+C+D+E
2024-02-06 03:03:38,559 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 03:03:38,559 	Gloss Alignment :	         
2024-02-06 03:03:38,559 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 03:03:38,560 	Text Reference  :	******* ****** dahiya could potentially bring home india's second gold   medal
2024-02-06 03:03:38,560 	Text Hypothesis :	abhinav bindra was    the   first       time  to   win     a      silver medal
2024-02-06 03:03:38,560 	Text Alignment  :	I       I      S      S     S           S     S    S       S      S           
2024-02-06 03:03:38,560 ========================================================================================================================
2024-02-06 03:03:38,560 Logging Sequence: 94_2.00
2024-02-06 03:03:38,560 	Gloss Reference :	A B+C+D+E
2024-02-06 03:03:38,561 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 03:03:38,561 	Gloss Alignment :	         
2024-02-06 03:03:38,561 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 03:03:38,563 	Text Reference  :	****** ******* **** ******* the   icc   odi    men' world cup 2023 will be hosted by      india on 5th october 2023    
2024-02-06 03:03:38,563 	Text Hypothesis :	indian cricket team chennai super kings during the  world cup **** **** ** match  between india ** *** and     pakistan
2024-02-06 03:03:38,563 	Text Alignment  :	I      I       I    I       S     S     S      S              D    D    D  S      S             D  D   S       S       
2024-02-06 03:03:38,564 ========================================================================================================================
2024-02-06 03:03:38,564 Logging Sequence: 165_453.00
2024-02-06 03:03:38,564 	Gloss Reference :	A B+C+D+E
2024-02-06 03:03:38,564 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 03:03:38,564 	Gloss Alignment :	         
2024-02-06 03:03:38,565 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 03:03:38,566 	Text Reference  :	*** **** ******* icc  did not     agree to **** ** *** ******** sehwag' decision of  wearing a  numberless jersey
2024-02-06 03:03:38,566 	Text Hypothesis :	his team members have a   strange due   to this is his accident and     did      not want    it is         more  
2024-02-06 03:03:38,566 	Text Alignment  :	I   I    I       S    S   S       S        I    I  I   I        S       S        S   S       S  S          S     
2024-02-06 03:03:38,566 ========================================================================================================================
2024-02-06 03:03:38,566 Logging Sequence: 139_46.00
2024-02-06 03:03:38,567 	Gloss Reference :	A B+C+D+E
2024-02-06 03:03:38,567 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 03:03:38,567 	Gloss Alignment :	         
2024-02-06 03:03:38,567 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 03:03:38,569 	Text Reference  :	everyone thought it would be a       one sided  match because morocco is an amateur team  and ****** belgium ranks 2nd in the world
2024-02-06 03:03:38,569 	Text Hypothesis :	******** ******* ** ***** ** however the second match ******* ******* ** ** between india and kuwait at      the   end of the qatar
2024-02-06 03:03:38,569 	Text Alignment  :	D        D       D  D     D  S       S   S            D       D       D  D  S       S         I      S       S     S   S      S    
2024-02-06 03:03:38,569 ========================================================================================================================
2024-02-06 03:03:45,045 Epoch 236: Total Training Recognition Loss 0.11  Total Training Translation Loss 4.76 
2024-02-06 03:03:45,045 EPOCH 237
2024-02-06 03:03:55,775 Epoch 237: Total Training Recognition Loss 0.11  Total Training Translation Loss 5.23 
2024-02-06 03:03:55,775 EPOCH 238
2024-02-06 03:04:06,493 Epoch 238: Total Training Recognition Loss 0.14  Total Training Translation Loss 6.83 
2024-02-06 03:04:06,494 EPOCH 239
2024-02-06 03:04:17,177 Epoch 239: Total Training Recognition Loss 0.13  Total Training Translation Loss 12.22 
2024-02-06 03:04:17,177 EPOCH 240
2024-02-06 03:04:27,889 Epoch 240: Total Training Recognition Loss 0.20  Total Training Translation Loss 12.20 
2024-02-06 03:04:27,890 EPOCH 241
2024-02-06 03:04:38,548 Epoch 241: Total Training Recognition Loss 0.17  Total Training Translation Loss 8.43 
2024-02-06 03:04:38,548 EPOCH 242
2024-02-06 03:04:39,120 [Epoch: 242 Step: 00004100] Batch Recognition Loss:   0.004099 => Gls Tokens per Sec:     3366 || Batch Translation Loss:   0.453553 => Txt Tokens per Sec:     8634 || Lr: 0.000100
2024-02-06 03:04:49,081 Epoch 242: Total Training Recognition Loss 0.15  Total Training Translation Loss 6.72 
2024-02-06 03:04:49,082 EPOCH 243
2024-02-06 03:04:59,822 Epoch 243: Total Training Recognition Loss 0.12  Total Training Translation Loss 5.81 
2024-02-06 03:04:59,823 EPOCH 244
2024-02-06 03:05:10,452 Epoch 244: Total Training Recognition Loss 0.12  Total Training Translation Loss 5.67 
2024-02-06 03:05:10,453 EPOCH 245
2024-02-06 03:05:21,254 Epoch 245: Total Training Recognition Loss 0.14  Total Training Translation Loss 5.26 
2024-02-06 03:05:21,254 EPOCH 246
2024-02-06 03:05:32,165 Epoch 246: Total Training Recognition Loss 0.16  Total Training Translation Loss 4.48 
2024-02-06 03:05:32,166 EPOCH 247
2024-02-06 03:05:43,018 Epoch 247: Total Training Recognition Loss 0.12  Total Training Translation Loss 4.19 
2024-02-06 03:05:43,019 EPOCH 248
2024-02-06 03:05:43,125 [Epoch: 248 Step: 00004200] Batch Recognition Loss:   0.011423 => Gls Tokens per Sec:     6095 || Batch Translation Loss:   0.090866 => Txt Tokens per Sec:     9533 || Lr: 0.000100
2024-02-06 03:05:53,927 Epoch 248: Total Training Recognition Loss 0.13  Total Training Translation Loss 3.88 
2024-02-06 03:05:53,928 EPOCH 249
2024-02-06 03:06:04,778 Epoch 249: Total Training Recognition Loss 0.11  Total Training Translation Loss 3.83 
2024-02-06 03:06:04,778 EPOCH 250
2024-02-06 03:06:15,611 Epoch 250: Total Training Recognition Loss 0.10  Total Training Translation Loss 3.72 
2024-02-06 03:06:15,612 EPOCH 251
2024-02-06 03:06:26,422 Epoch 251: Total Training Recognition Loss 0.11  Total Training Translation Loss 3.78 
2024-02-06 03:06:26,423 EPOCH 252
2024-02-06 03:06:37,285 Epoch 252: Total Training Recognition Loss 0.11  Total Training Translation Loss 3.50 
2024-02-06 03:06:37,286 EPOCH 253
2024-02-06 03:06:46,093 [Epoch: 253 Step: 00004300] Batch Recognition Loss:   0.006292 => Gls Tokens per Sec:     1133 || Batch Translation Loss:   0.191218 => Txt Tokens per Sec:     3089 || Lr: 0.000100
2024-02-06 03:06:47,771 Epoch 253: Total Training Recognition Loss 0.10  Total Training Translation Loss 3.47 
2024-02-06 03:06:47,771 EPOCH 254
2024-02-06 03:06:58,634 Epoch 254: Total Training Recognition Loss 0.10  Total Training Translation Loss 3.55 
2024-02-06 03:06:58,634 EPOCH 255
2024-02-06 03:07:09,207 Epoch 255: Total Training Recognition Loss 0.09  Total Training Translation Loss 3.46 
2024-02-06 03:07:09,208 EPOCH 256
2024-02-06 03:07:19,880 Epoch 256: Total Training Recognition Loss 0.10  Total Training Translation Loss 3.42 
2024-02-06 03:07:19,881 EPOCH 257
2024-02-06 03:07:30,496 Epoch 257: Total Training Recognition Loss 0.11  Total Training Translation Loss 3.43 
2024-02-06 03:07:30,496 EPOCH 258
2024-02-06 03:07:41,158 Epoch 258: Total Training Recognition Loss 0.10  Total Training Translation Loss 3.63 
2024-02-06 03:07:41,159 EPOCH 259
2024-02-06 03:07:47,348 [Epoch: 259 Step: 00004400] Batch Recognition Loss:   0.003307 => Gls Tokens per Sec:     1448 || Batch Translation Loss:   0.149649 => Txt Tokens per Sec:     3914 || Lr: 0.000100
2024-02-06 03:07:52,111 Epoch 259: Total Training Recognition Loss 0.08  Total Training Translation Loss 3.63 
2024-02-06 03:07:52,111 EPOCH 260
2024-02-06 03:08:02,847 Epoch 260: Total Training Recognition Loss 0.10  Total Training Translation Loss 3.43 
2024-02-06 03:08:02,848 EPOCH 261
2024-02-06 03:08:13,647 Epoch 261: Total Training Recognition Loss 0.10  Total Training Translation Loss 3.46 
2024-02-06 03:08:13,648 EPOCH 262
2024-02-06 03:08:24,454 Epoch 262: Total Training Recognition Loss 0.10  Total Training Translation Loss 3.51 
2024-02-06 03:08:24,455 EPOCH 263
2024-02-06 03:08:35,199 Epoch 263: Total Training Recognition Loss 0.11  Total Training Translation Loss 3.62 
2024-02-06 03:08:35,200 EPOCH 264
2024-02-06 03:08:45,975 Epoch 264: Total Training Recognition Loss 0.09  Total Training Translation Loss 3.40 
2024-02-06 03:08:45,975 EPOCH 265
2024-02-06 03:08:52,961 [Epoch: 265 Step: 00004500] Batch Recognition Loss:   0.011530 => Gls Tokens per Sec:     1100 || Batch Translation Loss:   0.091004 => Txt Tokens per Sec:     2940 || Lr: 0.000100
2024-02-06 03:08:56,541 Epoch 265: Total Training Recognition Loss 0.09  Total Training Translation Loss 3.39 
2024-02-06 03:08:56,541 EPOCH 266
2024-02-06 03:09:07,255 Epoch 266: Total Training Recognition Loss 0.08  Total Training Translation Loss 3.47 
2024-02-06 03:09:07,255 EPOCH 267
2024-02-06 03:09:17,937 Epoch 267: Total Training Recognition Loss 0.10  Total Training Translation Loss 3.57 
2024-02-06 03:09:17,938 EPOCH 268
2024-02-06 03:09:28,725 Epoch 268: Total Training Recognition Loss 0.09  Total Training Translation Loss 5.32 
2024-02-06 03:09:28,726 EPOCH 269
2024-02-06 03:09:39,541 Epoch 269: Total Training Recognition Loss 0.21  Total Training Translation Loss 31.65 
2024-02-06 03:09:39,541 EPOCH 270
2024-02-06 03:09:50,349 Epoch 270: Total Training Recognition Loss 0.37  Total Training Translation Loss 26.46 
2024-02-06 03:09:50,350 EPOCH 271
2024-02-06 03:09:56,564 [Epoch: 271 Step: 00004600] Batch Recognition Loss:   0.011001 => Gls Tokens per Sec:      988 || Batch Translation Loss:   0.698493 => Txt Tokens per Sec:     2711 || Lr: 0.000100
2024-02-06 03:10:01,449 Epoch 271: Total Training Recognition Loss 0.27  Total Training Translation Loss 14.49 
2024-02-06 03:10:01,449 EPOCH 272
2024-02-06 03:10:12,345 Epoch 272: Total Training Recognition Loss 0.21  Total Training Translation Loss 8.32 
2024-02-06 03:10:12,345 EPOCH 273
2024-02-06 03:10:22,814 Epoch 273: Total Training Recognition Loss 0.17  Total Training Translation Loss 5.04 
2024-02-06 03:10:22,814 EPOCH 274
2024-02-06 03:10:33,765 Epoch 274: Total Training Recognition Loss 0.16  Total Training Translation Loss 4.30 
2024-02-06 03:10:33,765 EPOCH 275
2024-02-06 03:10:44,508 Epoch 275: Total Training Recognition Loss 0.14  Total Training Translation Loss 3.74 
2024-02-06 03:10:44,509 EPOCH 276
2024-02-06 03:10:55,238 Epoch 276: Total Training Recognition Loss 0.12  Total Training Translation Loss 3.43 
2024-02-06 03:10:55,239 EPOCH 277
2024-02-06 03:10:58,260 [Epoch: 277 Step: 00004700] Batch Recognition Loss:   0.005576 => Gls Tokens per Sec:     1695 || Batch Translation Loss:   0.160332 => Txt Tokens per Sec:     4418 || Lr: 0.000100
2024-02-06 03:11:05,762 Epoch 277: Total Training Recognition Loss 0.13  Total Training Translation Loss 2.97 
2024-02-06 03:11:05,763 EPOCH 278
2024-02-06 03:11:16,465 Epoch 278: Total Training Recognition Loss 0.10  Total Training Translation Loss 2.89 
2024-02-06 03:11:16,465 EPOCH 279
2024-02-06 03:11:27,159 Epoch 279: Total Training Recognition Loss 0.10  Total Training Translation Loss 2.70 
2024-02-06 03:11:27,159 EPOCH 280
2024-02-06 03:11:37,922 Epoch 280: Total Training Recognition Loss 0.09  Total Training Translation Loss 2.67 
2024-02-06 03:11:37,923 EPOCH 281
2024-02-06 03:11:48,663 Epoch 281: Total Training Recognition Loss 0.09  Total Training Translation Loss 2.50 
2024-02-06 03:11:48,664 EPOCH 282
2024-02-06 03:11:59,411 Epoch 282: Total Training Recognition Loss 0.09  Total Training Translation Loss 2.79 
2024-02-06 03:11:59,412 EPOCH 283
2024-02-06 03:12:03,968 [Epoch: 283 Step: 00004800] Batch Recognition Loss:   0.002122 => Gls Tokens per Sec:      843 || Batch Translation Loss:   0.189623 => Txt Tokens per Sec:     2507 || Lr: 0.000100
2024-02-06 03:12:10,007 Epoch 283: Total Training Recognition Loss 0.09  Total Training Translation Loss 2.34 
2024-02-06 03:12:10,007 EPOCH 284
2024-02-06 03:12:20,987 Epoch 284: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.24 
2024-02-06 03:12:20,988 EPOCH 285
2024-02-06 03:12:31,860 Epoch 285: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.16 
2024-02-06 03:12:31,860 EPOCH 286
2024-02-06 03:12:42,697 Epoch 286: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.10 
2024-02-06 03:12:42,697 EPOCH 287
2024-02-06 03:12:53,446 Epoch 287: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.09 
2024-02-06 03:12:53,446 EPOCH 288
2024-02-06 03:13:04,381 Epoch 288: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.95 
2024-02-06 03:13:04,382 EPOCH 289
2024-02-06 03:13:07,269 [Epoch: 289 Step: 00004900] Batch Recognition Loss:   0.001454 => Gls Tokens per Sec:      887 || Batch Translation Loss:   0.118257 => Txt Tokens per Sec:     2657 || Lr: 0.000100
2024-02-06 03:13:15,205 Epoch 289: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.95 
2024-02-06 03:13:15,206 EPOCH 290
2024-02-06 03:13:25,873 Epoch 290: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.00 
2024-02-06 03:13:25,874 EPOCH 291
2024-02-06 03:13:36,477 Epoch 291: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.04 
2024-02-06 03:13:36,478 EPOCH 292
2024-02-06 03:13:47,345 Epoch 292: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.96 
2024-02-06 03:13:47,346 EPOCH 293
2024-02-06 03:13:58,099 Epoch 293: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.93 
2024-02-06 03:13:58,100 EPOCH 294
2024-02-06 03:14:08,804 Epoch 294: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.98 
2024-02-06 03:14:08,805 EPOCH 295
2024-02-06 03:14:09,285 [Epoch: 295 Step: 00005000] Batch Recognition Loss:   0.002629 => Gls Tokens per Sec:     2672 || Batch Translation Loss:   0.132091 => Txt Tokens per Sec:     6407 || Lr: 0.000100
2024-02-06 03:14:19,666 Epoch 295: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.37 
2024-02-06 03:14:19,667 EPOCH 296
2024-02-06 03:14:30,584 Epoch 296: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.29 
2024-02-06 03:14:30,585 EPOCH 297
2024-02-06 03:14:41,441 Epoch 297: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.52 
2024-02-06 03:14:41,441 EPOCH 298
2024-02-06 03:14:51,946 Epoch 298: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.18 
2024-02-06 03:14:51,947 EPOCH 299
2024-02-06 03:15:02,810 Epoch 299: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.10 
2024-02-06 03:15:02,811 EPOCH 300
2024-02-06 03:15:13,518 [Epoch: 300 Step: 00005100] Batch Recognition Loss:   0.002032 => Gls Tokens per Sec:      992 || Batch Translation Loss:   0.131769 => Txt Tokens per Sec:     2744 || Lr: 0.000100
2024-02-06 03:15:13,518 Epoch 300: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.02 
2024-02-06 03:15:13,519 EPOCH 301
2024-02-06 03:15:23,971 Epoch 301: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.92 
2024-02-06 03:15:23,972 EPOCH 302
2024-02-06 03:15:34,663 Epoch 302: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.99 
2024-02-06 03:15:34,664 EPOCH 303
2024-02-06 03:15:45,364 Epoch 303: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.01 
2024-02-06 03:15:45,365 EPOCH 304
2024-02-06 03:15:55,867 Epoch 304: Total Training Recognition Loss 0.09  Total Training Translation Loss 2.25 
2024-02-06 03:15:55,868 EPOCH 305
2024-02-06 03:16:06,527 Epoch 305: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.11 
2024-02-06 03:16:06,528 EPOCH 306
2024-02-06 03:16:15,338 [Epoch: 306 Step: 00005200] Batch Recognition Loss:   0.001761 => Gls Tokens per Sec:     1060 || Batch Translation Loss:   0.088789 => Txt Tokens per Sec:     2891 || Lr: 0.000100
2024-02-06 03:16:17,420 Epoch 306: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.13 
2024-02-06 03:16:17,420 EPOCH 307
2024-02-06 03:16:28,235 Epoch 307: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.93 
2024-02-06 03:16:28,235 EPOCH 308
2024-02-06 03:16:38,956 Epoch 308: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.98 
2024-02-06 03:16:38,957 EPOCH 309
2024-02-06 03:16:49,693 Epoch 309: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.99 
2024-02-06 03:16:49,693 EPOCH 310
2024-02-06 03:17:00,657 Epoch 310: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.16 
2024-02-06 03:17:00,658 EPOCH 311
2024-02-06 03:17:11,439 Epoch 311: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.20 
2024-02-06 03:17:11,440 EPOCH 312
2024-02-06 03:17:17,607 [Epoch: 312 Step: 00005300] Batch Recognition Loss:   0.002229 => Gls Tokens per Sec:     1349 || Batch Translation Loss:   0.104442 => Txt Tokens per Sec:     3556 || Lr: 0.000100
2024-02-06 03:17:22,404 Epoch 312: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.05 
2024-02-06 03:17:22,404 EPOCH 313
2024-02-06 03:17:33,178 Epoch 313: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.99 
2024-02-06 03:17:33,179 EPOCH 314
2024-02-06 03:17:43,475 Epoch 314: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.98 
2024-02-06 03:17:43,475 EPOCH 315
2024-02-06 03:17:53,299 Epoch 315: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.24 
2024-02-06 03:17:53,300 EPOCH 316
2024-02-06 03:18:03,910 Epoch 316: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.00 
2024-02-06 03:18:03,911 EPOCH 317
2024-02-06 03:18:14,653 Epoch 317: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.99 
2024-02-06 03:18:14,654 EPOCH 318
2024-02-06 03:18:22,776 [Epoch: 318 Step: 00005400] Batch Recognition Loss:   0.003124 => Gls Tokens per Sec:      835 || Batch Translation Loss:   0.182480 => Txt Tokens per Sec:     2358 || Lr: 0.000100
2024-02-06 03:18:25,473 Epoch 318: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.08 
2024-02-06 03:18:25,473 EPOCH 319
2024-02-06 03:18:35,981 Epoch 319: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.28 
2024-02-06 03:18:35,981 EPOCH 320
2024-02-06 03:18:46,980 Epoch 320: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.61 
2024-02-06 03:18:46,982 EPOCH 321
2024-02-06 03:18:57,928 Epoch 321: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.55 
2024-02-06 03:18:57,928 EPOCH 322
2024-02-06 03:19:08,639 Epoch 322: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.13 
2024-02-06 03:19:08,640 EPOCH 323
2024-02-06 03:19:19,297 Epoch 323: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.47 
2024-02-06 03:19:19,297 EPOCH 324
2024-02-06 03:19:26,661 [Epoch: 324 Step: 00005500] Batch Recognition Loss:   0.005168 => Gls Tokens per Sec:      747 || Batch Translation Loss:   0.212042 => Txt Tokens per Sec:     2154 || Lr: 0.000100
2024-02-06 03:19:30,253 Epoch 324: Total Training Recognition Loss 0.08  Total Training Translation Loss 3.61 
2024-02-06 03:19:30,253 EPOCH 325
2024-02-06 03:19:40,853 Epoch 325: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.62 
2024-02-06 03:19:40,854 EPOCH 326
2024-02-06 03:19:51,226 Epoch 326: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.67 
2024-02-06 03:19:51,226 EPOCH 327
2024-02-06 03:20:02,082 Epoch 327: Total Training Recognition Loss 0.08  Total Training Translation Loss 3.76 
2024-02-06 03:20:02,082 EPOCH 328
2024-02-06 03:20:12,946 Epoch 328: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.81 
2024-02-06 03:20:12,947 EPOCH 329
2024-02-06 03:20:23,701 Epoch 329: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.55 
2024-02-06 03:20:23,701 EPOCH 330
2024-02-06 03:20:25,460 [Epoch: 330 Step: 00005600] Batch Recognition Loss:   0.002424 => Gls Tokens per Sec:     2550 || Batch Translation Loss:   0.191378 => Txt Tokens per Sec:     6606 || Lr: 0.000100
2024-02-06 03:20:34,589 Epoch 330: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.05 
2024-02-06 03:20:34,589 EPOCH 331
2024-02-06 03:20:45,415 Epoch 331: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.62 
2024-02-06 03:20:45,415 EPOCH 332
2024-02-06 03:20:56,093 Epoch 332: Total Training Recognition Loss 0.09  Total Training Translation Loss 2.81 
2024-02-06 03:20:56,093 EPOCH 333
2024-02-06 03:21:06,988 Epoch 333: Total Training Recognition Loss 0.08  Total Training Translation Loss 3.75 
2024-02-06 03:21:06,989 EPOCH 334
2024-02-06 03:21:17,395 Epoch 334: Total Training Recognition Loss 0.07  Total Training Translation Loss 4.38 
2024-02-06 03:21:17,395 EPOCH 335
2024-02-06 03:21:28,040 Epoch 335: Total Training Recognition Loss 0.10  Total Training Translation Loss 4.58 
2024-02-06 03:21:28,041 EPOCH 336
2024-02-06 03:21:31,377 [Epoch: 336 Step: 00005700] Batch Recognition Loss:   0.006698 => Gls Tokens per Sec:      882 || Batch Translation Loss:   0.311026 => Txt Tokens per Sec:     2593 || Lr: 0.000100
2024-02-06 03:21:38,471 Epoch 336: Total Training Recognition Loss 0.14  Total Training Translation Loss 4.52 
2024-02-06 03:21:38,472 EPOCH 337
2024-02-06 03:21:49,220 Epoch 337: Total Training Recognition Loss 0.10  Total Training Translation Loss 3.83 
2024-02-06 03:21:49,221 EPOCH 338
2024-02-06 03:22:00,034 Epoch 338: Total Training Recognition Loss 0.13  Total Training Translation Loss 3.54 
2024-02-06 03:22:00,034 EPOCH 339
2024-02-06 03:22:10,590 Epoch 339: Total Training Recognition Loss 0.13  Total Training Translation Loss 2.87 
2024-02-06 03:22:10,591 EPOCH 340
2024-02-06 03:22:21,037 Epoch 340: Total Training Recognition Loss 0.09  Total Training Translation Loss 2.70 
2024-02-06 03:22:21,038 EPOCH 341
2024-02-06 03:22:31,902 Epoch 341: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.08 
2024-02-06 03:22:31,902 EPOCH 342
2024-02-06 03:22:32,538 [Epoch: 342 Step: 00005800] Batch Recognition Loss:   0.001794 => Gls Tokens per Sec:     3024 || Batch Translation Loss:   0.113814 => Txt Tokens per Sec:     8214 || Lr: 0.000100
2024-02-06 03:22:42,481 Epoch 342: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.96 
2024-02-06 03:22:42,481 EPOCH 343
2024-02-06 03:22:53,286 Epoch 343: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.15 
2024-02-06 03:22:53,287 EPOCH 344
2024-02-06 03:23:03,940 Epoch 344: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.83 
2024-02-06 03:23:03,941 EPOCH 345
2024-02-06 03:23:14,599 Epoch 345: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.66 
2024-02-06 03:23:14,600 EPOCH 346
2024-02-06 03:23:25,377 Epoch 346: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.76 
2024-02-06 03:23:25,378 EPOCH 347
2024-02-06 03:23:36,047 Epoch 347: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.64 
2024-02-06 03:23:36,047 EPOCH 348
2024-02-06 03:23:36,376 [Epoch: 348 Step: 00005900] Batch Recognition Loss:   0.001400 => Gls Tokens per Sec:     1963 || Batch Translation Loss:   0.086409 => Txt Tokens per Sec:     5880 || Lr: 0.000100
2024-02-06 03:23:46,957 Epoch 348: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.61 
2024-02-06 03:23:46,958 EPOCH 349
2024-02-06 03:23:57,575 Epoch 349: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.59 
2024-02-06 03:23:57,575 EPOCH 350
2024-02-06 03:24:08,060 Epoch 350: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.81 
2024-02-06 03:24:08,061 EPOCH 351
2024-02-06 03:24:18,764 Epoch 351: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.85 
2024-02-06 03:24:18,765 EPOCH 352
2024-02-06 03:24:29,617 Epoch 352: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.69 
2024-02-06 03:24:29,618 EPOCH 353
2024-02-06 03:24:40,089 [Epoch: 353 Step: 00006000] Batch Recognition Loss:   0.001530 => Gls Tokens per Sec:      953 || Batch Translation Loss:   0.104467 => Txt Tokens per Sec:     2641 || Lr: 0.000100
2024-02-06 03:25:20,621 Validation result at epoch 353, step     6000: duration: 40.5314s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.12260	Translation Loss: 86180.67188	PPL: 5473.85010
	Eval Metric: BLEU
	WER 5.72	(DEL: 0.00,	INS: 0.00,	SUB: 5.72)
	BLEU-4 0.45	(BLEU-1: 10.74,	BLEU-2: 3.26,	BLEU-3: 1.14,	BLEU-4: 0.45)
	CHRF 17.88	ROUGE 8.86
2024-02-06 03:25:20,623 Logging Recognition and Translation Outputs
2024-02-06 03:25:20,624 ========================================================================================================================
2024-02-06 03:25:20,624 Logging Sequence: 160_153.00
2024-02-06 03:25:20,624 	Gloss Reference :	A B+C+D+E
2024-02-06 03:25:20,624 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 03:25:20,624 	Gloss Alignment :	         
2024-02-06 03:25:20,625 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 03:25:20,627 	Text Reference  :	*** i         have no  hard feelings towards rohit   sharma and  he    will     always have my  full support as he    is  my    teammate
2024-02-06 03:25:20,628 	Text Hypothesis :	now according to   the bcci if       the     liberty to     pick their decision as     you  can not  allowed to watch the right pad     
2024-02-06 03:25:20,628 	Text Alignment  :	I   S         S    S   S    S        S       S       S      S    S     S        S      S    S   S    S       S  S     S   S     S       
2024-02-06 03:25:20,628 ========================================================================================================================
2024-02-06 03:25:20,628 Logging Sequence: 103_253.00
2024-02-06 03:25:20,628 	Gloss Reference :	A B+C+D+E
2024-02-06 03:25:20,628 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 03:25:20,629 	Gloss Alignment :	         
2024-02-06 03:25:20,629 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 03:25:20,629 	Text Reference  :	canada is 3rd with 92      medals 
2024-02-06 03:25:20,629 	Text Hypothesis :	this   is why the  various winners
2024-02-06 03:25:20,630 	Text Alignment  :	S         S   S    S       S      
2024-02-06 03:25:20,630 ========================================================================================================================
2024-02-06 03:25:20,630 Logging Sequence: 155_25.00
2024-02-06 03:25:20,630 	Gloss Reference :	A B+C+D+E
2024-02-06 03:25:20,631 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 03:25:20,631 	Gloss Alignment :	         
2024-02-06 03:25:20,631 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 03:25:20,632 	Text Reference  :	* this is   because  taliban overthrew the    afghan government and took   over the  country
2024-02-06 03:25:20,632 	Text Hypothesis :	i am   very grateful for     my        family as     well       so  sports or   into tv     
2024-02-06 03:25:20,632 	Text Alignment  :	I S    S    S        S       S         S      S      S          S   S      S    S    S      
2024-02-06 03:25:20,633 ========================================================================================================================
2024-02-06 03:25:20,633 Logging Sequence: 81_105.00
2024-02-06 03:25:20,633 	Gloss Reference :	A B+C+D+E
2024-02-06 03:25:20,633 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 03:25:20,633 	Gloss Alignment :	         
2024-02-06 03:25:20,633 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 03:25:20,635 	Text Reference  :	****** ***** ** ***** dhoni was     tagged      in multiple such     posts as he was the brand ambassador
2024-02-06 03:25:20,635 	Text Hypothesis :	people loved to learn about dhoni's invovlement in ******** amrapali group as ** *** the ***** builder   
2024-02-06 03:25:20,635 	Text Alignment  :	I      I     I  I     S     S       S              D        S        S        D  D       D     S         
2024-02-06 03:25:20,635 ========================================================================================================================
2024-02-06 03:25:20,635 Logging Sequence: 105_136.00
2024-02-06 03:25:20,635 	Gloss Reference :	A B+C+D+E
2024-02-06 03:25:20,636 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 03:25:20,636 	Gloss Alignment :	         
2024-02-06 03:25:20,636 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 03:25:20,637 	Text Reference  :	beating him  once is  my      biggest dream  
2024-02-06 03:25:20,637 	Text Hypothesis :	so      they go   for careful the     morning
2024-02-06 03:25:20,637 	Text Alignment  :	S       S    S    S   S       S       S      
2024-02-06 03:25:20,637 ========================================================================================================================
2024-02-06 03:25:20,990 Epoch 353: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.01 
2024-02-06 03:25:20,990 EPOCH 354
2024-02-06 03:25:32,347 Epoch 354: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.01 
2024-02-06 03:25:32,347 EPOCH 355
2024-02-06 03:25:43,206 Epoch 355: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.70 
2024-02-06 03:25:43,206 EPOCH 356
2024-02-06 03:25:54,010 Epoch 356: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.48 
2024-02-06 03:25:54,011 EPOCH 357
2024-02-06 03:26:04,821 Epoch 357: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.69 
2024-02-06 03:26:04,821 EPOCH 358
2024-02-06 03:26:15,735 Epoch 358: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.49 
2024-02-06 03:26:15,736 EPOCH 359
2024-02-06 03:26:22,251 [Epoch: 359 Step: 00006100] Batch Recognition Loss:   0.002848 => Gls Tokens per Sec:     1336 || Batch Translation Loss:   0.065763 => Txt Tokens per Sec:     3539 || Lr: 0.000100
2024-02-06 03:26:26,416 Epoch 359: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.01 
2024-02-06 03:26:26,416 EPOCH 360
2024-02-06 03:26:37,165 Epoch 360: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.88 
2024-02-06 03:26:37,165 EPOCH 361
2024-02-06 03:26:47,733 Epoch 361: Total Training Recognition Loss 0.08  Total Training Translation Loss 3.25 
2024-02-06 03:26:47,733 EPOCH 362
2024-02-06 03:26:58,461 Epoch 362: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.08 
2024-02-06 03:26:58,462 EPOCH 363
2024-02-06 03:27:09,178 Epoch 363: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.64 
2024-02-06 03:27:09,179 EPOCH 364
2024-02-06 03:27:19,755 Epoch 364: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.51 
2024-02-06 03:27:19,756 EPOCH 365
2024-02-06 03:27:27,078 [Epoch: 365 Step: 00006200] Batch Recognition Loss:   0.001384 => Gls Tokens per Sec:     1049 || Batch Translation Loss:   0.195676 => Txt Tokens per Sec:     2989 || Lr: 0.000100
2024-02-06 03:27:30,311 Epoch 365: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.60 
2024-02-06 03:27:30,311 EPOCH 366
2024-02-06 03:27:40,958 Epoch 366: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.15 
2024-02-06 03:27:40,958 EPOCH 367
2024-02-06 03:27:51,782 Epoch 367: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.20 
2024-02-06 03:27:51,783 EPOCH 368
2024-02-06 03:28:02,770 Epoch 368: Total Training Recognition Loss 0.08  Total Training Translation Loss 3.48 
2024-02-06 03:28:02,770 EPOCH 369
2024-02-06 03:28:13,656 Epoch 369: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.98 
2024-02-06 03:28:13,656 EPOCH 370
2024-02-06 03:28:24,305 Epoch 370: Total Training Recognition Loss 0.08  Total Training Translation Loss 3.03 
2024-02-06 03:28:24,305 EPOCH 371
2024-02-06 03:28:26,568 [Epoch: 371 Step: 00006300] Batch Recognition Loss:   0.001334 => Gls Tokens per Sec:     2829 || Batch Translation Loss:   0.098036 => Txt Tokens per Sec:     7444 || Lr: 0.000100
2024-02-06 03:28:35,070 Epoch 371: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.39 
2024-02-06 03:28:35,071 EPOCH 372
2024-02-06 03:28:46,039 Epoch 372: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.12 
2024-02-06 03:28:46,039 EPOCH 373
2024-02-06 03:28:56,714 Epoch 373: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.77 
2024-02-06 03:28:56,715 EPOCH 374
2024-02-06 03:29:07,602 Epoch 374: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.78 
2024-02-06 03:29:07,602 EPOCH 375
2024-02-06 03:29:18,387 Epoch 375: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.83 
2024-02-06 03:29:18,388 EPOCH 376
2024-02-06 03:29:29,120 Epoch 376: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.70 
2024-02-06 03:29:29,120 EPOCH 377
2024-02-06 03:29:36,452 [Epoch: 377 Step: 00006400] Batch Recognition Loss:   0.011066 => Gls Tokens per Sec:      663 || Batch Translation Loss:   0.077324 => Txt Tokens per Sec:     1891 || Lr: 0.000100
2024-02-06 03:29:40,031 Epoch 377: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.48 
2024-02-06 03:29:40,032 EPOCH 378
2024-02-06 03:29:50,552 Epoch 378: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.69 
2024-02-06 03:29:50,553 EPOCH 379
2024-02-06 03:30:01,524 Epoch 379: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.36 
2024-02-06 03:30:01,524 EPOCH 380
2024-02-06 03:30:12,113 Epoch 380: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.33 
2024-02-06 03:30:12,113 EPOCH 381
2024-02-06 03:30:22,807 Epoch 381: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.26 
2024-02-06 03:30:22,807 EPOCH 382
2024-02-06 03:30:33,785 Epoch 382: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.09 
2024-02-06 03:30:33,786 EPOCH 383
2024-02-06 03:30:38,676 [Epoch: 383 Step: 00006500] Batch Recognition Loss:   0.006246 => Gls Tokens per Sec:      786 || Batch Translation Loss:   0.074073 => Txt Tokens per Sec:     2202 || Lr: 0.000100
2024-02-06 03:30:44,640 Epoch 383: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.28 
2024-02-06 03:30:44,640 EPOCH 384
2024-02-06 03:30:55,404 Epoch 384: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.59 
2024-02-06 03:30:55,405 EPOCH 385
2024-02-06 03:31:06,079 Epoch 385: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.02 
2024-02-06 03:31:06,080 EPOCH 386
2024-02-06 03:31:16,788 Epoch 386: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.06 
2024-02-06 03:31:16,788 EPOCH 387
2024-02-06 03:31:27,858 Epoch 387: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.60 
2024-02-06 03:31:27,858 EPOCH 388
2024-02-06 03:31:38,679 Epoch 388: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.45 
2024-02-06 03:31:38,680 EPOCH 389
2024-02-06 03:31:39,426 [Epoch: 389 Step: 00006600] Batch Recognition Loss:   0.002860 => Gls Tokens per Sec:     3441 || Batch Translation Loss:   0.141823 => Txt Tokens per Sec:     8622 || Lr: 0.000100
2024-02-06 03:31:49,124 Epoch 389: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.51 
2024-02-06 03:31:49,124 EPOCH 390
2024-02-06 03:31:59,748 Epoch 390: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.83 
2024-02-06 03:31:59,748 EPOCH 391
2024-02-06 03:32:10,602 Epoch 391: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.92 
2024-02-06 03:32:10,602 EPOCH 392
2024-02-06 03:32:21,400 Epoch 392: Total Training Recognition Loss 0.06  Total Training Translation Loss 6.12 
2024-02-06 03:32:21,400 EPOCH 393
2024-02-06 03:32:32,146 Epoch 393: Total Training Recognition Loss 0.10  Total Training Translation Loss 10.56 
2024-02-06 03:32:32,147 EPOCH 394
2024-02-06 03:32:43,017 Epoch 394: Total Training Recognition Loss 0.23  Total Training Translation Loss 25.90 
2024-02-06 03:32:43,018 EPOCH 395
2024-02-06 03:32:43,385 [Epoch: 395 Step: 00006700] Batch Recognition Loss:   0.015546 => Gls Tokens per Sec:     3497 || Batch Translation Loss:   0.450522 => Txt Tokens per Sec:     8339 || Lr: 0.000100
2024-02-06 03:32:53,536 Epoch 395: Total Training Recognition Loss 0.18  Total Training Translation Loss 13.30 
2024-02-06 03:32:53,536 EPOCH 396
2024-02-06 03:33:04,431 Epoch 396: Total Training Recognition Loss 0.19  Total Training Translation Loss 7.06 
2024-02-06 03:33:04,432 EPOCH 397
2024-02-06 03:33:15,150 Epoch 397: Total Training Recognition Loss 0.12  Total Training Translation Loss 4.86 
2024-02-06 03:33:15,151 EPOCH 398
2024-02-06 03:33:25,967 Epoch 398: Total Training Recognition Loss 0.11  Total Training Translation Loss 2.82 
2024-02-06 03:33:25,968 EPOCH 399
2024-02-06 03:33:36,901 Epoch 399: Total Training Recognition Loss 0.09  Total Training Translation Loss 2.12 
2024-02-06 03:33:36,902 EPOCH 400
2024-02-06 03:33:47,621 [Epoch: 400 Step: 00006800] Batch Recognition Loss:   0.001481 => Gls Tokens per Sec:      991 || Batch Translation Loss:   0.161181 => Txt Tokens per Sec:     2741 || Lr: 0.000100
2024-02-06 03:33:47,621 Epoch 400: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.75 
2024-02-06 03:33:47,622 EPOCH 401
2024-02-06 03:33:57,864 Epoch 401: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.54 
2024-02-06 03:33:57,865 EPOCH 402
2024-02-06 03:34:08,674 Epoch 402: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.34 
2024-02-06 03:34:08,674 EPOCH 403
2024-02-06 03:34:19,525 Epoch 403: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.12 
2024-02-06 03:34:19,526 EPOCH 404
2024-02-06 03:34:30,259 Epoch 404: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.04 
2024-02-06 03:34:30,259 EPOCH 405
2024-02-06 03:34:41,302 Epoch 405: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.02 
2024-02-06 03:34:41,303 EPOCH 406
2024-02-06 03:34:50,153 [Epoch: 406 Step: 00006900] Batch Recognition Loss:   0.008211 => Gls Tokens per Sec:     1056 || Batch Translation Loss:   0.025959 => Txt Tokens per Sec:     2864 || Lr: 0.000100
2024-02-06 03:34:52,091 Epoch 406: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.01 
2024-02-06 03:34:52,092 EPOCH 407
2024-02-06 03:35:02,841 Epoch 407: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.91 
2024-02-06 03:35:02,841 EPOCH 408
2024-02-06 03:35:13,531 Epoch 408: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.02 
2024-02-06 03:35:13,532 EPOCH 409
2024-02-06 03:35:24,153 Epoch 409: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.79 
2024-02-06 03:35:24,154 EPOCH 410
2024-02-06 03:35:34,967 Epoch 410: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.89 
2024-02-06 03:35:34,967 EPOCH 411
2024-02-06 03:35:45,439 Epoch 411: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.79 
2024-02-06 03:35:45,440 EPOCH 412
2024-02-06 03:35:53,753 [Epoch: 412 Step: 00007000] Batch Recognition Loss:   0.001481 => Gls Tokens per Sec:      970 || Batch Translation Loss:   0.040749 => Txt Tokens per Sec:     2699 || Lr: 0.000100
2024-02-06 03:35:56,183 Epoch 412: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.74 
2024-02-06 03:35:56,183 EPOCH 413
2024-02-06 03:36:06,773 Epoch 413: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.75 
2024-02-06 03:36:06,773 EPOCH 414
2024-02-06 03:36:17,515 Epoch 414: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.83 
2024-02-06 03:36:17,516 EPOCH 415
2024-02-06 03:36:28,188 Epoch 415: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.85 
2024-02-06 03:36:28,189 EPOCH 416
2024-02-06 03:36:38,814 Epoch 416: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.82 
2024-02-06 03:36:38,815 EPOCH 417
2024-02-06 03:36:49,354 Epoch 417: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.83 
2024-02-06 03:36:49,354 EPOCH 418
2024-02-06 03:36:53,325 [Epoch: 418 Step: 00007100] Batch Recognition Loss:   0.002757 => Gls Tokens per Sec:     1774 || Batch Translation Loss:   0.017540 => Txt Tokens per Sec:     4579 || Lr: 0.000100
2024-02-06 03:37:00,263 Epoch 418: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.69 
2024-02-06 03:37:00,264 EPOCH 419
2024-02-06 03:37:10,943 Epoch 419: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.76 
2024-02-06 03:37:10,943 EPOCH 420
2024-02-06 03:37:21,403 Epoch 420: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.69 
2024-02-06 03:37:21,404 EPOCH 421
2024-02-06 03:37:32,195 Epoch 421: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.69 
2024-02-06 03:37:32,195 EPOCH 422
2024-02-06 03:37:42,946 Epoch 422: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.62 
2024-02-06 03:37:42,947 EPOCH 423
2024-02-06 03:37:53,869 Epoch 423: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.62 
2024-02-06 03:37:53,869 EPOCH 424
2024-02-06 03:37:57,947 [Epoch: 424 Step: 00007200] Batch Recognition Loss:   0.000947 => Gls Tokens per Sec:     1413 || Batch Translation Loss:   0.037741 => Txt Tokens per Sec:     4008 || Lr: 0.000100
2024-02-06 03:38:04,619 Epoch 424: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.61 
2024-02-06 03:38:04,620 EPOCH 425
2024-02-06 03:38:15,399 Epoch 425: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.71 
2024-02-06 03:38:15,399 EPOCH 426
2024-02-06 03:38:25,994 Epoch 426: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.68 
2024-02-06 03:38:25,995 EPOCH 427
2024-02-06 03:38:36,962 Epoch 427: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.71 
2024-02-06 03:38:36,962 EPOCH 428
2024-02-06 03:38:47,620 Epoch 428: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.58 
2024-02-06 03:38:47,621 EPOCH 429
2024-02-06 03:38:58,593 Epoch 429: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.64 
2024-02-06 03:38:58,594 EPOCH 430
2024-02-06 03:39:02,380 [Epoch: 430 Step: 00007300] Batch Recognition Loss:   0.003890 => Gls Tokens per Sec:     1115 || Batch Translation Loss:   0.038580 => Txt Tokens per Sec:     3017 || Lr: 0.000100
2024-02-06 03:39:09,413 Epoch 430: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.72 
2024-02-06 03:39:09,414 EPOCH 431
2024-02-06 03:39:20,143 Epoch 431: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.84 
2024-02-06 03:39:20,144 EPOCH 432
2024-02-06 03:39:31,006 Epoch 432: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.02 
2024-02-06 03:39:31,007 EPOCH 433
2024-02-06 03:39:41,671 Epoch 433: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.14 
2024-02-06 03:39:41,672 EPOCH 434
2024-02-06 03:39:52,377 Epoch 434: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.18 
2024-02-06 03:39:52,378 EPOCH 435
2024-02-06 03:40:03,028 Epoch 435: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.04 
2024-02-06 03:40:03,029 EPOCH 436
2024-02-06 03:40:04,084 [Epoch: 436 Step: 00007400] Batch Recognition Loss:   0.001094 => Gls Tokens per Sec:     3032 || Batch Translation Loss:   0.057400 => Txt Tokens per Sec:     8300 || Lr: 0.000100
2024-02-06 03:40:13,836 Epoch 436: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.99 
2024-02-06 03:40:13,836 EPOCH 437
2024-02-06 03:40:24,506 Epoch 437: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.22 
2024-02-06 03:40:24,507 EPOCH 438
2024-02-06 03:40:35,204 Epoch 438: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.93 
2024-02-06 03:40:35,205 EPOCH 439
2024-02-06 03:40:46,223 Epoch 439: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.88 
2024-02-06 03:40:46,223 EPOCH 440
2024-02-06 03:40:57,189 Epoch 440: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.86 
2024-02-06 03:40:57,190 EPOCH 441
2024-02-06 03:41:08,047 Epoch 441: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.99 
2024-02-06 03:41:08,048 EPOCH 442
2024-02-06 03:41:11,671 [Epoch: 442 Step: 00007500] Batch Recognition Loss:   0.001149 => Gls Tokens per Sec:      530 || Batch Translation Loss:   0.051488 => Txt Tokens per Sec:     1635 || Lr: 0.000100
2024-02-06 03:41:18,828 Epoch 442: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.09 
2024-02-06 03:41:18,829 EPOCH 443
2024-02-06 03:41:29,431 Epoch 443: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.21 
2024-02-06 03:41:29,431 EPOCH 444
2024-02-06 03:41:40,143 Epoch 444: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.04 
2024-02-06 03:41:40,143 EPOCH 445
2024-02-06 03:41:50,723 Epoch 445: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.96 
2024-02-06 03:41:50,723 EPOCH 446
2024-02-06 03:42:01,264 Epoch 446: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.08 
2024-02-06 03:42:01,266 EPOCH 447
2024-02-06 03:42:11,972 Epoch 447: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.30 
2024-02-06 03:42:11,972 EPOCH 448
2024-02-06 03:42:14,373 [Epoch: 448 Step: 00007600] Batch Recognition Loss:   0.038985 => Gls Tokens per Sec:      158 || Batch Translation Loss:   0.059825 => Txt Tokens per Sec:      565 || Lr: 0.000100
2024-02-06 03:42:22,677 Epoch 448: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.64 
2024-02-06 03:42:22,678 EPOCH 449
2024-02-06 03:42:33,230 Epoch 449: Total Training Recognition Loss 0.11  Total Training Translation Loss 1.67 
2024-02-06 03:42:33,231 EPOCH 450
2024-02-06 03:42:43,952 Epoch 450: Total Training Recognition Loss 0.10  Total Training Translation Loss 2.52 
2024-02-06 03:42:43,952 EPOCH 451
2024-02-06 03:42:54,502 Epoch 451: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.75 
2024-02-06 03:42:54,503 EPOCH 452
2024-02-06 03:43:05,520 Epoch 452: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.56 
2024-02-06 03:43:05,521 EPOCH 453
2024-02-06 03:43:16,166 [Epoch: 453 Step: 00007700] Batch Recognition Loss:   0.003955 => Gls Tokens per Sec:      938 || Batch Translation Loss:   0.243777 => Txt Tokens per Sec:     2607 || Lr: 0.000100
2024-02-06 03:43:16,394 Epoch 453: Total Training Recognition Loss 0.08  Total Training Translation Loss 4.02 
2024-02-06 03:43:16,394 EPOCH 454
2024-02-06 03:43:27,056 Epoch 454: Total Training Recognition Loss 0.07  Total Training Translation Loss 4.18 
2024-02-06 03:43:27,057 EPOCH 455
2024-02-06 03:43:37,633 Epoch 455: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.71 
2024-02-06 03:43:37,633 EPOCH 456
2024-02-06 03:43:48,412 Epoch 456: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.86 
2024-02-06 03:43:48,412 EPOCH 457
2024-02-06 03:43:59,118 Epoch 457: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.39 
2024-02-06 03:43:59,119 EPOCH 458
2024-02-06 03:44:09,916 Epoch 458: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.10 
2024-02-06 03:44:09,916 EPOCH 459
2024-02-06 03:44:20,245 [Epoch: 459 Step: 00007800] Batch Recognition Loss:   0.000941 => Gls Tokens per Sec:      843 || Batch Translation Loss:   0.192846 => Txt Tokens per Sec:     2386 || Lr: 0.000100
2024-02-06 03:44:20,774 Epoch 459: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.73 
2024-02-06 03:44:20,774 EPOCH 460
2024-02-06 03:44:31,352 Epoch 460: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.74 
2024-02-06 03:44:31,352 EPOCH 461
2024-02-06 03:44:41,775 Epoch 461: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.60 
2024-02-06 03:44:41,776 EPOCH 462
2024-02-06 03:44:52,714 Epoch 462: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.47 
2024-02-06 03:44:52,714 EPOCH 463
2024-02-06 03:45:03,500 Epoch 463: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.25 
2024-02-06 03:45:03,500 EPOCH 464
2024-02-06 03:45:14,003 Epoch 464: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.48 
2024-02-06 03:45:14,003 EPOCH 465
2024-02-06 03:45:21,247 [Epoch: 465 Step: 00007900] Batch Recognition Loss:   0.003109 => Gls Tokens per Sec:     1061 || Batch Translation Loss:   0.072630 => Txt Tokens per Sec:     2888 || Lr: 0.000100
2024-02-06 03:45:24,688 Epoch 465: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.24 
2024-02-06 03:45:24,688 EPOCH 466
2024-02-06 03:45:35,410 Epoch 466: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.16 
2024-02-06 03:45:35,411 EPOCH 467
2024-02-06 03:45:46,022 Epoch 467: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.18 
2024-02-06 03:45:46,022 EPOCH 468
2024-02-06 03:45:56,755 Epoch 468: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.19 
2024-02-06 03:45:56,755 EPOCH 469
2024-02-06 03:46:07,446 Epoch 469: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.29 
2024-02-06 03:46:07,446 EPOCH 470
2024-02-06 03:46:18,233 Epoch 470: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.32 
2024-02-06 03:46:18,234 EPOCH 471
2024-02-06 03:46:22,332 [Epoch: 471 Step: 00008000] Batch Recognition Loss:   0.001463 => Gls Tokens per Sec:     1499 || Batch Translation Loss:   0.114679 => Txt Tokens per Sec:     3876 || Lr: 0.000100
2024-02-06 03:47:02,879 Validation result at epoch 471, step     8000: duration: 40.5460s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.01267	Translation Loss: 90590.53125	PPL: 8503.17480
	Eval Metric: BLEU
	WER 5.30	(DEL: 0.00,	INS: 0.00,	SUB: 5.30)
	BLEU-4 0.65	(BLEU-1: 11.78,	BLEU-2: 3.85,	BLEU-3: 1.45,	BLEU-4: 0.65)
	CHRF 17.76	ROUGE 10.22
2024-02-06 03:47:02,881 Logging Recognition and Translation Outputs
2024-02-06 03:47:02,882 ========================================================================================================================
2024-02-06 03:47:02,882 Logging Sequence: 180_236.00
2024-02-06 03:47:02,882 	Gloss Reference :	A B+C+D+E
2024-02-06 03:47:02,882 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 03:47:02,882 	Gloss Alignment :	         
2024-02-06 03:47:02,882 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 03:47:02,884 	Text Reference  :	however the wrestlers returned to        the protest *** site at  jantar  mantar with     thier demands
2024-02-06 03:47:02,884 	Text Hypothesis :	however the wrestlers ******** continued the protest and has  not allowed to     complete the   panel  
2024-02-06 03:47:02,884 	Text Alignment  :	                      D        S                     I   S    S   S       S      S        S     S      
2024-02-06 03:47:02,884 ========================================================================================================================
2024-02-06 03:47:02,884 Logging Sequence: 111_154.00
2024-02-06 03:47:02,885 	Gloss Reference :	A B+C+D+E  
2024-02-06 03:47:02,885 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-06 03:47:02,885 	Gloss Alignment :	  S        
2024-02-06 03:47:02,885 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 03:47:02,886 	Text Reference  :	********* ** ********** ** *** *** due to  csk's   slow over rate    dhoni was fined rs   12  lakh   
2024-02-06 03:47:02,887 	Text Hypothesis :	over-rate is calculated at the end of  the captain by   the  umpires and   if  they  find the captain
2024-02-06 03:47:02,887 	Text Alignment  :	I         I  I          I  I   I   S   S   S       S    S    S       S     S   S     S    S   S      
2024-02-06 03:47:02,887 ========================================================================================================================
2024-02-06 03:47:02,887 Logging Sequence: 118_314.00
2024-02-06 03:47:02,888 	Gloss Reference :	A B+C+D+E
2024-02-06 03:47:02,888 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 03:47:02,888 	Gloss Alignment :	         
2024-02-06 03:47:02,888 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 03:47:02,889 	Text Reference  :	** wow  even the president had come to   watch
2024-02-06 03:47:02,889 	Text Hypothesis :	so here are  the ********* *** **** same time 
2024-02-06 03:47:02,889 	Text Alignment  :	I  S    S        D         D   D    S    S    
2024-02-06 03:47:02,889 ========================================================================================================================
2024-02-06 03:47:02,889 Logging Sequence: 156_197.00
2024-02-06 03:47:02,889 	Gloss Reference :	A B+C+D+E
2024-02-06 03:47:02,890 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 03:47:02,890 	Gloss Alignment :	         
2024-02-06 03:47:02,890 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 03:47:02,891 	Text Reference  :	seattle orcas sor ****** is   owned       by  many   investors including satya nadella microsoft ceo    
2024-02-06 03:47:02,891 	Text Hypothesis :	******* ***** sor bowler from afghanistan and others have      invested  in    the     trent     rockets
2024-02-06 03:47:02,891 	Text Alignment  :	D       D         I      S    S           S   S      S         S         S     S       S         S      
2024-02-06 03:47:02,892 ========================================================================================================================
2024-02-06 03:47:02,892 Logging Sequence: 183_159.00
2024-02-06 03:47:02,892 	Gloss Reference :	A B+C+D+E
2024-02-06 03:47:02,892 	Gloss Hypothesis:	A B+C+D  
2024-02-06 03:47:02,892 	Gloss Alignment :	  S      
2024-02-06 03:47:02,893 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 03:47:02,895 	Text Reference  :	however an exception to this is virat kohli  and  his     wife anushka sharma who refuse to      share images of     their daughter
2024-02-06 03:47:02,895 	Text Hypothesis :	******* ** ********* ** **** ** later rooney then invited the  first   time   a   new    zealand and   then   joined the   umpire  
2024-02-06 03:47:02,895 	Text Alignment  :	D       D  D         D  D    D  S     S      S    S       S    S       S      S   S      S       S     S      S      S     S       
2024-02-06 03:47:02,895 ========================================================================================================================
2024-02-06 03:47:09,739 Epoch 471: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.19 
2024-02-06 03:47:09,740 EPOCH 472
2024-02-06 03:47:20,842 Epoch 472: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.51 
2024-02-06 03:47:20,842 EPOCH 473
2024-02-06 03:47:31,765 Epoch 473: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.47 
2024-02-06 03:47:31,765 EPOCH 474
2024-02-06 03:47:42,464 Epoch 474: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.29 
2024-02-06 03:47:42,464 EPOCH 475
2024-02-06 03:47:53,096 Epoch 475: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.16 
2024-02-06 03:47:53,097 EPOCH 476
2024-02-06 03:48:03,360 Epoch 476: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.12 
2024-02-06 03:48:03,360 EPOCH 477
2024-02-06 03:48:04,897 [Epoch: 477 Step: 00008100] Batch Recognition Loss:   0.001015 => Gls Tokens per Sec:     3333 || Batch Translation Loss:   0.081984 => Txt Tokens per Sec:     8706 || Lr: 0.000100
2024-02-06 03:48:13,622 Epoch 477: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.26 
2024-02-06 03:48:13,623 EPOCH 478
2024-02-06 03:48:24,405 Epoch 478: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.48 
2024-02-06 03:48:24,406 EPOCH 479
2024-02-06 03:48:34,919 Epoch 479: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.30 
2024-02-06 03:48:34,920 EPOCH 480
2024-02-06 03:48:45,545 Epoch 480: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.31 
2024-02-06 03:48:45,545 EPOCH 481
2024-02-06 03:48:56,285 Epoch 481: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.52 
2024-02-06 03:48:56,285 EPOCH 482
2024-02-06 03:49:07,095 Epoch 482: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.69 
2024-02-06 03:49:07,096 EPOCH 483
2024-02-06 03:49:11,806 [Epoch: 483 Step: 00008200] Batch Recognition Loss:   0.002368 => Gls Tokens per Sec:      815 || Batch Translation Loss:   0.136474 => Txt Tokens per Sec:     2465 || Lr: 0.000100
2024-02-06 03:49:17,818 Epoch 483: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.52 
2024-02-06 03:49:17,818 EPOCH 484
2024-02-06 03:49:28,601 Epoch 484: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.53 
2024-02-06 03:49:28,602 EPOCH 485
2024-02-06 03:49:39,288 Epoch 485: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.52 
2024-02-06 03:49:39,289 EPOCH 486
2024-02-06 03:49:50,009 Epoch 486: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.57 
2024-02-06 03:49:50,009 EPOCH 487
2024-02-06 03:50:00,791 Epoch 487: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.23 
2024-02-06 03:50:00,791 EPOCH 488
2024-02-06 03:50:11,257 Epoch 488: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.20 
2024-02-06 03:50:11,258 EPOCH 489
2024-02-06 03:50:13,371 [Epoch: 489 Step: 00008300] Batch Recognition Loss:   0.000559 => Gls Tokens per Sec:     1213 || Batch Translation Loss:   0.070079 => Txt Tokens per Sec:     3333 || Lr: 0.000100
2024-02-06 03:50:21,952 Epoch 489: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.12 
2024-02-06 03:50:21,953 EPOCH 490
2024-02-06 03:50:32,448 Epoch 490: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.32 
2024-02-06 03:50:32,449 EPOCH 491
2024-02-06 03:50:43,177 Epoch 491: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.55 
2024-02-06 03:50:43,178 EPOCH 492
2024-02-06 03:50:53,841 Epoch 492: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.22 
2024-02-06 03:50:53,841 EPOCH 493
2024-02-06 03:51:04,532 Epoch 493: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.29 
2024-02-06 03:51:04,532 EPOCH 494
2024-02-06 03:51:15,129 Epoch 494: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.16 
2024-02-06 03:51:15,130 EPOCH 495
2024-02-06 03:51:15,403 [Epoch: 495 Step: 00008400] Batch Recognition Loss:   0.002988 => Gls Tokens per Sec:     4689 || Batch Translation Loss:   0.040512 => Txt Tokens per Sec:     8916 || Lr: 0.000100
2024-02-06 03:51:25,921 Epoch 495: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.13 
2024-02-06 03:51:25,922 EPOCH 496
2024-02-06 03:51:36,496 Epoch 496: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.05 
2024-02-06 03:51:36,496 EPOCH 497
2024-02-06 03:51:47,346 Epoch 497: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.18 
2024-02-06 03:51:47,346 EPOCH 498
2024-02-06 03:51:58,437 Epoch 498: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.54 
2024-02-06 03:51:58,438 EPOCH 499
2024-02-06 03:52:09,406 Epoch 499: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.88 
2024-02-06 03:52:09,407 EPOCH 500
2024-02-06 03:52:20,112 [Epoch: 500 Step: 00008500] Batch Recognition Loss:   0.001085 => Gls Tokens per Sec:      992 || Batch Translation Loss:   0.168239 => Txt Tokens per Sec:     2745 || Lr: 0.000100
2024-02-06 03:52:20,113 Epoch 500: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.42 
2024-02-06 03:52:20,113 EPOCH 501
2024-02-06 03:52:30,794 Epoch 501: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.02 
2024-02-06 03:52:30,795 EPOCH 502
2024-02-06 03:52:41,606 Epoch 502: Total Training Recognition Loss 0.08  Total Training Translation Loss 3.07 
2024-02-06 03:52:41,607 EPOCH 503
2024-02-06 03:52:52,303 Epoch 503: Total Training Recognition Loss 0.16  Total Training Translation Loss 2.35 
2024-02-06 03:52:52,303 EPOCH 504
2024-02-06 03:53:02,956 Epoch 504: Total Training Recognition Loss 0.25  Total Training Translation Loss 1.90 
2024-02-06 03:53:02,957 EPOCH 505
2024-02-06 03:53:13,685 Epoch 505: Total Training Recognition Loss 0.91  Total Training Translation Loss 3.57 
2024-02-06 03:53:13,685 EPOCH 506
2024-02-06 03:53:23,980 [Epoch: 506 Step: 00008600] Batch Recognition Loss:   2.665514 => Gls Tokens per Sec:      908 || Batch Translation Loss:   1.306985 => Txt Tokens per Sec:     2581 || Lr: 0.000100
2024-02-06 03:53:24,393 Epoch 506: Total Training Recognition Loss 6.02  Total Training Translation Loss 6.16 
2024-02-06 03:53:24,394 EPOCH 507
2024-02-06 03:53:35,014 Epoch 507: Total Training Recognition Loss 8.72  Total Training Translation Loss 11.86 
2024-02-06 03:53:35,015 EPOCH 508
2024-02-06 03:53:45,829 Epoch 508: Total Training Recognition Loss 8.33  Total Training Translation Loss 15.57 
2024-02-06 03:53:45,829 EPOCH 509
2024-02-06 03:53:56,641 Epoch 509: Total Training Recognition Loss 3.33  Total Training Translation Loss 7.60 
2024-02-06 03:53:56,642 EPOCH 510
2024-02-06 03:54:07,418 Epoch 510: Total Training Recognition Loss 3.32  Total Training Translation Loss 3.75 
2024-02-06 03:54:07,418 EPOCH 511
2024-02-06 03:54:17,967 Epoch 511: Total Training Recognition Loss 1.12  Total Training Translation Loss 2.46 
2024-02-06 03:54:17,967 EPOCH 512
2024-02-06 03:54:27,649 [Epoch: 512 Step: 00008700] Batch Recognition Loss:   0.000538 => Gls Tokens per Sec:      833 || Batch Translation Loss:   0.070160 => Txt Tokens per Sec:     2329 || Lr: 0.000100
2024-02-06 03:54:28,604 Epoch 512: Total Training Recognition Loss 0.20  Total Training Translation Loss 1.59 
2024-02-06 03:54:28,604 EPOCH 513
2024-02-06 03:54:39,239 Epoch 513: Total Training Recognition Loss 0.12  Total Training Translation Loss 1.09 
2024-02-06 03:54:39,239 EPOCH 514
2024-02-06 03:54:49,977 Epoch 514: Total Training Recognition Loss 0.07  Total Training Translation Loss 0.96 
2024-02-06 03:54:49,978 EPOCH 515
2024-02-06 03:55:00,741 Epoch 515: Total Training Recognition Loss 0.07  Total Training Translation Loss 0.82 
2024-02-06 03:55:00,742 EPOCH 516
2024-02-06 03:55:11,153 Epoch 516: Total Training Recognition Loss 0.07  Total Training Translation Loss 0.79 
2024-02-06 03:55:11,153 EPOCH 517
2024-02-06 03:55:21,731 Epoch 517: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.76 
2024-02-06 03:55:21,731 EPOCH 518
2024-02-06 03:55:25,949 [Epoch: 518 Step: 00008800] Batch Recognition Loss:   0.001574 => Gls Tokens per Sec:     1670 || Batch Translation Loss:   0.025872 => Txt Tokens per Sec:     4476 || Lr: 0.000100
2024-02-06 03:55:32,280 Epoch 518: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.71 
2024-02-06 03:55:32,280 EPOCH 519
2024-02-06 03:55:43,261 Epoch 519: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.61 
2024-02-06 03:55:43,261 EPOCH 520
2024-02-06 03:55:54,087 Epoch 520: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.65 
2024-02-06 03:55:54,088 EPOCH 521
2024-02-06 03:56:04,634 Epoch 521: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.74 
2024-02-06 03:56:04,634 EPOCH 522
2024-02-06 03:56:15,387 Epoch 522: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.65 
2024-02-06 03:56:15,388 EPOCH 523
2024-02-06 03:56:26,078 Epoch 523: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.55 
2024-02-06 03:56:26,078 EPOCH 524
2024-02-06 03:56:33,425 [Epoch: 524 Step: 00008900] Batch Recognition Loss:   0.004848 => Gls Tokens per Sec:      749 || Batch Translation Loss:   0.042534 => Txt Tokens per Sec:     2061 || Lr: 0.000100
2024-02-06 03:56:36,990 Epoch 524: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.55 
2024-02-06 03:56:36,991 EPOCH 525
2024-02-06 03:56:47,964 Epoch 525: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.55 
2024-02-06 03:56:47,964 EPOCH 526
2024-02-06 03:56:58,471 Epoch 526: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.57 
2024-02-06 03:56:58,472 EPOCH 527
2024-02-06 03:57:09,392 Epoch 527: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.61 
2024-02-06 03:57:09,393 EPOCH 528
2024-02-06 03:57:20,078 Epoch 528: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.78 
2024-02-06 03:57:20,079 EPOCH 529
2024-02-06 03:57:30,832 Epoch 529: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.72 
2024-02-06 03:57:30,833 EPOCH 530
2024-02-06 03:57:33,757 [Epoch: 530 Step: 00009000] Batch Recognition Loss:   0.005575 => Gls Tokens per Sec:     1533 || Batch Translation Loss:   0.046296 => Txt Tokens per Sec:     4121 || Lr: 0.000100
2024-02-06 03:57:41,407 Epoch 530: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.64 
2024-02-06 03:57:41,407 EPOCH 531
2024-02-06 03:57:52,088 Epoch 531: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.56 
2024-02-06 03:57:52,088 EPOCH 532
2024-02-06 03:58:02,680 Epoch 532: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.56 
2024-02-06 03:58:02,680 EPOCH 533
2024-02-06 03:58:13,377 Epoch 533: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.55 
2024-02-06 03:58:13,378 EPOCH 534
2024-02-06 03:58:24,081 Epoch 534: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.50 
2024-02-06 03:58:24,082 EPOCH 535
2024-02-06 03:58:34,958 Epoch 535: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.48 
2024-02-06 03:58:34,958 EPOCH 536
2024-02-06 03:58:35,981 [Epoch: 536 Step: 00009100] Batch Recognition Loss:   0.000265 => Gls Tokens per Sec:     3132 || Batch Translation Loss:   0.022961 => Txt Tokens per Sec:     7561 || Lr: 0.000100
2024-02-06 03:58:45,923 Epoch 536: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.53 
2024-02-06 03:58:45,924 EPOCH 537
2024-02-06 03:58:56,532 Epoch 537: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.50 
2024-02-06 03:58:56,533 EPOCH 538
2024-02-06 03:59:07,448 Epoch 538: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.49 
2024-02-06 03:59:07,449 EPOCH 539
2024-02-06 03:59:18,284 Epoch 539: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.40 
2024-02-06 03:59:18,285 EPOCH 540
2024-02-06 03:59:28,900 Epoch 540: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.40 
2024-02-06 03:59:28,900 EPOCH 541
2024-02-06 03:59:39,735 Epoch 541: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.43 
2024-02-06 03:59:39,735 EPOCH 542
2024-02-06 03:59:40,392 [Epoch: 542 Step: 00009200] Batch Recognition Loss:   0.000167 => Gls Tokens per Sec:     2927 || Batch Translation Loss:   0.025226 => Txt Tokens per Sec:     7628 || Lr: 0.000100
2024-02-06 03:59:50,532 Epoch 542: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.47 
2024-02-06 03:59:50,533 EPOCH 543
2024-02-06 04:00:01,148 Epoch 543: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.51 
2024-02-06 04:00:01,149 EPOCH 544
2024-02-06 04:00:11,753 Epoch 544: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.53 
2024-02-06 04:00:11,753 EPOCH 545
2024-02-06 04:00:22,456 Epoch 545: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.57 
2024-02-06 04:00:22,457 EPOCH 546
2024-02-06 04:00:33,365 Epoch 546: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.52 
2024-02-06 04:00:33,366 EPOCH 547
2024-02-06 04:00:44,198 Epoch 547: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.48 
2024-02-06 04:00:44,199 EPOCH 548
2024-02-06 04:00:44,366 [Epoch: 548 Step: 00009300] Batch Recognition Loss:   0.000285 => Gls Tokens per Sec:     3855 || Batch Translation Loss:   0.037852 => Txt Tokens per Sec:     9572 || Lr: 0.000100
2024-02-06 04:00:54,699 Epoch 548: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-06 04:00:54,699 EPOCH 549
2024-02-06 04:01:05,275 Epoch 549: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.83 
2024-02-06 04:01:05,275 EPOCH 550
2024-02-06 04:01:15,902 Epoch 550: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.85 
2024-02-06 04:01:15,902 EPOCH 551
2024-02-06 04:01:26,525 Epoch 551: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.70 
2024-02-06 04:01:26,526 EPOCH 552
2024-02-06 04:01:37,143 Epoch 552: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.05 
2024-02-06 04:01:37,144 EPOCH 553
2024-02-06 04:01:46,199 [Epoch: 553 Step: 00009400] Batch Recognition Loss:   0.000839 => Gls Tokens per Sec:     1102 || Batch Translation Loss:   0.030775 => Txt Tokens per Sec:     3016 || Lr: 0.000100
2024-02-06 04:01:47,936 Epoch 553: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.95 
2024-02-06 04:01:47,936 EPOCH 554
2024-02-06 04:01:58,716 Epoch 554: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.93 
2024-02-06 04:01:58,717 EPOCH 555
2024-02-06 04:02:09,193 Epoch 555: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.79 
2024-02-06 04:02:09,193 EPOCH 556
2024-02-06 04:02:20,217 Epoch 556: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.82 
2024-02-06 04:02:20,218 EPOCH 557
2024-02-06 04:02:30,981 Epoch 557: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.76 
2024-02-06 04:02:30,982 EPOCH 558
2024-02-06 04:02:41,435 Epoch 558: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.69 
2024-02-06 04:02:41,436 EPOCH 559
2024-02-06 04:02:49,158 [Epoch: 559 Step: 00009500] Batch Recognition Loss:   0.000661 => Gls Tokens per Sec:     1160 || Batch Translation Loss:   0.017574 => Txt Tokens per Sec:     3160 || Lr: 0.000100
2024-02-06 04:02:52,167 Epoch 559: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.72 
2024-02-06 04:02:52,167 EPOCH 560
2024-02-06 04:03:02,797 Epoch 560: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-06 04:03:02,797 EPOCH 561
2024-02-06 04:03:13,513 Epoch 561: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.64 
2024-02-06 04:03:13,514 EPOCH 562
2024-02-06 04:03:24,197 Epoch 562: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-06 04:03:24,198 EPOCH 563
2024-02-06 04:03:34,906 Epoch 563: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-06 04:03:34,907 EPOCH 564
2024-02-06 04:03:45,652 Epoch 564: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.94 
2024-02-06 04:03:45,653 EPOCH 565
2024-02-06 04:03:51,448 [Epoch: 565 Step: 00009600] Batch Recognition Loss:   0.000321 => Gls Tokens per Sec:     1326 || Batch Translation Loss:   0.076142 => Txt Tokens per Sec:     3663 || Lr: 0.000100
2024-02-06 04:03:56,245 Epoch 565: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.13 
2024-02-06 04:03:56,246 EPOCH 566
2024-02-06 04:04:06,978 Epoch 566: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.35 
2024-02-06 04:04:06,978 EPOCH 567
2024-02-06 04:04:17,594 Epoch 567: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.72 
2024-02-06 04:04:17,595 EPOCH 568
2024-02-06 04:04:28,333 Epoch 568: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.89 
2024-02-06 04:04:28,334 EPOCH 569
2024-02-06 04:04:39,089 Epoch 569: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.24 
2024-02-06 04:04:39,090 EPOCH 570
2024-02-06 04:04:49,777 Epoch 570: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.14 
2024-02-06 04:04:49,778 EPOCH 571
2024-02-06 04:04:55,372 [Epoch: 571 Step: 00009700] Batch Recognition Loss:   0.000303 => Gls Tokens per Sec:     1145 || Batch Translation Loss:   0.208302 => Txt Tokens per Sec:     3233 || Lr: 0.000100
2024-02-06 04:05:00,343 Epoch 571: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.90 
2024-02-06 04:05:00,343 EPOCH 572
2024-02-06 04:05:10,928 Epoch 572: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.74 
2024-02-06 04:05:10,929 EPOCH 573
2024-02-06 04:05:21,623 Epoch 573: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.65 
2024-02-06 04:05:21,624 EPOCH 574
2024-02-06 04:05:32,611 Epoch 574: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.32 
2024-02-06 04:05:32,612 EPOCH 575
2024-02-06 04:05:43,348 Epoch 575: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.30 
2024-02-06 04:05:43,349 EPOCH 576
2024-02-06 04:05:54,297 Epoch 576: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.45 
2024-02-06 04:05:54,298 EPOCH 577
2024-02-06 04:05:56,361 [Epoch: 577 Step: 00009800] Batch Recognition Loss:   0.000716 => Gls Tokens per Sec:     2483 || Batch Translation Loss:   0.068694 => Txt Tokens per Sec:     6954 || Lr: 0.000100
2024-02-06 04:06:05,006 Epoch 577: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.45 
2024-02-06 04:06:05,007 EPOCH 578
2024-02-06 04:06:15,834 Epoch 578: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.33 
2024-02-06 04:06:15,834 EPOCH 579
2024-02-06 04:06:26,520 Epoch 579: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.53 
2024-02-06 04:06:26,521 EPOCH 580
2024-02-06 04:06:37,053 Epoch 580: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.70 
2024-02-06 04:06:37,054 EPOCH 581
2024-02-06 04:06:47,674 Epoch 581: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.65 
2024-02-06 04:06:47,674 EPOCH 582
2024-02-06 04:06:58,359 Epoch 582: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.59 
2024-02-06 04:06:58,360 EPOCH 583
2024-02-06 04:07:02,528 [Epoch: 583 Step: 00009900] Batch Recognition Loss:   0.000738 => Gls Tokens per Sec:      922 || Batch Translation Loss:   0.073380 => Txt Tokens per Sec:     2697 || Lr: 0.000100
2024-02-06 04:07:08,941 Epoch 583: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.48 
2024-02-06 04:07:08,941 EPOCH 584
2024-02-06 04:07:19,698 Epoch 584: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.26 
2024-02-06 04:07:19,699 EPOCH 585
2024-02-06 04:07:30,406 Epoch 585: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.80 
2024-02-06 04:07:30,406 EPOCH 586
2024-02-06 04:07:41,035 Epoch 586: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.66 
2024-02-06 04:07:41,035 EPOCH 587
2024-02-06 04:07:51,946 Epoch 587: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.66 
2024-02-06 04:07:51,947 EPOCH 588
2024-02-06 04:08:02,878 Epoch 588: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.59 
2024-02-06 04:08:02,878 EPOCH 589
2024-02-06 04:08:05,284 [Epoch: 589 Step: 00010000] Batch Recognition Loss:   0.000509 => Gls Tokens per Sec:     1065 || Batch Translation Loss:   0.030087 => Txt Tokens per Sec:     3157 || Lr: 0.000100
2024-02-06 04:08:46,170 Hooray! New best validation result [eval_metric]!
2024-02-06 04:08:46,171 Saving new checkpoint.
2024-02-06 04:08:46,480 Validation result at epoch 589, step    10000: duration: 41.1959s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.81317	Translation Loss: 91838.87500	PPL: 9632.32715
	Eval Metric: BLEU
	WER 4.38	(DEL: 0.00,	INS: 0.00,	SUB: 4.38)
	BLEU-4 1.00	(BLEU-1: 11.60,	BLEU-2: 3.96,	BLEU-3: 1.80,	BLEU-4: 1.00)
	CHRF 17.36	ROUGE 9.91
2024-02-06 04:08:46,482 Logging Recognition and Translation Outputs
2024-02-06 04:08:46,482 ========================================================================================================================
2024-02-06 04:08:46,482 Logging Sequence: 123_147.00
2024-02-06 04:08:46,483 	Gloss Reference :	A B+C+D+E
2024-02-06 04:08:46,483 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 04:08:46,483 	Gloss Alignment :	         
2024-02-06 04:08:46,483 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 04:08:46,484 	Text Reference  :	the former captain also owns the pontiac firebird trans am  car worth    rs         68 lakh    
2024-02-06 04:08:46,485 	Text Hypothesis :	*** ****** ******* **** **** *** ******* ******** dhoni has a   stunning collection of vehicles
2024-02-06 04:08:46,485 	Text Alignment  :	D   D      D       D    D    D   D       D        S     S   S   S        S          S  S       
2024-02-06 04:08:46,485 ========================================================================================================================
2024-02-06 04:08:46,485 Logging Sequence: 58_196.00
2024-02-06 04:08:46,485 	Gloss Reference :	A B+C+D+E
2024-02-06 04:08:46,485 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 04:08:46,486 	Gloss Alignment :	         
2024-02-06 04:08:46,486 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 04:08:46,487 	Text Reference  :	** the talents and  skills of our  athletes knows   no     bounds
2024-02-06 04:08:46,487 	Text Hypothesis :	so we  have    been set    to know in       olympic medals wow   
2024-02-06 04:08:46,488 	Text Alignment  :	I  S   S       S    S      S  S    S        S       S      S     
2024-02-06 04:08:46,488 ========================================================================================================================
2024-02-06 04:08:46,488 Logging Sequence: 168_184.00
2024-02-06 04:08:46,488 	Gloss Reference :	A B+C+D+E
2024-02-06 04:08:46,488 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 04:08:46,488 	Gloss Alignment :	         
2024-02-06 04:08:46,488 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 04:08:46,490 	Text Reference  :	people say that we  may  get     a  true glimpse of vamika in february 2022 when she turns  1    year          old    
2024-02-06 04:08:46,490 	Text Hypothesis :	****** who is   now been brought to bag  because of ****** ** ******** **** csk  to  retire from international matches
2024-02-06 04:08:46,490 	Text Alignment  :	D      S   S    S   S    S       S  S    S          D      D  D        D    S    S   S      S    S             S      
2024-02-06 04:08:46,491 ========================================================================================================================
2024-02-06 04:08:46,491 Logging Sequence: 87_123.00
2024-02-06 04:08:46,491 	Gloss Reference :	A B+C+D+E
2024-02-06 04:08:46,491 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 04:08:46,491 	Gloss Alignment :	         
2024-02-06 04:08:46,491 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 04:08:46,493 	Text Reference  :	****** *** ***** he  said that  he  hoped    kl   rahul would      be  fit for  the upcoming world   cup   
2024-02-06 04:08:46,493 	Text Hypothesis :	during the world cup 2022 india and pakistan have faced footballer due to  play the ******** british empire
2024-02-06 04:08:46,493 	Text Alignment  :	I      I   I     S   S    S     S   S        S    S     S          S   S   S        D        S       S     
2024-02-06 04:08:46,493 ========================================================================================================================
2024-02-06 04:08:46,494 Logging Sequence: 144_154.00
2024-02-06 04:08:46,494 	Gloss Reference :	A B+C+D+E
2024-02-06 04:08:46,494 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 04:08:46,494 	Gloss Alignment :	         
2024-02-06 04:08:46,494 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 04:08:46,496 	Text Reference  :	****** **** ****** she also participated in the rural  olympic games organised in   rajasthan a few     months 
2024-02-06 04:08:46,496 	Text Hypothesis :	people were amazed by  her  way          in *** india' world   cup   for       this was       a similar message
2024-02-06 04:08:46,496 	Text Alignment  :	I      I    I      S   S    S               D   S      S       S     S         S    S           S       S      
2024-02-06 04:08:46,496 ========================================================================================================================
2024-02-06 04:08:55,133 Epoch 589: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.61 
2024-02-06 04:08:55,133 EPOCH 590
2024-02-06 04:09:05,835 Epoch 590: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.62 
2024-02-06 04:09:05,835 EPOCH 591
2024-02-06 04:09:16,619 Epoch 591: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.65 
2024-02-06 04:09:16,620 EPOCH 592
2024-02-06 04:09:27,637 Epoch 592: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.78 
2024-02-06 04:09:27,637 EPOCH 593
2024-02-06 04:09:38,169 Epoch 593: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.80 
2024-02-06 04:09:38,169 EPOCH 594
2024-02-06 04:09:49,096 Epoch 594: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.91 
2024-02-06 04:09:49,097 EPOCH 595
2024-02-06 04:09:49,571 [Epoch: 595 Step: 00010100] Batch Recognition Loss:   0.000302 => Gls Tokens per Sec:     2706 || Batch Translation Loss:   0.062708 => Txt Tokens per Sec:     7617 || Lr: 0.000100
2024-02-06 04:09:59,829 Epoch 595: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.18 
2024-02-06 04:09:59,830 EPOCH 596
2024-02-06 04:10:10,430 Epoch 596: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.32 
2024-02-06 04:10:10,431 EPOCH 597
2024-02-06 04:10:21,169 Epoch 597: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.74 
2024-02-06 04:10:21,169 EPOCH 598
2024-02-06 04:10:31,891 Epoch 598: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.56 
2024-02-06 04:10:31,891 EPOCH 599
2024-02-06 04:10:42,372 Epoch 599: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.08 
2024-02-06 04:10:42,372 EPOCH 600
2024-02-06 04:10:53,141 [Epoch: 600 Step: 00010200] Batch Recognition Loss:   0.000646 => Gls Tokens per Sec:      986 || Batch Translation Loss:   0.054610 => Txt Tokens per Sec:     2729 || Lr: 0.000100
2024-02-06 04:10:53,142 Epoch 600: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.97 
2024-02-06 04:10:53,142 EPOCH 601
2024-02-06 04:11:03,923 Epoch 601: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.99 
2024-02-06 04:11:03,924 EPOCH 602
2024-02-06 04:11:14,391 Epoch 602: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.88 
2024-02-06 04:11:14,391 EPOCH 603
2024-02-06 04:11:25,035 Epoch 603: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.06 
2024-02-06 04:11:25,036 EPOCH 604
2024-02-06 04:11:35,641 Epoch 604: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.08 
2024-02-06 04:11:35,641 EPOCH 605
2024-02-06 04:11:46,357 Epoch 605: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.21 
2024-02-06 04:11:46,358 EPOCH 606
2024-02-06 04:11:55,086 [Epoch: 606 Step: 00010300] Batch Recognition Loss:   0.000492 => Gls Tokens per Sec:     1070 || Batch Translation Loss:   0.054338 => Txt Tokens per Sec:     2966 || Lr: 0.000100
2024-02-06 04:11:57,010 Epoch 606: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.29 
2024-02-06 04:11:57,010 EPOCH 607
2024-02-06 04:12:07,860 Epoch 607: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.50 
2024-02-06 04:12:07,861 EPOCH 608
2024-02-06 04:12:18,641 Epoch 608: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.27 
2024-02-06 04:12:18,642 EPOCH 609
2024-02-06 04:12:29,330 Epoch 609: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.13 
2024-02-06 04:12:29,330 EPOCH 610
2024-02-06 04:12:40,114 Epoch 610: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.22 
2024-02-06 04:12:40,115 EPOCH 611
2024-02-06 04:12:50,781 Epoch 611: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.11 
2024-02-06 04:12:50,782 EPOCH 612
2024-02-06 04:12:58,991 [Epoch: 612 Step: 00010400] Batch Recognition Loss:   0.000292 => Gls Tokens per Sec:      982 || Batch Translation Loss:   0.110502 => Txt Tokens per Sec:     2703 || Lr: 0.000100
2024-02-06 04:13:01,685 Epoch 612: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.94 
2024-02-06 04:13:01,685 EPOCH 613
2024-02-06 04:13:12,651 Epoch 613: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.92 
2024-02-06 04:13:12,651 EPOCH 614
2024-02-06 04:13:23,340 Epoch 614: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.85 
2024-02-06 04:13:23,341 EPOCH 615
2024-02-06 04:13:34,287 Epoch 615: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.95 
2024-02-06 04:13:34,288 EPOCH 616
2024-02-06 04:13:45,261 Epoch 616: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.71 
2024-02-06 04:13:45,262 EPOCH 617
2024-02-06 04:13:55,726 Epoch 617: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.66 
2024-02-06 04:13:55,727 EPOCH 618
2024-02-06 04:14:03,597 [Epoch: 618 Step: 00010500] Batch Recognition Loss:   0.000652 => Gls Tokens per Sec:      862 || Batch Translation Loss:   0.022055 => Txt Tokens per Sec:     2375 || Lr: 0.000100
2024-02-06 04:14:06,613 Epoch 618: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-06 04:14:06,614 EPOCH 619
2024-02-06 04:14:17,374 Epoch 619: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.63 
2024-02-06 04:14:17,375 EPOCH 620
2024-02-06 04:14:28,219 Epoch 620: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-06 04:14:28,220 EPOCH 621
2024-02-06 04:14:38,935 Epoch 621: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.67 
2024-02-06 04:14:38,936 EPOCH 622
2024-02-06 04:14:49,720 Epoch 622: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.70 
2024-02-06 04:14:49,720 EPOCH 623
2024-02-06 04:15:00,447 Epoch 623: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.00 
2024-02-06 04:15:00,447 EPOCH 624
2024-02-06 04:15:04,049 [Epoch: 624 Step: 00010600] Batch Recognition Loss:   0.001005 => Gls Tokens per Sec:     1600 || Batch Translation Loss:   0.075422 => Txt Tokens per Sec:     4186 || Lr: 0.000100
2024-02-06 04:15:11,063 Epoch 624: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.12 
2024-02-06 04:15:11,064 EPOCH 625
2024-02-06 04:15:21,900 Epoch 625: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.09 
2024-02-06 04:15:21,901 EPOCH 626
2024-02-06 04:15:32,687 Epoch 626: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.46 
2024-02-06 04:15:32,687 EPOCH 627
2024-02-06 04:15:43,283 Epoch 627: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.38 
2024-02-06 04:15:43,284 EPOCH 628
2024-02-06 04:15:53,498 Epoch 628: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.71 
2024-02-06 04:15:53,499 EPOCH 629
2024-02-06 04:16:04,345 Epoch 629: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.68 
2024-02-06 04:16:04,345 EPOCH 630
2024-02-06 04:16:08,936 [Epoch: 630 Step: 00010700] Batch Recognition Loss:   0.000519 => Gls Tokens per Sec:      976 || Batch Translation Loss:   0.290655 => Txt Tokens per Sec:     2799 || Lr: 0.000100
2024-02-06 04:16:15,148 Epoch 630: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.23 
2024-02-06 04:16:15,149 EPOCH 631
2024-02-06 04:16:26,015 Epoch 631: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.45 
2024-02-06 04:16:26,016 EPOCH 632
2024-02-06 04:16:36,689 Epoch 632: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.46 
2024-02-06 04:16:36,689 EPOCH 633
2024-02-06 04:16:47,665 Epoch 633: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.24 
2024-02-06 04:16:47,665 EPOCH 634
2024-02-06 04:16:58,280 Epoch 634: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.30 
2024-02-06 04:16:58,281 EPOCH 635
2024-02-06 04:17:08,916 Epoch 635: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.38 
2024-02-06 04:17:08,917 EPOCH 636
2024-02-06 04:17:10,092 [Epoch: 636 Step: 00010800] Batch Recognition Loss:   0.001517 => Gls Tokens per Sec:     2724 || Batch Translation Loss:   0.066314 => Txt Tokens per Sec:     7544 || Lr: 0.000100
2024-02-06 04:17:19,504 Epoch 636: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.70 
2024-02-06 04:17:19,504 EPOCH 637
2024-02-06 04:17:30,346 Epoch 637: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.90 
2024-02-06 04:17:30,347 EPOCH 638
2024-02-06 04:17:41,130 Epoch 638: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.52 
2024-02-06 04:17:41,131 EPOCH 639
2024-02-06 04:17:51,922 Epoch 639: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.96 
2024-02-06 04:17:51,923 EPOCH 640
2024-02-06 04:18:02,580 Epoch 640: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.37 
2024-02-06 04:18:02,581 EPOCH 641
2024-02-06 04:18:13,319 Epoch 641: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.42 
2024-02-06 04:18:13,319 EPOCH 642
2024-02-06 04:18:15,425 [Epoch: 642 Step: 00010900] Batch Recognition Loss:   0.000794 => Gls Tokens per Sec:      912 || Batch Translation Loss:   0.358549 => Txt Tokens per Sec:     2732 || Lr: 0.000100
2024-02-06 04:18:24,088 Epoch 642: Total Training Recognition Loss 0.05  Total Training Translation Loss 7.90 
2024-02-06 04:18:24,089 EPOCH 643
2024-02-06 04:18:35,016 Epoch 643: Total Training Recognition Loss 0.07  Total Training Translation Loss 5.47 
2024-02-06 04:18:35,017 EPOCH 644
2024-02-06 04:18:45,736 Epoch 644: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.91 
2024-02-06 04:18:45,737 EPOCH 645
2024-02-06 04:18:56,589 Epoch 645: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.33 
2024-02-06 04:18:56,590 EPOCH 646
2024-02-06 04:19:07,521 Epoch 646: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.45 
2024-02-06 04:19:07,522 EPOCH 647
2024-02-06 04:19:18,397 Epoch 647: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.08 
2024-02-06 04:19:18,398 EPOCH 648
2024-02-06 04:19:18,634 [Epoch: 648 Step: 00011000] Batch Recognition Loss:   0.000708 => Gls Tokens per Sec:     2735 || Batch Translation Loss:   0.072069 => Txt Tokens per Sec:     7398 || Lr: 0.000100
2024-02-06 04:19:29,142 Epoch 648: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.94 
2024-02-06 04:19:29,143 EPOCH 649
2024-02-06 04:19:39,807 Epoch 649: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.84 
2024-02-06 04:19:39,808 EPOCH 650
2024-02-06 04:19:50,610 Epoch 650: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.84 
2024-02-06 04:19:50,610 EPOCH 651
2024-02-06 04:20:01,345 Epoch 651: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.72 
2024-02-06 04:20:01,345 EPOCH 652
2024-02-06 04:20:12,323 Epoch 652: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.68 
2024-02-06 04:20:12,323 EPOCH 653
2024-02-06 04:20:22,740 [Epoch: 653 Step: 00011100] Batch Recognition Loss:   0.000620 => Gls Tokens per Sec:      958 || Batch Translation Loss:   0.049630 => Txt Tokens per Sec:     2626 || Lr: 0.000100
2024-02-06 04:20:23,104 Epoch 653: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.59 
2024-02-06 04:20:23,104 EPOCH 654
2024-02-06 04:20:33,746 Epoch 654: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.62 
2024-02-06 04:20:33,747 EPOCH 655
2024-02-06 04:20:44,352 Epoch 655: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.60 
2024-02-06 04:20:44,352 EPOCH 656
2024-02-06 04:20:55,329 Epoch 656: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.58 
2024-02-06 04:20:55,330 EPOCH 657
2024-02-06 04:21:06,236 Epoch 657: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.64 
2024-02-06 04:21:06,236 EPOCH 658
2024-02-06 04:21:16,845 Epoch 658: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.78 
2024-02-06 04:21:16,846 EPOCH 659
2024-02-06 04:21:26,813 [Epoch: 659 Step: 00011200] Batch Recognition Loss:   0.000978 => Gls Tokens per Sec:      873 || Batch Translation Loss:   0.026659 => Txt Tokens per Sec:     2372 || Lr: 0.000100
2024-02-06 04:21:27,639 Epoch 659: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.69 
2024-02-06 04:21:27,639 EPOCH 660
2024-02-06 04:21:38,324 Epoch 660: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.60 
2024-02-06 04:21:38,325 EPOCH 661
2024-02-06 04:21:48,914 Epoch 661: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.57 
2024-02-06 04:21:48,914 EPOCH 662
2024-02-06 04:21:59,671 Epoch 662: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.73 
2024-02-06 04:21:59,672 EPOCH 663
2024-02-06 04:22:10,410 Epoch 663: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.00 
2024-02-06 04:22:10,411 EPOCH 664
2024-02-06 04:22:20,993 Epoch 664: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.10 
2024-02-06 04:22:20,994 EPOCH 665
2024-02-06 04:22:26,315 [Epoch: 665 Step: 00011300] Batch Recognition Loss:   0.000675 => Gls Tokens per Sec:     1444 || Batch Translation Loss:   0.119060 => Txt Tokens per Sec:     3896 || Lr: 0.000100
2024-02-06 04:22:31,345 Epoch 665: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.30 
2024-02-06 04:22:31,345 EPOCH 666
2024-02-06 04:22:42,138 Epoch 666: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.91 
2024-02-06 04:22:42,139 EPOCH 667
2024-02-06 04:22:53,101 Epoch 667: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.79 
2024-02-06 04:22:53,102 EPOCH 668
2024-02-06 04:23:03,864 Epoch 668: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.65 
2024-02-06 04:23:03,865 EPOCH 669
2024-02-06 04:23:14,855 Epoch 669: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-06 04:23:14,856 EPOCH 670
2024-02-06 04:23:25,662 Epoch 670: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-06 04:23:25,663 EPOCH 671
2024-02-06 04:23:31,426 [Epoch: 671 Step: 00011400] Batch Recognition Loss:   0.000375 => Gls Tokens per Sec:     1066 || Batch Translation Loss:   0.021933 => Txt Tokens per Sec:     2823 || Lr: 0.000100
2024-02-06 04:23:36,399 Epoch 671: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-06 04:23:36,400 EPOCH 672
2024-02-06 04:23:47,138 Epoch 672: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-06 04:23:47,139 EPOCH 673
2024-02-06 04:23:57,722 Epoch 673: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-06 04:23:57,723 EPOCH 674
2024-02-06 04:24:08,392 Epoch 674: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-06 04:24:08,392 EPOCH 675
2024-02-06 04:24:19,026 Epoch 675: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.61 
2024-02-06 04:24:19,027 EPOCH 676
2024-02-06 04:24:29,837 Epoch 676: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.77 
2024-02-06 04:24:29,838 EPOCH 677
2024-02-06 04:24:34,576 [Epoch: 677 Step: 00011500] Batch Recognition Loss:   0.002000 => Gls Tokens per Sec:     1081 || Batch Translation Loss:   0.043501 => Txt Tokens per Sec:     2863 || Lr: 0.000100
2024-02-06 04:24:40,439 Epoch 677: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.87 
2024-02-06 04:24:40,440 EPOCH 678
2024-02-06 04:24:51,344 Epoch 678: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.12 
2024-02-06 04:24:51,345 EPOCH 679
2024-02-06 04:25:02,015 Epoch 679: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.07 
2024-02-06 04:25:02,016 EPOCH 680
2024-02-06 04:25:12,893 Epoch 680: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.12 
2024-02-06 04:25:12,894 EPOCH 681
2024-02-06 04:25:23,778 Epoch 681: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.86 
2024-02-06 04:25:23,779 EPOCH 682
2024-02-06 04:25:34,438 Epoch 682: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.72 
2024-02-06 04:25:34,439 EPOCH 683
2024-02-06 04:25:39,589 [Epoch: 683 Step: 00011600] Batch Recognition Loss:   0.001595 => Gls Tokens per Sec:      695 || Batch Translation Loss:   0.043649 => Txt Tokens per Sec:     1953 || Lr: 0.000100
2024-02-06 04:25:45,290 Epoch 683: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.65 
2024-02-06 04:25:45,291 EPOCH 684
2024-02-06 04:25:55,880 Epoch 684: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-06 04:25:55,880 EPOCH 685
2024-02-06 04:26:06,716 Epoch 685: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-06 04:26:06,717 EPOCH 686
2024-02-06 04:26:17,376 Epoch 686: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.70 
2024-02-06 04:26:17,376 EPOCH 687
2024-02-06 04:26:28,160 Epoch 687: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.97 
2024-02-06 04:26:28,160 EPOCH 688
2024-02-06 04:26:39,005 Epoch 688: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.32 
2024-02-06 04:26:39,006 EPOCH 689
2024-02-06 04:26:39,773 [Epoch: 689 Step: 00011700] Batch Recognition Loss:   0.000757 => Gls Tokens per Sec:     3341 || Batch Translation Loss:   0.034834 => Txt Tokens per Sec:     8967 || Lr: 0.000100
2024-02-06 04:26:49,476 Epoch 689: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.86 
2024-02-06 04:26:49,476 EPOCH 690
2024-02-06 04:27:00,299 Epoch 690: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.93 
2024-02-06 04:27:00,299 EPOCH 691
2024-02-06 04:27:11,127 Epoch 691: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.07 
2024-02-06 04:27:11,127 EPOCH 692
2024-02-06 04:27:21,827 Epoch 692: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.11 
2024-02-06 04:27:21,827 EPOCH 693
2024-02-06 04:27:32,589 Epoch 693: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.56 
2024-02-06 04:27:32,589 EPOCH 694
2024-02-06 04:27:43,598 Epoch 694: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.27 
2024-02-06 04:27:43,598 EPOCH 695
2024-02-06 04:27:44,188 [Epoch: 695 Step: 00011800] Batch Recognition Loss:   0.000546 => Gls Tokens per Sec:     2181 || Batch Translation Loss:   0.046202 => Txt Tokens per Sec:     6286 || Lr: 0.000100
2024-02-06 04:27:54,279 Epoch 695: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.38 
2024-02-06 04:27:54,279 EPOCH 696
2024-02-06 04:28:04,575 Epoch 696: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.83 
2024-02-06 04:28:04,575 EPOCH 697
2024-02-06 04:28:15,307 Epoch 697: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.66 
2024-02-06 04:28:15,308 EPOCH 698
2024-02-06 04:28:25,994 Epoch 698: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.42 
2024-02-06 04:28:25,995 EPOCH 699
2024-02-06 04:28:36,637 Epoch 699: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.44 
2024-02-06 04:28:36,637 EPOCH 700
2024-02-06 04:28:47,481 [Epoch: 700 Step: 00011900] Batch Recognition Loss:   0.001221 => Gls Tokens per Sec:      979 || Batch Translation Loss:   0.114504 => Txt Tokens per Sec:     2710 || Lr: 0.000100
2024-02-06 04:28:47,482 Epoch 700: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.03 
2024-02-06 04:28:47,482 EPOCH 701
2024-02-06 04:28:58,046 Epoch 701: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.95 
2024-02-06 04:28:58,047 EPOCH 702
2024-02-06 04:29:08,628 Epoch 702: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.18 
2024-02-06 04:29:08,629 EPOCH 703
2024-02-06 04:29:19,290 Epoch 703: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.26 
2024-02-06 04:29:19,291 EPOCH 704
2024-02-06 04:29:29,866 Epoch 704: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.48 
2024-02-06 04:29:29,867 EPOCH 705
2024-02-06 04:29:40,790 Epoch 705: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.59 
2024-02-06 04:29:40,791 EPOCH 706
2024-02-06 04:29:51,447 [Epoch: 706 Step: 00012000] Batch Recognition Loss:   0.001074 => Gls Tokens per Sec:      877 || Batch Translation Loss:   0.054978 => Txt Tokens per Sec:     2466 || Lr: 0.000100
2024-02-06 04:30:31,928 Validation result at epoch 706, step    12000: duration: 40.4801s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.94851	Translation Loss: 91750.09375	PPL: 9547.28320
	Eval Metric: BLEU
	WER 4.59	(DEL: 0.00,	INS: 0.00,	SUB: 4.59)
	BLEU-4 0.49	(BLEU-1: 10.66,	BLEU-2: 3.33,	BLEU-3: 1.26,	BLEU-4: 0.49)
	CHRF 16.76	ROUGE 9.14
2024-02-06 04:30:31,929 Logging Recognition and Translation Outputs
2024-02-06 04:30:31,929 ========================================================================================================================
2024-02-06 04:30:31,930 Logging Sequence: 168_56.00
2024-02-06 04:30:31,930 	Gloss Reference :	A B+C+D+E
2024-02-06 04:30:31,930 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 04:30:31,930 	Gloss Alignment :	         
2024-02-06 04:30:31,930 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 04:30:31,931 	Text Reference  :	fans have    been waiting to          see vamika for      a  long time 
2024-02-06 04:30:31,931 	Text Hypothesis :	**** sameeha is   amazing performance the most   followed by 8    years
2024-02-06 04:30:31,932 	Text Alignment  :	D    S       S    S       S           S   S      S        S  S    S    
2024-02-06 04:30:31,932 ========================================================================================================================
2024-02-06 04:30:31,932 Logging Sequence: 161_74.00
2024-02-06 04:30:31,932 	Gloss Reference :	A B+C+D+E
2024-02-06 04:30:31,932 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 04:30:31,932 	Gloss Alignment :	         
2024-02-06 04:30:31,933 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 04:30:31,933 	Text Reference  :	*** ****** **** **** i   am    proud of  the    indian  team's achievements
2024-02-06 04:30:31,934 	Text Hypothesis :	the second time they are angry at    sky sports stadium in     mumbai      
2024-02-06 04:30:31,934 	Text Alignment  :	I   I      I    I    S   S     S     S   S      S       S      S           
2024-02-06 04:30:31,934 ========================================================================================================================
2024-02-06 04:30:31,934 Logging Sequence: 111_83.00
2024-02-06 04:30:31,934 	Gloss Reference :	A B+C+D+E
2024-02-06 04:30:31,934 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 04:30:31,934 	Gloss Alignment :	         
2024-02-06 04:30:31,935 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 04:30:31,937 	Text Reference  :	and the other 10        team members    are fined 25  of the match ** *** ******* *** ** **** **** fee or   rs   6   lakh
2024-02-06 04:30:31,937 	Text Hypothesis :	*** *** ***** over-rate is   calculated at  the   end of the match by the umpires and if they find the over rate was slow
2024-02-06 04:30:31,937 	Text Alignment  :	D   D   D     S         S    S          S   S     S                I  I   I       I   I  I    I    S   S    S    S   S   
2024-02-06 04:30:31,937 ========================================================================================================================
2024-02-06 04:30:31,937 Logging Sequence: 61_218.00
2024-02-06 04:30:31,937 	Gloss Reference :	A B+C+D+E
2024-02-06 04:30:31,938 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 04:30:31,938 	Gloss Alignment :	         
2024-02-06 04:30:31,938 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 04:30:31,939 	Text Reference  :	in 2020 a    woman had said at     the press conference
2024-02-06 04:30:31,939 	Text Hypothesis :	in **** 2019 shaw  has been become a   very  well      
2024-02-06 04:30:31,939 	Text Alignment  :	   D    S    S     S   S    S      S   S     S         
2024-02-06 04:30:31,939 ========================================================================================================================
2024-02-06 04:30:31,939 Logging Sequence: 94_123.00
2024-02-06 04:30:31,940 	Gloss Reference :	A B+C+D+E
2024-02-06 04:30:31,940 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 04:30:31,940 	Gloss Alignment :	         
2024-02-06 04:30:31,940 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 04:30:31,942 	Text Reference  :	*** the venue narendra modi stadium for the india-pakistan match  has been   kept the ******** *** *** same    people can book flights etc  
2024-02-06 04:30:31,942 	Text Hypothesis :	for the ***** ******** one  day     of  the series         should be  played at   the stadiums and not majorly over   it  does not     known
2024-02-06 04:30:31,942 	Text Alignment  :	I       D     D        S    S       S       S              S      S   S      S        I        I   I   S       S      S   S    S       S    
2024-02-06 04:30:31,943 ========================================================================================================================
2024-02-06 04:30:32,416 Epoch 706: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.40 
2024-02-06 04:30:32,417 EPOCH 707
2024-02-06 04:30:43,667 Epoch 707: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.68 
2024-02-06 04:30:43,667 EPOCH 708
2024-02-06 04:30:54,418 Epoch 708: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.43 
2024-02-06 04:30:54,419 EPOCH 709
2024-02-06 04:31:05,291 Epoch 709: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.30 
2024-02-06 04:31:05,292 EPOCH 710
2024-02-06 04:31:16,149 Epoch 710: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.16 
2024-02-06 04:31:16,149 EPOCH 711
2024-02-06 04:31:27,084 Epoch 711: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.15 
2024-02-06 04:31:27,084 EPOCH 712
2024-02-06 04:31:37,017 [Epoch: 712 Step: 00012100] Batch Recognition Loss:   0.000769 => Gls Tokens per Sec:      812 || Batch Translation Loss:   0.077024 => Txt Tokens per Sec:     2260 || Lr: 0.000100
2024-02-06 04:31:38,055 Epoch 712: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.85 
2024-02-06 04:31:38,055 EPOCH 713
2024-02-06 04:31:48,738 Epoch 713: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.95 
2024-02-06 04:31:48,739 EPOCH 714
2024-02-06 04:31:59,794 Epoch 714: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.90 
2024-02-06 04:31:59,795 EPOCH 715
2024-02-06 04:32:10,562 Epoch 715: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.81 
2024-02-06 04:32:10,563 EPOCH 716
2024-02-06 04:32:21,509 Epoch 716: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.85 
2024-02-06 04:32:21,510 EPOCH 717
2024-02-06 04:32:32,400 Epoch 717: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.03 
2024-02-06 04:32:32,400 EPOCH 718
2024-02-06 04:32:40,323 [Epoch: 718 Step: 00012200] Batch Recognition Loss:   0.000452 => Gls Tokens per Sec:      856 || Batch Translation Loss:   0.027098 => Txt Tokens per Sec:     2477 || Lr: 0.000100
2024-02-06 04:32:43,062 Epoch 718: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.85 
2024-02-06 04:32:43,063 EPOCH 719
2024-02-06 04:32:53,914 Epoch 719: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.84 
2024-02-06 04:32:53,915 EPOCH 720
2024-02-06 04:33:04,634 Epoch 720: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-06 04:33:04,635 EPOCH 721
2024-02-06 04:33:15,328 Epoch 721: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.70 
2024-02-06 04:33:15,329 EPOCH 722
2024-02-06 04:33:26,100 Epoch 722: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.67 
2024-02-06 04:33:26,100 EPOCH 723
2024-02-06 04:33:36,714 Epoch 723: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.60 
2024-02-06 04:33:36,714 EPOCH 724
2024-02-06 04:33:40,469 [Epoch: 724 Step: 00012300] Batch Recognition Loss:   0.000876 => Gls Tokens per Sec:     1535 || Batch Translation Loss:   0.010216 => Txt Tokens per Sec:     4165 || Lr: 0.000100
2024-02-06 04:33:47,430 Epoch 724: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.53 
2024-02-06 04:33:47,431 EPOCH 725
2024-02-06 04:33:58,256 Epoch 725: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-06 04:33:58,257 EPOCH 726
2024-02-06 04:34:09,274 Epoch 726: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-06 04:34:09,274 EPOCH 727
2024-02-06 04:34:19,930 Epoch 727: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-06 04:34:19,931 EPOCH 728
2024-02-06 04:34:30,484 Epoch 728: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.39 
2024-02-06 04:34:30,484 EPOCH 729
2024-02-06 04:34:41,441 Epoch 729: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-06 04:34:41,441 EPOCH 730
2024-02-06 04:34:44,253 [Epoch: 730 Step: 00012400] Batch Recognition Loss:   0.003165 => Gls Tokens per Sec:     1594 || Batch Translation Loss:   0.018688 => Txt Tokens per Sec:     4150 || Lr: 0.000100
2024-02-06 04:34:52,077 Epoch 730: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.32 
2024-02-06 04:34:52,077 EPOCH 731
2024-02-06 04:35:02,849 Epoch 731: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-06 04:35:02,849 EPOCH 732
2024-02-06 04:35:13,489 Epoch 732: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.41 
2024-02-06 04:35:13,490 EPOCH 733
2024-02-06 04:35:24,273 Epoch 733: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.52 
2024-02-06 04:35:24,274 EPOCH 734
2024-02-06 04:35:34,940 Epoch 734: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.70 
2024-02-06 04:35:34,941 EPOCH 735
2024-02-06 04:35:45,529 Epoch 735: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.84 
2024-02-06 04:35:45,529 EPOCH 736
2024-02-06 04:35:50,280 [Epoch: 736 Step: 00012500] Batch Recognition Loss:   0.000410 => Gls Tokens per Sec:      619 || Batch Translation Loss:   0.099937 => Txt Tokens per Sec:     1933 || Lr: 0.000100
2024-02-06 04:35:56,145 Epoch 736: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.68 
2024-02-06 04:35:56,145 EPOCH 737
2024-02-06 04:36:07,076 Epoch 737: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.51 
2024-02-06 04:36:07,076 EPOCH 738
2024-02-06 04:36:17,986 Epoch 738: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.29 
2024-02-06 04:36:17,987 EPOCH 739
2024-02-06 04:36:28,813 Epoch 739: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.10 
2024-02-06 04:36:28,814 EPOCH 740
2024-02-06 04:36:39,539 Epoch 740: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.57 
2024-02-06 04:36:39,539 EPOCH 741
2024-02-06 04:36:50,300 Epoch 741: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.66 
2024-02-06 04:36:50,300 EPOCH 742
2024-02-06 04:36:52,618 [Epoch: 742 Step: 00012600] Batch Recognition Loss:   0.000885 => Gls Tokens per Sec:      829 || Batch Translation Loss:   0.056667 => Txt Tokens per Sec:     2145 || Lr: 0.000100
2024-02-06 04:37:00,998 Epoch 742: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.03 
2024-02-06 04:37:00,998 EPOCH 743
2024-02-06 04:37:11,699 Epoch 743: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.86 
2024-02-06 04:37:11,699 EPOCH 744
2024-02-06 04:37:22,339 Epoch 744: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.46 
2024-02-06 04:37:22,340 EPOCH 745
2024-02-06 04:37:33,104 Epoch 745: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.58 
2024-02-06 04:37:33,104 EPOCH 746
2024-02-06 04:37:43,802 Epoch 746: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.37 
2024-02-06 04:37:43,802 EPOCH 747
2024-02-06 04:37:54,513 Epoch 747: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.12 
2024-02-06 04:37:54,513 EPOCH 748
2024-02-06 04:37:54,745 [Epoch: 748 Step: 00012700] Batch Recognition Loss:   0.001087 => Gls Tokens per Sec:     2783 || Batch Translation Loss:   0.057611 => Txt Tokens per Sec:     7761 || Lr: 0.000100
2024-02-06 04:38:05,628 Epoch 748: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.00 
2024-02-06 04:38:05,629 EPOCH 749
2024-02-06 04:38:16,381 Epoch 749: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.75 
2024-02-06 04:38:16,381 EPOCH 750
2024-02-06 04:38:27,087 Epoch 750: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.69 
2024-02-06 04:38:27,087 EPOCH 751
2024-02-06 04:38:37,994 Epoch 751: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.76 
2024-02-06 04:38:37,994 EPOCH 752
2024-02-06 04:38:48,699 Epoch 752: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.81 
2024-02-06 04:38:48,699 EPOCH 753
2024-02-06 04:38:59,182 [Epoch: 753 Step: 00012800] Batch Recognition Loss:   0.000565 => Gls Tokens per Sec:      952 || Batch Translation Loss:   0.070360 => Txt Tokens per Sec:     2669 || Lr: 0.000100
2024-02-06 04:38:59,326 Epoch 753: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.28 
2024-02-06 04:38:59,326 EPOCH 754
2024-02-06 04:39:09,965 Epoch 754: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.95 
2024-02-06 04:39:09,966 EPOCH 755
2024-02-06 04:39:20,661 Epoch 755: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.16 
2024-02-06 04:39:20,662 EPOCH 756
2024-02-06 04:39:31,294 Epoch 756: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.52 
2024-02-06 04:39:31,295 EPOCH 757
2024-02-06 04:39:42,069 Epoch 757: Total Training Recognition Loss 0.05  Total Training Translation Loss 5.97 
2024-02-06 04:39:42,070 EPOCH 758
2024-02-06 04:39:52,964 Epoch 758: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.36 
2024-02-06 04:39:52,964 EPOCH 759
2024-02-06 04:40:01,493 [Epoch: 759 Step: 00012900] Batch Recognition Loss:   0.001223 => Gls Tokens per Sec:     1020 || Batch Translation Loss:   0.091142 => Txt Tokens per Sec:     2734 || Lr: 0.000100
2024-02-06 04:40:03,747 Epoch 759: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.91 
2024-02-06 04:40:03,747 EPOCH 760
2024-02-06 04:40:14,654 Epoch 760: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.28 
2024-02-06 04:40:14,655 EPOCH 761
2024-02-06 04:40:25,219 Epoch 761: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.91 
2024-02-06 04:40:25,221 EPOCH 762
2024-02-06 04:40:35,845 Epoch 762: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.84 
2024-02-06 04:40:35,845 EPOCH 763
2024-02-06 04:40:46,476 Epoch 763: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.82 
2024-02-06 04:40:46,476 EPOCH 764
2024-02-06 04:40:56,872 Epoch 764: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.92 
2024-02-06 04:40:56,872 EPOCH 765
2024-02-06 04:41:04,993 [Epoch: 765 Step: 00013000] Batch Recognition Loss:   0.000706 => Gls Tokens per Sec:      914 || Batch Translation Loss:   0.029580 => Txt Tokens per Sec:     2519 || Lr: 0.000100
2024-02-06 04:41:07,631 Epoch 765: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.74 
2024-02-06 04:41:07,631 EPOCH 766
2024-02-06 04:41:18,240 Epoch 766: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.52 
2024-02-06 04:41:18,240 EPOCH 767
2024-02-06 04:41:29,095 Epoch 767: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.57 
2024-02-06 04:41:29,095 EPOCH 768
2024-02-06 04:41:39,583 Epoch 768: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.67 
2024-02-06 04:41:39,584 EPOCH 769
2024-02-06 04:41:50,197 Epoch 769: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.43 
2024-02-06 04:41:50,198 EPOCH 770
2024-02-06 04:42:00,884 Epoch 770: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.42 
2024-02-06 04:42:00,885 EPOCH 771
2024-02-06 04:42:04,394 [Epoch: 771 Step: 00013100] Batch Recognition Loss:   0.005532 => Gls Tokens per Sec:     1825 || Batch Translation Loss:   0.009374 => Txt Tokens per Sec:     4759 || Lr: 0.000100
2024-02-06 04:42:11,625 Epoch 771: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.48 
2024-02-06 04:42:11,625 EPOCH 772
2024-02-06 04:42:22,330 Epoch 772: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-06 04:42:22,330 EPOCH 773
2024-02-06 04:42:33,023 Epoch 773: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-06 04:42:33,023 EPOCH 774
2024-02-06 04:42:43,869 Epoch 774: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.44 
2024-02-06 04:42:43,870 EPOCH 775
2024-02-06 04:42:54,594 Epoch 775: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-06 04:42:54,595 EPOCH 776
2024-02-06 04:43:05,516 Epoch 776: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-06 04:43:05,516 EPOCH 777
2024-02-06 04:43:08,914 [Epoch: 777 Step: 00013200] Batch Recognition Loss:   0.000350 => Gls Tokens per Sec:     1507 || Batch Translation Loss:   0.018202 => Txt Tokens per Sec:     4051 || Lr: 0.000100
2024-02-06 04:43:16,170 Epoch 777: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-06 04:43:16,171 EPOCH 778
2024-02-06 04:43:27,183 Epoch 778: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-06 04:43:27,184 EPOCH 779
2024-02-06 04:43:38,038 Epoch 779: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-06 04:43:38,039 EPOCH 780
2024-02-06 04:43:49,000 Epoch 780: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-06 04:43:49,000 EPOCH 781
2024-02-06 04:43:59,901 Epoch 781: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-06 04:43:59,901 EPOCH 782
2024-02-06 04:44:10,529 Epoch 782: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.94 
2024-02-06 04:44:10,529 EPOCH 783
2024-02-06 04:44:13,262 [Epoch: 783 Step: 00013300] Batch Recognition Loss:   0.001523 => Gls Tokens per Sec:     1405 || Batch Translation Loss:   0.028318 => Txt Tokens per Sec:     3924 || Lr: 0.000100
2024-02-06 04:44:21,116 Epoch 783: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.78 
2024-02-06 04:44:21,117 EPOCH 784
2024-02-06 04:44:31,987 Epoch 784: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.05 
2024-02-06 04:44:31,987 EPOCH 785
2024-02-06 04:44:42,540 Epoch 785: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.33 
2024-02-06 04:44:42,541 EPOCH 786
2024-02-06 04:44:53,483 Epoch 786: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.75 
2024-02-06 04:44:53,484 EPOCH 787
2024-02-06 04:45:04,200 Epoch 787: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.99 
2024-02-06 04:45:04,201 EPOCH 788
2024-02-06 04:45:15,180 Epoch 788: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.33 
2024-02-06 04:45:15,180 EPOCH 789
2024-02-06 04:45:21,658 [Epoch: 789 Step: 00013400] Batch Recognition Loss:   0.002454 => Gls Tokens per Sec:      355 || Batch Translation Loss:   0.059763 => Txt Tokens per Sec:     1188 || Lr: 0.000100
2024-02-06 04:45:26,004 Epoch 789: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.37 
2024-02-06 04:45:26,005 EPOCH 790
2024-02-06 04:45:36,790 Epoch 790: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.24 
2024-02-06 04:45:36,791 EPOCH 791
2024-02-06 04:45:47,417 Epoch 791: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.08 
2024-02-06 04:45:47,417 EPOCH 792
2024-02-06 04:45:58,075 Epoch 792: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.91 
2024-02-06 04:45:58,075 EPOCH 793
2024-02-06 04:46:08,630 Epoch 793: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.89 
2024-02-06 04:46:08,631 EPOCH 794
2024-02-06 04:46:19,627 Epoch 794: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.89 
2024-02-06 04:46:19,627 EPOCH 795
2024-02-06 04:46:21,440 [Epoch: 795 Step: 00013500] Batch Recognition Loss:   0.001468 => Gls Tokens per Sec:      707 || Batch Translation Loss:   0.045126 => Txt Tokens per Sec:     2221 || Lr: 0.000100
2024-02-06 04:46:30,303 Epoch 795: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.69 
2024-02-06 04:46:30,304 EPOCH 796
2024-02-06 04:46:40,768 Epoch 796: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-06 04:46:40,768 EPOCH 797
2024-02-06 04:46:51,692 Epoch 797: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-06 04:46:51,693 EPOCH 798
2024-02-06 04:47:02,350 Epoch 798: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-06 04:47:02,351 EPOCH 799
2024-02-06 04:47:12,913 Epoch 799: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-06 04:47:12,914 EPOCH 800
2024-02-06 04:47:23,701 [Epoch: 800 Step: 00013600] Batch Recognition Loss:   0.000431 => Gls Tokens per Sec:      985 || Batch Translation Loss:   0.021330 => Txt Tokens per Sec:     2724 || Lr: 0.000100
2024-02-06 04:47:23,701 Epoch 800: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-06 04:47:23,701 EPOCH 801
2024-02-06 04:47:34,223 Epoch 801: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.35 
2024-02-06 04:47:34,223 EPOCH 802
2024-02-06 04:47:44,985 Epoch 802: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.37 
2024-02-06 04:47:44,986 EPOCH 803
2024-02-06 04:47:55,618 Epoch 803: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.33 
2024-02-06 04:47:55,619 EPOCH 804
2024-02-06 04:48:06,248 Epoch 804: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-06 04:48:06,248 EPOCH 805
2024-02-06 04:48:16,814 Epoch 805: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-06 04:48:16,814 EPOCH 806
2024-02-06 04:48:25,750 [Epoch: 806 Step: 00013700] Batch Recognition Loss:   0.001329 => Gls Tokens per Sec:     1045 || Batch Translation Loss:   0.023112 => Txt Tokens per Sec:     2843 || Lr: 0.000100
2024-02-06 04:48:27,611 Epoch 806: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-06 04:48:27,611 EPOCH 807
2024-02-06 04:48:38,355 Epoch 807: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-06 04:48:38,355 EPOCH 808
2024-02-06 04:48:48,913 Epoch 808: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-06 04:48:48,914 EPOCH 809
2024-02-06 04:48:59,650 Epoch 809: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-06 04:48:59,651 EPOCH 810
2024-02-06 04:49:10,447 Epoch 810: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-06 04:49:10,448 EPOCH 811
2024-02-06 04:49:21,139 Epoch 811: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.60 
2024-02-06 04:49:21,139 EPOCH 812
2024-02-06 04:49:29,275 [Epoch: 812 Step: 00013800] Batch Recognition Loss:   0.000230 => Gls Tokens per Sec:      991 || Batch Translation Loss:   0.028890 => Txt Tokens per Sec:     2669 || Lr: 0.000100
2024-02-06 04:49:32,071 Epoch 812: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.56 
2024-02-06 04:49:32,072 EPOCH 813
2024-02-06 04:49:43,086 Epoch 813: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-06 04:49:43,086 EPOCH 814
2024-02-06 04:49:53,892 Epoch 814: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-06 04:49:53,892 EPOCH 815
2024-02-06 04:50:04,790 Epoch 815: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.86 
2024-02-06 04:50:04,791 EPOCH 816
2024-02-06 04:50:15,319 Epoch 816: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.91 
2024-02-06 04:50:15,319 EPOCH 817
2024-02-06 04:50:26,449 Epoch 817: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.14 
2024-02-06 04:50:26,450 EPOCH 818
2024-02-06 04:50:34,277 [Epoch: 818 Step: 00013900] Batch Recognition Loss:   0.003384 => Gls Tokens per Sec:      866 || Batch Translation Loss:   0.138317 => Txt Tokens per Sec:     2491 || Lr: 0.000100
2024-02-06 04:50:36,915 Epoch 818: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.71 
2024-02-06 04:50:36,915 EPOCH 819
2024-02-06 04:50:47,319 Epoch 819: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.88 
2024-02-06 04:50:47,320 EPOCH 820
2024-02-06 04:50:58,353 Epoch 820: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.90 
2024-02-06 04:50:58,354 EPOCH 821
2024-02-06 04:51:09,204 Epoch 821: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.34 
2024-02-06 04:51:09,205 EPOCH 822
2024-02-06 04:51:20,000 Epoch 822: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.54 
2024-02-06 04:51:20,001 EPOCH 823
2024-02-06 04:51:30,716 Epoch 823: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.99 
2024-02-06 04:51:30,717 EPOCH 824
2024-02-06 04:51:35,064 [Epoch: 824 Step: 00014000] Batch Recognition Loss:   0.000593 => Gls Tokens per Sec:     1266 || Batch Translation Loss:   0.123088 => Txt Tokens per Sec:     3526 || Lr: 0.000100
2024-02-06 04:52:15,680 Validation result at epoch 824, step    14000: duration: 40.6154s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.90456	Translation Loss: 92802.53906	PPL: 10605.52930
	Eval Metric: BLEU
	WER 4.66	(DEL: 0.00,	INS: 0.00,	SUB: 4.66)
	BLEU-4 0.35	(BLEU-1: 11.06,	BLEU-2: 3.05,	BLEU-3: 1.02,	BLEU-4: 0.35)
	CHRF 17.01	ROUGE 9.14
2024-02-06 04:52:15,682 Logging Recognition and Translation Outputs
2024-02-06 04:52:15,683 ========================================================================================================================
2024-02-06 04:52:15,683 Logging Sequence: 177_50.00
2024-02-06 04:52:15,683 	Gloss Reference :	A B+C+D+E
2024-02-06 04:52:15,683 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 04:52:15,683 	Gloss Alignment :	         
2024-02-06 04:52:15,684 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 04:52:15,685 	Text Reference  :	a similar reward of rs       50000 was   announced for ***** ***** information against his    associate ajay kumar
2024-02-06 04:52:15,685 	Text Hypothesis :	* then    came   in lockdown for   delhi police    for rana' death on          10th    august which     is   delhi
2024-02-06 04:52:15,686 	Text Alignment  :	D S       S      S  S        S     S     S             I     I     S           S       S      S         S    S    
2024-02-06 04:52:15,686 ========================================================================================================================
2024-02-06 04:52:15,686 Logging Sequence: 136_175.00
2024-02-06 04:52:15,686 	Gloss Reference :	A B+C+D+E  
2024-02-06 04:52:15,686 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-06 04:52:15,686 	Gloss Alignment :	  S        
2024-02-06 04:52:15,687 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 04:52:15,687 	Text Reference  :	after 49 years india' hockey team beat  britain and  qualified for the semi-finals
2024-02-06 04:52:15,688 	Text Hypothesis :	***** ** ***** ****** ****** the  first case    will start     in  the tournament 
2024-02-06 04:52:15,688 	Text Alignment  :	D     D  D     D      D      S    S     S       S    S         S       S          
2024-02-06 04:52:15,688 ========================================================================================================================
2024-02-06 04:52:15,688 Logging Sequence: 126_159.00
2024-02-06 04:52:15,688 	Gloss Reference :	A B+C+D+E
2024-02-06 04:52:15,688 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 04:52:15,688 	Gloss Alignment :	         
2024-02-06 04:52:15,688 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 04:52:15,689 	Text Reference  :	despite multiple challenges and injuries  you did not give up   
2024-02-06 04:52:15,689 	Text Hypothesis :	he      was      sad        and overjoyed as  he  was sure about
2024-02-06 04:52:15,689 	Text Alignment  :	S       S        S              S         S   S   S   S    S    
2024-02-06 04:52:15,690 ========================================================================================================================
2024-02-06 04:52:15,690 Logging Sequence: 70_88.00
2024-02-06 04:52:15,690 	Gloss Reference :	A B+C+D+E
2024-02-06 04:52:15,690 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 04:52:15,690 	Gloss Alignment :	         
2024-02-06 04:52:15,690 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 04:52:15,691 	Text Reference  :	******* two coca-cola bottles were placed  on  the  table next     to   the mic    
2024-02-06 04:52:15,692 	Text Hypothesis :	ronaldo has admitted  that    his  fitness but they were  infected with two matches
2024-02-06 04:52:15,692 	Text Alignment  :	I       S   S         S       S    S       S   S    S     S        S    S   S      
2024-02-06 04:52:15,692 ========================================================================================================================
2024-02-06 04:52:15,692 Logging Sequence: 54_201.00
2024-02-06 04:52:15,692 	Gloss Reference :	A B+C+D+E
2024-02-06 04:52:15,692 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 04:52:15,693 	Gloss Alignment :	         
2024-02-06 04:52:15,693 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 04:52:15,696 	Text Reference  :	there is a huge demand       mostly from    non-resident indians nris who  are    excited   to see          the match ******* and they    have booked the       hotel ******* ***** ** rooms
2024-02-06 04:52:15,697 	Text Hypothesis :	***** ** * the  middle-class fans   usually decide       at      the  last moment depending on availability of  match tickets if  tickets are  not    available hotel booking would go waste
2024-02-06 04:52:15,697 	Text Alignment  :	D     D  D S    S            S      S       S            S       S    S    S      S         S  S            S         I       S   S       S    S      S               I       I     I  S    
2024-02-06 04:52:15,697 ========================================================================================================================
2024-02-06 04:52:22,457 Epoch 824: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.76 
2024-02-06 04:52:22,458 EPOCH 825
2024-02-06 04:52:33,368 Epoch 825: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.97 
2024-02-06 04:52:33,369 EPOCH 826
2024-02-06 04:52:44,030 Epoch 826: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.63 
2024-02-06 04:52:44,030 EPOCH 827
2024-02-06 04:52:54,757 Epoch 827: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.33 
2024-02-06 04:52:54,758 EPOCH 828
2024-02-06 04:53:05,478 Epoch 828: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.16 
2024-02-06 04:53:05,479 EPOCH 829
2024-02-06 04:53:16,060 Epoch 829: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.18 
2024-02-06 04:53:16,060 EPOCH 830
2024-02-06 04:53:19,774 [Epoch: 830 Step: 00014100] Batch Recognition Loss:   0.001365 => Gls Tokens per Sec:     1137 || Batch Translation Loss:   0.065957 => Txt Tokens per Sec:     3092 || Lr: 0.000100
2024-02-06 04:53:26,829 Epoch 830: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.47 
2024-02-06 04:53:26,830 EPOCH 831
2024-02-06 04:53:37,331 Epoch 831: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.06 
2024-02-06 04:53:37,332 EPOCH 832
2024-02-06 04:53:48,125 Epoch 832: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.92 
2024-02-06 04:53:48,126 EPOCH 833
2024-02-06 04:53:58,802 Epoch 833: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.96 
2024-02-06 04:53:58,803 EPOCH 834
2024-02-06 04:54:09,425 Epoch 834: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.75 
2024-02-06 04:54:09,425 EPOCH 835
2024-02-06 04:54:19,989 Epoch 835: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.95 
2024-02-06 04:54:19,990 EPOCH 836
2024-02-06 04:54:26,219 [Epoch: 836 Step: 00014200] Batch Recognition Loss:   0.000726 => Gls Tokens per Sec:      472 || Batch Translation Loss:   0.059959 => Txt Tokens per Sec:     1403 || Lr: 0.000100
2024-02-06 04:54:30,770 Epoch 836: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.84 
2024-02-06 04:54:30,770 EPOCH 837
2024-02-06 04:54:41,278 Epoch 837: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-06 04:54:41,279 EPOCH 838
2024-02-06 04:54:52,134 Epoch 838: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.64 
2024-02-06 04:54:52,135 EPOCH 839
2024-02-06 04:55:02,987 Epoch 839: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.54 
2024-02-06 04:55:02,988 EPOCH 840
2024-02-06 04:55:13,874 Epoch 840: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-06 04:55:13,874 EPOCH 841
2024-02-06 04:55:24,715 Epoch 841: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-06 04:55:24,716 EPOCH 842
2024-02-06 04:55:26,905 [Epoch: 842 Step: 00014300] Batch Recognition Loss:   0.000275 => Gls Tokens per Sec:      878 || Batch Translation Loss:   0.040257 => Txt Tokens per Sec:     2426 || Lr: 0.000100
2024-02-06 04:55:35,566 Epoch 842: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-06 04:55:35,567 EPOCH 843
2024-02-06 04:55:46,386 Epoch 843: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-06 04:55:46,387 EPOCH 844
2024-02-06 04:55:57,008 Epoch 844: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-06 04:55:57,009 EPOCH 845
2024-02-06 04:56:07,642 Epoch 845: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.47 
2024-02-06 04:56:07,643 EPOCH 846
2024-02-06 04:56:18,481 Epoch 846: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-06 04:56:18,482 EPOCH 847
2024-02-06 04:56:28,979 Epoch 847: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.59 
2024-02-06 04:56:28,980 EPOCH 848
2024-02-06 04:56:29,118 [Epoch: 848 Step: 00014400] Batch Recognition Loss:   0.000551 => Gls Tokens per Sec:     4671 || Batch Translation Loss:   0.054661 => Txt Tokens per Sec:     9547 || Lr: 0.000100
2024-02-06 04:56:39,414 Epoch 848: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-06 04:56:39,414 EPOCH 849
2024-02-06 04:56:50,362 Epoch 849: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.54 
2024-02-06 04:56:50,363 EPOCH 850
2024-02-06 04:57:01,030 Epoch 850: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.49 
2024-02-06 04:57:01,030 EPOCH 851
2024-02-06 04:57:12,031 Epoch 851: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-06 04:57:12,031 EPOCH 852
2024-02-06 04:57:22,953 Epoch 852: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.92 
2024-02-06 04:57:22,953 EPOCH 853
2024-02-06 04:57:33,375 [Epoch: 853 Step: 00014500] Batch Recognition Loss:   0.000866 => Gls Tokens per Sec:      958 || Batch Translation Loss:   0.020247 => Txt Tokens per Sec:     2696 || Lr: 0.000100
2024-02-06 04:57:33,505 Epoch 853: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.97 
2024-02-06 04:57:33,506 EPOCH 854
2024-02-06 04:57:44,029 Epoch 854: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.25 
2024-02-06 04:57:44,029 EPOCH 855
2024-02-06 04:57:54,546 Epoch 855: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.03 
2024-02-06 04:57:54,547 EPOCH 856
2024-02-06 04:58:05,377 Epoch 856: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.97 
2024-02-06 04:58:05,378 EPOCH 857
2024-02-06 04:58:16,021 Epoch 857: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-06 04:58:16,022 EPOCH 858
2024-02-06 04:58:26,657 Epoch 858: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.94 
2024-02-06 04:58:26,658 EPOCH 859
2024-02-06 04:58:36,666 [Epoch: 859 Step: 00014600] Batch Recognition Loss:   0.002216 => Gls Tokens per Sec:      870 || Batch Translation Loss:   0.047783 => Txt Tokens per Sec:     2431 || Lr: 0.000100
2024-02-06 04:58:37,398 Epoch 859: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.79 
2024-02-06 04:58:37,399 EPOCH 860
2024-02-06 04:58:48,180 Epoch 860: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.73 
2024-02-06 04:58:48,181 EPOCH 861
2024-02-06 04:58:58,810 Epoch 861: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.99 
2024-02-06 04:58:58,810 EPOCH 862
2024-02-06 04:59:09,507 Epoch 862: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.85 
2024-02-06 04:59:09,508 EPOCH 863
2024-02-06 04:59:20,186 Epoch 863: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.08 
2024-02-06 04:59:20,187 EPOCH 864
2024-02-06 04:59:30,937 Epoch 864: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.99 
2024-02-06 04:59:30,938 EPOCH 865
2024-02-06 04:59:35,239 [Epoch: 865 Step: 00014700] Batch Recognition Loss:   0.008881 => Gls Tokens per Sec:     1786 || Batch Translation Loss:   0.205247 => Txt Tokens per Sec:     4793 || Lr: 0.000100
2024-02-06 04:59:41,576 Epoch 865: Total Training Recognition Loss 0.32  Total Training Translation Loss 6.65 
2024-02-06 04:59:41,576 EPOCH 866
2024-02-06 04:59:52,476 Epoch 866: Total Training Recognition Loss 2.43  Total Training Translation Loss 6.58 
2024-02-06 04:59:52,476 EPOCH 867
2024-02-06 05:00:03,474 Epoch 867: Total Training Recognition Loss 2.79  Total Training Translation Loss 11.01 
2024-02-06 05:00:03,475 EPOCH 868
2024-02-06 05:00:13,954 Epoch 868: Total Training Recognition Loss 0.36  Total Training Translation Loss 3.73 
2024-02-06 05:00:13,954 EPOCH 869
2024-02-06 05:00:24,394 Epoch 869: Total Training Recognition Loss 0.18  Total Training Translation Loss 2.18 
2024-02-06 05:00:24,395 EPOCH 870
2024-02-06 05:00:35,207 Epoch 870: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.16 
2024-02-06 05:00:35,207 EPOCH 871
2024-02-06 05:00:42,798 [Epoch: 871 Step: 00014800] Batch Recognition Loss:   0.000730 => Gls Tokens per Sec:      809 || Batch Translation Loss:   0.043014 => Txt Tokens per Sec:     2228 || Lr: 0.000100
2024-02-06 05:00:45,911 Epoch 871: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.79 
2024-02-06 05:00:45,911 EPOCH 872
2024-02-06 05:00:56,508 Epoch 872: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.67 
2024-02-06 05:00:56,508 EPOCH 873
2024-02-06 05:01:07,232 Epoch 873: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.65 
2024-02-06 05:01:07,232 EPOCH 874
2024-02-06 05:01:18,097 Epoch 874: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.52 
2024-02-06 05:01:18,098 EPOCH 875
2024-02-06 05:01:28,687 Epoch 875: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.47 
2024-02-06 05:01:28,688 EPOCH 876
2024-02-06 05:01:39,540 Epoch 876: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.39 
2024-02-06 05:01:39,540 EPOCH 877
2024-02-06 05:01:42,862 [Epoch: 877 Step: 00014900] Batch Recognition Loss:   0.000401 => Gls Tokens per Sec:     1542 || Batch Translation Loss:   0.012882 => Txt Tokens per Sec:     3981 || Lr: 0.000100
2024-02-06 05:01:50,406 Epoch 877: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.39 
2024-02-06 05:01:50,406 EPOCH 878
2024-02-06 05:02:01,114 Epoch 878: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.39 
2024-02-06 05:02:01,114 EPOCH 879
2024-02-06 05:02:11,764 Epoch 879: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.36 
2024-02-06 05:02:11,764 EPOCH 880
2024-02-06 05:02:22,784 Epoch 880: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.36 
2024-02-06 05:02:22,784 EPOCH 881
2024-02-06 05:02:33,778 Epoch 881: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.33 
2024-02-06 05:02:33,778 EPOCH 882
2024-02-06 05:02:44,418 Epoch 882: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.35 
2024-02-06 05:02:44,418 EPOCH 883
2024-02-06 05:02:50,865 [Epoch: 883 Step: 00015000] Batch Recognition Loss:   0.000585 => Gls Tokens per Sec:      555 || Batch Translation Loss:   0.011360 => Txt Tokens per Sec:     1448 || Lr: 0.000100
2024-02-06 05:02:55,430 Epoch 883: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 05:02:55,431 EPOCH 884
2024-02-06 05:03:06,148 Epoch 884: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.31 
2024-02-06 05:03:06,149 EPOCH 885
2024-02-06 05:03:17,209 Epoch 885: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 05:03:17,210 EPOCH 886
2024-02-06 05:03:28,121 Epoch 886: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 05:03:28,121 EPOCH 887
2024-02-06 05:03:38,777 Epoch 887: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.26 
2024-02-06 05:03:38,778 EPOCH 888
2024-02-06 05:03:49,531 Epoch 888: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 05:03:49,532 EPOCH 889
2024-02-06 05:03:52,043 [Epoch: 889 Step: 00015100] Batch Recognition Loss:   0.000299 => Gls Tokens per Sec:     1020 || Batch Translation Loss:   0.010585 => Txt Tokens per Sec:     2991 || Lr: 0.000100
2024-02-06 05:04:00,493 Epoch 889: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 05:04:00,494 EPOCH 890
2024-02-06 05:04:11,282 Epoch 890: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 05:04:11,283 EPOCH 891
2024-02-06 05:04:21,845 Epoch 891: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-06 05:04:21,846 EPOCH 892
2024-02-06 05:04:32,528 Epoch 892: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 05:04:32,528 EPOCH 893
2024-02-06 05:04:43,431 Epoch 893: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 05:04:43,432 EPOCH 894
2024-02-06 05:04:54,296 Epoch 894: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.26 
2024-02-06 05:04:54,297 EPOCH 895
2024-02-06 05:04:54,730 [Epoch: 895 Step: 00015200] Batch Recognition Loss:   0.001203 => Gls Tokens per Sec:     2970 || Batch Translation Loss:   0.012701 => Txt Tokens per Sec:     8158 || Lr: 0.000100
2024-02-06 05:05:04,967 Epoch 895: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.25 
2024-02-06 05:05:04,967 EPOCH 896
2024-02-06 05:05:15,527 Epoch 896: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 05:05:15,528 EPOCH 897
2024-02-06 05:05:26,373 Epoch 897: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 05:05:26,374 EPOCH 898
2024-02-06 05:05:36,766 Epoch 898: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 05:05:36,767 EPOCH 899
2024-02-06 05:05:47,776 Epoch 899: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 05:05:47,776 EPOCH 900
2024-02-06 05:05:58,472 [Epoch: 900 Step: 00015300] Batch Recognition Loss:   0.000149 => Gls Tokens per Sec:      993 || Batch Translation Loss:   0.014109 => Txt Tokens per Sec:     2747 || Lr: 0.000100
2024-02-06 05:05:58,473 Epoch 900: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 05:05:58,473 EPOCH 901
2024-02-06 05:06:09,338 Epoch 901: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 05:06:09,339 EPOCH 902
2024-02-06 05:06:19,795 Epoch 902: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 05:06:19,795 EPOCH 903
2024-02-06 05:06:30,665 Epoch 903: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 05:06:30,665 EPOCH 904
2024-02-06 05:06:41,417 Epoch 904: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 05:06:41,418 EPOCH 905
2024-02-06 05:06:52,316 Epoch 905: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 05:06:52,317 EPOCH 906
2024-02-06 05:07:00,356 [Epoch: 906 Step: 00015400] Batch Recognition Loss:   0.000110 => Gls Tokens per Sec:     1195 || Batch Translation Loss:   0.015687 => Txt Tokens per Sec:     3240 || Lr: 0.000100
2024-02-06 05:07:03,094 Epoch 906: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 05:07:03,094 EPOCH 907
2024-02-06 05:07:13,888 Epoch 907: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 05:07:13,889 EPOCH 908
2024-02-06 05:07:24,632 Epoch 908: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.24 
2024-02-06 05:07:24,633 EPOCH 909
2024-02-06 05:07:35,262 Epoch 909: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 05:07:35,262 EPOCH 910
2024-02-06 05:07:46,181 Epoch 910: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 05:07:46,181 EPOCH 911
2024-02-06 05:07:57,200 Epoch 911: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 05:07:57,200 EPOCH 912
2024-02-06 05:08:04,211 [Epoch: 912 Step: 00015500] Batch Recognition Loss:   0.000193 => Gls Tokens per Sec:     1150 || Batch Translation Loss:   0.010727 => Txt Tokens per Sec:     3079 || Lr: 0.000100
2024-02-06 05:08:08,171 Epoch 912: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 05:08:08,171 EPOCH 913
2024-02-06 05:08:18,813 Epoch 913: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 05:08:18,814 EPOCH 914
2024-02-06 05:08:29,692 Epoch 914: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.24 
2024-02-06 05:08:29,693 EPOCH 915
2024-02-06 05:08:40,352 Epoch 915: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 05:08:40,352 EPOCH 916
2024-02-06 05:08:51,074 Epoch 916: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 05:08:51,074 EPOCH 917
2024-02-06 05:09:01,787 Epoch 917: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-06 05:09:01,788 EPOCH 918
2024-02-06 05:09:08,009 [Epoch: 918 Step: 00015600] Batch Recognition Loss:   0.002992 => Gls Tokens per Sec:     1090 || Batch Translation Loss:   0.054893 => Txt Tokens per Sec:     2941 || Lr: 0.000100
2024-02-06 05:09:12,560 Epoch 918: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.10 
2024-02-06 05:09:12,560 EPOCH 919
2024-02-06 05:09:23,202 Epoch 919: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.86 
2024-02-06 05:09:23,202 EPOCH 920
2024-02-06 05:09:33,987 Epoch 920: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.27 
2024-02-06 05:09:33,988 EPOCH 921
2024-02-06 05:09:44,972 Epoch 921: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.11 
2024-02-06 05:09:44,973 EPOCH 922
2024-02-06 05:09:55,701 Epoch 922: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.14 
2024-02-06 05:09:55,701 EPOCH 923
2024-02-06 05:10:06,243 Epoch 923: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.82 
2024-02-06 05:10:06,243 EPOCH 924
2024-02-06 05:10:11,702 [Epoch: 924 Step: 00015700] Batch Recognition Loss:   0.001341 => Gls Tokens per Sec:     1008 || Batch Translation Loss:   0.223548 => Txt Tokens per Sec:     2585 || Lr: 0.000100
2024-02-06 05:10:16,851 Epoch 924: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.21 
2024-02-06 05:10:16,851 EPOCH 925
2024-02-06 05:10:27,717 Epoch 925: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.09 
2024-02-06 05:10:27,717 EPOCH 926
2024-02-06 05:10:38,381 Epoch 926: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.54 
2024-02-06 05:10:38,382 EPOCH 927
2024-02-06 05:10:49,092 Epoch 927: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.31 
2024-02-06 05:10:49,092 EPOCH 928
2024-02-06 05:10:59,742 Epoch 928: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.97 
2024-02-06 05:10:59,743 EPOCH 929
2024-02-06 05:11:10,684 Epoch 929: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-06 05:11:10,685 EPOCH 930
2024-02-06 05:11:12,227 [Epoch: 930 Step: 00015800] Batch Recognition Loss:   0.000522 => Gls Tokens per Sec:     2908 || Batch Translation Loss:   0.026605 => Txt Tokens per Sec:     7296 || Lr: 0.000100
2024-02-06 05:11:21,458 Epoch 930: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.67 
2024-02-06 05:11:21,459 EPOCH 931
2024-02-06 05:11:32,298 Epoch 931: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-06 05:11:32,298 EPOCH 932
2024-02-06 05:11:43,061 Epoch 932: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-06 05:11:43,061 EPOCH 933
2024-02-06 05:11:53,898 Epoch 933: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-06 05:11:53,899 EPOCH 934
2024-02-06 05:12:04,636 Epoch 934: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.69 
2024-02-06 05:12:04,637 EPOCH 935
2024-02-06 05:12:15,125 Epoch 935: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-06 05:12:15,125 EPOCH 936
2024-02-06 05:12:19,148 [Epoch: 936 Step: 00015900] Batch Recognition Loss:   0.000484 => Gls Tokens per Sec:      796 || Batch Translation Loss:   0.027572 => Txt Tokens per Sec:     2021 || Lr: 0.000100
2024-02-06 05:12:25,893 Epoch 936: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-06 05:12:25,893 EPOCH 937
2024-02-06 05:12:36,460 Epoch 937: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.59 
2024-02-06 05:12:36,461 EPOCH 938
2024-02-06 05:12:47,231 Epoch 938: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-06 05:12:47,231 EPOCH 939
2024-02-06 05:12:57,890 Epoch 939: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-06 05:12:57,891 EPOCH 940
2024-02-06 05:13:08,680 Epoch 940: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-06 05:13:08,681 EPOCH 941
2024-02-06 05:13:19,383 Epoch 941: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-06 05:13:19,383 EPOCH 942
2024-02-06 05:13:19,974 [Epoch: 942 Step: 00016000] Batch Recognition Loss:   0.000473 => Gls Tokens per Sec:     3260 || Batch Translation Loss:   0.023817 => Txt Tokens per Sec:     8131 || Lr: 0.000100
2024-02-06 05:14:00,407 Validation result at epoch 942, step    16000: duration: 40.4330s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.73278	Translation Loss: 92758.89844	PPL: 10559.39746
	Eval Metric: BLEU
	WER 4.31	(DEL: 0.00,	INS: 0.00,	SUB: 4.31)
	BLEU-4 0.91	(BLEU-1: 11.81,	BLEU-2: 3.94,	BLEU-3: 1.70,	BLEU-4: 0.91)
	CHRF 17.82	ROUGE 9.72
2024-02-06 05:14:00,410 Logging Recognition and Translation Outputs
2024-02-06 05:14:00,410 ========================================================================================================================
2024-02-06 05:14:00,410 Logging Sequence: 163_116.00
2024-02-06 05:14:00,410 	Gloss Reference :	A B+C+D+E
2024-02-06 05:14:00,410 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 05:14:00,410 	Gloss Alignment :	         
2024-02-06 05:14:00,411 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 05:14:00,412 	Text Reference  :	people said that  she looked    similar to   virat   
2024-02-06 05:14:00,412 	Text Hypothesis :	bcci   can  image on  instagram story   with pictures
2024-02-06 05:14:00,412 	Text Alignment  :	S      S    S     S   S         S       S    S       
2024-02-06 05:14:00,413 ========================================================================================================================
2024-02-06 05:14:00,413 Logging Sequence: 53_161.00
2024-02-06 05:14:00,413 	Gloss Reference :	A B+C+D+E
2024-02-06 05:14:00,413 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 05:14:00,413 	Gloss Alignment :	         
2024-02-06 05:14:00,413 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 05:14:00,415 	Text Reference  :	rashid has       also      been      urging people to   donate to   his rashid **** khan foundation and afghanistan cricket     association
2024-02-06 05:14:00,415 	Text Hypothesis :	****** meanwhile sunrisers hyderabad has    said   that both   nabi and rashid will be   available  for the         rescheduled ipl        
2024-02-06 05:14:00,415 	Text Alignment  :	D      S         S         S         S      S      S    S      S    S          I    S    S          S   S           S           S          
2024-02-06 05:14:00,415 ========================================================================================================================
2024-02-06 05:14:00,416 Logging Sequence: 67_73.00
2024-02-06 05:14:00,416 	Gloss Reference :	A B+C+D+E
2024-02-06 05:14:00,416 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 05:14:00,416 	Gloss Alignment :	         
2024-02-06 05:14:00,416 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 05:14:00,417 	Text Reference  :	*** *** ***** in  his tweet     he    also said     
2024-02-06 05:14:00,417 	Text Hypothesis :	the t20 world cup are currently going on   australia
2024-02-06 05:14:00,417 	Text Alignment  :	I   I   I     S   S   S         S     S    S        
2024-02-06 05:14:00,417 ========================================================================================================================
2024-02-06 05:14:00,418 Logging Sequence: 137_44.00
2024-02-06 05:14:00,418 	Gloss Reference :	A B+C+D+E
2024-02-06 05:14:00,418 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 05:14:00,418 	Gloss Alignment :	         
2024-02-06 05:14:00,419 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 05:14:00,420 	Text Reference  :	let me tell you the rules that qatar has announced for the      fans travelling  for       the ******* world cup   
2024-02-06 05:14:00,420 	Text Hypothesis :	*** ** **** *** *** ***** **** they  all posed     for pictures amid celebratory fireworks the stadium in    mumbai
2024-02-06 05:14:00,421 	Text Alignment  :	D   D  D    D   D   D     D    S     S   S             S        S    S           S             I       S     S     
2024-02-06 05:14:00,421 ========================================================================================================================
2024-02-06 05:14:00,421 Logging Sequence: 99_158.00
2024-02-06 05:14:00,421 	Gloss Reference :	A B+C+D+E
2024-02-06 05:14:00,421 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 05:14:00,421 	Gloss Alignment :	         
2024-02-06 05:14:00,421 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 05:14:00,422 	Text Reference  :	****** the incident occured     in   dubai and   it     was  extremely shameful
2024-02-06 05:14:00,422 	Text Hypothesis :	people had high     expectation with each  other famous lost the       match   
2024-02-06 05:14:00,423 	Text Alignment  :	I      S   S        S           S    S     S     S      S    S         S       
2024-02-06 05:14:00,423 ========================================================================================================================
2024-02-06 05:14:11,325 Epoch 942: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-06 05:14:11,325 EPOCH 943
2024-02-06 05:14:22,157 Epoch 943: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.85 
2024-02-06 05:14:22,157 EPOCH 944
2024-02-06 05:14:32,991 Epoch 944: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-06 05:14:32,991 EPOCH 945
2024-02-06 05:14:43,588 Epoch 945: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-06 05:14:43,589 EPOCH 946
2024-02-06 05:14:54,066 Epoch 946: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-06 05:14:54,067 EPOCH 947
2024-02-06 05:15:04,690 Epoch 947: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-06 05:15:04,690 EPOCH 948
2024-02-06 05:15:04,783 [Epoch: 948 Step: 00016100] Batch Recognition Loss:   0.000343 => Gls Tokens per Sec:     6956 || Batch Translation Loss:   0.007908 => Txt Tokens per Sec:    10902 || Lr: 0.000100
2024-02-06 05:15:15,376 Epoch 948: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-06 05:15:15,376 EPOCH 949
2024-02-06 05:15:26,010 Epoch 949: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-06 05:15:26,010 EPOCH 950
2024-02-06 05:15:36,571 Epoch 950: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-06 05:15:36,572 EPOCH 951
2024-02-06 05:15:46,780 Epoch 951: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-06 05:15:46,781 EPOCH 952
2024-02-06 05:15:57,598 Epoch 952: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-06 05:15:57,598 EPOCH 953
2024-02-06 05:16:08,273 [Epoch: 953 Step: 00016200] Batch Recognition Loss:   0.001824 => Gls Tokens per Sec:      935 || Batch Translation Loss:   0.020867 => Txt Tokens per Sec:     2565 || Lr: 0.000100
2024-02-06 05:16:08,652 Epoch 953: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-06 05:16:08,652 EPOCH 954
2024-02-06 05:16:19,308 Epoch 954: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.79 
2024-02-06 05:16:19,308 EPOCH 955
2024-02-06 05:16:30,123 Epoch 955: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.70 
2024-02-06 05:16:30,124 EPOCH 956
2024-02-06 05:16:40,600 Epoch 956: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.75 
2024-02-06 05:16:40,600 EPOCH 957
2024-02-06 05:16:51,746 Epoch 957: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.13 
2024-02-06 05:16:51,746 EPOCH 958
2024-02-06 05:17:02,130 Epoch 958: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.90 
2024-02-06 05:17:02,131 EPOCH 959
2024-02-06 05:17:10,083 [Epoch: 959 Step: 00016300] Batch Recognition Loss:   0.000464 => Gls Tokens per Sec:     1127 || Batch Translation Loss:   0.192295 => Txt Tokens per Sec:     3068 || Lr: 0.000100
2024-02-06 05:17:13,156 Epoch 959: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.18 
2024-02-06 05:17:13,157 EPOCH 960
2024-02-06 05:17:23,846 Epoch 960: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.19 
2024-02-06 05:17:23,847 EPOCH 961
2024-02-06 05:17:34,613 Epoch 961: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.32 
2024-02-06 05:17:34,614 EPOCH 962
2024-02-06 05:17:45,095 Epoch 962: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.18 
2024-02-06 05:17:45,096 EPOCH 963
2024-02-06 05:17:55,827 Epoch 963: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.18 
2024-02-06 05:17:55,828 EPOCH 964
2024-02-06 05:18:06,527 Epoch 964: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.66 
2024-02-06 05:18:06,527 EPOCH 965
2024-02-06 05:18:13,763 [Epoch: 965 Step: 00016400] Batch Recognition Loss:   0.001860 => Gls Tokens per Sec:     1062 || Batch Translation Loss:   0.067413 => Txt Tokens per Sec:     2968 || Lr: 0.000100
2024-02-06 05:18:16,999 Epoch 965: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.86 
2024-02-06 05:18:16,999 EPOCH 966
2024-02-06 05:18:27,711 Epoch 966: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.67 
2024-02-06 05:18:27,712 EPOCH 967
2024-02-06 05:18:38,265 Epoch 967: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.58 
2024-02-06 05:18:38,265 EPOCH 968
2024-02-06 05:18:49,135 Epoch 968: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.82 
2024-02-06 05:18:49,135 EPOCH 969
2024-02-06 05:19:00,632 Epoch 969: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.21 
2024-02-06 05:19:00,632 EPOCH 970
2024-02-06 05:19:11,454 Epoch 970: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.37 
2024-02-06 05:19:11,455 EPOCH 971
2024-02-06 05:19:15,593 [Epoch: 971 Step: 00016500] Batch Recognition Loss:   0.000705 => Gls Tokens per Sec:     1547 || Batch Translation Loss:   0.029969 => Txt Tokens per Sec:     4020 || Lr: 0.000100
2024-02-06 05:19:22,461 Epoch 971: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.92 
2024-02-06 05:19:22,462 EPOCH 972
2024-02-06 05:19:33,309 Epoch 972: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.71 
2024-02-06 05:19:33,310 EPOCH 973
2024-02-06 05:19:44,001 Epoch 973: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.64 
2024-02-06 05:19:44,002 EPOCH 974
2024-02-06 05:19:55,009 Epoch 974: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-06 05:19:55,010 EPOCH 975
2024-02-06 05:20:05,702 Epoch 975: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-06 05:20:05,702 EPOCH 976
2024-02-06 05:20:16,775 Epoch 976: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.55 
2024-02-06 05:20:16,776 EPOCH 977
2024-02-06 05:20:20,583 [Epoch: 977 Step: 00016600] Batch Recognition Loss:   0.000239 => Gls Tokens per Sec:     1278 || Batch Translation Loss:   0.026746 => Txt Tokens per Sec:     3364 || Lr: 0.000100
2024-02-06 05:20:27,409 Epoch 977: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-06 05:20:27,410 EPOCH 978
2024-02-06 05:20:38,311 Epoch 978: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.45 
2024-02-06 05:20:38,312 EPOCH 979
2024-02-06 05:20:49,043 Epoch 979: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.47 
2024-02-06 05:20:49,044 EPOCH 980
2024-02-06 05:20:59,759 Epoch 980: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-06 05:20:59,760 EPOCH 981
2024-02-06 05:21:10,454 Epoch 981: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.37 
2024-02-06 05:21:10,454 EPOCH 982
2024-02-06 05:21:21,189 Epoch 982: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-06 05:21:21,190 EPOCH 983
2024-02-06 05:21:24,126 [Epoch: 983 Step: 00016700] Batch Recognition Loss:   0.000433 => Gls Tokens per Sec:     1308 || Batch Translation Loss:   0.043929 => Txt Tokens per Sec:     3573 || Lr: 0.000100
2024-02-06 05:21:31,905 Epoch 983: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-06 05:21:31,906 EPOCH 984
2024-02-06 05:21:42,673 Epoch 984: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-06 05:21:42,674 EPOCH 985
2024-02-06 05:21:53,338 Epoch 985: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-06 05:21:53,339 EPOCH 986
2024-02-06 05:22:03,884 Epoch 986: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.54 
2024-02-06 05:22:03,885 EPOCH 987
2024-02-06 05:22:14,673 Epoch 987: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-06 05:22:14,674 EPOCH 988
2024-02-06 05:22:25,620 Epoch 988: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-06 05:22:25,621 EPOCH 989
2024-02-06 05:22:28,274 [Epoch: 989 Step: 00016800] Batch Recognition Loss:   0.001489 => Gls Tokens per Sec:      965 || Batch Translation Loss:   0.008112 => Txt Tokens per Sec:     2601 || Lr: 0.000100
2024-02-06 05:22:36,244 Epoch 989: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-06 05:22:36,245 EPOCH 990
2024-02-06 05:22:47,079 Epoch 990: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-06 05:22:47,080 EPOCH 991
2024-02-06 05:22:57,693 Epoch 991: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-06 05:22:57,694 EPOCH 992
2024-02-06 05:23:08,599 Epoch 992: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-06 05:23:08,600 EPOCH 993
2024-02-06 05:23:19,391 Epoch 993: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-06 05:23:19,391 EPOCH 994
2024-02-06 05:23:30,097 Epoch 994: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-06 05:23:30,098 EPOCH 995
2024-02-06 05:23:30,457 [Epoch: 995 Step: 00016900] Batch Recognition Loss:   0.000312 => Gls Tokens per Sec:     3585 || Batch Translation Loss:   0.028633 => Txt Tokens per Sec:     8793 || Lr: 0.000100
2024-02-06 05:23:40,900 Epoch 995: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-06 05:23:40,901 EPOCH 996
2024-02-06 05:23:51,408 Epoch 996: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.75 
2024-02-06 05:23:51,409 EPOCH 997
2024-02-06 05:24:02,012 Epoch 997: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-06 05:24:02,012 EPOCH 998
2024-02-06 05:24:12,631 Epoch 998: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.01 
2024-02-06 05:24:12,632 EPOCH 999
2024-02-06 05:24:23,361 Epoch 999: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-06 05:24:23,361 EPOCH 1000
2024-02-06 05:24:33,931 [Epoch: 1000 Step: 00017000] Batch Recognition Loss:   0.002117 => Gls Tokens per Sec:     1005 || Batch Translation Loss:   0.084382 => Txt Tokens per Sec:     2780 || Lr: 0.000100
2024-02-06 05:24:33,932 Epoch 1000: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.14 
2024-02-06 05:24:33,932 EPOCH 1001
2024-02-06 05:24:44,519 Epoch 1001: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.17 
2024-02-06 05:24:44,519 EPOCH 1002
2024-02-06 05:24:55,182 Epoch 1002: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.23 
2024-02-06 05:24:55,184 EPOCH 1003
2024-02-06 05:25:06,207 Epoch 1003: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.88 
2024-02-06 05:25:06,208 EPOCH 1004
2024-02-06 05:25:16,845 Epoch 1004: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.06 
2024-02-06 05:25:16,846 EPOCH 1005
2024-02-06 05:25:27,445 Epoch 1005: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.76 
2024-02-06 05:25:27,445 EPOCH 1006
2024-02-06 05:25:35,824 [Epoch: 1006 Step: 00017100] Batch Recognition Loss:   0.000803 => Gls Tokens per Sec:     1115 || Batch Translation Loss:   0.076669 => Txt Tokens per Sec:     3018 || Lr: 0.000100
2024-02-06 05:25:38,165 Epoch 1006: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.02 
2024-02-06 05:25:38,165 EPOCH 1007
2024-02-06 05:25:49,035 Epoch 1007: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.79 
2024-02-06 05:25:49,035 EPOCH 1008
2024-02-06 05:25:59,716 Epoch 1008: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.50 
2024-02-06 05:25:59,716 EPOCH 1009
2024-02-06 05:26:10,328 Epoch 1009: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.69 
2024-02-06 05:26:10,328 EPOCH 1010
2024-02-06 05:26:20,975 Epoch 1010: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.30 
2024-02-06 05:26:20,976 EPOCH 1011
2024-02-06 05:26:31,636 Epoch 1011: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.92 
2024-02-06 05:26:31,637 EPOCH 1012
2024-02-06 05:26:38,070 [Epoch: 1012 Step: 00017200] Batch Recognition Loss:   0.000975 => Gls Tokens per Sec:     1253 || Batch Translation Loss:   0.135903 => Txt Tokens per Sec:     3312 || Lr: 0.000100
2024-02-06 05:26:42,182 Epoch 1012: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.47 
2024-02-06 05:26:42,182 EPOCH 1013
2024-02-06 05:26:53,017 Epoch 1013: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.45 
2024-02-06 05:26:53,017 EPOCH 1014
2024-02-06 05:27:03,798 Epoch 1014: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.01 
2024-02-06 05:27:03,798 EPOCH 1015
2024-02-06 05:27:14,765 Epoch 1015: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.69 
2024-02-06 05:27:14,766 EPOCH 1016
2024-02-06 05:27:25,746 Epoch 1016: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.54 
2024-02-06 05:27:25,747 EPOCH 1017
2024-02-06 05:27:36,520 Epoch 1017: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.64 
2024-02-06 05:27:36,521 EPOCH 1018
2024-02-06 05:27:42,956 [Epoch: 1018 Step: 00017300] Batch Recognition Loss:   0.000786 => Gls Tokens per Sec:     1054 || Batch Translation Loss:   0.040702 => Txt Tokens per Sec:     2866 || Lr: 0.000100
2024-02-06 05:27:47,508 Epoch 1018: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.46 
2024-02-06 05:27:47,509 EPOCH 1019
2024-02-06 05:27:58,245 Epoch 1019: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.52 
2024-02-06 05:27:58,245 EPOCH 1020
2024-02-06 05:28:09,206 Epoch 1020: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.48 
2024-02-06 05:28:09,206 EPOCH 1021
2024-02-06 05:28:19,967 Epoch 1021: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.61 
2024-02-06 05:28:19,968 EPOCH 1022
2024-02-06 05:28:30,940 Epoch 1022: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-06 05:28:30,941 EPOCH 1023
2024-02-06 05:28:41,657 Epoch 1023: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-06 05:28:41,658 EPOCH 1024
2024-02-06 05:28:49,099 [Epoch: 1024 Step: 00017400] Batch Recognition Loss:   0.000268 => Gls Tokens per Sec:      739 || Batch Translation Loss:   0.019038 => Txt Tokens per Sec:     2108 || Lr: 0.000100
2024-02-06 05:28:52,453 Epoch 1024: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-06 05:28:52,453 EPOCH 1025
2024-02-06 05:29:02,960 Epoch 1025: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-06 05:29:02,961 EPOCH 1026
2024-02-06 05:29:13,864 Epoch 1026: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.53 
2024-02-06 05:29:13,864 EPOCH 1027
2024-02-06 05:29:24,699 Epoch 1027: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-06 05:29:24,700 EPOCH 1028
2024-02-06 05:29:35,326 Epoch 1028: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-06 05:29:35,327 EPOCH 1029
2024-02-06 05:29:45,830 Epoch 1029: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-06 05:29:45,831 EPOCH 1030
2024-02-06 05:29:49,072 [Epoch: 1030 Step: 00017500] Batch Recognition Loss:   0.000310 => Gls Tokens per Sec:     1383 || Batch Translation Loss:   0.032126 => Txt Tokens per Sec:     4056 || Lr: 0.000100
2024-02-06 05:29:56,545 Epoch 1030: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-06 05:29:56,546 EPOCH 1031
2024-02-06 05:30:07,357 Epoch 1031: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-06 05:30:07,358 EPOCH 1032
2024-02-06 05:30:17,995 Epoch 1032: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-06 05:30:17,995 EPOCH 1033
2024-02-06 05:30:28,698 Epoch 1033: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-06 05:30:28,699 EPOCH 1034
2024-02-06 05:30:39,382 Epoch 1034: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-06 05:30:39,383 EPOCH 1035
2024-02-06 05:30:49,973 Epoch 1035: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-06 05:30:49,974 EPOCH 1036
2024-02-06 05:30:52,777 [Epoch: 1036 Step: 00017600] Batch Recognition Loss:   0.000303 => Gls Tokens per Sec:     1142 || Batch Translation Loss:   0.017595 => Txt Tokens per Sec:     3136 || Lr: 0.000100
2024-02-06 05:31:00,857 Epoch 1036: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-06 05:31:00,858 EPOCH 1037
2024-02-06 05:31:11,599 Epoch 1037: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-06 05:31:11,599 EPOCH 1038
2024-02-06 05:31:22,291 Epoch 1038: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-06 05:31:22,291 EPOCH 1039
2024-02-06 05:31:32,977 Epoch 1039: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-06 05:31:32,978 EPOCH 1040
2024-02-06 05:31:43,924 Epoch 1040: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-06 05:31:43,924 EPOCH 1041
2024-02-06 05:31:54,747 Epoch 1041: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-06 05:31:54,748 EPOCH 1042
2024-02-06 05:31:55,274 [Epoch: 1042 Step: 00017700] Batch Recognition Loss:   0.000188 => Gls Tokens per Sec:     3660 || Batch Translation Loss:   0.028009 => Txt Tokens per Sec:     8648 || Lr: 0.000100
2024-02-06 05:32:05,628 Epoch 1042: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.89 
2024-02-06 05:32:05,629 EPOCH 1043
2024-02-06 05:32:16,421 Epoch 1043: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.50 
2024-02-06 05:32:16,421 EPOCH 1044
2024-02-06 05:32:27,201 Epoch 1044: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.59 
2024-02-06 05:32:27,202 EPOCH 1045
2024-02-06 05:32:38,010 Epoch 1045: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.96 
2024-02-06 05:32:38,010 EPOCH 1046
2024-02-06 05:32:48,884 Epoch 1046: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.15 
2024-02-06 05:32:48,885 EPOCH 1047
2024-02-06 05:32:59,593 Epoch 1047: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.78 
2024-02-06 05:32:59,594 EPOCH 1048
2024-02-06 05:32:59,825 [Epoch: 1048 Step: 00017800] Batch Recognition Loss:   0.000711 => Gls Tokens per Sec:     2783 || Batch Translation Loss:   0.133151 => Txt Tokens per Sec:     7378 || Lr: 0.000100
2024-02-06 05:33:10,347 Epoch 1048: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.13 
2024-02-06 05:33:10,348 EPOCH 1049
2024-02-06 05:33:21,118 Epoch 1049: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.10 
2024-02-06 05:33:21,118 EPOCH 1050
2024-02-06 05:33:31,970 Epoch 1050: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.67 
2024-02-06 05:33:31,971 EPOCH 1051
2024-02-06 05:33:42,904 Epoch 1051: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.93 
2024-02-06 05:33:42,905 EPOCH 1052
2024-02-06 05:33:53,852 Epoch 1052: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.45 
2024-02-06 05:33:53,852 EPOCH 1053
2024-02-06 05:34:04,441 [Epoch: 1053 Step: 00017900] Batch Recognition Loss:   0.001013 => Gls Tokens per Sec:      943 || Batch Translation Loss:   0.025551 => Txt Tokens per Sec:     2603 || Lr: 0.000100
2024-02-06 05:34:04,741 Epoch 1053: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.93 
2024-02-06 05:34:04,741 EPOCH 1054
2024-02-06 05:34:15,490 Epoch 1054: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.67 
2024-02-06 05:34:15,491 EPOCH 1055
2024-02-06 05:34:26,133 Epoch 1055: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.54 
2024-02-06 05:34:26,133 EPOCH 1056
2024-02-06 05:34:36,823 Epoch 1056: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-06 05:34:36,824 EPOCH 1057
2024-02-06 05:34:47,327 Epoch 1057: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.41 
2024-02-06 05:34:47,327 EPOCH 1058
2024-02-06 05:34:58,273 Epoch 1058: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.47 
2024-02-06 05:34:58,273 EPOCH 1059
2024-02-06 05:35:08,523 [Epoch: 1059 Step: 00018000] Batch Recognition Loss:   0.000966 => Gls Tokens per Sec:      849 || Batch Translation Loss:   0.023686 => Txt Tokens per Sec:     2440 || Lr: 0.000100
2024-02-06 05:35:48,991 Validation result at epoch 1059, step    18000: duration: 40.4653s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.87539	Translation Loss: 92851.71875	PPL: 10657.75488
	Eval Metric: BLEU
	WER 4.73	(DEL: 0.00,	INS: 0.00,	SUB: 4.73)
	BLEU-4 0.60	(BLEU-1: 11.44,	BLEU-2: 3.50,	BLEU-3: 1.33,	BLEU-4: 0.60)
	CHRF 17.39	ROUGE 9.49
2024-02-06 05:35:48,993 Logging Recognition and Translation Outputs
2024-02-06 05:35:48,993 ========================================================================================================================
2024-02-06 05:35:48,993 Logging Sequence: 179_309.00
2024-02-06 05:35:48,993 	Gloss Reference :	A B+C+D+E
2024-02-06 05:35:48,994 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 05:35:48,994 	Gloss Alignment :	         
2024-02-06 05:35:48,994 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 05:35:48,995 	Text Reference  :	before the ioa could send the notice wfi has  asked phogat      to explain her     indiscipline
2024-02-06 05:35:48,995 	Text Hypothesis :	****** *** *** here  is   one day    she does not   participate in the     penalty shootout    
2024-02-06 05:35:48,995 	Text Alignment  :	D      D   D   S     S    S   S      S   S    S     S           S  S       S       S           
2024-02-06 05:35:48,995 ========================================================================================================================
2024-02-06 05:35:48,995 Logging Sequence: 156_35.00
2024-02-06 05:35:48,996 	Gloss Reference :	A B+C+D+E
2024-02-06 05:35:48,996 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 05:35:48,996 	Gloss Alignment :	         
2024-02-06 05:35:48,996 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 05:35:48,998 	Text Reference  :	the first season of mlc   began    on      13th july   2023    and      ended  on  30th     july    2023 with  six teams
2024-02-06 05:35:48,998 	Text Hypothesis :	*** ***** ****** ** miny' original captain was  kieron pollard nicholas pooran was stand-in captain in   place of  him  
2024-02-06 05:35:48,998 	Text Alignment  :	D   D     D      D  S     S        S       S    S      S       S        S      S   S        S       S    S     S   S    
2024-02-06 05:35:48,998 ========================================================================================================================
2024-02-06 05:35:48,998 Logging Sequence: 129_45.00
2024-02-06 05:35:48,999 	Gloss Reference :	A B+C+D+E
2024-02-06 05:35:48,999 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 05:35:48,999 	Gloss Alignment :	         
2024-02-06 05:35:48,999 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 05:35:49,001 	Text Reference  :	suga then announced that from    5   july onwards japan will  be       in  a      state of   emergency
2024-02-06 05:35:49,001 	Text Hypothesis :	**** **** ********* **** however the end  of      the   covid pandemic the rising cases were postponed
2024-02-06 05:35:49,001 	Text Alignment  :	D    D    D         D    S       S   S    S       S     S     S        S   S      S     S    S        
2024-02-06 05:35:49,001 ========================================================================================================================
2024-02-06 05:35:49,001 Logging Sequence: 56_17.00
2024-02-06 05:35:49,002 	Gloss Reference :	A B+C+D+E  
2024-02-06 05:35:49,002 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-06 05:35:49,002 	Gloss Alignment :	  S        
2024-02-06 05:35:49,002 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 05:35:49,003 	Text Reference  :	****** **** it    was held  at      mumbai's wankhede stadium
2024-02-06 05:35:49,003 	Text Hypothesis :	people were glued to  their screens for      the      match  
2024-02-06 05:35:49,003 	Text Alignment  :	I      I    S     S   S     S       S        S        S      
2024-02-06 05:35:49,003 ========================================================================================================================
2024-02-06 05:35:49,003 Logging Sequence: 152_73.00
2024-02-06 05:35:49,004 	Gloss Reference :	A B+C+D+E
2024-02-06 05:35:49,004 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 05:35:49,004 	Gloss Alignment :	         
2024-02-06 05:35:49,004 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 05:35:49,005 	Text Reference  :	**** **** **** ***** *** eventually he      too got      out    by   shaheen  afridi
2024-02-06 05:35:49,005 	Text Hypothesis :	they will face fines and hearing    friends of  pakistan trophy with pakistan times 
2024-02-06 05:35:49,005 	Text Alignment  :	I    I    I    I     I   S          S       S   S        S      S    S        S     
2024-02-06 05:35:49,005 ========================================================================================================================
2024-02-06 05:35:49,703 Epoch 1059: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.52 
2024-02-06 05:35:49,703 EPOCH 1060
2024-02-06 05:36:01,039 Epoch 1060: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-06 05:36:01,039 EPOCH 1061
2024-02-06 05:36:11,769 Epoch 1061: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-06 05:36:11,769 EPOCH 1062
2024-02-06 05:36:22,417 Epoch 1062: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.35 
2024-02-06 05:36:22,418 EPOCH 1063
2024-02-06 05:36:33,005 Epoch 1063: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-06 05:36:33,006 EPOCH 1064
2024-02-06 05:36:43,953 Epoch 1064: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 05:36:43,953 EPOCH 1065
2024-02-06 05:36:51,991 [Epoch: 1065 Step: 00018100] Batch Recognition Loss:   0.000378 => Gls Tokens per Sec:      923 || Batch Translation Loss:   0.016839 => Txt Tokens per Sec:     2698 || Lr: 0.000100
2024-02-06 05:36:54,655 Epoch 1065: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 05:36:54,655 EPOCH 1066
2024-02-06 05:37:05,500 Epoch 1066: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 05:37:05,500 EPOCH 1067
2024-02-06 05:37:16,179 Epoch 1067: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-06 05:37:16,180 EPOCH 1068
2024-02-06 05:37:26,982 Epoch 1068: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-06 05:37:26,982 EPOCH 1069
2024-02-06 05:37:37,672 Epoch 1069: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-06 05:37:37,672 EPOCH 1070
2024-02-06 05:37:48,414 Epoch 1070: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-06 05:37:48,414 EPOCH 1071
2024-02-06 05:37:54,859 [Epoch: 1071 Step: 00018200] Batch Recognition Loss:   0.000262 => Gls Tokens per Sec:      953 || Batch Translation Loss:   0.022726 => Txt Tokens per Sec:     2611 || Lr: 0.000100
2024-02-06 05:37:59,243 Epoch 1071: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-06 05:37:59,243 EPOCH 1072
2024-02-06 05:38:09,989 Epoch 1072: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-06 05:38:09,989 EPOCH 1073
2024-02-06 05:38:20,748 Epoch 1073: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.70 
2024-02-06 05:38:20,749 EPOCH 1074
2024-02-06 05:38:31,420 Epoch 1074: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.84 
2024-02-06 05:38:31,421 EPOCH 1075
2024-02-06 05:38:42,299 Epoch 1075: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.81 
2024-02-06 05:38:42,299 EPOCH 1076
2024-02-06 05:38:53,163 Epoch 1076: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.71 
2024-02-06 05:38:53,163 EPOCH 1077
2024-02-06 05:38:59,241 [Epoch: 1077 Step: 00018300] Batch Recognition Loss:   0.000289 => Gls Tokens per Sec:      800 || Batch Translation Loss:   0.026765 => Txt Tokens per Sec:     2438 || Lr: 0.000100
2024-02-06 05:39:03,839 Epoch 1077: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.70 
2024-02-06 05:39:03,839 EPOCH 1078
2024-02-06 05:39:14,462 Epoch 1078: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-06 05:39:14,463 EPOCH 1079
2024-02-06 05:39:25,179 Epoch 1079: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-06 05:39:25,180 EPOCH 1080
2024-02-06 05:39:35,894 Epoch 1080: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-06 05:39:35,894 EPOCH 1081
2024-02-06 05:39:46,575 Epoch 1081: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.85 
2024-02-06 05:39:46,575 EPOCH 1082
2024-02-06 05:39:57,206 Epoch 1082: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.15 
2024-02-06 05:39:57,207 EPOCH 1083
2024-02-06 05:39:58,339 [Epoch: 1083 Step: 00018400] Batch Recognition Loss:   0.000269 => Gls Tokens per Sec:     3395 || Batch Translation Loss:   0.065519 => Txt Tokens per Sec:     8858 || Lr: 0.000100
2024-02-06 05:40:07,783 Epoch 1083: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.00 
2024-02-06 05:40:07,783 EPOCH 1084
2024-02-06 05:40:18,429 Epoch 1084: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-06 05:40:18,429 EPOCH 1085
2024-02-06 05:40:29,076 Epoch 1085: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.36 
2024-02-06 05:40:29,077 EPOCH 1086
2024-02-06 05:40:39,833 Epoch 1086: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.29 
2024-02-06 05:40:39,834 EPOCH 1087
2024-02-06 05:40:50,522 Epoch 1087: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.24 
2024-02-06 05:40:50,523 EPOCH 1088
2024-02-06 05:41:01,282 Epoch 1088: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.94 
2024-02-06 05:41:01,282 EPOCH 1089
2024-02-06 05:41:06,354 [Epoch: 1089 Step: 00018500] Batch Recognition Loss:   0.000620 => Gls Tokens per Sec:      454 || Batch Translation Loss:   0.054039 => Txt Tokens per Sec:     1404 || Lr: 0.000100
2024-02-06 05:41:12,070 Epoch 1089: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.86 
2024-02-06 05:41:12,071 EPOCH 1090
2024-02-06 05:41:22,785 Epoch 1090: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.82 
2024-02-06 05:41:22,785 EPOCH 1091
2024-02-06 05:41:33,754 Epoch 1091: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.72 
2024-02-06 05:41:33,755 EPOCH 1092
2024-02-06 05:41:44,610 Epoch 1092: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-06 05:41:44,610 EPOCH 1093
2024-02-06 05:41:55,192 Epoch 1093: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.11 
2024-02-06 05:41:55,193 EPOCH 1094
2024-02-06 05:42:06,231 Epoch 1094: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.88 
2024-02-06 05:42:06,231 EPOCH 1095
2024-02-06 05:42:06,727 [Epoch: 1095 Step: 00018600] Batch Recognition Loss:   0.000295 => Gls Tokens per Sec:     2595 || Batch Translation Loss:   0.016823 => Txt Tokens per Sec:     6803 || Lr: 0.000100
2024-02-06 05:42:17,131 Epoch 1095: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.75 
2024-02-06 05:42:17,131 EPOCH 1096
2024-02-06 05:42:27,962 Epoch 1096: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.79 
2024-02-06 05:42:27,963 EPOCH 1097
2024-02-06 05:42:38,910 Epoch 1097: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.02 
2024-02-06 05:42:38,911 EPOCH 1098
2024-02-06 05:42:49,771 Epoch 1098: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-06 05:42:49,772 EPOCH 1099
2024-02-06 05:43:00,707 Epoch 1099: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.48 
2024-02-06 05:43:00,708 EPOCH 1100
2024-02-06 05:43:11,255 [Epoch: 1100 Step: 00018700] Batch Recognition Loss:   0.000260 => Gls Tokens per Sec:     1007 || Batch Translation Loss:   0.029576 => Txt Tokens per Sec:     2786 || Lr: 0.000100
2024-02-06 05:43:11,256 Epoch 1100: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-06 05:43:11,256 EPOCH 1101
2024-02-06 05:43:21,908 Epoch 1101: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.36 
2024-02-06 05:43:21,909 EPOCH 1102
2024-02-06 05:43:32,601 Epoch 1102: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.52 
2024-02-06 05:43:32,601 EPOCH 1103
2024-02-06 05:43:43,481 Epoch 1103: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-06 05:43:43,482 EPOCH 1104
2024-02-06 05:43:53,903 Epoch 1104: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.38 
2024-02-06 05:43:53,904 EPOCH 1105
2024-02-06 05:44:04,701 Epoch 1105: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.49 
2024-02-06 05:44:04,702 EPOCH 1106
2024-02-06 05:44:15,098 [Epoch: 1106 Step: 00018800] Batch Recognition Loss:   0.000761 => Gls Tokens per Sec:      899 || Batch Translation Loss:   0.114718 => Txt Tokens per Sec:     2524 || Lr: 0.000100
2024-02-06 05:44:15,503 Epoch 1106: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-06 05:44:15,503 EPOCH 1107
2024-02-06 05:44:26,344 Epoch 1107: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.74 
2024-02-06 05:44:26,345 EPOCH 1108
2024-02-06 05:44:37,298 Epoch 1108: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.65 
2024-02-06 05:44:37,299 EPOCH 1109
2024-02-06 05:44:48,027 Epoch 1109: Total Training Recognition Loss 0.06  Total Training Translation Loss 0.76 
2024-02-06 05:44:48,028 EPOCH 1110
2024-02-06 05:44:58,996 Epoch 1110: Total Training Recognition Loss 0.14  Total Training Translation Loss 0.93 
2024-02-06 05:44:58,997 EPOCH 1111
2024-02-06 05:45:09,477 Epoch 1111: Total Training Recognition Loss 0.70  Total Training Translation Loss 0.88 
2024-02-06 05:45:09,478 EPOCH 1112
2024-02-06 05:45:19,430 [Epoch: 1112 Step: 00018900] Batch Recognition Loss:   0.003071 => Gls Tokens per Sec:      810 || Batch Translation Loss:   0.044169 => Txt Tokens per Sec:     2275 || Lr: 0.000100
2024-02-06 05:45:20,299 Epoch 1112: Total Training Recognition Loss 2.29  Total Training Translation Loss 0.90 
2024-02-06 05:45:20,299 EPOCH 1113
2024-02-06 05:45:30,969 Epoch 1113: Total Training Recognition Loss 5.88  Total Training Translation Loss 1.63 
2024-02-06 05:45:30,970 EPOCH 1114
2024-02-06 05:45:41,497 Epoch 1114: Total Training Recognition Loss 2.51  Total Training Translation Loss 1.82 
2024-02-06 05:45:41,497 EPOCH 1115
2024-02-06 05:45:52,451 Epoch 1115: Total Training Recognition Loss 6.39  Total Training Translation Loss 15.45 
2024-02-06 05:45:52,452 EPOCH 1116
2024-02-06 05:46:03,300 Epoch 1116: Total Training Recognition Loss 1.66  Total Training Translation Loss 7.43 
2024-02-06 05:46:03,301 EPOCH 1117
2024-02-06 05:46:13,994 Epoch 1117: Total Training Recognition Loss 2.67  Total Training Translation Loss 3.56 
2024-02-06 05:46:13,995 EPOCH 1118
2024-02-06 05:46:21,689 [Epoch: 1118 Step: 00019000] Batch Recognition Loss:   0.018816 => Gls Tokens per Sec:      881 || Batch Translation Loss:   0.124882 => Txt Tokens per Sec:     2530 || Lr: 0.000100
2024-02-06 05:46:24,661 Epoch 1118: Total Training Recognition Loss 3.84  Total Training Translation Loss 2.10 
2024-02-06 05:46:24,661 EPOCH 1119
2024-02-06 05:46:35,461 Epoch 1119: Total Training Recognition Loss 0.65  Total Training Translation Loss 1.54 
2024-02-06 05:46:35,462 EPOCH 1120
2024-02-06 05:46:46,045 Epoch 1120: Total Training Recognition Loss 0.45  Total Training Translation Loss 0.92 
2024-02-06 05:46:46,046 EPOCH 1121
2024-02-06 05:46:56,680 Epoch 1121: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.80 
2024-02-06 05:46:56,681 EPOCH 1122
2024-02-06 05:47:07,370 Epoch 1122: Total Training Recognition Loss 0.09  Total Training Translation Loss 0.64 
2024-02-06 05:47:07,371 EPOCH 1123
2024-02-06 05:47:18,243 Epoch 1123: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.50 
2024-02-06 05:47:18,244 EPOCH 1124
2024-02-06 05:47:26,003 [Epoch: 1124 Step: 00019100] Batch Recognition Loss:   0.000209 => Gls Tokens per Sec:      709 || Batch Translation Loss:   0.031996 => Txt Tokens per Sec:     1975 || Lr: 0.000100
2024-02-06 05:47:29,273 Epoch 1124: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.46 
2024-02-06 05:47:29,274 EPOCH 1125
2024-02-06 05:47:40,054 Epoch 1125: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.41 
2024-02-06 05:47:40,055 EPOCH 1126
2024-02-06 05:47:50,745 Epoch 1126: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.43 
2024-02-06 05:47:50,746 EPOCH 1127
2024-02-06 05:48:01,767 Epoch 1127: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.37 
2024-02-06 05:48:01,768 EPOCH 1128
2024-02-06 05:48:12,410 Epoch 1128: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.37 
2024-02-06 05:48:12,411 EPOCH 1129
2024-02-06 05:48:23,082 Epoch 1129: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.36 
2024-02-06 05:48:23,082 EPOCH 1130
2024-02-06 05:48:26,064 [Epoch: 1130 Step: 00019200] Batch Recognition Loss:   0.000182 => Gls Tokens per Sec:     1503 || Batch Translation Loss:   0.015015 => Txt Tokens per Sec:     3820 || Lr: 0.000100
2024-02-06 05:48:33,871 Epoch 1130: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.32 
2024-02-06 05:48:33,872 EPOCH 1131
2024-02-06 05:48:44,813 Epoch 1131: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.28 
2024-02-06 05:48:44,813 EPOCH 1132
2024-02-06 05:48:55,795 Epoch 1132: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.26 
2024-02-06 05:48:55,796 EPOCH 1133
2024-02-06 05:49:06,367 Epoch 1133: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 05:49:06,368 EPOCH 1134
2024-02-06 05:49:16,937 Epoch 1134: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.29 
2024-02-06 05:49:16,938 EPOCH 1135
2024-02-06 05:49:27,601 Epoch 1135: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.31 
2024-02-06 05:49:27,602 EPOCH 1136
2024-02-06 05:49:32,128 [Epoch: 1136 Step: 00019300] Batch Recognition Loss:   0.000467 => Gls Tokens per Sec:      707 || Batch Translation Loss:   0.020282 => Txt Tokens per Sec:     2110 || Lr: 0.000100
2024-02-06 05:49:38,468 Epoch 1136: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.33 
2024-02-06 05:49:38,469 EPOCH 1137
2024-02-06 05:49:49,085 Epoch 1137: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.27 
2024-02-06 05:49:49,086 EPOCH 1138
2024-02-06 05:49:59,758 Epoch 1138: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 05:49:59,758 EPOCH 1139
2024-02-06 05:50:10,530 Epoch 1139: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.57 
2024-02-06 05:50:10,530 EPOCH 1140
2024-02-06 05:50:21,360 Epoch 1140: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.27 
2024-02-06 05:50:21,360 EPOCH 1141
2024-02-06 05:50:32,233 Epoch 1141: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 05:50:32,234 EPOCH 1142
2024-02-06 05:50:32,865 [Epoch: 1142 Step: 00019400] Batch Recognition Loss:   0.000223 => Gls Tokens per Sec:     3048 || Batch Translation Loss:   0.005780 => Txt Tokens per Sec:     7771 || Lr: 0.000100
2024-02-06 05:50:42,979 Epoch 1142: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.27 
2024-02-06 05:50:42,979 EPOCH 1143
2024-02-06 05:50:53,603 Epoch 1143: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.28 
2024-02-06 05:50:53,604 EPOCH 1144
2024-02-06 05:51:04,348 Epoch 1144: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 05:51:04,349 EPOCH 1145
2024-02-06 05:51:15,201 Epoch 1145: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-06 05:51:15,202 EPOCH 1146
2024-02-06 05:51:26,109 Epoch 1146: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-06 05:51:26,110 EPOCH 1147
2024-02-06 05:51:36,667 Epoch 1147: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.35 
2024-02-06 05:51:36,667 EPOCH 1148
2024-02-06 05:51:38,371 [Epoch: 1148 Step: 00019500] Batch Recognition Loss:   0.000736 => Gls Tokens per Sec:      376 || Batch Translation Loss:   0.021132 => Txt Tokens per Sec:     1274 || Lr: 0.000100
2024-02-06 05:51:47,530 Epoch 1148: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.31 
2024-02-06 05:51:47,531 EPOCH 1149
2024-02-06 05:51:58,218 Epoch 1149: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 05:51:58,218 EPOCH 1150
2024-02-06 05:52:09,070 Epoch 1150: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.27 
2024-02-06 05:52:09,071 EPOCH 1151
2024-02-06 05:52:19,716 Epoch 1151: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 05:52:19,717 EPOCH 1152
2024-02-06 05:52:30,672 Epoch 1152: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-06 05:52:30,672 EPOCH 1153
2024-02-06 05:52:40,915 [Epoch: 1153 Step: 00019600] Batch Recognition Loss:   0.000865 => Gls Tokens per Sec:      974 || Batch Translation Loss:   0.020766 => Txt Tokens per Sec:     2670 || Lr: 0.000100
2024-02-06 05:52:41,246 Epoch 1153: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-06 05:52:41,246 EPOCH 1154
2024-02-06 05:52:52,113 Epoch 1154: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-06 05:52:52,114 EPOCH 1155
2024-02-06 05:53:02,844 Epoch 1155: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-06 05:53:02,844 EPOCH 1156
2024-02-06 05:53:13,550 Epoch 1156: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 05:53:13,551 EPOCH 1157
2024-02-06 05:53:23,915 Epoch 1157: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 05:53:23,916 EPOCH 1158
2024-02-06 05:53:34,744 Epoch 1158: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 05:53:34,745 EPOCH 1159
2024-02-06 05:53:44,806 [Epoch: 1159 Step: 00019700] Batch Recognition Loss:   0.000248 => Gls Tokens per Sec:      865 || Batch Translation Loss:   0.006884 => Txt Tokens per Sec:     2399 || Lr: 0.000100
2024-02-06 05:53:45,618 Epoch 1159: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 05:53:45,618 EPOCH 1160
2024-02-06 05:53:56,255 Epoch 1160: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 05:53:56,256 EPOCH 1161
2024-02-06 05:54:06,998 Epoch 1161: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 05:54:06,999 EPOCH 1162
2024-02-06 05:54:17,570 Epoch 1162: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 05:54:17,570 EPOCH 1163
2024-02-06 05:54:28,302 Epoch 1163: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.23 
2024-02-06 05:54:28,302 EPOCH 1164
2024-02-06 05:54:38,995 Epoch 1164: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 05:54:38,995 EPOCH 1165
2024-02-06 05:54:48,643 [Epoch: 1165 Step: 00019800] Batch Recognition Loss:   0.000597 => Gls Tokens per Sec:      769 || Batch Translation Loss:   0.015319 => Txt Tokens per Sec:     2150 || Lr: 0.000100
2024-02-06 05:54:49,781 Epoch 1165: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-06 05:54:49,781 EPOCH 1166
2024-02-06 05:55:00,665 Epoch 1166: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.15 
2024-02-06 05:55:00,666 EPOCH 1167
2024-02-06 05:55:11,185 Epoch 1167: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.40 
2024-02-06 05:55:11,185 EPOCH 1168
2024-02-06 05:55:21,771 Epoch 1168: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.29 
2024-02-06 05:55:21,771 EPOCH 1169
2024-02-06 05:55:32,634 Epoch 1169: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.12 
2024-02-06 05:55:32,635 EPOCH 1170
2024-02-06 05:55:43,346 Epoch 1170: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.06 
2024-02-06 05:55:43,347 EPOCH 1171
2024-02-06 05:55:51,143 [Epoch: 1171 Step: 00019900] Batch Recognition Loss:   0.002655 => Gls Tokens per Sec:      788 || Batch Translation Loss:   0.075659 => Txt Tokens per Sec:     2266 || Lr: 0.000100
2024-02-06 05:55:53,985 Epoch 1171: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.88 
2024-02-06 05:55:53,986 EPOCH 1172
2024-02-06 05:56:04,602 Epoch 1172: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.89 
2024-02-06 05:56:04,603 EPOCH 1173
2024-02-06 05:56:15,362 Epoch 1173: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.60 
2024-02-06 05:56:15,363 EPOCH 1174
2024-02-06 05:56:26,135 Epoch 1174: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.32 
2024-02-06 05:56:26,136 EPOCH 1175
2024-02-06 05:56:36,896 Epoch 1175: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.19 
2024-02-06 05:56:36,897 EPOCH 1176
2024-02-06 05:56:47,711 Epoch 1176: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.75 
2024-02-06 05:56:47,712 EPOCH 1177
2024-02-06 05:56:54,923 [Epoch: 1177 Step: 00020000] Batch Recognition Loss:   0.000842 => Gls Tokens per Sec:      674 || Batch Translation Loss:   0.066685 => Txt Tokens per Sec:     1833 || Lr: 0.000100
2024-02-06 05:57:35,447 Validation result at epoch 1177, step    20000: duration: 40.5232s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.58054	Translation Loss: 92957.46094	PPL: 10770.90723
	Eval Metric: BLEU
	WER 2.75	(DEL: 0.00,	INS: 0.00,	SUB: 2.75)
	BLEU-4 0.62	(BLEU-1: 11.84,	BLEU-2: 3.90,	BLEU-3: 1.52,	BLEU-4: 0.62)
	CHRF 17.57	ROUGE 9.99
2024-02-06 05:57:35,449 Logging Recognition and Translation Outputs
2024-02-06 05:57:35,449 ========================================================================================================================
2024-02-06 05:57:35,449 Logging Sequence: 120_7.00
2024-02-06 05:57:35,450 	Gloss Reference :	A B+C+D+E
2024-02-06 05:57:35,450 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 05:57:35,450 	Gloss Alignment :	         
2024-02-06 05:57:35,450 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 05:57:35,451 	Text Reference  :	he had tested positive for covid-19 on    may 19   
2024-02-06 05:57:35,451 	Text Hypothesis :	he *** ****** was      in  the      state of  india
2024-02-06 05:57:35,451 	Text Alignment  :	   D   D      S        S   S        S     S   S    
2024-02-06 05:57:35,452 ========================================================================================================================
2024-02-06 05:57:35,452 Logging Sequence: 148_186.00
2024-02-06 05:57:35,452 	Gloss Reference :	A B+C+D+E
2024-02-06 05:57:35,452 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 05:57:35,452 	Gloss Alignment :	         
2024-02-06 05:57:35,452 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 05:57:35,455 	Text Reference  :	*** ********* ***** siraj      also took  four wickets in 1 over thus becoming the  record-holder for most wickets in an over in odis
2024-02-06 05:57:35,455 	Text Hypothesis :	the badminton world federation bwf  which is   held    in * uae  with him      from india         to  15   wickets ** ** **** ** ****
2024-02-06 05:57:35,455 	Text Alignment  :	I   I         I     S          S    S     S    S          D S    S    S        S    S             S   S            D  D  D    D  D   
2024-02-06 05:57:35,455 ========================================================================================================================
2024-02-06 05:57:35,455 Logging Sequence: 67_73.00
2024-02-06 05:57:35,455 	Gloss Reference :	A B+C+D+E
2024-02-06 05:57:35,456 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 05:57:35,456 	Gloss Alignment :	         
2024-02-06 05:57:35,456 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 05:57:35,456 	Text Reference  :	*** ****** **** in       his tweet he   also said
2024-02-06 05:57:35,456 	Text Hypothesis :	the finals were supposed to  be    held in   uae 
2024-02-06 05:57:35,457 	Text Alignment  :	I   I      I    S        S   S     S    S    S   
2024-02-06 05:57:35,457 ========================================================================================================================
2024-02-06 05:57:35,457 Logging Sequence: 164_526.00
2024-02-06 05:57:35,457 	Gloss Reference :	A B+C+D+E
2024-02-06 05:57:35,457 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 05:57:35,458 	Gloss Alignment :	         
2024-02-06 05:57:35,458 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 05:57:35,459 	Text Reference  :	you are aware that viacom18 bought the *** **** *** **** broadcast rights of  ipl   
2024-02-06 05:57:35,459 	Text Hypothesis :	in  the first ball bounces  on     the off side and then goes      on     the stumps
2024-02-06 05:57:35,459 	Text Alignment  :	S   S   S     S    S        S          I   I    I   I    S         S      S   S     
2024-02-06 05:57:35,460 ========================================================================================================================
2024-02-06 05:57:35,460 Logging Sequence: 108_28.00
2024-02-06 05:57:35,460 	Gloss Reference :	A B+C+D+E
2024-02-06 05:57:35,460 	Gloss Hypothesis:	A B+C+D  
2024-02-06 05:57:35,460 	Gloss Alignment :	  S      
2024-02-06 05:57:35,460 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 05:57:35,462 	Text Reference  :	the 10 teams bought 204 players including 67      foreign players after spending a  total of     rs 55170 crore   
2024-02-06 05:57:35,462 	Text Hypothesis :	the ** ***** ****** *** ******* ipl       matches will    be      held  only     in pune  mumbai at 25    capacity
2024-02-06 05:57:35,462 	Text Alignment  :	    D  D     D      D   D       S         S       S       S       S     S        S  S     S      S  S     S       
2024-02-06 05:57:35,462 ========================================================================================================================
2024-02-06 05:57:39,317 Epoch 1177: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.64 
2024-02-06 05:57:39,317 EPOCH 1178
2024-02-06 05:57:50,127 Epoch 1178: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-06 05:57:50,128 EPOCH 1179
2024-02-06 05:58:00,706 Epoch 1179: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-06 05:58:00,706 EPOCH 1180
2024-02-06 05:58:11,368 Epoch 1180: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-06 05:58:11,369 EPOCH 1181
2024-02-06 05:58:22,134 Epoch 1181: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-06 05:58:22,134 EPOCH 1182
2024-02-06 05:58:32,611 Epoch 1182: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-06 05:58:32,612 EPOCH 1183
2024-02-06 05:58:36,640 [Epoch: 1183 Step: 00020100] Batch Recognition Loss:   0.000283 => Gls Tokens per Sec:      953 || Batch Translation Loss:   0.015023 => Txt Tokens per Sec:     2461 || Lr: 0.000100
2024-02-06 05:58:43,243 Epoch 1183: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-06 05:58:43,243 EPOCH 1184
2024-02-06 05:58:53,644 Epoch 1184: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-06 05:58:53,645 EPOCH 1185
2024-02-06 05:59:04,451 Epoch 1185: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-06 05:59:04,452 EPOCH 1186
2024-02-06 05:59:14,982 Epoch 1186: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-06 05:59:14,983 EPOCH 1187
2024-02-06 05:59:25,621 Epoch 1187: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.30 
2024-02-06 05:59:25,621 EPOCH 1188
2024-02-06 05:59:36,433 Epoch 1188: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 05:59:36,434 EPOCH 1189
2024-02-06 05:59:38,786 [Epoch: 1189 Step: 00020200] Batch Recognition Loss:   0.000156 => Gls Tokens per Sec:     1090 || Batch Translation Loss:   0.029765 => Txt Tokens per Sec:     3330 || Lr: 0.000100
2024-02-06 05:59:47,055 Epoch 1189: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-06 05:59:47,056 EPOCH 1190
2024-02-06 05:59:57,815 Epoch 1190: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-06 05:59:57,816 EPOCH 1191
2024-02-06 06:00:08,665 Epoch 1191: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-06 06:00:08,665 EPOCH 1192
2024-02-06 06:00:19,509 Epoch 1192: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 06:00:19,510 EPOCH 1193
2024-02-06 06:00:29,946 Epoch 1193: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 06:00:29,946 EPOCH 1194
2024-02-06 06:00:40,676 Epoch 1194: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.30 
2024-02-06 06:00:40,677 EPOCH 1195
2024-02-06 06:00:41,103 [Epoch: 1195 Step: 00020300] Batch Recognition Loss:   0.000230 => Gls Tokens per Sec:     3012 || Batch Translation Loss:   0.011104 => Txt Tokens per Sec:     8144 || Lr: 0.000100
2024-02-06 06:00:51,178 Epoch 1195: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.33 
2024-02-06 06:00:51,179 EPOCH 1196
2024-02-06 06:01:01,835 Epoch 1196: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-06 06:01:01,836 EPOCH 1197
2024-02-06 06:01:12,693 Epoch 1197: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-06 06:01:12,693 EPOCH 1198
2024-02-06 06:01:23,300 Epoch 1198: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 06:01:23,301 EPOCH 1199
2024-02-06 06:01:34,085 Epoch 1199: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-06 06:01:34,085 EPOCH 1200
2024-02-06 06:01:45,065 [Epoch: 1200 Step: 00020400] Batch Recognition Loss:   0.000243 => Gls Tokens per Sec:      967 || Batch Translation Loss:   0.024913 => Txt Tokens per Sec:     2677 || Lr: 0.000100
2024-02-06 06:01:45,065 Epoch 1200: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.41 
2024-02-06 06:01:45,066 EPOCH 1201
2024-02-06 06:01:55,882 Epoch 1201: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-06 06:01:55,882 EPOCH 1202
2024-02-06 06:02:06,660 Epoch 1202: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-06 06:02:06,661 EPOCH 1203
2024-02-06 06:02:17,499 Epoch 1203: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.96 
2024-02-06 06:02:17,500 EPOCH 1204
2024-02-06 06:02:28,110 Epoch 1204: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.37 
2024-02-06 06:02:28,111 EPOCH 1205
2024-02-06 06:02:38,945 Epoch 1205: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.84 
2024-02-06 06:02:38,945 EPOCH 1206
2024-02-06 06:02:49,102 [Epoch: 1206 Step: 00020500] Batch Recognition Loss:   0.000318 => Gls Tokens per Sec:      920 || Batch Translation Loss:   0.083060 => Txt Tokens per Sec:     2586 || Lr: 0.000100
2024-02-06 06:02:49,468 Epoch 1206: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.07 
2024-02-06 06:02:49,468 EPOCH 1207
2024-02-06 06:03:00,236 Epoch 1207: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.09 
2024-02-06 06:03:00,237 EPOCH 1208
2024-02-06 06:03:10,834 Epoch 1208: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.50 
2024-02-06 06:03:10,835 EPOCH 1209
2024-02-06 06:03:21,454 Epoch 1209: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.46 
2024-02-06 06:03:21,455 EPOCH 1210
2024-02-06 06:03:32,036 Epoch 1210: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.32 
2024-02-06 06:03:32,037 EPOCH 1211
2024-02-06 06:03:42,647 Epoch 1211: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.14 
2024-02-06 06:03:42,648 EPOCH 1212
2024-02-06 06:03:49,124 [Epoch: 1212 Step: 00020600] Batch Recognition Loss:   0.000487 => Gls Tokens per Sec:     1245 || Batch Translation Loss:   0.033169 => Txt Tokens per Sec:     3256 || Lr: 0.000100
2024-02-06 06:03:53,224 Epoch 1212: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.13 
2024-02-06 06:03:53,224 EPOCH 1213
2024-02-06 06:04:03,943 Epoch 1213: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.92 
2024-02-06 06:04:03,943 EPOCH 1214
2024-02-06 06:04:14,428 Epoch 1214: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.85 
2024-02-06 06:04:14,428 EPOCH 1215
2024-02-06 06:04:25,054 Epoch 1215: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-06 06:04:25,055 EPOCH 1216
2024-02-06 06:04:35,770 Epoch 1216: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.47 
2024-02-06 06:04:35,770 EPOCH 1217
2024-02-06 06:04:46,383 Epoch 1217: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-06 06:04:46,383 EPOCH 1218
2024-02-06 06:04:54,260 [Epoch: 1218 Step: 00020700] Batch Recognition Loss:   0.000491 => Gls Tokens per Sec:      861 || Batch Translation Loss:   0.008873 => Txt Tokens per Sec:     2373 || Lr: 0.000100
2024-02-06 06:04:57,315 Epoch 1218: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 06:04:57,316 EPOCH 1219
2024-02-06 06:05:08,029 Epoch 1219: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 06:05:08,029 EPOCH 1220
2024-02-06 06:05:18,678 Epoch 1220: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 06:05:18,679 EPOCH 1221
2024-02-06 06:05:29,341 Epoch 1221: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 06:05:29,342 EPOCH 1222
2024-02-06 06:05:40,416 Epoch 1222: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-06 06:05:40,417 EPOCH 1223
2024-02-06 06:05:51,062 Epoch 1223: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-06 06:05:51,063 EPOCH 1224
2024-02-06 06:05:58,647 [Epoch: 1224 Step: 00020800] Batch Recognition Loss:   0.000227 => Gls Tokens per Sec:      725 || Batch Translation Loss:   0.101140 => Txt Tokens per Sec:     2081 || Lr: 0.000100
2024-02-06 06:06:01,889 Epoch 1224: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-06 06:06:01,890 EPOCH 1225
2024-02-06 06:06:12,678 Epoch 1225: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-06 06:06:12,678 EPOCH 1226
2024-02-06 06:06:23,249 Epoch 1226: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-06 06:06:23,249 EPOCH 1227
2024-02-06 06:06:34,008 Epoch 1227: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-06 06:06:34,009 EPOCH 1228
2024-02-06 06:06:44,797 Epoch 1228: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-06 06:06:44,798 EPOCH 1229
2024-02-06 06:06:55,810 Epoch 1229: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-06 06:06:55,811 EPOCH 1230
2024-02-06 06:07:00,853 [Epoch: 1230 Step: 00020900] Batch Recognition Loss:   0.000188 => Gls Tokens per Sec:      837 || Batch Translation Loss:   0.092201 => Txt Tokens per Sec:     2282 || Lr: 0.000100
2024-02-06 06:07:06,691 Epoch 1230: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-06 06:07:06,692 EPOCH 1231
2024-02-06 06:07:17,370 Epoch 1231: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-06 06:07:17,370 EPOCH 1232
2024-02-06 06:07:28,066 Epoch 1232: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-06 06:07:28,067 EPOCH 1233
2024-02-06 06:07:38,714 Epoch 1233: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-06 06:07:38,715 EPOCH 1234
2024-02-06 06:07:49,414 Epoch 1234: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-06 06:07:49,414 EPOCH 1235
2024-02-06 06:08:00,351 Epoch 1235: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-06 06:08:00,352 EPOCH 1236
2024-02-06 06:08:04,755 [Epoch: 1236 Step: 00021000] Batch Recognition Loss:   0.000411 => Gls Tokens per Sec:      727 || Batch Translation Loss:   0.094782 => Txt Tokens per Sec:     2208 || Lr: 0.000100
2024-02-06 06:08:11,217 Epoch 1236: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-06 06:08:11,218 EPOCH 1237
2024-02-06 06:08:21,782 Epoch 1237: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-06 06:08:21,783 EPOCH 1238
2024-02-06 06:08:32,660 Epoch 1238: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-06 06:08:32,661 EPOCH 1239
2024-02-06 06:08:43,390 Epoch 1239: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.87 
2024-02-06 06:08:43,391 EPOCH 1240
2024-02-06 06:08:54,064 Epoch 1240: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.73 
2024-02-06 06:08:54,064 EPOCH 1241
2024-02-06 06:09:04,863 Epoch 1241: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-06 06:09:04,863 EPOCH 1242
2024-02-06 06:09:05,388 [Epoch: 1242 Step: 00021100] Batch Recognition Loss:   0.000231 => Gls Tokens per Sec:     3664 || Batch Translation Loss:   0.016904 => Txt Tokens per Sec:     9011 || Lr: 0.000100
2024-02-06 06:09:15,586 Epoch 1242: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.70 
2024-02-06 06:09:15,587 EPOCH 1243
2024-02-06 06:09:26,460 Epoch 1243: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.82 
2024-02-06 06:09:26,461 EPOCH 1244
2024-02-06 06:09:37,160 Epoch 1244: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.73 
2024-02-06 06:09:37,161 EPOCH 1245
2024-02-06 06:09:48,136 Epoch 1245: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-06 06:09:48,137 EPOCH 1246
2024-02-06 06:09:58,778 Epoch 1246: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-06 06:09:58,779 EPOCH 1247
2024-02-06 06:10:09,873 Epoch 1247: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.84 
2024-02-06 06:10:09,874 EPOCH 1248
2024-02-06 06:10:10,094 [Epoch: 1248 Step: 00021200] Batch Recognition Loss:   0.000330 => Gls Tokens per Sec:     2922 || Batch Translation Loss:   0.026613 => Txt Tokens per Sec:     8183 || Lr: 0.000100
2024-02-06 06:10:20,476 Epoch 1248: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.70 
2024-02-06 06:10:20,476 EPOCH 1249
2024-02-06 06:10:31,317 Epoch 1249: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-06 06:10:31,318 EPOCH 1250
2024-02-06 06:10:42,279 Epoch 1250: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.98 
2024-02-06 06:10:42,279 EPOCH 1251
2024-02-06 06:10:52,864 Epoch 1251: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.18 
2024-02-06 06:10:52,864 EPOCH 1252
2024-02-06 06:11:03,653 Epoch 1252: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.57 
2024-02-06 06:11:03,654 EPOCH 1253
2024-02-06 06:11:14,455 [Epoch: 1253 Step: 00021300] Batch Recognition Loss:   0.000652 => Gls Tokens per Sec:      924 || Batch Translation Loss:   0.030489 => Txt Tokens per Sec:     2586 || Lr: 0.000100
2024-02-06 06:11:14,618 Epoch 1253: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.18 
2024-02-06 06:11:14,618 EPOCH 1254
2024-02-06 06:11:25,261 Epoch 1254: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.15 
2024-02-06 06:11:25,261 EPOCH 1255
2024-02-06 06:11:35,860 Epoch 1255: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.10 
2024-02-06 06:11:35,860 EPOCH 1256
2024-02-06 06:11:46,905 Epoch 1256: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.25 
2024-02-06 06:11:46,906 EPOCH 1257
2024-02-06 06:11:57,611 Epoch 1257: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.13 
2024-02-06 06:11:57,612 EPOCH 1258
2024-02-06 06:12:08,481 Epoch 1258: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.18 
2024-02-06 06:12:08,481 EPOCH 1259
2024-02-06 06:12:18,450 [Epoch: 1259 Step: 00021400] Batch Recognition Loss:   0.000286 => Gls Tokens per Sec:      873 || Batch Translation Loss:   0.055692 => Txt Tokens per Sec:     2431 || Lr: 0.000100
2024-02-06 06:12:19,156 Epoch 1259: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.89 
2024-02-06 06:12:19,156 EPOCH 1260
2024-02-06 06:12:29,609 Epoch 1260: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-06 06:12:29,609 EPOCH 1261
2024-02-06 06:12:40,555 Epoch 1261: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-06 06:12:40,556 EPOCH 1262
2024-02-06 06:12:51,393 Epoch 1262: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-06 06:12:51,393 EPOCH 1263
2024-02-06 06:13:01,993 Epoch 1263: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-06 06:13:01,994 EPOCH 1264
2024-02-06 06:13:12,699 Epoch 1264: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.84 
2024-02-06 06:13:12,699 EPOCH 1265
2024-02-06 06:13:22,609 [Epoch: 1265 Step: 00021500] Batch Recognition Loss:   0.000255 => Gls Tokens per Sec:      749 || Batch Translation Loss:   0.041800 => Txt Tokens per Sec:     2219 || Lr: 0.000100
2024-02-06 06:13:23,483 Epoch 1265: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-06 06:13:23,483 EPOCH 1266
2024-02-06 06:13:34,301 Epoch 1266: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.76 
2024-02-06 06:13:34,301 EPOCH 1267
2024-02-06 06:13:45,104 Epoch 1267: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.12 
2024-02-06 06:13:45,104 EPOCH 1268
2024-02-06 06:13:55,944 Epoch 1268: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.47 
2024-02-06 06:13:55,944 EPOCH 1269
2024-02-06 06:14:06,843 Epoch 1269: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.41 
2024-02-06 06:14:06,843 EPOCH 1270
2024-02-06 06:14:17,701 Epoch 1270: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.45 
2024-02-06 06:14:17,701 EPOCH 1271
2024-02-06 06:14:25,154 [Epoch: 1271 Step: 00021600] Batch Recognition Loss:   0.000414 => Gls Tokens per Sec:      824 || Batch Translation Loss:   0.134011 => Txt Tokens per Sec:     2266 || Lr: 0.000100
2024-02-06 06:14:28,436 Epoch 1271: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.42 
2024-02-06 06:14:28,436 EPOCH 1272
2024-02-06 06:14:39,089 Epoch 1272: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.08 
2024-02-06 06:14:39,090 EPOCH 1273
2024-02-06 06:14:49,798 Epoch 1273: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.77 
2024-02-06 06:14:49,799 EPOCH 1274
2024-02-06 06:15:00,544 Epoch 1274: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-06 06:15:00,545 EPOCH 1275
2024-02-06 06:15:11,504 Epoch 1275: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-06 06:15:11,505 EPOCH 1276
2024-02-06 06:15:21,918 Epoch 1276: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-06 06:15:21,919 EPOCH 1277
2024-02-06 06:15:27,554 [Epoch: 1277 Step: 00021700] Batch Recognition Loss:   0.000362 => Gls Tokens per Sec:      863 || Batch Translation Loss:   0.037116 => Txt Tokens per Sec:     2316 || Lr: 0.000100
2024-02-06 06:15:32,648 Epoch 1277: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-06 06:15:32,649 EPOCH 1278
2024-02-06 06:15:43,249 Epoch 1278: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-06 06:15:43,250 EPOCH 1279
2024-02-06 06:15:53,598 Epoch 1279: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-06 06:15:53,598 EPOCH 1280
2024-02-06 06:16:04,253 Epoch 1280: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-06 06:16:04,253 EPOCH 1281
2024-02-06 06:16:14,885 Epoch 1281: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-06 06:16:14,886 EPOCH 1282
2024-02-06 06:16:25,867 Epoch 1282: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.02 
2024-02-06 06:16:25,868 EPOCH 1283
2024-02-06 06:16:32,701 [Epoch: 1283 Step: 00021800] Batch Recognition Loss:   0.000308 => Gls Tokens per Sec:      524 || Batch Translation Loss:   0.057927 => Txt Tokens per Sec:     1577 || Lr: 0.000100
2024-02-06 06:16:36,903 Epoch 1283: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.62 
2024-02-06 06:16:36,904 EPOCH 1284
2024-02-06 06:16:47,536 Epoch 1284: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.32 
2024-02-06 06:16:47,536 EPOCH 1285
2024-02-06 06:16:58,420 Epoch 1285: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.53 
2024-02-06 06:16:58,421 EPOCH 1286
2024-02-06 06:17:09,352 Epoch 1286: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.52 
2024-02-06 06:17:09,353 EPOCH 1287
2024-02-06 06:17:20,242 Epoch 1287: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.13 
2024-02-06 06:17:20,242 EPOCH 1288
2024-02-06 06:17:31,180 Epoch 1288: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.23 
2024-02-06 06:17:31,181 EPOCH 1289
2024-02-06 06:17:36,191 [Epoch: 1289 Step: 00021900] Batch Recognition Loss:   0.001574 => Gls Tokens per Sec:      459 || Batch Translation Loss:   0.053868 => Txt Tokens per Sec:     1456 || Lr: 0.000100
2024-02-06 06:17:42,109 Epoch 1289: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.78 
2024-02-06 06:17:42,110 EPOCH 1290
2024-02-06 06:17:53,059 Epoch 1290: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-06 06:17:53,060 EPOCH 1291
2024-02-06 06:18:03,690 Epoch 1291: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.52 
2024-02-06 06:18:03,690 EPOCH 1292
2024-02-06 06:18:14,346 Epoch 1292: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-06 06:18:14,347 EPOCH 1293
2024-02-06 06:18:24,841 Epoch 1293: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-06 06:18:24,842 EPOCH 1294
2024-02-06 06:18:35,197 Epoch 1294: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-06 06:18:35,198 EPOCH 1295
2024-02-06 06:18:37,387 [Epoch: 1295 Step: 00022000] Batch Recognition Loss:   0.000662 => Gls Tokens per Sec:      585 || Batch Translation Loss:   0.018425 => Txt Tokens per Sec:     1852 || Lr: 0.000100
2024-02-06 06:19:17,961 Validation result at epoch 1295, step    22000: duration: 40.5740s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.71131	Translation Loss: 92772.14844	PPL: 10573.38477
	Eval Metric: BLEU
	WER 3.60	(DEL: 0.00,	INS: 0.00,	SUB: 3.60)
	BLEU-4 0.54	(BLEU-1: 11.38,	BLEU-2: 3.45,	BLEU-3: 1.25,	BLEU-4: 0.54)
	CHRF 17.17	ROUGE 9.47
2024-02-06 06:19:17,963 Logging Recognition and Translation Outputs
2024-02-06 06:19:17,963 ========================================================================================================================
2024-02-06 06:19:17,963 Logging Sequence: 179_2.00
2024-02-06 06:19:17,964 	Gloss Reference :	A B+C+D+E
2024-02-06 06:19:17,964 	Gloss Hypothesis:	A B+C+D  
2024-02-06 06:19:17,964 	Gloss Alignment :	  S      
2024-02-06 06:19:17,965 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 06:19:17,966 	Text Reference  :	*** **** ******** ** *** ** *** vinesh phogat is        a    well   known wrestler
2024-02-06 06:19:17,966 	Text Hypothesis :	the vast majority of the of the family member countries were former and   it      
2024-02-06 06:19:17,967 	Text Alignment  :	I   I    I        I  I   I  I   S      S      S         S    S      S     S       
2024-02-06 06:19:17,967 ========================================================================================================================
2024-02-06 06:19:17,967 Logging Sequence: 55_124.00
2024-02-06 06:19:17,967 	Gloss Reference :	A B+C+D+E
2024-02-06 06:19:17,967 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 06:19:17,967 	Gloss Alignment :	         
2024-02-06 06:19:17,967 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 06:19:17,968 	Text Reference  :	**** ** ***** next   to       him with      the patel jersey was ***** ajaz patel
2024-02-06 06:19:17,969 	Text Hypothesis :	many of these places swimwear is  extremely fit when  it     was named as   well 
2024-02-06 06:19:17,969 	Text Alignment  :	I    I  I     S      S        S   S         S   S     S          I     S    S    
2024-02-06 06:19:17,969 ========================================================================================================================
2024-02-06 06:19:17,969 Logging Sequence: 148_105.00
2024-02-06 06:19:17,969 	Gloss Reference :	A B+C+D+E
2024-02-06 06:19:17,969 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 06:19:17,969 	Gloss Alignment :	         
2024-02-06 06:19:17,969 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 06:19:17,971 	Text Reference  :	later with amazing bowling by hardik pandya  and ******* kuldeep yadav sri    lanka were all out       in ** ******* ******* just     50 runs     
2024-02-06 06:19:17,972 	Text Hypothesis :	***** **** ******* ******* ** star   batsmen and bowlers of      the   indian team  were *** dismissed in an innings against pakistan in bengaluru
2024-02-06 06:19:17,972 	Text Alignment  :	D     D    D       D       D  S      S           I       S       S     S      S          D   S            I  I       I       S        S  S        
2024-02-06 06:19:17,972 ========================================================================================================================
2024-02-06 06:19:17,972 Logging Sequence: 125_165.00
2024-02-06 06:19:17,972 	Gloss Reference :	A B+C+D+E
2024-02-06 06:19:17,972 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 06:19:17,972 	Gloss Alignment :	         
2024-02-06 06:19:17,973 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 06:19:17,974 	Text Reference  :	** please do  not target nadeem we speak to each other       and share a good    bond   
2024-02-06 06:19:17,974 	Text Hypothesis :	so here   are not ****** ****** ** ***** ** **** comfortable and ***** i finally mirabai
2024-02-06 06:19:17,974 	Text Alignment  :	I  S      S       D      D      D  D     D  D    S               D     S S       S      
2024-02-06 06:19:17,974 ========================================================================================================================
2024-02-06 06:19:17,974 Logging Sequence: 77_52.00
2024-02-06 06:19:17,974 	Gloss Reference :	A B+C+D+E
2024-02-06 06:19:17,974 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 06:19:17,975 	Gloss Alignment :	         
2024-02-06 06:19:17,975 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 06:19:17,976 	Text Reference  :	kane williamson held down the fort for   hyderabad by scoring 66    runs and  ended the ********** match in      a tie  
2024-02-06 06:19:17,977 	Text Hypothesis :	**** ********** **** **** *** **** after that      an olympic games was  held in    the tournament while playing 6 balls
2024-02-06 06:19:17,977 	Text Alignment  :	D    D          D    D    D   D    S     S         S  S       S     S    S    S         I          S     S       S S    
2024-02-06 06:19:17,977 ========================================================================================================================
2024-02-06 06:19:27,097 Epoch 1295: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-06 06:19:27,098 EPOCH 1296
2024-02-06 06:19:38,139 Epoch 1296: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 06:19:38,140 EPOCH 1297
2024-02-06 06:19:48,896 Epoch 1297: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 06:19:48,897 EPOCH 1298
2024-02-06 06:19:59,718 Epoch 1298: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 06:19:59,718 EPOCH 1299
2024-02-06 06:20:10,522 Epoch 1299: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 06:20:10,522 EPOCH 1300
2024-02-06 06:20:21,049 [Epoch: 1300 Step: 00022100] Batch Recognition Loss:   0.000452 => Gls Tokens per Sec:     1009 || Batch Translation Loss:   0.030558 => Txt Tokens per Sec:     2792 || Lr: 0.000100
2024-02-06 06:20:21,049 Epoch 1300: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 06:20:21,049 EPOCH 1301
2024-02-06 06:20:31,752 Epoch 1301: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 06:20:31,753 EPOCH 1302
2024-02-06 06:20:42,513 Epoch 1302: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 06:20:42,514 EPOCH 1303
2024-02-06 06:20:53,214 Epoch 1303: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-06 06:20:53,215 EPOCH 1304
2024-02-06 06:21:04,046 Epoch 1304: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-06 06:21:04,047 EPOCH 1305
2024-02-06 06:21:14,716 Epoch 1305: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.46 
2024-02-06 06:21:14,716 EPOCH 1306
2024-02-06 06:21:24,766 [Epoch: 1306 Step: 00022200] Batch Recognition Loss:   0.001190 => Gls Tokens per Sec:      929 || Batch Translation Loss:   0.021360 => Txt Tokens per Sec:     2587 || Lr: 0.000100
2024-02-06 06:21:25,216 Epoch 1306: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-06 06:21:25,217 EPOCH 1307
2024-02-06 06:21:35,910 Epoch 1307: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-06 06:21:35,911 EPOCH 1308
2024-02-06 06:21:46,522 Epoch 1308: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 06:21:46,522 EPOCH 1309
2024-02-06 06:21:57,018 Epoch 1309: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-06 06:21:57,019 EPOCH 1310
2024-02-06 06:22:07,551 Epoch 1310: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-06 06:22:07,551 EPOCH 1311
2024-02-06 06:22:18,527 Epoch 1311: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-06 06:22:18,528 EPOCH 1312
2024-02-06 06:22:26,866 [Epoch: 1312 Step: 00022300] Batch Recognition Loss:   0.000161 => Gls Tokens per Sec:      967 || Batch Translation Loss:   0.016238 => Txt Tokens per Sec:     2651 || Lr: 0.000100
2024-02-06 06:22:29,342 Epoch 1312: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-06 06:22:29,342 EPOCH 1313
2024-02-06 06:22:40,083 Epoch 1313: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-06 06:22:40,083 EPOCH 1314
2024-02-06 06:22:50,907 Epoch 1314: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-06 06:22:50,908 EPOCH 1315
2024-02-06 06:23:01,646 Epoch 1315: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-06 06:23:01,646 EPOCH 1316
2024-02-06 06:23:12,419 Epoch 1316: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-06 06:23:12,420 EPOCH 1317
2024-02-06 06:23:23,208 Epoch 1317: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-06 06:23:23,209 EPOCH 1318
2024-02-06 06:23:30,745 [Epoch: 1318 Step: 00022400] Batch Recognition Loss:   0.000229 => Gls Tokens per Sec:      934 || Batch Translation Loss:   0.022624 => Txt Tokens per Sec:     2766 || Lr: 0.000100
2024-02-06 06:23:34,198 Epoch 1318: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-06 06:23:34,198 EPOCH 1319
2024-02-06 06:23:44,907 Epoch 1319: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-06 06:23:44,907 EPOCH 1320
2024-02-06 06:23:55,822 Epoch 1320: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-06 06:23:55,822 EPOCH 1321
2024-02-06 06:24:06,400 Epoch 1321: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-06 06:24:06,400 EPOCH 1322
2024-02-06 06:24:17,166 Epoch 1322: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-06 06:24:17,166 EPOCH 1323
2024-02-06 06:24:27,858 Epoch 1323: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.40 
2024-02-06 06:24:27,859 EPOCH 1324
2024-02-06 06:24:35,214 [Epoch: 1324 Step: 00022500] Batch Recognition Loss:   0.000275 => Gls Tokens per Sec:      748 || Batch Translation Loss:   0.024270 => Txt Tokens per Sec:     2141 || Lr: 0.000100
2024-02-06 06:24:38,601 Epoch 1324: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.57 
2024-02-06 06:24:38,602 EPOCH 1325
2024-02-06 06:24:49,479 Epoch 1325: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-06 06:24:49,479 EPOCH 1326
2024-02-06 06:25:00,015 Epoch 1326: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-06 06:25:00,015 EPOCH 1327
2024-02-06 06:25:10,625 Epoch 1327: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.89 
2024-02-06 06:25:10,625 EPOCH 1328
2024-02-06 06:25:21,361 Epoch 1328: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-06 06:25:21,361 EPOCH 1329
2024-02-06 06:25:32,010 Epoch 1329: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.90 
2024-02-06 06:25:32,011 EPOCH 1330
2024-02-06 06:25:37,617 [Epoch: 1330 Step: 00022600] Batch Recognition Loss:   0.004712 => Gls Tokens per Sec:      753 || Batch Translation Loss:   0.081550 => Txt Tokens per Sec:     2294 || Lr: 0.000100
2024-02-06 06:25:42,849 Epoch 1330: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.93 
2024-02-06 06:25:42,849 EPOCH 1331
2024-02-06 06:25:53,444 Epoch 1331: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.29 
2024-02-06 06:25:53,445 EPOCH 1332
2024-02-06 06:26:03,991 Epoch 1332: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.76 
2024-02-06 06:26:03,992 EPOCH 1333
2024-02-06 06:26:14,556 Epoch 1333: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.32 
2024-02-06 06:26:14,557 EPOCH 1334
2024-02-06 06:26:25,331 Epoch 1334: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.89 
2024-02-06 06:26:25,331 EPOCH 1335
2024-02-06 06:26:36,174 Epoch 1335: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.10 
2024-02-06 06:26:36,174 EPOCH 1336
2024-02-06 06:26:40,667 [Epoch: 1336 Step: 00022700] Batch Recognition Loss:   0.001809 => Gls Tokens per Sec:      713 || Batch Translation Loss:   0.377175 => Txt Tokens per Sec:     2016 || Lr: 0.000100
2024-02-06 06:26:46,974 Epoch 1336: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.28 
2024-02-06 06:26:46,974 EPOCH 1337
2024-02-06 06:26:57,529 Epoch 1337: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.53 
2024-02-06 06:26:57,529 EPOCH 1338
2024-02-06 06:27:08,184 Epoch 1338: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.79 
2024-02-06 06:27:08,185 EPOCH 1339
2024-02-06 06:27:18,943 Epoch 1339: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.90 
2024-02-06 06:27:18,944 EPOCH 1340
2024-02-06 06:27:30,050 Epoch 1340: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.29 
2024-02-06 06:27:30,050 EPOCH 1341
2024-02-06 06:27:40,775 Epoch 1341: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.79 
2024-02-06 06:27:40,776 EPOCH 1342
2024-02-06 06:27:42,889 [Epoch: 1342 Step: 00022800] Batch Recognition Loss:   0.001119 => Gls Tokens per Sec:      909 || Batch Translation Loss:   0.025380 => Txt Tokens per Sec:     2856 || Lr: 0.000100
2024-02-06 06:27:51,394 Epoch 1342: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.64 
2024-02-06 06:27:51,394 EPOCH 1343
2024-02-06 06:28:02,098 Epoch 1343: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.83 
2024-02-06 06:28:02,099 EPOCH 1344
2024-02-06 06:28:12,782 Epoch 1344: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.75 
2024-02-06 06:28:12,782 EPOCH 1345
2024-02-06 06:28:23,229 Epoch 1345: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.37 
2024-02-06 06:28:23,230 EPOCH 1346
2024-02-06 06:28:34,036 Epoch 1346: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-06 06:28:34,037 EPOCH 1347
2024-02-06 06:28:44,765 Epoch 1347: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-06 06:28:44,766 EPOCH 1348
2024-02-06 06:28:44,935 [Epoch: 1348 Step: 00022900] Batch Recognition Loss:   0.000382 => Gls Tokens per Sec:     3810 || Batch Translation Loss:   0.014819 => Txt Tokens per Sec:     9542 || Lr: 0.000100
2024-02-06 06:28:54,995 Epoch 1348: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 06:28:54,996 EPOCH 1349
2024-02-06 06:29:05,741 Epoch 1349: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 06:29:05,741 EPOCH 1350
2024-02-06 06:29:16,420 Epoch 1350: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 06:29:16,421 EPOCH 1351
2024-02-06 06:29:27,066 Epoch 1351: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 06:29:27,066 EPOCH 1352
2024-02-06 06:29:37,928 Epoch 1352: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 06:29:37,928 EPOCH 1353
2024-02-06 06:29:46,986 [Epoch: 1353 Step: 00023000] Batch Recognition Loss:   0.000600 => Gls Tokens per Sec:     1102 || Batch Translation Loss:   0.036238 => Txt Tokens per Sec:     3015 || Lr: 0.000100
2024-02-06 06:29:48,761 Epoch 1353: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-06 06:29:48,761 EPOCH 1354
2024-02-06 06:29:59,401 Epoch 1354: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-06 06:29:59,402 EPOCH 1355
2024-02-06 06:30:10,427 Epoch 1355: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-06 06:30:10,427 EPOCH 1356
2024-02-06 06:30:21,160 Epoch 1356: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-06 06:30:21,161 EPOCH 1357
2024-02-06 06:30:31,936 Epoch 1357: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 06:30:31,936 EPOCH 1358
2024-02-06 06:30:42,579 Epoch 1358: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-06 06:30:42,580 EPOCH 1359
2024-02-06 06:30:52,310 [Epoch: 1359 Step: 00023100] Batch Recognition Loss:   0.000317 => Gls Tokens per Sec:      894 || Batch Translation Loss:   0.019971 => Txt Tokens per Sec:     2521 || Lr: 0.000100
2024-02-06 06:30:52,921 Epoch 1359: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.61 
2024-02-06 06:30:52,921 EPOCH 1360
2024-02-06 06:31:03,754 Epoch 1360: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.80 
2024-02-06 06:31:03,754 EPOCH 1361
2024-02-06 06:31:14,760 Epoch 1361: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-06 06:31:14,761 EPOCH 1362
2024-02-06 06:31:25,582 Epoch 1362: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-06 06:31:25,583 EPOCH 1363
2024-02-06 06:31:36,406 Epoch 1363: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-06 06:31:36,407 EPOCH 1364
2024-02-06 06:31:47,019 Epoch 1364: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-06 06:31:47,020 EPOCH 1365
2024-02-06 06:31:54,911 [Epoch: 1365 Step: 00023200] Batch Recognition Loss:   0.001080 => Gls Tokens per Sec:      940 || Batch Translation Loss:   0.019924 => Txt Tokens per Sec:     2576 || Lr: 0.000100
2024-02-06 06:31:57,675 Epoch 1365: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-06 06:31:57,675 EPOCH 1366
2024-02-06 06:32:08,359 Epoch 1366: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-06 06:32:08,360 EPOCH 1367
2024-02-06 06:32:18,900 Epoch 1367: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-06 06:32:18,901 EPOCH 1368
2024-02-06 06:32:29,559 Epoch 1368: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 06:32:29,560 EPOCH 1369
2024-02-06 06:32:40,114 Epoch 1369: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 06:32:40,114 EPOCH 1370
2024-02-06 06:32:50,823 Epoch 1370: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 06:32:50,824 EPOCH 1371
2024-02-06 06:32:58,300 [Epoch: 1371 Step: 00023300] Batch Recognition Loss:   0.000168 => Gls Tokens per Sec:      821 || Batch Translation Loss:   0.012911 => Txt Tokens per Sec:     2341 || Lr: 0.000100
2024-02-06 06:33:01,663 Epoch 1371: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.30 
2024-02-06 06:33:01,664 EPOCH 1372
2024-02-06 06:33:12,252 Epoch 1372: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-06 06:33:12,252 EPOCH 1373
2024-02-06 06:33:22,816 Epoch 1373: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 06:33:22,816 EPOCH 1374
2024-02-06 06:33:33,647 Epoch 1374: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 06:33:33,648 EPOCH 1375
2024-02-06 06:33:44,268 Epoch 1375: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 06:33:44,269 EPOCH 1376
2024-02-06 06:33:54,890 Epoch 1376: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-06 06:33:54,891 EPOCH 1377
2024-02-06 06:33:58,440 [Epoch: 1377 Step: 00023400] Batch Recognition Loss:   0.000151 => Gls Tokens per Sec:     1443 || Batch Translation Loss:   0.018468 => Txt Tokens per Sec:     3729 || Lr: 0.000100
2024-02-06 06:34:05,726 Epoch 1377: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-06 06:34:05,727 EPOCH 1378
2024-02-06 06:34:16,373 Epoch 1378: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-06 06:34:16,374 EPOCH 1379
2024-02-06 06:34:27,279 Epoch 1379: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-06 06:34:27,279 EPOCH 1380
2024-02-06 06:34:38,373 Epoch 1380: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-06 06:34:38,373 EPOCH 1381
2024-02-06 06:34:48,791 Epoch 1381: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-06 06:34:48,792 EPOCH 1382
2024-02-06 06:34:59,567 Epoch 1382: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-06 06:34:59,568 EPOCH 1383
2024-02-06 06:35:03,340 [Epoch: 1383 Step: 00023500] Batch Recognition Loss:   0.000170 => Gls Tokens per Sec:      949 || Batch Translation Loss:   0.016925 => Txt Tokens per Sec:     2677 || Lr: 0.000100
2024-02-06 06:35:10,309 Epoch 1383: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-06 06:35:10,310 EPOCH 1384
2024-02-06 06:35:21,248 Epoch 1384: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.86 
2024-02-06 06:35:21,248 EPOCH 1385
2024-02-06 06:35:31,796 Epoch 1385: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.79 
2024-02-06 06:35:31,797 EPOCH 1386
2024-02-06 06:35:42,703 Epoch 1386: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.24 
2024-02-06 06:35:42,704 EPOCH 1387
2024-02-06 06:35:53,511 Epoch 1387: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.01 
2024-02-06 06:35:53,511 EPOCH 1388
2024-02-06 06:36:04,180 Epoch 1388: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.33 
2024-02-06 06:36:04,181 EPOCH 1389
2024-02-06 06:36:06,486 [Epoch: 1389 Step: 00023600] Batch Recognition Loss:   0.002858 => Gls Tokens per Sec:     1111 || Batch Translation Loss:   0.108319 => Txt Tokens per Sec:     2574 || Lr: 0.000100
2024-02-06 06:36:14,789 Epoch 1389: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.25 
2024-02-06 06:36:14,789 EPOCH 1390
2024-02-06 06:36:25,392 Epoch 1390: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.65 
2024-02-06 06:36:25,393 EPOCH 1391
2024-02-06 06:36:36,179 Epoch 1391: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.70 
2024-02-06 06:36:36,179 EPOCH 1392
2024-02-06 06:36:47,005 Epoch 1392: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.93 
2024-02-06 06:36:47,006 EPOCH 1393
2024-02-06 06:36:57,778 Epoch 1393: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.62 
2024-02-06 06:36:57,778 EPOCH 1394
2024-02-06 06:37:08,491 Epoch 1394: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.68 
2024-02-06 06:37:08,491 EPOCH 1395
2024-02-06 06:37:11,220 [Epoch: 1395 Step: 00023700] Batch Recognition Loss:   0.000993 => Gls Tokens per Sec:      374 || Batch Translation Loss:   0.037539 => Txt Tokens per Sec:     1245 || Lr: 0.000100
2024-02-06 06:37:19,284 Epoch 1395: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.87 
2024-02-06 06:37:19,285 EPOCH 1396
2024-02-06 06:37:29,988 Epoch 1396: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.66 
2024-02-06 06:37:29,988 EPOCH 1397
2024-02-06 06:37:40,590 Epoch 1397: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.93 
2024-02-06 06:37:40,590 EPOCH 1398
2024-02-06 06:37:51,246 Epoch 1398: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.52 
2024-02-06 06:37:51,247 EPOCH 1399
2024-02-06 06:38:01,958 Epoch 1399: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.84 
2024-02-06 06:38:01,959 EPOCH 1400
2024-02-06 06:38:12,291 [Epoch: 1400 Step: 00023800] Batch Recognition Loss:   0.001192 => Gls Tokens per Sec:     1028 || Batch Translation Loss:   0.046564 => Txt Tokens per Sec:     2844 || Lr: 0.000100
2024-02-06 06:38:12,291 Epoch 1400: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.57 
2024-02-06 06:38:12,292 EPOCH 1401
2024-02-06 06:38:22,924 Epoch 1401: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.04 
2024-02-06 06:38:22,925 EPOCH 1402
2024-02-06 06:38:33,607 Epoch 1402: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.79 
2024-02-06 06:38:33,607 EPOCH 1403
2024-02-06 06:38:44,317 Epoch 1403: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-06 06:38:44,318 EPOCH 1404
2024-02-06 06:38:55,279 Epoch 1404: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.46 
2024-02-06 06:38:55,280 EPOCH 1405
2024-02-06 06:39:06,104 Epoch 1405: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.42 
2024-02-06 06:39:06,105 EPOCH 1406
2024-02-06 06:39:16,305 [Epoch: 1406 Step: 00023900] Batch Recognition Loss:   0.001138 => Gls Tokens per Sec:      916 || Batch Translation Loss:   0.017486 => Txt Tokens per Sec:     2544 || Lr: 0.000100
2024-02-06 06:39:16,742 Epoch 1406: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.38 
2024-02-06 06:39:16,742 EPOCH 1407
2024-02-06 06:39:27,236 Epoch 1407: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-06 06:39:27,236 EPOCH 1408
2024-02-06 06:39:37,979 Epoch 1408: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-06 06:39:37,980 EPOCH 1409
2024-02-06 06:39:48,735 Epoch 1409: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-06 06:39:48,736 EPOCH 1410
2024-02-06 06:39:59,317 Epoch 1410: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-06 06:39:59,318 EPOCH 1411
2024-02-06 06:40:09,944 Epoch 1411: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-06 06:40:09,945 EPOCH 1412
2024-02-06 06:40:18,401 [Epoch: 1412 Step: 00024000] Batch Recognition Loss:   0.000915 => Gls Tokens per Sec:      953 || Batch Translation Loss:   0.010868 => Txt Tokens per Sec:     2703 || Lr: 0.000100
2024-02-06 06:40:58,894 Validation result at epoch 1412, step    24000: duration: 40.4926s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.82587	Translation Loss: 93251.09375	PPL: 11091.47852
	Eval Metric: BLEU
	WER 3.67	(DEL: 0.00,	INS: 0.00,	SUB: 3.67)
	BLEU-4 0.65	(BLEU-1: 11.73,	BLEU-2: 3.81,	BLEU-3: 1.46,	BLEU-4: 0.65)
	CHRF 17.48	ROUGE 9.73
2024-02-06 06:40:58,896 Logging Recognition and Translation Outputs
2024-02-06 06:40:58,896 ========================================================================================================================
2024-02-06 06:40:58,897 Logging Sequence: 171_2.00
2024-02-06 06:40:58,898 	Gloss Reference :	A B+C+D+E
2024-02-06 06:40:58,898 	Gloss Hypothesis:	A B+C+D  
2024-02-06 06:40:58,898 	Gloss Alignment :	  S      
2024-02-06 06:40:58,898 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 06:40:58,900 	Text Reference  :	as you might all know      that the  ipl is   about to      end   the       finals are     on    28th may   
2024-02-06 06:40:58,900 	Text Hypothesis :	** *** ***** *** yesterday on   23rd may 2021 in    javelin throw including an     olympic games in   mumbai
2024-02-06 06:40:58,900 	Text Alignment  :	D  D   D     D   S         S    S    S   S    S     S       S     S         S      S       S     S    S     
2024-02-06 06:40:58,900 ========================================================================================================================
2024-02-06 06:40:58,901 Logging Sequence: 119_33.00
2024-02-06 06:40:58,901 	Gloss Reference :	A B+C+D+E
2024-02-06 06:40:58,901 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 06:40:58,901 	Gloss Alignment :	         
2024-02-06 06:40:58,901 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 06:40:58,902 	Text Reference  :	he   wanted to * gift ********* ** *** *** ******* *** *** ***** 35 people    wow wonderful
2024-02-06 06:40:58,902 	Text Hypothesis :	this led    to a gift something to all the players and the staff to celebrate the moment   
2024-02-06 06:40:58,902 	Text Alignment  :	S    S         I      I         I  I   I   I       I   I   I     S  S         S   S        
2024-02-06 06:40:58,903 ========================================================================================================================
2024-02-06 06:40:58,903 Logging Sequence: 158_131.00
2024-02-06 06:40:58,903 	Gloss Reference :	A B+C+D+E
2024-02-06 06:40:58,903 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 06:40:58,903 	Gloss Alignment :	         
2024-02-06 06:40:58,903 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 06:40:58,905 	Text Reference  :	**** *** on   10th april 2023 there was ******** a  match between rcb and     lsg in *** bengaluru
2024-02-06 06:40:58,905 	Text Hypothesis :	this was when rcb  lost  the  match was escorted by virat kohli   is  india's 5th in the auction  
2024-02-06 06:40:58,905 	Text Alignment  :	I    I   S    S    S     S    S         I        S  S     S       S   S       S      I   S        
2024-02-06 06:40:58,905 ========================================================================================================================
2024-02-06 06:40:58,905 Logging Sequence: 164_412.00
2024-02-06 06:40:58,906 	Gloss Reference :	A B+C+D+E
2024-02-06 06:40:58,906 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 06:40:58,906 	Gloss Alignment :	         
2024-02-06 06:40:58,906 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 06:40:58,908 	Text Reference  :	if you divide these two  figures you will be    shocked to know  that each ball's worth is   rs     50 lakhs
2024-02-06 06:40:58,908 	Text Hypothesis :	** *** ****** ***** will bowl    for 120  balls in      20 overs with each ****** ***** over having to bowl 
2024-02-06 06:40:58,908 	Text Alignment  :	D  D   D      D     S    S       S   S    S     S       S  S     S         D      D     S    S      S  S    
2024-02-06 06:40:58,908 ========================================================================================================================
2024-02-06 06:40:58,908 Logging Sequence: 159_112.00
2024-02-06 06:40:58,908 	Gloss Reference :	A B+C+D+E
2024-02-06 06:40:58,908 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 06:40:58,908 	Gloss Alignment :	         
2024-02-06 06:40:58,909 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 06:40:58,910 	Text Reference  :	******** kohli had revealed that before the tournament he   did  not    touch his bat for a month yes 1  month
2024-02-06 06:40:58,910 	Text Hypothesis :	mohammed shami has said     that ****** the ********** pani puri seller on    his *** *** * ***** *** in odi  
2024-02-06 06:40:58,910 	Text Alignment  :	I        S     S   S             D          D          S    S    S      S         D   D   D D     D   S  S    
2024-02-06 06:40:58,911 ========================================================================================================================
2024-02-06 06:41:01,467 Epoch 1412: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.33 
2024-02-06 06:41:01,468 EPOCH 1413
2024-02-06 06:41:12,568 Epoch 1413: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 06:41:12,569 EPOCH 1414
2024-02-06 06:41:23,362 Epoch 1414: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 06:41:23,363 EPOCH 1415
2024-02-06 06:41:34,014 Epoch 1415: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 06:41:34,015 EPOCH 1416
2024-02-06 06:41:44,943 Epoch 1416: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 06:41:44,943 EPOCH 1417
2024-02-06 06:41:55,492 Epoch 1417: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 06:41:55,493 EPOCH 1418
2024-02-06 06:42:03,035 [Epoch: 1418 Step: 00024100] Batch Recognition Loss:   0.001806 => Gls Tokens per Sec:      899 || Batch Translation Loss:   0.005761 => Txt Tokens per Sec:     2414 || Lr: 0.000100
2024-02-06 06:42:06,263 Epoch 1418: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 06:42:06,263 EPOCH 1419
2024-02-06 06:42:16,858 Epoch 1419: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.23 
2024-02-06 06:42:16,858 EPOCH 1420
2024-02-06 06:42:27,534 Epoch 1420: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 06:42:27,535 EPOCH 1421
2024-02-06 06:42:38,194 Epoch 1421: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-06 06:42:38,195 EPOCH 1422
2024-02-06 06:42:48,870 Epoch 1422: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-06 06:42:48,871 EPOCH 1423
2024-02-06 06:42:59,680 Epoch 1423: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-06 06:42:59,680 EPOCH 1424
2024-02-06 06:43:05,382 [Epoch: 1424 Step: 00024200] Batch Recognition Loss:   0.000292 => Gls Tokens per Sec:      965 || Batch Translation Loss:   0.011781 => Txt Tokens per Sec:     2496 || Lr: 0.000100
2024-02-06 06:43:10,623 Epoch 1424: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-06 06:43:10,623 EPOCH 1425
2024-02-06 06:43:21,419 Epoch 1425: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-06 06:43:21,420 EPOCH 1426
2024-02-06 06:43:32,275 Epoch 1426: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-06 06:43:32,275 EPOCH 1427
2024-02-06 06:43:43,229 Epoch 1427: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-06 06:43:43,230 EPOCH 1428
2024-02-06 06:43:54,134 Epoch 1428: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.98 
2024-02-06 06:43:54,134 EPOCH 1429
2024-02-06 06:44:04,905 Epoch 1429: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-06 06:44:04,905 EPOCH 1430
2024-02-06 06:44:08,766 [Epoch: 1430 Step: 00024300] Batch Recognition Loss:   0.000309 => Gls Tokens per Sec:     1093 || Batch Translation Loss:   0.030625 => Txt Tokens per Sec:     2846 || Lr: 0.000100
2024-02-06 06:44:15,746 Epoch 1430: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.77 
2024-02-06 06:44:15,746 EPOCH 1431
2024-02-06 06:44:26,418 Epoch 1431: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.72 
2024-02-06 06:44:26,419 EPOCH 1432
2024-02-06 06:44:37,196 Epoch 1432: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.66 
2024-02-06 06:44:37,197 EPOCH 1433
2024-02-06 06:44:47,845 Epoch 1433: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.63 
2024-02-06 06:44:47,846 EPOCH 1434
2024-02-06 06:44:58,471 Epoch 1434: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.35 
2024-02-06 06:44:58,471 EPOCH 1435
2024-02-06 06:45:08,903 Epoch 1435: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.29 
2024-02-06 06:45:08,903 EPOCH 1436
2024-02-06 06:45:11,278 [Epoch: 1436 Step: 00024400] Batch Recognition Loss:   0.000267 => Gls Tokens per Sec:     1348 || Batch Translation Loss:   0.017056 => Txt Tokens per Sec:     3335 || Lr: 0.000100
2024-02-06 06:45:19,628 Epoch 1436: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.26 
2024-02-06 06:45:19,629 EPOCH 1437
2024-02-06 06:45:30,218 Epoch 1437: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.28 
2024-02-06 06:45:30,219 EPOCH 1438
2024-02-06 06:45:40,912 Epoch 1438: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.28 
2024-02-06 06:45:40,913 EPOCH 1439
2024-02-06 06:45:51,524 Epoch 1439: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-06 06:45:51,524 EPOCH 1440
2024-02-06 06:46:02,045 Epoch 1440: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 06:46:02,045 EPOCH 1441
2024-02-06 06:46:12,623 Epoch 1441: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 06:46:12,624 EPOCH 1442
2024-02-06 06:46:13,096 [Epoch: 1442 Step: 00024500] Batch Recognition Loss:   0.000123 => Gls Tokens per Sec:     4085 || Batch Translation Loss:   0.012828 => Txt Tokens per Sec:     9347 || Lr: 0.000100
2024-02-06 06:46:23,119 Epoch 1442: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 06:46:23,120 EPOCH 1443
2024-02-06 06:46:33,814 Epoch 1443: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 06:46:33,815 EPOCH 1444
2024-02-06 06:46:44,765 Epoch 1444: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 06:46:44,765 EPOCH 1445
2024-02-06 06:46:55,503 Epoch 1445: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 06:46:55,503 EPOCH 1446
2024-02-06 06:47:06,244 Epoch 1446: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-06 06:47:06,245 EPOCH 1447
2024-02-06 06:47:16,827 Epoch 1447: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-06 06:47:16,828 EPOCH 1448
2024-02-06 06:47:17,119 [Epoch: 1448 Step: 00024600] Batch Recognition Loss:   0.000499 => Gls Tokens per Sec:     2205 || Batch Translation Loss:   0.020770 => Txt Tokens per Sec:     6913 || Lr: 0.000100
2024-02-06 06:47:27,604 Epoch 1448: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.64 
2024-02-06 06:47:27,604 EPOCH 1449
2024-02-06 06:47:37,994 Epoch 1449: Total Training Recognition Loss 0.09  Total Training Translation Loss 0.73 
2024-02-06 06:47:37,994 EPOCH 1450
2024-02-06 06:47:48,905 Epoch 1450: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.10 
2024-02-06 06:47:48,905 EPOCH 1451
2024-02-06 06:47:59,613 Epoch 1451: Total Training Recognition Loss 0.70  Total Training Translation Loss 1.87 
2024-02-06 06:47:59,613 EPOCH 1452
2024-02-06 06:48:10,259 Epoch 1452: Total Training Recognition Loss 1.04  Total Training Translation Loss 3.72 
2024-02-06 06:48:10,260 EPOCH 1453
2024-02-06 06:48:20,697 [Epoch: 1453 Step: 00024700] Batch Recognition Loss:   0.007406 => Gls Tokens per Sec:      956 || Batch Translation Loss:   0.080047 => Txt Tokens per Sec:     2619 || Lr: 0.000100
2024-02-06 06:48:21,017 Epoch 1453: Total Training Recognition Loss 0.80  Total Training Translation Loss 5.13 
2024-02-06 06:48:21,018 EPOCH 1454
2024-02-06 06:48:31,779 Epoch 1454: Total Training Recognition Loss 1.60  Total Training Translation Loss 4.54 
2024-02-06 06:48:31,779 EPOCH 1455
2024-02-06 06:48:42,652 Epoch 1455: Total Training Recognition Loss 0.60  Total Training Translation Loss 3.51 
2024-02-06 06:48:42,652 EPOCH 1456
2024-02-06 06:48:53,132 Epoch 1456: Total Training Recognition Loss 0.09  Total Training Translation Loss 3.65 
2024-02-06 06:48:53,132 EPOCH 1457
2024-02-06 06:49:03,806 Epoch 1457: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.86 
2024-02-06 06:49:03,806 EPOCH 1458
2024-02-06 06:49:14,730 Epoch 1458: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.36 
2024-02-06 06:49:14,730 EPOCH 1459
2024-02-06 06:49:24,775 [Epoch: 1459 Step: 00024800] Batch Recognition Loss:   0.003194 => Gls Tokens per Sec:      866 || Batch Translation Loss:   0.078555 => Txt Tokens per Sec:     2468 || Lr: 0.000100
2024-02-06 06:49:25,312 Epoch 1459: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.92 
2024-02-06 06:49:25,312 EPOCH 1460
2024-02-06 06:49:36,131 Epoch 1460: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.70 
2024-02-06 06:49:36,132 EPOCH 1461
2024-02-06 06:49:46,698 Epoch 1461: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.53 
2024-02-06 06:49:46,698 EPOCH 1462
2024-02-06 06:49:57,535 Epoch 1462: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.45 
2024-02-06 06:49:57,535 EPOCH 1463
2024-02-06 06:50:08,052 Epoch 1463: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-06 06:50:08,053 EPOCH 1464
2024-02-06 06:50:18,655 Epoch 1464: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.34 
2024-02-06 06:50:18,656 EPOCH 1465
2024-02-06 06:50:26,445 [Epoch: 1465 Step: 00024900] Batch Recognition Loss:   0.000646 => Gls Tokens per Sec:      953 || Batch Translation Loss:   0.025540 => Txt Tokens per Sec:     2682 || Lr: 0.000100
2024-02-06 06:50:29,206 Epoch 1465: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-06 06:50:29,206 EPOCH 1466
2024-02-06 06:50:40,007 Epoch 1466: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.36 
2024-02-06 06:50:40,007 EPOCH 1467
2024-02-06 06:50:50,834 Epoch 1467: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 06:50:50,834 EPOCH 1468
2024-02-06 06:51:01,629 Epoch 1468: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.28 
2024-02-06 06:51:01,630 EPOCH 1469
2024-02-06 06:51:12,093 Epoch 1469: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-06 06:51:12,094 EPOCH 1470
2024-02-06 06:51:22,851 Epoch 1470: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.47 
2024-02-06 06:51:22,852 EPOCH 1471
2024-02-06 06:51:28,992 [Epoch: 1471 Step: 00025000] Batch Recognition Loss:   0.001242 => Gls Tokens per Sec:     1000 || Batch Translation Loss:   0.017879 => Txt Tokens per Sec:     2784 || Lr: 0.000100
2024-02-06 06:51:33,362 Epoch 1471: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-06 06:51:33,363 EPOCH 1472
2024-02-06 06:51:44,054 Epoch 1472: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.75 
2024-02-06 06:51:44,054 EPOCH 1473
2024-02-06 06:51:54,727 Epoch 1473: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-06 06:51:54,728 EPOCH 1474
2024-02-06 06:52:05,651 Epoch 1474: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-06 06:52:05,652 EPOCH 1475
2024-02-06 06:52:16,585 Epoch 1475: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-06 06:52:16,586 EPOCH 1476
2024-02-06 06:52:27,402 Epoch 1476: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 06:52:27,403 EPOCH 1477
2024-02-06 06:52:32,622 [Epoch: 1477 Step: 00025100] Batch Recognition Loss:   0.000492 => Gls Tokens per Sec:      981 || Batch Translation Loss:   0.026592 => Txt Tokens per Sec:     2816 || Lr: 0.000100
2024-02-06 06:52:38,371 Epoch 1477: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 06:52:38,372 EPOCH 1478
2024-02-06 06:52:49,212 Epoch 1478: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-06 06:52:49,213 EPOCH 1479
2024-02-06 06:52:59,914 Epoch 1479: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 06:52:59,915 EPOCH 1480
2024-02-06 06:53:10,427 Epoch 1480: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 06:53:10,427 EPOCH 1481
2024-02-06 06:53:21,071 Epoch 1481: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-06 06:53:21,072 EPOCH 1482
2024-02-06 06:53:31,408 Epoch 1482: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.31 
2024-02-06 06:53:31,408 EPOCH 1483
2024-02-06 06:53:34,697 [Epoch: 1483 Step: 00025200] Batch Recognition Loss:   0.000187 => Gls Tokens per Sec:     1168 || Batch Translation Loss:   0.025038 => Txt Tokens per Sec:     3280 || Lr: 0.000100
2024-02-06 06:53:42,124 Epoch 1483: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-06 06:53:42,125 EPOCH 1484
2024-02-06 06:53:53,212 Epoch 1484: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-06 06:53:53,212 EPOCH 1485
2024-02-06 06:54:03,919 Epoch 1485: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-06 06:54:03,919 EPOCH 1486
2024-02-06 06:54:14,752 Epoch 1486: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-06 06:54:14,752 EPOCH 1487
2024-02-06 06:54:25,479 Epoch 1487: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-06 06:54:25,479 EPOCH 1488
2024-02-06 06:54:36,217 Epoch 1488: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-06 06:54:36,217 EPOCH 1489
2024-02-06 06:54:39,199 [Epoch: 1489 Step: 00025300] Batch Recognition Loss:   0.000555 => Gls Tokens per Sec:      772 || Batch Translation Loss:   0.016113 => Txt Tokens per Sec:     2252 || Lr: 0.000100
2024-02-06 06:54:46,927 Epoch 1489: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 06:54:46,927 EPOCH 1490
2024-02-06 06:54:57,486 Epoch 1490: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 06:54:57,486 EPOCH 1491
2024-02-06 06:55:08,244 Epoch 1491: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 06:55:08,245 EPOCH 1492
2024-02-06 06:55:18,943 Epoch 1492: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-06 06:55:18,944 EPOCH 1493
2024-02-06 06:55:29,696 Epoch 1493: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-06 06:55:29,697 EPOCH 1494
2024-02-06 06:55:40,236 Epoch 1494: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-06 06:55:40,236 EPOCH 1495
2024-02-06 06:55:40,540 [Epoch: 1495 Step: 00025400] Batch Recognition Loss:   0.000220 => Gls Tokens per Sec:     4232 || Batch Translation Loss:   0.012319 => Txt Tokens per Sec:    10200 || Lr: 0.000100
2024-02-06 06:55:50,834 Epoch 1495: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 06:55:50,835 EPOCH 1496
2024-02-06 06:56:01,586 Epoch 1496: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 06:56:01,586 EPOCH 1497
2024-02-06 06:56:12,168 Epoch 1497: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-06 06:56:12,168 EPOCH 1498
2024-02-06 06:56:22,821 Epoch 1498: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-06 06:56:22,822 EPOCH 1499
2024-02-06 06:56:33,578 Epoch 1499: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-06 06:56:33,579 EPOCH 1500
2024-02-06 06:56:44,065 [Epoch: 1500 Step: 00025500] Batch Recognition Loss:   0.000903 => Gls Tokens per Sec:     1013 || Batch Translation Loss:   0.022489 => Txt Tokens per Sec:     2802 || Lr: 0.000100
2024-02-06 06:56:44,066 Epoch 1500: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-06 06:56:44,066 EPOCH 1501
2024-02-06 06:56:54,772 Epoch 1501: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-06 06:56:54,772 EPOCH 1502
2024-02-06 06:57:05,415 Epoch 1502: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.70 
2024-02-06 06:57:05,416 EPOCH 1503
2024-02-06 06:57:15,951 Epoch 1503: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-06 06:57:15,952 EPOCH 1504
2024-02-06 06:57:26,656 Epoch 1504: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.15 
2024-02-06 06:57:26,657 EPOCH 1505
2024-02-06 06:57:37,653 Epoch 1505: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-06 06:57:37,653 EPOCH 1506
2024-02-06 06:57:46,463 [Epoch: 1506 Step: 00025600] Batch Recognition Loss:   0.000313 => Gls Tokens per Sec:     1061 || Batch Translation Loss:   0.088052 => Txt Tokens per Sec:     2942 || Lr: 0.000100
2024-02-06 06:57:48,398 Epoch 1506: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.40 
2024-02-06 06:57:48,399 EPOCH 1507
2024-02-06 06:57:59,148 Epoch 1507: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.92 
2024-02-06 06:57:59,148 EPOCH 1508
2024-02-06 06:58:10,169 Epoch 1508: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.82 
2024-02-06 06:58:10,169 EPOCH 1509
2024-02-06 06:58:21,023 Epoch 1509: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.28 
2024-02-06 06:58:21,023 EPOCH 1510
2024-02-06 06:58:31,527 Epoch 1510: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.80 
2024-02-06 06:58:31,527 EPOCH 1511
2024-02-06 06:58:42,158 Epoch 1511: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.73 
2024-02-06 06:58:42,158 EPOCH 1512
2024-02-06 06:58:50,389 [Epoch: 1512 Step: 00025700] Batch Recognition Loss:   0.000314 => Gls Tokens per Sec:      979 || Batch Translation Loss:   0.065327 => Txt Tokens per Sec:     2802 || Lr: 0.000100
2024-02-06 06:58:52,826 Epoch 1512: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.73 
2024-02-06 06:58:52,826 EPOCH 1513
2024-02-06 06:59:03,592 Epoch 1513: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.85 
2024-02-06 06:59:03,593 EPOCH 1514
2024-02-06 06:59:14,185 Epoch 1514: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.82 
2024-02-06 06:59:14,185 EPOCH 1515
2024-02-06 06:59:24,536 Epoch 1515: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.17 
2024-02-06 06:59:24,537 EPOCH 1516
2024-02-06 06:59:35,275 Epoch 1516: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.15 
2024-02-06 06:59:35,276 EPOCH 1517
2024-02-06 06:59:46,133 Epoch 1517: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.71 
2024-02-06 06:59:46,134 EPOCH 1518
2024-02-06 06:59:52,300 [Epoch: 1518 Step: 00025800] Batch Recognition Loss:   0.001042 => Gls Tokens per Sec:     1100 || Batch Translation Loss:   0.093505 => Txt Tokens per Sec:     2880 || Lr: 0.000100
2024-02-06 06:59:56,777 Epoch 1518: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.02 
2024-02-06 06:59:56,778 EPOCH 1519
2024-02-06 07:00:07,279 Epoch 1519: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.50 
2024-02-06 07:00:07,280 EPOCH 1520
2024-02-06 07:00:17,900 Epoch 1520: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.96 
2024-02-06 07:00:17,900 EPOCH 1521
2024-02-06 07:00:28,711 Epoch 1521: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.11 
2024-02-06 07:00:28,711 EPOCH 1522
2024-02-06 07:00:39,234 Epoch 1522: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.92 
2024-02-06 07:00:39,235 EPOCH 1523
2024-02-06 07:00:50,045 Epoch 1523: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.32 
2024-02-06 07:00:50,045 EPOCH 1524
2024-02-06 07:00:53,524 [Epoch: 1524 Step: 00025900] Batch Recognition Loss:   0.001119 => Gls Tokens per Sec:     1656 || Batch Translation Loss:   0.030652 => Txt Tokens per Sec:     4520 || Lr: 0.000100
2024-02-06 07:01:00,637 Epoch 1524: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.86 
2024-02-06 07:01:00,638 EPOCH 1525
2024-02-06 07:01:11,445 Epoch 1525: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-06 07:01:11,445 EPOCH 1526
2024-02-06 07:01:22,090 Epoch 1526: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-06 07:01:22,091 EPOCH 1527
2024-02-06 07:01:32,841 Epoch 1527: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-06 07:01:32,842 EPOCH 1528
2024-02-06 07:01:43,525 Epoch 1528: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-06 07:01:43,526 EPOCH 1529
2024-02-06 07:01:54,474 Epoch 1529: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-06 07:01:54,475 EPOCH 1530
2024-02-06 07:02:00,143 [Epoch: 1530 Step: 00026000] Batch Recognition Loss:   0.000277 => Gls Tokens per Sec:      745 || Batch Translation Loss:   0.018459 => Txt Tokens per Sec:     2115 || Lr: 0.000100
2024-02-06 07:02:40,517 Validation result at epoch 1530, step    26000: duration: 40.3713s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.64269	Translation Loss: 94343.39844	PPL: 12370.02930
	Eval Metric: BLEU
	WER 3.25	(DEL: 0.00,	INS: 0.00,	SUB: 3.25)
	BLEU-4 0.70	(BLEU-1: 11.28,	BLEU-2: 3.82,	BLEU-3: 1.52,	BLEU-4: 0.70)
	CHRF 17.17	ROUGE 9.34
2024-02-06 07:02:40,519 Logging Recognition and Translation Outputs
2024-02-06 07:02:40,519 ========================================================================================================================
2024-02-06 07:02:40,519 Logging Sequence: 166_243.00
2024-02-06 07:02:40,519 	Gloss Reference :	A B+C+D+E
2024-02-06 07:02:40,520 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 07:02:40,520 	Gloss Alignment :	         
2024-02-06 07:02:40,520 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 07:02:40,522 	Text Reference  :	*** ********* *********** ********* *** ***** ** icc     worked with members boards like bcci pcb   cricket australia etc 
2024-02-06 07:02:40,522 	Text Hypothesis :	the broadcast advertisers ticketing etc would be decided by     the  board   of     the  2    teams playing the       test
2024-02-06 07:02:40,522 	Text Alignment  :	I   I         I           I         I   I     I  S       S      S    S       S      S    S    S     S       S         S   
2024-02-06 07:02:40,522 ========================================================================================================================
2024-02-06 07:02:40,523 Logging Sequence: 59_152.00
2024-02-06 07:02:40,523 	Gloss Reference :	A B+C+D+E
2024-02-06 07:02:40,523 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 07:02:40,523 	Gloss Alignment :	         
2024-02-06 07:02:40,523 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 07:02:40,524 	Text Reference  :	**** the organisers encouraged athletes to    use      the condoms in their home countries
2024-02-06 07:02:40,525 	Text Hypothesis :	well the ********** ********** ******** tokyo olympics in  2021    on 1st   may  2021     
2024-02-06 07:02:40,525 	Text Alignment  :	I        D          D          D        S     S        S   S       S  S     S    S        
2024-02-06 07:02:40,525 ========================================================================================================================
2024-02-06 07:02:40,525 Logging Sequence: 145_52.00
2024-02-06 07:02:40,525 	Gloss Reference :	A B+C+D+E
2024-02-06 07:02:40,525 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 07:02:40,526 	Gloss Alignment :	         
2024-02-06 07:02:40,526 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 07:02:40,527 	Text Reference  :	her name was dropped despite having qualified as she   was   the only female athlete
2024-02-06 07:02:40,527 	Text Hypothesis :	*** **** *** ******* ******* the    finals    of their child has a    strong athlete
2024-02-06 07:02:40,527 	Text Alignment  :	D   D    D   D       D       S      S         S  S     S     S   S    S             
2024-02-06 07:02:40,527 ========================================================================================================================
2024-02-06 07:02:40,528 Logging Sequence: 172_163.00
2024-02-06 07:02:40,528 	Gloss Reference :	A B+C+D+E
2024-02-06 07:02:40,528 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 07:02:40,528 	Gloss Alignment :	         
2024-02-06 07:02:40,528 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 07:02:40,530 	Text Reference  :	if the match starts anywhere between 730 pm ** to 935   pm     a       full   20-over match can be played 
2024-02-06 07:02:40,530 	Text Hypothesis :	** the match starts ******** at      730 pm if a  major tennis players leaves the     field for 9  minutes
2024-02-06 07:02:40,530 	Text Alignment  :	D                   D        S              I  S  S     S      S       S      S       S     S   S  S      
2024-02-06 07:02:40,530 ========================================================================================================================
2024-02-06 07:02:40,530 Logging Sequence: 150_20.00
2024-02-06 07:02:40,531 	Gloss Reference :	A B+C+D+E
2024-02-06 07:02:40,531 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 07:02:40,531 	Gloss Alignment :	         
2024-02-06 07:02:40,531 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 07:02:40,532 	Text Reference  :	** ** *** after   a    tough         match india won the saff championship 2023  title
2024-02-06 07:02:40,532 	Text Hypothesis :	he is now retired from international team  has   won the **** ************ world cup  
2024-02-06 07:02:40,532 	Text Alignment  :	I  I  I   S       S    S             S     S             D    D            S     S    
2024-02-06 07:02:40,532 ========================================================================================================================
2024-02-06 07:02:46,198 Epoch 1530: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-06 07:02:46,198 EPOCH 1531
2024-02-06 07:02:56,890 Epoch 1531: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-06 07:02:56,891 EPOCH 1532
2024-02-06 07:03:07,940 Epoch 1532: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-06 07:03:07,941 EPOCH 1533
2024-02-06 07:03:18,484 Epoch 1533: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-06 07:03:18,485 EPOCH 1534
2024-02-06 07:03:29,116 Epoch 1534: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-06 07:03:29,116 EPOCH 1535
2024-02-06 07:03:39,898 Epoch 1535: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 07:03:39,898 EPOCH 1536
2024-02-06 07:03:42,978 [Epoch: 1536 Step: 00026100] Batch Recognition Loss:   0.000985 => Gls Tokens per Sec:      955 || Batch Translation Loss:   0.010946 => Txt Tokens per Sec:     2439 || Lr: 0.000100
2024-02-06 07:03:50,603 Epoch 1536: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-06 07:03:50,604 EPOCH 1537
2024-02-06 07:04:01,474 Epoch 1537: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-06 07:04:01,475 EPOCH 1538
2024-02-06 07:04:12,351 Epoch 1538: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 07:04:12,351 EPOCH 1539
2024-02-06 07:04:23,064 Epoch 1539: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 07:04:23,064 EPOCH 1540
2024-02-06 07:04:33,772 Epoch 1540: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-06 07:04:33,773 EPOCH 1541
2024-02-06 07:04:44,508 Epoch 1541: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-06 07:04:44,509 EPOCH 1542
2024-02-06 07:04:45,177 [Epoch: 1542 Step: 00026200] Batch Recognition Loss:   0.000174 => Gls Tokens per Sec:     2879 || Batch Translation Loss:   0.033962 => Txt Tokens per Sec:     8024 || Lr: 0.000100
2024-02-06 07:04:55,274 Epoch 1542: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-06 07:04:55,275 EPOCH 1543
2024-02-06 07:05:05,896 Epoch 1543: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-06 07:05:05,897 EPOCH 1544
2024-02-06 07:05:16,728 Epoch 1544: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-06 07:05:16,729 EPOCH 1545
2024-02-06 07:05:27,724 Epoch 1545: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-06 07:05:27,725 EPOCH 1546
2024-02-06 07:05:38,472 Epoch 1546: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-06 07:05:38,472 EPOCH 1547
2024-02-06 07:05:49,116 Epoch 1547: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-06 07:05:49,117 EPOCH 1548
2024-02-06 07:05:49,551 [Epoch: 1548 Step: 00026300] Batch Recognition Loss:   0.000536 => Gls Tokens per Sec:     1481 || Batch Translation Loss:   0.017427 => Txt Tokens per Sec:     4678 || Lr: 0.000100
2024-02-06 07:05:59,812 Epoch 1548: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 07:05:59,812 EPOCH 1549
2024-02-06 07:06:10,512 Epoch 1549: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-06 07:06:10,513 EPOCH 1550
2024-02-06 07:06:21,166 Epoch 1550: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-06 07:06:21,167 EPOCH 1551
2024-02-06 07:06:31,756 Epoch 1551: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-06 07:06:31,757 EPOCH 1552
2024-02-06 07:06:42,188 Epoch 1552: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.78 
2024-02-06 07:06:42,188 EPOCH 1553
2024-02-06 07:06:51,304 [Epoch: 1553 Step: 00026400] Batch Recognition Loss:   0.000770 => Gls Tokens per Sec:     1095 || Batch Translation Loss:   0.015601 => Txt Tokens per Sec:     2983 || Lr: 0.000100
2024-02-06 07:06:52,929 Epoch 1553: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-06 07:06:52,929 EPOCH 1554
2024-02-06 07:07:03,501 Epoch 1554: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-06 07:07:03,502 EPOCH 1555
2024-02-06 07:07:14,231 Epoch 1555: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-06 07:07:14,232 EPOCH 1556
2024-02-06 07:07:24,924 Epoch 1556: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-06 07:07:24,925 EPOCH 1557
2024-02-06 07:07:35,970 Epoch 1557: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.82 
2024-02-06 07:07:35,971 EPOCH 1558
2024-02-06 07:07:46,885 Epoch 1558: Total Training Recognition Loss 0.05  Total Training Translation Loss 13.32 
2024-02-06 07:07:46,886 EPOCH 1559
2024-02-06 07:07:55,200 [Epoch: 1559 Step: 00026500] Batch Recognition Loss:   0.001618 => Gls Tokens per Sec:     1047 || Batch Translation Loss:   0.125783 => Txt Tokens per Sec:     2937 || Lr: 0.000100
2024-02-06 07:07:57,510 Epoch 1559: Total Training Recognition Loss 0.09  Total Training Translation Loss 5.99 
2024-02-06 07:07:57,511 EPOCH 1560
2024-02-06 07:08:08,581 Epoch 1560: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.67 
2024-02-06 07:08:08,582 EPOCH 1561
2024-02-06 07:08:19,475 Epoch 1561: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.21 
2024-02-06 07:08:19,476 EPOCH 1562
2024-02-06 07:08:30,350 Epoch 1562: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.76 
2024-02-06 07:08:30,351 EPOCH 1563
2024-02-06 07:08:41,068 Epoch 1563: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-06 07:08:41,069 EPOCH 1564
2024-02-06 07:08:51,722 Epoch 1564: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.44 
2024-02-06 07:08:51,722 EPOCH 1565
2024-02-06 07:08:59,393 [Epoch: 1565 Step: 00026600] Batch Recognition Loss:   0.000621 => Gls Tokens per Sec:      967 || Batch Translation Loss:   0.020166 => Txt Tokens per Sec:     2646 || Lr: 0.000100
2024-02-06 07:09:02,298 Epoch 1565: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.41 
2024-02-06 07:09:02,298 EPOCH 1566
2024-02-06 07:09:13,119 Epoch 1566: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-06 07:09:13,120 EPOCH 1567
2024-02-06 07:09:23,977 Epoch 1567: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-06 07:09:23,978 EPOCH 1568
2024-02-06 07:09:34,611 Epoch 1568: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-06 07:09:34,612 EPOCH 1569
2024-02-06 07:09:45,480 Epoch 1569: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-06 07:09:45,481 EPOCH 1570
2024-02-06 07:09:56,168 Epoch 1570: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-06 07:09:56,169 EPOCH 1571
2024-02-06 07:10:02,263 [Epoch: 1571 Step: 00026700] Batch Recognition Loss:   0.000421 => Gls Tokens per Sec:     1008 || Batch Translation Loss:   0.066963 => Txt Tokens per Sec:     2835 || Lr: 0.000100
2024-02-06 07:10:06,833 Epoch 1571: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-06 07:10:06,834 EPOCH 1572
2024-02-06 07:10:17,556 Epoch 1572: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-06 07:10:17,557 EPOCH 1573
2024-02-06 07:10:28,184 Epoch 1573: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-06 07:10:28,185 EPOCH 1574
2024-02-06 07:10:38,884 Epoch 1574: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-06 07:10:38,884 EPOCH 1575
2024-02-06 07:10:49,385 Epoch 1575: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 07:10:49,386 EPOCH 1576
2024-02-06 07:11:00,141 Epoch 1576: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 07:11:00,142 EPOCH 1577
2024-02-06 07:11:07,569 [Epoch: 1577 Step: 00026800] Batch Recognition Loss:   0.000296 => Gls Tokens per Sec:      654 || Batch Translation Loss:   0.022702 => Txt Tokens per Sec:     1963 || Lr: 0.000100
2024-02-06 07:11:10,816 Epoch 1577: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 07:11:10,816 EPOCH 1578
2024-02-06 07:11:21,687 Epoch 1578: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 07:11:21,688 EPOCH 1579
2024-02-06 07:11:32,630 Epoch 1579: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-06 07:11:32,631 EPOCH 1580
2024-02-06 07:11:43,557 Epoch 1580: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.72 
2024-02-06 07:11:43,558 EPOCH 1581
2024-02-06 07:11:54,346 Epoch 1581: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.64 
2024-02-06 07:11:54,347 EPOCH 1582
2024-02-06 07:12:05,148 Epoch 1582: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-06 07:12:05,149 EPOCH 1583
2024-02-06 07:12:09,525 [Epoch: 1583 Step: 00026900] Batch Recognition Loss:   0.000362 => Gls Tokens per Sec:      878 || Batch Translation Loss:   0.023763 => Txt Tokens per Sec:     2559 || Lr: 0.000100
2024-02-06 07:12:15,785 Epoch 1583: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-06 07:12:15,785 EPOCH 1584
2024-02-06 07:12:26,339 Epoch 1584: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-06 07:12:26,339 EPOCH 1585
2024-02-06 07:12:37,206 Epoch 1585: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-06 07:12:37,206 EPOCH 1586
2024-02-06 07:12:47,821 Epoch 1586: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-06 07:12:47,821 EPOCH 1587
2024-02-06 07:12:58,498 Epoch 1587: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-06 07:12:58,499 EPOCH 1588
2024-02-06 07:13:09,129 Epoch 1588: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-06 07:13:09,129 EPOCH 1589
2024-02-06 07:13:11,434 [Epoch: 1589 Step: 00027000] Batch Recognition Loss:   0.000291 => Gls Tokens per Sec:     1111 || Batch Translation Loss:   0.025999 => Txt Tokens per Sec:     3260 || Lr: 0.000100
2024-02-06 07:13:19,655 Epoch 1589: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-06 07:13:19,655 EPOCH 1590
2024-02-06 07:13:30,219 Epoch 1590: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-06 07:13:30,220 EPOCH 1591
2024-02-06 07:13:41,064 Epoch 1591: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-06 07:13:41,064 EPOCH 1592
2024-02-06 07:13:51,866 Epoch 1592: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-06 07:13:51,866 EPOCH 1593
2024-02-06 07:14:02,661 Epoch 1593: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-06 07:14:02,661 EPOCH 1594
2024-02-06 07:14:13,243 Epoch 1594: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-06 07:14:13,244 EPOCH 1595
2024-02-06 07:14:13,724 [Epoch: 1595 Step: 00027100] Batch Recognition Loss:   0.000199 => Gls Tokens per Sec:     2672 || Batch Translation Loss:   0.018559 => Txt Tokens per Sec:     7263 || Lr: 0.000100
2024-02-06 07:14:24,097 Epoch 1595: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-06 07:14:24,098 EPOCH 1596
2024-02-06 07:14:34,899 Epoch 1596: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-06 07:14:34,899 EPOCH 1597
2024-02-06 07:14:45,535 Epoch 1597: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-06 07:14:45,535 EPOCH 1598
2024-02-06 07:14:56,215 Epoch 1598: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-06 07:14:56,215 EPOCH 1599
2024-02-06 07:15:06,867 Epoch 1599: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-06 07:15:06,868 EPOCH 1600
2024-02-06 07:15:17,667 [Epoch: 1600 Step: 00027200] Batch Recognition Loss:   0.000183 => Gls Tokens per Sec:      984 || Batch Translation Loss:   0.011992 => Txt Tokens per Sec:     2721 || Lr: 0.000100
2024-02-06 07:15:17,668 Epoch 1600: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-06 07:15:17,668 EPOCH 1601
2024-02-06 07:15:28,595 Epoch 1601: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-06 07:15:28,596 EPOCH 1602
2024-02-06 07:15:39,420 Epoch 1602: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-06 07:15:39,421 EPOCH 1603
2024-02-06 07:15:50,202 Epoch 1603: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-06 07:15:50,203 EPOCH 1604
2024-02-06 07:16:00,969 Epoch 1604: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-06 07:16:00,970 EPOCH 1605
2024-02-06 07:16:11,611 Epoch 1605: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 07:16:11,612 EPOCH 1606
2024-02-06 07:16:17,717 [Epoch: 1606 Step: 00027300] Batch Recognition Loss:   0.000176 => Gls Tokens per Sec:     1573 || Batch Translation Loss:   0.051972 => Txt Tokens per Sec:     4227 || Lr: 0.000100
2024-02-06 07:16:22,257 Epoch 1606: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-06 07:16:22,257 EPOCH 1607
2024-02-06 07:16:33,292 Epoch 1607: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.63 
2024-02-06 07:16:33,293 EPOCH 1608
2024-02-06 07:16:44,079 Epoch 1608: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.73 
2024-02-06 07:16:44,080 EPOCH 1609
2024-02-06 07:16:54,846 Epoch 1609: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-06 07:16:54,847 EPOCH 1610
2024-02-06 07:17:05,469 Epoch 1610: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-06 07:17:05,470 EPOCH 1611
2024-02-06 07:17:16,122 Epoch 1611: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.50 
2024-02-06 07:17:16,123 EPOCH 1612
2024-02-06 07:17:24,585 [Epoch: 1612 Step: 00027400] Batch Recognition Loss:   0.000761 => Gls Tokens per Sec:      953 || Batch Translation Loss:   0.020578 => Txt Tokens per Sec:     2646 || Lr: 0.000100
2024-02-06 07:17:26,972 Epoch 1612: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-06 07:17:26,973 EPOCH 1613
2024-02-06 07:17:37,677 Epoch 1613: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-06 07:17:37,678 EPOCH 1614
2024-02-06 07:17:48,326 Epoch 1614: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-06 07:17:48,327 EPOCH 1615
2024-02-06 07:17:58,587 Epoch 1615: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-06 07:17:58,588 EPOCH 1616
2024-02-06 07:18:09,210 Epoch 1616: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.77 
2024-02-06 07:18:09,211 EPOCH 1617
2024-02-06 07:18:19,982 Epoch 1617: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-06 07:18:19,983 EPOCH 1618
2024-02-06 07:18:27,585 [Epoch: 1618 Step: 00027500] Batch Recognition Loss:   0.000382 => Gls Tokens per Sec:      892 || Batch Translation Loss:   0.033646 => Txt Tokens per Sec:     2405 || Lr: 0.000100
2024-02-06 07:18:30,608 Epoch 1618: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-06 07:18:30,608 EPOCH 1619
2024-02-06 07:18:41,240 Epoch 1619: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.99 
2024-02-06 07:18:41,240 EPOCH 1620
2024-02-06 07:18:51,830 Epoch 1620: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-06 07:18:51,830 EPOCH 1621
2024-02-06 07:19:02,505 Epoch 1621: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-06 07:19:02,506 EPOCH 1622
2024-02-06 07:19:13,266 Epoch 1622: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.82 
2024-02-06 07:19:13,267 EPOCH 1623
2024-02-06 07:19:24,095 Epoch 1623: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.25 
2024-02-06 07:19:24,096 EPOCH 1624
2024-02-06 07:19:31,782 [Epoch: 1624 Step: 00027600] Batch Recognition Loss:   0.000370 => Gls Tokens per Sec:      716 || Batch Translation Loss:   0.077383 => Txt Tokens per Sec:     2031 || Lr: 0.000100
2024-02-06 07:19:34,913 Epoch 1624: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.90 
2024-02-06 07:19:34,913 EPOCH 1625
2024-02-06 07:19:45,289 Epoch 1625: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.05 
2024-02-06 07:19:45,290 EPOCH 1626
2024-02-06 07:19:56,058 Epoch 1626: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.00 
2024-02-06 07:19:56,059 EPOCH 1627
2024-02-06 07:20:06,827 Epoch 1627: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.01 
2024-02-06 07:20:06,827 EPOCH 1628
2024-02-06 07:20:17,550 Epoch 1628: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.96 
2024-02-06 07:20:17,550 EPOCH 1629
2024-02-06 07:20:28,223 Epoch 1629: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.88 
2024-02-06 07:20:28,223 EPOCH 1630
2024-02-06 07:20:29,314 [Epoch: 1630 Step: 00027700] Batch Recognition Loss:   0.001790 => Gls Tokens per Sec:     4114 || Batch Translation Loss:   0.479842 => Txt Tokens per Sec:     9587 || Lr: 0.000100
2024-02-06 07:20:39,203 Epoch 1630: Total Training Recognition Loss 0.05  Total Training Translation Loss 4.26 
2024-02-06 07:20:39,203 EPOCH 1631
2024-02-06 07:20:50,061 Epoch 1631: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.90 
2024-02-06 07:20:50,062 EPOCH 1632
2024-02-06 07:21:00,764 Epoch 1632: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.89 
2024-02-06 07:21:00,764 EPOCH 1633
2024-02-06 07:21:11,438 Epoch 1633: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.25 
2024-02-06 07:21:11,438 EPOCH 1634
2024-02-06 07:21:22,152 Epoch 1634: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.97 
2024-02-06 07:21:22,153 EPOCH 1635
2024-02-06 07:21:32,930 Epoch 1635: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.82 
2024-02-06 07:21:32,931 EPOCH 1636
2024-02-06 07:21:34,097 [Epoch: 1636 Step: 00027800] Batch Recognition Loss:   0.000329 => Gls Tokens per Sec:     2748 || Batch Translation Loss:   0.039005 => Txt Tokens per Sec:     7408 || Lr: 0.000100
2024-02-06 07:21:43,470 Epoch 1636: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.79 
2024-02-06 07:21:43,471 EPOCH 1637
2024-02-06 07:21:54,476 Epoch 1637: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-06 07:21:54,476 EPOCH 1638
2024-02-06 07:22:04,982 Epoch 1638: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.42 
2024-02-06 07:22:04,982 EPOCH 1639
2024-02-06 07:22:15,679 Epoch 1639: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-06 07:22:15,680 EPOCH 1640
2024-02-06 07:22:26,191 Epoch 1640: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-06 07:22:26,192 EPOCH 1641
2024-02-06 07:22:36,828 Epoch 1641: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-06 07:22:36,828 EPOCH 1642
2024-02-06 07:22:40,921 [Epoch: 1642 Step: 00027900] Batch Recognition Loss:   0.000550 => Gls Tokens per Sec:      469 || Batch Translation Loss:   0.117077 => Txt Tokens per Sec:     1509 || Lr: 0.000100
2024-02-06 07:22:47,880 Epoch 1642: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-06 07:22:47,881 EPOCH 1643
2024-02-06 07:22:58,428 Epoch 1643: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-06 07:22:58,429 EPOCH 1644
2024-02-06 07:23:09,467 Epoch 1644: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-06 07:23:09,468 EPOCH 1645
2024-02-06 07:23:20,218 Epoch 1645: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-06 07:23:20,219 EPOCH 1646
2024-02-06 07:23:30,876 Epoch 1646: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-06 07:23:30,877 EPOCH 1647
2024-02-06 07:23:41,423 Epoch 1647: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.34 
2024-02-06 07:23:41,424 EPOCH 1648
2024-02-06 07:23:41,586 [Epoch: 1648 Step: 00028000] Batch Recognition Loss:   0.000331 => Gls Tokens per Sec:     3975 || Batch Translation Loss:   0.013095 => Txt Tokens per Sec:     8913 || Lr: 0.000100
2024-02-06 07:24:21,773 Validation result at epoch 1648, step    28000: duration: 40.1862s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.72721	Translation Loss: 94826.33594	PPL: 12981.33398
	Eval Metric: BLEU
	WER 3.04	(DEL: 0.00,	INS: 0.00,	SUB: 3.04)
	BLEU-4 0.52	(BLEU-1: 10.41,	BLEU-2: 3.31,	BLEU-3: 1.19,	BLEU-4: 0.52)
	CHRF 16.55	ROUGE 8.96
2024-02-06 07:24:21,775 Logging Recognition and Translation Outputs
2024-02-06 07:24:21,775 ========================================================================================================================
2024-02-06 07:24:21,775 Logging Sequence: 156_288.00
2024-02-06 07:24:21,775 	Gloss Reference :	A B+C+D+E
2024-02-06 07:24:21,775 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 07:24:21,775 	Gloss Alignment :	         
2024-02-06 07:24:21,776 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 07:24:21,777 	Text Reference  :	**** *** pooran  led the team to   victory  miny became winners of the  1st season
2024-02-06 07:24:21,777 	Text Hypothesis :	this was because of  the **** most pandemic we   will   have    to wait and see   
2024-02-06 07:24:21,777 	Text Alignment  :	I    I   S       S       D    S    S        S    S      S       S  S    S   S     
2024-02-06 07:24:21,777 ========================================================================================================================
2024-02-06 07:24:21,778 Logging Sequence: 98_135.00
2024-02-06 07:24:21,778 	Gloss Reference :	A B+C+D+E
2024-02-06 07:24:21,778 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 07:24:21,778 	Gloss Alignment :	         
2024-02-06 07:24:21,778 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 07:24:21,779 	Text Reference  :	however due to the ***** rise in   coronavirus cases the tournament was    shifted
2024-02-06 07:24:21,779 	Text Hypothesis :	******* *** ** the girls were very happy       and   the ********** police filed  
2024-02-06 07:24:21,779 	Text Alignment  :	D       D   D      I     S    S    S           S         D          S      S      
2024-02-06 07:24:21,779 ========================================================================================================================
2024-02-06 07:24:21,780 Logging Sequence: 161_47.00
2024-02-06 07:24:21,780 	Gloss Reference :	A B+C+D+E
2024-02-06 07:24:21,780 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 07:24:21,780 	Gloss Alignment :	         
2024-02-06 07:24:21,781 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 07:24:21,782 	Text Reference  :	he requested confidentiality as he      was    planning to  make     an   official announcement
2024-02-06 07:24:21,782 	Text Hypothesis :	** ********* *************** ** anushka sharma and      his stepping down as       well        
2024-02-06 07:24:21,782 	Text Alignment  :	D  D         D               D  S       S      S        S   S        S    S        S           
2024-02-06 07:24:21,782 ========================================================================================================================
2024-02-06 07:24:21,782 Logging Sequence: 131_159.00
2024-02-06 07:24:21,782 	Gloss Reference :	A B+C+D+E
2024-02-06 07:24:21,782 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 07:24:21,782 	Gloss Alignment :	         
2024-02-06 07:24:21,783 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 07:24:21,784 	Text Reference  :	**** chanu also met       biren singh following the meeting singh    described chanu as ** ** **** our   nation' pride
2024-02-06 07:24:21,784 	Text Hypothesis :	fans are   now  wondering why   he    did       not say     anything about     dhoni as he is very close to      work 
2024-02-06 07:24:21,784 	Text Alignment  :	I    S     S    S         S     S     S         S   S       S        S         S        I  I  I    S     S       S    
2024-02-06 07:24:21,785 ========================================================================================================================
2024-02-06 07:24:21,785 Logging Sequence: 137_167.00
2024-02-06 07:24:21,785 	Gloss Reference :	A B+C+D+E
2024-02-06 07:24:21,785 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 07:24:21,785 	Gloss Alignment :	         
2024-02-06 07:24:21,785 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 07:24:21,787 	Text Reference  :	however after 630 pm     there will be     certain fan   zones where    beer will be    available and  nowhere else
2024-02-06 07:24:21,787 	Text Hypothesis :	******* ***** the indian team  was  played between india and   pakistan lost the  match on        11th october 2022
2024-02-06 07:24:21,787 	Text Alignment  :	D       D     S   S      S     S    S      S       S     S     S        S    S    S     S         S    S       S   
2024-02-06 07:24:21,788 ========================================================================================================================
2024-02-06 07:24:33,033 Epoch 1648: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 07:24:33,033 EPOCH 1649
2024-02-06 07:24:43,770 Epoch 1649: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 07:24:43,770 EPOCH 1650
2024-02-06 07:24:54,518 Epoch 1650: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 07:24:54,518 EPOCH 1651
2024-02-06 07:25:05,427 Epoch 1651: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 07:25:05,428 EPOCH 1652
2024-02-06 07:25:16,127 Epoch 1652: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-06 07:25:16,128 EPOCH 1653
2024-02-06 07:25:26,687 [Epoch: 1653 Step: 00028100] Batch Recognition Loss:   0.000346 => Gls Tokens per Sec:      945 || Batch Translation Loss:   0.008514 => Txt Tokens per Sec:     2648 || Lr: 0.000100
2024-02-06 07:25:26,837 Epoch 1653: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 07:25:26,837 EPOCH 1654
2024-02-06 07:25:37,728 Epoch 1654: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-06 07:25:37,729 EPOCH 1655
2024-02-06 07:25:48,352 Epoch 1655: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 07:25:48,353 EPOCH 1656
2024-02-06 07:25:58,970 Epoch 1656: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 07:25:58,971 EPOCH 1657
2024-02-06 07:26:09,952 Epoch 1657: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 07:26:09,953 EPOCH 1658
2024-02-06 07:26:20,650 Epoch 1658: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 07:26:20,651 EPOCH 1659
2024-02-06 07:26:30,705 [Epoch: 1659 Step: 00028200] Batch Recognition Loss:   0.000210 => Gls Tokens per Sec:      865 || Batch Translation Loss:   0.015362 => Txt Tokens per Sec:     2420 || Lr: 0.000100
2024-02-06 07:26:31,434 Epoch 1659: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 07:26:31,435 EPOCH 1660
2024-02-06 07:26:42,038 Epoch 1660: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 07:26:42,039 EPOCH 1661
2024-02-06 07:26:52,805 Epoch 1661: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 07:26:52,806 EPOCH 1662
2024-02-06 07:27:03,430 Epoch 1662: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 07:27:03,431 EPOCH 1663
2024-02-06 07:27:14,187 Epoch 1663: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-06 07:27:14,188 EPOCH 1664
2024-02-06 07:27:25,084 Epoch 1664: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 07:27:25,085 EPOCH 1665
2024-02-06 07:27:32,353 [Epoch: 1665 Step: 00028300] Batch Recognition Loss:   0.000212 => Gls Tokens per Sec:     1057 || Batch Translation Loss:   0.013300 => Txt Tokens per Sec:     3014 || Lr: 0.000100
2024-02-06 07:27:35,613 Epoch 1665: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 07:27:35,613 EPOCH 1666
2024-02-06 07:27:46,432 Epoch 1666: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-06 07:27:46,433 EPOCH 1667
2024-02-06 07:27:57,466 Epoch 1667: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-06 07:27:57,467 EPOCH 1668
2024-02-06 07:28:08,229 Epoch 1668: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 07:28:08,230 EPOCH 1669
2024-02-06 07:28:18,749 Epoch 1669: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 07:28:18,749 EPOCH 1670
2024-02-06 07:28:29,693 Epoch 1670: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-06 07:28:29,693 EPOCH 1671
2024-02-06 07:28:35,466 [Epoch: 1671 Step: 00028400] Batch Recognition Loss:   0.000285 => Gls Tokens per Sec:     1064 || Batch Translation Loss:   0.038495 => Txt Tokens per Sec:     3014 || Lr: 0.000100
2024-02-06 07:28:40,324 Epoch 1671: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-06 07:28:40,325 EPOCH 1672
2024-02-06 07:28:51,041 Epoch 1672: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-06 07:28:51,041 EPOCH 1673
2024-02-06 07:29:01,732 Epoch 1673: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-06 07:29:01,733 EPOCH 1674
2024-02-06 07:29:12,648 Epoch 1674: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.92 
2024-02-06 07:29:12,649 EPOCH 1675
2024-02-06 07:29:23,642 Epoch 1675: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.21 
2024-02-06 07:29:23,643 EPOCH 1676
2024-02-06 07:29:34,428 Epoch 1676: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.17 
2024-02-06 07:29:34,428 EPOCH 1677
2024-02-06 07:29:39,381 [Epoch: 1677 Step: 00028500] Batch Recognition Loss:   0.000270 => Gls Tokens per Sec:     1034 || Batch Translation Loss:   0.215940 => Txt Tokens per Sec:     2932 || Lr: 0.000100
2024-02-06 07:29:45,416 Epoch 1677: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.32 
2024-02-06 07:29:45,417 EPOCH 1678
2024-02-06 07:29:56,447 Epoch 1678: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.15 
2024-02-06 07:29:56,447 EPOCH 1679
2024-02-06 07:30:07,333 Epoch 1679: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.48 
2024-02-06 07:30:07,334 EPOCH 1680
2024-02-06 07:30:18,025 Epoch 1680: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.89 
2024-02-06 07:30:18,025 EPOCH 1681
2024-02-06 07:30:28,561 Epoch 1681: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.10 
2024-02-06 07:30:28,561 EPOCH 1682
2024-02-06 07:30:39,128 Epoch 1682: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.88 
2024-02-06 07:30:39,129 EPOCH 1683
2024-02-06 07:30:40,276 [Epoch: 1683 Step: 00028600] Batch Recognition Loss:   0.000463 => Gls Tokens per Sec:     3348 || Batch Translation Loss:   0.168822 => Txt Tokens per Sec:     8077 || Lr: 0.000100
2024-02-06 07:30:49,683 Epoch 1683: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.96 
2024-02-06 07:30:49,684 EPOCH 1684
2024-02-06 07:31:00,308 Epoch 1684: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.98 
2024-02-06 07:31:00,308 EPOCH 1685
2024-02-06 07:31:11,171 Epoch 1685: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.13 
2024-02-06 07:31:11,172 EPOCH 1686
2024-02-06 07:31:21,972 Epoch 1686: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.39 
2024-02-06 07:31:21,972 EPOCH 1687
2024-02-06 07:31:32,674 Epoch 1687: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.58 
2024-02-06 07:31:32,675 EPOCH 1688
2024-02-06 07:31:43,576 Epoch 1688: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.84 
2024-02-06 07:31:43,577 EPOCH 1689
2024-02-06 07:31:44,278 [Epoch: 1689 Step: 00028700] Batch Recognition Loss:   0.000455 => Gls Tokens per Sec:     3657 || Batch Translation Loss:   0.041137 => Txt Tokens per Sec:     9749 || Lr: 0.000100
2024-02-06 07:31:54,177 Epoch 1689: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.65 
2024-02-06 07:31:54,178 EPOCH 1690
2024-02-06 07:32:04,970 Epoch 1690: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-06 07:32:04,971 EPOCH 1691
2024-02-06 07:32:15,713 Epoch 1691: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.41 
2024-02-06 07:32:15,713 EPOCH 1692
2024-02-06 07:32:26,636 Epoch 1692: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-06 07:32:26,637 EPOCH 1693
2024-02-06 07:32:37,212 Epoch 1693: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-06 07:32:37,213 EPOCH 1694
2024-02-06 07:32:48,159 Epoch 1694: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-06 07:32:48,160 EPOCH 1695
2024-02-06 07:32:49,915 [Epoch: 1695 Step: 00028800] Batch Recognition Loss:   0.000790 => Gls Tokens per Sec:      729 || Batch Translation Loss:   0.009864 => Txt Tokens per Sec:     1830 || Lr: 0.000100
2024-02-06 07:32:58,865 Epoch 1695: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-06 07:32:58,866 EPOCH 1696
2024-02-06 07:33:09,549 Epoch 1696: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 07:33:09,549 EPOCH 1697
2024-02-06 07:33:20,459 Epoch 1697: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 07:33:20,460 EPOCH 1698
2024-02-06 07:33:31,262 Epoch 1698: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 07:33:31,263 EPOCH 1699
2024-02-06 07:33:41,908 Epoch 1699: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 07:33:41,909 EPOCH 1700
2024-02-06 07:33:52,667 [Epoch: 1700 Step: 00028900] Batch Recognition Loss:   0.000294 => Gls Tokens per Sec:      987 || Batch Translation Loss:   0.016463 => Txt Tokens per Sec:     2731 || Lr: 0.000100
2024-02-06 07:33:52,668 Epoch 1700: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 07:33:52,668 EPOCH 1701
2024-02-06 07:34:03,391 Epoch 1701: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 07:34:03,392 EPOCH 1702
2024-02-06 07:34:14,131 Epoch 1702: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 07:34:14,132 EPOCH 1703
2024-02-06 07:34:25,251 Epoch 1703: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 07:34:25,252 EPOCH 1704
2024-02-06 07:34:36,045 Epoch 1704: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 07:34:36,045 EPOCH 1705
2024-02-06 07:34:46,932 Epoch 1705: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 07:34:46,933 EPOCH 1706
2024-02-06 07:34:55,882 [Epoch: 1706 Step: 00029000] Batch Recognition Loss:   0.000149 => Gls Tokens per Sec:     1044 || Batch Translation Loss:   0.012727 => Txt Tokens per Sec:     2814 || Lr: 0.000100
2024-02-06 07:34:57,838 Epoch 1706: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 07:34:57,838 EPOCH 1707
2024-02-06 07:35:08,715 Epoch 1707: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 07:35:08,716 EPOCH 1708
2024-02-06 07:35:19,472 Epoch 1708: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 07:35:19,473 EPOCH 1709
2024-02-06 07:35:30,399 Epoch 1709: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-06 07:35:30,400 EPOCH 1710
2024-02-06 07:35:41,069 Epoch 1710: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 07:35:41,070 EPOCH 1711
2024-02-06 07:35:51,649 Epoch 1711: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-06 07:35:51,649 EPOCH 1712
2024-02-06 07:35:59,428 [Epoch: 1712 Step: 00029100] Batch Recognition Loss:   0.000273 => Gls Tokens per Sec:     1036 || Batch Translation Loss:   0.017516 => Txt Tokens per Sec:     2835 || Lr: 0.000100
2024-02-06 07:36:02,371 Epoch 1712: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-06 07:36:02,372 EPOCH 1713
2024-02-06 07:36:13,350 Epoch 1713: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 07:36:13,350 EPOCH 1714
2024-02-06 07:36:24,073 Epoch 1714: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-06 07:36:24,074 EPOCH 1715
2024-02-06 07:36:35,013 Epoch 1715: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 07:36:35,014 EPOCH 1716
2024-02-06 07:36:45,749 Epoch 1716: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 07:36:45,750 EPOCH 1717
2024-02-06 07:36:56,512 Epoch 1717: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 07:36:56,512 EPOCH 1718
2024-02-06 07:37:02,903 [Epoch: 1718 Step: 00029200] Batch Recognition Loss:   0.000131 => Gls Tokens per Sec:     1061 || Batch Translation Loss:   0.010096 => Txt Tokens per Sec:     3055 || Lr: 0.000100
2024-02-06 07:37:07,341 Epoch 1718: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.30 
2024-02-06 07:37:07,342 EPOCH 1719
2024-02-06 07:37:18,197 Epoch 1719: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-06 07:37:18,197 EPOCH 1720
2024-02-06 07:37:29,037 Epoch 1720: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-06 07:37:29,037 EPOCH 1721
2024-02-06 07:37:39,862 Epoch 1721: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-06 07:37:39,863 EPOCH 1722
2024-02-06 07:37:50,804 Epoch 1722: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-06 07:37:50,805 EPOCH 1723
2024-02-06 07:38:01,518 Epoch 1723: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-06 07:38:01,518 EPOCH 1724
2024-02-06 07:38:08,268 [Epoch: 1724 Step: 00029300] Batch Recognition Loss:   0.000270 => Gls Tokens per Sec:      853 || Batch Translation Loss:   0.023962 => Txt Tokens per Sec:     2492 || Lr: 0.000100
2024-02-06 07:38:12,123 Epoch 1724: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-06 07:38:12,124 EPOCH 1725
2024-02-06 07:38:23,164 Epoch 1725: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-06 07:38:23,164 EPOCH 1726
2024-02-06 07:38:34,198 Epoch 1726: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-06 07:38:34,199 EPOCH 1727
2024-02-06 07:38:44,978 Epoch 1727: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-06 07:38:44,978 EPOCH 1728
2024-02-06 07:38:55,393 Epoch 1728: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-06 07:38:55,394 EPOCH 1729
2024-02-06 07:39:06,233 Epoch 1729: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-06 07:39:06,234 EPOCH 1730
2024-02-06 07:39:11,080 [Epoch: 1730 Step: 00029400] Batch Recognition Loss:   0.000201 => Gls Tokens per Sec:      925 || Batch Translation Loss:   0.016771 => Txt Tokens per Sec:     2739 || Lr: 0.000100
2024-02-06 07:39:17,061 Epoch 1730: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-06 07:39:17,062 EPOCH 1731
2024-02-06 07:39:27,822 Epoch 1731: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-06 07:39:27,822 EPOCH 1732
2024-02-06 07:39:38,461 Epoch 1732: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.96 
2024-02-06 07:39:38,461 EPOCH 1733
2024-02-06 07:39:49,343 Epoch 1733: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.13 
2024-02-06 07:39:49,343 EPOCH 1734
2024-02-06 07:39:59,955 Epoch 1734: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.40 
2024-02-06 07:39:59,956 EPOCH 1735
2024-02-06 07:40:10,562 Epoch 1735: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.56 
2024-02-06 07:40:10,563 EPOCH 1736
2024-02-06 07:40:13,139 [Epoch: 1736 Step: 00029500] Batch Recognition Loss:   0.000352 => Gls Tokens per Sec:     1242 || Batch Translation Loss:   0.051954 => Txt Tokens per Sec:     3549 || Lr: 0.000100
2024-02-06 07:40:21,141 Epoch 1736: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.52 
2024-02-06 07:40:21,141 EPOCH 1737
2024-02-06 07:40:31,822 Epoch 1737: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.55 
2024-02-06 07:40:31,823 EPOCH 1738
2024-02-06 07:40:42,468 Epoch 1738: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.43 
2024-02-06 07:40:42,469 EPOCH 1739
2024-02-06 07:40:53,147 Epoch 1739: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.49 
2024-02-06 07:40:53,148 EPOCH 1740
2024-02-06 07:41:04,076 Epoch 1740: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.89 
2024-02-06 07:41:04,077 EPOCH 1741
2024-02-06 07:41:14,906 Epoch 1741: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.82 
2024-02-06 07:41:14,907 EPOCH 1742
2024-02-06 07:41:15,781 [Epoch: 1742 Step: 00029600] Batch Recognition Loss:   0.000287 => Gls Tokens per Sec:     2200 || Batch Translation Loss:   0.032823 => Txt Tokens per Sec:     6292 || Lr: 0.000100
2024-02-06 07:41:25,746 Epoch 1742: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-06 07:41:25,747 EPOCH 1743
2024-02-06 07:41:36,843 Epoch 1743: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.83 
2024-02-06 07:41:36,844 EPOCH 1744
2024-02-06 07:41:47,501 Epoch 1744: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.82 
2024-02-06 07:41:47,502 EPOCH 1745
2024-02-06 07:41:58,458 Epoch 1745: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.84 
2024-02-06 07:41:58,458 EPOCH 1746
2024-02-06 07:42:09,185 Epoch 1746: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.80 
2024-02-06 07:42:09,186 EPOCH 1747
2024-02-06 07:42:19,945 Epoch 1747: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-06 07:42:19,946 EPOCH 1748
2024-02-06 07:42:21,882 [Epoch: 1748 Step: 00029700] Batch Recognition Loss:   0.003809 => Gls Tokens per Sec:      331 || Batch Translation Loss:   0.043196 => Txt Tokens per Sec:     1146 || Lr: 0.000100
2024-02-06 07:42:30,734 Epoch 1748: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.52 
2024-02-06 07:42:30,735 EPOCH 1749
2024-02-06 07:42:41,501 Epoch 1749: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-06 07:42:41,502 EPOCH 1750
2024-02-06 07:42:52,041 Epoch 1750: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.55 
2024-02-06 07:42:52,042 EPOCH 1751
2024-02-06 07:43:02,888 Epoch 1751: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.73 
2024-02-06 07:43:02,889 EPOCH 1752
2024-02-06 07:43:13,730 Epoch 1752: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-06 07:43:13,731 EPOCH 1753
2024-02-06 07:43:24,183 [Epoch: 1753 Step: 00029800] Batch Recognition Loss:   0.001035 => Gls Tokens per Sec:      955 || Batch Translation Loss:   0.021182 => Txt Tokens per Sec:     2630 || Lr: 0.000100
2024-02-06 07:43:24,428 Epoch 1753: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.56 
2024-02-06 07:43:24,428 EPOCH 1754
2024-02-06 07:43:34,993 Epoch 1754: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.99 
2024-02-06 07:43:34,994 EPOCH 1755
2024-02-06 07:43:45,536 Epoch 1755: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.83 
2024-02-06 07:43:45,537 EPOCH 1756
2024-02-06 07:43:56,411 Epoch 1756: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.34 
2024-02-06 07:43:56,411 EPOCH 1757
2024-02-06 07:44:07,233 Epoch 1757: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.37 
2024-02-06 07:44:07,233 EPOCH 1758
2024-02-06 07:44:18,094 Epoch 1758: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.97 
2024-02-06 07:44:18,095 EPOCH 1759
2024-02-06 07:44:27,930 [Epoch: 1759 Step: 00029900] Batch Recognition Loss:   0.000707 => Gls Tokens per Sec:      885 || Batch Translation Loss:   0.047613 => Txt Tokens per Sec:     2523 || Lr: 0.000100
2024-02-06 07:44:28,647 Epoch 1759: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.95 
2024-02-06 07:44:28,647 EPOCH 1760
2024-02-06 07:44:39,514 Epoch 1760: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.73 
2024-02-06 07:44:39,514 EPOCH 1761
2024-02-06 07:44:50,291 Epoch 1761: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-06 07:44:50,291 EPOCH 1762
2024-02-06 07:45:00,983 Epoch 1762: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-06 07:45:00,983 EPOCH 1763
2024-02-06 07:45:11,669 Epoch 1763: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-06 07:45:11,670 EPOCH 1764
2024-02-06 07:45:22,371 Epoch 1764: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-06 07:45:22,372 EPOCH 1765
2024-02-06 07:45:30,210 [Epoch: 1765 Step: 00030000] Batch Recognition Loss:   0.000618 => Gls Tokens per Sec:      947 || Batch Translation Loss:   0.026595 => Txt Tokens per Sec:     2573 || Lr: 0.000100
2024-02-06 07:46:10,888 Validation result at epoch 1765, step    30000: duration: 40.6781s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.68213	Translation Loss: 95288.54688	PPL: 13594.67090
	Eval Metric: BLEU
	WER 3.25	(DEL: 0.00,	INS: 0.00,	SUB: 3.25)
	BLEU-4 0.61	(BLEU-1: 9.49,	BLEU-2: 2.78,	BLEU-3: 1.19,	BLEU-4: 0.61)
	CHRF 16.73	ROUGE 7.92
2024-02-06 07:46:10,890 Logging Recognition and Translation Outputs
2024-02-06 07:46:10,890 ========================================================================================================================
2024-02-06 07:46:10,890 Logging Sequence: 146_102.00
2024-02-06 07:46:10,890 	Gloss Reference :	A B+C+D+E
2024-02-06 07:46:10,891 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 07:46:10,891 	Gloss Alignment :	         
2024-02-06 07:46:10,891 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 07:46:10,892 	Text Reference  :	famous indian champion players like   kidambi srikanth and ashwini ponappa have  tested positive for coronavirus
2024-02-06 07:46:10,892 	Text Hypothesis :	****** ****** ******** rohit   sharma also    known    as  the     support staff who    also     an  amazing    
2024-02-06 07:46:10,893 	Text Alignment  :	D      D      D        S       S      S       S        S   S       S       S     S      S        S   S          
2024-02-06 07:46:10,893 ========================================================================================================================
2024-02-06 07:46:10,893 Logging Sequence: 53_178.00
2024-02-06 07:46:10,893 	Gloss Reference :	A B+C+D+E
2024-02-06 07:46:10,893 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 07:46:10,893 	Gloss Alignment :	         
2024-02-06 07:46:10,893 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 07:46:10,894 	Text Reference  :	the money would help all those     affected by the humanitarian crisis in        afghanistan
2024-02-06 07:46:10,895 	Text Hypothesis :	*** ***** ***** i    am  extremely sadened  by *** this         fan    following etc        
2024-02-06 07:46:10,895 	Text Alignment  :	D   D     D     S    S   S         S           D   S            S      S         S          
2024-02-06 07:46:10,895 ========================================================================================================================
2024-02-06 07:46:10,895 Logging Sequence: 129_200.00
2024-02-06 07:46:10,895 	Gloss Reference :	A B+C+D+E      
2024-02-06 07:46:10,895 	Gloss Hypothesis:	A B+C+D+E+D+E+D
2024-02-06 07:46:10,896 	Gloss Alignment :	  S            
2024-02-06 07:46:10,896 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 07:46:10,897 	Text Reference  :	the ioc would lose about 4    billion if the olympics were to   be     cancelled
2024-02-06 07:46:10,897 	Text Hypothesis :	*** *** he    won  a     gold medal   at the ******** **** 2012 london olympics 
2024-02-06 07:46:10,897 	Text Alignment  :	D   D   S     S    S     S    S       S      D        D    S    S      S        
2024-02-06 07:46:10,897 ========================================================================================================================
2024-02-06 07:46:10,897 Logging Sequence: 77_2.00
2024-02-06 07:46:10,898 	Gloss Reference :	A B+C+D+E
2024-02-06 07:46:10,898 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 07:46:10,898 	Gloss Alignment :	         
2024-02-06 07:46:10,898 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 07:46:10,900 	Text Reference  :	on ** **** 25th april the *** ipl   match between sunrisers hyderabad and  delhi   capitals ended in a tie
2024-02-06 07:46:10,900 	Text Hypothesis :	on 16 july 2021 in    the t20 world cup   sri     lanka     had       also against each     other in * uae
2024-02-06 07:46:10,900 	Text Alignment  :	   I  I    S    S         I   S     S     S       S         S         S    S       S        S        D S  
2024-02-06 07:46:10,900 ========================================================================================================================
2024-02-06 07:46:10,900 Logging Sequence: 119_170.00
2024-02-06 07:46:10,900 	Gloss Reference :	A B+C+D+E
2024-02-06 07:46:10,901 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 07:46:10,901 	Gloss Alignment :	         
2024-02-06 07:46:10,901 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 07:46:10,902 	Text Reference  :	they said it was a     proud    moment messi is        a  big hearted man  
2024-02-06 07:46:10,902 	Text Hypothesis :	**** **** ** *** messi intended to     gift  something to all the     match
2024-02-06 07:46:10,902 	Text Alignment  :	D    D    D  D   S     S        S      S     S         S  S   S       S    
2024-02-06 07:46:10,902 ========================================================================================================================
2024-02-06 07:46:14,294 Epoch 1765: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-06 07:46:14,295 EPOCH 1766
2024-02-06 07:46:25,466 Epoch 1766: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-06 07:46:25,467 EPOCH 1767
2024-02-06 07:46:36,342 Epoch 1767: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-06 07:46:36,342 EPOCH 1768
2024-02-06 07:46:47,072 Epoch 1768: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-06 07:46:47,073 EPOCH 1769
2024-02-06 07:46:57,984 Epoch 1769: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.43 
2024-02-06 07:46:57,985 EPOCH 1770
2024-02-06 07:47:08,709 Epoch 1770: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-06 07:47:08,710 EPOCH 1771
2024-02-06 07:47:13,749 [Epoch: 1771 Step: 00030100] Batch Recognition Loss:   0.000455 => Gls Tokens per Sec:     1271 || Batch Translation Loss:   0.026368 => Txt Tokens per Sec:     3454 || Lr: 0.000100
2024-02-06 07:47:19,395 Epoch 1771: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-06 07:47:19,395 EPOCH 1772
2024-02-06 07:47:29,981 Epoch 1772: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-06 07:47:29,981 EPOCH 1773
2024-02-06 07:47:40,685 Epoch 1773: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-06 07:47:40,685 EPOCH 1774
2024-02-06 07:47:51,277 Epoch 1774: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.87 
2024-02-06 07:47:51,278 EPOCH 1775
2024-02-06 07:48:01,985 Epoch 1775: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-06 07:48:01,985 EPOCH 1776
2024-02-06 07:48:12,935 Epoch 1776: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-06 07:48:12,936 EPOCH 1777
2024-02-06 07:48:16,238 [Epoch: 1777 Step: 00030200] Batch Recognition Loss:   0.000241 => Gls Tokens per Sec:     1551 || Batch Translation Loss:   0.084905 => Txt Tokens per Sec:     4527 || Lr: 0.000100
2024-02-06 07:48:23,482 Epoch 1777: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.75 
2024-02-06 07:48:23,483 EPOCH 1778
2024-02-06 07:48:34,165 Epoch 1778: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-06 07:48:34,166 EPOCH 1779
2024-02-06 07:48:44,963 Epoch 1779: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-06 07:48:44,964 EPOCH 1780
2024-02-06 07:48:55,743 Epoch 1780: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.86 
2024-02-06 07:48:55,743 EPOCH 1781
2024-02-06 07:49:06,523 Epoch 1781: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-06 07:49:06,524 EPOCH 1782
2024-02-06 07:49:17,528 Epoch 1782: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.73 
2024-02-06 07:49:17,529 EPOCH 1783
2024-02-06 07:49:18,727 [Epoch: 1783 Step: 00030300] Batch Recognition Loss:   0.000377 => Gls Tokens per Sec:     3208 || Batch Translation Loss:   0.010520 => Txt Tokens per Sec:     8073 || Lr: 0.000100
2024-02-06 07:49:28,041 Epoch 1783: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-06 07:49:28,042 EPOCH 1784
2024-02-06 07:49:39,004 Epoch 1784: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-06 07:49:39,004 EPOCH 1785
2024-02-06 07:49:49,500 Epoch 1785: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-06 07:49:49,501 EPOCH 1786
2024-02-06 07:50:00,203 Epoch 1786: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-06 07:50:00,204 EPOCH 1787
2024-02-06 07:50:10,916 Epoch 1787: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-06 07:50:10,917 EPOCH 1788
2024-02-06 07:50:21,352 Epoch 1788: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-06 07:50:21,353 EPOCH 1789
2024-02-06 07:50:23,398 [Epoch: 1789 Step: 00030400] Batch Recognition Loss:   0.000360 => Gls Tokens per Sec:     1253 || Batch Translation Loss:   0.027168 => Txt Tokens per Sec:     3083 || Lr: 0.000100
2024-02-06 07:50:32,019 Epoch 1789: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-06 07:50:32,019 EPOCH 1790
2024-02-06 07:50:42,608 Epoch 1790: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-06 07:50:42,609 EPOCH 1791
2024-02-06 07:50:53,305 Epoch 1791: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.87 
2024-02-06 07:50:53,306 EPOCH 1792
2024-02-06 07:51:03,896 Epoch 1792: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.54 
2024-02-06 07:51:03,896 EPOCH 1793
2024-02-06 07:51:14,639 Epoch 1793: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.26 
2024-02-06 07:51:14,639 EPOCH 1794
2024-02-06 07:51:25,378 Epoch 1794: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.59 
2024-02-06 07:51:25,378 EPOCH 1795
2024-02-06 07:51:27,235 [Epoch: 1795 Step: 00030500] Batch Recognition Loss:   0.000404 => Gls Tokens per Sec:      690 || Batch Translation Loss:   0.043712 => Txt Tokens per Sec:     1996 || Lr: 0.000100
2024-02-06 07:51:36,147 Epoch 1795: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.60 
2024-02-06 07:51:36,148 EPOCH 1796
2024-02-06 07:51:46,875 Epoch 1796: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.92 
2024-02-06 07:51:46,876 EPOCH 1797
2024-02-06 07:51:57,521 Epoch 1797: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.01 
2024-02-06 07:51:57,521 EPOCH 1798
2024-02-06 07:52:08,203 Epoch 1798: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.82 
2024-02-06 07:52:08,204 EPOCH 1799
2024-02-06 07:52:18,997 Epoch 1799: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-06 07:52:18,998 EPOCH 1800
2024-02-06 07:52:29,722 [Epoch: 1800 Step: 00030600] Batch Recognition Loss:   0.000605 => Gls Tokens per Sec:      990 || Batch Translation Loss:   0.046802 => Txt Tokens per Sec:     2740 || Lr: 0.000100
2024-02-06 07:52:29,723 Epoch 1800: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.73 
2024-02-06 07:52:29,724 EPOCH 1801
2024-02-06 07:52:40,160 Epoch 1801: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.34 
2024-02-06 07:52:40,160 EPOCH 1802
2024-02-06 07:52:51,240 Epoch 1802: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.14 
2024-02-06 07:52:51,241 EPOCH 1803
2024-02-06 07:53:01,881 Epoch 1803: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.64 
2024-02-06 07:53:01,882 EPOCH 1804
2024-02-06 07:53:12,611 Epoch 1804: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.82 
2024-02-06 07:53:12,611 EPOCH 1805
2024-02-06 07:53:23,326 Epoch 1805: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.53 
2024-02-06 07:53:23,327 EPOCH 1806
2024-02-06 07:53:33,586 [Epoch: 1806 Step: 00030700] Batch Recognition Loss:   0.000628 => Gls Tokens per Sec:      911 || Batch Translation Loss:   0.019416 => Txt Tokens per Sec:     2495 || Lr: 0.000100
2024-02-06 07:53:34,195 Epoch 1806: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-06 07:53:34,196 EPOCH 1807
2024-02-06 07:53:44,737 Epoch 1807: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-06 07:53:44,737 EPOCH 1808
2024-02-06 07:53:55,284 Epoch 1808: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-06 07:53:55,285 EPOCH 1809
2024-02-06 07:54:06,033 Epoch 1809: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 07:54:06,033 EPOCH 1810
2024-02-06 07:54:16,936 Epoch 1810: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.26 
2024-02-06 07:54:16,937 EPOCH 1811
2024-02-06 07:54:27,773 Epoch 1811: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 07:54:27,774 EPOCH 1812
2024-02-06 07:54:37,812 [Epoch: 1812 Step: 00030800] Batch Recognition Loss:   0.000497 => Gls Tokens per Sec:      803 || Batch Translation Loss:   0.015813 => Txt Tokens per Sec:     2257 || Lr: 0.000100
2024-02-06 07:54:38,791 Epoch 1812: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 07:54:38,792 EPOCH 1813
2024-02-06 07:54:49,674 Epoch 1813: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 07:54:49,674 EPOCH 1814
2024-02-06 07:55:00,291 Epoch 1814: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 07:55:00,292 EPOCH 1815
2024-02-06 07:55:10,699 Epoch 1815: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-06 07:55:10,700 EPOCH 1816
2024-02-06 07:55:21,596 Epoch 1816: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 07:55:21,596 EPOCH 1817
2024-02-06 07:55:32,252 Epoch 1817: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 07:55:32,253 EPOCH 1818
2024-02-06 07:55:37,707 [Epoch: 1818 Step: 00030900] Batch Recognition Loss:   0.000193 => Gls Tokens per Sec:     1291 || Batch Translation Loss:   0.011244 => Txt Tokens per Sec:     3434 || Lr: 0.000100
2024-02-06 07:55:42,840 Epoch 1818: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 07:55:42,840 EPOCH 1819
2024-02-06 07:55:53,550 Epoch 1819: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-06 07:55:53,551 EPOCH 1820
2024-02-06 07:56:04,264 Epoch 1820: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-06 07:56:04,265 EPOCH 1821
2024-02-06 07:56:14,977 Epoch 1821: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-06 07:56:14,977 EPOCH 1822
2024-02-06 07:56:25,615 Epoch 1822: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-06 07:56:25,615 EPOCH 1823
2024-02-06 07:56:36,513 Epoch 1823: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-06 07:56:36,513 EPOCH 1824
2024-02-06 07:56:43,318 [Epoch: 1824 Step: 00031000] Batch Recognition Loss:   0.000209 => Gls Tokens per Sec:      847 || Batch Translation Loss:   0.044862 => Txt Tokens per Sec:     2515 || Lr: 0.000100
2024-02-06 07:56:47,144 Epoch 1824: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-06 07:56:47,144 EPOCH 1825
2024-02-06 07:56:57,879 Epoch 1825: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-06 07:56:57,880 EPOCH 1826
2024-02-06 07:57:08,538 Epoch 1826: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-06 07:57:08,539 EPOCH 1827
2024-02-06 07:57:19,294 Epoch 1827: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-06 07:57:19,295 EPOCH 1828
2024-02-06 07:57:29,824 Epoch 1828: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-06 07:57:29,825 EPOCH 1829
2024-02-06 07:57:40,637 Epoch 1829: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-06 07:57:40,638 EPOCH 1830
2024-02-06 07:57:47,299 [Epoch: 1830 Step: 00031100] Batch Recognition Loss:   0.001664 => Gls Tokens per Sec:      673 || Batch Translation Loss:   0.044194 => Txt Tokens per Sec:     2100 || Lr: 0.000100
2024-02-06 07:57:51,356 Epoch 1830: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.85 
2024-02-06 07:57:51,357 EPOCH 1831
2024-02-06 07:58:02,164 Epoch 1831: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.77 
2024-02-06 07:58:02,164 EPOCH 1832
2024-02-06 07:58:13,009 Epoch 1832: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.84 
2024-02-06 07:58:13,010 EPOCH 1833
2024-02-06 07:58:23,816 Epoch 1833: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.84 
2024-02-06 07:58:23,817 EPOCH 1834
2024-02-06 07:58:34,378 Epoch 1834: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.80 
2024-02-06 07:58:34,379 EPOCH 1835
2024-02-06 07:58:45,046 Epoch 1835: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.79 
2024-02-06 07:58:45,046 EPOCH 1836
2024-02-06 07:58:46,315 [Epoch: 1836 Step: 00031200] Batch Recognition Loss:   0.000595 => Gls Tokens per Sec:     2525 || Batch Translation Loss:   0.010065 => Txt Tokens per Sec:     6706 || Lr: 0.000100
2024-02-06 07:58:55,875 Epoch 1836: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.78 
2024-02-06 07:58:55,876 EPOCH 1837
2024-02-06 07:59:06,803 Epoch 1837: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-06 07:59:06,804 EPOCH 1838
2024-02-06 07:59:17,618 Epoch 1838: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-06 07:59:17,618 EPOCH 1839
2024-02-06 07:59:28,265 Epoch 1839: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.87 
2024-02-06 07:59:28,265 EPOCH 1840
2024-02-06 07:59:38,794 Epoch 1840: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.14 
2024-02-06 07:59:38,795 EPOCH 1841
2024-02-06 07:59:49,722 Epoch 1841: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-06 07:59:49,722 EPOCH 1842
2024-02-06 07:59:50,446 [Epoch: 1842 Step: 00031300] Batch Recognition Loss:   0.000260 => Gls Tokens per Sec:     2662 || Batch Translation Loss:   0.024658 => Txt Tokens per Sec:     7238 || Lr: 0.000100
2024-02-06 08:00:00,692 Epoch 1842: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-06 08:00:00,693 EPOCH 1843
2024-02-06 08:00:11,413 Epoch 1843: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-06 08:00:11,413 EPOCH 1844
2024-02-06 08:00:22,186 Epoch 1844: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-06 08:00:22,187 EPOCH 1845
2024-02-06 08:00:33,005 Epoch 1845: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-06 08:00:33,005 EPOCH 1846
2024-02-06 08:00:43,961 Epoch 1846: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-06 08:00:43,962 EPOCH 1847
2024-02-06 08:00:54,903 Epoch 1847: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-06 08:00:54,903 EPOCH 1848
2024-02-06 08:00:56,776 [Epoch: 1848 Step: 00031400] Batch Recognition Loss:   0.000349 => Gls Tokens per Sec:      342 || Batch Translation Loss:   0.034488 => Txt Tokens per Sec:     1100 || Lr: 0.000100
2024-02-06 08:01:05,894 Epoch 1848: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-06 08:01:05,894 EPOCH 1849
2024-02-06 08:01:16,556 Epoch 1849: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 08:01:16,557 EPOCH 1850
2024-02-06 08:01:27,269 Epoch 1850: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-06 08:01:27,269 EPOCH 1851
2024-02-06 08:01:38,108 Epoch 1851: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-06 08:01:38,109 EPOCH 1852
2024-02-06 08:01:49,106 Epoch 1852: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-06 08:01:49,106 EPOCH 1853
2024-02-06 08:01:57,711 [Epoch: 1853 Step: 00031500] Batch Recognition Loss:   0.000216 => Gls Tokens per Sec:     1160 || Batch Translation Loss:   0.016454 => Txt Tokens per Sec:     3157 || Lr: 0.000100
2024-02-06 08:01:59,815 Epoch 1853: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-06 08:01:59,816 EPOCH 1854
2024-02-06 08:02:10,628 Epoch 1854: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-06 08:02:10,628 EPOCH 1855
2024-02-06 08:02:21,511 Epoch 1855: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-06 08:02:21,511 EPOCH 1856
2024-02-06 08:02:32,262 Epoch 1856: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-06 08:02:32,263 EPOCH 1857
2024-02-06 08:02:43,033 Epoch 1857: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-06 08:02:43,034 EPOCH 1858
2024-02-06 08:02:53,849 Epoch 1858: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-06 08:02:53,850 EPOCH 1859
2024-02-06 08:03:02,400 [Epoch: 1859 Step: 00031600] Batch Recognition Loss:   0.000352 => Gls Tokens per Sec:     1018 || Batch Translation Loss:   0.019874 => Txt Tokens per Sec:     2787 || Lr: 0.000100
2024-02-06 08:03:04,646 Epoch 1859: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-06 08:03:04,646 EPOCH 1860
2024-02-06 08:03:15,298 Epoch 1860: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-06 08:03:15,299 EPOCH 1861
2024-02-06 08:03:25,984 Epoch 1861: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-06 08:03:25,984 EPOCH 1862
2024-02-06 08:03:36,641 Epoch 1862: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-06 08:03:36,641 EPOCH 1863
2024-02-06 08:03:47,221 Epoch 1863: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-06 08:03:47,221 EPOCH 1864
2024-02-06 08:03:57,846 Epoch 1864: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.51 
2024-02-06 08:03:57,846 EPOCH 1865
2024-02-06 08:04:05,583 [Epoch: 1865 Step: 00031700] Batch Recognition Loss:   0.000820 => Gls Tokens per Sec:      959 || Batch Translation Loss:   0.012665 => Txt Tokens per Sec:     2728 || Lr: 0.000100
2024-02-06 08:04:08,321 Epoch 1865: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.38 
2024-02-06 08:04:08,321 EPOCH 1866
2024-02-06 08:04:19,140 Epoch 1866: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-06 08:04:19,141 EPOCH 1867
2024-02-06 08:04:29,927 Epoch 1867: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-06 08:04:29,928 EPOCH 1868
2024-02-06 08:04:40,499 Epoch 1868: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-06 08:04:40,500 EPOCH 1869
2024-02-06 08:04:51,219 Epoch 1869: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.42 
2024-02-06 08:04:51,220 EPOCH 1870
2024-02-06 08:05:01,914 Epoch 1870: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.34 
2024-02-06 08:05:01,914 EPOCH 1871
2024-02-06 08:05:11,507 [Epoch: 1871 Step: 00031800] Batch Recognition Loss:   0.000263 => Gls Tokens per Sec:      640 || Batch Translation Loss:   0.023406 => Txt Tokens per Sec:     1979 || Lr: 0.000100
2024-02-06 08:05:12,893 Epoch 1871: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-06 08:05:12,894 EPOCH 1872
2024-02-06 08:05:23,659 Epoch 1872: Total Training Recognition Loss 0.20  Total Training Translation Loss 0.31 
2024-02-06 08:05:23,660 EPOCH 1873
2024-02-06 08:05:34,496 Epoch 1873: Total Training Recognition Loss 3.30  Total Training Translation Loss 0.52 
2024-02-06 08:05:34,497 EPOCH 1874
2024-02-06 08:05:45,345 Epoch 1874: Total Training Recognition Loss 1.72  Total Training Translation Loss 0.67 
2024-02-06 08:05:45,346 EPOCH 1875
2024-02-06 08:05:56,234 Epoch 1875: Total Training Recognition Loss 2.88  Total Training Translation Loss 1.28 
2024-02-06 08:05:56,234 EPOCH 1876
2024-02-06 08:06:07,013 Epoch 1876: Total Training Recognition Loss 1.69  Total Training Translation Loss 2.22 
2024-02-06 08:06:07,014 EPOCH 1877
2024-02-06 08:06:13,700 [Epoch: 1877 Step: 00031900] Batch Recognition Loss:   0.220675 => Gls Tokens per Sec:      727 || Batch Translation Loss:   0.199699 => Txt Tokens per Sec:     1914 || Lr: 0.000100
2024-02-06 08:06:17,877 Epoch 1877: Total Training Recognition Loss 1.56  Total Training Translation Loss 2.15 
2024-02-06 08:06:17,877 EPOCH 1878
2024-02-06 08:06:28,759 Epoch 1878: Total Training Recognition Loss 0.44  Total Training Translation Loss 2.21 
2024-02-06 08:06:28,760 EPOCH 1879
2024-02-06 08:06:39,363 Epoch 1879: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.69 
2024-02-06 08:06:39,364 EPOCH 1880
2024-02-06 08:06:50,095 Epoch 1880: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.39 
2024-02-06 08:06:50,096 EPOCH 1881
2024-02-06 08:07:00,714 Epoch 1881: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.29 
2024-02-06 08:07:00,715 EPOCH 1882
2024-02-06 08:07:11,547 Epoch 1882: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.09 
2024-02-06 08:07:11,548 EPOCH 1883
2024-02-06 08:07:13,973 [Epoch: 1883 Step: 00032000] Batch Recognition Loss:   0.000504 => Gls Tokens per Sec:     1584 || Batch Translation Loss:   0.046166 => Txt Tokens per Sec:     4129 || Lr: 0.000100
2024-02-06 08:07:54,718 Validation result at epoch 1883, step    32000: duration: 40.7440s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.49410	Translation Loss: 94886.08594	PPL: 13059.03906
	Eval Metric: BLEU
	WER 3.04	(DEL: 0.00,	INS: 0.00,	SUB: 3.04)
	BLEU-4 0.53	(BLEU-1: 10.48,	BLEU-2: 2.99,	BLEU-3: 1.11,	BLEU-4: 0.53)
	CHRF 17.02	ROUGE 8.67
2024-02-06 08:07:54,720 Logging Recognition and Translation Outputs
2024-02-06 08:07:54,720 ========================================================================================================================
2024-02-06 08:07:54,721 Logging Sequence: 162_133.00
2024-02-06 08:07:54,721 	Gloss Reference :	A B+C+D+E
2024-02-06 08:07:54,721 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 08:07:54,721 	Gloss Alignment :	         
2024-02-06 08:07:54,721 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 08:07:54,723 	Text Reference  :	***** they    also sent rape threats to   his   9-month old  daughter  
2024-02-06 08:07:54,723 	Text Hypothesis :	these attacks said they got  down    well these are     very protective
2024-02-06 08:07:54,723 	Text Alignment  :	I     S       S    S    S    S       S    S     S       S    S         
2024-02-06 08:07:54,723 ========================================================================================================================
2024-02-06 08:07:54,723 Logging Sequence: 134_236.00
2024-02-06 08:07:54,723 	Gloss Reference :	A B+C+D+E
2024-02-06 08:07:54,724 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 08:07:54,724 	Gloss Alignment :	         
2024-02-06 08:07:54,724 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 08:07:54,725 	Text Reference  :	** **** ** ** * after  the interaction modi  tweeted the **** *** images and captioned it   saying
2024-02-06 08:07:54,725 	Text Hypothesis :	pm said it is a matter of  great       pride in      the even his father and uncle     also come  
2024-02-06 08:07:54,725 	Text Alignment  :	I  I    I  I  I S      S   S           S     S           I    I   S          S         S    S     
2024-02-06 08:07:54,726 ========================================================================================================================
2024-02-06 08:07:54,726 Logging Sequence: 145_52.00
2024-02-06 08:07:54,726 	Gloss Reference :	A B+C+D+E
2024-02-06 08:07:54,726 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 08:07:54,726 	Gloss Alignment :	         
2024-02-06 08:07:54,726 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 08:07:54,727 	Text Reference  :	her name was dropped   despite having qualified as    she            was the only female athlete
2024-02-06 08:07:54,727 	Text Hypothesis :	*** **** the wrestlers wanted  to     have      their representative in  the **** ****** panel  
2024-02-06 08:07:54,728 	Text Alignment  :	D   D    S   S         S       S      S         S     S              S       D    D      S      
2024-02-06 08:07:54,728 ========================================================================================================================
2024-02-06 08:07:54,728 Logging Sequence: 175_40.00
2024-02-06 08:07:54,728 	Gloss Reference :	A B+C+D+E
2024-02-06 08:07:54,728 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 08:07:54,728 	Gloss Alignment :	         
2024-02-06 08:07:54,728 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 08:07:54,730 	Text Reference  :	* *** ** ***** ******* **** ******* soumyadeep and shreya bagged three medals each including a            silver medal each
2024-02-06 08:07:54,730 	Text Hypothesis :	a few of these matches were playing well       and ****** losing the   world  deaf badminton championship is     being held
2024-02-06 08:07:54,730 	Text Alignment  :	I I   I  I     I       I    I       S              D      S      S     S      S    S         S            S      S     S   
2024-02-06 08:07:54,730 ========================================================================================================================
2024-02-06 08:07:54,730 Logging Sequence: 156_51.00
2024-02-06 08:07:54,730 	Gloss Reference :	A B+C+D+E
2024-02-06 08:07:54,731 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 08:07:54,731 	Gloss Alignment :	         
2024-02-06 08:07:54,731 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 08:07:54,731 	Text Reference  :	the selection of the players was  similar  to that        of  ipl    
2024-02-06 08:07:54,732 	Text Hypothesis :	*** ********* ** he  has     been selected as commentator and analyst
2024-02-06 08:07:54,732 	Text Alignment  :	D   D         D  S   S       S    S        S  S           S   S      
2024-02-06 08:07:54,732 ========================================================================================================================
2024-02-06 08:08:03,531 Epoch 1883: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.72 
2024-02-06 08:08:03,531 EPOCH 1884
2024-02-06 08:08:14,701 Epoch 1884: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.61 
2024-02-06 08:08:14,702 EPOCH 1885
2024-02-06 08:08:25,147 Epoch 1885: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.70 
2024-02-06 08:08:25,147 EPOCH 1886
2024-02-06 08:08:35,907 Epoch 1886: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.60 
2024-02-06 08:08:35,908 EPOCH 1887
2024-02-06 08:08:46,682 Epoch 1887: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.43 
2024-02-06 08:08:46,682 EPOCH 1888
2024-02-06 08:08:57,422 Epoch 1888: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.57 
2024-02-06 08:08:57,423 EPOCH 1889
2024-02-06 08:09:00,507 [Epoch: 1889 Step: 00032100] Batch Recognition Loss:   0.001760 => Gls Tokens per Sec:      746 || Batch Translation Loss:   0.014672 => Txt Tokens per Sec:     2170 || Lr: 0.000100
2024-02-06 08:09:08,221 Epoch 1889: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.42 
2024-02-06 08:09:08,221 EPOCH 1890
2024-02-06 08:09:18,817 Epoch 1890: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.36 
2024-02-06 08:09:18,817 EPOCH 1891
2024-02-06 08:09:29,216 Epoch 1891: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.53 
2024-02-06 08:09:29,216 EPOCH 1892
2024-02-06 08:09:40,024 Epoch 1892: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.45 
2024-02-06 08:09:40,025 EPOCH 1893
2024-02-06 08:09:50,838 Epoch 1893: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.45 
2024-02-06 08:09:50,839 EPOCH 1894
2024-02-06 08:10:01,390 Epoch 1894: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-06 08:10:01,391 EPOCH 1895
2024-02-06 08:10:02,043 [Epoch: 1895 Step: 00032200] Batch Recognition Loss:   0.002327 => Gls Tokens per Sec:     1965 || Batch Translation Loss:   0.025703 => Txt Tokens per Sec:     5841 || Lr: 0.000100
2024-02-06 08:10:12,224 Epoch 1895: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-06 08:10:12,225 EPOCH 1896
2024-02-06 08:10:23,072 Epoch 1896: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 08:10:23,073 EPOCH 1897
2024-02-06 08:10:33,655 Epoch 1897: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 08:10:33,655 EPOCH 1898
2024-02-06 08:10:44,330 Epoch 1898: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-06 08:10:44,331 EPOCH 1899
2024-02-06 08:10:55,060 Epoch 1899: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 08:10:55,061 EPOCH 1900
2024-02-06 08:11:05,828 [Epoch: 1900 Step: 00032300] Batch Recognition Loss:   0.000480 => Gls Tokens per Sec:      987 || Batch Translation Loss:   0.017553 => Txt Tokens per Sec:     2729 || Lr: 0.000100
2024-02-06 08:11:05,828 Epoch 1900: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 08:11:05,828 EPOCH 1901
2024-02-06 08:11:16,485 Epoch 1901: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 08:11:16,486 EPOCH 1902
2024-02-06 08:11:26,780 Epoch 1902: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 08:11:26,781 EPOCH 1903
2024-02-06 08:11:37,760 Epoch 1903: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-06 08:11:37,761 EPOCH 1904
2024-02-06 08:11:48,635 Epoch 1904: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 08:11:48,636 EPOCH 1905
2024-02-06 08:11:59,225 Epoch 1905: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 08:11:59,226 EPOCH 1906
2024-02-06 08:12:05,890 [Epoch: 1906 Step: 00032400] Batch Recognition Loss:   0.000536 => Gls Tokens per Sec:     1441 || Batch Translation Loss:   0.007361 => Txt Tokens per Sec:     3879 || Lr: 0.000100
2024-02-06 08:12:09,939 Epoch 1906: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 08:12:09,940 EPOCH 1907
2024-02-06 08:12:20,704 Epoch 1907: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-06 08:12:20,705 EPOCH 1908
2024-02-06 08:12:31,601 Epoch 1908: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 08:12:31,601 EPOCH 1909
2024-02-06 08:12:42,336 Epoch 1909: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 08:12:42,336 EPOCH 1910
2024-02-06 08:12:53,157 Epoch 1910: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-06 08:12:53,158 EPOCH 1911
2024-02-06 08:13:03,943 Epoch 1911: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 08:13:03,944 EPOCH 1912
2024-02-06 08:13:12,508 [Epoch: 1912 Step: 00032500] Batch Recognition Loss:   0.000127 => Gls Tokens per Sec:      941 || Batch Translation Loss:   0.012956 => Txt Tokens per Sec:     2597 || Lr: 0.000100
2024-02-06 08:13:14,791 Epoch 1912: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 08:13:14,791 EPOCH 1913
2024-02-06 08:13:25,561 Epoch 1913: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 08:13:25,562 EPOCH 1914
2024-02-06 08:13:36,421 Epoch 1914: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 08:13:36,422 EPOCH 1915
2024-02-06 08:13:46,985 Epoch 1915: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-06 08:13:46,986 EPOCH 1916
2024-02-06 08:13:57,683 Epoch 1916: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-06 08:13:57,684 EPOCH 1917
2024-02-06 08:14:08,542 Epoch 1917: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-06 08:14:08,543 EPOCH 1918
2024-02-06 08:14:15,900 [Epoch: 1918 Step: 00032600] Batch Recognition Loss:   0.000192 => Gls Tokens per Sec:      957 || Batch Translation Loss:   0.012880 => Txt Tokens per Sec:     2799 || Lr: 0.000100
2024-02-06 08:14:19,253 Epoch 1918: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-06 08:14:19,253 EPOCH 1919
2024-02-06 08:14:29,965 Epoch 1919: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-06 08:14:29,966 EPOCH 1920
2024-02-06 08:14:40,671 Epoch 1920: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-06 08:14:40,672 EPOCH 1921
2024-02-06 08:14:51,320 Epoch 1921: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-06 08:14:51,320 EPOCH 1922
2024-02-06 08:15:02,098 Epoch 1922: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-06 08:15:02,098 EPOCH 1923
2024-02-06 08:15:12,834 Epoch 1923: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-06 08:15:12,835 EPOCH 1924
2024-02-06 08:15:18,536 [Epoch: 1924 Step: 00032700] Batch Recognition Loss:   0.000251 => Gls Tokens per Sec:      965 || Batch Translation Loss:   0.026186 => Txt Tokens per Sec:     2800 || Lr: 0.000100
2024-02-06 08:15:23,404 Epoch 1924: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-06 08:15:23,404 EPOCH 1925
2024-02-06 08:15:33,937 Epoch 1925: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-06 08:15:33,938 EPOCH 1926
2024-02-06 08:15:44,395 Epoch 1926: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-06 08:15:44,396 EPOCH 1927
2024-02-06 08:15:55,191 Epoch 1927: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.70 
2024-02-06 08:15:55,191 EPOCH 1928
2024-02-06 08:16:05,993 Epoch 1928: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.80 
2024-02-06 08:16:05,994 EPOCH 1929
2024-02-06 08:16:16,574 Epoch 1929: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-06 08:16:16,574 EPOCH 1930
2024-02-06 08:16:19,653 [Epoch: 1930 Step: 00032800] Batch Recognition Loss:   0.000308 => Gls Tokens per Sec:     1455 || Batch Translation Loss:   0.086610 => Txt Tokens per Sec:     3967 || Lr: 0.000100
2024-02-06 08:16:27,156 Epoch 1930: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.48 
2024-02-06 08:16:27,156 EPOCH 1931
2024-02-06 08:16:37,904 Epoch 1931: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.68 
2024-02-06 08:16:37,905 EPOCH 1932
2024-02-06 08:16:48,572 Epoch 1932: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.78 
2024-02-06 08:16:48,572 EPOCH 1933
2024-02-06 08:16:59,450 Epoch 1933: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.24 
2024-02-06 08:16:59,450 EPOCH 1934
2024-02-06 08:17:10,106 Epoch 1934: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.45 
2024-02-06 08:17:10,106 EPOCH 1935
2024-02-06 08:17:20,924 Epoch 1935: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.79 
2024-02-06 08:17:20,924 EPOCH 1936
2024-02-06 08:17:24,194 [Epoch: 1936 Step: 00032900] Batch Recognition Loss:   0.000950 => Gls Tokens per Sec:      900 || Batch Translation Loss:   0.058956 => Txt Tokens per Sec:     2394 || Lr: 0.000100
2024-02-06 08:17:31,388 Epoch 1936: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.64 
2024-02-06 08:17:31,389 EPOCH 1937
2024-02-06 08:17:42,148 Epoch 1937: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.25 
2024-02-06 08:17:42,149 EPOCH 1938
2024-02-06 08:17:52,954 Epoch 1938: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.88 
2024-02-06 08:17:52,955 EPOCH 1939
2024-02-06 08:18:03,798 Epoch 1939: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.66 
2024-02-06 08:18:03,799 EPOCH 1940
2024-02-06 08:18:14,402 Epoch 1940: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-06 08:18:14,403 EPOCH 1941
2024-02-06 08:18:24,873 Epoch 1941: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-06 08:18:24,874 EPOCH 1942
2024-02-06 08:18:25,635 [Epoch: 1942 Step: 00033000] Batch Recognition Loss:   0.000474 => Gls Tokens per Sec:     2525 || Batch Translation Loss:   0.018089 => Txt Tokens per Sec:     7108 || Lr: 0.000100
2024-02-06 08:18:35,640 Epoch 1942: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-06 08:18:35,641 EPOCH 1943
2024-02-06 08:18:46,457 Epoch 1943: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-06 08:18:46,458 EPOCH 1944
2024-02-06 08:18:57,337 Epoch 1944: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-06 08:18:57,337 EPOCH 1945
2024-02-06 08:19:08,093 Epoch 1945: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 08:19:08,094 EPOCH 1946
2024-02-06 08:19:18,925 Epoch 1946: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 08:19:18,926 EPOCH 1947
2024-02-06 08:19:29,874 Epoch 1947: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 08:19:29,875 EPOCH 1948
2024-02-06 08:19:30,115 [Epoch: 1948 Step: 00033100] Batch Recognition Loss:   0.000747 => Gls Tokens per Sec:     2661 || Batch Translation Loss:   0.013285 => Txt Tokens per Sec:     7920 || Lr: 0.000100
2024-02-06 08:19:40,593 Epoch 1948: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 08:19:40,593 EPOCH 1949
2024-02-06 08:19:51,086 Epoch 1949: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-06 08:19:51,087 EPOCH 1950
2024-02-06 08:20:01,512 Epoch 1950: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-06 08:20:01,513 EPOCH 1951
2024-02-06 08:20:12,231 Epoch 1951: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 08:20:12,231 EPOCH 1952
2024-02-06 08:20:22,882 Epoch 1952: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 08:20:22,883 EPOCH 1953
2024-02-06 08:20:33,241 [Epoch: 1953 Step: 00033200] Batch Recognition Loss:   0.000233 => Gls Tokens per Sec:      963 || Batch Translation Loss:   0.017752 => Txt Tokens per Sec:     2660 || Lr: 0.000100
2024-02-06 08:20:33,498 Epoch 1953: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 08:20:33,498 EPOCH 1954
2024-02-06 08:20:44,001 Epoch 1954: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.25 
2024-02-06 08:20:44,002 EPOCH 1955
2024-02-06 08:20:54,931 Epoch 1955: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 08:20:54,931 EPOCH 1956
2024-02-06 08:21:05,414 Epoch 1956: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 08:21:05,414 EPOCH 1957
2024-02-06 08:21:16,117 Epoch 1957: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-06 08:21:16,118 EPOCH 1958
2024-02-06 08:21:26,867 Epoch 1958: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-06 08:21:26,868 EPOCH 1959
2024-02-06 08:21:35,076 [Epoch: 1959 Step: 00033300] Batch Recognition Loss:   0.000183 => Gls Tokens per Sec:     1060 || Batch Translation Loss:   0.012188 => Txt Tokens per Sec:     2915 || Lr: 0.000100
2024-02-06 08:21:37,473 Epoch 1959: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 08:21:37,474 EPOCH 1960
2024-02-06 08:21:48,050 Epoch 1960: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 08:21:48,050 EPOCH 1961
2024-02-06 08:21:58,761 Epoch 1961: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 08:21:58,762 EPOCH 1962
2024-02-06 08:22:09,705 Epoch 1962: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 08:22:09,706 EPOCH 1963
2024-02-06 08:22:20,679 Epoch 1963: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 08:22:20,679 EPOCH 1964
2024-02-06 08:22:31,542 Epoch 1964: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 08:22:31,543 EPOCH 1965
2024-02-06 08:22:39,715 [Epoch: 1965 Step: 00033400] Batch Recognition Loss:   0.000217 => Gls Tokens per Sec:      908 || Batch Translation Loss:   0.010494 => Txt Tokens per Sec:     2623 || Lr: 0.000100
2024-02-06 08:22:42,179 Epoch 1965: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 08:22:42,180 EPOCH 1966
2024-02-06 08:22:52,985 Epoch 1966: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 08:22:52,986 EPOCH 1967
2024-02-06 08:23:03,726 Epoch 1967: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 08:23:03,727 EPOCH 1968
2024-02-06 08:23:14,471 Epoch 1968: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 08:23:14,472 EPOCH 1969
2024-02-06 08:23:25,165 Epoch 1969: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-06 08:23:25,166 EPOCH 1970
2024-02-06 08:23:35,792 Epoch 1970: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.73 
2024-02-06 08:23:35,793 EPOCH 1971
2024-02-06 08:23:43,442 [Epoch: 1971 Step: 00033500] Batch Recognition Loss:   0.000182 => Gls Tokens per Sec:      803 || Batch Translation Loss:   0.033395 => Txt Tokens per Sec:     2188 || Lr: 0.000100
2024-02-06 08:23:46,655 Epoch 1971: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.76 
2024-02-06 08:23:46,655 EPOCH 1972
2024-02-06 08:23:57,273 Epoch 1972: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.73 
2024-02-06 08:23:57,273 EPOCH 1973
2024-02-06 08:24:07,889 Epoch 1973: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.65 
2024-02-06 08:24:07,889 EPOCH 1974
2024-02-06 08:24:18,562 Epoch 1974: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.89 
2024-02-06 08:24:18,563 EPOCH 1975
2024-02-06 08:24:29,122 Epoch 1975: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.09 
2024-02-06 08:24:29,123 EPOCH 1976
2024-02-06 08:24:39,882 Epoch 1976: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.10 
2024-02-06 08:24:39,883 EPOCH 1977
2024-02-06 08:24:42,923 [Epoch: 1977 Step: 00033600] Batch Recognition Loss:   0.001446 => Gls Tokens per Sec:     1685 || Batch Translation Loss:   0.041042 => Txt Tokens per Sec:     4342 || Lr: 0.000100
2024-02-06 08:24:50,101 Epoch 1977: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-06 08:24:50,102 EPOCH 1978
2024-02-06 08:25:01,065 Epoch 1978: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-06 08:25:01,065 EPOCH 1979
2024-02-06 08:25:11,596 Epoch 1979: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.95 
2024-02-06 08:25:11,596 EPOCH 1980
2024-02-06 08:25:22,177 Epoch 1980: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.29 
2024-02-06 08:25:22,177 EPOCH 1981
2024-02-06 08:25:33,050 Epoch 1981: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.39 
2024-02-06 08:25:33,050 EPOCH 1982
2024-02-06 08:25:43,905 Epoch 1982: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.90 
2024-02-06 08:25:43,905 EPOCH 1983
2024-02-06 08:25:49,974 [Epoch: 1983 Step: 00033700] Batch Recognition Loss:   0.000242 => Gls Tokens per Sec:      633 || Batch Translation Loss:   0.027433 => Txt Tokens per Sec:     1847 || Lr: 0.000100
2024-02-06 08:25:54,677 Epoch 1983: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.92 
2024-02-06 08:25:54,677 EPOCH 1984
2024-02-06 08:26:05,216 Epoch 1984: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.88 
2024-02-06 08:26:05,216 EPOCH 1985
2024-02-06 08:26:15,862 Epoch 1985: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.94 
2024-02-06 08:26:15,862 EPOCH 1986
2024-02-06 08:26:26,644 Epoch 1986: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.84 
2024-02-06 08:26:26,645 EPOCH 1987
2024-02-06 08:26:37,347 Epoch 1987: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.73 
2024-02-06 08:26:37,348 EPOCH 1988
2024-02-06 08:26:48,064 Epoch 1988: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.57 
2024-02-06 08:26:48,065 EPOCH 1989
2024-02-06 08:26:50,641 [Epoch: 1989 Step: 00033800] Batch Recognition Loss:   0.000426 => Gls Tokens per Sec:      995 || Batch Translation Loss:   0.027633 => Txt Tokens per Sec:     2743 || Lr: 0.000100
2024-02-06 08:26:58,965 Epoch 1989: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-06 08:26:58,966 EPOCH 1990
2024-02-06 08:27:09,622 Epoch 1990: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.40 
2024-02-06 08:27:09,622 EPOCH 1991
2024-02-06 08:27:20,223 Epoch 1991: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-06 08:27:20,224 EPOCH 1992
2024-02-06 08:27:31,099 Epoch 1992: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-06 08:27:31,099 EPOCH 1993
2024-02-06 08:27:41,669 Epoch 1993: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-06 08:27:41,669 EPOCH 1994
2024-02-06 08:27:52,460 Epoch 1994: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-06 08:27:52,461 EPOCH 1995
2024-02-06 08:27:53,063 [Epoch: 1995 Step: 00033900] Batch Recognition Loss:   0.000133 => Gls Tokens per Sec:     2130 || Batch Translation Loss:   0.013743 => Txt Tokens per Sec:     6245 || Lr: 0.000100
2024-02-06 08:28:03,493 Epoch 1995: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 08:28:03,494 EPOCH 1996
2024-02-06 08:28:14,432 Epoch 1996: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 08:28:14,432 EPOCH 1997
2024-02-06 08:28:25,083 Epoch 1997: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 08:28:25,083 EPOCH 1998
2024-02-06 08:28:35,732 Epoch 1998: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-06 08:28:35,733 EPOCH 1999
2024-02-06 08:28:46,601 Epoch 1999: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 08:28:46,602 EPOCH 2000
2024-02-06 08:28:57,374 [Epoch: 2000 Step: 00034000] Batch Recognition Loss:   0.000272 => Gls Tokens per Sec:      986 || Batch Translation Loss:   0.009872 => Txt Tokens per Sec:     2728 || Lr: 0.000100
2024-02-06 08:29:37,685 Validation result at epoch 2000, step    34000: duration: 40.3081s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.58007	Translation Loss: 95321.49219	PPL: 13639.48633
	Eval Metric: BLEU
	WER 2.90	(DEL: 0.00,	INS: 0.00,	SUB: 2.90)
	BLEU-4 0.52	(BLEU-1: 10.65,	BLEU-2: 3.31,	BLEU-3: 1.19,	BLEU-4: 0.52)
	CHRF 17.11	ROUGE 9.03
2024-02-06 08:29:37,687 Logging Recognition and Translation Outputs
2024-02-06 08:29:37,687 ========================================================================================================================
2024-02-06 08:29:37,687 Logging Sequence: 171_158.00
2024-02-06 08:29:37,688 	Gloss Reference :	A B+C+D+E
2024-02-06 08:29:37,688 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 08:29:37,688 	Gloss Alignment :	         
2024-02-06 08:29:37,688 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 08:29:37,690 	Text Reference  :	with speculations of      dhoni being banned are spreading many say that it is  unlikely to  happen  
2024-02-06 08:29:37,691 	Text Hypothesis :	**** ************ however the   match ended  in  a         tie  the end  of the finals   has happened
2024-02-06 08:29:37,691 	Text Alignment  :	D    D            S       S     S     S      S   S         S    S   S    S  S   S        S   S       
2024-02-06 08:29:37,691 ========================================================================================================================
2024-02-06 08:29:37,691 Logging Sequence: 108_235.00
2024-02-06 08:29:37,691 	Gloss Reference :	A B+C+D+E
2024-02-06 08:29:37,691 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 08:29:37,692 	Gloss Alignment :	         
2024-02-06 08:29:37,692 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 08:29:37,693 	Text Reference  :	he     was  taken to          the hospital and it        was reported that he    is not  in  any    danger
2024-02-06 08:29:37,693 	Text Hypothesis :	people were seen  celebrating the ******** *** cricketer was ******** **** never do with his sudden crore 
2024-02-06 08:29:37,693 	Text Alignment  :	S      S    S     S               D        D   S             D        D    S     S  S    S   S      S     
2024-02-06 08:29:37,694 ========================================================================================================================
2024-02-06 08:29:37,694 Logging Sequence: 153_206.00
2024-02-06 08:29:37,694 	Gloss Reference :	A B+C+D+E
2024-02-06 08:29:37,694 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 08:29:37,694 	Gloss Alignment :	         
2024-02-06 08:29:37,694 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 08:29:37,695 	Text Reference  :	*** now  on    13th november everyone is    hoping pakistan **** *** *** **** **** ***** rewrites history
2024-02-06 08:29:37,696 	Text Hypothesis :	the 2022 final was  played   between  india and    pakistan have won the same time after 7        goals  
2024-02-06 08:29:37,696 	Text Alignment  :	I   S    S     S    S        S        S     S               I    I   I   I    I    I     S        S      
2024-02-06 08:29:37,696 ========================================================================================================================
2024-02-06 08:29:37,696 Logging Sequence: 87_202.00
2024-02-06 08:29:37,696 	Gloss Reference :	A B+C+D+E
2024-02-06 08:29:37,696 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 08:29:37,697 	Gloss Alignment :	         
2024-02-06 08:29:37,697 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 08:29:37,698 	Text Reference  :	*** **** ******** ** *** ** *** ** *** i  love      our  players and         i  love my      country
2024-02-06 08:29:37,698 	Text Hypothesis :	the vast majority of the of the of the 56 countries were former  territories of the  british empire 
2024-02-06 08:29:37,698 	Text Alignment  :	I   I    I        I  I   I  I   I  I   S  S         S    S       S           S  S    S       S      
2024-02-06 08:29:37,698 ========================================================================================================================
2024-02-06 08:29:37,698 Logging Sequence: 84_2.00
2024-02-06 08:29:37,699 	Gloss Reference :	A B+C+D+E
2024-02-06 08:29:37,699 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 08:29:37,699 	Gloss Alignment :	         
2024-02-06 08:29:37,699 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 08:29:37,701 	Text Reference  :	the 2022 fifa football world cup is      going on in qatar   from 20th   november 2022 to 18th december 2022
2024-02-06 08:29:37,701 	Text Hypothesis :	*** **** **** so       what  a   cricket teams on ** winning the  sports be       held at the  world    cup 
2024-02-06 08:29:37,701 	Text Alignment  :	D   D    D    S        S     S   S       S        D  S       S    S      S        S    S  S    S        S   
2024-02-06 08:29:37,701 ========================================================================================================================
2024-02-06 08:29:37,705 Epoch 2000: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 08:29:37,705 EPOCH 2001
2024-02-06 08:29:49,282 Epoch 2001: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 08:29:49,282 EPOCH 2002
2024-02-06 08:29:59,927 Epoch 2002: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-06 08:29:59,928 EPOCH 2003
2024-02-06 08:30:10,707 Epoch 2003: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-06 08:30:10,708 EPOCH 2004
2024-02-06 08:30:21,698 Epoch 2004: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-06 08:30:21,698 EPOCH 2005
2024-02-06 08:30:32,698 Epoch 2005: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-06 08:30:32,699 EPOCH 2006
2024-02-06 08:30:41,805 [Epoch: 2006 Step: 00034100] Batch Recognition Loss:   0.000186 => Gls Tokens per Sec:     1026 || Batch Translation Loss:   0.014217 => Txt Tokens per Sec:     2881 || Lr: 0.000100
2024-02-06 08:30:43,561 Epoch 2006: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-06 08:30:43,562 EPOCH 2007
2024-02-06 08:30:54,385 Epoch 2007: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-06 08:30:54,385 EPOCH 2008
2024-02-06 08:31:05,167 Epoch 2008: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-06 08:31:05,168 EPOCH 2009
2024-02-06 08:31:16,157 Epoch 2009: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-06 08:31:16,158 EPOCH 2010
2024-02-06 08:31:26,708 Epoch 2010: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.80 
2024-02-06 08:31:26,709 EPOCH 2011
2024-02-06 08:31:37,452 Epoch 2011: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.72 
2024-02-06 08:31:37,453 EPOCH 2012
2024-02-06 08:31:45,247 [Epoch: 2012 Step: 00034200] Batch Recognition Loss:   0.000333 => Gls Tokens per Sec:     1034 || Batch Translation Loss:   0.050140 => Txt Tokens per Sec:     2856 || Lr: 0.000100
2024-02-06 08:31:47,937 Epoch 2012: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.87 
2024-02-06 08:31:47,938 EPOCH 2013
2024-02-06 08:31:58,582 Epoch 2013: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.00 
2024-02-06 08:31:58,583 EPOCH 2014
2024-02-06 08:32:09,500 Epoch 2014: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.03 
2024-02-06 08:32:09,500 EPOCH 2015
2024-02-06 08:32:20,279 Epoch 2015: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.99 
2024-02-06 08:32:20,280 EPOCH 2016
2024-02-06 08:32:30,933 Epoch 2016: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-06 08:32:30,933 EPOCH 2017
2024-02-06 08:32:41,442 Epoch 2017: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.83 
2024-02-06 08:32:41,443 EPOCH 2018
2024-02-06 08:32:48,452 [Epoch: 2018 Step: 00034300] Batch Recognition Loss:   0.000404 => Gls Tokens per Sec:     1005 || Batch Translation Loss:   0.043944 => Txt Tokens per Sec:     2672 || Lr: 0.000100
2024-02-06 08:32:52,260 Epoch 2018: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.90 
2024-02-06 08:32:52,260 EPOCH 2019
2024-02-06 08:33:03,224 Epoch 2019: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.81 
2024-02-06 08:33:03,225 EPOCH 2020
2024-02-06 08:33:14,021 Epoch 2020: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.82 
2024-02-06 08:33:14,022 EPOCH 2021
2024-02-06 08:33:24,674 Epoch 2021: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.75 
2024-02-06 08:33:24,674 EPOCH 2022
2024-02-06 08:33:35,689 Epoch 2022: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-06 08:33:35,690 EPOCH 2023
2024-02-06 08:33:46,412 Epoch 2023: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-06 08:33:46,412 EPOCH 2024
2024-02-06 08:33:52,145 [Epoch: 2024 Step: 00034400] Batch Recognition Loss:   0.000363 => Gls Tokens per Sec:      960 || Batch Translation Loss:   0.039221 => Txt Tokens per Sec:     2746 || Lr: 0.000100
2024-02-06 08:33:56,938 Epoch 2024: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-06 08:33:56,938 EPOCH 2025
2024-02-06 08:34:07,613 Epoch 2025: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-06 08:34:07,614 EPOCH 2026
2024-02-06 08:34:18,402 Epoch 2026: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-06 08:34:18,403 EPOCH 2027
2024-02-06 08:34:29,091 Epoch 2027: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-06 08:34:29,092 EPOCH 2028
2024-02-06 08:34:39,664 Epoch 2028: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.50 
2024-02-06 08:34:39,665 EPOCH 2029
2024-02-06 08:34:50,367 Epoch 2029: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-06 08:34:50,369 EPOCH 2030
2024-02-06 08:34:54,948 [Epoch: 2030 Step: 00034500] Batch Recognition Loss:   0.000678 => Gls Tokens per Sec:      979 || Batch Translation Loss:   0.048073 => Txt Tokens per Sec:     2533 || Lr: 0.000100
2024-02-06 08:35:01,389 Epoch 2030: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-06 08:35:01,390 EPOCH 2031
2024-02-06 08:35:11,918 Epoch 2031: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.98 
2024-02-06 08:35:11,919 EPOCH 2032
2024-02-06 08:35:22,801 Epoch 2032: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.01 
2024-02-06 08:35:22,802 EPOCH 2033
2024-02-06 08:35:33,617 Epoch 2033: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.32 
2024-02-06 08:35:33,617 EPOCH 2034
2024-02-06 08:35:44,419 Epoch 2034: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.82 
2024-02-06 08:35:44,419 EPOCH 2035
2024-02-06 08:35:55,116 Epoch 2035: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.65 
2024-02-06 08:35:55,116 EPOCH 2036
2024-02-06 08:35:57,470 [Epoch: 2036 Step: 00034600] Batch Recognition Loss:   0.001459 => Gls Tokens per Sec:     1360 || Batch Translation Loss:   0.084228 => Txt Tokens per Sec:     3720 || Lr: 0.000100
2024-02-06 08:36:05,761 Epoch 2036: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.49 
2024-02-06 08:36:05,762 EPOCH 2037
2024-02-06 08:36:16,491 Epoch 2037: Total Training Recognition Loss 0.05  Total Training Translation Loss 7.95 
2024-02-06 08:36:16,491 EPOCH 2038
2024-02-06 08:36:26,940 Epoch 2038: Total Training Recognition Loss 0.11  Total Training Translation Loss 5.59 
2024-02-06 08:36:26,941 EPOCH 2039
2024-02-06 08:36:37,764 Epoch 2039: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.40 
2024-02-06 08:36:37,764 EPOCH 2040
2024-02-06 08:36:48,521 Epoch 2040: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.77 
2024-02-06 08:36:48,522 EPOCH 2041
2024-02-06 08:36:59,182 Epoch 2041: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.53 
2024-02-06 08:36:59,182 EPOCH 2042
2024-02-06 08:37:01,695 [Epoch: 2042 Step: 00034700] Batch Recognition Loss:   0.000760 => Gls Tokens per Sec:      764 || Batch Translation Loss:   0.069521 => Txt Tokens per Sec:     2347 || Lr: 0.000100
2024-02-06 08:37:09,780 Epoch 2042: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-06 08:37:09,781 EPOCH 2043
2024-02-06 08:37:20,537 Epoch 2043: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.42 
2024-02-06 08:37:20,537 EPOCH 2044
2024-02-06 08:37:31,249 Epoch 2044: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.35 
2024-02-06 08:37:31,249 EPOCH 2045
2024-02-06 08:37:41,973 Epoch 2045: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 08:37:41,973 EPOCH 2046
2024-02-06 08:37:52,801 Epoch 2046: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 08:37:52,802 EPOCH 2047
2024-02-06 08:38:03,612 Epoch 2047: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 08:38:03,613 EPOCH 2048
2024-02-06 08:38:03,822 [Epoch: 2048 Step: 00034800] Batch Recognition Loss:   0.000236 => Gls Tokens per Sec:     3077 || Batch Translation Loss:   0.012835 => Txt Tokens per Sec:     7625 || Lr: 0.000100
2024-02-06 08:38:14,475 Epoch 2048: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 08:38:14,475 EPOCH 2049
2024-02-06 08:38:25,427 Epoch 2049: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.28 
2024-02-06 08:38:25,428 EPOCH 2050
2024-02-06 08:38:36,438 Epoch 2050: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 08:38:36,439 EPOCH 2051
2024-02-06 08:38:47,267 Epoch 2051: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 08:38:47,267 EPOCH 2052
2024-02-06 08:38:57,858 Epoch 2052: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 08:38:57,858 EPOCH 2053
2024-02-06 08:39:08,498 [Epoch: 2053 Step: 00034900] Batch Recognition Loss:   0.000580 => Gls Tokens per Sec:      938 || Batch Translation Loss:   0.025077 => Txt Tokens per Sec:     2640 || Lr: 0.000100
2024-02-06 08:39:08,676 Epoch 2053: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 08:39:08,677 EPOCH 2054
2024-02-06 08:39:19,515 Epoch 2054: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 08:39:19,516 EPOCH 2055
2024-02-06 08:39:30,487 Epoch 2055: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 08:39:30,487 EPOCH 2056
2024-02-06 08:39:41,253 Epoch 2056: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-06 08:39:41,253 EPOCH 2057
2024-02-06 08:39:52,157 Epoch 2057: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 08:39:52,157 EPOCH 2058
2024-02-06 08:40:02,798 Epoch 2058: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 08:40:02,798 EPOCH 2059
2024-02-06 08:40:12,723 [Epoch: 2059 Step: 00035000] Batch Recognition Loss:   0.000220 => Gls Tokens per Sec:      877 || Batch Translation Loss:   0.010445 => Txt Tokens per Sec:     2381 || Lr: 0.000100
2024-02-06 08:40:13,681 Epoch 2059: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-06 08:40:13,682 EPOCH 2060
2024-02-06 08:40:24,690 Epoch 2060: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-06 08:40:24,690 EPOCH 2061
2024-02-06 08:40:35,367 Epoch 2061: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-06 08:40:35,368 EPOCH 2062
2024-02-06 08:40:46,226 Epoch 2062: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-06 08:40:46,227 EPOCH 2063
2024-02-06 08:40:56,967 Epoch 2063: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-06 08:40:56,967 EPOCH 2064
2024-02-06 08:41:07,641 Epoch 2064: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-06 08:41:07,641 EPOCH 2065
2024-02-06 08:41:15,699 [Epoch: 2065 Step: 00035100] Batch Recognition Loss:   0.000160 => Gls Tokens per Sec:      921 || Batch Translation Loss:   0.012790 => Txt Tokens per Sec:     2502 || Lr: 0.000100
2024-02-06 08:41:18,353 Epoch 2065: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-06 08:41:18,354 EPOCH 2066
2024-02-06 08:41:29,113 Epoch 2066: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-06 08:41:29,113 EPOCH 2067
2024-02-06 08:41:39,882 Epoch 2067: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-06 08:41:39,882 EPOCH 2068
2024-02-06 08:41:50,526 Epoch 2068: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-06 08:41:50,526 EPOCH 2069
2024-02-06 08:42:01,209 Epoch 2069: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-06 08:42:01,210 EPOCH 2070
2024-02-06 08:42:12,050 Epoch 2070: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-06 08:42:12,050 EPOCH 2071
2024-02-06 08:42:21,138 [Epoch: 2071 Step: 00035200] Batch Recognition Loss:   0.000273 => Gls Tokens per Sec:      676 || Batch Translation Loss:   0.012837 => Txt Tokens per Sec:     1860 || Lr: 0.000100
2024-02-06 08:42:22,922 Epoch 2071: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-06 08:42:22,922 EPOCH 2072
2024-02-06 08:42:33,602 Epoch 2072: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-06 08:42:33,603 EPOCH 2073
2024-02-06 08:42:44,172 Epoch 2073: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-06 08:42:44,172 EPOCH 2074
2024-02-06 08:42:55,149 Epoch 2074: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-06 08:42:55,150 EPOCH 2075
2024-02-06 08:43:05,696 Epoch 2075: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-06 08:43:05,696 EPOCH 2076
2024-02-06 08:43:16,441 Epoch 2076: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-06 08:43:16,442 EPOCH 2077
2024-02-06 08:43:23,378 [Epoch: 2077 Step: 00035300] Batch Recognition Loss:   0.000140 => Gls Tokens per Sec:      701 || Batch Translation Loss:   0.012854 => Txt Tokens per Sec:     2090 || Lr: 0.000100
2024-02-06 08:43:27,003 Epoch 2077: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-06 08:43:27,004 EPOCH 2078
2024-02-06 08:43:37,362 Epoch 2078: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 08:43:37,363 EPOCH 2079
2024-02-06 08:43:47,966 Epoch 2079: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-06 08:43:47,967 EPOCH 2080
2024-02-06 08:43:58,486 Epoch 2080: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 08:43:58,486 EPOCH 2081
2024-02-06 08:44:09,280 Epoch 2081: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-06 08:44:09,281 EPOCH 2082
2024-02-06 08:44:20,098 Epoch 2082: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 08:44:20,099 EPOCH 2083
2024-02-06 08:44:25,372 [Epoch: 2083 Step: 00035400] Batch Recognition Loss:   0.000361 => Gls Tokens per Sec:      679 || Batch Translation Loss:   0.042749 => Txt Tokens per Sec:     1845 || Lr: 0.000100
2024-02-06 08:44:31,031 Epoch 2083: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-06 08:44:31,032 EPOCH 2084
2024-02-06 08:44:41,757 Epoch 2084: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-06 08:44:41,758 EPOCH 2085
2024-02-06 08:44:52,618 Epoch 2085: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-06 08:44:52,619 EPOCH 2086
2024-02-06 08:45:03,409 Epoch 2086: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-06 08:45:03,409 EPOCH 2087
2024-02-06 08:45:14,044 Epoch 2087: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-06 08:45:14,045 EPOCH 2088
2024-02-06 08:45:24,770 Epoch 2088: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-06 08:45:24,770 EPOCH 2089
2024-02-06 08:45:27,992 [Epoch: 2089 Step: 00035500] Batch Recognition Loss:   0.000239 => Gls Tokens per Sec:      714 || Batch Translation Loss:   0.034042 => Txt Tokens per Sec:     2124 || Lr: 0.000100
2024-02-06 08:45:35,718 Epoch 2089: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.95 
2024-02-06 08:45:35,718 EPOCH 2090
2024-02-06 08:45:46,431 Epoch 2090: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.68 
2024-02-06 08:45:46,432 EPOCH 2091
2024-02-06 08:45:57,032 Epoch 2091: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.00 
2024-02-06 08:45:57,033 EPOCH 2092
2024-02-06 08:46:07,506 Epoch 2092: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.56 
2024-02-06 08:46:07,506 EPOCH 2093
2024-02-06 08:46:18,247 Epoch 2093: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.26 
2024-02-06 08:46:18,248 EPOCH 2094
2024-02-06 08:46:28,877 Epoch 2094: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.06 
2024-02-06 08:46:28,877 EPOCH 2095
2024-02-06 08:46:30,725 [Epoch: 2095 Step: 00035600] Batch Recognition Loss:   0.000486 => Gls Tokens per Sec:      693 || Batch Translation Loss:   0.085396 => Txt Tokens per Sec:     1890 || Lr: 0.000100
2024-02-06 08:46:39,682 Epoch 2095: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.89 
2024-02-06 08:46:39,682 EPOCH 2096
2024-02-06 08:46:50,443 Epoch 2096: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.12 
2024-02-06 08:46:50,444 EPOCH 2097
2024-02-06 08:47:01,133 Epoch 2097: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.09 
2024-02-06 08:47:01,134 EPOCH 2098
2024-02-06 08:47:11,943 Epoch 2098: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.08 
2024-02-06 08:47:11,944 EPOCH 2099
2024-02-06 08:47:22,847 Epoch 2099: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.99 
2024-02-06 08:47:22,848 EPOCH 2100
2024-02-06 08:47:33,579 [Epoch: 2100 Step: 00035700] Batch Recognition Loss:   0.000523 => Gls Tokens per Sec:      990 || Batch Translation Loss:   0.040400 => Txt Tokens per Sec:     2738 || Lr: 0.000100
2024-02-06 08:47:33,580 Epoch 2100: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.83 
2024-02-06 08:47:33,580 EPOCH 2101
2024-02-06 08:47:44,290 Epoch 2101: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.09 
2024-02-06 08:47:44,291 EPOCH 2102
2024-02-06 08:47:55,353 Epoch 2102: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.03 
2024-02-06 08:47:55,354 EPOCH 2103
2024-02-06 08:48:06,233 Epoch 2103: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.78 
2024-02-06 08:48:06,233 EPOCH 2104
2024-02-06 08:48:16,960 Epoch 2104: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.79 
2024-02-06 08:48:16,961 EPOCH 2105
2024-02-06 08:48:27,836 Epoch 2105: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.44 
2024-02-06 08:48:27,836 EPOCH 2106
2024-02-06 08:48:35,761 [Epoch: 2106 Step: 00035800] Batch Recognition Loss:   0.003231 => Gls Tokens per Sec:     1211 || Batch Translation Loss:   0.020160 => Txt Tokens per Sec:     3411 || Lr: 0.000100
2024-02-06 08:48:38,327 Epoch 2106: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-06 08:48:38,328 EPOCH 2107
2024-02-06 08:48:49,136 Epoch 2107: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 08:48:49,137 EPOCH 2108
2024-02-06 08:48:59,820 Epoch 2108: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 08:48:59,820 EPOCH 2109
2024-02-06 08:49:10,754 Epoch 2109: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 08:49:10,754 EPOCH 2110
2024-02-06 08:49:21,674 Epoch 2110: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 08:49:21,674 EPOCH 2111
2024-02-06 08:49:32,246 Epoch 2111: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 08:49:32,246 EPOCH 2112
2024-02-06 08:49:41,475 [Epoch: 2112 Step: 00035900] Batch Recognition Loss:   0.000193 => Gls Tokens per Sec:      873 || Batch Translation Loss:   0.019188 => Txt Tokens per Sec:     2516 || Lr: 0.000100
2024-02-06 08:49:42,225 Epoch 2112: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-06 08:49:42,225 EPOCH 2113
2024-02-06 08:49:52,956 Epoch 2113: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-06 08:49:52,956 EPOCH 2114
2024-02-06 08:50:03,804 Epoch 2114: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-06 08:50:03,805 EPOCH 2115
2024-02-06 08:50:14,238 Epoch 2115: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-06 08:50:14,238 EPOCH 2116
2024-02-06 08:50:24,673 Epoch 2116: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-06 08:50:24,674 EPOCH 2117
2024-02-06 08:50:35,277 Epoch 2117: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-06 08:50:35,278 EPOCH 2118
2024-02-06 08:50:43,131 [Epoch: 2118 Step: 00036000] Batch Recognition Loss:   0.000231 => Gls Tokens per Sec:      863 || Batch Translation Loss:   0.047371 => Txt Tokens per Sec:     2442 || Lr: 0.000100
2024-02-06 08:51:23,589 Validation result at epoch 2118, step    36000: duration: 40.4560s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.66059	Translation Loss: 98032.39844	PPL: 17880.92188
	Eval Metric: BLEU
	WER 2.82	(DEL: 0.00,	INS: 0.00,	SUB: 2.82)
	BLEU-4 0.50	(BLEU-1: 10.25,	BLEU-2: 3.04,	BLEU-3: 1.12,	BLEU-4: 0.50)
	CHRF 16.81	ROUGE 8.91
2024-02-06 08:51:23,591 Logging Recognition and Translation Outputs
2024-02-06 08:51:23,591 ========================================================================================================================
2024-02-06 08:51:23,591 Logging Sequence: 153_36.00
2024-02-06 08:51:23,591 	Gloss Reference :	A B+C+D+E
2024-02-06 08:51:23,591 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 08:51:23,592 	Gloss Alignment :	         
2024-02-06 08:51:23,592 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 08:51:23,593 	Text Reference  :	india **** *** made  a    good  score of 1686    in 20 overs
2024-02-06 08:51:23,594 	Text Hypothesis :	india lost the match when india lost  8  wickets oh my god  
2024-02-06 08:51:23,594 	Text Alignment  :	      I    I   S     S    S     S     S  S       S  S  S    
2024-02-06 08:51:23,594 ========================================================================================================================
2024-02-06 08:51:23,594 Logging Sequence: 163_30.00
2024-02-06 08:51:23,594 	Gloss Reference :	A B+C+D+E
2024-02-06 08:51:23,594 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 08:51:23,594 	Gloss Alignment :	         
2024-02-06 08:51:23,595 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 08:51:23,595 	Text Reference  :	they ****** ** ***** ***** never  permitted anyone to reveal her face
2024-02-06 08:51:23,596 	Text Hypothesis :	they wanted to limit their travel and       i      am so     far etc 
2024-02-06 08:51:23,596 	Text Alignment  :	     I      I  I     I     S      S         S      S  S      S   S   
2024-02-06 08:51:23,596 ========================================================================================================================
2024-02-06 08:51:23,596 Logging Sequence: 167_60.00
2024-02-06 08:51:23,596 	Gloss Reference :	A B+C+D+E
2024-02-06 08:51:23,597 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 08:51:23,597 	Gloss Alignment :	         
2024-02-06 08:51:23,597 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 08:51:23,598 	Text Reference  :	camel flu spreads rapidly when    one   comes in  close contact with  the     infected
2024-02-06 08:51:23,598 	Text Hypothesis :	there was a       stadium between virat kohli and watch the     match between him     
2024-02-06 08:51:23,598 	Text Alignment  :	S     S   S       S       S       S     S     S   S     S       S     S       S       
2024-02-06 08:51:23,598 ========================================================================================================================
2024-02-06 08:51:23,599 Logging Sequence: 84_35.00
2024-02-06 08:51:23,599 	Gloss Reference :	A B+C+D+E
2024-02-06 08:51:23,599 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 08:51:23,599 	Gloss Alignment :	         
2024-02-06 08:51:23,599 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 08:51:23,600 	Text Reference  :	******* here is the reason why they covered their mouth   
2024-02-06 08:51:23,600 	Text Hypothesis :	however he   is *** ****** *** what a       huge  argument
2024-02-06 08:51:23,600 	Text Alignment  :	I       S       D   D      D   S    S       S     S       
2024-02-06 08:51:23,600 ========================================================================================================================
2024-02-06 08:51:23,600 Logging Sequence: 96_2.00
2024-02-06 08:51:23,600 	Gloss Reference :	A B+C+D+E        
2024-02-06 08:51:23,601 	Gloss Hypothesis:	A B+C+D+E+D+E+D+E
2024-02-06 08:51:23,601 	Gloss Alignment :	  S              
2024-02-06 08:51:23,601 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 08:51:23,603 	Text Reference  :	the world is preparing for the      t20     world cup scheduled to start from 16th  october this year
2024-02-06 08:51:23,603 	Text Hypothesis :	the ***** ** ********* icc under-19 cricket world cup ********* ** ***** was  first played  in   1988
2024-02-06 08:51:23,603 	Text Alignment  :	    D     D  D         S   S        S                 D         D  D     S    S     S       S    S   
2024-02-06 08:51:23,603 ========================================================================================================================
2024-02-06 08:51:26,896 Epoch 2118: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-06 08:51:26,896 EPOCH 2119
2024-02-06 08:51:37,755 Epoch 2119: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-06 08:51:37,755 EPOCH 2120
2024-02-06 08:51:48,634 Epoch 2120: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-06 08:51:48,635 EPOCH 2121
2024-02-06 08:51:59,298 Epoch 2121: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-06 08:51:59,299 EPOCH 2122
2024-02-06 08:52:09,983 Epoch 2122: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 08:52:09,983 EPOCH 2123
2024-02-06 08:52:20,445 Epoch 2123: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 08:52:20,446 EPOCH 2124
2024-02-06 08:52:27,722 [Epoch: 2124 Step: 00036100] Batch Recognition Loss:   0.000179 => Gls Tokens per Sec:      756 || Batch Translation Loss:   0.014127 => Txt Tokens per Sec:     2068 || Lr: 0.000050
2024-02-06 08:52:31,023 Epoch 2124: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-06 08:52:31,023 EPOCH 2125
2024-02-06 08:52:41,749 Epoch 2125: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-06 08:52:41,750 EPOCH 2126
2024-02-06 08:52:52,267 Epoch 2126: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 08:52:52,268 EPOCH 2127
2024-02-06 08:53:03,163 Epoch 2127: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 08:53:03,164 EPOCH 2128
2024-02-06 08:53:13,931 Epoch 2128: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-06 08:53:13,931 EPOCH 2129
2024-02-06 08:53:24,575 Epoch 2129: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 08:53:24,575 EPOCH 2130
2024-02-06 08:53:29,925 [Epoch: 2130 Step: 00036200] Batch Recognition Loss:   0.000370 => Gls Tokens per Sec:      789 || Batch Translation Loss:   0.011725 => Txt Tokens per Sec:     2217 || Lr: 0.000050
2024-02-06 08:53:35,550 Epoch 2130: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-06 08:53:35,551 EPOCH 2131
2024-02-06 08:53:46,228 Epoch 2131: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-06 08:53:46,228 EPOCH 2132
2024-02-06 08:53:56,874 Epoch 2132: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-06 08:53:56,875 EPOCH 2133
2024-02-06 08:54:07,469 Epoch 2133: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-06 08:54:07,470 EPOCH 2134
2024-02-06 08:54:18,080 Epoch 2134: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-06 08:54:18,080 EPOCH 2135
2024-02-06 08:54:28,869 Epoch 2135: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-06 08:54:28,870 EPOCH 2136
2024-02-06 08:54:30,362 [Epoch: 2136 Step: 00036300] Batch Recognition Loss:   0.000186 => Gls Tokens per Sec:     2145 || Batch Translation Loss:   0.026001 => Txt Tokens per Sec:     6354 || Lr: 0.000050
2024-02-06 08:54:39,599 Epoch 2136: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-06 08:54:39,599 EPOCH 2137
2024-02-06 08:54:50,427 Epoch 2137: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 08:54:50,428 EPOCH 2138
2024-02-06 08:55:01,158 Epoch 2138: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-06 08:55:01,158 EPOCH 2139
2024-02-06 08:55:12,087 Epoch 2139: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-06 08:55:12,088 EPOCH 2140
2024-02-06 08:55:22,849 Epoch 2140: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-06 08:55:22,850 EPOCH 2141
2024-02-06 08:55:33,452 Epoch 2141: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-06 08:55:33,453 EPOCH 2142
2024-02-06 08:55:35,994 [Epoch: 2142 Step: 00036400] Batch Recognition Loss:   0.000151 => Gls Tokens per Sec:      756 || Batch Translation Loss:   0.011468 => Txt Tokens per Sec:     2408 || Lr: 0.000050
2024-02-06 08:55:44,218 Epoch 2142: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-06 08:55:44,219 EPOCH 2143
2024-02-06 08:55:54,882 Epoch 2143: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-06 08:55:54,882 EPOCH 2144
2024-02-06 08:56:05,413 Epoch 2144: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-06 08:56:05,414 EPOCH 2145
2024-02-06 08:56:16,263 Epoch 2145: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-06 08:56:16,264 EPOCH 2146
2024-02-06 08:56:26,768 Epoch 2146: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-06 08:56:26,768 EPOCH 2147
2024-02-06 08:56:37,449 Epoch 2147: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-06 08:56:37,450 EPOCH 2148
2024-02-06 08:56:37,615 [Epoch: 2148 Step: 00036500] Batch Recognition Loss:   0.000116 => Gls Tokens per Sec:     3902 || Batch Translation Loss:   0.008523 => Txt Tokens per Sec:    10165 || Lr: 0.000050
2024-02-06 08:56:47,932 Epoch 2148: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-06 08:56:47,932 EPOCH 2149
2024-02-06 08:56:58,517 Epoch 2149: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-06 08:56:58,518 EPOCH 2150
2024-02-06 08:57:09,393 Epoch 2150: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-06 08:57:09,393 EPOCH 2151
2024-02-06 08:57:19,987 Epoch 2151: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-06 08:57:19,987 EPOCH 2152
2024-02-06 08:57:30,680 Epoch 2152: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-06 08:57:30,680 EPOCH 2153
2024-02-06 08:57:39,164 [Epoch: 2153 Step: 00036600] Batch Recognition Loss:   0.000106 => Gls Tokens per Sec:     1207 || Batch Translation Loss:   0.008040 => Txt Tokens per Sec:     3304 || Lr: 0.000050
2024-02-06 08:57:41,657 Epoch 2153: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-06 08:57:41,657 EPOCH 2154
2024-02-06 08:57:52,415 Epoch 2154: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-06 08:57:52,415 EPOCH 2155
2024-02-06 08:58:03,215 Epoch 2155: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-06 08:58:03,215 EPOCH 2156
2024-02-06 08:58:13,956 Epoch 2156: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-06 08:58:13,957 EPOCH 2157
2024-02-06 08:58:24,704 Epoch 2157: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-06 08:58:24,705 EPOCH 2158
2024-02-06 08:58:35,343 Epoch 2158: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-06 08:58:35,344 EPOCH 2159
2024-02-06 08:58:43,865 [Epoch: 2159 Step: 00036700] Batch Recognition Loss:   0.000247 => Gls Tokens per Sec:     1021 || Batch Translation Loss:   0.004667 => Txt Tokens per Sec:     2740 || Lr: 0.000050
2024-02-06 08:58:46,080 Epoch 2159: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-06 08:58:46,081 EPOCH 2160
2024-02-06 08:58:56,618 Epoch 2160: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-06 08:58:56,618 EPOCH 2161
2024-02-06 08:59:07,391 Epoch 2161: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-06 08:59:07,392 EPOCH 2162
2024-02-06 08:59:17,584 Epoch 2162: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-06 08:59:17,584 EPOCH 2163
2024-02-06 08:59:28,393 Epoch 2163: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 08:59:28,393 EPOCH 2164
2024-02-06 08:59:39,015 Epoch 2164: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-06 08:59:39,016 EPOCH 2165
2024-02-06 08:59:46,731 [Epoch: 2165 Step: 00036800] Batch Recognition Loss:   0.000277 => Gls Tokens per Sec:      962 || Batch Translation Loss:   0.010941 => Txt Tokens per Sec:     2716 || Lr: 0.000050
2024-02-06 08:59:49,659 Epoch 2165: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-06 08:59:49,660 EPOCH 2166
2024-02-06 09:00:00,352 Epoch 2166: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-06 09:00:00,353 EPOCH 2167
2024-02-06 09:00:10,739 Epoch 2167: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-06 09:00:10,739 EPOCH 2168
2024-02-06 09:00:21,729 Epoch 2168: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-06 09:00:21,729 EPOCH 2169
2024-02-06 09:00:32,560 Epoch 2169: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-06 09:00:32,561 EPOCH 2170
2024-02-06 09:00:43,121 Epoch 2170: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-06 09:00:43,121 EPOCH 2171
2024-02-06 09:00:48,708 [Epoch: 2171 Step: 00036900] Batch Recognition Loss:   0.000221 => Gls Tokens per Sec:     1146 || Batch Translation Loss:   0.015521 => Txt Tokens per Sec:     3146 || Lr: 0.000050
2024-02-06 09:00:53,889 Epoch 2171: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 09:00:53,889 EPOCH 2172
2024-02-06 09:01:04,680 Epoch 2172: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-06 09:01:04,681 EPOCH 2173
2024-02-06 09:01:15,616 Epoch 2173: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-06 09:01:15,616 EPOCH 2174
2024-02-06 09:01:26,226 Epoch 2174: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-06 09:01:26,227 EPOCH 2175
2024-02-06 09:01:36,823 Epoch 2175: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 09:01:36,824 EPOCH 2176
2024-02-06 09:01:47,671 Epoch 2176: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-06 09:01:47,671 EPOCH 2177
2024-02-06 09:01:54,116 [Epoch: 2177 Step: 00037000] Batch Recognition Loss:   0.000481 => Gls Tokens per Sec:      795 || Batch Translation Loss:   0.009286 => Txt Tokens per Sec:     2187 || Lr: 0.000050
2024-02-06 09:01:58,516 Epoch 2177: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 09:01:58,517 EPOCH 2178
2024-02-06 09:02:08,991 Epoch 2178: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 09:02:08,992 EPOCH 2179
2024-02-06 09:02:19,890 Epoch 2179: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 09:02:19,890 EPOCH 2180
2024-02-06 09:02:30,737 Epoch 2180: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-06 09:02:30,738 EPOCH 2181
2024-02-06 09:02:41,399 Epoch 2181: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-06 09:02:41,400 EPOCH 2182
2024-02-06 09:02:52,347 Epoch 2182: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-06 09:02:52,348 EPOCH 2183
2024-02-06 09:02:56,682 [Epoch: 2183 Step: 00037100] Batch Recognition Loss:   0.000132 => Gls Tokens per Sec:      886 || Batch Translation Loss:   0.014618 => Txt Tokens per Sec:     2561 || Lr: 0.000050
2024-02-06 09:03:03,094 Epoch 2183: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 09:03:03,095 EPOCH 2184
2024-02-06 09:03:13,700 Epoch 2184: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-06 09:03:13,701 EPOCH 2185
2024-02-06 09:03:24,374 Epoch 2185: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 09:03:24,375 EPOCH 2186
2024-02-06 09:03:35,266 Epoch 2186: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 09:03:35,266 EPOCH 2187
2024-02-06 09:03:45,980 Epoch 2187: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 09:03:45,981 EPOCH 2188
2024-02-06 09:03:56,642 Epoch 2188: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 09:03:56,643 EPOCH 2189
2024-02-06 09:03:59,699 [Epoch: 2189 Step: 00037200] Batch Recognition Loss:   0.000176 => Gls Tokens per Sec:      753 || Batch Translation Loss:   0.006571 => Txt Tokens per Sec:     1909 || Lr: 0.000050
2024-02-06 09:04:07,542 Epoch 2189: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 09:04:07,543 EPOCH 2190
2024-02-06 09:04:18,135 Epoch 2190: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-06 09:04:18,135 EPOCH 2191
2024-02-06 09:04:28,759 Epoch 2191: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-06 09:04:28,760 EPOCH 2192
2024-02-06 09:04:39,615 Epoch 2192: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-06 09:04:39,616 EPOCH 2193
2024-02-06 09:04:50,302 Epoch 2193: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-06 09:04:50,302 EPOCH 2194
2024-02-06 09:05:00,870 Epoch 2194: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-06 09:05:00,870 EPOCH 2195
2024-02-06 09:05:01,337 [Epoch: 2195 Step: 00037300] Batch Recognition Loss:   0.000486 => Gls Tokens per Sec:     2753 || Batch Translation Loss:   0.023054 => Txt Tokens per Sec:     8034 || Lr: 0.000050
2024-02-06 09:05:11,356 Epoch 2195: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-06 09:05:11,356 EPOCH 2196
2024-02-06 09:05:22,076 Epoch 2196: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-06 09:05:22,077 EPOCH 2197
2024-02-06 09:05:32,695 Epoch 2197: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-06 09:05:32,695 EPOCH 2198
2024-02-06 09:05:43,535 Epoch 2198: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-06 09:05:43,535 EPOCH 2199
2024-02-06 09:05:54,336 Epoch 2199: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-06 09:05:54,337 EPOCH 2200
2024-02-06 09:06:05,027 [Epoch: 2200 Step: 00037400] Batch Recognition Loss:   0.000435 => Gls Tokens per Sec:      994 || Batch Translation Loss:   0.039674 => Txt Tokens per Sec:     2749 || Lr: 0.000050
2024-02-06 09:06:05,027 Epoch 2200: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.80 
2024-02-06 09:06:05,028 EPOCH 2201
2024-02-06 09:06:15,808 Epoch 2201: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.76 
2024-02-06 09:06:15,809 EPOCH 2202
2024-02-06 09:06:26,370 Epoch 2202: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-06 09:06:26,370 EPOCH 2203
2024-02-06 09:06:37,120 Epoch 2203: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-06 09:06:37,120 EPOCH 2204
2024-02-06 09:06:47,986 Epoch 2204: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-06 09:06:47,987 EPOCH 2205
2024-02-06 09:06:59,041 Epoch 2205: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-06 09:06:59,042 EPOCH 2206
2024-02-06 09:07:09,290 [Epoch: 2206 Step: 00037500] Batch Recognition Loss:   0.000211 => Gls Tokens per Sec:      912 || Batch Translation Loss:   0.011258 => Txt Tokens per Sec:     2604 || Lr: 0.000050
2024-02-06 09:07:09,616 Epoch 2206: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-06 09:07:09,616 EPOCH 2207
2024-02-06 09:07:20,307 Epoch 2207: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 09:07:20,307 EPOCH 2208
2024-02-06 09:07:30,875 Epoch 2208: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 09:07:30,876 EPOCH 2209
2024-02-06 09:07:41,524 Epoch 2209: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-06 09:07:41,525 EPOCH 2210
2024-02-06 09:07:52,030 Epoch 2210: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-06 09:07:52,030 EPOCH 2211
2024-02-06 09:08:02,673 Epoch 2211: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-06 09:08:02,674 EPOCH 2212
2024-02-06 09:08:10,259 [Epoch: 2212 Step: 00037600] Batch Recognition Loss:   0.000261 => Gls Tokens per Sec:     1097 || Batch Translation Loss:   0.062658 => Txt Tokens per Sec:     3040 || Lr: 0.000050
2024-02-06 09:08:13,372 Epoch 2212: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-06 09:08:13,373 EPOCH 2213
2024-02-06 09:08:24,199 Epoch 2213: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-06 09:08:24,199 EPOCH 2214
2024-02-06 09:08:34,818 Epoch 2214: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-06 09:08:34,819 EPOCH 2215
2024-02-06 09:08:45,635 Epoch 2215: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-06 09:08:45,636 EPOCH 2216
2024-02-06 09:08:56,573 Epoch 2216: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-06 09:08:56,574 EPOCH 2217
2024-02-06 09:09:07,470 Epoch 2217: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-06 09:09:07,471 EPOCH 2218
2024-02-06 09:09:14,679 [Epoch: 2218 Step: 00037700] Batch Recognition Loss:   0.000135 => Gls Tokens per Sec:      977 || Batch Translation Loss:   0.010092 => Txt Tokens per Sec:     2705 || Lr: 0.000050
2024-02-06 09:09:18,232 Epoch 2218: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 09:09:18,233 EPOCH 2219
2024-02-06 09:09:28,931 Epoch 2219: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 09:09:28,932 EPOCH 2220
2024-02-06 09:09:39,494 Epoch 2220: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-06 09:09:39,494 EPOCH 2221
2024-02-06 09:09:50,254 Epoch 2221: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-06 09:09:50,254 EPOCH 2222
2024-02-06 09:10:01,029 Epoch 2222: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 09:10:01,029 EPOCH 2223
2024-02-06 09:10:11,874 Epoch 2223: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-06 09:10:11,875 EPOCH 2224
2024-02-06 09:10:17,374 [Epoch: 2224 Step: 00037800] Batch Recognition Loss:   0.000198 => Gls Tokens per Sec:     1000 || Batch Translation Loss:   0.007361 => Txt Tokens per Sec:     2597 || Lr: 0.000050
2024-02-06 09:10:22,593 Epoch 2224: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-06 09:10:22,593 EPOCH 2225
2024-02-06 09:10:33,267 Epoch 2225: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-06 09:10:33,267 EPOCH 2226
2024-02-06 09:10:43,907 Epoch 2226: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-06 09:10:43,907 EPOCH 2227
2024-02-06 09:10:54,608 Epoch 2227: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-06 09:10:54,608 EPOCH 2228
2024-02-06 09:11:05,456 Epoch 2228: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-06 09:11:05,457 EPOCH 2229
2024-02-06 09:11:16,321 Epoch 2229: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-06 09:11:16,321 EPOCH 2230
2024-02-06 09:11:20,166 [Epoch: 2230 Step: 00037900] Batch Recognition Loss:   0.000238 => Gls Tokens per Sec:     1098 || Batch Translation Loss:   0.024718 => Txt Tokens per Sec:     2976 || Lr: 0.000050
2024-02-06 09:11:27,119 Epoch 2230: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-06 09:11:27,119 EPOCH 2231
2024-02-06 09:11:38,195 Epoch 2231: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-06 09:11:38,196 EPOCH 2232
2024-02-06 09:11:49,105 Epoch 2232: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-06 09:11:49,105 EPOCH 2233
2024-02-06 09:11:59,713 Epoch 2233: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 09:11:59,714 EPOCH 2234
2024-02-06 09:12:10,685 Epoch 2234: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-06 09:12:10,686 EPOCH 2235
2024-02-06 09:12:21,662 Epoch 2235: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-06 09:12:21,663 EPOCH 2236
2024-02-06 09:12:24,822 [Epoch: 2236 Step: 00038000] Batch Recognition Loss:   0.000142 => Gls Tokens per Sec:      930 || Batch Translation Loss:   0.009223 => Txt Tokens per Sec:     2389 || Lr: 0.000050
2024-02-06 09:13:05,450 Validation result at epoch 2236, step    38000: duration: 40.6263s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.66129	Translation Loss: 95858.27344	PPL: 14390.70801
	Eval Metric: BLEU
	WER 2.90	(DEL: 0.00,	INS: 0.00,	SUB: 2.90)
	BLEU-4 0.43	(BLEU-1: 10.60,	BLEU-2: 3.09,	BLEU-3: 1.07,	BLEU-4: 0.43)
	CHRF 16.93	ROUGE 9.16
2024-02-06 09:13:05,452 Logging Recognition and Translation Outputs
2024-02-06 09:13:05,452 ========================================================================================================================
2024-02-06 09:13:05,452 Logging Sequence: 59_152.00
2024-02-06 09:13:05,452 	Gloss Reference :	A B+C+D+E
2024-02-06 09:13:05,452 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 09:13:05,453 	Gloss Alignment :	         
2024-02-06 09:13:05,453 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 09:13:05,454 	Text Reference  :	**** ** the ***** organisers encouraged athletes to     use    the condoms in their home  countries
2024-02-06 09:13:05,454 	Text Hypothesis :	well is the first time       both       are      always during the ******* ** t20   world cup      
2024-02-06 09:13:05,455 	Text Alignment  :	I    I      I     S          S          S        S      S          D       D  S     S     S        
2024-02-06 09:13:05,455 ========================================================================================================================
2024-02-06 09:13:05,455 Logging Sequence: 155_78.00
2024-02-06 09:13:05,455 	Gloss Reference :	A B+C+D+E
2024-02-06 09:13:05,455 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 09:13:05,455 	Gloss Alignment :	         
2024-02-06 09:13:05,456 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 09:13:05,458 	Text Reference  :	it was difficult for icc   to disqualify the   afghan team   at   the last minute so they included them    as  per    the schedule
2024-02-06 09:13:05,458 	Text Hypothesis :	if you don't     go  ahead of support    staff and    others come the **** ****** ** **** teams    playing the number of  matches 
2024-02-06 09:13:05,458 	Text Alignment  :	S  S   S         S   S     S  S          S     S      S      S        D    D      D  D    S        S       S   S      S   S       
2024-02-06 09:13:05,458 ========================================================================================================================
2024-02-06 09:13:05,458 Logging Sequence: 102_147.00
2024-02-06 09:13:05,459 	Gloss Reference :	A B+C+D+E
2024-02-06 09:13:05,459 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 09:13:05,459 	Gloss Alignment :	         
2024-02-06 09:13:05,459 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 09:13:05,461 	Text Reference  :	despite the muscle cramps this young boy      lifted such a    huge weight and    made   the country proud by securing a gold    medal   
2024-02-06 09:13:05,461 	Text Hypothesis :	as      the ****** ****** goal is    defended shami  them away 6    medal  sheuli lifted the ******* ***** ** ******** * penalty shootout
2024-02-06 09:13:05,461 	Text Alignment  :	S           D      D      S    S     S        S      S    S    S    S      S      S          D       D     D  D        D S       S       
2024-02-06 09:13:05,461 ========================================================================================================================
2024-02-06 09:13:05,462 Logging Sequence: 105_2.00
2024-02-06 09:13:05,462 	Gloss Reference :	A B+C+D+E  
2024-02-06 09:13:05,462 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-06 09:13:05,462 	Gloss Alignment :	  S        
2024-02-06 09:13:05,462 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 09:13:05,463 	Text Reference  :	***** * ********* ** ********* the airthings masters   tournament is an        online chess     tournament
2024-02-06 09:13:05,463 	Text Hypothesis :	group a consisted of australia new zealand   singapore and        13 countries of     caribbean region    
2024-02-06 09:13:05,463 	Text Alignment  :	I     I I         I  I         S   S         S         S          S  S         S      S         S         
2024-02-06 09:13:05,464 ========================================================================================================================
2024-02-06 09:13:05,464 Logging Sequence: 96_31.00
2024-02-06 09:13:05,464 	Gloss Reference :	A B+C+D+E
2024-02-06 09:13:05,464 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 09:13:05,464 	Gloss Alignment :	         
2024-02-06 09:13:05,464 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 09:13:05,465 	Text Reference  :	and then 2 teams will go on    to  play the ***** final
2024-02-06 09:13:05,465 	Text Hypothesis :	*** **** * ***** **** ** india had won  the world cup  
2024-02-06 09:13:05,465 	Text Alignment  :	D   D    D D     D    D  S     S   S        I     S    
2024-02-06 09:13:05,465 ========================================================================================================================
2024-02-06 09:13:13,396 Epoch 2236: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-06 09:13:13,397 EPOCH 2237
2024-02-06 09:13:24,060 Epoch 2237: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-06 09:13:24,060 EPOCH 2238
2024-02-06 09:13:35,036 Epoch 2238: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 09:13:35,037 EPOCH 2239
2024-02-06 09:13:45,760 Epoch 2239: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 09:13:45,761 EPOCH 2240
2024-02-06 09:13:56,689 Epoch 2240: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 09:13:56,690 EPOCH 2241
2024-02-06 09:14:07,613 Epoch 2241: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-06 09:14:07,613 EPOCH 2242
2024-02-06 09:14:10,024 [Epoch: 2242 Step: 00038100] Batch Recognition Loss:   0.000157 => Gls Tokens per Sec:      797 || Batch Translation Loss:   0.027378 => Txt Tokens per Sec:     2245 || Lr: 0.000050
2024-02-06 09:14:18,199 Epoch 2242: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-06 09:14:18,200 EPOCH 2243
2024-02-06 09:14:29,068 Epoch 2243: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-06 09:14:29,068 EPOCH 2244
2024-02-06 09:14:39,904 Epoch 2244: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-06 09:14:39,904 EPOCH 2245
2024-02-06 09:14:50,534 Epoch 2245: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-06 09:14:50,535 EPOCH 2246
2024-02-06 09:15:01,256 Epoch 2246: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-06 09:15:01,256 EPOCH 2247
2024-02-06 09:15:11,991 Epoch 2247: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-06 09:15:11,992 EPOCH 2248
2024-02-06 09:15:14,045 [Epoch: 2248 Step: 00038200] Batch Recognition Loss:   0.000543 => Gls Tokens per Sec:      312 || Batch Translation Loss:   0.033687 => Txt Tokens per Sec:     1082 || Lr: 0.000050
2024-02-06 09:15:22,763 Epoch 2248: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 09:15:22,764 EPOCH 2249
2024-02-06 09:15:33,620 Epoch 2249: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 09:15:33,620 EPOCH 2250
2024-02-06 09:15:44,365 Epoch 2250: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-06 09:15:44,366 EPOCH 2251
2024-02-06 09:15:55,110 Epoch 2251: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-06 09:15:55,110 EPOCH 2252
2024-02-06 09:16:05,659 Epoch 2252: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-06 09:16:05,659 EPOCH 2253
2024-02-06 09:16:16,301 [Epoch: 2253 Step: 00038300] Batch Recognition Loss:   0.000183 => Gls Tokens per Sec:      938 || Batch Translation Loss:   0.028055 => Txt Tokens per Sec:     2569 || Lr: 0.000050
2024-02-06 09:16:16,623 Epoch 2253: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-06 09:16:16,623 EPOCH 2254
2024-02-06 09:16:27,444 Epoch 2254: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 09:16:27,445 EPOCH 2255
2024-02-06 09:16:38,032 Epoch 2255: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 09:16:38,033 EPOCH 2256
2024-02-06 09:16:48,700 Epoch 2256: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.33 
2024-02-06 09:16:48,701 EPOCH 2257
2024-02-06 09:16:59,427 Epoch 2257: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-06 09:16:59,427 EPOCH 2258
2024-02-06 09:17:10,015 Epoch 2258: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.42 
2024-02-06 09:17:10,016 EPOCH 2259
2024-02-06 09:17:19,980 [Epoch: 2259 Step: 00038400] Batch Recognition Loss:   0.000594 => Gls Tokens per Sec:      873 || Batch Translation Loss:   0.046488 => Txt Tokens per Sec:     2458 || Lr: 0.000050
2024-02-06 09:17:20,650 Epoch 2259: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-06 09:17:20,650 EPOCH 2260
2024-02-06 09:17:31,015 Epoch 2260: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-06 09:17:31,016 EPOCH 2261
2024-02-06 09:17:41,732 Epoch 2261: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.55 
2024-02-06 09:17:41,733 EPOCH 2262
2024-02-06 09:17:52,679 Epoch 2262: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-06 09:17:52,680 EPOCH 2263
2024-02-06 09:18:03,384 Epoch 2263: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-06 09:18:03,384 EPOCH 2264
2024-02-06 09:18:14,353 Epoch 2264: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-06 09:18:14,354 EPOCH 2265
2024-02-06 09:18:21,581 [Epoch: 2265 Step: 00038500] Batch Recognition Loss:   0.000165 => Gls Tokens per Sec:     1063 || Batch Translation Loss:   0.021012 => Txt Tokens per Sec:     2897 || Lr: 0.000050
2024-02-06 09:18:25,033 Epoch 2265: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-06 09:18:25,034 EPOCH 2266
2024-02-06 09:18:35,670 Epoch 2266: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-06 09:18:35,671 EPOCH 2267
2024-02-06 09:18:46,759 Epoch 2267: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-06 09:18:46,760 EPOCH 2268
2024-02-06 09:18:57,287 Epoch 2268: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-06 09:18:57,287 EPOCH 2269
2024-02-06 09:19:07,516 Epoch 2269: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-06 09:19:07,517 EPOCH 2270
2024-02-06 09:19:18,131 Epoch 2270: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 09:19:18,132 EPOCH 2271
2024-02-06 09:19:21,841 [Epoch: 2271 Step: 00038600] Batch Recognition Loss:   0.000206 => Gls Tokens per Sec:     1726 || Batch Translation Loss:   0.013664 => Txt Tokens per Sec:     4528 || Lr: 0.000050
2024-02-06 09:19:29,095 Epoch 2271: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 09:19:29,095 EPOCH 2272
2024-02-06 09:19:40,057 Epoch 2272: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-06 09:19:40,057 EPOCH 2273
2024-02-06 09:19:50,524 Epoch 2273: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-06 09:19:50,525 EPOCH 2274
2024-02-06 09:20:01,248 Epoch 2274: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-06 09:20:01,248 EPOCH 2275
2024-02-06 09:20:11,836 Epoch 2275: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-06 09:20:11,837 EPOCH 2276
2024-02-06 09:20:22,469 Epoch 2276: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.35 
2024-02-06 09:20:22,470 EPOCH 2277
2024-02-06 09:20:27,104 [Epoch: 2277 Step: 00038700] Batch Recognition Loss:   0.000436 => Gls Tokens per Sec:     1105 || Batch Translation Loss:   0.014667 => Txt Tokens per Sec:     2978 || Lr: 0.000050
2024-02-06 09:20:33,147 Epoch 2277: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-06 09:20:33,147 EPOCH 2278
2024-02-06 09:20:43,924 Epoch 2278: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-06 09:20:43,925 EPOCH 2279
2024-02-06 09:20:54,443 Epoch 2279: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 09:20:54,444 EPOCH 2280
2024-02-06 09:21:05,302 Epoch 2280: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 09:21:05,303 EPOCH 2281
2024-02-06 09:21:15,851 Epoch 2281: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 09:21:15,852 EPOCH 2282
2024-02-06 09:21:26,778 Epoch 2282: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 09:21:26,779 EPOCH 2283
2024-02-06 09:21:29,526 [Epoch: 2283 Step: 00038800] Batch Recognition Loss:   0.000177 => Gls Tokens per Sec:     1398 || Batch Translation Loss:   0.019166 => Txt Tokens per Sec:     3745 || Lr: 0.000050
2024-02-06 09:21:37,443 Epoch 2283: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 09:21:37,444 EPOCH 2284
2024-02-06 09:21:48,202 Epoch 2284: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-06 09:21:48,202 EPOCH 2285
2024-02-06 09:21:59,199 Epoch 2285: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-06 09:21:59,200 EPOCH 2286
2024-02-06 09:22:10,088 Epoch 2286: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 09:22:10,089 EPOCH 2287
2024-02-06 09:22:20,934 Epoch 2287: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-06 09:22:20,934 EPOCH 2288
2024-02-06 09:22:31,557 Epoch 2288: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 09:22:31,557 EPOCH 2289
2024-02-06 09:22:36,119 [Epoch: 2289 Step: 00038900] Batch Recognition Loss:   0.000215 => Gls Tokens per Sec:      504 || Batch Translation Loss:   0.018004 => Txt Tokens per Sec:     1525 || Lr: 0.000050
2024-02-06 09:22:42,073 Epoch 2289: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 09:22:42,074 EPOCH 2290
2024-02-06 09:22:52,910 Epoch 2290: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 09:22:52,910 EPOCH 2291
2024-02-06 09:23:04,005 Epoch 2291: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 09:23:04,006 EPOCH 2292
2024-02-06 09:23:14,797 Epoch 2292: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 09:23:14,797 EPOCH 2293
2024-02-06 09:23:25,470 Epoch 2293: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 09:23:25,470 EPOCH 2294
2024-02-06 09:23:36,406 Epoch 2294: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 09:23:36,407 EPOCH 2295
2024-02-06 09:23:38,291 [Epoch: 2295 Step: 00039000] Batch Recognition Loss:   0.000109 => Gls Tokens per Sec:      680 || Batch Translation Loss:   0.009743 => Txt Tokens per Sec:     1868 || Lr: 0.000050
2024-02-06 09:23:47,219 Epoch 2295: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 09:23:47,219 EPOCH 2296
2024-02-06 09:23:57,873 Epoch 2296: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 09:23:57,873 EPOCH 2297
2024-02-06 09:24:08,692 Epoch 2297: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-06 09:24:08,693 EPOCH 2298
2024-02-06 09:24:19,407 Epoch 2298: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-06 09:24:19,407 EPOCH 2299
2024-02-06 09:24:30,390 Epoch 2299: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 09:24:30,391 EPOCH 2300
2024-02-06 09:24:41,178 [Epoch: 2300 Step: 00039100] Batch Recognition Loss:   0.000118 => Gls Tokens per Sec:      985 || Batch Translation Loss:   0.009067 => Txt Tokens per Sec:     2724 || Lr: 0.000050
2024-02-06 09:24:41,178 Epoch 2300: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-06 09:24:41,178 EPOCH 2301
2024-02-06 09:24:51,541 Epoch 2301: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-06 09:24:51,541 EPOCH 2302
2024-02-06 09:25:02,220 Epoch 2302: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-06 09:25:02,220 EPOCH 2303
2024-02-06 09:25:12,805 Epoch 2303: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 09:25:12,806 EPOCH 2304
2024-02-06 09:25:23,532 Epoch 2304: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-06 09:25:23,533 EPOCH 2305
2024-02-06 09:25:34,254 Epoch 2305: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 09:25:34,255 EPOCH 2306
2024-02-06 09:25:42,965 [Epoch: 2306 Step: 00039200] Batch Recognition Loss:   0.000230 => Gls Tokens per Sec:     1072 || Batch Translation Loss:   0.012008 => Txt Tokens per Sec:     2916 || Lr: 0.000050
2024-02-06 09:25:44,869 Epoch 2306: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-06 09:25:44,869 EPOCH 2307
2024-02-06 09:25:55,556 Epoch 2307: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 09:25:55,557 EPOCH 2308
2024-02-06 09:26:06,315 Epoch 2308: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 09:26:06,316 EPOCH 2309
2024-02-06 09:26:17,011 Epoch 2309: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 09:26:17,012 EPOCH 2310
2024-02-06 09:26:27,631 Epoch 2310: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 09:26:27,631 EPOCH 2311
2024-02-06 09:26:38,322 Epoch 2311: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 09:26:38,323 EPOCH 2312
2024-02-06 09:26:45,958 [Epoch: 2312 Step: 00039300] Batch Recognition Loss:   0.000239 => Gls Tokens per Sec:     1090 || Batch Translation Loss:   0.022867 => Txt Tokens per Sec:     2937 || Lr: 0.000050
2024-02-06 09:26:49,155 Epoch 2312: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 09:26:49,155 EPOCH 2313
2024-02-06 09:26:59,786 Epoch 2313: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 09:26:59,787 EPOCH 2314
2024-02-06 09:27:10,521 Epoch 2314: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-06 09:27:10,521 EPOCH 2315
2024-02-06 09:27:21,144 Epoch 2315: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.48 
2024-02-06 09:27:21,145 EPOCH 2316
2024-02-06 09:27:31,853 Epoch 2316: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-06 09:27:31,853 EPOCH 2317
2024-02-06 09:27:42,716 Epoch 2317: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-06 09:27:42,717 EPOCH 2318
2024-02-06 09:27:48,066 [Epoch: 2318 Step: 00039400] Batch Recognition Loss:   0.000265 => Gls Tokens per Sec:     1316 || Batch Translation Loss:   0.026701 => Txt Tokens per Sec:     3583 || Lr: 0.000050
2024-02-06 09:27:53,506 Epoch 2318: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-06 09:27:53,506 EPOCH 2319
2024-02-06 09:28:04,211 Epoch 2319: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-06 09:28:04,212 EPOCH 2320
2024-02-06 09:28:14,604 Epoch 2320: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-06 09:28:14,605 EPOCH 2321
2024-02-06 09:28:25,611 Epoch 2321: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-06 09:28:25,611 EPOCH 2322
2024-02-06 09:28:36,323 Epoch 2322: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.75 
2024-02-06 09:28:36,323 EPOCH 2323
2024-02-06 09:28:47,081 Epoch 2323: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.90 
2024-02-06 09:28:47,081 EPOCH 2324
2024-02-06 09:28:52,762 [Epoch: 2324 Step: 00039500] Batch Recognition Loss:   0.000655 => Gls Tokens per Sec:      968 || Batch Translation Loss:   0.035315 => Txt Tokens per Sec:     2547 || Lr: 0.000050
2024-02-06 09:28:57,772 Epoch 2324: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.99 
2024-02-06 09:28:57,772 EPOCH 2325
2024-02-06 09:29:08,485 Epoch 2325: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.26 
2024-02-06 09:29:08,486 EPOCH 2326
2024-02-06 09:29:19,114 Epoch 2326: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.15 
2024-02-06 09:29:19,114 EPOCH 2327
2024-02-06 09:29:29,691 Epoch 2327: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.86 
2024-02-06 09:29:29,691 EPOCH 2328
2024-02-06 09:29:40,535 Epoch 2328: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.72 
2024-02-06 09:29:40,535 EPOCH 2329
2024-02-06 09:29:51,137 Epoch 2329: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-06 09:29:51,137 EPOCH 2330
2024-02-06 09:29:57,432 [Epoch: 2330 Step: 00039600] Batch Recognition Loss:   0.000177 => Gls Tokens per Sec:      712 || Batch Translation Loss:   0.037184 => Txt Tokens per Sec:     1983 || Lr: 0.000050
2024-02-06 09:30:01,937 Epoch 2330: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-06 09:30:01,938 EPOCH 2331
2024-02-06 09:30:12,482 Epoch 2331: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-06 09:30:12,483 EPOCH 2332
2024-02-06 09:30:23,436 Epoch 2332: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-06 09:30:23,436 EPOCH 2333
2024-02-06 09:30:34,145 Epoch 2333: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-06 09:30:34,146 EPOCH 2334
2024-02-06 09:30:44,824 Epoch 2334: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 09:30:44,825 EPOCH 2335
2024-02-06 09:30:55,591 Epoch 2335: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 09:30:55,592 EPOCH 2336
2024-02-06 09:30:58,738 [Epoch: 2336 Step: 00039700] Batch Recognition Loss:   0.002453 => Gls Tokens per Sec:     1018 || Batch Translation Loss:   0.011907 => Txt Tokens per Sec:     2877 || Lr: 0.000050
2024-02-06 09:31:06,440 Epoch 2336: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 09:31:06,441 EPOCH 2337
2024-02-06 09:31:17,130 Epoch 2337: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 09:31:17,131 EPOCH 2338
2024-02-06 09:31:27,661 Epoch 2338: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 09:31:27,662 EPOCH 2339
2024-02-06 09:31:38,330 Epoch 2339: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 09:31:38,331 EPOCH 2340
2024-02-06 09:31:48,984 Epoch 2340: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-06 09:31:48,984 EPOCH 2341
2024-02-06 09:31:59,868 Epoch 2341: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-06 09:31:59,868 EPOCH 2342
2024-02-06 09:32:00,351 [Epoch: 2342 Step: 00039800] Batch Recognition Loss:   0.000154 => Gls Tokens per Sec:     3992 || Batch Translation Loss:   0.012035 => Txt Tokens per Sec:     8844 || Lr: 0.000050
2024-02-06 09:32:10,546 Epoch 2342: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 09:32:10,546 EPOCH 2343
2024-02-06 09:32:21,427 Epoch 2343: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 09:32:21,428 EPOCH 2344
2024-02-06 09:32:32,034 Epoch 2344: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-06 09:32:32,035 EPOCH 2345
2024-02-06 09:32:42,848 Epoch 2345: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-06 09:32:42,848 EPOCH 2346
2024-02-06 09:32:53,491 Epoch 2346: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-06 09:32:53,492 EPOCH 2347
2024-02-06 09:33:04,238 Epoch 2347: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 09:33:04,239 EPOCH 2348
2024-02-06 09:33:04,603 [Epoch: 2348 Step: 00039900] Batch Recognition Loss:   0.000250 => Gls Tokens per Sec:     1758 || Batch Translation Loss:   0.014507 => Txt Tokens per Sec:     5591 || Lr: 0.000050
2024-02-06 09:33:14,815 Epoch 2348: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 09:33:14,815 EPOCH 2349
2024-02-06 09:33:25,564 Epoch 2349: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 09:33:25,565 EPOCH 2350
2024-02-06 09:33:36,337 Epoch 2350: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 09:33:36,338 EPOCH 2351
2024-02-06 09:33:47,073 Epoch 2351: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-06 09:33:47,074 EPOCH 2352
2024-02-06 09:33:58,161 Epoch 2352: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 09:33:58,161 EPOCH 2353
2024-02-06 09:34:08,723 [Epoch: 2353 Step: 00040000] Batch Recognition Loss:   0.000143 => Gls Tokens per Sec:      945 || Batch Translation Loss:   0.019344 => Txt Tokens per Sec:     2620 || Lr: 0.000050
2024-02-06 09:34:49,322 Validation result at epoch 2353, step    40000: duration: 40.5990s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.66600	Translation Loss: 96632.87500	PPL: 15548.27148
	Eval Metric: BLEU
	WER 2.90	(DEL: 0.00,	INS: 0.00,	SUB: 2.90)
	BLEU-4 0.49	(BLEU-1: 11.09,	BLEU-2: 3.17,	BLEU-3: 1.12,	BLEU-4: 0.49)
	CHRF 17.29	ROUGE 9.09
2024-02-06 09:34:49,324 Logging Recognition and Translation Outputs
2024-02-06 09:34:49,324 ========================================================================================================================
2024-02-06 09:34:49,324 Logging Sequence: 86_84.00
2024-02-06 09:34:49,325 	Gloss Reference :	A B+C+D+E
2024-02-06 09:34:49,325 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 09:34:49,325 	Gloss Alignment :	         
2024-02-06 09:34:49,325 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 09:34:49,327 	Text Reference  :	amassing 8933 runs which included 21 centuries with a   highest score of  201    not out 
2024-02-06 09:34:49,327 	Text Hypothesis :	******** **** **** ***** ******** an indian    team was run     by    the scored 260 runs
2024-02-06 09:34:49,327 	Text Alignment  :	D        D    D    D     D        S  S         S    S   S       S     S   S      S   S   
2024-02-06 09:34:49,327 ========================================================================================================================
2024-02-06 09:34:49,328 Logging Sequence: 179_110.00
2024-02-06 09:34:49,328 	Gloss Reference :	A B+C+D+E
2024-02-06 09:34:49,328 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 09:34:49,328 	Gloss Alignment :	         
2024-02-06 09:34:49,328 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 09:34:49,330 	Text Reference  :	**** *** **** ***** phogat refused to    stay     in *** **** the ******* ***** *** **** same      room with other   indian female wrestlers
2024-02-06 09:34:49,330 	Text Hypothesis :	then the gold medal tally  in      tokyo olympics in one over the singlet which had been sponsored by   a    company called shiv   naresh   
2024-02-06 09:34:49,330 	Text Alignment  :	I    I   I    I     S      S       S     S           I   I        I       I     I   I    S         S    S    S       S      S      S        
2024-02-06 09:34:49,330 ========================================================================================================================
2024-02-06 09:34:49,330 Logging Sequence: 102_2.00
2024-02-06 09:34:49,331 	Gloss Reference :	A B+C+D+E    
2024-02-06 09:34:49,331 	Gloss Hypothesis:	A B+C+D+E+D+E
2024-02-06 09:34:49,331 	Gloss Alignment :	  S          
2024-02-06 09:34:49,331 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 09:34:49,332 	Text Reference  :	commonwealth games are among the world's most  recognised gaming championships after  the     olympics
2024-02-06 09:34:49,332 	Text Hypothesis :	************ ***** *** ***** 4   anyone  found this       for    test          series against england 
2024-02-06 09:34:49,332 	Text Alignment  :	D            D     D   D     S   S       S     S          S      S             S      S       S       
2024-02-06 09:34:49,332 ========================================================================================================================
2024-02-06 09:34:49,332 Logging Sequence: 60_195.00
2024-02-06 09:34:49,333 	Gloss Reference :	A B+C+D+E
2024-02-06 09:34:49,333 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 09:34:49,333 	Gloss Alignment :	         
2024-02-06 09:34:49,333 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 09:34:49,334 	Text Reference  :	** **** *** **** ******* *** ** ***** people loved to      watch his aggressive expressions and his  bowling
2024-02-06 09:34:49,334 	Text Hypothesis :	as bcci has been planned say if there are    also  present at    the same       with        the same thing  
2024-02-06 09:34:49,335 	Text Alignment  :	I  I    I   I    I       I   I  I     S      S     S       S     S   S          S           S   S    S      
2024-02-06 09:34:49,335 ========================================================================================================================
2024-02-06 09:34:49,335 Logging Sequence: 70_200.00
2024-02-06 09:34:49,335 	Gloss Reference :	A B+C+D+E
2024-02-06 09:34:49,335 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 09:34:49,336 	Gloss Alignment :	         
2024-02-06 09:34:49,336 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 09:34:49,336 	Text Reference  :	***** *** * ******* ******* ** showing ronaldo whole-heartedly endorsing the ***** brand
2024-02-06 09:34:49,336 	Text Hypothesis :	about the 4 matches started in 15      months  has             won       the world cup  
2024-02-06 09:34:49,337 	Text Alignment  :	I     I   I I       I       I  S       S       S               S             I     S    
2024-02-06 09:34:49,337 ========================================================================================================================
2024-02-06 09:34:49,661 Epoch 2353: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 09:34:49,661 EPOCH 2354
2024-02-06 09:35:00,863 Epoch 2354: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 09:35:00,864 EPOCH 2355
2024-02-06 09:35:11,495 Epoch 2355: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-06 09:35:11,495 EPOCH 2356
2024-02-06 09:35:22,160 Epoch 2356: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 09:35:22,160 EPOCH 2357
2024-02-06 09:35:32,838 Epoch 2357: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 09:35:32,838 EPOCH 2358
2024-02-06 09:35:43,543 Epoch 2358: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-06 09:35:43,543 EPOCH 2359
2024-02-06 09:35:53,597 [Epoch: 2359 Step: 00040100] Batch Recognition Loss:   0.000170 => Gls Tokens per Sec:      865 || Batch Translation Loss:   0.014907 => Txt Tokens per Sec:     2379 || Lr: 0.000050
2024-02-06 09:35:54,307 Epoch 2359: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-06 09:35:54,307 EPOCH 2360
2024-02-06 09:36:05,154 Epoch 2360: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-06 09:36:05,154 EPOCH 2361
2024-02-06 09:36:15,666 Epoch 2361: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-06 09:36:15,666 EPOCH 2362
2024-02-06 09:36:26,262 Epoch 2362: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-06 09:36:26,262 EPOCH 2363
2024-02-06 09:36:37,033 Epoch 2363: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-06 09:36:37,034 EPOCH 2364
2024-02-06 09:36:47,962 Epoch 2364: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-06 09:36:47,962 EPOCH 2365
2024-02-06 09:36:53,409 [Epoch: 2365 Step: 00040200] Batch Recognition Loss:   0.000226 => Gls Tokens per Sec:     1410 || Batch Translation Loss:   0.022993 => Txt Tokens per Sec:     3760 || Lr: 0.000050
2024-02-06 09:36:58,609 Epoch 2365: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-06 09:36:58,609 EPOCH 2366
2024-02-06 09:37:09,544 Epoch 2366: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 09:37:09,544 EPOCH 2367
2024-02-06 09:37:19,983 Epoch 2367: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 09:37:19,984 EPOCH 2368
2024-02-06 09:37:30,848 Epoch 2368: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 09:37:30,848 EPOCH 2369
2024-02-06 09:37:41,456 Epoch 2369: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 09:37:41,456 EPOCH 2370
2024-02-06 09:37:52,045 Epoch 2370: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-06 09:37:52,046 EPOCH 2371
2024-02-06 09:37:55,947 [Epoch: 2371 Step: 00040300] Batch Recognition Loss:   0.000155 => Gls Tokens per Sec:     1641 || Batch Translation Loss:   0.012245 => Txt Tokens per Sec:     4147 || Lr: 0.000050
2024-02-06 09:38:02,894 Epoch 2371: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 09:38:02,894 EPOCH 2372
2024-02-06 09:38:14,007 Epoch 2372: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 09:38:14,007 EPOCH 2373
2024-02-06 09:38:24,809 Epoch 2373: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 09:38:24,809 EPOCH 2374
2024-02-06 09:38:35,440 Epoch 2374: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-06 09:38:35,441 EPOCH 2375
2024-02-06 09:38:46,311 Epoch 2375: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-06 09:38:46,311 EPOCH 2376
2024-02-06 09:38:57,284 Epoch 2376: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-06 09:38:57,284 EPOCH 2377
2024-02-06 09:39:02,949 [Epoch: 2377 Step: 00040400] Batch Recognition Loss:   0.000225 => Gls Tokens per Sec:      858 || Batch Translation Loss:   0.021322 => Txt Tokens per Sec:     2446 || Lr: 0.000050
2024-02-06 09:39:07,946 Epoch 2377: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 09:39:07,946 EPOCH 2378
2024-02-06 09:39:18,650 Epoch 2378: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-06 09:39:18,650 EPOCH 2379
2024-02-06 09:39:29,455 Epoch 2379: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-06 09:39:29,455 EPOCH 2380
2024-02-06 09:39:40,350 Epoch 2380: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-06 09:39:40,351 EPOCH 2381
2024-02-06 09:39:50,918 Epoch 2381: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-06 09:39:50,919 EPOCH 2382
2024-02-06 09:40:01,497 Epoch 2382: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-06 09:40:01,497 EPOCH 2383
2024-02-06 09:40:04,472 [Epoch: 2383 Step: 00040500] Batch Recognition Loss:   0.000228 => Gls Tokens per Sec:     1291 || Batch Translation Loss:   0.056767 => Txt Tokens per Sec:     3660 || Lr: 0.000050
2024-02-06 09:40:12,317 Epoch 2383: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-06 09:40:12,318 EPOCH 2384
2024-02-06 09:40:23,126 Epoch 2384: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-06 09:40:23,126 EPOCH 2385
2024-02-06 09:40:33,823 Epoch 2385: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-06 09:40:33,823 EPOCH 2386
2024-02-06 09:40:44,507 Epoch 2386: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-06 09:40:44,507 EPOCH 2387
2024-02-06 09:40:55,342 Epoch 2387: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.73 
2024-02-06 09:40:55,342 EPOCH 2388
2024-02-06 09:41:05,902 Epoch 2388: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-06 09:41:05,903 EPOCH 2389
2024-02-06 09:41:10,089 [Epoch: 2389 Step: 00040600] Batch Recognition Loss:   0.000703 => Gls Tokens per Sec:      612 || Batch Translation Loss:   0.034080 => Txt Tokens per Sec:     1947 || Lr: 0.000050
2024-02-06 09:41:16,659 Epoch 2389: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-06 09:41:16,659 EPOCH 2390
2024-02-06 09:41:27,448 Epoch 2390: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-06 09:41:27,448 EPOCH 2391
2024-02-06 09:41:38,255 Epoch 2391: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-06 09:41:38,256 EPOCH 2392
2024-02-06 09:41:49,306 Epoch 2392: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-06 09:41:49,307 EPOCH 2393
2024-02-06 09:42:00,172 Epoch 2393: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 09:42:00,172 EPOCH 2394
2024-02-06 09:42:10,953 Epoch 2394: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 09:42:10,953 EPOCH 2395
2024-02-06 09:42:13,482 [Epoch: 2395 Step: 00040700] Batch Recognition Loss:   0.000184 => Gls Tokens per Sec:      403 || Batch Translation Loss:   0.008032 => Txt Tokens per Sec:      932 || Lr: 0.000050
2024-02-06 09:42:21,600 Epoch 2395: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 09:42:21,601 EPOCH 2396
2024-02-06 09:42:32,062 Epoch 2396: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 09:42:32,063 EPOCH 2397
2024-02-06 09:42:42,511 Epoch 2397: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 09:42:42,511 EPOCH 2398
2024-02-06 09:42:53,065 Epoch 2398: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 09:42:53,066 EPOCH 2399
2024-02-06 09:43:03,913 Epoch 2399: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-06 09:43:03,914 EPOCH 2400
2024-02-06 09:43:14,633 [Epoch: 2400 Step: 00040800] Batch Recognition Loss:   0.000161 => Gls Tokens per Sec:      991 || Batch Translation Loss:   0.017745 => Txt Tokens per Sec:     2741 || Lr: 0.000050
2024-02-06 09:43:14,634 Epoch 2400: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-06 09:43:14,634 EPOCH 2401
2024-02-06 09:43:25,335 Epoch 2401: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 09:43:25,336 EPOCH 2402
2024-02-06 09:43:35,905 Epoch 2402: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 09:43:35,906 EPOCH 2403
2024-02-06 09:43:46,680 Epoch 2403: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 09:43:46,681 EPOCH 2404
2024-02-06 09:43:57,169 Epoch 2404: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 09:43:57,170 EPOCH 2405
2024-02-06 09:44:07,975 Epoch 2405: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 09:44:07,976 EPOCH 2406
2024-02-06 09:44:16,021 [Epoch: 2406 Step: 00040900] Batch Recognition Loss:   0.000167 => Gls Tokens per Sec:     1194 || Batch Translation Loss:   0.024227 => Txt Tokens per Sec:     3270 || Lr: 0.000050
2024-02-06 09:44:18,760 Epoch 2406: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-06 09:44:18,760 EPOCH 2407
2024-02-06 09:44:29,795 Epoch 2407: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-06 09:44:29,796 EPOCH 2408
2024-02-06 09:44:40,211 Epoch 2408: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 09:44:40,212 EPOCH 2409
2024-02-06 09:44:51,072 Epoch 2409: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-06 09:44:51,073 EPOCH 2410
2024-02-06 09:45:01,925 Epoch 2410: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-06 09:45:01,926 EPOCH 2411
2024-02-06 09:45:12,800 Epoch 2411: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-06 09:45:12,801 EPOCH 2412
2024-02-06 09:45:21,102 [Epoch: 2412 Step: 00041000] Batch Recognition Loss:   0.000189 => Gls Tokens per Sec:      971 || Batch Translation Loss:   0.039479 => Txt Tokens per Sec:     2719 || Lr: 0.000050
2024-02-06 09:45:23,294 Epoch 2412: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-06 09:45:23,294 EPOCH 2413
2024-02-06 09:45:33,943 Epoch 2413: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-06 09:45:33,943 EPOCH 2414
2024-02-06 09:45:44,822 Epoch 2414: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-06 09:45:44,823 EPOCH 2415
2024-02-06 09:45:55,565 Epoch 2415: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-06 09:45:55,565 EPOCH 2416
2024-02-06 09:46:06,124 Epoch 2416: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-06 09:46:06,124 EPOCH 2417
2024-02-06 09:46:16,715 Epoch 2417: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-06 09:46:16,716 EPOCH 2418
2024-02-06 09:46:25,920 [Epoch: 2418 Step: 00041100] Batch Recognition Loss:   0.000268 => Gls Tokens per Sec:      737 || Batch Translation Loss:   0.019317 => Txt Tokens per Sec:     2110 || Lr: 0.000050
2024-02-06 09:46:27,214 Epoch 2418: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-06 09:46:27,215 EPOCH 2419
2024-02-06 09:46:37,770 Epoch 2419: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-06 09:46:37,771 EPOCH 2420
2024-02-06 09:46:48,478 Epoch 2420: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 09:46:48,479 EPOCH 2421
2024-02-06 09:46:59,155 Epoch 2421: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-06 09:46:59,156 EPOCH 2422
2024-02-06 09:47:09,912 Epoch 2422: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 09:47:09,913 EPOCH 2423
2024-02-06 09:47:20,498 Epoch 2423: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-06 09:47:20,498 EPOCH 2424
2024-02-06 09:47:27,246 [Epoch: 2424 Step: 00041200] Batch Recognition Loss:   0.000210 => Gls Tokens per Sec:      854 || Batch Translation Loss:   0.009028 => Txt Tokens per Sec:     2437 || Lr: 0.000050
2024-02-06 09:47:31,202 Epoch 2424: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 09:47:31,203 EPOCH 2425
2024-02-06 09:47:41,803 Epoch 2425: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-06 09:47:41,804 EPOCH 2426
2024-02-06 09:47:52,648 Epoch 2426: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 09:47:52,649 EPOCH 2427
2024-02-06 09:48:03,213 Epoch 2427: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-06 09:48:03,214 EPOCH 2428
2024-02-06 09:48:13,906 Epoch 2428: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 09:48:13,907 EPOCH 2429
2024-02-06 09:48:24,533 Epoch 2429: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-06 09:48:24,534 EPOCH 2430
2024-02-06 09:48:29,370 [Epoch: 2430 Step: 00041300] Batch Recognition Loss:   0.001494 => Gls Tokens per Sec:      926 || Batch Translation Loss:   0.017545 => Txt Tokens per Sec:     2641 || Lr: 0.000050
2024-02-06 09:48:35,269 Epoch 2430: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 09:48:35,269 EPOCH 2431
2024-02-06 09:48:45,899 Epoch 2431: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 09:48:45,899 EPOCH 2432
2024-02-06 09:48:56,444 Epoch 2432: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-06 09:48:56,445 EPOCH 2433
2024-02-06 09:49:07,185 Epoch 2433: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-06 09:49:07,185 EPOCH 2434
2024-02-06 09:49:17,671 Epoch 2434: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-06 09:49:17,671 EPOCH 2435
2024-02-06 09:49:28,370 Epoch 2435: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 09:49:28,370 EPOCH 2436
2024-02-06 09:49:29,133 [Epoch: 2436 Step: 00041400] Batch Recognition Loss:   0.000123 => Gls Tokens per Sec:     4203 || Batch Translation Loss:   0.014238 => Txt Tokens per Sec:    10029 || Lr: 0.000050
2024-02-06 09:49:38,889 Epoch 2436: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 09:49:38,890 EPOCH 2437
2024-02-06 09:49:49,416 Epoch 2437: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.26 
2024-02-06 09:49:49,416 EPOCH 2438
2024-02-06 09:50:00,293 Epoch 2438: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.24 
2024-02-06 09:50:00,294 EPOCH 2439
2024-02-06 09:50:11,039 Epoch 2439: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.23 
2024-02-06 09:50:11,040 EPOCH 2440
2024-02-06 09:50:21,592 Epoch 2440: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.23 
2024-02-06 09:50:21,593 EPOCH 2441
2024-02-06 09:50:32,176 Epoch 2441: Total Training Recognition Loss 0.10  Total Training Translation Loss 0.23 
2024-02-06 09:50:32,176 EPOCH 2442
2024-02-06 09:50:36,078 [Epoch: 2442 Step: 00041500] Batch Recognition Loss:   0.000199 => Gls Tokens per Sec:      492 || Batch Translation Loss:   0.013139 => Txt Tokens per Sec:     1610 || Lr: 0.000050
2024-02-06 09:50:42,896 Epoch 2442: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.23 
2024-02-06 09:50:42,896 EPOCH 2443
2024-02-06 09:50:53,505 Epoch 2443: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.26 
2024-02-06 09:50:53,506 EPOCH 2444
2024-02-06 09:51:04,145 Epoch 2444: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.24 
2024-02-06 09:51:04,146 EPOCH 2445
2024-02-06 09:51:15,003 Epoch 2445: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 09:51:15,004 EPOCH 2446
2024-02-06 09:51:25,455 Epoch 2446: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 09:51:25,456 EPOCH 2447
2024-02-06 09:51:36,397 Epoch 2447: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 09:51:36,397 EPOCH 2448
2024-02-06 09:51:36,648 [Epoch: 2448 Step: 00041600] Batch Recognition Loss:   0.000149 => Gls Tokens per Sec:     2570 || Batch Translation Loss:   0.013115 => Txt Tokens per Sec:     7602 || Lr: 0.000050
2024-02-06 09:51:46,872 Epoch 2448: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.26 
2024-02-06 09:51:46,872 EPOCH 2449
2024-02-06 09:51:57,532 Epoch 2449: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-06 09:51:57,532 EPOCH 2450
2024-02-06 09:52:08,066 Epoch 2450: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 09:52:08,067 EPOCH 2451
2024-02-06 09:52:18,949 Epoch 2451: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 09:52:18,950 EPOCH 2452
2024-02-06 09:52:29,770 Epoch 2452: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-06 09:52:29,771 EPOCH 2453
2024-02-06 09:52:38,129 [Epoch: 2453 Step: 00041700] Batch Recognition Loss:   0.000510 => Gls Tokens per Sec:     1194 || Batch Translation Loss:   0.053220 => Txt Tokens per Sec:     3266 || Lr: 0.000050
2024-02-06 09:52:39,919 Epoch 2453: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-06 09:52:39,920 EPOCH 2454
2024-02-06 09:52:50,688 Epoch 2454: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-06 09:52:50,689 EPOCH 2455
2024-02-06 09:53:01,493 Epoch 2455: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.14 
2024-02-06 09:53:01,494 EPOCH 2456
2024-02-06 09:53:12,583 Epoch 2456: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.66 
2024-02-06 09:53:12,583 EPOCH 2457
2024-02-06 09:53:23,461 Epoch 2457: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.14 
2024-02-06 09:53:23,461 EPOCH 2458
2024-02-06 09:53:34,341 Epoch 2458: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.96 
2024-02-06 09:53:34,342 EPOCH 2459
2024-02-06 09:53:44,386 [Epoch: 2459 Step: 00041800] Batch Recognition Loss:   0.000823 => Gls Tokens per Sec:      866 || Batch Translation Loss:   0.041973 => Txt Tokens per Sec:     2436 || Lr: 0.000050
2024-02-06 09:53:45,099 Epoch 2459: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.80 
2024-02-06 09:53:45,099 EPOCH 2460
2024-02-06 09:53:55,684 Epoch 2460: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-06 09:53:55,684 EPOCH 2461
2024-02-06 09:54:06,326 Epoch 2461: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-06 09:54:06,327 EPOCH 2462
2024-02-06 09:54:16,957 Epoch 2462: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-06 09:54:16,958 EPOCH 2463
2024-02-06 09:54:27,463 Epoch 2463: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-06 09:54:27,463 EPOCH 2464
2024-02-06 09:54:38,132 Epoch 2464: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-06 09:54:38,133 EPOCH 2465
2024-02-06 09:54:43,417 [Epoch: 2465 Step: 00041900] Batch Recognition Loss:   0.000244 => Gls Tokens per Sec:     1454 || Batch Translation Loss:   0.021941 => Txt Tokens per Sec:     3880 || Lr: 0.000050
2024-02-06 09:54:48,598 Epoch 2465: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-06 09:54:48,599 EPOCH 2466
2024-02-06 09:54:59,467 Epoch 2466: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 09:54:59,468 EPOCH 2467
2024-02-06 09:55:10,110 Epoch 2467: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 09:55:10,111 EPOCH 2468
2024-02-06 09:55:20,734 Epoch 2468: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 09:55:20,734 EPOCH 2469
2024-02-06 09:55:31,313 Epoch 2469: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 09:55:31,314 EPOCH 2470
2024-02-06 09:55:41,979 Epoch 2470: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-06 09:55:41,980 EPOCH 2471
2024-02-06 09:55:47,341 [Epoch: 2471 Step: 00042000] Batch Recognition Loss:   0.000227 => Gls Tokens per Sec:     1194 || Batch Translation Loss:   0.019552 => Txt Tokens per Sec:     3450 || Lr: 0.000050
2024-02-06 09:56:27,663 Validation result at epoch 2471, step    42000: duration: 40.3209s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.62329	Translation Loss: 98780.53906	PPL: 19268.24609
	Eval Metric: BLEU
	WER 2.82	(DEL: 0.00,	INS: 0.00,	SUB: 2.82)
	BLEU-4 0.00	(BLEU-1: 10.10,	BLEU-2: 2.95,	BLEU-3: 1.00,	BLEU-4: 0.00)
	CHRF 16.67	ROUGE 8.61
2024-02-06 09:56:27,665 Logging Recognition and Translation Outputs
2024-02-06 09:56:27,665 ========================================================================================================================
2024-02-06 09:56:27,665 Logging Sequence: 154_94.00
2024-02-06 09:56:27,665 	Gloss Reference :	A B+C+D+E
2024-02-06 09:56:27,665 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 09:56:27,666 	Gloss Alignment :	         
2024-02-06 09:56:27,666 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 09:56:27,668 	Text Reference  :	*** the ipl will  also be  held in   uae   from september 19 to  october   15     
2024-02-06 09:56:27,668 	Text Hypothesis :	for the *** first time the high room rates are  because   of his religious beliefs
2024-02-06 09:56:27,668 	Text Alignment  :	I       D   S     S    S   S    S    S     S    S         S  S   S         S      
2024-02-06 09:56:27,668 ========================================================================================================================
2024-02-06 09:56:27,668 Logging Sequence: 118_2.00
2024-02-06 09:56:27,669 	Gloss Reference :	A B+C+D+E
2024-02-06 09:56:27,669 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 09:56:27,669 	Gloss Alignment :	         
2024-02-06 09:56:27,669 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 09:56:27,670 	Text Reference  :	yesterday was a very exciting day   people across   the world were watching 
2024-02-06 09:56:27,670 	Text Hypothesis :	********* *** * **** the      video has    garnered a   huge  fan  following
2024-02-06 09:56:27,670 	Text Alignment  :	D         D   D D    S        S     S      S        S   S     S    S        
2024-02-06 09:56:27,670 ========================================================================================================================
2024-02-06 09:56:27,670 Logging Sequence: 165_453.00
2024-02-06 09:56:27,671 	Gloss Reference :	A B+C+D+E
2024-02-06 09:56:27,671 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 09:56:27,671 	Gloss Alignment :	         
2024-02-06 09:56:27,671 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 09:56:27,672 	Text Reference  :	icc did not  agree to sehwag' decision of wearing a numberless jersey 
2024-02-06 09:56:27,672 	Text Hypothesis :	*** he  came out   to ******* ******** ** ******* * retire     someday
2024-02-06 09:56:27,672 	Text Alignment  :	D   S   S    S        D       D        D  D       D S          S      
2024-02-06 09:56:27,672 ========================================================================================================================
2024-02-06 09:56:27,672 Logging Sequence: 126_163.00
2024-02-06 09:56:27,672 	Gloss Reference :	A B+C+D+E
2024-02-06 09:56:27,673 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 09:56:27,673 	Gloss Alignment :	         
2024-02-06 09:56:27,673 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 09:56:27,674 	Text Reference  :	* your hard work  has helped secure a   medal at  the        tokyo      olympics
2024-02-06 09:56:27,674 	Text Hypothesis :	i am   very grate to  my     fans   and media for constantly supporting me      
2024-02-06 09:56:27,674 	Text Alignment  :	I S    S    S     S   S      S      S   S     S   S          S          S       
2024-02-06 09:56:27,675 ========================================================================================================================
2024-02-06 09:56:27,675 Logging Sequence: 84_2.00
2024-02-06 09:56:27,675 	Gloss Reference :	A B+C+D+E
2024-02-06 09:56:27,675 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 09:56:27,676 	Gloss Alignment :	         
2024-02-06 09:56:27,676 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 09:56:27,677 	Text Reference  :	the 2022 fifa football world         cup  is  going on    in qatar from 20th november 2022 to 18th   december 2022 
2024-02-06 09:56:27,677 	Text Hypothesis :	the **** **** sports   personalities like our 5     times in qatar from **** ******** **** ** around the      world
2024-02-06 09:56:27,677 	Text Alignment  :	    D    D    S        S             S    S   S     S                   D    D        D    D  S      S        S    
2024-02-06 09:56:27,678 ========================================================================================================================
2024-02-06 09:56:33,073 Epoch 2471: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-06 09:56:33,073 EPOCH 2472
2024-02-06 09:56:44,148 Epoch 2472: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-06 09:56:44,149 EPOCH 2473
2024-02-06 09:56:55,024 Epoch 2473: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 09:56:55,024 EPOCH 2474
2024-02-06 09:57:05,624 Epoch 2474: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 09:57:05,624 EPOCH 2475
2024-02-06 09:57:16,291 Epoch 2475: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 09:57:16,292 EPOCH 2476
2024-02-06 09:57:26,619 Epoch 2476: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-06 09:57:26,619 EPOCH 2477
2024-02-06 09:57:32,087 [Epoch: 2477 Step: 00042100] Batch Recognition Loss:   0.000168 => Gls Tokens per Sec:      889 || Batch Translation Loss:   0.016558 => Txt Tokens per Sec:     2435 || Lr: 0.000050
2024-02-06 09:57:37,408 Epoch 2477: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 09:57:37,408 EPOCH 2478
2024-02-06 09:57:47,832 Epoch 2478: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 09:57:47,833 EPOCH 2479
2024-02-06 09:57:58,447 Epoch 2479: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 09:57:58,448 EPOCH 2480
2024-02-06 09:58:08,392 Epoch 2480: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-06 09:58:08,392 EPOCH 2481
2024-02-06 09:58:19,066 Epoch 2481: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-06 09:58:19,067 EPOCH 2482
2024-02-06 09:58:29,860 Epoch 2482: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-06 09:58:29,861 EPOCH 2483
2024-02-06 09:58:33,605 [Epoch: 2483 Step: 00042200] Batch Recognition Loss:   0.000143 => Gls Tokens per Sec:      956 || Batch Translation Loss:   0.018384 => Txt Tokens per Sec:     2778 || Lr: 0.000050
2024-02-06 09:58:40,465 Epoch 2483: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 09:58:40,466 EPOCH 2484
2024-02-06 09:58:51,277 Epoch 2484: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 09:58:51,277 EPOCH 2485
2024-02-06 09:59:01,911 Epoch 2485: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-06 09:59:01,912 EPOCH 2486
2024-02-06 09:59:12,544 Epoch 2486: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 09:59:12,544 EPOCH 2487
2024-02-06 09:59:23,127 Epoch 2487: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-06 09:59:23,128 EPOCH 2488
2024-02-06 09:59:33,941 Epoch 2488: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 09:59:33,941 EPOCH 2489
2024-02-06 09:59:36,749 [Epoch: 2489 Step: 00042300] Batch Recognition Loss:   0.000485 => Gls Tokens per Sec:      912 || Batch Translation Loss:   0.023720 => Txt Tokens per Sec:     2895 || Lr: 0.000050
2024-02-06 09:59:44,525 Epoch 2489: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-06 09:59:44,525 EPOCH 2490
2024-02-06 09:59:55,214 Epoch 2490: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 09:59:55,214 EPOCH 2491
2024-02-06 10:00:05,888 Epoch 2491: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-06 10:00:05,889 EPOCH 2492
2024-02-06 10:00:16,802 Epoch 2492: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-06 10:00:16,802 EPOCH 2493
2024-02-06 10:00:27,401 Epoch 2493: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-06 10:00:27,402 EPOCH 2494
2024-02-06 10:00:38,215 Epoch 2494: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 10:00:38,215 EPOCH 2495
2024-02-06 10:00:40,380 [Epoch: 2495 Step: 00042400] Batch Recognition Loss:   0.000356 => Gls Tokens per Sec:      592 || Batch Translation Loss:   0.021325 => Txt Tokens per Sec:     1796 || Lr: 0.000050
2024-02-06 10:00:48,905 Epoch 2495: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 10:00:48,905 EPOCH 2496
2024-02-06 10:00:59,907 Epoch 2496: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 10:00:59,908 EPOCH 2497
2024-02-06 10:01:10,780 Epoch 2497: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 10:01:10,781 EPOCH 2498
2024-02-06 10:01:21,452 Epoch 2498: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 10:01:21,453 EPOCH 2499
2024-02-06 10:01:32,268 Epoch 2499: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-06 10:01:32,268 EPOCH 2500
2024-02-06 10:01:43,148 [Epoch: 2500 Step: 00042500] Batch Recognition Loss:   0.000392 => Gls Tokens per Sec:      976 || Batch Translation Loss:   0.030346 => Txt Tokens per Sec:     2701 || Lr: 0.000050
2024-02-06 10:01:43,149 Epoch 2500: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-06 10:01:43,149 EPOCH 2501
2024-02-06 10:01:53,762 Epoch 2501: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-06 10:01:53,763 EPOCH 2502
2024-02-06 10:02:04,431 Epoch 2502: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 10:02:04,431 EPOCH 2503
2024-02-06 10:02:15,317 Epoch 2503: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 10:02:15,318 EPOCH 2504
2024-02-06 10:02:26,064 Epoch 2504: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-06 10:02:26,065 EPOCH 2505
2024-02-06 10:02:36,964 Epoch 2505: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 10:02:36,965 EPOCH 2506
2024-02-06 10:02:47,512 [Epoch: 2506 Step: 00042600] Batch Recognition Loss:   0.000293 => Gls Tokens per Sec:      886 || Batch Translation Loss:   0.022584 => Txt Tokens per Sec:     2517 || Lr: 0.000050
2024-02-06 10:02:47,925 Epoch 2506: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-06 10:02:47,925 EPOCH 2507
2024-02-06 10:02:58,746 Epoch 2507: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-06 10:02:58,747 EPOCH 2508
2024-02-06 10:03:09,613 Epoch 2508: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-06 10:03:09,613 EPOCH 2509
2024-02-06 10:03:20,451 Epoch 2509: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-06 10:03:20,452 EPOCH 2510
2024-02-06 10:03:31,039 Epoch 2510: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-06 10:03:31,040 EPOCH 2511
2024-02-06 10:03:41,825 Epoch 2511: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-06 10:03:41,825 EPOCH 2512
2024-02-06 10:03:51,692 [Epoch: 2512 Step: 00042700] Batch Recognition Loss:   0.000707 => Gls Tokens per Sec:      817 || Batch Translation Loss:   0.069245 => Txt Tokens per Sec:     2308 || Lr: 0.000050
2024-02-06 10:03:52,600 Epoch 2512: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.15 
2024-02-06 10:03:52,600 EPOCH 2513
2024-02-06 10:04:03,348 Epoch 2513: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.08 
2024-02-06 10:04:03,348 EPOCH 2514
2024-02-06 10:04:14,068 Epoch 2514: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.81 
2024-02-06 10:04:14,068 EPOCH 2515
2024-02-06 10:04:24,658 Epoch 2515: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-06 10:04:24,659 EPOCH 2516
2024-02-06 10:04:35,529 Epoch 2516: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-06 10:04:35,530 EPOCH 2517
2024-02-06 10:04:46,168 Epoch 2517: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.39 
2024-02-06 10:04:46,169 EPOCH 2518
2024-02-06 10:04:55,442 [Epoch: 2518 Step: 00042800] Batch Recognition Loss:   0.000484 => Gls Tokens per Sec:      731 || Batch Translation Loss:   0.025912 => Txt Tokens per Sec:     2102 || Lr: 0.000050
2024-02-06 10:04:56,792 Epoch 2518: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-06 10:04:56,792 EPOCH 2519
2024-02-06 10:05:07,365 Epoch 2519: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-06 10:05:07,365 EPOCH 2520
2024-02-06 10:05:18,035 Epoch 2520: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-06 10:05:18,036 EPOCH 2521
2024-02-06 10:05:29,002 Epoch 2521: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 10:05:29,002 EPOCH 2522
2024-02-06 10:05:39,848 Epoch 2522: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 10:05:39,848 EPOCH 2523
2024-02-06 10:05:50,454 Epoch 2523: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 10:05:50,454 EPOCH 2524
2024-02-06 10:05:54,094 [Epoch: 2524 Step: 00042900] Batch Recognition Loss:   0.000244 => Gls Tokens per Sec:     1583 || Batch Translation Loss:   0.027607 => Txt Tokens per Sec:     4261 || Lr: 0.000050
2024-02-06 10:06:00,998 Epoch 2524: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 10:06:00,999 EPOCH 2525
2024-02-06 10:06:11,698 Epoch 2525: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 10:06:11,699 EPOCH 2526
2024-02-06 10:06:22,360 Epoch 2526: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 10:06:22,361 EPOCH 2527
2024-02-06 10:06:33,294 Epoch 2527: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 10:06:33,295 EPOCH 2528
2024-02-06 10:06:44,154 Epoch 2528: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 10:06:44,155 EPOCH 2529
2024-02-06 10:06:54,998 Epoch 2529: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 10:06:54,998 EPOCH 2530
2024-02-06 10:07:01,963 [Epoch: 2530 Step: 00043000] Batch Recognition Loss:   0.000260 => Gls Tokens per Sec:      606 || Batch Translation Loss:   0.018126 => Txt Tokens per Sec:     1724 || Lr: 0.000050
2024-02-06 10:07:05,853 Epoch 2530: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 10:07:05,853 EPOCH 2531
2024-02-06 10:07:16,410 Epoch 2531: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-06 10:07:16,411 EPOCH 2532
2024-02-06 10:07:27,213 Epoch 2532: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-06 10:07:27,214 EPOCH 2533
2024-02-06 10:07:38,032 Epoch 2533: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-06 10:07:38,033 EPOCH 2534
2024-02-06 10:07:48,962 Epoch 2534: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-06 10:07:48,962 EPOCH 2535
2024-02-06 10:07:59,452 Epoch 2535: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-06 10:07:59,453 EPOCH 2536
2024-02-06 10:08:02,594 [Epoch: 2536 Step: 00043100] Batch Recognition Loss:   0.000144 => Gls Tokens per Sec:     1019 || Batch Translation Loss:   0.086526 => Txt Tokens per Sec:     3002 || Lr: 0.000050
2024-02-06 10:08:10,301 Epoch 2536: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-06 10:08:10,301 EPOCH 2537
2024-02-06 10:08:20,916 Epoch 2537: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-06 10:08:20,917 EPOCH 2538
2024-02-06 10:08:31,736 Epoch 2538: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-06 10:08:31,736 EPOCH 2539
2024-02-06 10:08:42,553 Epoch 2539: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-06 10:08:42,554 EPOCH 2540
2024-02-06 10:08:53,081 Epoch 2540: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-06 10:08:53,081 EPOCH 2541
2024-02-06 10:09:03,759 Epoch 2541: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.35 
2024-02-06 10:09:03,760 EPOCH 2542
2024-02-06 10:09:04,376 [Epoch: 2542 Step: 00043200] Batch Recognition Loss:   0.000184 => Gls Tokens per Sec:     3122 || Batch Translation Loss:   0.019084 => Txt Tokens per Sec:     8433 || Lr: 0.000050
2024-02-06 10:09:14,374 Epoch 2542: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 10:09:14,375 EPOCH 2543
2024-02-06 10:09:25,115 Epoch 2543: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-06 10:09:25,116 EPOCH 2544
2024-02-06 10:09:35,719 Epoch 2544: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-06 10:09:35,719 EPOCH 2545
2024-02-06 10:09:46,472 Epoch 2545: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-06 10:09:46,472 EPOCH 2546
2024-02-06 10:09:57,313 Epoch 2546: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-06 10:09:57,314 EPOCH 2547
2024-02-06 10:10:07,911 Epoch 2547: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-06 10:10:07,911 EPOCH 2548
2024-02-06 10:10:08,119 [Epoch: 2548 Step: 00043300] Batch Recognition Loss:   0.000216 => Gls Tokens per Sec:     3092 || Batch Translation Loss:   0.019859 => Txt Tokens per Sec:     8367 || Lr: 0.000050
2024-02-06 10:10:18,594 Epoch 2548: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-06 10:10:18,594 EPOCH 2549
2024-02-06 10:10:29,229 Epoch 2549: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-06 10:10:29,230 EPOCH 2550
2024-02-06 10:10:39,952 Epoch 2550: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-06 10:10:39,952 EPOCH 2551
2024-02-06 10:10:50,590 Epoch 2551: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-06 10:10:50,591 EPOCH 2552
2024-02-06 10:11:01,657 Epoch 2552: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-06 10:11:01,657 EPOCH 2553
2024-02-06 10:11:10,232 [Epoch: 2553 Step: 00043400] Batch Recognition Loss:   0.000450 => Gls Tokens per Sec:     1164 || Batch Translation Loss:   0.025611 => Txt Tokens per Sec:     3188 || Lr: 0.000050
2024-02-06 10:11:11,967 Epoch 2553: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 10:11:11,968 EPOCH 2554
2024-02-06 10:11:22,639 Epoch 2554: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 10:11:22,640 EPOCH 2555
2024-02-06 10:11:33,543 Epoch 2555: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 10:11:33,544 EPOCH 2556
2024-02-06 10:11:44,479 Epoch 2556: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 10:11:44,479 EPOCH 2557
2024-02-06 10:11:55,366 Epoch 2557: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-06 10:11:55,366 EPOCH 2558
2024-02-06 10:12:06,336 Epoch 2558: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 10:12:06,336 EPOCH 2559
2024-02-06 10:12:13,866 [Epoch: 2559 Step: 00043500] Batch Recognition Loss:   0.000626 => Gls Tokens per Sec:     1190 || Batch Translation Loss:   0.022110 => Txt Tokens per Sec:     3322 || Lr: 0.000050
2024-02-06 10:12:16,794 Epoch 2559: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 10:12:16,794 EPOCH 2560
2024-02-06 10:12:27,654 Epoch 2560: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 10:12:27,655 EPOCH 2561
2024-02-06 10:12:38,083 Epoch 2561: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-06 10:12:38,084 EPOCH 2562
2024-02-06 10:12:48,513 Epoch 2562: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-06 10:12:48,513 EPOCH 2563
2024-02-06 10:12:59,414 Epoch 2563: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-06 10:12:59,414 EPOCH 2564
2024-02-06 10:13:10,088 Epoch 2564: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-06 10:13:10,089 EPOCH 2565
2024-02-06 10:13:18,322 [Epoch: 2565 Step: 00043600] Batch Recognition Loss:   0.000275 => Gls Tokens per Sec:      901 || Batch Translation Loss:   0.065316 => Txt Tokens per Sec:     2458 || Lr: 0.000050
2024-02-06 10:13:20,962 Epoch 2565: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-06 10:13:20,962 EPOCH 2566
2024-02-06 10:13:31,794 Epoch 2566: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-06 10:13:31,795 EPOCH 2567
2024-02-06 10:13:42,257 Epoch 2567: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-06 10:13:42,258 EPOCH 2568
2024-02-06 10:13:53,048 Epoch 2568: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-06 10:13:53,049 EPOCH 2569
2024-02-06 10:14:03,549 Epoch 2569: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-06 10:14:03,550 EPOCH 2570
2024-02-06 10:14:14,283 Epoch 2570: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-06 10:14:14,284 EPOCH 2571
2024-02-06 10:14:20,333 [Epoch: 2571 Step: 00043700] Batch Recognition Loss:   0.000163 => Gls Tokens per Sec:     1015 || Batch Translation Loss:   0.016745 => Txt Tokens per Sec:     2828 || Lr: 0.000050
2024-02-06 10:14:24,850 Epoch 2571: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-06 10:14:24,850 EPOCH 2572
2024-02-06 10:14:35,408 Epoch 2572: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.36 
2024-02-06 10:14:35,408 EPOCH 2573
2024-02-06 10:14:46,146 Epoch 2573: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-06 10:14:46,147 EPOCH 2574
2024-02-06 10:14:56,984 Epoch 2574: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-06 10:14:56,985 EPOCH 2575
2024-02-06 10:15:07,851 Epoch 2575: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-06 10:15:07,852 EPOCH 2576
2024-02-06 10:15:18,615 Epoch 2576: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-06 10:15:18,615 EPOCH 2577
2024-02-06 10:15:24,148 [Epoch: 2577 Step: 00043800] Batch Recognition Loss:   0.000902 => Gls Tokens per Sec:      878 || Batch Translation Loss:   0.020811 => Txt Tokens per Sec:     2421 || Lr: 0.000050
2024-02-06 10:15:29,202 Epoch 2577: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-06 10:15:29,202 EPOCH 2578
2024-02-06 10:15:39,574 Epoch 2578: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-06 10:15:39,575 EPOCH 2579
2024-02-06 10:15:50,011 Epoch 2579: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-06 10:15:50,012 EPOCH 2580
2024-02-06 10:16:00,801 Epoch 2580: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 10:16:00,801 EPOCH 2581
2024-02-06 10:16:11,189 Epoch 2581: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-06 10:16:11,189 EPOCH 2582
2024-02-06 10:16:22,025 Epoch 2582: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-06 10:16:22,025 EPOCH 2583
2024-02-06 10:16:24,882 [Epoch: 2583 Step: 00043900] Batch Recognition Loss:   0.000163 => Gls Tokens per Sec:     1345 || Batch Translation Loss:   0.019983 => Txt Tokens per Sec:     4073 || Lr: 0.000050
2024-02-06 10:16:32,556 Epoch 2583: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-06 10:16:32,556 EPOCH 2584
2024-02-06 10:16:43,355 Epoch 2584: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-06 10:16:43,356 EPOCH 2585
2024-02-06 10:16:54,035 Epoch 2585: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-06 10:16:54,036 EPOCH 2586
2024-02-06 10:17:04,394 Epoch 2586: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-06 10:17:04,394 EPOCH 2587
2024-02-06 10:17:15,317 Epoch 2587: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-06 10:17:15,317 EPOCH 2588
2024-02-06 10:17:25,880 Epoch 2588: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-06 10:17:25,881 EPOCH 2589
2024-02-06 10:17:28,476 [Epoch: 2589 Step: 00044000] Batch Recognition Loss:   0.000204 => Gls Tokens per Sec:      987 || Batch Translation Loss:   0.023316 => Txt Tokens per Sec:     2755 || Lr: 0.000050
2024-02-06 10:18:09,182 Validation result at epoch 2589, step    44000: duration: 40.7059s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.60208	Translation Loss: 98297.53906	PPL: 18360.78125
	Eval Metric: BLEU
	WER 2.97	(DEL: 0.00,	INS: 0.00,	SUB: 2.97)
	BLEU-4 0.50	(BLEU-1: 10.11,	BLEU-2: 3.01,	BLEU-3: 1.15,	BLEU-4: 0.50)
	CHRF 17.00	ROUGE 8.54
2024-02-06 10:18:09,184 Logging Recognition and Translation Outputs
2024-02-06 10:18:09,184 ========================================================================================================================
2024-02-06 10:18:09,185 Logging Sequence: 57_104.00
2024-02-06 10:18:09,185 	Gloss Reference :	A B+C+D+E
2024-02-06 10:18:09,185 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 10:18:09,185 	Gloss Alignment :	         
2024-02-06 10:18:09,185 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 10:18:09,188 	Text Reference  :	the next day kohli and kl rahul continued from where they  had    left and     displayed amazing batting performance without losing their wickets
2024-02-06 10:18:09,188 	Text Hypothesis :	*** **** *** ***** *** ** ***** ********* **** ms    dhoni played 3    players to        wear    their   innings     without losing any   wicket 
2024-02-06 10:18:09,188 	Text Alignment  :	D   D    D   D     D   D  D     D         D    S     S     S      S    S       S         S       S       S                          S     S      
2024-02-06 10:18:09,189 ========================================================================================================================
2024-02-06 10:18:09,189 Logging Sequence: 136_64.00
2024-02-06 10:18:09,189 	Gloss Reference :	A B+C+D+E
2024-02-06 10:18:09,189 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 10:18:09,189 	Gloss Alignment :	         
2024-02-06 10:18:09,189 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 10:18:09,190 	Text Reference  :	* ** in   all        she   has won   2  medals  
2024-02-06 10:18:09,190 	Text Hypothesis :	i am very particular about my  never my feelings
2024-02-06 10:18:09,190 	Text Alignment  :	I I  S    S          S     S   S     S  S       
2024-02-06 10:18:09,191 ========================================================================================================================
2024-02-06 10:18:09,191 Logging Sequence: 54_123.00
2024-02-06 10:18:09,192 	Gloss Reference :	A B+C+D+E
2024-02-06 10:18:09,192 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 10:18:09,192 	Gloss Alignment :	         
2024-02-06 10:18:09,192 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 10:18:09,193 	Text Reference  :	vips sponsors international cricket groups have already booked their     hotel  rooms
2024-02-06 10:18:09,193 	Text Hypothesis :	**** ******** however       he      had    not  given   a      chartered flight wow  
2024-02-06 10:18:09,194 	Text Alignment  :	D    D        S             S       S      S    S       S      S         S      S    
2024-02-06 10:18:09,194 ========================================================================================================================
2024-02-06 10:18:09,194 Logging Sequence: 168_115.00
2024-02-06 10:18:09,194 	Gloss Reference :	A B+C+D+E
2024-02-06 10:18:09,195 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 10:18:09,195 	Gloss Alignment :	         
2024-02-06 10:18:09,195 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 10:18:09,196 	Text Reference  :	**** ***** this   has     sparked a    major discussion on   social media
2024-02-06 10:18:09,196 	Text Hypothesis :	bcci chief sourav ganguly on      28th june  said       that the    team 
2024-02-06 10:18:09,196 	Text Alignment  :	I    I     S      S       S       S    S     S          S    S      S    
2024-02-06 10:18:09,197 ========================================================================================================================
2024-02-06 10:18:09,197 Logging Sequence: 121_132.00
2024-02-06 10:18:09,197 	Gloss Reference :	A B+C+D+E
2024-02-06 10:18:09,197 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 10:18:09,198 	Gloss Alignment :	         
2024-02-06 10:18:09,198 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 10:18:09,199 	Text Reference  :	******* which is   why    they will be retesting her  to check if   she       consumed any stamina enhancing drugs
2024-02-06 10:18:09,200 	Text Hypothesis :	however he    said people set  very be ********* held in a     same melbourne stadium  and may     be        held 
2024-02-06 10:18:09,200 	Text Alignment  :	I       S     S    S      S    S       D         S    S  S     S    S         S        S   S       S         S    
2024-02-06 10:18:09,200 ========================================================================================================================
2024-02-06 10:18:17,880 Epoch 2589: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-06 10:18:17,881 EPOCH 2590
2024-02-06 10:18:28,655 Epoch 2590: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-06 10:18:28,656 EPOCH 2591
2024-02-06 10:18:38,823 Epoch 2591: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-06 10:18:38,823 EPOCH 2592
2024-02-06 10:18:49,628 Epoch 2592: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-06 10:18:49,629 EPOCH 2593
2024-02-06 10:19:00,314 Epoch 2593: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-06 10:19:00,315 EPOCH 2594
2024-02-06 10:19:11,413 Epoch 2594: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-06 10:19:11,413 EPOCH 2595
2024-02-06 10:19:13,595 [Epoch: 2595 Step: 00044100] Batch Recognition Loss:   0.000471 => Gls Tokens per Sec:      587 || Batch Translation Loss:   0.027953 => Txt Tokens per Sec:     1881 || Lr: 0.000050
2024-02-06 10:19:22,330 Epoch 2595: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-06 10:19:22,331 EPOCH 2596
2024-02-06 10:19:33,097 Epoch 2596: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-06 10:19:33,098 EPOCH 2597
2024-02-06 10:19:43,951 Epoch 2597: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 10:19:43,952 EPOCH 2598
2024-02-06 10:19:54,446 Epoch 2598: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 10:19:54,446 EPOCH 2599
2024-02-06 10:20:05,366 Epoch 2599: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 10:20:05,366 EPOCH 2600
2024-02-06 10:20:16,139 [Epoch: 2600 Step: 00044200] Batch Recognition Loss:   0.000126 => Gls Tokens per Sec:      986 || Batch Translation Loss:   0.011441 => Txt Tokens per Sec:     2727 || Lr: 0.000050
2024-02-06 10:20:16,140 Epoch 2600: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 10:20:16,140 EPOCH 2601
2024-02-06 10:20:26,718 Epoch 2601: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 10:20:26,719 EPOCH 2602
2024-02-06 10:20:37,411 Epoch 2602: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 10:20:37,412 EPOCH 2603
2024-02-06 10:20:48,178 Epoch 2603: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 10:20:48,178 EPOCH 2604
2024-02-06 10:20:58,731 Epoch 2604: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 10:20:58,732 EPOCH 2605
2024-02-06 10:21:09,402 Epoch 2605: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-06 10:21:09,403 EPOCH 2606
2024-02-06 10:21:18,261 [Epoch: 2606 Step: 00044300] Batch Recognition Loss:   0.000125 => Gls Tokens per Sec:     1055 || Batch Translation Loss:   0.011128 => Txt Tokens per Sec:     2908 || Lr: 0.000050
2024-02-06 10:21:20,163 Epoch 2606: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-06 10:21:20,163 EPOCH 2607
2024-02-06 10:21:30,880 Epoch 2607: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-06 10:21:30,880 EPOCH 2608
2024-02-06 10:21:41,811 Epoch 2608: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-06 10:21:41,812 EPOCH 2609
2024-02-06 10:21:52,398 Epoch 2609: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-06 10:21:52,399 EPOCH 2610
2024-02-06 10:22:02,747 Epoch 2610: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-06 10:22:02,748 EPOCH 2611
2024-02-06 10:22:12,895 Epoch 2611: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 10:22:12,896 EPOCH 2612
2024-02-06 10:22:22,853 [Epoch: 2612 Step: 00044400] Batch Recognition Loss:   0.000178 => Gls Tokens per Sec:      810 || Batch Translation Loss:   0.031543 => Txt Tokens per Sec:     2306 || Lr: 0.000050
2024-02-06 10:22:23,664 Epoch 2612: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 10:22:23,665 EPOCH 2613
2024-02-06 10:22:34,272 Epoch 2613: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-06 10:22:34,272 EPOCH 2614
2024-02-06 10:22:44,834 Epoch 2614: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 10:22:44,835 EPOCH 2615
2024-02-06 10:22:55,414 Epoch 2615: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 10:22:55,415 EPOCH 2616
2024-02-06 10:23:06,088 Epoch 2616: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 10:23:06,089 EPOCH 2617
2024-02-06 10:23:16,643 Epoch 2617: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 10:23:16,644 EPOCH 2618
2024-02-06 10:23:26,039 [Epoch: 2618 Step: 00044500] Batch Recognition Loss:   0.000385 => Gls Tokens per Sec:      722 || Batch Translation Loss:   0.020680 => Txt Tokens per Sec:     1987 || Lr: 0.000050
2024-02-06 10:23:27,519 Epoch 2618: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-06 10:23:27,520 EPOCH 2619
2024-02-06 10:23:38,028 Epoch 2619: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-06 10:23:38,029 EPOCH 2620
2024-02-06 10:23:48,122 Epoch 2620: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-06 10:23:48,122 EPOCH 2621
2024-02-06 10:23:58,904 Epoch 2621: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-06 10:23:58,905 EPOCH 2622
2024-02-06 10:24:09,647 Epoch 2622: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 10:24:09,648 EPOCH 2623
2024-02-06 10:24:20,241 Epoch 2623: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-06 10:24:20,242 EPOCH 2624
2024-02-06 10:24:27,713 [Epoch: 2624 Step: 00044600] Batch Recognition Loss:   0.000145 => Gls Tokens per Sec:      736 || Batch Translation Loss:   0.021191 => Txt Tokens per Sec:     1962 || Lr: 0.000050
2024-02-06 10:24:31,172 Epoch 2624: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-06 10:24:31,172 EPOCH 2625
2024-02-06 10:24:41,575 Epoch 2625: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-06 10:24:41,576 EPOCH 2626
2024-02-06 10:24:52,274 Epoch 2626: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-06 10:24:52,275 EPOCH 2627
2024-02-06 10:25:02,784 Epoch 2627: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-06 10:25:02,785 EPOCH 2628
2024-02-06 10:25:13,491 Epoch 2628: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 10:25:13,491 EPOCH 2629
2024-02-06 10:25:24,065 Epoch 2629: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-06 10:25:24,066 EPOCH 2630
2024-02-06 10:25:27,100 [Epoch: 2630 Step: 00044700] Batch Recognition Loss:   0.000145 => Gls Tokens per Sec:     1477 || Batch Translation Loss:   0.011841 => Txt Tokens per Sec:     4120 || Lr: 0.000050
2024-02-06 10:25:34,557 Epoch 2630: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-06 10:25:34,557 EPOCH 2631
2024-02-06 10:25:45,253 Epoch 2631: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-06 10:25:45,253 EPOCH 2632
2024-02-06 10:25:55,978 Epoch 2632: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-06 10:25:55,978 EPOCH 2633
2024-02-06 10:26:06,719 Epoch 2633: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-06 10:26:06,720 EPOCH 2634
2024-02-06 10:26:17,489 Epoch 2634: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-06 10:26:17,489 EPOCH 2635
2024-02-06 10:26:28,145 Epoch 2635: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-06 10:26:28,146 EPOCH 2636
2024-02-06 10:26:31,384 [Epoch: 2636 Step: 00044800] Batch Recognition Loss:   0.000224 => Gls Tokens per Sec:      908 || Batch Translation Loss:   0.030843 => Txt Tokens per Sec:     2527 || Lr: 0.000050
2024-02-06 10:26:38,665 Epoch 2636: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-06 10:26:38,666 EPOCH 2637
2024-02-06 10:26:49,391 Epoch 2637: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 10:26:49,391 EPOCH 2638
2024-02-06 10:27:00,114 Epoch 2638: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 10:27:00,114 EPOCH 2639
2024-02-06 10:27:10,570 Epoch 2639: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 10:27:10,570 EPOCH 2640
2024-02-06 10:27:21,276 Epoch 2640: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-06 10:27:21,277 EPOCH 2641
2024-02-06 10:27:32,041 Epoch 2641: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-06 10:27:32,041 EPOCH 2642
2024-02-06 10:27:34,732 [Epoch: 2642 Step: 00044900] Batch Recognition Loss:   0.000470 => Gls Tokens per Sec:      617 || Batch Translation Loss:   0.014676 => Txt Tokens per Sec:     1608 || Lr: 0.000050
2024-02-06 10:27:42,704 Epoch 2642: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 10:27:42,705 EPOCH 2643
2024-02-06 10:27:53,609 Epoch 2643: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 10:27:53,610 EPOCH 2644
2024-02-06 10:28:04,237 Epoch 2644: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 10:28:04,237 EPOCH 2645
2024-02-06 10:28:14,976 Epoch 2645: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-06 10:28:14,976 EPOCH 2646
2024-02-06 10:28:25,585 Epoch 2646: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-06 10:28:25,586 EPOCH 2647
2024-02-06 10:28:36,216 Epoch 2647: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-06 10:28:36,217 EPOCH 2648
2024-02-06 10:28:38,605 [Epoch: 2648 Step: 00045000] Batch Recognition Loss:   0.000589 => Gls Tokens per Sec:      159 || Batch Translation Loss:   0.013532 => Txt Tokens per Sec:      569 || Lr: 0.000050
2024-02-06 10:28:46,983 Epoch 2648: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-06 10:28:46,984 EPOCH 2649
2024-02-06 10:28:57,720 Epoch 2649: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-06 10:28:57,720 EPOCH 2650
2024-02-06 10:29:07,954 Epoch 2650: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 10:29:07,955 EPOCH 2651
2024-02-06 10:29:18,714 Epoch 2651: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 10:29:18,715 EPOCH 2652
2024-02-06 10:29:29,431 Epoch 2652: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-06 10:29:29,431 EPOCH 2653
2024-02-06 10:29:40,112 [Epoch: 2653 Step: 00045100] Batch Recognition Loss:   0.000158 => Gls Tokens per Sec:      935 || Batch Translation Loss:   0.034679 => Txt Tokens per Sec:     2656 || Lr: 0.000050
2024-02-06 10:29:40,217 Epoch 2653: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-06 10:29:40,217 EPOCH 2654
2024-02-06 10:29:50,903 Epoch 2654: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-06 10:29:50,904 EPOCH 2655
2024-02-06 10:30:01,527 Epoch 2655: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-06 10:30:01,528 EPOCH 2656
2024-02-06 10:30:12,044 Epoch 2656: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-06 10:30:12,044 EPOCH 2657
2024-02-06 10:30:22,774 Epoch 2657: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.20 
2024-02-06 10:30:22,774 EPOCH 2658
2024-02-06 10:30:33,469 Epoch 2658: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.82 
2024-02-06 10:30:33,469 EPOCH 2659
2024-02-06 10:30:40,055 [Epoch: 2659 Step: 00045200] Batch Recognition Loss:   0.001222 => Gls Tokens per Sec:     1321 || Batch Translation Loss:   0.155389 => Txt Tokens per Sec:     3507 || Lr: 0.000050
2024-02-06 10:30:44,178 Epoch 2659: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.68 
2024-02-06 10:30:44,179 EPOCH 2660
2024-02-06 10:30:54,879 Epoch 2660: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.52 
2024-02-06 10:30:54,879 EPOCH 2661
2024-02-06 10:31:05,344 Epoch 2661: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.18 
2024-02-06 10:31:05,344 EPOCH 2662
2024-02-06 10:31:16,032 Epoch 2662: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.48 
2024-02-06 10:31:16,032 EPOCH 2663
2024-02-06 10:31:26,712 Epoch 2663: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.43 
2024-02-06 10:31:26,713 EPOCH 2664
2024-02-06 10:31:37,602 Epoch 2664: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.83 
2024-02-06 10:31:37,603 EPOCH 2665
2024-02-06 10:31:43,828 [Epoch: 2665 Step: 00045300] Batch Recognition Loss:   0.000615 => Gls Tokens per Sec:     1192 || Batch Translation Loss:   0.037501 => Txt Tokens per Sec:     3221 || Lr: 0.000050
2024-02-06 10:31:48,284 Epoch 2665: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-06 10:31:48,284 EPOCH 2666
2024-02-06 10:31:59,116 Epoch 2666: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-06 10:31:59,116 EPOCH 2667
2024-02-06 10:32:09,985 Epoch 2667: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-06 10:32:09,986 EPOCH 2668
2024-02-06 10:32:20,749 Epoch 2668: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-06 10:32:20,749 EPOCH 2669
2024-02-06 10:32:31,655 Epoch 2669: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.28 
2024-02-06 10:32:31,656 EPOCH 2670
2024-02-06 10:32:42,297 Epoch 2670: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-06 10:32:42,297 EPOCH 2671
2024-02-06 10:32:49,081 [Epoch: 2671 Step: 00045400] Batch Recognition Loss:   0.000231 => Gls Tokens per Sec:      944 || Batch Translation Loss:   0.016610 => Txt Tokens per Sec:     2702 || Lr: 0.000050
2024-02-06 10:32:52,900 Epoch 2671: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 10:32:52,900 EPOCH 2672
2024-02-06 10:33:03,380 Epoch 2672: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 10:33:03,381 EPOCH 2673
2024-02-06 10:33:14,073 Epoch 2673: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 10:33:14,074 EPOCH 2674
2024-02-06 10:33:24,979 Epoch 2674: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 10:33:24,980 EPOCH 2675
2024-02-06 10:33:35,752 Epoch 2675: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 10:33:35,753 EPOCH 2676
2024-02-06 10:33:46,396 Epoch 2676: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 10:33:46,396 EPOCH 2677
2024-02-06 10:33:51,702 [Epoch: 2677 Step: 00045500] Batch Recognition Loss:   0.000144 => Gls Tokens per Sec:      965 || Batch Translation Loss:   0.010869 => Txt Tokens per Sec:     2789 || Lr: 0.000050
2024-02-06 10:33:57,328 Epoch 2677: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 10:33:57,329 EPOCH 2678
2024-02-06 10:34:07,976 Epoch 2678: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 10:34:07,977 EPOCH 2679
2024-02-06 10:34:18,659 Epoch 2679: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 10:34:18,660 EPOCH 2680
2024-02-06 10:34:29,412 Epoch 2680: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 10:34:29,413 EPOCH 2681
2024-02-06 10:34:39,948 Epoch 2681: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 10:34:39,948 EPOCH 2682
2024-02-06 10:34:50,635 Epoch 2682: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 10:34:50,636 EPOCH 2683
2024-02-06 10:34:51,817 [Epoch: 2683 Step: 00045600] Batch Recognition Loss:   0.000176 => Gls Tokens per Sec:     3251 || Batch Translation Loss:   0.009964 => Txt Tokens per Sec:     8018 || Lr: 0.000050
2024-02-06 10:35:01,255 Epoch 2683: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 10:35:01,255 EPOCH 2684
2024-02-06 10:35:11,799 Epoch 2684: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 10:35:11,800 EPOCH 2685
2024-02-06 10:35:22,464 Epoch 2685: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 10:35:22,465 EPOCH 2686
2024-02-06 10:35:33,078 Epoch 2686: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-06 10:35:33,079 EPOCH 2687
2024-02-06 10:35:43,674 Epoch 2687: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 10:35:43,675 EPOCH 2688
2024-02-06 10:35:54,367 Epoch 2688: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 10:35:54,367 EPOCH 2689
2024-02-06 10:35:56,577 [Epoch: 2689 Step: 00045700] Batch Recognition Loss:   0.000132 => Gls Tokens per Sec:     1159 || Batch Translation Loss:   0.025339 => Txt Tokens per Sec:     3420 || Lr: 0.000050
2024-02-06 10:36:05,061 Epoch 2689: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 10:36:05,062 EPOCH 2690
2024-02-06 10:36:15,845 Epoch 2690: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 10:36:15,846 EPOCH 2691
2024-02-06 10:36:26,463 Epoch 2691: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 10:36:26,463 EPOCH 2692
2024-02-06 10:36:37,312 Epoch 2692: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-06 10:36:37,312 EPOCH 2693
2024-02-06 10:36:47,877 Epoch 2693: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 10:36:47,878 EPOCH 2694
2024-02-06 10:36:58,425 Epoch 2694: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 10:36:58,426 EPOCH 2695
2024-02-06 10:37:00,591 [Epoch: 2695 Step: 00045800] Batch Recognition Loss:   0.000521 => Gls Tokens per Sec:      592 || Batch Translation Loss:   0.019828 => Txt Tokens per Sec:     1770 || Lr: 0.000050
2024-02-06 10:37:09,109 Epoch 2695: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-06 10:37:09,109 EPOCH 2696
2024-02-06 10:37:19,773 Epoch 2696: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 10:37:19,774 EPOCH 2697
2024-02-06 10:37:30,368 Epoch 2697: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 10:37:30,368 EPOCH 2698
2024-02-06 10:37:41,225 Epoch 2698: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 10:37:41,225 EPOCH 2699
2024-02-06 10:37:51,983 Epoch 2699: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-06 10:37:51,984 EPOCH 2700
2024-02-06 10:38:02,794 [Epoch: 2700 Step: 00045900] Batch Recognition Loss:   0.000329 => Gls Tokens per Sec:      982 || Batch Translation Loss:   0.006537 => Txt Tokens per Sec:     2718 || Lr: 0.000050
2024-02-06 10:38:02,795 Epoch 2700: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 10:38:02,795 EPOCH 2701
2024-02-06 10:38:13,916 Epoch 2701: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 10:38:13,917 EPOCH 2702
2024-02-06 10:38:24,607 Epoch 2702: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 10:38:24,608 EPOCH 2703
2024-02-06 10:38:35,402 Epoch 2703: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 10:38:35,402 EPOCH 2704
2024-02-06 10:38:46,096 Epoch 2704: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-06 10:38:46,097 EPOCH 2705
2024-02-06 10:38:56,831 Epoch 2705: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 10:38:56,831 EPOCH 2706
2024-02-06 10:39:06,973 [Epoch: 2706 Step: 00046000] Batch Recognition Loss:   0.000343 => Gls Tokens per Sec:      921 || Batch Translation Loss:   0.006442 => Txt Tokens per Sec:     2514 || Lr: 0.000050
2024-02-06 10:39:47,702 Validation result at epoch 2706, step    46000: duration: 40.7281s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.63471	Translation Loss: 98773.25000	PPL: 19254.23047
	Eval Metric: BLEU
	WER 3.04	(DEL: 0.00,	INS: 0.00,	SUB: 3.04)
	BLEU-4 0.47	(BLEU-1: 10.83,	BLEU-2: 3.28,	BLEU-3: 1.21,	BLEU-4: 0.47)
	CHRF 16.92	ROUGE 8.74
2024-02-06 10:39:47,704 Logging Recognition and Translation Outputs
2024-02-06 10:39:47,704 ========================================================================================================================
2024-02-06 10:39:47,704 Logging Sequence: 87_207.00
2024-02-06 10:39:47,705 	Gloss Reference :	A B+C+D+E
2024-02-06 10:39:47,705 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 10:39:47,705 	Gloss Alignment :	         
2024-02-06 10:39:47,705 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 10:39:47,707 	Text Reference  :	there were     2-3   pakistanis who were  speaking anti-india things and   things on    kashmir
2024-02-06 10:39:47,707 	Text Hypothesis :	this  amrapali group paid       a   total of       rs         4222   crore to     rhiti sports 
2024-02-06 10:39:47,707 	Text Alignment  :	S     S        S     S          S   S     S        S          S      S     S      S     S      
2024-02-06 10:39:47,707 ========================================================================================================================
2024-02-06 10:39:47,707 Logging Sequence: 67_73.00
2024-02-06 10:39:47,708 	Gloss Reference :	A B+C+D+E
2024-02-06 10:39:47,708 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 10:39:47,708 	Gloss Alignment :	         
2024-02-06 10:39:47,708 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 10:39:47,708 	Text Reference  :	**** *** in    his tweet   he     also  said 
2024-02-06 10:39:47,709 	Text Hypothesis :	shah and india had various awards their first
2024-02-06 10:39:47,709 	Text Alignment  :	I    I   S     S   S       S      S     S    
2024-02-06 10:39:47,709 ========================================================================================================================
2024-02-06 10:39:47,709 Logging Sequence: 172_267.00
2024-02-06 10:39:47,709 	Gloss Reference :	A B+C+D+E
2024-02-06 10:39:47,710 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 10:39:47,710 	Gloss Alignment :	         
2024-02-06 10:39:47,710 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 10:39:47,711 	Text Reference  :	**** such provisions have been made  
2024-02-06 10:39:47,711 	Text Hypothesis :	this is   why        it   was  halted
2024-02-06 10:39:47,711 	Text Alignment  :	I    S    S          S    S    S     
2024-02-06 10:39:47,711 ========================================================================================================================
2024-02-06 10:39:47,711 Logging Sequence: 144_23.00
2024-02-06 10:39:47,711 	Gloss Reference :	A B+C+D+E
2024-02-06 10:39:47,712 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 10:39:47,712 	Gloss Alignment :	         
2024-02-06 10:39:47,712 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 10:39:47,713 	Text Reference  :	the girl is 14-year-old mumal mehar and she       is     from kanasar village of      barmer in rajasthan
2024-02-06 10:39:47,713 	Text Hypothesis :	*** **** ** *********** ***** ***** *** argentina scored 2    goals   however towards the    9  minutes  
2024-02-06 10:39:47,713 	Text Alignment  :	D   D    D  D           D     D     D   S         S      S    S       S       S       S      S  S        
2024-02-06 10:39:47,713 ========================================================================================================================
2024-02-06 10:39:47,714 Logging Sequence: 133_202.00
2024-02-06 10:39:47,714 	Gloss Reference :	A B+C+D+E
2024-02-06 10:39:47,714 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 10:39:47,714 	Gloss Alignment :	         
2024-02-06 10:39:47,714 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 10:39:47,715 	Text Reference  :	australia has already qualified for     the  final   if  india wins it will    face australia
2024-02-06 10:39:47,715 	Text Hypothesis :	********* *** ******* pakistani batsmen kept scoring run but   lost 5  wickets very soon     
2024-02-06 10:39:47,715 	Text Alignment  :	D         D   D       S         S       S    S       S   S     S    S  S       S    S        
2024-02-06 10:39:47,716 ========================================================================================================================
2024-02-06 10:39:48,486 Epoch 2706: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 10:39:48,486 EPOCH 2707
2024-02-06 10:39:59,637 Epoch 2707: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 10:39:59,637 EPOCH 2708
2024-02-06 10:40:10,373 Epoch 2708: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-06 10:40:10,373 EPOCH 2709
2024-02-06 10:40:20,659 Epoch 2709: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 10:40:20,659 EPOCH 2710
2024-02-06 10:40:31,220 Epoch 2710: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-06 10:40:31,220 EPOCH 2711
2024-02-06 10:40:41,899 Epoch 2711: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 10:40:41,900 EPOCH 2712
2024-02-06 10:40:50,483 [Epoch: 2712 Step: 00046100] Batch Recognition Loss:   0.000159 => Gls Tokens per Sec:      939 || Batch Translation Loss:   0.012735 => Txt Tokens per Sec:     2594 || Lr: 0.000050
2024-02-06 10:40:52,674 Epoch 2712: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 10:40:52,674 EPOCH 2713
2024-02-06 10:41:03,266 Epoch 2713: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-06 10:41:03,267 EPOCH 2714
2024-02-06 10:41:14,121 Epoch 2714: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 10:41:14,122 EPOCH 2715
2024-02-06 10:41:25,049 Epoch 2715: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-06 10:41:25,050 EPOCH 2716
2024-02-06 10:41:35,740 Epoch 2716: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-06 10:41:35,741 EPOCH 2717
2024-02-06 10:41:46,758 Epoch 2717: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.98 
2024-02-06 10:41:46,759 EPOCH 2718
2024-02-06 10:41:54,022 [Epoch: 2718 Step: 00046200] Batch Recognition Loss:   0.000290 => Gls Tokens per Sec:      934 || Batch Translation Loss:   0.029819 => Txt Tokens per Sec:     2513 || Lr: 0.000050
2024-02-06 10:41:57,502 Epoch 2718: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.78 
2024-02-06 10:41:57,502 EPOCH 2719
2024-02-06 10:42:08,350 Epoch 2719: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.90 
2024-02-06 10:42:08,351 EPOCH 2720
2024-02-06 10:42:19,043 Epoch 2720: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-06 10:42:19,043 EPOCH 2721
2024-02-06 10:42:29,597 Epoch 2721: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-06 10:42:29,598 EPOCH 2722
2024-02-06 10:42:39,977 Epoch 2722: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-06 10:42:39,978 EPOCH 2723
2024-02-06 10:42:50,796 Epoch 2723: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 10:42:50,797 EPOCH 2724
2024-02-06 10:42:55,819 [Epoch: 2724 Step: 00046300] Batch Recognition Loss:   0.000214 => Gls Tokens per Sec:     1147 || Batch Translation Loss:   0.013377 => Txt Tokens per Sec:     3154 || Lr: 0.000050
2024-02-06 10:43:01,198 Epoch 2724: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 10:43:01,199 EPOCH 2725
2024-02-06 10:43:11,925 Epoch 2725: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 10:43:11,925 EPOCH 2726
2024-02-06 10:43:22,561 Epoch 2726: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 10:43:22,561 EPOCH 2727
2024-02-06 10:43:33,272 Epoch 2727: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 10:43:33,272 EPOCH 2728
2024-02-06 10:43:43,910 Epoch 2728: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 10:43:43,911 EPOCH 2729
2024-02-06 10:43:54,496 Epoch 2729: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 10:43:54,497 EPOCH 2730
2024-02-06 10:43:59,613 [Epoch: 2730 Step: 00046400] Batch Recognition Loss:   0.000121 => Gls Tokens per Sec:      876 || Batch Translation Loss:   0.009457 => Txt Tokens per Sec:     2583 || Lr: 0.000050
2024-02-06 10:44:05,268 Epoch 2730: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 10:44:05,268 EPOCH 2731
2024-02-06 10:44:16,127 Epoch 2731: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 10:44:16,128 EPOCH 2732
2024-02-06 10:44:26,856 Epoch 2732: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 10:44:26,857 EPOCH 2733
2024-02-06 10:44:37,720 Epoch 2733: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-06 10:44:37,721 EPOCH 2734
2024-02-06 10:44:48,440 Epoch 2734: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-06 10:44:48,440 EPOCH 2735
2024-02-06 10:44:59,341 Epoch 2735: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-06 10:44:59,342 EPOCH 2736
2024-02-06 10:45:00,531 [Epoch: 2736 Step: 00046500] Batch Recognition Loss:   0.000350 => Gls Tokens per Sec:     2691 || Batch Translation Loss:   0.020044 => Txt Tokens per Sec:     7760 || Lr: 0.000050
2024-02-06 10:45:09,943 Epoch 2736: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-06 10:45:09,944 EPOCH 2737
2024-02-06 10:45:20,579 Epoch 2737: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-06 10:45:20,579 EPOCH 2738
2024-02-06 10:45:31,271 Epoch 2738: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 10:45:31,272 EPOCH 2739
2024-02-06 10:45:41,855 Epoch 2739: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 10:45:41,855 EPOCH 2740
2024-02-06 10:45:52,947 Epoch 2740: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-06 10:45:52,948 EPOCH 2741
2024-02-06 10:46:03,667 Epoch 2741: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-06 10:46:03,668 EPOCH 2742
2024-02-06 10:46:06,175 [Epoch: 2742 Step: 00046600] Batch Recognition Loss:   0.000159 => Gls Tokens per Sec:      766 || Batch Translation Loss:   0.012106 => Txt Tokens per Sec:     2154 || Lr: 0.000050
2024-02-06 10:46:14,655 Epoch 2742: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 10:46:14,655 EPOCH 2743
2024-02-06 10:46:25,330 Epoch 2743: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 10:46:25,331 EPOCH 2744
2024-02-06 10:46:36,192 Epoch 2744: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-06 10:46:36,193 EPOCH 2745
2024-02-06 10:46:46,979 Epoch 2745: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 10:46:46,979 EPOCH 2746
2024-02-06 10:46:57,439 Epoch 2746: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-06 10:46:57,439 EPOCH 2747
2024-02-06 10:47:08,081 Epoch 2747: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 10:47:08,081 EPOCH 2748
2024-02-06 10:47:08,405 [Epoch: 2748 Step: 00046700] Batch Recognition Loss:   0.000169 => Gls Tokens per Sec:     1986 || Batch Translation Loss:   0.014416 => Txt Tokens per Sec:     6289 || Lr: 0.000050
2024-02-06 10:47:18,936 Epoch 2748: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 10:47:18,936 EPOCH 2749
2024-02-06 10:47:29,874 Epoch 2749: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-06 10:47:29,875 EPOCH 2750
2024-02-06 10:47:40,442 Epoch 2750: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-06 10:47:40,442 EPOCH 2751
2024-02-06 10:47:51,072 Epoch 2751: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-06 10:47:51,073 EPOCH 2752
2024-02-06 10:48:01,753 Epoch 2752: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.02 
2024-02-06 10:48:01,754 EPOCH 2753
2024-02-06 10:48:12,452 [Epoch: 2753 Step: 00046800] Batch Recognition Loss:   0.000259 => Gls Tokens per Sec:      933 || Batch Translation Loss:   0.066057 => Txt Tokens per Sec:     2654 || Lr: 0.000050
2024-02-06 10:48:12,583 Epoch 2753: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.99 
2024-02-06 10:48:12,583 EPOCH 2754
2024-02-06 10:48:23,340 Epoch 2754: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.37 
2024-02-06 10:48:23,340 EPOCH 2755
2024-02-06 10:48:33,980 Epoch 2755: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.83 
2024-02-06 10:48:33,981 EPOCH 2756
2024-02-06 10:48:44,757 Epoch 2756: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-06 10:48:44,758 EPOCH 2757
2024-02-06 10:48:55,513 Epoch 2757: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-06 10:48:55,514 EPOCH 2758
2024-02-06 10:49:06,164 Epoch 2758: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-06 10:49:06,164 EPOCH 2759
2024-02-06 10:49:14,236 [Epoch: 2759 Step: 00046900] Batch Recognition Loss:   0.000200 => Gls Tokens per Sec:     1078 || Batch Translation Loss:   0.015362 => Txt Tokens per Sec:     2901 || Lr: 0.000050
2024-02-06 10:49:16,742 Epoch 2759: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-06 10:49:16,742 EPOCH 2760
2024-02-06 10:49:27,277 Epoch 2760: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-06 10:49:27,278 EPOCH 2761
2024-02-06 10:49:38,232 Epoch 2761: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-06 10:49:38,233 EPOCH 2762
2024-02-06 10:49:48,794 Epoch 2762: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 10:49:48,795 EPOCH 2763
2024-02-06 10:49:59,643 Epoch 2763: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-06 10:49:59,644 EPOCH 2764
2024-02-06 10:50:10,201 Epoch 2764: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 10:50:10,202 EPOCH 2765
2024-02-06 10:50:18,260 [Epoch: 2765 Step: 00047000] Batch Recognition Loss:   0.000903 => Gls Tokens per Sec:      921 || Batch Translation Loss:   0.014196 => Txt Tokens per Sec:     2547 || Lr: 0.000050
2024-02-06 10:50:20,830 Epoch 2765: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 10:50:20,830 EPOCH 2766
2024-02-06 10:50:31,343 Epoch 2766: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 10:50:31,343 EPOCH 2767
2024-02-06 10:50:42,043 Epoch 2767: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 10:50:42,044 EPOCH 2768
2024-02-06 10:50:52,742 Epoch 2768: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-06 10:50:52,743 EPOCH 2769
2024-02-06 10:51:03,613 Epoch 2769: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 10:51:03,614 EPOCH 2770
2024-02-06 10:51:14,309 Epoch 2770: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 10:51:14,310 EPOCH 2771
2024-02-06 10:51:19,972 [Epoch: 2771 Step: 00047100] Batch Recognition Loss:   0.000138 => Gls Tokens per Sec:     1085 || Batch Translation Loss:   0.012408 => Txt Tokens per Sec:     2833 || Lr: 0.000050
2024-02-06 10:51:25,168 Epoch 2771: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 10:51:25,168 EPOCH 2772
2024-02-06 10:51:35,892 Epoch 2772: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-06 10:51:35,892 EPOCH 2773
2024-02-06 10:51:46,732 Epoch 2773: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 10:51:46,733 EPOCH 2774
2024-02-06 10:51:57,564 Epoch 2774: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 10:51:57,564 EPOCH 2775
2024-02-06 10:52:08,531 Epoch 2775: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 10:52:08,531 EPOCH 2776
2024-02-06 10:52:19,189 Epoch 2776: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 10:52:19,190 EPOCH 2777
2024-02-06 10:52:22,471 [Epoch: 2777 Step: 00047200] Batch Recognition Loss:   0.000121 => Gls Tokens per Sec:     1561 || Batch Translation Loss:   0.011839 => Txt Tokens per Sec:     4153 || Lr: 0.000050
2024-02-06 10:52:29,752 Epoch 2777: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 10:52:29,752 EPOCH 2778
2024-02-06 10:52:40,092 Epoch 2778: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 10:52:40,093 EPOCH 2779
2024-02-06 10:52:50,836 Epoch 2779: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 10:52:50,836 EPOCH 2780
2024-02-06 10:53:01,659 Epoch 2780: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 10:53:01,659 EPOCH 2781
2024-02-06 10:53:12,238 Epoch 2781: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 10:53:12,239 EPOCH 2782
2024-02-06 10:53:22,975 Epoch 2782: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-06 10:53:22,976 EPOCH 2783
2024-02-06 10:53:26,301 [Epoch: 2783 Step: 00047300] Batch Recognition Loss:   0.000942 => Gls Tokens per Sec:     1155 || Batch Translation Loss:   0.018555 => Txt Tokens per Sec:     3188 || Lr: 0.000050
2024-02-06 10:53:33,791 Epoch 2783: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 10:53:33,791 EPOCH 2784
2024-02-06 10:53:44,482 Epoch 2784: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-06 10:53:44,482 EPOCH 2785
2024-02-06 10:53:55,161 Epoch 2785: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 10:53:55,162 EPOCH 2786
2024-02-06 10:54:05,879 Epoch 2786: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-06 10:54:05,879 EPOCH 2787
2024-02-06 10:54:16,391 Epoch 2787: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 10:54:16,392 EPOCH 2788
2024-02-06 10:54:27,121 Epoch 2788: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 10:54:27,122 EPOCH 2789
2024-02-06 10:54:27,766 [Epoch: 2789 Step: 00047400] Batch Recognition Loss:   0.000168 => Gls Tokens per Sec:     3981 || Batch Translation Loss:   0.005658 => Txt Tokens per Sec:     9250 || Lr: 0.000050
2024-02-06 10:54:37,845 Epoch 2789: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-06 10:54:37,845 EPOCH 2790
2024-02-06 10:54:48,434 Epoch 2790: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 10:54:48,435 EPOCH 2791
2024-02-06 10:54:59,245 Epoch 2791: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-06 10:54:59,245 EPOCH 2792
2024-02-06 10:55:10,024 Epoch 2792: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 10:55:10,025 EPOCH 2793
2024-02-06 10:55:20,910 Epoch 2793: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 10:55:20,910 EPOCH 2794
2024-02-06 10:55:31,855 Epoch 2794: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 10:55:31,855 EPOCH 2795
2024-02-06 10:55:32,326 [Epoch: 2795 Step: 00047500] Batch Recognition Loss:   0.000217 => Gls Tokens per Sec:     2729 || Batch Translation Loss:   0.009385 => Txt Tokens per Sec:     7158 || Lr: 0.000050
2024-02-06 10:55:42,803 Epoch 2795: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 10:55:42,804 EPOCH 2796
2024-02-06 10:55:53,463 Epoch 2796: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 10:55:53,463 EPOCH 2797
2024-02-06 10:56:04,101 Epoch 2797: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 10:56:04,101 EPOCH 2798
2024-02-06 10:56:14,703 Epoch 2798: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-06 10:56:14,704 EPOCH 2799
2024-02-06 10:56:25,452 Epoch 2799: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 10:56:25,452 EPOCH 2800
2024-02-06 10:56:36,192 [Epoch: 2800 Step: 00047600] Batch Recognition Loss:   0.001678 => Gls Tokens per Sec:      989 || Batch Translation Loss:   0.064918 => Txt Tokens per Sec:     2736 || Lr: 0.000050
2024-02-06 10:56:36,193 Epoch 2800: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.35 
2024-02-06 10:56:36,193 EPOCH 2801
2024-02-06 10:56:46,789 Epoch 2801: Total Training Recognition Loss 0.13  Total Training Translation Loss 0.44 
2024-02-06 10:56:46,789 EPOCH 2802
2024-02-06 10:56:57,418 Epoch 2802: Total Training Recognition Loss 0.91  Total Training Translation Loss 0.77 
2024-02-06 10:56:57,419 EPOCH 2803
2024-02-06 10:57:08,083 Epoch 2803: Total Training Recognition Loss 1.24  Total Training Translation Loss 1.07 
2024-02-06 10:57:08,084 EPOCH 2804
2024-02-06 10:57:18,798 Epoch 2804: Total Training Recognition Loss 0.14  Total Training Translation Loss 0.99 
2024-02-06 10:57:18,798 EPOCH 2805
2024-02-06 10:57:29,662 Epoch 2805: Total Training Recognition Loss 0.07  Total Training Translation Loss 0.89 
2024-02-06 10:57:29,663 EPOCH 2806
2024-02-06 10:57:39,778 [Epoch: 2806 Step: 00047700] Batch Recognition Loss:   0.000533 => Gls Tokens per Sec:      923 || Batch Translation Loss:   0.042089 => Txt Tokens per Sec:     2568 || Lr: 0.000050
2024-02-06 10:57:40,229 Epoch 2806: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.70 
2024-02-06 10:57:40,230 EPOCH 2807
2024-02-06 10:57:50,491 Epoch 2807: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.59 
2024-02-06 10:57:50,491 EPOCH 2808
2024-02-06 10:58:01,096 Epoch 2808: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.50 
2024-02-06 10:58:01,096 EPOCH 2809
2024-02-06 10:58:12,105 Epoch 2809: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.40 
2024-02-06 10:58:12,106 EPOCH 2810
2024-02-06 10:58:22,748 Epoch 2810: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.39 
2024-02-06 10:58:22,749 EPOCH 2811
2024-02-06 10:58:33,510 Epoch 2811: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-06 10:58:33,510 EPOCH 2812
2024-02-06 10:58:39,944 [Epoch: 2812 Step: 00047800] Batch Recognition Loss:   0.000145 => Gls Tokens per Sec:     1253 || Batch Translation Loss:   0.015860 => Txt Tokens per Sec:     3357 || Lr: 0.000050
2024-02-06 10:58:43,946 Epoch 2812: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 10:58:43,947 EPOCH 2813
2024-02-06 10:58:54,975 Epoch 2813: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 10:58:54,976 EPOCH 2814
2024-02-06 10:59:05,861 Epoch 2814: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 10:59:05,862 EPOCH 2815
2024-02-06 10:59:16,626 Epoch 2815: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 10:59:16,627 EPOCH 2816
2024-02-06 10:59:27,257 Epoch 2816: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 10:59:27,258 EPOCH 2817
2024-02-06 10:59:38,093 Epoch 2817: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 10:59:38,094 EPOCH 2818
2024-02-06 10:59:45,999 [Epoch: 2818 Step: 00047900] Batch Recognition Loss:   0.001586 => Gls Tokens per Sec:      858 || Batch Translation Loss:   0.021848 => Txt Tokens per Sec:     2291 || Lr: 0.000050
2024-02-06 10:59:48,983 Epoch 2818: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 10:59:48,983 EPOCH 2819
2024-02-06 10:59:59,521 Epoch 2819: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 10:59:59,522 EPOCH 2820
2024-02-06 11:00:10,336 Epoch 2820: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 11:00:10,336 EPOCH 2821
2024-02-06 11:00:20,903 Epoch 2821: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 11:00:20,904 EPOCH 2822
2024-02-06 11:00:31,489 Epoch 2822: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 11:00:31,489 EPOCH 2823
2024-02-06 11:00:42,051 Epoch 2823: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 11:00:42,052 EPOCH 2824
2024-02-06 11:00:46,017 [Epoch: 2824 Step: 00048000] Batch Recognition Loss:   0.000166 => Gls Tokens per Sec:     1453 || Batch Translation Loss:   0.014351 => Txt Tokens per Sec:     4110 || Lr: 0.000050
2024-02-06 11:01:26,562 Validation result at epoch 2824, step    48000: duration: 40.5438s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.53513	Translation Loss: 99988.21094	PPL: 21738.41797
	Eval Metric: BLEU
	WER 2.68	(DEL: 0.00,	INS: 0.00,	SUB: 2.68)
	BLEU-4 0.00	(BLEU-1: 10.17,	BLEU-2: 2.76,	BLEU-3: 0.88,	BLEU-4: 0.00)
	CHRF 16.56	ROUGE 8.66
2024-02-06 11:01:26,564 Logging Recognition and Translation Outputs
2024-02-06 11:01:26,564 ========================================================================================================================
2024-02-06 11:01:26,565 Logging Sequence: 96_93.00
2024-02-06 11:01:26,565 	Gloss Reference :	A B+C+D+E
2024-02-06 11:01:26,565 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 11:01:26,565 	Gloss Alignment :	         
2024-02-06 11:01:26,565 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 11:01:26,566 	Text Reference  :	bhuvneshwar kumar took 4    wickets and  hardik pandya took  3    wickets wonderful
2024-02-06 11:01:26,567 	Text Hypothesis :	*********** ***** the  next man     sent to     know   about pant and     won      
2024-02-06 11:01:26,567 	Text Alignment  :	D           D     S    S    S       S    S      S      S     S    S       S        
2024-02-06 11:01:26,567 ========================================================================================================================
2024-02-06 11:01:26,567 Logging Sequence: 144_2.00
2024-02-06 11:01:26,567 	Gloss Reference :	A B+C+D+E
2024-02-06 11:01:26,567 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 11:01:26,568 	Gloss Alignment :	         
2024-02-06 11:01:26,568 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 11:01:26,569 	Text Reference  :	a girl posted a video of herself playing cricket  on   a village farm  on     social media    the video has  gone viral  
2024-02-06 11:01:26,570 	Text Hypothesis :	* **** ****** * ***** ** ******* police  detained over a ******* dozen people in     brussels and eight more in   antwerp
2024-02-06 11:01:26,570 	Text Alignment  :	D D    D      D D     D  D       S       S        S      D       S     S      S      S        S   S     S    S    S      
2024-02-06 11:01:26,570 ========================================================================================================================
2024-02-06 11:01:26,570 Logging Sequence: 178_83.00
2024-02-06 11:01:26,570 	Gloss Reference :	A B+C+D+E
2024-02-06 11:01:26,571 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 11:01:26,571 	Gloss Alignment :	         
2024-02-06 11:01:26,571 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 11:01:26,572 	Text Reference  :	and the police still     haven't apprehended the    wrestler
2024-02-06 11:01:26,572 	Text Hypothesis :	and *** named  prominent indian  wrestler    sushil kumar   
2024-02-06 11:01:26,572 	Text Alignment  :	    D   S      S         S       S           S      S       
2024-02-06 11:01:26,572 ========================================================================================================================
2024-02-06 11:01:26,572 Logging Sequence: 169_214.00
2024-02-06 11:01:26,572 	Gloss Reference :	A B+C+D+E
2024-02-06 11:01:26,572 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 11:01:26,573 	Gloss Alignment :	         
2024-02-06 11:01:26,573 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 11:01:26,574 	Text Reference  :	virat kohli said that ***** though arshdeep dropped the catch he is still a       strong part of       the indian       team 
2024-02-06 11:01:26,575 	Text Hypothesis :	***** ***** the  that panel will   be       in      the ***** ** ** ***** british to     keep watching all commonwealth games
2024-02-06 11:01:26,575 	Text Alignment  :	D     D     S         I     S      S        S           D     D  D  D     S       S      S    S        S   S            S    
2024-02-06 11:01:26,575 ========================================================================================================================
2024-02-06 11:01:26,575 Logging Sequence: 147_202.00
2024-02-06 11:01:26,575 	Gloss Reference :	A B+C+D+E
2024-02-06 11:01:26,575 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 11:01:26,575 	Gloss Alignment :	         
2024-02-06 11:01:26,576 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 11:01:26,578 	Text Reference  :	were impressed that she  took the  difficult decision to  withdraw from the ****** ***** ***** olympics and       focus on  her mental health
2024-02-06 11:01:26,578 	Text Hypothesis :	3    if        your love to   tell you       are      now i        want the medals about group is       extremely fit   and are sold   out   
2024-02-06 11:01:26,578 	Text Alignment  :	S    S         S    S    S    S    S         S        S   S        S        I      I     I     S        S         S     S   S   S      S     
2024-02-06 11:01:26,579 ========================================================================================================================
2024-02-06 11:01:33,723 Epoch 2824: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 11:01:33,724 EPOCH 2825
2024-02-06 11:01:44,656 Epoch 2825: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 11:01:44,656 EPOCH 2826
2024-02-06 11:01:55,219 Epoch 2826: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 11:01:55,219 EPOCH 2827
2024-02-06 11:02:05,811 Epoch 2827: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 11:02:05,811 EPOCH 2828
2024-02-06 11:02:16,394 Epoch 2828: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 11:02:16,394 EPOCH 2829
2024-02-06 11:02:27,313 Epoch 2829: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-06 11:02:27,313 EPOCH 2830
2024-02-06 11:02:32,419 [Epoch: 2830 Step: 00048100] Batch Recognition Loss:   0.000387 => Gls Tokens per Sec:      878 || Batch Translation Loss:   0.015241 => Txt Tokens per Sec:     2517 || Lr: 0.000050
2024-02-06 11:02:38,207 Epoch 2830: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 11:02:38,207 EPOCH 2831
2024-02-06 11:02:48,689 Epoch 2831: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-06 11:02:48,690 EPOCH 2832
2024-02-06 11:02:59,234 Epoch 2832: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-06 11:02:59,234 EPOCH 2833
2024-02-06 11:03:09,926 Epoch 2833: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-06 11:03:09,927 EPOCH 2834
2024-02-06 11:03:20,598 Epoch 2834: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 11:03:20,599 EPOCH 2835
2024-02-06 11:03:31,168 Epoch 2835: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-06 11:03:31,169 EPOCH 2836
2024-02-06 11:03:35,467 [Epoch: 2836 Step: 00048200] Batch Recognition Loss:   0.000617 => Gls Tokens per Sec:      745 || Batch Translation Loss:   0.022206 => Txt Tokens per Sec:     2263 || Lr: 0.000050
2024-02-06 11:03:41,959 Epoch 2836: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-06 11:03:41,960 EPOCH 2837
2024-02-06 11:03:52,514 Epoch 2837: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 11:03:52,515 EPOCH 2838
2024-02-06 11:04:03,247 Epoch 2838: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 11:04:03,247 EPOCH 2839
2024-02-06 11:04:13,822 Epoch 2839: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 11:04:13,822 EPOCH 2840
2024-02-06 11:04:24,589 Epoch 2840: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-06 11:04:24,590 EPOCH 2841
2024-02-06 11:04:35,552 Epoch 2841: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-06 11:04:35,553 EPOCH 2842
2024-02-06 11:04:38,114 [Epoch: 2842 Step: 00048300] Batch Recognition Loss:   0.000134 => Gls Tokens per Sec:      750 || Batch Translation Loss:   0.012645 => Txt Tokens per Sec:     2314 || Lr: 0.000050
2024-02-06 11:04:46,215 Epoch 2842: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 11:04:46,215 EPOCH 2843
2024-02-06 11:04:56,790 Epoch 2843: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 11:04:56,791 EPOCH 2844
2024-02-06 11:05:07,379 Epoch 2844: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 11:05:07,379 EPOCH 2845
2024-02-06 11:05:18,048 Epoch 2845: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 11:05:18,048 EPOCH 2846
2024-02-06 11:05:28,585 Epoch 2846: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 11:05:28,586 EPOCH 2847
2024-02-06 11:05:39,628 Epoch 2847: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-06 11:05:39,629 EPOCH 2848
2024-02-06 11:05:39,896 [Epoch: 2848 Step: 00048400] Batch Recognition Loss:   0.000213 => Gls Tokens per Sec:     2415 || Batch Translation Loss:   0.022593 => Txt Tokens per Sec:     6977 || Lr: 0.000050
2024-02-06 11:05:50,272 Epoch 2848: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-06 11:05:50,273 EPOCH 2849
2024-02-06 11:06:01,097 Epoch 2849: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-06 11:06:01,098 EPOCH 2850
2024-02-06 11:06:11,648 Epoch 2850: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-06 11:06:11,648 EPOCH 2851
2024-02-06 11:06:22,288 Epoch 2851: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.91 
2024-02-06 11:06:22,289 EPOCH 2852
2024-02-06 11:06:33,000 Epoch 2852: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.11 
2024-02-06 11:06:33,001 EPOCH 2853
2024-02-06 11:06:43,777 [Epoch: 2853 Step: 00048500] Batch Recognition Loss:   0.000325 => Gls Tokens per Sec:      926 || Batch Translation Loss:   0.025830 => Txt Tokens per Sec:     2560 || Lr: 0.000050
2024-02-06 11:06:44,005 Epoch 2853: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-06 11:06:44,005 EPOCH 2854
2024-02-06 11:06:54,841 Epoch 2854: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-06 11:06:54,841 EPOCH 2855
2024-02-06 11:07:05,540 Epoch 2855: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-06 11:07:05,540 EPOCH 2856
2024-02-06 11:07:16,142 Epoch 2856: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-06 11:07:16,142 EPOCH 2857
2024-02-06 11:07:26,731 Epoch 2857: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-06 11:07:26,732 EPOCH 2858
2024-02-06 11:07:37,651 Epoch 2858: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-06 11:07:37,651 EPOCH 2859
2024-02-06 11:07:47,652 [Epoch: 2859 Step: 00048600] Batch Recognition Loss:   0.000131 => Gls Tokens per Sec:      870 || Batch Translation Loss:   0.036415 => Txt Tokens per Sec:     2454 || Lr: 0.000050
2024-02-06 11:07:48,374 Epoch 2859: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-06 11:07:48,375 EPOCH 2860
2024-02-06 11:07:59,146 Epoch 2860: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-06 11:07:59,147 EPOCH 2861
2024-02-06 11:08:09,896 Epoch 2861: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-06 11:08:09,897 EPOCH 2862
2024-02-06 11:08:20,922 Epoch 2862: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-06 11:08:20,923 EPOCH 2863
2024-02-06 11:08:31,616 Epoch 2863: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-06 11:08:31,616 EPOCH 2864
2024-02-06 11:08:42,404 Epoch 2864: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-06 11:08:42,405 EPOCH 2865
2024-02-06 11:08:50,088 [Epoch: 2865 Step: 00048700] Batch Recognition Loss:   0.000197 => Gls Tokens per Sec:      966 || Batch Translation Loss:   0.016368 => Txt Tokens per Sec:     2568 || Lr: 0.000050
2024-02-06 11:08:53,337 Epoch 2865: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-06 11:08:53,337 EPOCH 2866
2024-02-06 11:09:03,933 Epoch 2866: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-06 11:09:03,933 EPOCH 2867
2024-02-06 11:09:14,423 Epoch 2867: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.40 
2024-02-06 11:09:14,423 EPOCH 2868
2024-02-06 11:09:25,331 Epoch 2868: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-06 11:09:25,332 EPOCH 2869
2024-02-06 11:09:35,876 Epoch 2869: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-06 11:09:35,876 EPOCH 2870
2024-02-06 11:09:46,673 Epoch 2870: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-06 11:09:46,673 EPOCH 2871
2024-02-06 11:09:50,228 [Epoch: 2871 Step: 00048800] Batch Recognition Loss:   0.000182 => Gls Tokens per Sec:     1801 || Batch Translation Loss:   0.021078 => Txt Tokens per Sec:     4751 || Lr: 0.000050
2024-02-06 11:09:57,191 Epoch 2871: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-06 11:09:57,191 EPOCH 2872
2024-02-06 11:10:07,918 Epoch 2872: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 11:10:07,918 EPOCH 2873
2024-02-06 11:10:18,664 Epoch 2873: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 11:10:18,665 EPOCH 2874
2024-02-06 11:10:29,319 Epoch 2874: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-06 11:10:29,319 EPOCH 2875
2024-02-06 11:10:39,943 Epoch 2875: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-06 11:10:39,943 EPOCH 2876
2024-02-06 11:10:50,639 Epoch 2876: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-06 11:10:50,640 EPOCH 2877
2024-02-06 11:10:55,420 [Epoch: 2877 Step: 00048900] Batch Recognition Loss:   0.000167 => Gls Tokens per Sec:     1071 || Batch Translation Loss:   0.017389 => Txt Tokens per Sec:     2842 || Lr: 0.000050
2024-02-06 11:11:01,328 Epoch 2877: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-06 11:11:01,328 EPOCH 2878
2024-02-06 11:11:12,119 Epoch 2878: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 11:11:12,120 EPOCH 2879
2024-02-06 11:11:22,881 Epoch 2879: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 11:11:22,882 EPOCH 2880
2024-02-06 11:11:33,783 Epoch 2880: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 11:11:33,784 EPOCH 2881
2024-02-06 11:11:44,140 Epoch 2881: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 11:11:44,141 EPOCH 2882
2024-02-06 11:11:55,088 Epoch 2882: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 11:11:55,088 EPOCH 2883
2024-02-06 11:11:58,539 [Epoch: 2883 Step: 00049000] Batch Recognition Loss:   0.000401 => Gls Tokens per Sec:     1038 || Batch Translation Loss:   0.009055 => Txt Tokens per Sec:     2737 || Lr: 0.000050
2024-02-06 11:12:05,809 Epoch 2883: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 11:12:05,810 EPOCH 2884
2024-02-06 11:12:16,505 Epoch 2884: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 11:12:16,505 EPOCH 2885
2024-02-06 11:12:27,364 Epoch 2885: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 11:12:27,364 EPOCH 2886
2024-02-06 11:12:38,281 Epoch 2886: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 11:12:38,282 EPOCH 2887
2024-02-06 11:12:48,874 Epoch 2887: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 11:12:48,874 EPOCH 2888
2024-02-06 11:12:59,837 Epoch 2888: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-06 11:12:59,838 EPOCH 2889
2024-02-06 11:13:03,619 [Epoch: 2889 Step: 00049100] Batch Recognition Loss:   0.000264 => Gls Tokens per Sec:      677 || Batch Translation Loss:   0.017353 => Txt Tokens per Sec:     1998 || Lr: 0.000050
2024-02-06 11:13:10,284 Epoch 2889: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-06 11:13:10,284 EPOCH 2890
2024-02-06 11:13:21,098 Epoch 2890: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 11:13:21,098 EPOCH 2891
2024-02-06 11:13:32,092 Epoch 2891: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-06 11:13:32,093 EPOCH 2892
2024-02-06 11:13:42,698 Epoch 2892: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 11:13:42,698 EPOCH 2893
2024-02-06 11:13:53,136 Epoch 2893: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 11:13:53,136 EPOCH 2894
2024-02-06 11:14:03,713 Epoch 2894: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-06 11:14:03,714 EPOCH 2895
2024-02-06 11:14:04,218 [Epoch: 2895 Step: 00049200] Batch Recognition Loss:   0.000148 => Gls Tokens per Sec:     2545 || Batch Translation Loss:   0.054391 => Txt Tokens per Sec:     7656 || Lr: 0.000050
2024-02-06 11:14:14,175 Epoch 2895: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-06 11:14:14,175 EPOCH 2896
2024-02-06 11:14:24,995 Epoch 2896: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-06 11:14:24,995 EPOCH 2897
2024-02-06 11:14:35,808 Epoch 2897: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-06 11:14:35,808 EPOCH 2898
2024-02-06 11:14:46,535 Epoch 2898: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.58 
2024-02-06 11:14:46,536 EPOCH 2899
2024-02-06 11:14:57,368 Epoch 2899: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-06 11:14:57,368 EPOCH 2900
2024-02-06 11:15:08,067 [Epoch: 2900 Step: 00049300] Batch Recognition Loss:   0.000311 => Gls Tokens per Sec:      993 || Batch Translation Loss:   0.018090 => Txt Tokens per Sec:     2747 || Lr: 0.000050
2024-02-06 11:15:08,068 Epoch 2900: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-06 11:15:08,068 EPOCH 2901
2024-02-06 11:15:18,607 Epoch 2901: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-06 11:15:18,608 EPOCH 2902
2024-02-06 11:15:29,386 Epoch 2902: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-06 11:15:29,387 EPOCH 2903
2024-02-06 11:15:40,093 Epoch 2903: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-06 11:15:40,094 EPOCH 2904
2024-02-06 11:15:50,855 Epoch 2904: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-06 11:15:50,855 EPOCH 2905
2024-02-06 11:16:01,526 Epoch 2905: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-06 11:16:01,527 EPOCH 2906
2024-02-06 11:16:11,267 [Epoch: 2906 Step: 00049400] Batch Recognition Loss:   0.001167 => Gls Tokens per Sec:      959 || Batch Translation Loss:   0.029034 => Txt Tokens per Sec:     2613 || Lr: 0.000050
2024-02-06 11:16:11,924 Epoch 2906: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-06 11:16:11,924 EPOCH 2907
2024-02-06 11:16:22,596 Epoch 2907: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 11:16:22,596 EPOCH 2908
2024-02-06 11:16:33,227 Epoch 2908: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-06 11:16:33,227 EPOCH 2909
2024-02-06 11:16:43,770 Epoch 2909: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-06 11:16:43,771 EPOCH 2910
2024-02-06 11:16:54,413 Epoch 2910: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 11:16:54,413 EPOCH 2911
2024-02-06 11:17:04,806 Epoch 2911: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 11:17:04,807 EPOCH 2912
2024-02-06 11:17:12,769 [Epoch: 2912 Step: 00049500] Batch Recognition Loss:   0.000162 => Gls Tokens per Sec:     1013 || Batch Translation Loss:   0.015426 => Txt Tokens per Sec:     2792 || Lr: 0.000050
2024-02-06 11:17:15,387 Epoch 2912: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-06 11:17:15,388 EPOCH 2913
2024-02-06 11:17:26,210 Epoch 2913: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 11:17:26,211 EPOCH 2914
2024-02-06 11:17:36,687 Epoch 2914: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 11:17:36,687 EPOCH 2915
2024-02-06 11:17:47,436 Epoch 2915: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-06 11:17:47,436 EPOCH 2916
2024-02-06 11:17:57,773 Epoch 2916: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-06 11:17:57,774 EPOCH 2917
2024-02-06 11:18:08,719 Epoch 2917: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-06 11:18:08,720 EPOCH 2918
2024-02-06 11:18:13,724 [Epoch: 2918 Step: 00049600] Batch Recognition Loss:   0.000718 => Gls Tokens per Sec:     1407 || Batch Translation Loss:   0.010636 => Txt Tokens per Sec:     3703 || Lr: 0.000050
2024-02-06 11:18:19,368 Epoch 2918: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 11:18:19,369 EPOCH 2919
2024-02-06 11:18:29,997 Epoch 2919: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 11:18:29,997 EPOCH 2920
2024-02-06 11:18:40,769 Epoch 2920: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-06 11:18:40,770 EPOCH 2921
2024-02-06 11:18:51,647 Epoch 2921: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-06 11:18:51,647 EPOCH 2922
2024-02-06 11:19:02,238 Epoch 2922: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-06 11:19:02,239 EPOCH 2923
2024-02-06 11:19:12,901 Epoch 2923: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-06 11:19:12,901 EPOCH 2924
2024-02-06 11:19:18,905 [Epoch: 2924 Step: 00049700] Batch Recognition Loss:   0.000381 => Gls Tokens per Sec:      916 || Batch Translation Loss:   0.019809 => Txt Tokens per Sec:     2536 || Lr: 0.000050
2024-02-06 11:19:23,398 Epoch 2924: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-06 11:19:23,398 EPOCH 2925
2024-02-06 11:19:33,888 Epoch 2925: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 11:19:33,889 EPOCH 2926
2024-02-06 11:19:44,549 Epoch 2926: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-06 11:19:44,549 EPOCH 2927
2024-02-06 11:19:55,418 Epoch 2927: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-06 11:19:55,419 EPOCH 2928
2024-02-06 11:20:06,240 Epoch 2928: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-06 11:20:06,240 EPOCH 2929
2024-02-06 11:20:16,907 Epoch 2929: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-06 11:20:16,908 EPOCH 2930
2024-02-06 11:20:19,948 [Epoch: 2930 Step: 00049800] Batch Recognition Loss:   0.000213 => Gls Tokens per Sec:     1475 || Batch Translation Loss:   0.023498 => Txt Tokens per Sec:     3959 || Lr: 0.000050
2024-02-06 11:20:27,641 Epoch 2930: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-06 11:20:27,641 EPOCH 2931
2024-02-06 11:20:38,424 Epoch 2931: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-06 11:20:38,425 EPOCH 2932
2024-02-06 11:20:49,316 Epoch 2932: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-06 11:20:49,316 EPOCH 2933
2024-02-06 11:21:00,262 Epoch 2933: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-06 11:21:00,263 EPOCH 2934
2024-02-06 11:21:11,058 Epoch 2934: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-06 11:21:11,059 EPOCH 2935
2024-02-06 11:21:21,921 Epoch 2935: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-06 11:21:21,922 EPOCH 2936
2024-02-06 11:21:24,618 [Epoch: 2936 Step: 00049900] Batch Recognition Loss:   0.000748 => Gls Tokens per Sec:     1188 || Batch Translation Loss:   0.046167 => Txt Tokens per Sec:     3128 || Lr: 0.000050
2024-02-06 11:21:32,713 Epoch 2936: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-06 11:21:32,713 EPOCH 2937
2024-02-06 11:21:43,488 Epoch 2937: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-06 11:21:43,489 EPOCH 2938
2024-02-06 11:21:54,183 Epoch 2938: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-06 11:21:54,184 EPOCH 2939
2024-02-06 11:22:04,769 Epoch 2939: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-06 11:22:04,770 EPOCH 2940
2024-02-06 11:22:15,552 Epoch 2940: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-06 11:22:15,553 EPOCH 2941
2024-02-06 11:22:26,040 Epoch 2941: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 11:22:26,041 EPOCH 2942
2024-02-06 11:22:30,207 [Epoch: 2942 Step: 00050000] Batch Recognition Loss:   0.000309 => Gls Tokens per Sec:      399 || Batch Translation Loss:   0.020988 => Txt Tokens per Sec:     1196 || Lr: 0.000050
2024-02-06 11:23:10,634 Validation result at epoch 2942, step    50000: duration: 40.4260s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.58569	Translation Loss: 100497.74219	PPL: 22873.36133
	Eval Metric: BLEU
	WER 2.97	(DEL: 0.00,	INS: 0.00,	SUB: 2.97)
	BLEU-4 0.34	(BLEU-1: 10.31,	BLEU-2: 2.84,	BLEU-3: 0.97,	BLEU-4: 0.34)
	CHRF 16.80	ROUGE 8.46
2024-02-06 11:23:10,636 Logging Recognition and Translation Outputs
2024-02-06 11:23:10,636 ========================================================================================================================
2024-02-06 11:23:10,636 Logging Sequence: 178_157.00
2024-02-06 11:23:10,636 	Gloss Reference :	A B+C+D+E
2024-02-06 11:23:10,636 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 11:23:10,636 	Gloss Alignment :	         
2024-02-06 11:23:10,637 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 11:23:10,638 	Text Reference  :	** *** this is  why sushil     kumar will have   to    be arrested
2024-02-06 11:23:10,638 	Text Hypothesis :	on 5th may  the was questioned by    the  police filed a  few     
2024-02-06 11:23:10,638 	Text Alignment  :	I  I   S    S   S   S          S     S    S      S     S  S       
2024-02-06 11:23:10,638 ========================================================================================================================
2024-02-06 11:23:10,638 Logging Sequence: 118_111.00
2024-02-06 11:23:10,638 	Gloss Reference :	A B+C+D+E
2024-02-06 11:23:10,639 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 11:23:10,639 	Gloss Alignment :	         
2024-02-06 11:23:10,639 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 11:23:10,641 	Text Reference  :	***** and people encourage him  have hope  for the  next world cup 
2024-02-06 11:23:10,641 	Text Hypothesis :	since the match  are       made 4    times i   will be   made  them
2024-02-06 11:23:10,641 	Text Alignment  :	I     S   S      S         S    S    S     S   S    S    S     S   
2024-02-06 11:23:10,641 ========================================================================================================================
2024-02-06 11:23:10,641 Logging Sequence: 148_2.00
2024-02-06 11:23:10,642 	Gloss Reference :	A B+C+D+E
2024-02-06 11:23:10,642 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 11:23:10,642 	Gloss Alignment :	         
2024-02-06 11:23:10,642 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 11:23:10,644 	Text Reference  :	the final of the  asia  cup 2023 cricket   tournament was  played between india   and sri lanka   on 17th september 2023
2024-02-06 11:23:10,645 	Text Hypothesis :	*** ***** ** like india has bcci secretary jay        shah for    1       century and 50  million on 12th january   2022
2024-02-06 11:23:10,645 	Text Alignment  :	D   D     D  S    S     S   S    S         S          S    S      S       S           S   S          S    S         S   
2024-02-06 11:23:10,645 ========================================================================================================================
2024-02-06 11:23:10,645 Logging Sequence: 83_129.00
2024-02-06 11:23:10,645 	Gloss Reference :	A B+C+D+E
2024-02-06 11:23:10,645 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 11:23:10,646 	Gloss Alignment :	         
2024-02-06 11:23:10,646 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 11:23:10,646 	Text Reference  :	** *** later the    denmark football association tweeted
2024-02-06 11:23:10,646 	Text Hypothesis :	he was just  joking around  with     each        other  
2024-02-06 11:23:10,646 	Text Alignment  :	I  I   S     S      S       S        S           S      
2024-02-06 11:23:10,647 ========================================================================================================================
2024-02-06 11:23:10,647 Logging Sequence: 99_158.00
2024-02-06 11:23:10,647 	Gloss Reference :	A B+C+D+E
2024-02-06 11:23:10,647 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 11:23:10,647 	Gloss Alignment :	         
2024-02-06 11:23:10,647 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 11:23:10,648 	Text Reference  :	the    incident occured in dubai and it was extremely shameful
2024-02-06 11:23:10,648 	Text Hypothesis :	people had      met     in ***** *** ** *** ********* home    
2024-02-06 11:23:10,648 	Text Alignment  :	S      S        S          D     D   D  D   D         S       
2024-02-06 11:23:10,648 ========================================================================================================================
2024-02-06 11:23:17,625 Epoch 2942: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 11:23:17,626 EPOCH 2943
2024-02-06 11:23:28,691 Epoch 2943: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 11:23:28,692 EPOCH 2944
2024-02-06 11:23:39,644 Epoch 2944: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 11:23:39,645 EPOCH 2945
2024-02-06 11:23:50,311 Epoch 2945: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 11:23:50,311 EPOCH 2946
2024-02-06 11:24:01,168 Epoch 2946: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 11:24:01,169 EPOCH 2947
2024-02-06 11:24:11,668 Epoch 2947: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 11:24:11,668 EPOCH 2948
2024-02-06 11:24:11,805 [Epoch: 2948 Step: 00050100] Batch Recognition Loss:   0.000154 => Gls Tokens per Sec:     4706 || Batch Translation Loss:   0.009572 => Txt Tokens per Sec:     9434 || Lr: 0.000050
2024-02-06 11:24:22,401 Epoch 2948: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-06 11:24:22,402 EPOCH 2949
2024-02-06 11:24:33,072 Epoch 2949: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 11:24:33,073 EPOCH 2950
2024-02-06 11:24:43,629 Epoch 2950: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-06 11:24:43,630 EPOCH 2951
2024-02-06 11:24:54,361 Epoch 2951: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-06 11:24:54,362 EPOCH 2952
2024-02-06 11:25:04,893 Epoch 2952: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-06 11:25:04,893 EPOCH 2953
2024-02-06 11:25:15,482 [Epoch: 2953 Step: 00050200] Batch Recognition Loss:   0.000609 => Gls Tokens per Sec:      943 || Batch Translation Loss:   0.018881 => Txt Tokens per Sec:     2642 || Lr: 0.000050
2024-02-06 11:25:15,658 Epoch 2953: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 11:25:15,659 EPOCH 2954
2024-02-06 11:25:26,286 Epoch 2954: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 11:25:26,286 EPOCH 2955
2024-02-06 11:25:37,142 Epoch 2955: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-06 11:25:37,142 EPOCH 2956
2024-02-06 11:25:47,765 Epoch 2956: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-06 11:25:47,766 EPOCH 2957
2024-02-06 11:25:58,599 Epoch 2957: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 11:25:58,600 EPOCH 2958
2024-02-06 11:26:09,407 Epoch 2958: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 11:26:09,408 EPOCH 2959
2024-02-06 11:26:19,748 [Epoch: 2959 Step: 00050300] Batch Recognition Loss:   0.002080 => Gls Tokens per Sec:      842 || Batch Translation Loss:   0.007696 => Txt Tokens per Sec:     2386 || Lr: 0.000050
2024-02-06 11:26:20,299 Epoch 2959: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 11:26:20,299 EPOCH 2960
2024-02-06 11:26:30,995 Epoch 2960: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.81 
2024-02-06 11:26:30,995 EPOCH 2961
2024-02-06 11:26:41,612 Epoch 2961: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.90 
2024-02-06 11:26:41,612 EPOCH 2962
2024-02-06 11:26:52,471 Epoch 2962: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.72 
2024-02-06 11:26:52,472 EPOCH 2963
2024-02-06 11:27:03,317 Epoch 2963: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-06 11:27:03,318 EPOCH 2964
2024-02-06 11:27:14,213 Epoch 2964: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-06 11:27:14,214 EPOCH 2965
2024-02-06 11:27:22,537 [Epoch: 2965 Step: 00050400] Batch Recognition Loss:   0.000418 => Gls Tokens per Sec:      892 || Batch Translation Loss:   0.025573 => Txt Tokens per Sec:     2529 || Lr: 0.000050
2024-02-06 11:27:24,927 Epoch 2965: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.31 
2024-02-06 11:27:24,927 EPOCH 2966
2024-02-06 11:27:35,247 Epoch 2966: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.61 
2024-02-06 11:27:35,248 EPOCH 2967
2024-02-06 11:27:45,905 Epoch 2967: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.52 
2024-02-06 11:27:45,905 EPOCH 2968
2024-02-06 11:27:56,611 Epoch 2968: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.94 
2024-02-06 11:27:56,612 EPOCH 2969
2024-02-06 11:28:07,432 Epoch 2969: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-06 11:28:07,433 EPOCH 2970
2024-02-06 11:28:17,990 Epoch 2970: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-06 11:28:17,991 EPOCH 2971
2024-02-06 11:28:23,714 [Epoch: 2971 Step: 00050500] Batch Recognition Loss:   0.001047 => Gls Tokens per Sec:     1073 || Batch Translation Loss:   0.017977 => Txt Tokens per Sec:     2925 || Lr: 0.000050
2024-02-06 11:28:28,611 Epoch 2971: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-06 11:28:28,611 EPOCH 2972
2024-02-06 11:28:39,233 Epoch 2972: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 11:28:39,233 EPOCH 2973
2024-02-06 11:28:49,864 Epoch 2973: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 11:28:49,864 EPOCH 2974
2024-02-06 11:29:00,378 Epoch 2974: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 11:29:00,379 EPOCH 2975
2024-02-06 11:29:11,193 Epoch 2975: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 11:29:11,194 EPOCH 2976
2024-02-06 11:29:22,034 Epoch 2976: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 11:29:22,035 EPOCH 2977
2024-02-06 11:29:28,519 [Epoch: 2977 Step: 00050600] Batch Recognition Loss:   0.000212 => Gls Tokens per Sec:      790 || Batch Translation Loss:   0.011362 => Txt Tokens per Sec:     2211 || Lr: 0.000050
2024-02-06 11:29:32,784 Epoch 2977: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 11:29:32,784 EPOCH 2978
2024-02-06 11:29:43,239 Epoch 2978: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 11:29:43,240 EPOCH 2979
2024-02-06 11:29:54,076 Epoch 2979: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 11:29:54,077 EPOCH 2980
2024-02-06 11:30:04,989 Epoch 2980: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 11:30:04,990 EPOCH 2981
2024-02-06 11:30:15,821 Epoch 2981: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 11:30:15,821 EPOCH 2982
2024-02-06 11:30:26,651 Epoch 2982: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 11:30:26,652 EPOCH 2983
2024-02-06 11:30:33,708 [Epoch: 2983 Step: 00050700] Batch Recognition Loss:   0.000269 => Gls Tokens per Sec:      507 || Batch Translation Loss:   0.025339 => Txt Tokens per Sec:     1454 || Lr: 0.000050
2024-02-06 11:30:37,705 Epoch 2983: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 11:30:37,706 EPOCH 2984
2024-02-06 11:30:48,239 Epoch 2984: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 11:30:48,239 EPOCH 2985
2024-02-06 11:30:59,011 Epoch 2985: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-06 11:30:59,012 EPOCH 2986
2024-02-06 11:31:09,863 Epoch 2986: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-06 11:31:09,864 EPOCH 2987
2024-02-06 11:31:20,808 Epoch 2987: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 11:31:20,809 EPOCH 2988
2024-02-06 11:31:31,614 Epoch 2988: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 11:31:31,614 EPOCH 2989
2024-02-06 11:31:33,995 [Epoch: 2989 Step: 00050800] Batch Recognition Loss:   0.000392 => Gls Tokens per Sec:     1076 || Batch Translation Loss:   0.019796 => Txt Tokens per Sec:     3231 || Lr: 0.000050
2024-02-06 11:31:42,095 Epoch 2989: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 11:31:42,095 EPOCH 2990
2024-02-06 11:31:52,951 Epoch 2990: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 11:31:52,952 EPOCH 2991
2024-02-06 11:32:03,710 Epoch 2991: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-06 11:32:03,711 EPOCH 2992
2024-02-06 11:32:14,577 Epoch 2992: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 11:32:14,578 EPOCH 2993
2024-02-06 11:32:25,369 Epoch 2993: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 11:32:25,369 EPOCH 2994
2024-02-06 11:32:35,878 Epoch 2994: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 11:32:35,879 EPOCH 2995
2024-02-06 11:32:36,194 [Epoch: 2995 Step: 00050900] Batch Recognition Loss:   0.000128 => Gls Tokens per Sec:     4076 || Batch Translation Loss:   0.011091 => Txt Tokens per Sec:     9048 || Lr: 0.000050
2024-02-06 11:32:46,748 Epoch 2995: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 11:32:46,748 EPOCH 2996
2024-02-06 11:32:57,397 Epoch 2996: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-06 11:32:57,398 EPOCH 2997
2024-02-06 11:33:07,935 Epoch 2997: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 11:33:07,935 EPOCH 2998
2024-02-06 11:33:18,824 Epoch 2998: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 11:33:18,824 EPOCH 2999
2024-02-06 11:33:29,480 Epoch 2999: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-06 11:33:29,481 EPOCH 3000
2024-02-06 11:33:40,129 [Epoch: 3000 Step: 00051000] Batch Recognition Loss:   0.000160 => Gls Tokens per Sec:      998 || Batch Translation Loss:   0.012062 => Txt Tokens per Sec:     2760 || Lr: 0.000050
2024-02-06 11:33:40,130 Epoch 3000: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 11:33:40,130 EPOCH 3001
2024-02-06 11:33:50,768 Epoch 3001: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 11:33:50,768 EPOCH 3002
2024-02-06 11:34:01,844 Epoch 3002: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-06 11:34:01,845 EPOCH 3003
2024-02-06 11:34:12,565 Epoch 3003: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-06 11:34:12,566 EPOCH 3004
2024-02-06 11:34:23,194 Epoch 3004: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 11:34:23,194 EPOCH 3005
2024-02-06 11:34:34,037 Epoch 3005: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 11:34:34,038 EPOCH 3006
2024-02-06 11:34:44,034 [Epoch: 3006 Step: 00051100] Batch Recognition Loss:   0.000116 => Gls Tokens per Sec:      934 || Batch Translation Loss:   0.010199 => Txt Tokens per Sec:     2553 || Lr: 0.000050
2024-02-06 11:34:44,740 Epoch 3006: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-06 11:34:44,740 EPOCH 3007
2024-02-06 11:34:55,417 Epoch 3007: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-06 11:34:55,417 EPOCH 3008
2024-02-06 11:35:06,227 Epoch 3008: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 11:35:06,228 EPOCH 3009
2024-02-06 11:35:17,066 Epoch 3009: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 11:35:17,066 EPOCH 3010
2024-02-06 11:35:27,895 Epoch 3010: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-06 11:35:27,896 EPOCH 3011
2024-02-06 11:35:38,677 Epoch 3011: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-06 11:35:38,677 EPOCH 3012
2024-02-06 11:35:48,400 [Epoch: 3012 Step: 00051200] Batch Recognition Loss:   0.000161 => Gls Tokens per Sec:      829 || Batch Translation Loss:   0.029112 => Txt Tokens per Sec:     2317 || Lr: 0.000050
2024-02-06 11:35:49,360 Epoch 3012: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 11:35:49,361 EPOCH 3013
2024-02-06 11:36:00,182 Epoch 3013: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 11:36:00,182 EPOCH 3014
2024-02-06 11:36:11,333 Epoch 3014: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-06 11:36:11,333 EPOCH 3015
2024-02-06 11:36:22,050 Epoch 3015: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 11:36:22,051 EPOCH 3016
2024-02-06 11:36:32,587 Epoch 3016: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 11:36:32,588 EPOCH 3017
2024-02-06 11:36:43,208 Epoch 3017: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 11:36:43,209 EPOCH 3018
2024-02-06 11:36:52,236 [Epoch: 3018 Step: 00051300] Batch Recognition Loss:   0.000212 => Gls Tokens per Sec:      751 || Batch Translation Loss:   0.021748 => Txt Tokens per Sec:     2073 || Lr: 0.000050
2024-02-06 11:36:53,927 Epoch 3018: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-06 11:36:53,928 EPOCH 3019
2024-02-06 11:37:04,532 Epoch 3019: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-06 11:37:04,532 EPOCH 3020
2024-02-06 11:37:15,403 Epoch 3020: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-06 11:37:15,403 EPOCH 3021
2024-02-06 11:37:25,951 Epoch 3021: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-06 11:37:25,952 EPOCH 3022
2024-02-06 11:37:36,402 Epoch 3022: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-06 11:37:36,402 EPOCH 3023
2024-02-06 11:37:47,192 Epoch 3023: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.72 
2024-02-06 11:37:47,192 EPOCH 3024
2024-02-06 11:37:50,632 [Epoch: 3024 Step: 00051400] Batch Recognition Loss:   0.001403 => Gls Tokens per Sec:     1675 || Batch Translation Loss:   0.043755 => Txt Tokens per Sec:     4532 || Lr: 0.000050
2024-02-06 11:37:57,844 Epoch 3024: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-06 11:37:57,845 EPOCH 3025
2024-02-06 11:38:08,513 Epoch 3025: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-06 11:38:08,513 EPOCH 3026
2024-02-06 11:38:19,213 Epoch 3026: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-06 11:38:19,214 EPOCH 3027
2024-02-06 11:38:30,112 Epoch 3027: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-06 11:38:30,113 EPOCH 3028
2024-02-06 11:38:40,694 Epoch 3028: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-06 11:38:40,694 EPOCH 3029
2024-02-06 11:38:51,290 Epoch 3029: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-06 11:38:51,291 EPOCH 3030
2024-02-06 11:38:54,360 [Epoch: 3030 Step: 00051500] Batch Recognition Loss:   0.000257 => Gls Tokens per Sec:     1460 || Batch Translation Loss:   0.019750 => Txt Tokens per Sec:     3906 || Lr: 0.000050
2024-02-06 11:39:01,719 Epoch 3030: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-06 11:39:01,720 EPOCH 3031
2024-02-06 11:39:12,316 Epoch 3031: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-06 11:39:12,317 EPOCH 3032
2024-02-06 11:39:23,199 Epoch 3032: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-06 11:39:23,200 EPOCH 3033
2024-02-06 11:39:33,724 Epoch 3033: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-06 11:39:33,724 EPOCH 3034
2024-02-06 11:39:44,278 Epoch 3034: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-06 11:39:44,279 EPOCH 3035
2024-02-06 11:39:55,002 Epoch 3035: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-06 11:39:55,003 EPOCH 3036
2024-02-06 11:39:59,751 [Epoch: 3036 Step: 00051600] Batch Recognition Loss:   0.000125 => Gls Tokens per Sec:      619 || Batch Translation Loss:   0.018530 => Txt Tokens per Sec:     1734 || Lr: 0.000050
2024-02-06 11:40:05,874 Epoch 3036: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 11:40:05,874 EPOCH 3037
2024-02-06 11:40:16,704 Epoch 3037: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 11:40:16,704 EPOCH 3038
2024-02-06 11:40:27,417 Epoch 3038: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 11:40:27,418 EPOCH 3039
2024-02-06 11:40:38,261 Epoch 3039: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 11:40:38,261 EPOCH 3040
2024-02-06 11:40:49,036 Epoch 3040: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 11:40:49,037 EPOCH 3041
2024-02-06 11:40:59,441 Epoch 3041: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 11:40:59,442 EPOCH 3042
2024-02-06 11:41:00,094 [Epoch: 3042 Step: 00051700] Batch Recognition Loss:   0.000177 => Gls Tokens per Sec:     2954 || Batch Translation Loss:   0.013869 => Txt Tokens per Sec:     7747 || Lr: 0.000050
2024-02-06 11:41:10,054 Epoch 3042: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-06 11:41:10,054 EPOCH 3043
2024-02-06 11:41:21,043 Epoch 3043: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-06 11:41:21,043 EPOCH 3044
2024-02-06 11:41:31,867 Epoch 3044: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-06 11:41:31,867 EPOCH 3045
2024-02-06 11:41:42,627 Epoch 3045: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-06 11:41:42,628 EPOCH 3046
2024-02-06 11:41:53,288 Epoch 3046: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-06 11:41:53,288 EPOCH 3047
2024-02-06 11:42:03,987 Epoch 3047: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-06 11:42:03,988 EPOCH 3048
2024-02-06 11:42:04,159 [Epoch: 3048 Step: 00051800] Batch Recognition Loss:   0.000246 => Gls Tokens per Sec:     3787 || Batch Translation Loss:   0.034256 => Txt Tokens per Sec:     9550 || Lr: 0.000050
2024-02-06 11:42:14,333 Epoch 3048: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.98 
2024-02-06 11:42:14,334 EPOCH 3049
2024-02-06 11:42:24,962 Epoch 3049: Total Training Recognition Loss 0.01  Total Training Translation Loss 6.70 
2024-02-06 11:42:24,962 EPOCH 3050
2024-02-06 11:42:35,445 Epoch 3050: Total Training Recognition Loss 0.04  Total Training Translation Loss 6.21 
2024-02-06 11:42:35,446 EPOCH 3051
2024-02-06 11:42:46,436 Epoch 3051: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.71 
2024-02-06 11:42:46,436 EPOCH 3052
2024-02-06 11:42:56,933 Epoch 3052: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.95 
2024-02-06 11:42:56,933 EPOCH 3053
2024-02-06 11:43:05,801 [Epoch: 3053 Step: 00051900] Batch Recognition Loss:   0.001358 => Gls Tokens per Sec:     1126 || Batch Translation Loss:   0.035312 => Txt Tokens per Sec:     3081 || Lr: 0.000050
2024-02-06 11:43:07,673 Epoch 3053: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.55 
2024-02-06 11:43:07,673 EPOCH 3054
2024-02-06 11:43:18,487 Epoch 3054: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-06 11:43:18,487 EPOCH 3055
2024-02-06 11:43:29,119 Epoch 3055: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-06 11:43:29,120 EPOCH 3056
2024-02-06 11:43:39,430 Epoch 3056: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-06 11:43:39,431 EPOCH 3057
2024-02-06 11:43:50,653 Epoch 3057: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 11:43:50,654 EPOCH 3058
2024-02-06 11:44:01,988 Epoch 3058: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-06 11:44:01,989 EPOCH 3059
2024-02-06 11:44:12,497 [Epoch: 3059 Step: 00052000] Batch Recognition Loss:   0.000544 => Gls Tokens per Sec:      828 || Batch Translation Loss:   0.011583 => Txt Tokens per Sec:     2327 || Lr: 0.000050
2024-02-06 11:44:55,360 Validation result at epoch 3059, step    52000: duration: 42.8620s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.52367	Translation Loss: 100813.95312	PPL: 23607.32031
	Eval Metric: BLEU
	WER 2.97	(DEL: 0.00,	INS: 0.00,	SUB: 2.97)
	BLEU-4 0.54	(BLEU-1: 10.10,	BLEU-2: 3.11,	BLEU-3: 1.14,	BLEU-4: 0.54)
	CHRF 16.56	ROUGE 8.58
2024-02-06 11:44:55,362 Logging Recognition and Translation Outputs
2024-02-06 11:44:55,362 ========================================================================================================================
2024-02-06 11:44:55,362 Logging Sequence: 59_101.00
2024-02-06 11:44:55,363 	Gloss Reference :	A B+C+D+E
2024-02-06 11:44:55,363 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 11:44:55,363 	Gloss Alignment :	         
2024-02-06 11:44:55,363 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 11:44:55,366 	Text Reference  :	did you see the     video fox said        she      won her medals  because of a        condom and        is very happy
2024-02-06 11:44:55,366 	Text Hypothesis :	*** a   vow renewal is    a   celebratory ceremony for a   married couple  to reaffirm their  commitment to each other
2024-02-06 11:44:55,366 	Text Alignment  :	D   S   S   S       S     S   S           S        S   S   S       S       S  S        S      S          S  S    S    
2024-02-06 11:44:55,366 ========================================================================================================================
2024-02-06 11:44:55,366 Logging Sequence: 103_112.00
2024-02-06 11:44:55,366 	Gloss Reference :	A B+C+D+E
2024-02-06 11:44:55,366 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 11:44:55,366 	Gloss Alignment :	         
2024-02-06 11:44:55,367 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 11:44:55,368 	Text Reference  :	you are aware that     earlier the britishers had colonized a   lot of     countries in the ***** world  
2024-02-06 11:44:55,368 	Text Hypothesis :	and the vast  majority of      the ********** *** of        the 56  member countries of the delta variant
2024-02-06 11:44:55,368 	Text Alignment  :	S   S   S     S        S           D          D   S         S   S   S                S      I     S      
2024-02-06 11:44:55,368 ========================================================================================================================
2024-02-06 11:44:55,368 Logging Sequence: 143_11.00
2024-02-06 11:44:55,369 	Gloss Reference :	A B+C+D+E
2024-02-06 11:44:55,369 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 11:44:55,369 	Gloss Alignment :	         
2024-02-06 11:44:55,369 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 11:44:55,370 	Text Reference  :	ronaldo has also become the first person to have 500 million followers on      instagram he is the   most    loved    footballer
2024-02-06 11:44:55,370 	Text Hypothesis :	******* *** **** ****** the ***** ****** ** **** ban would   be        applied when      he ** joins another football club      
2024-02-06 11:44:55,371 	Text Alignment  :	D       D   D    D          D     D      D  D    S   S       S         S       S            D  S     S       S        S         
2024-02-06 11:44:55,371 ========================================================================================================================
2024-02-06 11:44:55,371 Logging Sequence: 183_23.00
2024-02-06 11:44:55,371 	Gloss Reference :	A B+C+D+E
2024-02-06 11:44:55,371 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 11:44:55,371 	Gloss Alignment :	         
2024-02-06 11:44:55,371 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 11:44:55,372 	Text Reference  :	however everybody has been waiting for them to   announce  the name of  the child
2024-02-06 11:44:55,372 	Text Hypothesis :	like    india     has **** ******* *** **** bcci secretary jay shah for his wife 
2024-02-06 11:44:55,372 	Text Alignment  :	S       S             D    D       D   D    S    S         S   S    S   S   S    
2024-02-06 11:44:55,373 ========================================================================================================================
2024-02-06 11:44:55,373 Logging Sequence: 169_165.00
2024-02-06 11:44:55,373 	Gloss Reference :	A B+C+D+E
2024-02-06 11:44:55,373 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 11:44:55,373 	Gloss Alignment :	         
2024-02-06 11:44:55,373 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 11:44:55,375 	Text Reference  :	the indian government was outraged  by      the  incident and         these changes were undone by     wikipedia
2024-02-06 11:44:55,375 	Text Hypothesis :	do  you    know       the remaining matches like this     information on    celebs  like their  height age      
2024-02-06 11:44:55,375 	Text Alignment  :	S   S      S          S   S         S       S    S        S           S     S       S    S      S      S        
2024-02-06 11:44:55,375 ========================================================================================================================
2024-02-06 11:44:56,285 Epoch 3059: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 11:44:56,285 EPOCH 3060
2024-02-06 11:45:07,692 Epoch 3060: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 11:45:07,692 EPOCH 3061
2024-02-06 11:45:18,511 Epoch 3061: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 11:45:18,512 EPOCH 3062
2024-02-06 11:45:29,674 Epoch 3062: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 11:45:29,674 EPOCH 3063
2024-02-06 11:45:40,225 Epoch 3063: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 11:45:40,226 EPOCH 3064
2024-02-06 11:45:51,297 Epoch 3064: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 11:45:51,297 EPOCH 3065
2024-02-06 11:45:59,663 [Epoch: 3065 Step: 00052100] Batch Recognition Loss:   0.000358 => Gls Tokens per Sec:      887 || Batch Translation Loss:   0.014193 => Txt Tokens per Sec:     2576 || Lr: 0.000050
2024-02-06 11:46:02,098 Epoch 3065: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 11:46:02,099 EPOCH 3066
2024-02-06 11:46:13,033 Epoch 3066: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 11:46:13,033 EPOCH 3067
2024-02-06 11:46:23,670 Epoch 3067: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 11:46:23,671 EPOCH 3068
2024-02-06 11:46:34,180 Epoch 3068: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 11:46:34,181 EPOCH 3069
2024-02-06 11:46:44,851 Epoch 3069: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-06 11:46:44,851 EPOCH 3070
2024-02-06 11:46:55,600 Epoch 3070: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 11:46:55,600 EPOCH 3071
2024-02-06 11:46:59,342 [Epoch: 3071 Step: 00052200] Batch Recognition Loss:   0.000254 => Gls Tokens per Sec:     1711 || Batch Translation Loss:   0.013945 => Txt Tokens per Sec:     4599 || Lr: 0.000050
2024-02-06 11:47:06,487 Epoch 3071: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 11:47:06,488 EPOCH 3072
2024-02-06 11:47:17,360 Epoch 3072: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 11:47:17,360 EPOCH 3073
2024-02-06 11:47:28,202 Epoch 3073: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 11:47:28,203 EPOCH 3074
2024-02-06 11:47:39,006 Epoch 3074: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 11:47:39,007 EPOCH 3075
2024-02-06 11:47:49,756 Epoch 3075: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 11:47:49,757 EPOCH 3076
2024-02-06 11:48:00,453 Epoch 3076: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-06 11:48:00,454 EPOCH 3077
2024-02-06 11:48:06,859 [Epoch: 3077 Step: 00052300] Batch Recognition Loss:   0.000482 => Gls Tokens per Sec:      800 || Batch Translation Loss:   0.017137 => Txt Tokens per Sec:     2398 || Lr: 0.000050
2024-02-06 11:48:11,069 Epoch 3077: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 11:48:11,070 EPOCH 3078
2024-02-06 11:48:21,639 Epoch 3078: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 11:48:21,640 EPOCH 3079
2024-02-06 11:48:32,291 Epoch 3079: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-06 11:48:32,292 EPOCH 3080
2024-02-06 11:48:43,209 Epoch 3080: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 11:48:43,210 EPOCH 3081
2024-02-06 11:48:54,087 Epoch 3081: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 11:48:54,088 EPOCH 3082
2024-02-06 11:49:04,800 Epoch 3082: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 11:49:04,801 EPOCH 3083
2024-02-06 11:49:07,514 [Epoch: 3083 Step: 00052400] Batch Recognition Loss:   0.000333 => Gls Tokens per Sec:     1416 || Batch Translation Loss:   0.016517 => Txt Tokens per Sec:     3614 || Lr: 0.000050
2024-02-06 11:49:15,245 Epoch 3083: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-06 11:49:15,246 EPOCH 3084
2024-02-06 11:49:26,105 Epoch 3084: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-06 11:49:26,106 EPOCH 3085
2024-02-06 11:49:36,775 Epoch 3085: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 11:49:36,776 EPOCH 3086
2024-02-06 11:49:47,670 Epoch 3086: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 11:49:47,671 EPOCH 3087
2024-02-06 11:49:58,207 Epoch 3087: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 11:49:58,208 EPOCH 3088
2024-02-06 11:50:09,027 Epoch 3088: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 11:50:09,028 EPOCH 3089
2024-02-06 11:50:10,012 [Epoch: 3089 Step: 00052500] Batch Recognition Loss:   0.000161 => Gls Tokens per Sec:     2605 || Batch Translation Loss:   0.011511 => Txt Tokens per Sec:     7254 || Lr: 0.000050
2024-02-06 11:50:19,850 Epoch 3089: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-06 11:50:19,851 EPOCH 3090
2024-02-06 11:50:30,486 Epoch 3090: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 11:50:30,487 EPOCH 3091
2024-02-06 11:50:41,298 Epoch 3091: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-06 11:50:41,298 EPOCH 3092
2024-02-06 11:50:52,167 Epoch 3092: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-06 11:50:52,168 EPOCH 3093
2024-02-06 11:51:03,001 Epoch 3093: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 11:51:03,002 EPOCH 3094
2024-02-06 11:51:13,705 Epoch 3094: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 11:51:13,705 EPOCH 3095
2024-02-06 11:51:14,081 [Epoch: 3095 Step: 00052600] Batch Recognition Loss:   0.000193 => Gls Tokens per Sec:     3413 || Batch Translation Loss:   0.012032 => Txt Tokens per Sec:     8715 || Lr: 0.000050
2024-02-06 11:51:24,375 Epoch 3095: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 11:51:24,375 EPOCH 3096
2024-02-06 11:51:35,092 Epoch 3096: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 11:51:35,092 EPOCH 3097
2024-02-06 11:51:45,994 Epoch 3097: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-06 11:51:45,995 EPOCH 3098
2024-02-06 11:51:56,836 Epoch 3098: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-06 11:51:56,837 EPOCH 3099
2024-02-06 11:52:07,567 Epoch 3099: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-06 11:52:07,568 EPOCH 3100
2024-02-06 11:52:18,342 [Epoch: 3100 Step: 00052700] Batch Recognition Loss:   0.000515 => Gls Tokens per Sec:      986 || Batch Translation Loss:   0.011826 => Txt Tokens per Sec:     2727 || Lr: 0.000050
2024-02-06 11:52:18,343 Epoch 3100: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-06 11:52:18,343 EPOCH 3101
2024-02-06 11:52:29,084 Epoch 3101: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-06 11:52:29,084 EPOCH 3102
2024-02-06 11:52:40,160 Epoch 3102: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 11:52:40,161 EPOCH 3103
2024-02-06 11:52:50,962 Epoch 3103: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-06 11:52:50,962 EPOCH 3104
2024-02-06 11:53:01,757 Epoch 3104: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-06 11:53:01,758 EPOCH 3105
2024-02-06 11:53:12,657 Epoch 3105: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-06 11:53:12,658 EPOCH 3106
2024-02-06 11:53:21,518 [Epoch: 3106 Step: 00052800] Batch Recognition Loss:   0.000132 => Gls Tokens per Sec:     1054 || Batch Translation Loss:   0.015387 => Txt Tokens per Sec:     2878 || Lr: 0.000050
2024-02-06 11:53:23,486 Epoch 3106: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-06 11:53:23,486 EPOCH 3107
2024-02-06 11:53:34,613 Epoch 3107: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 11:53:34,614 EPOCH 3108
2024-02-06 11:53:45,660 Epoch 3108: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 11:53:45,661 EPOCH 3109
2024-02-06 11:53:56,458 Epoch 3109: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-06 11:53:56,458 EPOCH 3110
2024-02-06 11:54:07,019 Epoch 3110: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-06 11:54:07,020 EPOCH 3111
2024-02-06 11:54:17,600 Epoch 3111: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-06 11:54:17,601 EPOCH 3112
2024-02-06 11:54:27,423 [Epoch: 3112 Step: 00052900] Batch Recognition Loss:   0.000143 => Gls Tokens per Sec:      821 || Batch Translation Loss:   0.017575 => Txt Tokens per Sec:     2371 || Lr: 0.000050
2024-02-06 11:54:28,210 Epoch 3112: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-06 11:54:28,211 EPOCH 3113
2024-02-06 11:54:38,763 Epoch 3113: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-06 11:54:38,764 EPOCH 3114
2024-02-06 11:54:49,402 Epoch 3114: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-06 11:54:49,402 EPOCH 3115
2024-02-06 11:55:00,067 Epoch 3115: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-06 11:55:00,068 EPOCH 3116
2024-02-06 11:55:10,757 Epoch 3116: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-06 11:55:10,758 EPOCH 3117
2024-02-06 11:55:21,189 Epoch 3117: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.72 
2024-02-06 11:55:21,190 EPOCH 3118
2024-02-06 11:55:30,606 [Epoch: 3118 Step: 00053000] Batch Recognition Loss:   0.000409 => Gls Tokens per Sec:      720 || Batch Translation Loss:   0.041133 => Txt Tokens per Sec:     2035 || Lr: 0.000050
2024-02-06 11:55:31,922 Epoch 3118: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-06 11:55:31,922 EPOCH 3119
2024-02-06 11:55:42,772 Epoch 3119: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.82 
2024-02-06 11:55:42,772 EPOCH 3120
2024-02-06 11:55:53,531 Epoch 3120: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-06 11:55:53,532 EPOCH 3121
2024-02-06 11:56:04,364 Epoch 3121: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.75 
2024-02-06 11:56:04,365 EPOCH 3122
2024-02-06 11:56:15,099 Epoch 3122: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-06 11:56:15,100 EPOCH 3123
2024-02-06 11:56:25,906 Epoch 3123: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-06 11:56:25,906 EPOCH 3124
2024-02-06 11:56:31,978 [Epoch: 3124 Step: 00053100] Batch Recognition Loss:   0.000304 => Gls Tokens per Sec:      906 || Batch Translation Loss:   0.029538 => Txt Tokens per Sec:     2499 || Lr: 0.000050
2024-02-06 11:56:36,650 Epoch 3124: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-06 11:56:36,650 EPOCH 3125
2024-02-06 11:56:47,438 Epoch 3125: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-06 11:56:47,438 EPOCH 3126
2024-02-06 11:56:58,128 Epoch 3126: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-06 11:56:58,129 EPOCH 3127
2024-02-06 11:57:08,585 Epoch 3127: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 11:57:08,586 EPOCH 3128
2024-02-06 11:57:19,381 Epoch 3128: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 11:57:19,382 EPOCH 3129
2024-02-06 11:57:29,912 Epoch 3129: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-06 11:57:29,912 EPOCH 3130
2024-02-06 11:57:36,328 [Epoch: 3130 Step: 00053200] Batch Recognition Loss:   0.000335 => Gls Tokens per Sec:      658 || Batch Translation Loss:   0.021392 => Txt Tokens per Sec:     1883 || Lr: 0.000050
2024-02-06 11:57:40,487 Epoch 3130: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-06 11:57:40,488 EPOCH 3131
2024-02-06 11:57:51,360 Epoch 3131: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-06 11:57:51,360 EPOCH 3132
2024-02-06 11:58:02,158 Epoch 3132: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-06 11:58:02,159 EPOCH 3133
2024-02-06 11:58:12,786 Epoch 3133: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-06 11:58:12,787 EPOCH 3134
2024-02-06 11:58:23,430 Epoch 3134: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-06 11:58:23,431 EPOCH 3135
2024-02-06 11:58:34,128 Epoch 3135: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-06 11:58:34,128 EPOCH 3136
2024-02-06 11:58:37,436 [Epoch: 3136 Step: 00053300] Batch Recognition Loss:   0.002631 => Gls Tokens per Sec:      889 || Batch Translation Loss:   0.060151 => Txt Tokens per Sec:     2433 || Lr: 0.000050
2024-02-06 11:58:44,790 Epoch 3136: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-06 11:58:44,791 EPOCH 3137
2024-02-06 11:58:55,600 Epoch 3137: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-06 11:58:55,600 EPOCH 3138
2024-02-06 11:59:06,036 Epoch 3138: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-06 11:59:06,037 EPOCH 3139
2024-02-06 11:59:16,854 Epoch 3139: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 11:59:16,855 EPOCH 3140
2024-02-06 11:59:27,690 Epoch 3140: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 11:59:27,691 EPOCH 3141
2024-02-06 11:59:38,453 Epoch 3141: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 11:59:38,454 EPOCH 3142
2024-02-06 11:59:39,098 [Epoch: 3142 Step: 00053400] Batch Recognition Loss:   0.000212 => Gls Tokens per Sec:     2986 || Batch Translation Loss:   0.019746 => Txt Tokens per Sec:     7731 || Lr: 0.000050
2024-02-06 11:59:49,131 Epoch 3142: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 11:59:49,132 EPOCH 3143
2024-02-06 11:59:59,843 Epoch 3143: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 11:59:59,844 EPOCH 3144
2024-02-06 12:00:10,566 Epoch 3144: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 12:00:10,566 EPOCH 3145
2024-02-06 12:00:21,265 Epoch 3145: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 12:00:21,265 EPOCH 3146
2024-02-06 12:00:32,030 Epoch 3146: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 12:00:32,031 EPOCH 3147
2024-02-06 12:00:42,510 Epoch 3147: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 12:00:42,511 EPOCH 3148
2024-02-06 12:00:42,748 [Epoch: 3148 Step: 00053500] Batch Recognition Loss:   0.000177 => Gls Tokens per Sec:     2712 || Batch Translation Loss:   0.012741 => Txt Tokens per Sec:     7407 || Lr: 0.000050
2024-02-06 12:00:53,400 Epoch 3148: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 12:00:53,401 EPOCH 3149
2024-02-06 12:01:03,951 Epoch 3149: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 12:01:03,952 EPOCH 3150
2024-02-06 12:01:14,675 Epoch 3150: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-06 12:01:14,675 EPOCH 3151
2024-02-06 12:01:25,419 Epoch 3151: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-06 12:01:25,420 EPOCH 3152
2024-02-06 12:01:36,387 Epoch 3152: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 12:01:36,387 EPOCH 3153
2024-02-06 12:01:46,640 [Epoch: 3153 Step: 00053600] Batch Recognition Loss:   0.000146 => Gls Tokens per Sec:      973 || Batch Translation Loss:   0.028161 => Txt Tokens per Sec:     2670 || Lr: 0.000050
2024-02-06 12:01:46,968 Epoch 3153: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-06 12:01:46,968 EPOCH 3154
2024-02-06 12:01:57,544 Epoch 3154: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 12:01:57,545 EPOCH 3155
2024-02-06 12:02:08,267 Epoch 3155: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-06 12:02:08,267 EPOCH 3156
2024-02-06 12:02:18,509 Epoch 3156: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-06 12:02:18,510 EPOCH 3157
2024-02-06 12:02:29,098 Epoch 3157: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-06 12:02:29,099 EPOCH 3158
2024-02-06 12:02:39,742 Epoch 3158: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-06 12:02:39,742 EPOCH 3159
2024-02-06 12:02:48,403 [Epoch: 3159 Step: 00053700] Batch Recognition Loss:   0.000159 => Gls Tokens per Sec:     1005 || Batch Translation Loss:   0.011520 => Txt Tokens per Sec:     2793 || Lr: 0.000050
2024-02-06 12:02:50,477 Epoch 3159: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 12:02:50,478 EPOCH 3160
2024-02-06 12:03:01,063 Epoch 3160: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 12:03:01,064 EPOCH 3161
2024-02-06 12:03:12,043 Epoch 3161: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 12:03:12,043 EPOCH 3162
2024-02-06 12:03:22,799 Epoch 3162: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 12:03:22,799 EPOCH 3163
2024-02-06 12:03:33,483 Epoch 3163: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 12:03:33,484 EPOCH 3164
2024-02-06 12:03:44,156 Epoch 3164: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-06 12:03:44,156 EPOCH 3165
2024-02-06 12:03:50,872 [Epoch: 3165 Step: 00053800] Batch Recognition Loss:   0.001108 => Gls Tokens per Sec:     1105 || Batch Translation Loss:   0.055737 => Txt Tokens per Sec:     2973 || Lr: 0.000050
2024-02-06 12:03:54,855 Epoch 3165: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-06 12:03:54,855 EPOCH 3166
2024-02-06 12:04:05,779 Epoch 3166: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.42 
2024-02-06 12:04:05,780 EPOCH 3167
2024-02-06 12:04:16,535 Epoch 3167: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.03 
2024-02-06 12:04:16,535 EPOCH 3168
2024-02-06 12:04:27,214 Epoch 3168: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.84 
2024-02-06 12:04:27,215 EPOCH 3169
2024-02-06 12:04:37,821 Epoch 3169: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.43 
2024-02-06 12:04:37,821 EPOCH 3170
2024-02-06 12:04:48,624 Epoch 3170: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.29 
2024-02-06 12:04:48,624 EPOCH 3171
2024-02-06 12:04:54,354 [Epoch: 3171 Step: 00053900] Batch Recognition Loss:   0.001773 => Gls Tokens per Sec:     1072 || Batch Translation Loss:   0.025383 => Txt Tokens per Sec:     2694 || Lr: 0.000050
2024-02-06 12:04:59,446 Epoch 3171: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.71 
2024-02-06 12:04:59,446 EPOCH 3172
2024-02-06 12:05:10,054 Epoch 3172: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-06 12:05:10,054 EPOCH 3173
2024-02-06 12:05:20,802 Epoch 3173: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-06 12:05:20,802 EPOCH 3174
2024-02-06 12:05:31,592 Epoch 3174: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-06 12:05:31,592 EPOCH 3175
2024-02-06 12:05:42,149 Epoch 3175: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 12:05:42,149 EPOCH 3176
2024-02-06 12:05:52,900 Epoch 3176: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 12:05:52,901 EPOCH 3177
2024-02-06 12:05:58,706 [Epoch: 3177 Step: 00054000] Batch Recognition Loss:   0.004532 => Gls Tokens per Sec:      837 || Batch Translation Loss:   0.017156 => Txt Tokens per Sec:     2358 || Lr: 0.000050
2024-02-06 12:06:38,870 Validation result at epoch 3177, step    54000: duration: 40.1636s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.57843	Translation Loss: 101027.69531	PPL: 24116.70703
	Eval Metric: BLEU
	WER 2.97	(DEL: 0.00,	INS: 0.00,	SUB: 2.97)
	BLEU-4 0.51	(BLEU-1: 10.40,	BLEU-2: 3.23,	BLEU-3: 1.17,	BLEU-4: 0.51)
	CHRF 16.63	ROUGE 8.83
2024-02-06 12:06:38,872 Logging Recognition and Translation Outputs
2024-02-06 12:06:38,872 ========================================================================================================================
2024-02-06 12:06:38,872 Logging Sequence: 166_243.00
2024-02-06 12:06:38,872 	Gloss Reference :	A B+C+D+E
2024-02-06 12:06:38,873 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 12:06:38,873 	Gloss Alignment :	         
2024-02-06 12:06:38,873 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 12:06:38,874 	Text Reference  :	icc worked with members boards like bcci pcb cricket australia etc 
2024-02-06 12:06:38,874 	Text Hypothesis :	*** ****** the  auction was    now  take for the     first     time
2024-02-06 12:06:38,874 	Text Alignment  :	D   D      S    S       S      S    S    S   S       S         S   
2024-02-06 12:06:38,874 ========================================================================================================================
2024-02-06 12:06:38,874 Logging Sequence: 179_409.00
2024-02-06 12:06:38,874 	Gloss Reference :	A B+C+D+E
2024-02-06 12:06:38,874 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 12:06:38,875 	Gloss Alignment :	         
2024-02-06 12:06:38,875 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 12:06:38,875 	Text Reference  :	*** **** the passport was     at the wfi office   in ***** delhi
2024-02-06 12:06:38,876 	Text Hypothesis :	but then the team     members of the *** olympics in short while
2024-02-06 12:06:38,876 	Text Alignment  :	I   I        S        S       S      D   S           I     S    
2024-02-06 12:06:38,876 ========================================================================================================================
2024-02-06 12:06:38,876 Logging Sequence: 81_407.00
2024-02-06 12:06:38,876 	Gloss Reference :	A B+C+D+E
2024-02-06 12:06:38,877 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 12:06:38,877 	Gloss Alignment :	         
2024-02-06 12:06:38,877 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 12:06:38,878 	Text Reference  :	***** the government company -  national buildings construction corporation and they     will complete them in a time-bound manner    
2024-02-06 12:06:38,878 	Text Hypothesis :	since the ********** start   of the      t20       world        cup         the amrapali will ******** **** ** * also       interested
2024-02-06 12:06:38,878 	Text Alignment  :	I         D          S       S  S        S         S            S           S   S             D        D    D  D S          S         
2024-02-06 12:06:38,879 ========================================================================================================================
2024-02-06 12:06:38,879 Logging Sequence: 96_31.00
2024-02-06 12:06:38,879 	Gloss Reference :	A B+C+D+E
2024-02-06 12:06:38,879 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 12:06:38,879 	Gloss Alignment :	         
2024-02-06 12:06:38,879 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 12:06:38,880 	Text Reference  :	and then    2       teams will go on      to  play   the  final  
2024-02-06 12:06:38,880 	Text Hypothesis :	*** however india's lost  8    of india's win people felt relaxed
2024-02-06 12:06:38,880 	Text Alignment  :	D   S       S       S     S    S  S       S   S      S    S      
2024-02-06 12:06:38,880 ========================================================================================================================
2024-02-06 12:06:38,881 Logging Sequence: 160_87.00
2024-02-06 12:06:38,881 	Gloss Reference :	A B+C+D+E
2024-02-06 12:06:38,881 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 12:06:38,881 	Gloss Alignment :	         
2024-02-06 12:06:38,881 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 12:06:38,882 	Text Reference  :	**** ** kohli held a    press conference and said 
2024-02-06 12:06:38,882 	Text Hypothesis :	when we will  have sent to    the        t20 match
2024-02-06 12:06:38,882 	Text Alignment  :	I    I  S     S    S    S     S          S   S    
2024-02-06 12:06:38,882 ========================================================================================================================
2024-02-06 12:06:44,324 Epoch 3177: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 12:06:44,325 EPOCH 3178
2024-02-06 12:06:55,049 Epoch 3178: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 12:06:55,050 EPOCH 3179
2024-02-06 12:07:05,523 Epoch 3179: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 12:07:05,523 EPOCH 3180
2024-02-06 12:07:16,475 Epoch 3180: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 12:07:16,476 EPOCH 3181
2024-02-06 12:07:27,320 Epoch 3181: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 12:07:27,320 EPOCH 3182
2024-02-06 12:07:38,314 Epoch 3182: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 12:07:38,315 EPOCH 3183
2024-02-06 12:07:44,895 [Epoch: 3183 Step: 00054100] Batch Recognition Loss:   0.000610 => Gls Tokens per Sec:      544 || Batch Translation Loss:   0.023135 => Txt Tokens per Sec:     1559 || Lr: 0.000050
2024-02-06 12:07:49,304 Epoch 3183: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 12:07:49,304 EPOCH 3184
2024-02-06 12:08:00,090 Epoch 3184: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 12:08:00,090 EPOCH 3185
2024-02-06 12:08:10,867 Epoch 3185: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.23 
2024-02-06 12:08:10,867 EPOCH 3186
2024-02-06 12:08:21,749 Epoch 3186: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 12:08:21,749 EPOCH 3187
2024-02-06 12:08:32,728 Epoch 3187: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 12:08:32,729 EPOCH 3188
2024-02-06 12:08:43,316 Epoch 3188: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 12:08:43,316 EPOCH 3189
2024-02-06 12:08:44,211 [Epoch: 3189 Step: 00054200] Batch Recognition Loss:   0.000119 => Gls Tokens per Sec:     2868 || Batch Translation Loss:   0.009885 => Txt Tokens per Sec:     7066 || Lr: 0.000050
2024-02-06 12:08:54,137 Epoch 3189: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 12:08:54,138 EPOCH 3190
2024-02-06 12:09:05,029 Epoch 3190: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 12:09:05,029 EPOCH 3191
2024-02-06 12:09:15,939 Epoch 3191: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 12:09:15,940 EPOCH 3192
2024-02-06 12:09:26,713 Epoch 3192: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 12:09:26,713 EPOCH 3193
2024-02-06 12:09:37,571 Epoch 3193: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-06 12:09:37,572 EPOCH 3194
2024-02-06 12:09:48,382 Epoch 3194: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 12:09:48,382 EPOCH 3195
2024-02-06 12:09:50,605 [Epoch: 3195 Step: 00054300] Batch Recognition Loss:   0.000137 => Gls Tokens per Sec:      576 || Batch Translation Loss:   0.012230 => Txt Tokens per Sec:     1766 || Lr: 0.000050
2024-02-06 12:09:59,165 Epoch 3195: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 12:09:59,166 EPOCH 3196
2024-02-06 12:10:09,695 Epoch 3196: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 12:10:09,696 EPOCH 3197
2024-02-06 12:10:20,461 Epoch 3197: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 12:10:20,462 EPOCH 3198
2024-02-06 12:10:31,130 Epoch 3198: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 12:10:31,131 EPOCH 3199
2024-02-06 12:10:41,772 Epoch 3199: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-06 12:10:41,773 EPOCH 3200
2024-02-06 12:10:52,784 [Epoch: 3200 Step: 00054400] Batch Recognition Loss:   0.001090 => Gls Tokens per Sec:      965 || Batch Translation Loss:   0.011571 => Txt Tokens per Sec:     2669 || Lr: 0.000050
2024-02-06 12:10:52,785 Epoch 3200: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-06 12:10:52,785 EPOCH 3201
2024-02-06 12:11:03,458 Epoch 3201: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 12:11:03,458 EPOCH 3202
2024-02-06 12:11:14,238 Epoch 3202: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 12:11:14,238 EPOCH 3203
2024-02-06 12:11:24,985 Epoch 3203: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-06 12:11:24,986 EPOCH 3204
2024-02-06 12:11:35,540 Epoch 3204: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 12:11:35,540 EPOCH 3205
2024-02-06 12:11:46,200 Epoch 3205: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 12:11:46,201 EPOCH 3206
2024-02-06 12:11:56,660 [Epoch: 3206 Step: 00054500] Batch Recognition Loss:   0.004282 => Gls Tokens per Sec:      893 || Batch Translation Loss:   0.016069 => Txt Tokens per Sec:     2520 || Lr: 0.000050
2024-02-06 12:11:57,059 Epoch 3206: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 12:11:57,059 EPOCH 3207
2024-02-06 12:12:08,040 Epoch 3207: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 12:12:08,041 EPOCH 3208
2024-02-06 12:12:18,783 Epoch 3208: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 12:12:18,784 EPOCH 3209
2024-02-06 12:12:29,616 Epoch 3209: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 12:12:29,616 EPOCH 3210
2024-02-06 12:12:40,339 Epoch 3210: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 12:12:40,339 EPOCH 3211
2024-02-06 12:12:50,988 Epoch 3211: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-06 12:12:50,988 EPOCH 3212
2024-02-06 12:12:57,069 [Epoch: 3212 Step: 00054600] Batch Recognition Loss:   0.000221 => Gls Tokens per Sec:     1369 || Batch Translation Loss:   0.005321 => Txt Tokens per Sec:     3696 || Lr: 0.000050
2024-02-06 12:13:01,719 Epoch 3212: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 12:13:01,720 EPOCH 3213
2024-02-06 12:13:12,419 Epoch 3213: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-06 12:13:12,419 EPOCH 3214
2024-02-06 12:13:23,465 Epoch 3214: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-06 12:13:23,466 EPOCH 3215
2024-02-06 12:13:33,972 Epoch 3215: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-06 12:13:33,972 EPOCH 3216
2024-02-06 12:13:44,756 Epoch 3216: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-06 12:13:44,756 EPOCH 3217
2024-02-06 12:13:55,905 Epoch 3217: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-06 12:13:55,905 EPOCH 3218
2024-02-06 12:14:03,831 [Epoch: 3218 Step: 00054700] Batch Recognition Loss:   0.000337 => Gls Tokens per Sec:      856 || Batch Translation Loss:   0.007163 => Txt Tokens per Sec:     2356 || Lr: 0.000050
2024-02-06 12:14:06,598 Epoch 3218: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-06 12:14:06,598 EPOCH 3219
2024-02-06 12:14:17,230 Epoch 3219: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-06 12:14:17,230 EPOCH 3220
2024-02-06 12:14:27,886 Epoch 3220: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-06 12:14:27,886 EPOCH 3221
2024-02-06 12:14:38,705 Epoch 3221: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-06 12:14:38,706 EPOCH 3222
2024-02-06 12:14:49,774 Epoch 3222: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-06 12:14:49,775 EPOCH 3223
2024-02-06 12:15:00,314 Epoch 3223: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-06 12:15:00,315 EPOCH 3224
2024-02-06 12:15:08,003 [Epoch: 3224 Step: 00054800] Batch Recognition Loss:   0.000335 => Gls Tokens per Sec:      716 || Batch Translation Loss:   0.013004 => Txt Tokens per Sec:     2043 || Lr: 0.000050
2024-02-06 12:15:11,051 Epoch 3224: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 12:15:11,051 EPOCH 3225
2024-02-06 12:15:21,820 Epoch 3225: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 12:15:21,821 EPOCH 3226
2024-02-06 12:15:32,595 Epoch 3226: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-06 12:15:32,595 EPOCH 3227
2024-02-06 12:15:43,469 Epoch 3227: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-06 12:15:43,470 EPOCH 3228
2024-02-06 12:15:54,497 Epoch 3228: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-06 12:15:54,498 EPOCH 3229
2024-02-06 12:16:05,573 Epoch 3229: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-06 12:16:05,574 EPOCH 3230
2024-02-06 12:16:08,833 [Epoch: 3230 Step: 00054900] Batch Recognition Loss:   0.000177 => Gls Tokens per Sec:     1375 || Batch Translation Loss:   0.011667 => Txt Tokens per Sec:     3662 || Lr: 0.000050
2024-02-06 12:16:15,963 Epoch 3230: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-06 12:16:15,963 EPOCH 3231
2024-02-06 12:16:27,148 Epoch 3231: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-06 12:16:27,148 EPOCH 3232
2024-02-06 12:16:37,919 Epoch 3232: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-06 12:16:37,919 EPOCH 3233
2024-02-06 12:16:48,504 Epoch 3233: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-06 12:16:48,504 EPOCH 3234
2024-02-06 12:16:59,212 Epoch 3234: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-06 12:16:59,213 EPOCH 3235
2024-02-06 12:17:09,967 Epoch 3235: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 12:17:09,968 EPOCH 3236
2024-02-06 12:17:16,138 [Epoch: 3236 Step: 00055000] Batch Recognition Loss:   0.000364 => Gls Tokens per Sec:      476 || Batch Translation Loss:   0.024345 => Txt Tokens per Sec:     1515 || Lr: 0.000050
2024-02-06 12:17:20,589 Epoch 3236: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-06 12:17:20,589 EPOCH 3237
2024-02-06 12:17:31,363 Epoch 3237: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 12:17:31,363 EPOCH 3238
2024-02-06 12:17:41,839 Epoch 3238: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-06 12:17:41,840 EPOCH 3239
2024-02-06 12:17:52,381 Epoch 3239: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-06 12:17:52,382 EPOCH 3240
2024-02-06 12:18:03,159 Epoch 3240: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 12:18:03,160 EPOCH 3241
2024-02-06 12:18:13,732 Epoch 3241: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-06 12:18:13,733 EPOCH 3242
2024-02-06 12:18:14,229 [Epoch: 3242 Step: 00055100] Batch Recognition Loss:   0.000274 => Gls Tokens per Sec:     3882 || Batch Translation Loss:   0.017592 => Txt Tokens per Sec:     9664 || Lr: 0.000050
2024-02-06 12:18:24,255 Epoch 3242: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-06 12:18:24,256 EPOCH 3243
2024-02-06 12:18:34,962 Epoch 3243: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-06 12:18:34,963 EPOCH 3244
2024-02-06 12:18:45,616 Epoch 3244: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-06 12:18:45,617 EPOCH 3245
2024-02-06 12:18:55,857 Epoch 3245: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-06 12:18:55,857 EPOCH 3246
2024-02-06 12:19:06,418 Epoch 3246: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-06 12:19:06,419 EPOCH 3247
2024-02-06 12:19:17,122 Epoch 3247: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-06 12:19:17,122 EPOCH 3248
2024-02-06 12:19:18,921 [Epoch: 3248 Step: 00055200] Batch Recognition Loss:   0.000348 => Gls Tokens per Sec:      356 || Batch Translation Loss:   0.058605 => Txt Tokens per Sec:     1160 || Lr: 0.000050
2024-02-06 12:19:27,840 Epoch 3248: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-06 12:19:27,841 EPOCH 3249
2024-02-06 12:19:38,924 Epoch 3249: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.79 
2024-02-06 12:19:38,925 EPOCH 3250
2024-02-06 12:19:49,711 Epoch 3250: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-06 12:19:49,711 EPOCH 3251
2024-02-06 12:20:00,834 Epoch 3251: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-06 12:20:00,835 EPOCH 3252
2024-02-06 12:20:11,681 Epoch 3252: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-06 12:20:11,681 EPOCH 3253
2024-02-06 12:20:22,350 [Epoch: 3253 Step: 00055300] Batch Recognition Loss:   0.000262 => Gls Tokens per Sec:      936 || Batch Translation Loss:   0.026363 => Txt Tokens per Sec:     2617 || Lr: 0.000050
2024-02-06 12:20:22,522 Epoch 3253: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-06 12:20:22,522 EPOCH 3254
2024-02-06 12:20:33,459 Epoch 3254: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-06 12:20:33,460 EPOCH 3255
2024-02-06 12:20:44,424 Epoch 3255: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-06 12:20:44,424 EPOCH 3256
2024-02-06 12:20:55,139 Epoch 3256: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 12:20:55,140 EPOCH 3257
2024-02-06 12:21:06,090 Epoch 3257: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 12:21:06,091 EPOCH 3258
2024-02-06 12:21:17,028 Epoch 3258: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 12:21:17,028 EPOCH 3259
2024-02-06 12:21:27,463 [Epoch: 3259 Step: 00055400] Batch Recognition Loss:   0.000389 => Gls Tokens per Sec:      834 || Batch Translation Loss:   0.009299 => Txt Tokens per Sec:     2373 || Lr: 0.000050
2024-02-06 12:21:28,050 Epoch 3259: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 12:21:28,050 EPOCH 3260
2024-02-06 12:21:38,862 Epoch 3260: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-06 12:21:38,863 EPOCH 3261
2024-02-06 12:21:49,926 Epoch 3261: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-06 12:21:49,927 EPOCH 3262
2024-02-06 12:22:00,732 Epoch 3262: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-06 12:22:00,733 EPOCH 3263
2024-02-06 12:22:11,701 Epoch 3263: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-06 12:22:11,702 EPOCH 3264
2024-02-06 12:22:22,615 Epoch 3264: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.70 
2024-02-06 12:22:22,616 EPOCH 3265
2024-02-06 12:22:31,067 [Epoch: 3265 Step: 00055500] Batch Recognition Loss:   0.000260 => Gls Tokens per Sec:      878 || Batch Translation Loss:   0.040276 => Txt Tokens per Sec:     2471 || Lr: 0.000050
2024-02-06 12:22:33,538 Epoch 3265: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-06 12:22:33,538 EPOCH 3266
2024-02-06 12:22:44,236 Epoch 3266: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-06 12:22:44,237 EPOCH 3267
2024-02-06 12:22:54,975 Epoch 3267: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-06 12:22:54,976 EPOCH 3268
2024-02-06 12:23:05,586 Epoch 3268: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-06 12:23:05,586 EPOCH 3269
2024-02-06 12:23:16,561 Epoch 3269: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-06 12:23:16,562 EPOCH 3270
2024-02-06 12:23:27,109 Epoch 3270: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-06 12:23:27,109 EPOCH 3271
2024-02-06 12:23:34,052 [Epoch: 3271 Step: 00055600] Batch Recognition Loss:   0.000154 => Gls Tokens per Sec:      922 || Batch Translation Loss:   0.016558 => Txt Tokens per Sec:     2578 || Lr: 0.000050
2024-02-06 12:23:37,917 Epoch 3271: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-06 12:23:37,917 EPOCH 3272
2024-02-06 12:23:48,521 Epoch 3272: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-06 12:23:48,522 EPOCH 3273
2024-02-06 12:23:59,240 Epoch 3273: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-06 12:23:59,241 EPOCH 3274
2024-02-06 12:24:09,908 Epoch 3274: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 12:24:09,909 EPOCH 3275
2024-02-06 12:24:20,339 Epoch 3275: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 12:24:20,340 EPOCH 3276
2024-02-06 12:24:31,153 Epoch 3276: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-06 12:24:31,153 EPOCH 3277
2024-02-06 12:24:36,752 [Epoch: 3277 Step: 00055700] Batch Recognition Loss:   0.000618 => Gls Tokens per Sec:      868 || Batch Translation Loss:   0.015773 => Txt Tokens per Sec:     2341 || Lr: 0.000050
2024-02-06 12:24:41,854 Epoch 3277: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-06 12:24:41,854 EPOCH 3278
2024-02-06 12:24:52,789 Epoch 3278: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 12:24:52,790 EPOCH 3279
2024-02-06 12:25:03,621 Epoch 3279: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 12:25:03,622 EPOCH 3280
2024-02-06 12:25:14,449 Epoch 3280: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 12:25:14,449 EPOCH 3281
2024-02-06 12:25:25,100 Epoch 3281: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 12:25:25,100 EPOCH 3282
2024-02-06 12:25:36,045 Epoch 3282: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-06 12:25:36,045 EPOCH 3283
2024-02-06 12:25:38,527 [Epoch: 3283 Step: 00055800] Batch Recognition Loss:   0.000119 => Gls Tokens per Sec:     1548 || Batch Translation Loss:   0.008174 => Txt Tokens per Sec:     3889 || Lr: 0.000050
2024-02-06 12:25:46,536 Epoch 3283: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 12:25:46,537 EPOCH 3284
2024-02-06 12:25:57,516 Epoch 3284: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-06 12:25:57,517 EPOCH 3285
2024-02-06 12:26:08,042 Epoch 3285: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 12:26:08,043 EPOCH 3286
2024-02-06 12:26:18,938 Epoch 3286: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-06 12:26:18,938 EPOCH 3287
2024-02-06 12:26:29,761 Epoch 3287: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-06 12:26:29,762 EPOCH 3288
2024-02-06 12:26:40,179 Epoch 3288: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-06 12:26:40,179 EPOCH 3289
2024-02-06 12:26:42,677 [Epoch: 3289 Step: 00055900] Batch Recognition Loss:   0.000431 => Gls Tokens per Sec:     1026 || Batch Translation Loss:   0.017366 => Txt Tokens per Sec:     2608 || Lr: 0.000050
2024-02-06 12:26:50,705 Epoch 3289: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 12:26:50,706 EPOCH 3290
2024-02-06 12:27:01,233 Epoch 3290: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 12:27:01,234 EPOCH 3291
2024-02-06 12:27:12,064 Epoch 3291: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 12:27:12,065 EPOCH 3292
2024-02-06 12:27:22,913 Epoch 3292: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-06 12:27:22,914 EPOCH 3293
2024-02-06 12:27:33,711 Epoch 3293: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-06 12:27:33,712 EPOCH 3294
2024-02-06 12:27:44,601 Epoch 3294: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-06 12:27:44,601 EPOCH 3295
2024-02-06 12:27:45,070 [Epoch: 3295 Step: 00056000] Batch Recognition Loss:   0.000172 => Gls Tokens per Sec:     2735 || Batch Translation Loss:   0.025627 => Txt Tokens per Sec:     7891 || Lr: 0.000050
2024-02-06 12:28:25,557 Validation result at epoch 3295, step    56000: duration: 40.4863s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.58749	Translation Loss: 101558.12500	PPL: 25428.84766
	Eval Metric: BLEU
	WER 2.90	(DEL: 0.00,	INS: 0.00,	SUB: 2.90)
	BLEU-4 0.00	(BLEU-1: 9.92,	BLEU-2: 2.57,	BLEU-3: 0.75,	BLEU-4: 0.00)
	CHRF 16.30	ROUGE 8.10
2024-02-06 12:28:25,558 Logging Recognition and Translation Outputs
2024-02-06 12:28:25,559 ========================================================================================================================
2024-02-06 12:28:25,559 Logging Sequence: 177_167.00
2024-02-06 12:28:25,559 	Gloss Reference :	A B+C+D+E
2024-02-06 12:28:25,559 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 12:28:25,560 	Gloss Alignment :	         
2024-02-06 12:28:25,560 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 12:28:25,561 	Text Reference  :	******* * ****** this is because    sushil wanted to establish his fear to     ensure no  one   would oppose him  
2024-02-06 12:28:25,562 	Text Hypothesis :	however a police want to interogate sushil ajay   to find      out the  motive behind the brawl which went   viral
2024-02-06 12:28:25,562 	Text Alignment  :	I       I I      S    S  S                 S         S         S   S    S      S      S   S     S     S      S    
2024-02-06 12:28:25,562 ========================================================================================================================
2024-02-06 12:28:25,562 Logging Sequence: 127_140.00
2024-02-06 12:28:25,562 	Gloss Reference :	A B+C+D+E
2024-02-06 12:28:25,562 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 12:28:25,563 	Gloss Alignment :	         
2024-02-06 12:28:25,563 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 12:28:25,565 	Text Reference  :	this is  india'    3rd medal   in the world athletics championships he is very talented and his ******* performance is    highly impressive
2024-02-06 12:28:25,565 	Text Hypothesis :	the  now cricketer is  because of the ***** ********* ************* ** ** same room     for his alleged private     chats leaked in        
2024-02-06 12:28:25,565 	Text Alignment  :	S    S   S         S   S       S      D     D         D             D  D  S    S        S       I       S           S     S      S         
2024-02-06 12:28:25,565 ========================================================================================================================
2024-02-06 12:28:25,565 Logging Sequence: 126_200.00
2024-02-06 12:28:25,565 	Gloss Reference :	A B+C+D+E
2024-02-06 12:28:25,566 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 12:28:25,566 	Gloss Alignment :	         
2024-02-06 12:28:25,566 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 12:28:25,566 	Text Reference  :	let me   tell you  about them
2024-02-06 12:28:25,566 	Text Hypothesis :	we  were in   same in    goa 
2024-02-06 12:28:25,567 	Text Alignment  :	S   S    S    S    S     S   
2024-02-06 12:28:25,567 ========================================================================================================================
2024-02-06 12:28:25,567 Logging Sequence: 104_119.00
2024-02-06 12:28:25,567 	Gloss Reference :	A B+C+D+E    
2024-02-06 12:28:25,567 	Gloss Hypothesis:	A B+C+D+E+D+E
2024-02-06 12:28:25,567 	Gloss Alignment :	  S          
2024-02-06 12:28:25,567 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 12:28:25,569 	Text Reference  :	famous chess players like viswanathan anand  and praggnanandhaa's coach  r      b ramesh congratulated him     for his     impressive performance
2024-02-06 12:28:25,569 	Text Hypothesis :	****** ***** ******* the  third       charge and that             vinesh posted a other  celeb         parents and weekend double     headers    
2024-02-06 12:28:25,569 	Text Alignment  :	D      D     D       S    S           S          S                S      S      S S      S             S       S   S       S          S          
2024-02-06 12:28:25,569 ========================================================================================================================
2024-02-06 12:28:25,570 Logging Sequence: 172_267.00
2024-02-06 12:28:25,570 	Gloss Reference :	A B+C+D+E
2024-02-06 12:28:25,570 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 12:28:25,570 	Gloss Alignment :	         
2024-02-06 12:28:25,570 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 12:28:25,570 	Text Reference  :	such provisions have  been made 
2024-02-06 12:28:25,571 	Text Hypothesis :	**** and        media went viral
2024-02-06 12:28:25,571 	Text Alignment  :	D    S          S     S    S    
2024-02-06 12:28:25,571 ========================================================================================================================
2024-02-06 12:28:36,166 Epoch 3295: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-06 12:28:36,167 EPOCH 3296
2024-02-06 12:28:46,767 Epoch 3296: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-06 12:28:46,767 EPOCH 3297
2024-02-06 12:28:57,653 Epoch 3297: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-06 12:28:57,653 EPOCH 3298
2024-02-06 12:29:08,643 Epoch 3298: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-06 12:29:08,643 EPOCH 3299
2024-02-06 12:29:19,353 Epoch 3299: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-06 12:29:19,354 EPOCH 3300
2024-02-06 12:29:29,916 [Epoch: 3300 Step: 00056100] Batch Recognition Loss:   0.000316 => Gls Tokens per Sec:     1006 || Batch Translation Loss:   0.031590 => Txt Tokens per Sec:     2782 || Lr: 0.000050
2024-02-06 12:29:29,917 Epoch 3300: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-06 12:29:29,917 EPOCH 3301
2024-02-06 12:29:40,562 Epoch 3301: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-06 12:29:40,563 EPOCH 3302
2024-02-06 12:29:51,209 Epoch 3302: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-06 12:29:51,209 EPOCH 3303
2024-02-06 12:30:02,001 Epoch 3303: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-06 12:30:02,001 EPOCH 3304
2024-02-06 12:30:12,694 Epoch 3304: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-06 12:30:12,695 EPOCH 3305
2024-02-06 12:30:23,324 Epoch 3305: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-06 12:30:23,325 EPOCH 3306
2024-02-06 12:30:33,568 [Epoch: 3306 Step: 00056200] Batch Recognition Loss:   0.000457 => Gls Tokens per Sec:      912 || Batch Translation Loss:   0.064946 => Txt Tokens per Sec:     2548 || Lr: 0.000050
2024-02-06 12:30:33,939 Epoch 3306: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-06 12:30:33,939 EPOCH 3307
2024-02-06 12:30:44,611 Epoch 3307: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-06 12:30:44,611 EPOCH 3308
2024-02-06 12:30:55,437 Epoch 3308: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-06 12:30:55,438 EPOCH 3309
2024-02-06 12:31:06,096 Epoch 3309: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-06 12:31:06,097 EPOCH 3310
2024-02-06 12:31:16,792 Epoch 3310: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-06 12:31:16,792 EPOCH 3311
2024-02-06 12:31:27,541 Epoch 3311: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-06 12:31:27,541 EPOCH 3312
2024-02-06 12:31:37,481 [Epoch: 3312 Step: 00056300] Batch Recognition Loss:   0.001288 => Gls Tokens per Sec:      811 || Batch Translation Loss:   0.028618 => Txt Tokens per Sec:     2252 || Lr: 0.000050
2024-02-06 12:31:38,513 Epoch 3312: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-06 12:31:38,514 EPOCH 3313
2024-02-06 12:31:49,260 Epoch 3313: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-06 12:31:49,261 EPOCH 3314
2024-02-06 12:31:59,972 Epoch 3314: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-06 12:31:59,972 EPOCH 3315
2024-02-06 12:32:10,708 Epoch 3315: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 12:32:10,708 EPOCH 3316
2024-02-06 12:32:21,565 Epoch 3316: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 12:32:21,565 EPOCH 3317
2024-02-06 12:32:32,413 Epoch 3317: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 12:32:32,414 EPOCH 3318
2024-02-06 12:32:39,934 [Epoch: 3318 Step: 00056400] Batch Recognition Loss:   0.000201 => Gls Tokens per Sec:      936 || Batch Translation Loss:   0.011399 => Txt Tokens per Sec:     2726 || Lr: 0.000050
2024-02-06 12:32:43,365 Epoch 3318: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-06 12:32:43,365 EPOCH 3319
2024-02-06 12:32:54,249 Epoch 3319: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 12:32:54,249 EPOCH 3320
2024-02-06 12:33:05,202 Epoch 3320: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 12:33:05,202 EPOCH 3321
2024-02-06 12:33:15,995 Epoch 3321: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 12:33:15,995 EPOCH 3322
2024-02-06 12:33:26,773 Epoch 3322: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 12:33:26,774 EPOCH 3323
2024-02-06 12:33:37,402 Epoch 3323: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 12:33:37,403 EPOCH 3324
2024-02-06 12:33:45,166 [Epoch: 3324 Step: 00056500] Batch Recognition Loss:   0.000149 => Gls Tokens per Sec:      709 || Batch Translation Loss:   0.012364 => Txt Tokens per Sec:     2036 || Lr: 0.000050
2024-02-06 12:33:48,278 Epoch 3324: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-06 12:33:48,278 EPOCH 3325
2024-02-06 12:33:59,035 Epoch 3325: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 12:33:59,035 EPOCH 3326
2024-02-06 12:34:09,535 Epoch 3326: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 12:34:09,536 EPOCH 3327
2024-02-06 12:34:20,544 Epoch 3327: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-06 12:34:20,544 EPOCH 3328
2024-02-06 12:34:31,146 Epoch 3328: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 12:34:31,147 EPOCH 3329
2024-02-06 12:34:41,958 Epoch 3329: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-06 12:34:41,959 EPOCH 3330
2024-02-06 12:34:47,093 [Epoch: 3330 Step: 00056600] Batch Recognition Loss:   0.000301 => Gls Tokens per Sec:      822 || Batch Translation Loss:   0.013808 => Txt Tokens per Sec:     2291 || Lr: 0.000050
2024-02-06 12:34:52,735 Epoch 3330: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-06 12:34:52,736 EPOCH 3331
2024-02-06 12:35:03,313 Epoch 3331: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-06 12:35:03,314 EPOCH 3332
2024-02-06 12:35:13,887 Epoch 3332: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 12:35:13,888 EPOCH 3333
2024-02-06 12:35:24,817 Epoch 3333: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-06 12:35:24,818 EPOCH 3334
2024-02-06 12:35:35,978 Epoch 3334: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 12:35:35,979 EPOCH 3335
2024-02-06 12:35:46,724 Epoch 3335: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-06 12:35:46,724 EPOCH 3336
2024-02-06 12:35:49,965 [Epoch: 3336 Step: 00056700] Batch Recognition Loss:   0.000215 => Gls Tokens per Sec:      907 || Batch Translation Loss:   0.020958 => Txt Tokens per Sec:     2427 || Lr: 0.000050
2024-02-06 12:35:57,567 Epoch 3336: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-06 12:35:57,568 EPOCH 3337
2024-02-06 12:36:08,539 Epoch 3337: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 12:36:08,539 EPOCH 3338
2024-02-06 12:36:19,396 Epoch 3338: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 12:36:19,397 EPOCH 3339
2024-02-06 12:36:30,284 Epoch 3339: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 12:36:30,284 EPOCH 3340
2024-02-06 12:36:41,074 Epoch 3340: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 12:36:41,075 EPOCH 3341
2024-02-06 12:36:51,861 Epoch 3341: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-06 12:36:51,861 EPOCH 3342
2024-02-06 12:36:52,538 [Epoch: 3342 Step: 00056800] Batch Recognition Loss:   0.000148 => Gls Tokens per Sec:     2843 || Batch Translation Loss:   0.019147 => Txt Tokens per Sec:     7134 || Lr: 0.000050
2024-02-06 12:37:02,592 Epoch 3342: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-06 12:37:02,592 EPOCH 3343
2024-02-06 12:37:13,343 Epoch 3343: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.09 
2024-02-06 12:37:13,344 EPOCH 3344
2024-02-06 12:37:24,322 Epoch 3344: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.41 
2024-02-06 12:37:24,322 EPOCH 3345
2024-02-06 12:37:35,259 Epoch 3345: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.83 
2024-02-06 12:37:35,260 EPOCH 3346
2024-02-06 12:37:45,969 Epoch 3346: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.27 
2024-02-06 12:37:45,970 EPOCH 3347
2024-02-06 12:37:56,733 Epoch 3347: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-06 12:37:56,733 EPOCH 3348
2024-02-06 12:37:57,087 [Epoch: 3348 Step: 00056900] Batch Recognition Loss:   0.000853 => Gls Tokens per Sec:     1813 || Batch Translation Loss:   0.031458 => Txt Tokens per Sec:     5751 || Lr: 0.000050
2024-02-06 12:38:07,402 Epoch 3348: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-06 12:38:07,403 EPOCH 3349
2024-02-06 12:38:17,904 Epoch 3349: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-06 12:38:17,905 EPOCH 3350
2024-02-06 12:38:28,521 Epoch 3350: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-06 12:38:28,521 EPOCH 3351
2024-02-06 12:38:39,343 Epoch 3351: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.41 
2024-02-06 12:38:39,343 EPOCH 3352
2024-02-06 12:38:49,704 Epoch 3352: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-06 12:38:49,705 EPOCH 3353
2024-02-06 12:39:00,223 [Epoch: 3353 Step: 00057000] Batch Recognition Loss:   0.000278 => Gls Tokens per Sec:      949 || Batch Translation Loss:   0.020218 => Txt Tokens per Sec:     2622 || Lr: 0.000050
2024-02-06 12:39:00,432 Epoch 3353: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.41 
2024-02-06 12:39:00,432 EPOCH 3354
2024-02-06 12:39:11,138 Epoch 3354: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-06 12:39:11,139 EPOCH 3355
2024-02-06 12:39:21,798 Epoch 3355: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 12:39:21,799 EPOCH 3356
2024-02-06 12:39:32,429 Epoch 3356: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 12:39:32,430 EPOCH 3357
2024-02-06 12:39:43,184 Epoch 3357: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 12:39:43,185 EPOCH 3358
2024-02-06 12:39:53,751 Epoch 3358: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.24 
2024-02-06 12:39:53,751 EPOCH 3359
2024-02-06 12:40:02,092 [Epoch: 3359 Step: 00057100] Batch Recognition Loss:   0.000156 => Gls Tokens per Sec:     1043 || Batch Translation Loss:   0.013393 => Txt Tokens per Sec:     2920 || Lr: 0.000050
2024-02-06 12:40:04,532 Epoch 3359: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.24 
2024-02-06 12:40:04,532 EPOCH 3360
2024-02-06 12:40:15,331 Epoch 3360: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 12:40:15,332 EPOCH 3361
2024-02-06 12:40:25,875 Epoch 3361: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 12:40:25,876 EPOCH 3362
2024-02-06 12:40:36,467 Epoch 3362: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 12:40:36,467 EPOCH 3363
2024-02-06 12:40:47,265 Epoch 3363: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 12:40:47,265 EPOCH 3364
2024-02-06 12:40:58,228 Epoch 3364: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-06 12:40:58,229 EPOCH 3365
2024-02-06 12:41:04,934 [Epoch: 3365 Step: 00057200] Batch Recognition Loss:   0.000219 => Gls Tokens per Sec:     1107 || Batch Translation Loss:   0.026533 => Txt Tokens per Sec:     2993 || Lr: 0.000050
2024-02-06 12:41:09,073 Epoch 3365: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 12:41:09,073 EPOCH 3366
2024-02-06 12:41:20,105 Epoch 3366: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-06 12:41:20,105 EPOCH 3367
2024-02-06 12:41:30,978 Epoch 3367: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 12:41:30,978 EPOCH 3368
2024-02-06 12:41:41,812 Epoch 3368: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 12:41:41,812 EPOCH 3369
2024-02-06 12:41:52,445 Epoch 3369: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-06 12:41:52,445 EPOCH 3370
2024-02-06 12:42:03,078 Epoch 3370: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 12:42:03,079 EPOCH 3371
2024-02-06 12:42:12,202 [Epoch: 3371 Step: 00057300] Batch Recognition Loss:   0.000184 => Gls Tokens per Sec:      673 || Batch Translation Loss:   0.013393 => Txt Tokens per Sec:     1887 || Lr: 0.000050
2024-02-06 12:42:13,903 Epoch 3371: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-06 12:42:13,903 EPOCH 3372
2024-02-06 12:42:24,566 Epoch 3372: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 12:42:24,566 EPOCH 3373
2024-02-06 12:42:35,425 Epoch 3373: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 12:42:35,426 EPOCH 3374
2024-02-06 12:42:46,514 Epoch 3374: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 12:42:46,514 EPOCH 3375
2024-02-06 12:42:57,284 Epoch 3375: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 12:42:57,284 EPOCH 3376
2024-02-06 12:43:08,026 Epoch 3376: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 12:43:08,026 EPOCH 3377
2024-02-06 12:43:13,117 [Epoch: 3377 Step: 00057400] Batch Recognition Loss:   0.000313 => Gls Tokens per Sec:     1006 || Batch Translation Loss:   0.016216 => Txt Tokens per Sec:     2851 || Lr: 0.000050
2024-02-06 12:43:18,760 Epoch 3377: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 12:43:18,761 EPOCH 3378
2024-02-06 12:43:29,399 Epoch 3378: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 12:43:29,400 EPOCH 3379
2024-02-06 12:43:40,083 Epoch 3379: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 12:43:40,083 EPOCH 3380
2024-02-06 12:43:51,046 Epoch 3380: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 12:43:51,047 EPOCH 3381
2024-02-06 12:44:01,811 Epoch 3381: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-06 12:44:01,811 EPOCH 3382
2024-02-06 12:44:12,529 Epoch 3382: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 12:44:12,530 EPOCH 3383
2024-02-06 12:44:15,989 [Epoch: 3383 Step: 00057500] Batch Recognition Loss:   0.000177 => Gls Tokens per Sec:     1035 || Batch Translation Loss:   0.013759 => Txt Tokens per Sec:     2843 || Lr: 0.000050
2024-02-06 12:44:23,125 Epoch 3383: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 12:44:23,125 EPOCH 3384
2024-02-06 12:44:33,812 Epoch 3384: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 12:44:33,813 EPOCH 3385
2024-02-06 12:44:44,325 Epoch 3385: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-06 12:44:44,326 EPOCH 3386
2024-02-06 12:44:55,103 Epoch 3386: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-06 12:44:55,104 EPOCH 3387
2024-02-06 12:45:05,730 Epoch 3387: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-06 12:45:05,730 EPOCH 3388
2024-02-06 12:45:16,457 Epoch 3388: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-06 12:45:16,457 EPOCH 3389
2024-02-06 12:45:19,056 [Epoch: 3389 Step: 00057600] Batch Recognition Loss:   0.000138 => Gls Tokens per Sec:      986 || Batch Translation Loss:   0.066639 => Txt Tokens per Sec:     2998 || Lr: 0.000050
2024-02-06 12:45:26,989 Epoch 3389: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 12:45:26,989 EPOCH 3390
2024-02-06 12:45:37,586 Epoch 3390: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-06 12:45:37,586 EPOCH 3391
2024-02-06 12:45:48,583 Epoch 3391: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 12:45:48,583 EPOCH 3392
2024-02-06 12:45:59,292 Epoch 3392: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-06 12:45:59,292 EPOCH 3393
2024-02-06 12:46:10,101 Epoch 3393: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-06 12:46:10,102 EPOCH 3394
2024-02-06 12:46:21,028 Epoch 3394: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-06 12:46:21,028 EPOCH 3395
2024-02-06 12:46:22,911 [Epoch: 3395 Step: 00057700] Batch Recognition Loss:   0.000255 => Gls Tokens per Sec:      680 || Batch Translation Loss:   0.024519 => Txt Tokens per Sec:     1858 || Lr: 0.000050
2024-02-06 12:46:31,919 Epoch 3395: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-06 12:46:31,919 EPOCH 3396
2024-02-06 12:46:42,844 Epoch 3396: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.79 
2024-02-06 12:46:42,845 EPOCH 3397
2024-02-06 12:46:53,682 Epoch 3397: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-06 12:46:53,682 EPOCH 3398
2024-02-06 12:47:04,114 Epoch 3398: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-06 12:47:04,115 EPOCH 3399
2024-02-06 12:47:14,868 Epoch 3399: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-06 12:47:14,869 EPOCH 3400
2024-02-06 12:47:25,462 [Epoch: 3400 Step: 00057800] Batch Recognition Loss:   0.000175 => Gls Tokens per Sec:     1003 || Batch Translation Loss:   0.038334 => Txt Tokens per Sec:     2774 || Lr: 0.000050
2024-02-06 12:47:25,462 Epoch 3400: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-06 12:47:25,462 EPOCH 3401
2024-02-06 12:47:36,397 Epoch 3401: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-06 12:47:36,397 EPOCH 3402
2024-02-06 12:47:47,217 Epoch 3402: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-06 12:47:47,218 EPOCH 3403
2024-02-06 12:47:58,094 Epoch 3403: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-06 12:47:58,095 EPOCH 3404
2024-02-06 12:48:09,082 Epoch 3404: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-06 12:48:09,083 EPOCH 3405
2024-02-06 12:48:19,800 Epoch 3405: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-06 12:48:19,801 EPOCH 3406
2024-02-06 12:48:28,659 [Epoch: 3406 Step: 00057900] Batch Recognition Loss:   0.000281 => Gls Tokens per Sec:     1055 || Batch Translation Loss:   0.115679 => Txt Tokens per Sec:     2886 || Lr: 0.000050
2024-02-06 12:48:30,702 Epoch 3406: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.24 
2024-02-06 12:48:30,702 EPOCH 3407
2024-02-06 12:48:41,622 Epoch 3407: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.42 
2024-02-06 12:48:41,623 EPOCH 3408
2024-02-06 12:48:52,083 Epoch 3408: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.85 
2024-02-06 12:48:52,083 EPOCH 3409
2024-02-06 12:49:02,709 Epoch 3409: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-06 12:49:02,710 EPOCH 3410
2024-02-06 12:49:13,403 Epoch 3410: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-06 12:49:13,403 EPOCH 3411
2024-02-06 12:49:24,001 Epoch 3411: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-06 12:49:24,001 EPOCH 3412
2024-02-06 12:49:30,781 [Epoch: 3412 Step: 00058000] Batch Recognition Loss:   0.000441 => Gls Tokens per Sec:     1189 || Batch Translation Loss:   0.021750 => Txt Tokens per Sec:     3193 || Lr: 0.000050
2024-02-06 12:50:11,294 Validation result at epoch 3412, step    58000: duration: 40.5106s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.56330	Translation Loss: 102287.64844	PPL: 27350.90820
	Eval Metric: BLEU
	WER 2.75	(DEL: 0.00,	INS: 0.00,	SUB: 2.75)
	BLEU-4 0.53	(BLEU-1: 10.70,	BLEU-2: 3.09,	BLEU-3: 1.15,	BLEU-4: 0.53)
	CHRF 17.01	ROUGE 8.74
2024-02-06 12:50:11,295 Logging Recognition and Translation Outputs
2024-02-06 12:50:11,296 ========================================================================================================================
2024-02-06 12:50:11,297 Logging Sequence: 60_264.00
2024-02-06 12:50:11,298 	Gloss Reference :	A B+C+D+E
2024-02-06 12:50:11,298 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 12:50:11,298 	Gloss Alignment :	         
2024-02-06 12:50:11,298 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 12:50:11,300 	Text Reference  :	******* plus  do  you     know      that a sex tape of his     with two     women  had gone viral
2024-02-06 12:50:11,300 	Text Hypothesis :	however there was playing instagram as   a *** **** ** message on   twitter handle and the  fifa 
2024-02-06 12:50:11,300 	Text Alignment  :	I       S     S   S       S         S      D   D    D  S       S    S       S      S   S    S    
2024-02-06 12:50:11,300 ========================================================================================================================
2024-02-06 12:50:11,300 Logging Sequence: 100_50.00
2024-02-06 12:50:11,301 	Gloss Reference :	A B+C+D+E
2024-02-06 12:50:11,301 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 12:50:11,301 	Gloss Alignment :	         
2024-02-06 12:50:11,301 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 12:50:11,302 	Text Reference  :	******* with virat kohli as      the captain
2024-02-06 12:50:11,302 	Text Hypothesis :	because of   the   same  reports are delayed
2024-02-06 12:50:11,302 	Text Alignment  :	I       S    S     S     S       S   S      
2024-02-06 12:50:11,302 ========================================================================================================================
2024-02-06 12:50:11,302 Logging Sequence: 137_44.00
2024-02-06 12:50:11,302 	Gloss Reference :	A B+C+D+E
2024-02-06 12:50:11,302 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 12:50:11,303 	Gloss Alignment :	         
2024-02-06 12:50:11,303 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 12:50:11,304 	Text Reference  :	let me tell you the rules that qatar has announced for          the fans travelling for the world cup   
2024-02-06 12:50:11,304 	Text Hypothesis :	*** ** **** *** *** ***** **** raina has been      announcement but what happened   at  the ***** couple
2024-02-06 12:50:11,304 	Text Alignment  :	D   D  D    D   D   D     D    S         S         S            S   S    S          S       D     S     
2024-02-06 12:50:11,304 ========================================================================================================================
2024-02-06 12:50:11,305 Logging Sequence: 58_27.00
2024-02-06 12:50:11,305 	Gloss Reference :	A B+C+D+E
2024-02-06 12:50:11,305 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 12:50:11,305 	Gloss Alignment :	         
2024-02-06 12:50:11,305 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 12:50:11,306 	Text Reference  :	the 19th asian games 2022 were to    be   held in    hangzhou china
2024-02-06 12:50:11,306 	Text Hypothesis :	*** **** ***** india won  the  match with 263  balls they     lose 
2024-02-06 12:50:11,306 	Text Alignment  :	D   D    D     S     S    S    S     S    S    S     S        S    
2024-02-06 12:50:11,306 ========================================================================================================================
2024-02-06 12:50:11,307 Logging Sequence: 75_255.00
2024-02-06 12:50:11,307 	Gloss Reference :	A B+C+D+E  
2024-02-06 12:50:11,307 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-06 12:50:11,307 	Gloss Alignment :	  S        
2024-02-06 12:50:11,307 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 12:50:11,309 	Text Reference  :	we miss our  baby      boy    with    this ronaldo' total     baby count has      reached 5   with 2     boys 3   girls
2024-02-06 12:50:11,309 	Text Hypothesis :	** as   bcci president sourav ganguly and  board    secretary jay  shah  welcomed the     two new  teams to   the ipl  
2024-02-06 12:50:11,309 	Text Alignment  :	D  S    S    S         S      S       S    S        S         S    S     S        S       S   S    S     S    S   S    
2024-02-06 12:50:11,310 ========================================================================================================================
2024-02-06 12:50:15,299 Epoch 3412: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-06 12:50:15,299 EPOCH 3413
2024-02-06 12:50:26,193 Epoch 3413: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-06 12:50:26,194 EPOCH 3414
2024-02-06 12:50:36,635 Epoch 3414: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-06 12:50:36,636 EPOCH 3415
2024-02-06 12:50:47,419 Epoch 3415: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 12:50:47,420 EPOCH 3416
2024-02-06 12:50:58,105 Epoch 3416: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 12:50:58,106 EPOCH 3417
2024-02-06 12:51:08,672 Epoch 3417: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 12:51:08,673 EPOCH 3418
2024-02-06 12:51:18,007 [Epoch: 3418 Step: 00058100] Batch Recognition Loss:   0.000289 => Gls Tokens per Sec:      726 || Batch Translation Loss:   0.024760 => Txt Tokens per Sec:     2096 || Lr: 0.000050
2024-02-06 12:51:19,312 Epoch 3418: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 12:51:19,312 EPOCH 3419
2024-02-06 12:51:29,936 Epoch 3419: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 12:51:29,937 EPOCH 3420
2024-02-06 12:51:40,752 Epoch 3420: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 12:51:40,753 EPOCH 3421
2024-02-06 12:51:51,671 Epoch 3421: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-06 12:51:51,671 EPOCH 3422
2024-02-06 12:52:02,381 Epoch 3422: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 12:52:02,382 EPOCH 3423
2024-02-06 12:52:13,141 Epoch 3423: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 12:52:13,142 EPOCH 3424
2024-02-06 12:52:18,878 [Epoch: 3424 Step: 00058200] Batch Recognition Loss:   0.000128 => Gls Tokens per Sec:      959 || Batch Translation Loss:   0.016226 => Txt Tokens per Sec:     2595 || Lr: 0.000050
2024-02-06 12:52:23,994 Epoch 3424: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 12:52:23,994 EPOCH 3425
2024-02-06 12:52:34,656 Epoch 3425: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-06 12:52:34,656 EPOCH 3426
2024-02-06 12:52:45,500 Epoch 3426: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 12:52:45,500 EPOCH 3427
2024-02-06 12:52:56,373 Epoch 3427: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 12:52:56,374 EPOCH 3428
2024-02-06 12:53:06,967 Epoch 3428: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-06 12:53:06,968 EPOCH 3429
2024-02-06 12:53:17,640 Epoch 3429: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 12:53:17,640 EPOCH 3430
2024-02-06 12:53:21,076 [Epoch: 3430 Step: 00058300] Batch Recognition Loss:   0.000193 => Gls Tokens per Sec:     1304 || Batch Translation Loss:   0.013043 => Txt Tokens per Sec:     3623 || Lr: 0.000050
2024-02-06 12:53:28,370 Epoch 3430: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-06 12:53:28,370 EPOCH 3431
2024-02-06 12:53:39,196 Epoch 3431: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 12:53:39,197 EPOCH 3432
2024-02-06 12:53:50,050 Epoch 3432: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-06 12:53:50,051 EPOCH 3433
2024-02-06 12:54:01,099 Epoch 3433: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-06 12:54:01,100 EPOCH 3434
2024-02-06 12:54:11,973 Epoch 3434: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-06 12:54:11,974 EPOCH 3435
2024-02-06 12:54:22,663 Epoch 3435: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.20 
2024-02-06 12:54:22,664 EPOCH 3436
2024-02-06 12:54:25,162 [Epoch: 3436 Step: 00058400] Batch Recognition Loss:   0.000237 => Gls Tokens per Sec:     1282 || Batch Translation Loss:   0.007305 => Txt Tokens per Sec:     3620 || Lr: 0.000050
2024-02-06 12:54:33,353 Epoch 3436: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 12:54:33,354 EPOCH 3437
2024-02-06 12:54:44,190 Epoch 3437: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 12:54:44,191 EPOCH 3438
2024-02-06 12:54:54,687 Epoch 3438: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-06 12:54:54,688 EPOCH 3439
2024-02-06 12:55:05,426 Epoch 3439: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 12:55:05,427 EPOCH 3440
2024-02-06 12:55:16,250 Epoch 3440: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-06 12:55:16,251 EPOCH 3441
2024-02-06 12:55:26,985 Epoch 3441: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-06 12:55:26,986 EPOCH 3442
2024-02-06 12:55:29,792 [Epoch: 3442 Step: 00058500] Batch Recognition Loss:   0.000135 => Gls Tokens per Sec:      592 || Batch Translation Loss:   0.008351 => Txt Tokens per Sec:     1665 || Lr: 0.000050
2024-02-06 12:55:37,637 Epoch 3442: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 12:55:37,637 EPOCH 3443
2024-02-06 12:55:48,206 Epoch 3443: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 12:55:48,207 EPOCH 3444
2024-02-06 12:55:58,897 Epoch 3444: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-06 12:55:58,898 EPOCH 3445
2024-02-06 12:56:09,558 Epoch 3445: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-06 12:56:09,558 EPOCH 3446
2024-02-06 12:56:20,219 Epoch 3446: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.12 
2024-02-06 12:56:20,219 EPOCH 3447
2024-02-06 12:56:30,923 Epoch 3447: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.12 
2024-02-06 12:56:30,924 EPOCH 3448
2024-02-06 12:56:31,181 [Epoch: 3448 Step: 00058600] Batch Recognition Loss:   0.000347 => Gls Tokens per Sec:     2504 || Batch Translation Loss:   0.028867 => Txt Tokens per Sec:     7020 || Lr: 0.000050
2024-02-06 12:56:41,527 Epoch 3448: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-06 12:56:41,528 EPOCH 3449
2024-02-06 12:56:52,347 Epoch 3449: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-06 12:56:52,347 EPOCH 3450
2024-02-06 12:57:02,907 Epoch 3450: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-06 12:57:02,907 EPOCH 3451
2024-02-06 12:57:14,058 Epoch 3451: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-06 12:57:14,059 EPOCH 3452
2024-02-06 12:57:24,774 Epoch 3452: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-06 12:57:24,774 EPOCH 3453
2024-02-06 12:57:35,482 [Epoch: 3453 Step: 00058700] Batch Recognition Loss:   0.000311 => Gls Tokens per Sec:      932 || Batch Translation Loss:   0.025201 => Txt Tokens per Sec:     2610 || Lr: 0.000050
2024-02-06 12:57:35,649 Epoch 3453: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-06 12:57:35,649 EPOCH 3454
2024-02-06 12:57:46,312 Epoch 3454: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-06 12:57:46,314 EPOCH 3455
2024-02-06 12:57:57,145 Epoch 3455: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 12:57:57,145 EPOCH 3456
2024-02-06 12:58:07,928 Epoch 3456: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 12:58:07,929 EPOCH 3457
2024-02-06 12:58:18,750 Epoch 3457: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 12:58:18,750 EPOCH 3458
2024-02-06 12:58:29,523 Epoch 3458: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 12:58:29,524 EPOCH 3459
2024-02-06 12:58:37,532 [Epoch: 3459 Step: 00058800] Batch Recognition Loss:   0.000421 => Gls Tokens per Sec:     1087 || Batch Translation Loss:   0.008169 => Txt Tokens per Sec:     2956 || Lr: 0.000050
2024-02-06 12:58:40,084 Epoch 3459: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 12:58:40,084 EPOCH 3460
2024-02-06 12:58:50,686 Epoch 3460: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 12:58:50,687 EPOCH 3461
2024-02-06 12:59:01,311 Epoch 3461: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 12:59:01,312 EPOCH 3462
2024-02-06 12:59:11,811 Epoch 3462: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-06 12:59:11,811 EPOCH 3463
2024-02-06 12:59:22,880 Epoch 3463: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.36 
2024-02-06 12:59:22,881 EPOCH 3464
2024-02-06 12:59:33,507 Epoch 3464: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-06 12:59:33,508 EPOCH 3465
2024-02-06 12:59:41,717 [Epoch: 3465 Step: 00058900] Batch Recognition Loss:   0.000467 => Gls Tokens per Sec:      904 || Batch Translation Loss:   0.301990 => Txt Tokens per Sec:     2548 || Lr: 0.000050
2024-02-06 12:59:44,308 Epoch 3465: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.69 
2024-02-06 12:59:44,308 EPOCH 3466
2024-02-06 12:59:55,099 Epoch 3466: Total Training Recognition Loss 0.01  Total Training Translation Loss 5.62 
2024-02-06 12:59:55,099 EPOCH 3467
2024-02-06 13:00:05,760 Epoch 3467: Total Training Recognition Loss 0.05  Total Training Translation Loss 6.55 
2024-02-06 13:00:05,760 EPOCH 3468
2024-02-06 13:00:16,568 Epoch 3468: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.48 
2024-02-06 13:00:16,569 EPOCH 3469
2024-02-06 13:00:27,269 Epoch 3469: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.40 
2024-02-06 13:00:27,270 EPOCH 3470
2024-02-06 13:00:38,040 Epoch 3470: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.62 
2024-02-06 13:00:38,041 EPOCH 3471
2024-02-06 13:00:44,494 [Epoch: 3471 Step: 00059000] Batch Recognition Loss:   0.000357 => Gls Tokens per Sec:      992 || Batch Translation Loss:   0.017367 => Txt Tokens per Sec:     2619 || Lr: 0.000050
2024-02-06 13:00:48,631 Epoch 3471: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.37 
2024-02-06 13:00:48,631 EPOCH 3472
2024-02-06 13:00:59,175 Epoch 3472: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.34 
2024-02-06 13:00:59,175 EPOCH 3473
2024-02-06 13:01:09,875 Epoch 3473: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 13:01:09,876 EPOCH 3474
2024-02-06 13:01:20,575 Epoch 3474: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 13:01:20,575 EPOCH 3475
2024-02-06 13:01:31,366 Epoch 3475: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 13:01:31,366 EPOCH 3476
2024-02-06 13:01:42,015 Epoch 3476: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 13:01:42,015 EPOCH 3477
2024-02-06 13:01:45,217 [Epoch: 3477 Step: 00059100] Batch Recognition Loss:   0.000358 => Gls Tokens per Sec:     1599 || Batch Translation Loss:   0.019760 => Txt Tokens per Sec:     4137 || Lr: 0.000050
2024-02-06 13:01:52,510 Epoch 3477: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 13:01:52,511 EPOCH 3478
2024-02-06 13:02:03,272 Epoch 3478: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 13:02:03,273 EPOCH 3479
2024-02-06 13:02:14,106 Epoch 3479: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 13:02:14,106 EPOCH 3480
2024-02-06 13:02:24,843 Epoch 3480: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 13:02:24,843 EPOCH 3481
2024-02-06 13:02:35,419 Epoch 3481: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 13:02:35,420 EPOCH 3482
2024-02-06 13:02:46,496 Epoch 3482: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 13:02:46,496 EPOCH 3483
2024-02-06 13:02:49,463 [Epoch: 3483 Step: 00059200] Batch Recognition Loss:   0.000366 => Gls Tokens per Sec:     1295 || Batch Translation Loss:   0.017623 => Txt Tokens per Sec:     3755 || Lr: 0.000050
2024-02-06 13:02:57,318 Epoch 3483: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 13:02:57,318 EPOCH 3484
2024-02-06 13:03:07,861 Epoch 3484: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 13:03:07,862 EPOCH 3485
2024-02-06 13:03:18,785 Epoch 3485: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 13:03:18,785 EPOCH 3486
2024-02-06 13:03:29,543 Epoch 3486: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 13:03:29,544 EPOCH 3487
2024-02-06 13:03:40,221 Epoch 3487: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 13:03:40,222 EPOCH 3488
2024-02-06 13:03:50,683 Epoch 3488: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 13:03:50,683 EPOCH 3489
2024-02-06 13:03:51,513 [Epoch: 3489 Step: 00059300] Batch Recognition Loss:   0.000205 => Gls Tokens per Sec:     3088 || Batch Translation Loss:   0.010293 => Txt Tokens per Sec:     8312 || Lr: 0.000050
2024-02-06 13:04:01,150 Epoch 3489: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-06 13:04:01,150 EPOCH 3490
2024-02-06 13:04:11,771 Epoch 3490: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 13:04:11,772 EPOCH 3491
2024-02-06 13:04:22,492 Epoch 3491: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-06 13:04:22,493 EPOCH 3492
2024-02-06 13:04:32,820 Epoch 3492: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 13:04:32,820 EPOCH 3493
2024-02-06 13:04:43,642 Epoch 3493: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-06 13:04:43,642 EPOCH 3494
2024-02-06 13:04:54,456 Epoch 3494: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 13:04:54,457 EPOCH 3495
2024-02-06 13:04:56,421 [Epoch: 3495 Step: 00059400] Batch Recognition Loss:   0.000147 => Gls Tokens per Sec:      653 || Batch Translation Loss:   0.010942 => Txt Tokens per Sec:     1905 || Lr: 0.000050
2024-02-06 13:05:05,251 Epoch 3495: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 13:05:05,252 EPOCH 3496
2024-02-06 13:05:15,969 Epoch 3496: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 13:05:15,970 EPOCH 3497
2024-02-06 13:05:26,729 Epoch 3497: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-06 13:05:26,729 EPOCH 3498
2024-02-06 13:05:37,486 Epoch 3498: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-06 13:05:37,486 EPOCH 3499
2024-02-06 13:05:48,440 Epoch 3499: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-06 13:05:48,441 EPOCH 3500
2024-02-06 13:05:59,155 [Epoch: 3500 Step: 00059500] Batch Recognition Loss:   0.000237 => Gls Tokens per Sec:      991 || Batch Translation Loss:   0.014501 => Txt Tokens per Sec:     2743 || Lr: 0.000050
2024-02-06 13:05:59,156 Epoch 3500: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 13:05:59,156 EPOCH 3501
2024-02-06 13:06:09,939 Epoch 3501: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-06 13:06:09,940 EPOCH 3502
2024-02-06 13:06:20,814 Epoch 3502: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-06 13:06:20,815 EPOCH 3503
2024-02-06 13:06:31,248 Epoch 3503: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-06 13:06:31,248 EPOCH 3504
2024-02-06 13:06:42,048 Epoch 3504: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 13:06:42,048 EPOCH 3505
2024-02-06 13:06:52,834 Epoch 3505: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 13:06:52,835 EPOCH 3506
2024-02-06 13:07:03,063 [Epoch: 3506 Step: 00059600] Batch Recognition Loss:   0.000252 => Gls Tokens per Sec:      913 || Batch Translation Loss:   0.014120 => Txt Tokens per Sec:     2564 || Lr: 0.000050
2024-02-06 13:07:03,414 Epoch 3506: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-06 13:07:03,414 EPOCH 3507
2024-02-06 13:07:14,028 Epoch 3507: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 13:07:14,029 EPOCH 3508
2024-02-06 13:07:24,770 Epoch 3508: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-06 13:07:24,770 EPOCH 3509
2024-02-06 13:07:35,481 Epoch 3509: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-06 13:07:35,481 EPOCH 3510
2024-02-06 13:07:46,268 Epoch 3510: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-06 13:07:46,269 EPOCH 3511
2024-02-06 13:07:56,987 Epoch 3511: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-06 13:07:56,988 EPOCH 3512
2024-02-06 13:08:02,964 [Epoch: 3512 Step: 00059700] Batch Recognition Loss:   0.000138 => Gls Tokens per Sec:     1392 || Batch Translation Loss:   0.018877 => Txt Tokens per Sec:     3765 || Lr: 0.000050
2024-02-06 13:08:07,942 Epoch 3512: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-06 13:08:07,942 EPOCH 3513
2024-02-06 13:08:18,925 Epoch 3513: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-06 13:08:18,926 EPOCH 3514
2024-02-06 13:08:29,712 Epoch 3514: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-06 13:08:29,713 EPOCH 3515
2024-02-06 13:08:40,524 Epoch 3515: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-06 13:08:40,524 EPOCH 3516
2024-02-06 13:08:50,986 Epoch 3516: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-06 13:08:50,986 EPOCH 3517
2024-02-06 13:09:01,742 Epoch 3517: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-06 13:09:01,743 EPOCH 3518
2024-02-06 13:09:08,773 [Epoch: 3518 Step: 00059800] Batch Recognition Loss:   0.000123 => Gls Tokens per Sec:     1001 || Batch Translation Loss:   0.007433 => Txt Tokens per Sec:     2719 || Lr: 0.000050
2024-02-06 13:09:12,426 Epoch 3518: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-06 13:09:12,427 EPOCH 3519
2024-02-06 13:09:23,189 Epoch 3519: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-06 13:09:23,190 EPOCH 3520
2024-02-06 13:09:33,982 Epoch 3520: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-06 13:09:33,982 EPOCH 3521
2024-02-06 13:09:44,834 Epoch 3521: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-06 13:09:44,835 EPOCH 3522
2024-02-06 13:09:55,637 Epoch 3522: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 13:09:55,638 EPOCH 3523
2024-02-06 13:10:06,619 Epoch 3523: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-06 13:10:06,619 EPOCH 3524
2024-02-06 13:10:14,612 [Epoch: 3524 Step: 00059900] Batch Recognition Loss:   0.000116 => Gls Tokens per Sec:      688 || Batch Translation Loss:   0.021479 => Txt Tokens per Sec:     2108 || Lr: 0.000050
2024-02-06 13:10:17,388 Epoch 3524: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-06 13:10:17,388 EPOCH 3525
2024-02-06 13:10:27,754 Epoch 3525: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-06 13:10:27,755 EPOCH 3526
2024-02-06 13:10:38,557 Epoch 3526: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 13:10:38,557 EPOCH 3527
2024-02-06 13:10:49,280 Epoch 3527: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 13:10:49,281 EPOCH 3528
2024-02-06 13:10:59,918 Epoch 3528: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-06 13:10:59,918 EPOCH 3529
2024-02-06 13:11:10,708 Epoch 3529: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 13:11:10,708 EPOCH 3530
2024-02-06 13:11:16,483 [Epoch: 3530 Step: 00060000] Batch Recognition Loss:   0.000173 => Gls Tokens per Sec:      731 || Batch Translation Loss:   0.014409 => Txt Tokens per Sec:     2118 || Lr: 0.000050
2024-02-06 13:11:56,949 Validation result at epoch 3530, step    60000: duration: 40.4653s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.61630	Translation Loss: 103086.14062	PPL: 29621.56445
	Eval Metric: BLEU
	WER 2.90	(DEL: 0.00,	INS: 0.00,	SUB: 2.90)
	BLEU-4 0.00	(BLEU-1: 10.40,	BLEU-2: 3.14,	BLEU-3: 1.04,	BLEU-4: 0.00)
	CHRF 16.78	ROUGE 8.83
2024-02-06 13:11:56,950 Logging Recognition and Translation Outputs
2024-02-06 13:11:56,950 ========================================================================================================================
2024-02-06 13:11:56,951 Logging Sequence: 75_58.00
2024-02-06 13:11:56,951 	Gloss Reference :	A B+C+D+E
2024-02-06 13:11:56,951 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 13:11:56,951 	Gloss Alignment :	         
2024-02-06 13:11:56,952 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 13:11:56,954 	Text Reference  :	it seems like he like to date women and does    not     want  to get  married people seem  to respect his       choices  
2024-02-06 13:11:56,954 	Text Hypothesis :	it ***** **** ** **** ** **** was   an  intense bidding dhoni as well in      the    start of the     different countries
2024-02-06 13:11:56,954 	Text Alignment  :	   D     D    D  D    D  D    S     S   S       S       S     S  S    S       S      S     S  S       S         S        
2024-02-06 13:11:56,954 ========================================================================================================================
2024-02-06 13:11:56,954 Logging Sequence: 152_113.00
2024-02-06 13:11:56,954 	Gloss Reference :	A B+C+D+E
2024-02-06 13:11:56,955 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 13:11:56,955 	Gloss Alignment :	         
2024-02-06 13:11:56,955 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 13:11:56,956 	Text Reference  :	** ** ** *** **** ** *** indians hoping for   a        victory were distraught at    the defeat    
2024-02-06 13:11:56,956 	Text Hypothesis :	if it is not part of the match   virat  kohli received this    is   important  while the tournament
2024-02-06 13:11:56,956 	Text Alignment  :	I  I  I  I   I    I  I   S       S      S     S        S       S    S          S         S         
2024-02-06 13:11:56,956 ========================================================================================================================
2024-02-06 13:11:56,956 Logging Sequence: 176_41.00
2024-02-06 13:11:56,957 	Gloss Reference :	A B+C+D+E
2024-02-06 13:11:56,957 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 13:11:56,957 	Gloss Alignment :	         
2024-02-06 13:11:56,957 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 13:11:56,958 	Text Reference  :	dahiya did not loose hope and put       up a        strong fight
2024-02-06 13:11:56,958 	Text Hypothesis :	it     was a   close call but coca-cola an official the    game 
2024-02-06 13:11:56,958 	Text Alignment  :	S      S   S   S     S    S   S         S  S        S      S    
2024-02-06 13:11:56,958 ========================================================================================================================
2024-02-06 13:11:56,958 Logging Sequence: 77_190.00
2024-02-06 13:11:56,959 	Gloss Reference :	A B+C+D+E
2024-02-06 13:11:56,959 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 13:11:56,959 	Gloss Alignment :	         
2024-02-06 13:11:56,959 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 13:11:56,960 	Text Reference  :	there are many batsmen who   have scrored 36     runs in 6    balls
2024-02-06 13:11:56,960 	Text Hypothesis :	***** to  know this    began a    large   number of   us know about
2024-02-06 13:11:56,960 	Text Alignment  :	D     S   S    S       S     S    S       S      S    S  S    S    
2024-02-06 13:11:56,960 ========================================================================================================================
2024-02-06 13:11:56,960 Logging Sequence: 155_170.00
2024-02-06 13:11:56,961 	Gloss Reference :	A B+C+D+E
2024-02-06 13:11:56,961 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 13:11:56,961 	Gloss Alignment :	         
2024-02-06 13:11:56,961 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 13:11:56,962 	Text Reference  :	india lost the  matches and    could  not secure  a  place in    the  semi  final
2024-02-06 13:11:56,962 	Text Hypothesis :	***** we   were just    joking around the support of the   media from their life 
2024-02-06 13:11:56,962 	Text Alignment  :	D     S    S    S       S      S      S   S       S  S     S     S    S     S    
2024-02-06 13:11:56,963 ========================================================================================================================
2024-02-06 13:12:02,460 Epoch 3530: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-06 13:12:02,460 EPOCH 3531
2024-02-06 13:12:13,219 Epoch 3531: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-06 13:12:13,220 EPOCH 3532
2024-02-06 13:12:24,152 Epoch 3532: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 13:12:24,152 EPOCH 3533
2024-02-06 13:12:34,878 Epoch 3533: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-06 13:12:34,879 EPOCH 3534
2024-02-06 13:12:45,804 Epoch 3534: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-06 13:12:45,805 EPOCH 3535
2024-02-06 13:12:56,491 Epoch 3535: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-06 13:12:56,492 EPOCH 3536
2024-02-06 13:12:58,849 [Epoch: 3536 Step: 00060100] Batch Recognition Loss:   0.000397 => Gls Tokens per Sec:     1358 || Batch Translation Loss:   0.013539 => Txt Tokens per Sec:     3536 || Lr: 0.000050
2024-02-06 13:13:07,238 Epoch 3536: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 13:13:07,238 EPOCH 3537
2024-02-06 13:13:17,908 Epoch 3537: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-06 13:13:17,909 EPOCH 3538
2024-02-06 13:13:28,224 Epoch 3538: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-06 13:13:28,225 EPOCH 3539
2024-02-06 13:13:39,119 Epoch 3539: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 13:13:39,120 EPOCH 3540
2024-02-06 13:13:50,025 Epoch 3540: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-06 13:13:50,026 EPOCH 3541
2024-02-06 13:14:00,807 Epoch 3541: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-06 13:14:00,808 EPOCH 3542
2024-02-06 13:14:02,784 [Epoch: 3542 Step: 00060200] Batch Recognition Loss:   0.000158 => Gls Tokens per Sec:      972 || Batch Translation Loss:   0.011123 => Txt Tokens per Sec:     2496 || Lr: 0.000050
2024-02-06 13:14:11,818 Epoch 3542: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-06 13:14:11,818 EPOCH 3543
2024-02-06 13:14:22,598 Epoch 3543: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-06 13:14:22,599 EPOCH 3544
2024-02-06 13:14:33,299 Epoch 3544: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-06 13:14:33,300 EPOCH 3545
2024-02-06 13:14:44,051 Epoch 3545: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.77 
2024-02-06 13:14:44,052 EPOCH 3546
2024-02-06 13:14:54,783 Epoch 3546: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.91 
2024-02-06 13:14:54,783 EPOCH 3547
2024-02-06 13:15:05,623 Epoch 3547: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.00 
2024-02-06 13:15:05,624 EPOCH 3548
2024-02-06 13:15:07,594 [Epoch: 3548 Step: 00060300] Batch Recognition Loss:   0.000740 => Gls Tokens per Sec:      325 || Batch Translation Loss:   0.058920 => Txt Tokens per Sec:     1123 || Lr: 0.000050
2024-02-06 13:15:16,064 Epoch 3548: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-06 13:15:16,065 EPOCH 3549
2024-02-06 13:15:26,830 Epoch 3549: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-06 13:15:26,831 EPOCH 3550
2024-02-06 13:15:37,458 Epoch 3550: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-06 13:15:37,459 EPOCH 3551
2024-02-06 13:15:48,105 Epoch 3551: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-06 13:15:48,105 EPOCH 3552
2024-02-06 13:15:58,833 Epoch 3552: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-06 13:15:58,833 EPOCH 3553
2024-02-06 13:16:07,511 [Epoch: 3553 Step: 00060400] Batch Recognition Loss:   0.000310 => Gls Tokens per Sec:     1150 || Batch Translation Loss:   0.009910 => Txt Tokens per Sec:     3129 || Lr: 0.000050
2024-02-06 13:16:09,491 Epoch 3553: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-06 13:16:09,491 EPOCH 3554
2024-02-06 13:16:20,149 Epoch 3554: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-06 13:16:20,150 EPOCH 3555
2024-02-06 13:16:30,781 Epoch 3555: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-06 13:16:30,782 EPOCH 3556
2024-02-06 13:16:41,408 Epoch 3556: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-06 13:16:41,409 EPOCH 3557
2024-02-06 13:16:52,155 Epoch 3557: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-06 13:16:52,156 EPOCH 3558
2024-02-06 13:17:03,065 Epoch 3558: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 13:17:03,066 EPOCH 3559
2024-02-06 13:17:13,123 [Epoch: 3559 Step: 00060500] Batch Recognition Loss:   0.000362 => Gls Tokens per Sec:      865 || Batch Translation Loss:   0.016397 => Txt Tokens per Sec:     2420 || Lr: 0.000050
2024-02-06 13:17:13,736 Epoch 3559: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 13:17:13,736 EPOCH 3560
2024-02-06 13:17:24,421 Epoch 3560: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-06 13:17:24,422 EPOCH 3561
2024-02-06 13:17:35,027 Epoch 3561: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 13:17:35,028 EPOCH 3562
2024-02-06 13:17:45,703 Epoch 3562: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-06 13:17:45,704 EPOCH 3563
2024-02-06 13:17:56,295 Epoch 3563: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-06 13:17:56,296 EPOCH 3564
2024-02-06 13:18:07,103 Epoch 3564: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-06 13:18:07,103 EPOCH 3565
2024-02-06 13:18:14,951 [Epoch: 3565 Step: 00060600] Batch Recognition Loss:   0.000416 => Gls Tokens per Sec:      946 || Batch Translation Loss:   0.024714 => Txt Tokens per Sec:     2596 || Lr: 0.000050
2024-02-06 13:18:17,854 Epoch 3565: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-06 13:18:17,854 EPOCH 3566
2024-02-06 13:18:28,778 Epoch 3566: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.30 
2024-02-06 13:18:28,779 EPOCH 3567
2024-02-06 13:18:39,419 Epoch 3567: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 13:18:39,420 EPOCH 3568
2024-02-06 13:18:50,011 Epoch 3568: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 13:18:50,012 EPOCH 3569
2024-02-06 13:19:00,637 Epoch 3569: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 13:19:00,638 EPOCH 3570
2024-02-06 13:19:11,569 Epoch 3570: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-06 13:19:11,569 EPOCH 3571
2024-02-06 13:19:16,783 [Epoch: 3571 Step: 00060700] Batch Recognition Loss:   0.000379 => Gls Tokens per Sec:     1228 || Batch Translation Loss:   0.012682 => Txt Tokens per Sec:     3408 || Lr: 0.000050
2024-02-06 13:19:22,359 Epoch 3571: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 13:19:22,359 EPOCH 3572
2024-02-06 13:19:33,007 Epoch 3572: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 13:19:33,007 EPOCH 3573
2024-02-06 13:19:43,716 Epoch 3573: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 13:19:43,716 EPOCH 3574
2024-02-06 13:19:54,429 Epoch 3574: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-06 13:19:54,430 EPOCH 3575
2024-02-06 13:20:05,156 Epoch 3575: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 13:20:05,157 EPOCH 3576
2024-02-06 13:20:15,750 Epoch 3576: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 13:20:15,750 EPOCH 3577
2024-02-06 13:20:20,656 [Epoch: 3577 Step: 00060800] Batch Recognition Loss:   0.000185 => Gls Tokens per Sec:     1044 || Batch Translation Loss:   0.009064 => Txt Tokens per Sec:     2759 || Lr: 0.000050
2024-02-06 13:20:26,521 Epoch 3577: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-06 13:20:26,521 EPOCH 3578
2024-02-06 13:20:37,208 Epoch 3578: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 13:20:37,208 EPOCH 3579
2024-02-06 13:20:47,770 Epoch 3579: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-06 13:20:47,770 EPOCH 3580
2024-02-06 13:20:58,459 Epoch 3580: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-06 13:20:58,459 EPOCH 3581
2024-02-06 13:21:09,094 Epoch 3581: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 13:21:09,094 EPOCH 3582
2024-02-06 13:21:19,859 Epoch 3582: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 13:21:19,860 EPOCH 3583
2024-02-06 13:21:21,150 [Epoch: 3583 Step: 00060900] Batch Recognition Loss:   0.000751 => Gls Tokens per Sec:     2979 || Batch Translation Loss:   0.015007 => Txt Tokens per Sec:     8082 || Lr: 0.000050
2024-02-06 13:21:30,556 Epoch 3583: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-06 13:21:30,557 EPOCH 3584
2024-02-06 13:21:41,506 Epoch 3584: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 13:21:41,507 EPOCH 3585
2024-02-06 13:21:52,320 Epoch 3585: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-06 13:21:52,321 EPOCH 3586
2024-02-06 13:22:02,915 Epoch 3586: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 13:22:02,916 EPOCH 3587
2024-02-06 13:22:13,667 Epoch 3587: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-06 13:22:13,667 EPOCH 3588
2024-02-06 13:22:24,338 Epoch 3588: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.76 
2024-02-06 13:22:24,339 EPOCH 3589
2024-02-06 13:22:24,988 [Epoch: 3589 Step: 00061000] Batch Recognition Loss:   0.000791 => Gls Tokens per Sec:     3951 || Batch Translation Loss:   0.092142 => Txt Tokens per Sec:     8900 || Lr: 0.000050
2024-02-06 13:22:34,840 Epoch 3589: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.73 
2024-02-06 13:22:34,840 EPOCH 3590
2024-02-06 13:22:45,475 Epoch 3590: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-06 13:22:45,475 EPOCH 3591
2024-02-06 13:22:56,235 Epoch 3591: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-06 13:22:56,236 EPOCH 3592
2024-02-06 13:23:07,326 Epoch 3592: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.58 
2024-02-06 13:23:07,327 EPOCH 3593
2024-02-06 13:23:18,104 Epoch 3593: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-06 13:23:18,105 EPOCH 3594
2024-02-06 13:23:28,761 Epoch 3594: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-06 13:23:28,761 EPOCH 3595
2024-02-06 13:23:31,420 [Epoch: 3595 Step: 00061100] Batch Recognition Loss:   0.000287 => Gls Tokens per Sec:      384 || Batch Translation Loss:   0.018656 => Txt Tokens per Sec:     1131 || Lr: 0.000050
2024-02-06 13:23:39,387 Epoch 3595: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-06 13:23:39,387 EPOCH 3596
2024-02-06 13:23:50,177 Epoch 3596: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-06 13:23:50,178 EPOCH 3597
2024-02-06 13:24:00,975 Epoch 3597: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-06 13:24:00,975 EPOCH 3598
2024-02-06 13:24:11,667 Epoch 3598: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-06 13:24:11,667 EPOCH 3599
2024-02-06 13:24:22,396 Epoch 3599: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.31 
2024-02-06 13:24:22,397 EPOCH 3600
2024-02-06 13:24:33,250 [Epoch: 3600 Step: 00061200] Batch Recognition Loss:   0.000126 => Gls Tokens per Sec:      979 || Batch Translation Loss:   0.013719 => Txt Tokens per Sec:     2708 || Lr: 0.000050
2024-02-06 13:24:33,251 Epoch 3600: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 13:24:33,251 EPOCH 3601
2024-02-06 13:24:43,890 Epoch 3601: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-06 13:24:43,891 EPOCH 3602
2024-02-06 13:24:54,626 Epoch 3602: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-06 13:24:54,627 EPOCH 3603
2024-02-06 13:25:05,151 Epoch 3603: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-06 13:25:05,152 EPOCH 3604
2024-02-06 13:25:15,730 Epoch 3604: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-06 13:25:15,731 EPOCH 3605
2024-02-06 13:25:26,165 Epoch 3605: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-06 13:25:26,166 EPOCH 3606
2024-02-06 13:25:36,770 [Epoch: 3606 Step: 00061300] Batch Recognition Loss:   0.000185 => Gls Tokens per Sec:      881 || Batch Translation Loss:   0.015074 => Txt Tokens per Sec:     2496 || Lr: 0.000050
2024-02-06 13:25:37,097 Epoch 3606: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-06 13:25:37,097 EPOCH 3607
2024-02-06 13:25:47,700 Epoch 3607: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-06 13:25:47,700 EPOCH 3608
2024-02-06 13:25:58,542 Epoch 3608: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.59 
2024-02-06 13:25:58,542 EPOCH 3609
2024-02-06 13:26:09,325 Epoch 3609: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.74 
2024-02-06 13:26:09,326 EPOCH 3610
2024-02-06 13:26:19,915 Epoch 3610: Total Training Recognition Loss 0.25  Total Training Translation Loss 2.53 
2024-02-06 13:26:19,916 EPOCH 3611
2024-02-06 13:26:30,626 Epoch 3611: Total Training Recognition Loss 0.32  Total Training Translation Loss 2.02 
2024-02-06 13:26:30,626 EPOCH 3612
2024-02-06 13:26:40,167 [Epoch: 3612 Step: 00061400] Batch Recognition Loss:   0.008413 => Gls Tokens per Sec:      845 || Batch Translation Loss:   0.031497 => Txt Tokens per Sec:     2383 || Lr: 0.000050
2024-02-06 13:26:41,143 Epoch 3612: Total Training Recognition Loss 0.51  Total Training Translation Loss 1.20 
2024-02-06 13:26:41,143 EPOCH 3613
2024-02-06 13:26:51,861 Epoch 3613: Total Training Recognition Loss 1.17  Total Training Translation Loss 0.59 
2024-02-06 13:26:51,862 EPOCH 3614
2024-02-06 13:27:02,762 Epoch 3614: Total Training Recognition Loss 0.23  Total Training Translation Loss 0.45 
2024-02-06 13:27:02,762 EPOCH 3615
2024-02-06 13:27:13,489 Epoch 3615: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.36 
2024-02-06 13:27:13,489 EPOCH 3616
2024-02-06 13:27:24,379 Epoch 3616: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.35 
2024-02-06 13:27:24,379 EPOCH 3617
2024-02-06 13:27:34,861 Epoch 3617: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.39 
2024-02-06 13:27:34,861 EPOCH 3618
2024-02-06 13:27:43,050 [Epoch: 3618 Step: 00061500] Batch Recognition Loss:   0.000460 => Gls Tokens per Sec:      828 || Batch Translation Loss:   0.029644 => Txt Tokens per Sec:     2426 || Lr: 0.000050
2024-02-06 13:27:45,785 Epoch 3618: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 13:27:45,786 EPOCH 3619
2024-02-06 13:27:56,844 Epoch 3619: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 13:27:56,844 EPOCH 3620
2024-02-06 13:28:07,529 Epoch 3620: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 13:28:07,529 EPOCH 3621
2024-02-06 13:28:18,398 Epoch 3621: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 13:28:18,398 EPOCH 3622
2024-02-06 13:28:29,043 Epoch 3622: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 13:28:29,044 EPOCH 3623
2024-02-06 13:28:39,606 Epoch 3623: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 13:28:39,607 EPOCH 3624
2024-02-06 13:28:46,716 [Epoch: 3624 Step: 00061600] Batch Recognition Loss:   0.000160 => Gls Tokens per Sec:      774 || Batch Translation Loss:   0.014160 => Txt Tokens per Sec:     2208 || Lr: 0.000050
2024-02-06 13:28:50,312 Epoch 3624: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 13:28:50,312 EPOCH 3625
2024-02-06 13:29:01,003 Epoch 3625: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 13:29:01,004 EPOCH 3626
2024-02-06 13:29:11,739 Epoch 3626: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 13:29:11,740 EPOCH 3627
2024-02-06 13:29:22,255 Epoch 3627: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 13:29:22,256 EPOCH 3628
2024-02-06 13:29:32,862 Epoch 3628: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 13:29:32,862 EPOCH 3629
2024-02-06 13:29:43,666 Epoch 3629: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 13:29:43,667 EPOCH 3630
2024-02-06 13:29:47,417 [Epoch: 3630 Step: 00061700] Batch Recognition Loss:   0.000203 => Gls Tokens per Sec:     1126 || Batch Translation Loss:   0.009983 => Txt Tokens per Sec:     2731 || Lr: 0.000050
2024-02-06 13:29:54,608 Epoch 3630: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 13:29:54,608 EPOCH 3631
2024-02-06 13:30:05,419 Epoch 3631: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 13:30:05,419 EPOCH 3632
2024-02-06 13:30:16,491 Epoch 3632: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 13:30:16,491 EPOCH 3633
2024-02-06 13:30:27,291 Epoch 3633: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 13:30:27,292 EPOCH 3634
2024-02-06 13:30:38,069 Epoch 3634: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 13:30:38,070 EPOCH 3635
2024-02-06 13:30:48,827 Epoch 3635: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-06 13:30:48,827 EPOCH 3636
2024-02-06 13:30:50,266 [Epoch: 3636 Step: 00061800] Batch Recognition Loss:   0.000156 => Gls Tokens per Sec:     2226 || Batch Translation Loss:   0.011292 => Txt Tokens per Sec:     6428 || Lr: 0.000050
2024-02-06 13:30:59,654 Epoch 3636: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-06 13:30:59,654 EPOCH 3637
2024-02-06 13:31:10,605 Epoch 3637: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 13:31:10,605 EPOCH 3638
2024-02-06 13:31:21,235 Epoch 3638: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 13:31:21,236 EPOCH 3639
2024-02-06 13:31:32,002 Epoch 3639: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 13:31:32,003 EPOCH 3640
2024-02-06 13:31:42,810 Epoch 3640: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 13:31:42,810 EPOCH 3641
2024-02-06 13:31:53,722 Epoch 3641: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 13:31:53,723 EPOCH 3642
2024-02-06 13:31:54,399 [Epoch: 3642 Step: 00061900] Batch Recognition Loss:   0.000350 => Gls Tokens per Sec:     2851 || Batch Translation Loss:   0.008359 => Txt Tokens per Sec:     6971 || Lr: 0.000050
2024-02-06 13:32:04,505 Epoch 3642: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 13:32:04,506 EPOCH 3643
2024-02-06 13:32:15,425 Epoch 3643: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 13:32:15,426 EPOCH 3644
2024-02-06 13:32:26,341 Epoch 3644: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 13:32:26,341 EPOCH 3645
2024-02-06 13:32:37,175 Epoch 3645: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-06 13:32:37,176 EPOCH 3646
2024-02-06 13:32:47,856 Epoch 3646: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-06 13:32:47,857 EPOCH 3647
2024-02-06 13:32:58,726 Epoch 3647: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-06 13:32:58,726 EPOCH 3648
2024-02-06 13:32:58,965 [Epoch: 3648 Step: 00062000] Batch Recognition Loss:   0.000131 => Gls Tokens per Sec:     2689 || Batch Translation Loss:   0.023397 => Txt Tokens per Sec:     7714 || Lr: 0.000050
2024-02-06 13:33:39,501 Validation result at epoch 3648, step    62000: duration: 40.5357s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.49768	Translation Loss: 103802.05469	PPL: 31817.23828
	Eval Metric: BLEU
	WER 2.54	(DEL: 0.00,	INS: 0.00,	SUB: 2.54)
	BLEU-4 0.00	(BLEU-1: 9.77,	BLEU-2: 2.73,	BLEU-3: 0.87,	BLEU-4: 0.00)
	CHRF 16.94	ROUGE 8.11
2024-02-06 13:33:39,503 Logging Recognition and Translation Outputs
2024-02-06 13:33:39,505 ========================================================================================================================
2024-02-06 13:33:39,506 Logging Sequence: 165_523.00
2024-02-06 13:33:39,506 	Gloss Reference :	A B+C+D+E
2024-02-06 13:33:39,506 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 13:33:39,506 	Gloss Alignment :	         
2024-02-06 13:33:39,507 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 13:33:39,507 	Text Reference  :	as he believed that his team might lose if   he takes off his batting pads
2024-02-06 13:33:39,507 	Text Hypothesis :	** ** ******** **** *** **** ***** **** when he ***** was 62  years   old 
2024-02-06 13:33:39,508 	Text Alignment  :	D  D  D        D    D   D    D     D    S       D     S   S   S       S   
2024-02-06 13:33:39,508 ========================================================================================================================
2024-02-06 13:33:39,508 Logging Sequence: 165_233.00
2024-02-06 13:33:39,508 	Gloss Reference :	A B+C+D+E
2024-02-06 13:33:39,508 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 13:33:39,508 	Gloss Alignment :	         
2024-02-06 13:33:39,509 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 13:33:39,511 	Text Reference  :	irrespective of whether he was playing the match or not he always sat    with his     bag   he    was happy when the team won 
2024-02-06 13:33:39,511 	Text Hypothesis :	************ ** ******* it was ******* *** ***** ** *** a  huge   number of   chennai super kings csk has   won  the **** game
2024-02-06 13:33:39,511 	Text Alignment  :	D            D  D       S      D       D   D     D  D   S  S      S      S    S       S     S     S   S     S        D    S   
2024-02-06 13:33:39,511 ========================================================================================================================
2024-02-06 13:33:39,511 Logging Sequence: 169_214.00
2024-02-06 13:33:39,511 	Gloss Reference :	A B+C+D+E
2024-02-06 13:33:39,512 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 13:33:39,512 	Gloss Alignment :	         
2024-02-06 13:33:39,512 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 13:33:39,513 	Text Reference  :	virat kohli said         that  though    arshdeep     dropped the catch he is still a strong  part      of    the    indian      team
2024-02-06 13:33:39,513 	Text Hypothesis :	***** the   commonwealth games encourage independence from    the ***** ** ** ***** * british democracy human rights development etc 
2024-02-06 13:33:39,514 	Text Alignment  :	D     S     S            S     S         S            S           D     D  D  D     D S       S         S     S      S           S   
2024-02-06 13:33:39,514 ========================================================================================================================
2024-02-06 13:33:39,514 Logging Sequence: 88_67.00
2024-02-06 13:33:39,514 	Gloss Reference :	A B+C+D+E
2024-02-06 13:33:39,514 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 13:33:39,514 	Gloss Alignment :	         
2024-02-06 13:33:39,514 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 13:33:39,517 	Text Reference  :	*** ***** pablo javkin the ****** mayor of   rosario  is also  a        drug trafficker so he  won't take     care of     you 
2024-02-06 13:33:39,517 	Text Hypothesis :	the mayor added that   the police never does anything to catch culprits and  that       is why the   culprits have become bold
2024-02-06 13:33:39,517 	Text Alignment  :	I   I     S     S          I      S     S    S        S  S     S        S    S          S  S   S     S        S    S      S   
2024-02-06 13:33:39,517 ========================================================================================================================
2024-02-06 13:33:39,517 Logging Sequence: 69_95.00
2024-02-06 13:33:39,517 	Gloss Reference :	A B+C+D+E
2024-02-06 13:33:39,517 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 13:33:39,518 	Gloss Alignment :	         
2024-02-06 13:33:39,518 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 13:33:39,519 	Text Reference  :	***** a   six    and    a    four  sealed csk's victory and the **** ***** ** **** team      won   the match
2024-02-06 13:33:39,519 	Text Hypothesis :	after the defeat people were glued to     their batting at  the same place at fans including watch the video
2024-02-06 13:33:39,519 	Text Alignment  :	I     S   S      S      S    S     S      S     S       S       I    I     I  I    S         S         S    
2024-02-06 13:33:39,520 ========================================================================================================================
2024-02-06 13:33:39,530 Training ended since there were no improvements inthe last learning rate step: 0.000050
2024-02-06 13:33:39,533 Best validation result at step    10000:   1.00 eval_metric.
2024-02-06 13:34:05,032 ------------------------------------------------------------
2024-02-06 13:34:05,033 [DEV] partition [RECOGNITION] experiment [BW]: 1
2024-02-06 13:34:45,946 finished in 40.9116s 
2024-02-06 13:34:45,949 ************************************************************
2024-02-06 13:34:45,949 [DEV] partition [RECOGNITION] results:
	New Best CTC Decode Beam Size: 1
	WER 4.38	(DEL: 0.00,	INS: 0.00,	SUB: 4.38)
2024-02-06 13:34:45,949 ************************************************************
2024-02-06 13:34:45,949 ------------------------------------------------------------
2024-02-06 13:34:45,949 [DEV] partition [RECOGNITION] experiment [BW]: 2
2024-02-06 13:35:26,237 finished in 40.2879s 
2024-02-06 13:35:26,238 ------------------------------------------------------------
2024-02-06 13:35:26,238 [DEV] partition [RECOGNITION] experiment [BW]: 3
2024-02-06 13:36:06,421 finished in 40.1832s 
2024-02-06 13:36:06,422 ------------------------------------------------------------
2024-02-06 13:36:06,422 [DEV] partition [RECOGNITION] experiment [BW]: 4
2024-02-06 13:36:46,601 finished in 40.1796s 
2024-02-06 13:36:46,601 ------------------------------------------------------------
2024-02-06 13:36:46,602 [DEV] partition [RECOGNITION] experiment [BW]: 5
2024-02-06 13:37:26,746 finished in 40.1426s 
2024-02-06 13:37:26,746 ------------------------------------------------------------
2024-02-06 13:37:26,746 [DEV] partition [RECOGNITION] experiment [BW]: 6
2024-02-06 13:38:07,017 finished in 40.2707s 
2024-02-06 13:38:07,017 ------------------------------------------------------------
2024-02-06 13:38:07,017 [DEV] partition [RECOGNITION] experiment [BW]: 7
2024-02-06 13:38:47,288 finished in 40.2711s 
2024-02-06 13:38:47,288 ------------------------------------------------------------
2024-02-06 13:38:47,289 [DEV] partition [RECOGNITION] experiment [BW]: 8
2024-02-06 13:39:27,575 finished in 40.2860s 
2024-02-06 13:39:27,576 ------------------------------------------------------------
2024-02-06 13:39:27,576 [DEV] partition [RECOGNITION] experiment [BW]: 9
2024-02-06 13:40:08,012 finished in 40.4359s 
2024-02-06 13:40:08,012 ------------------------------------------------------------
2024-02-06 13:40:08,013 [DEV] partition [RECOGNITION] experiment [BW]: 10
2024-02-06 13:40:48,196 finished in 40.1826s 
2024-02-06 13:40:48,196 ============================================================
2024-02-06 13:41:28,205 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 1 and Alpha: -1
	BLEU-4 1.00	(BLEU-1: 11.60,	BLEU-2: 3.96,	BLEU-3: 1.80,	BLEU-4: 1.00)
	CHRF 17.36	ROUGE 9.91
2024-02-06 13:41:28,205 ------------------------------------------------------------
2024-02-06 16:27:34,346 ************************************************************
2024-02-06 16:27:34,349 [DEV] partition [Recognition & Translation] results:
	Best CTC Decode Beam Size: 1
	Best Translation Beam Size: 1 and Alpha: -1
	WER 4.38	(DEL: 0.00,	INS: 0.00,	SUB: 4.38)
	BLEU-4 1.00	(BLEU-1: 11.60,	BLEU-2: 3.96,	BLEU-3: 1.80,	BLEU-4: 1.00)
	CHRF 17.36	ROUGE 9.91
2024-02-06 16:27:34,350 ************************************************************
2024-02-06 16:28:43,784 [TEST] partition [Recognition & Translation] results:
	Best CTC Decode Beam Size: 1
	Best Translation Beam Size: 1 and Alpha: -1
	WER 3.67	(DEL: 0.00,	INS: 0.00,	SUB: 3.67)
	BLEU-4 0.86	(BLEU-1: 10.99,	BLEU-2: 3.29,	BLEU-3: 1.46,	BLEU-4: 0.86)
	CHRF 17.16	ROUGE 9.24
2024-02-06 16:28:43,786 ************************************************************
