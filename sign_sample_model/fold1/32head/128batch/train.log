2024-02-03 10:30:53,420 Hello! This is Joey-NMT.
2024-02-03 10:30:53,428 Total params: 25618440
2024-02-03 10:30:53,429 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'gloss_output_layer.bias', 'gloss_output_layer.weight', 'sgn_embed.ln.bias', 'sgn_embed.ln.weight', 'sgn_embed.norm.norm.bias', 'sgn_embed.norm.norm.weight', 'txt_embed.norm.norm.bias', 'txt_embed.norm.norm.weight']
2024-02-03 10:30:54,524 cfg.name                           : sign_experiment
2024-02-03 10:30:54,524 cfg.data.data_path                 : ./data/Sports_dataset/1/
2024-02-03 10:30:54,524 cfg.data.version                   : phoenix_2014_trans
2024-02-03 10:30:54,524 cfg.data.sgn                       : sign
2024-02-03 10:30:54,525 cfg.data.txt                       : text
2024-02-03 10:30:54,525 cfg.data.gls                       : gloss
2024-02-03 10:30:54,525 cfg.data.train                     : excel_data.train
2024-02-03 10:30:54,525 cfg.data.dev                       : excel_data.dev
2024-02-03 10:30:54,525 cfg.data.test                      : excel_data.test
2024-02-03 10:30:54,525 cfg.data.feature_size              : 2560
2024-02-03 10:30:54,525 cfg.data.level                     : word
2024-02-03 10:30:54,525 cfg.data.txt_lowercase             : True
2024-02-03 10:30:54,525 cfg.data.max_sent_length           : 500
2024-02-03 10:30:54,526 cfg.data.random_train_subset       : -1
2024-02-03 10:30:54,526 cfg.data.random_dev_subset         : -1
2024-02-03 10:30:54,526 cfg.testing.recognition_beam_sizes : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
2024-02-03 10:30:54,526 cfg.testing.translation_beam_sizes : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
2024-02-03 10:30:54,526 cfg.testing.translation_beam_alphas : [-1, 0, 1, 2, 3, 4, 5]
2024-02-03 10:30:54,526 cfg.training.reset_best_ckpt       : False
2024-02-03 10:30:54,526 cfg.training.reset_scheduler       : False
2024-02-03 10:30:54,526 cfg.training.reset_optimizer       : False
2024-02-03 10:30:54,527 cfg.training.random_seed           : 42
2024-02-03 10:30:54,527 cfg.training.model_dir             : ./sign_sample_model/fold1/32head/128batch
2024-02-03 10:30:54,527 cfg.training.recognition_loss_weight : 1.0
2024-02-03 10:30:54,527 cfg.training.translation_loss_weight : 1.0
2024-02-03 10:30:54,527 cfg.training.eval_metric           : bleu
2024-02-03 10:30:54,527 cfg.training.optimizer             : adam
2024-02-03 10:30:54,527 cfg.training.learning_rate         : 0.0001
2024-02-03 10:30:54,527 cfg.training.batch_size            : 128
2024-02-03 10:30:54,527 cfg.training.num_valid_log         : 5
2024-02-03 10:30:54,528 cfg.training.epochs                : 50000
2024-02-03 10:30:54,528 cfg.training.early_stopping_metric : eval_metric
2024-02-03 10:30:54,528 cfg.training.batch_type            : sentence
2024-02-03 10:30:54,528 cfg.training.translation_normalization : batch
2024-02-03 10:30:54,528 cfg.training.eval_recognition_beam_size : 1
2024-02-03 10:30:54,528 cfg.training.eval_translation_beam_size : 1
2024-02-03 10:30:54,528 cfg.training.eval_translation_beam_alpha : -1
2024-02-03 10:30:54,528 cfg.training.overwrite             : True
2024-02-03 10:30:54,529 cfg.training.shuffle               : True
2024-02-03 10:30:54,529 cfg.training.use_cuda              : True
2024-02-03 10:30:54,529 cfg.training.translation_max_output_length : 40
2024-02-03 10:30:54,529 cfg.training.keep_last_ckpts       : 1
2024-02-03 10:30:54,529 cfg.training.batch_multiplier      : 1
2024-02-03 10:30:54,529 cfg.training.logging_freq          : 100
2024-02-03 10:30:54,529 cfg.training.validation_freq       : 2000
2024-02-03 10:30:54,529 cfg.training.betas                 : [0.9, 0.998]
2024-02-03 10:30:54,530 cfg.training.scheduling            : plateau
2024-02-03 10:30:54,530 cfg.training.learning_rate_min     : 1e-08
2024-02-03 10:30:54,530 cfg.training.weight_decay          : 0.0001
2024-02-03 10:30:54,530 cfg.training.patience              : 12
2024-02-03 10:30:54,530 cfg.training.decrease_factor       : 0.5
2024-02-03 10:30:54,530 cfg.training.label_smoothing       : 0.0
2024-02-03 10:30:54,530 cfg.model.initializer              : xavier
2024-02-03 10:30:54,530 cfg.model.bias_initializer         : zeros
2024-02-03 10:30:54,530 cfg.model.init_gain                : 1.0
2024-02-03 10:30:54,531 cfg.model.embed_initializer        : xavier
2024-02-03 10:30:54,531 cfg.model.embed_init_gain          : 1.0
2024-02-03 10:30:54,531 cfg.model.tied_softmax             : True
2024-02-03 10:30:54,531 cfg.model.encoder.type             : transformer
2024-02-03 10:30:54,531 cfg.model.encoder.num_layers       : 3
2024-02-03 10:30:54,531 cfg.model.encoder.num_heads        : 32
2024-02-03 10:30:54,531 cfg.model.encoder.embeddings.embedding_dim : 512
2024-02-03 10:30:54,531 cfg.model.encoder.embeddings.scale : False
2024-02-03 10:30:54,531 cfg.model.encoder.embeddings.dropout : 0.1
2024-02-03 10:30:54,532 cfg.model.encoder.embeddings.norm_type : batch
2024-02-03 10:30:54,532 cfg.model.encoder.embeddings.activation_type : softsign
2024-02-03 10:30:54,532 cfg.model.encoder.hidden_size      : 512
2024-02-03 10:30:54,532 cfg.model.encoder.ff_size          : 2048
2024-02-03 10:30:54,532 cfg.model.encoder.dropout          : 0.1
2024-02-03 10:30:54,532 cfg.model.decoder.type             : transformer
2024-02-03 10:30:54,532 cfg.model.decoder.num_layers       : 3
2024-02-03 10:30:54,532 cfg.model.decoder.num_heads        : 32
2024-02-03 10:30:54,533 cfg.model.decoder.embeddings.embedding_dim : 512
2024-02-03 10:30:54,533 cfg.model.decoder.embeddings.scale : False
2024-02-03 10:30:54,533 cfg.model.decoder.embeddings.dropout : 0.1
2024-02-03 10:30:54,533 cfg.model.decoder.embeddings.norm_type : batch
2024-02-03 10:30:54,533 cfg.model.decoder.embeddings.activation_type : softsign
2024-02-03 10:30:54,533 cfg.model.decoder.hidden_size      : 512
2024-02-03 10:30:54,533 cfg.model.decoder.ff_size          : 2048
2024-02-03 10:30:54,533 cfg.model.decoder.dropout          : 0.1
2024-02-03 10:30:54,533 Data set sizes: 
	train 2126,
	valid 707,
	test 708
2024-02-03 10:30:54,534 First training example:
	[GLS] A B C D E
	[TXT] another problem is that because of ipl and t20 world cup many players may not be available for the matches
2024-02-03 10:30:54,534 First 10 words (gls): (0) <si> (1) <unk> (2) <pad> (3) A (4) B (5) C (6) D (7) E
2024-02-03 10:30:54,534 First 10 words (txt): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) the (5) to (6) and (7) in (8) a (9) of
2024-02-03 10:30:54,534 Number of unique glosses (types): 8
2024-02-03 10:30:54,534 Number of unique words (types): 4355
2024-02-03 10:30:54,534 SignModel(
	encoder=TransformerEncoder(num_layers=3, num_heads=32),
	decoder=TransformerDecoder(num_layers=3, num_heads=32),
	sgn_embed=SpatialEmbeddings(embedding_dim=512, input_size=2560),
	txt_embed=Embeddings(embedding_dim=512, vocab_size=4355))
2024-02-03 10:30:54,538 EPOCH 1
2024-02-03 10:31:17,070 Epoch   1: Total Training Recognition Loss 148.00  Total Training Translation Loss 1794.57 
2024-02-03 10:31:17,071 EPOCH 2
2024-02-03 10:31:32,348 Epoch   2: Total Training Recognition Loss 72.04  Total Training Translation Loss 1632.51 
2024-02-03 10:31:32,348 EPOCH 3
2024-02-03 10:31:46,130 Epoch   3: Total Training Recognition Loss 58.68  Total Training Translation Loss 1556.94 
2024-02-03 10:31:46,131 EPOCH 4
2024-02-03 10:32:00,205 Epoch   4: Total Training Recognition Loss 50.56  Total Training Translation Loss 1523.68 
2024-02-03 10:32:00,206 EPOCH 5
2024-02-03 10:32:13,694 Epoch   5: Total Training Recognition Loss 45.68  Total Training Translation Loss 1508.39 
2024-02-03 10:32:13,694 EPOCH 6
2024-02-03 10:32:25,294 [Epoch: 006 Step: 00000100] Batch Recognition Loss:   3.796055 => Gls Tokens per Sec:      806 || Batch Translation Loss: 118.400131 => Txt Tokens per Sec:     2227 || Lr: 0.000100
2024-02-03 10:32:27,063 Epoch   6: Total Training Recognition Loss 41.29  Total Training Translation Loss 1499.72 
2024-02-03 10:32:27,064 EPOCH 7
2024-02-03 10:32:40,554 Epoch   7: Total Training Recognition Loss 38.44  Total Training Translation Loss 1489.22 
2024-02-03 10:32:40,554 EPOCH 8
2024-02-03 10:32:53,772 Epoch   8: Total Training Recognition Loss 41.76  Total Training Translation Loss 1475.67 
2024-02-03 10:32:53,772 EPOCH 9
2024-02-03 10:33:07,182 Epoch   9: Total Training Recognition Loss 34.48  Total Training Translation Loss 1454.19 
2024-02-03 10:33:07,183 EPOCH 10
2024-02-03 10:33:20,562 Epoch  10: Total Training Recognition Loss 29.73  Total Training Translation Loss 1426.46 
2024-02-03 10:33:20,563 EPOCH 11
2024-02-03 10:33:33,885 Epoch  11: Total Training Recognition Loss 28.17  Total Training Translation Loss 1397.40 
2024-02-03 10:33:33,886 EPOCH 12
2024-02-03 10:33:44,813 [Epoch: 012 Step: 00000200] Batch Recognition Loss:   3.459977 => Gls Tokens per Sec:      739 || Batch Translation Loss: 101.450500 => Txt Tokens per Sec:     2080 || Lr: 0.000100
2024-02-03 10:33:47,102 Epoch  12: Total Training Recognition Loss 25.89  Total Training Translation Loss 1370.80 
2024-02-03 10:33:47,102 EPOCH 13
2024-02-03 10:34:00,111 Epoch  13: Total Training Recognition Loss 21.14  Total Training Translation Loss 1340.49 
2024-02-03 10:34:00,111 EPOCH 14
2024-02-03 10:34:13,471 Epoch  14: Total Training Recognition Loss 18.61  Total Training Translation Loss 1313.26 
2024-02-03 10:34:13,472 EPOCH 15
2024-02-03 10:34:26,580 Epoch  15: Total Training Recognition Loss 16.56  Total Training Translation Loss 1291.37 
2024-02-03 10:34:26,580 EPOCH 16
2024-02-03 10:34:39,835 Epoch  16: Total Training Recognition Loss 14.99  Total Training Translation Loss 1260.27 
2024-02-03 10:34:39,836 EPOCH 17
2024-02-03 10:34:53,094 Epoch  17: Total Training Recognition Loss 13.88  Total Training Translation Loss 1228.22 
2024-02-03 10:34:53,094 EPOCH 18
2024-02-03 10:34:58,205 [Epoch: 018 Step: 00000300] Batch Recognition Loss:   0.813663 => Gls Tokens per Sec:     1378 || Batch Translation Loss:  91.599983 => Txt Tokens per Sec:     3681 || Lr: 0.000100
2024-02-03 10:35:06,210 Epoch  18: Total Training Recognition Loss 12.25  Total Training Translation Loss 1203.26 
2024-02-03 10:35:06,211 EPOCH 19
2024-02-03 10:35:19,400 Epoch  19: Total Training Recognition Loss 12.06  Total Training Translation Loss 1180.66 
2024-02-03 10:35:19,401 EPOCH 20
2024-02-03 10:35:32,945 Epoch  20: Total Training Recognition Loss 12.33  Total Training Translation Loss 1158.23 
2024-02-03 10:35:32,945 EPOCH 21
2024-02-03 10:35:46,113 Epoch  21: Total Training Recognition Loss 11.79  Total Training Translation Loss 1148.67 
2024-02-03 10:35:46,113 EPOCH 22
2024-02-03 10:35:59,610 Epoch  22: Total Training Recognition Loss 10.42  Total Training Translation Loss 1122.63 
2024-02-03 10:35:59,611 EPOCH 23
2024-02-03 10:36:12,714 Epoch  23: Total Training Recognition Loss 9.74  Total Training Translation Loss 1106.06 
2024-02-03 10:36:12,715 EPOCH 24
2024-02-03 10:36:22,897 [Epoch: 024 Step: 00000400] Batch Recognition Loss:   0.463105 => Gls Tokens per Sec:      541 || Batch Translation Loss:  68.953865 => Txt Tokens per Sec:     1661 || Lr: 0.000100
2024-02-03 10:36:25,948 Epoch  24: Total Training Recognition Loss 8.93  Total Training Translation Loss 1087.01 
2024-02-03 10:36:25,948 EPOCH 25
2024-02-03 10:36:39,288 Epoch  25: Total Training Recognition Loss 7.59  Total Training Translation Loss 1062.75 
2024-02-03 10:36:39,289 EPOCH 26
2024-02-03 10:36:52,730 Epoch  26: Total Training Recognition Loss 6.99  Total Training Translation Loss 1030.08 
2024-02-03 10:36:52,730 EPOCH 27
2024-02-03 10:37:06,174 Epoch  27: Total Training Recognition Loss 6.72  Total Training Translation Loss 1006.99 
2024-02-03 10:37:06,174 EPOCH 28
2024-02-03 10:37:19,544 Epoch  28: Total Training Recognition Loss 6.26  Total Training Translation Loss 988.41 
2024-02-03 10:37:19,544 EPOCH 29
2024-02-03 10:37:32,619 Epoch  29: Total Training Recognition Loss 6.06  Total Training Translation Loss 968.38 
2024-02-03 10:37:32,620 EPOCH 30
2024-02-03 10:37:36,952 [Epoch: 030 Step: 00000500] Batch Recognition Loss:   1.002077 => Gls Tokens per Sec:     1035 || Batch Translation Loss:  72.846672 => Txt Tokens per Sec:     2886 || Lr: 0.000100
2024-02-03 10:37:45,873 Epoch  30: Total Training Recognition Loss 6.37  Total Training Translation Loss 953.56 
2024-02-03 10:37:45,874 EPOCH 31
2024-02-03 10:37:58,962 Epoch  31: Total Training Recognition Loss 5.57  Total Training Translation Loss 932.12 
2024-02-03 10:37:58,963 EPOCH 32
2024-02-03 10:38:12,101 Epoch  32: Total Training Recognition Loss 5.67  Total Training Translation Loss 917.65 
2024-02-03 10:38:12,101 EPOCH 33
2024-02-03 10:38:25,347 Epoch  33: Total Training Recognition Loss 5.29  Total Training Translation Loss 901.10 
2024-02-03 10:38:25,348 EPOCH 34
2024-02-03 10:38:38,476 Epoch  34: Total Training Recognition Loss 5.08  Total Training Translation Loss 886.76 
2024-02-03 10:38:38,477 EPOCH 35
2024-02-03 10:38:51,973 Epoch  35: Total Training Recognition Loss 4.91  Total Training Translation Loss 871.04 
2024-02-03 10:38:51,973 EPOCH 36
2024-02-03 10:38:58,599 [Epoch: 036 Step: 00000600] Batch Recognition Loss:   0.178925 => Gls Tokens per Sec:      445 || Batch Translation Loss:  53.084721 => Txt Tokens per Sec:     1357 || Lr: 0.000100
2024-02-03 10:39:05,203 Epoch  36: Total Training Recognition Loss 4.63  Total Training Translation Loss 847.25 
2024-02-03 10:39:05,204 EPOCH 37
2024-02-03 10:39:18,273 Epoch  37: Total Training Recognition Loss 4.36  Total Training Translation Loss 825.61 
2024-02-03 10:39:18,274 EPOCH 38
2024-02-03 10:39:31,590 Epoch  38: Total Training Recognition Loss 4.20  Total Training Translation Loss 813.60 
2024-02-03 10:39:31,590 EPOCH 39
2024-02-03 10:39:44,512 Epoch  39: Total Training Recognition Loss 4.15  Total Training Translation Loss 795.92 
2024-02-03 10:39:44,513 EPOCH 40
2024-02-03 10:39:57,734 Epoch  40: Total Training Recognition Loss 4.09  Total Training Translation Loss 773.33 
2024-02-03 10:39:57,734 EPOCH 41
2024-02-03 10:40:11,030 Epoch  41: Total Training Recognition Loss 3.89  Total Training Translation Loss 762.69 
2024-02-03 10:40:11,030 EPOCH 42
2024-02-03 10:40:11,752 [Epoch: 042 Step: 00000700] Batch Recognition Loss:   0.382427 => Gls Tokens per Sec:     2663 || Batch Translation Loss:  22.045670 => Txt Tokens per Sec:     7044 || Lr: 0.000100
2024-02-03 10:40:24,298 Epoch  42: Total Training Recognition Loss 3.75  Total Training Translation Loss 754.51 
2024-02-03 10:40:24,299 EPOCH 43
2024-02-03 10:40:37,379 Epoch  43: Total Training Recognition Loss 3.67  Total Training Translation Loss 737.78 
2024-02-03 10:40:37,380 EPOCH 44
2024-02-03 10:40:50,588 Epoch  44: Total Training Recognition Loss 3.78  Total Training Translation Loss 715.94 
2024-02-03 10:40:50,588 EPOCH 45
2024-02-03 10:41:04,017 Epoch  45: Total Training Recognition Loss 3.69  Total Training Translation Loss 701.03 
2024-02-03 10:41:04,017 EPOCH 46
2024-02-03 10:41:17,251 Epoch  46: Total Training Recognition Loss 3.49  Total Training Translation Loss 688.36 
2024-02-03 10:41:17,251 EPOCH 47
2024-02-03 10:41:29,980 Epoch  47: Total Training Recognition Loss 3.52  Total Training Translation Loss 670.42 
2024-02-03 10:41:29,981 EPOCH 48
2024-02-03 10:41:30,296 [Epoch: 048 Step: 00000800] Batch Recognition Loss:   0.100256 => Gls Tokens per Sec:     2038 || Batch Translation Loss:  38.468788 => Txt Tokens per Sec:     5682 || Lr: 0.000100
2024-02-03 10:41:43,305 Epoch  48: Total Training Recognition Loss 3.37  Total Training Translation Loss 650.31 
2024-02-03 10:41:43,306 EPOCH 49
2024-02-03 10:41:57,553 Epoch  49: Total Training Recognition Loss 3.37  Total Training Translation Loss 638.05 
2024-02-03 10:41:57,554 EPOCH 50
2024-02-03 10:42:11,303 Epoch  50: Total Training Recognition Loss 3.43  Total Training Translation Loss 623.59 
2024-02-03 10:42:11,303 EPOCH 51
2024-02-03 10:42:24,934 Epoch  51: Total Training Recognition Loss 3.41  Total Training Translation Loss 611.25 
2024-02-03 10:42:24,935 EPOCH 52
2024-02-03 10:42:38,169 Epoch  52: Total Training Recognition Loss 3.48  Total Training Translation Loss 600.06 
2024-02-03 10:42:38,170 EPOCH 53
2024-02-03 10:42:50,937 [Epoch: 053 Step: 00000900] Batch Recognition Loss:   0.083184 => Gls Tokens per Sec:      783 || Batch Translation Loss:  26.850519 => Txt Tokens per Sec:     2153 || Lr: 0.000100
2024-02-03 10:42:51,282 Epoch  53: Total Training Recognition Loss 3.30  Total Training Translation Loss 587.84 
2024-02-03 10:42:51,282 EPOCH 54
2024-02-03 10:43:04,430 Epoch  54: Total Training Recognition Loss 3.14  Total Training Translation Loss 570.18 
2024-02-03 10:43:04,430 EPOCH 55
2024-02-03 10:43:17,847 Epoch  55: Total Training Recognition Loss 3.06  Total Training Translation Loss 564.11 
2024-02-03 10:43:17,848 EPOCH 56
2024-02-03 10:43:31,043 Epoch  56: Total Training Recognition Loss 3.07  Total Training Translation Loss 548.61 
2024-02-03 10:43:31,043 EPOCH 57
2024-02-03 10:43:44,320 Epoch  57: Total Training Recognition Loss 3.09  Total Training Translation Loss 527.87 
2024-02-03 10:43:44,320 EPOCH 58
2024-02-03 10:43:57,323 Epoch  58: Total Training Recognition Loss 3.10  Total Training Translation Loss 513.29 
2024-02-03 10:43:57,323 EPOCH 59
2024-02-03 10:44:09,866 [Epoch: 059 Step: 00001000] Batch Recognition Loss:   0.113328 => Gls Tokens per Sec:      695 || Batch Translation Loss:  26.664856 => Txt Tokens per Sec:     1952 || Lr: 0.000100
2024-02-03 10:44:10,596 Epoch  59: Total Training Recognition Loss 3.38  Total Training Translation Loss 495.49 
2024-02-03 10:44:10,596 EPOCH 60
2024-02-03 10:44:23,866 Epoch  60: Total Training Recognition Loss 2.92  Total Training Translation Loss 481.97 
2024-02-03 10:44:23,866 EPOCH 61
2024-02-03 10:44:36,916 Epoch  61: Total Training Recognition Loss 2.92  Total Training Translation Loss 471.32 
2024-02-03 10:44:36,916 EPOCH 62
2024-02-03 10:44:49,975 Epoch  62: Total Training Recognition Loss 2.76  Total Training Translation Loss 453.46 
2024-02-03 10:44:49,976 EPOCH 63
2024-02-03 10:45:03,449 Epoch  63: Total Training Recognition Loss 2.76  Total Training Translation Loss 443.83 
2024-02-03 10:45:03,450 EPOCH 64
2024-02-03 10:45:16,530 Epoch  64: Total Training Recognition Loss 2.78  Total Training Translation Loss 431.07 
2024-02-03 10:45:16,531 EPOCH 65
2024-02-03 10:45:28,232 [Epoch: 065 Step: 00001100] Batch Recognition Loss:   0.127723 => Gls Tokens per Sec:      635 || Batch Translation Loss:  25.134880 => Txt Tokens per Sec:     1700 || Lr: 0.000100
2024-02-03 10:45:29,738 Epoch  65: Total Training Recognition Loss 2.72  Total Training Translation Loss 416.82 
2024-02-03 10:45:29,738 EPOCH 66
2024-02-03 10:45:43,152 Epoch  66: Total Training Recognition Loss 2.64  Total Training Translation Loss 405.24 
2024-02-03 10:45:43,153 EPOCH 67
2024-02-03 10:45:56,459 Epoch  67: Total Training Recognition Loss 2.62  Total Training Translation Loss 391.63 
2024-02-03 10:45:56,460 EPOCH 68
2024-02-03 10:46:09,915 Epoch  68: Total Training Recognition Loss 2.65  Total Training Translation Loss 378.04 
2024-02-03 10:46:09,916 EPOCH 69
2024-02-03 10:46:23,122 Epoch  69: Total Training Recognition Loss 2.53  Total Training Translation Loss 367.47 
2024-02-03 10:46:23,123 EPOCH 70
2024-02-03 10:46:36,148 Epoch  70: Total Training Recognition Loss 2.59  Total Training Translation Loss 356.56 
2024-02-03 10:46:36,149 EPOCH 71
2024-02-03 10:46:42,378 [Epoch: 071 Step: 00001200] Batch Recognition Loss:   0.136652 => Gls Tokens per Sec:     1028 || Batch Translation Loss:  13.082948 => Txt Tokens per Sec:     2772 || Lr: 0.000100
2024-02-03 10:46:49,378 Epoch  71: Total Training Recognition Loss 2.65  Total Training Translation Loss 353.81 
2024-02-03 10:46:49,378 EPOCH 72
2024-02-03 10:47:02,579 Epoch  72: Total Training Recognition Loss 2.72  Total Training Translation Loss 346.83 
2024-02-03 10:47:02,579 EPOCH 73
2024-02-03 10:47:15,804 Epoch  73: Total Training Recognition Loss 2.84  Total Training Translation Loss 342.96 
2024-02-03 10:47:15,805 EPOCH 74
2024-02-03 10:47:28,921 Epoch  74: Total Training Recognition Loss 2.62  Total Training Translation Loss 322.42 
2024-02-03 10:47:28,922 EPOCH 75
2024-02-03 10:47:42,373 Epoch  75: Total Training Recognition Loss 2.52  Total Training Translation Loss 311.14 
2024-02-03 10:47:42,374 EPOCH 76
2024-02-03 10:47:55,415 Epoch  76: Total Training Recognition Loss 2.67  Total Training Translation Loss 297.16 
2024-02-03 10:47:55,416 EPOCH 77
2024-02-03 10:48:02,789 [Epoch: 077 Step: 00001300] Batch Recognition Loss:   0.127637 => Gls Tokens per Sec:      661 || Batch Translation Loss:  19.299185 => Txt Tokens per Sec:     1910 || Lr: 0.000100
2024-02-03 10:48:08,974 Epoch  77: Total Training Recognition Loss 2.71  Total Training Translation Loss 291.21 
2024-02-03 10:48:08,975 EPOCH 78
2024-02-03 10:48:22,194 Epoch  78: Total Training Recognition Loss 2.50  Total Training Translation Loss 273.68 
2024-02-03 10:48:22,195 EPOCH 79
2024-02-03 10:48:35,436 Epoch  79: Total Training Recognition Loss 2.39  Total Training Translation Loss 262.11 
2024-02-03 10:48:35,437 EPOCH 80
2024-02-03 10:48:48,495 Epoch  80: Total Training Recognition Loss 2.41  Total Training Translation Loss 251.70 
2024-02-03 10:48:48,495 EPOCH 81
2024-02-03 10:49:01,701 Epoch  81: Total Training Recognition Loss 2.38  Total Training Translation Loss 242.66 
2024-02-03 10:49:01,701 EPOCH 82
2024-02-03 10:49:15,238 Epoch  82: Total Training Recognition Loss 2.33  Total Training Translation Loss 236.72 
2024-02-03 10:49:15,238 EPOCH 83
2024-02-03 10:49:17,724 [Epoch: 083 Step: 00001400] Batch Recognition Loss:   0.170031 => Gls Tokens per Sec:     1545 || Batch Translation Loss:  17.127672 => Txt Tokens per Sec:     3893 || Lr: 0.000100
2024-02-03 10:49:28,200 Epoch  83: Total Training Recognition Loss 2.38  Total Training Translation Loss 228.33 
2024-02-03 10:49:28,201 EPOCH 84
2024-02-03 10:49:41,469 Epoch  84: Total Training Recognition Loss 2.25  Total Training Translation Loss 225.66 
2024-02-03 10:49:41,470 EPOCH 85
2024-02-03 10:49:54,672 Epoch  85: Total Training Recognition Loss 2.29  Total Training Translation Loss 216.30 
2024-02-03 10:49:54,673 EPOCH 86
2024-02-03 10:50:07,770 Epoch  86: Total Training Recognition Loss 2.30  Total Training Translation Loss 205.17 
2024-02-03 10:50:07,771 EPOCH 87
2024-02-03 10:50:21,227 Epoch  87: Total Training Recognition Loss 2.21  Total Training Translation Loss 198.09 
2024-02-03 10:50:21,228 EPOCH 88
2024-02-03 10:50:34,121 Epoch  88: Total Training Recognition Loss 2.23  Total Training Translation Loss 189.73 
2024-02-03 10:50:34,121 EPOCH 89
2024-02-03 10:50:40,596 [Epoch: 089 Step: 00001500] Batch Recognition Loss:   0.146187 => Gls Tokens per Sec:      357 || Batch Translation Loss:  13.879002 => Txt Tokens per Sec:     1123 || Lr: 0.000100
2024-02-03 10:50:47,603 Epoch  89: Total Training Recognition Loss 2.27  Total Training Translation Loss 181.93 
2024-02-03 10:50:47,604 EPOCH 90
2024-02-03 10:51:00,805 Epoch  90: Total Training Recognition Loss 2.32  Total Training Translation Loss 184.43 
2024-02-03 10:51:00,805 EPOCH 91
2024-02-03 10:51:14,263 Epoch  91: Total Training Recognition Loss 2.14  Total Training Translation Loss 170.91 
2024-02-03 10:51:14,263 EPOCH 92
2024-02-03 10:51:27,334 Epoch  92: Total Training Recognition Loss 2.16  Total Training Translation Loss 159.95 
2024-02-03 10:51:27,334 EPOCH 93
2024-02-03 10:51:39,975 Epoch  93: Total Training Recognition Loss 2.07  Total Training Translation Loss 152.75 
2024-02-03 10:51:39,976 EPOCH 94
2024-02-03 10:51:53,305 Epoch  94: Total Training Recognition Loss 2.16  Total Training Translation Loss 147.48 
2024-02-03 10:51:53,305 EPOCH 95
2024-02-03 10:51:53,864 [Epoch: 095 Step: 00001600] Batch Recognition Loss:   0.085309 => Gls Tokens per Sec:     2298 || Batch Translation Loss:   9.277814 => Txt Tokens per Sec:     6824 || Lr: 0.000100
2024-02-03 10:52:06,351 Epoch  95: Total Training Recognition Loss 2.01  Total Training Translation Loss 142.11 
2024-02-03 10:52:06,352 EPOCH 96
2024-02-03 10:52:19,726 Epoch  96: Total Training Recognition Loss 2.00  Total Training Translation Loss 133.89 
2024-02-03 10:52:19,726 EPOCH 97
2024-02-03 10:52:32,916 Epoch  97: Total Training Recognition Loss 2.07  Total Training Translation Loss 132.82 
2024-02-03 10:52:32,916 EPOCH 98
2024-02-03 10:52:46,092 Epoch  98: Total Training Recognition Loss 1.95  Total Training Translation Loss 123.57 
2024-02-03 10:52:46,093 EPOCH 99
2024-02-03 10:52:59,513 Epoch  99: Total Training Recognition Loss 1.95  Total Training Translation Loss 117.34 
2024-02-03 10:52:59,513 EPOCH 100
2024-02-03 10:53:12,691 [Epoch: 100 Step: 00001700] Batch Recognition Loss:   0.058259 => Gls Tokens per Sec:      807 || Batch Translation Loss:   5.444930 => Txt Tokens per Sec:     2243 || Lr: 0.000100
2024-02-03 10:53:12,692 Epoch 100: Total Training Recognition Loss 1.87  Total Training Translation Loss 113.89 
2024-02-03 10:53:12,692 EPOCH 101
2024-02-03 10:53:26,023 Epoch 101: Total Training Recognition Loss 1.87  Total Training Translation Loss 109.96 
2024-02-03 10:53:26,023 EPOCH 102
2024-02-03 10:53:39,338 Epoch 102: Total Training Recognition Loss 2.00  Total Training Translation Loss 112.39 
2024-02-03 10:53:39,338 EPOCH 103
2024-02-03 10:53:52,526 Epoch 103: Total Training Recognition Loss 1.89  Total Training Translation Loss 109.55 
2024-02-03 10:53:52,527 EPOCH 104
2024-02-03 10:54:05,790 Epoch 104: Total Training Recognition Loss 1.85  Total Training Translation Loss 101.32 
2024-02-03 10:54:05,791 EPOCH 105
2024-02-03 10:54:18,762 Epoch 105: Total Training Recognition Loss 1.83  Total Training Translation Loss 93.29 
2024-02-03 10:54:18,763 EPOCH 106
2024-02-03 10:54:31,449 [Epoch: 106 Step: 00001800] Batch Recognition Loss:   0.172783 => Gls Tokens per Sec:      737 || Batch Translation Loss:   2.496196 => Txt Tokens per Sec:     2054 || Lr: 0.000100
2024-02-03 10:54:31,996 Epoch 106: Total Training Recognition Loss 1.71  Total Training Translation Loss 89.09 
2024-02-03 10:54:31,996 EPOCH 107
2024-02-03 10:54:45,303 Epoch 107: Total Training Recognition Loss 1.70  Total Training Translation Loss 84.59 
2024-02-03 10:54:45,303 EPOCH 108
2024-02-03 10:54:58,552 Epoch 108: Total Training Recognition Loss 1.62  Total Training Translation Loss 80.63 
2024-02-03 10:54:58,552 EPOCH 109
2024-02-03 10:55:11,801 Epoch 109: Total Training Recognition Loss 1.61  Total Training Translation Loss 77.51 
2024-02-03 10:55:11,801 EPOCH 110
2024-02-03 10:55:25,331 Epoch 110: Total Training Recognition Loss 1.63  Total Training Translation Loss 74.04 
2024-02-03 10:55:25,331 EPOCH 111
2024-02-03 10:55:38,285 Epoch 111: Total Training Recognition Loss 1.62  Total Training Translation Loss 70.10 
2024-02-03 10:55:38,286 EPOCH 112
2024-02-03 10:55:46,543 [Epoch: 112 Step: 00001900] Batch Recognition Loss:   0.081563 => Gls Tokens per Sec:     1008 || Batch Translation Loss:   4.774201 => Txt Tokens per Sec:     2750 || Lr: 0.000100
2024-02-03 10:55:51,553 Epoch 112: Total Training Recognition Loss 1.55  Total Training Translation Loss 70.90 
2024-02-03 10:55:51,554 EPOCH 113
2024-02-03 10:56:04,700 Epoch 113: Total Training Recognition Loss 1.64  Total Training Translation Loss 71.70 
2024-02-03 10:56:04,701 EPOCH 114
2024-02-03 10:56:18,152 Epoch 114: Total Training Recognition Loss 1.48  Total Training Translation Loss 70.03 
2024-02-03 10:56:18,153 EPOCH 115
2024-02-03 10:56:31,137 Epoch 115: Total Training Recognition Loss 1.51  Total Training Translation Loss 64.92 
2024-02-03 10:56:31,138 EPOCH 116
2024-02-03 10:56:44,524 Epoch 116: Total Training Recognition Loss 1.50  Total Training Translation Loss 62.29 
2024-02-03 10:56:44,525 EPOCH 117
2024-02-03 10:56:57,549 Epoch 117: Total Training Recognition Loss 1.51  Total Training Translation Loss 57.21 
2024-02-03 10:56:57,550 EPOCH 118
2024-02-03 10:57:04,918 [Epoch: 118 Step: 00002000] Batch Recognition Loss:   0.077268 => Gls Tokens per Sec:      922 || Batch Translation Loss:   2.139286 => Txt Tokens per Sec:     2414 || Lr: 0.000100
2024-02-03 10:58:13,745 Hooray! New best validation result [eval_metric]!
2024-02-03 10:58:13,747 Saving new checkpoint.
2024-02-03 10:58:14,091 Validation result at epoch 118, step     2000: duration: 69.1719s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.79326	Translation Loss: 67823.36719	PPL: 1146.40674
	Eval Metric: BLEU
	WER 8.27	(DEL: 0.00,	INS: 0.00,	SUB: 8.27)
	BLEU-4 0.93	(BLEU-1: 13.07,	BLEU-2: 4.54,	BLEU-3: 1.88,	BLEU-4: 0.93)
	CHRF 17.94	ROUGE 11.04
2024-02-03 10:58:14,093 Logging Recognition and Translation Outputs
2024-02-03 10:58:14,093 ========================================================================================================================
2024-02-03 10:58:14,093 Logging Sequence: 143_161.00
2024-02-03 10:58:14,093 	Gloss Reference :	A B+C+D+E
2024-02-03 10:58:14,094 	Gloss Hypothesis:	A B+C+E  
2024-02-03 10:58:14,094 	Gloss Alignment :	  S      
2024-02-03 10:58:14,094 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 10:58:14,094 	Text Reference  :	there   is no response from them as  yet 
2024-02-03 10:58:14,095 	Text Hypothesis :	ronaldo is ** ******** **** why  the same
2024-02-03 10:58:14,095 	Text Alignment  :	S          D  D        D    S    S   S   
2024-02-03 10:58:14,095 ========================================================================================================================
2024-02-03 10:58:14,095 Logging Sequence: 63_21.00
2024-02-03 10:58:14,095 	Gloss Reference :	A B+C+D+E
2024-02-03 10:58:14,095 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 10:58:14,095 	Gloss Alignment :	         
2024-02-03 10:58:14,095 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 10:58:14,097 	Text Reference  :	*** ***** ** *** *** *** **** however the ******* teams  will be announced only on 25th october 2021       
2024-02-03 10:58:14,097 	Text Hypothesis :	the final of the ipl was held at      the highest bidder will be ********* **** ** at   the     semi-finals
2024-02-03 10:58:14,097 	Text Alignment  :	I   I     I  I   I   I   I    S           I       S              D         D    D  S    S       S          
2024-02-03 10:58:14,097 ========================================================================================================================
2024-02-03 10:58:14,097 Logging Sequence: 122_147.00
2024-02-03 10:58:14,097 	Gloss Reference :	A B+C+D+E
2024-02-03 10:58:14,097 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 10:58:14,098 	Gloss Alignment :	         
2024-02-03 10:58:14,098 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 10:58:14,099 	Text Reference  :	***** chanu had  been  working as   a    tickets inspector in      the indian railways
2024-02-03 10:58:14,099 	Text Hypothesis :	after the   gold medal they    will give you     mirabai   saikhom as  a      pizza   
2024-02-03 10:58:14,099 	Text Alignment  :	I     S     S    S     S       S    S    S       S         S       S   S      S       
2024-02-03 10:58:14,099 ========================================================================================================================
2024-02-03 10:58:14,099 Logging Sequence: 87_196.00
2024-02-03 10:58:14,099 	Gloss Reference :	A B+C+D+E
2024-02-03 10:58:14,099 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 10:58:14,100 	Gloss Alignment :	         
2024-02-03 10:58:14,100 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 10:58:14,101 	Text Reference  :	****** **** ******* ** my  expression was ****** not against dhoni    or    kohli
2024-02-03 10:58:14,101 	Text Hypothesis :	people were stunned by the video      was called for the     upcoming world cup  
2024-02-03 10:58:14,101 	Text Alignment  :	I      I    I       I  S   S              I      S   S       S        S     S    
2024-02-03 10:58:14,101 ========================================================================================================================
2024-02-03 10:58:14,101 Logging Sequence: 114_153.00
2024-02-03 10:58:14,101 	Gloss Reference :	A B+C+D+E
2024-02-03 10:58:14,101 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 10:58:14,101 	Gloss Alignment :	         
2024-02-03 10:58:14,102 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 10:58:14,102 	Text Reference  :	another big   football news  is        that the copa america final
2024-02-03 10:58:14,102 	Text Hypothesis :	******* after 28       years argentina won  the copa america *****
2024-02-03 10:58:14,102 	Text Alignment  :	D       S     S        S     S         S                     D    
2024-02-03 10:58:14,103 ========================================================================================================================
2024-02-03 10:58:19,874 Epoch 118: Total Training Recognition Loss 1.41  Total Training Translation Loss 56.02 
2024-02-03 10:58:19,874 EPOCH 119
2024-02-03 10:58:33,154 Epoch 119: Total Training Recognition Loss 1.46  Total Training Translation Loss 52.98 
2024-02-03 10:58:33,154 EPOCH 120
2024-02-03 10:58:46,224 Epoch 120: Total Training Recognition Loss 1.43  Total Training Translation Loss 50.35 
2024-02-03 10:58:46,224 EPOCH 121
2024-02-03 10:58:59,325 Epoch 121: Total Training Recognition Loss 1.33  Total Training Translation Loss 47.85 
2024-02-03 10:58:59,326 EPOCH 122
2024-02-03 10:59:12,546 Epoch 122: Total Training Recognition Loss 1.33  Total Training Translation Loss 46.36 
2024-02-03 10:59:12,547 EPOCH 123
2024-02-03 10:59:25,869 Epoch 123: Total Training Recognition Loss 1.28  Total Training Translation Loss 45.57 
2024-02-03 10:59:25,869 EPOCH 124
2024-02-03 10:59:27,657 [Epoch: 124 Step: 00002100] Batch Recognition Loss:   0.033081 => Gls Tokens per Sec:     3225 || Batch Translation Loss:   2.326316 => Txt Tokens per Sec:     7980 || Lr: 0.000100
2024-02-03 10:59:39,058 Epoch 124: Total Training Recognition Loss 1.30  Total Training Translation Loss 44.66 
2024-02-03 10:59:39,059 EPOCH 125
2024-02-03 10:59:52,401 Epoch 125: Total Training Recognition Loss 1.32  Total Training Translation Loss 44.26 
2024-02-03 10:59:52,401 EPOCH 126
2024-02-03 11:00:05,504 Epoch 126: Total Training Recognition Loss 1.30  Total Training Translation Loss 44.46 
2024-02-03 11:00:05,504 EPOCH 127
2024-02-03 11:00:18,822 Epoch 127: Total Training Recognition Loss 1.25  Total Training Translation Loss 42.28 
2024-02-03 11:00:18,823 EPOCH 128
2024-02-03 11:00:31,886 Epoch 128: Total Training Recognition Loss 1.27  Total Training Translation Loss 39.67 
2024-02-03 11:00:31,886 EPOCH 129
2024-02-03 11:00:45,149 Epoch 129: Total Training Recognition Loss 1.23  Total Training Translation Loss 39.19 
2024-02-03 11:00:45,149 EPOCH 130
2024-02-03 11:00:48,213 [Epoch: 130 Step: 00002200] Batch Recognition Loss:   0.049623 => Gls Tokens per Sec:     1463 || Batch Translation Loss:   2.530075 => Txt Tokens per Sec:     4207 || Lr: 0.000100
2024-02-03 11:00:58,283 Epoch 130: Total Training Recognition Loss 1.20  Total Training Translation Loss 36.77 
2024-02-03 11:00:58,283 EPOCH 131
2024-02-03 11:01:11,549 Epoch 131: Total Training Recognition Loss 1.13  Total Training Translation Loss 35.41 
2024-02-03 11:01:11,550 EPOCH 132
2024-02-03 11:01:24,878 Epoch 132: Total Training Recognition Loss 1.14  Total Training Translation Loss 34.55 
2024-02-03 11:01:24,878 EPOCH 133
2024-02-03 11:01:38,240 Epoch 133: Total Training Recognition Loss 1.11  Total Training Translation Loss 32.91 
2024-02-03 11:01:38,241 EPOCH 134
2024-02-03 11:01:51,420 Epoch 134: Total Training Recognition Loss 1.11  Total Training Translation Loss 32.15 
2024-02-03 11:01:51,420 EPOCH 135
2024-02-03 11:02:04,541 Epoch 135: Total Training Recognition Loss 1.07  Total Training Translation Loss 30.67 
2024-02-03 11:02:04,541 EPOCH 136
2024-02-03 11:02:05,523 [Epoch: 136 Step: 00002300] Batch Recognition Loss:   0.044448 => Gls Tokens per Sec:     3265 || Batch Translation Loss:   2.116006 => Txt Tokens per Sec:     8071 || Lr: 0.000100
2024-02-03 11:02:17,805 Epoch 136: Total Training Recognition Loss 1.09  Total Training Translation Loss 30.54 
2024-02-03 11:02:17,806 EPOCH 137
2024-02-03 11:02:31,046 Epoch 137: Total Training Recognition Loss 1.08  Total Training Translation Loss 29.51 
2024-02-03 11:02:31,046 EPOCH 138
2024-02-03 11:02:44,453 Epoch 138: Total Training Recognition Loss 1.09  Total Training Translation Loss 29.45 
2024-02-03 11:02:44,453 EPOCH 139
2024-02-03 11:02:57,650 Epoch 139: Total Training Recognition Loss 1.05  Total Training Translation Loss 29.08 
2024-02-03 11:02:57,651 EPOCH 140
2024-02-03 11:03:10,936 Epoch 140: Total Training Recognition Loss 1.02  Total Training Translation Loss 28.27 
2024-02-03 11:03:10,937 EPOCH 141
2024-02-03 11:03:23,990 Epoch 141: Total Training Recognition Loss 1.00  Total Training Translation Loss 27.46 
2024-02-03 11:03:23,991 EPOCH 142
2024-02-03 11:03:24,509 [Epoch: 142 Step: 00002400] Batch Recognition Loss:   0.022945 => Gls Tokens per Sec:     3714 || Batch Translation Loss:   1.391444 => Txt Tokens per Sec:     8712 || Lr: 0.000100
2024-02-03 11:03:37,205 Epoch 142: Total Training Recognition Loss 0.98  Total Training Translation Loss 26.28 
2024-02-03 11:03:37,206 EPOCH 143
2024-02-03 11:03:50,608 Epoch 143: Total Training Recognition Loss 0.94  Total Training Translation Loss 25.35 
2024-02-03 11:03:50,609 EPOCH 144
2024-02-03 11:04:03,446 Epoch 144: Total Training Recognition Loss 0.89  Total Training Translation Loss 24.59 
2024-02-03 11:04:03,446 EPOCH 145
2024-02-03 11:04:16,109 Epoch 145: Total Training Recognition Loss 0.89  Total Training Translation Loss 23.72 
2024-02-03 11:04:16,110 EPOCH 146
2024-02-03 11:04:29,721 Epoch 146: Total Training Recognition Loss 0.91  Total Training Translation Loss 23.17 
2024-02-03 11:04:29,722 EPOCH 147
2024-02-03 11:04:42,854 Epoch 147: Total Training Recognition Loss 0.89  Total Training Translation Loss 22.57 
2024-02-03 11:04:42,854 EPOCH 148
2024-02-03 11:04:43,189 [Epoch: 148 Step: 00002500] Batch Recognition Loss:   0.038110 => Gls Tokens per Sec:     1913 || Batch Translation Loss:   1.419699 => Txt Tokens per Sec:     5659 || Lr: 0.000100
2024-02-03 11:04:56,496 Epoch 148: Total Training Recognition Loss 0.85  Total Training Translation Loss 21.87 
2024-02-03 11:04:56,497 EPOCH 149
2024-02-03 11:05:09,853 Epoch 149: Total Training Recognition Loss 0.88  Total Training Translation Loss 21.60 
2024-02-03 11:05:09,853 EPOCH 150
2024-02-03 11:05:23,207 Epoch 150: Total Training Recognition Loss 0.81  Total Training Translation Loss 20.77 
2024-02-03 11:05:23,207 EPOCH 151
2024-02-03 11:05:36,544 Epoch 151: Total Training Recognition Loss 0.84  Total Training Translation Loss 20.52 
2024-02-03 11:05:36,545 EPOCH 152
2024-02-03 11:05:49,649 Epoch 152: Total Training Recognition Loss 0.81  Total Training Translation Loss 20.18 
2024-02-03 11:05:49,650 EPOCH 153
2024-02-03 11:06:02,652 [Epoch: 153 Step: 00002600] Batch Recognition Loss:   0.044165 => Gls Tokens per Sec:      768 || Batch Translation Loss:   1.472148 => Txt Tokens per Sec:     2159 || Lr: 0.000100
2024-02-03 11:06:02,817 Epoch 153: Total Training Recognition Loss 0.78  Total Training Translation Loss 19.88 
2024-02-03 11:06:02,818 EPOCH 154
2024-02-03 11:06:16,099 Epoch 154: Total Training Recognition Loss 0.83  Total Training Translation Loss 19.08 
2024-02-03 11:06:16,100 EPOCH 155
2024-02-03 11:06:29,478 Epoch 155: Total Training Recognition Loss 0.82  Total Training Translation Loss 18.76 
2024-02-03 11:06:29,479 EPOCH 156
2024-02-03 11:06:42,607 Epoch 156: Total Training Recognition Loss 0.76  Total Training Translation Loss 19.01 
2024-02-03 11:06:42,608 EPOCH 157
2024-02-03 11:06:55,834 Epoch 157: Total Training Recognition Loss 0.73  Total Training Translation Loss 18.14 
2024-02-03 11:06:55,835 EPOCH 158
2024-02-03 11:07:08,940 Epoch 158: Total Training Recognition Loss 0.76  Total Training Translation Loss 19.62 
2024-02-03 11:07:08,940 EPOCH 159
2024-02-03 11:07:21,322 [Epoch: 159 Step: 00002700] Batch Recognition Loss:   0.041599 => Gls Tokens per Sec:      704 || Batch Translation Loss:   1.177601 => Txt Tokens per Sec:     2013 || Lr: 0.000100
2024-02-03 11:07:22,039 Epoch 159: Total Training Recognition Loss 0.78  Total Training Translation Loss 18.72 
2024-02-03 11:07:22,040 EPOCH 160
2024-02-03 11:07:35,493 Epoch 160: Total Training Recognition Loss 0.78  Total Training Translation Loss 18.74 
2024-02-03 11:07:35,493 EPOCH 161
2024-02-03 11:07:48,840 Epoch 161: Total Training Recognition Loss 0.78  Total Training Translation Loss 18.10 
2024-02-03 11:07:48,841 EPOCH 162
2024-02-03 11:08:01,750 Epoch 162: Total Training Recognition Loss 0.75  Total Training Translation Loss 16.83 
2024-02-03 11:08:01,751 EPOCH 163
2024-02-03 11:08:15,022 Epoch 163: Total Training Recognition Loss 0.70  Total Training Translation Loss 15.94 
2024-02-03 11:08:15,022 EPOCH 164
2024-02-03 11:08:28,382 Epoch 164: Total Training Recognition Loss 0.69  Total Training Translation Loss 15.65 
2024-02-03 11:08:28,383 EPOCH 165
2024-02-03 11:08:39,196 [Epoch: 165 Step: 00002800] Batch Recognition Loss:   0.052472 => Gls Tokens per Sec:      687 || Batch Translation Loss:   1.217054 => Txt Tokens per Sec:     1933 || Lr: 0.000100
2024-02-03 11:08:41,835 Epoch 165: Total Training Recognition Loss 0.70  Total Training Translation Loss 15.54 
2024-02-03 11:08:41,836 EPOCH 166
2024-02-03 11:08:54,992 Epoch 166: Total Training Recognition Loss 0.67  Total Training Translation Loss 14.93 
2024-02-03 11:08:54,993 EPOCH 167
2024-02-03 11:09:08,268 Epoch 167: Total Training Recognition Loss 0.65  Total Training Translation Loss 14.80 
2024-02-03 11:09:08,269 EPOCH 168
2024-02-03 11:09:21,555 Epoch 168: Total Training Recognition Loss 0.71  Total Training Translation Loss 14.21 
2024-02-03 11:09:21,555 EPOCH 169
2024-02-03 11:09:34,771 Epoch 169: Total Training Recognition Loss 0.66  Total Training Translation Loss 14.13 
2024-02-03 11:09:34,771 EPOCH 170
2024-02-03 11:09:47,916 Epoch 170: Total Training Recognition Loss 0.62  Total Training Translation Loss 14.45 
2024-02-03 11:09:47,916 EPOCH 171
2024-02-03 11:09:57,108 [Epoch: 171 Step: 00002900] Batch Recognition Loss:   0.025364 => Gls Tokens per Sec:      669 || Batch Translation Loss:   0.865001 => Txt Tokens per Sec:     1908 || Lr: 0.000100
2024-02-03 11:10:01,243 Epoch 171: Total Training Recognition Loss 0.68  Total Training Translation Loss 13.80 
2024-02-03 11:10:01,243 EPOCH 172
2024-02-03 11:10:14,297 Epoch 172: Total Training Recognition Loss 0.64  Total Training Translation Loss 13.49 
2024-02-03 11:10:14,298 EPOCH 173
2024-02-03 11:10:27,583 Epoch 173: Total Training Recognition Loss 0.60  Total Training Translation Loss 13.08 
2024-02-03 11:10:27,584 EPOCH 174
2024-02-03 11:10:40,748 Epoch 174: Total Training Recognition Loss 0.60  Total Training Translation Loss 13.09 
2024-02-03 11:10:40,749 EPOCH 175
2024-02-03 11:10:53,962 Epoch 175: Total Training Recognition Loss 0.61  Total Training Translation Loss 12.84 
2024-02-03 11:10:53,962 EPOCH 176
2024-02-03 11:11:07,245 Epoch 176: Total Training Recognition Loss 0.56  Total Training Translation Loss 12.63 
2024-02-03 11:11:07,246 EPOCH 177
2024-02-03 11:11:17,302 [Epoch: 177 Step: 00003000] Batch Recognition Loss:   0.023361 => Gls Tokens per Sec:      484 || Batch Translation Loss:   0.698956 => Txt Tokens per Sec:     1452 || Lr: 0.000100
2024-02-03 11:11:20,484 Epoch 177: Total Training Recognition Loss 0.58  Total Training Translation Loss 12.36 
2024-02-03 11:11:20,484 EPOCH 178
2024-02-03 11:11:33,868 Epoch 178: Total Training Recognition Loss 0.58  Total Training Translation Loss 11.81 
2024-02-03 11:11:33,869 EPOCH 179
2024-02-03 11:11:46,803 Epoch 179: Total Training Recognition Loss 0.56  Total Training Translation Loss 12.05 
2024-02-03 11:11:46,803 EPOCH 180
2024-02-03 11:12:00,257 Epoch 180: Total Training Recognition Loss 0.61  Total Training Translation Loss 11.79 
2024-02-03 11:12:00,257 EPOCH 181
2024-02-03 11:12:13,554 Epoch 181: Total Training Recognition Loss 0.56  Total Training Translation Loss 11.79 
2024-02-03 11:12:13,554 EPOCH 182
2024-02-03 11:12:26,848 Epoch 182: Total Training Recognition Loss 0.54  Total Training Translation Loss 12.00 
2024-02-03 11:12:26,848 EPOCH 183
2024-02-03 11:12:36,251 [Epoch: 183 Step: 00003100] Batch Recognition Loss:   0.059467 => Gls Tokens per Sec:      382 || Batch Translation Loss:   0.897701 => Txt Tokens per Sec:     1179 || Lr: 0.000100
2024-02-03 11:12:40,221 Epoch 183: Total Training Recognition Loss 0.53  Total Training Translation Loss 11.46 
2024-02-03 11:12:40,222 EPOCH 184
2024-02-03 11:12:53,281 Epoch 184: Total Training Recognition Loss 0.58  Total Training Translation Loss 10.91 
2024-02-03 11:12:53,282 EPOCH 185
2024-02-03 11:13:06,697 Epoch 185: Total Training Recognition Loss 0.53  Total Training Translation Loss 10.10 
2024-02-03 11:13:06,698 EPOCH 186
2024-02-03 11:13:19,999 Epoch 186: Total Training Recognition Loss 0.49  Total Training Translation Loss 9.82 
2024-02-03 11:13:20,000 EPOCH 187
2024-02-03 11:13:33,504 Epoch 187: Total Training Recognition Loss 0.50  Total Training Translation Loss 9.69 
2024-02-03 11:13:33,505 EPOCH 188
2024-02-03 11:13:46,888 Epoch 188: Total Training Recognition Loss 0.51  Total Training Translation Loss 9.61 
2024-02-03 11:13:46,888 EPOCH 189
2024-02-03 11:13:50,439 [Epoch: 189 Step: 00003200] Batch Recognition Loss:   0.012483 => Gls Tokens per Sec:      721 || Batch Translation Loss:   0.507114 => Txt Tokens per Sec:     1926 || Lr: 0.000100
2024-02-03 11:14:00,225 Epoch 189: Total Training Recognition Loss 0.48  Total Training Translation Loss 9.68 
2024-02-03 11:14:00,225 EPOCH 190
2024-02-03 11:14:13,778 Epoch 190: Total Training Recognition Loss 0.48  Total Training Translation Loss 9.20 
2024-02-03 11:14:13,779 EPOCH 191
2024-02-03 11:14:27,047 Epoch 191: Total Training Recognition Loss 0.47  Total Training Translation Loss 9.16 
2024-02-03 11:14:27,047 EPOCH 192
2024-02-03 11:14:40,066 Epoch 192: Total Training Recognition Loss 0.47  Total Training Translation Loss 9.35 
2024-02-03 11:14:40,067 EPOCH 193
2024-02-03 11:14:53,360 Epoch 193: Total Training Recognition Loss 0.46  Total Training Translation Loss 10.04 
2024-02-03 11:14:53,361 EPOCH 194
2024-02-03 11:15:06,574 Epoch 194: Total Training Recognition Loss 0.47  Total Training Translation Loss 14.44 
2024-02-03 11:15:06,575 EPOCH 195
2024-02-03 11:15:06,905 [Epoch: 195 Step: 00003300] Batch Recognition Loss:   0.022116 => Gls Tokens per Sec:     3891 || Batch Translation Loss:   1.107884 => Txt Tokens per Sec:     9456 || Lr: 0.000100
2024-02-03 11:15:19,718 Epoch 195: Total Training Recognition Loss 1.97  Total Training Translation Loss 53.24 
2024-02-03 11:15:19,719 EPOCH 196
2024-02-03 11:15:32,906 Epoch 196: Total Training Recognition Loss 1.60  Total Training Translation Loss 55.71 
2024-02-03 11:15:32,907 EPOCH 197
2024-02-03 11:15:46,297 Epoch 197: Total Training Recognition Loss 1.57  Total Training Translation Loss 44.18 
2024-02-03 11:15:46,298 EPOCH 198
2024-02-03 11:15:59,285 Epoch 198: Total Training Recognition Loss 1.10  Total Training Translation Loss 30.06 
2024-02-03 11:15:59,286 EPOCH 199
2024-02-03 11:16:12,775 Epoch 199: Total Training Recognition Loss 0.95  Total Training Translation Loss 17.98 
2024-02-03 11:16:12,775 EPOCH 200
2024-02-03 11:16:25,971 [Epoch: 200 Step: 00003400] Batch Recognition Loss:   0.018141 => Gls Tokens per Sec:      806 || Batch Translation Loss:   0.721997 => Txt Tokens per Sec:     2240 || Lr: 0.000100
2024-02-03 11:16:25,972 Epoch 200: Total Training Recognition Loss 0.69  Total Training Translation Loss 13.26 
2024-02-03 11:16:25,972 EPOCH 201
2024-02-03 11:16:39,263 Epoch 201: Total Training Recognition Loss 0.64  Total Training Translation Loss 10.35 
2024-02-03 11:16:39,263 EPOCH 202
2024-02-03 11:16:52,238 Epoch 202: Total Training Recognition Loss 0.53  Total Training Translation Loss 8.92 
2024-02-03 11:16:52,238 EPOCH 203
2024-02-03 11:17:05,549 Epoch 203: Total Training Recognition Loss 0.51  Total Training Translation Loss 8.20 
2024-02-03 11:17:05,550 EPOCH 204
2024-02-03 11:17:18,711 Epoch 204: Total Training Recognition Loss 0.52  Total Training Translation Loss 7.57 
2024-02-03 11:17:18,711 EPOCH 205
2024-02-03 11:17:31,771 Epoch 205: Total Training Recognition Loss 0.46  Total Training Translation Loss 7.21 
2024-02-03 11:17:31,772 EPOCH 206
2024-02-03 11:17:44,615 [Epoch: 206 Step: 00003500] Batch Recognition Loss:   0.048804 => Gls Tokens per Sec:      728 || Batch Translation Loss:   0.389101 => Txt Tokens per Sec:     2020 || Lr: 0.000100
2024-02-03 11:17:45,146 Epoch 206: Total Training Recognition Loss 0.43  Total Training Translation Loss 7.06 
2024-02-03 11:17:45,146 EPOCH 207
2024-02-03 11:17:58,670 Epoch 207: Total Training Recognition Loss 0.43  Total Training Translation Loss 6.81 
2024-02-03 11:17:58,670 EPOCH 208
2024-02-03 11:18:11,943 Epoch 208: Total Training Recognition Loss 0.42  Total Training Translation Loss 6.40 
2024-02-03 11:18:11,943 EPOCH 209
2024-02-03 11:18:25,376 Epoch 209: Total Training Recognition Loss 0.40  Total Training Translation Loss 6.24 
2024-02-03 11:18:25,376 EPOCH 210
2024-02-03 11:18:38,576 Epoch 210: Total Training Recognition Loss 0.37  Total Training Translation Loss 6.02 
2024-02-03 11:18:38,576 EPOCH 211
2024-02-03 11:18:51,556 Epoch 211: Total Training Recognition Loss 0.38  Total Training Translation Loss 6.06 
2024-02-03 11:18:51,556 EPOCH 212
2024-02-03 11:18:59,680 [Epoch: 212 Step: 00003600] Batch Recognition Loss:   0.048271 => Gls Tokens per Sec:     1024 || Batch Translation Loss:   0.468921 => Txt Tokens per Sec:     2827 || Lr: 0.000100
2024-02-03 11:19:04,600 Epoch 212: Total Training Recognition Loss 0.42  Total Training Translation Loss 5.77 
2024-02-03 11:19:04,600 EPOCH 213
2024-02-03 11:19:17,874 Epoch 213: Total Training Recognition Loss 0.40  Total Training Translation Loss 5.59 
2024-02-03 11:19:17,875 EPOCH 214
2024-02-03 11:19:31,518 Epoch 214: Total Training Recognition Loss 0.35  Total Training Translation Loss 5.67 
2024-02-03 11:19:31,519 EPOCH 215
2024-02-03 11:19:44,618 Epoch 215: Total Training Recognition Loss 0.38  Total Training Translation Loss 5.53 
2024-02-03 11:19:44,618 EPOCH 216
2024-02-03 11:19:57,638 Epoch 216: Total Training Recognition Loss 0.33  Total Training Translation Loss 5.30 
2024-02-03 11:19:57,639 EPOCH 217
2024-02-03 11:20:10,943 Epoch 217: Total Training Recognition Loss 0.34  Total Training Translation Loss 5.12 
2024-02-03 11:20:10,944 EPOCH 218
2024-02-03 11:20:18,870 [Epoch: 218 Step: 00003700] Batch Recognition Loss:   0.020260 => Gls Tokens per Sec:      857 || Batch Translation Loss:   0.354971 => Txt Tokens per Sec:     2340 || Lr: 0.000100
2024-02-03 11:20:24,091 Epoch 218: Total Training Recognition Loss 0.33  Total Training Translation Loss 4.95 
2024-02-03 11:20:24,092 EPOCH 219
2024-02-03 11:20:37,391 Epoch 219: Total Training Recognition Loss 0.33  Total Training Translation Loss 4.84 
2024-02-03 11:20:37,391 EPOCH 220
2024-02-03 11:20:50,580 Epoch 220: Total Training Recognition Loss 0.32  Total Training Translation Loss 4.90 
2024-02-03 11:20:50,580 EPOCH 221
2024-02-03 11:21:03,732 Epoch 221: Total Training Recognition Loss 0.32  Total Training Translation Loss 4.90 
2024-02-03 11:21:03,732 EPOCH 222
2024-02-03 11:21:16,906 Epoch 222: Total Training Recognition Loss 0.32  Total Training Translation Loss 4.65 
2024-02-03 11:21:16,906 EPOCH 223
2024-02-03 11:21:29,938 Epoch 223: Total Training Recognition Loss 0.33  Total Training Translation Loss 4.71 
2024-02-03 11:21:29,939 EPOCH 224
2024-02-03 11:21:37,073 [Epoch: 224 Step: 00003800] Batch Recognition Loss:   0.012844 => Gls Tokens per Sec:      772 || Batch Translation Loss:   0.213434 => Txt Tokens per Sec:     2130 || Lr: 0.000100
2024-02-03 11:21:43,116 Epoch 224: Total Training Recognition Loss 0.30  Total Training Translation Loss 4.65 
2024-02-03 11:21:43,117 EPOCH 225
2024-02-03 11:21:56,241 Epoch 225: Total Training Recognition Loss 0.32  Total Training Translation Loss 4.48 
2024-02-03 11:21:56,242 EPOCH 226
2024-02-03 11:22:09,130 Epoch 226: Total Training Recognition Loss 0.30  Total Training Translation Loss 4.39 
2024-02-03 11:22:09,130 EPOCH 227
2024-02-03 11:22:22,366 Epoch 227: Total Training Recognition Loss 0.29  Total Training Translation Loss 4.53 
2024-02-03 11:22:22,367 EPOCH 228
2024-02-03 11:22:35,846 Epoch 228: Total Training Recognition Loss 0.30  Total Training Translation Loss 4.47 
2024-02-03 11:22:35,846 EPOCH 229
2024-02-03 11:22:49,135 Epoch 229: Total Training Recognition Loss 0.27  Total Training Translation Loss 4.60 
2024-02-03 11:22:49,136 EPOCH 230
2024-02-03 11:22:54,898 [Epoch: 230 Step: 00003900] Batch Recognition Loss:   0.048615 => Gls Tokens per Sec:      778 || Batch Translation Loss:   0.130457 => Txt Tokens per Sec:     2088 || Lr: 0.000100
2024-02-03 11:23:02,456 Epoch 230: Total Training Recognition Loss 0.31  Total Training Translation Loss 4.24 
2024-02-03 11:23:02,457 EPOCH 231
2024-02-03 11:23:15,749 Epoch 231: Total Training Recognition Loss 0.27  Total Training Translation Loss 4.22 
2024-02-03 11:23:15,750 EPOCH 232
2024-02-03 11:23:28,790 Epoch 232: Total Training Recognition Loss 0.28  Total Training Translation Loss 4.34 
2024-02-03 11:23:28,791 EPOCH 233
2024-02-03 11:23:41,888 Epoch 233: Total Training Recognition Loss 0.30  Total Training Translation Loss 4.15 
2024-02-03 11:23:41,889 EPOCH 234
2024-02-03 11:23:55,041 Epoch 234: Total Training Recognition Loss 0.26  Total Training Translation Loss 4.07 
2024-02-03 11:23:55,042 EPOCH 235
2024-02-03 11:24:08,043 Epoch 235: Total Training Recognition Loss 0.26  Total Training Translation Loss 4.09 
2024-02-03 11:24:08,044 EPOCH 236
2024-02-03 11:24:09,164 [Epoch: 236 Step: 00004000] Batch Recognition Loss:   0.004277 => Gls Tokens per Sec:     2858 || Batch Translation Loss:   0.184896 => Txt Tokens per Sec:     7308 || Lr: 0.000100
2024-02-03 11:24:56,832 Validation result at epoch 236, step     4000: duration: 47.6667s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.84372	Translation Loss: 77641.47656	PPL: 3178.39844
	Eval Metric: BLEU
	WER 6.08	(DEL: 0.07,	INS: 0.00,	SUB: 6.01)
	BLEU-4 0.76	(BLEU-1: 11.89,	BLEU-2: 3.79,	BLEU-3: 1.57,	BLEU-4: 0.76)
	CHRF 17.52	ROUGE 9.98
2024-02-03 11:24:56,834 Logging Recognition and Translation Outputs
2024-02-03 11:24:56,834 ========================================================================================================================
2024-02-03 11:24:56,834 Logging Sequence: 52_208.00
2024-02-03 11:24:56,834 	Gloss Reference :	A B+C+D+E
2024-02-03 11:24:56,835 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 11:24:56,835 	Gloss Alignment :	         
2024-02-03 11:24:56,835 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 11:24:56,836 	Text Reference  :	**** ****** after   seeing    dhoni **** play within 3  hours      36  lakh   people   downloaded candy crush
2024-02-03 11:24:56,836 	Text Hypothesis :	when nitika jaiswal presented dhoni with the  tray   of chocolates the camera captured the        apple ipad 
2024-02-03 11:24:56,837 	Text Alignment  :	I    I      S       S               I    S    S      S  S          S   S      S        S          S     S    
2024-02-03 11:24:56,837 ========================================================================================================================
2024-02-03 11:24:56,837 Logging Sequence: 177_79.00
2024-02-03 11:24:56,837 	Gloss Reference :	A B+C+D+E
2024-02-03 11:24:56,837 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 11:24:56,837 	Gloss Alignment :	         
2024-02-03 11:24:56,838 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 11:24:56,839 	Text Reference  :	***** finally on         23rd   may both   sushil and ajay   were arrested in       delhi' mundka area    
2024-02-03 11:24:56,839 	Text Hypothesis :	delhi police  physically abused the arrest of     the police have tested   positive for    the    olympics
2024-02-03 11:24:56,839 	Text Alignment  :	I     S       S          S      S   S      S      S   S      S    S        S        S      S      S       
2024-02-03 11:24:56,839 ========================================================================================================================
2024-02-03 11:24:56,839 Logging Sequence: 107_94.00
2024-02-03 11:24:56,840 	Gloss Reference :	A B+C+D+E
2024-02-03 11:24:56,840 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 11:24:56,840 	Gloss Alignment :	         
2024-02-03 11:24:56,840 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 11:24:56,841 	Text Reference  :	and     currently development officer of  the bengal tennis association bta said
2024-02-03 11:24:56,841 	Text Hypothesis :	however new       zealand     had     won the ****** ****** *********** *** toss
2024-02-03 11:24:56,841 	Text Alignment  :	S       S         S           S       S       D      D      D           D   S   
2024-02-03 11:24:56,841 ========================================================================================================================
2024-02-03 11:24:56,842 Logging Sequence: 114_153.00
2024-02-03 11:24:56,842 	Gloss Reference :	A B+C+D+E
2024-02-03 11:24:56,842 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 11:24:56,842 	Gloss Alignment :	         
2024-02-03 11:24:56,842 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 11:24:56,843 	Text Reference  :	another big football news is   that the    copa america final       
2024-02-03 11:24:56,843 	Text Hypothesis :	******* *** but      had  lost to   france and  spain   respectively
2024-02-03 11:24:56,843 	Text Alignment  :	D       D   S        S    S    S    S      S    S       S           
2024-02-03 11:24:56,843 ========================================================================================================================
2024-02-03 11:24:56,843 Logging Sequence: 52_36.00
2024-02-03 11:24:56,844 	Gloss Reference :	A B+C+D+E    
2024-02-03 11:24:56,844 	Gloss Hypothesis:	A B+C+D+E+D+E
2024-02-03 11:24:56,844 	Gloss Alignment :	  S          
2024-02-03 11:24:56,844 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 11:24:56,845 	Text Reference  :	** **** ***** *** * **** ******* ** ********** ** recently dhoni was travellin on      an indigo flight 
2024-02-03 11:24:56,845 	Text Hypothesis :	he said there was a huge mistake of chocolates to the      game  was seen      october to the    stadium
2024-02-03 11:24:56,845 	Text Alignment  :	I  I    I     I   I I    I       I  I          I  S        S         S         S       S  S      S      
2024-02-03 11:24:56,845 ========================================================================================================================
2024-02-03 11:25:09,396 Epoch 236: Total Training Recognition Loss 0.25  Total Training Translation Loss 4.09 
2024-02-03 11:25:09,397 EPOCH 237
2024-02-03 11:25:22,640 Epoch 237: Total Training Recognition Loss 0.25  Total Training Translation Loss 4.03 
2024-02-03 11:25:22,641 EPOCH 238
2024-02-03 11:25:36,034 Epoch 238: Total Training Recognition Loss 0.26  Total Training Translation Loss 4.05 
2024-02-03 11:25:36,034 EPOCH 239
2024-02-03 11:25:49,169 Epoch 239: Total Training Recognition Loss 0.28  Total Training Translation Loss 4.04 
2024-02-03 11:25:49,169 EPOCH 240
2024-02-03 11:26:02,479 Epoch 240: Total Training Recognition Loss 0.26  Total Training Translation Loss 3.90 
2024-02-03 11:26:02,479 EPOCH 241
2024-02-03 11:26:15,713 Epoch 241: Total Training Recognition Loss 0.24  Total Training Translation Loss 4.09 
2024-02-03 11:26:15,713 EPOCH 242
2024-02-03 11:26:16,256 [Epoch: 242 Step: 00004100] Batch Recognition Loss:   0.013993 => Gls Tokens per Sec:     3542 || Batch Translation Loss:   0.215269 => Txt Tokens per Sec:     8609 || Lr: 0.000100
2024-02-03 11:26:28,914 Epoch 242: Total Training Recognition Loss 0.26  Total Training Translation Loss 4.14 
2024-02-03 11:26:28,914 EPOCH 243
2024-02-03 11:26:42,217 Epoch 243: Total Training Recognition Loss 0.24  Total Training Translation Loss 3.95 
2024-02-03 11:26:42,217 EPOCH 244
2024-02-03 11:26:55,513 Epoch 244: Total Training Recognition Loss 0.26  Total Training Translation Loss 3.90 
2024-02-03 11:26:55,514 EPOCH 245
2024-02-03 11:27:08,590 Epoch 245: Total Training Recognition Loss 0.24  Total Training Translation Loss 3.91 
2024-02-03 11:27:08,591 EPOCH 246
2024-02-03 11:27:21,713 Epoch 246: Total Training Recognition Loss 0.24  Total Training Translation Loss 3.85 
2024-02-03 11:27:21,713 EPOCH 247
2024-02-03 11:27:35,081 Epoch 247: Total Training Recognition Loss 0.24  Total Training Translation Loss 3.84 
2024-02-03 11:27:35,082 EPOCH 248
2024-02-03 11:27:36,712 [Epoch: 248 Step: 00004200] Batch Recognition Loss:   0.019187 => Gls Tokens per Sec:      393 || Batch Translation Loss:   0.292242 => Txt Tokens per Sec:     1349 || Lr: 0.000100
2024-02-03 11:27:48,465 Epoch 248: Total Training Recognition Loss 0.25  Total Training Translation Loss 3.77 
2024-02-03 11:27:48,465 EPOCH 249
2024-02-03 11:28:01,316 Epoch 249: Total Training Recognition Loss 0.22  Total Training Translation Loss 3.81 
2024-02-03 11:28:01,316 EPOCH 250
2024-02-03 11:28:14,838 Epoch 250: Total Training Recognition Loss 0.23  Total Training Translation Loss 4.12 
2024-02-03 11:28:14,838 EPOCH 251
2024-02-03 11:28:27,944 Epoch 251: Total Training Recognition Loss 0.24  Total Training Translation Loss 4.07 
2024-02-03 11:28:27,945 EPOCH 252
2024-02-03 11:28:40,724 Epoch 252: Total Training Recognition Loss 0.24  Total Training Translation Loss 4.13 
2024-02-03 11:28:40,725 EPOCH 253
2024-02-03 11:28:53,240 [Epoch: 253 Step: 00004300] Batch Recognition Loss:   0.007238 => Gls Tokens per Sec:      798 || Batch Translation Loss:   0.287876 => Txt Tokens per Sec:     2217 || Lr: 0.000100
2024-02-03 11:28:53,472 Epoch 253: Total Training Recognition Loss 0.25  Total Training Translation Loss 4.10 
2024-02-03 11:28:53,472 EPOCH 254
2024-02-03 11:29:06,963 Epoch 254: Total Training Recognition Loss 0.23  Total Training Translation Loss 3.86 
2024-02-03 11:29:06,964 EPOCH 255
2024-02-03 11:29:20,371 Epoch 255: Total Training Recognition Loss 0.21  Total Training Translation Loss 3.97 
2024-02-03 11:29:20,372 EPOCH 256
2024-02-03 11:29:33,661 Epoch 256: Total Training Recognition Loss 0.25  Total Training Translation Loss 4.15 
2024-02-03 11:29:33,662 EPOCH 257
2024-02-03 11:29:47,021 Epoch 257: Total Training Recognition Loss 0.22  Total Training Translation Loss 3.92 
2024-02-03 11:29:47,022 EPOCH 258
2024-02-03 11:30:00,200 Epoch 258: Total Training Recognition Loss 0.24  Total Training Translation Loss 3.99 
2024-02-03 11:30:00,201 EPOCH 259
2024-02-03 11:30:11,320 [Epoch: 259 Step: 00004400] Batch Recognition Loss:   0.028609 => Gls Tokens per Sec:      783 || Batch Translation Loss:   0.282766 => Txt Tokens per Sec:     2127 || Lr: 0.000100
2024-02-03 11:30:13,630 Epoch 259: Total Training Recognition Loss 0.24  Total Training Translation Loss 4.18 
2024-02-03 11:30:13,631 EPOCH 260
2024-02-03 11:30:26,774 Epoch 260: Total Training Recognition Loss 0.23  Total Training Translation Loss 4.35 
2024-02-03 11:30:26,774 EPOCH 261
2024-02-03 11:30:40,009 Epoch 261: Total Training Recognition Loss 0.23  Total Training Translation Loss 5.18 
2024-02-03 11:30:40,010 EPOCH 262
2024-02-03 11:30:53,347 Epoch 262: Total Training Recognition Loss 0.26  Total Training Translation Loss 5.39 
2024-02-03 11:30:53,347 EPOCH 263
2024-02-03 11:31:06,481 Epoch 263: Total Training Recognition Loss 0.27  Total Training Translation Loss 9.69 
2024-02-03 11:31:06,482 EPOCH 264
2024-02-03 11:31:20,204 Epoch 264: Total Training Recognition Loss 0.36  Total Training Translation Loss 12.20 
2024-02-03 11:31:20,205 EPOCH 265
2024-02-03 11:31:28,571 [Epoch: 265 Step: 00004500] Batch Recognition Loss:   0.019157 => Gls Tokens per Sec:      918 || Batch Translation Loss:   0.492157 => Txt Tokens per Sec:     2706 || Lr: 0.000100
2024-02-03 11:31:33,473 Epoch 265: Total Training Recognition Loss 0.56  Total Training Translation Loss 9.87 
2024-02-03 11:31:33,474 EPOCH 266
2024-02-03 11:31:46,628 Epoch 266: Total Training Recognition Loss 0.60  Total Training Translation Loss 8.17 
2024-02-03 11:31:46,629 EPOCH 267
2024-02-03 11:31:59,814 Epoch 267: Total Training Recognition Loss 0.38  Total Training Translation Loss 7.24 
2024-02-03 11:31:59,815 EPOCH 268
2024-02-03 11:32:13,004 Epoch 268: Total Training Recognition Loss 0.33  Total Training Translation Loss 7.98 
2024-02-03 11:32:13,004 EPOCH 269
2024-02-03 11:32:26,108 Epoch 269: Total Training Recognition Loss 0.39  Total Training Translation Loss 8.72 
2024-02-03 11:32:26,109 EPOCH 270
2024-02-03 11:32:39,257 Epoch 270: Total Training Recognition Loss 0.39  Total Training Translation Loss 13.97 
2024-02-03 11:32:39,258 EPOCH 271
2024-02-03 11:32:46,828 [Epoch: 271 Step: 00004600] Batch Recognition Loss:   0.027203 => Gls Tokens per Sec:      813 || Batch Translation Loss:   0.393742 => Txt Tokens per Sec:     2235 || Lr: 0.000100
2024-02-03 11:32:52,463 Epoch 271: Total Training Recognition Loss 0.53  Total Training Translation Loss 17.08 
2024-02-03 11:32:52,464 EPOCH 272
2024-02-03 11:33:05,698 Epoch 272: Total Training Recognition Loss 0.51  Total Training Translation Loss 22.29 
2024-02-03 11:33:05,698 EPOCH 273
2024-02-03 11:33:19,169 Epoch 273: Total Training Recognition Loss 0.47  Total Training Translation Loss 15.01 
2024-02-03 11:33:19,171 EPOCH 274
2024-02-03 11:33:32,525 Epoch 274: Total Training Recognition Loss 0.43  Total Training Translation Loss 6.97 
2024-02-03 11:33:32,526 EPOCH 275
2024-02-03 11:33:45,658 Epoch 275: Total Training Recognition Loss 0.33  Total Training Translation Loss 5.05 
2024-02-03 11:33:45,659 EPOCH 276
2024-02-03 11:33:58,990 Epoch 276: Total Training Recognition Loss 0.32  Total Training Translation Loss 4.24 
2024-02-03 11:33:58,991 EPOCH 277
2024-02-03 11:34:02,335 [Epoch: 277 Step: 00004700] Batch Recognition Loss:   0.013421 => Gls Tokens per Sec:     1532 || Batch Translation Loss:   0.143782 => Txt Tokens per Sec:     4214 || Lr: 0.000100
2024-02-03 11:34:12,501 Epoch 277: Total Training Recognition Loss 0.28  Total Training Translation Loss 3.75 
2024-02-03 11:34:12,502 EPOCH 278
2024-02-03 11:34:25,769 Epoch 278: Total Training Recognition Loss 0.24  Total Training Translation Loss 3.29 
2024-02-03 11:34:25,769 EPOCH 279
2024-02-03 11:34:39,087 Epoch 279: Total Training Recognition Loss 0.25  Total Training Translation Loss 3.06 
2024-02-03 11:34:39,088 EPOCH 280
2024-02-03 11:34:52,335 Epoch 280: Total Training Recognition Loss 0.23  Total Training Translation Loss 3.03 
2024-02-03 11:34:52,335 EPOCH 281
2024-02-03 11:35:05,464 Epoch 281: Total Training Recognition Loss 0.23  Total Training Translation Loss 2.89 
2024-02-03 11:35:05,465 EPOCH 282
2024-02-03 11:35:18,590 Epoch 282: Total Training Recognition Loss 0.21  Total Training Translation Loss 2.74 
2024-02-03 11:35:18,591 EPOCH 283
2024-02-03 11:35:19,733 [Epoch: 283 Step: 00004800] Batch Recognition Loss:   0.009472 => Gls Tokens per Sec:     3369 || Batch Translation Loss:   0.133153 => Txt Tokens per Sec:     8762 || Lr: 0.000100
2024-02-03 11:35:31,841 Epoch 283: Total Training Recognition Loss 0.20  Total Training Translation Loss 2.79 
2024-02-03 11:35:31,842 EPOCH 284
2024-02-03 11:35:45,564 Epoch 284: Total Training Recognition Loss 0.22  Total Training Translation Loss 2.58 
2024-02-03 11:35:45,565 EPOCH 285
2024-02-03 11:35:58,694 Epoch 285: Total Training Recognition Loss 0.21  Total Training Translation Loss 2.68 
2024-02-03 11:35:58,695 EPOCH 286
2024-02-03 11:36:12,106 Epoch 286: Total Training Recognition Loss 0.19  Total Training Translation Loss 2.67 
2024-02-03 11:36:12,107 EPOCH 287
2024-02-03 11:36:25,171 Epoch 287: Total Training Recognition Loss 0.18  Total Training Translation Loss 2.58 
2024-02-03 11:36:25,171 EPOCH 288
2024-02-03 11:36:38,075 Epoch 288: Total Training Recognition Loss 0.21  Total Training Translation Loss 2.39 
2024-02-03 11:36:38,076 EPOCH 289
2024-02-03 11:36:41,809 [Epoch: 289 Step: 00004900] Batch Recognition Loss:   0.003482 => Gls Tokens per Sec:      686 || Batch Translation Loss:   0.120725 => Txt Tokens per Sec:     2002 || Lr: 0.000100
2024-02-03 11:36:51,295 Epoch 289: Total Training Recognition Loss 0.18  Total Training Translation Loss 2.37 
2024-02-03 11:36:51,296 EPOCH 290
2024-02-03 11:37:04,662 Epoch 290: Total Training Recognition Loss 0.20  Total Training Translation Loss 2.50 
2024-02-03 11:37:04,664 EPOCH 291
2024-02-03 11:37:17,858 Epoch 291: Total Training Recognition Loss 0.21  Total Training Translation Loss 2.32 
2024-02-03 11:37:17,859 EPOCH 292
2024-02-03 11:37:31,142 Epoch 292: Total Training Recognition Loss 0.16  Total Training Translation Loss 2.40 
2024-02-03 11:37:31,142 EPOCH 293
2024-02-03 11:37:44,702 Epoch 293: Total Training Recognition Loss 0.17  Total Training Translation Loss 2.64 
2024-02-03 11:37:44,702 EPOCH 294
2024-02-03 11:37:57,962 Epoch 294: Total Training Recognition Loss 0.17  Total Training Translation Loss 2.46 
2024-02-03 11:37:57,962 EPOCH 295
2024-02-03 11:37:58,527 [Epoch: 295 Step: 00005000] Batch Recognition Loss:   0.010842 => Gls Tokens per Sec:     2274 || Batch Translation Loss:   0.139496 => Txt Tokens per Sec:     6805 || Lr: 0.000100
2024-02-03 11:38:11,039 Epoch 295: Total Training Recognition Loss 0.19  Total Training Translation Loss 2.23 
2024-02-03 11:38:11,040 EPOCH 296
2024-02-03 11:38:24,255 Epoch 296: Total Training Recognition Loss 0.16  Total Training Translation Loss 2.12 
2024-02-03 11:38:24,255 EPOCH 297
2024-02-03 11:38:37,384 Epoch 297: Total Training Recognition Loss 0.17  Total Training Translation Loss 2.23 
2024-02-03 11:38:37,385 EPOCH 298
2024-02-03 11:38:50,479 Epoch 298: Total Training Recognition Loss 0.16  Total Training Translation Loss 2.11 
2024-02-03 11:38:50,480 EPOCH 299
2024-02-03 11:39:03,725 Epoch 299: Total Training Recognition Loss 0.16  Total Training Translation Loss 2.06 
2024-02-03 11:39:03,725 EPOCH 300
2024-02-03 11:39:16,895 [Epoch: 300 Step: 00005100] Batch Recognition Loss:   0.004365 => Gls Tokens per Sec:      807 || Batch Translation Loss:   0.083944 => Txt Tokens per Sec:     2245 || Lr: 0.000100
2024-02-03 11:39:16,895 Epoch 300: Total Training Recognition Loss 0.16  Total Training Translation Loss 1.97 
2024-02-03 11:39:16,896 EPOCH 301
2024-02-03 11:39:30,014 Epoch 301: Total Training Recognition Loss 0.13  Total Training Translation Loss 1.96 
2024-02-03 11:39:30,015 EPOCH 302
2024-02-03 11:39:43,081 Epoch 302: Total Training Recognition Loss 0.15  Total Training Translation Loss 2.09 
2024-02-03 11:39:43,082 EPOCH 303
2024-02-03 11:39:56,638 Epoch 303: Total Training Recognition Loss 0.14  Total Training Translation Loss 2.07 
2024-02-03 11:39:56,639 EPOCH 304
2024-02-03 11:40:09,476 Epoch 304: Total Training Recognition Loss 0.11  Total Training Translation Loss 2.12 
2024-02-03 11:40:09,477 EPOCH 305
2024-02-03 11:40:22,749 Epoch 305: Total Training Recognition Loss 0.14  Total Training Translation Loss 2.04 
2024-02-03 11:40:22,750 EPOCH 306
2024-02-03 11:40:35,454 [Epoch: 306 Step: 00005200] Batch Recognition Loss:   0.006932 => Gls Tokens per Sec:      736 || Batch Translation Loss:   0.083158 => Txt Tokens per Sec:     2054 || Lr: 0.000100
2024-02-03 11:40:35,860 Epoch 306: Total Training Recognition Loss 0.13  Total Training Translation Loss 1.92 
2024-02-03 11:40:35,861 EPOCH 307
2024-02-03 11:40:48,975 Epoch 307: Total Training Recognition Loss 0.16  Total Training Translation Loss 1.84 
2024-02-03 11:40:48,976 EPOCH 308
2024-02-03 11:41:02,167 Epoch 308: Total Training Recognition Loss 0.14  Total Training Translation Loss 1.84 
2024-02-03 11:41:02,168 EPOCH 309
2024-02-03 11:41:15,473 Epoch 309: Total Training Recognition Loss 0.15  Total Training Translation Loss 1.93 
2024-02-03 11:41:15,473 EPOCH 310
2024-02-03 11:41:28,794 Epoch 310: Total Training Recognition Loss 0.15  Total Training Translation Loss 1.97 
2024-02-03 11:41:28,795 EPOCH 311
2024-02-03 11:41:42,029 Epoch 311: Total Training Recognition Loss 0.14  Total Training Translation Loss 1.90 
2024-02-03 11:41:42,029 EPOCH 312
2024-02-03 11:41:48,976 [Epoch: 312 Step: 00005300] Batch Recognition Loss:   0.008572 => Gls Tokens per Sec:     1198 || Batch Translation Loss:   0.106221 => Txt Tokens per Sec:     3274 || Lr: 0.000100
2024-02-03 11:41:55,271 Epoch 312: Total Training Recognition Loss 0.13  Total Training Translation Loss 1.82 
2024-02-03 11:41:55,272 EPOCH 313
2024-02-03 11:42:08,449 Epoch 313: Total Training Recognition Loss 0.13  Total Training Translation Loss 1.91 
2024-02-03 11:42:08,450 EPOCH 314
2024-02-03 11:42:21,614 Epoch 314: Total Training Recognition Loss 0.14  Total Training Translation Loss 2.19 
2024-02-03 11:42:21,614 EPOCH 315
2024-02-03 11:42:34,950 Epoch 315: Total Training Recognition Loss 0.18  Total Training Translation Loss 2.26 
2024-02-03 11:42:34,950 EPOCH 316
2024-02-03 11:42:48,388 Epoch 316: Total Training Recognition Loss 0.14  Total Training Translation Loss 2.41 
2024-02-03 11:42:48,388 EPOCH 317
2024-02-03 11:43:01,305 Epoch 317: Total Training Recognition Loss 0.17  Total Training Translation Loss 2.13 
2024-02-03 11:43:01,305 EPOCH 318
2024-02-03 11:43:11,919 [Epoch: 318 Step: 00005400] Batch Recognition Loss:   0.016992 => Gls Tokens per Sec:      640 || Batch Translation Loss:   0.133294 => Txt Tokens per Sec:     1764 || Lr: 0.000100
2024-02-03 11:43:14,972 Epoch 318: Total Training Recognition Loss 0.15  Total Training Translation Loss 2.29 
2024-02-03 11:43:14,972 EPOCH 319
2024-02-03 11:43:27,902 Epoch 319: Total Training Recognition Loss 0.13  Total Training Translation Loss 2.11 
2024-02-03 11:43:27,903 EPOCH 320
2024-02-03 11:43:41,153 Epoch 320: Total Training Recognition Loss 0.12  Total Training Translation Loss 2.27 
2024-02-03 11:43:41,154 EPOCH 321
2024-02-03 11:43:54,210 Epoch 321: Total Training Recognition Loss 0.12  Total Training Translation Loss 2.14 
2024-02-03 11:43:54,210 EPOCH 322
2024-02-03 11:44:07,574 Epoch 322: Total Training Recognition Loss 0.12  Total Training Translation Loss 2.03 
2024-02-03 11:44:07,575 EPOCH 323
2024-02-03 11:44:20,784 Epoch 323: Total Training Recognition Loss 0.11  Total Training Translation Loss 2.10 
2024-02-03 11:44:20,785 EPOCH 324
2024-02-03 11:44:24,176 [Epoch: 324 Step: 00005500] Batch Recognition Loss:   0.002702 => Gls Tokens per Sec:     1699 || Batch Translation Loss:   0.137350 => Txt Tokens per Sec:     4513 || Lr: 0.000100
2024-02-03 11:44:34,030 Epoch 324: Total Training Recognition Loss 0.11  Total Training Translation Loss 2.64 
2024-02-03 11:44:34,031 EPOCH 325
2024-02-03 11:44:47,450 Epoch 325: Total Training Recognition Loss 0.10  Total Training Translation Loss 2.84 
2024-02-03 11:44:47,451 EPOCH 326
2024-02-03 11:45:00,539 Epoch 326: Total Training Recognition Loss 0.15  Total Training Translation Loss 3.56 
2024-02-03 11:45:00,540 EPOCH 327
2024-02-03 11:45:13,688 Epoch 327: Total Training Recognition Loss 0.15  Total Training Translation Loss 4.87 
2024-02-03 11:45:13,689 EPOCH 328
2024-02-03 11:45:27,163 Epoch 328: Total Training Recognition Loss 0.18  Total Training Translation Loss 6.50 
2024-02-03 11:45:27,164 EPOCH 329
2024-02-03 11:45:40,417 Epoch 329: Total Training Recognition Loss 0.20  Total Training Translation Loss 6.98 
2024-02-03 11:45:40,418 EPOCH 330
2024-02-03 11:45:43,424 [Epoch: 330 Step: 00005600] Batch Recognition Loss:   0.012186 => Gls Tokens per Sec:     1491 || Batch Translation Loss:   0.447448 => Txt Tokens per Sec:     4305 || Lr: 0.000100
2024-02-03 11:45:53,592 Epoch 330: Total Training Recognition Loss 0.25  Total Training Translation Loss 6.39 
2024-02-03 11:45:53,592 EPOCH 331
2024-02-03 11:46:06,985 Epoch 331: Total Training Recognition Loss 0.23  Total Training Translation Loss 5.46 
2024-02-03 11:46:06,986 EPOCH 332
2024-02-03 11:46:20,503 Epoch 332: Total Training Recognition Loss 0.19  Total Training Translation Loss 4.31 
2024-02-03 11:46:20,504 EPOCH 333
2024-02-03 11:46:33,938 Epoch 333: Total Training Recognition Loss 0.19  Total Training Translation Loss 3.97 
2024-02-03 11:46:33,938 EPOCH 334
2024-02-03 11:46:47,378 Epoch 334: Total Training Recognition Loss 0.16  Total Training Translation Loss 3.51 
2024-02-03 11:46:47,378 EPOCH 335
2024-02-03 11:47:00,355 Epoch 335: Total Training Recognition Loss 0.16  Total Training Translation Loss 3.27 
2024-02-03 11:47:00,355 EPOCH 336
2024-02-03 11:47:05,591 [Epoch: 336 Step: 00005700] Batch Recognition Loss:   0.006986 => Gls Tokens per Sec:      611 || Batch Translation Loss:   0.151328 => Txt Tokens per Sec:     1838 || Lr: 0.000100
2024-02-03 11:47:13,505 Epoch 336: Total Training Recognition Loss 0.14  Total Training Translation Loss 2.56 
2024-02-03 11:47:13,505 EPOCH 337
2024-02-03 11:47:26,353 Epoch 337: Total Training Recognition Loss 0.13  Total Training Translation Loss 2.06 
2024-02-03 11:47:26,353 EPOCH 338
2024-02-03 11:47:39,522 Epoch 338: Total Training Recognition Loss 0.12  Total Training Translation Loss 2.02 
2024-02-03 11:47:39,522 EPOCH 339
2024-02-03 11:47:52,871 Epoch 339: Total Training Recognition Loss 0.13  Total Training Translation Loss 2.00 
2024-02-03 11:47:52,872 EPOCH 340
2024-02-03 11:48:06,004 Epoch 340: Total Training Recognition Loss 0.13  Total Training Translation Loss 2.12 
2024-02-03 11:48:06,004 EPOCH 341
2024-02-03 11:48:19,235 Epoch 341: Total Training Recognition Loss 0.13  Total Training Translation Loss 2.14 
2024-02-03 11:48:19,236 EPOCH 342
2024-02-03 11:48:23,587 [Epoch: 342 Step: 00005800] Batch Recognition Loss:   0.018565 => Gls Tokens per Sec:      384 || Batch Translation Loss:   0.162966 => Txt Tokens per Sec:      896 || Lr: 0.000100
2024-02-03 11:48:32,430 Epoch 342: Total Training Recognition Loss 0.13  Total Training Translation Loss 2.30 
2024-02-03 11:48:32,430 EPOCH 343
2024-02-03 11:48:45,771 Epoch 343: Total Training Recognition Loss 0.11  Total Training Translation Loss 2.86 
2024-02-03 11:48:45,772 EPOCH 344
2024-02-03 11:48:59,019 Epoch 344: Total Training Recognition Loss 0.12  Total Training Translation Loss 2.69 
2024-02-03 11:48:59,020 EPOCH 345
2024-02-03 11:49:12,030 Epoch 345: Total Training Recognition Loss 0.15  Total Training Translation Loss 2.26 
2024-02-03 11:49:12,030 EPOCH 346
2024-02-03 11:49:25,516 Epoch 346: Total Training Recognition Loss 0.13  Total Training Translation Loss 2.41 
2024-02-03 11:49:25,517 EPOCH 347
2024-02-03 11:49:38,708 Epoch 347: Total Training Recognition Loss 0.15  Total Training Translation Loss 2.11 
2024-02-03 11:49:38,709 EPOCH 348
2024-02-03 11:49:40,357 [Epoch: 348 Step: 00005900] Batch Recognition Loss:   0.007477 => Gls Tokens per Sec:      388 || Batch Translation Loss:   0.175028 => Txt Tokens per Sec:     1360 || Lr: 0.000100
2024-02-03 11:49:51,987 Epoch 348: Total Training Recognition Loss 0.12  Total Training Translation Loss 2.24 
2024-02-03 11:49:51,988 EPOCH 349
2024-02-03 11:50:05,200 Epoch 349: Total Training Recognition Loss 0.12  Total Training Translation Loss 2.04 
2024-02-03 11:50:05,200 EPOCH 350
2024-02-03 11:50:18,297 Epoch 350: Total Training Recognition Loss 0.11  Total Training Translation Loss 2.00 
2024-02-03 11:50:18,298 EPOCH 351
2024-02-03 11:50:31,722 Epoch 351: Total Training Recognition Loss 0.12  Total Training Translation Loss 1.83 
2024-02-03 11:50:31,722 EPOCH 352
2024-02-03 11:50:45,020 Epoch 352: Total Training Recognition Loss 0.11  Total Training Translation Loss 2.10 
2024-02-03 11:50:45,020 EPOCH 353
2024-02-03 11:50:56,346 [Epoch: 353 Step: 00006000] Batch Recognition Loss:   0.008074 => Gls Tokens per Sec:      882 || Batch Translation Loss:   0.079266 => Txt Tokens per Sec:     2413 || Lr: 0.000100
2024-02-03 11:51:44,727 Validation result at epoch 353, step     6000: duration: 48.3801s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.87595	Translation Loss: 83169.55469	PPL: 5643.72705
	Eval Metric: BLEU
	WER 4.81	(DEL: 0.00,	INS: 0.00,	SUB: 4.81)
	BLEU-4 0.66	(BLEU-1: 11.91,	BLEU-2: 3.83,	BLEU-3: 1.49,	BLEU-4: 0.66)
	CHRF 18.06	ROUGE 10.35
2024-02-03 11:51:44,729 Logging Recognition and Translation Outputs
2024-02-03 11:51:44,730 ========================================================================================================================
2024-02-03 11:51:44,730 Logging Sequence: 101_39.00
2024-02-03 11:51:44,730 	Gloss Reference :	A B+C+D+E
2024-02-03 11:51:44,731 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 11:51:44,731 	Gloss Alignment :	         
2024-02-03 11:51:44,731 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 11:51:44,733 	Text Reference  :	*** ***** star batsmen and  bowlers of      the indian   team   were  infected by the coronavirus
2024-02-03 11:51:44,733 	Text Hypothesis :	the world cup  2022    held in      england and pakistan played about it       is not known      
2024-02-03 11:51:44,733 	Text Alignment  :	I   I     S    S       S    S       S       S   S        S      S     S        S  S   S          
2024-02-03 11:51:44,733 ========================================================================================================================
2024-02-03 11:51:44,734 Logging Sequence: 105_139.00
2024-02-03 11:51:44,734 	Gloss Reference :	A B+C+D+E
2024-02-03 11:51:44,734 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 11:51:44,734 	Gloss Alignment :	         
2024-02-03 11:51:44,734 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 11:51:44,736 	Text Reference  :	**** ***** **** *** ** ***** **** and now  he has finally achieved his     dream by  defeating carlsen
2024-02-03 11:51:44,736 	Text Hypothesis :	they don't want him to train hard and that he *** was     just     because of    him and       issues 
2024-02-03 11:51:44,736 	Text Alignment  :	I    I     I    I   I  I     I        S       D   S       S        S       S     S   S         S      
2024-02-03 11:51:44,736 ========================================================================================================================
2024-02-03 11:51:44,736 Logging Sequence: 85_17.00
2024-02-03 11:51:44,737 	Gloss Reference :	A B+C+D+E
2024-02-03 11:51:44,737 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 11:51:44,737 	Gloss Alignment :	         
2024-02-03 11:51:44,737 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 11:51:44,738 	Text Reference  :	*** **** ****** *** ******* in      the 2003   world cup symonds scored an      unbeaten 143  against pakistan
2024-02-03 11:51:44,738 	Text Hypothesis :	the fast bowler pat cummins donated the golden glove for the     indian wickets in       just 111     match   
2024-02-03 11:51:44,739 	Text Alignment  :	I   I    I      I   I       S           S      S     S   S       S      S       S        S    S       S       
2024-02-03 11:51:44,739 ========================================================================================================================
2024-02-03 11:51:44,739 Logging Sequence: 150_98.00
2024-02-03 11:51:44,739 	Gloss Reference :	A B+C+D+E
2024-02-03 11:51:44,739 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 11:51:44,739 	Gloss Alignment :	         
2024-02-03 11:51:44,740 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 11:51:44,740 	Text Reference  :	chhetri was ***** ***** ****** the     captain
2024-02-03 11:51:44,740 	Text Hypothesis :	what    was dhoni being played against csk    
2024-02-03 11:51:44,740 	Text Alignment  :	S           I     I     I      S       S      
2024-02-03 11:51:44,741 ========================================================================================================================
2024-02-03 11:51:44,741 Logging Sequence: 147_76.00
2024-02-03 11:51:44,741 	Gloss Reference :	A B+C+D+E
2024-02-03 11:51:44,742 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 11:51:44,742 	Gloss Alignment :	         
2024-02-03 11:51:44,742 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 11:51:44,744 	Text Reference  :	****** ****** *** **** **** ******** *** ****** however on    28 july she     announced her withdrawal from the olympic games     
2024-02-03 11:51:44,744 	Text Hypothesis :	zaheer always has some were watching the reason however there is no   clarity on        the post       on   the ******* tournament
2024-02-03 11:51:44,744 	Text Alignment  :	I      I      I   I    I    I        I   I              S     S  S    S       S         S   S          S        D       S         
2024-02-03 11:51:44,745 ========================================================================================================================
2024-02-03 11:51:46,454 Epoch 353: Total Training Recognition Loss 0.10  Total Training Translation Loss 2.35 
2024-02-03 11:51:46,454 EPOCH 354
2024-02-03 11:52:00,118 Epoch 354: Total Training Recognition Loss 0.13  Total Training Translation Loss 2.44 
2024-02-03 11:52:00,119 EPOCH 355
2024-02-03 11:52:13,357 Epoch 355: Total Training Recognition Loss 0.13  Total Training Translation Loss 2.46 
2024-02-03 11:52:13,357 EPOCH 356
2024-02-03 11:52:26,309 Epoch 356: Total Training Recognition Loss 0.13  Total Training Translation Loss 2.08 
2024-02-03 11:52:26,309 EPOCH 357
2024-02-03 11:52:39,657 Epoch 357: Total Training Recognition Loss 0.11  Total Training Translation Loss 1.88 
2024-02-03 11:52:39,657 EPOCH 358
2024-02-03 11:52:52,788 Epoch 358: Total Training Recognition Loss 0.11  Total Training Translation Loss 2.00 
2024-02-03 11:52:52,789 EPOCH 359
2024-02-03 11:53:05,287 [Epoch: 359 Step: 00006100] Batch Recognition Loss:   0.008349 => Gls Tokens per Sec:      697 || Batch Translation Loss:   0.101535 => Txt Tokens per Sec:     1961 || Lr: 0.000100
2024-02-03 11:53:05,928 Epoch 359: Total Training Recognition Loss 0.12  Total Training Translation Loss 1.97 
2024-02-03 11:53:05,929 EPOCH 360
2024-02-03 11:53:18,936 Epoch 360: Total Training Recognition Loss 0.12  Total Training Translation Loss 2.01 
2024-02-03 11:53:18,937 EPOCH 361
2024-02-03 11:53:32,143 Epoch 361: Total Training Recognition Loss 0.09  Total Training Translation Loss 1.88 
2024-02-03 11:53:32,144 EPOCH 362
2024-02-03 11:53:44,913 Epoch 362: Total Training Recognition Loss 0.10  Total Training Translation Loss 1.97 
2024-02-03 11:53:44,914 EPOCH 363
2024-02-03 11:53:59,783 Epoch 363: Total Training Recognition Loss 0.10  Total Training Translation Loss 1.98 
2024-02-03 11:53:59,784 EPOCH 364
2024-02-03 11:54:14,731 Epoch 364: Total Training Recognition Loss 0.10  Total Training Translation Loss 1.92 
2024-02-03 11:54:14,732 EPOCH 365
2024-02-03 11:54:22,749 [Epoch: 365 Step: 00006200] Batch Recognition Loss:   0.008313 => Gls Tokens per Sec:      958 || Batch Translation Loss:   0.058971 => Txt Tokens per Sec:     2629 || Lr: 0.000100
2024-02-03 11:54:28,068 Epoch 365: Total Training Recognition Loss 0.11  Total Training Translation Loss 2.02 
2024-02-03 11:54:28,069 EPOCH 366
2024-02-03 11:54:41,039 Epoch 366: Total Training Recognition Loss 0.10  Total Training Translation Loss 2.13 
2024-02-03 11:54:41,039 EPOCH 367
2024-02-03 11:54:54,471 Epoch 367: Total Training Recognition Loss 0.12  Total Training Translation Loss 1.94 
2024-02-03 11:54:54,471 EPOCH 368
2024-02-03 11:55:08,266 Epoch 368: Total Training Recognition Loss 0.11  Total Training Translation Loss 2.33 
2024-02-03 11:55:08,267 EPOCH 369
2024-02-03 11:55:21,673 Epoch 369: Total Training Recognition Loss 0.11  Total Training Translation Loss 2.42 
2024-02-03 11:55:21,675 EPOCH 370
2024-02-03 11:55:34,775 Epoch 370: Total Training Recognition Loss 0.12  Total Training Translation Loss 2.34 
2024-02-03 11:55:34,776 EPOCH 371
2024-02-03 11:55:39,702 [Epoch: 371 Step: 00006300] Batch Recognition Loss:   0.003851 => Gls Tokens per Sec:     1300 || Batch Translation Loss:   0.268732 => Txt Tokens per Sec:     3526 || Lr: 0.000100
2024-02-03 11:55:47,886 Epoch 371: Total Training Recognition Loss 0.11  Total Training Translation Loss 2.67 
2024-02-03 11:55:47,887 EPOCH 372
2024-02-03 11:56:01,438 Epoch 372: Total Training Recognition Loss 0.15  Total Training Translation Loss 3.62 
2024-02-03 11:56:01,439 EPOCH 373
2024-02-03 11:56:14,598 Epoch 373: Total Training Recognition Loss 0.23  Total Training Translation Loss 17.77 
2024-02-03 11:56:14,598 EPOCH 374
2024-02-03 11:56:27,418 Epoch 374: Total Training Recognition Loss 0.49  Total Training Translation Loss 23.43 
2024-02-03 11:56:27,418 EPOCH 375
2024-02-03 11:56:40,635 Epoch 375: Total Training Recognition Loss 0.55  Total Training Translation Loss 16.48 
2024-02-03 11:56:40,636 EPOCH 376
2024-02-03 11:56:53,890 Epoch 376: Total Training Recognition Loss 0.54  Total Training Translation Loss 11.89 
2024-02-03 11:56:53,891 EPOCH 377
2024-02-03 11:56:57,131 [Epoch: 377 Step: 00006400] Batch Recognition Loss:   0.009128 => Gls Tokens per Sec:     1581 || Batch Translation Loss:   0.328887 => Txt Tokens per Sec:     4460 || Lr: 0.000100
2024-02-03 11:57:07,116 Epoch 377: Total Training Recognition Loss 0.26  Total Training Translation Loss 7.23 
2024-02-03 11:57:07,116 EPOCH 378
2024-02-03 11:57:20,375 Epoch 378: Total Training Recognition Loss 0.26  Total Training Translation Loss 3.93 
2024-02-03 11:57:20,376 EPOCH 379
2024-02-03 11:57:33,387 Epoch 379: Total Training Recognition Loss 0.19  Total Training Translation Loss 2.46 
2024-02-03 11:57:33,387 EPOCH 380
2024-02-03 11:57:46,537 Epoch 380: Total Training Recognition Loss 0.16  Total Training Translation Loss 2.12 
2024-02-03 11:57:46,538 EPOCH 381
2024-02-03 11:57:59,762 Epoch 381: Total Training Recognition Loss 0.17  Total Training Translation Loss 1.88 
2024-02-03 11:57:59,763 EPOCH 382
2024-02-03 11:58:12,882 Epoch 382: Total Training Recognition Loss 0.15  Total Training Translation Loss 1.69 
2024-02-03 11:58:12,882 EPOCH 383
2024-02-03 11:58:14,065 [Epoch: 383 Step: 00006500] Batch Recognition Loss:   0.004912 => Gls Tokens per Sec:     3251 || Batch Translation Loss:   0.085066 => Txt Tokens per Sec:     8897 || Lr: 0.000100
2024-02-03 11:58:25,689 Epoch 383: Total Training Recognition Loss 0.13  Total Training Translation Loss 1.51 
2024-02-03 11:58:25,691 EPOCH 384
2024-02-03 11:58:39,125 Epoch 384: Total Training Recognition Loss 0.13  Total Training Translation Loss 1.43 
2024-02-03 11:58:39,126 EPOCH 385
2024-02-03 11:58:52,572 Epoch 385: Total Training Recognition Loss 0.13  Total Training Translation Loss 1.30 
2024-02-03 11:58:52,572 EPOCH 386
2024-02-03 11:59:05,704 Epoch 386: Total Training Recognition Loss 0.11  Total Training Translation Loss 1.13 
2024-02-03 11:59:05,704 EPOCH 387
2024-02-03 11:59:18,954 Epoch 387: Total Training Recognition Loss 0.11  Total Training Translation Loss 1.22 
2024-02-03 11:59:18,955 EPOCH 388
2024-02-03 11:59:32,046 Epoch 388: Total Training Recognition Loss 0.10  Total Training Translation Loss 1.26 
2024-02-03 11:59:32,048 EPOCH 389
2024-02-03 11:59:34,363 [Epoch: 389 Step: 00006600] Batch Recognition Loss:   0.001574 => Gls Tokens per Sec:     1107 || Batch Translation Loss:   0.041273 => Txt Tokens per Sec:     3187 || Lr: 0.000100
2024-02-03 11:59:45,153 Epoch 389: Total Training Recognition Loss 0.10  Total Training Translation Loss 1.13 
2024-02-03 11:59:45,154 EPOCH 390
2024-02-03 11:59:58,234 Epoch 390: Total Training Recognition Loss 0.10  Total Training Translation Loss 1.21 
2024-02-03 11:59:58,235 EPOCH 391
2024-02-03 12:00:11,041 Epoch 391: Total Training Recognition Loss 0.11  Total Training Translation Loss 1.13 
2024-02-03 12:00:11,042 EPOCH 392
2024-02-03 12:00:24,326 Epoch 392: Total Training Recognition Loss 0.10  Total Training Translation Loss 1.07 
2024-02-03 12:00:24,327 EPOCH 393
2024-02-03 12:00:37,678 Epoch 393: Total Training Recognition Loss 0.10  Total Training Translation Loss 1.06 
2024-02-03 12:00:37,679 EPOCH 394
2024-02-03 12:00:50,573 Epoch 394: Total Training Recognition Loss 0.11  Total Training Translation Loss 1.17 
2024-02-03 12:00:50,573 EPOCH 395
2024-02-03 12:00:50,949 [Epoch: 395 Step: 00006700] Batch Recognition Loss:   0.005447 => Gls Tokens per Sec:     3422 || Batch Translation Loss:   0.045164 => Txt Tokens per Sec:     8455 || Lr: 0.000100
2024-02-03 12:01:03,993 Epoch 395: Total Training Recognition Loss 0.11  Total Training Translation Loss 0.99 
2024-02-03 12:01:03,995 EPOCH 396
2024-02-03 12:01:17,625 Epoch 396: Total Training Recognition Loss 0.10  Total Training Translation Loss 1.06 
2024-02-03 12:01:17,626 EPOCH 397
2024-02-03 12:01:30,691 Epoch 397: Total Training Recognition Loss 0.09  Total Training Translation Loss 1.04 
2024-02-03 12:01:30,691 EPOCH 398
2024-02-03 12:01:44,182 Epoch 398: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.13 
2024-02-03 12:01:44,182 EPOCH 399
2024-02-03 12:01:57,191 Epoch 399: Total Training Recognition Loss 0.07  Total Training Translation Loss 0.96 
2024-02-03 12:01:57,192 EPOCH 400
2024-02-03 12:02:10,664 [Epoch: 400 Step: 00006800] Batch Recognition Loss:   0.001635 => Gls Tokens per Sec:      789 || Batch Translation Loss:   0.050675 => Txt Tokens per Sec:     2194 || Lr: 0.000100
2024-02-03 12:02:10,665 Epoch 400: Total Training Recognition Loss 0.08  Total Training Translation Loss 0.96 
2024-02-03 12:02:10,665 EPOCH 401
2024-02-03 12:02:23,608 Epoch 401: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.05 
2024-02-03 12:02:23,609 EPOCH 402
2024-02-03 12:02:36,991 Epoch 402: Total Training Recognition Loss 0.08  Total Training Translation Loss 0.96 
2024-02-03 12:02:36,992 EPOCH 403
2024-02-03 12:02:50,415 Epoch 403: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.07 
2024-02-03 12:02:50,416 EPOCH 404
2024-02-03 12:03:03,645 Epoch 404: Total Training Recognition Loss 0.08  Total Training Translation Loss 0.92 
2024-02-03 12:03:03,645 EPOCH 405
2024-02-03 12:03:17,145 Epoch 405: Total Training Recognition Loss 0.08  Total Training Translation Loss 0.97 
2024-02-03 12:03:17,145 EPOCH 406
2024-02-03 12:03:29,960 [Epoch: 406 Step: 00006900] Batch Recognition Loss:   0.001438 => Gls Tokens per Sec:      730 || Batch Translation Loss:   0.100545 => Txt Tokens per Sec:     2010 || Lr: 0.000100
2024-02-03 12:03:30,648 Epoch 406: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.08 
2024-02-03 12:03:30,648 EPOCH 407
2024-02-03 12:03:44,064 Epoch 407: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.14 
2024-02-03 12:03:44,065 EPOCH 408
2024-02-03 12:03:57,603 Epoch 408: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.25 
2024-02-03 12:03:57,604 EPOCH 409
2024-02-03 12:04:10,757 Epoch 409: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.22 
2024-02-03 12:04:10,758 EPOCH 410
2024-02-03 12:04:24,153 Epoch 410: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.05 
2024-02-03 12:04:24,153 EPOCH 411
2024-02-03 12:04:37,673 Epoch 411: Total Training Recognition Loss 0.07  Total Training Translation Loss 0.98 
2024-02-03 12:04:37,674 EPOCH 412
2024-02-03 12:04:48,797 [Epoch: 412 Step: 00007000] Batch Recognition Loss:   0.009268 => Gls Tokens per Sec:      726 || Batch Translation Loss:   0.026058 => Txt Tokens per Sec:     2033 || Lr: 0.000100
2024-02-03 12:04:50,963 Epoch 412: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.00 
2024-02-03 12:04:50,963 EPOCH 413
2024-02-03 12:05:04,270 Epoch 413: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.05 
2024-02-03 12:05:04,270 EPOCH 414
2024-02-03 12:05:17,441 Epoch 414: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.15 
2024-02-03 12:05:17,442 EPOCH 415
2024-02-03 12:05:30,615 Epoch 415: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.15 
2024-02-03 12:05:30,615 EPOCH 416
2024-02-03 12:05:43,530 Epoch 416: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.24 
2024-02-03 12:05:43,531 EPOCH 417
2024-02-03 12:05:56,664 Epoch 417: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.39 
2024-02-03 12:05:56,664 EPOCH 418
2024-02-03 12:06:05,793 [Epoch: 418 Step: 00007100] Batch Recognition Loss:   0.004821 => Gls Tokens per Sec:      744 || Batch Translation Loss:   0.092375 => Txt Tokens per Sec:     2045 || Lr: 0.000100
2024-02-03 12:06:09,957 Epoch 418: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.11 
2024-02-03 12:06:09,957 EPOCH 419
2024-02-03 12:06:22,961 Epoch 419: Total Training Recognition Loss 0.08  Total Training Translation Loss 0.94 
2024-02-03 12:06:22,962 EPOCH 420
2024-02-03 12:06:36,173 Epoch 420: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.22 
2024-02-03 12:06:36,174 EPOCH 421
2024-02-03 12:06:48,621 Epoch 421: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.11 
2024-02-03 12:06:48,623 EPOCH 422
2024-02-03 12:07:02,057 Epoch 422: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.12 
2024-02-03 12:07:02,058 EPOCH 423
2024-02-03 12:07:15,225 Epoch 423: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.37 
2024-02-03 12:07:15,225 EPOCH 424
2024-02-03 12:07:18,365 [Epoch: 424 Step: 00007200] Batch Recognition Loss:   0.002099 => Gls Tokens per Sec:     1835 || Batch Translation Loss:   0.059323 => Txt Tokens per Sec:     5097 || Lr: 0.000100
2024-02-03 12:07:28,456 Epoch 424: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.61 
2024-02-03 12:07:28,457 EPOCH 425
2024-02-03 12:07:41,927 Epoch 425: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.16 
2024-02-03 12:07:41,929 EPOCH 426
2024-02-03 12:07:55,071 Epoch 426: Total Training Recognition Loss 0.09  Total Training Translation Loss 1.21 
2024-02-03 12:07:55,072 EPOCH 427
2024-02-03 12:08:08,247 Epoch 427: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.46 
2024-02-03 12:08:08,248 EPOCH 428
2024-02-03 12:08:21,524 Epoch 428: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.58 
2024-02-03 12:08:21,524 EPOCH 429
2024-02-03 12:08:34,483 Epoch 429: Total Training Recognition Loss 0.10  Total Training Translation Loss 1.95 
2024-02-03 12:08:34,484 EPOCH 430
2024-02-03 12:08:41,345 [Epoch: 430 Step: 00007300] Batch Recognition Loss:   0.004841 => Gls Tokens per Sec:      617 || Batch Translation Loss:   0.180151 => Txt Tokens per Sec:     1722 || Lr: 0.000100
2024-02-03 12:08:47,747 Epoch 430: Total Training Recognition Loss 0.10  Total Training Translation Loss 2.15 
2024-02-03 12:08:47,747 EPOCH 431
2024-02-03 12:09:03,321 Epoch 431: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.86 
2024-02-03 12:09:03,322 EPOCH 432
2024-02-03 12:09:16,581 Epoch 432: Total Training Recognition Loss 0.09  Total Training Translation Loss 2.69 
2024-02-03 12:09:16,582 EPOCH 433
2024-02-03 12:09:29,784 Epoch 433: Total Training Recognition Loss 0.09  Total Training Translation Loss 2.40 
2024-02-03 12:09:29,785 EPOCH 434
2024-02-03 12:09:43,023 Epoch 434: Total Training Recognition Loss 0.10  Total Training Translation Loss 1.97 
2024-02-03 12:09:43,024 EPOCH 435
2024-02-03 12:09:56,266 Epoch 435: Total Training Recognition Loss 0.09  Total Training Translation Loss 1.76 
2024-02-03 12:09:56,267 EPOCH 436
2024-02-03 12:09:57,167 [Epoch: 436 Step: 00007400] Batch Recognition Loss:   0.001990 => Gls Tokens per Sec:     3563 || Batch Translation Loss:   0.073717 => Txt Tokens per Sec:     8541 || Lr: 0.000100
2024-02-03 12:10:09,506 Epoch 436: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.87 
2024-02-03 12:10:09,506 EPOCH 437
2024-02-03 12:10:22,914 Epoch 437: Total Training Recognition Loss 0.09  Total Training Translation Loss 1.82 
2024-02-03 12:10:22,915 EPOCH 438
2024-02-03 12:10:35,734 Epoch 438: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.53 
2024-02-03 12:10:35,735 EPOCH 439
2024-02-03 12:10:49,351 Epoch 439: Total Training Recognition Loss 0.09  Total Training Translation Loss 1.83 
2024-02-03 12:10:49,352 EPOCH 440
2024-02-03 12:11:02,838 Epoch 440: Total Training Recognition Loss 0.10  Total Training Translation Loss 1.60 
2024-02-03 12:11:02,839 EPOCH 441
2024-02-03 12:11:16,034 Epoch 441: Total Training Recognition Loss 0.11  Total Training Translation Loss 1.95 
2024-02-03 12:11:16,035 EPOCH 442
2024-02-03 12:11:20,603 [Epoch: 442 Step: 00007500] Batch Recognition Loss:   0.010429 => Gls Tokens per Sec:      366 || Batch Translation Loss:   0.094492 => Txt Tokens per Sec:     1125 || Lr: 0.000100
2024-02-03 12:11:29,222 Epoch 442: Total Training Recognition Loss 0.09  Total Training Translation Loss 1.63 
2024-02-03 12:11:29,223 EPOCH 443
2024-02-03 12:11:42,106 Epoch 443: Total Training Recognition Loss 0.10  Total Training Translation Loss 1.73 
2024-02-03 12:11:42,107 EPOCH 444
2024-02-03 12:11:55,679 Epoch 444: Total Training Recognition Loss 0.09  Total Training Translation Loss 1.89 
2024-02-03 12:11:55,679 EPOCH 445
2024-02-03 12:12:08,947 Epoch 445: Total Training Recognition Loss 0.11  Total Training Translation Loss 1.92 
2024-02-03 12:12:08,947 EPOCH 446
2024-02-03 12:12:22,111 Epoch 446: Total Training Recognition Loss 0.09  Total Training Translation Loss 2.18 
2024-02-03 12:12:22,112 EPOCH 447
2024-02-03 12:12:35,427 Epoch 447: Total Training Recognition Loss 0.10  Total Training Translation Loss 2.37 
2024-02-03 12:12:35,427 EPOCH 448
2024-02-03 12:12:35,663 [Epoch: 448 Step: 00007600] Batch Recognition Loss:   0.003928 => Gls Tokens per Sec:     2740 || Batch Translation Loss:   0.157485 => Txt Tokens per Sec:     6953 || Lr: 0.000100
2024-02-03 12:12:48,704 Epoch 448: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.46 
2024-02-03 12:12:48,706 EPOCH 449
2024-02-03 12:13:02,429 Epoch 449: Total Training Recognition Loss 0.10  Total Training Translation Loss 2.27 
2024-02-03 12:13:02,430 EPOCH 450
2024-02-03 12:13:15,727 Epoch 450: Total Training Recognition Loss 0.11  Total Training Translation Loss 1.87 
2024-02-03 12:13:15,728 EPOCH 451
2024-02-03 12:13:29,069 Epoch 451: Total Training Recognition Loss 0.09  Total Training Translation Loss 1.92 
2024-02-03 12:13:29,069 EPOCH 452
2024-02-03 12:13:42,327 Epoch 452: Total Training Recognition Loss 0.09  Total Training Translation Loss 1.64 
2024-02-03 12:13:42,327 EPOCH 453
2024-02-03 12:13:51,055 [Epoch: 453 Step: 00007700] Batch Recognition Loss:   0.001343 => Gls Tokens per Sec:     1173 || Batch Translation Loss:   0.104483 => Txt Tokens per Sec:     3226 || Lr: 0.000100
2024-02-03 12:13:55,241 Epoch 453: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.90 
2024-02-03 12:13:55,242 EPOCH 454
2024-02-03 12:14:08,631 Epoch 454: Total Training Recognition Loss 0.09  Total Training Translation Loss 1.72 
2024-02-03 12:14:08,632 EPOCH 455
2024-02-03 12:14:21,898 Epoch 455: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.89 
2024-02-03 12:14:21,898 EPOCH 456
2024-02-03 12:14:34,969 Epoch 456: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.52 
2024-02-03 12:14:34,970 EPOCH 457
2024-02-03 12:14:48,343 Epoch 457: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.49 
2024-02-03 12:14:48,344 EPOCH 458
2024-02-03 12:15:01,676 Epoch 458: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.44 
2024-02-03 12:15:01,678 EPOCH 459
2024-02-03 12:15:12,847 [Epoch: 459 Step: 00007800] Batch Recognition Loss:   0.004817 => Gls Tokens per Sec:      780 || Batch Translation Loss:   0.135135 => Txt Tokens per Sec:     2147 || Lr: 0.000100
2024-02-03 12:15:14,957 Epoch 459: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.67 
2024-02-03 12:15:14,957 EPOCH 460
2024-02-03 12:15:28,479 Epoch 460: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.07 
2024-02-03 12:15:28,480 EPOCH 461
2024-02-03 12:15:41,525 Epoch 461: Total Training Recognition Loss 0.09  Total Training Translation Loss 2.24 
2024-02-03 12:15:41,525 EPOCH 462
2024-02-03 12:15:54,599 Epoch 462: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.96 
2024-02-03 12:15:54,600 EPOCH 463
2024-02-03 12:16:08,533 Epoch 463: Total Training Recognition Loss 0.08  Total Training Translation Loss 3.74 
2024-02-03 12:16:08,535 EPOCH 464
2024-02-03 12:16:21,829 Epoch 464: Total Training Recognition Loss 0.12  Total Training Translation Loss 4.81 
2024-02-03 12:16:21,830 EPOCH 465
2024-02-03 12:16:28,449 [Epoch: 465 Step: 00007900] Batch Recognition Loss:   0.005712 => Gls Tokens per Sec:     1160 || Batch Translation Loss:   0.317356 => Txt Tokens per Sec:     3173 || Lr: 0.000100
2024-02-03 12:16:35,240 Epoch 465: Total Training Recognition Loss 0.14  Total Training Translation Loss 6.64 
2024-02-03 12:16:35,241 EPOCH 466
2024-02-03 12:16:48,429 Epoch 466: Total Training Recognition Loss 0.14  Total Training Translation Loss 5.13 
2024-02-03 12:16:48,430 EPOCH 467
2024-02-03 12:17:01,690 Epoch 467: Total Training Recognition Loss 0.13  Total Training Translation Loss 4.03 
2024-02-03 12:17:01,691 EPOCH 468
2024-02-03 12:17:15,057 Epoch 468: Total Training Recognition Loss 0.11  Total Training Translation Loss 2.68 
2024-02-03 12:17:15,057 EPOCH 469
2024-02-03 12:17:28,357 Epoch 469: Total Training Recognition Loss 0.09  Total Training Translation Loss 2.18 
2024-02-03 12:17:28,358 EPOCH 470
2024-02-03 12:17:41,786 Epoch 470: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.80 
2024-02-03 12:17:41,787 EPOCH 471
2024-02-03 12:17:52,107 [Epoch: 471 Step: 00008000] Batch Recognition Loss:   0.008131 => Gls Tokens per Sec:      596 || Batch Translation Loss:   0.115760 => Txt Tokens per Sec:     1667 || Lr: 0.000100
2024-02-03 12:18:39,761 Validation result at epoch 471, step     8000: duration: 47.6525s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.82328	Translation Loss: 85953.71875	PPL: 7536.20508
	Eval Metric: BLEU
	WER 5.02	(DEL: 0.07,	INS: 0.00,	SUB: 4.95)
	BLEU-4 0.88	(BLEU-1: 11.32,	BLEU-2: 3.92,	BLEU-3: 1.79,	BLEU-4: 0.88)
	CHRF 17.30	ROUGE 9.51
2024-02-03 12:18:39,763 Logging Recognition and Translation Outputs
2024-02-03 12:18:39,763 ========================================================================================================================
2024-02-03 12:18:39,763 Logging Sequence: 109_16.00
2024-02-03 12:18:39,764 	Gloss Reference :	A B+C+D+E
2024-02-03 12:18:39,764 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 12:18:39,764 	Gloss Alignment :	         
2024-02-03 12:18:39,764 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 12:18:39,765 	Text Reference  :	********* however the match was  rescheduled as two kkr     players -       
2024-02-03 12:18:39,765 	Text Hypothesis :	currently in      the first team members     to the results were    negative
2024-02-03 12:18:39,765 	Text Alignment  :	I         S           S     S    S           S  S   S       S       S       
2024-02-03 12:18:39,765 ========================================================================================================================
2024-02-03 12:18:39,766 Logging Sequence: 156_272.00
2024-02-03 12:18:39,766 	Gloss Reference :	A B+C+D+E
2024-02-03 12:18:39,766 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 12:18:39,766 	Gloss Alignment :	         
2024-02-03 12:18:39,767 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 12:18:39,768 	Text Reference  :	miny' original captain was  kieron    pollard nicholas pooran was stand-in captain in    place of  him    
2024-02-03 12:18:39,768 	Text Hypothesis :	he    also     thanked bcci secretary jay     shah     for    his care     and     ended on    the country
2024-02-03 12:18:39,768 	Text Alignment  :	S     S        S       S    S         S       S        S      S   S        S       S     S     S   S      
2024-02-03 12:18:39,769 ========================================================================================================================
2024-02-03 12:18:39,769 Logging Sequence: 115_59.00
2024-02-03 12:18:39,769 	Gloss Reference :	A B+C+D+E
2024-02-03 12:18:39,769 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 12:18:39,769 	Gloss Alignment :	         
2024-02-03 12:18:39,769 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 12:18:39,770 	Text Reference  :	** *** **** **** *** she     now hosts several     cricket   programmes
2024-02-03 12:18:39,770 	Text Hypothesis :	on the 15th they got married in  a     traditional gurudwara wedding   
2024-02-03 12:18:39,770 	Text Alignment  :	I  I   I    I    I   S       S   S     S           S         S         
2024-02-03 12:18:39,770 ========================================================================================================================
2024-02-03 12:18:39,770 Logging Sequence: 63_35.00
2024-02-03 12:18:39,770 	Gloss Reference :	A B+C+D+E
2024-02-03 12:18:39,771 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 12:18:39,771 	Gloss Alignment :	         
2024-02-03 12:18:39,771 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 12:18:39,772 	Text Reference  :	companies interested in   buying the teams need to    fill the tender form by         paying rs   10 lakh
2024-02-03 12:18:39,772 	Text Hypothesis :	it        is         held at     the ***** **** start of   the ****** **** tournament and    held in ipl 
2024-02-03 12:18:39,772 	Text Alignment  :	S         S          S    S          D     D    S     S        D      D    S          S      S    S  S   
2024-02-03 12:18:39,772 ========================================================================================================================
2024-02-03 12:18:39,773 Logging Sequence: 100_97.00
2024-02-03 12:18:39,773 	Gloss Reference :	A B+C+D+E
2024-02-03 12:18:39,773 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 12:18:39,773 	Gloss Alignment :	         
2024-02-03 12:18:39,773 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 12:18:39,774 	Text Reference  :	** *** india had  to  play against pakistan in the sahara cup in  canada
2024-02-03 12:18:39,774 	Text Hypothesis :	in the first time and it   was     taken    to the team   on  his bowl  
2024-02-03 12:18:39,775 	Text Alignment  :	I  I   S     S    S   S    S       S        S      S      S   S   S     
2024-02-03 12:18:39,775 ========================================================================================================================
2024-02-03 12:18:43,255 Epoch 471: Total Training Recognition Loss 0.10  Total Training Translation Loss 1.59 
2024-02-03 12:18:43,256 EPOCH 472
2024-02-03 12:18:57,024 Epoch 472: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.35 
2024-02-03 12:18:57,025 EPOCH 473
2024-02-03 12:19:10,450 Epoch 473: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.17 
2024-02-03 12:19:10,451 EPOCH 474
2024-02-03 12:19:23,730 Epoch 474: Total Training Recognition Loss 0.07  Total Training Translation Loss 0.85 
2024-02-03 12:19:23,731 EPOCH 475
2024-02-03 12:19:37,314 Epoch 475: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.84 
2024-02-03 12:19:37,314 EPOCH 476
2024-02-03 12:19:50,592 Epoch 476: Total Training Recognition Loss 0.07  Total Training Translation Loss 0.99 
2024-02-03 12:19:50,593 EPOCH 477
2024-02-03 12:19:57,809 [Epoch: 477 Step: 00008100] Batch Recognition Loss:   0.004451 => Gls Tokens per Sec:      710 || Batch Translation Loss:   0.034483 => Txt Tokens per Sec:     2152 || Lr: 0.000100
2024-02-03 12:20:03,796 Epoch 477: Total Training Recognition Loss 0.06  Total Training Translation Loss 0.87 
2024-02-03 12:20:03,796 EPOCH 478
2024-02-03 12:20:16,922 Epoch 478: Total Training Recognition Loss 0.06  Total Training Translation Loss 0.73 
2024-02-03 12:20:16,922 EPOCH 479
2024-02-03 12:20:30,054 Epoch 479: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.78 
2024-02-03 12:20:30,055 EPOCH 480
2024-02-03 12:20:43,339 Epoch 480: Total Training Recognition Loss 0.07  Total Training Translation Loss 0.86 
2024-02-03 12:20:43,339 EPOCH 481
2024-02-03 12:20:56,599 Epoch 481: Total Training Recognition Loss 0.06  Total Training Translation Loss 0.84 
2024-02-03 12:20:56,600 EPOCH 482
2024-02-03 12:21:09,926 Epoch 482: Total Training Recognition Loss 0.06  Total Training Translation Loss 0.99 
2024-02-03 12:21:09,927 EPOCH 483
2024-02-03 12:21:14,966 [Epoch: 483 Step: 00008200] Batch Recognition Loss:   0.005415 => Gls Tokens per Sec:      762 || Batch Translation Loss:   0.033014 => Txt Tokens per Sec:     2054 || Lr: 0.000100
2024-02-03 12:21:23,111 Epoch 483: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.11 
2024-02-03 12:21:23,112 EPOCH 484
2024-02-03 12:21:36,392 Epoch 484: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.33 
2024-02-03 12:21:36,393 EPOCH 485
2024-02-03 12:21:49,596 Epoch 485: Total Training Recognition Loss 0.09  Total Training Translation Loss 3.10 
2024-02-03 12:21:49,596 EPOCH 486
2024-02-03 12:22:02,958 Epoch 486: Total Training Recognition Loss 0.11  Total Training Translation Loss 3.47 
2024-02-03 12:22:02,958 EPOCH 487
2024-02-03 12:22:15,899 Epoch 487: Total Training Recognition Loss 0.13  Total Training Translation Loss 3.50 
2024-02-03 12:22:15,900 EPOCH 488
2024-02-03 12:22:29,179 Epoch 488: Total Training Recognition Loss 0.14  Total Training Translation Loss 4.05 
2024-02-03 12:22:29,180 EPOCH 489
2024-02-03 12:22:30,266 [Epoch: 489 Step: 00008300] Batch Recognition Loss:   0.007096 => Gls Tokens per Sec:     2359 || Batch Translation Loss:   0.143991 => Txt Tokens per Sec:     6100 || Lr: 0.000100
2024-02-03 12:22:42,717 Epoch 489: Total Training Recognition Loss 0.17  Total Training Translation Loss 2.72 
2024-02-03 12:22:42,718 EPOCH 490
2024-02-03 12:22:56,057 Epoch 490: Total Training Recognition Loss 0.12  Total Training Translation Loss 2.08 
2024-02-03 12:22:56,058 EPOCH 491
2024-02-03 12:23:09,441 Epoch 491: Total Training Recognition Loss 0.10  Total Training Translation Loss 1.54 
2024-02-03 12:23:09,441 EPOCH 492
2024-02-03 12:23:22,707 Epoch 492: Total Training Recognition Loss 0.09  Total Training Translation Loss 1.36 
2024-02-03 12:23:22,708 EPOCH 493
2024-02-03 12:23:36,076 Epoch 493: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.23 
2024-02-03 12:23:36,077 EPOCH 494
2024-02-03 12:23:49,343 Epoch 494: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.10 
2024-02-03 12:23:49,343 EPOCH 495
2024-02-03 12:23:49,948 [Epoch: 495 Step: 00008400] Batch Recognition Loss:   0.007468 => Gls Tokens per Sec:     2129 || Batch Translation Loss:   0.041874 => Txt Tokens per Sec:     5945 || Lr: 0.000100
2024-02-03 12:24:02,580 Epoch 495: Total Training Recognition Loss 0.07  Total Training Translation Loss 0.84 
2024-02-03 12:24:02,580 EPOCH 496
2024-02-03 12:24:15,858 Epoch 496: Total Training Recognition Loss 0.06  Total Training Translation Loss 0.78 
2024-02-03 12:24:15,858 EPOCH 497
2024-02-03 12:24:29,237 Epoch 497: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.76 
2024-02-03 12:24:29,238 EPOCH 498
2024-02-03 12:24:42,747 Epoch 498: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.76 
2024-02-03 12:24:42,747 EPOCH 499
2024-02-03 12:24:55,730 Epoch 499: Total Training Recognition Loss 0.06  Total Training Translation Loss 0.63 
2024-02-03 12:24:55,730 EPOCH 500
2024-02-03 12:25:09,036 [Epoch: 500 Step: 00008500] Batch Recognition Loss:   0.001940 => Gls Tokens per Sec:      799 || Batch Translation Loss:   0.064964 => Txt Tokens per Sec:     2222 || Lr: 0.000100
2024-02-03 12:25:09,036 Epoch 500: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.57 
2024-02-03 12:25:09,037 EPOCH 501
2024-02-03 12:25:22,450 Epoch 501: Total Training Recognition Loss 0.06  Total Training Translation Loss 0.56 
2024-02-03 12:25:22,450 EPOCH 502
2024-02-03 12:25:35,856 Epoch 502: Total Training Recognition Loss 0.07  Total Training Translation Loss 0.57 
2024-02-03 12:25:35,857 EPOCH 503
2024-02-03 12:25:49,270 Epoch 503: Total Training Recognition Loss 0.06  Total Training Translation Loss 0.67 
2024-02-03 12:25:49,272 EPOCH 504
2024-02-03 12:26:02,477 Epoch 504: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.89 
2024-02-03 12:26:02,478 EPOCH 505
2024-02-03 12:26:15,469 Epoch 505: Total Training Recognition Loss 0.06  Total Training Translation Loss 0.79 
2024-02-03 12:26:15,470 EPOCH 506
2024-02-03 12:26:22,878 [Epoch: 506 Step: 00008600] Batch Recognition Loss:   0.000958 => Gls Tokens per Sec:     1296 || Batch Translation Loss:   0.021367 => Txt Tokens per Sec:     3503 || Lr: 0.000100
2024-02-03 12:26:28,816 Epoch 506: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.82 
2024-02-03 12:26:28,817 EPOCH 507
2024-02-03 12:26:42,495 Epoch 507: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.80 
2024-02-03 12:26:42,496 EPOCH 508
2024-02-03 12:26:55,833 Epoch 508: Total Training Recognition Loss 0.07  Total Training Translation Loss 0.93 
2024-02-03 12:26:55,833 EPOCH 509
2024-02-03 12:27:09,389 Epoch 509: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.02 
2024-02-03 12:27:09,390 EPOCH 510
2024-02-03 12:27:22,734 Epoch 510: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.38 
2024-02-03 12:27:22,735 EPOCH 511
2024-02-03 12:27:35,921 Epoch 511: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.66 
2024-02-03 12:27:35,922 EPOCH 512
2024-02-03 12:27:44,102 [Epoch: 512 Step: 00008700] Batch Recognition Loss:   0.001849 => Gls Tokens per Sec:     1017 || Batch Translation Loss:   0.091222 => Txt Tokens per Sec:     2817 || Lr: 0.000100
2024-02-03 12:27:48,954 Epoch 512: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.93 
2024-02-03 12:27:48,954 EPOCH 513
2024-02-03 12:28:02,047 Epoch 513: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.70 
2024-02-03 12:28:02,048 EPOCH 514
2024-02-03 12:28:15,375 Epoch 514: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.58 
2024-02-03 12:28:15,375 EPOCH 515
2024-02-03 12:28:28,624 Epoch 515: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.75 
2024-02-03 12:28:28,625 EPOCH 516
2024-02-03 12:28:41,820 Epoch 516: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.40 
2024-02-03 12:28:41,821 EPOCH 517
2024-02-03 12:28:54,946 Epoch 517: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.13 
2024-02-03 12:28:54,947 EPOCH 518
2024-02-03 12:29:02,459 [Epoch: 518 Step: 00008800] Batch Recognition Loss:   0.003806 => Gls Tokens per Sec:      904 || Batch Translation Loss:   0.062243 => Txt Tokens per Sec:     2345 || Lr: 0.000100
2024-02-03 12:29:08,096 Epoch 518: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.07 
2024-02-03 12:29:08,096 EPOCH 519
2024-02-03 12:29:21,179 Epoch 519: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.33 
2024-02-03 12:29:21,179 EPOCH 520
2024-02-03 12:29:34,062 Epoch 520: Total Training Recognition Loss 0.09  Total Training Translation Loss 2.57 
2024-02-03 12:29:34,063 EPOCH 521
2024-02-03 12:29:47,223 Epoch 521: Total Training Recognition Loss 0.08  Total Training Translation Loss 9.71 
2024-02-03 12:29:47,223 EPOCH 522
2024-02-03 12:30:00,318 Epoch 522: Total Training Recognition Loss 0.68  Total Training Translation Loss 31.81 
2024-02-03 12:30:00,318 EPOCH 523
2024-02-03 12:30:13,535 Epoch 523: Total Training Recognition Loss 2.34  Total Training Translation Loss 14.28 
2024-02-03 12:30:13,535 EPOCH 524
2024-02-03 12:30:25,195 [Epoch: 524 Step: 00008900] Batch Recognition Loss:   1.741160 => Gls Tokens per Sec:      473 || Batch Translation Loss:   0.372772 => Txt Tokens per Sec:     1413 || Lr: 0.000100
2024-02-03 12:30:27,127 Epoch 524: Total Training Recognition Loss 13.76  Total Training Translation Loss 8.54 
2024-02-03 12:30:27,127 EPOCH 525
2024-02-03 12:30:40,378 Epoch 525: Total Training Recognition Loss 24.22  Total Training Translation Loss 12.98 
2024-02-03 12:30:40,379 EPOCH 526
2024-02-03 12:30:53,889 Epoch 526: Total Training Recognition Loss 23.03  Total Training Translation Loss 13.86 
2024-02-03 12:30:53,890 EPOCH 527
2024-02-03 12:31:07,146 Epoch 527: Total Training Recognition Loss 5.50  Total Training Translation Loss 9.51 
2024-02-03 12:31:07,147 EPOCH 528
2024-02-03 12:31:20,621 Epoch 528: Total Training Recognition Loss 1.00  Total Training Translation Loss 3.58 
2024-02-03 12:31:20,622 EPOCH 529
2024-02-03 12:31:34,034 Epoch 529: Total Training Recognition Loss 0.54  Total Training Translation Loss 2.02 
2024-02-03 12:31:34,034 EPOCH 530
2024-02-03 12:31:41,085 [Epoch: 530 Step: 00009000] Batch Recognition Loss:   0.035716 => Gls Tokens per Sec:      600 || Batch Translation Loss:   0.094256 => Txt Tokens per Sec:     1686 || Lr: 0.000100
2024-02-03 12:31:47,711 Epoch 530: Total Training Recognition Loss 0.32  Total Training Translation Loss 1.47 
2024-02-03 12:31:47,712 EPOCH 531
2024-02-03 12:32:00,748 Epoch 531: Total Training Recognition Loss 0.27  Total Training Translation Loss 1.25 
2024-02-03 12:32:00,749 EPOCH 532
2024-02-03 12:32:14,425 Epoch 532: Total Training Recognition Loss 0.22  Total Training Translation Loss 1.06 
2024-02-03 12:32:14,426 EPOCH 533
2024-02-03 12:32:27,686 Epoch 533: Total Training Recognition Loss 0.19  Total Training Translation Loss 0.91 
2024-02-03 12:32:27,686 EPOCH 534
2024-02-03 12:32:40,718 Epoch 534: Total Training Recognition Loss 0.19  Total Training Translation Loss 0.89 
2024-02-03 12:32:40,718 EPOCH 535
2024-02-03 12:32:53,842 Epoch 535: Total Training Recognition Loss 0.18  Total Training Translation Loss 0.89 
2024-02-03 12:32:53,843 EPOCH 536
2024-02-03 12:32:59,140 [Epoch: 536 Step: 00009100] Batch Recognition Loss:   0.002068 => Gls Tokens per Sec:      604 || Batch Translation Loss:   0.035721 => Txt Tokens per Sec:     1840 || Lr: 0.000100
2024-02-03 12:33:07,083 Epoch 536: Total Training Recognition Loss 0.16  Total Training Translation Loss 0.78 
2024-02-03 12:33:07,084 EPOCH 537
2024-02-03 12:33:20,290 Epoch 537: Total Training Recognition Loss 0.14  Total Training Translation Loss 0.72 
2024-02-03 12:33:20,291 EPOCH 538
2024-02-03 12:33:33,615 Epoch 538: Total Training Recognition Loss 0.14  Total Training Translation Loss 0.75 
2024-02-03 12:33:33,616 EPOCH 539
2024-02-03 12:33:46,969 Epoch 539: Total Training Recognition Loss 0.14  Total Training Translation Loss 0.76 
2024-02-03 12:33:46,970 EPOCH 540
2024-02-03 12:34:00,282 Epoch 540: Total Training Recognition Loss 0.13  Total Training Translation Loss 0.75 
2024-02-03 12:34:00,282 EPOCH 541
2024-02-03 12:34:13,828 Epoch 541: Total Training Recognition Loss 0.12  Total Training Translation Loss 0.71 
2024-02-03 12:34:13,829 EPOCH 542
2024-02-03 12:34:22,370 [Epoch: 542 Step: 00009200] Batch Recognition Loss:   0.014454 => Gls Tokens per Sec:      196 || Batch Translation Loss:   0.062501 => Txt Tokens per Sec:      683 || Lr: 0.000100
2024-02-03 12:34:26,941 Epoch 542: Total Training Recognition Loss 0.11  Total Training Translation Loss 0.69 
2024-02-03 12:34:26,941 EPOCH 543
2024-02-03 12:34:39,954 Epoch 543: Total Training Recognition Loss 0.13  Total Training Translation Loss 0.70 
2024-02-03 12:34:39,955 EPOCH 544
2024-02-03 12:34:53,585 Epoch 544: Total Training Recognition Loss 0.12  Total Training Translation Loss 0.62 
2024-02-03 12:34:53,586 EPOCH 545
2024-02-03 12:35:07,063 Epoch 545: Total Training Recognition Loss 0.10  Total Training Translation Loss 0.67 
2024-02-03 12:35:07,064 EPOCH 546
2024-02-03 12:35:20,563 Epoch 546: Total Training Recognition Loss 0.09  Total Training Translation Loss 0.64 
2024-02-03 12:35:20,563 EPOCH 547
2024-02-03 12:35:33,568 Epoch 547: Total Training Recognition Loss 0.11  Total Training Translation Loss 0.67 
2024-02-03 12:35:33,568 EPOCH 548
2024-02-03 12:35:33,733 [Epoch: 548 Step: 00009300] Batch Recognition Loss:   0.001429 => Gls Tokens per Sec:     3951 || Batch Translation Loss:   0.029735 => Txt Tokens per Sec:     8383 || Lr: 0.000100
2024-02-03 12:35:46,789 Epoch 548: Total Training Recognition Loss 0.09  Total Training Translation Loss 0.60 
2024-02-03 12:35:46,789 EPOCH 549
2024-02-03 12:35:59,937 Epoch 549: Total Training Recognition Loss 0.08  Total Training Translation Loss 0.55 
2024-02-03 12:35:59,938 EPOCH 550
2024-02-03 12:36:13,220 Epoch 550: Total Training Recognition Loss 0.07  Total Training Translation Loss 0.55 
2024-02-03 12:36:13,221 EPOCH 551
2024-02-03 12:36:26,723 Epoch 551: Total Training Recognition Loss 0.08  Total Training Translation Loss 0.54 
2024-02-03 12:36:26,724 EPOCH 552
2024-02-03 12:36:40,027 Epoch 552: Total Training Recognition Loss 0.09  Total Training Translation Loss 0.55 
2024-02-03 12:36:40,028 EPOCH 553
2024-02-03 12:36:51,602 [Epoch: 553 Step: 00009400] Batch Recognition Loss:   0.022421 => Gls Tokens per Sec:      863 || Batch Translation Loss:   0.043470 => Txt Tokens per Sec:     2374 || Lr: 0.000100
2024-02-03 12:36:53,254 Epoch 553: Total Training Recognition Loss 0.08  Total Training Translation Loss 0.54 
2024-02-03 12:36:53,254 EPOCH 554
2024-02-03 12:37:06,606 Epoch 554: Total Training Recognition Loss 0.07  Total Training Translation Loss 0.47 
2024-02-03 12:37:06,606 EPOCH 555
2024-02-03 12:37:19,814 Epoch 555: Total Training Recognition Loss 0.08  Total Training Translation Loss 0.48 
2024-02-03 12:37:19,814 EPOCH 556
2024-02-03 12:37:32,605 Epoch 556: Total Training Recognition Loss 0.08  Total Training Translation Loss 0.47 
2024-02-03 12:37:32,607 EPOCH 557
2024-02-03 12:37:46,457 Epoch 557: Total Training Recognition Loss 0.07  Total Training Translation Loss 0.49 
2024-02-03 12:37:46,458 EPOCH 558
2024-02-03 12:37:59,692 Epoch 558: Total Training Recognition Loss 0.07  Total Training Translation Loss 0.49 
2024-02-03 12:37:59,693 EPOCH 559
2024-02-03 12:38:10,803 [Epoch: 559 Step: 00009500] Batch Recognition Loss:   0.006873 => Gls Tokens per Sec:      784 || Batch Translation Loss:   0.031829 => Txt Tokens per Sec:     2121 || Lr: 0.000100
2024-02-03 12:38:12,941 Epoch 559: Total Training Recognition Loss 0.06  Total Training Translation Loss 0.46 
2024-02-03 12:38:12,942 EPOCH 560
2024-02-03 12:38:26,117 Epoch 560: Total Training Recognition Loss 0.06  Total Training Translation Loss 0.45 
2024-02-03 12:38:26,117 EPOCH 561
2024-02-03 12:38:39,316 Epoch 561: Total Training Recognition Loss 0.06  Total Training Translation Loss 0.43 
2024-02-03 12:38:39,317 EPOCH 562
2024-02-03 12:38:52,812 Epoch 562: Total Training Recognition Loss 0.07  Total Training Translation Loss 0.53 
2024-02-03 12:38:52,813 EPOCH 563
2024-02-03 12:39:06,015 Epoch 563: Total Training Recognition Loss 0.07  Total Training Translation Loss 0.54 
2024-02-03 12:39:06,016 EPOCH 564
2024-02-03 12:39:19,475 Epoch 564: Total Training Recognition Loss 0.07  Total Training Translation Loss 0.44 
2024-02-03 12:39:19,476 EPOCH 565
2024-02-03 12:39:26,234 [Epoch: 565 Step: 00009600] Batch Recognition Loss:   0.001369 => Gls Tokens per Sec:     1137 || Batch Translation Loss:   0.014524 => Txt Tokens per Sec:     3167 || Lr: 0.000100
2024-02-03 12:39:32,592 Epoch 565: Total Training Recognition Loss 0.06  Total Training Translation Loss 0.43 
2024-02-03 12:39:32,592 EPOCH 566
2024-02-03 12:39:46,009 Epoch 566: Total Training Recognition Loss 0.07  Total Training Translation Loss 0.41 
2024-02-03 12:39:46,010 EPOCH 567
2024-02-03 12:39:59,551 Epoch 567: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.43 
2024-02-03 12:39:59,552 EPOCH 568
2024-02-03 12:40:12,695 Epoch 568: Total Training Recognition Loss 0.06  Total Training Translation Loss 0.46 
2024-02-03 12:40:12,696 EPOCH 569
2024-02-03 12:40:26,098 Epoch 569: Total Training Recognition Loss 0.06  Total Training Translation Loss 0.49 
2024-02-03 12:40:26,099 EPOCH 570
2024-02-03 12:40:39,223 Epoch 570: Total Training Recognition Loss 0.06  Total Training Translation Loss 0.47 
2024-02-03 12:40:39,225 EPOCH 571
2024-02-03 12:40:51,248 [Epoch: 571 Step: 00009700] Batch Recognition Loss:   0.004153 => Gls Tokens per Sec:      512 || Batch Translation Loss:   0.038258 => Txt Tokens per Sec:     1600 || Lr: 0.000100
2024-02-03 12:40:52,596 Epoch 571: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.51 
2024-02-03 12:40:52,596 EPOCH 572
2024-02-03 12:41:05,795 Epoch 572: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.58 
2024-02-03 12:41:05,795 EPOCH 573
2024-02-03 12:41:19,281 Epoch 573: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.52 
2024-02-03 12:41:19,282 EPOCH 574
2024-02-03 12:41:32,603 Epoch 574: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.58 
2024-02-03 12:41:32,604 EPOCH 575
2024-02-03 12:41:45,803 Epoch 575: Total Training Recognition Loss 0.06  Total Training Translation Loss 0.70 
2024-02-03 12:41:45,803 EPOCH 576
2024-02-03 12:41:59,135 Epoch 576: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.64 
2024-02-03 12:41:59,136 EPOCH 577
2024-02-03 12:42:02,035 [Epoch: 577 Step: 00009800] Batch Recognition Loss:   0.001514 => Gls Tokens per Sec:     1767 || Batch Translation Loss:   0.014794 => Txt Tokens per Sec:     4420 || Lr: 0.000100
2024-02-03 12:42:12,332 Epoch 577: Total Training Recognition Loss 0.07  Total Training Translation Loss 0.64 
2024-02-03 12:42:12,332 EPOCH 578
2024-02-03 12:42:25,735 Epoch 578: Total Training Recognition Loss 0.06  Total Training Translation Loss 0.65 
2024-02-03 12:42:25,735 EPOCH 579
2024-02-03 12:42:38,961 Epoch 579: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.54 
2024-02-03 12:42:38,962 EPOCH 580
2024-02-03 12:42:52,326 Epoch 580: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.59 
2024-02-03 12:42:52,327 EPOCH 581
2024-02-03 12:43:05,996 Epoch 581: Total Training Recognition Loss 0.06  Total Training Translation Loss 0.53 
2024-02-03 12:43:05,997 EPOCH 582
2024-02-03 12:43:19,324 Epoch 582: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.49 
2024-02-03 12:43:19,324 EPOCH 583
2024-02-03 12:43:25,927 [Epoch: 583 Step: 00009900] Batch Recognition Loss:   0.005845 => Gls Tokens per Sec:      544 || Batch Translation Loss:   0.010172 => Txt Tokens per Sec:     1420 || Lr: 0.000100
2024-02-03 12:43:32,670 Epoch 583: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.54 
2024-02-03 12:43:32,671 EPOCH 584
2024-02-03 12:43:45,835 Epoch 584: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.56 
2024-02-03 12:43:45,836 EPOCH 585
2024-02-03 12:43:59,441 Epoch 585: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.57 
2024-02-03 12:43:59,442 EPOCH 586
2024-02-03 12:44:12,554 Epoch 586: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.58 
2024-02-03 12:44:12,555 EPOCH 587
2024-02-03 12:44:26,168 Epoch 587: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.74 
2024-02-03 12:44:26,168 EPOCH 588
2024-02-03 12:44:39,457 Epoch 588: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.81 
2024-02-03 12:44:39,458 EPOCH 589
2024-02-03 12:44:40,196 [Epoch: 589 Step: 00010000] Batch Recognition Loss:   0.000534 => Gls Tokens per Sec:     3471 || Batch Translation Loss:   0.038363 => Txt Tokens per Sec:     8623 || Lr: 0.000100
2024-02-03 12:45:27,795 Validation result at epoch 589, step    10000: duration: 47.5983s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.58089	Translation Loss: 87705.36719	PPL: 9039.92773
	Eval Metric: BLEU
	WER 4.10	(DEL: 0.00,	INS: 0.00,	SUB: 4.10)
	BLEU-4 0.71	(BLEU-1: 10.95,	BLEU-2: 3.56,	BLEU-3: 1.43,	BLEU-4: 0.71)
	CHRF 17.26	ROUGE 9.12
2024-02-03 12:45:27,798 Logging Recognition and Translation Outputs
2024-02-03 12:45:27,798 ========================================================================================================================
2024-02-03 12:45:27,798 Logging Sequence: 78_43.00
2024-02-03 12:45:27,799 	Gloss Reference :	A B+C+D+E
2024-02-03 12:45:27,799 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 12:45:27,799 	Gloss Alignment :	         
2024-02-03 12:45:27,799 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 12:45:27,800 	Text Reference  :	* ** **** ***** however something happened which made  the team       very       happy
2024-02-03 12:45:27,801 	Text Hypothesis :	i am very grate to      my        fans     and   media for constantly supporting me   
2024-02-03 12:45:27,801 	Text Alignment  :	I I  I    I     S       S         S        S     S     S   S          S          S    
2024-02-03 12:45:27,801 ========================================================================================================================
2024-02-03 12:45:27,801 Logging Sequence: 98_97.00
2024-02-03 12:45:27,801 	Gloss Reference :	A B+C+D+E
2024-02-03 12:45:27,802 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 12:45:27,802 	Gloss Alignment :	         
2024-02-03 12:45:27,802 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 12:45:27,803 	Text Reference  :	the ***** teams in  the series were       india legends england legends sri  lanka legends   
2024-02-03 12:45:27,803 	Text Hypothesis :	the match fee   for the ****** tournament is    set     to      start   from the   tournament
2024-02-03 12:45:27,804 	Text Alignment  :	    I     S     S       D      S          S     S       S       S       S    S     S         
2024-02-03 12:45:27,804 ========================================================================================================================
2024-02-03 12:45:27,804 Logging Sequence: 143_11.00
2024-02-03 12:45:27,804 	Gloss Reference :	A B+C+D+E
2024-02-03 12:45:27,804 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 12:45:27,804 	Gloss Alignment :	         
2024-02-03 12:45:27,804 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 12:45:27,806 	Text Reference  :	ronaldo has also become the first     person to have 500 million followers on instagram he          is the most loved footballer
2024-02-03 12:45:27,806 	Text Hypothesis :	******* *** **** ****** a   complaint letter to **** *** ******* ********* ** these     association of the **** ***** team      
2024-02-03 12:45:27,806 	Text Alignment  :	D       D   D    D      S   S         S         D    D   D       D         D  S         S           S      D    D     S         
2024-02-03 12:45:27,806 ========================================================================================================================
2024-02-03 12:45:27,806 Logging Sequence: 179_386.00
2024-02-03 12:45:27,806 	Gloss Reference :	A B+C+D+E
2024-02-03 12:45:27,807 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 12:45:27,807 	Gloss Alignment :	         
2024-02-03 12:45:27,807 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 12:45:27,808 	Text Reference  :	and the federation or  sai  people are their servants  to  pick    up their passport *** *****
2024-02-03 12:45:27,808 	Text Hypothesis :	*** she might      all wear this   and many  wrestlers not picking up her   passport are false
2024-02-03 12:45:27,808 	Text Alignment  :	D   S   S          S   S    S      S   S     S         S   S          S              I   I    
2024-02-03 12:45:27,808 ========================================================================================================================
2024-02-03 12:45:27,808 Logging Sequence: 77_60.00
2024-02-03 12:45:27,809 	Gloss Reference :	A B+C+D+E
2024-02-03 12:45:27,809 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 12:45:27,809 	Gloss Alignment :	         
2024-02-03 12:45:27,809 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 12:45:27,809 	Text Reference  :	****** **** he   remained not out
2024-02-03 12:45:27,809 	Text Hypothesis :	people were very happy    to  see
2024-02-03 12:45:27,810 	Text Alignment  :	I      I    S    S        S   S  
2024-02-03 12:45:27,810 ========================================================================================================================
2024-02-03 12:45:40,747 Epoch 589: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.84 
2024-02-03 12:45:40,747 EPOCH 590
2024-02-03 12:45:54,485 Epoch 590: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.04 
2024-02-03 12:45:54,486 EPOCH 591
2024-02-03 12:46:07,869 Epoch 591: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.83 
2024-02-03 12:46:07,869 EPOCH 592
2024-02-03 12:46:21,123 Epoch 592: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.98 
2024-02-03 12:46:21,123 EPOCH 593
2024-02-03 12:46:34,662 Epoch 593: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.98 
2024-02-03 12:46:34,662 EPOCH 594
2024-02-03 12:46:47,996 Epoch 594: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.42 
2024-02-03 12:46:47,996 EPOCH 595
2024-02-03 12:46:51,215 [Epoch: 595 Step: 00010100] Batch Recognition Loss:   0.008144 => Gls Tokens per Sec:      398 || Batch Translation Loss:   0.067562 => Txt Tokens per Sec:     1234 || Lr: 0.000100
2024-02-03 12:47:01,481 Epoch 595: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.95 
2024-02-03 12:47:01,482 EPOCH 596
2024-02-03 12:47:14,407 Epoch 596: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.04 
2024-02-03 12:47:14,408 EPOCH 597
2024-02-03 12:47:27,810 Epoch 597: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.82 
2024-02-03 12:47:27,810 EPOCH 598
2024-02-03 12:47:41,149 Epoch 598: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.71 
2024-02-03 12:47:41,150 EPOCH 599
2024-02-03 12:47:54,361 Epoch 599: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.20 
2024-02-03 12:47:54,362 EPOCH 600
2024-02-03 12:48:07,536 [Epoch: 600 Step: 00010200] Batch Recognition Loss:   0.000614 => Gls Tokens per Sec:      807 || Batch Translation Loss:   0.059178 => Txt Tokens per Sec:     2244 || Lr: 0.000100
2024-02-03 12:48:07,536 Epoch 600: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.17 
2024-02-03 12:48:07,537 EPOCH 601
2024-02-03 12:48:20,817 Epoch 601: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.05 
2024-02-03 12:48:20,817 EPOCH 602
2024-02-03 12:48:33,927 Epoch 602: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.22 
2024-02-03 12:48:33,928 EPOCH 603
2024-02-03 12:48:47,556 Epoch 603: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.40 
2024-02-03 12:48:47,556 EPOCH 604
2024-02-03 12:49:00,800 Epoch 604: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.05 
2024-02-03 12:49:00,801 EPOCH 605
2024-02-03 12:49:13,919 Epoch 605: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.40 
2024-02-03 12:49:13,920 EPOCH 606
2024-02-03 12:49:26,144 [Epoch: 606 Step: 00010300] Batch Recognition Loss:   0.004603 => Gls Tokens per Sec:      765 || Batch Translation Loss:   0.091174 => Txt Tokens per Sec:     2119 || Lr: 0.000100
2024-02-03 12:49:26,732 Epoch 606: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.31 
2024-02-03 12:49:26,732 EPOCH 607
2024-02-03 12:49:40,011 Epoch 607: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.03 
2024-02-03 12:49:40,012 EPOCH 608
2024-02-03 12:49:53,241 Epoch 608: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.08 
2024-02-03 12:49:53,241 EPOCH 609
2024-02-03 12:50:06,446 Epoch 609: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.95 
2024-02-03 12:50:06,447 EPOCH 610
2024-02-03 12:50:19,707 Epoch 610: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.01 
2024-02-03 12:50:19,708 EPOCH 611
2024-02-03 12:50:33,059 Epoch 611: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.03 
2024-02-03 12:50:33,060 EPOCH 612
2024-02-03 12:50:41,216 [Epoch: 612 Step: 00010400] Batch Recognition Loss:   0.000874 => Gls Tokens per Sec:      990 || Batch Translation Loss:   0.046999 => Txt Tokens per Sec:     2678 || Lr: 0.000100
2024-02-03 12:50:46,134 Epoch 612: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.80 
2024-02-03 12:50:46,135 EPOCH 613
2024-02-03 12:50:59,293 Epoch 613: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.93 
2024-02-03 12:50:59,294 EPOCH 614
2024-02-03 12:51:12,839 Epoch 614: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.92 
2024-02-03 12:51:12,840 EPOCH 615
2024-02-03 12:51:26,092 Epoch 615: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.82 
2024-02-03 12:51:26,093 EPOCH 616
2024-02-03 12:51:39,311 Epoch 616: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.77 
2024-02-03 12:51:39,311 EPOCH 617
2024-02-03 12:51:52,708 Epoch 617: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.60 
2024-02-03 12:51:52,709 EPOCH 618
2024-02-03 12:51:56,355 [Epoch: 618 Step: 00010500] Batch Recognition Loss:   0.006319 => Gls Tokens per Sec:     1932 || Batch Translation Loss:   0.016187 => Txt Tokens per Sec:     5036 || Lr: 0.000100
2024-02-03 12:52:06,046 Epoch 618: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.86 
2024-02-03 12:52:06,046 EPOCH 619
2024-02-03 12:52:19,164 Epoch 619: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.76 
2024-02-03 12:52:19,165 EPOCH 620
2024-02-03 12:52:32,574 Epoch 620: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.63 
2024-02-03 12:52:32,575 EPOCH 621
2024-02-03 12:52:45,551 Epoch 621: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.67 
2024-02-03 12:52:45,551 EPOCH 622
2024-02-03 12:52:58,754 Epoch 622: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.69 
2024-02-03 12:52:58,755 EPOCH 623
2024-02-03 12:53:11,928 Epoch 623: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.79 
2024-02-03 12:53:11,928 EPOCH 624
2024-02-03 12:53:15,416 [Epoch: 624 Step: 00010600] Batch Recognition Loss:   0.005004 => Gls Tokens per Sec:     1652 || Batch Translation Loss:   0.012489 => Txt Tokens per Sec:     4453 || Lr: 0.000100
2024-02-03 12:53:25,186 Epoch 624: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.67 
2024-02-03 12:53:25,187 EPOCH 625
2024-02-03 12:53:38,729 Epoch 625: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.62 
2024-02-03 12:53:38,730 EPOCH 626
2024-02-03 12:53:51,965 Epoch 626: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.72 
2024-02-03 12:53:51,966 EPOCH 627
2024-02-03 12:54:05,247 Epoch 627: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.99 
2024-02-03 12:54:05,248 EPOCH 628
2024-02-03 12:54:20,140 Epoch 628: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.03 
2024-02-03 12:54:20,141 EPOCH 629
2024-02-03 12:54:33,600 Epoch 629: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.03 
2024-02-03 12:54:33,601 EPOCH 630
2024-02-03 12:54:40,652 [Epoch: 630 Step: 00010700] Batch Recognition Loss:   0.001190 => Gls Tokens per Sec:      600 || Batch Translation Loss:   0.066168 => Txt Tokens per Sec:     1648 || Lr: 0.000100
2024-02-03 12:54:47,048 Epoch 630: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.40 
2024-02-03 12:54:47,049 EPOCH 631
2024-02-03 12:55:00,212 Epoch 631: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.25 
2024-02-03 12:55:00,212 EPOCH 632
2024-02-03 12:55:13,569 Epoch 632: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.39 
2024-02-03 12:55:13,570 EPOCH 633
2024-02-03 12:55:26,596 Epoch 633: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.66 
2024-02-03 12:55:26,597 EPOCH 634
2024-02-03 12:55:39,371 Epoch 634: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.21 
2024-02-03 12:55:39,373 EPOCH 635
2024-02-03 12:55:52,751 Epoch 635: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.23 
2024-02-03 12:55:52,751 EPOCH 636
2024-02-03 12:55:57,645 [Epoch: 636 Step: 00010800] Batch Recognition Loss:   0.006198 => Gls Tokens per Sec:      603 || Batch Translation Loss:   0.045284 => Txt Tokens per Sec:     1516 || Lr: 0.000100
2024-02-03 12:56:06,079 Epoch 636: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.11 
2024-02-03 12:56:06,079 EPOCH 637
2024-02-03 12:56:19,240 Epoch 637: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.10 
2024-02-03 12:56:19,240 EPOCH 638
2024-02-03 12:56:32,348 Epoch 638: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.15 
2024-02-03 12:56:32,349 EPOCH 639
2024-02-03 12:56:45,478 Epoch 639: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.95 
2024-02-03 12:56:45,479 EPOCH 640
2024-02-03 12:56:58,949 Epoch 640: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.16 
2024-02-03 12:56:58,949 EPOCH 641
2024-02-03 12:57:12,070 Epoch 641: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.62 
2024-02-03 12:57:12,071 EPOCH 642
2024-02-03 12:57:14,091 [Epoch: 642 Step: 00010900] Batch Recognition Loss:   0.003510 => Gls Tokens per Sec:      951 || Batch Translation Loss:   0.067877 => Txt Tokens per Sec:     2659 || Lr: 0.000100
2024-02-03 12:57:25,482 Epoch 642: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.89 
2024-02-03 12:57:25,483 EPOCH 643
2024-02-03 12:57:38,677 Epoch 643: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.98 
2024-02-03 12:57:38,678 EPOCH 644
2024-02-03 12:57:51,708 Epoch 644: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.81 
2024-02-03 12:57:51,708 EPOCH 645
2024-02-03 12:58:05,104 Epoch 645: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.42 
2024-02-03 12:58:05,104 EPOCH 646
2024-02-03 12:58:18,320 Epoch 646: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.33 
2024-02-03 12:58:18,320 EPOCH 647
2024-02-03 12:58:31,460 Epoch 647: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.63 
2024-02-03 12:58:31,461 EPOCH 648
2024-02-03 12:58:31,778 [Epoch: 648 Step: 00011000] Batch Recognition Loss:   0.001357 => Gls Tokens per Sec:     2029 || Batch Translation Loss:   0.083039 => Txt Tokens per Sec:     6029 || Lr: 0.000100
2024-02-03 12:58:44,921 Epoch 648: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.40 
2024-02-03 12:58:44,922 EPOCH 649
2024-02-03 12:58:57,860 Epoch 649: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.95 
2024-02-03 12:58:57,860 EPOCH 650
2024-02-03 12:59:10,583 Epoch 650: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.38 
2024-02-03 12:59:10,584 EPOCH 651
2024-02-03 12:59:23,674 Epoch 651: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.86 
2024-02-03 12:59:23,675 EPOCH 652
2024-02-03 12:59:37,053 Epoch 652: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.14 
2024-02-03 12:59:37,053 EPOCH 653
2024-02-03 12:59:49,691 [Epoch: 653 Step: 00011100] Batch Recognition Loss:   0.001119 => Gls Tokens per Sec:      791 || Batch Translation Loss:   0.156881 => Txt Tokens per Sec:     2189 || Lr: 0.000100
2024-02-03 12:59:50,012 Epoch 653: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.46 
2024-02-03 12:59:50,012 EPOCH 654
2024-02-03 13:00:03,478 Epoch 654: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.17 
2024-02-03 13:00:03,479 EPOCH 655
2024-02-03 13:00:16,072 Epoch 655: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.64 
2024-02-03 13:00:16,072 EPOCH 656
2024-02-03 13:00:29,401 Epoch 656: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.33 
2024-02-03 13:00:29,402 EPOCH 657
2024-02-03 13:00:42,555 Epoch 657: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.00 
2024-02-03 13:00:42,556 EPOCH 658
2024-02-03 13:00:55,823 Epoch 658: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.94 
2024-02-03 13:00:55,824 EPOCH 659
2024-02-03 13:01:06,988 [Epoch: 659 Step: 00011200] Batch Recognition Loss:   0.002430 => Gls Tokens per Sec:      780 || Batch Translation Loss:   0.046134 => Txt Tokens per Sec:     2189 || Lr: 0.000100
2024-02-03 13:01:08,959 Epoch 659: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.79 
2024-02-03 13:01:08,959 EPOCH 660
2024-02-03 13:01:22,084 Epoch 660: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.76 
2024-02-03 13:01:22,085 EPOCH 661
2024-02-03 13:01:35,314 Epoch 661: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.67 
2024-02-03 13:01:35,314 EPOCH 662
2024-02-03 13:01:48,731 Epoch 662: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.76 
2024-02-03 13:01:48,732 EPOCH 663
2024-02-03 13:02:02,295 Epoch 663: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.73 
2024-02-03 13:02:02,295 EPOCH 664
2024-02-03 13:02:15,509 Epoch 664: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.59 
2024-02-03 13:02:15,511 EPOCH 665
2024-02-03 13:02:25,289 [Epoch: 665 Step: 00011300] Batch Recognition Loss:   0.001495 => Gls Tokens per Sec:      760 || Batch Translation Loss:   0.046592 => Txt Tokens per Sec:     2178 || Lr: 0.000100
2024-02-03 13:02:28,982 Epoch 665: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.58 
2024-02-03 13:02:28,982 EPOCH 666
2024-02-03 13:02:42,523 Epoch 666: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.57 
2024-02-03 13:02:42,523 EPOCH 667
2024-02-03 13:02:55,852 Epoch 667: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.76 
2024-02-03 13:02:55,853 EPOCH 668
2024-02-03 13:03:09,317 Epoch 668: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.13 
2024-02-03 13:03:09,318 EPOCH 669
2024-02-03 13:03:22,467 Epoch 669: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.78 
2024-02-03 13:03:22,467 EPOCH 670
2024-02-03 13:03:35,686 Epoch 670: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.82 
2024-02-03 13:03:35,686 EPOCH 671
2024-02-03 13:03:45,792 [Epoch: 671 Step: 00011400] Batch Recognition Loss:   0.007010 => Gls Tokens per Sec:      609 || Batch Translation Loss:   0.037192 => Txt Tokens per Sec:     1752 || Lr: 0.000100
2024-02-03 13:03:48,914 Epoch 671: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.63 
2024-02-03 13:03:48,914 EPOCH 672
2024-02-03 13:04:02,248 Epoch 672: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.62 
2024-02-03 13:04:02,249 EPOCH 673
2024-02-03 13:04:15,640 Epoch 673: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.63 
2024-02-03 13:04:15,640 EPOCH 674
2024-02-03 13:04:29,006 Epoch 674: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.56 
2024-02-03 13:04:29,007 EPOCH 675
2024-02-03 13:04:42,274 Epoch 675: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.77 
2024-02-03 13:04:42,275 EPOCH 676
2024-02-03 13:04:55,209 Epoch 676: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.85 
2024-02-03 13:04:55,210 EPOCH 677
2024-02-03 13:04:59,708 [Epoch: 677 Step: 00011500] Batch Recognition Loss:   0.002026 => Gls Tokens per Sec:     1138 || Batch Translation Loss:   0.042313 => Txt Tokens per Sec:     3003 || Lr: 0.000100
2024-02-03 13:05:08,645 Epoch 677: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.05 
2024-02-03 13:05:08,646 EPOCH 678
2024-02-03 13:05:31,849 Epoch 678: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.87 
2024-02-03 13:05:31,851 EPOCH 679
2024-02-03 13:05:52,910 Epoch 679: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.83 
2024-02-03 13:05:52,910 EPOCH 680
2024-02-03 13:06:13,606 Epoch 680: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.78 
2024-02-03 13:06:13,606 EPOCH 681
2024-02-03 13:06:28,801 Epoch 681: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.78 
2024-02-03 13:06:28,802 EPOCH 682
2024-02-03 13:06:46,820 Epoch 682: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.34 
2024-02-03 13:06:46,820 EPOCH 683
2024-02-03 13:06:51,597 [Epoch: 683 Step: 00011600] Batch Recognition Loss:   0.004865 => Gls Tokens per Sec:      804 || Batch Translation Loss:   0.089484 => Txt Tokens per Sec:     2291 || Lr: 0.000100
2024-02-03 13:07:01,401 Epoch 683: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.00 
2024-02-03 13:07:01,401 EPOCH 684
2024-02-03 13:07:14,849 Epoch 684: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.18 
2024-02-03 13:07:14,850 EPOCH 685
2024-02-03 13:07:28,178 Epoch 685: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.49 
2024-02-03 13:07:28,178 EPOCH 686
2024-02-03 13:07:41,583 Epoch 686: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.35 
2024-02-03 13:07:41,584 EPOCH 687
2024-02-03 13:07:54,776 Epoch 687: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.32 
2024-02-03 13:07:54,777 EPOCH 688
2024-02-03 13:08:07,992 Epoch 688: Total Training Recognition Loss 0.06  Total Training Translation Loss 4.02 
2024-02-03 13:08:07,992 EPOCH 689
2024-02-03 13:08:14,118 [Epoch: 689 Step: 00011700] Batch Recognition Loss:   0.002607 => Gls Tokens per Sec:      377 || Batch Translation Loss:   0.201606 => Txt Tokens per Sec:     1148 || Lr: 0.000100
2024-02-03 13:08:21,196 Epoch 689: Total Training Recognition Loss 0.07  Total Training Translation Loss 4.79 
2024-02-03 13:08:21,197 EPOCH 690
2024-02-03 13:08:34,600 Epoch 690: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.18 
2024-02-03 13:08:34,601 EPOCH 691
2024-02-03 13:08:47,677 Epoch 691: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.95 
2024-02-03 13:08:47,677 EPOCH 692
2024-02-03 13:09:00,801 Epoch 692: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.45 
2024-02-03 13:09:00,802 EPOCH 693
2024-02-03 13:09:15,433 Epoch 693: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.02 
2024-02-03 13:09:15,433 EPOCH 694
2024-02-03 13:09:28,783 Epoch 694: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.52 
2024-02-03 13:09:28,783 EPOCH 695
2024-02-03 13:09:30,497 [Epoch: 695 Step: 00011800] Batch Recognition Loss:   0.001254 => Gls Tokens per Sec:      747 || Batch Translation Loss:   0.049814 => Txt Tokens per Sec:     1971 || Lr: 0.000100
2024-02-03 13:09:42,079 Epoch 695: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.28 
2024-02-03 13:09:42,080 EPOCH 696
2024-02-03 13:09:55,306 Epoch 696: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.92 
2024-02-03 13:09:55,307 EPOCH 697
2024-02-03 13:10:08,457 Epoch 697: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.81 
2024-02-03 13:10:08,457 EPOCH 698
2024-02-03 13:10:21,710 Epoch 698: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.77 
2024-02-03 13:10:21,711 EPOCH 699
2024-02-03 13:10:34,968 Epoch 699: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.60 
2024-02-03 13:10:34,969 EPOCH 700
2024-02-03 13:10:48,454 [Epoch: 700 Step: 00011900] Batch Recognition Loss:   0.000949 => Gls Tokens per Sec:      788 || Batch Translation Loss:   0.029358 => Txt Tokens per Sec:     2192 || Lr: 0.000100
2024-02-03 13:10:48,455 Epoch 700: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.81 
2024-02-03 13:10:48,455 EPOCH 701
2024-02-03 13:11:01,559 Epoch 701: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.70 
2024-02-03 13:11:01,560 EPOCH 702
2024-02-03 13:11:15,150 Epoch 702: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.59 
2024-02-03 13:11:15,150 EPOCH 703
2024-02-03 13:11:28,489 Epoch 703: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.50 
2024-02-03 13:11:28,490 EPOCH 704
2024-02-03 13:11:41,746 Epoch 704: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.59 
2024-02-03 13:11:41,746 EPOCH 705
2024-02-03 13:11:54,978 Epoch 705: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.77 
2024-02-03 13:11:54,979 EPOCH 706
2024-02-03 13:12:07,616 [Epoch: 706 Step: 00012000] Batch Recognition Loss:   0.002340 => Gls Tokens per Sec:      740 || Batch Translation Loss:   0.142169 => Txt Tokens per Sec:     2081 || Lr: 0.000100
2024-02-03 13:12:56,118 Validation result at epoch 706, step    12000: duration: 48.5004s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.69887	Translation Loss: 89085.36719	PPL: 10433.09277
	Eval Metric: BLEU
	WER 4.10	(DEL: 0.00,	INS: 0.00,	SUB: 4.10)
	BLEU-4 0.81	(BLEU-1: 10.98,	BLEU-2: 3.75,	BLEU-3: 1.60,	BLEU-4: 0.81)
	CHRF 17.35	ROUGE 8.96
2024-02-03 13:12:56,120 Logging Recognition and Translation Outputs
2024-02-03 13:12:56,121 ========================================================================================================================
2024-02-03 13:12:56,121 Logging Sequence: 180_138.00
2024-02-03 13:12:56,122 	Gloss Reference :	A B+C+D+E
2024-02-03 13:12:56,122 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 13:12:56,122 	Gloss Alignment :	         
2024-02-03 13:12:56,122 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 13:12:56,124 	Text Reference  :	ioa president p t usha constituted a    seven-member panel   which included world champions from various sports to inquire into    the allegations
2024-02-03 13:12:56,125 	Text Hypothesis :	*** ********* * i have no          hard feelings     towards rohit sharma   and   he        will always  have   my full    support of  pride      
2024-02-03 13:12:56,125 	Text Alignment  :	D   D         D S S    S           S    S            S       S     S        S     S         S    S       S      S  S       S       S   S          
2024-02-03 13:12:56,125 ========================================================================================================================
2024-02-03 13:12:56,125 Logging Sequence: 126_99.00
2024-02-03 13:12:56,125 	Gloss Reference :	A B+C+D+E
2024-02-03 13:12:56,125 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 13:12:56,126 	Gloss Alignment :	         
2024-02-03 13:12:56,126 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 13:12:56,126 	Text Reference  :	*** he     dedicated the  medal to   sprinter milkha singh     
2024-02-03 13:12:56,126 	Text Hypothesis :	the nation is        very happy with a        very   particular
2024-02-03 13:12:56,127 	Text Alignment  :	I   S      S         S    S     S    S        S      S         
2024-02-03 13:12:56,127 ========================================================================================================================
2024-02-03 13:12:56,127 Logging Sequence: 90_146.00
2024-02-03 13:12:56,127 	Gloss Reference :	A B+C+D+E
2024-02-03 13:12:56,127 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 13:12:56,127 	Gloss Alignment :	         
2024-02-03 13:12:56,127 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 13:12:56,128 	Text Reference  :	similarly natasa and hardik both decided to   renew their vows 
2024-02-03 13:12:56,128 	Text Hypothesis :	********* i      am  sure   you  all     must have  seen  rooms
2024-02-03 13:12:56,129 	Text Alignment  :	D         S      S   S      S    S       S    S     S     S    
2024-02-03 13:12:56,129 ========================================================================================================================
2024-02-03 13:12:56,129 Logging Sequence: 79_198.00
2024-02-03 13:12:56,129 	Gloss Reference :	A B+C+D+E
2024-02-03 13:12:56,129 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 13:12:56,129 	Gloss Alignment :	         
2024-02-03 13:12:56,130 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 13:12:56,131 	Text Reference  :	will try to   reschedule the match before the finals now hope    the team  tests negative
2024-02-03 13:12:56,131 	Text Hypothesis :	**** all know that       in  2011  world  cup bcci   had allowed to  reach their decision
2024-02-03 13:12:56,131 	Text Alignment  :	D    S   S    S          S   S     S      S   S      S   S       S   S     S     S       
2024-02-03 13:12:56,131 ========================================================================================================================
2024-02-03 13:12:56,131 Logging Sequence: 138_182.00
2024-02-03 13:12:56,132 	Gloss Reference :	A B+C+D+E
2024-02-03 13:12:56,132 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 13:12:56,132 	Gloss Alignment :	         
2024-02-03 13:12:56,132 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 13:12:56,133 	Text Reference  :	** there is a    mural of  marcus  rashford's face in  machester
2024-02-03 13:12:56,133 	Text Hypothesis :	so then  in 2011 we    had allowed to         wear the country  
2024-02-03 13:12:56,133 	Text Alignment  :	I  S     S  S    S     S   S       S          S    S   S        
2024-02-03 13:12:56,134 ========================================================================================================================
2024-02-03 13:12:56,826 Epoch 706: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.93 
2024-02-03 13:12:56,826 EPOCH 707
2024-02-03 13:13:13,336 Epoch 707: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.96 
2024-02-03 13:13:13,337 EPOCH 708
2024-02-03 13:13:27,501 Epoch 708: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.96 
2024-02-03 13:13:27,502 EPOCH 709
2024-02-03 13:13:40,520 Epoch 709: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.91 
2024-02-03 13:13:40,520 EPOCH 710
2024-02-03 13:13:53,988 Epoch 710: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.96 
2024-02-03 13:13:53,989 EPOCH 711
2024-02-03 13:14:07,102 Epoch 711: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.90 
2024-02-03 13:14:07,103 EPOCH 712
2024-02-03 13:14:18,486 [Epoch: 712 Step: 00012100] Batch Recognition Loss:   0.001192 => Gls Tokens per Sec:      709 || Batch Translation Loss:   0.028417 => Txt Tokens per Sec:     2049 || Lr: 0.000100
2024-02-03 13:14:20,543 Epoch 712: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.75 
2024-02-03 13:14:20,543 EPOCH 713
2024-02-03 13:14:33,753 Epoch 713: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.77 
2024-02-03 13:14:33,754 EPOCH 714
2024-02-03 13:14:47,245 Epoch 714: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.81 
2024-02-03 13:14:47,246 EPOCH 715
2024-02-03 13:15:00,351 Epoch 715: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.84 
2024-02-03 13:15:00,352 EPOCH 716
2024-02-03 13:15:13,723 Epoch 716: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.83 
2024-02-03 13:15:13,724 EPOCH 717
2024-02-03 13:15:26,807 Epoch 717: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.20 
2024-02-03 13:15:26,808 EPOCH 718
2024-02-03 13:15:42,939 [Epoch: 718 Step: 00012200] Batch Recognition Loss:   0.005868 => Gls Tokens per Sec:      421 || Batch Translation Loss:   0.461009 => Txt Tokens per Sec:     1236 || Lr: 0.000100
2024-02-03 13:15:44,596 Epoch 718: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.46 
2024-02-03 13:15:44,597 EPOCH 719
2024-02-03 13:15:58,810 Epoch 719: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.06 
2024-02-03 13:15:58,811 EPOCH 720
2024-02-03 13:16:15,194 Epoch 720: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.93 
2024-02-03 13:16:15,195 EPOCH 721
2024-02-03 13:16:29,638 Epoch 721: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.04 
2024-02-03 13:16:29,639 EPOCH 722
2024-02-03 13:16:43,092 Epoch 722: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.04 
2024-02-03 13:16:43,092 EPOCH 723
2024-02-03 13:16:56,282 Epoch 723: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.03 
2024-02-03 13:16:56,282 EPOCH 724
2024-02-03 13:17:07,678 [Epoch: 724 Step: 00012300] Batch Recognition Loss:   0.000699 => Gls Tokens per Sec:      484 || Batch Translation Loss:   0.140201 => Txt Tokens per Sec:     1361 || Lr: 0.000100
2024-02-03 13:17:09,713 Epoch 724: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.31 
2024-02-03 13:17:09,713 EPOCH 725
2024-02-03 13:17:22,799 Epoch 725: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.07 
2024-02-03 13:17:22,800 EPOCH 726
2024-02-03 13:17:36,021 Epoch 726: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.37 
2024-02-03 13:17:36,021 EPOCH 727
2024-02-03 13:17:49,333 Epoch 727: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.02 
2024-02-03 13:17:49,333 EPOCH 728
2024-02-03 13:18:02,709 Epoch 728: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.93 
2024-02-03 13:18:02,709 EPOCH 729
2024-02-03 13:18:16,287 Epoch 729: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.76 
2024-02-03 13:18:16,288 EPOCH 730
2024-02-03 13:18:22,079 [Epoch: 730 Step: 00012400] Batch Recognition Loss:   0.001894 => Gls Tokens per Sec:      731 || Batch Translation Loss:   0.058415 => Txt Tokens per Sec:     2052 || Lr: 0.000100
2024-02-03 13:18:29,754 Epoch 730: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.84 
2024-02-03 13:18:29,754 EPOCH 731
2024-02-03 13:18:43,374 Epoch 731: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.87 
2024-02-03 13:18:43,375 EPOCH 732
2024-02-03 13:18:56,789 Epoch 732: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.73 
2024-02-03 13:18:56,790 EPOCH 733
2024-02-03 13:19:10,185 Epoch 733: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.80 
2024-02-03 13:19:10,186 EPOCH 734
2024-02-03 13:19:23,825 Epoch 734: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.07 
2024-02-03 13:19:23,826 EPOCH 735
2024-02-03 13:19:37,298 Epoch 735: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.76 
2024-02-03 13:19:37,299 EPOCH 736
2024-02-03 13:19:39,756 [Epoch: 736 Step: 00012500] Batch Recognition Loss:   0.002397 => Gls Tokens per Sec:     1303 || Batch Translation Loss:   0.053569 => Txt Tokens per Sec:     3486 || Lr: 0.000100
2024-02-03 13:19:50,780 Epoch 736: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.91 
2024-02-03 13:19:50,781 EPOCH 737
2024-02-03 13:20:04,036 Epoch 737: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.30 
2024-02-03 13:20:04,037 EPOCH 738
2024-02-03 13:20:17,424 Epoch 738: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.95 
2024-02-03 13:20:17,425 EPOCH 739
2024-02-03 13:20:30,779 Epoch 739: Total Training Recognition Loss 0.05  Total Training Translation Loss 4.45 
2024-02-03 13:20:30,780 EPOCH 740
2024-02-03 13:20:44,202 Epoch 740: Total Training Recognition Loss 0.18  Total Training Translation Loss 15.55 
2024-02-03 13:20:44,203 EPOCH 741
2024-02-03 13:20:57,185 Epoch 741: Total Training Recognition Loss 0.19  Total Training Translation Loss 8.83 
2024-02-03 13:20:57,186 EPOCH 742
2024-02-03 13:20:57,815 [Epoch: 742 Step: 00012600] Batch Recognition Loss:   0.005132 => Gls Tokens per Sec:     3057 || Batch Translation Loss:   0.250652 => Txt Tokens per Sec:     7938 || Lr: 0.000100
2024-02-03 13:21:10,682 Epoch 742: Total Training Recognition Loss 0.14  Total Training Translation Loss 4.64 
2024-02-03 13:21:10,683 EPOCH 743
2024-02-03 13:21:24,141 Epoch 743: Total Training Recognition Loss 0.12  Total Training Translation Loss 2.78 
2024-02-03 13:21:24,142 EPOCH 744
2024-02-03 13:21:37,451 Epoch 744: Total Training Recognition Loss 0.09  Total Training Translation Loss 1.49 
2024-02-03 13:21:37,452 EPOCH 745
2024-02-03 13:21:50,849 Epoch 745: Total Training Recognition Loss 0.06  Total Training Translation Loss 0.99 
2024-02-03 13:21:50,850 EPOCH 746
2024-02-03 13:22:03,802 Epoch 746: Total Training Recognition Loss 0.06  Total Training Translation Loss 0.67 
2024-02-03 13:22:03,803 EPOCH 747
2024-02-03 13:22:16,610 Epoch 747: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.65 
2024-02-03 13:22:16,610 EPOCH 748
2024-02-03 13:22:16,760 [Epoch: 748 Step: 00012700] Batch Recognition Loss:   0.002171 => Gls Tokens per Sec:     4324 || Batch Translation Loss:   0.027840 => Txt Tokens per Sec:    10054 || Lr: 0.000100
2024-02-03 13:22:30,073 Epoch 748: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.57 
2024-02-03 13:22:30,074 EPOCH 749
2024-02-03 13:22:43,335 Epoch 749: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.53 
2024-02-03 13:22:43,335 EPOCH 750
2024-02-03 13:22:56,851 Epoch 750: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.49 
2024-02-03 13:22:56,852 EPOCH 751
2024-02-03 13:23:10,078 Epoch 751: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.45 
2024-02-03 13:23:10,079 EPOCH 752
2024-02-03 13:23:23,365 Epoch 752: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.44 
2024-02-03 13:23:23,365 EPOCH 753
2024-02-03 13:23:36,393 [Epoch: 753 Step: 00012800] Batch Recognition Loss:   0.001614 => Gls Tokens per Sec:      767 || Batch Translation Loss:   0.017645 => Txt Tokens per Sec:     2108 || Lr: 0.000100
2024-02-03 13:23:36,857 Epoch 753: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.40 
2024-02-03 13:23:36,857 EPOCH 754
2024-02-03 13:23:50,280 Epoch 754: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.38 
2024-02-03 13:23:50,281 EPOCH 755
2024-02-03 13:24:03,675 Epoch 755: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.36 
2024-02-03 13:24:03,675 EPOCH 756
2024-02-03 13:24:16,819 Epoch 756: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.37 
2024-02-03 13:24:16,819 EPOCH 757
2024-02-03 13:24:30,253 Epoch 757: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.45 
2024-02-03 13:24:30,254 EPOCH 758
2024-02-03 13:24:43,456 Epoch 758: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.43 
2024-02-03 13:24:43,456 EPOCH 759
2024-02-03 13:24:54,791 [Epoch: 759 Step: 00012900] Batch Recognition Loss:   0.001334 => Gls Tokens per Sec:      769 || Batch Translation Loss:   0.029638 => Txt Tokens per Sec:     2133 || Lr: 0.000100
2024-02-03 13:24:56,862 Epoch 759: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.45 
2024-02-03 13:24:56,862 EPOCH 760
2024-02-03 13:25:09,909 Epoch 760: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.49 
2024-02-03 13:25:09,909 EPOCH 761
2024-02-03 13:25:23,278 Epoch 761: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.74 
2024-02-03 13:25:23,279 EPOCH 762
2024-02-03 13:25:36,446 Epoch 762: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.70 
2024-02-03 13:25:36,446 EPOCH 763
2024-02-03 13:25:49,729 Epoch 763: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.73 
2024-02-03 13:25:49,729 EPOCH 764
2024-02-03 13:26:02,672 Epoch 764: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.56 
2024-02-03 13:26:02,673 EPOCH 765
2024-02-03 13:26:11,850 [Epoch: 765 Step: 00013000] Batch Recognition Loss:   0.001938 => Gls Tokens per Sec:      810 || Batch Translation Loss:   0.074359 => Txt Tokens per Sec:     2244 || Lr: 0.000100
2024-02-03 13:26:15,735 Epoch 765: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.61 
2024-02-03 13:26:15,735 EPOCH 766
2024-02-03 13:26:34,000 Epoch 766: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.56 
2024-02-03 13:26:34,001 EPOCH 767
2024-02-03 13:26:49,462 Epoch 767: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.54 
2024-02-03 13:26:49,463 EPOCH 768
2024-02-03 13:27:04,203 Epoch 768: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.41 
2024-02-03 13:27:04,204 EPOCH 769
2024-02-03 13:27:18,798 Epoch 769: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.43 
2024-02-03 13:27:18,798 EPOCH 770
2024-02-03 13:27:40,866 Epoch 770: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.54 
2024-02-03 13:27:40,866 EPOCH 771
2024-02-03 13:28:02,448 [Epoch: 771 Step: 00013100] Batch Recognition Loss:   0.000699 => Gls Tokens per Sec:      285 || Batch Translation Loss:   0.087968 => Txt Tokens per Sec:      789 || Lr: 0.000100
2024-02-03 13:28:08,388 Epoch 771: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.59 
2024-02-03 13:28:08,388 EPOCH 772
2024-02-03 13:28:29,040 Epoch 772: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.82 
2024-02-03 13:28:29,041 EPOCH 773
2024-02-03 13:28:48,990 Epoch 773: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.90 
2024-02-03 13:28:48,992 EPOCH 774
2024-02-03 13:29:07,465 Epoch 774: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.82 
2024-02-03 13:29:07,467 EPOCH 775
2024-02-03 13:29:25,582 Epoch 775: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.75 
2024-02-03 13:29:25,584 EPOCH 776
2024-02-03 13:29:47,314 Epoch 776: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.93 
2024-02-03 13:29:47,315 EPOCH 777
2024-02-03 13:30:00,123 [Epoch: 777 Step: 00013200] Batch Recognition Loss:   0.004201 => Gls Tokens per Sec:      380 || Batch Translation Loss:   0.028112 => Txt Tokens per Sec:     1022 || Lr: 0.000100
2024-02-03 13:30:07,782 Epoch 777: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.02 
2024-02-03 13:30:07,782 EPOCH 778
2024-02-03 13:30:26,759 Epoch 778: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.84 
2024-02-03 13:30:26,760 EPOCH 779
2024-02-03 13:30:42,449 Epoch 779: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.84 
2024-02-03 13:30:42,451 EPOCH 780
2024-02-03 13:30:57,661 Epoch 780: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.76 
2024-02-03 13:30:57,661 EPOCH 781
2024-02-03 13:31:12,367 Epoch 781: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.76 
2024-02-03 13:31:12,367 EPOCH 782
2024-02-03 13:31:27,105 Epoch 782: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.92 
2024-02-03 13:31:27,106 EPOCH 783
2024-02-03 13:31:28,891 [Epoch: 783 Step: 00013300] Batch Recognition Loss:   0.000899 => Gls Tokens per Sec:     2152 || Batch Translation Loss:   0.055859 => Txt Tokens per Sec:     5936 || Lr: 0.000100
2024-02-03 13:31:43,638 Epoch 783: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.93 
2024-02-03 13:31:43,639 EPOCH 784
2024-02-03 13:32:01,774 Epoch 784: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.17 
2024-02-03 13:32:01,775 EPOCH 785
2024-02-03 13:32:20,419 Epoch 785: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.91 
2024-02-03 13:32:20,419 EPOCH 786
2024-02-03 13:32:34,592 Epoch 786: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.80 
2024-02-03 13:32:34,592 EPOCH 787
2024-02-03 13:32:49,749 Epoch 787: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.57 
2024-02-03 13:32:49,750 EPOCH 788
2024-02-03 13:33:06,452 Epoch 788: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.51 
2024-02-03 13:33:06,453 EPOCH 789
2024-02-03 13:33:07,250 [Epoch: 789 Step: 00013400] Batch Recognition Loss:   0.001103 => Gls Tokens per Sec:     3219 || Batch Translation Loss:   0.025350 => Txt Tokens per Sec:     7763 || Lr: 0.000100
2024-02-03 13:33:21,156 Epoch 789: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.56 
2024-02-03 13:33:21,157 EPOCH 790
2024-02-03 13:33:35,777 Epoch 790: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.57 
2024-02-03 13:33:35,778 EPOCH 791
2024-02-03 13:33:50,600 Epoch 791: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.48 
2024-02-03 13:33:50,601 EPOCH 792
2024-02-03 13:34:06,125 Epoch 792: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.39 
2024-02-03 13:34:06,126 EPOCH 793
2024-02-03 13:34:20,832 Epoch 793: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.39 
2024-02-03 13:34:20,833 EPOCH 794
2024-02-03 13:34:35,416 Epoch 794: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.44 
2024-02-03 13:34:35,417 EPOCH 795
2024-02-03 13:34:35,985 [Epoch: 795 Step: 00013500] Batch Recognition Loss:   0.000475 => Gls Tokens per Sec:     2258 || Batch Translation Loss:   0.020772 => Txt Tokens per Sec:     6325 || Lr: 0.000100
2024-02-03 13:34:50,853 Epoch 795: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.41 
2024-02-03 13:34:50,854 EPOCH 796
2024-02-03 13:35:06,660 Epoch 796: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.49 
2024-02-03 13:35:06,660 EPOCH 797
2024-02-03 13:35:21,555 Epoch 797: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.47 
2024-02-03 13:35:21,556 EPOCH 798
2024-02-03 13:35:36,558 Epoch 798: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.50 
2024-02-03 13:35:36,558 EPOCH 799
2024-02-03 13:35:49,975 Epoch 799: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.49 
2024-02-03 13:35:49,976 EPOCH 800
2024-02-03 13:36:03,600 [Epoch: 800 Step: 00013600] Batch Recognition Loss:   0.000679 => Gls Tokens per Sec:      780 || Batch Translation Loss:   0.034414 => Txt Tokens per Sec:     2170 || Lr: 0.000100
2024-02-03 13:36:03,601 Epoch 800: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.59 
2024-02-03 13:36:03,601 EPOCH 801
2024-02-03 13:36:18,350 Epoch 801: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.50 
2024-02-03 13:36:18,350 EPOCH 802
2024-02-03 13:36:31,856 Epoch 802: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.50 
2024-02-03 13:36:31,857 EPOCH 803
2024-02-03 13:36:45,015 Epoch 803: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.43 
2024-02-03 13:36:45,016 EPOCH 804
2024-02-03 13:36:58,267 Epoch 804: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.58 
2024-02-03 13:36:58,268 EPOCH 805
2024-02-03 13:37:11,692 Epoch 805: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.81 
2024-02-03 13:37:11,693 EPOCH 806
2024-02-03 13:37:24,600 [Epoch: 806 Step: 00013700] Batch Recognition Loss:   0.001126 => Gls Tokens per Sec:      724 || Batch Translation Loss:   0.131422 => Txt Tokens per Sec:     2003 || Lr: 0.000100
2024-02-03 13:37:25,133 Epoch 806: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.20 
2024-02-03 13:37:25,133 EPOCH 807
2024-02-03 13:37:38,352 Epoch 807: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.53 
2024-02-03 13:37:38,352 EPOCH 808
2024-02-03 13:37:51,920 Epoch 808: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.59 
2024-02-03 13:37:51,921 EPOCH 809
2024-02-03 13:38:05,309 Epoch 809: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.12 
2024-02-03 13:38:05,310 EPOCH 810
2024-02-03 13:38:18,650 Epoch 810: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.47 
2024-02-03 13:38:18,650 EPOCH 811
2024-02-03 13:38:32,009 Epoch 811: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.84 
2024-02-03 13:38:32,010 EPOCH 812
2024-02-03 13:38:40,439 [Epoch: 812 Step: 00013800] Batch Recognition Loss:   0.001584 => Gls Tokens per Sec:      957 || Batch Translation Loss:   0.189587 => Txt Tokens per Sec:     2663 || Lr: 0.000100
2024-02-03 13:38:45,234 Epoch 812: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.57 
2024-02-03 13:38:45,234 EPOCH 813
2024-02-03 13:38:58,716 Epoch 813: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.37 
2024-02-03 13:38:58,716 EPOCH 814
2024-02-03 13:39:12,050 Epoch 814: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.42 
2024-02-03 13:39:12,051 EPOCH 815
2024-02-03 13:39:24,847 Epoch 815: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.08 
2024-02-03 13:39:24,847 EPOCH 816
2024-02-03 13:39:38,588 Epoch 816: Total Training Recognition Loss 0.10  Total Training Translation Loss 8.02 
2024-02-03 13:39:38,589 EPOCH 817
2024-02-03 13:39:52,000 Epoch 817: Total Training Recognition Loss 0.09  Total Training Translation Loss 5.97 
2024-02-03 13:39:52,001 EPOCH 818
2024-02-03 13:39:58,538 [Epoch: 818 Step: 00013900] Batch Recognition Loss:   0.001629 => Gls Tokens per Sec:     1077 || Batch Translation Loss:   0.103126 => Txt Tokens per Sec:     2965 || Lr: 0.000100
2024-02-03 13:40:05,319 Epoch 818: Total Training Recognition Loss 0.09  Total Training Translation Loss 3.22 
2024-02-03 13:40:05,320 EPOCH 819
2024-02-03 13:40:18,410 Epoch 819: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.67 
2024-02-03 13:40:18,411 EPOCH 820
2024-02-03 13:40:32,020 Epoch 820: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.07 
2024-02-03 13:40:32,021 EPOCH 821
2024-02-03 13:40:45,630 Epoch 821: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.87 
2024-02-03 13:40:45,631 EPOCH 822
2024-02-03 13:40:58,878 Epoch 822: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.65 
2024-02-03 13:40:58,879 EPOCH 823
2024-02-03 13:41:12,388 Epoch 823: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.66 
2024-02-03 13:41:12,388 EPOCH 824
2024-02-03 13:41:18,423 [Epoch: 824 Step: 00014000] Batch Recognition Loss:   0.001846 => Gls Tokens per Sec:      955 || Batch Translation Loss:   0.029164 => Txt Tokens per Sec:     2759 || Lr: 0.000100
2024-02-03 13:42:06,554 Validation result at epoch 824, step    14000: duration: 48.1306s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.62929	Translation Loss: 87975.10938	PPL: 9296.77246
	Eval Metric: BLEU
	WER 4.17	(DEL: 0.00,	INS: 0.00,	SUB: 4.17)
	BLEU-4 0.66	(BLEU-1: 11.30,	BLEU-2: 3.69,	BLEU-3: 1.48,	BLEU-4: 0.66)
	CHRF 17.76	ROUGE 9.51
2024-02-03 13:42:06,556 Logging Recognition and Translation Outputs
2024-02-03 13:42:06,556 ========================================================================================================================
2024-02-03 13:42:06,556 Logging Sequence: 123_76.00
2024-02-03 13:42:06,556 	Gloss Reference :	A B+C+D+E
2024-02-03 13:42:06,556 	Gloss Hypothesis:	A B      
2024-02-03 13:42:06,556 	Gloss Alignment :	  S      
2024-02-03 13:42:06,557 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 13:42:06,557 	Text Reference  :	** **** here there are 
2024-02-03 13:42:06,557 	Text Hypothesis :	so what was  her   life
2024-02-03 13:42:06,557 	Text Alignment  :	I  I    S    S     S   
2024-02-03 13:42:06,557 ========================================================================================================================
2024-02-03 13:42:06,557 Logging Sequence: 87_224.00
2024-02-03 13:42:06,557 	Gloss Reference :	A B+C+D+E
2024-02-03 13:42:06,558 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 13:42:06,558 	Gloss Alignment :	         
2024-02-03 13:42:06,558 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 13:42:06,559 	Text Reference  :	******* ****** ** *** ****** so  i      lost   my     cool    and  it           was my    natural reaction
2024-02-03 13:42:06,559 	Text Hypothesis :	gambhir looked up and showed his middle finger before walking away nonchalantly the video went    viral   
2024-02-03 13:42:06,559 	Text Alignment  :	I       I      I  I   I      S   S      S      S      S       S    S            S   S     S       S       
2024-02-03 13:42:06,559 ========================================================================================================================
2024-02-03 13:42:06,559 Logging Sequence: 182_20.00
2024-02-03 13:42:06,560 	Gloss Reference :	A B+C+D+E
2024-02-03 13:42:06,560 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 13:42:06,560 	Gloss Alignment :	         
2024-02-03 13:42:06,560 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 13:42:06,562 	Text Reference  :	in 2019 june yuvraj shocked the world when he announced his    international retirement many people were sad      with the news
2024-02-03 13:42:06,562 	Text Hypothesis :	** **** **** ****** however the ***** **** ** bcci      posted a             picture    of   pant   and  welcomed him  to  god 
2024-02-03 13:42:06,562 	Text Alignment  :	D  D    D    D      S           D     D    D  S         S      S             S          S    S      S    S        S    S   S   
2024-02-03 13:42:06,562 ========================================================================================================================
2024-02-03 13:42:06,562 Logging Sequence: 116_133.00
2024-02-03 13:42:06,562 	Gloss Reference :	A B+C+D+E
2024-02-03 13:42:06,562 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 13:42:06,562 	Gloss Alignment :	         
2024-02-03 13:42:06,563 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 13:42:06,563 	Text Reference  :	he expressed his sadness in the    caption
2024-02-03 13:42:06,563 	Text Hypothesis :	** ********* *** it      is really bad    
2024-02-03 13:42:06,563 	Text Alignment  :	D  D         D   S       S  S      S      
2024-02-03 13:42:06,563 ========================================================================================================================
2024-02-03 13:42:06,563 Logging Sequence: 180_409.00
2024-02-03 13:42:06,564 	Gloss Reference :	A B+C+D+E
2024-02-03 13:42:06,564 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 13:42:06,564 	Gloss Alignment :	         
2024-02-03 13:42:06,564 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 13:42:06,565 	Text Reference  :	******* ******* **** ******** cricketer kapil dev       etc     have **** also shown support to the ******** wrestlers through their   tweets
2024-02-03 13:42:06,566 	Text Hypothesis :	several leaders from congress and       other political parties have come out  in    support of the athletes and       are     against singh 
2024-02-03 13:42:06,566 	Text Alignment  :	I       I       I    I        S         S     S         S            I    S    S             S      I        S         S       S       S     
2024-02-03 13:42:06,566 ========================================================================================================================
2024-02-03 13:42:13,728 Epoch 824: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.07 
2024-02-03 13:42:13,729 EPOCH 825
2024-02-03 13:42:27,174 Epoch 825: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.57 
2024-02-03 13:42:27,175 EPOCH 826
2024-02-03 13:42:40,519 Epoch 826: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.56 
2024-02-03 13:42:40,519 EPOCH 827
2024-02-03 13:42:53,906 Epoch 827: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.53 
2024-02-03 13:42:53,907 EPOCH 828
2024-02-03 13:43:07,118 Epoch 828: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.46 
2024-02-03 13:43:07,118 EPOCH 829
2024-02-03 13:43:20,667 Epoch 829: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.48 
2024-02-03 13:43:20,668 EPOCH 830
2024-02-03 13:43:23,356 [Epoch: 830 Step: 00014100] Batch Recognition Loss:   0.001204 => Gls Tokens per Sec:     1667 || Batch Translation Loss:   0.026101 => Txt Tokens per Sec:     4366 || Lr: 0.000100
2024-02-03 13:43:33,877 Epoch 830: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.41 
2024-02-03 13:43:33,877 EPOCH 831
2024-02-03 13:43:47,448 Epoch 831: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.42 
2024-02-03 13:43:47,448 EPOCH 832
2024-02-03 13:44:00,791 Epoch 832: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.43 
2024-02-03 13:44:00,792 EPOCH 833
2024-02-03 13:44:14,026 Epoch 833: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.40 
2024-02-03 13:44:14,027 EPOCH 834
2024-02-03 13:44:27,231 Epoch 834: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.41 
2024-02-03 13:44:27,232 EPOCH 835
2024-02-03 13:44:40,900 Epoch 835: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.37 
2024-02-03 13:44:40,901 EPOCH 836
2024-02-03 13:44:45,923 [Epoch: 836 Step: 00014200] Batch Recognition Loss:   0.000735 => Gls Tokens per Sec:      588 || Batch Translation Loss:   0.015535 => Txt Tokens per Sec:     1453 || Lr: 0.000100
2024-02-03 13:44:54,257 Epoch 836: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.46 
2024-02-03 13:44:54,258 EPOCH 837
2024-02-03 13:45:07,787 Epoch 837: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.40 
2024-02-03 13:45:07,787 EPOCH 838
2024-02-03 13:45:21,017 Epoch 838: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.40 
2024-02-03 13:45:21,017 EPOCH 839
2024-02-03 13:45:34,302 Epoch 839: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.52 
2024-02-03 13:45:34,303 EPOCH 840
2024-02-03 13:45:47,843 Epoch 840: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.35 
2024-02-03 13:45:47,844 EPOCH 841
2024-02-03 13:46:01,252 Epoch 841: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.38 
2024-02-03 13:46:01,253 EPOCH 842
2024-02-03 13:46:03,315 [Epoch: 842 Step: 00014300] Batch Recognition Loss:   0.000874 => Gls Tokens per Sec:      932 || Batch Translation Loss:   0.024773 => Txt Tokens per Sec:     2626 || Lr: 0.000100
2024-02-03 13:46:14,475 Epoch 842: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.39 
2024-02-03 13:46:14,475 EPOCH 843
2024-02-03 13:46:27,542 Epoch 843: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.37 
2024-02-03 13:46:27,543 EPOCH 844
2024-02-03 13:46:40,707 Epoch 844: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.43 
2024-02-03 13:46:40,708 EPOCH 845
2024-02-03 13:46:53,368 Epoch 845: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.50 
2024-02-03 13:46:53,368 EPOCH 846
2024-02-03 13:47:06,816 Epoch 846: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.41 
2024-02-03 13:47:06,816 EPOCH 847
2024-02-03 13:47:20,199 Epoch 847: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.45 
2024-02-03 13:47:20,200 EPOCH 848
2024-02-03 13:47:21,793 [Epoch: 848 Step: 00014400] Batch Recognition Loss:   0.001016 => Gls Tokens per Sec:      402 || Batch Translation Loss:   0.034106 => Txt Tokens per Sec:     1316 || Lr: 0.000100
2024-02-03 13:47:33,698 Epoch 848: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.48 
2024-02-03 13:47:33,699 EPOCH 849
2024-02-03 13:47:47,310 Epoch 849: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.50 
2024-02-03 13:47:47,311 EPOCH 850
2024-02-03 13:48:00,600 Epoch 850: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.61 
2024-02-03 13:48:00,600 EPOCH 851
2024-02-03 13:48:14,047 Epoch 851: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.70 
2024-02-03 13:48:14,048 EPOCH 852
2024-02-03 13:48:27,491 Epoch 852: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.74 
2024-02-03 13:48:27,491 EPOCH 853
2024-02-03 13:48:37,850 [Epoch: 853 Step: 00014500] Batch Recognition Loss:   0.001585 => Gls Tokens per Sec:      964 || Batch Translation Loss:   0.033885 => Txt Tokens per Sec:     2642 || Lr: 0.000100
2024-02-03 13:48:40,736 Epoch 853: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.92 
2024-02-03 13:48:40,737 EPOCH 854
2024-02-03 13:48:54,331 Epoch 854: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.93 
2024-02-03 13:48:54,331 EPOCH 855
2024-02-03 13:49:07,546 Epoch 855: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.99 
2024-02-03 13:49:07,546 EPOCH 856
2024-02-03 13:49:21,221 Epoch 856: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.05 
2024-02-03 13:49:21,221 EPOCH 857
2024-02-03 13:49:34,693 Epoch 857: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.16 
2024-02-03 13:49:34,693 EPOCH 858
2024-02-03 13:49:48,156 Epoch 858: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.89 
2024-02-03 13:49:48,157 EPOCH 859
2024-02-03 13:50:00,817 [Epoch: 859 Step: 00014600] Batch Recognition Loss:   0.005044 => Gls Tokens per Sec:      688 || Batch Translation Loss:   0.353629 => Txt Tokens per Sec:     1923 || Lr: 0.000100
2024-02-03 13:50:01,540 Epoch 859: Total Training Recognition Loss 0.17  Total Training Translation Loss 12.67 
2024-02-03 13:50:01,540 EPOCH 860
2024-02-03 13:50:14,712 Epoch 860: Total Training Recognition Loss 0.16  Total Training Translation Loss 5.02 
2024-02-03 13:50:14,712 EPOCH 861
2024-02-03 13:50:28,004 Epoch 861: Total Training Recognition Loss 0.15  Total Training Translation Loss 3.70 
2024-02-03 13:50:28,004 EPOCH 862
2024-02-03 13:50:41,483 Epoch 862: Total Training Recognition Loss 0.09  Total Training Translation Loss 2.29 
2024-02-03 13:50:41,484 EPOCH 863
2024-02-03 13:50:54,935 Epoch 863: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.41 
2024-02-03 13:50:54,936 EPOCH 864
2024-02-03 13:51:08,178 Epoch 864: Total Training Recognition Loss 0.06  Total Training Translation Loss 0.98 
2024-02-03 13:51:08,179 EPOCH 865
2024-02-03 13:51:20,653 [Epoch: 865 Step: 00014700] Batch Recognition Loss:   0.001931 => Gls Tokens per Sec:      596 || Batch Translation Loss:   0.046621 => Txt Tokens per Sec:     1690 || Lr: 0.000100
2024-02-03 13:51:21,736 Epoch 865: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.80 
2024-02-03 13:51:21,736 EPOCH 866
2024-02-03 13:51:34,962 Epoch 866: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.70 
2024-02-03 13:51:34,963 EPOCH 867
2024-02-03 13:51:48,216 Epoch 867: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.64 
2024-02-03 13:51:48,216 EPOCH 868
2024-02-03 13:52:01,279 Epoch 868: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.55 
2024-02-03 13:52:01,280 EPOCH 869
2024-02-03 13:52:14,672 Epoch 869: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.55 
2024-02-03 13:52:14,672 EPOCH 870
2024-02-03 13:52:28,120 Epoch 870: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.51 
2024-02-03 13:52:28,121 EPOCH 871
2024-02-03 13:52:31,722 [Epoch: 871 Step: 00014800] Batch Recognition Loss:   0.001203 => Gls Tokens per Sec:     1778 || Batch Translation Loss:   0.028554 => Txt Tokens per Sec:     4989 || Lr: 0.000100
2024-02-03 13:52:41,272 Epoch 871: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.40 
2024-02-03 13:52:41,273 EPOCH 872
2024-02-03 13:52:54,913 Epoch 872: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.39 
2024-02-03 13:52:54,914 EPOCH 873
2024-02-03 13:53:08,125 Epoch 873: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.37 
2024-02-03 13:53:08,126 EPOCH 874
2024-02-03 13:53:21,055 Epoch 874: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.37 
2024-02-03 13:53:21,055 EPOCH 875
2024-02-03 13:53:34,828 Epoch 875: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.38 
2024-02-03 13:53:34,829 EPOCH 876
2024-02-03 13:53:48,302 Epoch 876: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.55 
2024-02-03 13:53:48,303 EPOCH 877
2024-02-03 13:53:58,310 [Epoch: 877 Step: 00014900] Batch Recognition Loss:   0.000238 => Gls Tokens per Sec:      487 || Batch Translation Loss:   0.018056 => Txt Tokens per Sec:     1472 || Lr: 0.000100
2024-02-03 13:54:01,562 Epoch 877: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.65 
2024-02-03 13:54:01,563 EPOCH 878
2024-02-03 13:54:14,821 Epoch 878: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.34 
2024-02-03 13:54:14,822 EPOCH 879
2024-02-03 13:54:29,976 Epoch 879: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.32 
2024-02-03 13:54:29,977 EPOCH 880
2024-02-03 13:54:43,254 Epoch 880: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.31 
2024-02-03 13:54:43,255 EPOCH 881
2024-02-03 13:54:56,910 Epoch 881: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.34 
2024-02-03 13:54:56,911 EPOCH 882
2024-02-03 13:55:10,376 Epoch 882: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.46 
2024-02-03 13:55:10,376 EPOCH 883
2024-02-03 13:55:12,941 [Epoch: 883 Step: 00015000] Batch Recognition Loss:   0.001535 => Gls Tokens per Sec:     1498 || Batch Translation Loss:   0.022917 => Txt Tokens per Sec:     4266 || Lr: 0.000100
2024-02-03 13:55:23,534 Epoch 883: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.58 
2024-02-03 13:55:23,534 EPOCH 884
2024-02-03 13:55:36,960 Epoch 884: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.86 
2024-02-03 13:55:36,960 EPOCH 885
2024-02-03 13:55:50,294 Epoch 885: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.53 
2024-02-03 13:55:50,294 EPOCH 886
2024-02-03 13:56:03,603 Epoch 886: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.63 
2024-02-03 13:56:03,604 EPOCH 887
2024-02-03 13:56:17,167 Epoch 887: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.84 
2024-02-03 13:56:17,168 EPOCH 888
2024-02-03 13:56:30,633 Epoch 888: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.86 
2024-02-03 13:56:30,634 EPOCH 889
2024-02-03 13:56:31,692 [Epoch: 889 Step: 00015100] Batch Recognition Loss:   0.000582 => Gls Tokens per Sec:     2424 || Batch Translation Loss:   0.048842 => Txt Tokens per Sec:     6953 || Lr: 0.000100
2024-02-03 13:56:43,928 Epoch 889: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.64 
2024-02-03 13:56:43,929 EPOCH 890
2024-02-03 13:56:57,432 Epoch 890: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.74 
2024-02-03 13:56:57,433 EPOCH 891
2024-02-03 13:57:10,674 Epoch 891: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.64 
2024-02-03 13:57:10,675 EPOCH 892
2024-02-03 13:57:24,115 Epoch 892: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.67 
2024-02-03 13:57:24,115 EPOCH 893
2024-02-03 13:57:37,398 Epoch 893: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.68 
2024-02-03 13:57:37,399 EPOCH 894
2024-02-03 13:57:50,889 Epoch 894: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.65 
2024-02-03 13:57:50,889 EPOCH 895
2024-02-03 13:57:55,412 [Epoch: 895 Step: 00015200] Batch Recognition Loss:   0.000788 => Gls Tokens per Sec:      228 || Batch Translation Loss:   0.080257 => Txt Tokens per Sec:      762 || Lr: 0.000100
2024-02-03 13:58:04,289 Epoch 895: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.89 
2024-02-03 13:58:04,289 EPOCH 896
2024-02-03 13:58:17,529 Epoch 896: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.90 
2024-02-03 13:58:17,530 EPOCH 897
2024-02-03 13:58:31,041 Epoch 897: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.02 
2024-02-03 13:58:31,042 EPOCH 898
2024-02-03 13:58:44,326 Epoch 898: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.76 
2024-02-03 13:58:44,326 EPOCH 899
2024-02-03 13:58:57,592 Epoch 899: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.83 
2024-02-03 13:58:57,592 EPOCH 900
2024-02-03 13:59:10,971 [Epoch: 900 Step: 00015300] Batch Recognition Loss:   0.001622 => Gls Tokens per Sec:      795 || Batch Translation Loss:   0.092885 => Txt Tokens per Sec:     2210 || Lr: 0.000100
2024-02-03 13:59:10,972 Epoch 900: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.01 
2024-02-03 13:59:10,972 EPOCH 901
2024-02-03 13:59:24,410 Epoch 901: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.10 
2024-02-03 13:59:24,411 EPOCH 902
2024-02-03 13:59:37,735 Epoch 902: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.22 
2024-02-03 13:59:37,736 EPOCH 903
2024-02-03 13:59:51,140 Epoch 903: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.06 
2024-02-03 13:59:51,141 EPOCH 904
2024-02-03 14:00:04,628 Epoch 904: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.04 
2024-02-03 14:00:04,629 EPOCH 905
2024-02-03 14:00:18,327 Epoch 905: Total Training Recognition Loss 0.06  Total Training Translation Loss 0.86 
2024-02-03 14:00:18,328 EPOCH 906
2024-02-03 14:00:28,363 [Epoch: 906 Step: 00015400] Batch Recognition Loss:   0.001335 => Gls Tokens per Sec:      932 || Batch Translation Loss:   0.050539 => Txt Tokens per Sec:     2518 || Lr: 0.000100
2024-02-03 14:00:31,659 Epoch 906: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.90 
2024-02-03 14:00:31,659 EPOCH 907
2024-02-03 14:00:45,274 Epoch 907: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.96 
2024-02-03 14:00:45,275 EPOCH 908
2024-02-03 14:00:58,727 Epoch 908: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.09 
2024-02-03 14:00:58,727 EPOCH 909
2024-02-03 14:01:11,751 Epoch 909: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.15 
2024-02-03 14:01:11,752 EPOCH 910
2024-02-03 14:01:25,344 Epoch 910: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.11 
2024-02-03 14:01:25,344 EPOCH 911
2024-02-03 14:01:38,470 Epoch 911: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.05 
2024-02-03 14:01:38,470 EPOCH 912
2024-02-03 14:01:51,168 [Epoch: 912 Step: 00015500] Batch Recognition Loss:   0.001161 => Gls Tokens per Sec:      636 || Batch Translation Loss:   0.032538 => Txt Tokens per Sec:     1788 || Lr: 0.000100
2024-02-03 14:01:52,101 Epoch 912: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.85 
2024-02-03 14:01:52,101 EPOCH 913
2024-02-03 14:02:05,121 Epoch 913: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.08 
2024-02-03 14:02:05,121 EPOCH 914
2024-02-03 14:02:18,818 Epoch 914: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.53 
2024-02-03 14:02:18,819 EPOCH 915
2024-02-03 14:02:31,886 Epoch 915: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.82 
2024-02-03 14:02:31,887 EPOCH 916
2024-02-03 14:02:44,911 Epoch 916: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.63 
2024-02-03 14:02:44,912 EPOCH 917
2024-02-03 14:02:58,326 Epoch 917: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.48 
2024-02-03 14:02:58,326 EPOCH 918
2024-02-03 14:03:07,531 [Epoch: 918 Step: 00015600] Batch Recognition Loss:   0.000692 => Gls Tokens per Sec:      738 || Batch Translation Loss:   0.072722 => Txt Tokens per Sec:     2060 || Lr: 0.000100
2024-02-03 14:03:11,452 Epoch 918: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.68 
2024-02-03 14:03:11,453 EPOCH 919
2024-02-03 14:03:25,030 Epoch 919: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.68 
2024-02-03 14:03:25,031 EPOCH 920
2024-02-03 14:03:38,197 Epoch 920: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.16 
2024-02-03 14:03:38,198 EPOCH 921
2024-02-03 14:03:51,719 Epoch 921: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.43 
2024-02-03 14:03:51,720 EPOCH 922
2024-02-03 14:04:04,875 Epoch 922: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.40 
2024-02-03 14:04:04,876 EPOCH 923
2024-02-03 14:04:18,252 Epoch 923: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.17 
2024-02-03 14:04:18,253 EPOCH 924
2024-02-03 14:04:24,627 [Epoch: 924 Step: 00015700] Batch Recognition Loss:   0.000595 => Gls Tokens per Sec:      904 || Batch Translation Loss:   0.029571 => Txt Tokens per Sec:     2633 || Lr: 0.000100
2024-02-03 14:04:31,816 Epoch 924: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.98 
2024-02-03 14:04:31,816 EPOCH 925
2024-02-03 14:04:44,872 Epoch 925: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.67 
2024-02-03 14:04:44,873 EPOCH 926
2024-02-03 14:04:58,242 Epoch 926: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.69 
2024-02-03 14:04:58,243 EPOCH 927
2024-02-03 14:05:11,557 Epoch 927: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.66 
2024-02-03 14:05:11,558 EPOCH 928
2024-02-03 14:05:25,102 Epoch 928: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.69 
2024-02-03 14:05:25,102 EPOCH 929
2024-02-03 14:05:38,474 Epoch 929: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.56 
2024-02-03 14:05:38,475 EPOCH 930
2024-02-03 14:05:41,493 [Epoch: 930 Step: 00015800] Batch Recognition Loss:   0.002790 => Gls Tokens per Sec:     1485 || Batch Translation Loss:   0.016808 => Txt Tokens per Sec:     4003 || Lr: 0.000100
2024-02-03 14:05:51,989 Epoch 930: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.54 
2024-02-03 14:05:51,990 EPOCH 931
2024-02-03 14:06:05,546 Epoch 931: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.47 
2024-02-03 14:06:05,547 EPOCH 932
2024-02-03 14:06:18,772 Epoch 932: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.54 
2024-02-03 14:06:18,773 EPOCH 933
2024-02-03 14:06:32,098 Epoch 933: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.42 
2024-02-03 14:06:32,098 EPOCH 934
2024-02-03 14:06:45,303 Epoch 934: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.46 
2024-02-03 14:06:45,304 EPOCH 935
2024-02-03 14:06:58,378 Epoch 935: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.47 
2024-02-03 14:06:58,379 EPOCH 936
2024-02-03 14:07:05,049 [Epoch: 936 Step: 00015900] Batch Recognition Loss:   0.000297 => Gls Tokens per Sec:      442 || Batch Translation Loss:   0.016746 => Txt Tokens per Sec:     1301 || Lr: 0.000100
2024-02-03 14:07:11,859 Epoch 936: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.52 
2024-02-03 14:07:11,860 EPOCH 937
2024-02-03 14:07:25,442 Epoch 937: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.48 
2024-02-03 14:07:25,443 EPOCH 938
2024-02-03 14:07:38,687 Epoch 938: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.48 
2024-02-03 14:07:38,688 EPOCH 939
2024-02-03 14:07:52,007 Epoch 939: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.51 
2024-02-03 14:07:52,008 EPOCH 940
2024-02-03 14:08:05,861 Epoch 940: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.65 
2024-02-03 14:08:05,862 EPOCH 941
2024-02-03 14:08:19,168 Epoch 941: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.83 
2024-02-03 14:08:19,169 EPOCH 942
2024-02-03 14:08:20,008 [Epoch: 942 Step: 00016000] Batch Recognition Loss:   0.003975 => Gls Tokens per Sec:     2290 || Batch Translation Loss:   0.084366 => Txt Tokens per Sec:     6537 || Lr: 0.000100
2024-02-03 14:09:09,045 Validation result at epoch 942, step    16000: duration: 49.0370s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.68988	Translation Loss: 87283.57812	PPL: 8652.44824
	Eval Metric: BLEU
	WER 4.10	(DEL: 0.07,	INS: 0.00,	SUB: 4.03)
	BLEU-4 0.64	(BLEU-1: 10.63,	BLEU-2: 3.49,	BLEU-3: 1.31,	BLEU-4: 0.64)
	CHRF 17.42	ROUGE 8.68
2024-02-03 14:09:09,046 Logging Recognition and Translation Outputs
2024-02-03 14:09:09,046 ========================================================================================================================
2024-02-03 14:09:09,047 Logging Sequence: 182_20.00
2024-02-03 14:09:09,047 	Gloss Reference :	A B+C+D+E
2024-02-03 14:09:09,047 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 14:09:09,047 	Gloss Alignment :	         
2024-02-03 14:09:09,047 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 14:09:09,050 	Text Reference  :	************** ******** * in  2019 june   yuvraj shocked the world when he    announced his  international retirement many        people were  sad with        the    news  
2024-02-03 14:09:09,050 	Text Hypothesis :	praggnanandhaa received a lot of   mental health and     is  going to   coach announced they will          help       encouraging the    youth of  uttarakhand toward sports
2024-02-03 14:09:09,050 	Text Alignment  :	I              I        I S   S    S      S      S       S   S     S    S               S    S             S          S           S      S     S   S           S      S     
2024-02-03 14:09:09,050 ========================================================================================================================
2024-02-03 14:09:09,050 Logging Sequence: 180_409.00
2024-02-03 14:09:09,050 	Gloss Reference :	A B+C+D+E
2024-02-03 14:09:09,050 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 14:09:09,051 	Gloss Alignment :	         
2024-02-03 14:09:09,051 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 14:09:09,052 	Text Reference  :	***** ****** cricketer kapil  dev etc    have also shown support to **** the wrestlers through their tweets
2024-02-03 14:09:09,052 	Text Hypothesis :	since stokes had       broken his finger he   was  not   able    to file the wrestlers ******* ***** ******
2024-02-03 14:09:09,052 	Text Alignment  :	I     I      S         S      S   S      S    S    S     S          I                  D       D     D     
2024-02-03 14:09:09,052 ========================================================================================================================
2024-02-03 14:09:09,052 Logging Sequence: 59_2.00
2024-02-03 14:09:09,053 	Gloss Reference :	A B+C+D+E
2024-02-03 14:09:09,053 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 14:09:09,053 	Gloss Alignment :	         
2024-02-03 14:09:09,053 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 14:09:09,053 	Text Reference  :	at  the 2020      tokyo olympics in  japan
2024-02-03 14:09:09,053 	Text Hypothesis :	she was extremely happy with     her face 
2024-02-03 14:09:09,054 	Text Alignment  :	S   S   S         S     S        S   S    
2024-02-03 14:09:09,054 ========================================================================================================================
2024-02-03 14:09:09,054 Logging Sequence: 118_100.00
2024-02-03 14:09:09,054 	Gloss Reference :	A B+C+D+E
2024-02-03 14:09:09,054 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 14:09:09,054 	Gloss Alignment :	         
2024-02-03 14:09:09,054 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 14:09:09,055 	Text Reference  :	while the  french were        very heartbroken by      this  
2024-02-03 14:09:09,055 	Text Hypothesis :	***** this was    accompanied by   four        morning attack
2024-02-03 14:09:09,055 	Text Alignment  :	D     S    S      S           S    S           S       S     
2024-02-03 14:09:09,055 ========================================================================================================================
2024-02-03 14:09:09,055 Logging Sequence: 69_204.00
2024-02-03 14:09:09,056 	Gloss Reference :	A B+C+D+E
2024-02-03 14:09:09,056 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 14:09:09,056 	Gloss Alignment :	         
2024-02-03 14:09:09,056 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 14:09:09,056 	Text Reference  :	however i am hoping to  play the     next   season'
2024-02-03 14:09:09,056 	Text Hypothesis :	******* * ** ****** csk have finally called upon   
2024-02-03 14:09:09,057 	Text Alignment  :	D       D D  D      S   S    S       S      S      
2024-02-03 14:09:09,057 ========================================================================================================================
2024-02-03 14:09:22,884 Epoch 942: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.55 
2024-02-03 14:09:22,884 EPOCH 943
2024-02-03 14:09:36,330 Epoch 943: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.62 
2024-02-03 14:09:36,331 EPOCH 944
2024-02-03 14:09:49,786 Epoch 944: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.54 
2024-02-03 14:09:49,787 EPOCH 945
2024-02-03 14:10:03,288 Epoch 945: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.46 
2024-02-03 14:10:03,289 EPOCH 946
2024-02-03 14:10:16,706 Epoch 946: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.32 
2024-02-03 14:10:16,707 EPOCH 947
2024-02-03 14:10:29,940 Epoch 947: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.89 
2024-02-03 14:10:29,940 EPOCH 948
2024-02-03 14:10:30,296 [Epoch: 948 Step: 00016100] Batch Recognition Loss:   0.001881 => Gls Tokens per Sec:     1803 || Batch Translation Loss:   0.087351 => Txt Tokens per Sec:     5820 || Lr: 0.000100
2024-02-03 14:10:43,529 Epoch 948: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.93 
2024-02-03 14:10:43,530 EPOCH 949
2024-02-03 14:10:57,018 Epoch 949: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.84 
2024-02-03 14:10:57,019 EPOCH 950
2024-02-03 14:11:10,285 Epoch 950: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.96 
2024-02-03 14:11:10,286 EPOCH 951
2024-02-03 14:11:23,863 Epoch 951: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.73 
2024-02-03 14:11:23,864 EPOCH 952
2024-02-03 14:11:37,247 Epoch 952: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.57 
2024-02-03 14:11:37,248 EPOCH 953
2024-02-03 14:11:50,430 [Epoch: 953 Step: 00016200] Batch Recognition Loss:   0.001905 => Gls Tokens per Sec:      758 || Batch Translation Loss:   0.032396 => Txt Tokens per Sec:     2119 || Lr: 0.000100
2024-02-03 14:11:50,661 Epoch 953: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.55 
2024-02-03 14:11:50,662 EPOCH 954
2024-02-03 14:12:03,979 Epoch 954: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.60 
2024-02-03 14:12:03,980 EPOCH 955
2024-02-03 14:12:17,336 Epoch 955: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-03 14:12:17,337 EPOCH 956
2024-02-03 14:12:30,736 Epoch 956: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.70 
2024-02-03 14:12:30,737 EPOCH 957
2024-02-03 14:12:44,300 Epoch 957: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.59 
2024-02-03 14:12:44,301 EPOCH 958
2024-02-03 14:12:57,672 Epoch 958: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.55 
2024-02-03 14:12:57,672 EPOCH 959
2024-02-03 14:13:06,118 [Epoch: 959 Step: 00016300] Batch Recognition Loss:   0.004705 => Gls Tokens per Sec:     1061 || Batch Translation Loss:   0.010693 => Txt Tokens per Sec:     2848 || Lr: 0.000100
2024-02-03 14:13:12,553 Epoch 959: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.62 
2024-02-03 14:13:12,553 EPOCH 960
2024-02-03 14:13:26,072 Epoch 960: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.69 
2024-02-03 14:13:26,073 EPOCH 961
2024-02-03 14:13:39,342 Epoch 961: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.93 
2024-02-03 14:13:39,343 EPOCH 962
2024-02-03 14:13:52,494 Epoch 962: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.87 
2024-02-03 14:13:52,495 EPOCH 963
2024-02-03 14:14:05,880 Epoch 963: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.82 
2024-02-03 14:14:05,880 EPOCH 964
2024-02-03 14:14:19,182 Epoch 964: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.58 
2024-02-03 14:14:19,182 EPOCH 965
2024-02-03 14:14:31,244 [Epoch: 965 Step: 00016400] Batch Recognition Loss:   0.001874 => Gls Tokens per Sec:      616 || Batch Translation Loss:   0.028723 => Txt Tokens per Sec:     1741 || Lr: 0.000100
2024-02-03 14:14:32,522 Epoch 965: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.47 
2024-02-03 14:14:32,522 EPOCH 966
2024-02-03 14:14:45,612 Epoch 966: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.50 
2024-02-03 14:14:45,613 EPOCH 967
2024-02-03 14:14:58,943 Epoch 967: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.45 
2024-02-03 14:14:58,944 EPOCH 968
2024-02-03 14:15:12,145 Epoch 968: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.55 
2024-02-03 14:15:12,145 EPOCH 969
2024-02-03 14:15:25,573 Epoch 969: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.82 
2024-02-03 14:15:25,574 EPOCH 970
2024-02-03 14:15:39,035 Epoch 970: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.97 
2024-02-03 14:15:39,035 EPOCH 971
2024-02-03 14:15:42,376 [Epoch: 971 Step: 00016500] Batch Recognition Loss:   0.000425 => Gls Tokens per Sec:     1916 || Batch Translation Loss:   0.062642 => Txt Tokens per Sec:     5224 || Lr: 0.000100
2024-02-03 14:15:52,109 Epoch 971: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.32 
2024-02-03 14:15:52,110 EPOCH 972
2024-02-03 14:16:05,361 Epoch 972: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.65 
2024-02-03 14:16:05,362 EPOCH 973
2024-02-03 14:16:18,485 Epoch 973: Total Training Recognition Loss 0.58  Total Training Translation Loss 17.11 
2024-02-03 14:16:18,486 EPOCH 974
2024-02-03 14:16:31,707 Epoch 974: Total Training Recognition Loss 2.30  Total Training Translation Loss 17.59 
2024-02-03 14:16:31,708 EPOCH 975
2024-02-03 14:16:44,911 Epoch 975: Total Training Recognition Loss 3.84  Total Training Translation Loss 11.41 
2024-02-03 14:16:44,911 EPOCH 976
2024-02-03 14:16:58,380 Epoch 976: Total Training Recognition Loss 6.63  Total Training Translation Loss 9.15 
2024-02-03 14:16:58,380 EPOCH 977
2024-02-03 14:17:08,435 [Epoch: 977 Step: 00016600] Batch Recognition Loss:   0.008579 => Gls Tokens per Sec:      484 || Batch Translation Loss:   0.191661 => Txt Tokens per Sec:     1418 || Lr: 0.000100
2024-02-03 14:17:11,902 Epoch 977: Total Training Recognition Loss 5.95  Total Training Translation Loss 5.28 
2024-02-03 14:17:11,902 EPOCH 978
2024-02-03 14:17:25,376 Epoch 978: Total Training Recognition Loss 1.67  Total Training Translation Loss 3.02 
2024-02-03 14:17:25,377 EPOCH 979
2024-02-03 14:17:38,583 Epoch 979: Total Training Recognition Loss 0.42  Total Training Translation Loss 1.68 
2024-02-03 14:17:38,584 EPOCH 980
2024-02-03 14:17:52,113 Epoch 980: Total Training Recognition Loss 0.15  Total Training Translation Loss 1.05 
2024-02-03 14:17:52,114 EPOCH 981
2024-02-03 14:18:05,729 Epoch 981: Total Training Recognition Loss 0.09  Total Training Translation Loss 0.69 
2024-02-03 14:18:05,730 EPOCH 982
2024-02-03 14:18:18,790 Epoch 982: Total Training Recognition Loss 0.07  Total Training Translation Loss 0.62 
2024-02-03 14:18:18,790 EPOCH 983
2024-02-03 14:18:20,127 [Epoch: 983 Step: 00016700] Batch Recognition Loss:   0.002894 => Gls Tokens per Sec:     2874 || Batch Translation Loss:   0.035390 => Txt Tokens per Sec:     8172 || Lr: 0.000100
2024-02-03 14:18:32,057 Epoch 983: Total Training Recognition Loss 0.08  Total Training Translation Loss 0.53 
2024-02-03 14:18:32,058 EPOCH 984
2024-02-03 14:18:45,236 Epoch 984: Total Training Recognition Loss 0.07  Total Training Translation Loss 0.53 
2024-02-03 14:18:45,236 EPOCH 985
2024-02-03 14:18:58,486 Epoch 985: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.51 
2024-02-03 14:18:58,486 EPOCH 986
2024-02-03 14:19:11,945 Epoch 986: Total Training Recognition Loss 0.06  Total Training Translation Loss 0.44 
2024-02-03 14:19:11,946 EPOCH 987
2024-02-03 14:19:25,490 Epoch 987: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.44 
2024-02-03 14:19:25,491 EPOCH 988
2024-02-03 14:19:38,955 Epoch 988: Total Training Recognition Loss 0.06  Total Training Translation Loss 0.42 
2024-02-03 14:19:38,955 EPOCH 989
2024-02-03 14:19:39,851 [Epoch: 989 Step: 00016800] Batch Recognition Loss:   0.002204 => Gls Tokens per Sec:     2859 || Batch Translation Loss:   0.031928 => Txt Tokens per Sec:     7531 || Lr: 0.000100
2024-02-03 14:19:52,220 Epoch 989: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.44 
2024-02-03 14:19:52,221 EPOCH 990
2024-02-03 14:20:05,835 Epoch 990: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.40 
2024-02-03 14:20:05,836 EPOCH 991
2024-02-03 14:20:18,897 Epoch 991: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.45 
2024-02-03 14:20:18,898 EPOCH 992
2024-02-03 14:20:32,356 Epoch 992: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.41 
2024-02-03 14:20:32,357 EPOCH 993
2024-02-03 14:20:45,492 Epoch 993: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.41 
2024-02-03 14:20:45,492 EPOCH 994
2024-02-03 14:20:58,940 Epoch 994: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.36 
2024-02-03 14:20:58,941 EPOCH 995
2024-02-03 14:21:03,335 [Epoch: 995 Step: 00016900] Batch Recognition Loss:   0.008527 => Gls Tokens per Sec:      234 || Batch Translation Loss:   0.036639 => Txt Tokens per Sec:      747 || Lr: 0.000100
2024-02-03 14:21:12,074 Epoch 995: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.40 
2024-02-03 14:21:12,075 EPOCH 996
2024-02-03 14:21:25,510 Epoch 996: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.33 
2024-02-03 14:21:25,511 EPOCH 997
2024-02-03 14:21:38,865 Epoch 997: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.33 
2024-02-03 14:21:38,865 EPOCH 998
2024-02-03 14:21:51,931 Epoch 998: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.31 
2024-02-03 14:21:51,931 EPOCH 999
2024-02-03 14:22:05,393 Epoch 999: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.30 
2024-02-03 14:22:05,393 EPOCH 1000
2024-02-03 14:22:18,595 [Epoch: 1000 Step: 00017000] Batch Recognition Loss:   0.003143 => Gls Tokens per Sec:      805 || Batch Translation Loss:   0.032582 => Txt Tokens per Sec:     2239 || Lr: 0.000100
2024-02-03 14:22:18,596 Epoch 1000: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.29 
2024-02-03 14:22:18,596 EPOCH 1001
2024-02-03 14:22:31,563 Epoch 1001: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.32 
2024-02-03 14:22:31,563 EPOCH 1002
2024-02-03 14:22:44,701 Epoch 1002: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.32 
2024-02-03 14:22:44,701 EPOCH 1003
2024-02-03 14:22:58,131 Epoch 1003: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.37 
2024-02-03 14:22:58,132 EPOCH 1004
2024-02-03 14:23:11,502 Epoch 1004: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.30 
2024-02-03 14:23:11,503 EPOCH 1005
2024-02-03 14:23:24,876 Epoch 1005: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.31 
2024-02-03 14:23:24,876 EPOCH 1006
2024-02-03 14:23:35,036 [Epoch: 1006 Step: 00017100] Batch Recognition Loss:   0.000579 => Gls Tokens per Sec:      921 || Batch Translation Loss:   0.020115 => Txt Tokens per Sec:     2511 || Lr: 0.000100
2024-02-03 14:23:38,249 Epoch 1006: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.29 
2024-02-03 14:23:38,249 EPOCH 1007
2024-02-03 14:23:51,435 Epoch 1007: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.33 
2024-02-03 14:23:51,436 EPOCH 1008
2024-02-03 14:24:05,084 Epoch 1008: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.37 
2024-02-03 14:24:05,085 EPOCH 1009
2024-02-03 14:24:18,417 Epoch 1009: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.31 
2024-02-03 14:24:18,417 EPOCH 1010
2024-02-03 14:24:31,759 Epoch 1010: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.27 
2024-02-03 14:24:31,760 EPOCH 1011
2024-02-03 14:24:44,952 Epoch 1011: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.31 
2024-02-03 14:24:44,953 EPOCH 1012
2024-02-03 14:24:50,629 [Epoch: 1012 Step: 00017200] Batch Recognition Loss:   0.000765 => Gls Tokens per Sec:     1466 || Batch Translation Loss:   0.008278 => Txt Tokens per Sec:     4005 || Lr: 0.000100
2024-02-03 14:24:58,073 Epoch 1012: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.33 
2024-02-03 14:24:58,074 EPOCH 1013
2024-02-03 14:25:11,218 Epoch 1013: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.34 
2024-02-03 14:25:11,219 EPOCH 1014
2024-02-03 14:25:24,662 Epoch 1014: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.36 
2024-02-03 14:25:24,663 EPOCH 1015
2024-02-03 14:25:37,729 Epoch 1015: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.42 
2024-02-03 14:25:37,729 EPOCH 1016
2024-02-03 14:25:51,017 Epoch 1016: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.35 
2024-02-03 14:25:51,017 EPOCH 1017
2024-02-03 14:26:03,980 Epoch 1017: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.34 
2024-02-03 14:26:03,980 EPOCH 1018
2024-02-03 14:26:11,844 [Epoch: 1018 Step: 00017300] Batch Recognition Loss:   0.000781 => Gls Tokens per Sec:      864 || Batch Translation Loss:   0.015327 => Txt Tokens per Sec:     2412 || Lr: 0.000100
2024-02-03 14:26:17,010 Epoch 1018: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.35 
2024-02-03 14:26:17,010 EPOCH 1019
2024-02-03 14:26:30,559 Epoch 1019: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.33 
2024-02-03 14:26:30,560 EPOCH 1020
2024-02-03 14:26:44,014 Epoch 1020: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.35 
2024-02-03 14:26:44,014 EPOCH 1021
2024-02-03 14:26:57,383 Epoch 1021: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.41 
2024-02-03 14:26:57,384 EPOCH 1022
2024-02-03 14:27:10,606 Epoch 1022: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-03 14:27:10,607 EPOCH 1023
2024-02-03 14:27:23,848 Epoch 1023: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.40 
2024-02-03 14:27:23,849 EPOCH 1024
2024-02-03 14:27:29,874 [Epoch: 1024 Step: 00017400] Batch Recognition Loss:   0.000192 => Gls Tokens per Sec:      956 || Batch Translation Loss:   0.067228 => Txt Tokens per Sec:     2694 || Lr: 0.000100
2024-02-03 14:27:36,912 Epoch 1024: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.41 
2024-02-03 14:27:36,913 EPOCH 1025
2024-02-03 14:27:50,505 Epoch 1025: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.41 
2024-02-03 14:27:50,505 EPOCH 1026
2024-02-03 14:28:03,706 Epoch 1026: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-03 14:28:03,706 EPOCH 1027
2024-02-03 14:28:17,048 Epoch 1027: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.26 
2024-02-03 14:28:17,049 EPOCH 1028
2024-02-03 14:28:30,342 Epoch 1028: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.08 
2024-02-03 14:28:30,343 EPOCH 1029
2024-02-03 14:28:43,699 Epoch 1029: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.40 
2024-02-03 14:28:43,699 EPOCH 1030
2024-02-03 14:28:49,212 [Epoch: 1030 Step: 00017500] Batch Recognition Loss:   0.001325 => Gls Tokens per Sec:      813 || Batch Translation Loss:   0.128018 => Txt Tokens per Sec:     2215 || Lr: 0.000100
2024-02-03 14:28:56,806 Epoch 1030: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.62 
2024-02-03 14:28:56,806 EPOCH 1031
2024-02-03 14:29:09,682 Epoch 1031: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.06 
2024-02-03 14:29:09,683 EPOCH 1032
2024-02-03 14:29:22,943 Epoch 1032: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.82 
2024-02-03 14:29:22,944 EPOCH 1033
2024-02-03 14:29:36,539 Epoch 1033: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.83 
2024-02-03 14:29:36,539 EPOCH 1034
2024-02-03 14:29:49,813 Epoch 1034: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-03 14:29:49,813 EPOCH 1035
2024-02-03 14:30:03,350 Epoch 1035: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.58 
2024-02-03 14:30:03,350 EPOCH 1036
2024-02-03 14:30:11,257 [Epoch: 1036 Step: 00017600] Batch Recognition Loss:   0.000484 => Gls Tokens per Sec:      373 || Batch Translation Loss:   0.020980 => Txt Tokens per Sec:     1133 || Lr: 0.000100
2024-02-03 14:30:16,679 Epoch 1036: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.53 
2024-02-03 14:30:16,680 EPOCH 1037
2024-02-03 14:30:30,269 Epoch 1037: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.49 
2024-02-03 14:30:30,270 EPOCH 1038
2024-02-03 14:30:43,800 Epoch 1038: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.44 
2024-02-03 14:30:43,800 EPOCH 1039
2024-02-03 14:30:56,991 Epoch 1039: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.50 
2024-02-03 14:30:56,992 EPOCH 1040
2024-02-03 14:31:10,290 Epoch 1040: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.45 
2024-02-03 14:31:10,291 EPOCH 1041
2024-02-03 14:31:23,545 Epoch 1041: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.55 
2024-02-03 14:31:23,546 EPOCH 1042
2024-02-03 14:31:25,784 [Epoch: 1042 Step: 00017700] Batch Recognition Loss:   0.000904 => Gls Tokens per Sec:      858 || Batch Translation Loss:   0.029821 => Txt Tokens per Sec:     2435 || Lr: 0.000100
2024-02-03 14:31:36,969 Epoch 1042: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.80 
2024-02-03 14:31:36,970 EPOCH 1043
2024-02-03 14:31:50,170 Epoch 1043: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-03 14:31:50,171 EPOCH 1044
2024-02-03 14:32:03,763 Epoch 1044: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-03 14:32:03,764 EPOCH 1045
2024-02-03 14:32:17,047 Epoch 1045: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.64 
2024-02-03 14:32:17,048 EPOCH 1046
2024-02-03 14:32:30,526 Epoch 1046: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.87 
2024-02-03 14:32:30,527 EPOCH 1047
2024-02-03 14:32:43,645 Epoch 1047: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.70 
2024-02-03 14:32:43,646 EPOCH 1048
2024-02-03 14:32:45,289 [Epoch: 1048 Step: 00017800] Batch Recognition Loss:   0.000472 => Gls Tokens per Sec:      390 || Batch Translation Loss:   0.054518 => Txt Tokens per Sec:     1359 || Lr: 0.000100
2024-02-03 14:32:57,108 Epoch 1048: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-03 14:32:57,109 EPOCH 1049
2024-02-03 14:33:10,625 Epoch 1049: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.61 
2024-02-03 14:33:10,625 EPOCH 1050
2024-02-03 14:33:23,742 Epoch 1050: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.81 
2024-02-03 14:33:23,742 EPOCH 1051
2024-02-03 14:33:37,108 Epoch 1051: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.79 
2024-02-03 14:33:37,108 EPOCH 1052
2024-02-03 14:33:50,497 Epoch 1052: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.70 
2024-02-03 14:33:50,498 EPOCH 1053
2024-02-03 14:34:03,741 [Epoch: 1053 Step: 00017900] Batch Recognition Loss:   0.000608 => Gls Tokens per Sec:      754 || Batch Translation Loss:   0.027642 => Txt Tokens per Sec:     2127 || Lr: 0.000100
2024-02-03 14:34:03,889 Epoch 1053: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.70 
2024-02-03 14:34:03,889 EPOCH 1054
2024-02-03 14:34:17,195 Epoch 1054: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.57 
2024-02-03 14:34:17,196 EPOCH 1055
2024-02-03 14:34:30,802 Epoch 1055: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-03 14:34:30,802 EPOCH 1056
2024-02-03 14:34:43,741 Epoch 1056: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.55 
2024-02-03 14:34:43,742 EPOCH 1057
2024-02-03 14:34:56,729 Epoch 1057: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.93 
2024-02-03 14:34:56,730 EPOCH 1058
2024-02-03 14:35:09,919 Epoch 1058: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.82 
2024-02-03 14:35:09,920 EPOCH 1059
2024-02-03 14:35:22,618 [Epoch: 1059 Step: 00018000] Batch Recognition Loss:   0.000867 => Gls Tokens per Sec:      686 || Batch Translation Loss:   0.038098 => Txt Tokens per Sec:     1973 || Lr: 0.000100
2024-02-03 14:36:11,485 Validation result at epoch 1059, step    18000: duration: 48.8660s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.58057	Translation Loss: 89130.08594	PPL: 10481.67090
	Eval Metric: BLEU
	WER 4.10	(DEL: 0.00,	INS: 0.00,	SUB: 4.10)
	BLEU-4 0.70	(BLEU-1: 10.92,	BLEU-2: 3.32,	BLEU-3: 1.33,	BLEU-4: 0.70)
	CHRF 17.41	ROUGE 9.11
2024-02-03 14:36:11,486 Logging Recognition and Translation Outputs
2024-02-03 14:36:11,486 ========================================================================================================================
2024-02-03 14:36:11,486 Logging Sequence: 64_53.00
2024-02-03 14:36:11,487 	Gloss Reference :	A B+C+D+E
2024-02-03 14:36:11,487 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 14:36:11,487 	Gloss Alignment :	         
2024-02-03 14:36:11,488 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 14:36:11,489 	Text Reference  :	the  bcci     and ipl does not want to compromise on the safety of the ********* ******* players
2024-02-03 14:36:11,489 	Text Hypothesis :	that everyone is  ipl **** *** **** as well       at the end    of the scheduled cricket matches
2024-02-03 14:36:11,490 	Text Alignment  :	S    S        S       D    D   D    S  S          S      S             I         I       S      
2024-02-03 14:36:11,490 ========================================================================================================================
2024-02-03 14:36:11,490 Logging Sequence: 165_252.00
2024-02-03 14:36:11,490 	Gloss Reference :	A B+C+D+E
2024-02-03 14:36:11,490 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 14:36:11,490 	Gloss Alignment :	         
2024-02-03 14:36:11,491 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 14:36:11,492 	Text Reference  :	**** ********** **** **** *** **** *** ********* ** ******* ** 4    rahul dravid also has   his own superstitions
2024-02-03 14:36:11,492 	Text Hypothesis :	kane williamson held down the fort for hyderabad by scoring 66 runs and   ended  the  match in  a   tie          
2024-02-03 14:36:11,492 	Text Alignment  :	I    I          I    I    I   I    I   I         I  I       I  S    S     S      S    S     S   S   S            
2024-02-03 14:36:11,492 ========================================================================================================================
2024-02-03 14:36:11,492 Logging Sequence: 90_7.00
2024-02-03 14:36:11,492 	Gloss Reference :	A B+C+D+E
2024-02-03 14:36:11,492 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 14:36:11,493 	Gloss Alignment :	         
2024-02-03 14:36:11,493 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 14:36:11,494 	Text Reference  :	**** ******* **** let me  tell you   the exciting story of   how they were introduced
2024-02-03 14:36:11,494 	Text Hypothesis :	this intense game he  had gone viral and that     i     will now in   the  league    
2024-02-03 14:36:11,494 	Text Alignment  :	I    I       I    S   S   S    S     S   S        S     S    S   S    S    S         
2024-02-03 14:36:11,494 ========================================================================================================================
2024-02-03 14:36:11,494 Logging Sequence: 137_307.00
2024-02-03 14:36:11,494 	Gloss Reference :	A B+C+D+E
2024-02-03 14:36:11,495 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 14:36:11,495 	Gloss Alignment :	         
2024-02-03 14:36:11,495 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 14:36:11,496 	Text Reference  :	so to be extra careful the player's flight was     escorted by  the flighter jets    
2024-02-03 14:36:11,496 	Text Hypothesis :	** ** ** ***** ******* the ******** match  started to       see the ******** ceremony
2024-02-03 14:36:11,496 	Text Alignment  :	D  D  D  D     D           D        S      S       S        S       D        S       
2024-02-03 14:36:11,496 ========================================================================================================================
2024-02-03 14:36:11,496 Logging Sequence: 165_200.00
2024-02-03 14:36:11,496 	Gloss Reference :	A B+C+D+E
2024-02-03 14:36:11,496 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 14:36:11,497 	Gloss Alignment :	         
2024-02-03 14:36:11,497 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 14:36:11,498 	Text Reference  :	you may be wondering what bag remember in 2011 when     india won the *** world cup *** *** ****** ** ****
2024-02-03 14:36:11,498 	Text Hypothesis :	*** *** ** ********* **** *** ******** ** **** recently india won the t20 world cup for the umpire as well
2024-02-03 14:36:11,498 	Text Alignment  :	D   D   D  D         D    D   D        D  D    S                      I             I   I   I      I  I   
2024-02-03 14:36:11,498 ========================================================================================================================
2024-02-03 14:36:12,047 Epoch 1059: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.73 
2024-02-03 14:36:12,047 EPOCH 1060
2024-02-03 14:36:25,302 Epoch 1060: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.74 
2024-02-03 14:36:25,302 EPOCH 1061
2024-02-03 14:36:38,473 Epoch 1061: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.88 
2024-02-03 14:36:38,473 EPOCH 1062
2024-02-03 14:36:51,941 Epoch 1062: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.68 
2024-02-03 14:36:51,941 EPOCH 1063
2024-02-03 14:37:05,409 Epoch 1063: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.73 
2024-02-03 14:37:05,410 EPOCH 1064
2024-02-03 14:37:18,656 Epoch 1064: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.55 
2024-02-03 14:37:18,657 EPOCH 1065
2024-02-03 14:37:29,784 [Epoch: 1065 Step: 00018100] Batch Recognition Loss:   0.000514 => Gls Tokens per Sec:      668 || Batch Translation Loss:   0.036966 => Txt Tokens per Sec:     1908 || Lr: 0.000100
2024-02-03 14:37:32,163 Epoch 1065: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.58 
2024-02-03 14:37:32,164 EPOCH 1066
2024-02-03 14:37:45,349 Epoch 1066: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.81 
2024-02-03 14:37:45,350 EPOCH 1067
2024-02-03 14:37:58,819 Epoch 1067: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.81 
2024-02-03 14:37:58,820 EPOCH 1068
2024-02-03 14:38:12,419 Epoch 1068: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.86 
2024-02-03 14:38:12,420 EPOCH 1069
2024-02-03 14:38:26,049 Epoch 1069: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.92 
2024-02-03 14:38:26,050 EPOCH 1070
2024-02-03 14:38:39,380 Epoch 1070: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.05 
2024-02-03 14:38:39,381 EPOCH 1071
2024-02-03 14:38:49,913 [Epoch: 1071 Step: 00018200] Batch Recognition Loss:   0.001625 => Gls Tokens per Sec:      584 || Batch Translation Loss:   0.115970 => Txt Tokens per Sec:     1721 || Lr: 0.000100
2024-02-03 14:38:52,544 Epoch 1071: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.68 
2024-02-03 14:38:52,544 EPOCH 1072
2024-02-03 14:39:05,879 Epoch 1072: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.75 
2024-02-03 14:39:05,880 EPOCH 1073
2024-02-03 14:39:19,219 Epoch 1073: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.32 
2024-02-03 14:39:19,219 EPOCH 1074
2024-02-03 14:39:32,571 Epoch 1074: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.30 
2024-02-03 14:39:32,572 EPOCH 1075
2024-02-03 14:39:45,900 Epoch 1075: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.44 
2024-02-03 14:39:45,901 EPOCH 1076
2024-02-03 14:39:58,995 Epoch 1076: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.56 
2024-02-03 14:39:58,995 EPOCH 1077
2024-02-03 14:40:00,600 [Epoch: 1077 Step: 00018300] Batch Recognition Loss:   0.001342 => Gls Tokens per Sec:     3195 || Batch Translation Loss:   0.096648 => Txt Tokens per Sec:     7858 || Lr: 0.000100
2024-02-03 14:40:12,154 Epoch 1077: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.24 
2024-02-03 14:40:12,155 EPOCH 1078
2024-02-03 14:40:25,684 Epoch 1078: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.34 
2024-02-03 14:40:25,684 EPOCH 1079
2024-02-03 14:40:38,655 Epoch 1079: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.20 
2024-02-03 14:40:38,655 EPOCH 1080
2024-02-03 14:40:52,105 Epoch 1080: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.22 
2024-02-03 14:40:52,105 EPOCH 1081
2024-02-03 14:41:05,650 Epoch 1081: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.10 
2024-02-03 14:41:05,650 EPOCH 1082
2024-02-03 14:41:18,980 Epoch 1082: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.75 
2024-02-03 14:41:18,981 EPOCH 1083
2024-02-03 14:41:23,029 [Epoch: 1083 Step: 00018400] Batch Recognition Loss:   0.000718 => Gls Tokens per Sec:      949 || Batch Translation Loss:   0.035960 => Txt Tokens per Sec:     2700 || Lr: 0.000100
2024-02-03 14:41:32,106 Epoch 1083: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.74 
2024-02-03 14:41:32,106 EPOCH 1084
2024-02-03 14:41:45,416 Epoch 1084: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.72 
2024-02-03 14:41:45,417 EPOCH 1085
2024-02-03 14:41:58,928 Epoch 1085: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-03 14:41:58,928 EPOCH 1086
2024-02-03 14:42:12,246 Epoch 1086: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.54 
2024-02-03 14:42:12,246 EPOCH 1087
2024-02-03 14:42:25,430 Epoch 1087: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.55 
2024-02-03 14:42:25,430 EPOCH 1088
2024-02-03 14:42:38,760 Epoch 1088: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.54 
2024-02-03 14:42:38,761 EPOCH 1089
2024-02-03 14:42:43,586 [Epoch: 1089 Step: 00018500] Batch Recognition Loss:   0.002186 => Gls Tokens per Sec:      479 || Batch Translation Loss:   0.057512 => Txt Tokens per Sec:     1259 || Lr: 0.000100
2024-02-03 14:42:52,190 Epoch 1089: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.54 
2024-02-03 14:42:52,191 EPOCH 1090
2024-02-03 14:43:05,559 Epoch 1090: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.46 
2024-02-03 14:43:05,560 EPOCH 1091
2024-02-03 14:43:18,640 Epoch 1091: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.50 
2024-02-03 14:43:18,641 EPOCH 1092
2024-02-03 14:43:32,034 Epoch 1092: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.62 
2024-02-03 14:43:32,034 EPOCH 1093
2024-02-03 14:43:45,368 Epoch 1093: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-03 14:43:45,368 EPOCH 1094
2024-02-03 14:43:58,672 Epoch 1094: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.41 
2024-02-03 14:43:58,673 EPOCH 1095
2024-02-03 14:43:59,084 [Epoch: 1095 Step: 00018600] Batch Recognition Loss:   0.000294 => Gls Tokens per Sec:     3122 || Batch Translation Loss:   0.014973 => Txt Tokens per Sec:     8463 || Lr: 0.000100
2024-02-03 14:44:12,061 Epoch 1095: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.43 
2024-02-03 14:44:12,061 EPOCH 1096
2024-02-03 14:44:25,270 Epoch 1096: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.32 
2024-02-03 14:44:25,271 EPOCH 1097
2024-02-03 14:44:38,550 Epoch 1097: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-03 14:44:38,551 EPOCH 1098
2024-02-03 14:44:51,709 Epoch 1098: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-03 14:44:51,709 EPOCH 1099
2024-02-03 14:45:04,983 Epoch 1099: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.79 
2024-02-03 14:45:04,984 EPOCH 1100
2024-02-03 14:45:18,216 [Epoch: 1100 Step: 00018700] Batch Recognition Loss:   0.000693 => Gls Tokens per Sec:      803 || Batch Translation Loss:   0.046516 => Txt Tokens per Sec:     2234 || Lr: 0.000100
2024-02-03 14:45:18,217 Epoch 1100: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.98 
2024-02-03 14:45:18,217 EPOCH 1101
2024-02-03 14:45:31,554 Epoch 1101: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.09 
2024-02-03 14:45:31,555 EPOCH 1102
2024-02-03 14:45:44,988 Epoch 1102: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.29 
2024-02-03 14:45:44,988 EPOCH 1103
2024-02-03 14:45:58,400 Epoch 1103: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.14 
2024-02-03 14:45:58,400 EPOCH 1104
2024-02-03 14:46:11,819 Epoch 1104: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.04 
2024-02-03 14:46:11,820 EPOCH 1105
2024-02-03 14:46:24,668 Epoch 1105: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.85 
2024-02-03 14:46:24,668 EPOCH 1106
2024-02-03 14:46:33,409 [Epoch: 1106 Step: 00018800] Batch Recognition Loss:   0.000619 => Gls Tokens per Sec:     1098 || Batch Translation Loss:   0.039824 => Txt Tokens per Sec:     3049 || Lr: 0.000100
2024-02-03 14:46:37,809 Epoch 1106: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.00 
2024-02-03 14:46:37,810 EPOCH 1107
2024-02-03 14:46:51,167 Epoch 1107: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.93 
2024-02-03 14:46:51,168 EPOCH 1108
2024-02-03 14:47:04,596 Epoch 1108: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.30 
2024-02-03 14:47:04,596 EPOCH 1109
2024-02-03 14:47:18,001 Epoch 1109: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.15 
2024-02-03 14:47:18,002 EPOCH 1110
2024-02-03 14:47:31,521 Epoch 1110: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.11 
2024-02-03 14:47:31,522 EPOCH 1111
2024-02-03 14:47:45,000 Epoch 1111: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.02 
2024-02-03 14:47:45,000 EPOCH 1112
2024-02-03 14:47:51,963 [Epoch: 1112 Step: 00018900] Batch Recognition Loss:   0.000615 => Gls Tokens per Sec:     1195 || Batch Translation Loss:   0.099970 => Txt Tokens per Sec:     3234 || Lr: 0.000100
2024-02-03 14:47:58,544 Epoch 1112: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.18 
2024-02-03 14:47:58,544 EPOCH 1113
2024-02-03 14:48:11,831 Epoch 1113: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.08 
2024-02-03 14:48:11,831 EPOCH 1114
2024-02-03 14:48:25,333 Epoch 1114: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.12 
2024-02-03 14:48:25,333 EPOCH 1115
2024-02-03 14:48:38,441 Epoch 1115: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.18 
2024-02-03 14:48:38,442 EPOCH 1116
2024-02-03 14:48:51,740 Epoch 1116: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.19 
2024-02-03 14:48:51,740 EPOCH 1117
2024-02-03 14:49:04,948 Epoch 1117: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.61 
2024-02-03 14:49:04,948 EPOCH 1118
2024-02-03 14:49:15,391 [Epoch: 1118 Step: 00019000] Batch Recognition Loss:   0.001491 => Gls Tokens per Sec:      650 || Batch Translation Loss:   0.563447 => Txt Tokens per Sec:     1744 || Lr: 0.000100
2024-02-03 14:49:18,297 Epoch 1118: Total Training Recognition Loss 0.04  Total Training Translation Loss 8.00 
2024-02-03 14:49:18,298 EPOCH 1119
2024-02-03 14:49:31,732 Epoch 1119: Total Training Recognition Loss 0.31  Total Training Translation Loss 5.33 
2024-02-03 14:49:31,732 EPOCH 1120
2024-02-03 14:49:44,998 Epoch 1120: Total Training Recognition Loss 0.10  Total Training Translation Loss 6.61 
2024-02-03 14:49:44,999 EPOCH 1121
2024-02-03 14:49:58,397 Epoch 1121: Total Training Recognition Loss 0.15  Total Training Translation Loss 5.92 
2024-02-03 14:49:58,398 EPOCH 1122
2024-02-03 14:50:11,833 Epoch 1122: Total Training Recognition Loss 0.12  Total Training Translation Loss 3.28 
2024-02-03 14:50:11,834 EPOCH 1123
2024-02-03 14:50:25,196 Epoch 1123: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.25 
2024-02-03 14:50:25,196 EPOCH 1124
2024-02-03 14:50:28,710 [Epoch: 1124 Step: 00019100] Batch Recognition Loss:   0.001705 => Gls Tokens per Sec:     1640 || Batch Translation Loss:   0.061400 => Txt Tokens per Sec:     4380 || Lr: 0.000100
2024-02-03 14:50:38,551 Epoch 1124: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.31 
2024-02-03 14:50:38,551 EPOCH 1125
2024-02-03 14:50:51,841 Epoch 1125: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.03 
2024-02-03 14:50:51,842 EPOCH 1126
2024-02-03 14:51:05,148 Epoch 1126: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.69 
2024-02-03 14:51:05,149 EPOCH 1127
2024-02-03 14:51:18,555 Epoch 1127: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.58 
2024-02-03 14:51:18,555 EPOCH 1128
2024-02-03 14:51:31,977 Epoch 1128: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.51 
2024-02-03 14:51:31,977 EPOCH 1129
2024-02-03 14:51:45,426 Epoch 1129: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.50 
2024-02-03 14:51:45,426 EPOCH 1130
2024-02-03 14:51:48,285 [Epoch: 1130 Step: 00019200] Batch Recognition Loss:   0.002151 => Gls Tokens per Sec:     1568 || Batch Translation Loss:   0.010327 => Txt Tokens per Sec:     4104 || Lr: 0.000100
2024-02-03 14:51:58,699 Epoch 1130: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.45 
2024-02-03 14:51:58,700 EPOCH 1131
2024-02-03 14:52:12,039 Epoch 1131: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.37 
2024-02-03 14:52:12,040 EPOCH 1132
2024-02-03 14:52:25,160 Epoch 1132: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.37 
2024-02-03 14:52:25,161 EPOCH 1133
2024-02-03 14:52:38,874 Epoch 1133: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.36 
2024-02-03 14:52:38,874 EPOCH 1134
2024-02-03 14:52:52,392 Epoch 1134: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.45 
2024-02-03 14:52:52,393 EPOCH 1135
2024-02-03 14:53:05,994 Epoch 1135: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.46 
2024-02-03 14:53:05,994 EPOCH 1136
2024-02-03 14:53:11,464 [Epoch: 1136 Step: 00019300] Batch Recognition Loss:   0.001694 => Gls Tokens per Sec:      585 || Batch Translation Loss:   0.046402 => Txt Tokens per Sec:     1686 || Lr: 0.000100
2024-02-03 14:53:19,721 Epoch 1136: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.40 
2024-02-03 14:53:19,722 EPOCH 1137
2024-02-03 14:53:33,344 Epoch 1137: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.37 
2024-02-03 14:53:33,345 EPOCH 1138
2024-02-03 14:53:46,135 Epoch 1138: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.53 
2024-02-03 14:53:46,136 EPOCH 1139
2024-02-03 14:53:59,669 Epoch 1139: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.38 
2024-02-03 14:53:59,670 EPOCH 1140
2024-02-03 14:54:13,220 Epoch 1140: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-03 14:54:13,220 EPOCH 1141
2024-02-03 14:54:26,554 Epoch 1141: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.31 
2024-02-03 14:54:26,554 EPOCH 1142
2024-02-03 14:54:27,112 [Epoch: 1142 Step: 00019400] Batch Recognition Loss:   0.000595 => Gls Tokens per Sec:     3451 || Batch Translation Loss:   0.024261 => Txt Tokens per Sec:     8571 || Lr: 0.000100
2024-02-03 14:54:42,230 Epoch 1142: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-03 14:54:42,230 EPOCH 1143
2024-02-03 14:54:55,880 Epoch 1143: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.37 
2024-02-03 14:54:55,881 EPOCH 1144
2024-02-03 14:55:09,558 Epoch 1144: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-03 14:55:09,559 EPOCH 1145
2024-02-03 14:55:22,991 Epoch 1145: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-03 14:55:22,992 EPOCH 1146
2024-02-03 14:55:36,322 Epoch 1146: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.29 
2024-02-03 14:55:36,322 EPOCH 1147
2024-02-03 14:55:49,636 Epoch 1147: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-03 14:55:49,636 EPOCH 1148
2024-02-03 14:55:49,802 [Epoch: 1148 Step: 00019500] Batch Recognition Loss:   0.000545 => Gls Tokens per Sec:     3879 || Batch Translation Loss:   0.011753 => Txt Tokens per Sec:    10018 || Lr: 0.000100
2024-02-03 14:56:02,804 Epoch 1148: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-03 14:56:02,804 EPOCH 1149
2024-02-03 14:56:16,164 Epoch 1149: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-03 14:56:16,165 EPOCH 1150
2024-02-03 14:56:29,662 Epoch 1150: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-03 14:56:29,663 EPOCH 1151
2024-02-03 14:56:42,767 Epoch 1151: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-03 14:56:42,768 EPOCH 1152
2024-02-03 14:56:56,211 Epoch 1152: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-03 14:56:56,212 EPOCH 1153
2024-02-03 14:57:09,457 [Epoch: 1153 Step: 00019600] Batch Recognition Loss:   0.000482 => Gls Tokens per Sec:      754 || Batch Translation Loss:   0.013071 => Txt Tokens per Sec:     2158 || Lr: 0.000100
2024-02-03 14:57:09,568 Epoch 1153: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-03 14:57:09,568 EPOCH 1154
2024-02-03 14:57:22,842 Epoch 1154: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.30 
2024-02-03 14:57:22,843 EPOCH 1155
2024-02-03 14:57:36,014 Epoch 1155: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-03 14:57:36,015 EPOCH 1156
2024-02-03 14:57:49,607 Epoch 1156: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-03 14:57:49,608 EPOCH 1157
2024-02-03 14:58:02,900 Epoch 1157: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-03 14:58:02,901 EPOCH 1158
2024-02-03 14:58:16,474 Epoch 1158: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-03 14:58:16,475 EPOCH 1159
2024-02-03 14:58:28,041 [Epoch: 1159 Step: 00019700] Batch Recognition Loss:   0.000641 => Gls Tokens per Sec:      753 || Batch Translation Loss:   0.014234 => Txt Tokens per Sec:     2117 || Lr: 0.000100
2024-02-03 14:58:30,119 Epoch 1159: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.31 
2024-02-03 14:58:30,120 EPOCH 1160
2024-02-03 14:58:43,411 Epoch 1160: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.40 
2024-02-03 14:58:43,412 EPOCH 1161
2024-02-03 14:58:57,068 Epoch 1161: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-03 14:58:57,069 EPOCH 1162
2024-02-03 14:59:11,842 Epoch 1162: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.50 
2024-02-03 14:59:11,843 EPOCH 1163
2024-02-03 14:59:31,740 Epoch 1163: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.30 
2024-02-03 14:59:31,741 EPOCH 1164
2024-02-03 14:59:53,808 Epoch 1164: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.96 
2024-02-03 14:59:53,809 EPOCH 1165
2024-02-03 15:00:11,585 [Epoch: 1165 Step: 00019800] Batch Recognition Loss:   0.002670 => Gls Tokens per Sec:      418 || Batch Translation Loss:   0.291511 => Txt Tokens per Sec:     1224 || Lr: 0.000100
2024-02-03 15:00:12,588 Epoch 1165: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.58 
2024-02-03 15:00:12,588 EPOCH 1166
2024-02-03 15:00:27,045 Epoch 1166: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.76 
2024-02-03 15:00:27,046 EPOCH 1167
2024-02-03 15:00:41,709 Epoch 1167: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.50 
2024-02-03 15:00:41,710 EPOCH 1168
2024-02-03 15:00:56,967 Epoch 1168: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.27 
2024-02-03 15:00:56,968 EPOCH 1169
2024-02-03 15:01:11,051 Epoch 1169: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.70 
2024-02-03 15:01:11,052 EPOCH 1170
2024-02-03 15:01:24,815 Epoch 1170: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.65 
2024-02-03 15:01:24,815 EPOCH 1171
2024-02-03 15:01:33,427 [Epoch: 1171 Step: 00019900] Batch Recognition Loss:   0.000512 => Gls Tokens per Sec:      743 || Batch Translation Loss:   0.019967 => Txt Tokens per Sec:     2069 || Lr: 0.000100
2024-02-03 15:01:40,736 Epoch 1171: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.61 
2024-02-03 15:01:40,736 EPOCH 1172
2024-02-03 15:01:57,192 Epoch 1172: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.52 
2024-02-03 15:01:57,193 EPOCH 1173
2024-02-03 15:02:10,983 Epoch 1173: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.62 
2024-02-03 15:02:10,984 EPOCH 1174
2024-02-03 15:02:24,918 Epoch 1174: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.51 
2024-02-03 15:02:24,918 EPOCH 1175
2024-02-03 15:02:40,289 Epoch 1175: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.48 
2024-02-03 15:02:40,290 EPOCH 1176
2024-02-03 15:02:55,259 Epoch 1176: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.54 
2024-02-03 15:02:55,259 EPOCH 1177
2024-02-03 15:03:01,378 [Epoch: 1177 Step: 00020000] Batch Recognition Loss:   0.000404 => Gls Tokens per Sec:      837 || Batch Translation Loss:   0.025415 => Txt Tokens per Sec:     2116 || Lr: 0.000100
2024-02-03 15:03:53,845 Validation result at epoch 1177, step    20000: duration: 52.4658s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.52325	Translation Loss: 87833.08594	PPL: 9160.64648
	Eval Metric: BLEU
	WER 4.17	(DEL: 0.00,	INS: 0.00,	SUB: 4.17)
	BLEU-4 0.81	(BLEU-1: 10.94,	BLEU-2: 3.64,	BLEU-3: 1.61,	BLEU-4: 0.81)
	CHRF 17.67	ROUGE 9.14
2024-02-03 15:03:53,847 Logging Recognition and Translation Outputs
2024-02-03 15:03:53,847 ========================================================================================================================
2024-02-03 15:03:53,847 Logging Sequence: 153_218.00
2024-02-03 15:03:53,847 	Gloss Reference :	A B+C+D+E
2024-02-03 15:03:53,847 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 15:03:53,848 	Gloss Alignment :	         
2024-02-03 15:03:53,848 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 15:03:53,849 	Text Reference  :	the 2022 final match is being held   in   the    same     melbourne stadium where    the     1992 match was      held   
2024-02-03 15:03:53,849 	Text Hypothesis :	the **** ***** ***** ** ***** couple were having problems with      their   marriage because of   his   drinking problem
2024-02-03 15:03:53,850 	Text Alignment  :	    D    D     D     D  D     S      S    S      S        S         S       S        S       S    S     S        S      
2024-02-03 15:03:53,850 ========================================================================================================================
2024-02-03 15:03:53,850 Logging Sequence: 168_115.00
2024-02-03 15:03:53,850 	Gloss Reference :	A B+C+D+E
2024-02-03 15:03:53,850 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 15:03:53,851 	Gloss Alignment :	         
2024-02-03 15:03:53,851 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 15:03:53,852 	Text Reference  :	******* ***** ** ***** ** ****** *** **** *** **** this has sparked a   major discussion on social media
2024-02-03 15:03:53,852 	Text Hypothesis :	however dhoni is going to retire the team and they want to  wait    for the   youth      of the    team 
2024-02-03 15:03:53,852 	Text Alignment  :	I       I     I  I     I  I      I   I    I   I    S    S   S       S   S     S          S  S      S    
2024-02-03 15:03:53,852 ========================================================================================================================
2024-02-03 15:03:53,852 Logging Sequence: 180_82.00
2024-02-03 15:03:53,853 	Gloss Reference :	A B+C+D+E
2024-02-03 15:03:53,853 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 15:03:53,853 	Gloss Alignment :	         
2024-02-03 15:03:53,853 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 15:03:53,854 	Text Reference  :	let me tell you about    the protest that has  been  going on     since     23rd april 2023
2024-02-03 15:03:53,854 	Text Hypothesis :	*** ** **** *** shocking to  see     him  then filed a     police complaint of   the   game
2024-02-03 15:03:53,855 	Text Alignment  :	D   D  D    D   S        S   S       S    S    S     S     S      S         S    S     S   
2024-02-03 15:03:53,855 ========================================================================================================================
2024-02-03 15:03:53,855 Logging Sequence: 56_14.00
2024-02-03 15:03:53,855 	Gloss Reference :	A B+C+D+E
2024-02-03 15:03:53,855 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 15:03:53,855 	Gloss Alignment :	         
2024-02-03 15:03:53,855 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 15:03:53,856 	Text Reference  :	people were glued   to *** their screens       during      the match
2024-02-03 15:03:53,856 	Text Hypothesis :	****** **** amazing to see such  technological advancement in  ipl  
2024-02-03 15:03:53,856 	Text Alignment  :	D      D    S          I   S     S             S           S   S    
2024-02-03 15:03:53,856 ========================================================================================================================
2024-02-03 15:03:53,857 Logging Sequence: 62_135.00
2024-02-03 15:03:53,857 	Gloss Reference :	A B+C+D+E
2024-02-03 15:03:53,857 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 15:03:53,857 	Gloss Alignment :	         
2024-02-03 15:03:53,857 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 15:03:53,858 	Text Reference  :	*** *** a     grade player is   the one who  
2024-02-03 15:03:53,858 	Text Hypothesis :	his csk match was   pay    with the sri lanka
2024-02-03 15:03:53,858 	Text Alignment  :	I   I   S     S     S      S        S   S    
2024-02-03 15:03:53,858 ========================================================================================================================
2024-02-03 15:04:01,655 Epoch 1177: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.68 
2024-02-03 15:04:01,655 EPOCH 1178
2024-02-03 15:04:16,091 Epoch 1178: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.76 
2024-02-03 15:04:16,091 EPOCH 1179
2024-02-03 15:04:29,778 Epoch 1179: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.65 
2024-02-03 15:04:29,778 EPOCH 1180
2024-02-03 15:04:47,183 Epoch 1180: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.52 
2024-02-03 15:04:47,183 EPOCH 1181
2024-02-03 15:05:01,397 Epoch 1181: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.48 
2024-02-03 15:05:01,398 EPOCH 1182
2024-02-03 15:05:18,297 Epoch 1182: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.37 
2024-02-03 15:05:18,298 EPOCH 1183
2024-02-03 15:05:26,913 [Epoch: 1183 Step: 00020100] Batch Recognition Loss:   0.000658 => Gls Tokens per Sec:      446 || Batch Translation Loss:   0.032673 => Txt Tokens per Sec:     1393 || Lr: 0.000100
2024-02-03 15:05:33,404 Epoch 1183: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-03 15:05:33,405 EPOCH 1184
2024-02-03 15:05:48,702 Epoch 1184: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-03 15:05:48,703 EPOCH 1185
2024-02-03 15:06:02,445 Epoch 1185: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-03 15:06:02,446 EPOCH 1186
2024-02-03 15:06:16,213 Epoch 1186: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.54 
2024-02-03 15:06:16,214 EPOCH 1187
2024-02-03 15:06:29,759 Epoch 1187: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.65 
2024-02-03 15:06:29,760 EPOCH 1188
2024-02-03 15:06:43,235 Epoch 1188: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.42 
2024-02-03 15:06:43,235 EPOCH 1189
2024-02-03 15:06:48,252 [Epoch: 1189 Step: 00020200] Batch Recognition Loss:   0.001399 => Gls Tokens per Sec:      460 || Batch Translation Loss:   0.014286 => Txt Tokens per Sec:     1199 || Lr: 0.000100
2024-02-03 15:06:57,399 Epoch 1189: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-03 15:06:57,400 EPOCH 1190
2024-02-03 15:07:11,850 Epoch 1190: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-03 15:07:11,851 EPOCH 1191
2024-02-03 15:07:25,464 Epoch 1191: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-03 15:07:25,465 EPOCH 1192
2024-02-03 15:07:38,913 Epoch 1192: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.32 
2024-02-03 15:07:38,913 EPOCH 1193
2024-02-03 15:07:53,924 Epoch 1193: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.28 
2024-02-03 15:07:53,924 EPOCH 1194
2024-02-03 15:08:08,551 Epoch 1194: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-03 15:08:08,552 EPOCH 1195
2024-02-03 15:08:11,872 [Epoch: 1195 Step: 00020300] Batch Recognition Loss:   0.001058 => Gls Tokens per Sec:      386 || Batch Translation Loss:   0.021640 => Txt Tokens per Sec:     1159 || Lr: 0.000100
2024-02-03 15:08:22,593 Epoch 1195: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-03 15:08:22,594 EPOCH 1196
2024-02-03 15:08:36,424 Epoch 1196: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-03 15:08:36,425 EPOCH 1197
2024-02-03 15:08:49,728 Epoch 1197: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-03 15:08:49,728 EPOCH 1198
2024-02-03 15:09:03,639 Epoch 1198: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-03 15:09:03,639 EPOCH 1199
2024-02-03 15:09:19,386 Epoch 1199: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.39 
2024-02-03 15:09:19,387 EPOCH 1200
2024-02-03 15:09:35,654 [Epoch: 1200 Step: 00020400] Batch Recognition Loss:   0.000858 => Gls Tokens per Sec:      653 || Batch Translation Loss:   0.021700 => Txt Tokens per Sec:     1817 || Lr: 0.000100
2024-02-03 15:09:35,654 Epoch 1200: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-03 15:09:35,655 EPOCH 1201
2024-02-03 15:09:49,633 Epoch 1201: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-03 15:09:49,633 EPOCH 1202
2024-02-03 15:10:03,872 Epoch 1202: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-03 15:10:03,873 EPOCH 1203
2024-02-03 15:10:17,233 Epoch 1203: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-03 15:10:17,233 EPOCH 1204
2024-02-03 15:10:30,867 Epoch 1204: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-03 15:10:30,867 EPOCH 1205
2024-02-03 15:10:45,898 Epoch 1205: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-03 15:10:45,899 EPOCH 1206
2024-02-03 15:11:03,167 [Epoch: 1206 Step: 00020500] Batch Recognition Loss:   0.001042 => Gls Tokens per Sec:      542 || Batch Translation Loss:   0.006917 => Txt Tokens per Sec:     1515 || Lr: 0.000100
2024-02-03 15:11:03,656 Epoch 1206: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-03 15:11:03,657 EPOCH 1207
2024-02-03 15:11:17,991 Epoch 1207: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.30 
2024-02-03 15:11:17,992 EPOCH 1208
2024-02-03 15:11:35,690 Epoch 1208: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.32 
2024-02-03 15:11:35,691 EPOCH 1209
2024-02-03 15:11:52,489 Epoch 1209: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.43 
2024-02-03 15:11:52,490 EPOCH 1210
2024-02-03 15:12:06,562 Epoch 1210: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.73 
2024-02-03 15:12:06,563 EPOCH 1211
2024-02-03 15:12:20,269 Epoch 1211: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.14 
2024-02-03 15:12:20,269 EPOCH 1212
2024-02-03 15:12:30,185 [Epoch: 1212 Step: 00020600] Batch Recognition Loss:   0.000382 => Gls Tokens per Sec:      814 || Batch Translation Loss:   0.113708 => Txt Tokens per Sec:     2280 || Lr: 0.000100
2024-02-03 15:12:34,491 Epoch 1212: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.00 
2024-02-03 15:12:34,492 EPOCH 1213
2024-02-03 15:12:49,258 Epoch 1213: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.40 
2024-02-03 15:12:49,258 EPOCH 1214
2024-02-03 15:13:05,422 Epoch 1214: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.16 
2024-02-03 15:13:05,423 EPOCH 1215
2024-02-03 15:13:20,804 Epoch 1215: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.12 
2024-02-03 15:13:20,805 EPOCH 1216
2024-02-03 15:13:36,489 Epoch 1216: Total Training Recognition Loss 0.06  Total Training Translation Loss 6.55 
2024-02-03 15:13:36,490 EPOCH 1217
2024-02-03 15:13:51,380 Epoch 1217: Total Training Recognition Loss 0.24  Total Training Translation Loss 24.95 
2024-02-03 15:13:51,381 EPOCH 1218
2024-02-03 15:14:01,160 [Epoch: 1218 Step: 00020700] Batch Recognition Loss:   0.018639 => Gls Tokens per Sec:      694 || Batch Translation Loss:   0.575116 => Txt Tokens per Sec:     1842 || Lr: 0.000100
2024-02-03 15:14:07,387 Epoch 1218: Total Training Recognition Loss 0.21  Total Training Translation Loss 8.38 
2024-02-03 15:14:07,388 EPOCH 1219
2024-02-03 15:14:21,886 Epoch 1219: Total Training Recognition Loss 0.12  Total Training Translation Loss 3.37 
2024-02-03 15:14:21,886 EPOCH 1220
2024-02-03 15:14:36,214 Epoch 1220: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.50 
2024-02-03 15:14:36,214 EPOCH 1221
2024-02-03 15:14:50,498 Epoch 1221: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.97 
2024-02-03 15:14:50,499 EPOCH 1222
2024-02-03 15:15:04,689 Epoch 1222: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.80 
2024-02-03 15:15:04,690 EPOCH 1223
2024-02-03 15:15:18,780 Epoch 1223: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.67 
2024-02-03 15:15:18,780 EPOCH 1224
2024-02-03 15:15:23,704 [Epoch: 1224 Step: 00020800] Batch Recognition Loss:   0.001382 => Gls Tokens per Sec:     1170 || Batch Translation Loss:   0.050480 => Txt Tokens per Sec:     3073 || Lr: 0.000100
2024-02-03 15:15:32,686 Epoch 1224: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.56 
2024-02-03 15:15:32,687 EPOCH 1225
2024-02-03 15:15:46,903 Epoch 1225: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.49 
2024-02-03 15:15:46,904 EPOCH 1226
2024-02-03 15:16:01,092 Epoch 1226: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.47 
2024-02-03 15:16:01,092 EPOCH 1227
2024-02-03 15:16:15,539 Epoch 1227: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.41 
2024-02-03 15:16:15,539 EPOCH 1228
2024-02-03 15:16:29,543 Epoch 1228: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.41 
2024-02-03 15:16:29,544 EPOCH 1229
2024-02-03 15:16:43,024 Epoch 1229: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.35 
2024-02-03 15:16:43,024 EPOCH 1230
2024-02-03 15:16:46,045 [Epoch: 1230 Step: 00020900] Batch Recognition Loss:   0.000691 => Gls Tokens per Sec:     1483 || Batch Translation Loss:   0.021001 => Txt Tokens per Sec:     4144 || Lr: 0.000100
2024-02-03 15:16:56,226 Epoch 1230: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.36 
2024-02-03 15:16:56,226 EPOCH 1231
2024-02-03 15:17:09,781 Epoch 1231: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.34 
2024-02-03 15:17:09,782 EPOCH 1232
2024-02-03 15:17:22,954 Epoch 1232: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.33 
2024-02-03 15:17:22,955 EPOCH 1233
2024-02-03 15:17:36,381 Epoch 1233: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.33 
2024-02-03 15:17:36,382 EPOCH 1234
2024-02-03 15:17:49,516 Epoch 1234: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.33 
2024-02-03 15:17:49,517 EPOCH 1235
2024-02-03 15:18:03,102 Epoch 1235: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-03 15:18:03,103 EPOCH 1236
2024-02-03 15:18:03,890 [Epoch: 1236 Step: 00021000] Batch Recognition Loss:   0.000724 => Gls Tokens per Sec:     4066 || Batch Translation Loss:   0.014639 => Txt Tokens per Sec:     9497 || Lr: 0.000100
2024-02-03 15:18:16,109 Epoch 1236: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.32 
2024-02-03 15:18:16,109 EPOCH 1237
2024-02-03 15:18:29,334 Epoch 1237: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.30 
2024-02-03 15:18:29,335 EPOCH 1238
2024-02-03 15:18:42,560 Epoch 1238: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.31 
2024-02-03 15:18:42,560 EPOCH 1239
2024-02-03 15:18:55,778 Epoch 1239: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.32 
2024-02-03 15:18:55,779 EPOCH 1240
2024-02-03 15:19:09,291 Epoch 1240: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.33 
2024-02-03 15:19:09,292 EPOCH 1241
2024-02-03 15:19:22,749 Epoch 1241: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-03 15:19:22,749 EPOCH 1242
2024-02-03 15:19:23,256 [Epoch: 1242 Step: 00021100] Batch Recognition Loss:   0.000386 => Gls Tokens per Sec:     3794 || Batch Translation Loss:   0.014081 => Txt Tokens per Sec:     9251 || Lr: 0.000100
2024-02-03 15:19:35,631 Epoch 1242: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.29 
2024-02-03 15:19:35,631 EPOCH 1243
2024-02-03 15:19:48,397 Epoch 1243: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-03 15:19:48,397 EPOCH 1244
2024-02-03 15:20:01,652 Epoch 1244: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-03 15:20:01,652 EPOCH 1245
2024-02-03 15:20:15,203 Epoch 1245: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-03 15:20:15,203 EPOCH 1246
2024-02-03 15:20:28,703 Epoch 1246: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-03 15:20:28,704 EPOCH 1247
2024-02-03 15:20:42,305 Epoch 1247: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-03 15:20:42,306 EPOCH 1248
2024-02-03 15:20:46,472 [Epoch: 1248 Step: 00021200] Batch Recognition Loss:   0.001385 => Gls Tokens per Sec:       94 || Batch Translation Loss:   0.041994 => Txt Tokens per Sec:      339 || Lr: 0.000100
2024-02-03 15:20:55,760 Epoch 1248: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-03 15:20:55,761 EPOCH 1249
2024-02-03 15:21:12,733 Epoch 1249: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.28 
2024-02-03 15:21:12,733 EPOCH 1250
2024-02-03 15:21:30,159 Epoch 1250: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-03 15:21:30,160 EPOCH 1251
2024-02-03 15:21:54,121 Epoch 1251: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-03 15:21:54,122 EPOCH 1252
2024-02-03 15:22:13,671 Epoch 1252: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-03 15:22:13,672 EPOCH 1253
2024-02-03 15:22:27,395 [Epoch: 1253 Step: 00021300] Batch Recognition Loss:   0.000995 => Gls Tokens per Sec:      728 || Batch Translation Loss:   0.014764 => Txt Tokens per Sec:     2025 || Lr: 0.000100
2024-02-03 15:22:27,705 Epoch 1253: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-03 15:22:27,705 EPOCH 1254
2024-02-03 15:22:41,383 Epoch 1254: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.27 
2024-02-03 15:22:41,384 EPOCH 1255
2024-02-03 15:22:54,473 Epoch 1255: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-03 15:22:54,474 EPOCH 1256
2024-02-03 15:23:12,352 Epoch 1256: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.27 
2024-02-03 15:23:12,353 EPOCH 1257
2024-02-03 15:23:26,925 Epoch 1257: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-03 15:23:26,925 EPOCH 1258
2024-02-03 15:23:42,895 Epoch 1258: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-03 15:23:42,896 EPOCH 1259
2024-02-03 15:23:58,031 [Epoch: 1259 Step: 00021400] Batch Recognition Loss:   0.000250 => Gls Tokens per Sec:      575 || Batch Translation Loss:   0.013376 => Txt Tokens per Sec:     1570 || Lr: 0.000100
2024-02-03 15:23:58,951 Epoch 1259: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-03 15:23:58,952 EPOCH 1260
2024-02-03 15:24:13,251 Epoch 1260: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.31 
2024-02-03 15:24:13,251 EPOCH 1261
2024-02-03 15:24:27,043 Epoch 1261: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.29 
2024-02-03 15:24:27,044 EPOCH 1262
2024-02-03 15:24:41,420 Epoch 1262: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.29 
2024-02-03 15:24:41,421 EPOCH 1263
2024-02-03 15:24:55,341 Epoch 1263: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-03 15:24:55,342 EPOCH 1264
2024-02-03 15:25:09,481 Epoch 1264: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-03 15:25:09,481 EPOCH 1265
2024-02-03 15:25:20,990 [Epoch: 1265 Step: 00021500] Batch Recognition Loss:   0.000776 => Gls Tokens per Sec:      646 || Batch Translation Loss:   0.014428 => Txt Tokens per Sec:     1784 || Lr: 0.000100
2024-02-03 15:25:23,462 Epoch 1265: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.25 
2024-02-03 15:25:23,463 EPOCH 1266
2024-02-03 15:25:37,303 Epoch 1266: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-03 15:25:37,304 EPOCH 1267
2024-02-03 15:25:50,910 Epoch 1267: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-03 15:25:50,910 EPOCH 1268
2024-02-03 15:26:04,668 Epoch 1268: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-03 15:26:04,669 EPOCH 1269
2024-02-03 15:26:18,668 Epoch 1269: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-03 15:26:18,669 EPOCH 1270
2024-02-03 15:26:32,624 Epoch 1270: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-03 15:26:32,625 EPOCH 1271
2024-02-03 15:26:36,290 [Epoch: 1271 Step: 00021600] Batch Recognition Loss:   0.000088 => Gls Tokens per Sec:     1747 || Batch Translation Loss:   0.057503 => Txt Tokens per Sec:     4638 || Lr: 0.000100
2024-02-03 15:26:46,182 Epoch 1271: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-03 15:26:46,182 EPOCH 1272
2024-02-03 15:26:59,500 Epoch 1272: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.75 
2024-02-03 15:26:59,501 EPOCH 1273
2024-02-03 15:27:13,035 Epoch 1273: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.88 
2024-02-03 15:27:13,036 EPOCH 1274
2024-02-03 15:27:26,532 Epoch 1274: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.28 
2024-02-03 15:27:26,533 EPOCH 1275
2024-02-03 15:27:40,155 Epoch 1275: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.91 
2024-02-03 15:27:40,156 EPOCH 1276
2024-02-03 15:27:54,066 Epoch 1276: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.03 
2024-02-03 15:27:54,067 EPOCH 1277
2024-02-03 15:28:03,871 [Epoch: 1277 Step: 00021700] Batch Recognition Loss:   0.001506 => Gls Tokens per Sec:      497 || Batch Translation Loss:   0.102944 => Txt Tokens per Sec:     1402 || Lr: 0.000100
2024-02-03 15:28:07,470 Epoch 1277: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.15 
2024-02-03 15:28:07,471 EPOCH 1278
2024-02-03 15:28:21,329 Epoch 1278: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.21 
2024-02-03 15:28:21,330 EPOCH 1279
2024-02-03 15:28:34,675 Epoch 1279: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.28 
2024-02-03 15:28:34,676 EPOCH 1280
2024-02-03 15:28:48,240 Epoch 1280: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.73 
2024-02-03 15:28:48,241 EPOCH 1281
2024-02-03 15:29:04,255 Epoch 1281: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.52 
2024-02-03 15:29:04,255 EPOCH 1282
2024-02-03 15:29:18,527 Epoch 1282: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.01 
2024-02-03 15:29:18,527 EPOCH 1283
2024-02-03 15:29:26,465 [Epoch: 1283 Step: 00021800] Batch Recognition Loss:   0.001931 => Gls Tokens per Sec:      452 || Batch Translation Loss:   0.047929 => Txt Tokens per Sec:     1322 || Lr: 0.000100
2024-02-03 15:29:31,750 Epoch 1283: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.81 
2024-02-03 15:29:31,750 EPOCH 1284
2024-02-03 15:29:45,113 Epoch 1284: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.78 
2024-02-03 15:29:45,114 EPOCH 1285
2024-02-03 15:29:58,822 Epoch 1285: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.70 
2024-02-03 15:29:58,823 EPOCH 1286
2024-02-03 15:30:12,551 Epoch 1286: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.70 
2024-02-03 15:30:12,552 EPOCH 1287
2024-02-03 15:30:25,951 Epoch 1287: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.59 
2024-02-03 15:30:25,951 EPOCH 1288
2024-02-03 15:30:39,394 Epoch 1288: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.48 
2024-02-03 15:30:39,394 EPOCH 1289
2024-02-03 15:30:43,785 [Epoch: 1289 Step: 00021900] Batch Recognition Loss:   0.000219 => Gls Tokens per Sec:      583 || Batch Translation Loss:   0.024062 => Txt Tokens per Sec:     1607 || Lr: 0.000100
2024-02-03 15:30:56,438 Epoch 1289: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.50 
2024-02-03 15:30:56,438 EPOCH 1290
2024-02-03 15:31:10,956 Epoch 1290: Total Training Recognition Loss 0.07  Total Training Translation Loss 0.39 
2024-02-03 15:31:10,957 EPOCH 1291
2024-02-03 15:31:25,307 Epoch 1291: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.43 
2024-02-03 15:31:25,307 EPOCH 1292
2024-02-03 15:31:38,613 Epoch 1292: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.51 
2024-02-03 15:31:38,614 EPOCH 1293
2024-02-03 15:31:51,903 Epoch 1293: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.40 
2024-02-03 15:31:51,903 EPOCH 1294
2024-02-03 15:32:05,546 Epoch 1294: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-03 15:32:05,547 EPOCH 1295
2024-02-03 15:32:10,025 [Epoch: 1295 Step: 00022000] Batch Recognition Loss:   0.000230 => Gls Tokens per Sec:      230 || Batch Translation Loss:   0.019173 => Txt Tokens per Sec:      705 || Lr: 0.000100
2024-02-03 15:32:59,629 Validation result at epoch 1295, step    22000: duration: 49.6026s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.49215	Translation Loss: 88670.59375	PPL: 9993.18457
	Eval Metric: BLEU
	WER 3.82	(DEL: 0.00,	INS: 0.00,	SUB: 3.82)
	BLEU-4 0.53	(BLEU-1: 10.65,	BLEU-2: 3.23,	BLEU-3: 1.22,	BLEU-4: 0.53)
	CHRF 17.04	ROUGE 9.13
2024-02-03 15:32:59,631 Logging Recognition and Translation Outputs
2024-02-03 15:32:59,631 ========================================================================================================================
2024-02-03 15:32:59,631 Logging Sequence: 85_58.00
2024-02-03 15:32:59,631 	Gloss Reference :	A B+C+D+E
2024-02-03 15:32:59,631 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 15:32:59,632 	Gloss Alignment :	         
2024-02-03 15:32:59,632 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 15:32:59,632 	Text Reference  :	*** ********* ****** symonds has been an   amazing player
2024-02-03 15:32:59,633 	Text Hypothesis :	the wrestlers wanted to      see the  side in      tests 
2024-02-03 15:32:59,633 	Text Alignment  :	I   I         I      S       S   S    S    S       S     
2024-02-03 15:32:59,633 ========================================================================================================================
2024-02-03 15:32:59,633 Logging Sequence: 71_120.00
2024-02-03 15:32:59,633 	Gloss Reference :	A B+C+D+E
2024-02-03 15:32:59,633 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 15:32:59,633 	Gloss Alignment :	         
2024-02-03 15:32:59,634 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 15:32:59,635 	Text Reference  :	**** on  3rd august 2022  kartikeya met   his ******** family  and     posted a  heartwarming picture     with his           mother     
2024-02-03 15:32:59,635 	Text Hypothesis :	from the age of     15-24 kartikeya began his training without knowing when   he would        participate in   international tournaments
2024-02-03 15:32:59,636 	Text Alignment  :	I    S   S   S      S               S         I        S       S       S      S  S            S           S    S             S          
2024-02-03 15:32:59,636 ========================================================================================================================
2024-02-03 15:32:59,636 Logging Sequence: 174_64.00
2024-02-03 15:32:59,636 	Gloss Reference :	A B+C+D+E
2024-02-03 15:32:59,636 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 15:32:59,636 	Gloss Alignment :	         
2024-02-03 15:32:59,636 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 15:32:59,638 	Text Reference  :	** a   total of 22  matches will  be       played at  only  2   stadiums and not all   over  india  
2024-02-03 15:32:59,638 	Text Hypothesis :	if you wish  to see the     final schedule of     the world cup matches  you can visit icc's website
2024-02-03 15:32:59,638 	Text Alignment  :	I  S   S     S  S   S       S     S        S      S   S     S   S        S   S   S     S     S      
2024-02-03 15:32:59,638 ========================================================================================================================
2024-02-03 15:32:59,639 Logging Sequence: 93_2.00
2024-02-03 15:32:59,639 	Gloss Reference :	A B+C+D+E  
2024-02-03 15:32:59,639 	Gloss Hypothesis:	A B+D+B+E+D
2024-02-03 15:32:59,639 	Gloss Alignment :	  S        
2024-02-03 15:32:59,639 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 15:32:59,640 	Text Reference  :	*** ***** * ********* wayne rooney was    an amazing football player  
2024-02-03 15:32:59,640 	Text Hypothesis :	and filed a complaint of    the    police to ensure  the      armbands
2024-02-03 15:32:59,640 	Text Alignment  :	I   I     I I         S     S      S      S  S       S        S       
2024-02-03 15:32:59,640 ========================================================================================================================
2024-02-03 15:32:59,640 Logging Sequence: 172_270.00
2024-02-03 15:32:59,641 	Gloss Reference :	A B+C+D+E
2024-02-03 15:32:59,641 	Gloss Hypothesis:	A B+C+D+E
2024-02-03 15:32:59,641 	Gloss Alignment :	         
2024-02-03 15:32:59,641 	--------------------------------------------------------------------------------------------------------------------
2024-02-03 15:32:59,642 	Text Reference  :	*** **** *** ***** if it continues to rain       today the ****** tickets     will be    refunded
2024-02-03 15:32:59,642 	Text Hypothesis :	and hope the rules if a  lot       of chocolates to    the police authorities over their failure 
2024-02-03 15:32:59,642 	Text Alignment  :	I   I    I   I        S  S         S  S          S         I      S           S    S     S       
2024-02-03 15:32:59,642 ========================================================================================================================
2024-02-03 15:33:08,626 Epoch 1295: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.52 
2024-02-03 15:33:08,627 EPOCH 1296
2024-02-03 15:33:22,569 Epoch 1296: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.68 
2024-02-03 15:33:22,569 EPOCH 1297
2024-02-03 15:33:36,511 Epoch 1297: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.55 
2024-02-03 15:33:36,512 EPOCH 1298
2024-02-03 15:33:49,988 Epoch 1298: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.90 
2024-02-03 15:33:49,989 EPOCH 1299
2024-02-03 15:34:03,513 Epoch 1299: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.11 
2024-02-03 15:34:03,514 EPOCH 1300
2024-02-03 15:34:16,871 [Epoch: 1300 Step: 00022100] Batch Recognition Loss:   0.000779 => Gls Tokens per Sec:      796 || Batch Translation Loss:   0.028180 => Txt Tokens per Sec:     2214 || Lr: 0.000100
2024-02-03 15:34:16,871 Epoch 1300: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.83 
2024-02-03 15:34:16,872 EPOCH 1301
2024-02-03 15:34:30,426 Epoch 1301: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.64 
2024-02-03 15:34:30,427 EPOCH 1302
2024-02-03 15:34:43,886 Epoch 1302: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.52 
2024-02-03 15:34:43,887 EPOCH 1303
2024-02-03 15:34:57,080 Epoch 1303: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.51 
2024-02-03 15:34:57,080 EPOCH 1304
2024-02-03 15:35:10,407 Epoch 1304: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-03 15:35:10,408 EPOCH 1305
2024-02-03 15:35:23,817 Epoch 1305: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-03 15:35:23,818 EPOCH 1306
2024-02-03 15:35:36,745 [Epoch: 1306 Step: 00022200] Batch Recognition Loss:   0.000981 => Gls Tokens per Sec:      723 || Batch Translation Loss:   0.055618 => Txt Tokens per Sec:     2033 || Lr: 0.000100
2024-02-03 15:35:37,269 Epoch 1306: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.72 
2024-02-03 15:35:37,269 EPOCH 1307
2024-02-03 15:35:50,649 Epoch 1307: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.14 
2024-02-03 15:35:50,650 EPOCH 1308
2024-02-03 15:36:04,124 Epoch 1308: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.48 
2024-02-03 15:36:04,125 EPOCH 1309
2024-02-03 15:36:17,065 Epoch 1309: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.42 
2024-02-03 15:36:17,065 EPOCH 1310
2024-02-03 15:36:30,505 Epoch 1310: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.17 
2024-02-03 15:36:30,505 EPOCH 1311
2024-02-03 15:36:43,891 Epoch 1311: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.54 
2024-02-03 15:36:43,892 EPOCH 1312
2024-02-03 15:36:53,255 [Epoch: 1312 Step: 00022300] Batch Recognition Loss:   0.001013 => Gls Tokens per Sec:      862 || Batch Translation Loss:   0.052145 => Txt Tokens per Sec:     2321 || Lr: 0.000100
2024-02-03 15:36:56,982 Epoch 1312: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.14 
2024-02-03 15:36:56,983 EPOCH 1313
2024-02-03 15:37:10,353 Epoch 1313: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.88 
2024-02-03 15:37:10,354 EPOCH 1314
2024-02-03 15:37:23,661 Epoch 1314: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.77 
2024-02-03 15:37:23,662 EPOCH 1315
2024-02-03 15:37:36,763 Epoch 1315: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.06 
2024-02-03 15:37:36,763 EPOCH 1316
2024-02-03 15:37:50,083 Epoch 1316: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.48 
2024-02-03 15:37:50,083 EPOCH 1317
2024-02-03 15:38:03,374 Epoch 1317: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.34 
2024-02-03 15:38:03,375 EPOCH 1318
2024-02-03 15:38:11,205 [Epoch: 1318 Step: 00022400] Batch Recognition Loss:   0.001459 => Gls Tokens per Sec:      899 || Batch Translation Loss:   0.074132 => Txt Tokens per Sec:     2484 || Lr: 0.000100
2024-02-03 15:38:16,788 Epoch 1318: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.14 
2024-02-03 15:38:16,789 EPOCH 1319
2024-02-03 15:38:30,448 Epoch 1319: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.05 
2024-02-03 15:38:30,449 EPOCH 1320
2024-02-03 15:38:44,119 Epoch 1320: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.92 
2024-02-03 15:38:44,119 EPOCH 1321
2024-02-03 15:39:00,895 Epoch 1321: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.78 
2024-02-03 15:39:00,896 EPOCH 1322
2024-02-03 15:39:15,581 Epoch 1322: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.67 
2024-02-03 15:39:15,581 EPOCH 1323
2024-02-03 15:39:29,377 Epoch 1323: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.62 
2024-02-03 15:39:29,377 EPOCH 1324
2024-02-03 15:39:32,989 [Epoch: 1324 Step: 00022500] Batch Recognition Loss:   0.000722 => Gls Tokens per Sec:     1596 || Batch Translation Loss:   0.027958 => Txt Tokens per Sec:     4333 || Lr: 0.000100
2024-02-03 15:39:42,980 Epoch 1324: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.50 
2024-02-03 15:39:42,980 EPOCH 1325
2024-02-03 15:39:56,965 Epoch 1325: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.55 
2024-02-03 15:39:56,965 EPOCH 1326
2024-02-03 15:40:10,088 Epoch 1326: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.80 
2024-02-03 15:40:10,089 EPOCH 1327
2024-02-03 15:40:23,589 Epoch 1327: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.46 
2024-02-03 15:40:23,590 EPOCH 1328
2024-02-03 15:40:36,822 Epoch 1328: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.51 
2024-02-03 15:40:36,822 EPOCH 1329
2024-02-03 15:40:50,338 Epoch 1329: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.80 
2024-02-03 15:40:50,339 EPOCH 1330
2024-02-03 15:40:59,824 [Epoch: 1330 Step: 00022600] Batch Recognition Loss:   0.001041 => Gls Tokens per Sec:      446 || Batch Translation Loss:   0.026694 => Txt Tokens per Sec:     1249 || Lr: 0.000100
2024-02-03 15:41:03,804 Epoch 1330: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.61 
2024-02-03 15:41:03,805 EPOCH 1331
2024-02-03 15:41:17,068 Epoch 1331: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.38 
2024-02-03 15:41:17,068 EPOCH 1332
2024-02-03 15:41:30,474 Epoch 1332: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.44 
2024-02-03 15:41:30,474 EPOCH 1333
2024-02-03 15:41:44,239 Epoch 1333: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.46 
2024-02-03 15:41:44,240 EPOCH 1334
2024-02-03 15:41:57,212 Epoch 1334: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.56 
2024-02-03 15:41:57,212 EPOCH 1335
2024-02-03 15:42:10,700 Epoch 1335: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.47 
2024-02-03 15:42:10,700 EPOCH 1336
2024-02-03 15:42:17,398 [Epoch: 1336 Step: 00022700] Batch Recognition Loss:   0.001237 => Gls Tokens per Sec:      478 || Batch Translation Loss:   0.021860 => Txt Tokens per Sec:     1562 || Lr: 0.000100
2024-02-03 15:42:24,071 Epoch 1336: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-03 15:42:24,071 EPOCH 1337
2024-02-03 15:42:37,421 Epoch 1337: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-03 15:42:37,421 EPOCH 1338
2024-02-03 15:42:50,827 Epoch 1338: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.50 
2024-02-03 15:42:50,827 EPOCH 1339
2024-02-03 15:43:04,514 Epoch 1339: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.52 
2024-02-03 15:43:04,514 EPOCH 1340
2024-02-03 15:43:17,827 Epoch 1340: Total Training Recognition Loss 0.08  Total Training Translation Loss 0.43 
2024-02-03 15:43:17,827 EPOCH 1341
2024-02-03 15:43:31,366 Epoch 1341: Total Training Recognition Loss 0.23  Total Training Translation Loss 0.43 
2024-02-03 15:43:31,367 EPOCH 1342
2024-02-03 15:43:33,372 [Epoch: 1342 Step: 00022800] Batch Recognition Loss:   0.004236 => Gls Tokens per Sec:      958 || Batch Translation Loss:   0.020209 => Txt Tokens per Sec:     2953 || Lr: 0.000100
2024-02-03 15:43:44,525 Epoch 1342: Total Training Recognition Loss 2.98  Total Training Translation Loss 0.51 
2024-02-03 15:43:44,525 EPOCH 1343
2024-02-03 15:43:57,838 Epoch 1343: Total Training Recognition Loss 8.43  Total Training Translation Loss 2.26 
2024-02-03 15:43:57,839 EPOCH 1344
2024-02-03 15:44:11,409 Epoch 1344: Total Training Recognition Loss 5.04  Total Training Translation Loss 16.67 
2024-02-03 15:44:11,409 EPOCH 1345
2024-02-03 15:44:24,777 Epoch 1345: Total Training Recognition Loss 2.63  Total Training Translation Loss 17.65 
2024-02-03 15:44:24,778 EPOCH 1346
2024-02-03 15:44:38,078 Epoch 1346: Total Training Recognition Loss 1.03  Total Training Translation Loss 5.72 
2024-02-03 15:44:38,079 EPOCH 1347
2024-02-03 15:44:51,470 Epoch 1347: Total Training Recognition Loss 0.37  Total Training Translation Loss 2.25 
2024-02-03 15:44:51,471 EPOCH 1348
2024-02-03 15:44:51,638 [Epoch: 1348 Step: 00022900] Batch Recognition Loss:   0.005039 => Gls Tokens per Sec:     3855 || Batch Translation Loss:   0.075991 => Txt Tokens per Sec:     9994 || Lr: 0.000100
2024-02-03 15:45:06,237 Epoch 1348: Total Training Recognition Loss 0.15  Total Training Translation Loss 1.27 
2024-02-03 15:45:06,237 EPOCH 1349
2024-02-03 15:45:19,920 Epoch 1349: Total Training Recognition Loss 0.08  Total Training Translation Loss 0.74 
2024-02-03 15:45:19,920 EPOCH 1350
2024-02-03 15:45:33,469 Epoch 1350: Total Training Recognition Loss 0.07  Total Training Translation Loss 0.60 
2024-02-03 15:45:33,469 EPOCH 1351
2024-02-03 15:45:46,867 Epoch 1351: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.52 
2024-02-03 15:45:46,867 EPOCH 1352
2024-02-03 15:46:00,462 Epoch 1352: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.49 
2024-02-03 15:46:00,463 EPOCH 1353
2024-02-03 15:46:16,535 [Epoch: 1353 Step: 00023000] Batch Recognition Loss:   0.001767 => Gls Tokens per Sec:      622 || Batch Translation Loss:   0.053468 => Txt Tokens per Sec:     1736 || Lr: 0.000100
2024-02-03 15:46:16,768 Epoch 1353: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.51 
2024-02-03 15:46:16,768 EPOCH 1354
2024-02-03 15:46:32,819 Epoch 1354: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.48 
2024-02-03 15:46:32,819 EPOCH 1355
2024-02-03 15:46:47,334 Epoch 1355: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.46 
2024-02-03 15:46:47,335 EPOCH 1356
2024-02-03 15:47:01,648 Epoch 1356: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.39 
2024-02-03 15:47:01,648 EPOCH 1357
2024-02-03 15:47:14,961 Epoch 1357: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.38 
2024-02-03 15:47:14,962 EPOCH 1358
2024-02-03 15:47:30,805 Epoch 1358: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.35 
2024-02-03 15:47:30,805 EPOCH 1359
2024-02-03 15:47:46,770 [Epoch: 1359 Step: 00023100] Batch Recognition Loss:   0.000507 => Gls Tokens per Sec:      546 || Batch Translation Loss:   0.016854 => Txt Tokens per Sec:     1483 || Lr: 0.000100
2024-02-03 15:47:49,084 Epoch 1359: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.33 
2024-02-03 15:47:49,084 EPOCH 1360
2024-02-03 15:48:06,777 Epoch 1360: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.38 
2024-02-03 15:48:06,778 EPOCH 1361
2024-02-03 15:48:21,136 Epoch 1361: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.38 
2024-02-03 15:48:21,136 EPOCH 1362
2024-02-03 15:48:35,092 Epoch 1362: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.37 
2024-02-03 15:48:35,093 EPOCH 1363
2024-02-03 15:48:52,154 Epoch 1363: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.34 
2024-02-03 15:48:52,155 EPOCH 1364
2024-02-03 15:49:06,161 Epoch 1364: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.34 
2024-02-03 15:49:06,161 EPOCH 1365
2024-02-03 15:49:18,770 [Epoch: 1365 Step: 00023200] Batch Recognition Loss:   0.000350 => Gls Tokens per Sec:      589 || Batch Translation Loss:   0.015963 => Txt Tokens per Sec:     1600 || Lr: 0.000100
2024-02-03 15:49:24,025 Epoch 1365: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.31 
2024-02-03 15:49:24,025 EPOCH 1366
2024-02-03 15:49:38,145 Epoch 1366: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.34 
2024-02-03 15:49:38,146 EPOCH 1367
2024-02-03 15:49:57,586 Epoch 1367: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.31 
2024-02-03 15:49:57,586 EPOCH 1368
2024-02-03 15:50:10,945 Epoch 1368: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.31 
2024-02-03 15:50:10,945 EPOCH 1369
2024-02-03 15:50:25,663 Epoch 1369: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.27 
2024-02-03 15:50:25,664 EPOCH 1370
2024-02-03 15:50:39,689 Epoch 1370: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.32 
2024-02-03 15:50:39,689 EPOCH 1371
