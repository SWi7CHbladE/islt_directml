2024-02-05 20:32:54,273 Hello! This is Joey-NMT.
2024-02-05 20:32:54,282 Total params: 25642504
2024-02-05 20:32:54,283 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'gloss_output_layer.bias', 'gloss_output_layer.weight', 'sgn_embed.ln.bias', 'sgn_embed.ln.weight', 'sgn_embed.norm.norm.bias', 'sgn_embed.norm.norm.weight', 'txt_embed.norm.norm.bias', 'txt_embed.norm.norm.weight']
2024-02-05 20:32:55,393 cfg.name                           : sign_experiment
2024-02-05 20:32:55,394 cfg.data.data_path                 : ./data/Sports_dataset/8/
2024-02-05 20:32:55,394 cfg.data.version                   : phoenix_2014_trans
2024-02-05 20:32:55,394 cfg.data.sgn                       : sign
2024-02-05 20:32:55,394 cfg.data.txt                       : text
2024-02-05 20:32:55,394 cfg.data.gls                       : gloss
2024-02-05 20:32:55,394 cfg.data.train                     : excel_data.train
2024-02-05 20:32:55,394 cfg.data.dev                       : excel_data.dev
2024-02-05 20:32:55,394 cfg.data.test                      : excel_data.test
2024-02-05 20:32:55,395 cfg.data.feature_size              : 2560
2024-02-05 20:32:55,395 cfg.data.level                     : word
2024-02-05 20:32:55,395 cfg.data.txt_lowercase             : True
2024-02-05 20:32:55,395 cfg.data.max_sent_length           : 500
2024-02-05 20:32:55,395 cfg.data.random_train_subset       : -1
2024-02-05 20:32:55,395 cfg.data.random_dev_subset         : -1
2024-02-05 20:32:55,395 cfg.testing.recognition_beam_sizes : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
2024-02-05 20:32:55,396 cfg.testing.translation_beam_sizes : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
2024-02-05 20:32:55,396 cfg.testing.translation_beam_alphas : [-1, 0, 1, 2, 3, 4, 5]
2024-02-05 20:32:55,396 cfg.training.reset_best_ckpt       : False
2024-02-05 20:32:55,396 cfg.training.reset_scheduler       : False
2024-02-05 20:32:55,396 cfg.training.reset_optimizer       : False
2024-02-05 20:32:55,396 cfg.training.random_seed           : 42
2024-02-05 20:32:55,396 cfg.training.model_dir             : ./sign_sample_model/fold8/32head/32batch
2024-02-05 20:32:55,396 cfg.training.recognition_loss_weight : 1.0
2024-02-05 20:32:55,397 cfg.training.translation_loss_weight : 1.0
2024-02-05 20:32:55,397 cfg.training.eval_metric           : bleu
2024-02-05 20:32:55,397 cfg.training.optimizer             : adam
2024-02-05 20:32:55,397 cfg.training.learning_rate         : 0.0001
2024-02-05 20:32:55,397 cfg.training.batch_size            : 32
2024-02-05 20:32:55,397 cfg.training.num_valid_log         : 5
2024-02-05 20:32:55,397 cfg.training.epochs                : 50000
2024-02-05 20:32:55,398 cfg.training.early_stopping_metric : eval_metric
2024-02-05 20:32:55,398 cfg.training.batch_type            : sentence
2024-02-05 20:32:55,398 cfg.training.translation_normalization : batch
2024-02-05 20:32:55,398 cfg.training.eval_recognition_beam_size : 1
2024-02-05 20:32:55,398 cfg.training.eval_translation_beam_size : 1
2024-02-05 20:32:55,398 cfg.training.eval_translation_beam_alpha : -1
2024-02-05 20:32:55,398 cfg.training.overwrite             : True
2024-02-05 20:32:55,398 cfg.training.shuffle               : True
2024-02-05 20:32:55,399 cfg.training.use_cuda              : True
2024-02-05 20:32:55,399 cfg.training.translation_max_output_length : 40
2024-02-05 20:32:55,399 cfg.training.keep_last_ckpts       : 1
2024-02-05 20:32:55,399 cfg.training.batch_multiplier      : 1
2024-02-05 20:32:55,399 cfg.training.logging_freq          : 100
2024-02-05 20:32:55,399 cfg.training.validation_freq       : 2000
2024-02-05 20:32:55,399 cfg.training.betas                 : [0.9, 0.998]
2024-02-05 20:32:55,399 cfg.training.scheduling            : plateau
2024-02-05 20:32:55,399 cfg.training.learning_rate_min     : 1e-08
2024-02-05 20:32:55,400 cfg.training.weight_decay          : 0.0001
2024-02-05 20:32:55,400 cfg.training.patience              : 12
2024-02-05 20:32:55,400 cfg.training.decrease_factor       : 0.5
2024-02-05 20:32:55,400 cfg.training.label_smoothing       : 0.0
2024-02-05 20:32:55,400 cfg.model.initializer              : xavier
2024-02-05 20:32:55,400 cfg.model.bias_initializer         : zeros
2024-02-05 20:32:55,400 cfg.model.init_gain                : 1.0
2024-02-05 20:32:55,400 cfg.model.embed_initializer        : xavier
2024-02-05 20:32:55,400 cfg.model.embed_init_gain          : 1.0
2024-02-05 20:32:55,401 cfg.model.tied_softmax             : True
2024-02-05 20:32:55,401 cfg.model.encoder.type             : transformer
2024-02-05 20:32:55,401 cfg.model.encoder.num_layers       : 3
2024-02-05 20:32:55,401 cfg.model.encoder.num_heads        : 32
2024-02-05 20:32:55,401 cfg.model.encoder.embeddings.embedding_dim : 512
2024-02-05 20:32:55,401 cfg.model.encoder.embeddings.scale : False
2024-02-05 20:32:55,401 cfg.model.encoder.embeddings.dropout : 0.1
2024-02-05 20:32:55,402 cfg.model.encoder.embeddings.norm_type : batch
2024-02-05 20:32:55,402 cfg.model.encoder.embeddings.activation_type : softsign
2024-02-05 20:32:55,402 cfg.model.encoder.hidden_size      : 512
2024-02-05 20:32:55,402 cfg.model.encoder.ff_size          : 2048
2024-02-05 20:32:55,402 cfg.model.encoder.dropout          : 0.1
2024-02-05 20:32:55,402 cfg.model.decoder.type             : transformer
2024-02-05 20:32:55,402 cfg.model.decoder.num_layers       : 3
2024-02-05 20:32:55,402 cfg.model.decoder.num_heads        : 32
2024-02-05 20:32:55,402 cfg.model.decoder.embeddings.embedding_dim : 512
2024-02-05 20:32:55,403 cfg.model.decoder.embeddings.scale : False
2024-02-05 20:32:55,403 cfg.model.decoder.embeddings.dropout : 0.1
2024-02-05 20:32:55,403 cfg.model.decoder.embeddings.norm_type : batch
2024-02-05 20:32:55,403 cfg.model.decoder.embeddings.activation_type : softsign
2024-02-05 20:32:55,403 cfg.model.decoder.hidden_size      : 512
2024-02-05 20:32:55,403 cfg.model.decoder.ff_size          : 2048
2024-02-05 20:32:55,403 cfg.model.decoder.dropout          : 0.1
2024-02-05 20:32:55,403 Data set sizes: 
	train 2124,
	valid 708,
	test 708
2024-02-05 20:32:55,404 First training example:
	[GLS] A B C D E
	[TXT] how did she become a champion
2024-02-05 20:32:55,404 First 10 words (gls): (0) <si> (1) <unk> (2) <pad> (3) A (4) B (5) C (6) D (7) E
2024-02-05 20:32:55,404 First 10 words (txt): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) the (5) and (6) to (7) in (8) a (9) of
2024-02-05 20:32:55,404 Number of unique glosses (types): 8
2024-02-05 20:32:55,404 Number of unique words (types): 4402
2024-02-05 20:32:55,404 SignModel(
	encoder=TransformerEncoder(num_layers=3, num_heads=32),
	decoder=TransformerDecoder(num_layers=3, num_heads=32),
	sgn_embed=SpatialEmbeddings(embedding_dim=512, input_size=2560),
	txt_embed=Embeddings(embedding_dim=512, vocab_size=4402))
2024-02-05 20:32:55,408 EPOCH 1
2024-02-05 20:33:01,323 Epoch   1: Total Training Recognition Loss 279.21  Total Training Translation Loss 6501.15 
2024-02-05 20:33:01,323 EPOCH 2
2024-02-05 20:33:04,215 [Epoch: 002 Step: 00000100] Batch Recognition Loss:   1.118769 => Gls Tokens per Sec:     1827 || Batch Translation Loss:  85.757042 => Txt Tokens per Sec:     5138 || Lr: 0.000100
2024-02-05 20:33:06,755 Epoch   2: Total Training Recognition Loss 74.75  Total Training Translation Loss 5941.06 
2024-02-05 20:33:06,755 EPOCH 3
2024-02-05 20:33:11,690 [Epoch: 003 Step: 00000200] Batch Recognition Loss:   0.930459 => Gls Tokens per Sec:     2120 || Batch Translation Loss:  65.551003 => Txt Tokens per Sec:     5835 || Lr: 0.000100
2024-02-05 20:33:11,913 Epoch   3: Total Training Recognition Loss 52.89  Total Training Translation Loss 5839.00 
2024-02-05 20:33:11,913 EPOCH 4
2024-02-05 20:33:17,381 Epoch   4: Total Training Recognition Loss 64.26  Total Training Translation Loss 5675.44 
2024-02-05 20:33:17,382 EPOCH 5
2024-02-05 20:33:19,727 [Epoch: 005 Step: 00000300] Batch Recognition Loss:   1.614083 => Gls Tokens per Sec:     2185 || Batch Translation Loss: 100.207764 => Txt Tokens per Sec:     5776 || Lr: 0.000100
2024-02-05 20:33:22,792 Epoch   5: Total Training Recognition Loss 65.48  Total Training Translation Loss 5456.45 
2024-02-05 20:33:22,792 EPOCH 6
2024-02-05 20:33:27,863 [Epoch: 006 Step: 00000400] Batch Recognition Loss:   0.844744 => Gls Tokens per Sec:     2032 || Batch Translation Loss:  97.861549 => Txt Tokens per Sec:     5608 || Lr: 0.000100
2024-02-05 20:33:28,048 Epoch   6: Total Training Recognition Loss 34.28  Total Training Translation Loss 5236.36 
2024-02-05 20:33:28,049 EPOCH 7
2024-02-05 20:33:33,421 Epoch   7: Total Training Recognition Loss 25.09  Total Training Translation Loss 5031.40 
2024-02-05 20:33:33,422 EPOCH 8
2024-02-05 20:33:35,902 [Epoch: 008 Step: 00000500] Batch Recognition Loss:   0.210097 => Gls Tokens per Sec:     1960 || Batch Translation Loss:  50.569569 => Txt Tokens per Sec:     5402 || Lr: 0.000100
2024-02-05 20:33:38,704 Epoch   8: Total Training Recognition Loss 20.08  Total Training Translation Loss 4825.82 
2024-02-05 20:33:38,705 EPOCH 9
2024-02-05 20:33:44,102 [Epoch: 009 Step: 00000600] Batch Recognition Loss:   0.230150 => Gls Tokens per Sec:     1879 || Batch Translation Loss:  86.013443 => Txt Tokens per Sec:     5193 || Lr: 0.000100
2024-02-05 20:33:44,308 Epoch   9: Total Training Recognition Loss 18.52  Total Training Translation Loss 4639.22 
2024-02-05 20:33:44,308 EPOCH 10
2024-02-05 20:33:49,424 Epoch  10: Total Training Recognition Loss 13.48  Total Training Translation Loss 4454.24 
2024-02-05 20:33:49,425 EPOCH 11
2024-02-05 20:33:51,883 [Epoch: 011 Step: 00000700] Batch Recognition Loss:   0.176469 => Gls Tokens per Sec:     1954 || Batch Translation Loss:  41.837822 => Txt Tokens per Sec:     5265 || Lr: 0.000100
2024-02-05 20:33:54,931 Epoch  11: Total Training Recognition Loss 11.46  Total Training Translation Loss 4281.37 
2024-02-05 20:33:54,932 EPOCH 12
2024-02-05 20:33:59,539 [Epoch: 012 Step: 00000800] Batch Recognition Loss:   0.148978 => Gls Tokens per Sec:     2166 || Batch Translation Loss:  69.092438 => Txt Tokens per Sec:     6001 || Lr: 0.000100
2024-02-05 20:33:59,818 Epoch  12: Total Training Recognition Loss 9.95  Total Training Translation Loss 4116.50 
2024-02-05 20:33:59,818 EPOCH 13
2024-02-05 20:34:05,339 Epoch  13: Total Training Recognition Loss 8.89  Total Training Translation Loss 3945.49 
2024-02-05 20:34:05,340 EPOCH 14
2024-02-05 20:34:07,696 [Epoch: 014 Step: 00000900] Batch Recognition Loss:   0.109302 => Gls Tokens per Sec:     1970 || Batch Translation Loss:  61.687340 => Txt Tokens per Sec:     5715 || Lr: 0.000100
2024-02-05 20:34:10,578 Epoch  14: Total Training Recognition Loss 7.96  Total Training Translation Loss 3782.52 
2024-02-05 20:34:10,579 EPOCH 15
2024-02-05 20:34:15,362 [Epoch: 015 Step: 00001000] Batch Recognition Loss:   0.016024 => Gls Tokens per Sec:     2054 || Batch Translation Loss:  54.141323 => Txt Tokens per Sec:     5667 || Lr: 0.000100
2024-02-05 20:34:15,753 Epoch  15: Total Training Recognition Loss 7.49  Total Training Translation Loss 3641.68 
2024-02-05 20:34:15,753 EPOCH 16
2024-02-05 20:34:20,839 Epoch  16: Total Training Recognition Loss 7.14  Total Training Translation Loss 3508.32 
2024-02-05 20:34:20,840 EPOCH 17
2024-02-05 20:34:23,009 [Epoch: 017 Step: 00001100] Batch Recognition Loss:   0.108937 => Gls Tokens per Sec:     2021 || Batch Translation Loss:  40.103542 => Txt Tokens per Sec:     5387 || Lr: 0.000100
2024-02-05 20:34:26,227 Epoch  17: Total Training Recognition Loss 6.81  Total Training Translation Loss 3353.96 
2024-02-05 20:34:26,227 EPOCH 18
2024-02-05 20:34:30,630 [Epoch: 018 Step: 00001200] Batch Recognition Loss:   0.124655 => Gls Tokens per Sec:     2195 || Batch Translation Loss:  45.735218 => Txt Tokens per Sec:     6029 || Lr: 0.000100
2024-02-05 20:34:31,276 Epoch  18: Total Training Recognition Loss 6.11  Total Training Translation Loss 3195.85 
2024-02-05 20:34:31,276 EPOCH 19
2024-02-05 20:34:36,458 Epoch  19: Total Training Recognition Loss 5.75  Total Training Translation Loss 3035.82 
2024-02-05 20:34:36,458 EPOCH 20
2024-02-05 20:34:38,315 [Epoch: 020 Step: 00001300] Batch Recognition Loss:   0.036419 => Gls Tokens per Sec:     2327 || Batch Translation Loss:  40.125336 => Txt Tokens per Sec:     6251 || Lr: 0.000100
2024-02-05 20:34:41,659 Epoch  20: Total Training Recognition Loss 5.62  Total Training Translation Loss 2908.95 
2024-02-05 20:34:41,659 EPOCH 21
2024-02-05 20:34:46,124 [Epoch: 021 Step: 00001400] Batch Recognition Loss:   0.030549 => Gls Tokens per Sec:     2128 || Batch Translation Loss:  38.219257 => Txt Tokens per Sec:     5803 || Lr: 0.000100
2024-02-05 20:34:46,836 Epoch  21: Total Training Recognition Loss 5.48  Total Training Translation Loss 2775.29 
2024-02-05 20:34:46,836 EPOCH 22
2024-02-05 20:34:52,201 Epoch  22: Total Training Recognition Loss 5.21  Total Training Translation Loss 2634.03 
2024-02-05 20:34:52,202 EPOCH 23
2024-02-05 20:34:54,061 [Epoch: 023 Step: 00001500] Batch Recognition Loss:   0.020480 => Gls Tokens per Sec:     2238 || Batch Translation Loss:  37.200581 => Txt Tokens per Sec:     6055 || Lr: 0.000100
2024-02-05 20:34:57,018 Epoch  23: Total Training Recognition Loss 5.69  Total Training Translation Loss 2511.61 
2024-02-05 20:34:57,018 EPOCH 24
2024-02-05 20:35:01,711 [Epoch: 024 Step: 00001600] Batch Recognition Loss:   0.111751 => Gls Tokens per Sec:     1990 || Batch Translation Loss:  44.312504 => Txt Tokens per Sec:     5499 || Lr: 0.000100
2024-02-05 20:35:02,458 Epoch  24: Total Training Recognition Loss 5.29  Total Training Translation Loss 2372.49 
2024-02-05 20:35:02,458 EPOCH 25
2024-02-05 20:35:07,291 Epoch  25: Total Training Recognition Loss 5.01  Total Training Translation Loss 2231.07 
2024-02-05 20:35:07,292 EPOCH 26
2024-02-05 20:35:09,068 [Epoch: 026 Step: 00001700] Batch Recognition Loss:   0.064942 => Gls Tokens per Sec:     2253 || Batch Translation Loss:  43.196701 => Txt Tokens per Sec:     6091 || Lr: 0.000100
2024-02-05 20:35:12,037 Epoch  26: Total Training Recognition Loss 5.11  Total Training Translation Loss 2102.56 
2024-02-05 20:35:12,037 EPOCH 27
2024-02-05 20:35:16,538 [Epoch: 027 Step: 00001800] Batch Recognition Loss:   0.098922 => Gls Tokens per Sec:     2062 || Batch Translation Loss:  10.862187 => Txt Tokens per Sec:     5767 || Lr: 0.000100
2024-02-05 20:35:17,284 Epoch  27: Total Training Recognition Loss 4.81  Total Training Translation Loss 1987.62 
2024-02-05 20:35:17,284 EPOCH 28
2024-02-05 20:35:22,259 Epoch  28: Total Training Recognition Loss 4.76  Total Training Translation Loss 1874.09 
2024-02-05 20:35:22,259 EPOCH 29
2024-02-05 20:35:23,982 [Epoch: 029 Step: 00001900] Batch Recognition Loss:   0.054694 => Gls Tokens per Sec:     2172 || Batch Translation Loss:  23.445967 => Txt Tokens per Sec:     6215 || Lr: 0.000100
2024-02-05 20:35:27,576 Epoch  29: Total Training Recognition Loss 4.71  Total Training Translation Loss 1742.92 
2024-02-05 20:35:27,576 EPOCH 30
2024-02-05 20:35:31,786 [Epoch: 030 Step: 00002000] Batch Recognition Loss:   0.044610 => Gls Tokens per Sec:     2143 || Batch Translation Loss:  18.157986 => Txt Tokens per Sec:     5897 || Lr: 0.000100
2024-02-05 20:35:40,889 Hooray! New best validation result [eval_metric]!
2024-02-05 20:35:40,890 Saving new checkpoint.
2024-02-05 20:35:41,136 Validation result at epoch  30, step     2000: duration: 9.3499s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 3.12702	Translation Loss: 61835.24219	PPL: 481.11807
	Eval Metric: BLEU
	WER 6.57	(DEL: 0.00,	INS: 0.00,	SUB: 6.57)
	BLEU-4 0.87	(BLEU-1: 12.89,	BLEU-2: 4.57,	BLEU-3: 1.82,	BLEU-4: 0.87)
	CHRF 16.74	ROUGE 10.80
2024-02-05 20:35:41,137 Logging Recognition and Translation Outputs
2024-02-05 20:35:41,137 ========================================================================================================================
2024-02-05 20:35:41,137 Logging Sequence: 165_414.00
2024-02-05 20:35:41,138 	Gloss Reference :	A B+C+D+E
2024-02-05 20:35:41,138 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 20:35:41,138 	Gloss Alignment :	         
2024-02-05 20:35:41,138 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:35:41,140 	Text Reference  :	he felt sachin was  lucky so   he always gave his  sweater to give   it to  the umpire       
2024-02-05 20:35:41,140 	Text Hypothesis :	** the  ipl    will be    held in ipl    but  they have    a  person on his own superstitions
2024-02-05 20:35:41,140 	Text Alignment  :	D  S    S      S    S     S    S  S      S    S    S       S  S      S  S   S   S            
2024-02-05 20:35:41,140 ========================================================================================================================
2024-02-05 20:35:41,140 Logging Sequence: 169_268.00
2024-02-05 20:35:41,140 	Gloss Reference :	A B+C+D+E
2024-02-05 20:35:41,140 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 20:35:41,141 	Gloss Alignment :	         
2024-02-05 20:35:41,141 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:35:41,142 	Text Reference  :	**** ** *** **** **** shami supports arshdeep and many fans supported him  as  well     
2024-02-05 20:35:41,142 	Text Hypothesis :	this is why they were seen  seen     smashing the ball for  a         huge fan following
2024-02-05 20:35:41,142 	Text Alignment  :	I    I  I   I    I    S     S        S        S   S    S    S         S    S   S        
2024-02-05 20:35:41,142 ========================================================================================================================
2024-02-05 20:35:41,142 Logging Sequence: 172_15.00
2024-02-05 20:35:41,142 	Gloss Reference :	A B+C+D+E
2024-02-05 20:35:41,143 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 20:35:41,143 	Gloss Alignment :	         
2024-02-05 20:35:41,143 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:35:41,145 	Text Reference  :	now in the final match   on   28   may  2023   the two    teams  were up against each other at         the ********** same venue
2024-02-05 20:35:41,145 	Text Hypothesis :	*** ** *** ***** however they have been giving the golden golden boot as well    with the   tournament the tournament and  cafes
2024-02-05 20:35:41,145 	Text Alignment  :	D   D  D   D     S       S    S    S    S          S      S      S    S  S       S    S     S              I          S    S    
2024-02-05 20:35:41,145 ========================================================================================================================
2024-02-05 20:35:41,145 Logging Sequence: 96_158.00
2024-02-05 20:35:41,146 	Gloss Reference :	A B+C+D+E
2024-02-05 20:35:41,146 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 20:35:41,146 	Gloss Alignment :	         
2024-02-05 20:35:41,146 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:35:41,147 	Text Reference  :	*** ****** **** **** after   this pandya fell on  his knees in   disappointment
2024-02-05 20:35:41,147 	Text Hypothesis :	the couple were very worried and  want   to   see the next  next wicket        
2024-02-05 20:35:41,147 	Text Alignment  :	I   I      I    I    S       S    S      S    S   S   S     S    S             
2024-02-05 20:35:41,147 ========================================================================================================================
2024-02-05 20:35:41,147 Logging Sequence: 152_73.00
2024-02-05 20:35:41,148 	Gloss Reference :	A B+C+D+E
2024-02-05 20:35:41,148 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 20:35:41,148 	Gloss Alignment :	         
2024-02-05 20:35:41,148 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:35:41,149 	Text Reference  :	******* **** *** ***** *** **** eventually he  too  got out      by       shaheen  afridi
2024-02-05 20:35:41,149 	Text Hypothesis :	however when the match was held in         uae with his casteist casteist casteist slurts
2024-02-05 20:35:41,149 	Text Alignment  :	I       I    I   I     I   I    S          S   S    S   S        S        S        S     
2024-02-05 20:35:41,149 ========================================================================================================================
2024-02-05 20:35:41,965 Epoch  30: Total Training Recognition Loss 4.71  Total Training Translation Loss 1631.70 
2024-02-05 20:35:41,966 EPOCH 31
2024-02-05 20:35:47,559 Epoch  31: Total Training Recognition Loss 4.61  Total Training Translation Loss 1516.09 
2024-02-05 20:35:47,559 EPOCH 32
2024-02-05 20:35:49,356 [Epoch: 032 Step: 00002100] Batch Recognition Loss:   0.078977 => Gls Tokens per Sec:     1994 || Batch Translation Loss:  28.626492 => Txt Tokens per Sec:     5477 || Lr: 0.000100
2024-02-05 20:35:52,394 Epoch  32: Total Training Recognition Loss 4.47  Total Training Translation Loss 1411.37 
2024-02-05 20:35:52,394 EPOCH 33
2024-02-05 20:35:57,049 [Epoch: 033 Step: 00002200] Batch Recognition Loss:   0.082587 => Gls Tokens per Sec:     1904 || Batch Translation Loss:  19.673037 => Txt Tokens per Sec:     5141 || Lr: 0.000100
2024-02-05 20:35:58,121 Epoch  33: Total Training Recognition Loss 4.63  Total Training Translation Loss 1318.21 
2024-02-05 20:35:58,121 EPOCH 34
2024-02-05 20:36:03,403 Epoch  34: Total Training Recognition Loss 4.50  Total Training Translation Loss 1235.58 
2024-02-05 20:36:03,404 EPOCH 35
2024-02-05 20:36:05,033 [Epoch: 035 Step: 00002300] Batch Recognition Loss:   0.040313 => Gls Tokens per Sec:     2101 || Batch Translation Loss:  20.227282 => Txt Tokens per Sec:     5958 || Lr: 0.000100
2024-02-05 20:36:08,429 Epoch  35: Total Training Recognition Loss 4.33  Total Training Translation Loss 1118.70 
2024-02-05 20:36:08,430 EPOCH 36
2024-02-05 20:36:12,580 [Epoch: 036 Step: 00002400] Batch Recognition Loss:   0.104923 => Gls Tokens per Sec:     2097 || Batch Translation Loss:   6.259461 => Txt Tokens per Sec:     5730 || Lr: 0.000100
2024-02-05 20:36:13,749 Epoch  36: Total Training Recognition Loss 3.95  Total Training Translation Loss 1023.07 
2024-02-05 20:36:13,749 EPOCH 37
2024-02-05 20:36:18,894 Epoch  37: Total Training Recognition Loss 3.86  Total Training Translation Loss 948.39 
2024-02-05 20:36:18,894 EPOCH 38
2024-02-05 20:36:20,344 [Epoch: 038 Step: 00002500] Batch Recognition Loss:   0.074262 => Gls Tokens per Sec:     2319 || Batch Translation Loss:  14.245550 => Txt Tokens per Sec:     6366 || Lr: 0.000100
2024-02-05 20:36:23,985 Epoch  38: Total Training Recognition Loss 3.70  Total Training Translation Loss 881.19 
2024-02-05 20:36:23,985 EPOCH 39
2024-02-05 20:36:28,275 [Epoch: 039 Step: 00002600] Batch Recognition Loss:   0.029235 => Gls Tokens per Sec:     1992 || Batch Translation Loss:   9.800572 => Txt Tokens per Sec:     5534 || Lr: 0.000100
2024-02-05 20:36:29,431 Epoch  39: Total Training Recognition Loss 3.85  Total Training Translation Loss 803.40 
2024-02-05 20:36:29,431 EPOCH 40
2024-02-05 20:36:34,307 Epoch  40: Total Training Recognition Loss 3.75  Total Training Translation Loss 729.16 
2024-02-05 20:36:34,307 EPOCH 41
2024-02-05 20:36:35,889 [Epoch: 041 Step: 00002700] Batch Recognition Loss:   0.049761 => Gls Tokens per Sec:     2025 || Batch Translation Loss:  11.736375 => Txt Tokens per Sec:     5659 || Lr: 0.000100
2024-02-05 20:36:39,725 Epoch  41: Total Training Recognition Loss 3.77  Total Training Translation Loss 663.04 
2024-02-05 20:36:39,726 EPOCH 42
2024-02-05 20:36:43,265 [Epoch: 042 Step: 00002800] Batch Recognition Loss:   0.032361 => Gls Tokens per Sec:     2369 || Batch Translation Loss:   8.131509 => Txt Tokens per Sec:     6338 || Lr: 0.000100
2024-02-05 20:36:44,684 Epoch  42: Total Training Recognition Loss 3.31  Total Training Translation Loss 594.55 
2024-02-05 20:36:44,684 EPOCH 43
2024-02-05 20:36:50,230 Epoch  43: Total Training Recognition Loss 3.29  Total Training Translation Loss 544.43 
2024-02-05 20:36:50,230 EPOCH 44
2024-02-05 20:36:51,599 [Epoch: 044 Step: 00002900] Batch Recognition Loss:   0.017377 => Gls Tokens per Sec:     2222 || Batch Translation Loss:   6.639381 => Txt Tokens per Sec:     6074 || Lr: 0.000100
2024-02-05 20:36:55,412 Epoch  44: Total Training Recognition Loss 3.14  Total Training Translation Loss 487.53 
2024-02-05 20:36:55,413 EPOCH 45
2024-02-05 20:36:59,287 [Epoch: 045 Step: 00003000] Batch Recognition Loss:   0.034035 => Gls Tokens per Sec:     2122 || Batch Translation Loss:   9.108966 => Txt Tokens per Sec:     5791 || Lr: 0.000100
2024-02-05 20:37:00,636 Epoch  45: Total Training Recognition Loss 3.04  Total Training Translation Loss 448.26 
2024-02-05 20:37:00,637 EPOCH 46
2024-02-05 20:37:05,826 Epoch  46: Total Training Recognition Loss 2.87  Total Training Translation Loss 406.84 
2024-02-05 20:37:05,826 EPOCH 47
2024-02-05 20:37:07,279 [Epoch: 047 Step: 00003100] Batch Recognition Loss:   0.043153 => Gls Tokens per Sec:     1985 || Batch Translation Loss:   6.039704 => Txt Tokens per Sec:     5473 || Lr: 0.000100
2024-02-05 20:37:11,141 Epoch  47: Total Training Recognition Loss 3.01  Total Training Translation Loss 369.10 
2024-02-05 20:37:11,142 EPOCH 48
2024-02-05 20:37:15,424 [Epoch: 048 Step: 00003200] Batch Recognition Loss:   0.022557 => Gls Tokens per Sec:     1883 || Batch Translation Loss:   5.704175 => Txt Tokens per Sec:     5300 || Lr: 0.000100
2024-02-05 20:37:16,553 Epoch  48: Total Training Recognition Loss 2.68  Total Training Translation Loss 328.45 
2024-02-05 20:37:16,553 EPOCH 49
2024-02-05 20:37:21,750 Epoch  49: Total Training Recognition Loss 2.54  Total Training Translation Loss 296.78 
2024-02-05 20:37:21,750 EPOCH 50
2024-02-05 20:37:23,046 [Epoch: 050 Step: 00003300] Batch Recognition Loss:   0.014322 => Gls Tokens per Sec:     2100 || Batch Translation Loss:   3.648035 => Txt Tokens per Sec:     5785 || Lr: 0.000100
2024-02-05 20:37:27,100 Epoch  50: Total Training Recognition Loss 2.60  Total Training Translation Loss 275.83 
2024-02-05 20:37:27,100 EPOCH 51
2024-02-05 20:37:31,006 [Epoch: 051 Step: 00003400] Batch Recognition Loss:   0.022410 => Gls Tokens per Sec:     2024 || Batch Translation Loss:   4.052798 => Txt Tokens per Sec:     5612 || Lr: 0.000100
2024-02-05 20:37:32,388 Epoch  51: Total Training Recognition Loss 2.28  Total Training Translation Loss 251.91 
2024-02-05 20:37:32,389 EPOCH 52
2024-02-05 20:37:37,824 Epoch  52: Total Training Recognition Loss 2.27  Total Training Translation Loss 234.78 
2024-02-05 20:37:37,824 EPOCH 53
2024-02-05 20:37:38,977 [Epoch: 053 Step: 00003500] Batch Recognition Loss:   0.025839 => Gls Tokens per Sec:     2222 || Batch Translation Loss:   2.962451 => Txt Tokens per Sec:     6396 || Lr: 0.000100
2024-02-05 20:37:42,495 Epoch  53: Total Training Recognition Loss 2.30  Total Training Translation Loss 220.79 
2024-02-05 20:37:42,495 EPOCH 54
2024-02-05 20:37:46,525 [Epoch: 054 Step: 00003600] Batch Recognition Loss:   0.024871 => Gls Tokens per Sec:     1921 || Batch Translation Loss:   2.324133 => Txt Tokens per Sec:     5336 || Lr: 0.000100
2024-02-05 20:37:47,928 Epoch  54: Total Training Recognition Loss 2.20  Total Training Translation Loss 204.39 
2024-02-05 20:37:47,928 EPOCH 55
2024-02-05 20:37:53,196 Epoch  55: Total Training Recognition Loss 2.18  Total Training Translation Loss 187.17 
2024-02-05 20:37:53,197 EPOCH 56
2024-02-05 20:37:54,551 [Epoch: 056 Step: 00003700] Batch Recognition Loss:   0.018125 => Gls Tokens per Sec:     1700 || Batch Translation Loss:   2.909931 => Txt Tokens per Sec:     4885 || Lr: 0.000100
2024-02-05 20:37:58,487 Epoch  56: Total Training Recognition Loss 2.13  Total Training Translation Loss 171.61 
2024-02-05 20:37:58,487 EPOCH 57
2024-02-05 20:38:02,326 [Epoch: 057 Step: 00003800] Batch Recognition Loss:   0.058165 => Gls Tokens per Sec:     1975 || Batch Translation Loss:   1.912773 => Txt Tokens per Sec:     5451 || Lr: 0.000100
2024-02-05 20:38:03,813 Epoch  57: Total Training Recognition Loss 2.22  Total Training Translation Loss 155.22 
2024-02-05 20:38:03,814 EPOCH 58
2024-02-05 20:38:09,141 Epoch  58: Total Training Recognition Loss 1.70  Total Training Translation Loss 147.67 
2024-02-05 20:38:09,141 EPOCH 59
2024-02-05 20:38:10,219 [Epoch: 059 Step: 00003900] Batch Recognition Loss:   0.024436 => Gls Tokens per Sec:     2083 || Batch Translation Loss:   1.907344 => Txt Tokens per Sec:     5398 || Lr: 0.000100
2024-02-05 20:38:14,690 Epoch  59: Total Training Recognition Loss 1.76  Total Training Translation Loss 137.65 
2024-02-05 20:38:14,691 EPOCH 60
2024-02-05 20:38:18,725 [Epoch: 060 Step: 00004000] Batch Recognition Loss:   0.026191 => Gls Tokens per Sec:     1840 || Batch Translation Loss:   1.877780 => Txt Tokens per Sec:     5090 || Lr: 0.000100
2024-02-05 20:38:27,324 Validation result at epoch  60, step     4000: duration: 8.5985s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 3.23280	Translation Loss: 74213.03125	PPL: 1656.41003
	Eval Metric: BLEU
	WER 6.64	(DEL: 0.00,	INS: 0.00,	SUB: 6.64)
	BLEU-4 0.68	(BLEU-1: 10.84,	BLEU-2: 3.45,	BLEU-3: 1.35,	BLEU-4: 0.68)
	CHRF 17.09	ROUGE 9.11
2024-02-05 20:38:27,325 Logging Recognition and Translation Outputs
2024-02-05 20:38:27,325 ========================================================================================================================
2024-02-05 20:38:27,325 Logging Sequence: 112_165.00
2024-02-05 20:38:27,325 	Gloss Reference :	A B+C+D+E
2024-02-05 20:38:27,326 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 20:38:27,326 	Gloss Alignment :	         
2024-02-05 20:38:27,326 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:38:27,327 	Text Reference  :	the narendra modi stadium will be **** the home   for the   ahmedabad-based franchise
2024-02-05 20:38:27,327 	Text Hypothesis :	*** ******** **** they    will be sent to  prison for their own             years    
2024-02-05 20:38:27,327 	Text Alignment  :	D   D        D    S               I    S   S          S     S               S        
2024-02-05 20:38:27,327 ========================================================================================================================
2024-02-05 20:38:27,327 Logging Sequence: 176_154.00
2024-02-05 20:38:27,327 	Gloss Reference :	A B+C+D+E
2024-02-05 20:38:27,327 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 20:38:27,328 	Gloss Alignment :	         
2024-02-05 20:38:27,328 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:38:27,328 	Text Reference  :	******* **** *** dahiya could potentially bring        home india's second gold    medal
2024-02-05 20:38:27,329 	Text Hypothesis :	however they had to     play  against     kazakhstan's runs in      the    3-match match
2024-02-05 20:38:27,329 	Text Alignment  :	I       I    I   S      S     S           S            S    S       S      S       S    
2024-02-05 20:38:27,329 ========================================================================================================================
2024-02-05 20:38:27,329 Logging Sequence: 94_2.00
2024-02-05 20:38:27,329 	Gloss Reference :	A B+C+D+E
2024-02-05 20:38:27,329 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 20:38:27,329 	Gloss Alignment :	         
2024-02-05 20:38:27,329 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:38:27,331 	Text Reference  :	the icc odi   men' world cup  2023 will  be      hosted by  india on   5th october 2023   
2024-02-05 20:38:27,331 	Text Hypothesis :	*** *** india did  not   know this match between india  and open  with two tough   wickets
2024-02-05 20:38:27,331 	Text Alignment  :	D   D   S     S    S     S    S    S     S       S      S   S     S    S   S       S      
2024-02-05 20:38:27,331 ========================================================================================================================
2024-02-05 20:38:27,331 Logging Sequence: 165_453.00
2024-02-05 20:38:27,332 	Gloss Reference :	A B+C+D+E
2024-02-05 20:38:27,332 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 20:38:27,332 	Gloss Alignment :	         
2024-02-05 20:38:27,332 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:38:27,333 	Text Reference  :	****** icc did  not      agree   to sehwag' decision of wearing a numberless jersey
2024-02-05 20:38:27,333 	Text Hypothesis :	people had been shocking players to ******* ******** ** ******* * see        him   
2024-02-05 20:38:27,333 	Text Alignment  :	I      S   S    S        S          D       D        D  D       D S          S     
2024-02-05 20:38:27,333 ========================================================================================================================
2024-02-05 20:38:27,333 Logging Sequence: 139_46.00
2024-02-05 20:38:27,333 	Gloss Reference :	A B+C+D+E
2024-02-05 20:38:27,333 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 20:38:27,334 	Gloss Alignment :	         
2024-02-05 20:38:27,334 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:38:27,336 	Text Reference  :	everyone thought it would be a one sided match   because   morocco is        an       amateur team   and   belgium ranks 2nd  in the world
2024-02-05 20:38:27,336 	Text Hypothesis :	******** ******* ** ***** ** * *** 4     another argentine enzo    fernandez recieved the     golden glove for     being held in *** qatar
2024-02-05 20:38:27,336 	Text Alignment  :	D        D       D  D     D  D D   S     S       S         S       S         S        S       S      S     S       S     S       D   S    
2024-02-05 20:38:27,336 ========================================================================================================================
2024-02-05 20:38:28,978 Epoch  60: Total Training Recognition Loss 1.76  Total Training Translation Loss 130.35 
2024-02-05 20:38:28,978 EPOCH 61
2024-02-05 20:38:34,607 Epoch  61: Total Training Recognition Loss 1.72  Total Training Translation Loss 122.72 
2024-02-05 20:38:34,608 EPOCH 62
2024-02-05 20:38:35,604 [Epoch: 062 Step: 00004100] Batch Recognition Loss:   0.027202 => Gls Tokens per Sec:     2090 || Batch Translation Loss:   2.129956 => Txt Tokens per Sec:     5867 || Lr: 0.000100
2024-02-05 20:38:39,803 Epoch  62: Total Training Recognition Loss 1.59  Total Training Translation Loss 112.42 
2024-02-05 20:38:39,803 EPOCH 63
2024-02-05 20:38:43,674 [Epoch: 063 Step: 00004200] Batch Recognition Loss:   0.010925 => Gls Tokens per Sec:     1902 || Batch Translation Loss:   1.279594 => Txt Tokens per Sec:     5315 || Lr: 0.000100
2024-02-05 20:38:45,341 Epoch  63: Total Training Recognition Loss 1.58  Total Training Translation Loss 110.88 
2024-02-05 20:38:45,341 EPOCH 64
2024-02-05 20:38:50,850 Epoch  64: Total Training Recognition Loss 1.47  Total Training Translation Loss 99.89 
2024-02-05 20:38:50,850 EPOCH 65
2024-02-05 20:38:51,683 [Epoch: 065 Step: 00004300] Batch Recognition Loss:   0.035691 => Gls Tokens per Sec:     2310 || Batch Translation Loss:   0.920423 => Txt Tokens per Sec:     6048 || Lr: 0.000100
2024-02-05 20:38:56,368 Epoch  65: Total Training Recognition Loss 1.39  Total Training Translation Loss 98.64 
2024-02-05 20:38:56,369 EPOCH 66
2024-02-05 20:38:59,912 [Epoch: 066 Step: 00004400] Batch Recognition Loss:   0.019340 => Gls Tokens per Sec:     2005 || Batch Translation Loss:   1.749513 => Txt Tokens per Sec:     5605 || Lr: 0.000100
2024-02-05 20:39:01,671 Epoch  66: Total Training Recognition Loss 1.25  Total Training Translation Loss 92.22 
2024-02-05 20:39:01,672 EPOCH 67
2024-02-05 20:39:07,091 Epoch  67: Total Training Recognition Loss 1.31  Total Training Translation Loss 103.36 
2024-02-05 20:39:07,091 EPOCH 68
2024-02-05 20:39:07,927 [Epoch: 068 Step: 00004500] Batch Recognition Loss:   0.013698 => Gls Tokens per Sec:     2108 || Batch Translation Loss:   1.369483 => Txt Tokens per Sec:     5975 || Lr: 0.000100
2024-02-05 20:39:12,539 Epoch  68: Total Training Recognition Loss 1.54  Total Training Translation Loss 97.23 
2024-02-05 20:39:12,539 EPOCH 69
2024-02-05 20:39:16,082 [Epoch: 069 Step: 00004600] Batch Recognition Loss:   0.023761 => Gls Tokens per Sec:     1959 || Batch Translation Loss:   1.547156 => Txt Tokens per Sec:     5533 || Lr: 0.000100
2024-02-05 20:39:17,980 Epoch  69: Total Training Recognition Loss 1.45  Total Training Translation Loss 79.46 
2024-02-05 20:39:17,981 EPOCH 70
2024-02-05 20:39:23,514 Epoch  70: Total Training Recognition Loss 1.23  Total Training Translation Loss 76.35 
2024-02-05 20:39:23,514 EPOCH 71
2024-02-05 20:39:24,463 [Epoch: 071 Step: 00004700] Batch Recognition Loss:   0.025133 => Gls Tokens per Sec:     1688 || Batch Translation Loss:   1.043756 => Txt Tokens per Sec:     5135 || Lr: 0.000100
2024-02-05 20:39:28,818 Epoch  71: Total Training Recognition Loss 1.22  Total Training Translation Loss 72.74 
2024-02-05 20:39:28,819 EPOCH 72
2024-02-05 20:39:32,210 [Epoch: 072 Step: 00004800] Batch Recognition Loss:   0.002205 => Gls Tokens per Sec:     2000 || Batch Translation Loss:   1.063172 => Txt Tokens per Sec:     5487 || Lr: 0.000100
2024-02-05 20:39:34,261 Epoch  72: Total Training Recognition Loss 1.18  Total Training Translation Loss 69.89 
2024-02-05 20:39:34,262 EPOCH 73
2024-02-05 20:39:39,791 Epoch  73: Total Training Recognition Loss 1.23  Total Training Translation Loss 71.12 
2024-02-05 20:39:39,791 EPOCH 74
2024-02-05 20:39:40,707 [Epoch: 074 Step: 00004900] Batch Recognition Loss:   0.045442 => Gls Tokens per Sec:     1576 || Batch Translation Loss:   1.526542 => Txt Tokens per Sec:     4782 || Lr: 0.000100
2024-02-05 20:39:45,376 Epoch  74: Total Training Recognition Loss 1.24  Total Training Translation Loss 69.27 
2024-02-05 20:39:45,377 EPOCH 75
2024-02-05 20:39:48,671 [Epoch: 075 Step: 00005000] Batch Recognition Loss:   0.026963 => Gls Tokens per Sec:     2041 || Batch Translation Loss:   1.038107 => Txt Tokens per Sec:     5732 || Lr: 0.000100
2024-02-05 20:39:50,650 Epoch  75: Total Training Recognition Loss 1.27  Total Training Translation Loss 63.71 
2024-02-05 20:39:50,651 EPOCH 76
2024-02-05 20:39:56,109 Epoch  76: Total Training Recognition Loss 1.01  Total Training Translation Loss 58.26 
2024-02-05 20:39:56,109 EPOCH 77
2024-02-05 20:39:56,652 [Epoch: 077 Step: 00005100] Batch Recognition Loss:   0.010204 => Gls Tokens per Sec:     2366 || Batch Translation Loss:   0.686708 => Txt Tokens per Sec:     6553 || Lr: 0.000100
2024-02-05 20:40:01,161 Epoch  77: Total Training Recognition Loss 1.15  Total Training Translation Loss 58.92 
2024-02-05 20:40:01,162 EPOCH 78
2024-02-05 20:40:04,535 [Epoch: 078 Step: 00005200] Batch Recognition Loss:   0.010473 => Gls Tokens per Sec:     1946 || Batch Translation Loss:   0.923447 => Txt Tokens per Sec:     5306 || Lr: 0.000100
2024-02-05 20:40:06,637 Epoch  78: Total Training Recognition Loss 1.08  Total Training Translation Loss 60.03 
2024-02-05 20:40:06,637 EPOCH 79
2024-02-05 20:40:11,881 Epoch  79: Total Training Recognition Loss 1.05  Total Training Translation Loss 53.72 
2024-02-05 20:40:11,882 EPOCH 80
2024-02-05 20:40:12,529 [Epoch: 080 Step: 00005300] Batch Recognition Loss:   0.004713 => Gls Tokens per Sec:     1736 || Batch Translation Loss:   0.903820 => Txt Tokens per Sec:     5214 || Lr: 0.000100
2024-02-05 20:40:17,258 Epoch  80: Total Training Recognition Loss 1.46  Total Training Translation Loss 53.76 
2024-02-05 20:40:17,258 EPOCH 81
2024-02-05 20:40:20,321 [Epoch: 081 Step: 00005400] Batch Recognition Loss:   0.016127 => Gls Tokens per Sec:     2057 || Batch Translation Loss:   1.109797 => Txt Tokens per Sec:     5794 || Lr: 0.000100
2024-02-05 20:40:22,611 Epoch  81: Total Training Recognition Loss 1.07  Total Training Translation Loss 48.61 
2024-02-05 20:40:22,612 EPOCH 82
2024-02-05 20:40:28,094 Epoch  82: Total Training Recognition Loss 0.87  Total Training Translation Loss 50.45 
2024-02-05 20:40:28,094 EPOCH 83
2024-02-05 20:40:28,526 [Epoch: 083 Step: 00005500] Batch Recognition Loss:   0.041438 => Gls Tokens per Sec:     2227 || Batch Translation Loss:   0.898304 => Txt Tokens per Sec:     6376 || Lr: 0.000100
2024-02-05 20:40:33,059 Epoch  83: Total Training Recognition Loss 1.03  Total Training Translation Loss 46.70 
2024-02-05 20:40:33,060 EPOCH 84
2024-02-05 20:40:36,016 [Epoch: 084 Step: 00005600] Batch Recognition Loss:   0.020671 => Gls Tokens per Sec:     2112 || Batch Translation Loss:   1.014787 => Txt Tokens per Sec:     5765 || Lr: 0.000100
2024-02-05 20:40:38,339 Epoch  84: Total Training Recognition Loss 0.90  Total Training Translation Loss 50.81 
2024-02-05 20:40:38,339 EPOCH 85
2024-02-05 20:40:43,558 Epoch  85: Total Training Recognition Loss 1.02  Total Training Translation Loss 47.99 
2024-02-05 20:40:43,559 EPOCH 86
2024-02-05 20:40:43,887 [Epoch: 086 Step: 00005700] Batch Recognition Loss:   0.022024 => Gls Tokens per Sec:     2446 || Batch Translation Loss:   0.220692 => Txt Tokens per Sec:     6865 || Lr: 0.000100
2024-02-05 20:40:48,580 Epoch  86: Total Training Recognition Loss 1.07  Total Training Translation Loss 43.31 
2024-02-05 20:40:48,580 EPOCH 87
2024-02-05 20:40:51,421 [Epoch: 087 Step: 00005800] Batch Recognition Loss:   0.003018 => Gls Tokens per Sec:     2105 || Batch Translation Loss:   0.600937 => Txt Tokens per Sec:     5705 || Lr: 0.000100
2024-02-05 20:40:53,878 Epoch  87: Total Training Recognition Loss 0.85  Total Training Translation Loss 39.46 
2024-02-05 20:40:53,878 EPOCH 88
2024-02-05 20:40:58,760 Epoch  88: Total Training Recognition Loss 0.80  Total Training Translation Loss 41.82 
2024-02-05 20:40:58,760 EPOCH 89
2024-02-05 20:40:58,983 [Epoch: 089 Step: 00005900] Batch Recognition Loss:   0.009952 => Gls Tokens per Sec:     2883 || Batch Translation Loss:   0.282025 => Txt Tokens per Sec:     6815 || Lr: 0.000100
2024-02-05 20:41:04,018 Epoch  89: Total Training Recognition Loss 0.92  Total Training Translation Loss 40.71 
2024-02-05 20:41:04,019 EPOCH 90
2024-02-05 20:41:06,861 [Epoch: 090 Step: 00006000] Batch Recognition Loss:   0.073101 => Gls Tokens per Sec:     2049 || Batch Translation Loss:   0.467420 => Txt Tokens per Sec:     5721 || Lr: 0.000100
2024-02-05 20:41:15,286 Validation result at epoch  90, step     6000: duration: 8.4247s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 3.43156	Translation Loss: 81836.67969	PPL: 3547.00439
	Eval Metric: BLEU
	WER 6.07	(DEL: 0.00,	INS: 0.00,	SUB: 6.07)
	BLEU-4 0.84	(BLEU-1: 11.31,	BLEU-2: 3.81,	BLEU-3: 1.63,	BLEU-4: 0.84)
	CHRF 16.76	ROUGE 9.80
2024-02-05 20:41:15,287 Logging Recognition and Translation Outputs
2024-02-05 20:41:15,287 ========================================================================================================================
2024-02-05 20:41:15,287 Logging Sequence: 160_153.00
2024-02-05 20:41:15,288 	Gloss Reference :	A B+C+D+E
2024-02-05 20:41:15,288 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 20:41:15,288 	Gloss Alignment :	         
2024-02-05 20:41:15,288 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:41:15,290 	Text Reference  :	i have no hard feelings towards rohit sharma and       he will         always have     my  full support  as he   is  my   teammate
2024-02-05 20:41:15,290 	Text Hypothesis :	* **** ** **** isn't    that    such  a      statement of broadcasting the    decision for his  decision to play the next day     
2024-02-05 20:41:15,290 	Text Alignment  :	D D    D  D    S        S       S     S      S         S  S            S      S        S   S    S        S  S    S   S    S       
2024-02-05 20:41:15,290 ========================================================================================================================
2024-02-05 20:41:15,291 Logging Sequence: 103_253.00
2024-02-05 20:41:15,291 	Gloss Reference :	A B+C+D+E
2024-02-05 20:41:15,291 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 20:41:15,291 	Gloss Alignment :	         
2024-02-05 20:41:15,291 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:41:15,292 	Text Reference  :	***** canada is   3rd with    92     medals
2024-02-05 20:41:15,292 	Text Hypothesis :	these games  were the british empire games 
2024-02-05 20:41:15,292 	Text Alignment  :	I     S      S    S   S       S      S     
2024-02-05 20:41:15,292 ========================================================================================================================
2024-02-05 20:41:15,292 Logging Sequence: 155_25.00
2024-02-05 20:41:15,292 	Gloss Reference :	A B+C+D+E
2024-02-05 20:41:15,292 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 20:41:15,293 	Gloss Alignment :	         
2024-02-05 20:41:15,293 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:41:15,294 	Text Reference  :	this is because taliban      overthrew the afghan government and **** **** took over  the country
2024-02-05 20:41:15,294 	Text Hypothesis :	i    am very    disappointed by        the ****** news       and they will be   given a   crime  
2024-02-05 20:41:15,294 	Text Alignment  :	S    S  S       S            S             D      S              I    I    S    S     S   S      
2024-02-05 20:41:15,294 ========================================================================================================================
2024-02-05 20:41:15,294 Logging Sequence: 81_105.00
2024-02-05 20:41:15,295 	Gloss Reference :	A B+C+D+E
2024-02-05 20:41:15,295 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 20:41:15,295 	Gloss Alignment :	         
2024-02-05 20:41:15,295 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:41:15,295 	Text Reference  :	dhoni was tagged in multiple such posts as he   was  the   brand        ambassador
2024-02-05 20:41:15,296 	Text Hypothesis :	***** *** ****** ** ******** **** ***** ** many find their sponsorships william   
2024-02-05 20:41:15,296 	Text Alignment  :	D     D   D      D  D        D    D     D  S    S    S     S            S         
2024-02-05 20:41:15,296 ========================================================================================================================
2024-02-05 20:41:15,296 Logging Sequence: 105_136.00
2024-02-05 20:41:15,296 	Gloss Reference :	A B+C+D+E
2024-02-05 20:41:15,296 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 20:41:15,296 	Gloss Alignment :	         
2024-02-05 20:41:15,297 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:41:15,297 	Text Reference  :	** **** *** beating him  once is  my   biggest dream
2024-02-05 20:41:15,297 	Text Hypothesis :	as they are not     meet out  for them as      well 
2024-02-05 20:41:15,297 	Text Alignment  :	I  I    I   S       S    S    S   S    S       S    
2024-02-05 20:41:15,298 ========================================================================================================================
2024-02-05 20:41:17,666 Epoch  90: Total Training Recognition Loss 0.83  Total Training Translation Loss 42.31 
2024-02-05 20:41:17,667 EPOCH 91
2024-02-05 20:41:23,195 Epoch  91: Total Training Recognition Loss 0.84  Total Training Translation Loss 46.80 
2024-02-05 20:41:23,196 EPOCH 92
2024-02-05 20:41:23,431 [Epoch: 092 Step: 00006100] Batch Recognition Loss:   0.008704 => Gls Tokens per Sec:     2051 || Batch Translation Loss:   0.588624 => Txt Tokens per Sec:     6667 || Lr: 0.000100
2024-02-05 20:41:28,445 Epoch  92: Total Training Recognition Loss 0.90  Total Training Translation Loss 43.99 
2024-02-05 20:41:28,446 EPOCH 93
2024-02-05 20:41:31,186 [Epoch: 093 Step: 00006200] Batch Recognition Loss:   0.008029 => Gls Tokens per Sec:     2068 || Batch Translation Loss:   0.758328 => Txt Tokens per Sec:     5622 || Lr: 0.000100
2024-02-05 20:41:33,801 Epoch  93: Total Training Recognition Loss 0.86  Total Training Translation Loss 46.97 
2024-02-05 20:41:33,801 EPOCH 94
2024-02-05 20:41:38,769 Epoch  94: Total Training Recognition Loss 0.67  Total Training Translation Loss 42.00 
2024-02-05 20:41:38,770 EPOCH 95
2024-02-05 20:41:38,971 [Epoch: 095 Step: 00006300] Batch Recognition Loss:   0.007166 => Gls Tokens per Sec:     1600 || Batch Translation Loss:   0.356259 => Txt Tokens per Sec:     4365 || Lr: 0.000100
2024-02-05 20:41:44,365 Epoch  95: Total Training Recognition Loss 0.84  Total Training Translation Loss 35.18 
2024-02-05 20:41:44,365 EPOCH 96
2024-02-05 20:41:46,820 [Epoch: 096 Step: 00006400] Batch Recognition Loss:   0.018104 => Gls Tokens per Sec:     2241 || Batch Translation Loss:   0.865618 => Txt Tokens per Sec:     6203 || Lr: 0.000100
2024-02-05 20:41:49,342 Epoch  96: Total Training Recognition Loss 0.85  Total Training Translation Loss 41.91 
2024-02-05 20:41:49,343 EPOCH 97
2024-02-05 20:41:54,690 Epoch  97: Total Training Recognition Loss 0.71  Total Training Translation Loss 37.81 
2024-02-05 20:41:54,690 EPOCH 98
2024-02-05 20:41:54,750 [Epoch: 098 Step: 00006500] Batch Recognition Loss:   0.010690 => Gls Tokens per Sec:     2712 || Batch Translation Loss:   0.558576 => Txt Tokens per Sec:     7796 || Lr: 0.000100
2024-02-05 20:41:59,528 Epoch  98: Total Training Recognition Loss 0.73  Total Training Translation Loss 35.28 
2024-02-05 20:41:59,528 EPOCH 99
2024-02-05 20:42:02,203 [Epoch: 099 Step: 00006600] Batch Recognition Loss:   0.039433 => Gls Tokens per Sec:     1997 || Batch Translation Loss:   0.518577 => Txt Tokens per Sec:     5323 || Lr: 0.000100
2024-02-05 20:42:05,039 Epoch  99: Total Training Recognition Loss 0.80  Total Training Translation Loss 31.42 
2024-02-05 20:42:05,039 EPOCH 100
2024-02-05 20:42:10,191 [Epoch: 100 Step: 00006700] Batch Recognition Loss:   0.010445 => Gls Tokens per Sec:     2062 || Batch Translation Loss:   0.364635 => Txt Tokens per Sec:     5704 || Lr: 0.000100
2024-02-05 20:42:10,192 Epoch 100: Total Training Recognition Loss 0.65  Total Training Translation Loss 29.09 
2024-02-05 20:42:10,192 EPOCH 101
2024-02-05 20:42:15,418 Epoch 101: Total Training Recognition Loss 0.72  Total Training Translation Loss 25.98 
2024-02-05 20:42:15,418 EPOCH 102
2024-02-05 20:42:17,942 [Epoch: 102 Step: 00006800] Batch Recognition Loss:   0.001132 => Gls Tokens per Sec:     2053 || Batch Translation Loss:   0.564529 => Txt Tokens per Sec:     5843 || Lr: 0.000100
2024-02-05 20:42:20,642 Epoch 102: Total Training Recognition Loss 0.60  Total Training Translation Loss 28.72 
2024-02-05 20:42:20,643 EPOCH 103
2024-02-05 20:42:25,352 [Epoch: 103 Step: 00006900] Batch Recognition Loss:   0.001929 => Gls Tokens per Sec:     2222 || Batch Translation Loss:   0.378288 => Txt Tokens per Sec:     6123 || Lr: 0.000100
2024-02-05 20:42:25,515 Epoch 103: Total Training Recognition Loss 0.75  Total Training Translation Loss 31.18 
2024-02-05 20:42:25,515 EPOCH 104
2024-02-05 20:42:30,764 Epoch 104: Total Training Recognition Loss 0.83  Total Training Translation Loss 27.82 
2024-02-05 20:42:30,765 EPOCH 105
2024-02-05 20:42:33,165 [Epoch: 105 Step: 00007000] Batch Recognition Loss:   0.001816 => Gls Tokens per Sec:     2135 || Batch Translation Loss:   0.351372 => Txt Tokens per Sec:     5826 || Lr: 0.000100
2024-02-05 20:42:35,881 Epoch 105: Total Training Recognition Loss 0.96  Total Training Translation Loss 25.52 
2024-02-05 20:42:35,881 EPOCH 106
2024-02-05 20:42:40,960 [Epoch: 106 Step: 00007100] Batch Recognition Loss:   0.002041 => Gls Tokens per Sec:     2029 || Batch Translation Loss:   0.872590 => Txt Tokens per Sec:     5578 || Lr: 0.000100
2024-02-05 20:42:41,181 Epoch 106: Total Training Recognition Loss 3.59  Total Training Translation Loss 31.35 
2024-02-05 20:42:41,181 EPOCH 107
2024-02-05 20:42:46,175 Epoch 107: Total Training Recognition Loss 1.19  Total Training Translation Loss 33.47 
2024-02-05 20:42:46,175 EPOCH 108
2024-02-05 20:42:48,679 [Epoch: 108 Step: 00007200] Batch Recognition Loss:   0.001181 => Gls Tokens per Sec:     1983 || Batch Translation Loss:   0.331060 => Txt Tokens per Sec:     5219 || Lr: 0.000100
2024-02-05 20:42:51,569 Epoch 108: Total Training Recognition Loss 0.80  Total Training Translation Loss 31.29 
2024-02-05 20:42:51,569 EPOCH 109
2024-02-05 20:42:56,382 [Epoch: 109 Step: 00007300] Batch Recognition Loss:   0.005350 => Gls Tokens per Sec:     2107 || Batch Translation Loss:   0.310973 => Txt Tokens per Sec:     5840 || Lr: 0.000100
2024-02-05 20:42:56,656 Epoch 109: Total Training Recognition Loss 0.58  Total Training Translation Loss 29.34 
2024-02-05 20:42:56,656 EPOCH 110
2024-02-05 20:43:02,113 Epoch 110: Total Training Recognition Loss 0.58  Total Training Translation Loss 29.21 
2024-02-05 20:43:02,114 EPOCH 111
2024-02-05 20:43:04,431 [Epoch: 111 Step: 00007400] Batch Recognition Loss:   0.001753 => Gls Tokens per Sec:     2072 || Batch Translation Loss:   0.211361 => Txt Tokens per Sec:     5990 || Lr: 0.000100
2024-02-05 20:43:06,969 Epoch 111: Total Training Recognition Loss 0.61  Total Training Translation Loss 29.24 
2024-02-05 20:43:06,970 EPOCH 112
2024-02-05 20:43:12,120 [Epoch: 112 Step: 00007500] Batch Recognition Loss:   0.010247 => Gls Tokens per Sec:     1938 || Batch Translation Loss:   0.225765 => Txt Tokens per Sec:     5306 || Lr: 0.000100
2024-02-05 20:43:12,485 Epoch 112: Total Training Recognition Loss 0.47  Total Training Translation Loss 24.15 
2024-02-05 20:43:12,485 EPOCH 113
2024-02-05 20:43:17,185 Epoch 113: Total Training Recognition Loss 0.43  Total Training Translation Loss 24.50 
2024-02-05 20:43:17,185 EPOCH 114
2024-02-05 20:43:19,033 [Epoch: 114 Step: 00007600] Batch Recognition Loss:   0.004430 => Gls Tokens per Sec:     2512 || Batch Translation Loss:   0.513385 => Txt Tokens per Sec:     6871 || Lr: 0.000100
2024-02-05 20:43:21,888 Epoch 114: Total Training Recognition Loss 0.43  Total Training Translation Loss 22.08 
2024-02-05 20:43:21,888 EPOCH 115
2024-02-05 20:43:26,219 [Epoch: 115 Step: 00007700] Batch Recognition Loss:   0.029214 => Gls Tokens per Sec:     2268 || Batch Translation Loss:   0.424294 => Txt Tokens per Sec:     6289 || Lr: 0.000100
2024-02-05 20:43:26,543 Epoch 115: Total Training Recognition Loss 0.46  Total Training Translation Loss 23.58 
2024-02-05 20:43:26,543 EPOCH 116
2024-02-05 20:43:31,427 Epoch 116: Total Training Recognition Loss 0.58  Total Training Translation Loss 26.33 
2024-02-05 20:43:31,427 EPOCH 117
2024-02-05 20:43:33,636 [Epoch: 117 Step: 00007800] Batch Recognition Loss:   0.004505 => Gls Tokens per Sec:     2029 || Batch Translation Loss:   0.349572 => Txt Tokens per Sec:     5640 || Lr: 0.000100
2024-02-05 20:43:36,962 Epoch 117: Total Training Recognition Loss 0.67  Total Training Translation Loss 34.99 
2024-02-05 20:43:36,962 EPOCH 118
2024-02-05 20:43:41,742 [Epoch: 118 Step: 00007900] Batch Recognition Loss:   0.008506 => Gls Tokens per Sec:     2022 || Batch Translation Loss:   0.326621 => Txt Tokens per Sec:     5670 || Lr: 0.000100
2024-02-05 20:43:42,158 Epoch 118: Total Training Recognition Loss 0.56  Total Training Translation Loss 25.80 
2024-02-05 20:43:42,158 EPOCH 119
2024-02-05 20:43:47,213 Epoch 119: Total Training Recognition Loss 0.58  Total Training Translation Loss 20.60 
2024-02-05 20:43:47,213 EPOCH 120
2024-02-05 20:43:48,989 [Epoch: 120 Step: 00008000] Batch Recognition Loss:   0.011464 => Gls Tokens per Sec:     2377 || Batch Translation Loss:   0.345965 => Txt Tokens per Sec:     6397 || Lr: 0.000100
2024-02-05 20:43:57,395 Validation result at epoch 120, step     8000: duration: 8.4062s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 3.32115	Translation Loss: 86544.20312	PPL: 5676.25391
	Eval Metric: BLEU
	WER 5.93	(DEL: 0.00,	INS: 0.00,	SUB: 5.93)
	BLEU-4 0.75	(BLEU-1: 11.23,	BLEU-2: 3.75,	BLEU-3: 1.65,	BLEU-4: 0.75)
	CHRF 17.34	ROUGE 9.65
2024-02-05 20:43:57,396 Logging Recognition and Translation Outputs
2024-02-05 20:43:57,396 ========================================================================================================================
2024-02-05 20:43:57,397 Logging Sequence: 180_236.00
2024-02-05 20:43:57,397 	Gloss Reference :	A B+C+D+E
2024-02-05 20:43:57,397 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 20:43:57,397 	Gloss Alignment :	         
2024-02-05 20:43:57,397 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:43:57,398 	Text Reference  :	however the wrestlers returned to the protest  site    at   jantar mantar  with thier    demands   
2024-02-05 20:43:57,398 	Text Hypothesis :	******* *** wrestlers wanted   to *** practise however they are    ruining his  family's reputation
2024-02-05 20:43:57,399 	Text Alignment  :	D       D             S           D   S        S       S    S      S       S    S        S         
2024-02-05 20:43:57,399 ========================================================================================================================
2024-02-05 20:43:57,399 Logging Sequence: 111_154.00
2024-02-05 20:43:57,399 	Gloss Reference :	A B+C+D+E  
2024-02-05 20:43:57,399 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-05 20:43:57,399 	Gloss Alignment :	  S        
2024-02-05 20:43:57,399 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:43:57,400 	Text Reference  :	***** ****** due to   csk's slow    over rate dhoni was     fined    rs   12  lakh   
2024-02-05 20:43:57,401 	Text Hypothesis :	rohit sharma was seen the   captain of   his  16    players infected with the captain
2024-02-05 20:43:57,401 	Text Alignment  :	I     I      S   S    S     S       S    S    S     S       S        S    S   S      
2024-02-05 20:43:57,401 ========================================================================================================================
2024-02-05 20:43:57,401 Logging Sequence: 118_314.00
2024-02-05 20:43:57,401 	Gloss Reference :	A B+C+D+E
2024-02-05 20:43:57,401 	Gloss Hypothesis:	A B+C+D  
2024-02-05 20:43:57,401 	Gloss Alignment :	  S      
2024-02-05 20:43:57,401 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:43:57,402 	Text Reference  :	**** wow even the president had come  to  watch  
2024-02-05 20:43:57,402 	Text Hypothesis :	what is  why  the ********* *** match are delayed
2024-02-05 20:43:57,402 	Text Alignment  :	I    S   S        D         D   S     S   S      
2024-02-05 20:43:57,402 ========================================================================================================================
2024-02-05 20:43:57,403 Logging Sequence: 156_197.00
2024-02-05 20:43:57,403 	Gloss Reference :	A B+C+D+E
2024-02-05 20:43:57,403 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 20:43:57,403 	Gloss Alignment :	         
2024-02-05 20:43:57,403 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:43:57,404 	Text Reference  :	seattle orcas sor is owned by many investors including satya nadella microsoft ceo    
2024-02-05 20:43:57,404 	Text Hypothesis :	******* ***** *** ** ***** ** we   will      have      to    wait    for       updates
2024-02-05 20:43:57,404 	Text Alignment  :	D       D     D   D  D     D  S    S         S         S     S       S         S      
2024-02-05 20:43:57,404 ========================================================================================================================
2024-02-05 20:43:57,404 Logging Sequence: 183_159.00
2024-02-05 20:43:57,404 	Gloss Reference :	A B+C+D+E  
2024-02-05 20:43:57,405 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-05 20:43:57,405 	Gloss Alignment :	  S        
2024-02-05 20:43:57,405 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:43:57,407 	Text Reference  :	however an      exception to  this is   virat kohli     and his wife anushka sharma who refuse to share images of    their daughter
2024-02-05 20:43:57,407 	Text Hypothesis :	new     zealand players   get a    huge fan   following and his wife ******* ****** *** ****** ** ***** ****** hazel keech singh   
2024-02-05 20:43:57,407 	Text Alignment  :	S       S       S         S   S    S    S     S                      D       D      D   D      D  D     D      S     S     S       
2024-02-05 20:43:57,407 ========================================================================================================================
2024-02-05 20:44:00,710 Epoch 120: Total Training Recognition Loss 0.40  Total Training Translation Loss 19.30 
2024-02-05 20:44:00,710 EPOCH 121
2024-02-05 20:44:05,648 [Epoch: 121 Step: 00008100] Batch Recognition Loss:   0.006570 => Gls Tokens per Sec:     1924 || Batch Translation Loss:   0.337262 => Txt Tokens per Sec:     5350 || Lr: 0.000100
2024-02-05 20:44:06,157 Epoch 121: Total Training Recognition Loss 0.42  Total Training Translation Loss 17.45 
2024-02-05 20:44:06,157 EPOCH 122
2024-02-05 20:44:11,070 Epoch 122: Total Training Recognition Loss 0.40  Total Training Translation Loss 18.88 
2024-02-05 20:44:11,071 EPOCH 123
2024-02-05 20:44:13,306 [Epoch: 123 Step: 00008200] Batch Recognition Loss:   0.003449 => Gls Tokens per Sec:     1817 || Batch Translation Loss:   0.171234 => Txt Tokens per Sec:     5013 || Lr: 0.000100
2024-02-05 20:44:16,574 Epoch 123: Total Training Recognition Loss 0.40  Total Training Translation Loss 17.73 
2024-02-05 20:44:16,574 EPOCH 124
2024-02-05 20:44:20,524 [Epoch: 124 Step: 00008300] Batch Recognition Loss:   0.001640 => Gls Tokens per Sec:     2366 || Batch Translation Loss:   0.175076 => Txt Tokens per Sec:     6430 || Lr: 0.000100
2024-02-05 20:44:21,363 Epoch 124: Total Training Recognition Loss 0.45  Total Training Translation Loss 18.82 
2024-02-05 20:44:21,363 EPOCH 125
2024-02-05 20:44:26,808 Epoch 125: Total Training Recognition Loss 0.49  Total Training Translation Loss 20.15 
2024-02-05 20:44:26,809 EPOCH 126
2024-02-05 20:44:28,734 [Epoch: 126 Step: 00008400] Batch Recognition Loss:   0.021333 => Gls Tokens per Sec:     2079 || Batch Translation Loss:   0.367626 => Txt Tokens per Sec:     6094 || Lr: 0.000100
2024-02-05 20:44:31,787 Epoch 126: Total Training Recognition Loss 0.57  Total Training Translation Loss 19.85 
2024-02-05 20:44:31,787 EPOCH 127
2024-02-05 20:44:36,615 [Epoch: 127 Step: 00008500] Batch Recognition Loss:   0.005927 => Gls Tokens per Sec:     1902 || Batch Translation Loss:   0.274163 => Txt Tokens per Sec:     5359 || Lr: 0.000100
2024-02-05 20:44:37,318 Epoch 127: Total Training Recognition Loss 0.53  Total Training Translation Loss 24.29 
2024-02-05 20:44:37,318 EPOCH 128
2024-02-05 20:44:42,733 Epoch 128: Total Training Recognition Loss 0.51  Total Training Translation Loss 19.34 
2024-02-05 20:44:42,734 EPOCH 129
2024-02-05 20:44:44,311 [Epoch: 129 Step: 00008600] Batch Recognition Loss:   0.001752 => Gls Tokens per Sec:     2437 || Batch Translation Loss:   0.292465 => Txt Tokens per Sec:     6619 || Lr: 0.000100
2024-02-05 20:44:47,397 Epoch 129: Total Training Recognition Loss 0.31  Total Training Translation Loss 20.11 
2024-02-05 20:44:47,397 EPOCH 130
2024-02-05 20:44:51,851 [Epoch: 130 Step: 00008700] Batch Recognition Loss:   0.027466 => Gls Tokens per Sec:     2048 || Batch Translation Loss:   0.096530 => Txt Tokens per Sec:     5676 || Lr: 0.000100
2024-02-05 20:44:52,679 Epoch 130: Total Training Recognition Loss 0.44  Total Training Translation Loss 19.53 
2024-02-05 20:44:52,680 EPOCH 131
2024-02-05 20:44:57,784 Epoch 131: Total Training Recognition Loss 0.44  Total Training Translation Loss 22.67 
2024-02-05 20:44:57,785 EPOCH 132
2024-02-05 20:44:59,693 [Epoch: 132 Step: 00008800] Batch Recognition Loss:   0.009749 => Gls Tokens per Sec:     1877 || Batch Translation Loss:   0.550833 => Txt Tokens per Sec:     5187 || Lr: 0.000100
2024-02-05 20:45:03,212 Epoch 132: Total Training Recognition Loss 0.44  Total Training Translation Loss 26.05 
2024-02-05 20:45:03,212 EPOCH 133
2024-02-05 20:45:07,238 [Epoch: 133 Step: 00008900] Batch Recognition Loss:   0.001490 => Gls Tokens per Sec:     2201 || Batch Translation Loss:   0.679808 => Txt Tokens per Sec:     6044 || Lr: 0.000100
2024-02-05 20:45:08,017 Epoch 133: Total Training Recognition Loss 0.55  Total Training Translation Loss 26.14 
2024-02-05 20:45:08,017 EPOCH 134
2024-02-05 20:45:13,563 Epoch 134: Total Training Recognition Loss 0.44  Total Training Translation Loss 20.95 
2024-02-05 20:45:13,564 EPOCH 135
2024-02-05 20:45:15,111 [Epoch: 135 Step: 00009000] Batch Recognition Loss:   0.002261 => Gls Tokens per Sec:     2276 || Batch Translation Loss:   0.281222 => Txt Tokens per Sec:     5983 || Lr: 0.000100
2024-02-05 20:45:18,487 Epoch 135: Total Training Recognition Loss 0.46  Total Training Translation Loss 17.80 
2024-02-05 20:45:18,488 EPOCH 136
2024-02-05 20:45:22,696 [Epoch: 136 Step: 00009100] Batch Recognition Loss:   0.014792 => Gls Tokens per Sec:     2068 || Batch Translation Loss:   0.109298 => Txt Tokens per Sec:     5596 || Lr: 0.000100
2024-02-05 20:45:23,866 Epoch 136: Total Training Recognition Loss 0.39  Total Training Translation Loss 16.79 
2024-02-05 20:45:23,866 EPOCH 137
2024-02-05 20:45:28,827 Epoch 137: Total Training Recognition Loss 0.40  Total Training Translation Loss 16.67 
2024-02-05 20:45:28,828 EPOCH 138
2024-02-05 20:45:30,519 [Epoch: 138 Step: 00009200] Batch Recognition Loss:   0.004909 => Gls Tokens per Sec:     1989 || Batch Translation Loss:   0.558713 => Txt Tokens per Sec:     5583 || Lr: 0.000100
2024-02-05 20:45:34,074 Epoch 138: Total Training Recognition Loss 0.33  Total Training Translation Loss 20.23 
2024-02-05 20:45:34,074 EPOCH 139
2024-02-05 20:45:38,157 [Epoch: 139 Step: 00009300] Batch Recognition Loss:   0.003749 => Gls Tokens per Sec:     2092 || Batch Translation Loss:   0.575873 => Txt Tokens per Sec:     5790 || Lr: 0.000100
2024-02-05 20:45:39,201 Epoch 139: Total Training Recognition Loss 0.40  Total Training Translation Loss 19.65 
2024-02-05 20:45:39,202 EPOCH 140
2024-02-05 20:45:44,646 Epoch 140: Total Training Recognition Loss 0.49  Total Training Translation Loss 24.44 
2024-02-05 20:45:44,647 EPOCH 141
2024-02-05 20:45:45,907 [Epoch: 141 Step: 00009400] Batch Recognition Loss:   0.003430 => Gls Tokens per Sec:     2539 || Batch Translation Loss:   0.208106 => Txt Tokens per Sec:     6674 || Lr: 0.000100
2024-02-05 20:45:49,802 Epoch 141: Total Training Recognition Loss 0.40  Total Training Translation Loss 17.53 
2024-02-05 20:45:49,802 EPOCH 142
2024-02-05 20:45:53,559 [Epoch: 142 Step: 00009500] Batch Recognition Loss:   0.003937 => Gls Tokens per Sec:     2231 || Batch Translation Loss:   0.168007 => Txt Tokens per Sec:     6015 || Lr: 0.000100
2024-02-05 20:45:54,798 Epoch 142: Total Training Recognition Loss 0.51  Total Training Translation Loss 16.15 
2024-02-05 20:45:54,798 EPOCH 143
2024-02-05 20:46:00,158 Epoch 143: Total Training Recognition Loss 0.38  Total Training Translation Loss 17.80 
2024-02-05 20:46:00,158 EPOCH 144
2024-02-05 20:46:01,777 [Epoch: 144 Step: 00009600] Batch Recognition Loss:   0.008651 => Gls Tokens per Sec:     1878 || Batch Translation Loss:   0.181214 => Txt Tokens per Sec:     5406 || Lr: 0.000100
2024-02-05 20:46:05,166 Epoch 144: Total Training Recognition Loss 0.42  Total Training Translation Loss 15.13 
2024-02-05 20:46:05,166 EPOCH 145
2024-02-05 20:46:09,292 [Epoch: 145 Step: 00009700] Batch Recognition Loss:   0.010215 => Gls Tokens per Sec:     2017 || Batch Translation Loss:   0.102153 => Txt Tokens per Sec:     5548 || Lr: 0.000100
2024-02-05 20:46:10,535 Epoch 145: Total Training Recognition Loss 0.37  Total Training Translation Loss 13.99 
2024-02-05 20:46:10,536 EPOCH 146
2024-02-05 20:46:15,405 Epoch 146: Total Training Recognition Loss 0.46  Total Training Translation Loss 16.99 
2024-02-05 20:46:15,406 EPOCH 147
2024-02-05 20:46:16,704 [Epoch: 147 Step: 00009800] Batch Recognition Loss:   0.000984 => Gls Tokens per Sec:     2221 || Batch Translation Loss:   0.189676 => Txt Tokens per Sec:     6228 || Lr: 0.000100
2024-02-05 20:46:20,818 Epoch 147: Total Training Recognition Loss 0.30  Total Training Translation Loss 16.67 
2024-02-05 20:46:20,819 EPOCH 148
2024-02-05 20:46:24,606 [Epoch: 148 Step: 00009900] Batch Recognition Loss:   0.001238 => Gls Tokens per Sec:     2129 || Batch Translation Loss:   0.172897 => Txt Tokens per Sec:     5920 || Lr: 0.000100
2024-02-05 20:46:25,815 Epoch 148: Total Training Recognition Loss 0.38  Total Training Translation Loss 17.96 
2024-02-05 20:46:25,815 EPOCH 149
2024-02-05 20:46:31,520 Epoch 149: Total Training Recognition Loss 0.32  Total Training Translation Loss 23.35 
2024-02-05 20:46:31,520 EPOCH 150
2024-02-05 20:46:32,777 [Epoch: 150 Step: 00010000] Batch Recognition Loss:   0.002080 => Gls Tokens per Sec:     2088 || Batch Translation Loss:   0.158512 => Txt Tokens per Sec:     5727 || Lr: 0.000100
2024-02-05 20:46:41,448 Validation result at epoch 150, step    10000: duration: 8.6711s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 3.30891	Translation Loss: 89999.78906	PPL: 8015.97754
	Eval Metric: BLEU
	WER 5.44	(DEL: 0.00,	INS: 0.00,	SUB: 5.44)
	BLEU-4 0.49	(BLEU-1: 10.89,	BLEU-2: 3.03,	BLEU-3: 1.12,	BLEU-4: 0.49)
	CHRF 16.99	ROUGE 9.44
2024-02-05 20:46:41,449 Logging Recognition and Translation Outputs
2024-02-05 20:46:41,449 ========================================================================================================================
2024-02-05 20:46:41,449 Logging Sequence: 123_147.00
2024-02-05 20:46:41,449 	Gloss Reference :	A B+C+D+E
2024-02-05 20:46:41,449 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 20:46:41,450 	Gloss Alignment :	         
2024-02-05 20:46:41,450 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:46:41,450 	Text Reference  :	the former captain also owns the pontiac firebird trans am    car worth rs 68     lakh      
2024-02-05 20:46:41,451 	Text Hypothesis :	*** ****** ******* **** **** *** ******* ******** ***** dhoni has been  an ardent bike-lover
2024-02-05 20:46:41,451 	Text Alignment  :	D   D      D       D    D    D   D       D        D     S     S   S     S  S      S         
2024-02-05 20:46:41,451 ========================================================================================================================
2024-02-05 20:46:41,451 Logging Sequence: 58_196.00
2024-02-05 20:46:41,451 	Gloss Reference :	A B+C+D+E
2024-02-05 20:46:41,451 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 20:46:41,451 	Gloss Alignment :	         
2024-02-05 20:46:41,451 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:46:41,452 	Text Reference  :	the talents and     skills of  our athletes knows no bounds
2024-02-05 20:46:41,452 	Text Hypothesis :	*** two     cricket team   was in  england  for   a  games 
2024-02-05 20:46:41,452 	Text Alignment  :	D   S       S       S      S   S   S        S     S  S     
2024-02-05 20:46:41,452 ========================================================================================================================
2024-02-05 20:46:41,453 Logging Sequence: 168_184.00
2024-02-05 20:46:41,453 	Gloss Reference :	A B+C+D+E    
2024-02-05 20:46:41,453 	Gloss Hypothesis:	A B+C+D+E+D+E
2024-02-05 20:46:41,453 	Gloss Alignment :	  S          
2024-02-05 20:46:41,453 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:46:41,455 	Text Reference  :	people ** say that     we  may        get a    true glimpse of         vamika in february 2022 when she turns 1 year old
2024-02-05 20:46:41,455 	Text Hypothesis :	people do you remember the tournament so  many find their   respective teams  in ******** **** **** *** ***** * **** ipl
2024-02-05 20:46:41,455 	Text Alignment  :	       I  S   S        S   S          S   S    S    S       S          S         D        D    D    D   D     D D    S  
2024-02-05 20:46:41,455 ========================================================================================================================
2024-02-05 20:46:41,455 Logging Sequence: 87_123.00
2024-02-05 20:46:41,455 	Gloss Reference :	A B+C+D+E
2024-02-05 20:46:41,456 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 20:46:41,456 	Gloss Alignment :	         
2024-02-05 20:46:41,456 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:46:41,457 	Text Reference  :	**** *** he said that he    hoped kl     rahul would       be     fit for  the upcoming world cup    
2024-02-05 20:46:41,457 	Text Hypothesis :	well let me tell you  about the   middle east  respiratory system or  mers you can      be    working
2024-02-05 20:46:41,458 	Text Alignment  :	I    I   S  S    S    S     S     S      S     S           S      S   S    S   S        S     S      
2024-02-05 20:46:41,458 ========================================================================================================================
2024-02-05 20:46:41,458 Logging Sequence: 144_154.00
2024-02-05 20:46:41,458 	Gloss Reference :	A B+C+D+E
2024-02-05 20:46:41,458 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 20:46:41,458 	Gloss Alignment :	         
2024-02-05 20:46:41,458 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:46:41,459 	Text Reference  :	she also participated in the rural   olympic games    organised in  rajasthan a       few   months
2024-02-05 20:46:41,459 	Text Hypothesis :	*** **** ************ on her outfits were    designed by        the luxury    company louis runs  
2024-02-05 20:46:41,460 	Text Alignment  :	D   D    D            S  S   S       S       S        S         S   S         S       S     S     
2024-02-05 20:46:41,460 ========================================================================================================================
2024-02-05 20:46:45,528 Epoch 150: Total Training Recognition Loss 0.44  Total Training Translation Loss 20.60 
2024-02-05 20:46:45,528 EPOCH 151
2024-02-05 20:46:49,576 [Epoch: 151 Step: 00010100] Batch Recognition Loss:   0.000967 => Gls Tokens per Sec:     1952 || Batch Translation Loss:   0.166962 => Txt Tokens per Sec:     5244 || Lr: 0.000100
2024-02-05 20:46:51,008 Epoch 151: Total Training Recognition Loss 0.37  Total Training Translation Loss 16.33 
2024-02-05 20:46:51,008 EPOCH 152
2024-02-05 20:46:56,172 Epoch 152: Total Training Recognition Loss 0.36  Total Training Translation Loss 13.27 
2024-02-05 20:46:56,173 EPOCH 153
2024-02-05 20:46:57,695 [Epoch: 153 Step: 00010200] Batch Recognition Loss:   0.001345 => Gls Tokens per Sec:     1617 || Batch Translation Loss:   0.482170 => Txt Tokens per Sec:     4498 || Lr: 0.000100
2024-02-05 20:47:01,733 Epoch 153: Total Training Recognition Loss 0.42  Total Training Translation Loss 12.75 
2024-02-05 20:47:01,734 EPOCH 154
2024-02-05 20:47:05,110 [Epoch: 154 Step: 00010300] Batch Recognition Loss:   0.009248 => Gls Tokens per Sec:     2292 || Batch Translation Loss:   0.145564 => Txt Tokens per Sec:     6216 || Lr: 0.000100
2024-02-05 20:47:06,685 Epoch 154: Total Training Recognition Loss 0.39  Total Training Translation Loss 14.79 
2024-02-05 20:47:06,686 EPOCH 155
2024-02-05 20:47:12,062 Epoch 155: Total Training Recognition Loss 0.41  Total Training Translation Loss 19.33 
2024-02-05 20:47:12,063 EPOCH 156
2024-02-05 20:47:13,143 [Epoch: 156 Step: 00010400] Batch Recognition Loss:   0.002789 => Gls Tokens per Sec:     2224 || Batch Translation Loss:   0.132176 => Txt Tokens per Sec:     6043 || Lr: 0.000100
2024-02-05 20:47:17,211 Epoch 156: Total Training Recognition Loss 0.38  Total Training Translation Loss 21.39 
2024-02-05 20:47:17,212 EPOCH 157
2024-02-05 20:47:21,098 [Epoch: 157 Step: 00010500] Batch Recognition Loss:   0.002954 => Gls Tokens per Sec:     1977 || Batch Translation Loss:   0.091674 => Txt Tokens per Sec:     5528 || Lr: 0.000100
2024-02-05 20:47:22,729 Epoch 157: Total Training Recognition Loss 0.40  Total Training Translation Loss 18.91 
2024-02-05 20:47:22,729 EPOCH 158
2024-02-05 20:47:28,227 Epoch 158: Total Training Recognition Loss 0.34  Total Training Translation Loss 13.48 
2024-02-05 20:47:28,227 EPOCH 159
2024-02-05 20:47:29,302 [Epoch: 159 Step: 00010600] Batch Recognition Loss:   0.001586 => Gls Tokens per Sec:     2089 || Batch Translation Loss:   0.116037 => Txt Tokens per Sec:     5798 || Lr: 0.000100
2024-02-05 20:47:33,728 Epoch 159: Total Training Recognition Loss 0.39  Total Training Translation Loss 10.89 
2024-02-05 20:47:33,729 EPOCH 160
2024-02-05 20:47:37,655 [Epoch: 160 Step: 00010700] Batch Recognition Loss:   0.002604 => Gls Tokens per Sec:     1890 || Batch Translation Loss:   0.159308 => Txt Tokens per Sec:     5354 || Lr: 0.000100
2024-02-05 20:47:39,256 Epoch 160: Total Training Recognition Loss 0.39  Total Training Translation Loss 11.28 
2024-02-05 20:47:39,256 EPOCH 161
2024-02-05 20:47:44,504 Epoch 161: Total Training Recognition Loss 0.31  Total Training Translation Loss 13.60 
2024-02-05 20:47:44,505 EPOCH 162
2024-02-05 20:47:45,721 [Epoch: 162 Step: 00010800] Batch Recognition Loss:   0.002237 => Gls Tokens per Sec:     1713 || Batch Translation Loss:   0.275618 => Txt Tokens per Sec:     4789 || Lr: 0.000100
2024-02-05 20:47:50,099 Epoch 162: Total Training Recognition Loss 0.29  Total Training Translation Loss 18.69 
2024-02-05 20:47:50,100 EPOCH 163
2024-02-05 20:47:53,278 [Epoch: 163 Step: 00010900] Batch Recognition Loss:   0.032902 => Gls Tokens per Sec:     2316 || Batch Translation Loss:   0.323099 => Txt Tokens per Sec:     6345 || Lr: 0.000100
2024-02-05 20:47:55,207 Epoch 163: Total Training Recognition Loss 0.40  Total Training Translation Loss 22.48 
2024-02-05 20:47:55,207 EPOCH 164
2024-02-05 20:48:00,718 Epoch 164: Total Training Recognition Loss 0.34  Total Training Translation Loss 23.81 
2024-02-05 20:48:00,718 EPOCH 165
2024-02-05 20:48:01,533 [Epoch: 165 Step: 00011000] Batch Recognition Loss:   0.002378 => Gls Tokens per Sec:     2359 || Batch Translation Loss:   0.192426 => Txt Tokens per Sec:     6592 || Lr: 0.000100
2024-02-05 20:48:05,586 Epoch 165: Total Training Recognition Loss 0.42  Total Training Translation Loss 15.65 
2024-02-05 20:48:05,587 EPOCH 166
2024-02-05 20:48:09,361 [Epoch: 166 Step: 00011100] Batch Recognition Loss:   0.001715 => Gls Tokens per Sec:     1882 || Batch Translation Loss:   0.230032 => Txt Tokens per Sec:     5232 || Lr: 0.000100
2024-02-05 20:48:11,058 Epoch 166: Total Training Recognition Loss 0.29  Total Training Translation Loss 13.41 
2024-02-05 20:48:11,058 EPOCH 167
2024-02-05 20:48:16,125 Epoch 167: Total Training Recognition Loss 0.32  Total Training Translation Loss 12.01 
2024-02-05 20:48:16,125 EPOCH 168
2024-02-05 20:48:16,815 [Epoch: 168 Step: 00011200] Batch Recognition Loss:   0.001316 => Gls Tokens per Sec:     2557 || Batch Translation Loss:   0.123763 => Txt Tokens per Sec:     6146 || Lr: 0.000100
2024-02-05 20:48:21,519 Epoch 168: Total Training Recognition Loss 0.29  Total Training Translation Loss 12.80 
2024-02-05 20:48:21,519 EPOCH 169
2024-02-05 20:48:24,497 [Epoch: 169 Step: 00011300] Batch Recognition Loss:   0.000876 => Gls Tokens per Sec:     2331 || Batch Translation Loss:   0.227008 => Txt Tokens per Sec:     6457 || Lr: 0.000100
2024-02-05 20:48:26,308 Epoch 169: Total Training Recognition Loss 0.39  Total Training Translation Loss 10.54 
2024-02-05 20:48:26,308 EPOCH 170
2024-02-05 20:48:31,777 Epoch 170: Total Training Recognition Loss 0.37  Total Training Translation Loss 10.92 
2024-02-05 20:48:31,778 EPOCH 171
2024-02-05 20:48:32,462 [Epoch: 171 Step: 00011400] Batch Recognition Loss:   0.000864 => Gls Tokens per Sec:     2343 || Batch Translation Loss:   0.153179 => Txt Tokens per Sec:     6693 || Lr: 0.000100
2024-02-05 20:48:36,846 Epoch 171: Total Training Recognition Loss 0.34  Total Training Translation Loss 11.06 
2024-02-05 20:48:36,847 EPOCH 172
2024-02-05 20:48:40,283 [Epoch: 172 Step: 00011500] Batch Recognition Loss:   0.003129 => Gls Tokens per Sec:     1973 || Batch Translation Loss:   0.404103 => Txt Tokens per Sec:     5555 || Lr: 0.000100
2024-02-05 20:48:42,149 Epoch 172: Total Training Recognition Loss 0.32  Total Training Translation Loss 11.22 
2024-02-05 20:48:42,149 EPOCH 173
2024-02-05 20:48:47,232 Epoch 173: Total Training Recognition Loss 0.33  Total Training Translation Loss 11.59 
2024-02-05 20:48:47,232 EPOCH 174
2024-02-05 20:48:47,889 [Epoch: 174 Step: 00011600] Batch Recognition Loss:   0.001050 => Gls Tokens per Sec:     2046 || Batch Translation Loss:   0.035110 => Txt Tokens per Sec:     5582 || Lr: 0.000100
2024-02-05 20:48:52,765 Epoch 174: Total Training Recognition Loss 0.33  Total Training Translation Loss 10.72 
2024-02-05 20:48:52,765 EPOCH 175
2024-02-05 20:48:56,198 [Epoch: 175 Step: 00011700] Batch Recognition Loss:   0.002670 => Gls Tokens per Sec:     1929 || Batch Translation Loss:   0.115803 => Txt Tokens per Sec:     5224 || Lr: 0.000100
2024-02-05 20:48:58,212 Epoch 175: Total Training Recognition Loss 0.26  Total Training Translation Loss 9.16 
2024-02-05 20:48:58,213 EPOCH 176
2024-02-05 20:49:03,349 Epoch 176: Total Training Recognition Loss 0.31  Total Training Translation Loss 11.08 
2024-02-05 20:49:03,350 EPOCH 177
2024-02-05 20:49:04,061 [Epoch: 177 Step: 00011800] Batch Recognition Loss:   0.000812 => Gls Tokens per Sec:     1665 || Batch Translation Loss:   0.250238 => Txt Tokens per Sec:     5013 || Lr: 0.000100
2024-02-05 20:49:08,760 Epoch 177: Total Training Recognition Loss 0.41  Total Training Translation Loss 14.80 
2024-02-05 20:49:08,761 EPOCH 178
2024-02-05 20:49:11,902 [Epoch: 178 Step: 00011900] Batch Recognition Loss:   0.000759 => Gls Tokens per Sec:     2058 || Batch Translation Loss:   0.262197 => Txt Tokens per Sec:     5649 || Lr: 0.000100
2024-02-05 20:49:14,157 Epoch 178: Total Training Recognition Loss 0.25  Total Training Translation Loss 16.02 
2024-02-05 20:49:14,157 EPOCH 179
2024-02-05 20:49:19,639 Epoch 179: Total Training Recognition Loss 0.35  Total Training Translation Loss 17.03 
2024-02-05 20:49:19,640 EPOCH 180
2024-02-05 20:49:20,007 [Epoch: 180 Step: 00012000] Batch Recognition Loss:   0.002102 => Gls Tokens per Sec:     3052 || Batch Translation Loss:   0.148023 => Txt Tokens per Sec:     7052 || Lr: 0.000100
2024-02-05 20:49:28,726 Validation result at epoch 180, step    12000: duration: 8.7190s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 3.02282	Translation Loss: 90975.95312	PPL: 8836.89551
	Eval Metric: BLEU
	WER 5.01	(DEL: 0.00,	INS: 0.00,	SUB: 5.01)
	BLEU-4 0.37	(BLEU-1: 11.52,	BLEU-2: 3.59,	BLEU-3: 1.10,	BLEU-4: 0.37)
	CHRF 17.14	ROUGE 9.66
2024-02-05 20:49:28,727 Logging Recognition and Translation Outputs
2024-02-05 20:49:28,727 ========================================================================================================================
2024-02-05 20:49:28,727 Logging Sequence: 168_56.00
2024-02-05 20:49:28,728 	Gloss Reference :	A B+C+D+E
2024-02-05 20:49:28,728 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 20:49:28,728 	Gloss Alignment :	         
2024-02-05 20:49:28,728 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:49:28,729 	Text Reference  :	fans    have been waiting to   see vamika for     a    long time  
2024-02-05 20:49:28,729 	Text Hypothesis :	however some said that    they are no     clarity over the  reason
2024-02-05 20:49:28,729 	Text Alignment  :	S       S    S    S       S    S   S      S       S    S    S     
2024-02-05 20:49:28,729 ========================================================================================================================
2024-02-05 20:49:28,729 Logging Sequence: 161_74.00
2024-02-05 20:49:28,729 	Gloss Reference :	A B+C+D+E
2024-02-05 20:49:28,730 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 20:49:28,730 	Gloss Alignment :	         
2024-02-05 20:49:28,730 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:49:28,731 	Text Reference  :	i am   proud of  the **** indian team's   achievements
2024-02-05 20:49:28,731 	Text Hypothesis :	* this is    why the most news   agencies spotted     
2024-02-05 20:49:28,731 	Text Alignment  :	D S    S     S       I    S      S        S           
2024-02-05 20:49:28,731 ========================================================================================================================
2024-02-05 20:49:28,731 Logging Sequence: 111_83.00
2024-02-05 20:49:28,731 	Gloss Reference :	A B+C+D+E
2024-02-05 20:49:28,731 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 20:49:28,731 	Gloss Alignment :	         
2024-02-05 20:49:28,732 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:49:28,733 	Text Reference  :	and     the other 10    team members are     fined 25         of   the     match fee or rs  6       lakh     
2024-02-05 20:49:28,733 	Text Hypothesis :	however the ***** first time the     winning women cricketers were allowed to    pay in the winning over-rate
2024-02-05 20:49:28,733 	Text Alignment  :	S           D     S     S    S       S       S     S          S    S       S     S   S  S   S       S        
2024-02-05 20:49:28,734 ========================================================================================================================
2024-02-05 20:49:28,734 Logging Sequence: 61_218.00
2024-02-05 20:49:28,734 	Gloss Reference :	A B+C+D+E
2024-02-05 20:49:28,734 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 20:49:28,734 	Gloss Alignment :	         
2024-02-05 20:49:28,734 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:49:28,735 	Text Reference  :	******* ******** in    2020   a ***** ***** **** woman had   said  at the press conference
2024-02-05 20:49:28,735 	Text Hypothesis :	however recently sania shared a video along with his   insta story in the world cup       
2024-02-05 20:49:28,735 	Text Alignment  :	I       I        S     S        I     I     I    S     S     S     S      S     S         
2024-02-05 20:49:28,736 ========================================================================================================================
2024-02-05 20:49:28,736 Logging Sequence: 94_123.00
2024-02-05 20:49:28,736 	Gloss Reference :	A B+C+D+E
2024-02-05 20:49:28,736 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 20:49:28,736 	Gloss Alignment :	         
2024-02-05 20:49:28,736 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:49:28,738 	Text Reference  :	the venue narendra modi stadium for the  india-pakistan match has been    kept    the same people can book flights   etc 
2024-02-05 20:49:28,738 	Text Hypothesis :	*** only  1        day  earlier ie  14th october        was   not majorly disrupt the **** ****** *** most prominent over
2024-02-05 20:49:28,738 	Text Alignment  :	D   S     S        S    S       S   S    S              S     S   S       S           D    D      D   S    S         S   
2024-02-05 20:49:28,738 ========================================================================================================================
2024-02-05 20:49:33,940 Epoch 180: Total Training Recognition Loss 0.34  Total Training Translation Loss 18.38 
2024-02-05 20:49:33,940 EPOCH 181
2024-02-05 20:49:37,439 [Epoch: 181 Step: 00012100] Batch Recognition Loss:   0.008398 => Gls Tokens per Sec:     1802 || Batch Translation Loss:   0.332137 => Txt Tokens per Sec:     5137 || Lr: 0.000100
2024-02-05 20:49:39,491 Epoch 181: Total Training Recognition Loss 0.23  Total Training Translation Loss 17.96 
2024-02-05 20:49:39,492 EPOCH 182
2024-02-05 20:49:44,813 Epoch 182: Total Training Recognition Loss 0.34  Total Training Translation Loss 16.89 
2024-02-05 20:49:44,814 EPOCH 183
2024-02-05 20:49:45,275 [Epoch: 183 Step: 00012200] Batch Recognition Loss:   0.002218 => Gls Tokens per Sec:     2087 || Batch Translation Loss:   0.121495 => Txt Tokens per Sec:     5550 || Lr: 0.000100
2024-02-05 20:49:50,353 Epoch 183: Total Training Recognition Loss 0.44  Total Training Translation Loss 22.37 
2024-02-05 20:49:50,354 EPOCH 184
2024-02-05 20:49:53,377 [Epoch: 184 Step: 00012300] Batch Recognition Loss:   0.001774 => Gls Tokens per Sec:     2065 || Batch Translation Loss:   0.305939 => Txt Tokens per Sec:     5765 || Lr: 0.000100
2024-02-05 20:49:55,250 Epoch 184: Total Training Recognition Loss 0.46  Total Training Translation Loss 22.88 
2024-02-05 20:49:55,250 EPOCH 185
2024-02-05 20:50:00,523 Epoch 185: Total Training Recognition Loss 0.45  Total Training Translation Loss 18.50 
2024-02-05 20:50:00,524 EPOCH 186
2024-02-05 20:50:00,888 [Epoch: 186 Step: 00012400] Batch Recognition Loss:   0.005488 => Gls Tokens per Sec:     2204 || Batch Translation Loss:   0.133498 => Txt Tokens per Sec:     6234 || Lr: 0.000100
2024-02-05 20:50:06,000 Epoch 186: Total Training Recognition Loss 0.41  Total Training Translation Loss 13.85 
2024-02-05 20:50:06,000 EPOCH 187
2024-02-05 20:50:08,886 [Epoch: 187 Step: 00012500] Batch Recognition Loss:   0.002170 => Gls Tokens per Sec:     2108 || Batch Translation Loss:   0.403323 => Txt Tokens per Sec:     5788 || Lr: 0.000100
2024-02-05 20:50:11,205 Epoch 187: Total Training Recognition Loss 0.32  Total Training Translation Loss 9.53 
2024-02-05 20:50:11,205 EPOCH 188
2024-02-05 20:50:16,236 Epoch 188: Total Training Recognition Loss 0.33  Total Training Translation Loss 7.88 
2024-02-05 20:50:16,237 EPOCH 189
2024-02-05 20:50:16,570 [Epoch: 189 Step: 00012600] Batch Recognition Loss:   0.001797 => Gls Tokens per Sec:     1936 || Batch Translation Loss:   0.019209 => Txt Tokens per Sec:     4126 || Lr: 0.000100
2024-02-05 20:50:21,707 Epoch 189: Total Training Recognition Loss 0.29  Total Training Translation Loss 7.48 
2024-02-05 20:50:21,707 EPOCH 190
2024-02-05 20:50:24,172 [Epoch: 190 Step: 00012700] Batch Recognition Loss:   0.002972 => Gls Tokens per Sec:     2403 || Batch Translation Loss:   0.032139 => Txt Tokens per Sec:     6469 || Lr: 0.000100
2024-02-05 20:50:26,347 Epoch 190: Total Training Recognition Loss 0.26  Total Training Translation Loss 6.31 
2024-02-05 20:50:26,347 EPOCH 191
2024-02-05 20:50:31,828 Epoch 191: Total Training Recognition Loss 0.30  Total Training Translation Loss 9.42 
2024-02-05 20:50:31,828 EPOCH 192
2024-02-05 20:50:31,999 [Epoch: 192 Step: 00012800] Batch Recognition Loss:   0.000387 => Gls Tokens per Sec:     2824 || Batch Translation Loss:   0.069234 => Txt Tokens per Sec:     7159 || Lr: 0.000100
2024-02-05 20:50:36,543 Epoch 192: Total Training Recognition Loss 0.26  Total Training Translation Loss 11.50 
2024-02-05 20:50:36,543 EPOCH 193
2024-02-05 20:50:38,970 [Epoch: 193 Step: 00012900] Batch Recognition Loss:   0.017258 => Gls Tokens per Sec:     2374 || Batch Translation Loss:   0.103566 => Txt Tokens per Sec:     6483 || Lr: 0.000100
2024-02-05 20:50:41,177 Epoch 193: Total Training Recognition Loss 0.18  Total Training Translation Loss 12.94 
2024-02-05 20:50:41,177 EPOCH 194
2024-02-05 20:50:46,482 Epoch 194: Total Training Recognition Loss 0.17  Total Training Translation Loss 12.40 
2024-02-05 20:50:46,483 EPOCH 195
2024-02-05 20:50:46,769 [Epoch: 195 Step: 00013000] Batch Recognition Loss:   0.004943 => Gls Tokens per Sec:     1123 || Batch Translation Loss:   0.084799 => Txt Tokens per Sec:     3551 || Lr: 0.000100
2024-02-05 20:50:51,252 Epoch 195: Total Training Recognition Loss 0.25  Total Training Translation Loss 12.76 
2024-02-05 20:50:51,253 EPOCH 196
2024-02-05 20:50:54,296 [Epoch: 196 Step: 00013100] Batch Recognition Loss:   0.003381 => Gls Tokens per Sec:     1807 || Batch Translation Loss:   0.154635 => Txt Tokens per Sec:     4949 || Lr: 0.000100
2024-02-05 20:50:56,826 Epoch 196: Total Training Recognition Loss 0.51  Total Training Translation Loss 14.81 
2024-02-05 20:50:56,826 EPOCH 197
2024-02-05 20:51:02,075 Epoch 197: Total Training Recognition Loss 1.25  Total Training Translation Loss 11.52 
2024-02-05 20:51:02,076 EPOCH 198
2024-02-05 20:51:02,141 [Epoch: 198 Step: 00013200] Batch Recognition Loss:   0.002120 => Gls Tokens per Sec:     2462 || Batch Translation Loss:   0.096349 => Txt Tokens per Sec:     6723 || Lr: 0.000100
2024-02-05 20:51:07,171 Epoch 198: Total Training Recognition Loss 1.02  Total Training Translation Loss 12.99 
2024-02-05 20:51:07,171 EPOCH 199
2024-02-05 20:51:09,660 [Epoch: 199 Step: 00013300] Batch Recognition Loss:   0.001178 => Gls Tokens per Sec:     2146 || Batch Translation Loss:   0.216685 => Txt Tokens per Sec:     6055 || Lr: 0.000100
2024-02-05 20:51:12,245 Epoch 199: Total Training Recognition Loss 0.91  Total Training Translation Loss 15.79 
2024-02-05 20:51:12,246 EPOCH 200
2024-02-05 20:51:17,404 [Epoch: 200 Step: 00013400] Batch Recognition Loss:   0.002551 => Gls Tokens per Sec:     2059 || Batch Translation Loss:   0.180565 => Txt Tokens per Sec:     5697 || Lr: 0.000100
2024-02-05 20:51:17,404 Epoch 200: Total Training Recognition Loss 0.58  Total Training Translation Loss 14.33 
2024-02-05 20:51:17,404 EPOCH 201
2024-02-05 20:51:22,686 Epoch 201: Total Training Recognition Loss 0.32  Total Training Translation Loss 10.49 
2024-02-05 20:51:22,687 EPOCH 202
2024-02-05 20:51:25,119 [Epoch: 202 Step: 00013500] Batch Recognition Loss:   0.000552 => Gls Tokens per Sec:     2172 || Batch Translation Loss:   0.259098 => Txt Tokens per Sec:     5813 || Lr: 0.000100
2024-02-05 20:51:27,800 Epoch 202: Total Training Recognition Loss 0.31  Total Training Translation Loss 8.91 
2024-02-05 20:51:27,801 EPOCH 203
2024-02-05 20:51:32,999 [Epoch: 203 Step: 00013600] Batch Recognition Loss:   0.003502 => Gls Tokens per Sec:     2013 || Batch Translation Loss:   0.060978 => Txt Tokens per Sec:     5589 || Lr: 0.000100
2024-02-05 20:51:33,056 Epoch 203: Total Training Recognition Loss 0.26  Total Training Translation Loss 10.28 
2024-02-05 20:51:33,056 EPOCH 204
2024-02-05 20:51:38,033 Epoch 204: Total Training Recognition Loss 0.25  Total Training Translation Loss 13.93 
2024-02-05 20:51:38,033 EPOCH 205
2024-02-05 20:51:40,718 [Epoch: 205 Step: 00013700] Batch Recognition Loss:   0.000354 => Gls Tokens per Sec:     1908 || Batch Translation Loss:   0.472718 => Txt Tokens per Sec:     5338 || Lr: 0.000100
2024-02-05 20:51:43,442 Epoch 205: Total Training Recognition Loss 0.21  Total Training Translation Loss 14.58 
2024-02-05 20:51:43,442 EPOCH 206
2024-02-05 20:51:48,157 [Epoch: 206 Step: 00013800] Batch Recognition Loss:   0.000575 => Gls Tokens per Sec:     2184 || Batch Translation Loss:   0.147653 => Txt Tokens per Sec:     6015 || Lr: 0.000100
2024-02-05 20:51:48,332 Epoch 206: Total Training Recognition Loss 0.24  Total Training Translation Loss 14.31 
2024-02-05 20:51:48,333 EPOCH 207
2024-02-05 20:51:53,756 Epoch 207: Total Training Recognition Loss 0.25  Total Training Translation Loss 12.07 
2024-02-05 20:51:53,757 EPOCH 208
2024-02-05 20:51:55,901 [Epoch: 208 Step: 00013900] Batch Recognition Loss:   0.000592 => Gls Tokens per Sec:     2268 || Batch Translation Loss:   0.221235 => Txt Tokens per Sec:     6306 || Lr: 0.000100
2024-02-05 20:51:58,515 Epoch 208: Total Training Recognition Loss 0.30  Total Training Translation Loss 13.07 
2024-02-05 20:51:58,516 EPOCH 209
2024-02-05 20:52:04,015 [Epoch: 209 Step: 00014000] Batch Recognition Loss:   0.000773 => Gls Tokens per Sec:     1844 || Batch Translation Loss:   0.133479 => Txt Tokens per Sec:     5141 || Lr: 0.000100
2024-02-05 20:52:12,187 Validation result at epoch 209, step    14000: duration: 8.1713s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.86102	Translation Loss: 92108.77344	PPL: 9895.52246
	Eval Metric: BLEU
	WER 4.73	(DEL: 0.00,	INS: 0.00,	SUB: 4.73)
	BLEU-4 0.56	(BLEU-1: 11.07,	BLEU-2: 3.37,	BLEU-3: 1.33,	BLEU-4: 0.56)
	CHRF 16.72	ROUGE 9.34
2024-02-05 20:52:12,188 Logging Recognition and Translation Outputs
2024-02-05 20:52:12,188 ========================================================================================================================
2024-02-05 20:52:12,188 Logging Sequence: 177_50.00
2024-02-05 20:52:12,189 	Gloss Reference :	A B+C+D+E    
2024-02-05 20:52:12,189 	Gloss Hypothesis:	A B+C+D+E+D+E
2024-02-05 20:52:12,189 	Gloss Alignment :	  S          
2024-02-05 20:52:12,189 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:52:12,190 	Text Reference  :	***** a   similar  reward of    rs     50000 was announced    for    information against his associate ajay   kumar
2024-02-05 20:52:12,190 	Text Hypothesis :	after the olympics the    delhi police filed a   non-bailable arrest warrant     against *** ********* sushil kumar
2024-02-05 20:52:12,191 	Text Alignment  :	I     S   S        S      S     S      S     S   S            S      S                   D   D         S           
2024-02-05 20:52:12,191 ========================================================================================================================
2024-02-05 20:52:12,191 Logging Sequence: 136_175.00
2024-02-05 20:52:12,191 	Gloss Reference :	A B+C+D+E  
2024-02-05 20:52:12,191 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-05 20:52:12,191 	Gloss Alignment :	  S        
2024-02-05 20:52:12,192 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:52:12,193 	Text Reference  :	after 49 years india' hockey team beat   britain and qualified for  the    semi-finals
2024-02-05 20:52:12,193 	Text Hypothesis :	***** ** she   has    won    a    bronze medal   at  the       2012 london olympics   
2024-02-05 20:52:12,193 	Text Alignment  :	D     D  S     S      S      S    S      S       S   S         S    S      S          
2024-02-05 20:52:12,193 ========================================================================================================================
2024-02-05 20:52:12,193 Logging Sequence: 126_159.00
2024-02-05 20:52:12,193 	Gloss Reference :	A B+C+D+E
2024-02-05 20:52:12,193 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 20:52:12,194 	Gloss Alignment :	         
2024-02-05 20:52:12,194 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:52:12,194 	Text Reference  :	despite multiple challenges and injuries you   did not give  up 
2024-02-05 20:52:12,195 	Text Hypothesis :	******* ******** he         won a        medal in  the medal wow
2024-02-05 20:52:12,195 	Text Alignment  :	D       D        S          S   S        S     S   S   S     S  
2024-02-05 20:52:12,195 ========================================================================================================================
2024-02-05 20:52:12,195 Logging Sequence: 70_88.00
2024-02-05 20:52:12,195 	Gloss Reference :	A B+C+D+E
2024-02-05 20:52:12,195 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 20:52:12,195 	Gloss Alignment :	         
2024-02-05 20:52:12,196 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:52:12,197 	Text Reference  :	******** *** two coca-cola bottles    were placed on   the table next   to the mic  
2024-02-05 20:52:12,197 	Text Hypothesis :	actually won the press     conference in   euro   2020 is  also  placed in her press
2024-02-05 20:52:12,197 	Text Alignment  :	I        I   S   S         S          S    S      S    S   S     S      S  S   S    
2024-02-05 20:52:12,197 ========================================================================================================================
2024-02-05 20:52:12,197 Logging Sequence: 54_201.00
2024-02-05 20:52:12,197 	Gloss Reference :	A B+C+D+E    
2024-02-05 20:52:12,197 	Gloss Hypothesis:	A B+C+D+E+D+E
2024-02-05 20:52:12,198 	Gloss Alignment :	  S          
2024-02-05 20:52:12,198 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:52:12,199 	Text Reference  :	there is a huge demand mostly from non-resident indians nris who are excited to see the match and they have booked the hotel rooms
2024-02-05 20:52:12,199 	Text Hypothesis :	***** ** * **** ****** ****** **** we           have    won  icc has married to *** *** ***** *** **** know in     the 2016  games
2024-02-05 20:52:12,200 	Text Alignment  :	D     D  D D    D      D      D    S            S       S    S   S   S          D   D   D     D   D    S    S          S     S    
2024-02-05 20:52:12,200 ========================================================================================================================
2024-02-05 20:52:12,398 Epoch 209: Total Training Recognition Loss 0.26  Total Training Translation Loss 10.83 
2024-02-05 20:52:12,398 EPOCH 210
2024-02-05 20:52:17,720 Epoch 210: Total Training Recognition Loss 0.27  Total Training Translation Loss 9.01 
2024-02-05 20:52:17,721 EPOCH 211
2024-02-05 20:52:19,857 [Epoch: 211 Step: 00014100] Batch Recognition Loss:   0.001471 => Gls Tokens per Sec:     2200 || Batch Translation Loss:   0.106146 => Txt Tokens per Sec:     6312 || Lr: 0.000100
2024-02-05 20:52:22,910 Epoch 211: Total Training Recognition Loss 0.22  Total Training Translation Loss 7.06 
2024-02-05 20:52:22,911 EPOCH 212
2024-02-05 20:52:27,720 [Epoch: 212 Step: 00014200] Batch Recognition Loss:   0.004384 => Gls Tokens per Sec:     2076 || Batch Translation Loss:   0.088227 => Txt Tokens per Sec:     5774 || Lr: 0.000100
2024-02-05 20:52:27,947 Epoch 212: Total Training Recognition Loss 0.18  Total Training Translation Loss 8.38 
2024-02-05 20:52:27,947 EPOCH 213
2024-02-05 20:52:33,236 Epoch 213: Total Training Recognition Loss 0.18  Total Training Translation Loss 11.73 
2024-02-05 20:52:33,236 EPOCH 214
2024-02-05 20:52:35,369 [Epoch: 214 Step: 00014300] Batch Recognition Loss:   0.000795 => Gls Tokens per Sec:     2179 || Batch Translation Loss:   0.057086 => Txt Tokens per Sec:     5886 || Lr: 0.000100
2024-02-05 20:52:38,358 Epoch 214: Total Training Recognition Loss 0.26  Total Training Translation Loss 11.01 
2024-02-05 20:52:38,358 EPOCH 215
2024-02-05 20:52:43,393 [Epoch: 215 Step: 00014400] Batch Recognition Loss:   0.002544 => Gls Tokens per Sec:     1951 || Batch Translation Loss:   0.106924 => Txt Tokens per Sec:     5413 || Lr: 0.000100
2024-02-05 20:52:43,723 Epoch 215: Total Training Recognition Loss 0.24  Total Training Translation Loss 13.68 
2024-02-05 20:52:43,723 EPOCH 216
2024-02-05 20:52:48,650 Epoch 216: Total Training Recognition Loss 0.28  Total Training Translation Loss 11.89 
2024-02-05 20:52:48,650 EPOCH 217
2024-02-05 20:52:51,085 [Epoch: 217 Step: 00014500] Batch Recognition Loss:   0.002497 => Gls Tokens per Sec:     1800 || Batch Translation Loss:   0.148609 => Txt Tokens per Sec:     4924 || Lr: 0.000100
2024-02-05 20:52:54,221 Epoch 217: Total Training Recognition Loss 0.22  Total Training Translation Loss 11.88 
2024-02-05 20:52:54,221 EPOCH 218
2024-02-05 20:52:58,932 [Epoch: 218 Step: 00014600] Batch Recognition Loss:   0.001872 => Gls Tokens per Sec:     2052 || Batch Translation Loss:   0.150149 => Txt Tokens per Sec:     5690 || Lr: 0.000100
2024-02-05 20:52:59,406 Epoch 218: Total Training Recognition Loss 0.26  Total Training Translation Loss 12.85 
2024-02-05 20:52:59,407 EPOCH 219
2024-02-05 20:53:04,556 Epoch 219: Total Training Recognition Loss 0.24  Total Training Translation Loss 10.29 
2024-02-05 20:53:04,556 EPOCH 220
2024-02-05 20:53:06,394 [Epoch: 220 Step: 00014700] Batch Recognition Loss:   0.006855 => Gls Tokens per Sec:     2352 || Batch Translation Loss:   0.352102 => Txt Tokens per Sec:     6531 || Lr: 0.000100
2024-02-05 20:53:09,538 Epoch 220: Total Training Recognition Loss 0.23  Total Training Translation Loss 12.02 
2024-02-05 20:53:09,539 EPOCH 221
2024-02-05 20:53:14,238 [Epoch: 221 Step: 00014800] Batch Recognition Loss:   0.000606 => Gls Tokens per Sec:     2044 || Batch Translation Loss:   0.231499 => Txt Tokens per Sec:     5715 || Lr: 0.000100
2024-02-05 20:53:14,782 Epoch 221: Total Training Recognition Loss 0.25  Total Training Translation Loss 11.27 
2024-02-05 20:53:14,782 EPOCH 222
2024-02-05 20:53:20,142 Epoch 222: Total Training Recognition Loss 0.15  Total Training Translation Loss 10.58 
2024-02-05 20:53:20,143 EPOCH 223
2024-02-05 20:53:22,374 [Epoch: 223 Step: 00014900] Batch Recognition Loss:   0.000682 => Gls Tokens per Sec:     1865 || Batch Translation Loss:   0.177111 => Txt Tokens per Sec:     5353 || Lr: 0.000100
2024-02-05 20:53:25,218 Epoch 223: Total Training Recognition Loss 0.22  Total Training Translation Loss 11.51 
2024-02-05 20:53:25,218 EPOCH 224
2024-02-05 20:53:29,831 [Epoch: 224 Step: 00015000] Batch Recognition Loss:   0.000481 => Gls Tokens per Sec:     2025 || Batch Translation Loss:   0.169994 => Txt Tokens per Sec:     5607 || Lr: 0.000100
2024-02-05 20:53:30,545 Epoch 224: Total Training Recognition Loss 0.28  Total Training Translation Loss 10.60 
2024-02-05 20:53:30,545 EPOCH 225
2024-02-05 20:53:35,680 Epoch 225: Total Training Recognition Loss 0.21  Total Training Translation Loss 8.50 
2024-02-05 20:53:35,680 EPOCH 226
2024-02-05 20:53:37,782 [Epoch: 226 Step: 00015100] Batch Recognition Loss:   0.000798 => Gls Tokens per Sec:     1904 || Batch Translation Loss:   0.086209 => Txt Tokens per Sec:     5338 || Lr: 0.000100
2024-02-05 20:53:41,121 Epoch 226: Total Training Recognition Loss 0.19  Total Training Translation Loss 10.40 
2024-02-05 20:53:41,121 EPOCH 227
2024-02-05 20:53:45,065 [Epoch: 227 Step: 00015200] Batch Recognition Loss:   0.006057 => Gls Tokens per Sec:     2329 || Batch Translation Loss:   0.165737 => Txt Tokens per Sec:     6365 || Lr: 0.000100
2024-02-05 20:53:45,865 Epoch 227: Total Training Recognition Loss 0.28  Total Training Translation Loss 12.09 
2024-02-05 20:53:45,866 EPOCH 228
2024-02-05 20:53:51,317 Epoch 228: Total Training Recognition Loss 0.28  Total Training Translation Loss 12.66 
2024-02-05 20:53:51,318 EPOCH 229
2024-02-05 20:53:52,929 [Epoch: 229 Step: 00015300] Batch Recognition Loss:   0.001259 => Gls Tokens per Sec:     2323 || Batch Translation Loss:   0.119413 => Txt Tokens per Sec:     6299 || Lr: 0.000100
2024-02-05 20:53:56,366 Epoch 229: Total Training Recognition Loss 0.28  Total Training Translation Loss 10.34 
2024-02-05 20:53:56,366 EPOCH 230
2024-02-05 20:54:01,013 [Epoch: 230 Step: 00015400] Batch Recognition Loss:   0.001275 => Gls Tokens per Sec:     1942 || Batch Translation Loss:   0.308605 => Txt Tokens per Sec:     5432 || Lr: 0.000100
2024-02-05 20:54:01,735 Epoch 230: Total Training Recognition Loss 0.25  Total Training Translation Loss 12.43 
2024-02-05 20:54:01,735 EPOCH 231
2024-02-05 20:54:07,054 Epoch 231: Total Training Recognition Loss 0.25  Total Training Translation Loss 11.31 
2024-02-05 20:54:07,054 EPOCH 232
2024-02-05 20:54:09,359 [Epoch: 232 Step: 00015500] Batch Recognition Loss:   0.001803 => Gls Tokens per Sec:     1555 || Batch Translation Loss:   0.303200 => Txt Tokens per Sec:     4510 || Lr: 0.000100
2024-02-05 20:54:12,502 Epoch 232: Total Training Recognition Loss 0.18  Total Training Translation Loss 11.25 
2024-02-05 20:54:12,502 EPOCH 233
2024-02-05 20:54:16,768 [Epoch: 233 Step: 00015600] Batch Recognition Loss:   0.001517 => Gls Tokens per Sec:     2101 || Batch Translation Loss:   0.063191 => Txt Tokens per Sec:     5759 || Lr: 0.000100
2024-02-05 20:54:17,845 Epoch 233: Total Training Recognition Loss 0.19  Total Training Translation Loss 10.96 
2024-02-05 20:54:17,845 EPOCH 234
2024-02-05 20:54:22,976 Epoch 234: Total Training Recognition Loss 0.18  Total Training Translation Loss 11.29 
2024-02-05 20:54:22,976 EPOCH 235
2024-02-05 20:54:24,769 [Epoch: 235 Step: 00015700] Batch Recognition Loss:   0.000281 => Gls Tokens per Sec:     1964 || Batch Translation Loss:   0.127060 => Txt Tokens per Sec:     5499 || Lr: 0.000100
2024-02-05 20:54:28,449 Epoch 235: Total Training Recognition Loss 0.18  Total Training Translation Loss 8.80 
2024-02-05 20:54:28,450 EPOCH 236
2024-02-05 20:54:32,693 [Epoch: 236 Step: 00015800] Batch Recognition Loss:   0.001675 => Gls Tokens per Sec:     2051 || Batch Translation Loss:   0.090472 => Txt Tokens per Sec:     5630 || Lr: 0.000100
2024-02-05 20:54:33,633 Epoch 236: Total Training Recognition Loss 0.24  Total Training Translation Loss 9.48 
2024-02-05 20:54:33,633 EPOCH 237
2024-02-05 20:54:38,780 Epoch 237: Total Training Recognition Loss 0.23  Total Training Translation Loss 9.73 
2024-02-05 20:54:38,781 EPOCH 238
2024-02-05 20:54:40,272 [Epoch: 238 Step: 00015900] Batch Recognition Loss:   0.000649 => Gls Tokens per Sec:     2255 || Batch Translation Loss:   0.360661 => Txt Tokens per Sec:     6086 || Lr: 0.000100
2024-02-05 20:54:43,811 Epoch 238: Total Training Recognition Loss 0.19  Total Training Translation Loss 8.27 
2024-02-05 20:54:43,811 EPOCH 239
2024-02-05 20:54:48,168 [Epoch: 239 Step: 00016000] Batch Recognition Loss:   0.002935 => Gls Tokens per Sec:     1961 || Batch Translation Loss:   0.089444 => Txt Tokens per Sec:     5478 || Lr: 0.000100
2024-02-05 20:54:56,432 Validation result at epoch 239, step    16000: duration: 8.2627s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 3.45317	Translation Loss: 93788.28125	PPL: 11702.84375
	Eval Metric: BLEU
	WER 4.94	(DEL: 0.00,	INS: 0.00,	SUB: 4.94)
	BLEU-4 0.51	(BLEU-1: 11.27,	BLEU-2: 3.28,	BLEU-3: 1.18,	BLEU-4: 0.51)
	CHRF 16.79	ROUGE 9.53
2024-02-05 20:54:56,433 Logging Recognition and Translation Outputs
2024-02-05 20:54:56,433 ========================================================================================================================
2024-02-05 20:54:56,433 Logging Sequence: 163_116.00
2024-02-05 20:54:56,434 	Gloss Reference :	A B+C+D+E
2024-02-05 20:54:56,434 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 20:54:56,434 	Gloss Alignment :	         
2024-02-05 20:54:56,434 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:54:56,435 	Text Reference  :	******** ****** ***** ***** ******** people said  that she looked similar to **** *** virat  
2024-02-05 20:54:56,435 	Text Hypothesis :	whenever anyone talks about pictures of     their son  and some   staff   to wait and updates
2024-02-05 20:54:56,435 	Text Alignment  :	I        I      I     I     I        S      S     S    S   S      S          I    I   S      
2024-02-05 20:54:56,435 ========================================================================================================================
2024-02-05 20:54:56,435 Logging Sequence: 53_161.00
2024-02-05 20:54:56,436 	Gloss Reference :	A B+C+D+E
2024-02-05 20:54:56,436 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 20:54:56,436 	Gloss Alignment :	         
2024-02-05 20:54:56,436 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:54:56,437 	Text Reference  :	rashid has also been urging people    to donate to his   rashid khan     foundation and afghanistan cricket association
2024-02-05 20:54:56,437 	Text Hypothesis :	****** *** **** **** now    according to ****** a  habit of     shooting in         the 2008        olympic games      
2024-02-05 20:54:56,438 	Text Alignment  :	D      D   D    D    S      S            D      S  S     S      S        S          S   S           S       S          
2024-02-05 20:54:56,438 ========================================================================================================================
2024-02-05 20:54:56,438 Logging Sequence: 67_73.00
2024-02-05 20:54:56,438 	Gloss Reference :	A B+C+D+E
2024-02-05 20:54:56,438 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 20:54:56,438 	Gloss Alignment :	         
2024-02-05 20:54:56,438 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:54:56,439 	Text Reference  :	*** **** **** *** ******* in     his tweet he    also said
2024-02-05 20:54:56,439 	Text Hypothesis :	she said that the taliban wanted to  find  their own  etc 
2024-02-05 20:54:56,439 	Text Alignment  :	I   I    I    I   I       S      S   S     S     S    S   
2024-02-05 20:54:56,439 ========================================================================================================================
2024-02-05 20:54:56,439 Logging Sequence: 137_44.00
2024-02-05 20:54:56,440 	Gloss Reference :	A B+C+D+E
2024-02-05 20:54:56,440 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 20:54:56,440 	Gloss Alignment :	         
2024-02-05 20:54:56,440 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:54:56,441 	Text Reference  :	let me tell you the rules that qatar has announced for  the fans travelling for the world cup 
2024-02-05 20:54:56,441 	Text Hypothesis :	*** ** **** *** *** ***** **** ***** he  then      went to  be   played     at  the state team
2024-02-05 20:54:56,441 	Text Alignment  :	D   D  D    D   D   D     D    D     S   S         S    S   S    S          S       S     S   
2024-02-05 20:54:56,442 ========================================================================================================================
2024-02-05 20:54:56,442 Logging Sequence: 99_158.00
2024-02-05 20:54:56,442 	Gloss Reference :	A B+C+D+E
2024-02-05 20:54:56,442 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 20:54:56,442 	Gloss Alignment :	         
2024-02-05 20:54:56,442 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:54:56,443 	Text Reference  :	*** **** ** the incident occured in dubai and  it    was extremely shameful
2024-02-05 20:54:56,443 	Text Hypothesis :	his fans to see him      however he has   been going to  these     match   
2024-02-05 20:54:56,443 	Text Alignment  :	I   I    I  S   S        S       S  S     S    S     S   S         S       
2024-02-05 20:54:56,443 ========================================================================================================================
2024-02-05 20:54:57,481 Epoch 239: Total Training Recognition Loss 0.15  Total Training Translation Loss 8.40 
2024-02-05 20:54:57,482 EPOCH 240
2024-02-05 20:55:02,669 Epoch 240: Total Training Recognition Loss 0.16  Total Training Translation Loss 8.98 
2024-02-05 20:55:02,669 EPOCH 241
2024-02-05 20:55:04,003 [Epoch: 241 Step: 00016100] Batch Recognition Loss:   0.000486 => Gls Tokens per Sec:     2401 || Batch Translation Loss:   0.034568 => Txt Tokens per Sec:     6800 || Lr: 0.000100
2024-02-05 20:55:07,374 Epoch 241: Total Training Recognition Loss 0.27  Total Training Translation Loss 7.11 
2024-02-05 20:55:07,374 EPOCH 242
2024-02-05 20:55:11,658 [Epoch: 242 Step: 00016200] Batch Recognition Loss:   0.001131 => Gls Tokens per Sec:     1957 || Batch Translation Loss:   0.135403 => Txt Tokens per Sec:     5466 || Lr: 0.000100
2024-02-05 20:55:12,877 Epoch 242: Total Training Recognition Loss 0.15  Total Training Translation Loss 6.11 
2024-02-05 20:55:12,877 EPOCH 243
2024-02-05 20:55:18,120 Epoch 243: Total Training Recognition Loss 0.22  Total Training Translation Loss 9.91 
2024-02-05 20:55:18,120 EPOCH 244
2024-02-05 20:55:19,776 [Epoch: 244 Step: 00016300] Batch Recognition Loss:   0.000849 => Gls Tokens per Sec:     1837 || Batch Translation Loss:   0.077171 => Txt Tokens per Sec:     5001 || Lr: 0.000100
2024-02-05 20:55:23,667 Epoch 244: Total Training Recognition Loss 0.17  Total Training Translation Loss 11.28 
2024-02-05 20:55:23,668 EPOCH 245
2024-02-05 20:55:27,637 [Epoch: 245 Step: 00016400] Batch Recognition Loss:   0.003711 => Gls Tokens per Sec:     2072 || Batch Translation Loss:   0.284034 => Txt Tokens per Sec:     5754 || Lr: 0.000100
2024-02-05 20:55:28,777 Epoch 245: Total Training Recognition Loss 0.41  Total Training Translation Loss 13.41 
2024-02-05 20:55:28,777 EPOCH 246
2024-02-05 20:55:33,337 Epoch 246: Total Training Recognition Loss 0.26  Total Training Translation Loss 11.59 
2024-02-05 20:55:33,337 EPOCH 247
2024-02-05 20:55:34,793 [Epoch: 247 Step: 00016500] Batch Recognition Loss:   0.000775 => Gls Tokens per Sec:     1979 || Batch Translation Loss:   0.178640 => Txt Tokens per Sec:     5277 || Lr: 0.000100
2024-02-05 20:55:38,802 Epoch 247: Total Training Recognition Loss 0.32  Total Training Translation Loss 12.24 
2024-02-05 20:55:38,802 EPOCH 248
2024-02-05 20:55:42,463 [Epoch: 248 Step: 00016600] Batch Recognition Loss:   0.000368 => Gls Tokens per Sec:     2202 || Batch Translation Loss:   0.061993 => Txt Tokens per Sec:     6131 || Lr: 0.000100
2024-02-05 20:55:43,648 Epoch 248: Total Training Recognition Loss 0.27  Total Training Translation Loss 8.47 
2024-02-05 20:55:43,649 EPOCH 249
2024-02-05 20:55:49,449 Epoch 249: Total Training Recognition Loss 0.31  Total Training Translation Loss 8.60 
2024-02-05 20:55:49,450 EPOCH 250
2024-02-05 20:55:50,739 [Epoch: 250 Step: 00016700] Batch Recognition Loss:   0.000225 => Gls Tokens per Sec:     2112 || Batch Translation Loss:   0.149486 => Txt Tokens per Sec:     6139 || Lr: 0.000100
2024-02-05 20:55:54,540 Epoch 250: Total Training Recognition Loss 0.32  Total Training Translation Loss 9.91 
2024-02-05 20:55:54,541 EPOCH 251
2024-02-05 20:55:58,543 [Epoch: 251 Step: 00016800] Batch Recognition Loss:   0.000322 => Gls Tokens per Sec:     2000 || Batch Translation Loss:   0.306806 => Txt Tokens per Sec:     5474 || Lr: 0.000100
2024-02-05 20:56:00,061 Epoch 251: Total Training Recognition Loss 0.18  Total Training Translation Loss 9.77 
2024-02-05 20:56:00,061 EPOCH 252
2024-02-05 20:56:04,965 Epoch 252: Total Training Recognition Loss 0.17  Total Training Translation Loss 6.95 
2024-02-05 20:56:04,965 EPOCH 253
2024-02-05 20:56:06,116 [Epoch: 253 Step: 00016900] Batch Recognition Loss:   0.000512 => Gls Tokens per Sec:     2139 || Batch Translation Loss:   0.053641 => Txt Tokens per Sec:     5844 || Lr: 0.000100
2024-02-05 20:56:10,242 Epoch 253: Total Training Recognition Loss 0.13  Total Training Translation Loss 6.99 
2024-02-05 20:56:10,243 EPOCH 254
2024-02-05 20:56:14,198 [Epoch: 254 Step: 00017000] Batch Recognition Loss:   0.000630 => Gls Tokens per Sec:     1957 || Batch Translation Loss:   0.073219 => Txt Tokens per Sec:     5473 || Lr: 0.000100
2024-02-05 20:56:15,455 Epoch 254: Total Training Recognition Loss 0.12  Total Training Translation Loss 7.09 
2024-02-05 20:56:15,455 EPOCH 255
2024-02-05 20:56:20,961 Epoch 255: Total Training Recognition Loss 0.12  Total Training Translation Loss 7.79 
2024-02-05 20:56:20,961 EPOCH 256
2024-02-05 20:56:21,969 [Epoch: 256 Step: 00017100] Batch Recognition Loss:   0.000288 => Gls Tokens per Sec:     2384 || Batch Translation Loss:   0.058739 => Txt Tokens per Sec:     6285 || Lr: 0.000100
2024-02-05 20:56:26,145 Epoch 256: Total Training Recognition Loss 0.13  Total Training Translation Loss 5.16 
2024-02-05 20:56:26,145 EPOCH 257
2024-02-05 20:56:29,942 [Epoch: 257 Step: 00017200] Batch Recognition Loss:   0.004213 => Gls Tokens per Sec:     2023 || Batch Translation Loss:   0.053858 => Txt Tokens per Sec:     5517 || Lr: 0.000100
2024-02-05 20:56:31,684 Epoch 257: Total Training Recognition Loss 0.22  Total Training Translation Loss 7.82 
2024-02-05 20:56:31,685 EPOCH 258
2024-02-05 20:56:36,901 Epoch 258: Total Training Recognition Loss 0.23  Total Training Translation Loss 9.65 
2024-02-05 20:56:36,902 EPOCH 259
2024-02-05 20:56:38,176 [Epoch: 259 Step: 00017300] Batch Recognition Loss:   0.014359 => Gls Tokens per Sec:     1681 || Batch Translation Loss:   0.134162 => Txt Tokens per Sec:     4838 || Lr: 0.000100
2024-02-05 20:56:42,276 Epoch 259: Total Training Recognition Loss 0.29  Total Training Translation Loss 8.30 
2024-02-05 20:56:42,276 EPOCH 260
2024-02-05 20:56:46,010 [Epoch: 260 Step: 00017400] Batch Recognition Loss:   0.002182 => Gls Tokens per Sec:     1988 || Batch Translation Loss:   0.052889 => Txt Tokens per Sec:     5624 || Lr: 0.000100
2024-02-05 20:56:47,461 Epoch 260: Total Training Recognition Loss 0.18  Total Training Translation Loss 7.77 
2024-02-05 20:56:47,462 EPOCH 261
2024-02-05 20:56:52,808 Epoch 261: Total Training Recognition Loss 0.18  Total Training Translation Loss 7.59 
2024-02-05 20:56:52,808 EPOCH 262
2024-02-05 20:56:53,637 [Epoch: 262 Step: 00017500] Batch Recognition Loss:   0.001359 => Gls Tokens per Sec:     2513 || Batch Translation Loss:   0.103694 => Txt Tokens per Sec:     7014 || Lr: 0.000100
2024-02-05 20:56:58,253 Epoch 262: Total Training Recognition Loss 0.26  Total Training Translation Loss 11.58 
2024-02-05 20:56:58,253 EPOCH 263
2024-02-05 20:57:01,947 [Epoch: 263 Step: 00017600] Batch Recognition Loss:   0.000848 => Gls Tokens per Sec:     1966 || Batch Translation Loss:   0.087794 => Txt Tokens per Sec:     5487 || Lr: 0.000100
2024-02-05 20:57:03,687 Epoch 263: Total Training Recognition Loss 0.27  Total Training Translation Loss 14.76 
2024-02-05 20:57:03,687 EPOCH 264
2024-02-05 20:57:09,124 Epoch 264: Total Training Recognition Loss 0.22  Total Training Translation Loss 16.44 
2024-02-05 20:57:09,125 EPOCH 265
2024-02-05 20:57:10,156 [Epoch: 265 Step: 00017700] Batch Recognition Loss:   0.001153 => Gls Tokens per Sec:     1865 || Batch Translation Loss:   0.116295 => Txt Tokens per Sec:     5207 || Lr: 0.000100
2024-02-05 20:57:14,796 Epoch 265: Total Training Recognition Loss 0.25  Total Training Translation Loss 13.76 
2024-02-05 20:57:14,796 EPOCH 266
2024-02-05 20:57:18,464 [Epoch: 266 Step: 00017800] Batch Recognition Loss:   0.000606 => Gls Tokens per Sec:     1936 || Batch Translation Loss:   0.039006 => Txt Tokens per Sec:     5339 || Lr: 0.000100
2024-02-05 20:57:20,312 Epoch 266: Total Training Recognition Loss 0.23  Total Training Translation Loss 7.68 
2024-02-05 20:57:20,312 EPOCH 267
2024-02-05 20:57:25,564 Epoch 267: Total Training Recognition Loss 0.19  Total Training Translation Loss 7.74 
2024-02-05 20:57:25,565 EPOCH 268
2024-02-05 20:57:26,488 [Epoch: 268 Step: 00017900] Batch Recognition Loss:   0.001109 => Gls Tokens per Sec:     1910 || Batch Translation Loss:   0.256992 => Txt Tokens per Sec:     5289 || Lr: 0.000100
2024-02-05 20:57:31,147 Epoch 268: Total Training Recognition Loss 0.21  Total Training Translation Loss 8.80 
2024-02-05 20:57:31,148 EPOCH 269
2024-02-05 20:57:34,362 [Epoch: 269 Step: 00018000] Batch Recognition Loss:   0.001071 => Gls Tokens per Sec:     2160 || Batch Translation Loss:   0.090831 => Txt Tokens per Sec:     6042 || Lr: 0.000100
2024-02-05 20:57:43,023 Validation result at epoch 269, step    18000: duration: 8.6602s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 3.03277	Translation Loss: 93732.18750	PPL: 11637.45703
	Eval Metric: BLEU
	WER 4.59	(DEL: 0.00,	INS: 0.00,	SUB: 4.59)
	BLEU-4 0.44	(BLEU-1: 10.25,	BLEU-2: 3.03,	BLEU-3: 1.09,	BLEU-4: 0.44)
	CHRF 17.14	ROUGE 8.57
2024-02-05 20:57:43,024 Logging Recognition and Translation Outputs
2024-02-05 20:57:43,024 ========================================================================================================================
2024-02-05 20:57:43,024 Logging Sequence: 179_309.00
2024-02-05 20:57:43,024 	Gloss Reference :	A B+C+D+E
2024-02-05 20:57:43,024 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 20:57:43,024 	Gloss Alignment :	         
2024-02-05 20:57:43,024 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:57:43,026 	Text Reference  :	before the ioa could send the    notice wfi   has asked phogat to explain her  indiscipline
2024-02-05 20:57:43,026 	Text Hypothesis :	****** *** we  could not  travel to     delhi as  there was    no one     more controversy 
2024-02-05 20:57:43,026 	Text Alignment  :	D      D   S         S    S      S      S     S   S     S      S  S       S    S           
2024-02-05 20:57:43,026 ========================================================================================================================
2024-02-05 20:57:43,026 Logging Sequence: 156_35.00
2024-02-05 20:57:43,026 	Gloss Reference :	A B+C+D+E
2024-02-05 20:57:43,027 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 20:57:43,027 	Gloss Alignment :	         
2024-02-05 20:57:43,027 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:57:43,029 	Text Reference  :	the     first  season of mlc   began on        13th  july 2023 and  ended   on   30th   july 2023 with   six teams
2024-02-05 20:57:43,029 	Text Hypothesis :	kolkata knight riders is owned by    bollywood actor shah rukh khan actress juhi chawla and  her  spouse jay mehta
2024-02-05 20:57:43,029 	Text Alignment  :	S       S      S      S  S     S     S         S     S    S    S    S       S    S      S    S    S      S   S    
2024-02-05 20:57:43,029 ========================================================================================================================
2024-02-05 20:57:43,029 Logging Sequence: 129_45.00
2024-02-05 20:57:43,029 	Gloss Reference :	A B+C+D+E
2024-02-05 20:57:43,030 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 20:57:43,030 	Gloss Alignment :	         
2024-02-05 20:57:43,030 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:57:43,031 	Text Reference  :	suga then announced that from    5  july onwards japan    will   be  in **** a    state of      emergency
2024-02-05 20:57:43,031 	Text Hypothesis :	**** **** ********* **** amazing to the  covid   pandemic people are in july 2021 to    provide him      
2024-02-05 20:57:43,031 	Text Alignment  :	D    D    D         D    S       S  S    S       S        S      S      I    S    S     S       S        
2024-02-05 20:57:43,031 ========================================================================================================================
2024-02-05 20:57:43,032 Logging Sequence: 56_17.00
2024-02-05 20:57:43,032 	Gloss Reference :	A B+C+D+E    
2024-02-05 20:57:43,032 	Gloss Hypothesis:	A B+C+D+E+D+E
2024-02-05 20:57:43,032 	Gloss Alignment :	  S          
2024-02-05 20:57:43,032 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:57:43,033 	Text Reference  :	it     was  held at    mumbai's wankhede stadium
2024-02-05 20:57:43,033 	Text Hypothesis :	people were very happy that     the      stump  
2024-02-05 20:57:43,033 	Text Alignment  :	S      S    S    S     S        S        S      
2024-02-05 20:57:43,033 ========================================================================================================================
2024-02-05 20:57:43,033 Logging Sequence: 152_73.00
2024-02-05 20:57:43,033 	Gloss Reference :	A B+C+D+E
2024-02-05 20:57:43,033 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 20:57:43,033 	Gloss Alignment :	         
2024-02-05 20:57:43,034 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 20:57:43,035 	Text Reference  :	********** ******** ** *** ***** **** ** *** *** eventually he too  got out  by shaheen afridi
2024-02-05 20:57:43,035 	Text Hypothesis :	pakistan's decision to bat first time to win any national   or will be  held in 18      overs 
2024-02-05 20:57:43,035 	Text Alignment  :	I          I        I  I   I     I    I  I   I   S          S  S    S   S    S  S       S     
2024-02-05 20:57:43,035 ========================================================================================================================
2024-02-05 20:57:45,072 Epoch 269: Total Training Recognition Loss 0.17  Total Training Translation Loss 7.99 
2024-02-05 20:57:45,072 EPOCH 270
2024-02-05 20:57:50,576 Epoch 270: Total Training Recognition Loss 0.14  Total Training Translation Loss 10.35 
2024-02-05 20:57:50,577 EPOCH 271
2024-02-05 20:57:51,257 [Epoch: 271 Step: 00018100] Batch Recognition Loss:   0.001009 => Gls Tokens per Sec:     2209 || Batch Translation Loss:   0.124861 => Txt Tokens per Sec:     5411 || Lr: 0.000100
2024-02-05 20:57:55,854 Epoch 271: Total Training Recognition Loss 0.18  Total Training Translation Loss 16.13 
2024-02-05 20:57:55,855 EPOCH 272
2024-02-05 20:57:59,181 [Epoch: 272 Step: 00018200] Batch Recognition Loss:   0.001685 => Gls Tokens per Sec:     2069 || Batch Translation Loss:   0.065893 => Txt Tokens per Sec:     5728 || Lr: 0.000100
2024-02-05 20:58:01,236 Epoch 272: Total Training Recognition Loss 0.26  Total Training Translation Loss 10.99 
2024-02-05 20:58:01,236 EPOCH 273
2024-02-05 20:58:06,536 Epoch 273: Total Training Recognition Loss 0.18  Total Training Translation Loss 7.67 
2024-02-05 20:58:06,537 EPOCH 274
2024-02-05 20:58:07,119 [Epoch: 274 Step: 00018300] Batch Recognition Loss:   0.000701 => Gls Tokens per Sec:     2473 || Batch Translation Loss:   0.056636 => Txt Tokens per Sec:     6123 || Lr: 0.000100
2024-02-05 20:58:12,120 Epoch 274: Total Training Recognition Loss 0.19  Total Training Translation Loss 5.90 
2024-02-05 20:58:12,121 EPOCH 275
2024-02-05 20:58:15,589 [Epoch: 275 Step: 00018400] Batch Recognition Loss:   0.000850 => Gls Tokens per Sec:     1909 || Batch Translation Loss:   0.183784 => Txt Tokens per Sec:     5295 || Lr: 0.000100
2024-02-05 20:58:17,633 Epoch 275: Total Training Recognition Loss 0.14  Total Training Translation Loss 5.60 
2024-02-05 20:58:17,633 EPOCH 276
2024-02-05 20:58:22,811 Epoch 276: Total Training Recognition Loss 0.16  Total Training Translation Loss 5.52 
2024-02-05 20:58:22,811 EPOCH 277
2024-02-05 20:58:23,327 [Epoch: 277 Step: 00018500] Batch Recognition Loss:   0.000820 => Gls Tokens per Sec:     2494 || Batch Translation Loss:   0.060839 => Txt Tokens per Sec:     6245 || Lr: 0.000100
2024-02-05 20:58:28,086 Epoch 277: Total Training Recognition Loss 0.19  Total Training Translation Loss 6.14 
2024-02-05 20:58:28,087 EPOCH 278
2024-02-05 20:58:30,822 [Epoch: 278 Step: 00018600] Batch Recognition Loss:   0.000552 => Gls Tokens per Sec:     2363 || Batch Translation Loss:   0.031265 => Txt Tokens per Sec:     6398 || Lr: 0.000100
2024-02-05 20:58:33,225 Epoch 278: Total Training Recognition Loss 0.15  Total Training Translation Loss 5.45 
2024-02-05 20:58:33,226 EPOCH 279
2024-02-05 20:58:38,767 Epoch 279: Total Training Recognition Loss 0.27  Total Training Translation Loss 5.27 
2024-02-05 20:58:38,767 EPOCH 280
2024-02-05 20:58:39,245 [Epoch: 280 Step: 00018700] Batch Recognition Loss:   0.012835 => Gls Tokens per Sec:     2361 || Batch Translation Loss:   0.253499 => Txt Tokens per Sec:     6408 || Lr: 0.000100
2024-02-05 20:58:44,131 Epoch 280: Total Training Recognition Loss 0.26  Total Training Translation Loss 5.70 
2024-02-05 20:58:44,131 EPOCH 281
2024-02-05 20:58:47,444 [Epoch: 281 Step: 00018800] Batch Recognition Loss:   0.001455 => Gls Tokens per Sec:     1932 || Batch Translation Loss:   0.042832 => Txt Tokens per Sec:     5443 || Lr: 0.000100
2024-02-05 20:58:49,471 Epoch 281: Total Training Recognition Loss 0.40  Total Training Translation Loss 9.94 
2024-02-05 20:58:49,471 EPOCH 282
2024-02-05 20:58:54,833 Epoch 282: Total Training Recognition Loss 0.30  Total Training Translation Loss 13.88 
2024-02-05 20:58:54,834 EPOCH 283
2024-02-05 20:58:55,193 [Epoch: 283 Step: 00018900] Batch Recognition Loss:   0.024449 => Gls Tokens per Sec:     2682 || Batch Translation Loss:   0.278313 => Txt Tokens per Sec:     6799 || Lr: 0.000100
2024-02-05 20:59:00,028 Epoch 283: Total Training Recognition Loss 0.82  Total Training Translation Loss 17.91 
2024-02-05 20:59:00,028 EPOCH 284
2024-02-05 20:59:02,652 [Epoch: 284 Step: 00019000] Batch Recognition Loss:   0.007561 => Gls Tokens per Sec:     2379 || Batch Translation Loss:   0.216000 => Txt Tokens per Sec:     6488 || Lr: 0.000100
2024-02-05 20:59:05,137 Epoch 284: Total Training Recognition Loss 2.36  Total Training Translation Loss 17.03 
2024-02-05 20:59:05,137 EPOCH 285
2024-02-05 20:59:10,236 Epoch 285: Total Training Recognition Loss 1.32  Total Training Translation Loss 13.92 
2024-02-05 20:59:10,236 EPOCH 286
2024-02-05 20:59:10,634 [Epoch: 286 Step: 00019100] Batch Recognition Loss:   0.000629 => Gls Tokens per Sec:     2015 || Batch Translation Loss:   0.324582 => Txt Tokens per Sec:     6448 || Lr: 0.000100
2024-02-05 20:59:15,389 Epoch 286: Total Training Recognition Loss 0.36  Total Training Translation Loss 16.71 
2024-02-05 20:59:15,390 EPOCH 287
2024-02-05 20:59:18,457 [Epoch: 287 Step: 00019200] Batch Recognition Loss:   0.000348 => Gls Tokens per Sec:     1951 || Batch Translation Loss:   0.112859 => Txt Tokens per Sec:     5402 || Lr: 0.000100
2024-02-05 20:59:20,767 Epoch 287: Total Training Recognition Loss 0.29  Total Training Translation Loss 16.21 
2024-02-05 20:59:20,767 EPOCH 288
2024-02-05 20:59:26,136 Epoch 288: Total Training Recognition Loss 0.11  Total Training Translation Loss 8.67 
2024-02-05 20:59:26,136 EPOCH 289
2024-02-05 20:59:26,552 [Epoch: 289 Step: 00019300] Batch Recognition Loss:   0.002065 => Gls Tokens per Sec:     1542 || Batch Translation Loss:   0.143066 => Txt Tokens per Sec:     4894 || Lr: 0.000100
2024-02-05 20:59:31,280 Epoch 289: Total Training Recognition Loss 0.16  Total Training Translation Loss 4.92 
2024-02-05 20:59:31,280 EPOCH 290
2024-02-05 20:59:34,351 [Epoch: 290 Step: 00019400] Batch Recognition Loss:   0.000326 => Gls Tokens per Sec:     1929 || Batch Translation Loss:   0.069097 => Txt Tokens per Sec:     5430 || Lr: 0.000100
2024-02-05 20:59:36,542 Epoch 290: Total Training Recognition Loss 0.16  Total Training Translation Loss 5.24 
2024-02-05 20:59:36,542 EPOCH 291
2024-02-05 20:59:41,097 Epoch 291: Total Training Recognition Loss 0.15  Total Training Translation Loss 4.82 
2024-02-05 20:59:41,098 EPOCH 292
2024-02-05 20:59:41,332 [Epoch: 292 Step: 00019500] Batch Recognition Loss:   0.008350 => Gls Tokens per Sec:     2051 || Batch Translation Loss:   0.181343 => Txt Tokens per Sec:     5231 || Lr: 0.000100
2024-02-05 20:59:46,566 Epoch 292: Total Training Recognition Loss 0.14  Total Training Translation Loss 5.15 
2024-02-05 20:59:46,566 EPOCH 293
2024-02-05 20:59:49,209 [Epoch: 293 Step: 00019600] Batch Recognition Loss:   0.000399 => Gls Tokens per Sec:     2143 || Batch Translation Loss:   0.027698 => Txt Tokens per Sec:     6152 || Lr: 0.000100
2024-02-05 20:59:51,446 Epoch 293: Total Training Recognition Loss 0.16  Total Training Translation Loss 4.36 
2024-02-05 20:59:51,447 EPOCH 294
2024-02-05 20:59:56,867 Epoch 294: Total Training Recognition Loss 0.18  Total Training Translation Loss 3.69 
2024-02-05 20:59:56,867 EPOCH 295
2024-02-05 20:59:56,968 [Epoch: 295 Step: 00019700] Batch Recognition Loss:   0.000500 => Gls Tokens per Sec:     3200 || Batch Translation Loss:   0.020007 => Txt Tokens per Sec:     7440 || Lr: 0.000100
2024-02-05 21:00:01,654 Epoch 295: Total Training Recognition Loss 0.14  Total Training Translation Loss 5.38 
2024-02-05 21:00:01,654 EPOCH 296
2024-02-05 21:00:04,407 [Epoch: 296 Step: 00019800] Batch Recognition Loss:   0.000353 => Gls Tokens per Sec:     2036 || Batch Translation Loss:   0.069775 => Txt Tokens per Sec:     5541 || Lr: 0.000100
2024-02-05 21:00:07,173 Epoch 296: Total Training Recognition Loss 0.09  Total Training Translation Loss 5.85 
2024-02-05 21:00:07,173 EPOCH 297
2024-02-05 21:00:12,243 Epoch 297: Total Training Recognition Loss 0.12  Total Training Translation Loss 7.11 
2024-02-05 21:00:12,244 EPOCH 298
2024-02-05 21:00:12,336 [Epoch: 298 Step: 00019900] Batch Recognition Loss:   0.001748 => Gls Tokens per Sec:     1758 || Batch Translation Loss:   0.092579 => Txt Tokens per Sec:     5033 || Lr: 0.000100
2024-02-05 21:00:17,304 Epoch 298: Total Training Recognition Loss 0.13  Total Training Translation Loss 9.95 
2024-02-05 21:00:17,304 EPOCH 299
2024-02-05 21:00:19,761 [Epoch: 299 Step: 00020000] Batch Recognition Loss:   0.010714 => Gls Tokens per Sec:     2174 || Batch Translation Loss:   0.020830 => Txt Tokens per Sec:     6003 || Lr: 0.000100
2024-02-05 21:00:28,362 Validation result at epoch 299, step    20000: duration: 8.5997s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 3.02108	Translation Loss: 94263.98438	PPL: 12272.30176
	Eval Metric: BLEU
	WER 4.59	(DEL: 0.00,	INS: 0.00,	SUB: 4.59)
	BLEU-4 0.64	(BLEU-1: 11.10,	BLEU-2: 3.36,	BLEU-3: 1.34,	BLEU-4: 0.64)
	CHRF 17.06	ROUGE 9.28
2024-02-05 21:00:28,363 Logging Recognition and Translation Outputs
2024-02-05 21:00:28,363 ========================================================================================================================
2024-02-05 21:00:28,363 Logging Sequence: 120_7.00
2024-02-05 21:00:28,363 	Gloss Reference :	A B+C+D+E
2024-02-05 21:00:28,364 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:00:28,364 	Gloss Alignment :	         
2024-02-05 21:00:28,364 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:00:28,365 	Text Reference  :	** he          had    tested positive for    covid-19 on     may    19  
2024-02-05 21:00:28,365 	Text Hypothesis :	on questioning people about  the      murder the      police learnt that
2024-02-05 21:00:28,365 	Text Alignment  :	I  S           S      S      S        S      S        S      S      S   
2024-02-05 21:00:28,365 ========================================================================================================================
2024-02-05 21:00:28,365 Logging Sequence: 148_186.00
2024-02-05 21:00:28,365 	Gloss Reference :	A B+C+D+E
2024-02-05 21:00:28,365 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:00:28,366 	Gloss Alignment :	         
2024-02-05 21:00:28,366 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:00:28,368 	Text Reference  :	*** siraj  also    took     four wickets in 1   over   thus      becoming the    record-holder for most   wickets in        an over in         odis    
2024-02-05 21:00:28,368 	Text Hypothesis :	sri lankan batsmen remained the  fall    of the eighth encounter between  hardik pandya        and shobit were    dismissed at a    commercial partners
2024-02-05 21:00:28,368 	Text Alignment  :	I   S      S       S        S    S       S  S   S      S         S        S      S             S   S      S       S         S  S    S          S       
2024-02-05 21:00:28,369 ========================================================================================================================
2024-02-05 21:00:28,369 Logging Sequence: 67_73.00
2024-02-05 21:00:28,369 	Gloss Reference :	A B+C+D+E
2024-02-05 21:00:28,369 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:00:28,369 	Gloss Alignment :	         
2024-02-05 21:00:28,369 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:00:28,370 	Text Reference  :	***** in   his       tweet he      also said
2024-02-05 21:00:28,370 	Text Hypothesis :	there were jubiliant for   various news here
2024-02-05 21:00:28,370 	Text Alignment  :	I     S    S         S     S       S    S   
2024-02-05 21:00:28,370 ========================================================================================================================
2024-02-05 21:00:28,371 Logging Sequence: 164_526.00
2024-02-05 21:00:28,371 	Gloss Reference :	A B+C+D+E
2024-02-05 21:00:28,371 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:00:28,371 	Gloss Alignment :	         
2024-02-05 21:00:28,371 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:00:28,372 	Text Reference  :	*** *** ***** ********* you  are      aware that viacom18 bought the  broadcast rights of     ipl     
2024-02-05 21:00:28,373 	Text Hypothesis :	the two prime ministers were welcomed with  loud cheers   as     they took      a      sudden decision
2024-02-05 21:00:28,373 	Text Alignment  :	I   I   I     I         S    S        S     S    S        S      S    S         S      S      S       
2024-02-05 21:00:28,373 ========================================================================================================================
2024-02-05 21:00:28,373 Logging Sequence: 108_28.00
2024-02-05 21:00:28,373 	Gloss Reference :	A B+C+D+E
2024-02-05 21:00:28,373 	Gloss Hypothesis:	A B+C+D  
2024-02-05 21:00:28,374 	Gloss Alignment :	  S      
2024-02-05 21:00:28,374 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:00:28,375 	Text Reference  :	the 10 teams bought 204    players including 67  foreign players after spending a  total of       rs     55170 crore
2024-02-05 21:00:28,375 	Text Hypothesis :	*** ** ***** ****** handed over    the       ipl matches were    sold  out      to be    produced before any   crore
2024-02-05 21:00:28,376 	Text Alignment  :	D   D  D     D      S      S       S         S   S       S       S     S        S  S     S        S      S          
2024-02-05 21:00:28,376 ========================================================================================================================
2024-02-05 21:00:31,048 Epoch 299: Total Training Recognition Loss 0.18  Total Training Translation Loss 9.88 
2024-02-05 21:00:31,049 EPOCH 300
2024-02-05 21:00:36,449 [Epoch: 300 Step: 00020100] Batch Recognition Loss:   0.001490 => Gls Tokens per Sec:     1967 || Batch Translation Loss:   0.072072 => Txt Tokens per Sec:     5443 || Lr: 0.000100
2024-02-05 21:00:36,450 Epoch 300: Total Training Recognition Loss 0.14  Total Training Translation Loss 7.30 
2024-02-05 21:00:36,450 EPOCH 301
2024-02-05 21:00:42,013 Epoch 301: Total Training Recognition Loss 0.11  Total Training Translation Loss 8.08 
2024-02-05 21:00:42,014 EPOCH 302
2024-02-05 21:00:44,418 [Epoch: 302 Step: 00020200] Batch Recognition Loss:   0.001073 => Gls Tokens per Sec:     2155 || Batch Translation Loss:   0.152252 => Txt Tokens per Sec:     5824 || Lr: 0.000100
2024-02-05 21:00:47,423 Epoch 302: Total Training Recognition Loss 0.11  Total Training Translation Loss 8.74 
2024-02-05 21:00:47,423 EPOCH 303
2024-02-05 21:00:52,763 [Epoch: 303 Step: 00020300] Batch Recognition Loss:   0.000524 => Gls Tokens per Sec:     1960 || Batch Translation Loss:   0.118939 => Txt Tokens per Sec:     5409 || Lr: 0.000100
2024-02-05 21:00:52,861 Epoch 303: Total Training Recognition Loss 0.12  Total Training Translation Loss 8.88 
2024-02-05 21:00:52,862 EPOCH 304
2024-02-05 21:00:57,894 Epoch 304: Total Training Recognition Loss 0.08  Total Training Translation Loss 6.63 
2024-02-05 21:00:57,895 EPOCH 305
2024-02-05 21:01:00,780 [Epoch: 305 Step: 00020400] Batch Recognition Loss:   0.000409 => Gls Tokens per Sec:     1742 || Batch Translation Loss:   0.041756 => Txt Tokens per Sec:     5059 || Lr: 0.000100
2024-02-05 21:01:03,461 Epoch 305: Total Training Recognition Loss 0.18  Total Training Translation Loss 5.68 
2024-02-05 21:01:03,461 EPOCH 306
2024-02-05 21:01:08,705 [Epoch: 306 Step: 00020500] Batch Recognition Loss:   0.005588 => Gls Tokens per Sec:     1965 || Batch Translation Loss:   0.090465 => Txt Tokens per Sec:     5424 || Lr: 0.000100
2024-02-05 21:01:08,932 Epoch 306: Total Training Recognition Loss 0.17  Total Training Translation Loss 8.93 
2024-02-05 21:01:08,932 EPOCH 307
2024-02-05 21:01:14,365 Epoch 307: Total Training Recognition Loss 0.14  Total Training Translation Loss 8.56 
2024-02-05 21:01:14,365 EPOCH 308
2024-02-05 21:01:16,564 [Epoch: 308 Step: 00020600] Batch Recognition Loss:   0.000352 => Gls Tokens per Sec:     2211 || Batch Translation Loss:   0.177087 => Txt Tokens per Sec:     6095 || Lr: 0.000100
2024-02-05 21:01:19,069 Epoch 308: Total Training Recognition Loss 0.14  Total Training Translation Loss 9.53 
2024-02-05 21:01:19,070 EPOCH 309
2024-02-05 21:01:24,479 [Epoch: 309 Step: 00020700] Batch Recognition Loss:   0.000365 => Gls Tokens per Sec:     1875 || Batch Translation Loss:   0.051037 => Txt Tokens per Sec:     5195 || Lr: 0.000100
2024-02-05 21:01:24,694 Epoch 309: Total Training Recognition Loss 0.10  Total Training Translation Loss 8.75 
2024-02-05 21:01:24,694 EPOCH 310
2024-02-05 21:01:29,630 Epoch 310: Total Training Recognition Loss 0.12  Total Training Translation Loss 9.55 
2024-02-05 21:01:29,630 EPOCH 311
2024-02-05 21:01:31,574 [Epoch: 311 Step: 00020800] Batch Recognition Loss:   0.000737 => Gls Tokens per Sec:     2469 || Batch Translation Loss:   0.123341 => Txt Tokens per Sec:     6636 || Lr: 0.000100
2024-02-05 21:01:34,822 Epoch 311: Total Training Recognition Loss 0.11  Total Training Translation Loss 8.77 
2024-02-05 21:01:34,823 EPOCH 312
2024-02-05 21:01:39,454 [Epoch: 312 Step: 00020900] Batch Recognition Loss:   0.003948 => Gls Tokens per Sec:     2156 || Batch Translation Loss:   0.140154 => Txt Tokens per Sec:     5939 || Lr: 0.000100
2024-02-05 21:01:39,735 Epoch 312: Total Training Recognition Loss 0.16  Total Training Translation Loss 9.42 
2024-02-05 21:01:39,735 EPOCH 313
2024-02-05 21:01:44,937 Epoch 313: Total Training Recognition Loss 0.07  Total Training Translation Loss 8.81 
2024-02-05 21:01:44,938 EPOCH 314
2024-02-05 21:01:47,103 [Epoch: 314 Step: 00021000] Batch Recognition Loss:   0.000623 => Gls Tokens per Sec:     2098 || Batch Translation Loss:   0.011260 => Txt Tokens per Sec:     5693 || Lr: 0.000100
2024-02-05 21:01:50,011 Epoch 314: Total Training Recognition Loss 0.12  Total Training Translation Loss 7.24 
2024-02-05 21:01:50,012 EPOCH 315
2024-02-05 21:01:55,137 [Epoch: 315 Step: 00021100] Batch Recognition Loss:   0.004604 => Gls Tokens per Sec:     1916 || Batch Translation Loss:   0.084013 => Txt Tokens per Sec:     5328 || Lr: 0.000100
2024-02-05 21:01:55,494 Epoch 315: Total Training Recognition Loss 0.25  Total Training Translation Loss 9.48 
2024-02-05 21:01:55,494 EPOCH 316
2024-02-05 21:02:00,288 Epoch 316: Total Training Recognition Loss 0.14  Total Training Translation Loss 11.03 
2024-02-05 21:02:00,289 EPOCH 317
2024-02-05 21:02:02,692 [Epoch: 317 Step: 00021200] Batch Recognition Loss:   0.000426 => Gls Tokens per Sec:     1824 || Batch Translation Loss:   0.081124 => Txt Tokens per Sec:     5253 || Lr: 0.000100
2024-02-05 21:02:05,704 Epoch 317: Total Training Recognition Loss 0.14  Total Training Translation Loss 9.71 
2024-02-05 21:02:05,704 EPOCH 318
2024-02-05 21:02:10,103 [Epoch: 318 Step: 00021300] Batch Recognition Loss:   0.005200 => Gls Tokens per Sec:     2196 || Batch Translation Loss:   0.368053 => Txt Tokens per Sec:     6113 || Lr: 0.000100
2024-02-05 21:02:10,558 Epoch 318: Total Training Recognition Loss 0.19  Total Training Translation Loss 9.91 
2024-02-05 21:02:10,558 EPOCH 319
2024-02-05 21:02:16,170 Epoch 319: Total Training Recognition Loss 0.17  Total Training Translation Loss 7.76 
2024-02-05 21:02:16,170 EPOCH 320
2024-02-05 21:02:18,262 [Epoch: 320 Step: 00021400] Batch Recognition Loss:   0.002125 => Gls Tokens per Sec:     2066 || Batch Translation Loss:   0.110163 => Txt Tokens per Sec:     5587 || Lr: 0.000100
2024-02-05 21:02:21,600 Epoch 320: Total Training Recognition Loss 0.17  Total Training Translation Loss 7.43 
2024-02-05 21:02:21,600 EPOCH 321
2024-02-05 21:02:25,690 [Epoch: 321 Step: 00021500] Batch Recognition Loss:   0.000669 => Gls Tokens per Sec:     2324 || Batch Translation Loss:   0.031870 => Txt Tokens per Sec:     6424 || Lr: 0.000100
2024-02-05 21:02:26,470 Epoch 321: Total Training Recognition Loss 0.19  Total Training Translation Loss 4.66 
2024-02-05 21:02:26,470 EPOCH 322
2024-02-05 21:02:31,918 Epoch 322: Total Training Recognition Loss 0.19  Total Training Translation Loss 5.18 
2024-02-05 21:02:31,918 EPOCH 323
2024-02-05 21:02:33,902 [Epoch: 323 Step: 00021600] Batch Recognition Loss:   0.000736 => Gls Tokens per Sec:     2097 || Batch Translation Loss:   0.489639 => Txt Tokens per Sec:     6061 || Lr: 0.000100
2024-02-05 21:02:37,004 Epoch 323: Total Training Recognition Loss 0.17  Total Training Translation Loss 7.78 
2024-02-05 21:02:37,005 EPOCH 324
2024-02-05 21:02:41,629 [Epoch: 324 Step: 00021700] Batch Recognition Loss:   0.010457 => Gls Tokens per Sec:     2020 || Batch Translation Loss:   0.140988 => Txt Tokens per Sec:     5530 || Lr: 0.000100
2024-02-05 21:02:42,287 Epoch 324: Total Training Recognition Loss 0.18  Total Training Translation Loss 8.00 
2024-02-05 21:02:42,287 EPOCH 325
2024-02-05 21:02:47,351 Epoch 325: Total Training Recognition Loss 0.10  Total Training Translation Loss 9.71 
2024-02-05 21:02:47,352 EPOCH 326
2024-02-05 21:02:49,450 [Epoch: 326 Step: 00021800] Batch Recognition Loss:   0.001783 => Gls Tokens per Sec:     1860 || Batch Translation Loss:   0.029783 => Txt Tokens per Sec:     5043 || Lr: 0.000100
2024-02-05 21:02:52,982 Epoch 326: Total Training Recognition Loss 0.11  Total Training Translation Loss 8.74 
2024-02-05 21:02:52,983 EPOCH 327
2024-02-05 21:02:57,636 [Epoch: 327 Step: 00021900] Batch Recognition Loss:   0.002117 => Gls Tokens per Sec:     1973 || Batch Translation Loss:   0.022579 => Txt Tokens per Sec:     5479 || Lr: 0.000100
2024-02-05 21:02:58,257 Epoch 327: Total Training Recognition Loss 0.12  Total Training Translation Loss 6.23 
2024-02-05 21:02:58,257 EPOCH 328
2024-02-05 21:03:03,732 Epoch 328: Total Training Recognition Loss 0.10  Total Training Translation Loss 5.22 
2024-02-05 21:03:03,733 EPOCH 329
2024-02-05 21:03:05,724 [Epoch: 329 Step: 00022000] Batch Recognition Loss:   0.005974 => Gls Tokens per Sec:     1931 || Batch Translation Loss:   0.056805 => Txt Tokens per Sec:     5381 || Lr: 0.000100
2024-02-05 21:03:13,921 Validation result at epoch 329, step    22000: duration: 8.1970s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 3.14691	Translation Loss: 94200.10938	PPL: 12194.25293
	Eval Metric: BLEU
	WER 4.73	(DEL: 0.00,	INS: 0.00,	SUB: 4.73)
	BLEU-4 0.53	(BLEU-1: 10.96,	BLEU-2: 3.45,	BLEU-3: 1.22,	BLEU-4: 0.53)
	CHRF 16.59	ROUGE 9.36
2024-02-05 21:03:13,922 Logging Recognition and Translation Outputs
2024-02-05 21:03:13,922 ========================================================================================================================
2024-02-05 21:03:13,923 Logging Sequence: 179_2.00
2024-02-05 21:03:13,923 	Gloss Reference :	A B+C+D+E
2024-02-05 21:03:13,923 	Gloss Hypothesis:	A B+C+D  
2024-02-05 21:03:13,923 	Gloss Alignment :	  S      
2024-02-05 21:03:13,923 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:03:13,924 	Text Reference  :	*** *** **** vinesh phogat is           a  well known   wrestler
2024-02-05 21:03:13,924 	Text Hypothesis :	the csk team was    very   disappointed by the  winning it      
2024-02-05 21:03:13,924 	Text Alignment  :	I   I   I    S      S      S            S  S    S       S       
2024-02-05 21:03:13,924 ========================================================================================================================
2024-02-05 21:03:13,924 Logging Sequence: 55_124.00
2024-02-05 21:03:13,925 	Gloss Reference :	A B+C+D+E
2024-02-05 21:03:13,925 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:03:13,925 	Gloss Alignment :	         
2024-02-05 21:03:13,925 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:03:13,926 	Text Reference  :	*** next   to   him  with    the    patel    jersey was ajaz patel 
2024-02-05 21:03:13,926 	Text Hypothesis :	the mumbai team vice captain smriti mandhana is     in  the  reason
2024-02-05 21:03:13,926 	Text Alignment  :	I   S      S    S    S       S      S        S      S   S    S     
2024-02-05 21:03:13,926 ========================================================================================================================
2024-02-05 21:03:13,926 Logging Sequence: 148_105.00
2024-02-05 21:03:13,927 	Gloss Reference :	A B+C+D+E
2024-02-05 21:03:13,927 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:03:13,927 	Gloss Alignment :	         
2024-02-05 21:03:13,927 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:03:13,928 	Text Reference  :	later with amazing bowling by hardik pandya and kuldeep yadav sri  lanka were    all out in     just 50   runs     
2024-02-05 21:03:13,929 	Text Hypothesis :	***** **** ******* ******* ** ****** pandya *** led     his   team to    victory in  the indian team were postponed
2024-02-05 21:03:13,929 	Text Alignment  :	D     D    D       D       D  D             D   S       S     S    S     S       S   S   S      S    S    S        
2024-02-05 21:03:13,929 ========================================================================================================================
2024-02-05 21:03:13,929 Logging Sequence: 125_165.00
2024-02-05 21:03:13,929 	Gloss Reference :	A B+C+D+E
2024-02-05 21:03:13,929 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:03:13,929 	Gloss Alignment :	         
2024-02-05 21:03:13,929 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:03:13,931 	Text Reference  :	please do   not  target   nadeem we  speak to each other and share       a  good bond
2024-02-05 21:03:13,931 	Text Hypothesis :	****** many such comments does   not want  to **** ***** *** participate in the  2020
2024-02-05 21:03:13,931 	Text Alignment  :	D      S    S    S        S      S   S        D    D     D   S           S  S    S   
2024-02-05 21:03:13,931 ========================================================================================================================
2024-02-05 21:03:13,931 Logging Sequence: 77_52.00
2024-02-05 21:03:13,931 	Gloss Reference :	A B+C+D+E
2024-02-05 21:03:13,931 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:03:13,931 	Gloss Alignment :	         
2024-02-05 21:03:13,932 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:03:13,933 	Text Reference  :	kane    williamson held down the fort for hyderabad by      scoring 66   runs  and     ended the match in a     tie    
2024-02-05 21:03:13,933 	Text Hypothesis :	however some       said that the **** *** ********* taliban won     this while playing for   the ***** ** trent rockets
2024-02-05 21:03:13,933 	Text Alignment  :	S       S          S    S        D    D   D         S       S       S    S     S       S         D     D  S     S      
2024-02-05 21:03:13,933 ========================================================================================================================
2024-02-05 21:03:17,328 Epoch 329: Total Training Recognition Loss 0.11  Total Training Translation Loss 5.17 
2024-02-05 21:03:17,328 EPOCH 330
2024-02-05 21:03:21,551 [Epoch: 330 Step: 00022100] Batch Recognition Loss:   0.002632 => Gls Tokens per Sec:     2136 || Batch Translation Loss:   0.152742 => Txt Tokens per Sec:     5798 || Lr: 0.000100
2024-02-05 21:03:22,697 Epoch 330: Total Training Recognition Loss 0.13  Total Training Translation Loss 4.27 
2024-02-05 21:03:22,697 EPOCH 331
2024-02-05 21:03:28,144 Epoch 331: Total Training Recognition Loss 0.13  Total Training Translation Loss 3.90 
2024-02-05 21:03:28,144 EPOCH 332
2024-02-05 21:03:29,702 [Epoch: 332 Step: 00022200] Batch Recognition Loss:   0.000189 => Gls Tokens per Sec:     2364 || Batch Translation Loss:   0.031410 => Txt Tokens per Sec:     6428 || Lr: 0.000100
2024-02-05 21:03:33,099 Epoch 332: Total Training Recognition Loss 0.17  Total Training Translation Loss 4.66 
2024-02-05 21:03:33,099 EPOCH 333
2024-02-05 21:03:37,560 [Epoch: 333 Step: 00022300] Batch Recognition Loss:   0.000347 => Gls Tokens per Sec:     1987 || Batch Translation Loss:   0.081963 => Txt Tokens per Sec:     5490 || Lr: 0.000100
2024-02-05 21:03:38,541 Epoch 333: Total Training Recognition Loss 0.07  Total Training Translation Loss 7.07 
2024-02-05 21:03:38,542 EPOCH 334
2024-02-05 21:03:44,084 Epoch 334: Total Training Recognition Loss 0.15  Total Training Translation Loss 9.23 
2024-02-05 21:03:44,085 EPOCH 335
2024-02-05 21:03:45,641 [Epoch: 335 Step: 00022400] Batch Recognition Loss:   0.000424 => Gls Tokens per Sec:     2263 || Batch Translation Loss:   0.194276 => Txt Tokens per Sec:     6262 || Lr: 0.000100
2024-02-05 21:03:49,293 Epoch 335: Total Training Recognition Loss 0.17  Total Training Translation Loss 11.34 
2024-02-05 21:03:49,294 EPOCH 336
2024-02-05 21:03:53,790 [Epoch: 336 Step: 00022500] Batch Recognition Loss:   0.000913 => Gls Tokens per Sec:     1936 || Batch Translation Loss:   0.038680 => Txt Tokens per Sec:     5439 || Lr: 0.000100
2024-02-05 21:03:54,791 Epoch 336: Total Training Recognition Loss 0.17  Total Training Translation Loss 11.72 
2024-02-05 21:03:54,791 EPOCH 337
2024-02-05 21:04:00,024 Epoch 337: Total Training Recognition Loss 0.12  Total Training Translation Loss 9.27 
2024-02-05 21:04:00,024 EPOCH 338
2024-02-05 21:04:01,643 [Epoch: 338 Step: 00022600] Batch Recognition Loss:   0.028149 => Gls Tokens per Sec:     2017 || Batch Translation Loss:   0.109825 => Txt Tokens per Sec:     5413 || Lr: 0.000100
2024-02-05 21:04:05,434 Epoch 338: Total Training Recognition Loss 0.16  Total Training Translation Loss 7.40 
2024-02-05 21:04:05,434 EPOCH 339
2024-02-05 21:04:09,528 [Epoch: 339 Step: 00022700] Batch Recognition Loss:   0.002658 => Gls Tokens per Sec:     2086 || Batch Translation Loss:   0.060168 => Txt Tokens per Sec:     5803 || Lr: 0.000100
2024-02-05 21:04:10,547 Epoch 339: Total Training Recognition Loss 0.12  Total Training Translation Loss 7.86 
2024-02-05 21:04:10,547 EPOCH 340
2024-02-05 21:04:15,992 Epoch 340: Total Training Recognition Loss 0.13  Total Training Translation Loss 7.76 
2024-02-05 21:04:15,993 EPOCH 341
2024-02-05 21:04:17,684 [Epoch: 341 Step: 00022800] Batch Recognition Loss:   0.008883 => Gls Tokens per Sec:     1835 || Batch Translation Loss:   0.048700 => Txt Tokens per Sec:     5037 || Lr: 0.000100
2024-02-05 21:04:21,481 Epoch 341: Total Training Recognition Loss 0.20  Total Training Translation Loss 6.49 
2024-02-05 21:04:21,481 EPOCH 342
2024-02-05 21:04:25,648 [Epoch: 342 Step: 00022900] Batch Recognition Loss:   0.000585 => Gls Tokens per Sec:     2012 || Batch Translation Loss:   0.180908 => Txt Tokens per Sec:     5578 || Lr: 0.000100
2024-02-05 21:04:26,800 Epoch 342: Total Training Recognition Loss 0.07  Total Training Translation Loss 6.23 
2024-02-05 21:04:26,801 EPOCH 343
2024-02-05 21:04:32,200 Epoch 343: Total Training Recognition Loss 0.13  Total Training Translation Loss 6.92 
2024-02-05 21:04:32,201 EPOCH 344
2024-02-05 21:04:33,693 [Epoch: 344 Step: 00023000] Batch Recognition Loss:   0.001012 => Gls Tokens per Sec:     2039 || Batch Translation Loss:   0.078789 => Txt Tokens per Sec:     5410 || Lr: 0.000100
2024-02-05 21:04:37,795 Epoch 344: Total Training Recognition Loss 0.17  Total Training Translation Loss 6.11 
2024-02-05 21:04:37,796 EPOCH 345
2024-02-05 21:04:41,910 [Epoch: 345 Step: 00023100] Batch Recognition Loss:   0.000744 => Gls Tokens per Sec:     1999 || Batch Translation Loss:   0.036157 => Txt Tokens per Sec:     5538 || Lr: 0.000100
2024-02-05 21:04:43,101 Epoch 345: Total Training Recognition Loss 0.19  Total Training Translation Loss 6.71 
2024-02-05 21:04:43,102 EPOCH 346
2024-02-05 21:04:48,628 Epoch 346: Total Training Recognition Loss 0.10  Total Training Translation Loss 7.39 
2024-02-05 21:04:48,629 EPOCH 347
2024-02-05 21:04:50,085 [Epoch: 347 Step: 00023200] Batch Recognition Loss:   0.000437 => Gls Tokens per Sec:     1979 || Batch Translation Loss:   0.054993 => Txt Tokens per Sec:     5299 || Lr: 0.000100
2024-02-05 21:04:54,085 Epoch 347: Total Training Recognition Loss 0.25  Total Training Translation Loss 11.34 
2024-02-05 21:04:54,085 EPOCH 348
2024-02-05 21:04:58,210 [Epoch: 348 Step: 00023300] Batch Recognition Loss:   0.002277 => Gls Tokens per Sec:     1954 || Batch Translation Loss:   0.129186 => Txt Tokens per Sec:     5391 || Lr: 0.000100
2024-02-05 21:04:59,472 Epoch 348: Total Training Recognition Loss 0.25  Total Training Translation Loss 12.66 
2024-02-05 21:04:59,472 EPOCH 349
2024-02-05 21:05:04,835 Epoch 349: Total Training Recognition Loss 0.22  Total Training Translation Loss 8.38 
2024-02-05 21:05:04,835 EPOCH 350
2024-02-05 21:05:06,198 [Epoch: 350 Step: 00023400] Batch Recognition Loss:   0.000493 => Gls Tokens per Sec:     1997 || Batch Translation Loss:   0.042886 => Txt Tokens per Sec:     5327 || Lr: 0.000100
2024-02-05 21:05:10,367 Epoch 350: Total Training Recognition Loss 0.14  Total Training Translation Loss 4.40 
2024-02-05 21:05:10,367 EPOCH 351
2024-02-05 21:05:14,125 [Epoch: 351 Step: 00023500] Batch Recognition Loss:   0.015318 => Gls Tokens per Sec:     2103 || Batch Translation Loss:   0.150966 => Txt Tokens per Sec:     5852 || Lr: 0.000100
2024-02-05 21:05:15,444 Epoch 351: Total Training Recognition Loss 0.18  Total Training Translation Loss 7.24 
2024-02-05 21:05:15,444 EPOCH 352
2024-02-05 21:05:20,886 Epoch 352: Total Training Recognition Loss 0.14  Total Training Translation Loss 7.10 
2024-02-05 21:05:20,886 EPOCH 353
2024-02-05 21:05:22,007 [Epoch: 353 Step: 00023600] Batch Recognition Loss:   0.000230 => Gls Tokens per Sec:     2285 || Batch Translation Loss:   0.082422 => Txt Tokens per Sec:     6149 || Lr: 0.000100
2024-02-05 21:05:26,361 Epoch 353: Total Training Recognition Loss 0.12  Total Training Translation Loss 4.49 
2024-02-05 21:05:26,362 EPOCH 354
2024-02-05 21:05:30,497 [Epoch: 354 Step: 00023700] Batch Recognition Loss:   0.006798 => Gls Tokens per Sec:     1873 || Batch Translation Loss:   0.027030 => Txt Tokens per Sec:     5170 || Lr: 0.000100
2024-02-05 21:05:31,900 Epoch 354: Total Training Recognition Loss 0.12  Total Training Translation Loss 4.70 
2024-02-05 21:05:31,900 EPOCH 355
2024-02-05 21:05:37,422 Epoch 355: Total Training Recognition Loss 0.11  Total Training Translation Loss 4.01 
2024-02-05 21:05:37,423 EPOCH 356
2024-02-05 21:05:38,515 [Epoch: 356 Step: 00023800] Batch Recognition Loss:   0.001191 => Gls Tokens per Sec:     2202 || Batch Translation Loss:   0.017029 => Txt Tokens per Sec:     5868 || Lr: 0.000100
2024-02-05 21:05:42,666 Epoch 356: Total Training Recognition Loss 0.10  Total Training Translation Loss 5.15 
2024-02-05 21:05:42,667 EPOCH 357
2024-02-05 21:05:46,673 [Epoch: 357 Step: 00023900] Batch Recognition Loss:   0.000405 => Gls Tokens per Sec:     1893 || Batch Translation Loss:   0.049269 => Txt Tokens per Sec:     5312 || Lr: 0.000100
2024-02-05 21:05:48,147 Epoch 357: Total Training Recognition Loss 0.05  Total Training Translation Loss 4.72 
2024-02-05 21:05:48,148 EPOCH 358
2024-02-05 21:05:53,732 Epoch 358: Total Training Recognition Loss 0.08  Total Training Translation Loss 4.32 
2024-02-05 21:05:53,733 EPOCH 359
2024-02-05 21:05:54,852 [Epoch: 359 Step: 00024000] Batch Recognition Loss:   0.000564 => Gls Tokens per Sec:     2005 || Batch Translation Loss:   0.780096 => Txt Tokens per Sec:     5682 || Lr: 0.000100
2024-02-05 21:06:03,534 Validation result at epoch 359, step    24000: duration: 8.6817s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 3.25261	Translation Loss: 93765.86719	PPL: 11676.66699
	Eval Metric: BLEU
	WER 4.66	(DEL: 0.00,	INS: 0.00,	SUB: 4.66)
	BLEU-4 0.51	(BLEU-1: 10.21,	BLEU-2: 3.11,	BLEU-3: 1.17,	BLEU-4: 0.51)
	CHRF 17.00	ROUGE 8.68
2024-02-05 21:06:03,535 Logging Recognition and Translation Outputs
2024-02-05 21:06:03,535 ========================================================================================================================
2024-02-05 21:06:03,535 Logging Sequence: 171_2.00
2024-02-05 21:06:03,536 	Gloss Reference :	A B+C+D+E  
2024-02-05 21:06:03,536 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-05 21:06:03,536 	Gloss Alignment :	  S        
2024-02-05 21:06:03,536 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:06:03,537 	Text Reference  :	as you might all know that the ipl is      about to    end     the finals are    on       28th   may  
2024-02-05 21:06:03,537 	Text Hypothesis :	** *** ***** *** **** **** on  16  october in    hansi haryana the ****** police summoned yuvraj singh
2024-02-05 21:06:03,537 	Text Alignment  :	D  D   D     D   D    D    S   S   S       S     S     S           D      S      S        S      S    
2024-02-05 21:06:03,538 ========================================================================================================================
2024-02-05 21:06:03,538 Logging Sequence: 119_33.00
2024-02-05 21:06:03,538 	Gloss Reference :	A B+C+D+E
2024-02-05 21:06:03,538 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:06:03,538 	Gloss Alignment :	         
2024-02-05 21:06:03,538 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:06:03,539 	Text Reference  :	he **** ******* * **** *** *** ******* ********** ******* wanted to *** gift  35 people    wow wonderful
2024-02-05 21:06:03,539 	Text Hypothesis :	he then noticed a gift how was popular footballer numbers next   to the staff to celebrate the gift     
2024-02-05 21:06:03,539 	Text Alignment  :	   I    I       I I    I   I   I       I          I       S         I   S     S  S         S   S        
2024-02-05 21:06:03,540 ========================================================================================================================
2024-02-05 21:06:03,540 Logging Sequence: 158_131.00
2024-02-05 21:06:03,540 	Gloss Reference :	A B+C+D+E
2024-02-05 21:06:03,540 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:06:03,540 	Gloss Alignment :	         
2024-02-05 21:06:03,540 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:06:03,542 	Text Reference  :	on 10th april 2023 there was a match between rcb     and   lsg  in    bengaluru
2024-02-05 21:06:03,542 	Text Hypothesis :	do you  know  that daley is  a ***** deaf    cricket along with their choice   
2024-02-05 21:06:03,542 	Text Alignment  :	S  S    S     S    S     S     D     S       S       S     S    S     S        
2024-02-05 21:06:03,542 ========================================================================================================================
2024-02-05 21:06:03,542 Logging Sequence: 164_412.00
2024-02-05 21:06:03,542 	Gloss Reference :	A B+C+D+E
2024-02-05 21:06:03,542 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:06:03,542 	Gloss Alignment :	         
2024-02-05 21:06:03,543 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:06:03,544 	Text Reference  :	if you divide these two figures you will be shocked to know     that       each ball's worth  is rs  50         lakhs  
2024-02-05 21:06:03,544 	Text Hypothesis :	** *** ****** ***** *** ******* *** **** ** ******* ** reliance industries owns 51     shares of the viacom18's company
2024-02-05 21:06:03,544 	Text Alignment  :	D  D   D      D     D   D       D   D    D  D       D  S        S          S    S      S      S  S   S          S      
2024-02-05 21:06:03,544 ========================================================================================================================
2024-02-05 21:06:03,544 Logging Sequence: 159_112.00
2024-02-05 21:06:03,544 	Gloss Reference :	A B+C+D+E    
2024-02-05 21:06:03,545 	Gloss Hypothesis:	A B+C+D+E+D+E
2024-02-05 21:06:03,545 	Gloss Alignment :	  S          
2024-02-05 21:06:03,545 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:06:03,547 	Text Reference  :	******** kohli had revealed that **** before the tournament he    did not  touch his bat   for  a  month yes 1   month
2024-02-05 21:06:03,547 	Text Hypothesis :	mohammed shami has said     that both nabi   and t20        world cup many teams are being sent to focus on  his odi  
2024-02-05 21:06:03,547 	Text Alignment  :	I        S     S   S             I    S      S   S          S     S   S    S     S   S     S    S  S     S   S   S    
2024-02-05 21:06:03,547 ========================================================================================================================
2024-02-05 21:06:07,874 Epoch 359: Total Training Recognition Loss 0.08  Total Training Translation Loss 7.73 
2024-02-05 21:06:07,875 EPOCH 360
2024-02-05 21:06:11,718 [Epoch: 360 Step: 00024100] Batch Recognition Loss:   0.004915 => Gls Tokens per Sec:     1931 || Batch Translation Loss:   0.248798 => Txt Tokens per Sec:     5267 || Lr: 0.000100
2024-02-05 21:06:13,310 Epoch 360: Total Training Recognition Loss 0.12  Total Training Translation Loss 16.48 
2024-02-05 21:06:13,310 EPOCH 361
2024-02-05 21:06:18,410 Epoch 361: Total Training Recognition Loss 0.27  Total Training Translation Loss 12.30 
2024-02-05 21:06:18,410 EPOCH 362
2024-02-05 21:06:19,327 [Epoch: 362 Step: 00024200] Batch Recognition Loss:   0.000455 => Gls Tokens per Sec:     2271 || Batch Translation Loss:   0.117068 => Txt Tokens per Sec:     6016 || Lr: 0.000100
2024-02-05 21:06:23,895 Epoch 362: Total Training Recognition Loss 0.19  Total Training Translation Loss 9.47 
2024-02-05 21:06:23,895 EPOCH 363
2024-02-05 21:06:27,178 [Epoch: 363 Step: 00024300] Batch Recognition Loss:   0.003834 => Gls Tokens per Sec:     2212 || Batch Translation Loss:   0.701861 => Txt Tokens per Sec:     5996 || Lr: 0.000100
2024-02-05 21:06:29,167 Epoch 363: Total Training Recognition Loss 0.16  Total Training Translation Loss 12.41 
2024-02-05 21:06:29,168 EPOCH 364
2024-02-05 21:06:34,738 Epoch 364: Total Training Recognition Loss 0.14  Total Training Translation Loss 8.07 
2024-02-05 21:06:34,739 EPOCH 365
2024-02-05 21:06:35,689 [Epoch: 365 Step: 00024400] Batch Recognition Loss:   0.006094 => Gls Tokens per Sec:     2026 || Batch Translation Loss:   0.031985 => Txt Tokens per Sec:     5680 || Lr: 0.000100
2024-02-05 21:06:39,932 Epoch 365: Total Training Recognition Loss 0.10  Total Training Translation Loss 5.40 
2024-02-05 21:06:39,932 EPOCH 366
2024-02-05 21:06:43,797 [Epoch: 366 Step: 00024500] Batch Recognition Loss:   0.000604 => Gls Tokens per Sec:     1864 || Batch Translation Loss:   0.051763 => Txt Tokens per Sec:     5180 || Lr: 0.000100
2024-02-05 21:06:45,562 Epoch 366: Total Training Recognition Loss 0.12  Total Training Translation Loss 3.72 
2024-02-05 21:06:45,563 EPOCH 367
2024-02-05 21:06:50,885 Epoch 367: Total Training Recognition Loss 0.07  Total Training Translation Loss 4.23 
2024-02-05 21:06:50,885 EPOCH 368
2024-02-05 21:06:51,942 [Epoch: 368 Step: 00024600] Batch Recognition Loss:   0.000135 => Gls Tokens per Sec:     1667 || Batch Translation Loss:   0.029903 => Txt Tokens per Sec:     4834 || Lr: 0.000100
2024-02-05 21:06:56,437 Epoch 368: Total Training Recognition Loss 0.10  Total Training Translation Loss 3.79 
2024-02-05 21:06:56,438 EPOCH 369
2024-02-05 21:06:59,716 [Epoch: 369 Step: 00024700] Batch Recognition Loss:   0.001061 => Gls Tokens per Sec:     2149 || Batch Translation Loss:   0.029479 => Txt Tokens per Sec:     5951 || Lr: 0.000100
2024-02-05 21:07:01,561 Epoch 369: Total Training Recognition Loss 0.11  Total Training Translation Loss 5.44 
2024-02-05 21:07:01,561 EPOCH 370
2024-02-05 21:07:07,138 Epoch 370: Total Training Recognition Loss 0.15  Total Training Translation Loss 6.62 
2024-02-05 21:07:07,138 EPOCH 371
2024-02-05 21:07:07,894 [Epoch: 371 Step: 00024800] Batch Recognition Loss:   0.003997 => Gls Tokens per Sec:     2121 || Batch Translation Loss:   0.091610 => Txt Tokens per Sec:     5749 || Lr: 0.000100
2024-02-05 21:07:12,678 Epoch 371: Total Training Recognition Loss 0.10  Total Training Translation Loss 7.72 
2024-02-05 21:07:12,678 EPOCH 372
2024-02-05 21:07:15,940 [Epoch: 372 Step: 00024900] Batch Recognition Loss:   0.000481 => Gls Tokens per Sec:     2079 || Batch Translation Loss:   0.128935 => Txt Tokens per Sec:     5626 || Lr: 0.000100
2024-02-05 21:07:17,903 Epoch 372: Total Training Recognition Loss 0.11  Total Training Translation Loss 7.29 
2024-02-05 21:07:17,903 EPOCH 373
2024-02-05 21:07:23,327 Epoch 373: Total Training Recognition Loss 0.13  Total Training Translation Loss 9.42 
2024-02-05 21:07:23,327 EPOCH 374
2024-02-05 21:07:23,958 [Epoch: 374 Step: 00025000] Batch Recognition Loss:   0.011158 => Gls Tokens per Sec:     2283 || Batch Translation Loss:   0.091409 => Txt Tokens per Sec:     6322 || Lr: 0.000100
2024-02-05 21:07:28,496 Epoch 374: Total Training Recognition Loss 0.13  Total Training Translation Loss 9.40 
2024-02-05 21:07:28,497 EPOCH 375
2024-02-05 21:07:31,994 [Epoch: 375 Step: 00025100] Batch Recognition Loss:   0.004096 => Gls Tokens per Sec:     1922 || Batch Translation Loss:   0.064613 => Txt Tokens per Sec:     5359 || Lr: 0.000100
2024-02-05 21:07:34,009 Epoch 375: Total Training Recognition Loss 0.15  Total Training Translation Loss 7.73 
2024-02-05 21:07:34,010 EPOCH 376
2024-02-05 21:07:39,219 Epoch 376: Total Training Recognition Loss 0.08  Total Training Translation Loss 7.01 
2024-02-05 21:07:39,220 EPOCH 377
2024-02-05 21:07:39,798 [Epoch: 377 Step: 00025200] Batch Recognition Loss:   0.001546 => Gls Tokens per Sec:     2218 || Batch Translation Loss:   0.262683 => Txt Tokens per Sec:     6231 || Lr: 0.000100
2024-02-05 21:07:44,596 Epoch 377: Total Training Recognition Loss 0.12  Total Training Translation Loss 5.33 
2024-02-05 21:07:44,597 EPOCH 378
2024-02-05 21:07:48,004 [Epoch: 378 Step: 00025300] Batch Recognition Loss:   0.000679 => Gls Tokens per Sec:     1897 || Batch Translation Loss:   0.049530 => Txt Tokens per Sec:     5238 || Lr: 0.000100
2024-02-05 21:07:50,043 Epoch 378: Total Training Recognition Loss 0.11  Total Training Translation Loss 4.18 
2024-02-05 21:07:50,043 EPOCH 379
2024-02-05 21:07:55,563 Epoch 379: Total Training Recognition Loss 0.13  Total Training Translation Loss 4.39 
2024-02-05 21:07:55,564 EPOCH 380
2024-02-05 21:07:56,135 [Epoch: 380 Step: 00025400] Batch Recognition Loss:   0.003421 => Gls Tokens per Sec:     1965 || Batch Translation Loss:   0.036283 => Txt Tokens per Sec:     5760 || Lr: 0.000100
2024-02-05 21:08:00,852 Epoch 380: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.74 
2024-02-05 21:08:00,853 EPOCH 381
2024-02-05 21:08:03,503 [Epoch: 381 Step: 00025500] Batch Recognition Loss:   0.000802 => Gls Tokens per Sec:     2416 || Batch Translation Loss:   0.042791 => Txt Tokens per Sec:     6554 || Lr: 0.000100
2024-02-05 21:08:05,762 Epoch 381: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.20 
2024-02-05 21:08:05,763 EPOCH 382
2024-02-05 21:08:11,137 Epoch 382: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.55 
2024-02-05 21:08:11,137 EPOCH 383
2024-02-05 21:08:11,443 [Epoch: 383 Step: 00025600] Batch Recognition Loss:   0.000190 => Gls Tokens per Sec:     3143 || Batch Translation Loss:   0.015241 => Txt Tokens per Sec:     6570 || Lr: 0.000100
2024-02-05 21:08:16,248 Epoch 383: Total Training Recognition Loss 0.10  Total Training Translation Loss 6.27 
2024-02-05 21:08:16,249 EPOCH 384
2024-02-05 21:08:19,260 [Epoch: 384 Step: 00025700] Batch Recognition Loss:   0.000604 => Gls Tokens per Sec:     2073 || Batch Translation Loss:   0.279039 => Txt Tokens per Sec:     5585 || Lr: 0.000100
2024-02-05 21:08:21,395 Epoch 384: Total Training Recognition Loss 0.09  Total Training Translation Loss 8.48 
2024-02-05 21:08:21,395 EPOCH 385
2024-02-05 21:08:26,650 Epoch 385: Total Training Recognition Loss 0.15  Total Training Translation Loss 8.42 
2024-02-05 21:08:26,650 EPOCH 386
2024-02-05 21:08:27,080 [Epoch: 386 Step: 00025800] Batch Recognition Loss:   0.000238 => Gls Tokens per Sec:     1865 || Batch Translation Loss:   0.048424 => Txt Tokens per Sec:     5228 || Lr: 0.000100
2024-02-05 21:08:31,643 Epoch 386: Total Training Recognition Loss 0.20  Total Training Translation Loss 10.57 
2024-02-05 21:08:31,643 EPOCH 387
2024-02-05 21:08:34,819 [Epoch: 387 Step: 00025900] Batch Recognition Loss:   0.008423 => Gls Tokens per Sec:     1915 || Batch Translation Loss:   0.108955 => Txt Tokens per Sec:     5469 || Lr: 0.000100
2024-02-05 21:08:37,061 Epoch 387: Total Training Recognition Loss 0.17  Total Training Translation Loss 7.42 
2024-02-05 21:08:37,061 EPOCH 388
2024-02-05 21:08:42,190 Epoch 388: Total Training Recognition Loss 0.10  Total Training Translation Loss 8.50 
2024-02-05 21:08:42,190 EPOCH 389
2024-02-05 21:08:42,483 [Epoch: 389 Step: 00026000] Batch Recognition Loss:   0.000823 => Gls Tokens per Sec:     2192 || Batch Translation Loss:   0.029018 => Txt Tokens per Sec:     6161 || Lr: 0.000100
2024-02-05 21:08:50,893 Validation result at epoch 389, step    26000: duration: 8.4087s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 3.43288	Translation Loss: 92656.03125	PPL: 10451.46582
	Eval Metric: BLEU
	WER 4.24	(DEL: 0.00,	INS: 0.00,	SUB: 4.24)
	BLEU-4 0.52	(BLEU-1: 10.27,	BLEU-2: 2.88,	BLEU-3: 1.08,	BLEU-4: 0.52)
	CHRF 16.75	ROUGE 8.84
2024-02-05 21:08:50,894 Logging Recognition and Translation Outputs
2024-02-05 21:08:50,894 ========================================================================================================================
2024-02-05 21:08:50,894 Logging Sequence: 166_243.00
2024-02-05 21:08:50,895 	Gloss Reference :	A B+C+D+E
2024-02-05 21:08:50,895 	Gloss Hypothesis:	A B+C+D  
2024-02-05 21:08:50,895 	Gloss Alignment :	  S      
2024-02-05 21:08:50,895 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:08:50,896 	Text Reference  :	*** ********* *********** ********* *** ***** ** icc     worked with members boards like bcci pcb   cricket australia etc 
2024-02-05 21:08:50,897 	Text Hypothesis :	the broadcast advertisers ticketing etc would be decided by     the  board   of     the  2    teams playing the       test
2024-02-05 21:08:50,897 	Text Alignment  :	I   I         I           I         I   I     I  S       S      S    S       S      S    S    S     S       S         S   
2024-02-05 21:08:50,897 ========================================================================================================================
2024-02-05 21:08:50,897 Logging Sequence: 59_152.00
2024-02-05 21:08:50,897 	Gloss Reference :	A B+C+D+E
2024-02-05 21:08:50,897 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:08:50,897 	Gloss Alignment :	         
2024-02-05 21:08:50,898 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:08:50,899 	Text Reference  :	*** the organisers encouraged athletes to    use the    condoms in        their home countries
2024-02-05 21:08:50,899 	Text Hypothesis :	for the olympics   are        2        years and people are     scheduled on    23rd may      
2024-02-05 21:08:50,899 	Text Alignment  :	I       S          S          S        S     S   S      S       S         S     S    S        
2024-02-05 21:08:50,899 ========================================================================================================================
2024-02-05 21:08:50,899 Logging Sequence: 145_52.00
2024-02-05 21:08:50,899 	Gloss Reference :	A B+C+D+E
2024-02-05 21:08:50,899 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:08:50,900 	Gloss Alignment :	         
2024-02-05 21:08:50,900 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:08:50,900 	Text Reference  :	her name was dropped despite having qualified as she was   the only female               athlete
2024-02-05 21:08:50,900 	Text Hypothesis :	*** **** *** ******* ******* ****** ********* ** *** dhoni is  a    once-in-a-generation player 
2024-02-05 21:08:50,900 	Text Alignment  :	D   D    D   D       D       D      D         D  D   S     S   S    S                    S      
2024-02-05 21:08:50,901 ========================================================================================================================
2024-02-05 21:08:50,901 Logging Sequence: 172_163.00
2024-02-05 21:08:50,901 	Gloss Reference :	A B+C+D+E
2024-02-05 21:08:50,901 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:08:50,901 	Gloss Alignment :	         
2024-02-05 21:08:50,901 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:08:50,903 	Text Reference  :	** if  the match starts      anywhere between     730       pm  to   935  pm  a    full 20-over match can    be  played
2024-02-05 21:08:50,903 	Text Hypothesis :	as you are many  dignitaries indian   celebrities filmstars etc have been not stop her  probe   and   submit the report
2024-02-05 21:08:50,903 	Text Alignment  :	I  S   S   S     S           S        S           S         S   S    S    S   S    S    S       S     S      S   S     
2024-02-05 21:08:50,904 ========================================================================================================================
2024-02-05 21:08:50,904 Logging Sequence: 150_20.00
2024-02-05 21:08:50,904 	Gloss Reference :	A B+C+D+E
2024-02-05 21:08:50,904 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:08:50,904 	Gloss Alignment :	         
2024-02-05 21:08:50,904 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:08:50,905 	Text Reference  :	after *** ******** ** *** a      tough   match    india  won the saff  championship 2023 title
2024-02-05 21:08:50,905 	Text Hypothesis :	after his accident he was played between pakistan people are 4   years and          3    days 
2024-02-05 21:08:50,906 	Text Alignment  :	      I   I        I  I   S      S       S        S      S   S   S     S            S    S    
2024-02-05 21:08:50,906 ========================================================================================================================
2024-02-05 21:08:56,252 Epoch 389: Total Training Recognition Loss 0.11  Total Training Translation Loss 11.52 
2024-02-05 21:08:56,253 EPOCH 390
2024-02-05 21:08:58,809 [Epoch: 390 Step: 00026100] Batch Recognition Loss:   0.000528 => Gls Tokens per Sec:     2278 || Batch Translation Loss:   0.177018 => Txt Tokens per Sec:     6013 || Lr: 0.000100
2024-02-05 21:09:01,448 Epoch 390: Total Training Recognition Loss 0.18  Total Training Translation Loss 8.66 
2024-02-05 21:09:01,449 EPOCH 391
2024-02-05 21:09:06,824 Epoch 391: Total Training Recognition Loss 0.13  Total Training Translation Loss 9.45 
2024-02-05 21:09:06,824 EPOCH 392
2024-02-05 21:09:07,033 [Epoch: 392 Step: 00026200] Batch Recognition Loss:   0.001154 => Gls Tokens per Sec:     2319 || Batch Translation Loss:   0.104471 => Txt Tokens per Sec:     6894 || Lr: 0.000100
2024-02-05 21:09:12,220 Epoch 392: Total Training Recognition Loss 0.19  Total Training Translation Loss 6.56 
2024-02-05 21:09:12,221 EPOCH 393
2024-02-05 21:09:15,210 [Epoch: 393 Step: 00026300] Batch Recognition Loss:   0.001919 => Gls Tokens per Sec:     1895 || Batch Translation Loss:   0.052363 => Txt Tokens per Sec:     5302 || Lr: 0.000100
2024-02-05 21:09:17,503 Epoch 393: Total Training Recognition Loss 0.11  Total Training Translation Loss 4.37 
2024-02-05 21:09:17,503 EPOCH 394
2024-02-05 21:09:22,981 Epoch 394: Total Training Recognition Loss 0.11  Total Training Translation Loss 4.11 
2024-02-05 21:09:22,982 EPOCH 395
2024-02-05 21:09:23,163 [Epoch: 395 Step: 00026400] Batch Recognition Loss:   0.000277 => Gls Tokens per Sec:     1788 || Batch Translation Loss:   0.021139 => Txt Tokens per Sec:     4983 || Lr: 0.000100
2024-02-05 21:09:28,156 Epoch 395: Total Training Recognition Loss 0.16  Total Training Translation Loss 4.38 
2024-02-05 21:09:28,157 EPOCH 396
2024-02-05 21:09:31,088 [Epoch: 396 Step: 00026500] Batch Recognition Loss:   0.001252 => Gls Tokens per Sec:     1877 || Batch Translation Loss:   0.034439 => Txt Tokens per Sec:     5244 || Lr: 0.000100
2024-02-05 21:09:33,716 Epoch 396: Total Training Recognition Loss 0.10  Total Training Translation Loss 2.93 
2024-02-05 21:09:33,717 EPOCH 397
2024-02-05 21:09:38,785 Epoch 397: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.98 
2024-02-05 21:09:38,785 EPOCH 398
2024-02-05 21:09:38,854 [Epoch: 398 Step: 00026600] Batch Recognition Loss:   0.000459 => Gls Tokens per Sec:     2353 || Batch Translation Loss:   0.019588 => Txt Tokens per Sec:     6353 || Lr: 0.000100
2024-02-05 21:09:43,915 Epoch 398: Total Training Recognition Loss 0.08  Total Training Translation Loss 6.49 
2024-02-05 21:09:43,915 EPOCH 399
2024-02-05 21:09:46,359 [Epoch: 399 Step: 00026700] Batch Recognition Loss:   0.000248 => Gls Tokens per Sec:     2187 || Batch Translation Loss:   0.054350 => Txt Tokens per Sec:     5848 || Lr: 0.000100
2024-02-05 21:09:49,044 Epoch 399: Total Training Recognition Loss 0.12  Total Training Translation Loss 6.22 
2024-02-05 21:09:49,045 EPOCH 400
2024-02-05 21:09:54,455 [Epoch: 400 Step: 00026800] Batch Recognition Loss:   0.003312 => Gls Tokens per Sec:     1963 || Batch Translation Loss:   0.104618 => Txt Tokens per Sec:     5432 || Lr: 0.000100
2024-02-05 21:09:54,455 Epoch 400: Total Training Recognition Loss 0.12  Total Training Translation Loss 8.25 
2024-02-05 21:09:54,456 EPOCH 401
2024-02-05 21:09:59,344 Epoch 401: Total Training Recognition Loss 0.21  Total Training Translation Loss 12.95 
2024-02-05 21:09:59,345 EPOCH 402
2024-02-05 21:10:01,998 [Epoch: 402 Step: 00026900] Batch Recognition Loss:   0.000704 => Gls Tokens per Sec:     1990 || Batch Translation Loss:   0.040534 => Txt Tokens per Sec:     5269 || Lr: 0.000100
2024-02-05 21:10:04,915 Epoch 402: Total Training Recognition Loss 0.25  Total Training Translation Loss 10.06 
2024-02-05 21:10:04,915 EPOCH 403
2024-02-05 21:10:10,215 [Epoch: 403 Step: 00027000] Batch Recognition Loss:   0.001526 => Gls Tokens per Sec:     1974 || Batch Translation Loss:   0.069666 => Txt Tokens per Sec:     5471 || Lr: 0.000100
2024-02-05 21:10:10,271 Epoch 403: Total Training Recognition Loss 0.17  Total Training Translation Loss 6.31 
2024-02-05 21:10:10,271 EPOCH 404
2024-02-05 21:10:15,645 Epoch 404: Total Training Recognition Loss 0.15  Total Training Translation Loss 3.71 
2024-02-05 21:10:15,645 EPOCH 405
2024-02-05 21:10:17,840 [Epoch: 405 Step: 00027100] Batch Recognition Loss:   0.000316 => Gls Tokens per Sec:     2333 || Batch Translation Loss:   0.025373 => Txt Tokens per Sec:     6219 || Lr: 0.000100
2024-02-05 21:10:20,644 Epoch 405: Total Training Recognition Loss 0.08  Total Training Translation Loss 4.07 
2024-02-05 21:10:20,645 EPOCH 406
2024-02-05 21:10:26,155 [Epoch: 406 Step: 00027200] Batch Recognition Loss:   0.001490 => Gls Tokens per Sec:     1870 || Batch Translation Loss:   0.128342 => Txt Tokens per Sec:     5209 || Lr: 0.000100
2024-02-05 21:10:26,248 Epoch 406: Total Training Recognition Loss 0.13  Total Training Translation Loss 5.90 
2024-02-05 21:10:26,248 EPOCH 407
2024-02-05 21:10:30,993 Epoch 407: Total Training Recognition Loss 0.08  Total Training Translation Loss 5.83 
2024-02-05 21:10:30,994 EPOCH 408
2024-02-05 21:10:33,507 [Epoch: 408 Step: 00027300] Batch Recognition Loss:   0.000690 => Gls Tokens per Sec:     1975 || Batch Translation Loss:   0.060111 => Txt Tokens per Sec:     5269 || Lr: 0.000100
2024-02-05 21:10:36,409 Epoch 408: Total Training Recognition Loss 0.13  Total Training Translation Loss 4.23 
2024-02-05 21:10:36,409 EPOCH 409
2024-02-05 21:10:40,995 [Epoch: 409 Step: 00027400] Batch Recognition Loss:   0.000325 => Gls Tokens per Sec:     2212 || Batch Translation Loss:   0.025594 => Txt Tokens per Sec:     6109 || Lr: 0.000100
2024-02-05 21:10:41,241 Epoch 409: Total Training Recognition Loss 0.09  Total Training Translation Loss 3.92 
2024-02-05 21:10:41,242 EPOCH 410
2024-02-05 21:10:46,615 Epoch 410: Total Training Recognition Loss 0.10  Total Training Translation Loss 4.26 
2024-02-05 21:10:46,616 EPOCH 411
2024-02-05 21:10:48,499 [Epoch: 411 Step: 00027500] Batch Recognition Loss:   0.001035 => Gls Tokens per Sec:     2549 || Batch Translation Loss:   0.071814 => Txt Tokens per Sec:     6713 || Lr: 0.000100
2024-02-05 21:10:51,516 Epoch 411: Total Training Recognition Loss 0.07  Total Training Translation Loss 6.18 
2024-02-05 21:10:51,517 EPOCH 412
2024-02-05 21:10:56,629 [Epoch: 412 Step: 00027600] Batch Recognition Loss:   0.000258 => Gls Tokens per Sec:     1953 || Batch Translation Loss:   0.060737 => Txt Tokens per Sec:     5444 || Lr: 0.000100
2024-02-05 21:10:56,855 Epoch 412: Total Training Recognition Loss 0.05  Total Training Translation Loss 5.85 
2024-02-05 21:10:56,856 EPOCH 413
2024-02-05 21:11:01,895 Epoch 413: Total Training Recognition Loss 0.10  Total Training Translation Loss 7.01 
2024-02-05 21:11:01,895 EPOCH 414
2024-02-05 21:11:04,279 [Epoch: 414 Step: 00027700] Batch Recognition Loss:   0.000327 => Gls Tokens per Sec:     1948 || Batch Translation Loss:   0.042286 => Txt Tokens per Sec:     5333 || Lr: 0.000100
2024-02-05 21:11:07,326 Epoch 414: Total Training Recognition Loss 0.18  Total Training Translation Loss 9.01 
2024-02-05 21:11:07,327 EPOCH 415
2024-02-05 21:11:12,228 [Epoch: 415 Step: 00027800] Batch Recognition Loss:   0.018486 => Gls Tokens per Sec:     2004 || Batch Translation Loss:   0.074700 => Txt Tokens per Sec:     5516 || Lr: 0.000100
2024-02-05 21:11:12,666 Epoch 415: Total Training Recognition Loss 0.17  Total Training Translation Loss 9.11 
2024-02-05 21:11:12,667 EPOCH 416
2024-02-05 21:11:17,740 Epoch 416: Total Training Recognition Loss 0.21  Total Training Translation Loss 7.89 
2024-02-05 21:11:17,740 EPOCH 417
2024-02-05 21:11:19,476 [Epoch: 417 Step: 00027900] Batch Recognition Loss:   0.000304 => Gls Tokens per Sec:     2582 || Batch Translation Loss:   0.070051 => Txt Tokens per Sec:     7084 || Lr: 0.000100
2024-02-05 21:11:22,848 Epoch 417: Total Training Recognition Loss 0.17  Total Training Translation Loss 7.18 
2024-02-05 21:11:22,849 EPOCH 418
2024-02-05 21:11:27,535 [Epoch: 418 Step: 00028000] Batch Recognition Loss:   0.000419 => Gls Tokens per Sec:     2083 || Batch Translation Loss:   0.034236 => Txt Tokens per Sec:     5761 || Lr: 0.000100
2024-02-05 21:11:35,943 Validation result at epoch 418, step    28000: duration: 8.4077s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 3.61357	Translation Loss: 91873.75000	PPL: 9665.93359
	Eval Metric: BLEU
	WER 3.88	(DEL: 0.00,	INS: 0.00,	SUB: 3.88)
	BLEU-4 0.54	(BLEU-1: 10.92,	BLEU-2: 3.35,	BLEU-3: 1.26,	BLEU-4: 0.54)
	CHRF 17.36	ROUGE 9.17
2024-02-05 21:11:35,944 Logging Recognition and Translation Outputs
2024-02-05 21:11:35,944 ========================================================================================================================
2024-02-05 21:11:35,945 Logging Sequence: 156_288.00
2024-02-05 21:11:35,945 	Gloss Reference :	A B+C+D+E
2024-02-05 21:11:35,945 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:11:35,945 	Gloss Alignment :	         
2024-02-05 21:11:35,945 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:11:35,946 	Text Reference  :	pooran led the team to victory miny   became winners of          the **** 1st    season  
2024-02-05 21:11:35,946 	Text Hypothesis :	****** *** *** **** ** then    people have   seen    celebrating the most famous wrestler
2024-02-05 21:11:35,946 	Text Alignment  :	D      D   D   D    D  S       S      S      S       S               I    S      S       
2024-02-05 21:11:35,946 ========================================================================================================================
2024-02-05 21:11:35,947 Logging Sequence: 98_135.00
2024-02-05 21:11:35,947 	Gloss Reference :	A B+C+D+E
2024-02-05 21:11:35,947 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:11:35,947 	Gloss Alignment :	         
2024-02-05 21:11:35,947 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:11:35,948 	Text Reference  :	*** however due      to *** the rise in coronavirus cases the tournament was shifted
2024-02-05 21:11:35,948 	Text Hypothesis :	and is      expected to tie the **** ** number      of    the tournament *** *******
2024-02-05 21:11:35,948 	Text Alignment  :	I   S       S           I       D    D  S           S                    D   D      
2024-02-05 21:11:35,948 ========================================================================================================================
2024-02-05 21:11:35,948 Logging Sequence: 161_47.00
2024-02-05 21:11:35,949 	Gloss Reference :	A B+C+D+E
2024-02-05 21:11:35,949 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:11:35,949 	Gloss Alignment :	         
2024-02-05 21:11:35,949 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:11:35,950 	Text Reference  :	he requested confidentiality as he was planning to  make an  official announcement
2024-02-05 21:11:35,950 	Text Hypothesis :	** ********* *************** ** ** we  were     all out  for the      series      
2024-02-05 21:11:35,950 	Text Alignment  :	D  D         D               D  D  S   S        S   S    S   S        S           
2024-02-05 21:11:35,950 ========================================================================================================================
2024-02-05 21:11:35,950 Logging Sequence: 131_159.00
2024-02-05 21:11:35,950 	Gloss Reference :	A B+C+D+E  
2024-02-05 21:11:35,950 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-05 21:11:35,950 	Gloss Alignment :	  S        
2024-02-05 21:11:35,951 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:11:35,952 	Text Reference  :	****** chanu    also   met    biren singh   following the meeting singh described chanu as our nation' pride      
2024-02-05 21:11:35,952 	Text Hypothesis :	sports minister anurag thakur also  present at        the ******* ***** ********* ***** ** *** ******* interaction
2024-02-05 21:11:35,952 	Text Alignment  :	I      S        S      S      S     S       S             D       D     D         D     D  D   D       S          
2024-02-05 21:11:35,952 ========================================================================================================================
2024-02-05 21:11:35,952 Logging Sequence: 137_167.00
2024-02-05 21:11:35,952 	Gloss Reference :	A B+C+D+E
2024-02-05 21:11:35,952 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:11:35,953 	Gloss Alignment :	         
2024-02-05 21:11:35,953 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:11:35,954 	Text Reference  :	however after 630 pm there will be     certain fan  zones        where beer will    be        available     and *** nowhere else 
2024-02-05 21:11:35,954 	Text Hypothesis :	******* ***** *** ** ***** the  mumbai indians team participated in    the  winning athletics championships and way to      catch
2024-02-05 21:11:35,954 	Text Alignment  :	D       D     D   D  D     S    S      S       S    S            S     S    S       S         S                 I   S       S    
2024-02-05 21:11:35,955 ========================================================================================================================
2024-02-05 21:11:36,438 Epoch 418: Total Training Recognition Loss 0.48  Total Training Translation Loss 5.09 
2024-02-05 21:11:36,439 EPOCH 419
2024-02-05 21:11:43,191 Epoch 419: Total Training Recognition Loss 0.78  Total Training Translation Loss 3.72 
2024-02-05 21:11:43,192 EPOCH 420
2024-02-05 21:11:45,325 [Epoch: 420 Step: 00028100] Batch Recognition Loss:   0.000673 => Gls Tokens per Sec:     2027 || Batch Translation Loss:   0.029171 => Txt Tokens per Sec:     5636 || Lr: 0.000050
2024-02-05 21:11:48,159 Epoch 420: Total Training Recognition Loss 0.33  Total Training Translation Loss 1.77 
2024-02-05 21:11:48,159 EPOCH 421
2024-02-05 21:11:52,843 [Epoch: 421 Step: 00028200] Batch Recognition Loss:   0.000615 => Gls Tokens per Sec:     2029 || Batch Translation Loss:   0.015274 => Txt Tokens per Sec:     5575 || Lr: 0.000050
2024-02-05 21:11:53,538 Epoch 421: Total Training Recognition Loss 0.19  Total Training Translation Loss 1.42 
2024-02-05 21:11:53,539 EPOCH 422
2024-02-05 21:11:58,943 Epoch 422: Total Training Recognition Loss 0.12  Total Training Translation Loss 1.33 
2024-02-05 21:11:58,944 EPOCH 423
2024-02-05 21:12:01,035 [Epoch: 423 Step: 00028300] Batch Recognition Loss:   0.000600 => Gls Tokens per Sec:     1942 || Batch Translation Loss:   0.016084 => Txt Tokens per Sec:     5213 || Lr: 0.000050
2024-02-05 21:12:04,274 Epoch 423: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.22 
2024-02-05 21:12:04,274 EPOCH 424
2024-02-05 21:12:09,045 [Epoch: 424 Step: 00028400] Batch Recognition Loss:   0.000245 => Gls Tokens per Sec:     1958 || Batch Translation Loss:   0.012539 => Txt Tokens per Sec:     5444 || Lr: 0.000050
2024-02-05 21:12:09,652 Epoch 424: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.89 
2024-02-05 21:12:09,652 EPOCH 425
2024-02-05 21:12:14,769 Epoch 425: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.97 
2024-02-05 21:12:14,770 EPOCH 426
2024-02-05 21:12:17,001 [Epoch: 426 Step: 00028500] Batch Recognition Loss:   0.002586 => Gls Tokens per Sec:     1749 || Batch Translation Loss:   0.017666 => Txt Tokens per Sec:     5018 || Lr: 0.000050
2024-02-05 21:12:20,280 Epoch 426: Total Training Recognition Loss 0.12  Total Training Translation Loss 2.71 
2024-02-05 21:12:20,281 EPOCH 427
2024-02-05 21:12:24,632 [Epoch: 427 Step: 00028600] Batch Recognition Loss:   0.000771 => Gls Tokens per Sec:     2111 || Batch Translation Loss:   0.014641 => Txt Tokens per Sec:     5847 || Lr: 0.000050
2024-02-05 21:12:25,260 Epoch 427: Total Training Recognition Loss 0.10  Total Training Translation Loss 1.30 
2024-02-05 21:12:25,260 EPOCH 428
2024-02-05 21:12:30,396 Epoch 428: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.21 
2024-02-05 21:12:30,396 EPOCH 429
2024-02-05 21:12:32,255 [Epoch: 429 Step: 00028700] Batch Recognition Loss:   0.000301 => Gls Tokens per Sec:     2068 || Batch Translation Loss:   0.070642 => Txt Tokens per Sec:     5690 || Lr: 0.000050
2024-02-05 21:12:35,608 Epoch 429: Total Training Recognition Loss 0.09  Total Training Translation Loss 1.29 
2024-02-05 21:12:35,608 EPOCH 430
2024-02-05 21:12:39,993 [Epoch: 430 Step: 00028800] Batch Recognition Loss:   0.000122 => Gls Tokens per Sec:     2057 || Batch Translation Loss:   0.014043 => Txt Tokens per Sec:     5607 || Lr: 0.000050
2024-02-05 21:12:40,840 Epoch 430: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.11 
2024-02-05 21:12:40,840 EPOCH 431
2024-02-05 21:12:46,163 Epoch 431: Total Training Recognition Loss 0.06  Total Training Translation Loss 0.94 
2024-02-05 21:12:46,164 EPOCH 432
2024-02-05 21:12:48,066 [Epoch: 432 Step: 00028900] Batch Recognition Loss:   0.000237 => Gls Tokens per Sec:     1883 || Batch Translation Loss:   0.013721 => Txt Tokens per Sec:     5023 || Lr: 0.000050
2024-02-05 21:12:51,591 Epoch 432: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.02 
2024-02-05 21:12:51,592 EPOCH 433
2024-02-05 21:12:55,945 [Epoch: 433 Step: 00029000] Batch Recognition Loss:   0.000469 => Gls Tokens per Sec:     2058 || Batch Translation Loss:   0.005285 => Txt Tokens per Sec:     5679 || Lr: 0.000050
2024-02-05 21:12:56,948 Epoch 433: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.87 
2024-02-05 21:12:56,949 EPOCH 434
2024-02-05 21:13:02,108 Epoch 434: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.03 
2024-02-05 21:13:02,108 EPOCH 435
2024-02-05 21:13:03,568 [Epoch: 435 Step: 00029100] Batch Recognition Loss:   0.000199 => Gls Tokens per Sec:     2413 || Batch Translation Loss:   0.013221 => Txt Tokens per Sec:     6655 || Lr: 0.000050
2024-02-05 21:13:06,916 Epoch 435: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.92 
2024-02-05 21:13:06,916 EPOCH 436
2024-02-05 21:13:11,189 [Epoch: 436 Step: 00029200] Batch Recognition Loss:   0.000146 => Gls Tokens per Sec:     2037 || Batch Translation Loss:   0.015980 => Txt Tokens per Sec:     5506 || Lr: 0.000050
2024-02-05 21:13:12,291 Epoch 436: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.01 
2024-02-05 21:13:12,291 EPOCH 437
2024-02-05 21:13:16,906 Epoch 437: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.73 
2024-02-05 21:13:16,907 EPOCH 438
2024-02-05 21:13:18,211 [Epoch: 438 Step: 00029300] Batch Recognition Loss:   0.000200 => Gls Tokens per Sec:     2576 || Batch Translation Loss:   0.014384 => Txt Tokens per Sec:     7007 || Lr: 0.000050
2024-02-05 21:13:21,427 Epoch 438: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.64 
2024-02-05 21:13:21,427 EPOCH 439
2024-02-05 21:13:25,088 [Epoch: 439 Step: 00029400] Batch Recognition Loss:   0.000208 => Gls Tokens per Sec:     2360 || Batch Translation Loss:   0.014265 => Txt Tokens per Sec:     6483 || Lr: 0.000050
2024-02-05 21:13:25,988 Epoch 439: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.47 
2024-02-05 21:13:25,988 EPOCH 440
2024-02-05 21:13:31,518 Epoch 440: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.85 
2024-02-05 21:13:31,519 EPOCH 441
2024-02-05 21:13:32,785 [Epoch: 441 Step: 00029500] Batch Recognition Loss:   0.000142 => Gls Tokens per Sec:     2530 || Batch Translation Loss:   0.012447 => Txt Tokens per Sec:     6823 || Lr: 0.000050
2024-02-05 21:13:36,405 Epoch 441: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.91 
2024-02-05 21:13:36,405 EPOCH 442
2024-02-05 21:13:40,821 [Epoch: 442 Step: 00029600] Batch Recognition Loss:   0.000205 => Gls Tokens per Sec:     1921 || Batch Translation Loss:   0.015987 => Txt Tokens per Sec:     5376 || Lr: 0.000050
2024-02-05 21:13:41,814 Epoch 442: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.33 
2024-02-05 21:13:41,814 EPOCH 443
2024-02-05 21:13:46,605 Epoch 443: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.73 
2024-02-05 21:13:46,605 EPOCH 444
2024-02-05 21:13:47,881 [Epoch: 444 Step: 00029700] Batch Recognition Loss:   0.000294 => Gls Tokens per Sec:     2385 || Batch Translation Loss:   0.014218 => Txt Tokens per Sec:     6344 || Lr: 0.000050
2024-02-05 21:13:51,552 Epoch 444: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.84 
2024-02-05 21:13:51,552 EPOCH 445
2024-02-05 21:13:55,933 [Epoch: 445 Step: 00029800] Batch Recognition Loss:   0.000170 => Gls Tokens per Sec:     1877 || Batch Translation Loss:   0.022777 => Txt Tokens per Sec:     5210 || Lr: 0.000050
2024-02-05 21:13:56,990 Epoch 445: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.29 
2024-02-05 21:13:56,990 EPOCH 446
2024-02-05 21:14:01,923 Epoch 446: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.18 
2024-02-05 21:14:01,923 EPOCH 447
2024-02-05 21:14:03,687 [Epoch: 447 Step: 00029900] Batch Recognition Loss:   0.000268 => Gls Tokens per Sec:     1578 || Batch Translation Loss:   0.010116 => Txt Tokens per Sec:     4687 || Lr: 0.000050
2024-02-05 21:14:07,175 Epoch 447: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.30 
2024-02-05 21:14:07,175 EPOCH 448
2024-02-05 21:14:10,872 [Epoch: 448 Step: 00030000] Batch Recognition Loss:   0.000251 => Gls Tokens per Sec:     2181 || Batch Translation Loss:   0.114374 => Txt Tokens per Sec:     6103 || Lr: 0.000050
2024-02-05 21:14:19,241 Validation result at epoch 448, step    30000: duration: 8.3677s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 3.44275	Translation Loss: 92996.65625	PPL: 10813.15625
	Eval Metric: BLEU
	WER 4.52	(DEL: 0.00,	INS: 0.00,	SUB: 4.52)
	BLEU-4 0.81	(BLEU-1: 10.36,	BLEU-2: 3.55,	BLEU-3: 1.55,	BLEU-4: 0.81)
	CHRF 16.68	ROUGE 9.20
2024-02-05 21:14:19,242 Logging Recognition and Translation Outputs
2024-02-05 21:14:19,242 ========================================================================================================================
2024-02-05 21:14:19,242 Logging Sequence: 146_102.00
2024-02-05 21:14:19,243 	Gloss Reference :	A B+C+D+E
2024-02-05 21:14:19,243 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:14:19,243 	Gloss Alignment :	         
2024-02-05 21:14:19,243 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:14:19,244 	Text Reference  :	famous indian champion players like kidambi srikanth and ashwini ponappa have tested     positive for    coronavirus
2024-02-05 21:14:19,244 	Text Hypothesis :	****** ****** ******** ******* **** ******* ******** *** ******* usman   is   australia' first    muslim player     
2024-02-05 21:14:19,244 	Text Alignment  :	D      D      D        D       D    D       D        D   D       S       S    S          S        S      S          
2024-02-05 21:14:19,244 ========================================================================================================================
2024-02-05 21:14:19,244 Logging Sequence: 53_178.00
2024-02-05 21:14:19,245 	Gloss Reference :	A B+C+D+E
2024-02-05 21:14:19,245 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:14:19,245 	Gloss Alignment :	         
2024-02-05 21:14:19,245 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:14:19,246 	Text Reference  :	the money   would help  all   those affected by the humanitarian crisis in afghanistan
2024-02-05 21:14:19,246 	Text Hypothesis :	we  request the   other squad for   privacy  at the entire       family in afghanistan
2024-02-05 21:14:19,246 	Text Alignment  :	S   S       S     S     S     S     S        S      S            S                    
2024-02-05 21:14:19,246 ========================================================================================================================
2024-02-05 21:14:19,246 Logging Sequence: 129_200.00
2024-02-05 21:14:19,247 	Gloss Reference :	A B+C+D+E  
2024-02-05 21:14:19,247 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-05 21:14:19,247 	Gloss Alignment :	  S        
2024-02-05 21:14:19,247 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:14:19,248 	Text Reference  :	**** the ioc would lose  about 4   billion if  the olympics were to     be  cancelled
2024-02-05 21:14:19,248 	Text Hypothesis :	this is  how fox   could use   the condoms and her unique   idea helped her win      
2024-02-05 21:14:19,249 	Text Alignment  :	I    S   S   S     S     S     S   S       S   S   S        S    S      S   S        
2024-02-05 21:14:19,249 ========================================================================================================================
2024-02-05 21:14:19,249 Logging Sequence: 77_2.00
2024-02-05 21:14:19,249 	Gloss Reference :	A B+C+D+E  
2024-02-05 21:14:19,249 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-05 21:14:19,249 	Gloss Alignment :	  S        
2024-02-05 21:14:19,249 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:14:19,250 	Text Reference  :	on 25th april the    ipl match between sunrisers hyderabad and delhi capitals ended in a tie    
2024-02-05 21:14:19,250 	Text Hypothesis :	** **** he    played 3   out   of      the       event     and ***** ******** ***** ** * england
2024-02-05 21:14:19,251 	Text Alignment  :	D  D    S     S      S   S     S       S         S             D     D        D     D  D S      
2024-02-05 21:14:19,251 ========================================================================================================================
2024-02-05 21:14:19,251 Logging Sequence: 119_170.00
2024-02-05 21:14:19,251 	Gloss Reference :	A B+C+D+E
2024-02-05 21:14:19,251 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:14:19,251 	Gloss Alignment :	         
2024-02-05 21:14:19,251 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:14:19,252 	Text Reference  :	they said it was a        proud moment messi is a   big hearted  man 
2024-02-05 21:14:19,252 	Text Hypothesis :	**** **** ** *** whenever she   gave   birth to win an  official here
2024-02-05 21:14:19,252 	Text Alignment  :	D    D    D  D   S        S     S      S     S  S   S   S        S   
2024-02-05 21:14:19,252 ========================================================================================================================
2024-02-05 21:14:20,665 Epoch 448: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.60 
2024-02-05 21:14:20,666 EPOCH 449
2024-02-05 21:14:26,170 Epoch 449: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.03 
2024-02-05 21:14:26,171 EPOCH 450
2024-02-05 21:14:27,436 [Epoch: 450 Step: 00030100] Batch Recognition Loss:   0.000659 => Gls Tokens per Sec:     2152 || Batch Translation Loss:   0.045908 => Txt Tokens per Sec:     6086 || Lr: 0.000050
2024-02-05 21:14:31,266 Epoch 450: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.44 
2024-02-05 21:14:31,267 EPOCH 451
2024-02-05 21:14:35,333 [Epoch: 451 Step: 00030200] Batch Recognition Loss:   0.000316 => Gls Tokens per Sec:     1943 || Batch Translation Loss:   0.025356 => Txt Tokens per Sec:     5521 || Lr: 0.000050
2024-02-05 21:14:36,611 Epoch 451: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.65 
2024-02-05 21:14:36,612 EPOCH 452
2024-02-05 21:14:42,102 Epoch 452: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.04 
2024-02-05 21:14:42,103 EPOCH 453
2024-02-05 21:14:43,377 [Epoch: 453 Step: 00030300] Batch Recognition Loss:   0.000740 => Gls Tokens per Sec:     1933 || Batch Translation Loss:   0.013439 => Txt Tokens per Sec:     5312 || Lr: 0.000050
2024-02-05 21:14:47,049 Epoch 453: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.66 
2024-02-05 21:14:47,050 EPOCH 454
2024-02-05 21:14:50,694 [Epoch: 454 Step: 00030400] Batch Recognition Loss:   0.000217 => Gls Tokens per Sec:     2124 || Batch Translation Loss:   0.031283 => Txt Tokens per Sec:     5696 || Lr: 0.000050
2024-02-05 21:14:52,419 Epoch 454: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.87 
2024-02-05 21:14:52,419 EPOCH 455
2024-02-05 21:14:57,648 Epoch 455: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.92 
2024-02-05 21:14:57,648 EPOCH 456
2024-02-05 21:14:58,948 [Epoch: 456 Step: 00030500] Batch Recognition Loss:   0.000203 => Gls Tokens per Sec:     1771 || Batch Translation Loss:   0.020969 => Txt Tokens per Sec:     4941 || Lr: 0.000050
2024-02-05 21:15:03,033 Epoch 456: Total Training Recognition Loss 0.11  Total Training Translation Loss 2.21 
2024-02-05 21:15:03,034 EPOCH 457
2024-02-05 21:15:06,785 [Epoch: 457 Step: 00030600] Batch Recognition Loss:   0.000331 => Gls Tokens per Sec:     2022 || Batch Translation Loss:   0.007354 => Txt Tokens per Sec:     5665 || Lr: 0.000050
2024-02-05 21:15:08,317 Epoch 457: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.09 
2024-02-05 21:15:08,318 EPOCH 458
2024-02-05 21:15:13,603 Epoch 458: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.43 
2024-02-05 21:15:13,603 EPOCH 459
2024-02-05 21:15:14,625 [Epoch: 459 Step: 00030700] Batch Recognition Loss:   0.000146 => Gls Tokens per Sec:     2194 || Batch Translation Loss:   0.020949 => Txt Tokens per Sec:     6283 || Lr: 0.000050
2024-02-05 21:15:18,441 Epoch 459: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.12 
2024-02-05 21:15:18,442 EPOCH 460
2024-02-05 21:15:22,187 [Epoch: 460 Step: 00030800] Batch Recognition Loss:   0.000176 => Gls Tokens per Sec:     2009 || Batch Translation Loss:   0.016518 => Txt Tokens per Sec:     5465 || Lr: 0.000050
2024-02-05 21:15:23,788 Epoch 460: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.65 
2024-02-05 21:15:23,788 EPOCH 461
2024-02-05 21:15:29,281 Epoch 461: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.35 
2024-02-05 21:15:29,282 EPOCH 462
2024-02-05 21:15:30,150 [Epoch: 462 Step: 00030900] Batch Recognition Loss:   0.000156 => Gls Tokens per Sec:     2398 || Batch Translation Loss:   0.022993 => Txt Tokens per Sec:     6353 || Lr: 0.000050
2024-02-05 21:15:34,310 Epoch 462: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.80 
2024-02-05 21:15:34,310 EPOCH 463
2024-02-05 21:15:37,880 [Epoch: 463 Step: 00031000] Batch Recognition Loss:   0.000179 => Gls Tokens per Sec:     2034 || Batch Translation Loss:   0.020704 => Txt Tokens per Sec:     5677 || Lr: 0.000050
2024-02-05 21:15:39,530 Epoch 463: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.36 
2024-02-05 21:15:39,531 EPOCH 464
2024-02-05 21:15:44,824 Epoch 464: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.51 
2024-02-05 21:15:44,824 EPOCH 465
2024-02-05 21:15:45,532 [Epoch: 465 Step: 00031100] Batch Recognition Loss:   0.000237 => Gls Tokens per Sec:     2711 || Batch Translation Loss:   0.012607 => Txt Tokens per Sec:     6711 || Lr: 0.000050
2024-02-05 21:15:50,007 Epoch 465: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.03 
2024-02-05 21:15:50,007 EPOCH 466
2024-02-05 21:15:53,510 [Epoch: 466 Step: 00031200] Batch Recognition Loss:   0.000191 => Gls Tokens per Sec:     2028 || Batch Translation Loss:   0.011928 => Txt Tokens per Sec:     5880 || Lr: 0.000050
2024-02-05 21:15:55,163 Epoch 466: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.76 
2024-02-05 21:15:55,164 EPOCH 467
2024-02-05 21:16:00,478 Epoch 467: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.29 
2024-02-05 21:16:00,479 EPOCH 468
2024-02-05 21:16:01,280 [Epoch: 468 Step: 00031300] Batch Recognition Loss:   0.000658 => Gls Tokens per Sec:     2199 || Batch Translation Loss:   0.040351 => Txt Tokens per Sec:     6085 || Lr: 0.000050
2024-02-05 21:16:05,453 Epoch 468: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.68 
2024-02-05 21:16:05,453 EPOCH 469
2024-02-05 21:16:08,771 [Epoch: 469 Step: 00031400] Batch Recognition Loss:   0.000182 => Gls Tokens per Sec:     2093 || Batch Translation Loss:   0.144116 => Txt Tokens per Sec:     5783 || Lr: 0.000050
2024-02-05 21:16:10,806 Epoch 469: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.79 
2024-02-05 21:16:10,806 EPOCH 470
2024-02-05 21:16:15,605 Epoch 470: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.56 
2024-02-05 21:16:15,605 EPOCH 471
2024-02-05 21:16:16,290 [Epoch: 471 Step: 00031500] Batch Recognition Loss:   0.000221 => Gls Tokens per Sec:     2337 || Batch Translation Loss:   0.070301 => Txt Tokens per Sec:     6697 || Lr: 0.000050
2024-02-05 21:16:21,034 Epoch 471: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.22 
2024-02-05 21:16:21,035 EPOCH 472
2024-02-05 21:16:24,276 [Epoch: 472 Step: 00031600] Batch Recognition Loss:   0.000408 => Gls Tokens per Sec:     2124 || Batch Translation Loss:   0.011119 => Txt Tokens per Sec:     5708 || Lr: 0.000050
2024-02-05 21:16:26,333 Epoch 472: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.07 
2024-02-05 21:16:26,333 EPOCH 473
2024-02-05 21:16:31,771 Epoch 473: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.05 
2024-02-05 21:16:31,772 EPOCH 474
2024-02-05 21:16:32,380 [Epoch: 474 Step: 00031700] Batch Recognition Loss:   0.000291 => Gls Tokens per Sec:     2372 || Batch Translation Loss:   0.017703 => Txt Tokens per Sec:     6613 || Lr: 0.000050
2024-02-05 21:16:36,525 Epoch 474: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.36 
2024-02-05 21:16:36,525 EPOCH 475
2024-02-05 21:16:40,006 [Epoch: 475 Step: 00031800] Batch Recognition Loss:   0.000181 => Gls Tokens per Sec:     1903 || Batch Translation Loss:   0.009807 => Txt Tokens per Sec:     5286 || Lr: 0.000050
2024-02-05 21:16:42,057 Epoch 475: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.01 
2024-02-05 21:16:42,058 EPOCH 476
2024-02-05 21:16:47,635 Epoch 476: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.58 
2024-02-05 21:16:47,635 EPOCH 477
2024-02-05 21:16:48,168 [Epoch: 477 Step: 00031900] Batch Recognition Loss:   0.000266 => Gls Tokens per Sec:     2406 || Batch Translation Loss:   0.009334 => Txt Tokens per Sec:     6414 || Lr: 0.000050
2024-02-05 21:16:52,821 Epoch 477: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.56 
2024-02-05 21:16:52,822 EPOCH 478
2024-02-05 21:16:56,395 [Epoch: 478 Step: 00032000] Batch Recognition Loss:   0.000432 => Gls Tokens per Sec:     1809 || Batch Translation Loss:   0.016696 => Txt Tokens per Sec:     5046 || Lr: 0.000050
2024-02-05 21:17:05,020 Validation result at epoch 478, step    32000: duration: 8.6244s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 3.52171	Translation Loss: 91619.54688	PPL: 9423.60840
	Eval Metric: BLEU
	WER 4.10	(DEL: 0.00,	INS: 0.00,	SUB: 4.10)
	BLEU-4 0.73	(BLEU-1: 11.84,	BLEU-2: 3.73,	BLEU-3: 1.50,	BLEU-4: 0.73)
	CHRF 17.69	ROUGE 9.69
2024-02-05 21:17:05,021 Logging Recognition and Translation Outputs
2024-02-05 21:17:05,021 ========================================================================================================================
2024-02-05 21:17:05,021 Logging Sequence: 162_133.00
2024-02-05 21:17:05,022 	Gloss Reference :	A B+C+D+E
2024-02-05 21:17:05,022 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:17:05,022 	Gloss Alignment :	         
2024-02-05 21:17:05,022 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:17:05,023 	Text Reference  :	******* ** ****** *** they also sent  rape    threats to his 9-month old       daughter
2024-02-05 21:17:05,023 	Text Hypothesis :	'usman is muslim and does not  drink alcohol because of his ******* religious beliefs 
2024-02-05 21:17:05,023 	Text Alignment  :	I       I  I      I   S    S    S     S       S       S      D       S         S       
2024-02-05 21:17:05,023 ========================================================================================================================
2024-02-05 21:17:05,024 Logging Sequence: 134_236.00
2024-02-05 21:17:05,024 	Gloss Reference :	A B+C+D+E
2024-02-05 21:17:05,024 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:17:05,024 	Gloss Alignment :	         
2024-02-05 21:17:05,024 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:17:05,025 	Text Reference  :	after the ***** **** ****** *** ******* *** *********** **** interaction modi       tweeted the images and         captioned it  saying   
2024-02-05 21:17:05,025 	Text Hypothesis :	***** the first time hardik met natasha was interacting with india's     contingent at      the ****** deaflympics at        the residence
2024-02-05 21:17:05,026 	Text Alignment  :	D         I     I    I      I   I       I   I           I    S           S          S           D      S           S         S   S        
2024-02-05 21:17:05,026 ========================================================================================================================
2024-02-05 21:17:05,026 Logging Sequence: 145_52.00
2024-02-05 21:17:05,026 	Gloss Reference :	A B+C+D+E
2024-02-05 21:17:05,026 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:17:05,026 	Gloss Alignment :	         
2024-02-05 21:17:05,026 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:17:05,027 	Text Reference  :	her name was dropped despite having qualified as  she was   the only female athlete
2024-02-05 21:17:05,028 	Text Hypothesis :	*** **** *** ******* ******* india  has       won the world cup for  12     lakh   
2024-02-05 21:17:05,028 	Text Alignment  :	D   D    D   D       D       S      S         S   S   S     S   S    S      S      
2024-02-05 21:17:05,028 ========================================================================================================================
2024-02-05 21:17:05,028 Logging Sequence: 175_40.00
2024-02-05 21:17:05,028 	Gloss Reference :	A B+C+D+E
2024-02-05 21:17:05,028 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:17:05,028 	Gloss Alignment :	         
2024-02-05 21:17:05,028 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:17:05,030 	Text Reference  :	** *** *** ***** *** soumyadeep and shreya bagged   three medals each including a     silver medal each      
2024-02-05 21:17:05,030 	Text Hypothesis :	in the icc world cup india      and ****** pakistan have  faced  each ********* other seven  times previously
2024-02-05 21:17:05,030 	Text Alignment  :	I  I   I   I     I   S              D      S        S     S           D         S     S      S     S         
2024-02-05 21:17:05,030 ========================================================================================================================
2024-02-05 21:17:05,030 Logging Sequence: 156_51.00
2024-02-05 21:17:05,031 	Gloss Reference :	A B+C+D+E
2024-02-05 21:17:05,031 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:17:05,031 	Gloss Alignment :	         
2024-02-05 21:17:05,031 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:17:05,032 	Text Reference  :	******** ***************** the    selection of the players was similar to that  of   ipl   
2024-02-05 21:17:05,032 	Text Hypothesis :	multiple ticket-collection points would     be set up      at  venues  to avoid long queues
2024-02-05 21:17:05,032 	Text Alignment  :	I        I                 S      S         S  S   S       S   S          S     S    S     
2024-02-05 21:17:05,032 ========================================================================================================================
2024-02-05 21:17:07,043 Epoch 478: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.69 
2024-02-05 21:17:07,044 EPOCH 479
2024-02-05 21:17:12,656 Epoch 479: Total Training Recognition Loss 0.11  Total Training Translation Loss 4.99 
2024-02-05 21:17:12,657 EPOCH 480
2024-02-05 21:17:13,080 [Epoch: 480 Step: 00032100] Batch Recognition Loss:   0.010252 => Gls Tokens per Sec:     2654 || Batch Translation Loss:   0.038302 => Txt Tokens per Sec:     7136 || Lr: 0.000050
2024-02-05 21:17:18,032 Epoch 480: Total Training Recognition Loss 0.14  Total Training Translation Loss 6.99 
2024-02-05 21:17:18,032 EPOCH 481
2024-02-05 21:17:21,370 [Epoch: 481 Step: 00032200] Batch Recognition Loss:   0.000280 => Gls Tokens per Sec:     1888 || Batch Translation Loss:   0.070901 => Txt Tokens per Sec:     5240 || Lr: 0.000050
2024-02-05 21:17:23,416 Epoch 481: Total Training Recognition Loss 0.07  Total Training Translation Loss 5.05 
2024-02-05 21:17:23,416 EPOCH 482
2024-02-05 21:17:28,724 Epoch 482: Total Training Recognition Loss 0.11  Total Training Translation Loss 3.74 
2024-02-05 21:17:28,725 EPOCH 483
2024-02-05 21:17:29,198 [Epoch: 483 Step: 00032300] Batch Recognition Loss:   0.002452 => Gls Tokens per Sec:     2038 || Batch Translation Loss:   0.017745 => Txt Tokens per Sec:     5490 || Lr: 0.000050
2024-02-05 21:17:34,058 Epoch 483: Total Training Recognition Loss 0.08  Total Training Translation Loss 3.69 
2024-02-05 21:17:34,058 EPOCH 484
2024-02-05 21:17:37,235 [Epoch: 484 Step: 00032400] Batch Recognition Loss:   0.002735 => Gls Tokens per Sec:     1934 || Batch Translation Loss:   0.107261 => Txt Tokens per Sec:     5225 || Lr: 0.000050
2024-02-05 21:17:39,615 Epoch 484: Total Training Recognition Loss 0.06  Total Training Translation Loss 4.92 
2024-02-05 21:17:39,615 EPOCH 485
2024-02-05 21:17:44,658 Epoch 485: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.53 
2024-02-05 21:17:44,658 EPOCH 486
2024-02-05 21:17:45,020 [Epoch: 486 Step: 00032500] Batch Recognition Loss:   0.000469 => Gls Tokens per Sec:     2216 || Batch Translation Loss:   0.123185 => Txt Tokens per Sec:     4850 || Lr: 0.000050
2024-02-05 21:17:49,773 Epoch 486: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.01 
2024-02-05 21:17:49,774 EPOCH 487
2024-02-05 21:17:52,801 [Epoch: 487 Step: 00032600] Batch Recognition Loss:   0.000361 => Gls Tokens per Sec:     1976 || Batch Translation Loss:   0.028840 => Txt Tokens per Sec:     5497 || Lr: 0.000050
2024-02-05 21:17:54,924 Epoch 487: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.84 
2024-02-05 21:17:54,924 EPOCH 488
2024-02-05 21:18:00,212 Epoch 488: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.06 
2024-02-05 21:18:00,212 EPOCH 489
2024-02-05 21:18:00,464 [Epoch: 489 Step: 00032700] Batch Recognition Loss:   0.000251 => Gls Tokens per Sec:     2550 || Batch Translation Loss:   0.016727 => Txt Tokens per Sec:     6641 || Lr: 0.000050
2024-02-05 21:18:05,255 Epoch 489: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.38 
2024-02-05 21:18:05,256 EPOCH 490
2024-02-05 21:18:08,380 [Epoch: 490 Step: 00032800] Batch Recognition Loss:   0.000208 => Gls Tokens per Sec:     1863 || Batch Translation Loss:   0.021285 => Txt Tokens per Sec:     5321 || Lr: 0.000050
2024-02-05 21:18:10,561 Epoch 490: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.33 
2024-02-05 21:18:10,562 EPOCH 491
2024-02-05 21:18:15,593 Epoch 491: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.71 
2024-02-05 21:18:15,593 EPOCH 492
2024-02-05 21:18:15,765 [Epoch: 492 Step: 00032900] Batch Recognition Loss:   0.000134 => Gls Tokens per Sec:     2807 || Batch Translation Loss:   0.012354 => Txt Tokens per Sec:     7409 || Lr: 0.000050
2024-02-05 21:18:20,980 Epoch 492: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.18 
2024-02-05 21:18:20,980 EPOCH 493
2024-02-05 21:18:23,504 [Epoch: 493 Step: 00033000] Batch Recognition Loss:   0.000355 => Gls Tokens per Sec:     2283 || Batch Translation Loss:   0.019994 => Txt Tokens per Sec:     6265 || Lr: 0.000050
2024-02-05 21:18:25,701 Epoch 493: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.30 
2024-02-05 21:18:25,701 EPOCH 494
2024-02-05 21:18:31,216 Epoch 494: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.05 
2024-02-05 21:18:31,217 EPOCH 495
2024-02-05 21:18:31,322 [Epoch: 495 Step: 00033100] Batch Recognition Loss:   0.000123 => Gls Tokens per Sec:     3077 || Batch Translation Loss:   0.011750 => Txt Tokens per Sec:     7596 || Lr: 0.000050
2024-02-05 21:18:36,544 Epoch 495: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.72 
2024-02-05 21:18:36,545 EPOCH 496
2024-02-05 21:18:39,296 [Epoch: 496 Step: 00033200] Batch Recognition Loss:   0.000155 => Gls Tokens per Sec:     2036 || Batch Translation Loss:   0.016732 => Txt Tokens per Sec:     5632 || Lr: 0.000050
2024-02-05 21:18:41,734 Epoch 496: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.92 
2024-02-05 21:18:41,734 EPOCH 497
2024-02-05 21:18:46,842 Epoch 497: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.07 
2024-02-05 21:18:46,842 EPOCH 498
2024-02-05 21:18:46,963 [Epoch: 498 Step: 00033300] Batch Recognition Loss:   0.000415 => Gls Tokens per Sec:     1333 || Batch Translation Loss:   0.050361 => Txt Tokens per Sec:     4650 || Lr: 0.000050
2024-02-05 21:18:52,231 Epoch 498: Total Training Recognition Loss 0.05  Total Training Translation Loss 5.01 
2024-02-05 21:18:52,231 EPOCH 499
2024-02-05 21:18:54,562 [Epoch: 499 Step: 00033400] Batch Recognition Loss:   0.000726 => Gls Tokens per Sec:     2291 || Batch Translation Loss:   0.040794 => Txt Tokens per Sec:     6229 || Lr: 0.000050
2024-02-05 21:18:57,354 Epoch 499: Total Training Recognition Loss 0.07  Total Training Translation Loss 5.64 
2024-02-05 21:18:57,355 EPOCH 500
2024-02-05 21:19:02,438 [Epoch: 500 Step: 00033500] Batch Recognition Loss:   0.000326 => Gls Tokens per Sec:     2090 || Batch Translation Loss:   0.022742 => Txt Tokens per Sec:     5782 || Lr: 0.000050
2024-02-05 21:19:02,439 Epoch 500: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.51 
2024-02-05 21:19:02,439 EPOCH 501
2024-02-05 21:19:07,658 Epoch 501: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.62 
2024-02-05 21:19:07,658 EPOCH 502
2024-02-05 21:19:10,156 [Epoch: 502 Step: 00033600] Batch Recognition Loss:   0.001134 => Gls Tokens per Sec:     2075 || Batch Translation Loss:   0.098712 => Txt Tokens per Sec:     6017 || Lr: 0.000050
2024-02-05 21:19:12,693 Epoch 502: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.46 
2024-02-05 21:19:12,693 EPOCH 503
2024-02-05 21:19:17,939 [Epoch: 503 Step: 00033700] Batch Recognition Loss:   0.000557 => Gls Tokens per Sec:     1994 || Batch Translation Loss:   0.018413 => Txt Tokens per Sec:     5525 || Lr: 0.000050
2024-02-05 21:19:18,002 Epoch 503: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.53 
2024-02-05 21:19:18,002 EPOCH 504
2024-02-05 21:19:22,807 Epoch 504: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.95 
2024-02-05 21:19:22,807 EPOCH 505
2024-02-05 21:19:25,261 [Epoch: 505 Step: 00033800] Batch Recognition Loss:   0.001773 => Gls Tokens per Sec:     2046 || Batch Translation Loss:   0.017041 => Txt Tokens per Sec:     5516 || Lr: 0.000050
2024-02-05 21:19:28,217 Epoch 505: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.98 
2024-02-05 21:19:28,217 EPOCH 506
2024-02-05 21:19:33,043 [Epoch: 506 Step: 00033900] Batch Recognition Loss:   0.000332 => Gls Tokens per Sec:     2134 || Batch Translation Loss:   0.036211 => Txt Tokens per Sec:     5879 || Lr: 0.000050
2024-02-05 21:19:33,218 Epoch 506: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.08 
2024-02-05 21:19:33,218 EPOCH 507
2024-02-05 21:19:38,733 Epoch 507: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.38 
2024-02-05 21:19:38,733 EPOCH 508
2024-02-05 21:19:41,112 [Epoch: 508 Step: 00034000] Batch Recognition Loss:   0.000165 => Gls Tokens per Sec:     2086 || Batch Translation Loss:   0.014852 => Txt Tokens per Sec:     6070 || Lr: 0.000050
2024-02-05 21:19:49,871 Validation result at epoch 508, step    34000: duration: 8.7598s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 3.44541	Translation Loss: 90992.14062	PPL: 8851.19238
	Eval Metric: BLEU
	WER 4.31	(DEL: 0.00,	INS: 0.00,	SUB: 4.31)
	BLEU-4 0.72	(BLEU-1: 11.42,	BLEU-2: 3.59,	BLEU-3: 1.46,	BLEU-4: 0.72)
	CHRF 17.30	ROUGE 9.55
2024-02-05 21:19:49,872 Logging Recognition and Translation Outputs
2024-02-05 21:19:49,872 ========================================================================================================================
2024-02-05 21:19:49,872 Logging Sequence: 171_158.00
2024-02-05 21:19:49,873 	Gloss Reference :	A B+C+D+E
2024-02-05 21:19:49,873 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:19:49,873 	Gloss Alignment :	         
2024-02-05 21:19:49,873 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:19:49,874 	Text Reference  :	with    speculations of dhoni being banned are spreading many say   that it is        unlikely to  happen
2024-02-05 21:19:49,874 	Text Hypothesis :	however this         is why   the   finals are ********* **** going to   be completed with     the ipl   
2024-02-05 21:19:49,875 	Text Alignment  :	S       S            S  S     S     S          D         D    S     S    S  S         S        S   S     
2024-02-05 21:19:49,875 ========================================================================================================================
2024-02-05 21:19:49,875 Logging Sequence: 108_235.00
2024-02-05 21:19:49,875 	Gloss Reference :	A B+C+D+E
2024-02-05 21:19:49,875 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:19:49,875 	Gloss Alignment :	         
2024-02-05 21:19:49,875 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:19:49,877 	Text Reference  :	he was taken to   the           hospital and it   was  reported that he ****** is   not       in  any    danger   
2024-02-05 21:19:49,877 	Text Hypothesis :	** *** ***** many congratulated him      and said that they     hope he scores more centuries and double centuries
2024-02-05 21:19:49,877 	Text Alignment  :	D  D   D     S    S             S            S    S    S        S       I      S    S         S   S      S        
2024-02-05 21:19:49,877 ========================================================================================================================
2024-02-05 21:19:49,877 Logging Sequence: 153_206.00
2024-02-05 21:19:49,878 	Gloss Reference :	A B+C+D+E
2024-02-05 21:19:49,878 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:19:49,878 	Gloss Alignment :	         
2024-02-05 21:19:49,878 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:19:49,879 	Text Reference  :	*** ***** **** *** **** ***** now on   13th november everyone is   hoping pakistan rewrites history
2024-02-05 21:19:49,879 	Text Hypothesis :	the first time the 2022 final was held in   uae      with     2022 to     become   the      years  
2024-02-05 21:19:49,879 	Text Alignment  :	I   I     I    I   I    I     S   S    S    S        S        S    S      S        S        S      
2024-02-05 21:19:49,879 ========================================================================================================================
2024-02-05 21:19:49,879 Logging Sequence: 87_202.00
2024-02-05 21:19:49,880 	Gloss Reference :	A B+C+D+E
2024-02-05 21:19:49,880 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:19:49,880 	Gloss Alignment :	         
2024-02-05 21:19:49,880 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:19:49,881 	Text Reference  :	*** i      love    our players and  i   love  my country
2024-02-05 21:19:49,881 	Text Hypothesis :	she really created by  her     name was known as well   
2024-02-05 21:19:49,881 	Text Alignment  :	I   S      S       S   S       S    S   S     S  S      
2024-02-05 21:19:49,881 ========================================================================================================================
2024-02-05 21:19:49,881 Logging Sequence: 84_2.00
2024-02-05 21:19:49,881 	Gloss Reference :	A B+C+D+E
2024-02-05 21:19:49,881 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:19:49,882 	Gloss Alignment :	         
2024-02-05 21:19:49,882 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:19:49,883 	Text Reference  :	******* the 2022 fifa football world cup is    going on      in ******** qatar from 20th november 2022    to 18th  december 2022  
2024-02-05 21:19:49,884 	Text Hypothesis :	however the **** **** ******** ***** *** first time  sources in congress said  'i   am   very     excited to watch the      finals
2024-02-05 21:19:49,884 	Text Alignment  :	I           D    D    D        D     D   S     S     S          I        S     S    S    S        S          S     S        S     
2024-02-05 21:19:49,884 ========================================================================================================================
2024-02-05 21:19:52,440 Epoch 508: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.36 
2024-02-05 21:19:52,440 EPOCH 509
2024-02-05 21:19:57,567 [Epoch: 509 Step: 00034100] Batch Recognition Loss:   0.000391 => Gls Tokens per Sec:     1978 || Batch Translation Loss:   0.030351 => Txt Tokens per Sec:     5478 || Lr: 0.000050
2024-02-05 21:19:57,911 Epoch 509: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.22 
2024-02-05 21:19:57,912 EPOCH 510
2024-02-05 21:20:02,976 Epoch 510: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.32 
2024-02-05 21:20:02,977 EPOCH 511
2024-02-05 21:20:05,335 [Epoch: 511 Step: 00034200] Batch Recognition Loss:   0.000360 => Gls Tokens per Sec:     1993 || Batch Translation Loss:   0.066665 => Txt Tokens per Sec:     5241 || Lr: 0.000050
2024-02-05 21:20:08,411 Epoch 511: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.76 
2024-02-05 21:20:08,411 EPOCH 512
2024-02-05 21:20:13,652 [Epoch: 512 Step: 00034300] Batch Recognition Loss:   0.000510 => Gls Tokens per Sec:     1904 || Batch Translation Loss:   0.019705 => Txt Tokens per Sec:     5287 || Lr: 0.000050
2024-02-05 21:20:13,898 Epoch 512: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.81 
2024-02-05 21:20:13,899 EPOCH 513
2024-02-05 21:20:19,015 Epoch 513: Total Training Recognition Loss 0.11  Total Training Translation Loss 1.45 
2024-02-05 21:20:19,016 EPOCH 514
2024-02-05 21:20:20,771 [Epoch: 514 Step: 00034400] Batch Recognition Loss:   0.000115 => Gls Tokens per Sec:     2645 || Batch Translation Loss:   0.013614 => Txt Tokens per Sec:     6962 || Lr: 0.000050
2024-02-05 21:20:24,130 Epoch 514: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.65 
2024-02-05 21:20:24,131 EPOCH 515
2024-02-05 21:20:29,249 [Epoch: 515 Step: 00034500] Batch Recognition Loss:   0.000353 => Gls Tokens per Sec:     1919 || Batch Translation Loss:   0.029475 => Txt Tokens per Sec:     5336 || Lr: 0.000050
2024-02-05 21:20:29,564 Epoch 515: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.51 
2024-02-05 21:20:29,565 EPOCH 516
2024-02-05 21:20:34,984 Epoch 516: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.42 
2024-02-05 21:20:34,985 EPOCH 517
2024-02-05 21:20:36,931 [Epoch: 517 Step: 00034600] Batch Recognition Loss:   0.000407 => Gls Tokens per Sec:     2305 || Batch Translation Loss:   0.013808 => Txt Tokens per Sec:     6270 || Lr: 0.000050
2024-02-05 21:20:40,055 Epoch 517: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.63 
2024-02-05 21:20:40,055 EPOCH 518
2024-02-05 21:20:45,185 [Epoch: 518 Step: 00034700] Batch Recognition Loss:   0.000156 => Gls Tokens per Sec:     1883 || Batch Translation Loss:   0.229934 => Txt Tokens per Sec:     5277 || Lr: 0.000050
2024-02-05 21:20:45,562 Epoch 518: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.17 
2024-02-05 21:20:45,562 EPOCH 519
2024-02-05 21:20:50,578 Epoch 519: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.79 
2024-02-05 21:20:50,578 EPOCH 520
2024-02-05 21:20:52,711 [Epoch: 520 Step: 00034800] Batch Recognition Loss:   0.000305 => Gls Tokens per Sec:     2026 || Batch Translation Loss:   0.034595 => Txt Tokens per Sec:     5571 || Lr: 0.000050
2024-02-05 21:20:56,135 Epoch 520: Total Training Recognition Loss 0.05  Total Training Translation Loss 6.29 
2024-02-05 21:20:56,135 EPOCH 521
2024-02-05 21:21:00,438 [Epoch: 521 Step: 00034900] Batch Recognition Loss:   0.000269 => Gls Tokens per Sec:     2232 || Batch Translation Loss:   0.181251 => Txt Tokens per Sec:     6234 || Lr: 0.000050
2024-02-05 21:21:00,937 Epoch 521: Total Training Recognition Loss 0.09  Total Training Translation Loss 8.24 
2024-02-05 21:21:00,937 EPOCH 522
2024-02-05 21:21:06,510 Epoch 522: Total Training Recognition Loss 0.07  Total Training Translation Loss 7.59 
2024-02-05 21:21:06,511 EPOCH 523
2024-02-05 21:21:08,260 [Epoch: 523 Step: 00035000] Batch Recognition Loss:   0.007647 => Gls Tokens per Sec:     2379 || Batch Translation Loss:   0.027967 => Txt Tokens per Sec:     6454 || Lr: 0.000050
2024-02-05 21:21:11,649 Epoch 523: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.46 
2024-02-05 21:21:11,650 EPOCH 524
2024-02-05 21:21:16,291 [Epoch: 524 Step: 00035100] Batch Recognition Loss:   0.000215 => Gls Tokens per Sec:     2035 || Batch Translation Loss:   0.033991 => Txt Tokens per Sec:     5610 || Lr: 0.000050
2024-02-05 21:21:16,957 Epoch 524: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.74 
2024-02-05 21:21:16,957 EPOCH 525
2024-02-05 21:21:22,031 Epoch 525: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.87 
2024-02-05 21:21:22,031 EPOCH 526
2024-02-05 21:21:24,028 [Epoch: 526 Step: 00035200] Batch Recognition Loss:   0.002579 => Gls Tokens per Sec:     2004 || Batch Translation Loss:   0.023055 => Txt Tokens per Sec:     5128 || Lr: 0.000050
2024-02-05 21:21:27,637 Epoch 526: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.85 
2024-02-05 21:21:27,637 EPOCH 527
2024-02-05 21:21:31,777 [Epoch: 527 Step: 00035300] Batch Recognition Loss:   0.000474 => Gls Tokens per Sec:     2217 || Batch Translation Loss:   0.018912 => Txt Tokens per Sec:     6091 || Lr: 0.000050
2024-02-05 21:21:32,701 Epoch 527: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.58 
2024-02-05 21:21:32,701 EPOCH 528
2024-02-05 21:21:38,036 Epoch 528: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.25 
2024-02-05 21:21:38,037 EPOCH 529
2024-02-05 21:21:39,659 [Epoch: 529 Step: 00035400] Batch Recognition Loss:   0.000148 => Gls Tokens per Sec:     2306 || Batch Translation Loss:   0.016491 => Txt Tokens per Sec:     6485 || Lr: 0.000050
2024-02-05 21:21:43,048 Epoch 529: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.78 
2024-02-05 21:21:43,048 EPOCH 530
2024-02-05 21:21:47,533 [Epoch: 530 Step: 00035500] Batch Recognition Loss:   0.000151 => Gls Tokens per Sec:     2011 || Batch Translation Loss:   0.019067 => Txt Tokens per Sec:     5571 || Lr: 0.000050
2024-02-05 21:21:48,263 Epoch 530: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.84 
2024-02-05 21:21:48,263 EPOCH 531
2024-02-05 21:21:53,230 Epoch 531: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.52 
2024-02-05 21:21:53,230 EPOCH 532
2024-02-05 21:21:54,983 [Epoch: 532 Step: 00035600] Batch Recognition Loss:   0.000232 => Gls Tokens per Sec:     2044 || Batch Translation Loss:   0.016674 => Txt Tokens per Sec:     5619 || Lr: 0.000050
2024-02-05 21:21:58,575 Epoch 532: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.19 
2024-02-05 21:21:58,576 EPOCH 533
2024-02-05 21:22:03,053 [Epoch: 533 Step: 00035700] Batch Recognition Loss:   0.000144 => Gls Tokens per Sec:     1980 || Batch Translation Loss:   0.005707 => Txt Tokens per Sec:     5372 || Lr: 0.000050
2024-02-05 21:22:03,994 Epoch 533: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.28 
2024-02-05 21:22:03,994 EPOCH 534
2024-02-05 21:22:08,916 Epoch 534: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.15 
2024-02-05 21:22:08,916 EPOCH 535
2024-02-05 21:22:10,492 [Epoch: 535 Step: 00035800] Batch Recognition Loss:   0.000195 => Gls Tokens per Sec:     2236 || Batch Translation Loss:   0.018722 => Txt Tokens per Sec:     6137 || Lr: 0.000050
2024-02-05 21:22:14,181 Epoch 535: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.25 
2024-02-05 21:22:14,182 EPOCH 536
2024-02-05 21:22:18,445 [Epoch: 536 Step: 00035900] Batch Recognition Loss:   0.000932 => Gls Tokens per Sec:     2041 || Batch Translation Loss:   0.018275 => Txt Tokens per Sec:     5694 || Lr: 0.000050
2024-02-05 21:22:19,226 Epoch 536: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.20 
2024-02-05 21:22:19,226 EPOCH 537
2024-02-05 21:22:24,674 Epoch 537: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.36 
2024-02-05 21:22:24,675 EPOCH 538
2024-02-05 21:22:26,164 [Epoch: 538 Step: 00036000] Batch Recognition Loss:   0.000170 => Gls Tokens per Sec:     2257 || Batch Translation Loss:   0.021792 => Txt Tokens per Sec:     6050 || Lr: 0.000050
2024-02-05 21:22:34,448 Validation result at epoch 538, step    36000: duration: 8.2835s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 3.47123	Translation Loss: 93734.89844	PPL: 11640.61035
	Eval Metric: BLEU
	WER 4.38	(DEL: 0.00,	INS: 0.00,	SUB: 4.38)
	BLEU-4 0.50	(BLEU-1: 10.23,	BLEU-2: 3.57,	BLEU-3: 1.29,	BLEU-4: 0.50)
	CHRF 16.07	ROUGE 9.40
2024-02-05 21:22:34,449 Logging Recognition and Translation Outputs
2024-02-05 21:22:34,449 ========================================================================================================================
2024-02-05 21:22:34,449 Logging Sequence: 153_36.00
2024-02-05 21:22:34,450 	Gloss Reference :	A B+C+D+E
2024-02-05 21:22:34,450 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:22:34,450 	Gloss Alignment :	         
2024-02-05 21:22:34,450 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:22:34,451 	Text Reference  :	india made a good score of  1686  in 20 overs ******* ****** *** ****** ***
2024-02-05 21:22:34,451 	Text Hypothesis :	india **** * had  won   the match in 61 overs without losing any wicket wow
2024-02-05 21:22:34,451 	Text Alignment  :	      D    D S    S     S   S        S        I       I      I   I      I  
2024-02-05 21:22:34,451 ========================================================================================================================
2024-02-05 21:22:34,451 Logging Sequence: 163_30.00
2024-02-05 21:22:34,452 	Gloss Reference :	A B+C+D+E
2024-02-05 21:22:34,452 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:22:34,452 	Gloss Alignment :	         
2024-02-05 21:22:34,452 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:22:34,453 	Text Reference  :	**** ** they never permitted anyone to reveal her   face      
2024-02-05 21:22:34,453 	Text Hypothesis :	both of the  world cup       will   be in     quick succession
2024-02-05 21:22:34,453 	Text Alignment  :	I    I  S    S     S         S      S  S      S     S         
2024-02-05 21:22:34,453 ========================================================================================================================
2024-02-05 21:22:34,453 Logging Sequence: 167_60.00
2024-02-05 21:22:34,453 	Gloss Reference :	A B+C+D+E
2024-02-05 21:22:34,453 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:22:34,454 	Gloss Alignment :	         
2024-02-05 21:22:34,454 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:22:34,454 	Text Reference  :	camel flu spreads rapidly when one comes in close contact with the infected
2024-02-05 21:22:34,454 	Text Hypothesis :	***** *** ******* ******* **** *** ***** ** ***** ******* for  the stadium 
2024-02-05 21:22:34,454 	Text Alignment  :	D     D   D       D       D    D   D     D  D     D       S        S       
2024-02-05 21:22:34,454 ========================================================================================================================
2024-02-05 21:22:34,455 Logging Sequence: 84_35.00
2024-02-05 21:22:34,455 	Gloss Reference :	A B+C+D+E
2024-02-05 21:22:34,455 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:22:34,455 	Gloss Alignment :	         
2024-02-05 21:22:34,455 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:22:34,456 	Text Reference  :	here is the reason why they covered their mouth
2024-02-05 21:22:34,456 	Text Hypothesis :	he   is *** ****** *** we   decided to    win  
2024-02-05 21:22:34,456 	Text Alignment  :	S       D   D      D   S    S       S     S    
2024-02-05 21:22:34,456 ========================================================================================================================
2024-02-05 21:22:34,456 Logging Sequence: 96_2.00
2024-02-05 21:22:34,456 	Gloss Reference :	A B+C+D+E
2024-02-05 21:22:34,457 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:22:34,457 	Gloss Alignment :	         
2024-02-05 21:22:34,457 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:22:34,458 	Text Reference  :	the world is preparing for the      t20     world cup scheduled to start from 16th  october this year
2024-02-05 21:22:34,458 	Text Hypothesis :	the ***** ** ********* icc under-19 cricket world cup ********* ** ***** was  first played  in   uae 
2024-02-05 21:22:34,458 	Text Alignment  :	    D     D  D         S   S        S                 D         D  D     S    S     S       S    S   
2024-02-05 21:22:34,458 ========================================================================================================================
2024-02-05 21:22:38,231 Epoch 538: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.72 
2024-02-05 21:22:38,232 EPOCH 539
2024-02-05 21:22:42,593 [Epoch: 539 Step: 00036100] Batch Recognition Loss:   0.000223 => Gls Tokens per Sec:     1958 || Batch Translation Loss:   0.040794 => Txt Tokens per Sec:     5449 || Lr: 0.000050
2024-02-05 21:22:43,515 Epoch 539: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.77 
2024-02-05 21:22:43,515 EPOCH 540
2024-02-05 21:22:48,308 Epoch 540: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.88 
2024-02-05 21:22:48,308 EPOCH 541
2024-02-05 21:22:49,462 [Epoch: 541 Step: 00036200] Batch Recognition Loss:   0.000137 => Gls Tokens per Sec:     2779 || Batch Translation Loss:   0.019036 => Txt Tokens per Sec:     7111 || Lr: 0.000050
2024-02-05 21:22:53,581 Epoch 541: Total Training Recognition Loss 0.05  Total Training Translation Loss 5.51 
2024-02-05 21:22:53,582 EPOCH 542
2024-02-05 21:22:57,705 [Epoch: 542 Step: 00036300] Batch Recognition Loss:   0.001073 => Gls Tokens per Sec:     2034 || Batch Translation Loss:   0.033019 => Txt Tokens per Sec:     5625 || Lr: 0.000050
2024-02-05 21:22:58,786 Epoch 542: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.70 
2024-02-05 21:22:58,786 EPOCH 543
2024-02-05 21:23:04,186 Epoch 543: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.15 
2024-02-05 21:23:04,187 EPOCH 544
2024-02-05 21:23:05,518 [Epoch: 544 Step: 00036400] Batch Recognition Loss:   0.000266 => Gls Tokens per Sec:     2285 || Batch Translation Loss:   0.021606 => Txt Tokens per Sec:     6201 || Lr: 0.000050
2024-02-05 21:23:09,025 Epoch 544: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.75 
2024-02-05 21:23:09,026 EPOCH 545
2024-02-05 21:23:13,232 [Epoch: 545 Step: 00036500] Batch Recognition Loss:   0.000163 => Gls Tokens per Sec:     1954 || Batch Translation Loss:   0.016378 => Txt Tokens per Sec:     5371 || Lr: 0.000050
2024-02-05 21:23:14,596 Epoch 545: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.88 
2024-02-05 21:23:14,596 EPOCH 546
2024-02-05 21:23:19,815 Epoch 546: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.03 
2024-02-05 21:23:19,816 EPOCH 547
2024-02-05 21:23:21,580 [Epoch: 547 Step: 00036600] Batch Recognition Loss:   0.000124 => Gls Tokens per Sec:     1634 || Batch Translation Loss:   0.013547 => Txt Tokens per Sec:     4691 || Lr: 0.000050
2024-02-05 21:23:25,220 Epoch 547: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.67 
2024-02-05 21:23:25,220 EPOCH 548
2024-02-05 21:23:28,926 [Epoch: 548 Step: 00036700] Batch Recognition Loss:   0.000972 => Gls Tokens per Sec:     2202 || Batch Translation Loss:   0.006441 => Txt Tokens per Sec:     6105 || Lr: 0.000050
2024-02-05 21:23:30,261 Epoch 548: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.44 
2024-02-05 21:23:30,262 EPOCH 549
2024-02-05 21:23:35,531 Epoch 549: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.66 
2024-02-05 21:23:35,532 EPOCH 550
2024-02-05 21:23:36,844 [Epoch: 550 Step: 00036800] Batch Recognition Loss:   0.000172 => Gls Tokens per Sec:     2076 || Batch Translation Loss:   0.038581 => Txt Tokens per Sec:     6145 || Lr: 0.000050
2024-02-05 21:23:40,860 Epoch 550: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.51 
2024-02-05 21:23:40,860 EPOCH 551
2024-02-05 21:23:44,935 [Epoch: 551 Step: 00036900] Batch Recognition Loss:   0.000236 => Gls Tokens per Sec:     1939 || Batch Translation Loss:   0.052503 => Txt Tokens per Sec:     5445 || Lr: 0.000050
2024-02-05 21:23:46,222 Epoch 551: Total Training Recognition Loss 0.08  Total Training Translation Loss 3.83 
2024-02-05 21:23:46,222 EPOCH 552
2024-02-05 21:23:51,333 Epoch 552: Total Training Recognition Loss 0.06  Total Training Translation Loss 4.14 
2024-02-05 21:23:51,334 EPOCH 553
2024-02-05 21:23:52,594 [Epoch: 553 Step: 00037000] Batch Recognition Loss:   0.007002 => Gls Tokens per Sec:     2035 || Batch Translation Loss:   0.035390 => Txt Tokens per Sec:     5877 || Lr: 0.000050
2024-02-05 21:23:56,346 Epoch 553: Total Training Recognition Loss 0.05  Total Training Translation Loss 4.50 
2024-02-05 21:23:56,346 EPOCH 554
2024-02-05 21:24:00,151 [Epoch: 554 Step: 00037100] Batch Recognition Loss:   0.000229 => Gls Tokens per Sec:     2035 || Batch Translation Loss:   0.036136 => Txt Tokens per Sec:     5759 || Lr: 0.000050
2024-02-05 21:24:01,481 Epoch 554: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.75 
2024-02-05 21:24:01,481 EPOCH 555
2024-02-05 21:24:06,762 Epoch 555: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.96 
2024-02-05 21:24:06,763 EPOCH 556
2024-02-05 21:24:08,205 [Epoch: 556 Step: 00037200] Batch Recognition Loss:   0.000183 => Gls Tokens per Sec:     1665 || Batch Translation Loss:   0.021598 => Txt Tokens per Sec:     4889 || Lr: 0.000050
2024-02-05 21:24:12,214 Epoch 556: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.34 
2024-02-05 21:24:12,214 EPOCH 557
2024-02-05 21:24:15,876 [Epoch: 557 Step: 00037300] Batch Recognition Loss:   0.000234 => Gls Tokens per Sec:     2071 || Batch Translation Loss:   0.018077 => Txt Tokens per Sec:     5927 || Lr: 0.000050
2024-02-05 21:24:16,967 Epoch 557: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.88 
2024-02-05 21:24:16,968 EPOCH 558
2024-02-05 21:24:22,316 Epoch 558: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.11 
2024-02-05 21:24:22,316 EPOCH 559
2024-02-05 21:24:23,322 [Epoch: 559 Step: 00037400] Batch Recognition Loss:   0.000205 => Gls Tokens per Sec:     2232 || Batch Translation Loss:   0.024566 => Txt Tokens per Sec:     6431 || Lr: 0.000050
2024-02-05 21:24:27,489 Epoch 559: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.82 
2024-02-05 21:24:27,490 EPOCH 560
2024-02-05 21:24:31,413 [Epoch: 560 Step: 00037500] Batch Recognition Loss:   0.000218 => Gls Tokens per Sec:     1892 || Batch Translation Loss:   0.022156 => Txt Tokens per Sec:     5160 || Lr: 0.000050
2024-02-05 21:24:32,969 Epoch 560: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.84 
2024-02-05 21:24:32,970 EPOCH 561
2024-02-05 21:24:37,652 Epoch 561: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.58 
2024-02-05 21:24:37,653 EPOCH 562
2024-02-05 21:24:38,470 [Epoch: 562 Step: 00037600] Batch Recognition Loss:   0.000142 => Gls Tokens per Sec:     2549 || Batch Translation Loss:   0.015542 => Txt Tokens per Sec:     7111 || Lr: 0.000050
2024-02-05 21:24:42,843 Epoch 562: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.29 
2024-02-05 21:24:42,844 EPOCH 563
2024-02-05 21:24:46,246 [Epoch: 563 Step: 00037700] Batch Recognition Loss:   0.000424 => Gls Tokens per Sec:     2135 || Batch Translation Loss:   0.016717 => Txt Tokens per Sec:     5900 || Lr: 0.000050
2024-02-05 21:24:47,832 Epoch 563: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.62 
2024-02-05 21:24:47,833 EPOCH 564
2024-02-05 21:24:53,036 Epoch 564: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.41 
2024-02-05 21:24:53,037 EPOCH 565
2024-02-05 21:24:53,980 [Epoch: 565 Step: 00037800] Batch Recognition Loss:   0.000252 => Gls Tokens per Sec:     1932 || Batch Translation Loss:   0.012814 => Txt Tokens per Sec:     5278 || Lr: 0.000050
2024-02-05 21:24:58,534 Epoch 565: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.89 
2024-02-05 21:24:58,534 EPOCH 566
2024-02-05 21:25:01,577 [Epoch: 566 Step: 00037900] Batch Recognition Loss:   0.000322 => Gls Tokens per Sec:     2335 || Batch Translation Loss:   0.146508 => Txt Tokens per Sec:     6388 || Lr: 0.000050
2024-02-05 21:25:03,523 Epoch 566: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.14 
2024-02-05 21:25:03,524 EPOCH 567
2024-02-05 21:25:08,703 Epoch 567: Total Training Recognition Loss 0.04  Total Training Translation Loss 7.81 
2024-02-05 21:25:08,703 EPOCH 568
2024-02-05 21:25:09,388 [Epoch: 568 Step: 00038000] Batch Recognition Loss:   0.000974 => Gls Tokens per Sec:     2577 || Batch Translation Loss:   0.043222 => Txt Tokens per Sec:     6488 || Lr: 0.000050
2024-02-05 21:25:17,993 Validation result at epoch 568, step    38000: duration: 8.6051s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.94868	Translation Loss: 89063.64844	PPL: 7300.44092
	Eval Metric: BLEU
	WER 4.10	(DEL: 0.00,	INS: 0.00,	SUB: 4.10)
	BLEU-4 0.55	(BLEU-1: 10.26,	BLEU-2: 2.98,	BLEU-3: 1.16,	BLEU-4: 0.55)
	CHRF 16.34	ROUGE 8.63
2024-02-05 21:25:17,994 Logging Recognition and Translation Outputs
2024-02-05 21:25:17,994 ========================================================================================================================
2024-02-05 21:25:17,994 Logging Sequence: 59_152.00
2024-02-05 21:25:17,994 	Gloss Reference :	A B+C+D+E
2024-02-05 21:25:17,995 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:25:17,995 	Gloss Alignment :	         
2024-02-05 21:25:17,995 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:25:17,996 	Text Reference  :	**** ****** the       organisers encouraged athletes to use  the condoms in their home    countries
2024-02-05 21:25:17,996 	Text Hypothesis :	even though pathirana can        be         held     in 2020 but there   is no    clarity over     
2024-02-05 21:25:17,996 	Text Alignment  :	I    I      S         S          S          S        S  S    S   S       S  S     S       S        
2024-02-05 21:25:17,996 ========================================================================================================================
2024-02-05 21:25:17,997 Logging Sequence: 155_78.00
2024-02-05 21:25:17,997 	Gloss Reference :	A B+C+D+E
2024-02-05 21:25:17,997 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:25:17,997 	Gloss Alignment :	         
2024-02-05 21:25:17,997 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:25:18,000 	Text Reference  :	** **** it was difficult for icc to disqualify the afghan team at   the   last    minute     so  they *** included them as   per the schedule
2024-02-05 21:25:18,000 	Text Hypothesis :	in 2019 it was ********* *** set to ********** *** ****** **** take place between manchester and they had to       take part in  the field   
2024-02-05 21:25:18,000 	Text Alignment  :	I  I           D         D   S      D          D   D      D    S    S     S       S          S        I   S        S    S    S       S       
2024-02-05 21:25:18,000 ========================================================================================================================
2024-02-05 21:25:18,000 Logging Sequence: 102_147.00
2024-02-05 21:25:18,000 	Gloss Reference :	A B+C+D+E
2024-02-05 21:25:18,000 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:25:18,001 	Gloss Alignment :	         
2024-02-05 21:25:18,001 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:25:18,002 	Text Reference  :	despite the muscle cramps this young   boy     lifted such a huge weight and made the country proud by          securing a   gold     medal 
2024-02-05 21:25:18,002 	Text Hypothesis :	******* the ****** couple are  unclear whether she    is   a **** ****** *** **** *** ******* ***** grandfather for      his casteist slurts
2024-02-05 21:25:18,003 	Text Alignment  :	D           D      S      S    S       S       S      S      D    D      D   D    D   D       D     S           S        S   S        S     
2024-02-05 21:25:18,003 ========================================================================================================================
2024-02-05 21:25:18,003 Logging Sequence: 105_2.00
2024-02-05 21:25:18,003 	Gloss Reference :	A B+C+D+E
2024-02-05 21:25:18,003 	Gloss Hypothesis:	A B+C+D  
2024-02-05 21:25:18,003 	Gloss Alignment :	  S      
2024-02-05 21:25:18,003 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:25:18,004 	Text Reference  :	the airthings masters tournament is an online chess tournament
2024-02-05 21:25:18,004 	Text Hypothesis :	the ********* second  time       is in the    asian games     
2024-02-05 21:25:18,004 	Text Alignment  :	    D         S       S             S  S      S     S         
2024-02-05 21:25:18,004 ========================================================================================================================
2024-02-05 21:25:18,004 Logging Sequence: 96_31.00
2024-02-05 21:25:18,005 	Gloss Reference :	A B+C+D+E
2024-02-05 21:25:18,005 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:25:18,005 	Gloss Alignment :	         
2024-02-05 21:25:18,005 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:25:18,006 	Text Reference  :	and then 2 teams will  go  on  to   play the  final 
2024-02-05 21:25:18,006 	Text Hypothesis :	*** **** * ***** india had won with a    huge margin
2024-02-05 21:25:18,006 	Text Alignment  :	D   D    D D     S     S   S   S    S    S    S     
2024-02-05 21:25:18,006 ========================================================================================================================
2024-02-05 21:25:22,957 Epoch 568: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.23 
2024-02-05 21:25:22,957 EPOCH 569
2024-02-05 21:25:26,339 [Epoch: 569 Step: 00038100] Batch Recognition Loss:   0.000482 => Gls Tokens per Sec:     2082 || Batch Translation Loss:   0.025873 => Txt Tokens per Sec:     5824 || Lr: 0.000050
2024-02-05 21:25:28,095 Epoch 569: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.99 
2024-02-05 21:25:28,095 EPOCH 570
2024-02-05 21:25:33,201 Epoch 570: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.50 
2024-02-05 21:25:33,201 EPOCH 571
2024-02-05 21:25:33,986 [Epoch: 571 Step: 00038200] Batch Recognition Loss:   0.000639 => Gls Tokens per Sec:     2043 || Batch Translation Loss:   0.023837 => Txt Tokens per Sec:     5663 || Lr: 0.000050
2024-02-05 21:25:38,305 Epoch 571: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.38 
2024-02-05 21:25:38,305 EPOCH 572
2024-02-05 21:25:41,602 [Epoch: 572 Step: 00038300] Batch Recognition Loss:   0.000456 => Gls Tokens per Sec:     2057 || Batch Translation Loss:   0.020162 => Txt Tokens per Sec:     5725 || Lr: 0.000050
2024-02-05 21:25:43,510 Epoch 572: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.74 
2024-02-05 21:25:43,510 EPOCH 573
2024-02-05 21:25:48,625 Epoch 573: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.70 
2024-02-05 21:25:48,625 EPOCH 574
2024-02-05 21:25:49,294 [Epoch: 574 Step: 00038400] Batch Recognition Loss:   0.000271 => Gls Tokens per Sec:     2156 || Batch Translation Loss:   0.015672 => Txt Tokens per Sec:     6012 || Lr: 0.000050
2024-02-05 21:25:53,744 Epoch 574: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.31 
2024-02-05 21:25:53,744 EPOCH 575
2024-02-05 21:25:56,986 [Epoch: 575 Step: 00038500] Batch Recognition Loss:   0.000162 => Gls Tokens per Sec:     2042 || Batch Translation Loss:   0.023135 => Txt Tokens per Sec:     5664 || Lr: 0.000050
2024-02-05 21:25:58,841 Epoch 575: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.18 
2024-02-05 21:25:58,841 EPOCH 576
2024-02-05 21:26:04,215 Epoch 576: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.80 
2024-02-05 21:26:04,216 EPOCH 577
2024-02-05 21:26:04,908 [Epoch: 577 Step: 00038600] Batch Recognition Loss:   0.000173 => Gls Tokens per Sec:     1852 || Batch Translation Loss:   0.022783 => Txt Tokens per Sec:     4793 || Lr: 0.000050
2024-02-05 21:26:09,310 Epoch 577: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.83 
2024-02-05 21:26:09,311 EPOCH 578
2024-02-05 21:26:12,630 [Epoch: 578 Step: 00038700] Batch Recognition Loss:   0.000206 => Gls Tokens per Sec:     1946 || Batch Translation Loss:   0.028346 => Txt Tokens per Sec:     5365 || Lr: 0.000050
2024-02-05 21:26:14,785 Epoch 578: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.85 
2024-02-05 21:26:14,785 EPOCH 579
2024-02-05 21:26:19,545 Epoch 579: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.89 
2024-02-05 21:26:19,545 EPOCH 580
2024-02-05 21:26:20,045 [Epoch: 580 Step: 00038800] Batch Recognition Loss:   0.000172 => Gls Tokens per Sec:     2252 || Batch Translation Loss:   0.035427 => Txt Tokens per Sec:     5891 || Lr: 0.000050
2024-02-05 21:26:25,036 Epoch 580: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.59 
2024-02-05 21:26:25,036 EPOCH 581
2024-02-05 21:26:28,000 [Epoch: 581 Step: 00038900] Batch Recognition Loss:   0.000158 => Gls Tokens per Sec:     2160 || Batch Translation Loss:   0.016408 => Txt Tokens per Sec:     6065 || Lr: 0.000050
2024-02-05 21:26:30,076 Epoch 581: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.91 
2024-02-05 21:26:30,077 EPOCH 582
2024-02-05 21:26:35,418 Epoch 582: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.00 
2024-02-05 21:26:35,418 EPOCH 583
2024-02-05 21:26:35,893 [Epoch: 583 Step: 00039000] Batch Recognition Loss:   0.000280 => Gls Tokens per Sec:     2025 || Batch Translation Loss:   0.022623 => Txt Tokens per Sec:     6017 || Lr: 0.000050
2024-02-05 21:26:40,457 Epoch 583: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.26 
2024-02-05 21:26:40,458 EPOCH 584
2024-02-05 21:26:43,506 [Epoch: 584 Step: 00039100] Batch Recognition Loss:   0.000985 => Gls Tokens per Sec:     2048 || Batch Translation Loss:   0.016116 => Txt Tokens per Sec:     5694 || Lr: 0.000050
2024-02-05 21:26:45,653 Epoch 584: Total Training Recognition Loss 0.14  Total Training Translation Loss 1.62 
2024-02-05 21:26:45,653 EPOCH 585
2024-02-05 21:26:50,811 Epoch 585: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.37 
2024-02-05 21:26:50,812 EPOCH 586
2024-02-05 21:26:51,154 [Epoch: 586 Step: 00039200] Batch Recognition Loss:   0.000321 => Gls Tokens per Sec:     2346 || Batch Translation Loss:   0.022776 => Txt Tokens per Sec:     5695 || Lr: 0.000050
2024-02-05 21:26:56,003 Epoch 586: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.30 
2024-02-05 21:26:56,004 EPOCH 587
2024-02-05 21:26:58,757 [Epoch: 587 Step: 00039300] Batch Recognition Loss:   0.000218 => Gls Tokens per Sec:     2172 || Batch Translation Loss:   0.022320 => Txt Tokens per Sec:     6167 || Lr: 0.000050
2024-02-05 21:27:01,376 Epoch 587: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.52 
2024-02-05 21:27:01,377 EPOCH 588
2024-02-05 21:27:06,602 Epoch 588: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.20 
2024-02-05 21:27:06,602 EPOCH 589
2024-02-05 21:27:06,934 [Epoch: 589 Step: 00039400] Batch Recognition Loss:   0.000508 => Gls Tokens per Sec:     1934 || Batch Translation Loss:   0.035825 => Txt Tokens per Sec:     5492 || Lr: 0.000050
2024-02-05 21:27:12,045 Epoch 589: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.81 
2024-02-05 21:27:12,046 EPOCH 590
2024-02-05 21:27:14,595 [Epoch: 590 Step: 00039500] Batch Recognition Loss:   0.000143 => Gls Tokens per Sec:     2323 || Batch Translation Loss:   0.037451 => Txt Tokens per Sec:     6208 || Lr: 0.000050
2024-02-05 21:27:17,239 Epoch 590: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.33 
2024-02-05 21:27:17,240 EPOCH 591
2024-02-05 21:27:22,625 Epoch 591: Total Training Recognition Loss 0.27  Total Training Translation Loss 3.52 
2024-02-05 21:27:22,626 EPOCH 592
2024-02-05 21:27:22,786 [Epoch: 592 Step: 00039600] Batch Recognition Loss:   0.000336 => Gls Tokens per Sec:     3014 || Batch Translation Loss:   0.067349 => Txt Tokens per Sec:     6573 || Lr: 0.000050
2024-02-05 21:27:27,523 Epoch 592: Total Training Recognition Loss 0.29  Total Training Translation Loss 3.46 
2024-02-05 21:27:27,523 EPOCH 593
2024-02-05 21:27:30,590 [Epoch: 593 Step: 00039700] Batch Recognition Loss:   0.224687 => Gls Tokens per Sec:     1847 || Batch Translation Loss:   0.012511 => Txt Tokens per Sec:     4994 || Lr: 0.000050
2024-02-05 21:27:32,981 Epoch 593: Total Training Recognition Loss 0.38  Total Training Translation Loss 2.59 
2024-02-05 21:27:32,981 EPOCH 594
2024-02-05 21:27:37,824 Epoch 594: Total Training Recognition Loss 0.29  Total Training Translation Loss 2.56 
2024-02-05 21:27:37,825 EPOCH 595
2024-02-05 21:27:37,992 [Epoch: 595 Step: 00039800] Batch Recognition Loss:   0.000168 => Gls Tokens per Sec:     1928 || Batch Translation Loss:   0.015421 => Txt Tokens per Sec:     4494 || Lr: 0.000050
2024-02-05 21:27:43,370 Epoch 595: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.73 
2024-02-05 21:27:43,370 EPOCH 596
2024-02-05 21:27:45,777 [Epoch: 596 Step: 00039900] Batch Recognition Loss:   0.000176 => Gls Tokens per Sec:     2328 || Batch Translation Loss:   0.020135 => Txt Tokens per Sec:     6303 || Lr: 0.000050
2024-02-05 21:27:48,295 Epoch 596: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.88 
2024-02-05 21:27:48,296 EPOCH 597
2024-02-05 21:27:53,707 Epoch 597: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.22 
2024-02-05 21:27:53,707 EPOCH 598
2024-02-05 21:27:53,754 [Epoch: 598 Step: 00040000] Batch Recognition Loss:   0.000258 => Gls Tokens per Sec:     3479 || Batch Translation Loss:   0.007858 => Txt Tokens per Sec:     6153 || Lr: 0.000050
2024-02-05 21:28:02,336 Validation result at epoch 598, step    40000: duration: 8.5811s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 3.19013	Translation Loss: 90265.48438	PPL: 8231.54492
	Eval Metric: BLEU
	WER 4.03	(DEL: 0.00,	INS: 0.00,	SUB: 4.03)
	BLEU-4 0.70	(BLEU-1: 11.04,	BLEU-2: 3.50,	BLEU-3: 1.41,	BLEU-4: 0.70)
	CHRF 17.15	ROUGE 9.57
2024-02-05 21:28:02,337 Logging Recognition and Translation Outputs
2024-02-05 21:28:02,337 ========================================================================================================================
2024-02-05 21:28:02,337 Logging Sequence: 86_84.00
2024-02-05 21:28:02,338 	Gloss Reference :	A B+C+D+E
2024-02-05 21:28:02,338 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:28:02,338 	Gloss Alignment :	         
2024-02-05 21:28:02,338 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:28:02,339 	Text Reference  :	amassing 8933 runs which included 21   centuries with a     highest score of    201     not   out   
2024-02-05 21:28:02,339 	Text Hypothesis :	******** **** **** ***** he       also served    as   coach of      the   uttar pradesh ranji trophy
2024-02-05 21:28:02,339 	Text Alignment  :	D        D    D    D     S        S    S         S    S     S       S     S     S       S     S     
2024-02-05 21:28:02,340 ========================================================================================================================
2024-02-05 21:28:02,340 Logging Sequence: 179_110.00
2024-02-05 21:28:02,340 	Gloss Reference :	A B+C+D+E
2024-02-05 21:28:02,340 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:28:02,340 	Gloss Alignment :	         
2024-02-05 21:28:02,340 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:28:02,342 	Text Reference  :	*** *** *** *********** phogat    refused to     stay in   the    same  room with   other indian female wrestlers
2024-02-05 21:28:02,342 	Text Hypothesis :	the wfi has temporarily suspended vinesh  phogat on   10th august after her  return from  the    tokyo  olympics 
2024-02-05 21:28:02,342 	Text Alignment  :	I   I   I   I           S         S       S      S    S    S      S     S    S      S     S      S      S        
2024-02-05 21:28:02,342 ========================================================================================================================
2024-02-05 21:28:02,342 Logging Sequence: 102_2.00
2024-02-05 21:28:02,342 	Gloss Reference :	A B+C+D+E    
2024-02-05 21:28:02,342 	Gloss Hypothesis:	A B+C+D+E+D+E
2024-02-05 21:28:02,342 	Gloss Alignment :	  S          
2024-02-05 21:28:02,343 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:28:02,344 	Text Reference  :	commonwealth games are among the  world's most recognised gaming championships after the ***** olympics
2024-02-05 21:28:02,344 	Text Hypothesis :	for          the   4   days  from 8th     june 2023       on     instagram     with  the tokyo olympics
2024-02-05 21:28:02,344 	Text Alignment  :	S            S     S   S     S    S       S    S          S      S             S         I             
2024-02-05 21:28:02,344 ========================================================================================================================
2024-02-05 21:28:02,344 Logging Sequence: 60_195.00
2024-02-05 21:28:02,344 	Gloss Reference :	A B+C+D+E
2024-02-05 21:28:02,345 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:28:02,345 	Gloss Alignment :	         
2024-02-05 21:28:02,345 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:28:02,346 	Text Reference  :	** people loved to     watch    his  aggressive expressions and his  bowling 
2024-02-05 21:28:02,346 	Text Hypothesis :	if you    are   having problems with support    old         he  then returned
2024-02-05 21:28:02,346 	Text Alignment  :	I  S      S     S      S        S    S          S           S   S    S       
2024-02-05 21:28:02,346 ========================================================================================================================
2024-02-05 21:28:02,346 Logging Sequence: 70_200.00
2024-02-05 21:28:02,346 	Gloss Reference :	A B+C+D+E
2024-02-05 21:28:02,346 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:28:02,346 	Gloss Alignment :	         
2024-02-05 21:28:02,347 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:28:02,347 	Text Reference  :	** * *** showing ronaldo whole-heartedly endorsing the brand
2024-02-05 21:28:02,347 	Text Hypothesis :	in a few days    later   on              24        may 2023 
2024-02-05 21:28:02,347 	Text Alignment  :	I  I I   S       S       S               S         S   S    
2024-02-05 21:28:02,347 ========================================================================================================================
2024-02-05 21:28:07,713 Epoch 598: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.65 
2024-02-05 21:28:07,714 EPOCH 599
2024-02-05 21:28:10,140 [Epoch: 599 Step: 00040100] Batch Recognition Loss:   0.000201 => Gls Tokens per Sec:     2243 || Batch Translation Loss:   0.022822 => Txt Tokens per Sec:     6171 || Lr: 0.000050
2024-02-05 21:28:12,978 Epoch 599: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.58 
2024-02-05 21:28:12,978 EPOCH 600
2024-02-05 21:28:18,430 [Epoch: 600 Step: 00040200] Batch Recognition Loss:   0.000186 => Gls Tokens per Sec:     1949 || Batch Translation Loss:   0.019349 => Txt Tokens per Sec:     5391 || Lr: 0.000050
2024-02-05 21:28:18,430 Epoch 600: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.90 
2024-02-05 21:28:18,430 EPOCH 601
2024-02-05 21:28:23,544 Epoch 601: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.06 
2024-02-05 21:28:23,544 EPOCH 602
2024-02-05 21:28:26,455 [Epoch: 602 Step: 00040300] Batch Recognition Loss:   0.000128 => Gls Tokens per Sec:     1814 || Batch Translation Loss:   0.028906 => Txt Tokens per Sec:     5142 || Lr: 0.000050
2024-02-05 21:28:28,995 Epoch 602: Total Training Recognition Loss 0.08  Total Training Translation Loss 3.51 
2024-02-05 21:28:28,995 EPOCH 603
2024-02-05 21:28:34,387 [Epoch: 603 Step: 00040400] Batch Recognition Loss:   0.000122 => Gls Tokens per Sec:     1940 || Batch Translation Loss:   0.027195 => Txt Tokens per Sec:     5340 || Lr: 0.000050
2024-02-05 21:28:34,492 Epoch 603: Total Training Recognition Loss 0.17  Total Training Translation Loss 3.62 
2024-02-05 21:28:34,492 EPOCH 604
2024-02-05 21:28:39,785 Epoch 604: Total Training Recognition Loss 0.85  Total Training Translation Loss 1.99 
2024-02-05 21:28:39,785 EPOCH 605
2024-02-05 21:28:41,976 [Epoch: 605 Step: 00040500] Batch Recognition Loss:   0.000842 => Gls Tokens per Sec:     2338 || Batch Translation Loss:   0.034564 => Txt Tokens per Sec:     6507 || Lr: 0.000050
2024-02-05 21:28:44,802 Epoch 605: Total Training Recognition Loss 0.33  Total Training Translation Loss 2.26 
2024-02-05 21:28:44,802 EPOCH 606
2024-02-05 21:28:49,785 [Epoch: 606 Step: 00040600] Batch Recognition Loss:   0.001304 => Gls Tokens per Sec:     2067 || Batch Translation Loss:   0.036440 => Txt Tokens per Sec:     5715 || Lr: 0.000050
2024-02-05 21:28:49,941 Epoch 606: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.90 
2024-02-05 21:28:49,941 EPOCH 607
2024-02-05 21:28:55,194 Epoch 607: Total Training Recognition Loss 0.09  Total Training Translation Loss 2.35 
2024-02-05 21:28:55,195 EPOCH 608
2024-02-05 21:28:57,545 [Epoch: 608 Step: 00040700] Batch Recognition Loss:   0.000784 => Gls Tokens per Sec:     2113 || Batch Translation Loss:   0.015172 => Txt Tokens per Sec:     5753 || Lr: 0.000050
2024-02-05 21:29:00,408 Epoch 608: Total Training Recognition Loss 0.10  Total Training Translation Loss 2.06 
2024-02-05 21:29:00,408 EPOCH 609
2024-02-05 21:29:05,221 [Epoch: 609 Step: 00040800] Batch Recognition Loss:   0.000218 => Gls Tokens per Sec:     2107 || Batch Translation Loss:   0.023934 => Txt Tokens per Sec:     5825 || Lr: 0.000050
2024-02-05 21:29:05,571 Epoch 609: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.80 
2024-02-05 21:29:05,571 EPOCH 610
2024-02-05 21:29:10,641 Epoch 610: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.07 
2024-02-05 21:29:10,641 EPOCH 611
2024-02-05 21:29:13,153 [Epoch: 611 Step: 00040900] Batch Recognition Loss:   0.000281 => Gls Tokens per Sec:     1872 || Batch Translation Loss:   0.020577 => Txt Tokens per Sec:     5263 || Lr: 0.000050
2024-02-05 21:29:16,048 Epoch 611: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.58 
2024-02-05 21:29:16,048 EPOCH 612
2024-02-05 21:29:20,790 [Epoch: 612 Step: 00041000] Batch Recognition Loss:   0.000133 => Gls Tokens per Sec:     2105 || Batch Translation Loss:   0.015772 => Txt Tokens per Sec:     5806 || Lr: 0.000050
2024-02-05 21:29:21,049 Epoch 612: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.07 
2024-02-05 21:29:21,050 EPOCH 613
2024-02-05 21:29:26,467 Epoch 613: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.52 
2024-02-05 21:29:26,467 EPOCH 614
2024-02-05 21:29:28,352 [Epoch: 614 Step: 00041100] Batch Recognition Loss:   0.000142 => Gls Tokens per Sec:     2462 || Batch Translation Loss:   0.031809 => Txt Tokens per Sec:     6477 || Lr: 0.000050
2024-02-05 21:29:31,344 Epoch 614: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.39 
2024-02-05 21:29:31,344 EPOCH 615
2024-02-05 21:29:36,517 [Epoch: 615 Step: 00041200] Batch Recognition Loss:   0.000206 => Gls Tokens per Sec:     1899 || Batch Translation Loss:   0.049142 => Txt Tokens per Sec:     5235 || Lr: 0.000050
2024-02-05 21:29:36,924 Epoch 615: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.11 
2024-02-05 21:29:36,925 EPOCH 616
2024-02-05 21:29:41,898 Epoch 616: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.24 
2024-02-05 21:29:41,899 EPOCH 617
2024-02-05 21:29:44,385 [Epoch: 617 Step: 00041300] Batch Recognition Loss:   0.000758 => Gls Tokens per Sec:     1804 || Batch Translation Loss:   0.066139 => Txt Tokens per Sec:     4984 || Lr: 0.000050
2024-02-05 21:29:47,491 Epoch 617: Total Training Recognition Loss 0.04  Total Training Translation Loss 5.20 
2024-02-05 21:29:47,491 EPOCH 618
2024-02-05 21:29:51,983 [Epoch: 618 Step: 00041400] Batch Recognition Loss:   0.000353 => Gls Tokens per Sec:     2151 || Batch Translation Loss:   0.055004 => Txt Tokens per Sec:     5961 || Lr: 0.000050
2024-02-05 21:29:52,450 Epoch 618: Total Training Recognition Loss 0.07  Total Training Translation Loss 4.78 
2024-02-05 21:29:52,451 EPOCH 619
2024-02-05 21:29:57,786 Epoch 619: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.67 
2024-02-05 21:29:57,786 EPOCH 620
2024-02-05 21:29:59,831 [Epoch: 620 Step: 00041500] Batch Recognition Loss:   0.000331 => Gls Tokens per Sec:     2064 || Batch Translation Loss:   0.019080 => Txt Tokens per Sec:     5756 || Lr: 0.000050
2024-02-05 21:30:02,843 Epoch 620: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.39 
2024-02-05 21:30:02,844 EPOCH 621
2024-02-05 21:30:07,542 [Epoch: 621 Step: 00041600] Batch Recognition Loss:   0.000211 => Gls Tokens per Sec:     2023 || Batch Translation Loss:   0.026038 => Txt Tokens per Sec:     5572 || Lr: 0.000050
2024-02-05 21:30:08,094 Epoch 621: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.72 
2024-02-05 21:30:08,094 EPOCH 622
2024-02-05 21:30:13,141 Epoch 622: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.52 
2024-02-05 21:30:13,142 EPOCH 623
2024-02-05 21:30:15,320 [Epoch: 623 Step: 00041700] Batch Recognition Loss:   0.000254 => Gls Tokens per Sec:     1911 || Batch Translation Loss:   0.036897 => Txt Tokens per Sec:     5464 || Lr: 0.000050
2024-02-05 21:30:18,367 Epoch 623: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.75 
2024-02-05 21:30:18,367 EPOCH 624
2024-02-05 21:30:22,759 [Epoch: 624 Step: 00041800] Batch Recognition Loss:   0.000356 => Gls Tokens per Sec:     2127 || Batch Translation Loss:   0.021284 => Txt Tokens per Sec:     5811 || Lr: 0.000050
2024-02-05 21:30:23,544 Epoch 624: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.43 
2024-02-05 21:30:23,544 EPOCH 625
2024-02-05 21:30:28,655 Epoch 625: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.37 
2024-02-05 21:30:28,656 EPOCH 626
2024-02-05 21:30:30,390 [Epoch: 626 Step: 00041900] Batch Recognition Loss:   0.000201 => Gls Tokens per Sec:     2308 || Batch Translation Loss:   0.021240 => Txt Tokens per Sec:     6454 || Lr: 0.000050
2024-02-05 21:30:33,851 Epoch 626: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.15 
2024-02-05 21:30:33,852 EPOCH 627
2024-02-05 21:30:38,331 [Epoch: 627 Step: 00042000] Batch Recognition Loss:   0.000137 => Gls Tokens per Sec:     2050 || Batch Translation Loss:   0.017154 => Txt Tokens per Sec:     5728 || Lr: 0.000050
2024-02-05 21:30:46,927 Hooray! New best validation result [eval_metric]!
2024-02-05 21:30:46,928 Saving new checkpoint.
2024-02-05 21:30:47,219 Validation result at epoch 627, step    42000: duration: 8.8873s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.86793	Translation Loss: 90841.21875	PPL: 8718.77148
	Eval Metric: BLEU
	WER 3.88	(DEL: 0.00,	INS: 0.00,	SUB: 3.88)
	BLEU-4 0.89	(BLEU-1: 10.95,	BLEU-2: 3.73,	BLEU-3: 1.65,	BLEU-4: 0.89)
	CHRF 17.08	ROUGE 9.39
2024-02-05 21:30:47,220 Logging Recognition and Translation Outputs
2024-02-05 21:30:47,220 ========================================================================================================================
2024-02-05 21:30:47,220 Logging Sequence: 154_94.00
2024-02-05 21:30:47,220 	Gloss Reference :	A B+C+D+E
2024-02-05 21:30:47,220 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:30:47,220 	Gloss Alignment :	         
2024-02-05 21:30:47,221 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:30:47,222 	Text Reference  :	**** the ipl will also  be  held  in uae from september 19 to october 15       
2024-02-05 21:30:47,222 	Text Hypothesis :	have won a   t20  world cup match in *** **** ********* ** ** ******* ahmedabad
2024-02-05 21:30:47,222 	Text Alignment  :	I    S   S   S    S     S   S        D   D    D         D  D  D       S        
2024-02-05 21:30:47,222 ========================================================================================================================
2024-02-05 21:30:47,222 Logging Sequence: 118_2.00
2024-02-05 21:30:47,222 	Gloss Reference :	A B+C+D+E
2024-02-05 21:30:47,223 	Gloss Hypothesis:	A B+C+D  
2024-02-05 21:30:47,223 	Gloss Alignment :	  S      
2024-02-05 21:30:47,223 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:30:47,224 	Text Reference  :	yesterday was a   very exciting day people across the   world were   watching
2024-02-05 21:30:47,224 	Text Hypothesis :	********* you all know that     the match  was    going on    social media   
2024-02-05 21:30:47,224 	Text Alignment  :	D         S   S   S    S        S   S      S      S     S     S      S       
2024-02-05 21:30:47,224 ========================================================================================================================
2024-02-05 21:30:47,224 Logging Sequence: 165_453.00
2024-02-05 21:30:47,224 	Gloss Reference :	A B+C+D+E
2024-02-05 21:30:47,225 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:30:47,225 	Gloss Alignment :	         
2024-02-05 21:30:47,225 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:30:47,226 	Text Reference  :	** icc    did  not agree to sehwag' decision of wearing a   numberless jersey
2024-02-05 21:30:47,226 	Text Hypothesis :	he handed over the cup   to ******* ******** ** ******* his csk        team  
2024-02-05 21:30:47,226 	Text Alignment  :	I  S      S    S   S        D       D        D  D       S   S          S     
2024-02-05 21:30:47,226 ========================================================================================================================
2024-02-05 21:30:47,226 Logging Sequence: 126_163.00
2024-02-05 21:30:47,226 	Gloss Reference :	A B+C+D+E
2024-02-05 21:30:47,226 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:30:47,226 	Gloss Alignment :	         
2024-02-05 21:30:47,227 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:30:47,227 	Text Reference  :	your hard work has   helped secure a    medal    at     the tokyo olympics
2024-02-05 21:30:47,227 	Text Hypothesis :	**** **** **** those who    is     that everyone wanted to  do    anything
2024-02-05 21:30:47,228 	Text Alignment  :	D    D    D    S     S      S      S    S        S      S   S     S       
2024-02-05 21:30:47,228 ========================================================================================================================
2024-02-05 21:30:47,228 Logging Sequence: 84_2.00
2024-02-05 21:30:47,228 	Gloss Reference :	A B+C+D+E  
2024-02-05 21:30:47,228 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-05 21:30:47,228 	Gloss Alignment :	  S        
2024-02-05 21:30:47,228 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:30:47,230 	Text Reference  :	the 2022 fifa football world     cup is   going on  in        qatar from      20th november 2022 to    18th december 2022
2024-02-05 21:30:47,230 	Text Hypothesis :	*** **** **** ******** according to  shah this  was incorrect be    respected and  posted   a    crime of   51       runs
2024-02-05 21:30:47,230 	Text Alignment  :	D   D    D    D        S         S   S    S     S   S         S     S         S    S        S    S     S    S        S   
2024-02-05 21:30:47,230 ========================================================================================================================
2024-02-05 21:30:47,897 Epoch 627: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.08 
2024-02-05 21:30:47,897 EPOCH 628
2024-02-05 21:30:53,428 Epoch 628: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.10 
2024-02-05 21:30:53,429 EPOCH 629
2024-02-05 21:30:55,303 [Epoch: 629 Step: 00042100] Batch Recognition Loss:   0.000425 => Gls Tokens per Sec:     2050 || Batch Translation Loss:   0.019207 => Txt Tokens per Sec:     5586 || Lr: 0.000050
2024-02-05 21:30:58,733 Epoch 629: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.22 
2024-02-05 21:30:58,733 EPOCH 630
2024-02-05 21:31:03,279 [Epoch: 630 Step: 00042200] Batch Recognition Loss:   0.000337 => Gls Tokens per Sec:     2007 || Batch Translation Loss:   0.132495 => Txt Tokens per Sec:     5579 || Lr: 0.000050
2024-02-05 21:31:04,019 Epoch 630: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.68 
2024-02-05 21:31:04,019 EPOCH 631
2024-02-05 21:31:08,987 Epoch 631: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.93 
2024-02-05 21:31:08,988 EPOCH 632
2024-02-05 21:31:10,654 [Epoch: 632 Step: 00042300] Batch Recognition Loss:   0.002508 => Gls Tokens per Sec:     2150 || Batch Translation Loss:   0.036426 => Txt Tokens per Sec:     5858 || Lr: 0.000050
2024-02-05 21:31:14,291 Epoch 632: Total Training Recognition Loss 0.08  Total Training Translation Loss 4.11 
2024-02-05 21:31:14,292 EPOCH 633
2024-02-05 21:31:18,584 [Epoch: 633 Step: 00042400] Batch Recognition Loss:   0.000987 => Gls Tokens per Sec:     2065 || Batch Translation Loss:   0.065745 => Txt Tokens per Sec:     5698 || Lr: 0.000050
2024-02-05 21:31:19,435 Epoch 633: Total Training Recognition Loss 0.07  Total Training Translation Loss 4.82 
2024-02-05 21:31:19,435 EPOCH 634
2024-02-05 21:31:24,853 Epoch 634: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.61 
2024-02-05 21:31:24,853 EPOCH 635
2024-02-05 21:31:26,557 [Epoch: 635 Step: 00042500] Batch Recognition Loss:   0.000189 => Gls Tokens per Sec:     2067 || Batch Translation Loss:   0.016599 => Txt Tokens per Sec:     5981 || Lr: 0.000050
2024-02-05 21:31:29,644 Epoch 635: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.02 
2024-02-05 21:31:29,644 EPOCH 636
2024-02-05 21:31:34,171 [Epoch: 636 Step: 00042600] Batch Recognition Loss:   0.000201 => Gls Tokens per Sec:     1922 || Batch Translation Loss:   0.013406 => Txt Tokens per Sec:     5322 || Lr: 0.000050
2024-02-05 21:31:35,103 Epoch 636: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.66 
2024-02-05 21:31:35,103 EPOCH 637
2024-02-05 21:31:40,065 Epoch 637: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.54 
2024-02-05 21:31:40,066 EPOCH 638
2024-02-05 21:31:41,761 [Epoch: 638 Step: 00042700] Batch Recognition Loss:   0.000335 => Gls Tokens per Sec:     1983 || Batch Translation Loss:   0.022370 => Txt Tokens per Sec:     5326 || Lr: 0.000050
2024-02-05 21:31:45,544 Epoch 638: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.45 
2024-02-05 21:31:45,544 EPOCH 639
2024-02-05 21:31:49,516 [Epoch: 639 Step: 00042800] Batch Recognition Loss:   0.001882 => Gls Tokens per Sec:     2151 || Batch Translation Loss:   0.029106 => Txt Tokens per Sec:     6032 || Lr: 0.000050
2024-02-05 21:31:50,528 Epoch 639: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.64 
2024-02-05 21:31:50,528 EPOCH 640
2024-02-05 21:31:55,924 Epoch 640: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.00 
2024-02-05 21:31:55,925 EPOCH 641
2024-02-05 21:31:57,199 [Epoch: 641 Step: 00042900] Batch Recognition Loss:   0.000800 => Gls Tokens per Sec:     2433 || Batch Translation Loss:   0.026042 => Txt Tokens per Sec:     6595 || Lr: 0.000050
2024-02-05 21:32:01,057 Epoch 641: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.42 
2024-02-05 21:32:01,058 EPOCH 642
2024-02-05 21:32:05,370 [Epoch: 642 Step: 00043000] Batch Recognition Loss:   0.000357 => Gls Tokens per Sec:     1944 || Batch Translation Loss:   0.032736 => Txt Tokens per Sec:     5329 || Lr: 0.000050
2024-02-05 21:32:06,482 Epoch 642: Total Training Recognition Loss 0.05  Total Training Translation Loss 4.42 
2024-02-05 21:32:06,483 EPOCH 643
2024-02-05 21:32:11,204 Epoch 643: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.74 
2024-02-05 21:32:11,205 EPOCH 644
2024-02-05 21:32:12,751 [Epoch: 644 Step: 00043100] Batch Recognition Loss:   0.000233 => Gls Tokens per Sec:     1968 || Batch Translation Loss:   0.018743 => Txt Tokens per Sec:     5361 || Lr: 0.000050
2024-02-05 21:32:16,603 Epoch 644: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.85 
2024-02-05 21:32:16,603 EPOCH 645
2024-02-05 21:32:20,526 [Epoch: 645 Step: 00043200] Batch Recognition Loss:   0.000203 => Gls Tokens per Sec:     2096 || Batch Translation Loss:   0.011008 => Txt Tokens per Sec:     5745 || Lr: 0.000050
2024-02-05 21:32:21,835 Epoch 645: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.38 
2024-02-05 21:32:21,836 EPOCH 646
2024-02-05 21:32:27,247 Epoch 646: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.47 
2024-02-05 21:32:27,248 EPOCH 647
2024-02-05 21:32:28,618 [Epoch: 647 Step: 00043300] Batch Recognition Loss:   0.000171 => Gls Tokens per Sec:     2103 || Batch Translation Loss:   0.114849 => Txt Tokens per Sec:     5867 || Lr: 0.000050
2024-02-05 21:32:32,493 Epoch 647: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.43 
2024-02-05 21:32:32,494 EPOCH 648
2024-02-05 21:32:36,407 [Epoch: 648 Step: 00043400] Batch Recognition Loss:   0.000398 => Gls Tokens per Sec:     2086 || Batch Translation Loss:   0.012162 => Txt Tokens per Sec:     5823 || Lr: 0.000050
2024-02-05 21:32:37,659 Epoch 648: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.11 
2024-02-05 21:32:37,659 EPOCH 649
2024-02-05 21:32:42,796 Epoch 649: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.78 
2024-02-05 21:32:42,796 EPOCH 650
2024-02-05 21:32:44,092 [Epoch: 650 Step: 00043500] Batch Recognition Loss:   0.000875 => Gls Tokens per Sec:     2101 || Batch Translation Loss:   0.022807 => Txt Tokens per Sec:     5656 || Lr: 0.000050
2024-02-05 21:32:47,931 Epoch 650: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.48 
2024-02-05 21:32:47,931 EPOCH 651
2024-02-05 21:32:51,888 [Epoch: 651 Step: 00043600] Batch Recognition Loss:   0.000206 => Gls Tokens per Sec:     1998 || Batch Translation Loss:   0.018524 => Txt Tokens per Sec:     5509 || Lr: 0.000050
2024-02-05 21:32:53,274 Epoch 651: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.50 
2024-02-05 21:32:53,274 EPOCH 652
2024-02-05 21:32:58,302 Epoch 652: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.65 
2024-02-05 21:32:58,302 EPOCH 653
2024-02-05 21:32:59,552 [Epoch: 653 Step: 00043700] Batch Recognition Loss:   0.000214 => Gls Tokens per Sec:     2050 || Batch Translation Loss:   0.015783 => Txt Tokens per Sec:     5521 || Lr: 0.000050
2024-02-05 21:33:03,741 Epoch 653: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.96 
2024-02-05 21:33:03,742 EPOCH 654
2024-02-05 21:33:07,140 [Epoch: 654 Step: 00043800] Batch Recognition Loss:   0.000482 => Gls Tokens per Sec:     2278 || Batch Translation Loss:   0.031304 => Txt Tokens per Sec:     6321 || Lr: 0.000050
2024-02-05 21:33:08,418 Epoch 654: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.97 
2024-02-05 21:33:08,418 EPOCH 655
2024-02-05 21:33:14,098 Epoch 655: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.66 
2024-02-05 21:33:14,098 EPOCH 656
2024-02-05 21:33:15,159 [Epoch: 656 Step: 00043900] Batch Recognition Loss:   0.000262 => Gls Tokens per Sec:     2172 || Batch Translation Loss:   0.009812 => Txt Tokens per Sec:     5710 || Lr: 0.000050
2024-02-05 21:33:19,042 Epoch 656: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.91 
2024-02-05 21:33:19,043 EPOCH 657
2024-02-05 21:33:22,985 [Epoch: 657 Step: 00044000] Batch Recognition Loss:   0.000319 => Gls Tokens per Sec:     1924 || Batch Translation Loss:   0.012211 => Txt Tokens per Sec:     5341 || Lr: 0.000050
2024-02-05 21:33:31,616 Validation result at epoch 657, step    44000: duration: 8.6313s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 3.05679	Translation Loss: 90265.29688	PPL: 8231.39551
	Eval Metric: BLEU
	WER 3.81	(DEL: 0.00,	INS: 0.00,	SUB: 3.81)
	BLEU-4 0.52	(BLEU-1: 10.10,	BLEU-2: 2.87,	BLEU-3: 1.19,	BLEU-4: 0.52)
	CHRF 16.98	ROUGE 8.33
2024-02-05 21:33:31,617 Logging Recognition and Translation Outputs
2024-02-05 21:33:31,617 ========================================================================================================================
2024-02-05 21:33:31,617 Logging Sequence: 57_104.00
2024-02-05 21:33:31,618 	Gloss Reference :	A B+C+D+E
2024-02-05 21:33:31,618 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:33:31,618 	Gloss Alignment :	         
2024-02-05 21:33:31,618 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:33:31,620 	Text Reference  :	the next day kohli and kl   rahul continued from  where  they had   left     and displayed amazing batting performance without losing their wickets
2024-02-05 21:33:31,620 	Text Hypothesis :	*** **** *** ***** *** they have  hiked     their travel to   delhi capitals and ********* ******* ******* india       also    asked  for   them   
2024-02-05 21:33:31,620 	Text Alignment  :	D   D    D   D     D   S    S     S         S     S      S    S     S            D         D       D       S           S       S      S     S      
2024-02-05 21:33:31,620 ========================================================================================================================
2024-02-05 21:33:31,621 Logging Sequence: 136_64.00
2024-02-05 21:33:31,621 	Gloss Reference :	A B+C+D+E
2024-02-05 21:33:31,621 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:33:31,621 	Gloss Alignment :	         
2024-02-05 21:33:31,621 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:33:31,622 	Text Reference  :	in all she  has   won 2   medals
2024-02-05 21:33:31,622 	Text Hypothesis :	i  am  very sorry it  was done  
2024-02-05 21:33:31,622 	Text Alignment  :	S  S   S    S     S   S   S     
2024-02-05 21:33:31,622 ========================================================================================================================
2024-02-05 21:33:31,622 Logging Sequence: 54_123.00
2024-02-05 21:33:31,622 	Gloss Reference :	A B+C+D+E
2024-02-05 21:33:31,622 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:33:31,623 	Gloss Alignment :	         
2024-02-05 21:33:31,623 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:33:31,624 	Text Reference  :	**** ** vips    sponsors international cricket  groups have already booked their hotel rooms 
2024-02-05 21:33:31,624 	Text Hypothesis :	this is because of       the           covid-19 will   have to      see    their ***** choice
2024-02-05 21:33:31,624 	Text Alignment  :	I    I  S       S        S             S        S           S       S            D     S     
2024-02-05 21:33:31,624 ========================================================================================================================
2024-02-05 21:33:31,624 Logging Sequence: 168_115.00
2024-02-05 21:33:31,624 	Gloss Reference :	A B+C+D+E
2024-02-05 21:33:31,625 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:33:31,625 	Gloss Alignment :	         
2024-02-05 21:33:31,625 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:33:31,625 	Text Reference  :	****** **** this   has sparked a     major discussion on   social media
2024-02-05 21:33:31,626 	Text Hypothesis :	people were amazed by  his     child but   are        only 56     runs 
2024-02-05 21:33:31,626 	Text Alignment  :	I      I    S      S   S       S     S     S          S    S      S    
2024-02-05 21:33:31,626 ========================================================================================================================
2024-02-05 21:33:31,626 Logging Sequence: 121_132.00
2024-02-05 21:33:31,626 	Gloss Reference :	A B+C+D+E
2024-02-05 21:33:31,626 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:33:31,626 	Gloss Alignment :	         
2024-02-05 21:33:31,627 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:33:31,628 	Text Reference  :	which is why they will be       retesting her to  check if   she   consumed any stamina enhancing drugs     
2024-02-05 21:33:31,628 	Text Hypothesis :	***** ** as  per  the  olympics rules     if  hou has   been found to       be  using   such      substances
2024-02-05 21:33:31,628 	Text Alignment  :	D     D  S   S    S    S        S         S   S   S     S    S     S        S   S       S         S         
2024-02-05 21:33:31,628 ========================================================================================================================
2024-02-05 21:33:33,109 Epoch 657: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.92 
2024-02-05 21:33:33,109 EPOCH 658
2024-02-05 21:33:38,335 Epoch 658: Total Training Recognition Loss 0.04  Total Training Translation Loss 6.66 
2024-02-05 21:33:38,335 EPOCH 659
2024-02-05 21:33:39,521 [Epoch: 659 Step: 00044100] Batch Recognition Loss:   0.000389 => Gls Tokens per Sec:     1891 || Batch Translation Loss:   0.116945 => Txt Tokens per Sec:     5202 || Lr: 0.000050
2024-02-05 21:33:43,811 Epoch 659: Total Training Recognition Loss 0.07  Total Training Translation Loss 6.36 
2024-02-05 21:33:43,811 EPOCH 660
2024-02-05 21:33:47,031 [Epoch: 660 Step: 00044200] Batch Recognition Loss:   0.000661 => Gls Tokens per Sec:     2336 || Batch Translation Loss:   0.025018 => Txt Tokens per Sec:     6395 || Lr: 0.000050
2024-02-05 21:33:48,855 Epoch 660: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.34 
2024-02-05 21:33:48,856 EPOCH 661
2024-02-05 21:33:54,020 Epoch 661: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.04 
2024-02-05 21:33:54,020 EPOCH 662
2024-02-05 21:33:54,956 [Epoch: 662 Step: 00044300] Batch Recognition Loss:   0.000258 => Gls Tokens per Sec:     2224 || Batch Translation Loss:   0.015121 => Txt Tokens per Sec:     6383 || Lr: 0.000050
2024-02-05 21:33:59,163 Epoch 662: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.75 
2024-02-05 21:33:59,163 EPOCH 663
2024-02-05 21:34:02,576 [Epoch: 663 Step: 00044400] Batch Recognition Loss:   0.000215 => Gls Tokens per Sec:     2157 || Batch Translation Loss:   0.012805 => Txt Tokens per Sec:     5941 || Lr: 0.000050
2024-02-05 21:34:04,298 Epoch 663: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.99 
2024-02-05 21:34:04,298 EPOCH 664
2024-02-05 21:34:09,386 Epoch 664: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.36 
2024-02-05 21:34:09,387 EPOCH 665
2024-02-05 21:34:10,341 [Epoch: 665 Step: 00044500] Batch Recognition Loss:   0.000681 => Gls Tokens per Sec:     1910 || Batch Translation Loss:   0.018899 => Txt Tokens per Sec:     5108 || Lr: 0.000050
2024-02-05 21:34:14,740 Epoch 665: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.28 
2024-02-05 21:34:14,740 EPOCH 666
2024-02-05 21:34:18,362 [Epoch: 666 Step: 00044600] Batch Recognition Loss:   0.000667 => Gls Tokens per Sec:     1961 || Batch Translation Loss:   0.047669 => Txt Tokens per Sec:     5443 || Lr: 0.000050
2024-02-05 21:34:20,015 Epoch 666: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.83 
2024-02-05 21:34:20,015 EPOCH 667
2024-02-05 21:34:25,022 Epoch 667: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.71 
2024-02-05 21:34:25,022 EPOCH 668
2024-02-05 21:34:25,757 [Epoch: 668 Step: 00044700] Batch Recognition Loss:   0.000217 => Gls Tokens per Sec:     2260 || Batch Translation Loss:   0.016273 => Txt Tokens per Sec:     6145 || Lr: 0.000050
2024-02-05 21:34:30,455 Epoch 668: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.67 
2024-02-05 21:34:30,456 EPOCH 669
2024-02-05 21:34:33,736 [Epoch: 669 Step: 00044800] Batch Recognition Loss:   0.000114 => Gls Tokens per Sec:     2147 || Batch Translation Loss:   0.014168 => Txt Tokens per Sec:     6028 || Lr: 0.000050
2024-02-05 21:34:35,487 Epoch 669: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.86 
2024-02-05 21:34:35,487 EPOCH 670
2024-02-05 21:34:40,988 Epoch 670: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.76 
2024-02-05 21:34:40,988 EPOCH 671
2024-02-05 21:34:41,835 [Epoch: 671 Step: 00044900] Batch Recognition Loss:   0.000256 => Gls Tokens per Sec:     1895 || Batch Translation Loss:   0.019799 => Txt Tokens per Sec:     5815 || Lr: 0.000050
2024-02-05 21:34:45,766 Epoch 671: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.45 
2024-02-05 21:34:45,767 EPOCH 672
2024-02-05 21:34:49,465 [Epoch: 672 Step: 00045000] Batch Recognition Loss:   0.020050 => Gls Tokens per Sec:     1860 || Batch Translation Loss:   0.048306 => Txt Tokens per Sec:     5234 || Lr: 0.000050
2024-02-05 21:34:51,279 Epoch 672: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.70 
2024-02-05 21:34:51,279 EPOCH 673
2024-02-05 21:34:56,252 Epoch 673: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.43 
2024-02-05 21:34:56,252 EPOCH 674
2024-02-05 21:34:57,248 [Epoch: 674 Step: 00045100] Batch Recognition Loss:   0.000283 => Gls Tokens per Sec:     1449 || Batch Translation Loss:   0.021377 => Txt Tokens per Sec:     4433 || Lr: 0.000050
2024-02-05 21:35:01,762 Epoch 674: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.02 
2024-02-05 21:35:01,763 EPOCH 675
2024-02-05 21:35:04,831 [Epoch: 675 Step: 00045200] Batch Recognition Loss:   0.000137 => Gls Tokens per Sec:     2190 || Batch Translation Loss:   0.031051 => Txt Tokens per Sec:     6145 || Lr: 0.000050
2024-02-05 21:35:06,863 Epoch 675: Total Training Recognition Loss 0.06  Total Training Translation Loss 4.77 
2024-02-05 21:35:06,863 EPOCH 676
2024-02-05 21:35:12,242 Epoch 676: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.63 
2024-02-05 21:35:12,243 EPOCH 677
2024-02-05 21:35:12,808 [Epoch: 677 Step: 00045300] Batch Recognition Loss:   0.000358 => Gls Tokens per Sec:     2268 || Batch Translation Loss:   0.046724 => Txt Tokens per Sec:     6132 || Lr: 0.000050
2024-02-05 21:35:17,179 Epoch 677: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.23 
2024-02-05 21:35:17,180 EPOCH 678
2024-02-05 21:35:20,771 [Epoch: 678 Step: 00045400] Batch Recognition Loss:   0.000357 => Gls Tokens per Sec:     1828 || Batch Translation Loss:   0.033501 => Txt Tokens per Sec:     5185 || Lr: 0.000050
2024-02-05 21:35:22,765 Epoch 678: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.71 
2024-02-05 21:35:22,765 EPOCH 679
2024-02-05 21:35:27,805 Epoch 679: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.46 
2024-02-05 21:35:27,805 EPOCH 680
2024-02-05 21:35:28,478 [Epoch: 680 Step: 00045500] Batch Recognition Loss:   0.000317 => Gls Tokens per Sec:     1666 || Batch Translation Loss:   0.023621 => Txt Tokens per Sec:     5003 || Lr: 0.000050
2024-02-05 21:35:33,060 Epoch 680: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.28 
2024-02-05 21:35:33,060 EPOCH 681
2024-02-05 21:35:36,090 [Epoch: 681 Step: 00045600] Batch Recognition Loss:   0.000255 => Gls Tokens per Sec:     2080 || Batch Translation Loss:   0.021326 => Txt Tokens per Sec:     5902 || Lr: 0.000050
2024-02-05 21:35:38,197 Epoch 681: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.70 
2024-02-05 21:35:38,197 EPOCH 682
2024-02-05 21:35:43,424 Epoch 682: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.30 
2024-02-05 21:35:43,425 EPOCH 683
2024-02-05 21:35:43,902 [Epoch: 683 Step: 00045700] Batch Recognition Loss:   0.000148 => Gls Tokens per Sec:     1803 || Batch Translation Loss:   0.029249 => Txt Tokens per Sec:     4579 || Lr: 0.000050
2024-02-05 21:35:48,684 Epoch 683: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.82 
2024-02-05 21:35:48,685 EPOCH 684
2024-02-05 21:35:51,555 [Epoch: 684 Step: 00045800] Batch Recognition Loss:   0.000160 => Gls Tokens per Sec:     2140 || Batch Translation Loss:   0.030743 => Txt Tokens per Sec:     5697 || Lr: 0.000050
2024-02-05 21:35:53,660 Epoch 684: Total Training Recognition Loss 0.04  Total Training Translation Loss 4.44 
2024-02-05 21:35:53,660 EPOCH 685
2024-02-05 21:35:59,021 Epoch 685: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.31 
2024-02-05 21:35:59,022 EPOCH 686
2024-02-05 21:35:59,447 [Epoch: 686 Step: 00045900] Batch Recognition Loss:   0.000249 => Gls Tokens per Sec:     1890 || Batch Translation Loss:   0.028773 => Txt Tokens per Sec:     5735 || Lr: 0.000050
2024-02-05 21:36:03,942 Epoch 686: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.09 
2024-02-05 21:36:03,942 EPOCH 687
2024-02-05 21:36:07,263 [Epoch: 687 Step: 00046000] Batch Recognition Loss:   0.002675 => Gls Tokens per Sec:     1801 || Batch Translation Loss:   0.022184 => Txt Tokens per Sec:     4973 || Lr: 0.000050
2024-02-05 21:36:15,862 Validation result at epoch 687, step    46000: duration: 8.5985s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 3.14381	Translation Loss: 90116.02344	PPL: 8109.57764
	Eval Metric: BLEU
	WER 3.81	(DEL: 0.00,	INS: 0.00,	SUB: 3.81)
	BLEU-4 0.66	(BLEU-1: 10.78,	BLEU-2: 3.37,	BLEU-3: 1.30,	BLEU-4: 0.66)
	CHRF 16.83	ROUGE 9.01
2024-02-05 21:36:15,863 Logging Recognition and Translation Outputs
2024-02-05 21:36:15,863 ========================================================================================================================
2024-02-05 21:36:15,863 Logging Sequence: 87_207.00
2024-02-05 21:36:15,863 	Gloss Reference :	A B+C+D+E
2024-02-05 21:36:15,864 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:36:15,864 	Gloss Alignment :	         
2024-02-05 21:36:15,864 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:36:15,865 	Text Reference  :	there were 2-3  pakistanis who were speaking anti-india things and things  on     kashmir
2024-02-05 21:36:15,865 	Text Hypothesis :	***** **** this is         why he   was      tired      by     the british empire games  
2024-02-05 21:36:15,865 	Text Alignment  :	D     D    S    S          S   S    S        S          S      S   S       S      S      
2024-02-05 21:36:15,865 ========================================================================================================================
2024-02-05 21:36:15,865 Logging Sequence: 67_73.00
2024-02-05 21:36:15,866 	Gloss Reference :	A B+C+D+E
2024-02-05 21:36:15,866 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:36:15,866 	Gloss Alignment :	         
2024-02-05 21:36:15,866 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:36:15,866 	Text Reference  :	in  his   tweet he   also  said 
2024-02-05 21:36:15,867 	Text Hypothesis :	the girls had   made india proud
2024-02-05 21:36:15,867 	Text Alignment  :	S   S     S     S    S     S    
2024-02-05 21:36:15,867 ========================================================================================================================
2024-02-05 21:36:15,867 Logging Sequence: 172_267.00
2024-02-05 21:36:15,867 	Gloss Reference :	A B+C+D+E
2024-02-05 21:36:15,867 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:36:15,867 	Gloss Alignment :	         
2024-02-05 21:36:15,867 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:36:15,868 	Text Reference  :	**** ** **** such    provisions have been made
2024-02-05 21:36:15,868 	Text Hypothesis :	this is just rubbish this       is   all  fake
2024-02-05 21:36:15,868 	Text Alignment  :	I    I  I    S       S          S    S    S   
2024-02-05 21:36:15,868 ========================================================================================================================
2024-02-05 21:36:15,868 Logging Sequence: 144_23.00
2024-02-05 21:36:15,869 	Gloss Reference :	A B+C+D+E
2024-02-05 21:36:15,869 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:36:15,869 	Gloss Alignment :	         
2024-02-05 21:36:15,869 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:36:15,871 	Text Reference  :	the     girl is   14-year-old mumal mehar and     she   is **** **** *** ****** **** from kanasar village of ****** barmer in    rajasthan
2024-02-05 21:36:15,871 	Text Hypothesis :	however they were elated      by    your  victory there is back home but didn't have a    huge    number  of famous by     being time     
2024-02-05 21:36:15,871 	Text Alignment  :	S       S    S    S           S     S     S       S        I    I    I   I      I    S    S       S          I      S      S     S        
2024-02-05 21:36:15,871 ========================================================================================================================
2024-02-05 21:36:15,872 Logging Sequence: 133_202.00
2024-02-05 21:36:15,872 	Gloss Reference :	A B+C+D+E
2024-02-05 21:36:15,872 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:36:15,872 	Gloss Alignment :	         
2024-02-05 21:36:15,872 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:36:15,873 	Text Reference  :	australia has already qualified for   the final if  india wins it     will       face australia
2024-02-05 21:36:15,873 	Text Hypothesis :	********* as  per     the       rules of  ipl   but they  are  always approached for  covid-19 
2024-02-05 21:36:15,873 	Text Alignment  :	D         S   S       S         S     S   S     S   S     S    S      S          S    S        
2024-02-05 21:36:15,874 ========================================================================================================================
2024-02-05 21:36:18,093 Epoch 687: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.53 
2024-02-05 21:36:18,093 EPOCH 688
2024-02-05 21:36:23,068 Epoch 688: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.56 
2024-02-05 21:36:23,068 EPOCH 689
2024-02-05 21:36:23,389 [Epoch: 689 Step: 00046100] Batch Recognition Loss:   0.002081 => Gls Tokens per Sec:     2001 || Batch Translation Loss:   0.017199 => Txt Tokens per Sec:     6155 || Lr: 0.000050
2024-02-05 21:36:28,386 Epoch 689: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.91 
2024-02-05 21:36:28,386 EPOCH 690
2024-02-05 21:36:31,397 [Epoch: 690 Step: 00046200] Batch Recognition Loss:   0.000226 => Gls Tokens per Sec:     1934 || Batch Translation Loss:   0.016368 => Txt Tokens per Sec:     5440 || Lr: 0.000050
2024-02-05 21:36:33,732 Epoch 690: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.18 
2024-02-05 21:36:33,733 EPOCH 691
2024-02-05 21:36:39,128 Epoch 691: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.42 
2024-02-05 21:36:39,128 EPOCH 692
2024-02-05 21:36:39,397 [Epoch: 692 Step: 00046300] Batch Recognition Loss:   0.000222 => Gls Tokens per Sec:     1798 || Batch Translation Loss:   0.029047 => Txt Tokens per Sec:     5805 || Lr: 0.000050
2024-02-05 21:36:44,578 Epoch 692: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.23 
2024-02-05 21:36:44,579 EPOCH 693
2024-02-05 21:36:47,420 [Epoch: 693 Step: 00046400] Batch Recognition Loss:   0.000317 => Gls Tokens per Sec:     1993 || Batch Translation Loss:   0.017855 => Txt Tokens per Sec:     5719 || Lr: 0.000050
2024-02-05 21:36:49,765 Epoch 693: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.39 
2024-02-05 21:36:49,766 EPOCH 694
2024-02-05 21:36:55,340 Epoch 694: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.54 
2024-02-05 21:36:55,341 EPOCH 695
2024-02-05 21:36:55,431 [Epoch: 695 Step: 00046500] Batch Recognition Loss:   0.000108 => Gls Tokens per Sec:     3595 || Batch Translation Loss:   0.016978 => Txt Tokens per Sec:     7269 || Lr: 0.000050
2024-02-05 21:37:00,423 Epoch 695: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.46 
2024-02-05 21:37:00,423 EPOCH 696
2024-02-05 21:37:02,506 [Epoch: 696 Step: 00046600] Batch Recognition Loss:   0.000256 => Gls Tokens per Sec:     2690 || Batch Translation Loss:   0.005791 => Txt Tokens per Sec:     7037 || Lr: 0.000050
2024-02-05 21:37:04,985 Epoch 696: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.83 
2024-02-05 21:37:04,985 EPOCH 697
2024-02-05 21:37:09,560 Epoch 697: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.94 
2024-02-05 21:37:09,561 EPOCH 698
2024-02-05 21:37:09,621 [Epoch: 698 Step: 00046700] Batch Recognition Loss:   0.000312 => Gls Tokens per Sec:     2667 || Batch Translation Loss:   0.025683 => Txt Tokens per Sec:     6867 || Lr: 0.000050
2024-02-05 21:37:14,756 Epoch 698: Total Training Recognition Loss 0.07  Total Training Translation Loss 5.54 
2024-02-05 21:37:14,757 EPOCH 699
2024-02-05 21:37:17,490 [Epoch: 699 Step: 00046800] Batch Recognition Loss:   0.000497 => Gls Tokens per Sec:     1991 || Batch Translation Loss:   0.028816 => Txt Tokens per Sec:     5665 || Lr: 0.000050
2024-02-05 21:37:20,013 Epoch 699: Total Training Recognition Loss 0.05  Total Training Translation Loss 5.41 
2024-02-05 21:37:20,013 EPOCH 700
2024-02-05 21:37:25,647 [Epoch: 700 Step: 00046900] Batch Recognition Loss:   0.000458 => Gls Tokens per Sec:     1885 || Batch Translation Loss:   0.116674 => Txt Tokens per Sec:     5216 || Lr: 0.000050
2024-02-05 21:37:25,648 Epoch 700: Total Training Recognition Loss 0.09  Total Training Translation Loss 5.22 
2024-02-05 21:37:25,648 EPOCH 701
2024-02-05 21:37:30,592 Epoch 701: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.51 
2024-02-05 21:37:30,593 EPOCH 702
2024-02-05 21:37:33,049 [Epoch: 702 Step: 00047000] Batch Recognition Loss:   0.000336 => Gls Tokens per Sec:     2109 || Batch Translation Loss:   0.045696 => Txt Tokens per Sec:     5618 || Lr: 0.000050
2024-02-05 21:37:35,885 Epoch 702: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.34 
2024-02-05 21:37:35,885 EPOCH 703
2024-02-05 21:37:40,912 [Epoch: 703 Step: 00047100] Batch Recognition Loss:   0.000162 => Gls Tokens per Sec:     2081 || Batch Translation Loss:   0.024427 => Txt Tokens per Sec:     5752 || Lr: 0.000050
2024-02-05 21:37:40,977 Epoch 703: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.88 
2024-02-05 21:37:40,978 EPOCH 704
2024-02-05 21:37:46,461 Epoch 704: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.46 
2024-02-05 21:37:46,461 EPOCH 705
2024-02-05 21:37:48,822 [Epoch: 705 Step: 00047200] Batch Recognition Loss:   0.000383 => Gls Tokens per Sec:     2169 || Batch Translation Loss:   0.027071 => Txt Tokens per Sec:     5963 || Lr: 0.000050
2024-02-05 21:37:51,345 Epoch 705: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.15 
2024-02-05 21:37:51,345 EPOCH 706
2024-02-05 21:37:56,624 [Epoch: 706 Step: 00047300] Batch Recognition Loss:   0.000252 => Gls Tokens per Sec:     1952 || Batch Translation Loss:   0.017146 => Txt Tokens per Sec:     5404 || Lr: 0.000050
2024-02-05 21:37:56,774 Epoch 706: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.41 
2024-02-05 21:37:56,774 EPOCH 707
2024-02-05 21:38:01,832 Epoch 707: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.20 
2024-02-05 21:38:01,833 EPOCH 708
2024-02-05 21:38:04,692 [Epoch: 708 Step: 00047400] Batch Recognition Loss:   0.000141 => Gls Tokens per Sec:     1701 || Batch Translation Loss:   0.016423 => Txt Tokens per Sec:     4848 || Lr: 0.000050
2024-02-05 21:38:07,292 Epoch 708: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.54 
2024-02-05 21:38:07,293 EPOCH 709
2024-02-05 21:38:12,074 [Epoch: 709 Step: 00047500] Batch Recognition Loss:   0.001361 => Gls Tokens per Sec:     2121 || Batch Translation Loss:   0.008760 => Txt Tokens per Sec:     5834 || Lr: 0.000050
2024-02-05 21:38:12,335 Epoch 709: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.81 
2024-02-05 21:38:12,335 EPOCH 710
2024-02-05 21:38:17,583 Epoch 710: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.07 
2024-02-05 21:38:17,583 EPOCH 711
2024-02-05 21:38:19,735 [Epoch: 711 Step: 00047600] Batch Recognition Loss:   0.000277 => Gls Tokens per Sec:     2186 || Batch Translation Loss:   0.019419 => Txt Tokens per Sec:     6279 || Lr: 0.000050
2024-02-05 21:38:22,653 Epoch 711: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.80 
2024-02-05 21:38:22,654 EPOCH 712
2024-02-05 21:38:27,602 [Epoch: 712 Step: 00047700] Batch Recognition Loss:   0.000224 => Gls Tokens per Sec:     2017 || Batch Translation Loss:   0.019742 => Txt Tokens per Sec:     5596 || Lr: 0.000050
2024-02-05 21:38:27,878 Epoch 712: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.47 
2024-02-05 21:38:27,878 EPOCH 713
2024-02-05 21:38:33,130 Epoch 713: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.72 
2024-02-05 21:38:33,130 EPOCH 714
2024-02-05 21:38:35,500 [Epoch: 714 Step: 00047800] Batch Recognition Loss:   0.000184 => Gls Tokens per Sec:     1958 || Batch Translation Loss:   0.013749 => Txt Tokens per Sec:     5668 || Lr: 0.000050
2024-02-05 21:38:38,171 Epoch 714: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.71 
2024-02-05 21:38:38,172 EPOCH 715
2024-02-05 21:38:43,086 [Epoch: 715 Step: 00047900] Batch Recognition Loss:   0.000866 => Gls Tokens per Sec:     1999 || Batch Translation Loss:   0.044075 => Txt Tokens per Sec:     5544 || Lr: 0.000050
2024-02-05 21:38:43,419 Epoch 715: Total Training Recognition Loss 0.04  Total Training Translation Loss 4.00 
2024-02-05 21:38:43,419 EPOCH 716
2024-02-05 21:38:48,277 Epoch 716: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.60 
2024-02-05 21:38:48,277 EPOCH 717
2024-02-05 21:38:50,451 [Epoch: 717 Step: 00048000] Batch Recognition Loss:   0.000991 => Gls Tokens per Sec:     2017 || Batch Translation Loss:   0.047418 => Txt Tokens per Sec:     5780 || Lr: 0.000050
2024-02-05 21:38:58,724 Validation result at epoch 717, step    48000: duration: 8.2726s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.99906	Translation Loss: 90787.52344	PPL: 8672.13379
	Eval Metric: BLEU
	WER 3.74	(DEL: 0.00,	INS: 0.00,	SUB: 3.74)
	BLEU-4 0.70	(BLEU-1: 11.08,	BLEU-2: 3.52,	BLEU-3: 1.42,	BLEU-4: 0.70)
	CHRF 16.85	ROUGE 9.21
2024-02-05 21:38:58,725 Logging Recognition and Translation Outputs
2024-02-05 21:38:58,725 ========================================================================================================================
2024-02-05 21:38:58,725 Logging Sequence: 96_93.00
2024-02-05 21:38:58,725 	Gloss Reference :	A B+C+D+E
2024-02-05 21:38:58,726 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:38:58,726 	Gloss Alignment :	         
2024-02-05 21:38:58,726 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:38:58,727 	Text Reference  :	bhuvneshwar kumar took 4   wickets and     hardik pandya took 3      wickets wonderful
2024-02-05 21:38:58,727 	Text Hypothesis :	*********** ***** this was india'  innings when   he     was  bowled in      shot     
2024-02-05 21:38:58,727 	Text Alignment  :	D           D     S    S   S       S       S      S      S    S      S       S        
2024-02-05 21:38:58,727 ========================================================================================================================
2024-02-05 21:38:58,727 Logging Sequence: 144_2.00
2024-02-05 21:38:58,728 	Gloss Reference :	A B+C+D+E      
2024-02-05 21:38:58,728 	Gloss Hypothesis:	A B+C+D+E+D+E+D
2024-02-05 21:38:58,728 	Gloss Alignment :	  S            
2024-02-05 21:38:58,728 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:38:58,729 	Text Reference  :	a girl      posted a  video  of herself playing cricket on a village farm on social media the     video      has        gone      viral  
2024-02-05 21:38:58,729 	Text Hypothesis :	* moroccans living in cities of ******* ******* ******* ** * ******* **** ** ****** ***** belgium netherland celebrated morocco's victory
2024-02-05 21:38:58,730 	Text Alignment  :	D S         S      S  S         D       D       D       D  D D       D    D  D      D     S       S          S          S         S      
2024-02-05 21:38:58,730 ========================================================================================================================
2024-02-05 21:38:58,730 Logging Sequence: 178_83.00
2024-02-05 21:38:58,730 	Gloss Reference :	A B+C+D+E
2024-02-05 21:38:58,730 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:38:58,731 	Gloss Alignment :	         
2024-02-05 21:38:58,731 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:38:58,731 	Text Reference  :	and ***** *** the     police still haven't apprehended the wrestler
2024-02-05 21:38:58,732 	Text Hypothesis :	and weeks his country was    very  strong  but         he  died    
2024-02-05 21:38:58,732 	Text Alignment  :	    I     I   S       S      S     S       S           S   S       
2024-02-05 21:38:58,732 ========================================================================================================================
2024-02-05 21:38:58,732 Logging Sequence: 169_214.00
2024-02-05 21:38:58,732 	Gloss Reference :	A B+C+D+E
2024-02-05 21:38:58,732 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:38:58,733 	Gloss Alignment :	         
2024-02-05 21:38:58,733 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:38:58,734 	Text Reference  :	virat kohli said that though arshdeep dropped   the      catch       he is     still a strong part of the indian team    
2024-02-05 21:38:58,735 	Text Hypothesis :	***** you   know that ****** ******** wikipedia provides information on celebs like  a height age  of *** ****** vehicles
2024-02-05 21:38:58,735 	Text Alignment  :	D     S     S         D      D        S         S        S           S  S      S       S      S       D   D      S       
2024-02-05 21:38:58,735 ========================================================================================================================
2024-02-05 21:38:58,735 Logging Sequence: 147_202.00
2024-02-05 21:38:58,735 	Gloss Reference :	A B+C+D+E
2024-02-05 21:38:58,735 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:38:58,736 	Gloss Alignment :	         
2024-02-05 21:38:58,736 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:38:58,737 	Text Reference  :	**** were      impressed that  she    took  the difficult decision to withdraw from the   olympics   and focus on her     mental health 
2024-02-05 21:38:58,738 	Text Hypothesis :	when someone's team      loses people found the ********* ******** ** ******** cup  while performing and ***** ** secured a      victory
2024-02-05 21:38:58,738 	Text Alignment  :	I    S         S         S     S      S         D         D        D  D        S    S     S              D     D  S       S      S      
2024-02-05 21:38:58,738 ========================================================================================================================
2024-02-05 21:39:02,155 Epoch 717: Total Training Recognition Loss 0.11  Total Training Translation Loss 3.92 
2024-02-05 21:39:02,155 EPOCH 718
2024-02-05 21:39:06,781 [Epoch: 718 Step: 00048100] Batch Recognition Loss:   0.000169 => Gls Tokens per Sec:     2089 || Batch Translation Loss:   0.033778 => Txt Tokens per Sec:     5817 || Lr: 0.000050
2024-02-05 21:39:07,150 Epoch 718: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.84 
2024-02-05 21:39:07,151 EPOCH 719
2024-02-05 21:39:12,468 Epoch 719: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.51 
2024-02-05 21:39:12,469 EPOCH 720
2024-02-05 21:39:14,715 [Epoch: 720 Step: 00048200] Batch Recognition Loss:   0.006531 => Gls Tokens per Sec:     1924 || Batch Translation Loss:   0.021649 => Txt Tokens per Sec:     5481 || Lr: 0.000050
2024-02-05 21:39:17,521 Epoch 720: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.93 
2024-02-05 21:39:17,521 EPOCH 721
2024-02-05 21:39:22,508 [Epoch: 721 Step: 00048300] Batch Recognition Loss:   0.000184 => Gls Tokens per Sec:     1906 || Batch Translation Loss:   0.022004 => Txt Tokens per Sec:     5311 || Lr: 0.000050
2024-02-05 21:39:23,013 Epoch 721: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.62 
2024-02-05 21:39:23,013 EPOCH 722
2024-02-05 21:39:28,037 Epoch 722: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.19 
2024-02-05 21:39:28,038 EPOCH 723
2024-02-05 21:39:30,220 [Epoch: 723 Step: 00048400] Batch Recognition Loss:   0.000359 => Gls Tokens per Sec:     1862 || Batch Translation Loss:   0.055627 => Txt Tokens per Sec:     4937 || Lr: 0.000050
2024-02-05 21:39:33,490 Epoch 723: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.32 
2024-02-05 21:39:33,490 EPOCH 724
2024-02-05 21:39:37,672 [Epoch: 724 Step: 00048500] Batch Recognition Loss:   0.001132 => Gls Tokens per Sec:     2234 || Batch Translation Loss:   0.122275 => Txt Tokens per Sec:     6177 || Lr: 0.000050
2024-02-05 21:39:38,296 Epoch 724: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.74 
2024-02-05 21:39:38,296 EPOCH 725
2024-02-05 21:39:43,726 Epoch 725: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.70 
2024-02-05 21:39:43,726 EPOCH 726
2024-02-05 21:39:45,366 [Epoch: 726 Step: 00048600] Batch Recognition Loss:   0.000130 => Gls Tokens per Sec:     2380 || Batch Translation Loss:   0.023835 => Txt Tokens per Sec:     6558 || Lr: 0.000050
2024-02-05 21:39:48,638 Epoch 726: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.63 
2024-02-05 21:39:48,638 EPOCH 727
2024-02-05 21:39:53,234 [Epoch: 727 Step: 00048700] Batch Recognition Loss:   0.000307 => Gls Tokens per Sec:     1998 || Batch Translation Loss:   0.023823 => Txt Tokens per Sec:     5452 || Lr: 0.000050
2024-02-05 21:39:54,020 Epoch 727: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.34 
2024-02-05 21:39:54,020 EPOCH 728
2024-02-05 21:39:58,983 Epoch 728: Total Training Recognition Loss 0.06  Total Training Translation Loss 4.54 
2024-02-05 21:39:58,984 EPOCH 729
2024-02-05 21:40:01,007 [Epoch: 729 Step: 00048800] Batch Recognition Loss:   0.000405 => Gls Tokens per Sec:     1899 || Batch Translation Loss:   0.074792 => Txt Tokens per Sec:     5451 || Lr: 0.000050
2024-02-05 21:40:04,169 Epoch 729: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.48 
2024-02-05 21:40:04,169 EPOCH 730
2024-02-05 21:40:08,586 [Epoch: 730 Step: 00048900] Batch Recognition Loss:   0.000315 => Gls Tokens per Sec:     2042 || Batch Translation Loss:   0.036381 => Txt Tokens per Sec:     5740 || Lr: 0.000050
2024-02-05 21:40:09,252 Epoch 730: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.05 
2024-02-05 21:40:09,252 EPOCH 731
2024-02-05 21:40:14,425 Epoch 731: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.35 
2024-02-05 21:40:14,426 EPOCH 732
2024-02-05 21:40:15,979 [Epoch: 732 Step: 00049000] Batch Recognition Loss:   0.000182 => Gls Tokens per Sec:     2371 || Batch Translation Loss:   0.016079 => Txt Tokens per Sec:     6625 || Lr: 0.000050
2024-02-05 21:40:19,564 Epoch 732: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.04 
2024-02-05 21:40:19,565 EPOCH 733
2024-02-05 21:40:24,299 [Epoch: 733 Step: 00049100] Batch Recognition Loss:   0.000237 => Gls Tokens per Sec:     1872 || Batch Translation Loss:   0.021878 => Txt Tokens per Sec:     5296 || Lr: 0.000050
2024-02-05 21:40:25,084 Epoch 733: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.72 
2024-02-05 21:40:25,084 EPOCH 734
2024-02-05 21:40:29,770 Epoch 734: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.53 
2024-02-05 21:40:29,770 EPOCH 735
2024-02-05 21:40:31,737 [Epoch: 735 Step: 00049200] Batch Recognition Loss:   0.000513 => Gls Tokens per Sec:     1791 || Batch Translation Loss:   0.014401 => Txt Tokens per Sec:     4941 || Lr: 0.000050
2024-02-05 21:40:35,267 Epoch 735: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.08 
2024-02-05 21:40:35,267 EPOCH 736
2024-02-05 21:40:39,250 [Epoch: 736 Step: 00049300] Batch Recognition Loss:   0.000320 => Gls Tokens per Sec:     2185 || Batch Translation Loss:   0.017884 => Txt Tokens per Sec:     6147 || Lr: 0.000050
2024-02-05 21:40:40,059 Epoch 736: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.08 
2024-02-05 21:40:40,060 EPOCH 737
2024-02-05 21:40:45,436 Epoch 737: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.86 
2024-02-05 21:40:45,436 EPOCH 738
2024-02-05 21:40:46,793 [Epoch: 738 Step: 00049400] Batch Recognition Loss:   0.000148 => Gls Tokens per Sec:     2480 || Batch Translation Loss:   0.019448 => Txt Tokens per Sec:     6573 || Lr: 0.000050
2024-02-05 21:40:50,323 Epoch 738: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.98 
2024-02-05 21:40:50,324 EPOCH 739
2024-02-05 21:40:54,896 [Epoch: 739 Step: 00049500] Batch Recognition Loss:   0.000570 => Gls Tokens per Sec:     1869 || Batch Translation Loss:   0.011895 => Txt Tokens per Sec:     5161 || Lr: 0.000050
2024-02-05 21:40:55,796 Epoch 739: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.47 
2024-02-05 21:40:55,796 EPOCH 740
2024-02-05 21:41:00,944 Epoch 740: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.97 
2024-02-05 21:41:00,945 EPOCH 741
2024-02-05 21:41:02,568 [Epoch: 741 Step: 00049600] Batch Recognition Loss:   0.000216 => Gls Tokens per Sec:     1972 || Batch Translation Loss:   0.191573 => Txt Tokens per Sec:     5506 || Lr: 0.000050
2024-02-05 21:41:06,084 Epoch 741: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.82 
2024-02-05 21:41:06,085 EPOCH 742
2024-02-05 21:41:09,788 [Epoch: 742 Step: 00049700] Batch Recognition Loss:   0.000614 => Gls Tokens per Sec:     2264 || Batch Translation Loss:   0.020707 => Txt Tokens per Sec:     6241 || Lr: 0.000050
2024-02-05 21:41:10,818 Epoch 742: Total Training Recognition Loss 0.07  Total Training Translation Loss 4.74 
2024-02-05 21:41:10,818 EPOCH 743
2024-02-05 21:41:15,968 Epoch 743: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.76 
2024-02-05 21:41:15,968 EPOCH 744
2024-02-05 21:41:17,414 [Epoch: 744 Step: 00049800] Batch Recognition Loss:   0.000175 => Gls Tokens per Sec:     2036 || Batch Translation Loss:   0.042433 => Txt Tokens per Sec:     5577 || Lr: 0.000050
2024-02-05 21:41:20,995 Epoch 744: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.40 
2024-02-05 21:41:20,995 EPOCH 745
2024-02-05 21:41:24,822 [Epoch: 745 Step: 00049900] Batch Recognition Loss:   0.000497 => Gls Tokens per Sec:     2175 || Batch Translation Loss:   0.022174 => Txt Tokens per Sec:     5896 || Lr: 0.000050
2024-02-05 21:41:26,266 Epoch 745: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.12 
2024-02-05 21:41:26,266 EPOCH 746
2024-02-05 21:41:31,245 Epoch 746: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.75 
2024-02-05 21:41:31,245 EPOCH 747
2024-02-05 21:41:32,616 [Epoch: 747 Step: 00050000] Batch Recognition Loss:   0.006045 => Gls Tokens per Sec:     2030 || Batch Translation Loss:   0.016200 => Txt Tokens per Sec:     5899 || Lr: 0.000050
2024-02-05 21:41:41,186 Validation result at epoch 747, step    50000: duration: 8.5693s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.85102	Translation Loss: 90332.52344	PPL: 8286.84570
	Eval Metric: BLEU
	WER 3.88	(DEL: 0.00,	INS: 0.00,	SUB: 3.88)
	BLEU-4 0.36	(BLEU-1: 10.05,	BLEU-2: 2.97,	BLEU-3: 1.07,	BLEU-4: 0.36)
	CHRF 16.85	ROUGE 8.53
2024-02-05 21:41:41,187 Logging Recognition and Translation Outputs
2024-02-05 21:41:41,188 ========================================================================================================================
2024-02-05 21:41:41,188 Logging Sequence: 178_157.00
2024-02-05 21:41:41,188 	Gloss Reference :	A B+C+D+E
2024-02-05 21:41:41,188 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:41:41,188 	Gloss Alignment :	         
2024-02-05 21:41:41,188 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:41:41,189 	Text Reference  :	this is why sushil  kumar will have      to *** ** ** **** be  arrested
2024-02-05 21:41:41,189 	Text Hypothesis :	that is *** because this  was  postponed to win so us lost the match   
2024-02-05 21:41:41,189 	Text Alignment  :	S       D   S       S     S    S            I   I  I  I    S   S       
2024-02-05 21:41:41,190 ========================================================================================================================
2024-02-05 21:41:41,190 Logging Sequence: 118_111.00
2024-02-05 21:41:41,190 	Gloss Reference :	A B+C+D+E
2024-02-05 21:41:41,190 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:41:41,190 	Gloss Alignment :	         
2024-02-05 21:41:41,190 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:41:41,191 	Text Reference  :	and people encourage him have hope for    the ****** ***** ***** ***** **** next world cup        
2024-02-05 21:41:41,191 	Text Hypothesis :	*** this   is        not go   on   amidst the rising cases human lives need to   be    safeguarded
2024-02-05 21:41:41,192 	Text Alignment  :	D   S      S         S   S    S    S          I      I     I     I     I    S    S     S          
2024-02-05 21:41:41,192 ========================================================================================================================
2024-02-05 21:41:41,192 Logging Sequence: 148_2.00
2024-02-05 21:41:41,192 	Gloss Reference :	A B+C+D+E
2024-02-05 21:41:41,192 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:41:41,192 	Gloss Alignment :	         
2024-02-05 21:41:41,192 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:41:41,194 	Text Reference  :	the final of      the   asia cup  2023  cricket tournament was  played between india and     sri lanka on 17th september 2023
2024-02-05 21:41:41,194 	Text Hypothesis :	the ***** winners claim that both teams had     scored     only 2      wickets in    colombo sri lanka ** **** ********* ****
2024-02-05 21:41:41,194 	Text Alignment  :	    D     S       S     S    S    S     S       S          S    S      S       S     S                 D  D    D         D   
2024-02-05 21:41:41,194 ========================================================================================================================
2024-02-05 21:41:41,195 Logging Sequence: 83_129.00
2024-02-05 21:41:41,195 	Gloss Reference :	A B+C+D+E
2024-02-05 21:41:41,195 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:41:41,195 	Gloss Alignment :	         
2024-02-05 21:41:41,195 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:41:41,196 	Text Reference  :	later the   denmark football association tweeted
2024-02-05 21:41:41,196 	Text Hypothesis :	2     hours after   this     was         done   
2024-02-05 21:41:41,196 	Text Alignment  :	S     S     S       S        S           S      
2024-02-05 21:41:41,196 ========================================================================================================================
2024-02-05 21:41:41,196 Logging Sequence: 99_158.00
2024-02-05 21:41:41,196 	Gloss Reference :	A B+C+D+E
2024-02-05 21:41:41,197 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:41:41,197 	Gloss Alignment :	         
2024-02-05 21:41:41,197 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:41:41,198 	Text Reference  :	**** ** *** *** the incident occured in  dubai   and   it was  extremely shameful
2024-02-05 21:41:41,198 	Text Hypothesis :	many of you may be  believe  that    his support would be show in        love    
2024-02-05 21:41:41,198 	Text Alignment  :	I    I  I   I   S   S        S       S   S       S     S  S    S         S       
2024-02-05 21:41:41,198 ========================================================================================================================
2024-02-05 21:41:45,164 Epoch 747: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.30 
2024-02-05 21:41:45,165 EPOCH 748
2024-02-05 21:41:49,279 [Epoch: 748 Step: 00050100] Batch Recognition Loss:   0.000346 => Gls Tokens per Sec:     1960 || Batch Translation Loss:   0.059407 => Txt Tokens per Sec:     5545 || Lr: 0.000050
2024-02-05 21:41:50,443 Epoch 748: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.45 
2024-02-05 21:41:50,443 EPOCH 749
2024-02-05 21:41:56,003 Epoch 749: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.18 
2024-02-05 21:41:56,004 EPOCH 750
2024-02-05 21:41:57,189 [Epoch: 750 Step: 00050200] Batch Recognition Loss:   0.000232 => Gls Tokens per Sec:     2297 || Batch Translation Loss:   0.019704 => Txt Tokens per Sec:     5961 || Lr: 0.000050
2024-02-05 21:42:01,355 Epoch 750: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.94 
2024-02-05 21:42:01,356 EPOCH 751
2024-02-05 21:42:05,282 [Epoch: 751 Step: 00050300] Batch Recognition Loss:   0.001315 => Gls Tokens per Sec:     2013 || Batch Translation Loss:   0.026221 => Txt Tokens per Sec:     5527 || Lr: 0.000050
2024-02-05 21:42:06,735 Epoch 751: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.58 
2024-02-05 21:42:06,735 EPOCH 752
2024-02-05 21:42:11,747 Epoch 752: Total Training Recognition Loss 0.07  Total Training Translation Loss 5.00 
2024-02-05 21:42:11,747 EPOCH 753
2024-02-05 21:42:12,872 [Epoch: 753 Step: 00050400] Batch Recognition Loss:   0.000260 => Gls Tokens per Sec:     2277 || Batch Translation Loss:   0.009620 => Txt Tokens per Sec:     5718 || Lr: 0.000050
2024-02-05 21:42:17,179 Epoch 753: Total Training Recognition Loss 0.05  Total Training Translation Loss 7.54 
2024-02-05 21:42:17,179 EPOCH 754
2024-02-05 21:42:20,603 [Epoch: 754 Step: 00050500] Batch Recognition Loss:   0.000235 => Gls Tokens per Sec:     2261 || Batch Translation Loss:   0.030813 => Txt Tokens per Sec:     6206 || Lr: 0.000050
2024-02-05 21:42:21,952 Epoch 754: Total Training Recognition Loss 0.09  Total Training Translation Loss 4.55 
2024-02-05 21:42:21,952 EPOCH 755
2024-02-05 21:42:27,409 Epoch 755: Total Training Recognition Loss 0.10  Total Training Translation Loss 2.80 
2024-02-05 21:42:27,409 EPOCH 756
2024-02-05 21:42:28,571 [Epoch: 756 Step: 00050600] Batch Recognition Loss:   0.000217 => Gls Tokens per Sec:     2067 || Batch Translation Loss:   0.011881 => Txt Tokens per Sec:     5949 || Lr: 0.000050
2024-02-05 21:42:32,360 Epoch 756: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.54 
2024-02-05 21:42:32,360 EPOCH 757
2024-02-05 21:42:35,699 [Epoch: 757 Step: 00050700] Batch Recognition Loss:   0.000687 => Gls Tokens per Sec:     2301 || Batch Translation Loss:   0.024235 => Txt Tokens per Sec:     6290 || Lr: 0.000050
2024-02-05 21:42:36,963 Epoch 757: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.07 
2024-02-05 21:42:36,963 EPOCH 758
2024-02-05 21:42:41,546 Epoch 758: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.75 
2024-02-05 21:42:41,547 EPOCH 759
2024-02-05 21:42:42,354 [Epoch: 759 Step: 00050800] Batch Recognition Loss:   0.000216 => Gls Tokens per Sec:     2775 || Batch Translation Loss:   0.014956 => Txt Tokens per Sec:     7181 || Lr: 0.000050
2024-02-05 21:42:46,826 Epoch 759: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.93 
2024-02-05 21:42:46,827 EPOCH 760
2024-02-05 21:42:50,490 [Epoch: 760 Step: 00050900] Batch Recognition Loss:   0.000174 => Gls Tokens per Sec:     2027 || Batch Translation Loss:   0.008565 => Txt Tokens per Sec:     5740 || Lr: 0.000050
2024-02-05 21:42:51,970 Epoch 760: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.26 
2024-02-05 21:42:51,970 EPOCH 761
2024-02-05 21:42:57,420 Epoch 761: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.87 
2024-02-05 21:42:57,420 EPOCH 762
2024-02-05 21:42:58,258 [Epoch: 762 Step: 00051000] Batch Recognition Loss:   0.000174 => Gls Tokens per Sec:     2486 || Batch Translation Loss:   0.011448 => Txt Tokens per Sec:     6913 || Lr: 0.000050
2024-02-05 21:43:02,239 Epoch 762: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.51 
2024-02-05 21:43:02,239 EPOCH 763
2024-02-05 21:43:05,881 [Epoch: 763 Step: 00051100] Batch Recognition Loss:   0.000181 => Gls Tokens per Sec:     2021 || Batch Translation Loss:   0.007694 => Txt Tokens per Sec:     5455 || Lr: 0.000050
2024-02-05 21:43:07,682 Epoch 763: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.77 
2024-02-05 21:43:07,682 EPOCH 764
2024-02-05 21:43:12,511 Epoch 764: Total Training Recognition Loss 0.18  Total Training Translation Loss 1.52 
2024-02-05 21:43:12,511 EPOCH 765
2024-02-05 21:43:13,569 [Epoch: 765 Step: 00051200] Batch Recognition Loss:   0.000435 => Gls Tokens per Sec:     1818 || Batch Translation Loss:   0.010475 => Txt Tokens per Sec:     4913 || Lr: 0.000050
2024-02-05 21:43:18,093 Epoch 765: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.23 
2024-02-05 21:43:18,094 EPOCH 766
2024-02-05 21:43:20,969 [Epoch: 766 Step: 00051300] Batch Recognition Loss:   0.000129 => Gls Tokens per Sec:     2469 || Batch Translation Loss:   0.023770 => Txt Tokens per Sec:     6553 || Lr: 0.000050
2024-02-05 21:43:22,947 Epoch 766: Total Training Recognition Loss 0.11  Total Training Translation Loss 1.47 
2024-02-05 21:43:22,948 EPOCH 767
2024-02-05 21:43:28,436 Epoch 767: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.61 
2024-02-05 21:43:28,436 EPOCH 768
2024-02-05 21:43:29,245 [Epoch: 768 Step: 00051400] Batch Recognition Loss:   0.000345 => Gls Tokens per Sec:     2177 || Batch Translation Loss:   0.031064 => Txt Tokens per Sec:     6081 || Lr: 0.000050
2024-02-05 21:43:33,867 Epoch 768: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.84 
2024-02-05 21:43:33,868 EPOCH 769
2024-02-05 21:43:37,395 [Epoch: 769 Step: 00051500] Batch Recognition Loss:   0.000413 => Gls Tokens per Sec:     1968 || Batch Translation Loss:   0.040659 => Txt Tokens per Sec:     5481 || Lr: 0.000050
2024-02-05 21:43:39,065 Epoch 769: Total Training Recognition Loss 0.04  Total Training Translation Loss 4.27 
2024-02-05 21:43:39,066 EPOCH 770
2024-02-05 21:43:44,478 Epoch 770: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.63 
2024-02-05 21:43:44,478 EPOCH 771
2024-02-05 21:43:45,163 [Epoch: 771 Step: 00051600] Batch Recognition Loss:   0.000426 => Gls Tokens per Sec:     2339 || Batch Translation Loss:   0.017231 => Txt Tokens per Sec:     6592 || Lr: 0.000050
2024-02-05 21:43:49,252 Epoch 771: Total Training Recognition Loss 0.10  Total Training Translation Loss 1.77 
2024-02-05 21:43:49,253 EPOCH 772
2024-02-05 21:43:52,867 [Epoch: 772 Step: 00051700] Batch Recognition Loss:   0.000254 => Gls Tokens per Sec:     1905 || Batch Translation Loss:   0.039930 => Txt Tokens per Sec:     5379 || Lr: 0.000050
2024-02-05 21:43:54,686 Epoch 772: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.32 
2024-02-05 21:43:54,687 EPOCH 773
2024-02-05 21:44:00,113 Epoch 773: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.38 
2024-02-05 21:44:00,114 EPOCH 774
2024-02-05 21:44:00,834 [Epoch: 774 Step: 00051800] Batch Recognition Loss:   0.000478 => Gls Tokens per Sec:     2003 || Batch Translation Loss:   0.019277 => Txt Tokens per Sec:     5655 || Lr: 0.000050
2024-02-05 21:44:05,534 Epoch 774: Total Training Recognition Loss 0.04  Total Training Translation Loss 5.66 
2024-02-05 21:44:05,535 EPOCH 775
2024-02-05 21:44:08,760 [Epoch: 775 Step: 00051900] Batch Recognition Loss:   0.000933 => Gls Tokens per Sec:     2085 || Batch Translation Loss:   0.068661 => Txt Tokens per Sec:     5631 || Lr: 0.000050
2024-02-05 21:44:11,010 Epoch 775: Total Training Recognition Loss 0.10  Total Training Translation Loss 7.62 
2024-02-05 21:44:11,011 EPOCH 776
2024-02-05 21:44:16,066 Epoch 776: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.31 
2024-02-05 21:44:16,066 EPOCH 777
2024-02-05 21:44:16,635 [Epoch: 777 Step: 00052000] Batch Recognition Loss:   0.000674 => Gls Tokens per Sec:     2255 || Batch Translation Loss:   0.028671 => Txt Tokens per Sec:     5797 || Lr: 0.000050
2024-02-05 21:44:25,359 Hooray! New best validation result [eval_metric]!
2024-02-05 21:44:25,360 Saving new checkpoint.
2024-02-05 21:44:25,639 Validation result at epoch 777, step    52000: duration: 9.0043s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.72897	Translation Loss: 90569.69531	PPL: 8485.49902
	Eval Metric: BLEU
	WER 3.46	(DEL: 0.00,	INS: 0.00,	SUB: 3.46)
	BLEU-4 0.96	(BLEU-1: 12.10,	BLEU-2: 4.17,	BLEU-3: 1.82,	BLEU-4: 0.96)
	CHRF 17.34	ROUGE 10.18
2024-02-05 21:44:25,640 Logging Recognition and Translation Outputs
2024-02-05 21:44:25,640 ========================================================================================================================
2024-02-05 21:44:25,640 Logging Sequence: 59_101.00
2024-02-05 21:44:25,640 	Gloss Reference :	A B+C+D+E
2024-02-05 21:44:25,641 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:44:25,641 	Gloss Alignment :	         
2024-02-05 21:44:25,641 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:44:25,642 	Text Reference  :	** did you see  the video fox said she won her medals because of a condom and    is       very   happy
2024-02-05 21:44:25,642 	Text Hypothesis :	on 4th may 2023 the ***** *** **** *** *** *** ****** ******* ** * ****** indian everyone walked over 
2024-02-05 21:44:25,642 	Text Alignment  :	I  S   S   S        D     D   D    D   D   D   D      D       D  D D      S      S        S      S    
2024-02-05 21:44:25,643 ========================================================================================================================
2024-02-05 21:44:25,643 Logging Sequence: 103_112.00
2024-02-05 21:44:25,643 	Gloss Reference :	A B+C+D+E
2024-02-05 21:44:25,643 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:44:25,643 	Gloss Alignment :	         
2024-02-05 21:44:25,643 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:44:25,645 	Text Reference  :	you are aware that earlier the britishers had    colonized a   lot of   countries in   the ****** world
2024-02-05 21:44:25,645 	Text Hypothesis :	*** *** ***** the  rights  of  4          groups is        now the even richer    than the second time 
2024-02-05 21:44:25,645 	Text Alignment  :	D   D   D     S    S       S   S          S      S         S   S   S    S         S        I      S    
2024-02-05 21:44:25,645 ========================================================================================================================
2024-02-05 21:44:25,645 Logging Sequence: 143_11.00
2024-02-05 21:44:25,646 	Gloss Reference :	A B+C+D+E
2024-02-05 21:44:25,646 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:44:25,646 	Gloss Alignment :	         
2024-02-05 21:44:25,646 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:44:25,648 	Text Reference  :	ronaldo has also become the first person   to          have 500 million followers  on     instagram he      is  the most loved footballer
2024-02-05 21:44:25,648 	Text Hypothesis :	******* *** **** ****** the ***** football association fa   and the     merseyside police banned    ronaldo for the **** ***** tournament
2024-02-05 21:44:25,648 	Text Alignment  :	D       D   D    D          D     S        S           S    S   S       S          S      S         S       S       D    D     S         
2024-02-05 21:44:25,648 ========================================================================================================================
2024-02-05 21:44:25,648 Logging Sequence: 183_23.00
2024-02-05 21:44:25,648 	Gloss Reference :	A B+C+D+E
2024-02-05 21:44:25,649 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:44:25,649 	Gloss Alignment :	         
2024-02-05 21:44:25,649 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:44:25,650 	Text Reference  :	however everybody has been waiting for *** them  to     announce the      name of        the       child  
2024-02-05 21:44:25,650 	Text Hypothesis :	******* and       he  also played  for the great leader who      respects his  teammate' religious beliefs
2024-02-05 21:44:25,650 	Text Alignment  :	D       S         S   S    S           I   S     S      S        S        S    S         S         S      
2024-02-05 21:44:25,651 ========================================================================================================================
2024-02-05 21:44:25,651 Logging Sequence: 169_165.00
2024-02-05 21:44:25,651 	Gloss Reference :	A B+C+D+E
2024-02-05 21:44:25,651 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:44:25,651 	Gloss Alignment :	         
2024-02-05 21:44:25,651 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:44:25,653 	Text Reference  :	the ***** **** ** indian government was   outraged by  the incident and *** these changes were undone by    wikipedia
2024-02-05 21:44:25,653 	Text Hypothesis :	the teams need to be     edited     false info     can be  removed  and the right info    can  visit  icc's website  
2024-02-05 21:44:25,653 	Text Alignment  :	    I     I    I  S      S          S     S        S   S   S            I   S     S       S    S      S     S        
2024-02-05 21:44:25,653 ========================================================================================================================
2024-02-05 21:44:30,563 Epoch 777: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.69 
2024-02-05 21:44:30,564 EPOCH 778
2024-02-05 21:44:33,366 [Epoch: 778 Step: 00052100] Batch Recognition Loss:   0.000376 => Gls Tokens per Sec:     2306 || Batch Translation Loss:   0.017143 => Txt Tokens per Sec:     6204 || Lr: 0.000050
2024-02-05 21:44:35,421 Epoch 778: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.84 
2024-02-05 21:44:35,421 EPOCH 779
2024-02-05 21:44:40,579 Epoch 779: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.60 
2024-02-05 21:44:40,580 EPOCH 780
2024-02-05 21:44:41,147 [Epoch: 780 Step: 00052200] Batch Recognition Loss:   0.000401 => Gls Tokens per Sec:     1979 || Batch Translation Loss:   0.022781 => Txt Tokens per Sec:     5413 || Lr: 0.000050
2024-02-05 21:44:45,771 Epoch 780: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.73 
2024-02-05 21:44:45,772 EPOCH 781
2024-02-05 21:44:48,947 [Epoch: 781 Step: 00052300] Batch Recognition Loss:   0.000182 => Gls Tokens per Sec:     1986 || Batch Translation Loss:   0.010324 => Txt Tokens per Sec:     5457 || Lr: 0.000050
2024-02-05 21:44:51,254 Epoch 781: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.81 
2024-02-05 21:44:51,254 EPOCH 782
2024-02-05 21:44:56,556 Epoch 782: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.73 
2024-02-05 21:44:56,557 EPOCH 783
2024-02-05 21:44:57,202 [Epoch: 783 Step: 00052400] Batch Recognition Loss:   0.000326 => Gls Tokens per Sec:     1335 || Batch Translation Loss:   0.032434 => Txt Tokens per Sec:     3882 || Lr: 0.000050
2024-02-05 21:45:02,059 Epoch 783: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.03 
2024-02-05 21:45:02,059 EPOCH 784
2024-02-05 21:45:05,028 [Epoch: 784 Step: 00052500] Batch Recognition Loss:   0.000179 => Gls Tokens per Sec:     2103 || Batch Translation Loss:   0.021036 => Txt Tokens per Sec:     5926 || Lr: 0.000050
2024-02-05 21:45:06,901 Epoch 784: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.08 
2024-02-05 21:45:06,901 EPOCH 785
2024-02-05 21:45:11,559 Epoch 785: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.74 
2024-02-05 21:45:11,559 EPOCH 786
2024-02-05 21:45:11,945 [Epoch: 786 Step: 00052600] Batch Recognition Loss:   0.000229 => Gls Tokens per Sec:     2078 || Batch Translation Loss:   0.019286 => Txt Tokens per Sec:     6621 || Lr: 0.000050
2024-02-05 21:45:16,560 Epoch 786: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.63 
2024-02-05 21:45:16,561 EPOCH 787
2024-02-05 21:45:19,550 [Epoch: 787 Step: 00052700] Batch Recognition Loss:   0.000505 => Gls Tokens per Sec:     2036 || Batch Translation Loss:   0.031250 => Txt Tokens per Sec:     5653 || Lr: 0.000050
2024-02-05 21:45:21,862 Epoch 787: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.05 
2024-02-05 21:45:21,863 EPOCH 788
2024-02-05 21:45:26,935 Epoch 788: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.26 
2024-02-05 21:45:26,935 EPOCH 789
2024-02-05 21:45:27,344 [Epoch: 789 Step: 00052800] Batch Recognition Loss:   0.000590 => Gls Tokens per Sec:     1567 || Batch Translation Loss:   0.063233 => Txt Tokens per Sec:     4856 || Lr: 0.000050
2024-02-05 21:45:32,088 Epoch 789: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.85 
2024-02-05 21:45:32,089 EPOCH 790
2024-02-05 21:45:34,838 [Epoch: 790 Step: 00052900] Batch Recognition Loss:   0.001276 => Gls Tokens per Sec:     2117 || Batch Translation Loss:   0.043235 => Txt Tokens per Sec:     5933 || Lr: 0.000050
2024-02-05 21:45:37,283 Epoch 790: Total Training Recognition Loss 0.05  Total Training Translation Loss 4.00 
2024-02-05 21:45:37,284 EPOCH 791
2024-02-05 21:45:42,245 Epoch 791: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.79 
2024-02-05 21:45:42,245 EPOCH 792
2024-02-05 21:45:42,431 [Epoch: 792 Step: 00053000] Batch Recognition Loss:   0.000781 => Gls Tokens per Sec:     2595 || Batch Translation Loss:   0.038308 => Txt Tokens per Sec:     7038 || Lr: 0.000050
2024-02-05 21:45:47,561 Epoch 792: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.24 
2024-02-05 21:45:47,562 EPOCH 793
2024-02-05 21:45:50,260 [Epoch: 793 Step: 00053100] Batch Recognition Loss:   0.001486 => Gls Tokens per Sec:     2136 || Batch Translation Loss:   0.026921 => Txt Tokens per Sec:     5764 || Lr: 0.000050
2024-02-05 21:45:52,513 Epoch 793: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.59 
2024-02-05 21:45:52,513 EPOCH 794
2024-02-05 21:45:57,862 Epoch 794: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.41 
2024-02-05 21:45:57,863 EPOCH 795
2024-02-05 21:45:57,984 [Epoch: 795 Step: 00053200] Batch Recognition Loss:   0.000248 => Gls Tokens per Sec:     2689 || Batch Translation Loss:   0.019813 => Txt Tokens per Sec:     6765 || Lr: 0.000050
2024-02-05 21:46:02,719 Epoch 795: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.33 
2024-02-05 21:46:02,719 EPOCH 796
2024-02-05 21:46:05,550 [Epoch: 796 Step: 00053300] Batch Recognition Loss:   0.000328 => Gls Tokens per Sec:     1944 || Batch Translation Loss:   0.131060 => Txt Tokens per Sec:     5249 || Lr: 0.000050
2024-02-05 21:46:08,134 Epoch 796: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.14 
2024-02-05 21:46:08,134 EPOCH 797
2024-02-05 21:46:13,160 Epoch 797: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.19 
2024-02-05 21:46:13,161 EPOCH 798
2024-02-05 21:46:13,219 [Epoch: 798 Step: 00053400] Batch Recognition Loss:   0.004691 => Gls Tokens per Sec:     2807 || Batch Translation Loss:   0.004990 => Txt Tokens per Sec:     3596 || Lr: 0.000050
2024-02-05 21:46:18,810 Epoch 798: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.28 
2024-02-05 21:46:18,810 EPOCH 799
2024-02-05 21:46:21,294 [Epoch: 799 Step: 00053500] Batch Recognition Loss:   0.002892 => Gls Tokens per Sec:     2152 || Batch Translation Loss:   0.025407 => Txt Tokens per Sec:     6175 || Lr: 0.000050
2024-02-05 21:46:23,935 Epoch 799: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.19 
2024-02-05 21:46:23,935 EPOCH 800
2024-02-05 21:46:29,350 [Epoch: 800 Step: 00053600] Batch Recognition Loss:   0.000240 => Gls Tokens per Sec:     1961 || Batch Translation Loss:   0.023348 => Txt Tokens per Sec:     5426 || Lr: 0.000050
2024-02-05 21:46:29,351 Epoch 800: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.80 
2024-02-05 21:46:29,351 EPOCH 801
2024-02-05 21:46:34,138 Epoch 801: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.36 
2024-02-05 21:46:34,138 EPOCH 802
2024-02-05 21:46:36,881 [Epoch: 802 Step: 00053700] Batch Recognition Loss:   0.000164 => Gls Tokens per Sec:     1926 || Batch Translation Loss:   0.015580 => Txt Tokens per Sec:     5278 || Lr: 0.000050
2024-02-05 21:46:39,652 Epoch 802: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.13 
2024-02-05 21:46:39,653 EPOCH 803
2024-02-05 21:46:44,404 [Epoch: 803 Step: 00053800] Batch Recognition Loss:   0.000216 => Gls Tokens per Sec:     2202 || Batch Translation Loss:   0.013977 => Txt Tokens per Sec:     6078 || Lr: 0.000050
2024-02-05 21:46:44,483 Epoch 803: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.20 
2024-02-05 21:46:44,483 EPOCH 804
2024-02-05 21:46:49,895 Epoch 804: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.48 
2024-02-05 21:46:49,896 EPOCH 805
2024-02-05 21:46:52,435 [Epoch: 805 Step: 00053900] Batch Recognition Loss:   0.000921 => Gls Tokens per Sec:     2016 || Batch Translation Loss:   0.020428 => Txt Tokens per Sec:     5614 || Lr: 0.000050
2024-02-05 21:46:55,165 Epoch 805: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.51 
2024-02-05 21:46:55,165 EPOCH 806
2024-02-05 21:46:59,810 [Epoch: 806 Step: 00054000] Batch Recognition Loss:   0.002435 => Gls Tokens per Sec:     2218 || Batch Translation Loss:   0.038909 => Txt Tokens per Sec:     6134 || Lr: 0.000050
2024-02-05 21:47:08,545 Validation result at epoch 806, step    54000: duration: 8.7349s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.78763	Translation Loss: 90431.21094	PPL: 8368.93457
	Eval Metric: BLEU
	WER 3.53	(DEL: 0.00,	INS: 0.00,	SUB: 3.53)
	BLEU-4 0.48	(BLEU-1: 10.88,	BLEU-2: 3.41,	BLEU-3: 1.24,	BLEU-4: 0.48)
	CHRF 16.94	ROUGE 8.96
2024-02-05 21:47:08,547 Logging Recognition and Translation Outputs
2024-02-05 21:47:08,547 ========================================================================================================================
2024-02-05 21:47:08,547 Logging Sequence: 166_243.00
2024-02-05 21:47:08,547 	Gloss Reference :	A B+C+D+E  
2024-02-05 21:47:08,547 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-05 21:47:08,547 	Gloss Alignment :	  S        
2024-02-05 21:47:08,547 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:47:08,549 	Text Reference  :	*** ********* *********** ********* *** ***** ** icc     worked with members boards like bcci pcb   cricket australia etc 
2024-02-05 21:47:08,549 	Text Hypothesis :	the broadcast advertisers ticketing etc would be decided by     the  board   of     the  2    teams playing the       test
2024-02-05 21:47:08,549 	Text Alignment  :	I   I         I           I         I   I     I  S       S      S    S       S      S    S    S     S       S         S   
2024-02-05 21:47:08,549 ========================================================================================================================
2024-02-05 21:47:08,549 Logging Sequence: 179_409.00
2024-02-05 21:47:08,549 	Gloss Reference :	A B+C+D+E
2024-02-05 21:47:08,550 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:47:08,550 	Gloss Alignment :	         
2024-02-05 21:47:08,550 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:47:08,551 	Text Reference  :	************* *** *** the passport was      at   the ******** **** wfi    office in  delhi   
2024-02-05 21:47:08,551 	Text Hypothesis :	unfortunately she had a   heated   exchange with the olympics then handed over   the olympics
2024-02-05 21:47:08,551 	Text Alignment  :	I             I   I   S   S        S        S        I        I    S      S      S   S       
2024-02-05 21:47:08,551 ========================================================================================================================
2024-02-05 21:47:08,551 Logging Sequence: 81_407.00
2024-02-05 21:47:08,551 	Gloss Reference :	A B+C+D+E
2024-02-05 21:47:08,551 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:47:08,552 	Gloss Alignment :	         
2024-02-05 21:47:08,552 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:47:08,553 	Text Reference  :	the government company - national buildings construction corporation and they  will complete them in    a  time-bound manner
2024-02-05 21:47:08,553 	Text Hypothesis :	the ********** ******* * ******** amrapali  group        paid        a   total of   rs       4222 crore to rhiti      sports
2024-02-05 21:47:08,553 	Text Alignment  :	    D          D       D D        S         S            S           S   S     S    S        S    S     S  S          S     
2024-02-05 21:47:08,553 ========================================================================================================================
2024-02-05 21:47:08,554 Logging Sequence: 96_31.00
2024-02-05 21:47:08,554 	Gloss Reference :	A B+C+D+E
2024-02-05 21:47:08,554 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:47:08,554 	Gloss Alignment :	         
2024-02-05 21:47:08,554 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:47:08,555 	Text Reference  :	and then 2 teams will go   on      to  play   the  final  
2024-02-05 21:47:08,555 	Text Hypothesis :	*** **** * ***** **** with india's win people felt relaxed
2024-02-05 21:47:08,555 	Text Alignment  :	D   D    D D     D    S    S       S   S      S    S      
2024-02-05 21:47:08,555 ========================================================================================================================
2024-02-05 21:47:08,555 Logging Sequence: 160_87.00
2024-02-05 21:47:08,555 	Gloss Reference :	A B+C+D+E
2024-02-05 21:47:08,555 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:47:08,555 	Gloss Alignment :	         
2024-02-05 21:47:08,556 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:47:08,556 	Text Reference  :	******* ** ******* *** ***** ** ***** ******* kohli held a  press   conference and said  
2024-02-05 21:47:08,556 	Text Hypothesis :	however my country was glued to their screens as    this is excited to         the finals
2024-02-05 21:47:08,557 	Text Alignment  :	I       I  I       I   I     I  I     I       S     S    S  S       S          S   S     
2024-02-05 21:47:08,557 ========================================================================================================================
2024-02-05 21:47:08,730 Epoch 806: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.88 
2024-02-05 21:47:08,730 EPOCH 807
2024-02-05 21:47:14,470 Epoch 807: Total Training Recognition Loss 0.04  Total Training Translation Loss 4.32 
2024-02-05 21:47:14,470 EPOCH 808
2024-02-05 21:47:16,972 [Epoch: 808 Step: 00054100] Batch Recognition Loss:   0.000391 => Gls Tokens per Sec:     1943 || Batch Translation Loss:   0.035246 => Txt Tokens per Sec:     5247 || Lr: 0.000050
2024-02-05 21:47:19,540 Epoch 808: Total Training Recognition Loss 0.05  Total Training Translation Loss 5.18 
2024-02-05 21:47:19,541 EPOCH 809
2024-02-05 21:47:24,653 [Epoch: 809 Step: 00054200] Batch Recognition Loss:   0.000374 => Gls Tokens per Sec:     1983 || Batch Translation Loss:   0.198695 => Txt Tokens per Sec:     5503 || Lr: 0.000050
2024-02-05 21:47:24,871 Epoch 809: Total Training Recognition Loss 0.06  Total Training Translation Loss 4.22 
2024-02-05 21:47:24,871 EPOCH 810
2024-02-05 21:47:29,809 Epoch 810: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.31 
2024-02-05 21:47:29,809 EPOCH 811
2024-02-05 21:47:32,265 [Epoch: 811 Step: 00054300] Batch Recognition Loss:   0.000276 => Gls Tokens per Sec:     1914 || Batch Translation Loss:   0.039095 => Txt Tokens per Sec:     5280 || Lr: 0.000050
2024-02-05 21:47:35,152 Epoch 811: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.81 
2024-02-05 21:47:35,152 EPOCH 812
2024-02-05 21:47:39,849 [Epoch: 812 Step: 00054400] Batch Recognition Loss:   0.000286 => Gls Tokens per Sec:     2125 || Batch Translation Loss:   0.043122 => Txt Tokens per Sec:     5904 || Lr: 0.000050
2024-02-05 21:47:40,127 Epoch 812: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.48 
2024-02-05 21:47:40,128 EPOCH 813
2024-02-05 21:47:45,667 Epoch 813: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.90 
2024-02-05 21:47:45,668 EPOCH 814
2024-02-05 21:47:47,831 [Epoch: 814 Step: 00054500] Batch Recognition Loss:   0.000201 => Gls Tokens per Sec:     2146 || Batch Translation Loss:   0.025296 => Txt Tokens per Sec:     5935 || Lr: 0.000050
2024-02-05 21:47:50,781 Epoch 814: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.26 
2024-02-05 21:47:50,782 EPOCH 815
2024-02-05 21:47:55,863 [Epoch: 815 Step: 00054600] Batch Recognition Loss:   0.000211 => Gls Tokens per Sec:     1933 || Batch Translation Loss:   0.017593 => Txt Tokens per Sec:     5396 || Lr: 0.000050
2024-02-05 21:47:56,144 Epoch 815: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.59 
2024-02-05 21:47:56,144 EPOCH 816
2024-02-05 21:48:00,989 Epoch 816: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.87 
2024-02-05 21:48:00,990 EPOCH 817
2024-02-05 21:48:03,480 [Epoch: 817 Step: 00054700] Batch Recognition Loss:   0.000704 => Gls Tokens per Sec:     1800 || Batch Translation Loss:   0.028360 => Txt Tokens per Sec:     5096 || Lr: 0.000050
2024-02-05 21:48:06,453 Epoch 817: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.95 
2024-02-05 21:48:06,454 EPOCH 818
2024-02-05 21:48:10,997 [Epoch: 818 Step: 00054800] Batch Recognition Loss:   0.000279 => Gls Tokens per Sec:     2149 || Batch Translation Loss:   0.019391 => Txt Tokens per Sec:     5968 || Lr: 0.000050
2024-02-05 21:48:11,478 Epoch 818: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.36 
2024-02-05 21:48:11,478 EPOCH 819
2024-02-05 21:48:16,660 Epoch 819: Total Training Recognition Loss 0.09  Total Training Translation Loss 4.53 
2024-02-05 21:48:16,660 EPOCH 820
2024-02-05 21:48:18,588 [Epoch: 820 Step: 00054900] Batch Recognition Loss:   0.000862 => Gls Tokens per Sec:     2242 || Batch Translation Loss:   0.036191 => Txt Tokens per Sec:     6406 || Lr: 0.000050
2024-02-05 21:48:21,698 Epoch 820: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.70 
2024-02-05 21:48:21,698 EPOCH 821
2024-02-05 21:48:26,333 [Epoch: 821 Step: 00055000] Batch Recognition Loss:   0.000683 => Gls Tokens per Sec:     2051 || Batch Translation Loss:   0.016929 => Txt Tokens per Sec:     5599 || Lr: 0.000050
2024-02-05 21:48:26,934 Epoch 821: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.09 
2024-02-05 21:48:26,934 EPOCH 822
2024-02-05 21:48:32,423 Epoch 822: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.55 
2024-02-05 21:48:32,424 EPOCH 823
2024-02-05 21:48:34,379 [Epoch: 823 Step: 00055100] Batch Recognition Loss:   0.000459 => Gls Tokens per Sec:     2131 || Batch Translation Loss:   0.022218 => Txt Tokens per Sec:     5894 || Lr: 0.000050
2024-02-05 21:48:37,475 Epoch 823: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.53 
2024-02-05 21:48:37,475 EPOCH 824
2024-02-05 21:48:42,320 [Epoch: 824 Step: 00055200] Batch Recognition Loss:   0.000133 => Gls Tokens per Sec:     1929 || Batch Translation Loss:   0.016825 => Txt Tokens per Sec:     5426 || Lr: 0.000050
2024-02-05 21:48:42,946 Epoch 824: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.80 
2024-02-05 21:48:42,946 EPOCH 825
2024-02-05 21:48:48,046 Epoch 825: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.19 
2024-02-05 21:48:48,046 EPOCH 826
2024-02-05 21:48:49,878 [Epoch: 826 Step: 00055300] Batch Recognition Loss:   0.000197 => Gls Tokens per Sec:     2185 || Batch Translation Loss:   0.018499 => Txt Tokens per Sec:     6241 || Lr: 0.000050
2024-02-05 21:48:53,082 Epoch 826: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.29 
2024-02-05 21:48:53,083 EPOCH 827
2024-02-05 21:48:57,510 [Epoch: 827 Step: 00055400] Batch Recognition Loss:   0.000251 => Gls Tokens per Sec:     2075 || Batch Translation Loss:   0.012513 => Txt Tokens per Sec:     5740 || Lr: 0.000050
2024-02-05 21:48:58,200 Epoch 827: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.87 
2024-02-05 21:48:58,200 EPOCH 828
2024-02-05 21:49:03,286 Epoch 828: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.66 
2024-02-05 21:49:03,287 EPOCH 829
2024-02-05 21:49:05,259 [Epoch: 829 Step: 00055500] Batch Recognition Loss:   0.000362 => Gls Tokens per Sec:     1949 || Batch Translation Loss:   0.026982 => Txt Tokens per Sec:     5673 || Lr: 0.000050
2024-02-05 21:49:08,373 Epoch 829: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.42 
2024-02-05 21:49:08,373 EPOCH 830
2024-02-05 21:49:12,733 [Epoch: 830 Step: 00055600] Batch Recognition Loss:   0.000140 => Gls Tokens per Sec:     2069 || Batch Translation Loss:   0.017807 => Txt Tokens per Sec:     5695 || Lr: 0.000050
2024-02-05 21:49:13,633 Epoch 830: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.81 
2024-02-05 21:49:13,633 EPOCH 831
2024-02-05 21:49:18,652 Epoch 831: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.37 
2024-02-05 21:49:18,652 EPOCH 832
2024-02-05 21:49:20,060 [Epoch: 832 Step: 00055700] Batch Recognition Loss:   0.000221 => Gls Tokens per Sec:     2616 || Batch Translation Loss:   0.090182 => Txt Tokens per Sec:     6730 || Lr: 0.000050
2024-02-05 21:49:24,036 Epoch 832: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.45 
2024-02-05 21:49:24,037 EPOCH 833
2024-02-05 21:49:27,895 [Epoch: 833 Step: 00055800] Batch Recognition Loss:   0.000169 => Gls Tokens per Sec:     2297 || Batch Translation Loss:   0.068022 => Txt Tokens per Sec:     6268 || Lr: 0.000050
2024-02-05 21:49:28,894 Epoch 833: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.90 
2024-02-05 21:49:28,894 EPOCH 834
2024-02-05 21:49:34,396 Epoch 834: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.50 
2024-02-05 21:49:34,397 EPOCH 835
2024-02-05 21:49:36,179 [Epoch: 835 Step: 00055900] Batch Recognition Loss:   0.000227 => Gls Tokens per Sec:     1920 || Batch Translation Loss:   0.023737 => Txt Tokens per Sec:     5302 || Lr: 0.000050
2024-02-05 21:49:39,384 Epoch 835: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.42 
2024-02-05 21:49:39,385 EPOCH 836
2024-02-05 21:49:44,022 [Epoch: 836 Step: 00056000] Batch Recognition Loss:   0.000222 => Gls Tokens per Sec:     1876 || Batch Translation Loss:   0.025053 => Txt Tokens per Sec:     5242 || Lr: 0.000050
2024-02-05 21:49:52,379 Validation result at epoch 836, step    56000: duration: 8.3569s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 3.02736	Translation Loss: 91036.88281	PPL: 8890.83594
	Eval Metric: BLEU
	WER 3.46	(DEL: 0.00,	INS: 0.00,	SUB: 3.46)
	BLEU-4 0.64	(BLEU-1: 10.92,	BLEU-2: 3.66,	BLEU-3: 1.43,	BLEU-4: 0.64)
	CHRF 17.06	ROUGE 9.40
2024-02-05 21:49:52,380 Logging Recognition and Translation Outputs
2024-02-05 21:49:52,380 ========================================================================================================================
2024-02-05 21:49:52,380 Logging Sequence: 177_167.00
2024-02-05 21:49:52,380 	Gloss Reference :	A B+C+D+E
2024-02-05 21:49:52,380 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:49:52,381 	Gloss Alignment :	         
2024-02-05 21:49:52,381 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:49:52,382 	Text Reference  :	*** ****** this is because    sushil wanted to **** establish his fear   to     ensure no    one  would  oppose him 
2024-02-05 21:49:52,383 	Text Hypothesis :	the police want to interogate sushil ajay   to find out       the motive behind the    brawl that killed sagar  rana
2024-02-05 21:49:52,383 	Text Alignment  :	I   I      S    S  S                 S         I    S         S   S      S      S      S     S    S      S      S   
2024-02-05 21:49:52,383 ========================================================================================================================
2024-02-05 21:49:52,383 Logging Sequence: 127_140.00
2024-02-05 21:49:52,383 	Gloss Reference :	A B+C+D+E
2024-02-05 21:49:52,383 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:49:52,383 	Gloss Alignment :	         
2024-02-05 21:49:52,383 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:49:52,385 	Text Reference  :	*** this is  india' 3rd  medal in the world athletics championships he is very talented and his performance is      highly impressive
2024-02-05 21:49:52,385 	Text Hypothesis :	who had  won 4      gold medal at the world ********* ************* ** ** **** ******** *** *** cup         triumph in     2022      
2024-02-05 21:49:52,385 	Text Alignment  :	I   S    S   S      S          S            D         D             D  D  D    D        D   D   S           S       S      S         
2024-02-05 21:49:52,385 ========================================================================================================================
2024-02-05 21:49:52,386 Logging Sequence: 126_200.00
2024-02-05 21:49:52,386 	Gloss Reference :	A B+C+D+E
2024-02-05 21:49:52,386 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:49:52,386 	Gloss Alignment :	         
2024-02-05 21:49:52,386 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:49:52,386 	Text Reference  :	let me  tell    you about them
2024-02-05 21:49:52,387 	Text Hypothesis :	he  was granted by  the   bcci
2024-02-05 21:49:52,387 	Text Alignment  :	S   S   S       S   S     S   
2024-02-05 21:49:52,387 ========================================================================================================================
2024-02-05 21:49:52,387 Logging Sequence: 104_119.00
2024-02-05 21:49:52,387 	Gloss Reference :	A B+C+D+E    
2024-02-05 21:49:52,387 	Gloss Hypothesis:	A B+C+D+E+D+E
2024-02-05 21:49:52,387 	Gloss Alignment :	  S          
2024-02-05 21:49:52,387 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:49:52,389 	Text Reference  :	famous chess players like  viswanathan anand and   praggnanandhaa's coach r     b  ramesh congratulated him  for  his impressive performance
2024-02-05 21:49:52,389 	Text Hypothesis :	****** ***** the     young boy         was   fined rs               10    crore to decide the           will also be  returning  soon       
2024-02-05 21:49:52,389 	Text Alignment  :	D      D     S       S     S           S     S     S                S     S     S  S      S             S    S    S   S          S          
2024-02-05 21:49:52,390 ========================================================================================================================
2024-02-05 21:49:52,390 Logging Sequence: 172_267.00
2024-02-05 21:49:52,390 	Gloss Reference :	A B+C+D+E
2024-02-05 21:49:52,390 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:49:52,390 	Gloss Alignment :	         
2024-02-05 21:49:52,390 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:49:52,390 	Text Reference  :	such provisions have been made   
2024-02-05 21:49:52,391 	Text Hypothesis :	**** this       is   just rubbish
2024-02-05 21:49:52,391 	Text Alignment  :	D    S          S    S    S      
2024-02-05 21:49:52,391 ========================================================================================================================
2024-02-05 21:49:53,314 Epoch 836: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.06 
2024-02-05 21:49:53,315 EPOCH 837
2024-02-05 21:49:58,784 Epoch 837: Total Training Recognition Loss 0.10  Total Training Translation Loss 2.17 
2024-02-05 21:49:58,785 EPOCH 838
2024-02-05 21:50:00,633 [Epoch: 838 Step: 00056100] Batch Recognition Loss:   0.000215 => Gls Tokens per Sec:     1765 || Batch Translation Loss:   0.175566 => Txt Tokens per Sec:     4794 || Lr: 0.000050
2024-02-05 21:50:04,192 Epoch 838: Total Training Recognition Loss 0.09  Total Training Translation Loss 3.85 
2024-02-05 21:50:04,192 EPOCH 839
2024-02-05 21:50:07,968 [Epoch: 839 Step: 00056200] Batch Recognition Loss:   0.000525 => Gls Tokens per Sec:     2263 || Batch Translation Loss:   0.054140 => Txt Tokens per Sec:     6210 || Lr: 0.000050
2024-02-05 21:50:08,837 Epoch 839: Total Training Recognition Loss 0.08  Total Training Translation Loss 12.25 
2024-02-05 21:50:08,837 EPOCH 840
2024-02-05 21:50:14,413 Epoch 840: Total Training Recognition Loss 0.14  Total Training Translation Loss 10.20 
2024-02-05 21:50:14,414 EPOCH 841
2024-02-05 21:50:15,683 [Epoch: 841 Step: 00056300] Batch Recognition Loss:   0.001836 => Gls Tokens per Sec:     2524 || Batch Translation Loss:   0.034529 => Txt Tokens per Sec:     6867 || Lr: 0.000050
2024-02-05 21:50:19,553 Epoch 841: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.59 
2024-02-05 21:50:19,554 EPOCH 842
2024-02-05 21:50:23,622 [Epoch: 842 Step: 00056400] Batch Recognition Loss:   0.000278 => Gls Tokens per Sec:     2061 || Batch Translation Loss:   0.027826 => Txt Tokens per Sec:     5684 || Lr: 0.000050
2024-02-05 21:50:24,636 Epoch 842: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.06 
2024-02-05 21:50:24,636 EPOCH 843
2024-02-05 21:50:29,862 Epoch 843: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.58 
2024-02-05 21:50:29,863 EPOCH 844
2024-02-05 21:50:31,392 [Epoch: 844 Step: 00056500] Batch Recognition Loss:   0.000236 => Gls Tokens per Sec:     1990 || Batch Translation Loss:   0.029436 => Txt Tokens per Sec:     5236 || Lr: 0.000050
2024-02-05 21:50:35,315 Epoch 844: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.55 
2024-02-05 21:50:35,316 EPOCH 845
2024-02-05 21:50:39,630 [Epoch: 845 Step: 00056600] Batch Recognition Loss:   0.000192 => Gls Tokens per Sec:     1905 || Batch Translation Loss:   0.015511 => Txt Tokens per Sec:     5381 || Lr: 0.000050
2024-02-05 21:50:40,638 Epoch 845: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.74 
2024-02-05 21:50:40,638 EPOCH 846
2024-02-05 21:50:45,572 Epoch 846: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.65 
2024-02-05 21:50:45,572 EPOCH 847
2024-02-05 21:50:46,751 [Epoch: 847 Step: 00056700] Batch Recognition Loss:   0.000194 => Gls Tokens per Sec:     2445 || Batch Translation Loss:   0.017070 => Txt Tokens per Sec:     6795 || Lr: 0.000050
2024-02-05 21:50:50,781 Epoch 847: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.49 
2024-02-05 21:50:50,782 EPOCH 848
2024-02-05 21:50:54,655 [Epoch: 848 Step: 00056800] Batch Recognition Loss:   0.000148 => Gls Tokens per Sec:     2107 || Batch Translation Loss:   0.020649 => Txt Tokens per Sec:     5893 || Lr: 0.000050
2024-02-05 21:50:55,831 Epoch 848: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.61 
2024-02-05 21:50:55,831 EPOCH 849
2024-02-05 21:51:01,183 Epoch 849: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.51 
2024-02-05 21:51:01,184 EPOCH 850
2024-02-05 21:51:02,442 [Epoch: 850 Step: 00056900] Batch Recognition Loss:   0.000126 => Gls Tokens per Sec:     2163 || Batch Translation Loss:   0.015879 => Txt Tokens per Sec:     6075 || Lr: 0.000050
2024-02-05 21:51:06,098 Epoch 850: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.20 
2024-02-05 21:51:06,099 EPOCH 851
2024-02-05 21:51:10,009 [Epoch: 851 Step: 00057000] Batch Recognition Loss:   0.000285 => Gls Tokens per Sec:     2046 || Batch Translation Loss:   0.016303 => Txt Tokens per Sec:     5631 || Lr: 0.000050
2024-02-05 21:51:11,486 Epoch 851: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.11 
2024-02-05 21:51:11,486 EPOCH 852
2024-02-05 21:51:16,800 Epoch 852: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.11 
2024-02-05 21:51:16,800 EPOCH 853
2024-02-05 21:51:18,208 [Epoch: 853 Step: 00057100] Batch Recognition Loss:   0.000268 => Gls Tokens per Sec:     1819 || Batch Translation Loss:   0.017338 => Txt Tokens per Sec:     5235 || Lr: 0.000050
2024-02-05 21:51:22,294 Epoch 853: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.00 
2024-02-05 21:51:22,295 EPOCH 854
2024-02-05 21:51:25,756 [Epoch: 854 Step: 00057200] Batch Recognition Loss:   0.000261 => Gls Tokens per Sec:     2265 || Batch Translation Loss:   0.021020 => Txt Tokens per Sec:     6189 || Lr: 0.000050
2024-02-05 21:51:27,235 Epoch 854: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.42 
2024-02-05 21:51:27,236 EPOCH 855
2024-02-05 21:51:32,674 Epoch 855: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.66 
2024-02-05 21:51:32,675 EPOCH 856
2024-02-05 21:51:33,619 [Epoch: 856 Step: 00057300] Batch Recognition Loss:   0.000227 => Gls Tokens per Sec:     2547 || Batch Translation Loss:   0.048204 => Txt Tokens per Sec:     6579 || Lr: 0.000050
2024-02-05 21:51:37,398 Epoch 856: Total Training Recognition Loss 0.05  Total Training Translation Loss 5.53 
2024-02-05 21:51:37,398 EPOCH 857
2024-02-05 21:51:41,346 [Epoch: 857 Step: 00057400] Batch Recognition Loss:   0.000782 => Gls Tokens per Sec:     1920 || Batch Translation Loss:   0.081164 => Txt Tokens per Sec:     5287 || Lr: 0.000050
2024-02-05 21:51:43,052 Epoch 857: Total Training Recognition Loss 0.06  Total Training Translation Loss 6.43 
2024-02-05 21:51:43,052 EPOCH 858
2024-02-05 21:51:48,008 Epoch 858: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.92 
2024-02-05 21:51:48,008 EPOCH 859
2024-02-05 21:51:49,168 [Epoch: 859 Step: 00057500] Batch Recognition Loss:   0.000331 => Gls Tokens per Sec:     1933 || Batch Translation Loss:   0.030379 => Txt Tokens per Sec:     5376 || Lr: 0.000050
2024-02-05 21:51:53,386 Epoch 859: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.75 
2024-02-05 21:51:53,386 EPOCH 860
2024-02-05 21:51:57,423 [Epoch: 860 Step: 00057600] Batch Recognition Loss:   0.000373 => Gls Tokens per Sec:     1838 || Batch Translation Loss:   0.130717 => Txt Tokens per Sec:     5149 || Lr: 0.000050
2024-02-05 21:51:58,903 Epoch 860: Total Training Recognition Loss 0.05  Total Training Translation Loss 5.28 
2024-02-05 21:51:58,904 EPOCH 861
2024-02-05 21:52:04,402 Epoch 861: Total Training Recognition Loss 0.07  Total Training Translation Loss 4.49 
2024-02-05 21:52:04,403 EPOCH 862
2024-02-05 21:52:05,498 [Epoch: 862 Step: 00057700] Batch Recognition Loss:   0.000311 => Gls Tokens per Sec:     1808 || Batch Translation Loss:   0.019618 => Txt Tokens per Sec:     4884 || Lr: 0.000050
2024-02-05 21:52:09,979 Epoch 862: Total Training Recognition Loss 0.12  Total Training Translation Loss 2.41 
2024-02-05 21:52:09,979 EPOCH 863
2024-02-05 21:52:13,237 [Epoch: 863 Step: 00057800] Batch Recognition Loss:   0.000329 => Gls Tokens per Sec:     2229 || Batch Translation Loss:   0.009028 => Txt Tokens per Sec:     6272 || Lr: 0.000050
2024-02-05 21:52:14,583 Epoch 863: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.77 
2024-02-05 21:52:14,584 EPOCH 864
2024-02-05 21:52:20,096 Epoch 864: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.62 
2024-02-05 21:52:20,096 EPOCH 865
2024-02-05 21:52:21,094 [Epoch: 865 Step: 00057900] Batch Recognition Loss:   0.000198 => Gls Tokens per Sec:     1825 || Batch Translation Loss:   0.014122 => Txt Tokens per Sec:     4953 || Lr: 0.000050
2024-02-05 21:52:25,524 Epoch 865: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.85 
2024-02-05 21:52:25,525 EPOCH 866
2024-02-05 21:52:28,733 [Epoch: 866 Step: 00058000] Batch Recognition Loss:   0.000098 => Gls Tokens per Sec:     2245 || Batch Translation Loss:   0.014427 => Txt Tokens per Sec:     6020 || Lr: 0.000050
2024-02-05 21:52:37,091 Validation result at epoch 866, step    58000: duration: 8.3583s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.87459	Translation Loss: 90925.00781	PPL: 8792.04004
	Eval Metric: BLEU
	WER 3.81	(DEL: 0.00,	INS: 0.00,	SUB: 3.81)
	BLEU-4 0.51	(BLEU-1: 11.06,	BLEU-2: 3.29,	BLEU-3: 1.18,	BLEU-4: 0.51)
	CHRF 17.07	ROUGE 9.50
2024-02-05 21:52:37,093 Logging Recognition and Translation Outputs
2024-02-05 21:52:37,093 ========================================================================================================================
2024-02-05 21:52:37,093 Logging Sequence: 60_264.00
2024-02-05 21:52:37,093 	Gloss Reference :	A B+C+D+E
2024-02-05 21:52:37,093 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:52:37,093 	Gloss Alignment :	         
2024-02-05 21:52:37,094 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:52:37,095 	Text Reference  :	plus do you know that a sex tape    of his    with two  women     had gone     viral  
2024-02-05 21:52:37,095 	Text Hypothesis :	**** ** *** **** **** * *** however my family has  been embroiled in  multiple wickets
2024-02-05 21:52:37,095 	Text Alignment  :	D    D  D   D    D    D D   S       S  S      S    S    S         S   S        S      
2024-02-05 21:52:37,095 ========================================================================================================================
2024-02-05 21:52:37,095 Logging Sequence: 100_50.00
2024-02-05 21:52:37,095 	Gloss Reference :	A B+C+D+E
2024-02-05 21:52:37,096 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:52:37,096 	Gloss Alignment :	         
2024-02-05 21:52:37,096 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:52:37,096 	Text Reference  :	with virat kohli as the captain
2024-02-05 21:52:37,096 	Text Hypothesis :	**** ***** ***** ** how strange
2024-02-05 21:52:37,096 	Text Alignment  :	D    D     D     D  S   S      
2024-02-05 21:52:37,096 ========================================================================================================================
2024-02-05 21:52:37,097 Logging Sequence: 137_44.00
2024-02-05 21:52:37,097 	Gloss Reference :	A B+C+D+E
2024-02-05 21:52:37,097 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:52:37,097 	Gloss Alignment :	         
2024-02-05 21:52:37,097 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:52:37,099 	Text Reference  :	let me tell    you the  rules that       qatar          has  announced for     the     fans travelling for   the world cup 
2024-02-05 21:52:37,099 	Text Hypothesis :	*** ** india's 16  year old   rameshbabu praggnanandhaa from both      cricket council acc  whose      chief is  no    head
2024-02-05 21:52:37,099 	Text Alignment  :	D   D  S       S   S    S     S          S              S    S         S       S       S    S          S     S   S     S   
2024-02-05 21:52:37,099 ========================================================================================================================
2024-02-05 21:52:37,099 Logging Sequence: 58_27.00
2024-02-05 21:52:37,099 	Gloss Reference :	A B+C+D+E
2024-02-05 21:52:37,100 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:52:37,100 	Gloss Alignment :	         
2024-02-05 21:52:37,100 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:52:37,101 	Text Reference  :	*** *** ** ***** the     19th     asian games 2022 were  to be      held       in     hangzhou china     
2024-02-05 21:52:37,101 	Text Hypothesis :	ipl has 10 teams against pakistan in    the   same place of india's contingent during the      tournament
2024-02-05 21:52:37,101 	Text Alignment  :	I   I   I  I     S       S        S     S     S    S     S  S       S          S      S        S         
2024-02-05 21:52:37,101 ========================================================================================================================
2024-02-05 21:52:37,102 Logging Sequence: 75_255.00
2024-02-05 21:52:37,102 	Gloss Reference :	A B+C+D+E  
2024-02-05 21:52:37,102 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-05 21:52:37,102 	Gloss Alignment :	  S        
2024-02-05 21:52:37,102 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:52:37,104 	Text Reference  :	we miss our baby boy  with      this  ronaldo' total baby count       has reached 5     with     2  boys 3       girls
2024-02-05 21:52:37,104 	Text Hypothesis :	** **** *** a    more intensive audit revealed that  if   afghanistan had covered their decision on 25th october 2022 
2024-02-05 21:52:37,104 	Text Alignment  :	D  D    D   S    S    S         S     S        S     S    S           S   S       S     S        S  S    S       S    
2024-02-05 21:52:37,104 ========================================================================================================================
2024-02-05 21:52:39,180 Epoch 866: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.88 
2024-02-05 21:52:39,181 EPOCH 867
2024-02-05 21:52:44,260 Epoch 867: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.34 
2024-02-05 21:52:44,260 EPOCH 868
2024-02-05 21:52:45,311 [Epoch: 868 Step: 00058100] Batch Recognition Loss:   0.001512 => Gls Tokens per Sec:     1676 || Batch Translation Loss:   0.021447 => Txt Tokens per Sec:     5007 || Lr: 0.000050
2024-02-05 21:52:49,691 Epoch 868: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.26 
2024-02-05 21:52:49,691 EPOCH 869
2024-02-05 21:52:52,696 [Epoch: 869 Step: 00058200] Batch Recognition Loss:   0.000149 => Gls Tokens per Sec:     2311 || Batch Translation Loss:   0.027519 => Txt Tokens per Sec:     6250 || Lr: 0.000050
2024-02-05 21:52:54,617 Epoch 869: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.21 
2024-02-05 21:52:54,618 EPOCH 870
2024-02-05 21:53:00,252 Epoch 870: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.80 
2024-02-05 21:53:00,253 EPOCH 871
2024-02-05 21:53:00,976 [Epoch: 871 Step: 00058300] Batch Recognition Loss:   0.000315 => Gls Tokens per Sec:     2216 || Batch Translation Loss:   0.020832 => Txt Tokens per Sec:     6230 || Lr: 0.000050
2024-02-05 21:53:05,079 Epoch 871: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.73 
2024-02-05 21:53:05,079 EPOCH 872
2024-02-05 21:53:08,695 [Epoch: 872 Step: 00058400] Batch Recognition Loss:   0.000211 => Gls Tokens per Sec:     1876 || Batch Translation Loss:   0.023661 => Txt Tokens per Sec:     5292 || Lr: 0.000050
2024-02-05 21:53:10,546 Epoch 872: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.47 
2024-02-05 21:53:10,546 EPOCH 873
2024-02-05 21:53:15,808 Epoch 873: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.51 
2024-02-05 21:53:15,808 EPOCH 874
2024-02-05 21:53:16,469 [Epoch: 874 Step: 00058500] Batch Recognition Loss:   0.000212 => Gls Tokens per Sec:     2181 || Batch Translation Loss:   0.015161 => Txt Tokens per Sec:     5473 || Lr: 0.000050
2024-02-05 21:53:21,323 Epoch 874: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.49 
2024-02-05 21:53:21,324 EPOCH 875
2024-02-05 21:53:24,276 [Epoch: 875 Step: 00058600] Batch Recognition Loss:   0.000465 => Gls Tokens per Sec:     2244 || Batch Translation Loss:   0.012718 => Txt Tokens per Sec:     5985 || Lr: 0.000050
2024-02-05 21:53:26,374 Epoch 875: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.55 
2024-02-05 21:53:26,375 EPOCH 876
2024-02-05 21:53:31,814 Epoch 876: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.99 
2024-02-05 21:53:31,815 EPOCH 877
2024-02-05 21:53:32,281 [Epoch: 877 Step: 00058700] Batch Recognition Loss:   0.000231 => Gls Tokens per Sec:     2756 || Batch Translation Loss:   0.027427 => Txt Tokens per Sec:     6885 || Lr: 0.000050
2024-02-05 21:53:37,003 Epoch 877: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.36 
2024-02-05 21:53:37,004 EPOCH 878
2024-02-05 21:53:40,451 [Epoch: 878 Step: 00058800] Batch Recognition Loss:   0.000531 => Gls Tokens per Sec:     1874 || Batch Translation Loss:   0.055441 => Txt Tokens per Sec:     5191 || Lr: 0.000050
2024-02-05 21:53:42,576 Epoch 878: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.77 
2024-02-05 21:53:42,576 EPOCH 879
2024-02-05 21:53:48,218 Epoch 879: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.83 
2024-02-05 21:53:48,219 EPOCH 880
2024-02-05 21:53:48,783 [Epoch: 880 Step: 00058900] Batch Recognition Loss:   0.000573 => Gls Tokens per Sec:     1989 || Batch Translation Loss:   0.055776 => Txt Tokens per Sec:     5000 || Lr: 0.000050
2024-02-05 21:53:53,573 Epoch 880: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.28 
2024-02-05 21:53:53,574 EPOCH 881
2024-02-05 21:53:56,525 [Epoch: 881 Step: 00059000] Batch Recognition Loss:   0.000460 => Gls Tokens per Sec:     2135 || Batch Translation Loss:   0.060109 => Txt Tokens per Sec:     6029 || Lr: 0.000050
2024-02-05 21:53:58,376 Epoch 881: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.91 
2024-02-05 21:53:58,377 EPOCH 882
2024-02-05 21:54:03,800 Epoch 882: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.25 
2024-02-05 21:54:03,801 EPOCH 883
2024-02-05 21:54:04,166 [Epoch: 883 Step: 00059100] Batch Recognition Loss:   0.000498 => Gls Tokens per Sec:     2630 || Batch Translation Loss:   0.036326 => Txt Tokens per Sec:     6803 || Lr: 0.000050
2024-02-05 21:54:09,118 Epoch 883: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.78 
2024-02-05 21:54:09,119 EPOCH 884
2024-02-05 21:54:12,333 [Epoch: 884 Step: 00059200] Batch Recognition Loss:   0.000257 => Gls Tokens per Sec:     1943 || Batch Translation Loss:   0.020010 => Txt Tokens per Sec:     5329 || Lr: 0.000050
2024-02-05 21:54:14,565 Epoch 884: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.65 
2024-02-05 21:54:14,566 EPOCH 885
2024-02-05 21:54:19,328 Epoch 885: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.82 
2024-02-05 21:54:19,329 EPOCH 886
2024-02-05 21:54:19,760 [Epoch: 886 Step: 00059300] Batch Recognition Loss:   0.000223 => Gls Tokens per Sec:     1868 || Batch Translation Loss:   0.028246 => Txt Tokens per Sec:     5392 || Lr: 0.000050
2024-02-05 21:54:24,694 Epoch 886: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.67 
2024-02-05 21:54:24,695 EPOCH 887
2024-02-05 21:54:27,332 [Epoch: 887 Step: 00059400] Batch Recognition Loss:   0.000370 => Gls Tokens per Sec:     2307 || Batch Translation Loss:   0.030813 => Txt Tokens per Sec:     6353 || Lr: 0.000050
2024-02-05 21:54:29,581 Epoch 887: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.28 
2024-02-05 21:54:29,581 EPOCH 888
2024-02-05 21:54:34,974 Epoch 888: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.00 
2024-02-05 21:54:34,974 EPOCH 889
2024-02-05 21:54:35,223 [Epoch: 889 Step: 00059500] Batch Recognition Loss:   0.000314 => Gls Tokens per Sec:     2581 || Batch Translation Loss:   0.037926 => Txt Tokens per Sec:     6351 || Lr: 0.000050
2024-02-05 21:54:39,986 Epoch 889: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.65 
2024-02-05 21:54:39,987 EPOCH 890
2024-02-05 21:54:42,902 [Epoch: 890 Step: 00059600] Batch Recognition Loss:   0.000297 => Gls Tokens per Sec:     1997 || Batch Translation Loss:   0.022566 => Txt Tokens per Sec:     5357 || Lr: 0.000050
2024-02-05 21:54:45,303 Epoch 890: Total Training Recognition Loss 0.09  Total Training Translation Loss 2.05 
2024-02-05 21:54:45,303 EPOCH 891
2024-02-05 21:54:50,545 Epoch 891: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.93 
2024-02-05 21:54:50,546 EPOCH 892
2024-02-05 21:54:50,821 [Epoch: 892 Step: 00059700] Batch Recognition Loss:   0.000528 => Gls Tokens per Sec:     1752 || Batch Translation Loss:   0.022719 => Txt Tokens per Sec:     5588 || Lr: 0.000050
2024-02-05 21:54:56,114 Epoch 892: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.96 
2024-02-05 21:54:56,115 EPOCH 893
2024-02-05 21:54:59,285 [Epoch: 893 Step: 00059800] Batch Recognition Loss:   0.000575 => Gls Tokens per Sec:     1818 || Batch Translation Loss:   0.016259 => Txt Tokens per Sec:     5151 || Lr: 0.000050
2024-02-05 21:55:01,682 Epoch 893: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.45 
2024-02-05 21:55:01,683 EPOCH 894
2024-02-05 21:55:06,803 Epoch 894: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.52 
2024-02-05 21:55:06,804 EPOCH 895
2024-02-05 21:55:06,931 [Epoch: 895 Step: 00059900] Batch Recognition Loss:   0.000358 => Gls Tokens per Sec:     2520 || Batch Translation Loss:   0.016499 => Txt Tokens per Sec:     7157 || Lr: 0.000050
2024-02-05 21:55:12,241 Epoch 895: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.55 
2024-02-05 21:55:12,242 EPOCH 896
2024-02-05 21:55:14,969 [Epoch: 896 Step: 00060000] Batch Recognition Loss:   0.000190 => Gls Tokens per Sec:     2018 || Batch Translation Loss:   0.023824 => Txt Tokens per Sec:     5526 || Lr: 0.000050
2024-02-05 21:55:23,345 Validation result at epoch 896, step    60000: duration: 8.3760s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.50044	Translation Loss: 92219.97656	PPL: 10006.04004
	Eval Metric: BLEU
	WER 3.67	(DEL: 0.00,	INS: 0.00,	SUB: 3.67)
	BLEU-4 0.75	(BLEU-1: 11.23,	BLEU-2: 3.53,	BLEU-3: 1.47,	BLEU-4: 0.75)
	CHRF 16.97	ROUGE 9.41
2024-02-05 21:55:23,346 Logging Recognition and Translation Outputs
2024-02-05 21:55:23,346 ========================================================================================================================
2024-02-05 21:55:23,347 Logging Sequence: 75_58.00
2024-02-05 21:55:23,347 	Gloss Reference :	A B+C+D+E
2024-02-05 21:55:23,347 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:55:23,347 	Gloss Alignment :	         
2024-02-05 21:55:23,347 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:55:23,349 	Text Reference  :	it seems like  he like to      date   women and does not want to get married people seem to respect his  choices
2024-02-05 21:55:23,349 	Text Hypothesis :	it is    known as the  british empire games and **** *** **** ** *** ******* ****** **** ** are     very well   
2024-02-05 21:55:23,349 	Text Alignment  :	   S     S     S  S    S       S      S         D    D   D    D  D   D       D      D    D  S       S    S      
2024-02-05 21:55:23,349 ========================================================================================================================
2024-02-05 21:55:23,349 Logging Sequence: 152_113.00
2024-02-05 21:55:23,349 	Gloss Reference :	A B+C+D+E
2024-02-05 21:55:23,350 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:55:23,350 	Gloss Alignment :	         
2024-02-05 21:55:23,350 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:55:23,351 	Text Reference  :	**** **** *** indians hoping for a     victory were  distraught at    the   defeat
2024-02-05 21:55:23,351 	Text Hypothesis :	what were his happy   by     the event was     glued to         their first time  
2024-02-05 21:55:23,351 	Text Alignment  :	I    I    I   S       S      S   S     S       S     S          S     S     S     
2024-02-05 21:55:23,351 ========================================================================================================================
2024-02-05 21:55:23,351 Logging Sequence: 176_41.00
2024-02-05 21:55:23,351 	Gloss Reference :	A B+C+D+E
2024-02-05 21:55:23,351 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:55:23,352 	Gloss Alignment :	         
2024-02-05 21:55:23,352 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:55:23,352 	Text Reference  :	dahiya did not loose  hope and put     up  a  strong fight
2024-02-05 21:55:23,353 	Text Hypothesis :	it     was a   strong bout and sanayev was in the    lead 
2024-02-05 21:55:23,353 	Text Alignment  :	S      S   S   S      S        S       S   S  S      S    
2024-02-05 21:55:23,353 ========================================================================================================================
2024-02-05 21:55:23,353 Logging Sequence: 77_190.00
2024-02-05 21:55:23,353 	Gloss Reference :	A B+C+D+E
2024-02-05 21:55:23,353 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:55:23,353 	Gloss Alignment :	         
2024-02-05 21:55:23,354 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:55:23,355 	Text Reference  :	there are many  batsmen who   have   scrored 36    runs   in     6           balls
2024-02-05 21:55:23,355 	Text Hypothesis :	on    the first 3       balls bowled by      patel jadeja scored consecutive sixes
2024-02-05 21:55:23,355 	Text Alignment  :	S     S   S     S       S     S      S       S     S      S      S           S    
2024-02-05 21:55:23,355 ========================================================================================================================
2024-02-05 21:55:23,355 Logging Sequence: 155_170.00
2024-02-05 21:55:23,355 	Gloss Reference :	A B+C+D+E
2024-02-05 21:55:23,355 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:55:23,355 	Gloss Alignment :	         
2024-02-05 21:55:23,355 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:55:23,356 	Text Reference  :	india lost the matches and could not secure a place in  the semi final  
2024-02-05 21:55:23,356 	Text Hypothesis :	we    hope the ******* *** ***** *** ****** * bcci  has won many wickets
2024-02-05 21:55:23,357 	Text Alignment  :	S     S        D       D   D     D   D      D S     S   S   S    S      
2024-02-05 21:55:23,357 ========================================================================================================================
2024-02-05 21:55:26,007 Epoch 896: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.00 
2024-02-05 21:55:26,007 EPOCH 897
2024-02-05 21:55:31,392 Epoch 897: Total Training Recognition Loss 0.06  Total Training Translation Loss 5.01 
2024-02-05 21:55:31,392 EPOCH 898
2024-02-05 21:55:31,455 [Epoch: 898 Step: 00060100] Batch Recognition Loss:   0.000205 => Gls Tokens per Sec:     2581 || Batch Translation Loss:   0.040746 => Txt Tokens per Sec:     5129 || Lr: 0.000050
2024-02-05 21:55:36,724 Epoch 898: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.63 
2024-02-05 21:55:36,725 EPOCH 899
2024-02-05 21:55:39,414 [Epoch: 899 Step: 00060200] Batch Recognition Loss:   0.000295 => Gls Tokens per Sec:     2024 || Batch Translation Loss:   0.024346 => Txt Tokens per Sec:     5617 || Lr: 0.000050
2024-02-05 21:55:42,086 Epoch 899: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.88 
2024-02-05 21:55:42,087 EPOCH 900
2024-02-05 21:55:46,811 [Epoch: 900 Step: 00060300] Batch Recognition Loss:   0.000249 => Gls Tokens per Sec:     2248 || Batch Translation Loss:   0.036691 => Txt Tokens per Sec:     6219 || Lr: 0.000050
2024-02-05 21:55:46,811 Epoch 900: Total Training Recognition Loss 0.09  Total Training Translation Loss 3.35 
2024-02-05 21:55:46,811 EPOCH 901
2024-02-05 21:55:52,384 Epoch 901: Total Training Recognition Loss 0.09  Total Training Translation Loss 2.36 
2024-02-05 21:55:52,385 EPOCH 902
2024-02-05 21:55:54,877 [Epoch: 902 Step: 00060400] Batch Recognition Loss:   0.000223 => Gls Tokens per Sec:     2119 || Batch Translation Loss:   0.028344 => Txt Tokens per Sec:     5880 || Lr: 0.000050
2024-02-05 21:55:57,232 Epoch 902: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.53 
2024-02-05 21:55:57,232 EPOCH 903
2024-02-05 21:56:02,628 [Epoch: 903 Step: 00060500] Batch Recognition Loss:   0.000122 => Gls Tokens per Sec:     1939 || Batch Translation Loss:   0.020511 => Txt Tokens per Sec:     5349 || Lr: 0.000050
2024-02-05 21:56:02,698 Epoch 903: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.32 
2024-02-05 21:56:02,698 EPOCH 904
2024-02-05 21:56:07,526 Epoch 904: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.92 
2024-02-05 21:56:07,527 EPOCH 905
2024-02-05 21:56:10,106 [Epoch: 905 Step: 00060600] Batch Recognition Loss:   0.000186 => Gls Tokens per Sec:     1947 || Batch Translation Loss:   0.014158 => Txt Tokens per Sec:     5314 || Lr: 0.000050
2024-02-05 21:56:13,003 Epoch 905: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.30 
2024-02-05 21:56:13,003 EPOCH 906
2024-02-05 21:56:18,052 [Epoch: 906 Step: 00060700] Batch Recognition Loss:   0.000148 => Gls Tokens per Sec:     2061 || Batch Translation Loss:   0.016838 => Txt Tokens per Sec:     5717 || Lr: 0.000050
2024-02-05 21:56:18,228 Epoch 906: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.79 
2024-02-05 21:56:18,228 EPOCH 907
2024-02-05 21:56:23,376 Epoch 907: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.36 
2024-02-05 21:56:23,376 EPOCH 908
2024-02-05 21:56:25,502 [Epoch: 908 Step: 00060800] Batch Recognition Loss:   0.000412 => Gls Tokens per Sec:     2288 || Batch Translation Loss:   0.086412 => Txt Tokens per Sec:     6332 || Lr: 0.000050
2024-02-05 21:56:28,525 Epoch 908: Total Training Recognition Loss 0.10  Total Training Translation Loss 2.75 
2024-02-05 21:56:28,525 EPOCH 909
2024-02-05 21:56:33,605 [Epoch: 909 Step: 00060900] Batch Recognition Loss:   0.000255 => Gls Tokens per Sec:     1996 || Batch Translation Loss:   0.023161 => Txt Tokens per Sec:     5536 || Lr: 0.000050
2024-02-05 21:56:33,784 Epoch 909: Total Training Recognition Loss 0.06  Total Training Translation Loss 5.06 
2024-02-05 21:56:33,784 EPOCH 910
2024-02-05 21:56:39,072 Epoch 910: Total Training Recognition Loss 0.07  Total Training Translation Loss 5.63 
2024-02-05 21:56:39,073 EPOCH 911
2024-02-05 21:56:41,271 [Epoch: 911 Step: 00061000] Batch Recognition Loss:   0.000705 => Gls Tokens per Sec:     2185 || Batch Translation Loss:   0.030518 => Txt Tokens per Sec:     6003 || Lr: 0.000050
2024-02-05 21:56:44,018 Epoch 911: Total Training Recognition Loss 0.10  Total Training Translation Loss 5.57 
2024-02-05 21:56:44,018 EPOCH 912
2024-02-05 21:56:48,917 [Epoch: 912 Step: 00061100] Batch Recognition Loss:   0.000389 => Gls Tokens per Sec:     2037 || Batch Translation Loss:   0.035729 => Txt Tokens per Sec:     5614 || Lr: 0.000050
2024-02-05 21:56:49,253 Epoch 912: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.54 
2024-02-05 21:56:49,254 EPOCH 913
2024-02-05 21:56:54,652 Epoch 913: Total Training Recognition Loss 0.09  Total Training Translation Loss 2.14 
2024-02-05 21:56:54,653 EPOCH 914
2024-02-05 21:56:57,309 [Epoch: 914 Step: 00061200] Batch Recognition Loss:   0.000323 => Gls Tokens per Sec:     1710 || Batch Translation Loss:   0.030322 => Txt Tokens per Sec:     4951 || Lr: 0.000050
2024-02-05 21:57:00,105 Epoch 914: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.30 
2024-02-05 21:57:00,105 EPOCH 915
2024-02-05 21:57:04,500 [Epoch: 915 Step: 00061300] Batch Recognition Loss:   0.002015 => Gls Tokens per Sec:     2235 || Batch Translation Loss:   0.026703 => Txt Tokens per Sec:     6230 || Lr: 0.000050
2024-02-05 21:57:04,836 Epoch 915: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.64 
2024-02-05 21:57:04,837 EPOCH 916
2024-02-05 21:57:10,324 Epoch 916: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.36 
2024-02-05 21:57:10,325 EPOCH 917
2024-02-05 21:57:12,261 [Epoch: 917 Step: 00061400] Batch Recognition Loss:   0.000162 => Gls Tokens per Sec:     2263 || Batch Translation Loss:   0.011530 => Txt Tokens per Sec:     6232 || Lr: 0.000050
2024-02-05 21:57:15,088 Epoch 917: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.02 
2024-02-05 21:57:15,088 EPOCH 918
2024-02-05 21:57:20,167 [Epoch: 918 Step: 00061500] Batch Recognition Loss:   0.001244 => Gls Tokens per Sec:     1903 || Batch Translation Loss:   0.010910 => Txt Tokens per Sec:     5256 || Lr: 0.000050
2024-02-05 21:57:20,635 Epoch 918: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.37 
2024-02-05 21:57:20,635 EPOCH 919
2024-02-05 21:57:25,443 Epoch 919: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.77 
2024-02-05 21:57:25,443 EPOCH 920
2024-02-05 21:57:27,622 [Epoch: 920 Step: 00061600] Batch Recognition Loss:   0.001200 => Gls Tokens per Sec:     1938 || Batch Translation Loss:   0.008406 => Txt Tokens per Sec:     5180 || Lr: 0.000050
2024-02-05 21:57:30,876 Epoch 920: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.26 
2024-02-05 21:57:30,877 EPOCH 921
2024-02-05 21:57:35,365 [Epoch: 921 Step: 00061700] Batch Recognition Loss:   0.001422 => Gls Tokens per Sec:     2117 || Batch Translation Loss:   0.012158 => Txt Tokens per Sec:     5870 || Lr: 0.000050
2024-02-05 21:57:35,928 Epoch 921: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.15 
2024-02-05 21:57:35,928 EPOCH 922
2024-02-05 21:57:41,563 Epoch 922: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.46 
2024-02-05 21:57:41,564 EPOCH 923
2024-02-05 21:57:43,607 [Epoch: 923 Step: 00061800] Batch Recognition Loss:   0.000228 => Gls Tokens per Sec:     2037 || Batch Translation Loss:   0.020204 => Txt Tokens per Sec:     5795 || Lr: 0.000050
2024-02-05 21:57:46,832 Epoch 923: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.36 
2024-02-05 21:57:46,833 EPOCH 924
2024-02-05 21:57:51,383 [Epoch: 924 Step: 00061900] Batch Recognition Loss:   0.000471 => Gls Tokens per Sec:     2054 || Batch Translation Loss:   0.026307 => Txt Tokens per Sec:     5689 || Lr: 0.000050
2024-02-05 21:57:51,953 Epoch 924: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.18 
2024-02-05 21:57:51,953 EPOCH 925
2024-02-05 21:57:56,908 Epoch 925: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.56 
2024-02-05 21:57:56,909 EPOCH 926
2024-02-05 21:57:58,735 [Epoch: 926 Step: 00062000] Batch Recognition Loss:   0.000233 => Gls Tokens per Sec:     2193 || Batch Translation Loss:   0.033400 => Txt Tokens per Sec:     5939 || Lr: 0.000050
2024-02-05 21:58:07,372 Validation result at epoch 926, step    62000: duration: 8.6371s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.71431	Translation Loss: 91838.18750	PPL: 9631.66602
	Eval Metric: BLEU
	WER 2.97	(DEL: 0.00,	INS: 0.00,	SUB: 2.97)
	BLEU-4 0.42	(BLEU-1: 9.89,	BLEU-2: 3.04,	BLEU-3: 1.02,	BLEU-4: 0.42)
	CHRF 16.81	ROUGE 8.14
2024-02-05 21:58:07,373 Logging Recognition and Translation Outputs
2024-02-05 21:58:07,373 ========================================================================================================================
2024-02-05 21:58:07,374 Logging Sequence: 165_523.00
2024-02-05 21:58:07,374 	Gloss Reference :	A B+C+D+E
2024-02-05 21:58:07,374 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:58:07,374 	Gloss Alignment :	         
2024-02-05 21:58:07,374 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:58:07,375 	Text Reference  :	as he believed that his team might lose  if he takes off   his    batting pads
2024-02-05 21:58:07,376 	Text Hypothesis :	** he came     out  as  a    huge  shock as he was   never remove the     pads
2024-02-05 21:58:07,376 	Text Alignment  :	D     S        S    S   S    S     S     S     S     S     S      S           
2024-02-05 21:58:07,376 ========================================================================================================================
2024-02-05 21:58:07,376 Logging Sequence: 165_233.00
2024-02-05 21:58:07,376 	Gloss Reference :	A B+C+D+E
2024-02-05 21:58:07,376 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:58:07,376 	Gloss Alignment :	         
2024-02-05 21:58:07,377 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:58:07,378 	Text Reference  :	irrespective of whether he was playing the match or not he always sat with his bag he   was happy when the  team    won
2024-02-05 21:58:07,378 	Text Hypothesis :	************ ** ******* ** *** ******* *** ***** ** *** ** ****** *** **** his *** team was ***** so   what happens ipl
2024-02-05 21:58:07,378 	Text Alignment  :	D            D  D       D  D   D       D   D     D  D   D  D      D   D        D   S        D     S    S    S       S  
2024-02-05 21:58:07,378 ========================================================================================================================
2024-02-05 21:58:07,378 Logging Sequence: 169_214.00
2024-02-05 21:58:07,378 	Gloss Reference :	A B+C+D+E
2024-02-05 21:58:07,378 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:58:07,379 	Gloss Alignment :	         
2024-02-05 21:58:07,379 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:58:07,381 	Text Reference  :	virat kohli said that though arshdeep dropped   the      catch       he is     still a strong part   of  the    indian     team
2024-02-05 21:58:07,381 	Text Hypothesis :	***** you   know that ****** ******** wikipedia provides information on celebs like  a ****** height age family background etc 
2024-02-05 21:58:07,381 	Text Alignment  :	D     S     S         D      D        S         S        S           S  S      S       D      S      S   S      S          S   
2024-02-05 21:58:07,381 ========================================================================================================================
2024-02-05 21:58:07,381 Logging Sequence: 88_67.00
2024-02-05 21:58:07,381 	Gloss Reference :	A B+C+D+E
2024-02-05 21:58:07,382 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:58:07,382 	Gloss Alignment :	         
2024-02-05 21:58:07,382 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:58:07,384 	Text Reference  :	* pablo    javkin the    mayor of      rosario is     also a  drug trafficker so    he     won't  take  care of     you      
2024-02-05 21:58:07,384 	Text Hypothesis :	1 france's kylian mbappe was   awarded the     golden boot as with 8          goals really police filed a    police complaint
2024-02-05 21:58:07,384 	Text Alignment  :	I S        S      S      S     S       S       S      S    S  S    S          S     S      S      S     S    S      S        
2024-02-05 21:58:07,384 ========================================================================================================================
2024-02-05 21:58:07,384 Logging Sequence: 69_95.00
2024-02-05 21:58:07,385 	Gloss Reference :	A B+C+D+E
2024-02-05 21:58:07,385 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 21:58:07,385 	Gloss Alignment :	         
2024-02-05 21:58:07,385 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 21:58:07,387 	Text Reference  :	**** **** **** a  six and   a  four sealed   csk's victory and **** ** ****** the team won   the match
2024-02-05 21:58:07,387 	Text Hypothesis :	fans said that he was tired by the  exacting game  and     and when he wanted was to   watch the match
2024-02-05 21:58:07,387 	Text Alignment  :	I    I    I    S  S   S     S  S    S        S     S           I    I  I      S   S    S              
2024-02-05 21:58:07,387 ========================================================================================================================
2024-02-05 21:58:10,935 Epoch 926: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.76 
2024-02-05 21:58:10,936 EPOCH 927
2024-02-05 21:58:15,153 [Epoch: 927 Step: 00062100] Batch Recognition Loss:   0.000140 => Gls Tokens per Sec:     2177 || Batch Translation Loss:   0.028949 => Txt Tokens per Sec:     5960 || Lr: 0.000050
2024-02-05 21:58:16,056 Epoch 927: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.30 
2024-02-05 21:58:16,057 EPOCH 928
2024-02-05 21:58:21,512 Epoch 928: Total Training Recognition Loss 0.07  Total Training Translation Loss 6.54 
2024-02-05 21:58:21,512 EPOCH 929
2024-02-05 21:58:23,135 [Epoch: 929 Step: 00062200] Batch Recognition Loss:   0.000511 => Gls Tokens per Sec:     2367 || Batch Translation Loss:   0.129724 => Txt Tokens per Sec:     6587 || Lr: 0.000050
2024-02-05 21:58:26,554 Epoch 929: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.96 
2024-02-05 21:58:26,554 EPOCH 930
2024-02-05 21:58:30,867 [Epoch: 930 Step: 00062300] Batch Recognition Loss:   0.000192 => Gls Tokens per Sec:     2092 || Batch Translation Loss:   0.016229 => Txt Tokens per Sec:     5777 || Lr: 0.000050
2024-02-05 21:58:31,700 Epoch 930: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.19 
2024-02-05 21:58:31,700 EPOCH 931
2024-02-05 21:58:36,852 Epoch 931: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.69 
2024-02-05 21:58:36,852 EPOCH 932
2024-02-05 21:58:38,522 [Epoch: 932 Step: 00062400] Batch Recognition Loss:   0.000311 => Gls Tokens per Sec:     2145 || Batch Translation Loss:   0.022258 => Txt Tokens per Sec:     5557 || Lr: 0.000050
2024-02-05 21:58:41,991 Epoch 932: Total Training Recognition Loss 0.09  Total Training Translation Loss 2.24 
2024-02-05 21:58:41,991 EPOCH 933
2024-02-05 21:58:46,087 [Epoch: 933 Step: 00062500] Batch Recognition Loss:   0.001201 => Gls Tokens per Sec:     2164 || Batch Translation Loss:   0.028067 => Txt Tokens per Sec:     6093 || Lr: 0.000050
2024-02-05 21:58:46,692 Epoch 933: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.18 
2024-02-05 21:58:46,693 EPOCH 934
2024-02-05 21:58:51,916 Epoch 934: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.73 
2024-02-05 21:58:51,917 EPOCH 935
2024-02-05 21:58:53,633 [Epoch: 935 Step: 00062600] Batch Recognition Loss:   0.000174 => Gls Tokens per Sec:     1994 || Batch Translation Loss:   0.012377 => Txt Tokens per Sec:     5336 || Lr: 0.000050
2024-02-05 21:58:56,965 Epoch 935: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.81 
2024-02-05 21:58:56,965 EPOCH 936
2024-02-05 21:59:01,142 [Epoch: 936 Step: 00062700] Batch Recognition Loss:   0.000245 => Gls Tokens per Sec:     2083 || Batch Translation Loss:   0.024145 => Txt Tokens per Sec:     5704 || Lr: 0.000050
2024-02-05 21:59:02,207 Epoch 936: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.85 
2024-02-05 21:59:02,207 EPOCH 937
2024-02-05 21:59:07,329 Epoch 937: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.62 
2024-02-05 21:59:07,329 EPOCH 938
2024-02-05 21:59:08,822 [Epoch: 938 Step: 00062800] Batch Recognition Loss:   0.000250 => Gls Tokens per Sec:     2252 || Batch Translation Loss:   0.096546 => Txt Tokens per Sec:     6280 || Lr: 0.000050
2024-02-05 21:59:12,483 Epoch 938: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.38 
2024-02-05 21:59:12,483 EPOCH 939
2024-02-05 21:59:16,648 [Epoch: 939 Step: 00062900] Batch Recognition Loss:   0.000686 => Gls Tokens per Sec:     2051 || Batch Translation Loss:   0.005865 => Txt Tokens per Sec:     5701 || Lr: 0.000050
2024-02-05 21:59:17,581 Epoch 939: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.57 
2024-02-05 21:59:17,582 EPOCH 940
2024-02-05 21:59:22,670 Epoch 940: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.27 
2024-02-05 21:59:22,671 EPOCH 941
2024-02-05 21:59:24,316 [Epoch: 941 Step: 00063000] Batch Recognition Loss:   0.001228 => Gls Tokens per Sec:     1885 || Batch Translation Loss:   0.007507 => Txt Tokens per Sec:     5200 || Lr: 0.000050
2024-02-05 21:59:28,013 Epoch 941: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.67 
2024-02-05 21:59:28,013 EPOCH 942
2024-02-05 21:59:32,106 [Epoch: 942 Step: 00063100] Batch Recognition Loss:   0.000141 => Gls Tokens per Sec:     2047 || Batch Translation Loss:   0.013491 => Txt Tokens per Sec:     5744 || Lr: 0.000050
2024-02-05 21:59:33,108 Epoch 942: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.80 
2024-02-05 21:59:33,109 EPOCH 943
2024-02-05 21:59:38,193 Epoch 943: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.69 
2024-02-05 21:59:38,193 EPOCH 944
2024-02-05 21:59:39,502 [Epoch: 944 Step: 00063200] Batch Recognition Loss:   0.000255 => Gls Tokens per Sec:     2323 || Batch Translation Loss:   0.718703 => Txt Tokens per Sec:     6749 || Lr: 0.000050
2024-02-05 21:59:43,625 Epoch 944: Total Training Recognition Loss 0.05  Total Training Translation Loss 4.29 
2024-02-05 21:59:43,626 EPOCH 945
2024-02-05 21:59:47,714 [Epoch: 945 Step: 00063300] Batch Recognition Loss:   0.000203 => Gls Tokens per Sec:     2011 || Batch Translation Loss:   0.017885 => Txt Tokens per Sec:     5535 || Lr: 0.000050
2024-02-05 21:59:48,829 Epoch 945: Total Training Recognition Loss 0.11  Total Training Translation Loss 4.83 
2024-02-05 21:59:48,829 EPOCH 946
2024-02-05 21:59:54,418 Epoch 946: Total Training Recognition Loss 0.11  Total Training Translation Loss 3.18 
2024-02-05 21:59:54,418 EPOCH 947
2024-02-05 21:59:55,779 [Epoch: 947 Step: 00063400] Batch Recognition Loss:   0.000268 => Gls Tokens per Sec:     2044 || Batch Translation Loss:   0.024581 => Txt Tokens per Sec:     5586 || Lr: 0.000050
2024-02-05 21:59:59,452 Epoch 947: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.84 
2024-02-05 21:59:59,453 EPOCH 948
2024-02-05 22:00:03,273 [Epoch: 948 Step: 00063500] Batch Recognition Loss:   0.000532 => Gls Tokens per Sec:     2136 || Batch Translation Loss:   0.031539 => Txt Tokens per Sec:     5904 || Lr: 0.000050
2024-02-05 22:00:04,694 Epoch 948: Total Training Recognition Loss 0.09  Total Training Translation Loss 3.12 
2024-02-05 22:00:04,694 EPOCH 949
2024-02-05 22:00:09,861 Epoch 949: Total Training Recognition Loss 0.06  Total Training Translation Loss 4.11 
2024-02-05 22:00:09,861 EPOCH 950
2024-02-05 22:00:11,094 [Epoch: 950 Step: 00063600] Batch Recognition Loss:   0.001018 => Gls Tokens per Sec:     2126 || Batch Translation Loss:   0.032489 => Txt Tokens per Sec:     6141 || Lr: 0.000050
2024-02-05 22:00:15,062 Epoch 950: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.93 
2024-02-05 22:00:15,063 EPOCH 951
2024-02-05 22:00:18,808 [Epoch: 951 Step: 00063700] Batch Recognition Loss:   0.007184 => Gls Tokens per Sec:     2110 || Batch Translation Loss:   0.034923 => Txt Tokens per Sec:     5902 || Lr: 0.000050
2024-02-05 22:00:19,969 Epoch 951: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.29 
2024-02-05 22:00:19,969 EPOCH 952
2024-02-05 22:00:25,223 Epoch 952: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.84 
2024-02-05 22:00:25,224 EPOCH 953
2024-02-05 22:00:26,336 [Epoch: 953 Step: 00063800] Batch Recognition Loss:   0.000346 => Gls Tokens per Sec:     2305 || Batch Translation Loss:   0.030073 => Txt Tokens per Sec:     6278 || Lr: 0.000050
2024-02-05 22:00:30,684 Epoch 953: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.41 
2024-02-05 22:00:30,685 EPOCH 954
2024-02-05 22:00:34,711 [Epoch: 954 Step: 00063900] Batch Recognition Loss:   0.000168 => Gls Tokens per Sec:     1923 || Batch Translation Loss:   0.020506 => Txt Tokens per Sec:     5404 || Lr: 0.000050
2024-02-05 22:00:35,980 Epoch 954: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.33 
2024-02-05 22:00:35,980 EPOCH 955
2024-02-05 22:00:40,808 Epoch 955: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.88 
2024-02-05 22:00:40,809 EPOCH 956
2024-02-05 22:00:42,146 [Epoch: 956 Step: 00064000] Batch Recognition Loss:   0.000211 => Gls Tokens per Sec:     1796 || Batch Translation Loss:   0.018582 => Txt Tokens per Sec:     5052 || Lr: 0.000050
2024-02-05 22:00:50,446 Validation result at epoch 956, step    64000: duration: 8.2979s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.58785	Translation Loss: 92505.66406	PPL: 10295.67383
	Eval Metric: BLEU
	WER 3.32	(DEL: 0.00,	INS: 0.00,	SUB: 3.32)
	BLEU-4 0.39	(BLEU-1: 10.04,	BLEU-2: 2.76,	BLEU-3: 0.92,	BLEU-4: 0.39)
	CHRF 16.55	ROUGE 8.46
2024-02-05 22:00:50,447 Logging Recognition and Translation Outputs
2024-02-05 22:00:50,447 ========================================================================================================================
2024-02-05 22:00:50,447 Logging Sequence: 122_86.00
2024-02-05 22:00:50,447 	Gloss Reference :	A B+C+D+E
2024-02-05 22:00:50,447 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:00:50,447 	Gloss Alignment :	         
2024-02-05 22:00:50,448 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:00:50,448 	Text Reference  :	after   winning chanu spoke   to     the ***** *** **** media and said   
2024-02-05 22:00:50,448 	Text Hypothesis :	however a       few   minutes before the score was held in    the stadium
2024-02-05 22:00:50,449 	Text Alignment  :	S       S       S     S       S          I     I   I    S     S   S      
2024-02-05 22:00:50,449 ========================================================================================================================
2024-02-05 22:00:50,449 Logging Sequence: 82_81.00
2024-02-05 22:00:50,449 	Gloss Reference :	A B+C+D+E
2024-02-05 22:00:50,449 	Gloss Hypothesis:	A B+C+D  
2024-02-05 22:00:50,449 	Gloss Alignment :	  S      
2024-02-05 22:00:50,449 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:00:50,451 	Text Reference  :	since the couple were residents of **** mumbai ******* **** the     mumbai police  cyber cell     began investigating the   matter 
2024-02-05 22:00:50,451 	Text Hypothesis :	***** *** ****** **** then      of this mumbai indians team doctors nurses physios and   everyone else  for           their support
2024-02-05 22:00:50,451 	Text Alignment  :	D     D   D      D    S            I           I       I    S       S      S       S     S        S     S             S     S      
2024-02-05 22:00:50,451 ========================================================================================================================
2024-02-05 22:00:50,451 Logging Sequence: 61_65.00
2024-02-05 22:00:50,452 	Gloss Reference :	A B+C+D+E
2024-02-05 22:00:50,452 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:00:50,452 	Gloss Alignment :	         
2024-02-05 22:00:50,452 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:00:50,453 	Text Reference  :	the name seems indian but  whether it  has been made by   an      indian
2024-02-05 22:00:50,453 	Text Hypothesis :	*** **** ***** ****** that they    did not want to   play against chahal
2024-02-05 22:00:50,453 	Text Alignment  :	D   D    D     D      S    S       S   S   S    S    S    S       S     
2024-02-05 22:00:50,454 ========================================================================================================================
2024-02-05 22:00:50,454 Logging Sequence: 179_126.00
2024-02-05 22:00:50,454 	Gloss Reference :	A B+C+D+E
2024-02-05 22:00:50,454 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:00:50,454 	Gloss Alignment :	         
2024-02-05 22:00:50,454 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:00:50,456 	Text Reference  :	*** ********** ** vinesh argued that she might contract coronavirus since these wrestlers travelled from    india where there are many infections
2024-02-05 22:00:50,456 	Text Hypothesis :	the federation or sai    people want to  pick  their    office      at    par   with      the       airport on    the   day   of  her  win       
2024-02-05 22:00:50,457 	Text Alignment  :	I   I          I  S      S      S    S   S     S        S           S     S     S         S         S       S     S     S     S   S    S         
2024-02-05 22:00:50,457 ========================================================================================================================
2024-02-05 22:00:50,457 Logging Sequence: 62_24.00
2024-02-05 22:00:50,457 	Gloss Reference :	A B+C+D+E
2024-02-05 22:00:50,457 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:00:50,457 	Gloss Alignment :	         
2024-02-05 22:00:50,458 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:00:50,459 	Text Reference  :	now the women's cricket team too   is giving splendid performances which are  at       par with    the men's team   
2024-02-05 22:00:50,459 	Text Hypothesis :	*** do  you     know    that daley is ****** ******** ************ a     huge argument on  england in  uttar pradesh
2024-02-05 22:00:50,459 	Text Alignment  :	D   S   S       S       S    S        D      D        D            S     S    S        S   S       S   S     S      
2024-02-05 22:00:50,459 ========================================================================================================================
2024-02-05 22:00:54,579 Epoch 956: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.38 
2024-02-05 22:00:54,580 EPOCH 957
2024-02-05 22:00:58,088 [Epoch: 957 Step: 00064100] Batch Recognition Loss:   0.001583 => Gls Tokens per Sec:     2161 || Batch Translation Loss:   0.053513 => Txt Tokens per Sec:     6095 || Lr: 0.000050
2024-02-05 22:00:59,524 Epoch 957: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.30 
2024-02-05 22:00:59,524 EPOCH 958
2024-02-05 22:01:05,155 Epoch 958: Total Training Recognition Loss 0.06  Total Training Translation Loss 4.75 
2024-02-05 22:01:05,156 EPOCH 959
2024-02-05 22:01:06,217 [Epoch: 959 Step: 00064200] Batch Recognition Loss:   0.000331 => Gls Tokens per Sec:     2112 || Batch Translation Loss:   0.041815 => Txt Tokens per Sec:     6069 || Lr: 0.000050
2024-02-05 22:01:10,183 Epoch 959: Total Training Recognition Loss 0.08  Total Training Translation Loss 5.55 
2024-02-05 22:01:10,184 EPOCH 960
2024-02-05 22:01:14,108 [Epoch: 960 Step: 00064300] Batch Recognition Loss:   0.000373 => Gls Tokens per Sec:     1892 || Batch Translation Loss:   0.037253 => Txt Tokens per Sec:     5408 || Lr: 0.000050
2024-02-05 22:01:15,488 Epoch 960: Total Training Recognition Loss 0.08  Total Training Translation Loss 3.18 
2024-02-05 22:01:15,489 EPOCH 961
2024-02-05 22:01:20,668 Epoch 961: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.06 
2024-02-05 22:01:20,669 EPOCH 962
2024-02-05 22:01:21,536 [Epoch: 962 Step: 00064400] Batch Recognition Loss:   0.000354 => Gls Tokens per Sec:     2402 || Batch Translation Loss:   0.019414 => Txt Tokens per Sec:     6225 || Lr: 0.000050
2024-02-05 22:01:25,880 Epoch 962: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.78 
2024-02-05 22:01:25,880 EPOCH 963
2024-02-05 22:01:29,463 [Epoch: 963 Step: 00064500] Batch Recognition Loss:   0.000803 => Gls Tokens per Sec:     2055 || Batch Translation Loss:   0.006027 => Txt Tokens per Sec:     5631 || Lr: 0.000050
2024-02-05 22:01:31,154 Epoch 963: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.60 
2024-02-05 22:01:31,155 EPOCH 964
2024-02-05 22:01:36,030 Epoch 964: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.36 
2024-02-05 22:01:36,030 EPOCH 965
2024-02-05 22:01:36,687 [Epoch: 965 Step: 00064600] Batch Recognition Loss:   0.000200 => Gls Tokens per Sec:     2927 || Batch Translation Loss:   0.015187 => Txt Tokens per Sec:     7380 || Lr: 0.000050
2024-02-05 22:01:41,364 Epoch 965: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.66 
2024-02-05 22:01:41,365 EPOCH 966
2024-02-05 22:01:44,480 [Epoch: 966 Step: 00064700] Batch Recognition Loss:   0.000594 => Gls Tokens per Sec:     2280 || Batch Translation Loss:   0.011511 => Txt Tokens per Sec:     6035 || Lr: 0.000050
2024-02-05 22:01:46,248 Epoch 966: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.24 
2024-02-05 22:01:46,248 EPOCH 967
2024-02-05 22:01:51,591 Epoch 967: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.19 
2024-02-05 22:01:51,591 EPOCH 968
2024-02-05 22:01:52,312 [Epoch: 968 Step: 00064800] Batch Recognition Loss:   0.000424 => Gls Tokens per Sec:     2304 || Batch Translation Loss:   0.027373 => Txt Tokens per Sec:     6137 || Lr: 0.000050
2024-02-05 22:01:56,420 Epoch 968: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.48 
2024-02-05 22:01:56,421 EPOCH 969
2024-02-05 22:02:00,285 [Epoch: 969 Step: 00064900] Batch Recognition Loss:   0.000160 => Gls Tokens per Sec:     1797 || Batch Translation Loss:   0.014935 => Txt Tokens per Sec:     5017 || Lr: 0.000050
2024-02-05 22:02:01,908 Epoch 969: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.59 
2024-02-05 22:02:01,908 EPOCH 970
2024-02-05 22:02:06,906 Epoch 970: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.19 
2024-02-05 22:02:06,907 EPOCH 971
2024-02-05 22:02:07,748 [Epoch: 971 Step: 00065000] Batch Recognition Loss:   0.000296 => Gls Tokens per Sec:     1907 || Batch Translation Loss:   0.024323 => Txt Tokens per Sec:     4898 || Lr: 0.000050
2024-02-05 22:02:12,513 Epoch 971: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.21 
2024-02-05 22:02:12,513 EPOCH 972
2024-02-05 22:02:15,520 [Epoch: 972 Step: 00065100] Batch Recognition Loss:   0.000144 => Gls Tokens per Sec:     2289 || Batch Translation Loss:   0.036497 => Txt Tokens per Sec:     6396 || Lr: 0.000050
2024-02-05 22:02:17,149 Epoch 972: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.07 
2024-02-05 22:02:17,149 EPOCH 973
2024-02-05 22:02:21,671 Epoch 973: Total Training Recognition Loss 0.04  Total Training Translation Loss 4.24 
2024-02-05 22:02:21,671 EPOCH 974
2024-02-05 22:02:22,214 [Epoch: 974 Step: 00065200] Batch Recognition Loss:   0.000212 => Gls Tokens per Sec:     2657 || Batch Translation Loss:   0.070345 => Txt Tokens per Sec:     7066 || Lr: 0.000050
2024-02-05 22:02:27,024 Epoch 974: Total Training Recognition Loss 0.04  Total Training Translation Loss 4.33 
2024-02-05 22:02:27,024 EPOCH 975
2024-02-05 22:02:30,138 [Epoch: 975 Step: 00065300] Batch Recognition Loss:   0.000204 => Gls Tokens per Sec:     2159 || Batch Translation Loss:   0.054273 => Txt Tokens per Sec:     6077 || Lr: 0.000050
2024-02-05 22:02:31,883 Epoch 975: Total Training Recognition Loss 0.06  Total Training Translation Loss 4.50 
2024-02-05 22:02:31,883 EPOCH 976
2024-02-05 22:02:37,424 Epoch 976: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.16 
2024-02-05 22:02:37,425 EPOCH 977
2024-02-05 22:02:37,958 [Epoch: 977 Step: 00065400] Batch Recognition Loss:   0.000888 => Gls Tokens per Sec:     2406 || Batch Translation Loss:   0.020778 => Txt Tokens per Sec:     7064 || Lr: 0.000050
2024-02-05 22:02:42,276 Epoch 977: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.14 
2024-02-05 22:02:42,277 EPOCH 978
2024-02-05 22:02:45,389 [Epoch: 978 Step: 00065500] Batch Recognition Loss:   0.000287 => Gls Tokens per Sec:     2076 || Batch Translation Loss:   0.016573 => Txt Tokens per Sec:     5581 || Lr: 0.000050
2024-02-05 22:02:47,674 Epoch 978: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.36 
2024-02-05 22:02:47,674 EPOCH 979
2024-02-05 22:02:53,133 Epoch 979: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.98 
2024-02-05 22:02:53,133 EPOCH 980
2024-02-05 22:02:53,659 [Epoch: 980 Step: 00065600] Batch Recognition Loss:   0.001204 => Gls Tokens per Sec:     2135 || Batch Translation Loss:   0.030426 => Txt Tokens per Sec:     6111 || Lr: 0.000050
2024-02-05 22:02:58,330 Epoch 980: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.52 
2024-02-05 22:02:58,330 EPOCH 981
2024-02-05 22:03:01,525 [Epoch: 981 Step: 00065700] Batch Recognition Loss:   0.000538 => Gls Tokens per Sec:     1973 || Batch Translation Loss:   0.184508 => Txt Tokens per Sec:     5403 || Lr: 0.000050
2024-02-05 22:03:03,908 Epoch 981: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.67 
2024-02-05 22:03:03,909 EPOCH 982
2024-02-05 22:03:09,158 Epoch 982: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.95 
2024-02-05 22:03:09,158 EPOCH 983
2024-02-05 22:03:09,543 [Epoch: 983 Step: 00065800] Batch Recognition Loss:   0.000212 => Gls Tokens per Sec:     2507 || Batch Translation Loss:   0.008408 => Txt Tokens per Sec:     6637 || Lr: 0.000050
2024-02-05 22:03:14,537 Epoch 983: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.39 
2024-02-05 22:03:14,538 EPOCH 984
2024-02-05 22:03:17,537 [Epoch: 984 Step: 00065900] Batch Recognition Loss:   0.000344 => Gls Tokens per Sec:     2081 || Batch Translation Loss:   0.020048 => Txt Tokens per Sec:     5890 || Lr: 0.000050
2024-02-05 22:03:19,581 Epoch 984: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.57 
2024-02-05 22:03:19,581 EPOCH 985
2024-02-05 22:03:24,732 Epoch 985: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.54 
2024-02-05 22:03:24,732 EPOCH 986
2024-02-05 22:03:25,105 [Epoch: 986 Step: 00066000] Batch Recognition Loss:   0.000370 => Gls Tokens per Sec:     2151 || Batch Translation Loss:   0.028686 => Txt Tokens per Sec:     6355 || Lr: 0.000050
2024-02-05 22:03:33,567 Validation result at epoch 986, step    66000: duration: 8.4609s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.54453	Translation Loss: 92882.28125	PPL: 10690.32910
	Eval Metric: BLEU
	WER 3.60	(DEL: 0.00,	INS: 0.00,	SUB: 3.60)
	BLEU-4 0.46	(BLEU-1: 9.76,	BLEU-2: 3.00,	BLEU-3: 1.01,	BLEU-4: 0.46)
	CHRF 16.31	ROUGE 8.64
2024-02-05 22:03:33,568 Logging Recognition and Translation Outputs
2024-02-05 22:03:33,569 ========================================================================================================================
2024-02-05 22:03:33,569 Logging Sequence: 85_97.00
2024-02-05 22:03:33,569 	Gloss Reference :	A B+C+D+E
2024-02-05 22:03:33,569 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:03:33,569 	Gloss Alignment :	         
2024-02-05 22:03:33,569 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:03:33,570 	Text Reference  :	** *** ****** like    india's ******* bcci        australia has cricket australia
2024-02-05 22:03:33,570 	Text Hypothesis :	in his people perform india's amazing performance in        the indian  team     
2024-02-05 22:03:33,570 	Text Alignment  :	I  I   I      S               I       S           S         S   S       S        
2024-02-05 22:03:33,570 ========================================================================================================================
2024-02-05 22:03:33,570 Logging Sequence: 53_161.00
2024-02-05 22:03:33,571 	Gloss Reference :	A B+C+D+E
2024-02-05 22:03:33,571 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:03:33,571 	Gloss Alignment :	         
2024-02-05 22:03:33,571 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:03:33,572 	Text Reference  :	** rashid has also been urging people to donate      to his rashid khan   foundation and afghanistan cricket association
2024-02-05 22:03:33,573 	Text Hypothesis :	he too    has **** **** ****** ****** ** afghanistan to win the    silver medal      for rs          32      crore      
2024-02-05 22:03:33,573 	Text Alignment  :	I  S          D    D    D      D      D  S              S   S      S      S          S   S           S       S          
2024-02-05 22:03:33,573 ========================================================================================================================
2024-02-05 22:03:33,573 Logging Sequence: 101_97.00
2024-02-05 22:03:33,573 	Gloss Reference :	A B+C+D+E
2024-02-05 22:03:33,573 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:03:33,573 	Gloss Alignment :	         
2024-02-05 22:03:33,573 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:03:33,575 	Text Reference  :	2 of the  best   indian bowlers who   bowled  very well were raj bawa who took 5 wickets and ravi kumar who     took 4   wickets
2024-02-05 22:03:33,575 	Text Hypothesis :	* ** when sharma was    batting india playing very well **** *** **** *** **** * ******* and **** ***** england lost the team   
2024-02-05 22:03:33,576 	Text Alignment  :	D D  S    S      S      S       S     S                 D    D   D    D   D    D D           D    D     S       S    S   S      
2024-02-05 22:03:33,576 ========================================================================================================================
2024-02-05 22:03:33,576 Logging Sequence: 118_130.00
2024-02-05 22:03:33,576 	Gloss Reference :	A B+C+D+E
2024-02-05 22:03:33,576 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:03:33,576 	Gloss Alignment :	         
2024-02-05 22:03:33,576 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:03:33,577 	Text Reference  :	messi's fans the entire team were in tears everyone was overwhelmed by       the    victory
2024-02-05 22:03:33,577 	Text Hypothesis :	******* **** *** ****** **** **** ** ***** this     was krunal      pandya's maiden odi    
2024-02-05 22:03:33,577 	Text Alignment  :	D       D    D   D      D    D    D  D     S            S           S        S      S      
2024-02-05 22:03:33,577 ========================================================================================================================
2024-02-05 22:03:33,577 Logging Sequence: 170_195.00
2024-02-05 22:03:33,578 	Gloss Reference :	A B+C+D+E
2024-02-05 22:03:33,578 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:03:33,578 	Gloss Alignment :	         
2024-02-05 22:03:33,578 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:03:33,579 	Text Reference  :	i moved to      rajasthan royals team  as    they  paid me  8      crores
2024-02-05 22:03:33,579 	Text Hypothesis :	* as    neither of        the    teams could score in   the league ipl   
2024-02-05 22:03:33,579 	Text Alignment  :	D S     S       S         S      S     S     S     S    S   S      S     
2024-02-05 22:03:33,579 ========================================================================================================================
2024-02-05 22:03:38,645 Epoch 986: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.57 
2024-02-05 22:03:38,646 EPOCH 987
2024-02-05 22:03:41,202 [Epoch: 987 Step: 00066100] Batch Recognition Loss:   0.002493 => Gls Tokens per Sec:     2380 || Batch Translation Loss:   0.011225 => Txt Tokens per Sec:     6570 || Lr: 0.000050
2024-02-05 22:03:43,747 Epoch 987: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.15 
2024-02-05 22:03:43,748 EPOCH 988
2024-02-05 22:03:48,885 Epoch 988: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.17 
2024-02-05 22:03:48,885 EPOCH 989
2024-02-05 22:03:49,189 [Epoch: 989 Step: 00066200] Batch Recognition Loss:   0.000458 => Gls Tokens per Sec:     2121 || Batch Translation Loss:   0.038384 => Txt Tokens per Sec:     6133 || Lr: 0.000050
2024-02-05 22:03:54,089 Epoch 989: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.86 
2024-02-05 22:03:54,089 EPOCH 990
2024-02-05 22:03:56,713 [Epoch: 990 Step: 00066300] Batch Recognition Loss:   0.000209 => Gls Tokens per Sec:     2257 || Batch Translation Loss:   0.011388 => Txt Tokens per Sec:     6093 || Lr: 0.000050
2024-02-05 22:03:59,110 Epoch 990: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.37 
2024-02-05 22:03:59,110 EPOCH 991
2024-02-05 22:04:04,346 Epoch 991: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.42 
2024-02-05 22:04:04,347 EPOCH 992
2024-02-05 22:04:04,640 [Epoch: 992 Step: 00066400] Batch Recognition Loss:   0.001623 => Gls Tokens per Sec:     1646 || Batch Translation Loss:   0.021528 => Txt Tokens per Sec:     4998 || Lr: 0.000050
2024-02-05 22:04:09,388 Epoch 992: Total Training Recognition Loss 0.06  Total Training Translation Loss 4.46 
2024-02-05 22:04:09,388 EPOCH 993
2024-02-05 22:04:11,979 [Epoch: 993 Step: 00066500] Batch Recognition Loss:   0.000513 => Gls Tokens per Sec:     2224 || Batch Translation Loss:   0.025528 => Txt Tokens per Sec:     6129 || Lr: 0.000050
2024-02-05 22:04:14,719 Epoch 993: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.51 
2024-02-05 22:04:14,720 EPOCH 994
2024-02-05 22:04:19,942 Epoch 994: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.20 
2024-02-05 22:04:19,943 EPOCH 995
2024-02-05 22:04:20,095 [Epoch: 995 Step: 00066600] Batch Recognition Loss:   0.000142 => Gls Tokens per Sec:     2119 || Batch Translation Loss:   0.018852 => Txt Tokens per Sec:     5636 || Lr: 0.000050
2024-02-05 22:04:25,443 Epoch 995: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.17 
2024-02-05 22:04:25,443 EPOCH 996
2024-02-05 22:04:27,928 [Epoch: 996 Step: 00066700] Batch Recognition Loss:   0.000348 => Gls Tokens per Sec:     2255 || Batch Translation Loss:   0.028568 => Txt Tokens per Sec:     6030 || Lr: 0.000050
2024-02-05 22:04:30,798 Epoch 996: Total Training Recognition Loss 0.09  Total Training Translation Loss 2.19 
2024-02-05 22:04:30,799 EPOCH 997
2024-02-05 22:04:36,155 Epoch 997: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.62 
2024-02-05 22:04:36,156 EPOCH 998
2024-02-05 22:04:36,229 [Epoch: 998 Step: 00066800] Batch Recognition Loss:   0.000364 => Gls Tokens per Sec:     2192 || Batch Translation Loss:   0.022310 => Txt Tokens per Sec:     6260 || Lr: 0.000050
2024-02-05 22:04:41,531 Epoch 998: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.33 
2024-02-05 22:04:41,532 EPOCH 999
2024-02-05 22:04:44,333 [Epoch: 999 Step: 00066900] Batch Recognition Loss:   0.000289 => Gls Tokens per Sec:     1943 || Batch Translation Loss:   0.015431 => Txt Tokens per Sec:     5580 || Lr: 0.000050
2024-02-05 22:04:46,618 Epoch 999: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.59 
2024-02-05 22:04:46,618 EPOCH 1000
2024-02-05 22:04:52,214 [Epoch: 1000 Step: 00067000] Batch Recognition Loss:   0.000180 => Gls Tokens per Sec:     1898 || Batch Translation Loss:   0.019704 => Txt Tokens per Sec:     5251 || Lr: 0.000050
2024-02-05 22:04:52,215 Epoch 1000: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.33 
2024-02-05 22:04:52,215 EPOCH 1001
2024-02-05 22:04:57,344 Epoch 1001: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.87 
2024-02-05 22:04:57,344 EPOCH 1002
2024-02-05 22:04:59,650 [Epoch: 1002 Step: 00067100] Batch Recognition Loss:   0.000183 => Gls Tokens per Sec:     2291 || Batch Translation Loss:   0.044806 => Txt Tokens per Sec:     6367 || Lr: 0.000050
2024-02-05 22:05:02,564 Epoch 1002: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.30 
2024-02-05 22:05:02,564 EPOCH 1003
2024-02-05 22:05:07,787 [Epoch: 1003 Step: 00067200] Batch Recognition Loss:   0.000341 => Gls Tokens per Sec:     2003 || Batch Translation Loss:   0.018359 => Txt Tokens per Sec:     5550 || Lr: 0.000050
2024-02-05 22:05:07,842 Epoch 1003: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.76 
2024-02-05 22:05:07,843 EPOCH 1004
2024-02-05 22:05:13,093 Epoch 1004: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.75 
2024-02-05 22:05:13,094 EPOCH 1005
2024-02-05 22:05:15,534 [Epoch: 1005 Step: 00067300] Batch Recognition Loss:   0.000280 => Gls Tokens per Sec:     2059 || Batch Translation Loss:   0.015917 => Txt Tokens per Sec:     5824 || Lr: 0.000050
2024-02-05 22:05:18,000 Epoch 1005: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.37 
2024-02-05 22:05:18,000 EPOCH 1006
2024-02-05 22:05:23,400 [Epoch: 1006 Step: 00067400] Batch Recognition Loss:   0.000156 => Gls Tokens per Sec:     1908 || Batch Translation Loss:   0.020915 => Txt Tokens per Sec:     5309 || Lr: 0.000050
2024-02-05 22:05:23,526 Epoch 1006: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.02 
2024-02-05 22:05:23,526 EPOCH 1007
2024-02-05 22:05:28,335 Epoch 1007: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.49 
2024-02-05 22:05:28,335 EPOCH 1008
2024-02-05 22:05:30,924 [Epoch: 1008 Step: 00067500] Batch Recognition Loss:   0.000242 => Gls Tokens per Sec:     1878 || Batch Translation Loss:   0.021293 => Txt Tokens per Sec:     5153 || Lr: 0.000050
2024-02-05 22:05:33,871 Epoch 1008: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.14 
2024-02-05 22:05:33,871 EPOCH 1009
2024-02-05 22:05:39,027 [Epoch: 1009 Step: 00067600] Batch Recognition Loss:   0.000180 => Gls Tokens per Sec:     1967 || Batch Translation Loss:   0.019878 => Txt Tokens per Sec:     5491 || Lr: 0.000050
2024-02-05 22:05:39,207 Epoch 1009: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.69 
2024-02-05 22:05:39,207 EPOCH 1010
2024-02-05 22:05:44,440 Epoch 1010: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.33 
2024-02-05 22:05:44,440 EPOCH 1011
2024-02-05 22:05:46,481 [Epoch: 1011 Step: 00067700] Batch Recognition Loss:   0.000264 => Gls Tokens per Sec:     2303 || Batch Translation Loss:   0.014458 => Txt Tokens per Sec:     6290 || Lr: 0.000050
2024-02-05 22:05:49,528 Epoch 1011: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.14 
2024-02-05 22:05:49,528 EPOCH 1012
2024-02-05 22:05:54,811 [Epoch: 1012 Step: 00067800] Batch Recognition Loss:   0.000259 => Gls Tokens per Sec:     1890 || Batch Translation Loss:   0.007351 => Txt Tokens per Sec:     5292 || Lr: 0.000050
2024-02-05 22:05:55,030 Epoch 1012: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.36 
2024-02-05 22:05:55,030 EPOCH 1013
2024-02-05 22:05:59,841 Epoch 1013: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.03 
2024-02-05 22:05:59,841 EPOCH 1014
2024-02-05 22:06:02,123 [Epoch: 1014 Step: 00067900] Batch Recognition Loss:   0.000192 => Gls Tokens per Sec:     1991 || Batch Translation Loss:   0.020972 => Txt Tokens per Sec:     5412 || Lr: 0.000050
2024-02-05 22:06:05,372 Epoch 1014: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.98 
2024-02-05 22:06:05,373 EPOCH 1015
2024-02-05 22:06:09,649 [Epoch: 1015 Step: 00068000] Batch Recognition Loss:   0.000127 => Gls Tokens per Sec:     2320 || Batch Translation Loss:   0.012581 => Txt Tokens per Sec:     6374 || Lr: 0.000050
2024-02-05 22:06:18,121 Validation result at epoch 1015, step    68000: duration: 8.4728s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.47786	Translation Loss: 92505.24219	PPL: 10295.23242
	Eval Metric: BLEU
	WER 3.18	(DEL: 0.00,	INS: 0.00,	SUB: 3.18)
	BLEU-4 0.69	(BLEU-1: 11.20,	BLEU-2: 3.65,	BLEU-3: 1.39,	BLEU-4: 0.69)
	CHRF 17.20	ROUGE 9.37
2024-02-05 22:06:18,122 Logging Recognition and Translation Outputs
2024-02-05 22:06:18,122 ========================================================================================================================
2024-02-05 22:06:18,122 Logging Sequence: 148_186.00
2024-02-05 22:06:18,122 	Gloss Reference :	A B+C+D+E
2024-02-05 22:06:18,122 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:06:18,123 	Gloss Alignment :	         
2024-02-05 22:06:18,123 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:06:18,125 	Text Reference  :	*** siraj   also  took four  wickets in    1      over thus becoming the record-holder for most wickets in        an            over in odis
2024-02-05 22:06:18,125 	Text Hypothesis :	the winners claim that india beat    south africa and  put  etc      was named         as  the  world   athletics championships wac  in 2019
2024-02-05 22:06:18,125 	Text Alignment  :	I   S       S     S    S     S       S     S      S    S    S        S   S             S   S    S       S         S             S       S   
2024-02-05 22:06:18,126 ========================================================================================================================
2024-02-05 22:06:18,126 Logging Sequence: 61_181.00
2024-02-05 22:06:18,126 	Gloss Reference :	A B+C+D+E
2024-02-05 22:06:18,126 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:06:18,126 	Gloss Alignment :	         
2024-02-05 22:06:18,126 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:06:18,129 	Text Reference  :	* one  other fan  said it     is   babar's  personal chat   we    should    focuc    on this cricketing career and  not his personal life   
2024-02-05 22:06:18,129 	Text Hypothesis :	i will be    tell you  before your hardwork and      friend about extremely saddened by itc  hotels     in     2017 for his ******** parents
2024-02-05 22:06:18,129 	Text Alignment  :	I S    S     S    S    S      S    S        S        S      S     S         S        S  S    S          S      S    S       D        S      
2024-02-05 22:06:18,129 ========================================================================================================================
2024-02-05 22:06:18,129 Logging Sequence: 123_24.00
2024-02-05 22:06:18,129 	Gloss Reference :	A B+C+D+E
2024-02-05 22:06:18,130 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:06:18,130 	Gloss Alignment :	         
2024-02-05 22:06:18,130 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:06:18,131 	Text Reference  :	did you know  that other than cricket  dhoni has      another passion
2024-02-05 22:06:18,131 	Text Hypothesis :	*** *** dhoni was  a     huge surprise for   everyone as      well   
2024-02-05 22:06:18,131 	Text Alignment  :	D   D   S     S    S     S    S        S     S        S       S      
2024-02-05 22:06:18,131 ========================================================================================================================
2024-02-05 22:06:18,131 Logging Sequence: 84_76.00
2024-02-05 22:06:18,131 	Gloss Reference :	A B+C+D+E
2024-02-05 22:06:18,131 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:06:18,131 	Gloss Alignment :	         
2024-02-05 22:06:18,132 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:06:18,132 	Text Reference  :	** * ********* ** ** the teams wanted  to support but were  refused
2024-02-05 22:06:18,133 	Text Hypothesis :	if a situation is so bad then  covered to win     the photo session
2024-02-05 22:06:18,133 	Text Alignment  :	I  I I         I  I  S   S     S          S       S   S     S      
2024-02-05 22:06:18,133 ========================================================================================================================
2024-02-05 22:06:18,133 Logging Sequence: 126_188.00
2024-02-05 22:06:18,133 	Gloss Reference :	A B+C+D+E
2024-02-05 22:06:18,133 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:06:18,133 	Gloss Alignment :	         
2024-02-05 22:06:18,133 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:06:18,134 	Text Reference  :	now he  has  become a    gold       medalist at  the 2020 tokyo olympics
2024-02-05 22:06:18,134 	Text Hypothesis :	*** and they were   very particular as       now the **** ***** fans    
2024-02-05 22:06:18,134 	Text Alignment  :	D   S   S    S      S    S          S        S       D    D     S       
2024-02-05 22:06:18,135 ========================================================================================================================
2024-02-05 22:06:18,693 Epoch 1015: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.96 
2024-02-05 22:06:18,694 EPOCH 1016
2024-02-05 22:06:24,160 Epoch 1016: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.24 
2024-02-05 22:06:24,160 EPOCH 1017
2024-02-05 22:06:26,101 [Epoch: 1017 Step: 00068100] Batch Recognition Loss:   0.000203 => Gls Tokens per Sec:     2258 || Batch Translation Loss:   0.020471 => Txt Tokens per Sec:     6236 || Lr: 0.000050
2024-02-05 22:06:28,876 Epoch 1017: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.01 
2024-02-05 22:06:28,877 EPOCH 1018
2024-02-05 22:06:33,846 [Epoch: 1018 Step: 00068200] Batch Recognition Loss:   0.001203 => Gls Tokens per Sec:     1944 || Batch Translation Loss:   0.074566 => Txt Tokens per Sec:     5376 || Lr: 0.000050
2024-02-05 22:06:34,295 Epoch 1018: Total Training Recognition Loss 0.23  Total Training Translation Loss 3.68 
2024-02-05 22:06:34,295 EPOCH 1019
2024-02-05 22:06:39,448 Epoch 1019: Total Training Recognition Loss 0.48  Total Training Translation Loss 5.77 
2024-02-05 22:06:39,448 EPOCH 1020
2024-02-05 22:06:41,674 [Epoch: 1020 Step: 00068300] Batch Recognition Loss:   0.000170 => Gls Tokens per Sec:     1941 || Batch Translation Loss:   0.022816 => Txt Tokens per Sec:     5346 || Lr: 0.000050
2024-02-05 22:06:44,748 Epoch 1020: Total Training Recognition Loss 0.27  Total Training Translation Loss 5.33 
2024-02-05 22:06:44,748 EPOCH 1021
2024-02-05 22:06:48,783 [Epoch: 1021 Step: 00068400] Batch Recognition Loss:   0.000348 => Gls Tokens per Sec:     2355 || Batch Translation Loss:   0.107273 => Txt Tokens per Sec:     6486 || Lr: 0.000050
2024-02-05 22:06:49,470 Epoch 1021: Total Training Recognition Loss 0.20  Total Training Translation Loss 5.41 
2024-02-05 22:06:49,471 EPOCH 1022
2024-02-05 22:06:54,946 Epoch 1022: Total Training Recognition Loss 0.11  Total Training Translation Loss 7.93 
2024-02-05 22:06:54,947 EPOCH 1023
2024-02-05 22:06:56,752 [Epoch: 1023 Step: 00068500] Batch Recognition Loss:   0.000350 => Gls Tokens per Sec:     2306 || Batch Translation Loss:   0.036846 => Txt Tokens per Sec:     6387 || Lr: 0.000050
2024-02-05 22:06:59,996 Epoch 1023: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.73 
2024-02-05 22:06:59,997 EPOCH 1024
2024-02-05 22:07:04,599 [Epoch: 1024 Step: 00068600] Batch Recognition Loss:   0.000329 => Gls Tokens per Sec:     2030 || Batch Translation Loss:   0.072815 => Txt Tokens per Sec:     5569 || Lr: 0.000050
2024-02-05 22:07:05,334 Epoch 1024: Total Training Recognition Loss 0.09  Total Training Translation Loss 2.15 
2024-02-05 22:07:05,335 EPOCH 1025
2024-02-05 22:07:10,376 Epoch 1025: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.71 
2024-02-05 22:07:10,376 EPOCH 1026
2024-02-05 22:07:12,210 [Epoch: 1026 Step: 00068700] Batch Recognition Loss:   0.000252 => Gls Tokens per Sec:     2129 || Batch Translation Loss:   0.038250 => Txt Tokens per Sec:     5772 || Lr: 0.000050
2024-02-05 22:07:15,756 Epoch 1026: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.09 
2024-02-05 22:07:15,756 EPOCH 1027
2024-02-05 22:07:20,652 [Epoch: 1027 Step: 00068800] Batch Recognition Loss:   0.000210 => Gls Tokens per Sec:     1896 || Batch Translation Loss:   0.016391 => Txt Tokens per Sec:     5321 || Lr: 0.000050
2024-02-05 22:07:21,289 Epoch 1027: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.02 
2024-02-05 22:07:21,290 EPOCH 1028
2024-02-05 22:07:26,559 Epoch 1028: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.64 
2024-02-05 22:07:26,559 EPOCH 1029
2024-02-05 22:07:28,573 [Epoch: 1029 Step: 00068900] Batch Recognition Loss:   0.001463 => Gls Tokens per Sec:     1859 || Batch Translation Loss:   0.046479 => Txt Tokens per Sec:     5068 || Lr: 0.000050
2024-02-05 22:07:31,998 Epoch 1029: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.78 
2024-02-05 22:07:31,998 EPOCH 1030
2024-02-05 22:07:36,493 [Epoch: 1030 Step: 00069000] Batch Recognition Loss:   0.000161 => Gls Tokens per Sec:     2008 || Batch Translation Loss:   0.020560 => Txt Tokens per Sec:     5598 || Lr: 0.000050
2024-02-05 22:07:37,247 Epoch 1030: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.62 
2024-02-05 22:07:37,247 EPOCH 1031
2024-02-05 22:07:42,564 Epoch 1031: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.25 
2024-02-05 22:07:42,564 EPOCH 1032
2024-02-05 22:07:44,180 [Epoch: 1032 Step: 00069100] Batch Recognition Loss:   0.001149 => Gls Tokens per Sec:     2217 || Batch Translation Loss:   0.014407 => Txt Tokens per Sec:     6205 || Lr: 0.000050
2024-02-05 22:07:47,388 Epoch 1032: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.30 
2024-02-05 22:07:47,389 EPOCH 1033
2024-02-05 22:07:51,799 [Epoch: 1033 Step: 00069200] Batch Recognition Loss:   0.000227 => Gls Tokens per Sec:     2010 || Batch Translation Loss:   0.016243 => Txt Tokens per Sec:     5435 || Lr: 0.000050
2024-02-05 22:07:52,958 Epoch 1033: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.11 
2024-02-05 22:07:52,958 EPOCH 1034
2024-02-05 22:07:57,975 Epoch 1034: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.02 
2024-02-05 22:07:57,976 EPOCH 1035
2024-02-05 22:07:59,476 [Epoch: 1035 Step: 00069300] Batch Recognition Loss:   0.000200 => Gls Tokens per Sec:     2350 || Batch Translation Loss:   0.014293 => Txt Tokens per Sec:     6121 || Lr: 0.000050
2024-02-05 22:08:03,277 Epoch 1035: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.34 
2024-02-05 22:08:03,277 EPOCH 1036
2024-02-05 22:08:07,412 [Epoch: 1036 Step: 00069400] Batch Recognition Loss:   0.001779 => Gls Tokens per Sec:     2104 || Batch Translation Loss:   0.020797 => Txt Tokens per Sec:     5870 || Lr: 0.000050
2024-02-05 22:08:08,333 Epoch 1036: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.91 
2024-02-05 22:08:08,333 EPOCH 1037
2024-02-05 22:08:13,574 Epoch 1037: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.50 
2024-02-05 22:08:13,574 EPOCH 1038
2024-02-05 22:08:14,916 [Epoch: 1038 Step: 00069500] Batch Recognition Loss:   0.000235 => Gls Tokens per Sec:     2431 || Batch Translation Loss:   0.008760 => Txt Tokens per Sec:     6366 || Lr: 0.000050
2024-02-05 22:08:18,779 Epoch 1038: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.79 
2024-02-05 22:08:18,779 EPOCH 1039
2024-02-05 22:08:22,984 [Epoch: 1039 Step: 00069600] Batch Recognition Loss:   0.000195 => Gls Tokens per Sec:     2032 || Batch Translation Loss:   0.025237 => Txt Tokens per Sec:     5703 || Lr: 0.000050
2024-02-05 22:08:23,833 Epoch 1039: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.62 
2024-02-05 22:08:23,833 EPOCH 1040
2024-02-05 22:08:29,135 Epoch 1040: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.29 
2024-02-05 22:08:29,136 EPOCH 1041
2024-02-05 22:08:30,499 [Epoch: 1041 Step: 00069700] Batch Recognition Loss:   0.000259 => Gls Tokens per Sec:     2351 || Batch Translation Loss:   0.038042 => Txt Tokens per Sec:     6545 || Lr: 0.000050
2024-02-05 22:08:34,096 Epoch 1041: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.75 
2024-02-05 22:08:34,096 EPOCH 1042
2024-02-05 22:08:38,326 [Epoch: 1042 Step: 00069800] Batch Recognition Loss:   0.000238 => Gls Tokens per Sec:     1982 || Batch Translation Loss:   0.026428 => Txt Tokens per Sec:     5350 || Lr: 0.000050
2024-02-05 22:08:39,524 Epoch 1042: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.03 
2024-02-05 22:08:39,524 EPOCH 1043
2024-02-05 22:08:44,866 Epoch 1043: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.66 
2024-02-05 22:08:44,867 EPOCH 1044
2024-02-05 22:08:46,262 [Epoch: 1044 Step: 00069900] Batch Recognition Loss:   0.000999 => Gls Tokens per Sec:     2181 || Batch Translation Loss:   0.037062 => Txt Tokens per Sec:     5909 || Lr: 0.000050
2024-02-05 22:08:50,186 Epoch 1044: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.43 
2024-02-05 22:08:50,186 EPOCH 1045
2024-02-05 22:08:53,819 [Epoch: 1045 Step: 00070000] Batch Recognition Loss:   0.000841 => Gls Tokens per Sec:     2263 || Batch Translation Loss:   0.015208 => Txt Tokens per Sec:     6316 || Lr: 0.000050
2024-02-05 22:09:02,084 Validation result at epoch 1045, step    70000: duration: 8.2630s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.68700	Translation Loss: 93176.62500	PPL: 11009.29004
	Eval Metric: BLEU
	WER 3.25	(DEL: 0.00,	INS: 0.00,	SUB: 3.25)
	BLEU-4 0.44	(BLEU-1: 9.72,	BLEU-2: 2.99,	BLEU-3: 1.08,	BLEU-4: 0.44)
	CHRF 16.02	ROUGE 8.42
2024-02-05 22:09:02,085 Logging Recognition and Translation Outputs
2024-02-05 22:09:02,085 ========================================================================================================================
2024-02-05 22:09:02,085 Logging Sequence: 129_90.00
2024-02-05 22:09:02,085 	Gloss Reference :	A B+C+D+E
2024-02-05 22:09:02,085 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:09:02,085 	Gloss Alignment :	         
2024-02-05 22:09:02,085 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:09:02,087 	Text Reference  :	however because of the      emergency games will now be    held   without any spectators
2024-02-05 22:09:02,087 	Text Hypothesis :	******* ******* a  football match     lasts for  two equal halves of      45  minutes   
2024-02-05 22:09:02,087 	Text Alignment  :	D       D       S  S        S         S     S    S   S     S      S       S   S         
2024-02-05 22:09:02,087 ========================================================================================================================
2024-02-05 22:09:02,087 Logging Sequence: 179_378.00
2024-02-05 22:09:02,087 	Gloss Reference :	A B+C+D+E
2024-02-05 22:09:02,087 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:09:02,088 	Gloss Alignment :	         
2024-02-05 22:09:02,088 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:09:02,089 	Text Reference  :	these kids think that they are  going  to   the   olympics so they've become some    kind of  stars
2024-02-05 22:09:02,089 	Text Hypothesis :	***** **** ***** **** on   16th august 2022 there was      a  strong  match  between me   and japan
2024-02-05 22:09:02,089 	Text Alignment  :	D     D    D     D    S    S    S      S    S     S        S  S       S      S       S    S   S    
2024-02-05 22:09:02,089 ========================================================================================================================
2024-02-05 22:09:02,090 Logging Sequence: 162_20.00
2024-02-05 22:09:02,090 	Gloss Reference :	A B+C+D+E
2024-02-05 22:09:02,090 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:09:02,090 	Gloss Alignment :	         
2024-02-05 22:09:02,090 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:09:02,092 	Text Reference  :	***** not  only this but      they  blamed mohammed shami's religion    as     the reason for india's loss   
2024-02-05 22:09:02,092 	Text Hypothesis :	kohli said that he   supports shami 200    and      their   brotherhood cannot be  broken by  these   attacks
2024-02-05 22:09:02,092 	Text Alignment  :	I     S    S    S    S        S     S      S        S       S           S      S   S      S   S       S      
2024-02-05 22:09:02,092 ========================================================================================================================
2024-02-05 22:09:02,092 Logging Sequence: 106_169.00
2024-02-05 22:09:02,092 	Gloss Reference :	A B+C+D+E
2024-02-05 22:09:02,092 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:09:02,093 	Gloss Alignment :	         
2024-02-05 22:09:02,093 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:09:02,093 	Text Reference  :	prime minister narendra modi also expressed his happiness while congratulating the **** team on    twitter
2024-02-05 22:09:02,093 	Text Hypothesis :	***** ******** ******** **** **** ********* *** ********* he    reached        the toss and  women said   
2024-02-05 22:09:02,094 	Text Alignment  :	D     D        D        D    D    D         D   D         S     S                  I    S    S     S      
2024-02-05 22:09:02,094 ========================================================================================================================
2024-02-05 22:09:02,094 Logging Sequence: 65_77.00
2024-02-05 22:09:02,094 	Gloss Reference :	A B+C+D+E
2024-02-05 22:09:02,094 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:09:02,094 	Gloss Alignment :	         
2024-02-05 22:09:02,094 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:09:02,095 	Text Reference  :	** *** **** **** *** indian ******* ******* team ** travelling included 16    players
2024-02-05 22:09:02,095 	Text Hypothesis :	do you know that the indian women's cricket team is currently  in       south africa 
2024-02-05 22:09:02,095 	Text Alignment  :	I  I   I    I    I          I       I            I  S          S        S     S      
2024-02-05 22:09:02,095 ========================================================================================================================
2024-02-05 22:09:03,215 Epoch 1045: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.62 
2024-02-05 22:09:03,216 EPOCH 1046
2024-02-05 22:09:08,777 Epoch 1046: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.82 
2024-02-05 22:09:08,778 EPOCH 1047
2024-02-05 22:09:10,328 [Epoch: 1047 Step: 00070100] Batch Recognition Loss:   0.000182 => Gls Tokens per Sec:     1796 || Batch Translation Loss:   0.015584 => Txt Tokens per Sec:     5296 || Lr: 0.000050
2024-02-05 22:09:13,665 Epoch 1047: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.88 
2024-02-05 22:09:13,665 EPOCH 1048
2024-02-05 22:09:17,862 [Epoch: 1048 Step: 00070200] Batch Recognition Loss:   0.000219 => Gls Tokens per Sec:     1921 || Batch Translation Loss:   0.015071 => Txt Tokens per Sec:     5324 || Lr: 0.000050
2024-02-05 22:09:19,097 Epoch 1048: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.22 
2024-02-05 22:09:19,097 EPOCH 1049
2024-02-05 22:09:23,936 Epoch 1049: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.45 
2024-02-05 22:09:23,937 EPOCH 1050
2024-02-05 22:09:25,464 [Epoch: 1050 Step: 00070300] Batch Recognition Loss:   0.005982 => Gls Tokens per Sec:     1717 || Batch Translation Loss:   0.019496 => Txt Tokens per Sec:     4921 || Lr: 0.000050
2024-02-05 22:09:29,463 Epoch 1050: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.40 
2024-02-05 22:09:29,463 EPOCH 1051
2024-02-05 22:09:32,989 [Epoch: 1051 Step: 00070400] Batch Recognition Loss:   0.003362 => Gls Tokens per Sec:     2241 || Batch Translation Loss:   0.014830 => Txt Tokens per Sec:     6216 || Lr: 0.000050
2024-02-05 22:09:34,504 Epoch 1051: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.80 
2024-02-05 22:09:34,505 EPOCH 1052
2024-02-05 22:09:40,064 Epoch 1052: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.83 
2024-02-05 22:09:40,065 EPOCH 1053
2024-02-05 22:09:41,575 [Epoch: 1053 Step: 00070500] Batch Recognition Loss:   0.000474 => Gls Tokens per Sec:     1631 || Batch Translation Loss:   0.045259 => Txt Tokens per Sec:     4857 || Lr: 0.000050
2024-02-05 22:09:45,310 Epoch 1053: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.49 
2024-02-05 22:09:45,310 EPOCH 1054
2024-02-05 22:09:49,069 [Epoch: 1054 Step: 00070600] Batch Recognition Loss:   0.000141 => Gls Tokens per Sec:     2086 || Batch Translation Loss:   0.005524 => Txt Tokens per Sec:     5620 || Lr: 0.000050
2024-02-05 22:09:50,713 Epoch 1054: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.30 
2024-02-05 22:09:50,713 EPOCH 1055
2024-02-05 22:09:56,081 Epoch 1055: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.13 
2024-02-05 22:09:56,081 EPOCH 1056
2024-02-05 22:09:57,379 [Epoch: 1056 Step: 00070700] Batch Recognition Loss:   0.001734 => Gls Tokens per Sec:     1773 || Batch Translation Loss:   0.020637 => Txt Tokens per Sec:     5076 || Lr: 0.000050
2024-02-05 22:10:01,293 Epoch 1056: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.19 
2024-02-05 22:10:01,293 EPOCH 1057
2024-02-05 22:10:05,002 [Epoch: 1057 Step: 00070800] Batch Recognition Loss:   0.000234 => Gls Tokens per Sec:     2044 || Batch Translation Loss:   0.021843 => Txt Tokens per Sec:     5624 || Lr: 0.000050
2024-02-05 22:10:06,760 Epoch 1057: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.17 
2024-02-05 22:10:06,761 EPOCH 1058
2024-02-05 22:10:11,922 Epoch 1058: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.61 
2024-02-05 22:10:11,922 EPOCH 1059
2024-02-05 22:10:13,070 [Epoch: 1059 Step: 00070900] Batch Recognition Loss:   0.001478 => Gls Tokens per Sec:     1953 || Batch Translation Loss:   0.043848 => Txt Tokens per Sec:     5453 || Lr: 0.000050
2024-02-05 22:10:17,577 Epoch 1059: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.34 
2024-02-05 22:10:17,578 EPOCH 1060
2024-02-05 22:10:21,000 [Epoch: 1060 Step: 00071000] Batch Recognition Loss:   0.001757 => Gls Tokens per Sec:     2169 || Batch Translation Loss:   0.063957 => Txt Tokens per Sec:     5970 || Lr: 0.000050
2024-02-05 22:10:22,674 Epoch 1060: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.31 
2024-02-05 22:10:22,674 EPOCH 1061
2024-02-05 22:10:28,144 Epoch 1061: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.91 
2024-02-05 22:10:28,144 EPOCH 1062
2024-02-05 22:10:29,038 [Epoch: 1062 Step: 00071100] Batch Recognition Loss:   0.000346 => Gls Tokens per Sec:     2329 || Batch Translation Loss:   0.021693 => Txt Tokens per Sec:     6240 || Lr: 0.000050
2024-02-05 22:10:33,372 Epoch 1062: Total Training Recognition Loss 0.34  Total Training Translation Loss 2.83 
2024-02-05 22:10:33,372 EPOCH 1063
2024-02-05 22:10:37,172 [Epoch: 1063 Step: 00071200] Batch Recognition Loss:   0.000301 => Gls Tokens per Sec:     1938 || Batch Translation Loss:   0.020080 => Txt Tokens per Sec:     5485 || Lr: 0.000050
2024-02-05 22:10:38,840 Epoch 1063: Total Training Recognition Loss 2.17  Total Training Translation Loss 3.31 
2024-02-05 22:10:38,841 EPOCH 1064
2024-02-05 22:10:44,037 Epoch 1064: Total Training Recognition Loss 6.19  Total Training Translation Loss 3.35 
2024-02-05 22:10:44,038 EPOCH 1065
2024-02-05 22:10:45,174 [Epoch: 1065 Step: 00071300] Batch Recognition Loss:   0.000438 => Gls Tokens per Sec:     1692 || Batch Translation Loss:   0.030007 => Txt Tokens per Sec:     4896 || Lr: 0.000050
2024-02-05 22:10:49,394 Epoch 1065: Total Training Recognition Loss 0.35  Total Training Translation Loss 2.70 
2024-02-05 22:10:49,394 EPOCH 1066
2024-02-05 22:10:52,661 [Epoch: 1066 Step: 00071400] Batch Recognition Loss:   0.000740 => Gls Tokens per Sec:     2174 || Batch Translation Loss:   0.034459 => Txt Tokens per Sec:     5771 || Lr: 0.000050
2024-02-05 22:10:54,768 Epoch 1066: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.00 
2024-02-05 22:10:54,769 EPOCH 1067
2024-02-05 22:11:00,042 Epoch 1067: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.00 
2024-02-05 22:11:00,043 EPOCH 1068
2024-02-05 22:11:00,851 [Epoch: 1068 Step: 00071500] Batch Recognition Loss:   0.000117 => Gls Tokens per Sec:     2181 || Batch Translation Loss:   0.014804 => Txt Tokens per Sec:     6214 || Lr: 0.000050
2024-02-05 22:11:05,567 Epoch 1068: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.14 
2024-02-05 22:11:05,567 EPOCH 1069
2024-02-05 22:11:09,075 [Epoch: 1069 Step: 00071600] Batch Recognition Loss:   0.000242 => Gls Tokens per Sec:     1979 || Batch Translation Loss:   0.021595 => Txt Tokens per Sec:     5527 || Lr: 0.000050
2024-02-05 22:11:10,780 Epoch 1069: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.24 
2024-02-05 22:11:10,781 EPOCH 1070
2024-02-05 22:11:16,216 Epoch 1070: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.75 
2024-02-05 22:11:16,216 EPOCH 1071
2024-02-05 22:11:17,130 [Epoch: 1071 Step: 00071700] Batch Recognition Loss:   0.004271 => Gls Tokens per Sec:     1642 || Batch Translation Loss:   0.060892 => Txt Tokens per Sec:     4814 || Lr: 0.000050
2024-02-05 22:11:21,069 Epoch 1071: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.16 
2024-02-05 22:11:21,069 EPOCH 1072
2024-02-05 22:11:24,520 [Epoch: 1072 Step: 00071800] Batch Recognition Loss:   0.000118 => Gls Tokens per Sec:     1965 || Batch Translation Loss:   0.018025 => Txt Tokens per Sec:     5259 || Lr: 0.000050
2024-02-05 22:11:26,725 Epoch 1072: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.20 
2024-02-05 22:11:26,726 EPOCH 1073
2024-02-05 22:11:31,751 Epoch 1073: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.98 
2024-02-05 22:11:31,751 EPOCH 1074
2024-02-05 22:11:32,264 [Epoch: 1074 Step: 00071900] Batch Recognition Loss:   0.000205 => Gls Tokens per Sec:     2812 || Batch Translation Loss:   0.021259 => Txt Tokens per Sec:     7328 || Lr: 0.000050
2024-02-05 22:11:36,820 Epoch 1074: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.80 
2024-02-05 22:11:36,821 EPOCH 1075
2024-02-05 22:11:39,987 [Epoch: 1075 Step: 00072000] Batch Recognition Loss:   0.003580 => Gls Tokens per Sec:     2123 || Batch Translation Loss:   0.014180 => Txt Tokens per Sec:     5893 || Lr: 0.000050
2024-02-05 22:11:48,682 Validation result at epoch 1075, step    72000: duration: 8.6942s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.39236	Translation Loss: 93141.41406	PPL: 10970.63574
	Eval Metric: BLEU
	WER 2.61	(DEL: 0.00,	INS: 0.00,	SUB: 2.61)
	BLEU-4 0.53	(BLEU-1: 9.98,	BLEU-2: 3.11,	BLEU-3: 1.23,	BLEU-4: 0.53)
	CHRF 16.74	ROUGE 8.37
2024-02-05 22:11:48,684 Logging Recognition and Translation Outputs
2024-02-05 22:11:48,684 ========================================================================================================================
2024-02-05 22:11:48,684 Logging Sequence: 101_92.00
2024-02-05 22:11:48,684 	Gloss Reference :	A B+C+D+E
2024-02-05 22:11:48,684 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:11:48,684 	Gloss Alignment :	         
2024-02-05 22:11:48,685 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:11:48,685 	Text Reference  :	******** india had     to   score 190  runs to  win 
2024-02-05 22:11:48,685 	Text Hypothesis :	pakistan team  members know who   have left the loss
2024-02-05 22:11:48,685 	Text Alignment  :	I        S     S       S    S     S    S    S   S   
2024-02-05 22:11:48,686 ========================================================================================================================
2024-02-05 22:11:48,686 Logging Sequence: 164_412.00
2024-02-05 22:11:48,686 	Gloss Reference :	A B+C+D+E
2024-02-05 22:11:48,686 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:11:48,686 	Gloss Alignment :	         
2024-02-05 22:11:48,686 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:11:48,688 	Text Reference  :	if  you divide these two  figures you will be shocked to know that each ball's worth is   rs  50      lakhs  
2024-02-05 22:11:48,688 	Text Hypothesis :	but it  was    to    bcci do      you **** ** ******* ** know who  will have   to    wait for further updates
2024-02-05 22:11:48,688 	Text Alignment  :	S   S   S      S     S    S           D    D  D       D       S    S    S      S     S    S   S       S      
2024-02-05 22:11:48,688 ========================================================================================================================
2024-02-05 22:11:48,689 Logging Sequence: 177_160.00
2024-02-05 22:11:48,689 	Gloss Reference :	A B+C+D+E
2024-02-05 22:11:48,689 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:11:48,689 	Gloss Alignment :	         
2024-02-05 22:11:48,689 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:11:48,690 	Text Reference  :	the police also said    that sushil had     asked his friend to ** record a   video   
2024-02-05 22:11:48,690 	Text Hypothesis :	*** ****** **** however this was    because of    her strict to do with   the olympics
2024-02-05 22:11:48,690 	Text Alignment  :	D   D      D    S       S    S      S       S     S   S         I  S      S   S       
2024-02-05 22:11:48,691 ========================================================================================================================
2024-02-05 22:11:48,691 Logging Sequence: 124_62.00
2024-02-05 22:11:48,691 	Gloss Reference :	A B+C+D+E
2024-02-05 22:11:48,691 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:11:48,691 	Gloss Alignment :	         
2024-02-05 22:11:48,691 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:11:48,692 	Text Reference  :	however dhoni has said that he will continue to  play      for the team
2024-02-05 22:11:48,692 	Text Hypothesis :	******* dhoni has **** **** ** a    huge     fan following on  his well
2024-02-05 22:11:48,692 	Text Alignment  :	D                 D    D    D  S    S        S   S         S   S   S   
2024-02-05 22:11:48,692 ========================================================================================================================
2024-02-05 22:11:48,693 Logging Sequence: 71_149.00
2024-02-05 22:11:48,693 	Gloss Reference :	A B+C+D+E
2024-02-05 22:11:48,693 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:11:48,693 	Gloss Alignment :	         
2024-02-05 22:11:48,693 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:11:48,695 	Text Reference  :	**** *** ** his coach sanjay had  suggested his name for the  madhya pradesh  ranji trophy team 
2024-02-05 22:11:48,695 	Text Hypothesis :	just and he was not   by     your hardwork  and he   had also an     outbreak which went   viral
2024-02-05 22:11:48,695 	Text Alignment  :	I    I   I  S   S     S      S    S         S   S    S   S    S      S        S     S      S    
2024-02-05 22:11:48,695 ========================================================================================================================
2024-02-05 22:11:50,816 Epoch 1075: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.38 
2024-02-05 22:11:50,816 EPOCH 1076
2024-02-05 22:11:56,192 Epoch 1076: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.96 
2024-02-05 22:11:56,193 EPOCH 1077
2024-02-05 22:11:56,766 [Epoch: 1077 Step: 00072100] Batch Recognition Loss:   0.000242 => Gls Tokens per Sec:     2234 || Batch Translation Loss:   0.016875 => Txt Tokens per Sec:     6239 || Lr: 0.000050
2024-02-05 22:12:01,287 Epoch 1077: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.03 
2024-02-05 22:12:01,287 EPOCH 1078
2024-02-05 22:12:04,123 [Epoch: 1078 Step: 00072200] Batch Recognition Loss:   0.000134 => Gls Tokens per Sec:     2314 || Batch Translation Loss:   0.023120 => Txt Tokens per Sec:     6325 || Lr: 0.000050
2024-02-05 22:12:06,444 Epoch 1078: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.40 
2024-02-05 22:12:06,444 EPOCH 1079
2024-02-05 22:12:11,540 Epoch 1079: Total Training Recognition Loss 0.05  Total Training Translation Loss 4.56 
2024-02-05 22:12:11,541 EPOCH 1080
2024-02-05 22:12:12,016 [Epoch: 1080 Step: 00072300] Batch Recognition Loss:   0.000468 => Gls Tokens per Sec:     2355 || Batch Translation Loss:   0.034574 => Txt Tokens per Sec:     6759 || Lr: 0.000050
2024-02-05 22:12:16,747 Epoch 1080: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.56 
2024-02-05 22:12:16,747 EPOCH 1081
2024-02-05 22:12:19,670 [Epoch: 1081 Step: 00072400] Batch Recognition Loss:   0.001295 => Gls Tokens per Sec:     2191 || Batch Translation Loss:   0.032631 => Txt Tokens per Sec:     5900 || Lr: 0.000050
2024-02-05 22:12:21,857 Epoch 1081: Total Training Recognition Loss 0.05  Total Training Translation Loss 5.73 
2024-02-05 22:12:21,857 EPOCH 1082
2024-02-05 22:12:26,361 Epoch 1082: Total Training Recognition Loss 0.04  Total Training Translation Loss 4.05 
2024-02-05 22:12:26,361 EPOCH 1083
2024-02-05 22:12:26,778 [Epoch: 1083 Step: 00072500] Batch Recognition Loss:   0.000297 => Gls Tokens per Sec:     2308 || Batch Translation Loss:   0.018456 => Txt Tokens per Sec:     6250 || Lr: 0.000050
2024-02-05 22:12:31,724 Epoch 1083: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.80 
2024-02-05 22:12:31,724 EPOCH 1084
2024-02-05 22:12:34,761 [Epoch: 1084 Step: 00072600] Batch Recognition Loss:   0.000407 => Gls Tokens per Sec:     2022 || Batch Translation Loss:   0.011149 => Txt Tokens per Sec:     5655 || Lr: 0.000050
2024-02-05 22:12:37,016 Epoch 1084: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.61 
2024-02-05 22:12:37,016 EPOCH 1085
2024-02-05 22:12:42,485 Epoch 1085: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.41 
2024-02-05 22:12:42,486 EPOCH 1086
2024-02-05 22:12:42,808 [Epoch: 1086 Step: 00072700] Batch Recognition Loss:   0.000246 => Gls Tokens per Sec:     2496 || Batch Translation Loss:   0.022943 => Txt Tokens per Sec:     6709 || Lr: 0.000050
2024-02-05 22:12:47,105 Epoch 1086: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.25 
2024-02-05 22:12:47,105 EPOCH 1087
2024-02-05 22:12:50,315 [Epoch: 1087 Step: 00072800] Batch Recognition Loss:   0.000133 => Gls Tokens per Sec:     1895 || Batch Translation Loss:   0.014471 => Txt Tokens per Sec:     5161 || Lr: 0.000050
2024-02-05 22:12:52,587 Epoch 1087: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.21 
2024-02-05 22:12:52,587 EPOCH 1088
2024-02-05 22:12:57,463 Epoch 1088: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.59 
2024-02-05 22:12:57,464 EPOCH 1089
2024-02-05 22:12:57,701 [Epoch: 1089 Step: 00072900] Batch Recognition Loss:   0.000245 => Gls Tokens per Sec:     2712 || Batch Translation Loss:   0.005221 => Txt Tokens per Sec:     6148 || Lr: 0.000050
2024-02-05 22:13:02,952 Epoch 1089: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.70 
2024-02-05 22:13:02,953 EPOCH 1090
2024-02-05 22:13:05,482 [Epoch: 1090 Step: 00073000] Batch Recognition Loss:   0.000220 => Gls Tokens per Sec:     2342 || Batch Translation Loss:   0.069846 => Txt Tokens per Sec:     6627 || Lr: 0.000050
2024-02-05 22:13:07,815 Epoch 1090: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.71 
2024-02-05 22:13:07,815 EPOCH 1091
2024-02-05 22:13:13,136 Epoch 1091: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.42 
2024-02-05 22:13:13,137 EPOCH 1092
2024-02-05 22:13:13,334 [Epoch: 1092 Step: 00073100] Batch Recognition Loss:   0.000227 => Gls Tokens per Sec:     2437 || Batch Translation Loss:   0.018254 => Txt Tokens per Sec:     6254 || Lr: 0.000050
2024-02-05 22:13:17,677 Epoch 1092: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.49 
2024-02-05 22:13:17,677 EPOCH 1093
2024-02-05 22:13:20,237 [Epoch: 1093 Step: 00073200] Batch Recognition Loss:   0.000156 => Gls Tokens per Sec:     2251 || Batch Translation Loss:   0.017772 => Txt Tokens per Sec:     6439 || Lr: 0.000050
2024-02-05 22:13:22,624 Epoch 1093: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.47 
2024-02-05 22:13:22,624 EPOCH 1094
2024-02-05 22:13:28,009 Epoch 1094: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.30 
2024-02-05 22:13:28,009 EPOCH 1095
2024-02-05 22:13:28,197 [Epoch: 1095 Step: 00073300] Batch Recognition Loss:   0.000256 => Gls Tokens per Sec:     1711 || Batch Translation Loss:   0.017108 => Txt Tokens per Sec:     5652 || Lr: 0.000050
2024-02-05 22:13:33,013 Epoch 1095: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.06 
2024-02-05 22:13:33,014 EPOCH 1096
2024-02-05 22:13:35,671 [Epoch: 1096 Step: 00073400] Batch Recognition Loss:   0.001608 => Gls Tokens per Sec:     2108 || Batch Translation Loss:   0.013492 => Txt Tokens per Sec:     5677 || Lr: 0.000050
2024-02-05 22:13:38,238 Epoch 1096: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.05 
2024-02-05 22:13:38,238 EPOCH 1097
2024-02-05 22:13:43,458 Epoch 1097: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.27 
2024-02-05 22:13:43,458 EPOCH 1098
2024-02-05 22:13:43,518 [Epoch: 1098 Step: 00073500] Batch Recognition Loss:   0.000151 => Gls Tokens per Sec:     2759 || Batch Translation Loss:   0.017270 => Txt Tokens per Sec:     6759 || Lr: 0.000050
2024-02-05 22:13:48,454 Epoch 1098: Total Training Recognition Loss 0.04  Total Training Translation Loss 6.24 
2024-02-05 22:13:48,454 EPOCH 1099
2024-02-05 22:13:51,080 [Epoch: 1099 Step: 00073600] Batch Recognition Loss:   0.000240 => Gls Tokens per Sec:     2035 || Batch Translation Loss:   0.045119 => Txt Tokens per Sec:     5696 || Lr: 0.000050
2024-02-05 22:13:53,761 Epoch 1099: Total Training Recognition Loss 0.04  Total Training Translation Loss 7.33 
2024-02-05 22:13:53,761 EPOCH 1100
2024-02-05 22:13:58,805 [Epoch: 1100 Step: 00073700] Batch Recognition Loss:   0.000297 => Gls Tokens per Sec:     2106 || Batch Translation Loss:   0.033155 => Txt Tokens per Sec:     5826 || Lr: 0.000050
2024-02-05 22:13:58,806 Epoch 1100: Total Training Recognition Loss 0.04  Total Training Translation Loss 5.22 
2024-02-05 22:13:58,806 EPOCH 1101
2024-02-05 22:14:04,343 Epoch 1101: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.80 
2024-02-05 22:14:04,344 EPOCH 1102
2024-02-05 22:14:06,827 [Epoch: 1102 Step: 00073800] Batch Recognition Loss:   0.000256 => Gls Tokens per Sec:     2087 || Batch Translation Loss:   0.059794 => Txt Tokens per Sec:     5886 || Lr: 0.000050
2024-02-05 22:14:09,080 Epoch 1102: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.53 
2024-02-05 22:14:09,081 EPOCH 1103
2024-02-05 22:14:14,463 [Epoch: 1103 Step: 00073900] Batch Recognition Loss:   0.000241 => Gls Tokens per Sec:     1944 || Batch Translation Loss:   0.027343 => Txt Tokens per Sec:     5361 || Lr: 0.000050
2024-02-05 22:14:14,581 Epoch 1103: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.85 
2024-02-05 22:14:14,581 EPOCH 1104
2024-02-05 22:14:19,553 Epoch 1104: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.91 
2024-02-05 22:14:19,554 EPOCH 1105
2024-02-05 22:14:22,211 [Epoch: 1105 Step: 00074000] Batch Recognition Loss:   0.001910 => Gls Tokens per Sec:     1928 || Batch Translation Loss:   0.008591 => Txt Tokens per Sec:     5398 || Lr: 0.000050
2024-02-05 22:14:30,678 Validation result at epoch 1105, step    74000: duration: 8.4658s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.41774	Translation Loss: 92614.47656	PPL: 10408.17773
	Eval Metric: BLEU
	WER 2.82	(DEL: 0.00,	INS: 0.00,	SUB: 2.82)
	BLEU-4 0.52	(BLEU-1: 9.91,	BLEU-2: 2.96,	BLEU-3: 1.10,	BLEU-4: 0.52)
	CHRF 16.89	ROUGE 8.42
2024-02-05 22:14:30,679 Logging Recognition and Translation Outputs
2024-02-05 22:14:30,679 ========================================================================================================================
2024-02-05 22:14:30,679 Logging Sequence: 77_172.00
2024-02-05 22:14:30,679 	Gloss Reference :	A B+C+D+E
2024-02-05 22:14:30,680 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:14:30,680 	Gloss Alignment :	         
2024-02-05 22:14:30,680 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:14:30,682 	Text Reference  :	he scored 2 runs on      the fourth ball on     the       5th    6th   jadeja scored a          6      and a           boundary respectively
2024-02-05 22:14:30,682 	Text Hypothesis :	** ****** * **** towards the ****** sri  lankan stadium's ground staff who    toiled diligently during the rain-soaked asia     cup         
2024-02-05 22:14:30,682 	Text Alignment  :	D  D      D D    S           D      S    S      S         S      S     S      S      S          S      S   S           S        S           
2024-02-05 22:14:30,682 ========================================================================================================================
2024-02-05 22:14:30,682 Logging Sequence: 173_39.00
2024-02-05 22:14:30,682 	Gloss Reference :	A B+C+D+E
2024-02-05 22:14:30,682 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:14:30,682 	Gloss Alignment :	         
2024-02-05 22:14:30,683 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:14:30,683 	Text Reference  :	kohli will step down      as      india' captain
2024-02-05 22:14:30,683 	Text Hypothesis :	***** i    am   extremely sadened by     this   
2024-02-05 22:14:30,683 	Text Alignment  :	D     S    S    S         S       S      S      
2024-02-05 22:14:30,684 ========================================================================================================================
2024-02-05 22:14:30,684 Logging Sequence: 138_224.00
2024-02-05 22:14:30,684 	Gloss Reference :	A B+C+D+E
2024-02-05 22:14:30,684 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:14:30,684 	Gloss Alignment :	         
2024-02-05 22:14:30,684 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:14:30,685 	Text Reference  :	then people wrote positive messages and stuck them on  the plastic sheets  
2024-02-05 22:14:30,685 	Text Hypothesis :	**** ****** ***** they     will     be  sent  to   see the covid   pandemic
2024-02-05 22:14:30,685 	Text Alignment  :	D    D      D     S        S        S   S     S    S       S       S       
2024-02-05 22:14:30,685 ========================================================================================================================
2024-02-05 22:14:30,685 Logging Sequence: 128_98.00
2024-02-05 22:14:30,686 	Gloss Reference :	A B+C+D+E
2024-02-05 22:14:30,686 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:14:30,686 	Gloss Alignment :	         
2024-02-05 22:14:30,686 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:14:30,687 	Text Reference  :	with 8 wickets and 43 balls remaining they won the   match   in   such        a   short         time  
2024-02-05 22:14:30,687 	Text Hypothesis :	**** * ******* *** ** ***** ********* **** but their captain kane williamson' key contributions helped
2024-02-05 22:14:30,687 	Text Alignment  :	D    D D       D   D  D     D         D    S   S     S       S    S           S   S             S     
2024-02-05 22:14:30,687 ========================================================================================================================
2024-02-05 22:14:30,687 Logging Sequence: 126_159.00
2024-02-05 22:14:30,688 	Gloss Reference :	A B+C+D+E
2024-02-05 22:14:30,688 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:14:30,688 	Gloss Alignment :	         
2024-02-05 22:14:30,688 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:14:30,689 	Text Reference  :	despite multiple challenges and injuries you   did   not give up   
2024-02-05 22:14:30,689 	Text Hypothesis :	******* this     is         why the      medal stood in  the  medal
2024-02-05 22:14:30,689 	Text Alignment  :	D       S        S          S   S        S     S     S   S    S    
2024-02-05 22:14:30,689 ========================================================================================================================
2024-02-05 22:14:33,464 Epoch 1105: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.84 
2024-02-05 22:14:33,464 EPOCH 1106
2024-02-05 22:14:38,511 [Epoch: 1106 Step: 00074100] Batch Recognition Loss:   0.000254 => Gls Tokens per Sec:     2041 || Batch Translation Loss:   0.034749 => Txt Tokens per Sec:     5643 || Lr: 0.000050
2024-02-05 22:14:38,705 Epoch 1106: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.48 
2024-02-05 22:14:38,705 EPOCH 1107
2024-02-05 22:14:44,360 Epoch 1107: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.54 
2024-02-05 22:14:44,360 EPOCH 1108
2024-02-05 22:14:46,779 [Epoch: 1108 Step: 00074200] Batch Recognition Loss:   0.000150 => Gls Tokens per Sec:     2053 || Batch Translation Loss:   0.019574 => Txt Tokens per Sec:     5388 || Lr: 0.000050
2024-02-05 22:14:49,887 Epoch 1108: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.05 
2024-02-05 22:14:49,887 EPOCH 1109
2024-02-05 22:14:54,513 [Epoch: 1109 Step: 00074300] Batch Recognition Loss:   0.000261 => Gls Tokens per Sec:     2192 || Batch Translation Loss:   0.019906 => Txt Tokens per Sec:     6049 || Lr: 0.000050
2024-02-05 22:14:54,722 Epoch 1109: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.46 
2024-02-05 22:14:54,722 EPOCH 1110
2024-02-05 22:15:00,058 Epoch 1110: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.44 
2024-02-05 22:15:00,059 EPOCH 1111
2024-02-05 22:15:02,268 [Epoch: 1111 Step: 00074400] Batch Recognition Loss:   0.000187 => Gls Tokens per Sec:     2174 || Batch Translation Loss:   0.014793 => Txt Tokens per Sec:     6125 || Lr: 0.000050
2024-02-05 22:15:04,977 Epoch 1111: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.60 
2024-02-05 22:15:04,977 EPOCH 1112
2024-02-05 22:15:10,110 [Epoch: 1112 Step: 00074500] Batch Recognition Loss:   0.000212 => Gls Tokens per Sec:     1945 || Batch Translation Loss:   0.036948 => Txt Tokens per Sec:     5390 || Lr: 0.000050
2024-02-05 22:15:10,393 Epoch 1112: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.58 
2024-02-05 22:15:10,393 EPOCH 1113
2024-02-05 22:15:15,147 Epoch 1113: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.81 
2024-02-05 22:15:15,147 EPOCH 1114
2024-02-05 22:15:17,422 [Epoch: 1114 Step: 00074600] Batch Recognition Loss:   0.032269 => Gls Tokens per Sec:     2041 || Batch Translation Loss:   0.093578 => Txt Tokens per Sec:     5405 || Lr: 0.000050
2024-02-05 22:15:20,867 Epoch 1114: Total Training Recognition Loss 0.09  Total Training Translation Loss 8.81 
2024-02-05 22:15:20,867 EPOCH 1115
2024-02-05 22:15:25,165 [Epoch: 1115 Step: 00074700] Batch Recognition Loss:   0.000835 => Gls Tokens per Sec:     2286 || Batch Translation Loss:   0.041064 => Txt Tokens per Sec:     6273 || Lr: 0.000050
2024-02-05 22:15:25,658 Epoch 1115: Total Training Recognition Loss 0.11  Total Training Translation Loss 6.15 
2024-02-05 22:15:25,659 EPOCH 1116
2024-02-05 22:15:31,230 Epoch 1116: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.48 
2024-02-05 22:15:31,231 EPOCH 1117
2024-02-05 22:15:33,183 [Epoch: 1117 Step: 00074800] Batch Recognition Loss:   0.000621 => Gls Tokens per Sec:     2296 || Batch Translation Loss:   0.044487 => Txt Tokens per Sec:     6287 || Lr: 0.000050
2024-02-05 22:15:36,242 Epoch 1117: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.62 
2024-02-05 22:15:36,243 EPOCH 1118
2024-02-05 22:15:40,945 [Epoch: 1118 Step: 00074900] Batch Recognition Loss:   0.000261 => Gls Tokens per Sec:     2055 || Batch Translation Loss:   0.019869 => Txt Tokens per Sec:     5635 || Lr: 0.000050
2024-02-05 22:15:41,590 Epoch 1118: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.57 
2024-02-05 22:15:41,590 EPOCH 1119
2024-02-05 22:15:47,186 Epoch 1119: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.32 
2024-02-05 22:15:47,187 EPOCH 1120
2024-02-05 22:15:49,280 [Epoch: 1120 Step: 00075000] Batch Recognition Loss:   0.000134 => Gls Tokens per Sec:     2066 || Batch Translation Loss:   0.012905 => Txt Tokens per Sec:     5923 || Lr: 0.000050
2024-02-05 22:15:51,859 Epoch 1120: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.46 
2024-02-05 22:15:51,860 EPOCH 1121
2024-02-05 22:15:56,709 [Epoch: 1121 Step: 00075100] Batch Recognition Loss:   0.001100 => Gls Tokens per Sec:     1959 || Batch Translation Loss:   0.006686 => Txt Tokens per Sec:     5509 || Lr: 0.000050
2024-02-05 22:15:57,132 Epoch 1121: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.19 
2024-02-05 22:15:57,133 EPOCH 1122
2024-02-05 22:16:02,046 Epoch 1122: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.10 
2024-02-05 22:16:02,046 EPOCH 1123
2024-02-05 22:16:04,065 [Epoch: 1123 Step: 00075200] Batch Recognition Loss:   0.000130 => Gls Tokens per Sec:     2013 || Batch Translation Loss:   0.010394 => Txt Tokens per Sec:     5319 || Lr: 0.000050
2024-02-05 22:16:07,559 Epoch 1123: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.99 
2024-02-05 22:16:07,559 EPOCH 1124
2024-02-05 22:16:11,794 [Epoch: 1124 Step: 00075300] Batch Recognition Loss:   0.000140 => Gls Tokens per Sec:     2206 || Batch Translation Loss:   0.031433 => Txt Tokens per Sec:     6072 || Lr: 0.000050
2024-02-05 22:16:12,396 Epoch 1124: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.04 
2024-02-05 22:16:12,396 EPOCH 1125
2024-02-05 22:16:17,800 Epoch 1125: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.96 
2024-02-05 22:16:17,800 EPOCH 1126
2024-02-05 22:16:19,364 [Epoch: 1126 Step: 00075400] Batch Recognition Loss:   0.000797 => Gls Tokens per Sec:     2498 || Batch Translation Loss:   0.014413 => Txt Tokens per Sec:     6623 || Lr: 0.000050
2024-02-05 22:16:22,462 Epoch 1126: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.99 
2024-02-05 22:16:22,462 EPOCH 1127
2024-02-05 22:16:27,264 [Epoch: 1127 Step: 00075500] Batch Recognition Loss:   0.000934 => Gls Tokens per Sec:     1912 || Batch Translation Loss:   0.010899 => Txt Tokens per Sec:     5288 || Lr: 0.000050
2024-02-05 22:16:27,915 Epoch 1127: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.98 
2024-02-05 22:16:27,915 EPOCH 1128
2024-02-05 22:16:32,948 Epoch 1128: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.93 
2024-02-05 22:16:32,949 EPOCH 1129
2024-02-05 22:16:35,025 [Epoch: 1129 Step: 00075600] Batch Recognition Loss:   0.000195 => Gls Tokens per Sec:     1803 || Batch Translation Loss:   0.018252 => Txt Tokens per Sec:     5116 || Lr: 0.000050
2024-02-05 22:16:38,395 Epoch 1129: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.95 
2024-02-05 22:16:38,395 EPOCH 1130
2024-02-05 22:16:42,512 [Epoch: 1130 Step: 00075700] Batch Recognition Loss:   0.000136 => Gls Tokens per Sec:     2192 || Batch Translation Loss:   0.014287 => Txt Tokens per Sec:     6089 || Lr: 0.000050
2024-02-05 22:16:43,211 Epoch 1130: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.10 
2024-02-05 22:16:43,212 EPOCH 1131
2024-02-05 22:16:48,610 Epoch 1131: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.38 
2024-02-05 22:16:48,610 EPOCH 1132
2024-02-05 22:16:50,133 [Epoch: 1132 Step: 00075800] Batch Recognition Loss:   0.000145 => Gls Tokens per Sec:     2420 || Batch Translation Loss:   0.021077 => Txt Tokens per Sec:     6377 || Lr: 0.000050
2024-02-05 22:16:53,673 Epoch 1132: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.79 
2024-02-05 22:16:53,674 EPOCH 1133
2024-02-05 22:16:58,226 [Epoch: 1133 Step: 00075900] Batch Recognition Loss:   0.000349 => Gls Tokens per Sec:     1947 || Batch Translation Loss:   0.087385 => Txt Tokens per Sec:     5364 || Lr: 0.000050
2024-02-05 22:16:59,012 Epoch 1133: Total Training Recognition Loss 0.10  Total Training Translation Loss 8.92 
2024-02-05 22:16:59,013 EPOCH 1134
2024-02-05 22:17:04,086 Epoch 1134: Total Training Recognition Loss 0.06  Total Training Translation Loss 6.62 
2024-02-05 22:17:04,086 EPOCH 1135
2024-02-05 22:17:05,779 [Epoch: 1135 Step: 00076000] Batch Recognition Loss:   0.000828 => Gls Tokens per Sec:     2023 || Batch Translation Loss:   0.016387 => Txt Tokens per Sec:     5463 || Lr: 0.000050
2024-02-05 22:17:14,170 Validation result at epoch 1135, step    76000: duration: 8.3905s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.42274	Translation Loss: 93758.59375	PPL: 11668.18555
	Eval Metric: BLEU
	WER 2.90	(DEL: 0.00,	INS: 0.00,	SUB: 2.90)
	BLEU-4 0.40	(BLEU-1: 9.68,	BLEU-2: 2.79,	BLEU-3: 0.97,	BLEU-4: 0.40)
	CHRF 16.16	ROUGE 8.42
2024-02-05 22:17:14,171 Logging Recognition and Translation Outputs
2024-02-05 22:17:14,171 ========================================================================================================================
2024-02-05 22:17:14,172 Logging Sequence: 96_25.00
2024-02-05 22:17:14,172 	Gloss Reference :	A B+C+D+E
2024-02-05 22:17:14,172 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:17:14,172 	Gloss Alignment :	         
2024-02-05 22:17:14,172 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:17:14,173 	Text Reference  :	after the initial matches top 2 teams of each group will go on to play super 4   where they  will play      each other
2024-02-05 22:17:14,174 	Text Hypothesis :	***** *** ******* ******* *** * ***** ** **** ***** **** ** ** ** this 2022  win was   given a    pakistani sri  lanka
2024-02-05 22:17:14,174 	Text Alignment  :	D     D   D       D       D   D D     D  D    D     D    D  D  D  S    S     S   S     S     S    S         S    S    
2024-02-05 22:17:14,174 ========================================================================================================================
2024-02-05 22:17:14,174 Logging Sequence: 83_57.00
2024-02-05 22:17:14,174 	Gloss Reference :	A B+C+D+E
2024-02-05 22:17:14,174 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:17:14,174 	Gloss Alignment :	         
2024-02-05 22:17:14,175 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:17:14,175 	Text Reference  :	collapsed face      first on       the field he  was completely unconscious
2024-02-05 22:17:14,175 	Text Hypothesis :	the       wrestlers were  supposed to  have  fun in  the        usa        
2024-02-05 22:17:14,176 	Text Alignment  :	S         S         S     S        S   S     S   S   S          S          
2024-02-05 22:17:14,176 ========================================================================================================================
2024-02-05 22:17:14,176 Logging Sequence: 86_80.00
2024-02-05 22:17:14,176 	Gloss Reference :	A B+C+D+E
2024-02-05 22:17:14,176 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:17:14,176 	Gloss Alignment :	         
2024-02-05 22:17:14,176 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:17:14,177 	Text Reference  :	* ** **** yashpal    played 160 ranji   matches
2024-02-05 22:17:14,177 	Text Hypothesis :	i am very particular about  his morning walks  
2024-02-05 22:17:14,177 	Text Alignment  :	I I  I    S          S      S   S       S      
2024-02-05 22:17:14,177 ========================================================================================================================
2024-02-05 22:17:14,177 Logging Sequence: 121_200.00
2024-02-05 22:17:14,177 	Gloss Reference :	A B+C+D+E
2024-02-05 22:17:14,178 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:17:14,178 	Gloss Alignment :	         
2024-02-05 22:17:14,178 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:17:14,179 	Text Reference  :	****** **** ** ** ***** indian   players suffered massive losses table tennis  champ manika batra tennis player sumit nagal 
2024-02-05 22:17:14,179 	Text Hypothesis :	mumbai will be in tokyo olympics in      one      place   where  any   thrower can   use    it    this   was    the   injury
2024-02-05 22:17:14,180 	Text Alignment  :	I      I    I  I  I     S        S       S        S       S      S     S       S     S      S     S      S      S     S     
2024-02-05 22:17:14,180 ========================================================================================================================
2024-02-05 22:17:14,180 Logging Sequence: 155_154.00
2024-02-05 22:17:14,180 	Gloss Reference :	A B+C+D+E
2024-02-05 22:17:14,180 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:17:14,180 	Gloss Alignment :	         
2024-02-05 22:17:14,180 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:17:14,182 	Text Reference  :	however icc did not agree to the   demands the two sides eventually resolved their differences allowing  the     afghan team to participate
2024-02-05 22:17:14,182 	Text Hypothesis :	******* *** *** *** ***** we would auction the *** ***** ********** ******** ***** same        melbourne stadium it     was  20 overs      
2024-02-05 22:17:14,182 	Text Alignment  :	D       D   D   D   D     S  S     S           D   D     D          D        D     S           S         S       S      S    S  S          
2024-02-05 22:17:14,182 ========================================================================================================================
2024-02-05 22:17:17,768 Epoch 1135: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.02 
2024-02-05 22:17:17,768 EPOCH 1136
2024-02-05 22:17:22,508 [Epoch: 1136 Step: 00076100] Batch Recognition Loss:   0.000509 => Gls Tokens per Sec:     1857 || Batch Translation Loss:   0.024144 => Txt Tokens per Sec:     5212 || Lr: 0.000050
2024-02-05 22:17:23,425 Epoch 1136: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.91 
2024-02-05 22:17:23,425 EPOCH 1137
2024-02-05 22:17:28,436 Epoch 1137: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.47 
2024-02-05 22:17:28,436 EPOCH 1138
2024-02-05 22:17:29,862 [Epoch: 1138 Step: 00076200] Batch Recognition Loss:   0.000189 => Gls Tokens per Sec:     2359 || Batch Translation Loss:   0.012767 => Txt Tokens per Sec:     6768 || Lr: 0.000050
2024-02-05 22:17:33,695 Epoch 1138: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.99 
2024-02-05 22:17:33,696 EPOCH 1139
2024-02-05 22:17:38,003 [Epoch: 1139 Step: 00076300] Batch Recognition Loss:   0.000881 => Gls Tokens per Sec:     1983 || Batch Translation Loss:   0.020469 => Txt Tokens per Sec:     5452 || Lr: 0.000050
2024-02-05 22:17:39,012 Epoch 1139: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.09 
2024-02-05 22:17:39,012 EPOCH 1140
2024-02-05 22:17:43,977 Epoch 1140: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.98 
2024-02-05 22:17:43,977 EPOCH 1141
2024-02-05 22:17:45,673 [Epoch: 1141 Step: 00076400] Batch Recognition Loss:   0.000258 => Gls Tokens per Sec:     1829 || Batch Translation Loss:   0.014150 => Txt Tokens per Sec:     5241 || Lr: 0.000050
2024-02-05 22:17:49,506 Epoch 1141: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.39 
2024-02-05 22:17:49,506 EPOCH 1142
2024-02-05 22:17:53,848 [Epoch: 1142 Step: 00076500] Batch Recognition Loss:   0.000187 => Gls Tokens per Sec:     1954 || Batch Translation Loss:   0.015972 => Txt Tokens per Sec:     5432 || Lr: 0.000050
2024-02-05 22:17:54,989 Epoch 1142: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.35 
2024-02-05 22:17:54,989 EPOCH 1143
2024-02-05 22:18:00,002 Epoch 1143: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.43 
2024-02-05 22:18:00,002 EPOCH 1144
2024-02-05 22:18:01,463 [Epoch: 1144 Step: 00076600] Batch Recognition Loss:   0.000157 => Gls Tokens per Sec:     2082 || Batch Translation Loss:   0.033637 => Txt Tokens per Sec:     5821 || Lr: 0.000050
2024-02-05 22:18:05,210 Epoch 1144: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.48 
2024-02-05 22:18:05,210 EPOCH 1145
2024-02-05 22:18:09,178 [Epoch: 1145 Step: 00076700] Batch Recognition Loss:   0.000109 => Gls Tokens per Sec:     2072 || Batch Translation Loss:   0.014808 => Txt Tokens per Sec:     5694 || Lr: 0.000050
2024-02-05 22:18:10,325 Epoch 1145: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.74 
2024-02-05 22:18:10,325 EPOCH 1146
2024-02-05 22:18:15,756 Epoch 1146: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.67 
2024-02-05 22:18:15,757 EPOCH 1147
2024-02-05 22:18:17,159 [Epoch: 1147 Step: 00076800] Batch Recognition Loss:   0.000186 => Gls Tokens per Sec:     1984 || Batch Translation Loss:   0.042601 => Txt Tokens per Sec:     5333 || Lr: 0.000050
2024-02-05 22:18:20,537 Epoch 1147: Total Training Recognition Loss 0.12  Total Training Translation Loss 3.85 
2024-02-05 22:18:20,537 EPOCH 1148
2024-02-05 22:18:24,663 [Epoch: 1148 Step: 00076900] Batch Recognition Loss:   0.000299 => Gls Tokens per Sec:     1954 || Batch Translation Loss:   0.056301 => Txt Tokens per Sec:     5373 || Lr: 0.000050
2024-02-05 22:18:26,067 Epoch 1148: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.36 
2024-02-05 22:18:26,068 EPOCH 1149
2024-02-05 22:18:30,892 Epoch 1149: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.31 
2024-02-05 22:18:30,892 EPOCH 1150
2024-02-05 22:18:32,375 [Epoch: 1150 Step: 00077000] Batch Recognition Loss:   0.000233 => Gls Tokens per Sec:     1835 || Batch Translation Loss:   0.031328 => Txt Tokens per Sec:     5090 || Lr: 0.000050
2024-02-05 22:18:36,297 Epoch 1150: Total Training Recognition Loss 0.05  Total Training Translation Loss 4.94 
2024-02-05 22:18:36,297 EPOCH 1151
2024-02-05 22:18:39,963 [Epoch: 1151 Step: 00077100] Batch Recognition Loss:   0.000218 => Gls Tokens per Sec:     2156 || Batch Translation Loss:   0.022779 => Txt Tokens per Sec:     6113 || Lr: 0.000050
2024-02-05 22:18:41,357 Epoch 1151: Total Training Recognition Loss 0.07  Total Training Translation Loss 4.19 
2024-02-05 22:18:41,358 EPOCH 1152
2024-02-05 22:18:46,559 Epoch 1152: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.67 
2024-02-05 22:18:46,559 EPOCH 1153
2024-02-05 22:18:47,563 [Epoch: 1153 Step: 00077200] Batch Recognition Loss:   0.000335 => Gls Tokens per Sec:     2555 || Batch Translation Loss:   0.202130 => Txt Tokens per Sec:     6915 || Lr: 0.000050
2024-02-05 22:18:51,560 Epoch 1153: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.37 
2024-02-05 22:18:51,561 EPOCH 1154
2024-02-05 22:18:55,540 [Epoch: 1154 Step: 00077300] Batch Recognition Loss:   0.000281 => Gls Tokens per Sec:     1946 || Batch Translation Loss:   0.017037 => Txt Tokens per Sec:     5391 || Lr: 0.000050
2024-02-05 22:18:56,855 Epoch 1154: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.88 
2024-02-05 22:18:56,855 EPOCH 1155
2024-02-05 22:19:01,878 Epoch 1155: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.71 
2024-02-05 22:19:01,879 EPOCH 1156
2024-02-05 22:19:03,046 [Epoch: 1156 Step: 00077400] Batch Recognition Loss:   0.000798 => Gls Tokens per Sec:     2058 || Batch Translation Loss:   0.007585 => Txt Tokens per Sec:     5722 || Lr: 0.000050
2024-02-05 22:19:07,198 Epoch 1156: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.14 
2024-02-05 22:19:07,198 EPOCH 1157
2024-02-05 22:19:10,920 [Epoch: 1157 Step: 00077500] Batch Recognition Loss:   0.000241 => Gls Tokens per Sec:     2037 || Batch Translation Loss:   0.023843 => Txt Tokens per Sec:     5693 || Lr: 0.000050
2024-02-05 22:19:12,271 Epoch 1157: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.99 
2024-02-05 22:19:12,271 EPOCH 1158
2024-02-05 22:19:17,273 Epoch 1158: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.66 
2024-02-05 22:19:17,273 EPOCH 1159
2024-02-05 22:19:18,473 [Epoch: 1159 Step: 00077600] Batch Recognition Loss:   0.000147 => Gls Tokens per Sec:     1868 || Batch Translation Loss:   0.021436 => Txt Tokens per Sec:     5680 || Lr: 0.000050
2024-02-05 22:19:22,648 Epoch 1159: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.73 
2024-02-05 22:19:22,648 EPOCH 1160
2024-02-05 22:19:26,204 [Epoch: 1160 Step: 00077700] Batch Recognition Loss:   0.000785 => Gls Tokens per Sec:     2116 || Batch Translation Loss:   0.025496 => Txt Tokens per Sec:     5902 || Lr: 0.000050
2024-02-05 22:19:27,567 Epoch 1160: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.97 
2024-02-05 22:19:27,567 EPOCH 1161
2024-02-05 22:19:32,907 Epoch 1161: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.76 
2024-02-05 22:19:32,908 EPOCH 1162
2024-02-05 22:19:33,833 [Epoch: 1162 Step: 00077800] Batch Recognition Loss:   0.000277 => Gls Tokens per Sec:     2250 || Batch Translation Loss:   0.021237 => Txt Tokens per Sec:     5931 || Lr: 0.000050
2024-02-05 22:19:37,793 Epoch 1162: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.29 
2024-02-05 22:19:37,794 EPOCH 1163
2024-02-05 22:19:41,857 [Epoch: 1163 Step: 00077900] Batch Recognition Loss:   0.000188 => Gls Tokens per Sec:     1787 || Batch Translation Loss:   0.023351 => Txt Tokens per Sec:     5182 || Lr: 0.000050
2024-02-05 22:19:43,213 Epoch 1163: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.79 
2024-02-05 22:19:43,213 EPOCH 1164
2024-02-05 22:19:48,059 Epoch 1164: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.75 
2024-02-05 22:19:48,059 EPOCH 1165
2024-02-05 22:19:48,928 [Epoch: 1165 Step: 00078000] Batch Recognition Loss:   0.000220 => Gls Tokens per Sec:     2214 || Batch Translation Loss:   0.222564 => Txt Tokens per Sec:     6040 || Lr: 0.000050
2024-02-05 22:19:57,516 Validation result at epoch 1165, step    78000: duration: 8.5878s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.06859	Translation Loss: 92792.72656	PPL: 10595.13672
	Eval Metric: BLEU
	WER 2.75	(DEL: 0.00,	INS: 0.00,	SUB: 2.75)
	BLEU-4 0.53	(BLEU-1: 10.34,	BLEU-2: 3.18,	BLEU-3: 1.12,	BLEU-4: 0.53)
	CHRF 16.96	ROUGE 8.73
2024-02-05 22:19:57,517 Logging Recognition and Translation Outputs
2024-02-05 22:19:57,518 ========================================================================================================================
2024-02-05 22:19:57,518 Logging Sequence: 153_36.00
2024-02-05 22:19:57,518 	Gloss Reference :	A B+C+D+E
2024-02-05 22:19:57,518 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:19:57,518 	Gloss Alignment :	         
2024-02-05 22:19:57,518 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:19:57,519 	Text Reference  :	**** ** *** ***** *** india **** made a   good  score of  1686     in 20 overs
2024-02-05 22:19:57,519 	Text Hypothesis :	both of the world cup india lost the  t20 world cup   and pakistan in ** qatar
2024-02-05 22:19:57,519 	Text Alignment  :	I    I  I   I     I         I    S    S   S     S     S   S           D  S    
2024-02-05 22:19:57,520 ========================================================================================================================
2024-02-05 22:19:57,520 Logging Sequence: 181_46.00
2024-02-05 22:19:57,520 	Gloss Reference :	A B+C+D+E
2024-02-05 22:19:57,520 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:19:57,520 	Gloss Alignment :	         
2024-02-05 22:19:57,520 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:19:57,521 	Text Reference  :	*** people were overjoyed with the news singh        posted on   social media saying
2024-02-05 22:19:57,521 	Text Hypothesis :	but there  was  a         huge odi and  commonwealth games  like me     tell  you   
2024-02-05 22:19:57,522 	Text Alignment  :	I   S      S    S         S    S   S    S            S      S    S      S     S     
2024-02-05 22:19:57,522 ========================================================================================================================
2024-02-05 22:19:57,522 Logging Sequence: 59_2.00
2024-02-05 22:19:57,522 	Gloss Reference :	A B+C+D+E
2024-02-05 22:19:57,522 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:19:57,522 	Gloss Alignment :	         
2024-02-05 22:19:57,522 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:19:57,523 	Text Reference  :	** ***** **** *** **** at  the  2020 tokyo olympics in  japan
2024-02-05 22:19:57,523 	Text Hypothesis :	we don't know who will win will have to    wait     and watch
2024-02-05 22:19:57,523 	Text Alignment  :	I  I     I    I   I    S   S    S    S     S        S   S    
2024-02-05 22:19:57,523 ========================================================================================================================
2024-02-05 22:19:57,523 Logging Sequence: 123_14.00
2024-02-05 22:19:57,524 	Gloss Reference :	A B+C+D+E
2024-02-05 22:19:57,524 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:19:57,524 	Gloss Alignment :	         
2024-02-05 22:19:57,524 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:19:57,525 	Text Reference  :	he is very down to    earth   and amazing leader as  well    
2024-02-05 22:19:57,525 	Text Hypothesis :	** ** **** this upset gambhir a   lot     of     his decision
2024-02-05 22:19:57,525 	Text Alignment  :	D  D  D    S    S     S       S   S       S      S   S       
2024-02-05 22:19:57,525 ========================================================================================================================
2024-02-05 22:19:57,525 Logging Sequence: 166_120.00
2024-02-05 22:19:57,526 	Gloss Reference :	A B+C+D+E
2024-02-05 22:19:57,526 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:19:57,526 	Gloss Alignment :	         
2024-02-05 22:19:57,526 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:19:57,527 	Text Reference  :	this was the world test    championship let    me tell you more about it         
2024-02-05 22:19:57,527 	Text Hypothesis :	**** *** *** ***** gujarat titans       racked up and  it  was  not   comfortable
2024-02-05 22:19:57,527 	Text Alignment  :	D    D   D   D     S       S            S      S  S    S   S    S     S          
2024-02-05 22:19:57,527 ========================================================================================================================
2024-02-05 22:20:02,243 Epoch 1165: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.65 
2024-02-05 22:20:02,243 EPOCH 1166
2024-02-05 22:20:05,934 [Epoch: 1166 Step: 00078100] Batch Recognition Loss:   0.000325 => Gls Tokens per Sec:     1924 || Batch Translation Loss:   0.021438 => Txt Tokens per Sec:     5410 || Lr: 0.000025
2024-02-05 22:20:07,602 Epoch 1166: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.24 
2024-02-05 22:20:07,603 EPOCH 1167
2024-02-05 22:20:13,083 Epoch 1167: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.25 
2024-02-05 22:20:13,084 EPOCH 1168
2024-02-05 22:20:13,859 [Epoch: 1168 Step: 00078200] Batch Recognition Loss:   0.000155 => Gls Tokens per Sec:     2271 || Batch Translation Loss:   0.010421 => Txt Tokens per Sec:     5952 || Lr: 0.000025
2024-02-05 22:20:18,691 Epoch 1168: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.08 
2024-02-05 22:20:18,692 EPOCH 1169
2024-02-05 22:20:22,093 [Epoch: 1169 Step: 00078300] Batch Recognition Loss:   0.000137 => Gls Tokens per Sec:     2041 || Batch Translation Loss:   0.010472 => Txt Tokens per Sec:     5648 || Lr: 0.000025
2024-02-05 22:20:23,998 Epoch 1169: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.99 
2024-02-05 22:20:23,999 EPOCH 1170
2024-02-05 22:20:29,206 Epoch 1170: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.98 
2024-02-05 22:20:29,206 EPOCH 1171
2024-02-05 22:20:30,101 [Epoch: 1171 Step: 00078400] Batch Recognition Loss:   0.000146 => Gls Tokens per Sec:     1791 || Batch Translation Loss:   0.065392 => Txt Tokens per Sec:     5058 || Lr: 0.000025
2024-02-05 22:20:34,796 Epoch 1171: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.00 
2024-02-05 22:20:34,796 EPOCH 1172
2024-02-05 22:20:38,744 [Epoch: 1172 Step: 00078500] Batch Recognition Loss:   0.000145 => Gls Tokens per Sec:     1718 || Batch Translation Loss:   0.014348 => Txt Tokens per Sec:     4856 || Lr: 0.000025
2024-02-05 22:20:40,544 Epoch 1172: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.96 
2024-02-05 22:20:40,545 EPOCH 1173
2024-02-05 22:20:45,967 Epoch 1173: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.02 
2024-02-05 22:20:45,968 EPOCH 1174
2024-02-05 22:20:46,615 [Epoch: 1174 Step: 00078600] Batch Recognition Loss:   0.000255 => Gls Tokens per Sec:     2234 || Batch Translation Loss:   0.008472 => Txt Tokens per Sec:     5547 || Lr: 0.000025
2024-02-05 22:20:51,280 Epoch 1174: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.90 
2024-02-05 22:20:51,281 EPOCH 1175
2024-02-05 22:20:54,455 [Epoch: 1175 Step: 00078700] Batch Recognition Loss:   0.000705 => Gls Tokens per Sec:     2117 || Batch Translation Loss:   0.015493 => Txt Tokens per Sec:     5791 || Lr: 0.000025
2024-02-05 22:20:56,527 Epoch 1175: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.86 
2024-02-05 22:20:56,527 EPOCH 1176
2024-02-05 22:21:01,918 Epoch 1176: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.84 
2024-02-05 22:21:01,918 EPOCH 1177
2024-02-05 22:21:02,468 [Epoch: 1177 Step: 00078800] Batch Recognition Loss:   0.000155 => Gls Tokens per Sec:     2330 || Batch Translation Loss:   0.010619 => Txt Tokens per Sec:     6569 || Lr: 0.000025
2024-02-05 22:21:07,015 Epoch 1177: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.86 
2024-02-05 22:21:07,016 EPOCH 1178
2024-02-05 22:21:10,484 [Epoch: 1178 Step: 00078900] Batch Recognition Loss:   0.000109 => Gls Tokens per Sec:     1893 || Batch Translation Loss:   0.008528 => Txt Tokens per Sec:     5191 || Lr: 0.000025
2024-02-05 22:21:12,434 Epoch 1178: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.98 
2024-02-05 22:21:12,434 EPOCH 1179
2024-02-05 22:21:17,352 Epoch 1179: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.94 
2024-02-05 22:21:17,352 EPOCH 1180
2024-02-05 22:21:17,889 [Epoch: 1180 Step: 00079000] Batch Recognition Loss:   0.000171 => Gls Tokens per Sec:     2096 || Batch Translation Loss:   0.007812 => Txt Tokens per Sec:     5839 || Lr: 0.000025
2024-02-05 22:21:22,597 Epoch 1180: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.88 
2024-02-05 22:21:22,597 EPOCH 1181
2024-02-05 22:21:25,475 [Epoch: 1181 Step: 00079100] Batch Recognition Loss:   0.000178 => Gls Tokens per Sec:     2225 || Batch Translation Loss:   0.010505 => Txt Tokens per Sec:     6281 || Lr: 0.000025
2024-02-05 22:21:27,706 Epoch 1181: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.91 
2024-02-05 22:21:27,707 EPOCH 1182
2024-02-05 22:21:32,905 Epoch 1182: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.97 
2024-02-05 22:21:32,905 EPOCH 1183
2024-02-05 22:21:33,423 [Epoch: 1183 Step: 00079200] Batch Recognition Loss:   0.000183 => Gls Tokens per Sec:     1860 || Batch Translation Loss:   0.258067 => Txt Tokens per Sec:     5184 || Lr: 0.000025
2024-02-05 22:21:38,095 Epoch 1183: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.39 
2024-02-05 22:21:38,096 EPOCH 1184
2024-02-05 22:21:40,887 [Epoch: 1184 Step: 00079300] Batch Recognition Loss:   0.000120 => Gls Tokens per Sec:     2200 || Batch Translation Loss:   0.019351 => Txt Tokens per Sec:     6025 || Lr: 0.000025
2024-02-05 22:21:43,101 Epoch 1184: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.89 
2024-02-05 22:21:43,101 EPOCH 1185
2024-02-05 22:21:48,422 Epoch 1185: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.22 
2024-02-05 22:21:48,423 EPOCH 1186
2024-02-05 22:21:48,784 [Epoch: 1186 Step: 00079400] Batch Recognition Loss:   0.000127 => Gls Tokens per Sec:     2220 || Batch Translation Loss:   0.012844 => Txt Tokens per Sec:     6480 || Lr: 0.000025
2024-02-05 22:21:53,374 Epoch 1186: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.11 
2024-02-05 22:21:53,374 EPOCH 1187
2024-02-05 22:21:56,276 [Epoch: 1187 Step: 00079500] Batch Recognition Loss:   0.000156 => Gls Tokens per Sec:     2061 || Batch Translation Loss:   0.014246 => Txt Tokens per Sec:     5509 || Lr: 0.000025
2024-02-05 22:21:58,870 Epoch 1187: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.29 
2024-02-05 22:21:58,871 EPOCH 1188
2024-02-05 22:22:03,947 Epoch 1188: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.59 
2024-02-05 22:22:03,948 EPOCH 1189
2024-02-05 22:22:04,208 [Epoch: 1189 Step: 00079600] Batch Recognition Loss:   0.000123 => Gls Tokens per Sec:     2471 || Batch Translation Loss:   0.013699 => Txt Tokens per Sec:     7116 || Lr: 0.000025
2024-02-05 22:22:09,442 Epoch 1189: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.83 
2024-02-05 22:22:09,443 EPOCH 1190
2024-02-05 22:22:12,345 [Epoch: 1190 Step: 00079700] Batch Recognition Loss:   0.000154 => Gls Tokens per Sec:     2007 || Batch Translation Loss:   0.010614 => Txt Tokens per Sec:     5610 || Lr: 0.000025
2024-02-05 22:22:14,550 Epoch 1190: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.50 
2024-02-05 22:22:14,551 EPOCH 1191
2024-02-05 22:22:19,932 Epoch 1191: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.14 
2024-02-05 22:22:19,932 EPOCH 1192
2024-02-05 22:22:20,173 [Epoch: 1192 Step: 00079800] Batch Recognition Loss:   0.000233 => Gls Tokens per Sec:     2005 || Batch Translation Loss:   0.016081 => Txt Tokens per Sec:     6177 || Lr: 0.000025
2024-02-05 22:22:25,426 Epoch 1192: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.14 
2024-02-05 22:22:25,426 EPOCH 1193
2024-02-05 22:22:28,180 [Epoch: 1193 Step: 00079900] Batch Recognition Loss:   0.000252 => Gls Tokens per Sec:     2092 || Batch Translation Loss:   0.011649 => Txt Tokens per Sec:     5743 || Lr: 0.000025
2024-02-05 22:22:30,527 Epoch 1193: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.11 
2024-02-05 22:22:30,528 EPOCH 1194
2024-02-05 22:22:35,074 Epoch 1194: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.34 
2024-02-05 22:22:35,074 EPOCH 1195
2024-02-05 22:22:35,203 [Epoch: 1195 Step: 00080000] Batch Recognition Loss:   0.000182 => Gls Tokens per Sec:     2500 || Batch Translation Loss:   0.011163 => Txt Tokens per Sec:     6242 || Lr: 0.000025
2024-02-05 22:22:44,077 Validation result at epoch 1195, step    80000: duration: 8.8733s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.42593	Translation Loss: 93270.70312	PPL: 11113.22656
	Eval Metric: BLEU
	WER 3.18	(DEL: 0.00,	INS: 0.00,	SUB: 3.18)
	BLEU-4 0.69	(BLEU-1: 10.36,	BLEU-2: 3.30,	BLEU-3: 1.38,	BLEU-4: 0.69)
	CHRF 16.54	ROUGE 8.96
2024-02-05 22:22:44,078 Logging Recognition and Translation Outputs
2024-02-05 22:22:44,078 ========================================================================================================================
2024-02-05 22:22:44,078 Logging Sequence: 118_111.00
2024-02-05 22:22:44,078 	Gloss Reference :	A B+C+D+E
2024-02-05 22:22:44,078 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:22:44,078 	Gloss Alignment :	         
2024-02-05 22:22:44,079 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:22:44,079 	Text Reference  :	and  people encourage him have  hope for  the   next world cup      
2024-02-05 22:22:44,080 	Text Hypothesis :	this is     not       the first time that kohli will be    jubiliant
2024-02-05 22:22:44,080 	Text Alignment  :	S    S      S         S   S     S    S    S     S    S     S        
2024-02-05 22:22:44,080 ========================================================================================================================
2024-02-05 22:22:44,080 Logging Sequence: 95_16.00
2024-02-05 22:22:44,080 	Gloss Reference :	A B+C+D+E
2024-02-05 22:22:44,080 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:22:44,080 	Gloss Alignment :	         
2024-02-05 22:22:44,081 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:22:44,082 	Text Reference  :	guwahati hosts a very   few   international matches and     that   is  why   thousands of people had    thronged the stadium
2024-02-05 22:22:44,082 	Text Hypothesis :	******** ***** a maruti wagon r             car     circled around the pitch during    a  ranji  trophy match    in  delhi  
2024-02-05 22:22:44,082 	Text Alignment  :	D        D       S      S     S             S       S       S      S   S     S         S  S      S      S        S   S      
2024-02-05 22:22:44,083 ========================================================================================================================
2024-02-05 22:22:44,083 Logging Sequence: 114_2.00
2024-02-05 22:22:44,083 	Gloss Reference :	A B+C+D+E
2024-02-05 22:22:44,083 	Gloss Hypothesis:	A B+C+D  
2024-02-05 22:22:44,083 	Gloss Alignment :	  S      
2024-02-05 22:22:44,083 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:22:44,085 	Text Reference  :	******* the         euro    2020         was held  in the year 2021 as it was postponed due     to  the covid pandemic
2024-02-05 22:22:44,085 	Text Hypothesis :	england all-rounder natalie sciver-brunt was roped in *** **** **** ** ** by  indian    captain for rs  150   crore   
2024-02-05 22:22:44,085 	Text Alignment  :	I       S           S       S                S        D   D    D    D  D  S   S         S       S   S   S     S       
2024-02-05 22:22:44,085 ========================================================================================================================
2024-02-05 22:22:44,085 Logging Sequence: 179_126.00
2024-02-05 22:22:44,086 	Gloss Reference :	A B+C+D+E
2024-02-05 22:22:44,086 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:22:44,086 	Gloss Alignment :	         
2024-02-05 22:22:44,086 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:22:44,088 	Text Reference  :	vinesh argued that       she might contract coronavirus since these wrestlers travelled from     india where     there are many    infections
2024-02-05 22:22:44,088 	Text Hypothesis :	****** the    federation or  sai   did      not         want  to    pick      their     passport was   scheduled on    2nd october 2023      
2024-02-05 22:22:44,088 	Text Alignment  :	D      S      S          S   S     S        S           S     S     S         S         S        S     S         S     S   S       S         
2024-02-05 22:22:44,088 ========================================================================================================================
2024-02-05 22:22:44,088 Logging Sequence: 94_2.00
2024-02-05 22:22:44,088 	Gloss Reference :	A B+C+D+E
2024-02-05 22:22:44,089 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:22:44,089 	Gloss Alignment :	         
2024-02-05 22:22:44,089 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:22:44,090 	Text Reference  :	the icc odi men' world cup 2023 will be hosted by    india on  5th      october 2023 
2024-02-05 22:22:44,090 	Text Hypothesis :	*** *** *** **** india is  now  face of the    world deaf  and pakistan heart   emoji
2024-02-05 22:22:44,090 	Text Alignment  :	D   D   D   D    S     S   S    S    S  S      S     S     S   S        S       S    
2024-02-05 22:22:44,090 ========================================================================================================================
2024-02-05 22:22:49,347 Epoch 1195: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.77 
2024-02-05 22:22:49,347 EPOCH 1196
2024-02-05 22:22:52,325 [Epoch: 1196 Step: 00080100] Batch Recognition Loss:   0.000213 => Gls Tokens per Sec:     1848 || Batch Translation Loss:   0.018023 => Txt Tokens per Sec:     5161 || Lr: 0.000025
2024-02-05 22:22:54,756 Epoch 1196: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.33 
2024-02-05 22:22:54,757 EPOCH 1197
2024-02-05 22:22:59,565 Epoch 1197: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.31 
2024-02-05 22:22:59,565 EPOCH 1198
2024-02-05 22:22:59,633 [Epoch: 1198 Step: 00080200] Batch Recognition Loss:   0.000128 => Gls Tokens per Sec:     2370 || Batch Translation Loss:   0.045400 => Txt Tokens per Sec:     7745 || Lr: 0.000025
2024-02-05 22:23:05,051 Epoch 1198: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.38 
2024-02-05 22:23:05,051 EPOCH 1199
2024-02-05 22:23:07,412 [Epoch: 1199 Step: 00080300] Batch Recognition Loss:   0.000155 => Gls Tokens per Sec:     2305 || Batch Translation Loss:   0.010580 => Txt Tokens per Sec:     6288 || Lr: 0.000025
2024-02-05 22:23:09,788 Epoch 1199: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.64 
2024-02-05 22:23:09,788 EPOCH 1200
2024-02-05 22:23:15,310 [Epoch: 1200 Step: 00080400] Batch Recognition Loss:   0.000123 => Gls Tokens per Sec:     1924 || Batch Translation Loss:   0.012542 => Txt Tokens per Sec:     5323 || Lr: 0.000025
2024-02-05 22:23:15,310 Epoch 1200: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.33 
2024-02-05 22:23:15,310 EPOCH 1201
2024-02-05 22:23:20,094 Epoch 1201: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.52 
2024-02-05 22:23:20,095 EPOCH 1202
2024-02-05 22:23:22,709 [Epoch: 1202 Step: 00080500] Batch Recognition Loss:   0.000161 => Gls Tokens per Sec:     1982 || Batch Translation Loss:   0.014173 => Txt Tokens per Sec:     5331 || Lr: 0.000025
2024-02-05 22:23:25,717 Epoch 1202: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.37 
2024-02-05 22:23:25,717 EPOCH 1203
2024-02-05 22:23:30,286 [Epoch: 1203 Step: 00080600] Batch Recognition Loss:   0.000134 => Gls Tokens per Sec:     2289 || Batch Translation Loss:   0.009032 => Txt Tokens per Sec:     6311 || Lr: 0.000025
2024-02-05 22:23:30,430 Epoch 1203: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.25 
2024-02-05 22:23:30,430 EPOCH 1204
2024-02-05 22:23:35,922 Epoch 1204: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.22 
2024-02-05 22:23:35,923 EPOCH 1205
2024-02-05 22:23:38,107 [Epoch: 1205 Step: 00080700] Batch Recognition Loss:   0.000332 => Gls Tokens per Sec:     2300 || Batch Translation Loss:   0.026040 => Txt Tokens per Sec:     6238 || Lr: 0.000025
2024-02-05 22:23:40,781 Epoch 1205: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.08 
2024-02-05 22:23:40,781 EPOCH 1206
2024-02-05 22:23:46,093 [Epoch: 1206 Step: 00080800] Batch Recognition Loss:   0.000126 => Gls Tokens per Sec:     1939 || Batch Translation Loss:   0.013477 => Txt Tokens per Sec:     5360 || Lr: 0.000025
2024-02-05 22:23:46,261 Epoch 1206: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.02 
2024-02-05 22:23:46,262 EPOCH 1207
2024-02-05 22:23:51,747 Epoch 1207: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.03 
2024-02-05 22:23:51,748 EPOCH 1208
2024-02-05 22:23:54,119 [Epoch: 1208 Step: 00080900] Batch Recognition Loss:   0.000126 => Gls Tokens per Sec:     2093 || Batch Translation Loss:   0.042983 => Txt Tokens per Sec:     5719 || Lr: 0.000025
2024-02-05 22:23:56,805 Epoch 1208: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.99 
2024-02-05 22:23:56,806 EPOCH 1209
2024-02-05 22:24:01,542 [Epoch: 1209 Step: 00081000] Batch Recognition Loss:   0.000127 => Gls Tokens per Sec:     2141 || Batch Translation Loss:   0.014014 => Txt Tokens per Sec:     5903 || Lr: 0.000025
2024-02-05 22:24:01,819 Epoch 1209: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.72 
2024-02-05 22:24:01,819 EPOCH 1210
2024-02-05 22:24:07,151 Epoch 1210: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.05 
2024-02-05 22:24:07,151 EPOCH 1211
2024-02-05 22:24:09,201 [Epoch: 1211 Step: 00081100] Batch Recognition Loss:   0.000218 => Gls Tokens per Sec:     2294 || Batch Translation Loss:   0.006502 => Txt Tokens per Sec:     6085 || Lr: 0.000025
2024-02-05 22:24:12,368 Epoch 1211: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.00 
2024-02-05 22:24:12,369 EPOCH 1212
2024-02-05 22:24:17,005 [Epoch: 1212 Step: 00081200] Batch Recognition Loss:   0.000331 => Gls Tokens per Sec:     2154 || Batch Translation Loss:   0.007734 => Txt Tokens per Sec:     5915 || Lr: 0.000025
2024-02-05 22:24:17,442 Epoch 1212: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.13 
2024-02-05 22:24:17,443 EPOCH 1213
2024-02-05 22:24:22,653 Epoch 1213: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.06 
2024-02-05 22:24:22,654 EPOCH 1214
2024-02-05 22:24:24,802 [Epoch: 1214 Step: 00081300] Batch Recognition Loss:   0.000209 => Gls Tokens per Sec:     2161 || Batch Translation Loss:   0.016496 => Txt Tokens per Sec:     6147 || Lr: 0.000025
2024-02-05 22:24:27,550 Epoch 1214: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.94 
2024-02-05 22:24:27,550 EPOCH 1215
2024-02-05 22:24:32,351 [Epoch: 1215 Step: 00081400] Batch Recognition Loss:   0.000169 => Gls Tokens per Sec:     2046 || Batch Translation Loss:   0.042937 => Txt Tokens per Sec:     5652 || Lr: 0.000025
2024-02-05 22:24:32,800 Epoch 1215: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.42 
2024-02-05 22:24:32,800 EPOCH 1216
2024-02-05 22:24:37,743 Epoch 1216: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.30 
2024-02-05 22:24:37,743 EPOCH 1217
2024-02-05 22:24:39,813 [Epoch: 1217 Step: 00081500] Batch Recognition Loss:   0.000205 => Gls Tokens per Sec:     2165 || Batch Translation Loss:   0.029813 => Txt Tokens per Sec:     5844 || Lr: 0.000025
2024-02-05 22:24:43,355 Epoch 1217: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.13 
2024-02-05 22:24:43,355 EPOCH 1218
2024-02-05 22:24:48,216 [Epoch: 1218 Step: 00081600] Batch Recognition Loss:   0.000306 => Gls Tokens per Sec:     1988 || Batch Translation Loss:   0.021429 => Txt Tokens per Sec:     5554 || Lr: 0.000025
2024-02-05 22:24:48,578 Epoch 1218: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.24 
2024-02-05 22:24:48,579 EPOCH 1219
2024-02-05 22:24:53,664 Epoch 1219: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.44 
2024-02-05 22:24:53,665 EPOCH 1220
2024-02-05 22:24:55,687 [Epoch: 1220 Step: 00081700] Batch Recognition Loss:   0.000234 => Gls Tokens per Sec:     2137 || Batch Translation Loss:   0.020635 => Txt Tokens per Sec:     5995 || Lr: 0.000025
2024-02-05 22:24:58,937 Epoch 1220: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.17 
2024-02-05 22:24:58,937 EPOCH 1221
2024-02-05 22:25:03,712 [Epoch: 1221 Step: 00081800] Batch Recognition Loss:   0.000206 => Gls Tokens per Sec:     1991 || Batch Translation Loss:   0.024252 => Txt Tokens per Sec:     5503 || Lr: 0.000025
2024-02-05 22:25:04,244 Epoch 1221: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.32 
2024-02-05 22:25:04,245 EPOCH 1222
2024-02-05 22:25:09,362 Epoch 1222: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.45 
2024-02-05 22:25:09,362 EPOCH 1223
2024-02-05 22:25:11,346 [Epoch: 1223 Step: 00081900] Batch Recognition Loss:   0.000259 => Gls Tokens per Sec:     2098 || Batch Translation Loss:   0.022627 => Txt Tokens per Sec:     6041 || Lr: 0.000025
2024-02-05 22:25:14,697 Epoch 1223: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.02 
2024-02-05 22:25:14,697 EPOCH 1224
2024-02-05 22:25:18,983 [Epoch: 1224 Step: 00082000] Batch Recognition Loss:   0.000168 => Gls Tokens per Sec:     2181 || Batch Translation Loss:   0.017958 => Txt Tokens per Sec:     6061 || Lr: 0.000025
2024-02-05 22:25:27,706 Validation result at epoch 1224, step    82000: duration: 8.7227s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.27538	Translation Loss: 93018.75781	PPL: 10837.05566
	Eval Metric: BLEU
	WER 3.11	(DEL: 0.00,	INS: 0.00,	SUB: 3.11)
	BLEU-4 0.60	(BLEU-1: 10.28,	BLEU-2: 3.17,	BLEU-3: 1.22,	BLEU-4: 0.60)
	CHRF 16.69	ROUGE 8.71
2024-02-05 22:25:27,707 Logging Recognition and Translation Outputs
2024-02-05 22:25:27,707 ========================================================================================================================
2024-02-05 22:25:27,707 Logging Sequence: 96_203.00
2024-02-05 22:25:27,707 	Gloss Reference :	A B+C+D+E
2024-02-05 22:25:27,707 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:25:27,707 	Gloss Alignment :	         
2024-02-05 22:25:27,708 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:25:27,709 	Text Reference  :	hardik pandya who scored 33 runs in just 17  balls  was     given the   man   of   the match
2024-02-05 22:25:27,709 	Text Hypothesis :	hardik ****** *** ****** ** **** ** **** and natasa decided to    renew their vows in  2019 
2024-02-05 22:25:27,709 	Text Alignment  :	       D      D   D      D  D    D  D    S   S      S       S     S     S     S    S   S    
2024-02-05 22:25:27,709 ========================================================================================================================
2024-02-05 22:25:27,709 Logging Sequence: 115_44.00
2024-02-05 22:25:27,709 	Gloss Reference :	A B+C+D+E
2024-02-05 22:25:27,710 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:25:27,710 	Gloss Alignment :	         
2024-02-05 22:25:27,710 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:25:27,711 	Text Reference  :	** the *** ***** image was of   his marriage to          sanjana   ganesan
2024-02-05 22:25:27,711 	Text Hypothesis :	on the 4th match he    was held in  a        traditional gurudwara wedding
2024-02-05 22:25:27,711 	Text Alignment  :	I      I   I     S         S    S   S        S           S         S      
2024-02-05 22:25:27,711 ========================================================================================================================
2024-02-05 22:25:27,711 Logging Sequence: 83_57.00
2024-02-05 22:25:27,711 	Gloss Reference :	A B+C+D+E
2024-02-05 22:25:27,711 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:25:27,711 	Gloss Alignment :	         
2024-02-05 22:25:27,712 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:25:27,712 	Text Reference  :	collapsed face first on  the field  he was     completely unconscious
2024-02-05 22:25:27,712 	Text Hypothesis :	********* **** a     lot of  people is talking about      cricket    
2024-02-05 22:25:27,712 	Text Alignment  :	D         D    S     S   S   S      S  S       S          S          
2024-02-05 22:25:27,712 ========================================================================================================================
2024-02-05 22:25:27,713 Logging Sequence: 65_48.00
2024-02-05 22:25:27,713 	Gloss Reference :	A B+C+D+E
2024-02-05 22:25:27,713 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:25:27,713 	Gloss Alignment :	         
2024-02-05 22:25:27,713 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:25:27,715 	Text Reference  :	after which instead of returning to         india and  then  going to    west indies the team directly flew to west indies    
2024-02-05 22:25:27,715 	Text Hypothesis :	***** the   final   of the       tournament was   held every 4     years but  they   did not  take     part in the  tournament
2024-02-05 22:25:27,716 	Text Alignment  :	D     S     S          S         S          S     S    S     S     S     S    S      S   S    S        S    S  S    S         
2024-02-05 22:25:27,716 ========================================================================================================================
2024-02-05 22:25:27,716 Logging Sequence: 120_41.00
2024-02-05 22:25:27,716 	Gloss Reference :	A B+C+D+E
2024-02-05 22:25:27,716 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:25:27,716 	Gloss Alignment :	         
2024-02-05 22:25:27,716 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:25:27,718 	Text Reference  :	got infected with covid  and was   admitted to        the hospital on  26   may 2       days after her husband
2024-02-05 22:25:27,718 	Text Hypothesis :	*** ******** he   played 18  tests on       instagram and save     its upto a   whereas they will  be  played 
2024-02-05 22:25:27,718 	Text Alignment  :	D   D        S    S      S   S     S        S         S   S        S   S    S   S       S    S     S   S      
2024-02-05 22:25:27,718 ========================================================================================================================
2024-02-05 22:25:28,344 Epoch 1224: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.29 
2024-02-05 22:25:28,344 EPOCH 1225
2024-02-05 22:25:33,948 Epoch 1225: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.17 
2024-02-05 22:25:33,949 EPOCH 1226
2024-02-05 22:25:35,775 [Epoch: 1226 Step: 00082100] Batch Recognition Loss:   0.000345 => Gls Tokens per Sec:     2191 || Batch Translation Loss:   0.098704 => Txt Tokens per Sec:     5837 || Lr: 0.000025
2024-02-05 22:25:38,820 Epoch 1226: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.59 
2024-02-05 22:25:38,820 EPOCH 1227
2024-02-05 22:25:43,544 [Epoch: 1227 Step: 00082200] Batch Recognition Loss:   0.000294 => Gls Tokens per Sec:     1944 || Batch Translation Loss:   0.027236 => Txt Tokens per Sec:     5393 || Lr: 0.000025
2024-02-05 22:25:44,276 Epoch 1227: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.69 
2024-02-05 22:25:44,276 EPOCH 1228
2024-02-05 22:25:49,057 Epoch 1228: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.37 
2024-02-05 22:25:49,057 EPOCH 1229
2024-02-05 22:25:51,067 [Epoch: 1229 Step: 00082300] Batch Recognition Loss:   0.000106 => Gls Tokens per Sec:     1862 || Batch Translation Loss:   0.019638 => Txt Tokens per Sec:     5042 || Lr: 0.000025
2024-02-05 22:25:54,616 Epoch 1229: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.62 
2024-02-05 22:25:54,617 EPOCH 1230
2024-02-05 22:25:58,821 [Epoch: 1230 Step: 00082400] Batch Recognition Loss:   0.000165 => Gls Tokens per Sec:     2146 || Batch Translation Loss:   0.028181 => Txt Tokens per Sec:     5978 || Lr: 0.000025
2024-02-05 22:25:59,528 Epoch 1230: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.52 
2024-02-05 22:25:59,529 EPOCH 1231
2024-02-05 22:26:04,950 Epoch 1231: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.50 
2024-02-05 22:26:04,951 EPOCH 1232
2024-02-05 22:26:06,452 [Epoch: 1232 Step: 00082500] Batch Recognition Loss:   0.000306 => Gls Tokens per Sec:     2453 || Batch Translation Loss:   0.050226 => Txt Tokens per Sec:     6823 || Lr: 0.000025
2024-02-05 22:26:09,796 Epoch 1232: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.25 
2024-02-05 22:26:09,796 EPOCH 1233
2024-02-05 22:26:14,272 [Epoch: 1233 Step: 00082600] Batch Recognition Loss:   0.000147 => Gls Tokens per Sec:     1980 || Batch Translation Loss:   0.017598 => Txt Tokens per Sec:     5487 || Lr: 0.000025
2024-02-05 22:26:15,192 Epoch 1233: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.20 
2024-02-05 22:26:15,192 EPOCH 1234
2024-02-05 22:26:20,685 Epoch 1234: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.26 
2024-02-05 22:26:20,685 EPOCH 1235
2024-02-05 22:26:22,236 [Epoch: 1235 Step: 00082700] Batch Recognition Loss:   0.000149 => Gls Tokens per Sec:     2271 || Batch Translation Loss:   0.015320 => Txt Tokens per Sec:     6063 || Lr: 0.000025
2024-02-05 22:26:25,923 Epoch 1235: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.02 
2024-02-05 22:26:25,923 EPOCH 1236
2024-02-05 22:26:30,423 [Epoch: 1236 Step: 00082800] Batch Recognition Loss:   0.000127 => Gls Tokens per Sec:     1956 || Batch Translation Loss:   0.016665 => Txt Tokens per Sec:     5458 || Lr: 0.000025
2024-02-05 22:26:31,323 Epoch 1236: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.14 
2024-02-05 22:26:31,323 EPOCH 1237
2024-02-05 22:26:36,351 Epoch 1237: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.98 
2024-02-05 22:26:36,352 EPOCH 1238
2024-02-05 22:26:37,910 [Epoch: 1238 Step: 00082900] Batch Recognition Loss:   0.000190 => Gls Tokens per Sec:     2092 || Batch Translation Loss:   0.015957 => Txt Tokens per Sec:     5908 || Lr: 0.000025
2024-02-05 22:26:41,509 Epoch 1238: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.06 
2024-02-05 22:26:41,510 EPOCH 1239
2024-02-05 22:26:45,834 [Epoch: 1239 Step: 00083000] Batch Recognition Loss:   0.000149 => Gls Tokens per Sec:     1976 || Batch Translation Loss:   0.017619 => Txt Tokens per Sec:     5499 || Lr: 0.000025
2024-02-05 22:26:46,792 Epoch 1239: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.58 
2024-02-05 22:26:46,792 EPOCH 1240
2024-02-05 22:26:52,191 Epoch 1240: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.10 
2024-02-05 22:26:52,192 EPOCH 1241
2024-02-05 22:26:53,634 [Epoch: 1241 Step: 00083100] Batch Recognition Loss:   0.001220 => Gls Tokens per Sec:     2222 || Batch Translation Loss:   0.044286 => Txt Tokens per Sec:     6244 || Lr: 0.000025
2024-02-05 22:26:57,288 Epoch 1241: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.22 
2024-02-05 22:26:57,289 EPOCH 1242
2024-02-05 22:27:01,889 [Epoch: 1242 Step: 00083200] Batch Recognition Loss:   0.000133 => Gls Tokens per Sec:     1822 || Batch Translation Loss:   0.012426 => Txt Tokens per Sec:     5060 || Lr: 0.000025
2024-02-05 22:27:02,878 Epoch 1242: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.28 
2024-02-05 22:27:02,878 EPOCH 1243
2024-02-05 22:27:08,170 Epoch 1243: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.31 
2024-02-05 22:27:08,170 EPOCH 1244
2024-02-05 22:27:09,690 [Epoch: 1244 Step: 00083300] Batch Recognition Loss:   0.000150 => Gls Tokens per Sec:     2001 || Batch Translation Loss:   0.019745 => Txt Tokens per Sec:     5404 || Lr: 0.000025
2024-02-05 22:27:13,374 Epoch 1244: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.47 
2024-02-05 22:27:13,374 EPOCH 1245
2024-02-05 22:27:16,980 [Epoch: 1245 Step: 00083400] Batch Recognition Loss:   0.000199 => Gls Tokens per Sec:     2280 || Batch Translation Loss:   0.022083 => Txt Tokens per Sec:     6311 || Lr: 0.000025
2024-02-05 22:27:18,361 Epoch 1245: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.86 
2024-02-05 22:27:18,361 EPOCH 1246
2024-02-05 22:27:23,954 Epoch 1246: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.39 
2024-02-05 22:27:23,955 EPOCH 1247
2024-02-05 22:27:25,537 [Epoch: 1247 Step: 00083500] Batch Recognition Loss:   0.000471 => Gls Tokens per Sec:     1758 || Batch Translation Loss:   0.008450 => Txt Tokens per Sec:     5085 || Lr: 0.000025
2024-02-05 22:27:28,898 Epoch 1247: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.24 
2024-02-05 22:27:28,898 EPOCH 1248
2024-02-05 22:27:33,158 [Epoch: 1248 Step: 00083600] Batch Recognition Loss:   0.000169 => Gls Tokens per Sec:     1892 || Batch Translation Loss:   0.009364 => Txt Tokens per Sec:     5253 || Lr: 0.000025
2024-02-05 22:27:34,291 Epoch 1248: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.14 
2024-02-05 22:27:34,291 EPOCH 1249
2024-02-05 22:27:39,383 Epoch 1249: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.91 
2024-02-05 22:27:39,383 EPOCH 1250
2024-02-05 22:27:40,794 [Epoch: 1250 Step: 00083700] Batch Recognition Loss:   0.000229 => Gls Tokens per Sec:     1932 || Batch Translation Loss:   0.019816 => Txt Tokens per Sec:     5349 || Lr: 0.000025
2024-02-05 22:27:44,887 Epoch 1250: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.38 
2024-02-05 22:27:44,887 EPOCH 1251
2024-02-05 22:27:48,317 [Epoch: 1251 Step: 00083800] Batch Recognition Loss:   0.000168 => Gls Tokens per Sec:     2304 || Batch Translation Loss:   0.013069 => Txt Tokens per Sec:     6466 || Lr: 0.000025
2024-02-05 22:27:49,789 Epoch 1251: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.64 
2024-02-05 22:27:49,790 EPOCH 1252
2024-02-05 22:27:55,259 Epoch 1252: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.72 
2024-02-05 22:27:55,260 EPOCH 1253
2024-02-05 22:27:56,250 [Epoch: 1253 Step: 00083900] Batch Recognition Loss:   0.000207 => Gls Tokens per Sec:     2584 || Batch Translation Loss:   0.020269 => Txt Tokens per Sec:     6936 || Lr: 0.000025
2024-02-05 22:28:00,217 Epoch 1253: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.31 
2024-02-05 22:28:00,218 EPOCH 1254
2024-02-05 22:28:04,021 [Epoch: 1254 Step: 00084000] Batch Recognition Loss:   0.000114 => Gls Tokens per Sec:     2036 || Batch Translation Loss:   0.013286 => Txt Tokens per Sec:     5534 || Lr: 0.000025
2024-02-05 22:28:12,586 Validation result at epoch 1254, step    84000: duration: 8.5635s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.47259	Translation Loss: 93332.81250	PPL: 11182.37305
	Eval Metric: BLEU
	WER 3.18	(DEL: 0.00,	INS: 0.00,	SUB: 3.18)
	BLEU-4 0.69	(BLEU-1: 10.31,	BLEU-2: 3.50,	BLEU-3: 1.39,	BLEU-4: 0.69)
	CHRF 16.66	ROUGE 8.92
2024-02-05 22:28:12,587 Logging Recognition and Translation Outputs
2024-02-05 22:28:12,587 ========================================================================================================================
2024-02-05 22:28:12,587 Logging Sequence: 113_196.00
2024-02-05 22:28:12,588 	Gloss Reference :	A B+C+D+E
2024-02-05 22:28:12,588 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:28:12,588 	Gloss Alignment :	         
2024-02-05 22:28:12,588 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:28:12,589 	Text Reference  :	infidelity being the       reason  for separation is just a       rumour sad   
2024-02-05 22:28:12,589 	Text Hypothesis :	everyone   was   excitedly waiting for ********** ** the  auction in     mumbai
2024-02-05 22:28:12,589 	Text Alignment  :	S          S     S         S           D          D  S    S       S      S     
2024-02-05 22:28:12,589 ========================================================================================================================
2024-02-05 22:28:12,589 Logging Sequence: 138_48.00
2024-02-05 22:28:12,589 	Gloss Reference :	A B+C+D+E
2024-02-05 22:28:12,589 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:28:12,590 	Gloss Alignment :	         
2024-02-05 22:28:12,590 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:28:12,591 	Text Reference  :	******* *** ****** **** ** *** ***** similarly jadon sancho and        bukayo saka's shot    was blocked by the      goalkeeper
2024-02-05 22:28:12,591 	Text Hypothesis :	however the second half of the match went      viral a      five-match test   series against was ******* a  software engineer  
2024-02-05 22:28:12,591 	Text Alignment  :	I       I   I      I    I  I   I     S         S     S      S          S      S      S           D       S  S        S         
2024-02-05 22:28:12,591 ========================================================================================================================
2024-02-05 22:28:12,592 Logging Sequence: 180_114.00
2024-02-05 22:28:12,592 	Gloss Reference :	A B+C+D+E
2024-02-05 22:28:12,592 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:28:12,592 	Gloss Alignment :	         
2024-02-05 22:28:12,592 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:28:12,594 	Text Reference  :	the wrestlers then wrote a         complaint letter to      the indian olympic association ioa president pt    usha
2024-02-05 22:28:12,594 	Text Hypothesis :	he  said      it   was   wonderful by        your   victory and death  for     a           lot of        weeks ago 
2024-02-05 22:28:12,594 	Text Alignment  :	S   S         S    S     S         S         S      S       S   S      S       S           S   S         S     S   
2024-02-05 22:28:12,594 ========================================================================================================================
2024-02-05 22:28:12,594 Logging Sequence: 126_163.00
2024-02-05 22:28:12,594 	Gloss Reference :	A B+C+D+E
2024-02-05 22:28:12,595 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:28:12,595 	Gloss Alignment :	         
2024-02-05 22:28:12,595 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:28:12,596 	Text Reference  :	your hard work has       helped secure a         medal   at   the   tokyo olympics
2024-02-05 22:28:12,596 	Text Hypothesis :	**** **** **** harbhajan is     an     excellent spinner much loved by    all     
2024-02-05 22:28:12,596 	Text Alignment  :	D    D    D    S         S      S      S         S       S    S     S     S       
2024-02-05 22:28:12,596 ========================================================================================================================
2024-02-05 22:28:12,596 Logging Sequence: 169_165.00
2024-02-05 22:28:12,597 	Gloss Reference :	A B+C+D+E
2024-02-05 22:28:12,597 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:28:12,597 	Gloss Alignment :	         
2024-02-05 22:28:12,597 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:28:12,599 	Text Reference  :	* ***** ** ** the     indian government was    outraged by  the  incident   and ** *** these changes were undone    by  wikipedia
2024-02-05 22:28:12,599 	Text Hypothesis :	a total of 22 matches will   be         played at       par with spectators and is why they  did     not  revealing her face     
2024-02-05 22:28:12,599 	Text Alignment  :	I I     I  I  S       S      S          S      S        S   S    S              I  I   S     S       S    S         S   S        
2024-02-05 22:28:12,599 ========================================================================================================================
2024-02-05 22:28:14,110 Epoch 1254: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.32 
2024-02-05 22:28:14,110 EPOCH 1255
2024-02-05 22:28:19,638 Epoch 1255: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.08 
2024-02-05 22:28:19,639 EPOCH 1256
2024-02-05 22:28:20,777 [Epoch: 1256 Step: 00084100] Batch Recognition Loss:   0.000147 => Gls Tokens per Sec:     2110 || Batch Translation Loss:   0.010121 => Txt Tokens per Sec:     5488 || Lr: 0.000025
2024-02-05 22:28:24,938 Epoch 1256: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.03 
2024-02-05 22:28:24,938 EPOCH 1257
2024-02-05 22:28:28,432 [Epoch: 1257 Step: 00084200] Batch Recognition Loss:   0.000284 => Gls Tokens per Sec:     2170 || Batch Translation Loss:   0.023515 => Txt Tokens per Sec:     5973 || Lr: 0.000025
2024-02-05 22:28:30,007 Epoch 1257: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.08 
2024-02-05 22:28:30,007 EPOCH 1258
2024-02-05 22:28:35,178 Epoch 1258: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.25 
2024-02-05 22:28:35,179 EPOCH 1259
2024-02-05 22:28:36,262 [Epoch: 1259 Step: 00084300] Batch Recognition Loss:   0.000224 => Gls Tokens per Sec:     2068 || Batch Translation Loss:   0.016885 => Txt Tokens per Sec:     6037 || Lr: 0.000025
2024-02-05 22:28:40,458 Epoch 1259: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.42 
2024-02-05 22:28:40,458 EPOCH 1260
2024-02-05 22:28:44,238 [Epoch: 1260 Step: 00084400] Batch Recognition Loss:   0.009710 => Gls Tokens per Sec:     1963 || Batch Translation Loss:   0.026561 => Txt Tokens per Sec:     5484 || Lr: 0.000025
2024-02-05 22:28:45,694 Epoch 1260: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.55 
2024-02-05 22:28:45,694 EPOCH 1261
2024-02-05 22:28:50,898 Epoch 1261: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.16 
2024-02-05 22:28:50,899 EPOCH 1262
2024-02-05 22:28:51,813 [Epoch: 1262 Step: 00084500] Batch Recognition Loss:   0.000278 => Gls Tokens per Sec:     2278 || Batch Translation Loss:   0.019003 => Txt Tokens per Sec:     6669 || Lr: 0.000025
2024-02-05 22:28:55,953 Epoch 1262: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.24 
2024-02-05 22:28:55,953 EPOCH 1263
2024-02-05 22:28:59,728 [Epoch: 1263 Step: 00084600] Batch Recognition Loss:   0.000161 => Gls Tokens per Sec:     1924 || Batch Translation Loss:   0.017664 => Txt Tokens per Sec:     5454 || Lr: 0.000025
2024-02-05 22:29:01,290 Epoch 1263: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.64 
2024-02-05 22:29:01,291 EPOCH 1264
2024-02-05 22:29:06,567 Epoch 1264: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.61 
2024-02-05 22:29:06,568 EPOCH 1265
2024-02-05 22:29:07,460 [Epoch: 1265 Step: 00084700] Batch Recognition Loss:   0.000169 => Gls Tokens per Sec:     2155 || Batch Translation Loss:   0.009732 => Txt Tokens per Sec:     4932 || Lr: 0.000025
2024-02-05 22:29:12,011 Epoch 1265: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.57 
2024-02-05 22:29:12,012 EPOCH 1266
2024-02-05 22:29:15,134 [Epoch: 1266 Step: 00084800] Batch Recognition Loss:   0.001549 => Gls Tokens per Sec:     2274 || Batch Translation Loss:   0.033432 => Txt Tokens per Sec:     6292 || Lr: 0.000025
2024-02-05 22:29:16,635 Epoch 1266: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.71 
2024-02-05 22:29:16,636 EPOCH 1267
2024-02-05 22:29:22,215 Epoch 1267: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.55 
2024-02-05 22:29:22,215 EPOCH 1268
2024-02-05 22:29:22,948 [Epoch: 1268 Step: 00084900] Batch Recognition Loss:   0.000167 => Gls Tokens per Sec:     2404 || Batch Translation Loss:   0.010438 => Txt Tokens per Sec:     6691 || Lr: 0.000025
2024-02-05 22:29:27,058 Epoch 1268: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.35 
2024-02-05 22:29:27,059 EPOCH 1269
2024-02-05 22:29:30,398 [Epoch: 1269 Step: 00085000] Batch Recognition Loss:   0.000192 => Gls Tokens per Sec:     2109 || Batch Translation Loss:   0.033511 => Txt Tokens per Sec:     5818 || Lr: 0.000025
2024-02-05 22:29:32,608 Epoch 1269: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.79 
2024-02-05 22:29:32,609 EPOCH 1270
2024-02-05 22:29:37,419 Epoch 1270: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.42 
2024-02-05 22:29:37,419 EPOCH 1271
2024-02-05 22:29:38,182 [Epoch: 1271 Step: 00085100] Batch Recognition Loss:   0.000202 => Gls Tokens per Sec:     1968 || Batch Translation Loss:   0.022294 => Txt Tokens per Sec:     5265 || Lr: 0.000025
2024-02-05 22:29:42,773 Epoch 1271: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.65 
2024-02-05 22:29:42,773 EPOCH 1272
2024-02-05 22:29:45,874 [Epoch: 1272 Step: 00085200] Batch Recognition Loss:   0.000189 => Gls Tokens per Sec:     2188 || Batch Translation Loss:   0.010087 => Txt Tokens per Sec:     6145 || Lr: 0.000025
2024-02-05 22:29:47,922 Epoch 1272: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.23 
2024-02-05 22:29:47,923 EPOCH 1273
2024-02-05 22:29:53,131 Epoch 1273: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.23 
2024-02-05 22:29:53,131 EPOCH 1274
2024-02-05 22:29:53,688 [Epoch: 1274 Step: 00085300] Batch Recognition Loss:   0.000188 => Gls Tokens per Sec:     2590 || Batch Translation Loss:   0.017059 => Txt Tokens per Sec:     7088 || Lr: 0.000025
2024-02-05 22:29:58,339 Epoch 1274: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.17 
2024-02-05 22:29:58,339 EPOCH 1275
2024-02-05 22:30:01,443 [Epoch: 1275 Step: 00085400] Batch Recognition Loss:   0.000316 => Gls Tokens per Sec:     2134 || Batch Translation Loss:   0.056232 => Txt Tokens per Sec:     5917 || Lr: 0.000025
2024-02-05 22:30:03,369 Epoch 1275: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.20 
2024-02-05 22:30:03,369 EPOCH 1276
2024-02-05 22:30:08,690 Epoch 1276: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.22 
2024-02-05 22:30:08,691 EPOCH 1277
2024-02-05 22:30:09,435 [Epoch: 1277 Step: 00085500] Batch Recognition Loss:   0.000167 => Gls Tokens per Sec:     1723 || Batch Translation Loss:   0.012947 => Txt Tokens per Sec:     5308 || Lr: 0.000025
2024-02-05 22:30:13,937 Epoch 1277: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.15 
2024-02-05 22:30:13,938 EPOCH 1278
2024-02-05 22:30:17,336 [Epoch: 1278 Step: 00085600] Batch Recognition Loss:   0.000182 => Gls Tokens per Sec:     1902 || Batch Translation Loss:   0.025811 => Txt Tokens per Sec:     5319 || Lr: 0.000025
2024-02-05 22:30:19,339 Epoch 1278: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.23 
2024-02-05 22:30:19,339 EPOCH 1279
2024-02-05 22:30:24,080 Epoch 1279: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.31 
2024-02-05 22:30:24,081 EPOCH 1280
2024-02-05 22:30:24,730 [Epoch: 1280 Step: 00085700] Batch Recognition Loss:   0.000169 => Gls Tokens per Sec:     1729 || Batch Translation Loss:   0.009501 => Txt Tokens per Sec:     4768 || Lr: 0.000025
2024-02-05 22:30:29,551 Epoch 1280: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.17 
2024-02-05 22:30:29,552 EPOCH 1281
2024-02-05 22:30:32,537 [Epoch: 1281 Step: 00085800] Batch Recognition Loss:   0.000150 => Gls Tokens per Sec:     2145 || Batch Translation Loss:   0.015080 => Txt Tokens per Sec:     5717 || Lr: 0.000025
2024-02-05 22:30:35,069 Epoch 1281: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.05 
2024-02-05 22:30:35,070 EPOCH 1282
2024-02-05 22:30:40,512 Epoch 1282: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.18 
2024-02-05 22:30:40,513 EPOCH 1283
2024-02-05 22:30:40,931 [Epoch: 1283 Step: 00085900] Batch Recognition Loss:   0.000199 => Gls Tokens per Sec:     2302 || Batch Translation Loss:   0.014276 => Txt Tokens per Sec:     6350 || Lr: 0.000025
2024-02-05 22:30:45,105 Epoch 1283: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.61 
2024-02-05 22:30:45,105 EPOCH 1284
2024-02-05 22:30:48,454 [Epoch: 1284 Step: 00086000] Batch Recognition Loss:   0.000188 => Gls Tokens per Sec:     1834 || Batch Translation Loss:   0.024536 => Txt Tokens per Sec:     5004 || Lr: 0.000025
2024-02-05 22:30:56,805 Validation result at epoch 1284, step    86000: duration: 8.3510s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.43700	Translation Loss: 93002.00781	PPL: 10818.94336
	Eval Metric: BLEU
	WER 2.90	(DEL: 0.00,	INS: 0.00,	SUB: 2.90)
	BLEU-4 0.70	(BLEU-1: 11.08,	BLEU-2: 3.48,	BLEU-3: 1.35,	BLEU-4: 0.70)
	CHRF 17.06	ROUGE 9.29
2024-02-05 22:30:56,806 Logging Recognition and Translation Outputs
2024-02-05 22:30:56,806 ========================================================================================================================
2024-02-05 22:30:56,806 Logging Sequence: 161_52.00
2024-02-05 22:30:56,807 	Gloss Reference :	A B+C+D+E
2024-02-05 22:30:56,807 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:30:56,807 	Gloss Alignment :	         
2024-02-05 22:30:56,807 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:30:56,808 	Text Reference  :	the next day  on      15th january 2022 virat kohli      posted a       tweet saying
2024-02-05 22:30:56,808 	Text Hypothesis :	*** **** star batsmen and  bowlers of   the   tournament while  playing the   match 
2024-02-05 22:30:56,808 	Text Alignment  :	D   D    S    S       S    S       S    S     S          S      S       S     S     
2024-02-05 22:30:56,808 ========================================================================================================================
2024-02-05 22:30:56,809 Logging Sequence: 127_140.00
2024-02-05 22:30:56,809 	Gloss Reference :	A B+C+D+E
2024-02-05 22:30:56,809 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:30:56,809 	Gloss Alignment :	         
2024-02-05 22:30:56,809 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:30:56,811 	Text Reference  :	this is india' 3rd   medal in  the world athletics championships he   is  very  talented  and his performance is     highly impressive
2024-02-05 22:30:56,811 	Text Hypothesis :	**** ** ****** india had   won the ***** ********* match         with 263 balls remaining and *** without     losing any    wicket    
2024-02-05 22:30:56,811 	Text Alignment  :	D    D  D      S     S     S       D     D         S             S    S   S     S             D   S           S      S      S         
2024-02-05 22:30:56,811 ========================================================================================================================
2024-02-05 22:30:56,811 Logging Sequence: 104_110.00
2024-02-05 22:30:56,812 	Gloss Reference :	A B+C+D+E
2024-02-05 22:30:56,812 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:30:56,812 	Gloss Alignment :	         
2024-02-05 22:30:56,812 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:30:56,813 	Text Reference  :	however remember though praggnanandhaa stood second he   achieved this at a   young age         of 18  
2024-02-05 22:30:56,813 	Text Hypothesis :	******* ******** ****** ************** the   then   went to       this ** was very  heartbroken to this
2024-02-05 22:30:56,814 	Text Alignment  :	D       D        D      D              S     S      S    S             D  S   S     S           S  S   
2024-02-05 22:30:56,814 ========================================================================================================================
2024-02-05 22:30:56,814 Logging Sequence: 164_412.00
2024-02-05 22:30:56,814 	Gloss Reference :	A B+C+D+E
2024-02-05 22:30:56,814 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:30:56,814 	Gloss Alignment :	         
2024-02-05 22:30:56,814 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:30:56,816 	Text Reference  :	if you divide these two figures you will be shocked to    know that  each ball's worth is  rs      50  lakhs  
2024-02-05 22:30:56,816 	Text Hypothesis :	** *** ****** ***** *** ******* but will be in      front the  media and  media  for   his privacy was invaded
2024-02-05 22:30:56,816 	Text Alignment  :	D  D   D      D     D   D       S           S       S     S    S     S    S      S     S   S       S   S      
2024-02-05 22:30:56,816 ========================================================================================================================
2024-02-05 22:30:56,817 Logging Sequence: 154_2.00
2024-02-05 22:30:56,817 	Gloss Reference :	A B+C+D+E
2024-02-05 22:30:56,817 	Gloss Hypothesis:	A B+C+D  
2024-02-05 22:30:56,817 	Gloss Alignment :	  S      
2024-02-05 22:30:56,817 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:30:56,818 	Text Reference  :	****** ******* **** in      june the ******* icc   had given the        bcci  
2024-02-05 22:30:56,818 	Text Hypothesis :	indian cricket team members of   the wedding going to  their respective medals
2024-02-05 22:30:56,818 	Text Alignment  :	I      I       I    S       S        I       S     S   S     S          S     
2024-02-05 22:30:56,818 ========================================================================================================================
2024-02-05 22:30:59,027 Epoch 1284: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.23 
2024-02-05 22:30:59,028 EPOCH 1285
2024-02-05 22:31:04,475 Epoch 1285: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.68 
2024-02-05 22:31:04,476 EPOCH 1286
2024-02-05 22:31:04,921 [Epoch: 1286 Step: 00086100] Batch Recognition Loss:   0.000138 => Gls Tokens per Sec:     1802 || Batch Translation Loss:   0.010175 => Txt Tokens per Sec:     5009 || Lr: 0.000025
2024-02-05 22:31:10,258 Epoch 1286: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.01 
2024-02-05 22:31:10,259 EPOCH 1287
2024-02-05 22:31:13,161 [Epoch: 1287 Step: 00086200] Batch Recognition Loss:   0.000188 => Gls Tokens per Sec:     2061 || Batch Translation Loss:   0.019273 => Txt Tokens per Sec:     5572 || Lr: 0.000025
2024-02-05 22:31:15,425 Epoch 1287: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.61 
2024-02-05 22:31:15,425 EPOCH 1288
2024-02-05 22:31:20,427 Epoch 1288: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.34 
2024-02-05 22:31:20,428 EPOCH 1289
2024-02-05 22:31:20,942 [Epoch: 1289 Step: 00086300] Batch Recognition Loss:   0.000283 => Gls Tokens per Sec:     1248 || Batch Translation Loss:   0.022624 => Txt Tokens per Sec:     4134 || Lr: 0.000025
2024-02-05 22:31:25,580 Epoch 1289: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.54 
2024-02-05 22:31:25,580 EPOCH 1290
2024-02-05 22:31:28,381 [Epoch: 1290 Step: 00086400] Batch Recognition Loss:   0.000133 => Gls Tokens per Sec:     2079 || Batch Translation Loss:   0.020524 => Txt Tokens per Sec:     6027 || Lr: 0.000025
2024-02-05 22:31:30,453 Epoch 1290: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.35 
2024-02-05 22:31:30,453 EPOCH 1291
2024-02-05 22:31:35,935 Epoch 1291: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.32 
2024-02-05 22:31:35,935 EPOCH 1292
2024-02-05 22:31:36,133 [Epoch: 1292 Step: 00086500] Batch Recognition Loss:   0.000146 => Gls Tokens per Sec:     2437 || Batch Translation Loss:   0.016597 => Txt Tokens per Sec:     6624 || Lr: 0.000025
2024-02-05 22:31:40,543 Epoch 1292: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.76 
2024-02-05 22:31:40,543 EPOCH 1293
2024-02-05 22:31:42,847 [Epoch: 1293 Step: 00086600] Batch Recognition Loss:   0.000245 => Gls Tokens per Sec:     2457 || Batch Translation Loss:   0.019029 => Txt Tokens per Sec:     6865 || Lr: 0.000025
2024-02-05 22:31:45,022 Epoch 1293: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.87 
2024-02-05 22:31:45,022 EPOCH 1294
2024-02-05 22:31:50,339 Epoch 1294: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.22 
2024-02-05 22:31:50,339 EPOCH 1295
2024-02-05 22:31:50,509 [Epoch: 1295 Step: 00086700] Batch Recognition Loss:   0.000133 => Gls Tokens per Sec:     1894 || Batch Translation Loss:   0.011829 => Txt Tokens per Sec:     5243 || Lr: 0.000025
2024-02-05 22:31:55,238 Epoch 1295: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.08 
2024-02-05 22:31:55,238 EPOCH 1296
2024-02-05 22:31:57,863 [Epoch: 1296 Step: 00086800] Batch Recognition Loss:   0.000275 => Gls Tokens per Sec:     2134 || Batch Translation Loss:   0.018428 => Txt Tokens per Sec:     5812 || Lr: 0.000025
2024-02-05 22:32:00,608 Epoch 1296: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.11 
2024-02-05 22:32:00,609 EPOCH 1297
2024-02-05 22:32:05,403 Epoch 1297: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.32 
2024-02-05 22:32:05,404 EPOCH 1298
2024-02-05 22:32:05,552 [Epoch: 1298 Step: 00086900] Batch Recognition Loss:   0.001843 => Gls Tokens per Sec:     1088 || Batch Translation Loss:   0.022280 => Txt Tokens per Sec:     3748 || Lr: 0.000025
2024-02-05 22:32:10,864 Epoch 1298: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.60 
2024-02-05 22:32:10,865 EPOCH 1299
2024-02-05 22:32:13,354 [Epoch: 1299 Step: 00087000] Batch Recognition Loss:   0.000290 => Gls Tokens per Sec:     2147 || Batch Translation Loss:   0.033713 => Txt Tokens per Sec:     5785 || Lr: 0.000025
2024-02-05 22:32:16,093 Epoch 1299: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.24 
2024-02-05 22:32:16,094 EPOCH 1300
2024-02-05 22:32:21,521 [Epoch: 1300 Step: 00087100] Batch Recognition Loss:   0.000117 => Gls Tokens per Sec:     1957 || Batch Translation Loss:   0.008697 => Txt Tokens per Sec:     5415 || Lr: 0.000025
2024-02-05 22:32:21,522 Epoch 1300: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.22 
2024-02-05 22:32:21,522 EPOCH 1301
2024-02-05 22:32:26,552 Epoch 1301: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.31 
2024-02-05 22:32:26,553 EPOCH 1302
2024-02-05 22:32:29,057 [Epoch: 1302 Step: 00087200] Batch Recognition Loss:   0.000205 => Gls Tokens per Sec:     2069 || Batch Translation Loss:   0.036736 => Txt Tokens per Sec:     5598 || Lr: 0.000025
2024-02-05 22:32:31,963 Epoch 1302: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.09 
2024-02-05 22:32:31,963 EPOCH 1303
2024-02-05 22:32:36,797 [Epoch: 1303 Step: 00087300] Batch Recognition Loss:   0.000194 => Gls Tokens per Sec:     2164 || Batch Translation Loss:   0.035374 => Txt Tokens per Sec:     5983 || Lr: 0.000025
2024-02-05 22:32:36,883 Epoch 1303: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.12 
2024-02-05 22:32:36,884 EPOCH 1304
2024-02-05 22:32:42,256 Epoch 1304: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.23 
2024-02-05 22:32:42,256 EPOCH 1305
2024-02-05 22:32:44,254 [Epoch: 1305 Step: 00087400] Batch Recognition Loss:   0.000122 => Gls Tokens per Sec:     2565 || Batch Translation Loss:   0.013601 => Txt Tokens per Sec:     6901 || Lr: 0.000025
2024-02-05 22:32:47,411 Epoch 1305: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.25 
2024-02-05 22:32:47,412 EPOCH 1306
2024-02-05 22:32:52,349 [Epoch: 1306 Step: 00087500] Batch Recognition Loss:   0.000222 => Gls Tokens per Sec:     2087 || Batch Translation Loss:   0.029281 => Txt Tokens per Sec:     5756 || Lr: 0.000025
2024-02-05 22:32:52,516 Epoch 1306: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.37 
2024-02-05 22:32:52,516 EPOCH 1307
2024-02-05 22:32:57,926 Epoch 1307: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.58 
2024-02-05 22:32:57,926 EPOCH 1308
2024-02-05 22:32:59,984 [Epoch: 1308 Step: 00087600] Batch Recognition Loss:   0.000260 => Gls Tokens per Sec:     2364 || Batch Translation Loss:   0.021149 => Txt Tokens per Sec:     6207 || Lr: 0.000025
2024-02-05 22:33:02,947 Epoch 1308: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.46 
2024-02-05 22:33:02,947 EPOCH 1309
2024-02-05 22:33:08,013 [Epoch: 1309 Step: 00087700] Batch Recognition Loss:   0.000250 => Gls Tokens per Sec:     2002 || Batch Translation Loss:   0.018973 => Txt Tokens per Sec:     5559 || Lr: 0.000025
2024-02-05 22:33:08,198 Epoch 1309: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.29 
2024-02-05 22:33:08,198 EPOCH 1310
2024-02-05 22:33:13,019 Epoch 1310: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.16 
2024-02-05 22:33:13,019 EPOCH 1311
2024-02-05 22:33:15,440 [Epoch: 1311 Step: 00087800] Batch Recognition Loss:   0.000225 => Gls Tokens per Sec:     1983 || Batch Translation Loss:   0.014174 => Txt Tokens per Sec:     5645 || Lr: 0.000025
2024-02-05 22:33:18,394 Epoch 1311: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.07 
2024-02-05 22:33:18,394 EPOCH 1312
2024-02-05 22:33:23,097 [Epoch: 1312 Step: 00087900] Batch Recognition Loss:   0.000169 => Gls Tokens per Sec:     2122 || Batch Translation Loss:   0.011308 => Txt Tokens per Sec:     5903 || Lr: 0.000025
2024-02-05 22:33:23,352 Epoch 1312: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.07 
2024-02-05 22:33:23,352 EPOCH 1313
2024-02-05 22:33:28,821 Epoch 1313: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.24 
2024-02-05 22:33:28,821 EPOCH 1314
2024-02-05 22:33:30,787 [Epoch: 1314 Step: 00088000] Batch Recognition Loss:   0.000650 => Gls Tokens per Sec:     2361 || Batch Translation Loss:   0.052902 => Txt Tokens per Sec:     6369 || Lr: 0.000025
2024-02-05 22:33:39,713 Validation result at epoch 1314, step    88000: duration: 8.9263s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.27728	Translation Loss: 93364.28906	PPL: 11217.58789
	Eval Metric: BLEU
	WER 2.97	(DEL: 0.00,	INS: 0.00,	SUB: 2.97)
	BLEU-4 0.57	(BLEU-1: 9.89,	BLEU-2: 3.15,	BLEU-3: 1.24,	BLEU-4: 0.57)
	CHRF 16.86	ROUGE 8.45
2024-02-05 22:33:39,714 Logging Recognition and Translation Outputs
2024-02-05 22:33:39,714 ========================================================================================================================
2024-02-05 22:33:39,714 Logging Sequence: 173_2.00
2024-02-05 22:33:39,714 	Gloss Reference :	A B+C+D+E
2024-02-05 22:33:39,714 	Gloss Hypothesis:	A B+C+D  
2024-02-05 22:33:39,714 	Gloss Alignment :	  S      
2024-02-05 22:33:39,715 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:33:39,716 	Text Reference  :	****** ****** *** ***** virat kohli has   captained india        in 45 t20 internationals matches and 95  odis 
2024-02-05 22:33:39,716 	Text Hypothesis :	krunal became the first set   to    score a         half-century in ** 26  deliveries     on      his odi debut
2024-02-05 22:33:39,716 	Text Alignment  :	I      I      I   I     S     S     S     S         S               D  S   S              S       S   S   S    
2024-02-05 22:33:39,716 ========================================================================================================================
2024-02-05 22:33:39,716 Logging Sequence: 93_134.00
2024-02-05 22:33:39,717 	Gloss Reference :	A B+C+D+E
2024-02-05 22:33:39,717 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:33:39,717 	Gloss Alignment :	         
2024-02-05 22:33:39,717 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:33:39,717 	Text Reference  :	he  even      hugged her    and kissed her    
2024-02-05 22:33:39,717 	Text Hypothesis :	the cricketer was    former and ****** england
2024-02-05 22:33:39,718 	Text Alignment  :	S   S         S      S          D      S      
2024-02-05 22:33:39,718 ========================================================================================================================
2024-02-05 22:33:39,718 Logging Sequence: 63_44.00
2024-02-05 22:33:39,718 	Gloss Reference :	A B+C+D+E
2024-02-05 22:33:39,718 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:33:39,718 	Gloss Alignment :	         
2024-02-05 22:33:39,718 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:33:39,719 	Text Reference  :	**** ********* **** ****** **** ******* *** the   amount for the form  is  non-refundable
2024-02-05 22:33:39,719 	Text Hypothesis :	many companies will submit such tenders are their teams  for the final and time          
2024-02-05 22:33:39,719 	Text Alignment  :	I    I         I    I      I    I       I   S     S              S     S   S             
2024-02-05 22:33:39,719 ========================================================================================================================
2024-02-05 22:33:39,720 Logging Sequence: 164_394.00
2024-02-05 22:33:39,720 	Gloss Reference :	A B+C+D+E
2024-02-05 22:33:39,720 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:33:39,720 	Gloss Alignment :	         
2024-02-05 22:33:39,720 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:33:39,721 	Text Reference  :	calculating will bring the       total to   98400 balls  in 5   years
2024-02-05 22:33:39,721 	Text Hypothesis :	*********** the  bcci  secretary jay   shah was   amazed by his bcci 
2024-02-05 22:33:39,721 	Text Alignment  :	D           S    S     S         S     S    S     S      S  S   S    
2024-02-05 22:33:39,721 ========================================================================================================================
2024-02-05 22:33:39,721 Logging Sequence: 65_77.00
2024-02-05 22:33:39,722 	Gloss Reference :	A B+C+D+E
2024-02-05 22:33:39,722 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:33:39,722 	Gloss Alignment :	         
2024-02-05 22:33:39,722 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:33:39,723 	Text Reference  :	*** *** ***** ***** *** **** ** indian team   travelling included 16 players
2024-02-05 22:33:39,723 	Text Hypothesis :	for the third match was held at the    iconic wankhede   stadium  in mumbai 
2024-02-05 22:33:39,723 	Text Alignment  :	I   I   I     I     I   I    I  S      S      S          S        S  S      
2024-02-05 22:33:39,723 ========================================================================================================================
2024-02-05 22:33:42,996 Epoch 1314: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.54 
2024-02-05 22:33:42,996 EPOCH 1315
2024-02-05 22:33:48,069 [Epoch: 1315 Step: 00088100] Batch Recognition Loss:   0.000363 => Gls Tokens per Sec:     1956 || Batch Translation Loss:   0.010027 => Txt Tokens per Sec:     5441 || Lr: 0.000025
2024-02-05 22:33:48,445 Epoch 1315: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.10 
2024-02-05 22:33:48,445 EPOCH 1316
2024-02-05 22:33:53,920 Epoch 1316: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.36 
2024-02-05 22:33:53,921 EPOCH 1317
2024-02-05 22:33:56,304 [Epoch: 1317 Step: 00088200] Batch Recognition Loss:   0.000248 => Gls Tokens per Sec:     1881 || Batch Translation Loss:   0.025367 => Txt Tokens per Sec:     5114 || Lr: 0.000025
2024-02-05 22:33:59,201 Epoch 1317: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.50 
2024-02-05 22:33:59,201 EPOCH 1318
2024-02-05 22:34:03,947 [Epoch: 1318 Step: 00088300] Batch Recognition Loss:   0.000223 => Gls Tokens per Sec:     2035 || Batch Translation Loss:   0.012241 => Txt Tokens per Sec:     5576 || Lr: 0.000025
2024-02-05 22:34:04,629 Epoch 1318: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.57 
2024-02-05 22:34:04,629 EPOCH 1319
2024-02-05 22:34:09,738 Epoch 1319: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.88 
2024-02-05 22:34:09,738 EPOCH 1320
2024-02-05 22:34:11,862 [Epoch: 1320 Step: 00088400] Batch Recognition Loss:   0.000776 => Gls Tokens per Sec:     2035 || Batch Translation Loss:   0.030491 => Txt Tokens per Sec:     5965 || Lr: 0.000025
2024-02-05 22:34:14,851 Epoch 1320: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.91 
2024-02-05 22:34:14,851 EPOCH 1321
2024-02-05 22:34:19,645 [Epoch: 1321 Step: 00088500] Batch Recognition Loss:   0.000466 => Gls Tokens per Sec:     1982 || Batch Translation Loss:   0.024782 => Txt Tokens per Sec:     5490 || Lr: 0.000025
2024-02-05 22:34:20,184 Epoch 1321: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.01 
2024-02-05 22:34:20,184 EPOCH 1322
2024-02-05 22:34:25,643 Epoch 1322: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.45 
2024-02-05 22:34:25,644 EPOCH 1323
2024-02-05 22:34:27,670 [Epoch: 1323 Step: 00088600] Batch Recognition Loss:   0.000393 => Gls Tokens per Sec:     2005 || Batch Translation Loss:   0.020293 => Txt Tokens per Sec:     5743 || Lr: 0.000025
2024-02-05 22:34:30,436 Epoch 1323: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.29 
2024-02-05 22:34:30,436 EPOCH 1324
2024-02-05 22:34:35,313 [Epoch: 1324 Step: 00088700] Batch Recognition Loss:   0.000159 => Gls Tokens per Sec:     1916 || Batch Translation Loss:   0.018160 => Txt Tokens per Sec:     5389 || Lr: 0.000025
2024-02-05 22:34:35,833 Epoch 1324: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.22 
2024-02-05 22:34:35,833 EPOCH 1325
2024-02-05 22:34:40,629 Epoch 1325: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.20 
2024-02-05 22:34:40,629 EPOCH 1326
2024-02-05 22:34:42,635 [Epoch: 1326 Step: 00088800] Batch Recognition Loss:   0.000147 => Gls Tokens per Sec:     1994 || Batch Translation Loss:   0.012411 => Txt Tokens per Sec:     5516 || Lr: 0.000025
2024-02-05 22:34:46,087 Epoch 1326: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.28 
2024-02-05 22:34:46,087 EPOCH 1327
2024-02-05 22:34:50,259 [Epoch: 1327 Step: 00088900] Batch Recognition Loss:   0.001729 => Gls Tokens per Sec:     2201 || Batch Translation Loss:   0.025153 => Txt Tokens per Sec:     6061 || Lr: 0.000025
2024-02-05 22:34:50,863 Epoch 1327: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.61 
2024-02-05 22:34:50,864 EPOCH 1328
2024-02-05 22:34:56,344 Epoch 1328: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.82 
2024-02-05 22:34:56,344 EPOCH 1329
2024-02-05 22:34:58,208 [Epoch: 1329 Step: 00089000] Batch Recognition Loss:   0.000147 => Gls Tokens per Sec:     2007 || Batch Translation Loss:   0.019914 => Txt Tokens per Sec:     5483 || Lr: 0.000025
2024-02-05 22:35:01,340 Epoch 1329: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.51 
2024-02-05 22:35:01,341 EPOCH 1330
2024-02-05 22:35:05,996 [Epoch: 1330 Step: 00089100] Batch Recognition Loss:   0.000955 => Gls Tokens per Sec:     1938 || Batch Translation Loss:   0.029403 => Txt Tokens per Sec:     5350 || Lr: 0.000025
2024-02-05 22:35:06,761 Epoch 1330: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.33 
2024-02-05 22:35:06,761 EPOCH 1331
2024-02-05 22:35:12,181 Epoch 1331: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.52 
2024-02-05 22:35:12,182 EPOCH 1332
2024-02-05 22:35:14,027 [Epoch: 1332 Step: 00089200] Batch Recognition Loss:   0.000729 => Gls Tokens per Sec:     1942 || Batch Translation Loss:   0.016653 => Txt Tokens per Sec:     5565 || Lr: 0.000025
2024-02-05 22:35:17,328 Epoch 1332: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.26 
2024-02-05 22:35:17,328 EPOCH 1333
2024-02-05 22:35:21,855 [Epoch: 1333 Step: 00089300] Batch Recognition Loss:   0.000120 => Gls Tokens per Sec:     1958 || Batch Translation Loss:   0.014809 => Txt Tokens per Sec:     5415 || Lr: 0.000025
2024-02-05 22:35:22,765 Epoch 1333: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.13 
2024-02-05 22:35:22,765 EPOCH 1334
2024-02-05 22:35:28,080 Epoch 1334: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.18 
2024-02-05 22:35:28,080 EPOCH 1335
2024-02-05 22:35:29,606 [Epoch: 1335 Step: 00089400] Batch Recognition Loss:   0.000098 => Gls Tokens per Sec:     2309 || Batch Translation Loss:   0.015063 => Txt Tokens per Sec:     6101 || Lr: 0.000025
2024-02-05 22:35:33,496 Epoch 1335: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.60 
2024-02-05 22:35:33,496 EPOCH 1336
2024-02-05 22:35:37,633 [Epoch: 1336 Step: 00089500] Batch Recognition Loss:   0.000413 => Gls Tokens per Sec:     2128 || Batch Translation Loss:   0.025472 => Txt Tokens per Sec:     5946 || Lr: 0.000025
2024-02-05 22:35:38,659 Epoch 1336: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.08 
2024-02-05 22:35:38,660 EPOCH 1337
2024-02-05 22:35:44,053 Epoch 1337: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.55 
2024-02-05 22:35:44,054 EPOCH 1338
2024-02-05 22:35:45,533 [Epoch: 1338 Step: 00089600] Batch Recognition Loss:   0.000140 => Gls Tokens per Sec:     2274 || Batch Translation Loss:   0.023995 => Txt Tokens per Sec:     6485 || Lr: 0.000025
2024-02-05 22:35:49,126 Epoch 1338: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.29 
2024-02-05 22:35:49,127 EPOCH 1339
2024-02-05 22:35:53,201 [Epoch: 1339 Step: 00089700] Batch Recognition Loss:   0.000293 => Gls Tokens per Sec:     2097 || Batch Translation Loss:   0.022540 => Txt Tokens per Sec:     5825 || Lr: 0.000025
2024-02-05 22:35:54,045 Epoch 1339: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.35 
2024-02-05 22:35:54,045 EPOCH 1340
2024-02-05 22:35:58,882 Epoch 1340: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.14 
2024-02-05 22:35:58,883 EPOCH 1341
2024-02-05 22:36:00,730 [Epoch: 1341 Step: 00089800] Batch Recognition Loss:   0.000156 => Gls Tokens per Sec:     1734 || Batch Translation Loss:   0.017085 => Txt Tokens per Sec:     5184 || Lr: 0.000025
2024-02-05 22:36:04,371 Epoch 1341: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.15 
2024-02-05 22:36:04,371 EPOCH 1342
2024-02-05 22:36:08,366 [Epoch: 1342 Step: 00089900] Batch Recognition Loss:   0.000221 => Gls Tokens per Sec:     2099 || Batch Translation Loss:   0.016776 => Txt Tokens per Sec:     5762 || Lr: 0.000025
2024-02-05 22:36:09,538 Epoch 1342: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.07 
2024-02-05 22:36:09,538 EPOCH 1343
2024-02-05 22:36:14,882 Epoch 1343: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.07 
2024-02-05 22:36:14,883 EPOCH 1344
2024-02-05 22:36:16,076 [Epoch: 1344 Step: 00090000] Batch Recognition Loss:   0.000347 => Gls Tokens per Sec:     2550 || Batch Translation Loss:   0.017890 => Txt Tokens per Sec:     6524 || Lr: 0.000025
2024-02-05 22:36:24,777 Validation result at epoch 1344, step    90000: duration: 8.7012s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.20756	Translation Loss: 94108.99219	PPL: 12083.77734
	Eval Metric: BLEU
	WER 3.11	(DEL: 0.00,	INS: 0.00,	SUB: 3.11)
	BLEU-4 0.66	(BLEU-1: 10.11,	BLEU-2: 3.19,	BLEU-3: 1.30,	BLEU-4: 0.66)
	CHRF 16.63	ROUGE 8.84
2024-02-05 22:36:24,778 Logging Recognition and Translation Outputs
2024-02-05 22:36:24,778 ========================================================================================================================
2024-02-05 22:36:24,779 Logging Sequence: 105_42.00
2024-02-05 22:36:24,779 	Gloss Reference :	A B+C+D+E
2024-02-05 22:36:24,779 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:36:24,779 	Gloss Alignment :	         
2024-02-05 22:36:24,779 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:36:24,781 	Text Reference  :	**** *** ** * was  playing  against ** ******* 31 year old norwegian magnus carlsen who is world number one  player
2024-02-05 22:36:24,781 	Text Hypothesis :	this led to a huge argument against he debuted in his  ipl seasons   and    has     led to its   may    jump kohli 
2024-02-05 22:36:24,781 	Text Alignment  :	I    I   I  I S    S                I  I       S  S    S   S         S      S       S   S  S     S      S    S     
2024-02-05 22:36:24,781 ========================================================================================================================
2024-02-05 22:36:24,782 Logging Sequence: 118_232.00
2024-02-05 22:36:24,782 	Gloss Reference :	A B+C+D+E
2024-02-05 22:36:24,782 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:36:24,782 	Gloss Alignment :	         
2024-02-05 22:36:24,782 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:36:24,783 	Text Reference  :	messi was * ********** *** *** overjoyed upon recieving the award    
2024-02-05 22:36:24,783 	Text Hypothesis :	there was a government did not permit    it   this      is  incorrect
2024-02-05 22:36:24,783 	Text Alignment  :	S         I I          I   I   S         S    S         S   S        
2024-02-05 22:36:24,783 ========================================================================================================================
2024-02-05 22:36:24,783 Logging Sequence: 171_2.00
2024-02-05 22:36:24,783 	Gloss Reference :	A B+C+D+E
2024-02-05 22:36:24,784 	Gloss Hypothesis:	A B+C+D  
2024-02-05 22:36:24,784 	Gloss Alignment :	  S      
2024-02-05 22:36:24,784 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:36:24,785 	Text Reference  :	** as   you  might      all know that the ipl is about to end the finals are on  28th may        
2024-02-05 22:36:24,785 	Text Hypothesis :	in june 2020 cricketers all **** **** the ipl ** ***** ** *** *** ****** and has been quarantined
2024-02-05 22:36:24,785 	Text Alignment  :	I  S    S    S              D    D            D  D     D  D   D   D      S   S   S    S          
2024-02-05 22:36:24,785 ========================================================================================================================
2024-02-05 22:36:24,786 Logging Sequence: 136_107.00
2024-02-05 22:36:24,786 	Gloss Reference :	A B+C+D+E
2024-02-05 22:36:24,786 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:36:24,786 	Gloss Alignment :	         
2024-02-05 22:36:24,786 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:36:24,787 	Text Reference  :	sindhu replied that she had stop    eating icecream    because of      her  training
2024-02-05 22:36:24,787 	Text Hypothesis :	****** ******* **** *** new zealand beat   afghanistan and     secured many medals  
2024-02-05 22:36:24,787 	Text Alignment  :	D      D       D    D   S   S       S      S           S       S       S    S       
2024-02-05 22:36:24,787 ========================================================================================================================
2024-02-05 22:36:24,787 Logging Sequence: 93_93.00
2024-02-05 22:36:24,788 	Gloss Reference :	A B+C+D+E
2024-02-05 22:36:24,788 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:36:24,788 	Gloss Alignment :	         
2024-02-05 22:36:24,788 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:36:24,789 	Text Reference  :	****** ** rooney was at   the club as       well    
2024-02-05 22:36:24,789 	Text Hypothesis :	series on 4th    may 2021 the **** covid-19 pandemic
2024-02-05 22:36:24,789 	Text Alignment  :	I      I  S      S   S        D    S        S       
2024-02-05 22:36:24,789 ========================================================================================================================
2024-02-05 22:36:28,951 Epoch 1344: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.07 
2024-02-05 22:36:28,951 EPOCH 1345
2024-02-05 22:36:33,002 [Epoch: 1345 Step: 00090100] Batch Recognition Loss:   0.000126 => Gls Tokens per Sec:     2055 || Batch Translation Loss:   0.012344 => Txt Tokens per Sec:     5592 || Lr: 0.000025
2024-02-05 22:36:34,295 Epoch 1345: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.04 
2024-02-05 22:36:34,295 EPOCH 1346
2024-02-05 22:36:39,579 Epoch 1346: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.19 
2024-02-05 22:36:39,580 EPOCH 1347
2024-02-05 22:36:41,160 [Epoch: 1347 Step: 00090200] Batch Recognition Loss:   0.000169 => Gls Tokens per Sec:     1824 || Batch Translation Loss:   0.017465 => Txt Tokens per Sec:     5381 || Lr: 0.000025
2024-02-05 22:36:44,900 Epoch 1347: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.08 
2024-02-05 22:36:44,901 EPOCH 1348
2024-02-05 22:36:48,540 [Epoch: 1348 Step: 00090300] Batch Recognition Loss:   0.000299 => Gls Tokens per Sec:     2216 || Batch Translation Loss:   0.025289 => Txt Tokens per Sec:     6177 || Lr: 0.000025
2024-02-05 22:36:49,687 Epoch 1348: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.69 
2024-02-05 22:36:49,688 EPOCH 1349
2024-02-05 22:36:55,046 Epoch 1349: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.39 
2024-02-05 22:36:55,047 EPOCH 1350
2024-02-05 22:36:56,133 [Epoch: 1350 Step: 00090400] Batch Recognition Loss:   0.001019 => Gls Tokens per Sec:     2416 || Batch Translation Loss:   0.007579 => Txt Tokens per Sec:     6870 || Lr: 0.000025
2024-02-05 22:36:59,834 Epoch 1350: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.19 
2024-02-05 22:36:59,835 EPOCH 1351
2024-02-05 22:37:04,146 [Epoch: 1351 Step: 00090500] Batch Recognition Loss:   0.005460 => Gls Tokens per Sec:     1856 || Batch Translation Loss:   0.004665 => Txt Tokens per Sec:     5226 || Lr: 0.000025
2024-02-05 22:37:05,376 Epoch 1351: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.08 
2024-02-05 22:37:05,376 EPOCH 1352
2024-02-05 22:37:10,511 Epoch 1352: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.07 
2024-02-05 22:37:10,512 EPOCH 1353
2024-02-05 22:37:11,733 [Epoch: 1353 Step: 00090600] Batch Recognition Loss:   0.000160 => Gls Tokens per Sec:     2098 || Batch Translation Loss:   0.011598 => Txt Tokens per Sec:     5745 || Lr: 0.000025
2024-02-05 22:37:15,855 Epoch 1353: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.20 
2024-02-05 22:37:15,855 EPOCH 1354
2024-02-05 22:37:19,434 [Epoch: 1354 Step: 00090700] Batch Recognition Loss:   0.000311 => Gls Tokens per Sec:     2163 || Batch Translation Loss:   0.005029 => Txt Tokens per Sec:     5857 || Lr: 0.000025
2024-02-05 22:37:20,919 Epoch 1354: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.36 
2024-02-05 22:37:20,919 EPOCH 1355
2024-02-05 22:37:25,980 Epoch 1355: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.72 
2024-02-05 22:37:25,980 EPOCH 1356
2024-02-05 22:37:26,906 [Epoch: 1356 Step: 00090800] Batch Recognition Loss:   0.000491 => Gls Tokens per Sec:     2594 || Batch Translation Loss:   0.024151 => Txt Tokens per Sec:     6866 || Lr: 0.000025
2024-02-05 22:37:31,287 Epoch 1356: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.59 
2024-02-05 22:37:31,288 EPOCH 1357
2024-02-05 22:37:34,918 [Epoch: 1357 Step: 00090900] Batch Recognition Loss:   0.000198 => Gls Tokens per Sec:     2089 || Batch Translation Loss:   0.023197 => Txt Tokens per Sec:     5772 || Lr: 0.000025
2024-02-05 22:37:36,273 Epoch 1357: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.97 
2024-02-05 22:37:36,273 EPOCH 1358
2024-02-05 22:37:41,564 Epoch 1358: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.40 
2024-02-05 22:37:41,565 EPOCH 1359
2024-02-05 22:37:42,667 [Epoch: 1359 Step: 00091000] Batch Recognition Loss:   0.000484 => Gls Tokens per Sec:     2035 || Batch Translation Loss:   0.035948 => Txt Tokens per Sec:     5804 || Lr: 0.000025
2024-02-05 22:37:46,590 Epoch 1359: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.39 
2024-02-05 22:37:46,590 EPOCH 1360
2024-02-05 22:37:50,286 [Epoch: 1360 Step: 00091100] Batch Recognition Loss:   0.000100 => Gls Tokens per Sec:     2036 || Batch Translation Loss:   0.011206 => Txt Tokens per Sec:     5659 || Lr: 0.000025
2024-02-05 22:37:51,927 Epoch 1360: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.24 
2024-02-05 22:37:51,927 EPOCH 1361
2024-02-05 22:37:57,049 Epoch 1361: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.62 
2024-02-05 22:37:57,049 EPOCH 1362
2024-02-05 22:37:58,147 [Epoch: 1362 Step: 00091200] Batch Recognition Loss:   0.000133 => Gls Tokens per Sec:     1897 || Batch Translation Loss:   0.019933 => Txt Tokens per Sec:     5434 || Lr: 0.000025
2024-02-05 22:38:02,492 Epoch 1362: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.33 
2024-02-05 22:38:02,493 EPOCH 1363
2024-02-05 22:38:05,891 [Epoch: 1363 Step: 00091300] Batch Recognition Loss:   0.000181 => Gls Tokens per Sec:     2137 || Batch Translation Loss:   0.024288 => Txt Tokens per Sec:     5945 || Lr: 0.000025
2024-02-05 22:38:07,159 Epoch 1363: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.05 
2024-02-05 22:38:07,160 EPOCH 1364
2024-02-05 22:38:12,714 Epoch 1364: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.09 
2024-02-05 22:38:12,715 EPOCH 1365
2024-02-05 22:38:13,584 [Epoch: 1365 Step: 00091400] Batch Recognition Loss:   0.000325 => Gls Tokens per Sec:     2212 || Batch Translation Loss:   0.043650 => Txt Tokens per Sec:     6221 || Lr: 0.000025
2024-02-05 22:38:17,668 Epoch 1365: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.59 
2024-02-05 22:38:17,668 EPOCH 1366
2024-02-05 22:38:21,192 [Epoch: 1366 Step: 00091500] Batch Recognition Loss:   0.001312 => Gls Tokens per Sec:     2015 || Batch Translation Loss:   0.014305 => Txt Tokens per Sec:     5468 || Lr: 0.000025
2024-02-05 22:38:23,129 Epoch 1366: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.53 
2024-02-05 22:38:23,129 EPOCH 1367
2024-02-05 22:38:28,087 Epoch 1367: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.64 
2024-02-05 22:38:28,088 EPOCH 1368
2024-02-05 22:38:28,986 [Epoch: 1368 Step: 00091600] Batch Recognition Loss:   0.000207 => Gls Tokens per Sec:     1964 || Batch Translation Loss:   0.018085 => Txt Tokens per Sec:     5386 || Lr: 0.000025
2024-02-05 22:38:33,407 Epoch 1368: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.37 
2024-02-05 22:38:33,408 EPOCH 1369
2024-02-05 22:38:36,497 [Epoch: 1369 Step: 00091700] Batch Recognition Loss:   0.000301 => Gls Tokens per Sec:     2279 || Batch Translation Loss:   0.022520 => Txt Tokens per Sec:     6361 || Lr: 0.000025
2024-02-05 22:38:38,314 Epoch 1369: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.10 
2024-02-05 22:38:38,315 EPOCH 1370
2024-02-05 22:38:43,550 Epoch 1370: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.08 
2024-02-05 22:38:43,550 EPOCH 1371
2024-02-05 22:38:44,400 [Epoch: 1371 Step: 00091800] Batch Recognition Loss:   0.000204 => Gls Tokens per Sec:     1888 || Batch Translation Loss:   0.021048 => Txt Tokens per Sec:     5446 || Lr: 0.000025
2024-02-05 22:38:48,781 Epoch 1371: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.90 
2024-02-05 22:38:48,781 EPOCH 1372
2024-02-05 22:38:52,185 [Epoch: 1372 Step: 00091900] Batch Recognition Loss:   0.000980 => Gls Tokens per Sec:     2022 || Batch Translation Loss:   0.029780 => Txt Tokens per Sec:     5656 || Lr: 0.000025
2024-02-05 22:38:53,808 Epoch 1372: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.93 
2024-02-05 22:38:53,808 EPOCH 1373
2024-02-05 22:38:59,028 Epoch 1373: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.59 
2024-02-05 22:38:59,029 EPOCH 1374
2024-02-05 22:38:59,644 [Epoch: 1374 Step: 00092000] Batch Recognition Loss:   0.000164 => Gls Tokens per Sec:     2344 || Batch Translation Loss:   0.010769 => Txt Tokens per Sec:     6061 || Lr: 0.000025
2024-02-05 22:39:08,065 Validation result at epoch 1374, step    92000: duration: 8.4192s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.28181	Translation Loss: 94672.17188	PPL: 12782.97754
	Eval Metric: BLEU
	WER 3.18	(DEL: 0.00,	INS: 0.00,	SUB: 3.18)
	BLEU-4 0.80	(BLEU-1: 9.94,	BLEU-2: 3.23,	BLEU-3: 1.43,	BLEU-4: 0.80)
	CHRF 16.69	ROUGE 8.10
2024-02-05 22:39:08,066 Logging Recognition and Translation Outputs
2024-02-05 22:39:08,067 ========================================================================================================================
2024-02-05 22:39:08,067 Logging Sequence: 122_208.00
2024-02-05 22:39:08,067 	Gloss Reference :	A B+C+D+E
2024-02-05 22:39:08,067 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:39:08,067 	Gloss Alignment :	         
2024-02-05 22:39:08,067 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:39:08,069 	Text Reference  :	***** ****** **** **** *** **** ** in   the  equestrian category of   the olympics indrajit lamba qualified in     1996     
2024-02-05 22:39:08,069 	Text Hypothesis :	sadly nirmal kaur lost her life on 13th june as         they     must be  a        few      of    the       deadly infection
2024-02-05 22:39:08,069 	Text Alignment  :	I     I      I    I    I   I    I  S    S    S          S        S    S   S        S        S     S         S      S        
2024-02-05 22:39:08,069 ========================================================================================================================
2024-02-05 22:39:08,069 Logging Sequence: 161_37.00
2024-02-05 22:39:08,069 	Gloss Reference :	A B+C+D+E
2024-02-05 22:39:08,070 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:39:08,070 	Gloss Alignment :	         
2024-02-05 22:39:08,070 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:39:08,071 	Text Reference  :	the team was shocked by this and virat requested everyone to     keep it          confidential this   happened on        14th  january 2022   
2024-02-05 22:39:08,071 	Text Hypothesis :	*** **** *** ******* ** **** *** ***** however   they     played for  encouraging healthy      habits creating awareness about 9       minutes
2024-02-05 22:39:08,072 	Text Alignment  :	D   D    D   D       D  D    D   D     S         S        S      S    S           S            S      S        S         S     S       S      
2024-02-05 22:39:08,072 ========================================================================================================================
2024-02-05 22:39:08,072 Logging Sequence: 178_62.00
2024-02-05 22:39:08,072 	Gloss Reference :	A B+C+D+E
2024-02-05 22:39:08,072 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:39:08,072 	Gloss Alignment :	         
2024-02-05 22:39:08,072 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:39:08,073 	Text Reference  :	**** ** *** *** ****** ** ************ delhi court   had     issued a       non-bailable warrant against sushil
2024-02-05 22:39:08,074 	Text Hypothesis :	this is why the finals of broadcasting the   opening players were   allowed to           wear    clothes that  
2024-02-05 22:39:08,074 	Text Alignment  :	I    I  I   I   I      I  I            S     S       S       S      S       S            S       S       S     
2024-02-05 22:39:08,074 ========================================================================================================================
2024-02-05 22:39:08,074 Logging Sequence: 172_15.00
2024-02-05 22:39:08,074 	Gloss Reference :	A B+C+D+E
2024-02-05 22:39:08,074 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:39:08,074 	Gloss Alignment :	         
2024-02-05 22:39:08,074 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:39:08,076 	Text Reference  :	now in the final match     on     28   may   2023  the     two  teams were up  against each other at  the same venue
2024-02-05 22:39:08,077 	Text Hypothesis :	*** ** *** ***** generally people wear their goals however they won   a    new match   as   they  get out for  2-2  
2024-02-05 22:39:08,077 	Text Alignment  :	D   D  D   D     S         S      S    S     S     S       S    S     S    S   S       S    S     S   S   S    S    
2024-02-05 22:39:08,077 ========================================================================================================================
2024-02-05 22:39:08,077 Logging Sequence: 73_88.00
2024-02-05 22:39:08,077 	Gloss Reference :	A B+C+D+E
2024-02-05 22:39:08,077 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:39:08,078 	Gloss Alignment :	         
2024-02-05 22:39:08,078 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:39:08,080 	Text Reference  :	there   will be      chicken kebab pani puri sev     puri aloo     chaat etc it     will have dishes    from  different parts of the  country
2024-02-05 22:39:08,081 	Text Hypothesis :	however some players may     the   team was  wearing the  exacting game  and inputs off  the  bengaluru crowd asking    them  to join us     
2024-02-05 22:39:08,081 	Text Alignment  :	S       S    S       S       S     S    S    S       S    S        S     S   S      S    S    S         S     S         S     S  S    S      
2024-02-05 22:39:08,081 ========================================================================================================================
2024-02-05 22:39:12,778 Epoch 1374: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.31 
2024-02-05 22:39:12,778 EPOCH 1375
2024-02-05 22:39:15,640 [Epoch: 1375 Step: 00092100] Batch Recognition Loss:   0.000350 => Gls Tokens per Sec:     2315 || Batch Translation Loss:   0.006912 => Txt Tokens per Sec:     6334 || Lr: 0.000025
2024-02-05 22:39:17,567 Epoch 1375: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.16 
2024-02-05 22:39:17,567 EPOCH 1376
2024-02-05 22:39:22,965 Epoch 1376: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.34 
2024-02-05 22:39:22,966 EPOCH 1377
2024-02-05 22:39:23,507 [Epoch: 1377 Step: 00092200] Batch Recognition Loss:   0.000162 => Gls Tokens per Sec:     2370 || Batch Translation Loss:   0.021875 => Txt Tokens per Sec:     6782 || Lr: 0.000025
2024-02-05 22:39:27,939 Epoch 1377: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.55 
2024-02-05 22:39:27,940 EPOCH 1378
2024-02-05 22:39:31,310 [Epoch: 1378 Step: 00092300] Batch Recognition Loss:   0.000181 => Gls Tokens per Sec:     1918 || Batch Translation Loss:   0.024082 => Txt Tokens per Sec:     5337 || Lr: 0.000025
2024-02-05 22:39:33,228 Epoch 1378: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.27 
2024-02-05 22:39:33,228 EPOCH 1379
2024-02-05 22:39:38,294 Epoch 1379: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.62 
2024-02-05 22:39:38,294 EPOCH 1380
2024-02-05 22:39:38,747 [Epoch: 1380 Step: 00092400] Batch Recognition Loss:   0.000151 => Gls Tokens per Sec:     2482 || Batch Translation Loss:   0.053206 => Txt Tokens per Sec:     6385 || Lr: 0.000025
2024-02-05 22:39:43,412 Epoch 1380: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.01 
2024-02-05 22:39:43,412 EPOCH 1381
2024-02-05 22:39:46,509 [Epoch: 1381 Step: 00092500] Batch Recognition Loss:   0.000153 => Gls Tokens per Sec:     2035 || Batch Translation Loss:   0.016851 => Txt Tokens per Sec:     5634 || Lr: 0.000025
2024-02-05 22:39:48,686 Epoch 1381: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.74 
2024-02-05 22:39:48,686 EPOCH 1382
2024-02-05 22:39:53,783 Epoch 1382: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.59 
2024-02-05 22:39:53,783 EPOCH 1383
2024-02-05 22:39:54,305 [Epoch: 1383 Step: 00092600] Batch Recognition Loss:   0.000184 => Gls Tokens per Sec:     1844 || Batch Translation Loss:   0.022825 => Txt Tokens per Sec:     5592 || Lr: 0.000025
2024-02-05 22:39:59,164 Epoch 1383: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.84 
2024-02-05 22:39:59,165 EPOCH 1384
2024-02-05 22:40:02,170 [Epoch: 1384 Step: 00092700] Batch Recognition Loss:   0.000171 => Gls Tokens per Sec:     2078 || Batch Translation Loss:   0.012606 => Txt Tokens per Sec:     5864 || Lr: 0.000025
2024-02-05 22:40:04,081 Epoch 1384: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.39 
2024-02-05 22:40:04,082 EPOCH 1385
2024-02-05 22:40:09,611 Epoch 1385: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.17 
2024-02-05 22:40:09,611 EPOCH 1386
2024-02-05 22:40:09,925 [Epoch: 1386 Step: 00092800] Batch Recognition Loss:   0.000233 => Gls Tokens per Sec:     2564 || Batch Translation Loss:   0.012841 => Txt Tokens per Sec:     6401 || Lr: 0.000025
2024-02-05 22:40:14,983 Epoch 1386: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.16 
2024-02-05 22:40:14,983 EPOCH 1387
2024-02-05 22:40:17,821 [Epoch: 1387 Step: 00092900] Batch Recognition Loss:   0.000222 => Gls Tokens per Sec:     2109 || Batch Translation Loss:   0.087573 => Txt Tokens per Sec:     5769 || Lr: 0.000025
2024-02-05 22:40:20,310 Epoch 1387: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.28 
2024-02-05 22:40:20,311 EPOCH 1388
2024-02-05 22:40:25,567 Epoch 1388: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.66 
2024-02-05 22:40:25,567 EPOCH 1389
2024-02-05 22:40:25,898 [Epoch: 1389 Step: 00093000] Batch Recognition Loss:   0.000172 => Gls Tokens per Sec:     1939 || Batch Translation Loss:   0.024398 => Txt Tokens per Sec:     5458 || Lr: 0.000025
2024-02-05 22:40:31,045 Epoch 1389: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.86 
2024-02-05 22:40:31,046 EPOCH 1390
2024-02-05 22:40:33,591 [Epoch: 1390 Step: 00093100] Batch Recognition Loss:   0.000258 => Gls Tokens per Sec:     2289 || Batch Translation Loss:   0.024348 => Txt Tokens per Sec:     6378 || Lr: 0.000025
2024-02-05 22:40:36,197 Epoch 1390: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.71 
2024-02-05 22:40:36,198 EPOCH 1391
2024-02-05 22:40:41,643 Epoch 1391: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.64 
2024-02-05 22:40:41,644 EPOCH 1392
2024-02-05 22:40:41,827 [Epoch: 1392 Step: 00093200] Batch Recognition Loss:   0.000182 => Gls Tokens per Sec:     2637 || Batch Translation Loss:   0.015858 => Txt Tokens per Sec:     6467 || Lr: 0.000025
2024-02-05 22:40:46,787 Epoch 1392: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.62 
2024-02-05 22:40:46,787 EPOCH 1393
2024-02-05 22:40:49,768 [Epoch: 1393 Step: 00093300] Batch Recognition Loss:   0.000198 => Gls Tokens per Sec:     1900 || Batch Translation Loss:   0.041684 => Txt Tokens per Sec:     5234 || Lr: 0.000025
2024-02-05 22:40:52,293 Epoch 1393: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.38 
2024-02-05 22:40:52,293 EPOCH 1394
2024-02-05 22:40:57,053 Epoch 1394: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.16 
2024-02-05 22:40:57,053 EPOCH 1395
2024-02-05 22:40:57,185 [Epoch: 1395 Step: 00093400] Batch Recognition Loss:   0.000141 => Gls Tokens per Sec:     2443 || Batch Translation Loss:   0.011526 => Txt Tokens per Sec:     5229 || Lr: 0.000025
2024-02-05 22:41:02,587 Epoch 1395: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.19 
2024-02-05 22:41:02,587 EPOCH 1396
2024-02-05 22:41:05,389 [Epoch: 1396 Step: 00093500] Batch Recognition Loss:   0.000275 => Gls Tokens per Sec:     1964 || Batch Translation Loss:   0.019556 => Txt Tokens per Sec:     5713 || Lr: 0.000025
2024-02-05 22:41:07,456 Epoch 1396: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.14 
2024-02-05 22:41:07,457 EPOCH 1397
2024-02-05 22:41:12,827 Epoch 1397: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.15 
2024-02-05 22:41:12,827 EPOCH 1398
2024-02-05 22:41:12,903 [Epoch: 1398 Step: 00093600] Batch Recognition Loss:   0.000181 => Gls Tokens per Sec:     2154 || Batch Translation Loss:   0.016303 => Txt Tokens per Sec:     5922 || Lr: 0.000025
2024-02-05 22:41:17,664 Epoch 1398: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.21 
2024-02-05 22:41:17,665 EPOCH 1399
2024-02-05 22:41:20,359 [Epoch: 1399 Step: 00093700] Batch Recognition Loss:   0.000120 => Gls Tokens per Sec:     1983 || Batch Translation Loss:   0.014204 => Txt Tokens per Sec:     5403 || Lr: 0.000025
2024-02-05 22:41:23,085 Epoch 1399: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.61 
2024-02-05 22:41:23,086 EPOCH 1400
2024-02-05 22:41:28,328 [Epoch: 1400 Step: 00093800] Batch Recognition Loss:   0.000092 => Gls Tokens per Sec:     2026 || Batch Translation Loss:   0.012925 => Txt Tokens per Sec:     5606 || Lr: 0.000025
2024-02-05 22:41:28,329 Epoch 1400: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.09 
2024-02-05 22:41:28,329 EPOCH 1401
2024-02-05 22:41:33,390 Epoch 1401: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.11 
2024-02-05 22:41:33,390 EPOCH 1402
2024-02-05 22:41:35,511 [Epoch: 1402 Step: 00093900] Batch Recognition Loss:   0.000109 => Gls Tokens per Sec:     2441 || Batch Translation Loss:   0.014261 => Txt Tokens per Sec:     6739 || Lr: 0.000025
2024-02-05 22:41:38,338 Epoch 1402: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.36 
2024-02-05 22:41:38,338 EPOCH 1403
2024-02-05 22:41:43,526 [Epoch: 1403 Step: 00094000] Batch Recognition Loss:   0.000368 => Gls Tokens per Sec:     2017 || Batch Translation Loss:   0.047341 => Txt Tokens per Sec:     5579 || Lr: 0.000025
2024-02-05 22:41:52,053 Validation result at epoch 1403, step    94000: duration: 8.5255s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.12951	Translation Loss: 94575.17188	PPL: 12659.72949
	Eval Metric: BLEU
	WER 2.82	(DEL: 0.00,	INS: 0.00,	SUB: 2.82)
	BLEU-4 0.49	(BLEU-1: 10.63,	BLEU-2: 3.27,	BLEU-3: 1.11,	BLEU-4: 0.49)
	CHRF 16.93	ROUGE 8.74
2024-02-05 22:41:52,054 Logging Recognition and Translation Outputs
2024-02-05 22:41:52,054 ========================================================================================================================
2024-02-05 22:41:52,054 Logging Sequence: 107_135.00
2024-02-05 22:41:52,055 	Gloss Reference :	A B+C+D+E
2024-02-05 22:41:52,055 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:41:52,055 	Gloss Alignment :	         
2024-02-05 22:41:52,055 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:41:52,055 	Text Reference  :	** *** four indian tennis players such as     
2024-02-05 22:41:52,056 	Text Hypothesis :	if she does not    want   to      be   careful
2024-02-05 22:41:52,056 	Text Alignment  :	I  I   S    S      S      S       S    S      
2024-02-05 22:41:52,056 ========================================================================================================================
2024-02-05 22:41:52,056 Logging Sequence: 105_160.00
2024-02-05 22:41:52,056 	Gloss Reference :	A B+C+D+E
2024-02-05 22:41:52,056 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:41:52,056 	Gloss Alignment :	         
2024-02-05 22:41:52,056 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:41:52,057 	Text Reference  :	** *** ******** **** many people  tweeted congratulatory messages for praggnanandhaa
2024-02-05 22:41:52,057 	Text Hypothesis :	he was actually born and  brought him     in             the      his team          
2024-02-05 22:41:52,057 	Text Alignment  :	I  I   I        I    S    S       S       S              S        S   S             
2024-02-05 22:41:52,057 ========================================================================================================================
2024-02-05 22:41:52,058 Logging Sequence: 134_217.00
2024-02-05 22:41:52,058 	Gloss Reference :	A B+C+D+E
2024-02-05 22:41:52,058 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:41:52,058 	Gloss Alignment :	         
2024-02-05 22:41:52,058 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:41:52,061 	Text Reference  :	******** **** **** ** pm modi  told him that yoga and meditation would  help him concentrate and ****** ****** will      help him win    more  medals  
2024-02-05 22:41:52,061 	Text Hypothesis :	virendra said that 'i am happy but  i   want to   win more       medals as   my  uncle       and father always encourage me   to  attain newer heights'
2024-02-05 22:41:52,061 	Text Alignment  :	I        I    I    I  S  S     S    S   S    S    S   S          S      S    S   S               I      I      S         S    S   S      S     S       
2024-02-05 22:41:52,061 ========================================================================================================================
2024-02-05 22:41:52,061 Logging Sequence: 164_128.00
2024-02-05 22:41:52,062 	Gloss Reference :	A B+C+D+E
2024-02-05 22:41:52,062 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:41:52,062 	Gloss Alignment :	         
2024-02-05 22:41:52,062 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:41:52,064 	Text Reference  :	viacom18 grabbed the digital      rights of telecasting ipl matches for the next 5 years **** ** **** ***** for rs 20500 crore
2024-02-05 22:41:52,064 	Text Hypothesis :	one      of      the broadcasting rights of *********** ipl matches in  the next 5 years went to star india for rs 23575 crore
2024-02-05 22:41:52,064 	Text Alignment  :	S        S           S                      D                       S                    I    I  I    I            S          
2024-02-05 22:41:52,064 ========================================================================================================================
2024-02-05 22:41:52,065 Logging Sequence: 87_136.00
2024-02-05 22:41:52,065 	Gloss Reference :	A B+C+D+E
2024-02-05 22:41:52,065 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:41:52,065 	Gloss Alignment :	         
2024-02-05 22:41:52,065 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:41:52,067 	Text Reference  :	while kaif was in support of kl rahul gambhir was   in support of   ishan kishan due to     his   amazing batting     form     
2024-02-05 22:41:52,067 	Text Hypothesis :	***** **** *** ** ******* ** it is    really  known as your    have faced with   her punjab kings royal   challengers bangalore
2024-02-05 22:41:52,067 	Text Alignment  :	D     D    D   D  D       D  S  S     S       S     S  S       S    S     S      S   S      S     S       S           S        
2024-02-05 22:41:52,067 ========================================================================================================================
2024-02-05 22:41:52,144 Epoch 1403: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.35 
2024-02-05 22:41:52,144 EPOCH 1404
2024-02-05 22:41:57,821 Epoch 1404: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.25 
2024-02-05 22:41:57,821 EPOCH 1405
2024-02-05 22:42:00,483 [Epoch: 1405 Step: 00094100] Batch Recognition Loss:   0.000172 => Gls Tokens per Sec:     1887 || Batch Translation Loss:   0.024741 => Txt Tokens per Sec:     5292 || Lr: 0.000025
2024-02-05 22:42:02,852 Epoch 1405: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.62 
2024-02-05 22:42:02,852 EPOCH 1406
2024-02-05 22:42:07,861 [Epoch: 1406 Step: 00094200] Batch Recognition Loss:   0.005376 => Gls Tokens per Sec:     2056 || Batch Translation Loss:   0.025308 => Txt Tokens per Sec:     5674 || Lr: 0.000025
2024-02-05 22:42:08,099 Epoch 1406: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.07 
2024-02-05 22:42:08,099 EPOCH 1407
2024-02-05 22:42:13,069 Epoch 1407: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.09 
2024-02-05 22:42:13,069 EPOCH 1408
2024-02-05 22:42:15,677 [Epoch: 1408 Step: 00094300] Batch Recognition Loss:   0.000363 => Gls Tokens per Sec:     1903 || Batch Translation Loss:   0.007320 => Txt Tokens per Sec:     5412 || Lr: 0.000025
2024-02-05 22:42:18,491 Epoch 1408: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.71 
2024-02-05 22:42:18,491 EPOCH 1409
2024-02-05 22:42:23,224 [Epoch: 1409 Step: 00094400] Batch Recognition Loss:   0.000225 => Gls Tokens per Sec:     2143 || Batch Translation Loss:   0.048952 => Txt Tokens per Sec:     5948 || Lr: 0.000025
2024-02-05 22:42:23,421 Epoch 1409: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.09 
2024-02-05 22:42:23,421 EPOCH 1410
2024-02-05 22:42:28,909 Epoch 1410: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.53 
2024-02-05 22:42:28,909 EPOCH 1411
2024-02-05 22:42:31,341 [Epoch: 1411 Step: 00094500] Batch Recognition Loss:   0.000219 => Gls Tokens per Sec:     1934 || Batch Translation Loss:   0.020999 => Txt Tokens per Sec:     5657 || Lr: 0.000025
2024-02-05 22:42:33,864 Epoch 1411: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.59 
2024-02-05 22:42:33,865 EPOCH 1412
2024-02-05 22:42:39,035 [Epoch: 1412 Step: 00094600] Batch Recognition Loss:   0.000195 => Gls Tokens per Sec:     1931 || Batch Translation Loss:   0.019183 => Txt Tokens per Sec:     5411 || Lr: 0.000025
2024-02-05 22:42:39,301 Epoch 1412: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.48 
2024-02-05 22:42:39,301 EPOCH 1413
2024-02-05 22:42:44,430 Epoch 1413: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.23 
2024-02-05 22:42:44,431 EPOCH 1414
2024-02-05 22:42:46,818 [Epoch: 1414 Step: 00094700] Batch Recognition Loss:   0.002164 => Gls Tokens per Sec:     1903 || Batch Translation Loss:   0.025357 => Txt Tokens per Sec:     5406 || Lr: 0.000025
2024-02-05 22:42:49,664 Epoch 1414: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.26 
2024-02-05 22:42:49,664 EPOCH 1415
2024-02-05 22:42:54,822 [Epoch: 1415 Step: 00094800] Batch Recognition Loss:   0.000133 => Gls Tokens per Sec:     1904 || Batch Translation Loss:   0.036870 => Txt Tokens per Sec:     5245 || Lr: 0.000025
2024-02-05 22:42:55,237 Epoch 1415: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.39 
2024-02-05 22:42:55,238 EPOCH 1416
2024-02-05 22:43:00,957 Epoch 1416: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.38 
2024-02-05 22:43:00,957 EPOCH 1417
2024-02-05 22:43:03,131 [Epoch: 1417 Step: 00094900] Batch Recognition Loss:   0.000116 => Gls Tokens per Sec:     2063 || Batch Translation Loss:   0.010536 => Txt Tokens per Sec:     5767 || Lr: 0.000025
2024-02-05 22:43:06,282 Epoch 1417: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.33 
2024-02-05 22:43:06,282 EPOCH 1418
2024-02-05 22:43:10,839 [Epoch: 1418 Step: 00095000] Batch Recognition Loss:   0.000107 => Gls Tokens per Sec:     2120 || Batch Translation Loss:   0.017541 => Txt Tokens per Sec:     5804 || Lr: 0.000025
2024-02-05 22:43:11,398 Epoch 1418: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.28 
2024-02-05 22:43:11,398 EPOCH 1419
2024-02-05 22:43:16,731 Epoch 1419: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.13 
2024-02-05 22:43:16,731 EPOCH 1420
2024-02-05 22:43:18,641 [Epoch: 1420 Step: 00095100] Batch Recognition Loss:   0.000143 => Gls Tokens per Sec:     2264 || Batch Translation Loss:   0.006991 => Txt Tokens per Sec:     5929 || Lr: 0.000025
2024-02-05 22:43:22,204 Epoch 1420: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.02 
2024-02-05 22:43:22,205 EPOCH 1421
2024-02-05 22:43:26,856 [Epoch: 1421 Step: 00095200] Batch Recognition Loss:   0.000446 => Gls Tokens per Sec:     2043 || Batch Translation Loss:   0.021334 => Txt Tokens per Sec:     5683 || Lr: 0.000025
2024-02-05 22:43:27,369 Epoch 1421: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.03 
2024-02-05 22:43:27,369 EPOCH 1422
2024-02-05 22:43:32,831 Epoch 1422: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.02 
2024-02-05 22:43:32,831 EPOCH 1423
2024-02-05 22:43:34,540 [Epoch: 1423 Step: 00095300] Batch Recognition Loss:   0.000158 => Gls Tokens per Sec:     2437 || Batch Translation Loss:   0.039250 => Txt Tokens per Sec:     6440 || Lr: 0.000025
2024-02-05 22:43:37,757 Epoch 1423: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.71 
2024-02-05 22:43:37,758 EPOCH 1424
2024-02-05 22:43:41,757 [Epoch: 1424 Step: 00095400] Batch Recognition Loss:   0.000145 => Gls Tokens per Sec:     2336 || Batch Translation Loss:   0.024690 => Txt Tokens per Sec:     6439 || Lr: 0.000025
2024-02-05 22:43:42,392 Epoch 1424: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.27 
2024-02-05 22:43:42,393 EPOCH 1425
2024-02-05 22:43:47,972 Epoch 1425: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.77 
2024-02-05 22:43:47,972 EPOCH 1426
2024-02-05 22:43:50,152 [Epoch: 1426 Step: 00095500] Batch Recognition Loss:   0.000289 => Gls Tokens per Sec:     1837 || Batch Translation Loss:   0.021445 => Txt Tokens per Sec:     5293 || Lr: 0.000025
2024-02-05 22:43:53,473 Epoch 1426: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.72 
2024-02-05 22:43:53,473 EPOCH 1427
2024-02-05 22:43:58,213 [Epoch: 1427 Step: 00095600] Batch Recognition Loss:   0.000145 => Gls Tokens per Sec:     1937 || Batch Translation Loss:   0.013767 => Txt Tokens per Sec:     5315 || Lr: 0.000025
2024-02-05 22:43:58,878 Epoch 1427: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.53 
2024-02-05 22:43:58,878 EPOCH 1428
2024-02-05 22:44:04,303 Epoch 1428: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.33 
2024-02-05 22:44:04,303 EPOCH 1429
2024-02-05 22:44:06,204 [Epoch: 1429 Step: 00095700] Batch Recognition Loss:   0.000156 => Gls Tokens per Sec:     1968 || Batch Translation Loss:   0.012781 => Txt Tokens per Sec:     5521 || Lr: 0.000025
2024-02-05 22:44:09,771 Epoch 1429: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.67 
2024-02-05 22:44:09,771 EPOCH 1430
2024-02-05 22:44:14,259 [Epoch: 1430 Step: 00095800] Batch Recognition Loss:   0.000511 => Gls Tokens per Sec:     2011 || Batch Translation Loss:   0.026285 => Txt Tokens per Sec:     5472 || Lr: 0.000025
2024-02-05 22:44:15,216 Epoch 1430: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.58 
2024-02-05 22:44:15,216 EPOCH 1431
2024-02-05 22:44:20,368 Epoch 1431: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.20 
2024-02-05 22:44:20,368 EPOCH 1432
2024-02-05 22:44:22,013 [Epoch: 1432 Step: 00095900] Batch Recognition Loss:   0.000131 => Gls Tokens per Sec:     2178 || Batch Translation Loss:   0.012770 => Txt Tokens per Sec:     5953 || Lr: 0.000025
2024-02-05 22:44:25,323 Epoch 1432: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.13 
2024-02-05 22:44:25,324 EPOCH 1433
2024-02-05 22:44:29,650 [Epoch: 1433 Step: 00096000] Batch Recognition Loss:   0.000158 => Gls Tokens per Sec:     2072 || Batch Translation Loss:   0.023540 => Txt Tokens per Sec:     5751 || Lr: 0.000025
2024-02-05 22:44:38,165 Validation result at epoch 1433, step    96000: duration: 8.5158s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.15812	Translation Loss: 95439.68750	PPL: 13801.46191
	Eval Metric: BLEU
	WER 2.75	(DEL: 0.00,	INS: 0.00,	SUB: 2.75)
	BLEU-4 0.50	(BLEU-1: 9.63,	BLEU-2: 3.02,	BLEU-3: 1.15,	BLEU-4: 0.50)
	CHRF 16.53	ROUGE 8.19
2024-02-05 22:44:38,166 Logging Recognition and Translation Outputs
2024-02-05 22:44:38,166 ========================================================================================================================
2024-02-05 22:44:38,166 Logging Sequence: 178_77.00
2024-02-05 22:44:38,166 	Gloss Reference :	A B+C+D+E
2024-02-05 22:44:38,166 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:44:38,167 	Gloss Alignment :	         
2024-02-05 22:44:38,167 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:44:38,168 	Text Reference  :	its been more than    two  weeks since the murder on 4th    may  
2024-02-05 22:44:38,168 	Text Hypothesis :	*** **** was  dropped from 3     balls he  had    2  silver medal
2024-02-05 22:44:38,168 	Text Alignment  :	D   D    S    S       S    S     S     S   S      S  S      S    
2024-02-05 22:44:38,168 ========================================================================================================================
2024-02-05 22:44:38,168 Logging Sequence: 118_314.00
2024-02-05 22:44:38,168 	Gloss Reference :	A B+C+D+E
2024-02-05 22:44:38,168 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:44:38,169 	Gloss Alignment :	         
2024-02-05 22:44:38,169 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:44:38,169 	Text Reference  :	** wow even the president had come to watch
2024-02-05 22:44:38,169 	Text Hypothesis :	it is  why  the ********* *** **** ** match
2024-02-05 22:44:38,169 	Text Alignment  :	I  S   S        D         D   D    D  S    
2024-02-05 22:44:38,170 ========================================================================================================================
2024-02-05 22:44:38,170 Logging Sequence: 149_210.00
2024-02-05 22:44:38,170 	Gloss Reference :	A B+C+D+E
2024-02-05 22:44:38,170 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:44:38,170 	Gloss Alignment :	         
2024-02-05 22:44:38,170 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:44:38,172 	Text Reference  :	meanwhile in the t20 world cup pakistan will play new zealand in the first semi final on 9th of        november 2022   
2024-02-05 22:44:38,172 	Text Hypothesis :	********* ** the t20 world cup ******** will **** be  held    in *** ***** **** ***** ** *** australia new      zealand
2024-02-05 22:44:38,172 	Text Alignment  :	D         D                    D             D    S   S          D   D     D    D     D  D   S         S        S      
2024-02-05 22:44:38,172 ========================================================================================================================
2024-02-05 22:44:38,172 Logging Sequence: 155_25.00
2024-02-05 22:44:38,172 	Gloss Reference :	A B+C+D+E
2024-02-05 22:44:38,172 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:44:38,173 	Gloss Alignment :	         
2024-02-05 22:44:38,173 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:44:38,174 	Text Reference  :	this is because taliban overthrew the afghan government and took over   the  country
2024-02-05 22:44:38,174 	Text Hypothesis :	**** i  am      very    grate     to  my     fans       and the  family went viral  
2024-02-05 22:44:38,174 	Text Alignment  :	D    S  S       S       S         S   S      S              S    S      S    S      
2024-02-05 22:44:38,174 ========================================================================================================================
2024-02-05 22:44:38,174 Logging Sequence: 80_16.00
2024-02-05 22:44:38,174 	Gloss Reference :	A B+C+D+E
2024-02-05 22:44:38,174 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:44:38,175 	Gloss Alignment :	         
2024-02-05 22:44:38,175 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:44:38,176 	Text Reference  :	whenever yuzvendra travels anywhere in the ***** world  for matches dhanashree always accompanies him    
2024-02-05 22:44:38,176 	Text Hypothesis :	******** ********* in      one      of the india couple are still   going      on     his         arrival
2024-02-05 22:44:38,176 	Text Alignment  :	D        D         S       S        S      I     S      S   S       S          S      S           S      
2024-02-05 22:44:38,176 ========================================================================================================================
2024-02-05 22:44:39,115 Epoch 1433: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.16 
2024-02-05 22:44:39,115 EPOCH 1434
2024-02-05 22:44:44,613 Epoch 1434: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.58 
2024-02-05 22:44:44,614 EPOCH 1435
2024-02-05 22:44:46,318 [Epoch: 1435 Step: 00096100] Batch Recognition Loss:   0.000791 => Gls Tokens per Sec:     2068 || Batch Translation Loss:   0.050538 => Txt Tokens per Sec:     5715 || Lr: 0.000025
2024-02-05 22:44:49,925 Epoch 1435: Total Training Recognition Loss 0.16  Total Training Translation Loss 1.65 
2024-02-05 22:44:49,925 EPOCH 1436
2024-02-05 22:44:53,982 [Epoch: 1436 Step: 00096200] Batch Recognition Loss:   0.000269 => Gls Tokens per Sec:     2170 || Batch Translation Loss:   0.022913 => Txt Tokens per Sec:     5846 || Lr: 0.000025
2024-02-05 22:44:55,129 Epoch 1436: Total Training Recognition Loss 0.41  Total Training Translation Loss 1.71 
2024-02-05 22:44:55,129 EPOCH 1437
2024-02-05 22:45:00,055 Epoch 1437: Total Training Recognition Loss 0.38  Total Training Translation Loss 1.37 
2024-02-05 22:45:00,055 EPOCH 1438
2024-02-05 22:45:01,516 [Epoch: 1438 Step: 00096300] Batch Recognition Loss:   0.000176 => Gls Tokens per Sec:     2233 || Batch Translation Loss:   0.017168 => Txt Tokens per Sec:     6134 || Lr: 0.000025
2024-02-05 22:45:05,283 Epoch 1438: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.34 
2024-02-05 22:45:05,283 EPOCH 1439
2024-02-05 22:45:09,389 [Epoch: 1439 Step: 00096400] Batch Recognition Loss:   0.000120 => Gls Tokens per Sec:     2080 || Batch Translation Loss:   0.012024 => Txt Tokens per Sec:     5792 || Lr: 0.000025
2024-02-05 22:45:10,262 Epoch 1439: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.57 
2024-02-05 22:45:10,262 EPOCH 1440
2024-02-05 22:45:15,480 Epoch 1440: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.55 
2024-02-05 22:45:15,481 EPOCH 1441
2024-02-05 22:45:16,928 [Epoch: 1441 Step: 00096500] Batch Recognition Loss:   0.000113 => Gls Tokens per Sec:     2212 || Batch Translation Loss:   0.013240 => Txt Tokens per Sec:     6151 || Lr: 0.000025
2024-02-05 22:45:20,451 Epoch 1441: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.35 
2024-02-05 22:45:20,451 EPOCH 1442
2024-02-05 22:45:24,485 [Epoch: 1442 Step: 00096600] Batch Recognition Loss:   0.000164 => Gls Tokens per Sec:     2078 || Batch Translation Loss:   0.037819 => Txt Tokens per Sec:     5717 || Lr: 0.000025
2024-02-05 22:45:25,654 Epoch 1442: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.37 
2024-02-05 22:45:25,654 EPOCH 1443
2024-02-05 22:45:30,977 Epoch 1443: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.28 
2024-02-05 22:45:30,977 EPOCH 1444
2024-02-05 22:45:32,677 [Epoch: 1444 Step: 00096700] Batch Recognition Loss:   0.000152 => Gls Tokens per Sec:     1789 || Batch Translation Loss:   0.026618 => Txt Tokens per Sec:     5095 || Lr: 0.000025
2024-02-05 22:45:36,496 Epoch 1444: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.40 
2024-02-05 22:45:36,496 EPOCH 1445
2024-02-05 22:45:40,338 [Epoch: 1445 Step: 00096800] Batch Recognition Loss:   0.000167 => Gls Tokens per Sec:     2166 || Batch Translation Loss:   0.019952 => Txt Tokens per Sec:     5931 || Lr: 0.000025
2024-02-05 22:45:41,709 Epoch 1445: Total Training Recognition Loss 0.10  Total Training Translation Loss 2.16 
2024-02-05 22:45:41,709 EPOCH 1446
2024-02-05 22:45:47,098 Epoch 1446: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.58 
2024-02-05 22:45:47,099 EPOCH 1447
2024-02-05 22:45:48,409 [Epoch: 1447 Step: 00096900] Batch Recognition Loss:   0.000137 => Gls Tokens per Sec:     2199 || Batch Translation Loss:   0.013349 => Txt Tokens per Sec:     6298 || Lr: 0.000025
2024-02-05 22:45:52,561 Epoch 1447: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.84 
2024-02-05 22:45:52,561 EPOCH 1448
2024-02-05 22:45:56,652 [Epoch: 1448 Step: 00097000] Batch Recognition Loss:   0.000441 => Gls Tokens per Sec:     1971 || Batch Translation Loss:   0.026980 => Txt Tokens per Sec:     5465 || Lr: 0.000025
2024-02-05 22:45:57,763 Epoch 1448: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.17 
2024-02-05 22:45:57,763 EPOCH 1449
2024-02-05 22:46:03,211 Epoch 1449: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.03 
2024-02-05 22:46:03,212 EPOCH 1450
2024-02-05 22:46:04,515 [Epoch: 1450 Step: 00097100] Batch Recognition Loss:   0.000132 => Gls Tokens per Sec:     2012 || Batch Translation Loss:   0.009992 => Txt Tokens per Sec:     5398 || Lr: 0.000025
2024-02-05 22:46:08,571 Epoch 1450: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.36 
2024-02-05 22:46:08,572 EPOCH 1451
2024-02-05 22:46:12,834 [Epoch: 1451 Step: 00097200] Batch Recognition Loss:   0.000164 => Gls Tokens per Sec:     1854 || Batch Translation Loss:   0.020970 => Txt Tokens per Sec:     5119 || Lr: 0.000025
2024-02-05 22:46:14,047 Epoch 1451: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.18 
2024-02-05 22:46:14,047 EPOCH 1452
2024-02-05 22:46:19,424 Epoch 1452: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.24 
2024-02-05 22:46:19,425 EPOCH 1453
2024-02-05 22:46:20,692 [Epoch: 1453 Step: 00097300] Batch Recognition Loss:   0.000106 => Gls Tokens per Sec:     2023 || Batch Translation Loss:   0.054342 => Txt Tokens per Sec:     5416 || Lr: 0.000025
2024-02-05 22:46:24,812 Epoch 1453: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.71 
2024-02-05 22:46:24,813 EPOCH 1454
2024-02-05 22:46:28,547 [Epoch: 1454 Step: 00097400] Batch Recognition Loss:   0.000223 => Gls Tokens per Sec:     2074 || Batch Translation Loss:   0.103388 => Txt Tokens per Sec:     5759 || Lr: 0.000025
2024-02-05 22:46:29,998 Epoch 1454: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.54 
2024-02-05 22:46:29,999 EPOCH 1455
2024-02-05 22:46:35,332 Epoch 1455: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.70 
2024-02-05 22:46:35,332 EPOCH 1456
2024-02-05 22:46:36,330 [Epoch: 1456 Step: 00097500] Batch Recognition Loss:   0.000231 => Gls Tokens per Sec:     2307 || Batch Translation Loss:   0.009671 => Txt Tokens per Sec:     6057 || Lr: 0.000025
2024-02-05 22:46:40,070 Epoch 1456: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.68 
2024-02-05 22:46:40,071 EPOCH 1457
2024-02-05 22:46:44,289 [Epoch: 1457 Step: 00097600] Batch Recognition Loss:   0.000213 => Gls Tokens per Sec:     1797 || Batch Translation Loss:   0.021110 => Txt Tokens per Sec:     5177 || Lr: 0.000025
2024-02-05 22:46:45,561 Epoch 1457: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.85 
2024-02-05 22:46:45,561 EPOCH 1458
2024-02-05 22:46:50,383 Epoch 1458: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.55 
2024-02-05 22:46:50,384 EPOCH 1459
2024-02-05 22:46:51,469 [Epoch: 1459 Step: 00097700] Batch Recognition Loss:   0.000244 => Gls Tokens per Sec:     2068 || Batch Translation Loss:   0.023931 => Txt Tokens per Sec:     5589 || Lr: 0.000025
2024-02-05 22:46:55,871 Epoch 1459: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.33 
2024-02-05 22:46:55,872 EPOCH 1460
2024-02-05 22:46:59,579 [Epoch: 1460 Step: 00097800] Batch Recognition Loss:   0.000182 => Gls Tokens per Sec:     2029 || Batch Translation Loss:   0.006940 => Txt Tokens per Sec:     5429 || Lr: 0.000025
2024-02-05 22:47:01,451 Epoch 1460: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.34 
2024-02-05 22:47:01,451 EPOCH 1461
2024-02-05 22:47:06,262 Epoch 1461: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.28 
2024-02-05 22:47:06,262 EPOCH 1462
2024-02-05 22:47:07,410 [Epoch: 1462 Step: 00097900] Batch Recognition Loss:   0.000119 => Gls Tokens per Sec:     1814 || Batch Translation Loss:   0.013221 => Txt Tokens per Sec:     5088 || Lr: 0.000025
2024-02-05 22:47:11,742 Epoch 1462: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.13 
2024-02-05 22:47:11,742 EPOCH 1463
2024-02-05 22:47:15,131 [Epoch: 1463 Step: 00098000] Batch Recognition Loss:   0.000120 => Gls Tokens per Sec:     2143 || Batch Translation Loss:   0.014373 => Txt Tokens per Sec:     6001 || Lr: 0.000025
2024-02-05 22:47:23,675 Validation result at epoch 1463, step    98000: duration: 8.5440s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.30694	Translation Loss: 95593.66406	PPL: 14015.35547
	Eval Metric: BLEU
	WER 2.90	(DEL: 0.00,	INS: 0.00,	SUB: 2.90)
	BLEU-4 0.60	(BLEU-1: 10.23,	BLEU-2: 3.22,	BLEU-3: 1.23,	BLEU-4: 0.60)
	CHRF 16.67	ROUGE 8.58
2024-02-05 22:47:23,676 Logging Recognition and Translation Outputs
2024-02-05 22:47:23,676 ========================================================================================================================
2024-02-05 22:47:23,676 Logging Sequence: 82_81.00
2024-02-05 22:47:23,676 	Gloss Reference :	A B+C+D+E
2024-02-05 22:47:23,677 	Gloss Hypothesis:	A B+C+D  
2024-02-05 22:47:23,677 	Gloss Alignment :	  S      
2024-02-05 22:47:23,677 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:47:23,678 	Text Reference  :	since the couple were residents of mumbai the mumbai police cyber cell    began  investigating the matter 
2024-02-05 22:47:23,678 	Text Hypothesis :	***** *** ****** **** ********* ** ****** *** ****** ****** then  gujarat titans only          2   matches
2024-02-05 22:47:23,678 	Text Alignment  :	D     D   D      D    D         D  D      D   D      D      S     S       S      S             S   S      
2024-02-05 22:47:23,678 ========================================================================================================================
2024-02-05 22:47:23,678 Logging Sequence: 155_39.00
2024-02-05 22:47:23,678 	Gloss Reference :	A B+C+D+E
2024-02-05 22:47:23,678 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:47:23,679 	Gloss Alignment :	         
2024-02-05 22:47:23,679 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:47:23,681 	Text Reference  :	taliban considers itself as  the  government however there is no    actual government hence icc  would decide on  the inclusion of  the   team       
2024-02-05 22:47:23,681 	Text Hypothesis :	******* ********* ****** and they confirmed  they    had   a  great time   spending   the   game that  it     was b   for       its tough competition
2024-02-05 22:47:23,681 	Text Alignment  :	D       D         D      S   S    S          S       S     S  S     S      S          S     S    S     S      S   S   S         S   S     S          
2024-02-05 22:47:23,681 ========================================================================================================================
2024-02-05 22:47:23,681 Logging Sequence: 144_2.00
2024-02-05 22:47:23,682 	Gloss Reference :	A B+C+D+E  
2024-02-05 22:47:23,682 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-05 22:47:23,682 	Gloss Alignment :	  S        
2024-02-05 22:47:23,682 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:47:23,684 	Text Reference  :	a girl posted a video of herself playing  cricket on    a     village farm on social media the video     has      gone viral
2024-02-05 22:47:23,684 	Text Hypothesis :	* **** ****** * ***** ** ******* whenever anyone  talks about people  but  i  can't  video of  spreading pictures of   yadav
2024-02-05 22:47:23,684 	Text Alignment  :	D D    D      D D     D  D       S        S       S     S     S       S    S  S      S     S   S         S        S    S    
2024-02-05 22:47:23,684 ========================================================================================================================
2024-02-05 22:47:23,684 Logging Sequence: 105_104.00
2024-02-05 22:47:23,684 	Gloss Reference :	A B+C+D+E
2024-02-05 22:47:23,685 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:47:23,685 	Gloss Alignment :	         
2024-02-05 22:47:23,685 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:47:23,686 	Text Reference  :	four year  back when praggnanandhaa was ****** ** ***** **** **** ** 12   year old  
2024-02-05 22:47:23,686 	Text Hypothesis :	**** after her  no   one            was bowled by patel said that he lost the  match
2024-02-05 22:47:23,686 	Text Alignment  :	D    S     S    S    S                  I      I  I     I    I    I  S    S    S    
2024-02-05 22:47:23,686 ========================================================================================================================
2024-02-05 22:47:23,686 Logging Sequence: 71_149.00
2024-02-05 22:47:23,686 	Gloss Reference :	A B+C+D+E
2024-02-05 22:47:23,687 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:47:23,687 	Gloss Alignment :	         
2024-02-05 22:47:23,687 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:47:23,688 	Text Reference  :	his coach sanjay had suggested his  name for   the madhya  pradesh ranji  trophy  team       
2024-02-05 22:47:23,688 	Text Hypothesis :	*** ***** ****** *** just      like to   thank a   picture of      women' cricket association
2024-02-05 22:47:23,688 	Text Alignment  :	D   D     D      D   S         S    S    S     S   S       S       S      S       S          
2024-02-05 22:47:23,688 ========================================================================================================================
2024-02-05 22:47:25,120 Epoch 1463: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.13 
2024-02-05 22:47:25,121 EPOCH 1464
2024-02-05 22:47:30,757 Epoch 1464: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.18 
2024-02-05 22:47:30,758 EPOCH 1465
2024-02-05 22:47:31,604 [Epoch: 1465 Step: 00098100] Batch Recognition Loss:   0.000111 => Gls Tokens per Sec:     2271 || Batch Translation Loss:   0.011766 => Txt Tokens per Sec:     6003 || Lr: 0.000025
2024-02-05 22:47:35,756 Epoch 1465: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.54 
2024-02-05 22:47:35,757 EPOCH 1466
2024-02-05 22:47:39,357 [Epoch: 1466 Step: 00098200] Batch Recognition Loss:   0.000337 => Gls Tokens per Sec:     1973 || Batch Translation Loss:   0.027696 => Txt Tokens per Sec:     5361 || Lr: 0.000025
2024-02-05 22:47:41,216 Epoch 1466: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.57 
2024-02-05 22:47:41,216 EPOCH 1467
2024-02-05 22:47:46,379 Epoch 1467: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.33 
2024-02-05 22:47:46,380 EPOCH 1468
2024-02-05 22:47:47,090 [Epoch: 1468 Step: 00098300] Batch Recognition Loss:   0.000173 => Gls Tokens per Sec:     2481 || Batch Translation Loss:   0.012021 => Txt Tokens per Sec:     6288 || Lr: 0.000025
2024-02-05 22:47:51,591 Epoch 1468: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.76 
2024-02-05 22:47:51,591 EPOCH 1469
2024-02-05 22:47:54,829 [Epoch: 1469 Step: 00098400] Batch Recognition Loss:   0.000136 => Gls Tokens per Sec:     2144 || Batch Translation Loss:   0.021084 => Txt Tokens per Sec:     6025 || Lr: 0.000025
2024-02-05 22:47:56,799 Epoch 1469: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.02 
2024-02-05 22:47:56,800 EPOCH 1470
2024-02-05 22:48:01,986 Epoch 1470: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.22 
2024-02-05 22:48:01,987 EPOCH 1471
2024-02-05 22:48:02,615 [Epoch: 1471 Step: 00098500] Batch Recognition Loss:   0.000827 => Gls Tokens per Sec:     2552 || Batch Translation Loss:   0.012412 => Txt Tokens per Sec:     6914 || Lr: 0.000025
2024-02-05 22:48:07,131 Epoch 1471: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.87 
2024-02-05 22:48:07,132 EPOCH 1472
2024-02-05 22:48:10,472 [Epoch: 1472 Step: 00098600] Batch Recognition Loss:   0.000636 => Gls Tokens per Sec:     2060 || Batch Translation Loss:   0.023123 => Txt Tokens per Sec:     5824 || Lr: 0.000025
2024-02-05 22:48:12,192 Epoch 1472: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.43 
2024-02-05 22:48:12,192 EPOCH 1473
2024-02-05 22:48:17,299 Epoch 1473: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.66 
2024-02-05 22:48:17,300 EPOCH 1474
2024-02-05 22:48:18,056 [Epoch: 1474 Step: 00098700] Batch Recognition Loss:   0.001023 => Gls Tokens per Sec:     1907 || Batch Translation Loss:   0.026187 => Txt Tokens per Sec:     5352 || Lr: 0.000025
2024-02-05 22:48:22,578 Epoch 1474: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.42 
2024-02-05 22:48:22,578 EPOCH 1475
2024-02-05 22:48:26,081 [Epoch: 1475 Step: 00098800] Batch Recognition Loss:   0.000368 => Gls Tokens per Sec:     1919 || Batch Translation Loss:   0.027504 => Txt Tokens per Sec:     5310 || Lr: 0.000025
2024-02-05 22:48:27,993 Epoch 1475: Total Training Recognition Loss 0.19  Total Training Translation Loss 1.33 
2024-02-05 22:48:27,993 EPOCH 1476
2024-02-05 22:48:32,944 Epoch 1476: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.36 
2024-02-05 22:48:32,945 EPOCH 1477
2024-02-05 22:48:33,410 [Epoch: 1477 Step: 00098900] Batch Recognition Loss:   0.000155 => Gls Tokens per Sec:     2757 || Batch Translation Loss:   0.013919 => Txt Tokens per Sec:     7446 || Lr: 0.000025
2024-02-05 22:48:38,247 Epoch 1477: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.51 
2024-02-05 22:48:38,247 EPOCH 1478
2024-02-05 22:48:41,416 [Epoch: 1478 Step: 00099000] Batch Recognition Loss:   0.000241 => Gls Tokens per Sec:     2040 || Batch Translation Loss:   0.018115 => Txt Tokens per Sec:     5689 || Lr: 0.000025
2024-02-05 22:48:43,298 Epoch 1478: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.28 
2024-02-05 22:48:43,299 EPOCH 1479
2024-02-05 22:48:48,711 Epoch 1479: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.27 
2024-02-05 22:48:48,711 EPOCH 1480
2024-02-05 22:48:49,153 [Epoch: 1480 Step: 00099100] Batch Recognition Loss:   0.000168 => Gls Tokens per Sec:     2545 || Batch Translation Loss:   0.012560 => Txt Tokens per Sec:     6736 || Lr: 0.000025
2024-02-05 22:48:53,953 Epoch 1480: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.32 
2024-02-05 22:48:53,953 EPOCH 1481
2024-02-05 22:48:57,281 [Epoch: 1481 Step: 00099200] Batch Recognition Loss:   0.000820 => Gls Tokens per Sec:     1924 || Batch Translation Loss:   0.021516 => Txt Tokens per Sec:     5474 || Lr: 0.000025
2024-02-05 22:48:59,367 Epoch 1481: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.27 
2024-02-05 22:48:59,368 EPOCH 1482
2024-02-05 22:49:04,279 Epoch 1482: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.22 
2024-02-05 22:49:04,279 EPOCH 1483
2024-02-05 22:49:04,709 [Epoch: 1483 Step: 00099300] Batch Recognition Loss:   0.000105 => Gls Tokens per Sec:     2005 || Batch Translation Loss:   0.012029 => Txt Tokens per Sec:     5408 || Lr: 0.000025
2024-02-05 22:49:09,501 Epoch 1483: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.98 
2024-02-05 22:49:09,501 EPOCH 1484
2024-02-05 22:49:12,736 [Epoch: 1484 Step: 00099400] Batch Recognition Loss:   0.000214 => Gls Tokens per Sec:     1929 || Batch Translation Loss:   0.095526 => Txt Tokens per Sec:     5539 || Lr: 0.000025
2024-02-05 22:49:14,699 Epoch 1484: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.07 
2024-02-05 22:49:14,699 EPOCH 1485
2024-02-05 22:49:19,948 Epoch 1485: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.37 
2024-02-05 22:49:19,948 EPOCH 1486
2024-02-05 22:49:20,300 [Epoch: 1486 Step: 00099500] Batch Recognition Loss:   0.000210 => Gls Tokens per Sec:     2279 || Batch Translation Loss:   0.056607 => Txt Tokens per Sec:     6217 || Lr: 0.000025
2024-02-05 22:49:24,827 Epoch 1486: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.59 
2024-02-05 22:49:24,827 EPOCH 1487
2024-02-05 22:49:27,800 [Epoch: 1487 Step: 00099600] Batch Recognition Loss:   0.000427 => Gls Tokens per Sec:     2012 || Batch Translation Loss:   0.015961 => Txt Tokens per Sec:     5544 || Lr: 0.000025
2024-02-05 22:49:30,153 Epoch 1487: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.25 
2024-02-05 22:49:30,153 EPOCH 1488
2024-02-05 22:49:35,052 Epoch 1488: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.19 
2024-02-05 22:49:35,053 EPOCH 1489
2024-02-05 22:49:35,414 [Epoch: 1489 Step: 00099700] Batch Recognition Loss:   0.000174 => Gls Tokens per Sec:     1773 || Batch Translation Loss:   0.046576 => Txt Tokens per Sec:     5704 || Lr: 0.000025
2024-02-05 22:49:40,469 Epoch 1489: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.25 
2024-02-05 22:49:40,470 EPOCH 1490
2024-02-05 22:49:43,248 [Epoch: 1490 Step: 00099800] Batch Recognition Loss:   0.000257 => Gls Tokens per Sec:     2095 || Batch Translation Loss:   0.014389 => Txt Tokens per Sec:     5940 || Lr: 0.000025
2024-02-05 22:49:45,322 Epoch 1490: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.16 
2024-02-05 22:49:45,323 EPOCH 1491
2024-02-05 22:49:50,941 Epoch 1491: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.10 
2024-02-05 22:49:50,942 EPOCH 1492
2024-02-05 22:49:51,179 [Epoch: 1492 Step: 00099900] Batch Recognition Loss:   0.000227 => Gls Tokens per Sec:     2034 || Batch Translation Loss:   0.013526 => Txt Tokens per Sec:     5835 || Lr: 0.000025
2024-02-05 22:49:55,955 Epoch 1492: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.13 
2024-02-05 22:49:55,956 EPOCH 1493
2024-02-05 22:49:58,914 [Epoch: 1493 Step: 00100000] Batch Recognition Loss:   0.000130 => Gls Tokens per Sec:     1914 || Batch Translation Loss:   0.014630 => Txt Tokens per Sec:     5326 || Lr: 0.000025
2024-02-05 22:50:07,574 Validation result at epoch 1493, step   100000: duration: 8.6593s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.19674	Translation Loss: 95708.82031	PPL: 14177.48145
	Eval Metric: BLEU
	WER 3.11	(DEL: 0.00,	INS: 0.00,	SUB: 3.11)
	BLEU-4 0.47	(BLEU-1: 10.33,	BLEU-2: 3.29,	BLEU-3: 1.22,	BLEU-4: 0.47)
	CHRF 16.77	ROUGE 8.73
2024-02-05 22:50:07,575 Logging Recognition and Translation Outputs
2024-02-05 22:50:07,575 ========================================================================================================================
2024-02-05 22:50:07,575 Logging Sequence: 77_60.00
2024-02-05 22:50:07,576 	Gloss Reference :	A B+C+D+E
2024-02-05 22:50:07,576 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:50:07,576 	Gloss Alignment :	         
2024-02-05 22:50:07,576 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:50:07,576 	Text Reference  :	**** *** ******* *** he  remained not out  
2024-02-05 22:50:07,576 	Text Hypothesis :	this was waiting for the start    of  india
2024-02-05 22:50:07,577 	Text Alignment  :	I    I   I       I   S   S        S   S    
2024-02-05 22:50:07,577 ========================================================================================================================
2024-02-05 22:50:07,577 Logging Sequence: 81_8.00
2024-02-05 22:50:07,577 	Gloss Reference :	A B+C+D+E
2024-02-05 22:50:07,577 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:50:07,577 	Gloss Alignment :	         
2024-02-05 22:50:07,578 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:50:07,579 	Text Reference  :	have been involved in a huge controversy in   connection to real estate developer amrapali group since        last     7   years
2024-02-05 22:50:07,579 	Text Hypothesis :	**** **** ******** ** * **** he          also decided    to **** ****** ********* ******** ***** indefinitely postpone ipl 2021 
2024-02-05 22:50:07,579 	Text Alignment  :	D    D    D        D  D D    S           S    S             D    D      D         D        D     S            S        S   S    
2024-02-05 22:50:07,579 ========================================================================================================================
2024-02-05 22:50:07,579 Logging Sequence: 87_63.00
2024-02-05 22:50:07,579 	Gloss Reference :	A B+C+D+E
2024-02-05 22:50:07,579 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:50:07,579 	Gloss Alignment :	         
2024-02-05 22:50:07,580 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:50:07,580 	Text Reference  :	he even hit a massive six  which     helped india    win           the match
2024-02-05 22:50:07,581 	Text Hypothesis :	he **** *** * has     been embroiled in     multiple controversies as  well 
2024-02-05 22:50:07,581 	Text Alignment  :	   D    D   D S       S    S         S      S        S             S   S    
2024-02-05 22:50:07,581 ========================================================================================================================
2024-02-05 22:50:07,581 Logging Sequence: 126_231.00
2024-02-05 22:50:07,581 	Gloss Reference :	A B+C+D+E
2024-02-05 22:50:07,581 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:50:07,581 	Gloss Alignment :	         
2024-02-05 22:50:07,581 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:50:07,583 	Text Reference  :	and will be   creating a      special jersey   with number 8758 commemorating neeraj's throw distance that won him    the gold medal 
2024-02-05 22:50:07,584 	Text Hypothesis :	*** **** this was      india' best    olympics as   india  won  7             medals   -     1        gold 2   silver and 4    bronze
2024-02-05 22:50:07,584 	Text Alignment  :	D   D    S    S        S      S       S        S    S      S    S             S        S     S        S    S   S      S   S    S     
2024-02-05 22:50:07,584 ========================================================================================================================
2024-02-05 22:50:07,584 Logging Sequence: 121_164.00
2024-02-05 22:50:07,584 	Gloss Reference :	A B+C+D+E
2024-02-05 22:50:07,584 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:50:07,584 	Gloss Alignment :	         
2024-02-05 22:50:07,585 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:50:07,585 	Text Reference  :	******* *** * mirabai chanu  returned to  india  on     26   july evening
2024-02-05 22:50:07,585 	Text Hypothesis :	however for 3 weeks   sushil kumar    has evaded arrest from the  police 
2024-02-05 22:50:07,586 	Text Alignment  :	I       I   I S       S      S        S   S      S      S    S    S      
2024-02-05 22:50:07,586 ========================================================================================================================
2024-02-05 22:50:10,055 Epoch 1493: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.05 
2024-02-05 22:50:10,055 EPOCH 1494
2024-02-05 22:50:15,186 Epoch 1494: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.04 
2024-02-05 22:50:15,186 EPOCH 1495
2024-02-05 22:50:15,325 [Epoch: 1495 Step: 00100100] Batch Recognition Loss:   0.000141 => Gls Tokens per Sec:     2319 || Batch Translation Loss:   0.012736 => Txt Tokens per Sec:     6094 || Lr: 0.000025
2024-02-05 22:50:20,770 Epoch 1495: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.01 
2024-02-05 22:50:20,770 EPOCH 1496
2024-02-05 22:50:23,428 [Epoch: 1496 Step: 00100200] Batch Recognition Loss:   0.000211 => Gls Tokens per Sec:     2071 || Batch Translation Loss:   0.018412 => Txt Tokens per Sec:     5793 || Lr: 0.000025
2024-02-05 22:50:25,843 Epoch 1496: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.04 
2024-02-05 22:50:25,843 EPOCH 1497
2024-02-05 22:50:31,331 Epoch 1497: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.01 
2024-02-05 22:50:31,332 EPOCH 1498
2024-02-05 22:50:31,383 [Epoch: 1498 Step: 00100300] Batch Recognition Loss:   0.000141 => Gls Tokens per Sec:     3200 || Batch Translation Loss:   0.010277 => Txt Tokens per Sec:     6840 || Lr: 0.000025
2024-02-05 22:50:35,944 Epoch 1498: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.18 
2024-02-05 22:50:35,944 EPOCH 1499
2024-02-05 22:50:38,934 [Epoch: 1499 Step: 00100400] Batch Recognition Loss:   0.000628 => Gls Tokens per Sec:     1820 || Batch Translation Loss:   0.113200 => Txt Tokens per Sec:     5095 || Lr: 0.000025
2024-02-05 22:50:41,501 Epoch 1499: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.12 
2024-02-05 22:50:41,502 EPOCH 1500
2024-02-05 22:50:46,901 [Epoch: 1500 Step: 00100500] Batch Recognition Loss:   0.000216 => Gls Tokens per Sec:     1967 || Batch Translation Loss:   0.011813 => Txt Tokens per Sec:     5442 || Lr: 0.000025
2024-02-05 22:50:46,902 Epoch 1500: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.94 
2024-02-05 22:50:46,902 EPOCH 1501
2024-02-05 22:50:52,394 Epoch 1501: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.10 
2024-02-05 22:50:52,394 EPOCH 1502
2024-02-05 22:50:54,647 [Epoch: 1502 Step: 00100600] Batch Recognition Loss:   0.000201 => Gls Tokens per Sec:     2345 || Batch Translation Loss:   0.011792 => Txt Tokens per Sec:     6542 || Lr: 0.000025
2024-02-05 22:50:57,246 Epoch 1502: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.39 
2024-02-05 22:50:57,246 EPOCH 1503
2024-02-05 22:51:02,443 [Epoch: 1503 Step: 00100700] Batch Recognition Loss:   0.000736 => Gls Tokens per Sec:     2013 || Batch Translation Loss:   0.029009 => Txt Tokens per Sec:     5568 || Lr: 0.000025
2024-02-05 22:51:02,515 Epoch 1503: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.31 
2024-02-05 22:51:02,515 EPOCH 1504
2024-02-05 22:51:07,557 Epoch 1504: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.29 
2024-02-05 22:51:07,557 EPOCH 1505
2024-02-05 22:51:10,041 [Epoch: 1505 Step: 00100800] Batch Recognition Loss:   0.000527 => Gls Tokens per Sec:     2022 || Batch Translation Loss:   0.025286 => Txt Tokens per Sec:     5640 || Lr: 0.000025
2024-02-05 22:51:12,862 Epoch 1505: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.83 
2024-02-05 22:51:12,862 EPOCH 1506
2024-02-05 22:51:17,903 [Epoch: 1506 Step: 00100900] Batch Recognition Loss:   0.002815 => Gls Tokens per Sec:     2044 || Batch Translation Loss:   0.039830 => Txt Tokens per Sec:     5639 || Lr: 0.000025
2024-02-05 22:51:18,068 Epoch 1506: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.32 
2024-02-05 22:51:18,069 EPOCH 1507
2024-02-05 22:51:23,198 Epoch 1507: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.76 
2024-02-05 22:51:23,199 EPOCH 1508
2024-02-05 22:51:25,312 [Epoch: 1508 Step: 00101000] Batch Recognition Loss:   0.000638 => Gls Tokens per Sec:     2348 || Batch Translation Loss:   0.038888 => Txt Tokens per Sec:     6451 || Lr: 0.000025
2024-02-05 22:51:28,381 Epoch 1508: Total Training Recognition Loss 0.07  Total Training Translation Loss 4.82 
2024-02-05 22:51:28,382 EPOCH 1509
2024-02-05 22:51:33,143 [Epoch: 1509 Step: 00101100] Batch Recognition Loss:   0.000255 => Gls Tokens per Sec:     2130 || Batch Translation Loss:   0.023897 => Txt Tokens per Sec:     5834 || Lr: 0.000025
2024-02-05 22:51:33,412 Epoch 1509: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.66 
2024-02-05 22:51:33,412 EPOCH 1510
2024-02-05 22:51:38,854 Epoch 1510: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.42 
2024-02-05 22:51:38,854 EPOCH 1511
2024-02-05 22:51:41,057 [Epoch: 1511 Step: 00101200] Batch Recognition Loss:   0.000175 => Gls Tokens per Sec:     2135 || Batch Translation Loss:   0.013331 => Txt Tokens per Sec:     5924 || Lr: 0.000025
2024-02-05 22:51:43,803 Epoch 1511: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.33 
2024-02-05 22:51:43,803 EPOCH 1512
2024-02-05 22:51:49,006 [Epoch: 1512 Step: 00101300] Batch Recognition Loss:   0.000160 => Gls Tokens per Sec:     1918 || Batch Translation Loss:   0.013787 => Txt Tokens per Sec:     5314 || Lr: 0.000025
2024-02-05 22:51:49,287 Epoch 1512: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.20 
2024-02-05 22:51:49,287 EPOCH 1513
2024-02-05 22:51:54,090 Epoch 1513: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.10 
2024-02-05 22:51:54,090 EPOCH 1514
2024-02-05 22:51:56,363 [Epoch: 1514 Step: 00101400] Batch Recognition Loss:   0.000205 => Gls Tokens per Sec:     2042 || Batch Translation Loss:   0.015335 => Txt Tokens per Sec:     5615 || Lr: 0.000025
2024-02-05 22:51:59,524 Epoch 1514: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.15 
2024-02-05 22:51:59,525 EPOCH 1515
2024-02-05 22:52:04,009 [Epoch: 1515 Step: 00101500] Batch Recognition Loss:   0.000157 => Gls Tokens per Sec:     2191 || Batch Translation Loss:   0.015971 => Txt Tokens per Sec:     6029 || Lr: 0.000025
2024-02-05 22:52:04,367 Epoch 1515: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.25 
2024-02-05 22:52:04,367 EPOCH 1516
2024-02-05 22:52:09,860 Epoch 1516: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.31 
2024-02-05 22:52:09,861 EPOCH 1517
2024-02-05 22:52:11,939 [Epoch: 1517 Step: 00101600] Batch Recognition Loss:   0.000198 => Gls Tokens per Sec:     2157 || Batch Translation Loss:   0.033893 => Txt Tokens per Sec:     6035 || Lr: 0.000025
2024-02-05 22:52:14,646 Epoch 1517: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.19 
2024-02-05 22:52:14,646 EPOCH 1518
2024-02-05 22:52:19,447 [Epoch: 1518 Step: 00101700] Batch Recognition Loss:   0.000324 => Gls Tokens per Sec:     2013 || Batch Translation Loss:   0.028819 => Txt Tokens per Sec:     5574 || Lr: 0.000025
2024-02-05 22:52:19,924 Epoch 1518: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.12 
2024-02-05 22:52:19,924 EPOCH 1519
2024-02-05 22:52:24,959 Epoch 1519: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.16 
2024-02-05 22:52:24,960 EPOCH 1520
2024-02-05 22:52:26,908 [Epoch: 1520 Step: 00101800] Batch Recognition Loss:   0.000191 => Gls Tokens per Sec:     2166 || Batch Translation Loss:   0.018464 => Txt Tokens per Sec:     5913 || Lr: 0.000025
2024-02-05 22:52:30,268 Epoch 1520: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.16 
2024-02-05 22:52:30,268 EPOCH 1521
2024-02-05 22:52:34,724 [Epoch: 1521 Step: 00101900] Batch Recognition Loss:   0.000098 => Gls Tokens per Sec:     2132 || Batch Translation Loss:   0.013674 => Txt Tokens per Sec:     5907 || Lr: 0.000025
2024-02-05 22:52:35,213 Epoch 1521: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.21 
2024-02-05 22:52:35,213 EPOCH 1522
2024-02-05 22:52:40,794 Epoch 1522: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.11 
2024-02-05 22:52:40,795 EPOCH 1523
2024-02-05 22:52:42,568 [Epoch: 1523 Step: 00102000] Batch Recognition Loss:   0.000127 => Gls Tokens per Sec:     2290 || Batch Translation Loss:   0.011199 => Txt Tokens per Sec:     6286 || Lr: 0.000025
2024-02-05 22:52:51,094 Validation result at epoch 1523, step   102000: duration: 8.5262s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.21011	Translation Loss: 96323.50000	PPL: 15075.17773
	Eval Metric: BLEU
	WER 3.11	(DEL: 0.00,	INS: 0.00,	SUB: 3.11)
	BLEU-4 0.59	(BLEU-1: 10.45,	BLEU-2: 3.20,	BLEU-3: 1.20,	BLEU-4: 0.59)
	CHRF 16.62	ROUGE 8.94
2024-02-05 22:52:51,095 Logging Recognition and Translation Outputs
2024-02-05 22:52:51,095 ========================================================================================================================
2024-02-05 22:52:51,095 Logging Sequence: 168_63.00
2024-02-05 22:52:51,096 	Gloss Reference :	A B+C+D+E
2024-02-05 22:52:51,096 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:52:51,096 	Gloss Alignment :	         
2024-02-05 22:52:51,096 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:52:51,097 	Text Reference  :	kohli has always   been very    protective of vamika and never lets the ********* paps     see   her     
2024-02-05 22:52:51,097 	Text Hypothesis :	***** the document will contain details    of ****** *** ***** **** the company's finances staff salaries
2024-02-05 22:52:51,097 	Text Alignment  :	D     S   S        S    S       S             D      D   D     D        I         S        S     S       
2024-02-05 22:52:51,097 ========================================================================================================================
2024-02-05 22:52:51,098 Logging Sequence: 94_136.00
2024-02-05 22:52:51,098 	Gloss Reference :	A B+C+D+E
2024-02-05 22:52:51,098 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:52:51,098 	Gloss Alignment :	         
2024-02-05 22:52:51,098 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:52:51,099 	Text Reference  :	***** *** * similarly on    12th november 2023  england-pakistan match   as         scheduled in west bengal
2024-02-05 22:52:51,099 	Text Hypothesis :	india won a bronze    medal at   the      world deaf             cricket tournament held      in **** 2019  
2024-02-05 22:52:51,100 	Text Alignment  :	I     I   I S         S     S    S        S     S                S       S          S            D    S     
2024-02-05 22:52:51,100 ========================================================================================================================
2024-02-05 22:52:51,100 Logging Sequence: 51_152.00
2024-02-05 22:52:51,100 	Gloss Reference :	A B+C+D+E
2024-02-05 22:52:51,100 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:52:51,100 	Gloss Alignment :	         
2024-02-05 22:52:51,100 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:52:51,101 	Text Reference  :	australian players did  this to    celebrate their victory in     their      cultural way   
2024-02-05 22:52:51,101 	Text Hypothesis :	********** ******* they are  aware that      even  south   africa bangladesh south    africa
2024-02-05 22:52:51,101 	Text Alignment  :	D          D       S    S    S     S         S     S       S      S          S        S     
2024-02-05 22:52:51,102 ========================================================================================================================
2024-02-05 22:52:51,102 Logging Sequence: 112_2.00
2024-02-05 22:52:51,102 	Gloss Reference :	A B+C+D+E  
2024-02-05 22:52:51,102 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-05 22:52:51,102 	Gloss Alignment :	  S        
2024-02-05 22:52:51,102 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:52:51,104 	Text Reference  :	earlier the bcci had announced that the ipl in 2022      will have 2 more teams compared to the    8   teams earlier
2024-02-05 22:52:51,104 	Text Hypothesis :	******* *** **** *** ********* **** in  ipl ** currently his  have * **** ***** ******** ** mumbai and 1     death  
2024-02-05 22:52:51,104 	Text Alignment  :	D       D   D    D   D         D    S       D  S         S         D D    D     D        D  S      S   S     S      
2024-02-05 22:52:51,104 ========================================================================================================================
2024-02-05 22:52:51,104 Logging Sequence: 183_99.00
2024-02-05 22:52:51,104 	Gloss Reference :	A B+C+D+E
2024-02-05 22:52:51,104 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:52:51,104 	Gloss Alignment :	         
2024-02-05 22:52:51,105 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:52:51,106 	Text Reference  :	yuvraj decided to use 'keech' as the middle name as       it's his wife's surname the   post was widely shared  
2024-02-05 22:52:51,106 	Text Hypothesis :	****** ******* ** *** ******* ** *** he     is   survived by   his ****** wife    their son  and a      daughter
2024-02-05 22:52:51,106 	Text Alignment  :	D      D       D  D   D       D  D   S      S    S        S        D      S       S     S    S   S      S       
2024-02-05 22:52:51,106 ========================================================================================================================
2024-02-05 22:52:54,460 Epoch 1523: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.37 
2024-02-05 22:52:54,460 EPOCH 1524
2024-02-05 22:52:59,378 [Epoch: 1524 Step: 00102100] Batch Recognition Loss:   0.000156 => Gls Tokens per Sec:     1900 || Batch Translation Loss:   0.013332 => Txt Tokens per Sec:     5260 || Lr: 0.000025
2024-02-05 22:52:59,960 Epoch 1524: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.59 
2024-02-05 22:52:59,960 EPOCH 1525
2024-02-05 22:53:04,776 Epoch 1525: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.45 
2024-02-05 22:53:04,776 EPOCH 1526
2024-02-05 22:53:06,657 [Epoch: 1526 Step: 00102200] Batch Recognition Loss:   0.000130 => Gls Tokens per Sec:     2128 || Batch Translation Loss:   0.020259 => Txt Tokens per Sec:     6221 || Lr: 0.000025
2024-02-05 22:53:09,867 Epoch 1526: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.52 
2024-02-05 22:53:09,868 EPOCH 1527
2024-02-05 22:53:14,632 [Epoch: 1527 Step: 00102300] Batch Recognition Loss:   0.000249 => Gls Tokens per Sec:     1927 || Batch Translation Loss:   0.015543 => Txt Tokens per Sec:     5289 || Lr: 0.000025
2024-02-05 22:53:15,327 Epoch 1527: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.27 
2024-02-05 22:53:15,327 EPOCH 1528
2024-02-05 22:53:20,521 Epoch 1528: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.20 
2024-02-05 22:53:20,522 EPOCH 1529
2024-02-05 22:53:22,329 [Epoch: 1529 Step: 00102400] Batch Recognition Loss:   0.000615 => Gls Tokens per Sec:     2072 || Batch Translation Loss:   0.010686 => Txt Tokens per Sec:     5484 || Lr: 0.000025
2024-02-05 22:53:25,957 Epoch 1529: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.18 
2024-02-05 22:53:25,957 EPOCH 1530
2024-02-05 22:53:30,606 [Epoch: 1530 Step: 00102500] Batch Recognition Loss:   0.001666 => Gls Tokens per Sec:     1940 || Batch Translation Loss:   0.020171 => Txt Tokens per Sec:     5396 || Lr: 0.000025
2024-02-05 22:53:31,404 Epoch 1530: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.28 
2024-02-05 22:53:31,405 EPOCH 1531
2024-02-05 22:53:36,632 Epoch 1531: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.28 
2024-02-05 22:53:36,633 EPOCH 1532
2024-02-05 22:53:38,268 [Epoch: 1532 Step: 00102600] Batch Recognition Loss:   0.000175 => Gls Tokens per Sec:     2252 || Batch Translation Loss:   0.017561 => Txt Tokens per Sec:     6445 || Lr: 0.000025
2024-02-05 22:53:41,592 Epoch 1532: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.93 
2024-02-05 22:53:41,592 EPOCH 1533
2024-02-05 22:53:45,981 [Epoch: 1533 Step: 00102700] Batch Recognition Loss:   0.000374 => Gls Tokens per Sec:     2019 || Batch Translation Loss:   0.027505 => Txt Tokens per Sec:     5663 || Lr: 0.000025
2024-02-05 22:53:46,811 Epoch 1533: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.86 
2024-02-05 22:53:46,811 EPOCH 1534
2024-02-05 22:53:51,392 Epoch 1534: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.86 
2024-02-05 22:53:51,393 EPOCH 1535
2024-02-05 22:53:52,977 [Epoch: 1535 Step: 00102800] Batch Recognition Loss:   0.000238 => Gls Tokens per Sec:     2222 || Batch Translation Loss:   0.012268 => Txt Tokens per Sec:     5896 || Lr: 0.000025
2024-02-05 22:53:56,755 Epoch 1535: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.38 
2024-02-05 22:53:56,755 EPOCH 1536
2024-02-05 22:54:00,856 [Epoch: 1536 Step: 00102900] Batch Recognition Loss:   0.000126 => Gls Tokens per Sec:     2122 || Batch Translation Loss:   0.011733 => Txt Tokens per Sec:     5845 || Lr: 0.000025
2024-02-05 22:54:01,737 Epoch 1536: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.61 
2024-02-05 22:54:01,737 EPOCH 1537
2024-02-05 22:54:07,080 Epoch 1537: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.39 
2024-02-05 22:54:07,081 EPOCH 1538
2024-02-05 22:54:08,504 [Epoch: 1538 Step: 00103000] Batch Recognition Loss:   0.000236 => Gls Tokens per Sec:     2365 || Batch Translation Loss:   0.013755 => Txt Tokens per Sec:     6551 || Lr: 0.000025
2024-02-05 22:54:11,795 Epoch 1538: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.16 
2024-02-05 22:54:11,795 EPOCH 1539
2024-02-05 22:54:16,258 [Epoch: 1539 Step: 00103100] Batch Recognition Loss:   0.000196 => Gls Tokens per Sec:     1914 || Batch Translation Loss:   0.013055 => Txt Tokens per Sec:     5403 || Lr: 0.000025
2024-02-05 22:54:17,171 Epoch 1539: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.17 
2024-02-05 22:54:17,171 EPOCH 1540
2024-02-05 22:54:22,423 Epoch 1540: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.32 
2024-02-05 22:54:22,423 EPOCH 1541
2024-02-05 22:54:23,936 [Epoch: 1541 Step: 00103200] Batch Recognition Loss:   0.000159 => Gls Tokens per Sec:     2116 || Batch Translation Loss:   0.019684 => Txt Tokens per Sec:     6081 || Lr: 0.000025
2024-02-05 22:54:27,645 Epoch 1541: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.14 
2024-02-05 22:54:27,645 EPOCH 1542
2024-02-05 22:54:31,720 [Epoch: 1542 Step: 00103300] Batch Recognition Loss:   0.000146 => Gls Tokens per Sec:     2057 || Batch Translation Loss:   0.014630 => Txt Tokens per Sec:     5760 || Lr: 0.000025
2024-02-05 22:54:32,688 Epoch 1542: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.07 
2024-02-05 22:54:32,688 EPOCH 1543
2024-02-05 22:54:37,974 Epoch 1543: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.16 
2024-02-05 22:54:37,975 EPOCH 1544
2024-02-05 22:54:39,428 [Epoch: 1544 Step: 00103400] Batch Recognition Loss:   0.000184 => Gls Tokens per Sec:     2026 || Batch Translation Loss:   0.008997 => Txt Tokens per Sec:     5545 || Lr: 0.000025
2024-02-05 22:54:42,967 Epoch 1544: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.23 
2024-02-05 22:54:42,968 EPOCH 1545
2024-02-05 22:54:47,236 [Epoch: 1545 Step: 00103500] Batch Recognition Loss:   0.000147 => Gls Tokens per Sec:     1926 || Batch Translation Loss:   0.013509 => Txt Tokens per Sec:     5353 || Lr: 0.000025
2024-02-05 22:54:48,358 Epoch 1545: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.28 
2024-02-05 22:54:48,358 EPOCH 1546
2024-02-05 22:54:53,175 Epoch 1546: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.96 
2024-02-05 22:54:53,175 EPOCH 1547
2024-02-05 22:54:54,386 [Epoch: 1547 Step: 00103600] Batch Recognition Loss:   0.000178 => Gls Tokens per Sec:     2301 || Batch Translation Loss:   0.019015 => Txt Tokens per Sec:     6164 || Lr: 0.000025
2024-02-05 22:54:58,379 Epoch 1547: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.78 
2024-02-05 22:54:58,380 EPOCH 1548
2024-02-05 22:55:02,142 [Epoch: 1548 Step: 00103700] Batch Recognition Loss:   0.000172 => Gls Tokens per Sec:     2143 || Batch Translation Loss:   0.018120 => Txt Tokens per Sec:     5853 || Lr: 0.000025
2024-02-05 22:55:03,506 Epoch 1548: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.63 
2024-02-05 22:55:03,506 EPOCH 1549
2024-02-05 22:55:08,968 Epoch 1549: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.66 
2024-02-05 22:55:08,968 EPOCH 1550
2024-02-05 22:55:10,247 [Epoch: 1550 Step: 00103800] Batch Recognition Loss:   0.006753 => Gls Tokens per Sec:     2129 || Batch Translation Loss:   0.042967 => Txt Tokens per Sec:     6032 || Lr: 0.000025
2024-02-05 22:55:14,213 Epoch 1550: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.57 
2024-02-05 22:55:14,213 EPOCH 1551
2024-02-05 22:55:18,196 [Epoch: 1551 Step: 00103900] Batch Recognition Loss:   0.000161 => Gls Tokens per Sec:     1984 || Batch Translation Loss:   0.024993 => Txt Tokens per Sec:     5467 || Lr: 0.000025
2024-02-05 22:55:19,544 Epoch 1551: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.80 
2024-02-05 22:55:19,544 EPOCH 1552
2024-02-05 22:55:24,225 Epoch 1552: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.57 
2024-02-05 22:55:24,226 EPOCH 1553
2024-02-05 22:55:25,651 [Epoch: 1553 Step: 00104000] Batch Recognition Loss:   0.000436 => Gls Tokens per Sec:     1727 || Batch Translation Loss:   0.010774 => Txt Tokens per Sec:     4516 || Lr: 0.000025
2024-02-05 22:55:33,998 Validation result at epoch 1553, step   104000: duration: 8.3468s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.16600	Translation Loss: 95590.35156	PPL: 14010.71777
	Eval Metric: BLEU
	WER 3.18	(DEL: 0.00,	INS: 0.00,	SUB: 3.18)
	BLEU-4 0.43	(BLEU-1: 10.31,	BLEU-2: 3.06,	BLEU-3: 1.06,	BLEU-4: 0.43)
	CHRF 16.86	ROUGE 8.53
2024-02-05 22:55:33,999 Logging Recognition and Translation Outputs
2024-02-05 22:55:34,000 ========================================================================================================================
2024-02-05 22:55:34,000 Logging Sequence: 87_164.00
2024-02-05 22:55:34,000 	Gloss Reference :	A B+C+D+E
2024-02-05 22:55:34,000 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:55:34,000 	Gloss Alignment :	         
2024-02-05 22:55:34,000 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:55:34,001 	Text Reference  :	** *** **** when gambhir was heading back from the field
2024-02-05 22:55:34,001 	Text Hypothesis :	do you know that there   was ******* a    huge fan 2021 
2024-02-05 22:55:34,001 	Text Alignment  :	I  I   I    S    S           D       S    S    S   S    
2024-02-05 22:55:34,001 ========================================================================================================================
2024-02-05 22:55:34,002 Logging Sequence: 67_73.00
2024-02-05 22:55:34,002 	Gloss Reference :	A B+C+D+E
2024-02-05 22:55:34,002 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:55:34,002 	Gloss Alignment :	         
2024-02-05 22:55:34,002 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:55:34,003 	Text Reference  :	*** **** **** ***** in  his tweet he   also said 
2024-02-05 22:55:34,003 	Text Hypothesis :	the just like there has a   just  like the  other
2024-02-05 22:55:34,003 	Text Alignment  :	I   I    I    I     S   S   S     S    S    S    
2024-02-05 22:55:34,003 ========================================================================================================================
2024-02-05 22:55:34,003 Logging Sequence: 128_98.00
2024-02-05 22:55:34,003 	Gloss Reference :	A B+C+D+E
2024-02-05 22:55:34,003 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:55:34,004 	Gloss Alignment :	         
2024-02-05 22:55:34,004 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:55:34,005 	Text Reference  :	with 8 wickets and 43 balls remaining they won the   match   in   such        a   short         time  
2024-02-05 22:55:34,005 	Text Hypothesis :	**** * ******* *** ** ***** ********* **** but their captain kane williamson' key contributions helped
2024-02-05 22:55:34,005 	Text Alignment  :	D    D D       D   D  D     D         D    S   S     S       S    S           S   S             S     
2024-02-05 22:55:34,005 ========================================================================================================================
2024-02-05 22:55:34,005 Logging Sequence: 58_112.00
2024-02-05 22:55:34,005 	Gloss Reference :	A B+C+D+E
2024-02-05 22:55:34,005 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:55:34,006 	Gloss Alignment :	         
2024-02-05 22:55:34,006 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:55:34,007 	Text Reference  :	**** what a   proud moment for  india have a   look at     all   the athletes    and   their accomplisments
2024-02-05 22:55:34,007 	Text Hypothesis :	2022 was  the first time   that india **** won 16   medals since our deaflympics debut in    1965          
2024-02-05 22:55:34,007 	Text Alignment  :	I    S    S   S     S      S          D    S   S    S      S     S   S           S     S     S             
2024-02-05 22:55:34,008 ========================================================================================================================
2024-02-05 22:55:34,008 Logging Sequence: 51_152.00
2024-02-05 22:55:34,008 	Gloss Reference :	A B+C+D+E
2024-02-05 22:55:34,008 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 22:55:34,008 	Gloss Alignment :	         
2024-02-05 22:55:34,008 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 22:55:34,009 	Text Reference  :	australian players did  this     to celebrate their   victory   in their cultural way        
2024-02-05 22:55:34,009 	Text Hypothesis :	********** they    were supposed to play      against australia in ***** the      semi-finals
2024-02-05 22:55:34,009 	Text Alignment  :	D          S       S    S           S         S       S            D     S        S          
2024-02-05 22:55:34,010 ========================================================================================================================
2024-02-05 22:55:34,013 Training ended since there were no improvements inthe last learning rate step: 0.000025
2024-02-05 22:55:34,014 Best validation result at step    52000:   0.96 eval_metric.
2024-02-05 22:56:03,230 ------------------------------------------------------------
2024-02-05 22:56:03,231 [DEV] partition [RECOGNITION] experiment [BW]: 1
2024-02-05 22:56:11,668 finished in 8.4365s 
2024-02-05 22:56:11,669 ************************************************************
2024-02-05 22:56:11,669 [DEV] partition [RECOGNITION] results:
	New Best CTC Decode Beam Size: 1
	WER 3.46	(DEL: 0.00,	INS: 0.00,	SUB: 3.46)
2024-02-05 22:56:11,669 ************************************************************
2024-02-05 22:56:11,669 ------------------------------------------------------------
2024-02-05 22:56:11,669 [DEV] partition [RECOGNITION] experiment [BW]: 2
2024-02-05 22:56:19,869 finished in 8.2000s 
2024-02-05 22:56:19,869 ------------------------------------------------------------
2024-02-05 22:56:19,869 [DEV] partition [RECOGNITION] experiment [BW]: 3
2024-02-05 22:56:28,024 finished in 8.1555s 
2024-02-05 22:56:28,025 ------------------------------------------------------------
2024-02-05 22:56:28,025 [DEV] partition [RECOGNITION] experiment [BW]: 4
2024-02-05 22:56:36,228 finished in 8.2024s 
2024-02-05 22:56:36,228 ------------------------------------------------------------
2024-02-05 22:56:36,228 [DEV] partition [RECOGNITION] experiment [BW]: 5
2024-02-05 22:56:44,344 finished in 8.1167s 
2024-02-05 22:56:44,345 ------------------------------------------------------------
2024-02-05 22:56:44,345 [DEV] partition [RECOGNITION] experiment [BW]: 6
2024-02-05 22:56:52,628 finished in 8.2823s 
2024-02-05 22:56:52,628 ------------------------------------------------------------
2024-02-05 22:56:52,629 [DEV] partition [RECOGNITION] experiment [BW]: 7
2024-02-05 22:57:00,914 finished in 8.2853s 
2024-02-05 22:57:00,915 ------------------------------------------------------------
2024-02-05 22:57:00,915 [DEV] partition [RECOGNITION] experiment [BW]: 8
2024-02-05 22:57:09,363 finished in 8.4476s 
2024-02-05 22:57:09,364 ------------------------------------------------------------
2024-02-05 22:57:09,364 [DEV] partition [RECOGNITION] experiment [BW]: 9
2024-02-05 22:57:17,634 finished in 8.2695s 
2024-02-05 22:57:17,634 ------------------------------------------------------------
2024-02-05 22:57:17,634 [DEV] partition [RECOGNITION] experiment [BW]: 10
2024-02-05 22:57:25,992 finished in 8.3576s 
2024-02-05 22:57:25,993 ============================================================
2024-02-05 22:57:33,949 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 1 and Alpha: -1
	BLEU-4 0.96	(BLEU-1: 12.10,	BLEU-2: 4.17,	BLEU-3: 1.82,	BLEU-4: 0.96)
	CHRF 17.34	ROUGE 10.18
2024-02-05 22:57:33,949 ------------------------------------------------------------
2024-02-05 22:58:36,474 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 2 and Alpha: -1
	BLEU-4 1.08	(BLEU-1: 11.01,	BLEU-2: 4.04,	BLEU-3: 1.94,	BLEU-4: 1.08)
	CHRF 16.93	ROUGE 9.83
2024-02-05 22:58:36,475 ------------------------------------------------------------
2024-02-05 22:58:56,602 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 2 and Alpha: 1
	BLEU-4 1.10	(BLEU-1: 11.37,	BLEU-2: 4.17,	BLEU-3: 1.98,	BLEU-4: 1.10)
	CHRF 17.14	ROUGE 9.97
2024-02-05 22:58:56,602 ------------------------------------------------------------
2024-02-05 22:59:06,557 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 2 and Alpha: 2
	BLEU-4 1.11	(BLEU-1: 11.55,	BLEU-2: 4.25,	BLEU-3: 2.01,	BLEU-4: 1.11)
	CHRF 17.22	ROUGE 10.01
2024-02-05 22:59:06,557 ------------------------------------------------------------
2024-02-05 22:59:16,483 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 2 and Alpha: 3
	BLEU-4 1.12	(BLEU-1: 11.59,	BLEU-2: 4.27,	BLEU-3: 2.03,	BLEU-4: 1.12)
	CHRF 17.21	ROUGE 10.00
2024-02-05 22:59:16,483 ------------------------------------------------------------
2024-02-05 22:59:36,492 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 2 and Alpha: 5
	BLEU-4 1.12	(BLEU-1: 11.63,	BLEU-2: 4.30,	BLEU-3: 2.04,	BLEU-4: 1.12)
	CHRF 17.22	ROUGE 9.97
2024-02-05 22:59:36,492 ------------------------------------------------------------
2024-02-05 23:14:12,668 ************************************************************
2024-02-05 23:14:12,669 [DEV] partition [Recognition & Translation] results:
	Best CTC Decode Beam Size: 1
	Best Translation Beam Size: 2 and Alpha: 5
	WER 3.46	(DEL: 0.00,	INS: 0.00,	SUB: 3.46)
	BLEU-4 1.12	(BLEU-1: 11.63,	BLEU-2: 4.30,	BLEU-3: 2.04,	BLEU-4: 1.12)
	CHRF 17.22	ROUGE 9.97
2024-02-05 23:14:12,669 ************************************************************
2024-02-05 23:14:22,869 [TEST] partition [Recognition & Translation] results:
	Best CTC Decode Beam Size: 1
	Best Translation Beam Size: 2 and Alpha: 5
	WER 2.47	(DEL: 0.00,	INS: 0.00,	SUB: 2.47)
	BLEU-4 0.61	(BLEU-1: 11.08,	BLEU-2: 3.44,	BLEU-3: 1.26,	BLEU-4: 0.61)
	CHRF 16.87	ROUGE 9.07
2024-02-05 23:14:22,869 ************************************************************
