2024-02-04 10:00:38,080 Hello! This is Joey-NMT.
2024-02-04 10:00:38,096 Total params: 25639944
2024-02-04 10:00:38,097 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'gloss_output_layer.bias', 'gloss_output_layer.weight', 'sgn_embed.ln.bias', 'sgn_embed.ln.weight', 'sgn_embed.norm.norm.bias', 'sgn_embed.norm.norm.weight', 'txt_embed.norm.norm.bias', 'txt_embed.norm.norm.weight']
2024-02-04 10:00:39,260 cfg.name                           : sign_experiment
2024-02-04 10:00:39,261 cfg.data.data_path                 : ./data/Sports_dataset/9/
2024-02-04 10:00:39,261 cfg.data.version                   : phoenix_2014_trans
2024-02-04 10:00:39,261 cfg.data.sgn                       : sign
2024-02-04 10:00:39,261 cfg.data.txt                       : text
2024-02-04 10:00:39,261 cfg.data.gls                       : gloss
2024-02-04 10:00:39,261 cfg.data.train                     : excel_data.train
2024-02-04 10:00:39,262 cfg.data.dev                       : excel_data.dev
2024-02-04 10:00:39,262 cfg.data.test                      : excel_data.test
2024-02-04 10:00:39,262 cfg.data.feature_size              : 2560
2024-02-04 10:00:39,262 cfg.data.level                     : word
2024-02-04 10:00:39,262 cfg.data.txt_lowercase             : True
2024-02-04 10:00:39,262 cfg.data.max_sent_length           : 500
2024-02-04 10:00:39,262 cfg.data.random_train_subset       : -1
2024-02-04 10:00:39,262 cfg.data.random_dev_subset         : -1
2024-02-04 10:00:39,263 cfg.testing.recognition_beam_sizes : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
2024-02-04 10:00:39,263 cfg.testing.translation_beam_sizes : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
2024-02-04 10:00:39,263 cfg.testing.translation_beam_alphas : [-1, 0, 1, 2, 3, 4, 5]
2024-02-04 10:00:39,263 cfg.training.reset_best_ckpt       : False
2024-02-04 10:00:39,263 cfg.training.reset_scheduler       : False
2024-02-04 10:00:39,263 cfg.training.reset_optimizer       : False
2024-02-04 10:00:39,263 cfg.training.random_seed           : 42
2024-02-04 10:00:39,263 cfg.training.model_dir             : ./sign_sample_model/fold9/32head/128batch
2024-02-04 10:00:39,264 cfg.training.recognition_loss_weight : 1.0
2024-02-04 10:00:39,264 cfg.training.translation_loss_weight : 1.0
2024-02-04 10:00:39,264 cfg.training.eval_metric           : bleu
2024-02-04 10:00:39,264 cfg.training.optimizer             : adam
2024-02-04 10:00:39,264 cfg.training.learning_rate         : 0.0001
2024-02-04 10:00:39,264 cfg.training.batch_size            : 128
2024-02-04 10:00:39,264 cfg.training.num_valid_log         : 5
2024-02-04 10:00:39,264 cfg.training.epochs                : 50000
2024-02-04 10:00:39,265 cfg.training.early_stopping_metric : eval_metric
2024-02-04 10:00:39,265 cfg.training.batch_type            : sentence
2024-02-04 10:00:39,265 cfg.training.translation_normalization : batch
2024-02-04 10:00:39,265 cfg.training.eval_recognition_beam_size : 1
2024-02-04 10:00:39,265 cfg.training.eval_translation_beam_size : 1
2024-02-04 10:00:39,265 cfg.training.eval_translation_beam_alpha : -1
2024-02-04 10:00:39,265 cfg.training.overwrite             : True
2024-02-04 10:00:39,265 cfg.training.shuffle               : True
2024-02-04 10:00:39,266 cfg.training.use_cuda              : True
2024-02-04 10:00:39,266 cfg.training.translation_max_output_length : 40
2024-02-04 10:00:39,266 cfg.training.keep_last_ckpts       : 1
2024-02-04 10:00:39,266 cfg.training.batch_multiplier      : 1
2024-02-04 10:00:39,266 cfg.training.logging_freq          : 100
2024-02-04 10:00:39,266 cfg.training.validation_freq       : 2000
2024-02-04 10:00:39,266 cfg.training.betas                 : [0.9, 0.998]
2024-02-04 10:00:39,266 cfg.training.scheduling            : plateau
2024-02-04 10:00:39,267 cfg.training.learning_rate_min     : 1e-08
2024-02-04 10:00:39,267 cfg.training.weight_decay          : 0.0001
2024-02-04 10:00:39,267 cfg.training.patience              : 12
2024-02-04 10:00:39,267 cfg.training.decrease_factor       : 0.5
2024-02-04 10:00:39,267 cfg.training.label_smoothing       : 0.0
2024-02-04 10:00:39,267 cfg.model.initializer              : xavier
2024-02-04 10:00:39,267 cfg.model.bias_initializer         : zeros
2024-02-04 10:00:39,268 cfg.model.init_gain                : 1.0
2024-02-04 10:00:39,268 cfg.model.embed_initializer        : xavier
2024-02-04 10:00:39,268 cfg.model.embed_init_gain          : 1.0
2024-02-04 10:00:39,268 cfg.model.tied_softmax             : True
2024-02-04 10:00:39,268 cfg.model.encoder.type             : transformer
2024-02-04 10:00:39,268 cfg.model.encoder.num_layers       : 3
2024-02-04 10:00:39,268 cfg.model.encoder.num_heads        : 32
2024-02-04 10:00:39,268 cfg.model.encoder.embeddings.embedding_dim : 512
2024-02-04 10:00:39,269 cfg.model.encoder.embeddings.scale : False
2024-02-04 10:00:39,269 cfg.model.encoder.embeddings.dropout : 0.1
2024-02-04 10:00:39,269 cfg.model.encoder.embeddings.norm_type : batch
2024-02-04 10:00:39,269 cfg.model.encoder.embeddings.activation_type : softsign
2024-02-04 10:00:39,269 cfg.model.encoder.hidden_size      : 512
2024-02-04 10:00:39,269 cfg.model.encoder.ff_size          : 2048
2024-02-04 10:00:39,269 cfg.model.encoder.dropout          : 0.1
2024-02-04 10:00:39,270 cfg.model.decoder.type             : transformer
2024-02-04 10:00:39,270 cfg.model.decoder.num_layers       : 3
2024-02-04 10:00:39,270 cfg.model.decoder.num_heads        : 32
2024-02-04 10:00:39,270 cfg.model.decoder.embeddings.embedding_dim : 512
2024-02-04 10:00:39,270 cfg.model.decoder.embeddings.scale : False
2024-02-04 10:00:39,270 cfg.model.decoder.embeddings.dropout : 0.1
2024-02-04 10:00:39,270 cfg.model.decoder.embeddings.norm_type : batch
2024-02-04 10:00:39,270 cfg.model.decoder.embeddings.activation_type : softsign
2024-02-04 10:00:39,271 cfg.model.decoder.hidden_size      : 512
2024-02-04 10:00:39,271 cfg.model.decoder.ff_size          : 2048
2024-02-04 10:00:39,271 cfg.model.decoder.dropout          : 0.1
2024-02-04 10:00:39,271 Data set sizes: 
	train 2126,
	valid 708,
	test 706
2024-02-04 10:00:39,271 First training example:
	[GLS] A B C D E
	[TXT] although new zealand was disappointed to faltered at the finals against australia they did well throughout the tournament
2024-02-04 10:00:39,271 First 10 words (gls): (0) <si> (1) <unk> (2) <pad> (3) A (4) B (5) C (6) D (7) E
2024-02-04 10:00:39,271 First 10 words (txt): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) the (5) and (6) to (7) a (8) in (9) of
2024-02-04 10:00:39,271 Number of unique glosses (types): 8
2024-02-04 10:00:39,272 Number of unique words (types): 4397
2024-02-04 10:00:39,272 SignModel(
	encoder=TransformerEncoder(num_layers=3, num_heads=32),
	decoder=TransformerDecoder(num_layers=3, num_heads=32),
	sgn_embed=SpatialEmbeddings(embedding_dim=512, input_size=2560),
	txt_embed=Embeddings(embedding_dim=512, vocab_size=4397))
2024-02-04 10:00:39,283 EPOCH 1
2024-02-04 10:01:01,146 Epoch   1: Total Training Recognition Loss 118.16  Total Training Translation Loss 1801.04 
2024-02-04 10:01:01,147 EPOCH 2
2024-02-04 10:01:11,911 Epoch   2: Total Training Recognition Loss 31.04  Total Training Translation Loss 1637.42 
2024-02-04 10:01:11,911 EPOCH 3
2024-02-04 10:01:22,590 Epoch   3: Total Training Recognition Loss 13.94  Total Training Translation Loss 1558.06 
2024-02-04 10:01:22,591 EPOCH 4
2024-02-04 10:01:32,975 Epoch   4: Total Training Recognition Loss 7.50  Total Training Translation Loss 1521.35 
2024-02-04 10:01:32,976 EPOCH 5
2024-02-04 10:01:43,865 Epoch   5: Total Training Recognition Loss 3.80  Total Training Translation Loss 1504.49 
2024-02-04 10:01:43,865 EPOCH 6
2024-02-04 10:01:53,358 [Epoch: 006 Step: 00000100] Batch Recognition Loss:   0.078185 => Gls Tokens per Sec:      985 || Batch Translation Loss: 115.870621 => Txt Tokens per Sec:     2717 || Lr: 0.000100
2024-02-04 10:01:55,097 Epoch   6: Total Training Recognition Loss 1.77  Total Training Translation Loss 1493.21 
2024-02-04 10:01:55,097 EPOCH 7
2024-02-04 10:02:06,135 Epoch   7: Total Training Recognition Loss 0.89  Total Training Translation Loss 1480.19 
2024-02-04 10:02:06,136 EPOCH 8
2024-02-04 10:02:16,673 Epoch   8: Total Training Recognition Loss 0.53  Total Training Translation Loss 1462.61 
2024-02-04 10:02:16,673 EPOCH 9
2024-02-04 10:02:27,271 Epoch   9: Total Training Recognition Loss 0.37  Total Training Translation Loss 1439.56 
2024-02-04 10:02:27,272 EPOCH 10
2024-02-04 10:02:38,009 Epoch  10: Total Training Recognition Loss 0.28  Total Training Translation Loss 1408.54 
2024-02-04 10:02:38,010 EPOCH 11
2024-02-04 10:02:48,336 Epoch  11: Total Training Recognition Loss 0.22  Total Training Translation Loss 1377.16 
2024-02-04 10:02:48,336 EPOCH 12
2024-02-04 10:02:56,629 [Epoch: 012 Step: 00000200] Batch Recognition Loss:   0.009744 => Gls Tokens per Sec:      973 || Batch Translation Loss: 100.577057 => Txt Tokens per Sec:     2718 || Lr: 0.000100
2024-02-04 10:02:58,877 Epoch  12: Total Training Recognition Loss 0.19  Total Training Translation Loss 1347.89 
2024-02-04 10:02:58,877 EPOCH 13
2024-02-04 10:03:09,253 Epoch  13: Total Training Recognition Loss 0.17  Total Training Translation Loss 1315.40 
2024-02-04 10:03:09,254 EPOCH 14
2024-02-04 10:03:20,333 Epoch  14: Total Training Recognition Loss 0.16  Total Training Translation Loss 1287.33 
2024-02-04 10:03:20,334 EPOCH 15
2024-02-04 10:03:31,260 Epoch  15: Total Training Recognition Loss 0.15  Total Training Translation Loss 1257.44 
2024-02-04 10:03:31,261 EPOCH 16
2024-02-04 10:03:41,736 Epoch  16: Total Training Recognition Loss 0.15  Total Training Translation Loss 1230.49 
2024-02-04 10:03:41,737 EPOCH 17
2024-02-04 10:03:52,359 Epoch  17: Total Training Recognition Loss 0.13  Total Training Translation Loss 1205.62 
2024-02-04 10:03:52,359 EPOCH 18
2024-02-04 10:03:57,251 [Epoch: 018 Step: 00000300] Batch Recognition Loss:   0.008241 => Gls Tokens per Sec:     1440 || Batch Translation Loss:  90.899498 => Txt Tokens per Sec:     3857 || Lr: 0.000100
2024-02-04 10:04:02,676 Epoch  18: Total Training Recognition Loss 0.14  Total Training Translation Loss 1178.80 
2024-02-04 10:04:02,676 EPOCH 19
2024-02-04 10:04:13,161 Epoch  19: Total Training Recognition Loss 0.14  Total Training Translation Loss 1155.36 
2024-02-04 10:04:13,161 EPOCH 20
2024-02-04 10:04:23,832 Epoch  20: Total Training Recognition Loss 0.15  Total Training Translation Loss 1137.33 
2024-02-04 10:04:23,833 EPOCH 21
2024-02-04 10:04:34,172 Epoch  21: Total Training Recognition Loss 0.17  Total Training Translation Loss 1122.87 
2024-02-04 10:04:34,172 EPOCH 22
2024-02-04 10:04:44,785 Epoch  22: Total Training Recognition Loss 0.16  Total Training Translation Loss 1091.42 
2024-02-04 10:04:44,786 EPOCH 23
2024-02-04 10:04:55,355 Epoch  23: Total Training Recognition Loss 0.15  Total Training Translation Loss 1066.45 
2024-02-04 10:04:55,356 EPOCH 24
2024-02-04 10:05:02,811 [Epoch: 024 Step: 00000400] Batch Recognition Loss:   0.005637 => Gls Tokens per Sec:      739 || Batch Translation Loss:  63.268379 => Txt Tokens per Sec:     2259 || Lr: 0.000100
2024-02-04 10:05:05,961 Epoch  24: Total Training Recognition Loss 0.16  Total Training Translation Loss 1047.83 
2024-02-04 10:05:05,961 EPOCH 25
2024-02-04 10:05:16,465 Epoch  25: Total Training Recognition Loss 0.15  Total Training Translation Loss 1027.05 
2024-02-04 10:05:16,466 EPOCH 26
2024-02-04 10:05:26,725 Epoch  26: Total Training Recognition Loss 0.20  Total Training Translation Loss 997.39 
2024-02-04 10:05:26,726 EPOCH 27
2024-02-04 10:05:37,234 Epoch  27: Total Training Recognition Loss 0.15  Total Training Translation Loss 990.13 
2024-02-04 10:05:37,235 EPOCH 28
2024-02-04 10:05:47,617 Epoch  28: Total Training Recognition Loss 0.18  Total Training Translation Loss 960.94 
2024-02-04 10:05:47,617 EPOCH 29
2024-02-04 10:05:58,113 Epoch  29: Total Training Recognition Loss 0.18  Total Training Translation Loss 935.69 
2024-02-04 10:05:58,113 EPOCH 30
2024-02-04 10:06:01,464 [Epoch: 030 Step: 00000500] Batch Recognition Loss:   0.015788 => Gls Tokens per Sec:     1338 || Batch Translation Loss:  70.010872 => Txt Tokens per Sec:     3738 || Lr: 0.000100
2024-02-04 10:06:08,795 Epoch  30: Total Training Recognition Loss 0.16  Total Training Translation Loss 917.73 
2024-02-04 10:06:08,796 EPOCH 31
2024-02-04 10:06:19,580 Epoch  31: Total Training Recognition Loss 0.19  Total Training Translation Loss 897.44 
2024-02-04 10:06:19,581 EPOCH 32
2024-02-04 10:06:29,948 Epoch  32: Total Training Recognition Loss 0.18  Total Training Translation Loss 881.66 
2024-02-04 10:06:29,949 EPOCH 33
2024-02-04 10:06:40,282 Epoch  33: Total Training Recognition Loss 0.19  Total Training Translation Loss 864.00 
2024-02-04 10:06:40,283 EPOCH 34
2024-02-04 10:06:50,756 Epoch  34: Total Training Recognition Loss 0.19  Total Training Translation Loss 856.09 
2024-02-04 10:06:50,757 EPOCH 35
2024-02-04 10:07:01,219 Epoch  35: Total Training Recognition Loss 0.23  Total Training Translation Loss 832.34 
2024-02-04 10:07:01,219 EPOCH 36
2024-02-04 10:07:06,108 [Epoch: 036 Step: 00000600] Batch Recognition Loss:   0.009054 => Gls Tokens per Sec:      604 || Batch Translation Loss:  50.857498 => Txt Tokens per Sec:     1823 || Lr: 0.000100
2024-02-04 10:07:11,696 Epoch  36: Total Training Recognition Loss 0.22  Total Training Translation Loss 810.72 
2024-02-04 10:07:11,696 EPOCH 37
2024-02-04 10:07:22,105 Epoch  37: Total Training Recognition Loss 0.23  Total Training Translation Loss 794.22 
2024-02-04 10:07:22,106 EPOCH 38
2024-02-04 10:07:32,714 Epoch  38: Total Training Recognition Loss 0.22  Total Training Translation Loss 779.25 
2024-02-04 10:07:32,714 EPOCH 39
2024-02-04 10:07:43,044 Epoch  39: Total Training Recognition Loss 0.23  Total Training Translation Loss 761.33 
2024-02-04 10:07:43,045 EPOCH 40
2024-02-04 10:07:53,385 Epoch  40: Total Training Recognition Loss 0.26  Total Training Translation Loss 739.54 
2024-02-04 10:07:53,386 EPOCH 41
2024-02-04 10:08:03,690 Epoch  41: Total Training Recognition Loss 0.25  Total Training Translation Loss 728.81 
2024-02-04 10:08:03,691 EPOCH 42
2024-02-04 10:08:04,421 [Epoch: 042 Step: 00000700] Batch Recognition Loss:   0.017775 => Gls Tokens per Sec:     2633 || Batch Translation Loss:  21.009197 => Txt Tokens per Sec:     6913 || Lr: 0.000100
2024-02-04 10:08:14,199 Epoch  42: Total Training Recognition Loss 0.28  Total Training Translation Loss 709.80 
2024-02-04 10:08:14,199 EPOCH 43
2024-02-04 10:08:24,818 Epoch  43: Total Training Recognition Loss 0.27  Total Training Translation Loss 694.99 
2024-02-04 10:08:24,818 EPOCH 44
2024-02-04 10:08:35,406 Epoch  44: Total Training Recognition Loss 0.32  Total Training Translation Loss 674.50 
2024-02-04 10:08:35,407 EPOCH 45
2024-02-04 10:08:46,000 Epoch  45: Total Training Recognition Loss 0.29  Total Training Translation Loss 662.51 
2024-02-04 10:08:46,000 EPOCH 46
2024-02-04 10:08:56,440 Epoch  46: Total Training Recognition Loss 0.29  Total Training Translation Loss 646.40 
2024-02-04 10:08:56,441 EPOCH 47
2024-02-04 10:09:06,630 Epoch  47: Total Training Recognition Loss 0.33  Total Training Translation Loss 628.15 
2024-02-04 10:09:06,630 EPOCH 48
2024-02-04 10:09:06,846 [Epoch: 048 Step: 00000800] Batch Recognition Loss:   0.015050 => Gls Tokens per Sec:     2991 || Batch Translation Loss:  36.261951 => Txt Tokens per Sec:     8173 || Lr: 0.000100
2024-02-04 10:09:17,041 Epoch  48: Total Training Recognition Loss 0.30  Total Training Translation Loss 611.53 
2024-02-04 10:09:17,041 EPOCH 49
2024-02-04 10:09:27,836 Epoch  49: Total Training Recognition Loss 0.30  Total Training Translation Loss 600.21 
2024-02-04 10:09:27,836 EPOCH 50
2024-02-04 10:09:38,505 Epoch  50: Total Training Recognition Loss 0.32  Total Training Translation Loss 581.84 
2024-02-04 10:09:38,505 EPOCH 51
2024-02-04 10:09:49,058 Epoch  51: Total Training Recognition Loss 0.31  Total Training Translation Loss 569.26 
2024-02-04 10:09:49,058 EPOCH 52
2024-02-04 10:09:59,188 Epoch  52: Total Training Recognition Loss 0.37  Total Training Translation Loss 554.00 
2024-02-04 10:09:59,188 EPOCH 53
2024-02-04 10:10:09,377 [Epoch: 053 Step: 00000900] Batch Recognition Loss:   0.014952 => Gls Tokens per Sec:      981 || Batch Translation Loss:  27.069624 => Txt Tokens per Sec:     2699 || Lr: 0.000100
2024-02-04 10:10:09,729 Epoch  53: Total Training Recognition Loss 0.35  Total Training Translation Loss 545.20 
2024-02-04 10:10:09,729 EPOCH 54
2024-02-04 10:10:19,939 Epoch  54: Total Training Recognition Loss 0.38  Total Training Translation Loss 532.51 
2024-02-04 10:10:19,939 EPOCH 55
2024-02-04 10:10:30,425 Epoch  55: Total Training Recognition Loss 0.36  Total Training Translation Loss 522.19 
2024-02-04 10:10:30,426 EPOCH 56
2024-02-04 10:10:40,599 Epoch  56: Total Training Recognition Loss 0.37  Total Training Translation Loss 506.34 
2024-02-04 10:10:40,600 EPOCH 57
2024-02-04 10:10:51,000 Epoch  57: Total Training Recognition Loss 0.38  Total Training Translation Loss 487.63 
2024-02-04 10:10:51,000 EPOCH 58
2024-02-04 10:11:01,366 Epoch  58: Total Training Recognition Loss 0.35  Total Training Translation Loss 471.92 
2024-02-04 10:11:01,366 EPOCH 59
2024-02-04 10:11:11,148 [Epoch: 059 Step: 00001000] Batch Recognition Loss:   0.015786 => Gls Tokens per Sec:      891 || Batch Translation Loss:  24.821310 => Txt Tokens per Sec:     2501 || Lr: 0.000100
2024-02-04 10:11:11,843 Epoch  59: Total Training Recognition Loss 0.35  Total Training Translation Loss 455.97 
2024-02-04 10:11:11,843 EPOCH 60
2024-02-04 10:11:22,354 Epoch  60: Total Training Recognition Loss 0.36  Total Training Translation Loss 439.90 
2024-02-04 10:11:22,355 EPOCH 61
2024-02-04 10:11:32,518 Epoch  61: Total Training Recognition Loss 0.38  Total Training Translation Loss 429.46 
2024-02-04 10:11:32,518 EPOCH 62
2024-02-04 10:11:42,852 Epoch  62: Total Training Recognition Loss 0.36  Total Training Translation Loss 417.45 
2024-02-04 10:11:42,853 EPOCH 63
2024-02-04 10:11:53,398 Epoch  63: Total Training Recognition Loss 0.39  Total Training Translation Loss 410.18 
2024-02-04 10:11:53,398 EPOCH 64
2024-02-04 10:12:03,891 Epoch  64: Total Training Recognition Loss 0.36  Total Training Translation Loss 392.65 
2024-02-04 10:12:03,891 EPOCH 65
2024-02-04 10:12:13,048 [Epoch: 065 Step: 00001100] Batch Recognition Loss:   0.015246 => Gls Tokens per Sec:      812 || Batch Translation Loss:  22.792015 => Txt Tokens per Sec:     2169 || Lr: 0.000100
2024-02-04 10:12:14,732 Epoch  65: Total Training Recognition Loss 0.35  Total Training Translation Loss 376.74 
2024-02-04 10:12:14,733 EPOCH 66
2024-02-04 10:12:25,941 Epoch  66: Total Training Recognition Loss 0.40  Total Training Translation Loss 369.14 
2024-02-04 10:12:25,941 EPOCH 67
2024-02-04 10:12:36,462 Epoch  67: Total Training Recognition Loss 0.37  Total Training Translation Loss 356.26 
2024-02-04 10:12:36,463 EPOCH 68
2024-02-04 10:12:46,966 Epoch  68: Total Training Recognition Loss 0.38  Total Training Translation Loss 345.57 
2024-02-04 10:12:46,966 EPOCH 69
2024-02-04 10:12:57,546 Epoch  69: Total Training Recognition Loss 0.38  Total Training Translation Loss 332.52 
2024-02-04 10:12:57,547 EPOCH 70
2024-02-04 10:13:07,822 Epoch  70: Total Training Recognition Loss 0.37  Total Training Translation Loss 324.25 
2024-02-04 10:13:07,823 EPOCH 71
2024-02-04 10:13:12,833 [Epoch: 071 Step: 00001200] Batch Recognition Loss:   0.018532 => Gls Tokens per Sec:     1278 || Batch Translation Loss:  11.914307 => Txt Tokens per Sec:     3422 || Lr: 0.000100
2024-02-04 10:13:18,083 Epoch  71: Total Training Recognition Loss 0.37  Total Training Translation Loss 313.48 
2024-02-04 10:13:18,083 EPOCH 72
2024-02-04 10:13:28,350 Epoch  72: Total Training Recognition Loss 0.35  Total Training Translation Loss 302.31 
2024-02-04 10:13:28,351 EPOCH 73
2024-02-04 10:13:38,817 Epoch  73: Total Training Recognition Loss 0.37  Total Training Translation Loss 296.63 
2024-02-04 10:13:38,818 EPOCH 74
2024-02-04 10:13:49,427 Epoch  74: Total Training Recognition Loss 0.37  Total Training Translation Loss 279.95 
2024-02-04 10:13:49,428 EPOCH 75
2024-02-04 10:13:59,688 Epoch  75: Total Training Recognition Loss 0.36  Total Training Translation Loss 270.05 
2024-02-04 10:13:59,688 EPOCH 76
2024-02-04 10:14:10,405 Epoch  76: Total Training Recognition Loss 0.37  Total Training Translation Loss 261.06 
2024-02-04 10:14:10,406 EPOCH 77
2024-02-04 10:14:15,773 [Epoch: 077 Step: 00001300] Batch Recognition Loss:   0.014514 => Gls Tokens per Sec:      908 || Batch Translation Loss:  16.925461 => Txt Tokens per Sec:     2623 || Lr: 0.000100
2024-02-04 10:14:20,827 Epoch  77: Total Training Recognition Loss 0.39  Total Training Translation Loss 256.52 
2024-02-04 10:14:20,828 EPOCH 78
2024-02-04 10:14:31,154 Epoch  78: Total Training Recognition Loss 0.35  Total Training Translation Loss 240.67 
2024-02-04 10:14:31,155 EPOCH 79
2024-02-04 10:14:41,621 Epoch  79: Total Training Recognition Loss 0.33  Total Training Translation Loss 228.29 
2024-02-04 10:14:41,622 EPOCH 80
2024-02-04 10:14:52,052 Epoch  80: Total Training Recognition Loss 0.34  Total Training Translation Loss 217.87 
2024-02-04 10:14:52,052 EPOCH 81
2024-02-04 10:15:02,376 Epoch  81: Total Training Recognition Loss 0.33  Total Training Translation Loss 209.69 
2024-02-04 10:15:02,377 EPOCH 82
2024-02-04 10:15:13,174 Epoch  82: Total Training Recognition Loss 0.33  Total Training Translation Loss 203.71 
2024-02-04 10:15:13,175 EPOCH 83
2024-02-04 10:15:15,779 [Epoch: 083 Step: 00001400] Batch Recognition Loss:   0.026537 => Gls Tokens per Sec:     1475 || Batch Translation Loss:  14.046933 => Txt Tokens per Sec:     3660 || Lr: 0.000100
2024-02-04 10:15:23,740 Epoch  83: Total Training Recognition Loss 0.31  Total Training Translation Loss 197.64 
2024-02-04 10:15:23,740 EPOCH 84
2024-02-04 10:15:34,323 Epoch  84: Total Training Recognition Loss 0.33  Total Training Translation Loss 189.66 
2024-02-04 10:15:34,324 EPOCH 85
2024-02-04 10:15:45,041 Epoch  85: Total Training Recognition Loss 0.31  Total Training Translation Loss 185.51 
2024-02-04 10:15:45,042 EPOCH 86
2024-02-04 10:15:55,395 Epoch  86: Total Training Recognition Loss 0.34  Total Training Translation Loss 179.91 
2024-02-04 10:15:55,396 EPOCH 87
2024-02-04 10:16:05,879 Epoch  87: Total Training Recognition Loss 0.34  Total Training Translation Loss 170.39 
2024-02-04 10:16:05,880 EPOCH 88
2024-02-04 10:16:15,955 Epoch  88: Total Training Recognition Loss 0.31  Total Training Translation Loss 162.99 
2024-02-04 10:16:15,956 EPOCH 89
2024-02-04 10:16:20,486 [Epoch: 089 Step: 00001500] Batch Recognition Loss:   0.025319 => Gls Tokens per Sec:      510 || Batch Translation Loss:  12.547327 => Txt Tokens per Sec:     1642 || Lr: 0.000100
2024-02-04 10:16:26,350 Epoch  89: Total Training Recognition Loss 0.30  Total Training Translation Loss 154.52 
2024-02-04 10:16:26,351 EPOCH 90
2024-02-04 10:16:36,696 Epoch  90: Total Training Recognition Loss 0.30  Total Training Translation Loss 148.87 
2024-02-04 10:16:36,696 EPOCH 91
2024-02-04 10:16:47,300 Epoch  91: Total Training Recognition Loss 0.29  Total Training Translation Loss 141.97 
2024-02-04 10:16:47,301 EPOCH 92
2024-02-04 10:16:57,801 Epoch  92: Total Training Recognition Loss 0.27  Total Training Translation Loss 135.16 
2024-02-04 10:16:57,802 EPOCH 93
2024-02-04 10:17:08,121 Epoch  93: Total Training Recognition Loss 0.29  Total Training Translation Loss 130.72 
2024-02-04 10:17:08,121 EPOCH 94
2024-02-04 10:17:18,755 Epoch  94: Total Training Recognition Loss 0.27  Total Training Translation Loss 124.47 
2024-02-04 10:17:18,756 EPOCH 95
2024-02-04 10:17:19,269 [Epoch: 095 Step: 00001600] Batch Recognition Loss:   0.015994 => Gls Tokens per Sec:     2494 || Batch Translation Loss:   8.098663 => Txt Tokens per Sec:     7395 || Lr: 0.000100
2024-02-04 10:17:29,138 Epoch  95: Total Training Recognition Loss 0.26  Total Training Translation Loss 118.31 
2024-02-04 10:17:29,139 EPOCH 96
2024-02-04 10:17:39,583 Epoch  96: Total Training Recognition Loss 0.25  Total Training Translation Loss 112.79 
2024-02-04 10:17:39,584 EPOCH 97
2024-02-04 10:17:50,035 Epoch  97: Total Training Recognition Loss 0.26  Total Training Translation Loss 109.53 
2024-02-04 10:17:50,036 EPOCH 98
2024-02-04 10:18:00,446 Epoch  98: Total Training Recognition Loss 0.23  Total Training Translation Loss 103.93 
2024-02-04 10:18:00,447 EPOCH 99
2024-02-04 10:18:11,111 Epoch  99: Total Training Recognition Loss 0.24  Total Training Translation Loss 98.75 
2024-02-04 10:18:11,112 EPOCH 100
2024-02-04 10:18:21,543 [Epoch: 100 Step: 00001700] Batch Recognition Loss:   0.011631 => Gls Tokens per Sec:     1019 || Batch Translation Loss:   5.106503 => Txt Tokens per Sec:     2829 || Lr: 0.000100
2024-02-04 10:18:21,544 Epoch 100: Total Training Recognition Loss 0.24  Total Training Translation Loss 98.72 
2024-02-04 10:18:21,544 EPOCH 101
2024-02-04 10:18:31,880 Epoch 101: Total Training Recognition Loss 0.24  Total Training Translation Loss 94.53 
2024-02-04 10:18:31,880 EPOCH 102
2024-02-04 10:18:42,272 Epoch 102: Total Training Recognition Loss 0.23  Total Training Translation Loss 94.43 
2024-02-04 10:18:42,273 EPOCH 103
2024-02-04 10:18:52,755 Epoch 103: Total Training Recognition Loss 0.26  Total Training Translation Loss 90.35 
2024-02-04 10:18:52,755 EPOCH 104
2024-02-04 10:19:03,211 Epoch 104: Total Training Recognition Loss 0.23  Total Training Translation Loss 85.82 
2024-02-04 10:19:03,211 EPOCH 105
2024-02-04 10:19:13,578 Epoch 105: Total Training Recognition Loss 0.23  Total Training Translation Loss 79.66 
2024-02-04 10:19:13,578 EPOCH 106
2024-02-04 10:19:23,644 [Epoch: 106 Step: 00001800] Batch Recognition Loss:   0.012129 => Gls Tokens per Sec:      929 || Batch Translation Loss:   2.099618 => Txt Tokens per Sec:     2579 || Lr: 0.000100
2024-02-04 10:19:24,202 Epoch 106: Total Training Recognition Loss 0.22  Total Training Translation Loss 76.91 
2024-02-04 10:19:24,202 EPOCH 107
2024-02-04 10:19:34,649 Epoch 107: Total Training Recognition Loss 0.20  Total Training Translation Loss 73.89 
2024-02-04 10:19:34,649 EPOCH 108
2024-02-04 10:19:45,130 Epoch 108: Total Training Recognition Loss 0.21  Total Training Translation Loss 69.88 
2024-02-04 10:19:45,130 EPOCH 109
2024-02-04 10:19:55,579 Epoch 109: Total Training Recognition Loss 0.20  Total Training Translation Loss 67.66 
2024-02-04 10:19:55,580 EPOCH 110
2024-02-04 10:20:06,172 Epoch 110: Total Training Recognition Loss 0.18  Total Training Translation Loss 64.15 
2024-02-04 10:20:06,172 EPOCH 111
2024-02-04 10:20:16,533 Epoch 111: Total Training Recognition Loss 0.19  Total Training Translation Loss 61.49 
2024-02-04 10:20:16,533 EPOCH 112
2024-02-04 10:20:23,804 [Epoch: 112 Step: 00001900] Batch Recognition Loss:   0.010913 => Gls Tokens per Sec:     1145 || Batch Translation Loss:   3.926379 => Txt Tokens per Sec:     3136 || Lr: 0.000100
2024-02-04 10:20:27,269 Epoch 112: Total Training Recognition Loss 0.18  Total Training Translation Loss 60.23 
2024-02-04 10:20:27,270 EPOCH 113
2024-02-04 10:20:37,541 Epoch 113: Total Training Recognition Loss 0.18  Total Training Translation Loss 57.85 
2024-02-04 10:20:37,542 EPOCH 114
2024-02-04 10:20:48,020 Epoch 114: Total Training Recognition Loss 0.18  Total Training Translation Loss 55.74 
2024-02-04 10:20:48,021 EPOCH 115
2024-02-04 10:20:58,805 Epoch 115: Total Training Recognition Loss 0.17  Total Training Translation Loss 54.07 
2024-02-04 10:20:58,805 EPOCH 116
2024-02-04 10:21:09,237 Epoch 116: Total Training Recognition Loss 0.16  Total Training Translation Loss 52.40 
2024-02-04 10:21:09,238 EPOCH 117
2024-02-04 10:21:19,645 Epoch 117: Total Training Recognition Loss 0.15  Total Training Translation Loss 50.60 
2024-02-04 10:21:19,646 EPOCH 118
2024-02-04 10:21:25,215 [Epoch: 118 Step: 00002000] Batch Recognition Loss:   0.008102 => Gls Tokens per Sec:     1219 || Batch Translation Loss:   2.053139 => Txt Tokens per Sec:     3180 || Lr: 0.000100
2024-02-04 10:22:58,555 Hooray! New best validation result [eval_metric]!
2024-02-04 10:22:58,558 Saving new checkpoint.
2024-02-04 10:22:58,840 Validation result at epoch 118, step     2000: duration: 93.6245s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.01689	Translation Loss: 73564.42188	PPL: 1574.34656
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.69	(BLEU-1: 11.52,	BLEU-2: 3.85,	BLEU-3: 1.59,	BLEU-4: 0.69)
	CHRF 16.38	ROUGE 10.20
2024-02-04 10:22:58,841 Logging Recognition and Translation Outputs
2024-02-04 10:22:58,841 ========================================================================================================================
2024-02-04 10:22:58,841 Logging Sequence: 182_115.00
2024-02-04 10:22:58,842 	Gloss Reference :	A B+C+D+E
2024-02-04 10:22:58,842 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 10:22:58,842 	Gloss Alignment :	         
2024-02-04 10:22:58,842 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 10:22:58,844 	Text Reference  :	fans are unclear whether yuvraj will  be   returning to  play      test match odi or   in  t20 leagues from  february 2022 
2024-02-04 10:22:58,845 	Text Hypothesis :	**** *** it      is      not    known that they      are spreading many goals and then get a   huge    round of       pride
2024-02-04 10:22:58,845 	Text Alignment  :	D    D   S       S       S      S     S    S         S   S         S    S     S   S    S   S   S       S     S        S    
2024-02-04 10:22:58,845 ========================================================================================================================
2024-02-04 10:22:58,845 Logging Sequence: 140_120.00
2024-02-04 10:22:58,845 	Gloss Reference :	A B+C+D+E
2024-02-04 10:22:58,845 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 10:22:58,846 	Gloss Alignment :	         
2024-02-04 10:22:58,846 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 10:22:58,848 	Text Reference  :	but why so   it is  because pant is a talented player and **** it  will  help encouraging the  youth   of **** uttarakhand toward sports
2024-02-04 10:22:58,848 	Text Hypothesis :	he  has been a  lot of      pant ** * ******** ****** and made the world cup  but         also because of pant is          a      native
2024-02-04 10:22:58,848 	Text Alignment  :	S   S   S    S  S   S            D  D D        D          I    S   S     S    S           S    S          I    S           S      S     
2024-02-04 10:22:58,849 ========================================================================================================================
2024-02-04 10:22:58,849 Logging Sequence: 85_36.00
2024-02-04 10:22:58,849 	Gloss Reference :	A B+C+D+E
2024-02-04 10:22:58,849 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 10:22:58,849 	Gloss Alignment :	         
2024-02-04 10:22:58,849 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 10:22:58,850 	Text Reference  :	symonds has scored 2 centuries in 26 tests that he      played for his ******* **** country
2024-02-04 10:22:58,850 	Text Hypothesis :	he      has ****** * ********* ** ** been  a    strange player for his symonds also played 
2024-02-04 10:22:58,850 	Text Alignment  :	S           D      D D         D  D  S     S    S       S              I       I    S      
2024-02-04 10:22:58,851 ========================================================================================================================
2024-02-04 10:22:58,851 Logging Sequence: 164_100.00
2024-02-04 10:22:58,851 	Gloss Reference :	A B+C+D+E
2024-02-04 10:22:58,851 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 10:22:58,851 	Gloss Alignment :	         
2024-02-04 10:22:58,851 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 10:22:58,854 	Text Reference  :	the tv    rights for       broadcasting ipl   matches in     india for          the next 5 years went to  star india for rs 23575 crore
2024-02-04 10:22:58,854 	Text Hypothesis :	*** group a      consisted of           these these   rights of    broadcasting the **** * ipl   date has a    total of  rs 6     balls
2024-02-04 10:22:58,854 	Text Alignment  :	D   S     S      S         S            S     S       S      S     S                D    D S     S    S   S    S     S      S     S    
2024-02-04 10:22:58,854 ========================================================================================================================
2024-02-04 10:22:58,854 Logging Sequence: 76_79.00
2024-02-04 10:22:58,854 	Gloss Reference :	A B+C+D+E
2024-02-04 10:22:58,854 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 10:22:58,854 	Gloss Alignment :	         
2024-02-04 10:22:58,855 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 10:22:58,855 	Text Reference  :	** ** ********* ***** ** *** speaking to    ani csk  ceo kasi  viswanathan said
2024-02-04 10:22:58,856 	Text Hypothesis :	it is currently known as the ipl      there was held in  dubai on          12th
2024-02-04 10:22:58,856 	Text Alignment  :	I  I  I         I     I  I   S        S     S   S    S   S     S           S   
2024-02-04 10:22:58,856 ========================================================================================================================
2024-02-04 10:23:03,948 Epoch 118: Total Training Recognition Loss 0.15  Total Training Translation Loss 49.85 
2024-02-04 10:23:03,949 EPOCH 119
2024-02-04 10:23:14,722 Epoch 119: Total Training Recognition Loss 0.16  Total Training Translation Loss 49.14 
2024-02-04 10:23:14,722 EPOCH 120
2024-02-04 10:23:25,009 Epoch 120: Total Training Recognition Loss 0.17  Total Training Translation Loss 46.26 
2024-02-04 10:23:25,010 EPOCH 121
2024-02-04 10:23:35,974 Epoch 121: Total Training Recognition Loss 0.15  Total Training Translation Loss 44.30 
2024-02-04 10:23:35,975 EPOCH 122
2024-02-04 10:23:46,578 Epoch 122: Total Training Recognition Loss 0.15  Total Training Translation Loss 42.37 
2024-02-04 10:23:46,579 EPOCH 123
2024-02-04 10:23:57,361 Epoch 123: Total Training Recognition Loss 0.14  Total Training Translation Loss 41.61 
2024-02-04 10:23:57,361 EPOCH 124
2024-02-04 10:23:59,061 [Epoch: 124 Step: 00002100] Batch Recognition Loss:   0.006989 => Gls Tokens per Sec:     3392 || Batch Translation Loss:   1.992237 => Txt Tokens per Sec:     8364 || Lr: 0.000100
2024-02-04 10:24:07,782 Epoch 124: Total Training Recognition Loss 0.13  Total Training Translation Loss 39.69 
2024-02-04 10:24:07,784 EPOCH 125
2024-02-04 10:24:18,650 Epoch 125: Total Training Recognition Loss 0.13  Total Training Translation Loss 38.58 
2024-02-04 10:24:18,651 EPOCH 126
2024-02-04 10:24:29,265 Epoch 126: Total Training Recognition Loss 0.13  Total Training Translation Loss 37.71 
2024-02-04 10:24:29,266 EPOCH 127
2024-02-04 10:24:40,192 Epoch 127: Total Training Recognition Loss 0.13  Total Training Translation Loss 36.18 
2024-02-04 10:24:40,193 EPOCH 128
2024-02-04 10:24:50,606 Epoch 128: Total Training Recognition Loss 0.14  Total Training Translation Loss 35.56 
2024-02-04 10:24:50,608 EPOCH 129
2024-02-04 10:25:00,862 Epoch 129: Total Training Recognition Loss 0.13  Total Training Translation Loss 33.71 
2024-02-04 10:25:00,863 EPOCH 130
2024-02-04 10:25:03,947 [Epoch: 130 Step: 00002200] Batch Recognition Loss:   0.007573 => Gls Tokens per Sec:     1453 || Batch Translation Loss:   2.166969 => Txt Tokens per Sec:     4100 || Lr: 0.000100
2024-02-04 10:25:11,581 Epoch 130: Total Training Recognition Loss 0.12  Total Training Translation Loss 33.01 
2024-02-04 10:25:11,582 EPOCH 131
2024-02-04 10:25:22,420 Epoch 131: Total Training Recognition Loss 0.12  Total Training Translation Loss 34.40 
2024-02-04 10:25:22,420 EPOCH 132
2024-02-04 10:25:33,018 Epoch 132: Total Training Recognition Loss 0.13  Total Training Translation Loss 32.64 
2024-02-04 10:25:33,018 EPOCH 133
2024-02-04 10:25:43,672 Epoch 133: Total Training Recognition Loss 0.13  Total Training Translation Loss 31.47 
2024-02-04 10:25:43,673 EPOCH 134
2024-02-04 10:25:54,055 Epoch 134: Total Training Recognition Loss 0.12  Total Training Translation Loss 30.90 
2024-02-04 10:25:54,055 EPOCH 135
2024-02-04 10:26:04,605 Epoch 135: Total Training Recognition Loss 0.12  Total Training Translation Loss 29.11 
2024-02-04 10:26:04,608 EPOCH 136
2024-02-04 10:26:05,849 [Epoch: 136 Step: 00002300] Batch Recognition Loss:   0.006485 => Gls Tokens per Sec:     2582 || Batch Translation Loss:   1.887433 => Txt Tokens per Sec:     6321 || Lr: 0.000100
2024-02-04 10:26:15,600 Epoch 136: Total Training Recognition Loss 0.11  Total Training Translation Loss 28.35 
2024-02-04 10:26:15,601 EPOCH 137
2024-02-04 10:26:26,095 Epoch 137: Total Training Recognition Loss 0.11  Total Training Translation Loss 27.29 
2024-02-04 10:26:26,096 EPOCH 138
2024-02-04 10:26:36,928 Epoch 138: Total Training Recognition Loss 0.11  Total Training Translation Loss 26.07 
2024-02-04 10:26:36,928 EPOCH 139
2024-02-04 10:26:47,693 Epoch 139: Total Training Recognition Loss 0.10  Total Training Translation Loss 26.16 
2024-02-04 10:26:47,695 EPOCH 140
2024-02-04 10:26:58,602 Epoch 140: Total Training Recognition Loss 0.10  Total Training Translation Loss 26.04 
2024-02-04 10:26:58,602 EPOCH 141
2024-02-04 10:27:09,245 Epoch 141: Total Training Recognition Loss 0.11  Total Training Translation Loss 24.75 
2024-02-04 10:27:09,246 EPOCH 142
2024-02-04 10:27:09,750 [Epoch: 142 Step: 00002400] Batch Recognition Loss:   0.004919 => Gls Tokens per Sec:     3815 || Batch Translation Loss:   1.211427 => Txt Tokens per Sec:     8881 || Lr: 0.000100
2024-02-04 10:27:19,772 Epoch 142: Total Training Recognition Loss 0.10  Total Training Translation Loss 24.29 
2024-02-04 10:27:19,773 EPOCH 143
2024-02-04 10:27:30,506 Epoch 143: Total Training Recognition Loss 0.09  Total Training Translation Loss 23.60 
2024-02-04 10:27:30,507 EPOCH 144
2024-02-04 10:27:41,039 Epoch 144: Total Training Recognition Loss 0.09  Total Training Translation Loss 23.53 
2024-02-04 10:27:41,039 EPOCH 145
2024-02-04 10:27:51,564 Epoch 145: Total Training Recognition Loss 0.10  Total Training Translation Loss 22.81 
2024-02-04 10:27:51,565 EPOCH 146
2024-02-04 10:28:02,735 Epoch 146: Total Training Recognition Loss 0.09  Total Training Translation Loss 22.06 
2024-02-04 10:28:02,736 EPOCH 147
2024-02-04 10:28:13,310 Epoch 147: Total Training Recognition Loss 0.09  Total Training Translation Loss 21.09 
2024-02-04 10:28:13,311 EPOCH 148
2024-02-04 10:28:13,626 [Epoch: 148 Step: 00002500] Batch Recognition Loss:   0.005842 => Gls Tokens per Sec:     2032 || Batch Translation Loss:   1.245297 => Txt Tokens per Sec:     5844 || Lr: 0.000100
2024-02-04 10:28:23,854 Epoch 148: Total Training Recognition Loss 0.09  Total Training Translation Loss 20.04 
2024-02-04 10:28:23,856 EPOCH 149
2024-02-04 10:28:34,748 Epoch 149: Total Training Recognition Loss 0.09  Total Training Translation Loss 19.66 
2024-02-04 10:28:34,748 EPOCH 150
2024-02-04 10:28:45,545 Epoch 150: Total Training Recognition Loss 0.09  Total Training Translation Loss 18.86 
2024-02-04 10:28:45,545 EPOCH 151
2024-02-04 10:28:56,506 Epoch 151: Total Training Recognition Loss 0.09  Total Training Translation Loss 18.37 
2024-02-04 10:28:56,507 EPOCH 152
2024-02-04 10:29:07,110 Epoch 152: Total Training Recognition Loss 0.08  Total Training Translation Loss 18.30 
2024-02-04 10:29:07,111 EPOCH 153
2024-02-04 10:29:17,556 [Epoch: 153 Step: 00002600] Batch Recognition Loss:   0.005949 => Gls Tokens per Sec:      957 || Batch Translation Loss:   1.413093 => Txt Tokens per Sec:     2679 || Lr: 0.000100
2024-02-04 10:29:17,743 Epoch 153: Total Training Recognition Loss 0.08  Total Training Translation Loss 18.60 
2024-02-04 10:29:17,743 EPOCH 154
2024-02-04 10:29:28,378 Epoch 154: Total Training Recognition Loss 0.08  Total Training Translation Loss 17.78 
2024-02-04 10:29:28,379 EPOCH 155
2024-02-04 10:29:39,183 Epoch 155: Total Training Recognition Loss 0.08  Total Training Translation Loss 17.11 
2024-02-04 10:29:39,185 EPOCH 156
2024-02-04 10:29:49,607 Epoch 156: Total Training Recognition Loss 0.07  Total Training Translation Loss 16.98 
2024-02-04 10:29:49,608 EPOCH 157
2024-02-04 10:30:00,079 Epoch 157: Total Training Recognition Loss 0.08  Total Training Translation Loss 16.85 
2024-02-04 10:30:00,081 EPOCH 158
2024-02-04 10:30:10,866 Epoch 158: Total Training Recognition Loss 0.08  Total Training Translation Loss 17.25 
2024-02-04 10:30:10,867 EPOCH 159
2024-02-04 10:30:20,960 [Epoch: 159 Step: 00002700] Batch Recognition Loss:   0.004349 => Gls Tokens per Sec:      863 || Batch Translation Loss:   1.053790 => Txt Tokens per Sec:     2460 || Lr: 0.000100
2024-02-04 10:30:21,580 Epoch 159: Total Training Recognition Loss 0.08  Total Training Translation Loss 16.32 
2024-02-04 10:30:21,580 EPOCH 160
2024-02-04 10:30:32,036 Epoch 160: Total Training Recognition Loss 0.07  Total Training Translation Loss 15.88 
2024-02-04 10:30:32,038 EPOCH 161
2024-02-04 10:30:43,027 Epoch 161: Total Training Recognition Loss 0.08  Total Training Translation Loss 15.04 
2024-02-04 10:30:43,028 EPOCH 162
2024-02-04 10:30:52,943 Epoch 162: Total Training Recognition Loss 0.07  Total Training Translation Loss 14.94 
2024-02-04 10:30:52,944 EPOCH 163
2024-02-04 10:31:03,638 Epoch 163: Total Training Recognition Loss 0.07  Total Training Translation Loss 15.25 
2024-02-04 10:31:03,639 EPOCH 164
2024-02-04 10:31:14,070 Epoch 164: Total Training Recognition Loss 0.07  Total Training Translation Loss 14.62 
2024-02-04 10:31:14,071 EPOCH 165
2024-02-04 10:31:22,064 [Epoch: 165 Step: 00002800] Batch Recognition Loss:   0.005677 => Gls Tokens per Sec:      930 || Batch Translation Loss:   1.050083 => Txt Tokens per Sec:     2616 || Lr: 0.000100
2024-02-04 10:31:24,600 Epoch 165: Total Training Recognition Loss 0.07  Total Training Translation Loss 14.72 
2024-02-04 10:31:24,600 EPOCH 166
2024-02-04 10:31:35,168 Epoch 166: Total Training Recognition Loss 0.06  Total Training Translation Loss 13.98 
2024-02-04 10:31:35,169 EPOCH 167
2024-02-04 10:31:45,943 Epoch 167: Total Training Recognition Loss 0.07  Total Training Translation Loss 13.95 
2024-02-04 10:31:45,944 EPOCH 168
2024-02-04 10:31:56,999 Epoch 168: Total Training Recognition Loss 0.06  Total Training Translation Loss 13.42 
2024-02-04 10:31:56,999 EPOCH 169
2024-02-04 10:32:07,367 Epoch 169: Total Training Recognition Loss 0.06  Total Training Translation Loss 12.73 
2024-02-04 10:32:07,367 EPOCH 170
2024-02-04 10:32:18,100 Epoch 170: Total Training Recognition Loss 0.06  Total Training Translation Loss 12.47 
2024-02-04 10:32:18,101 EPOCH 171
2024-02-04 10:32:25,727 [Epoch: 171 Step: 00002900] Batch Recognition Loss:   0.004139 => Gls Tokens per Sec:      807 || Batch Translation Loss:   0.835839 => Txt Tokens per Sec:     2294 || Lr: 0.000100
2024-02-04 10:32:29,099 Epoch 171: Total Training Recognition Loss 0.07  Total Training Translation Loss 12.19 
2024-02-04 10:32:29,099 EPOCH 172
2024-02-04 10:32:39,824 Epoch 172: Total Training Recognition Loss 0.06  Total Training Translation Loss 12.09 
2024-02-04 10:32:39,825 EPOCH 173
2024-02-04 10:32:50,523 Epoch 173: Total Training Recognition Loss 0.06  Total Training Translation Loss 11.82 
2024-02-04 10:32:50,524 EPOCH 174
2024-02-04 10:33:01,181 Epoch 174: Total Training Recognition Loss 0.05  Total Training Translation Loss 11.83 
2024-02-04 10:33:01,181 EPOCH 175
2024-02-04 10:33:11,902 Epoch 175: Total Training Recognition Loss 0.06  Total Training Translation Loss 11.85 
2024-02-04 10:33:11,903 EPOCH 176
2024-02-04 10:33:22,421 Epoch 176: Total Training Recognition Loss 0.06  Total Training Translation Loss 11.75 
2024-02-04 10:33:22,421 EPOCH 177
2024-02-04 10:33:29,873 [Epoch: 177 Step: 00003000] Batch Recognition Loss:   0.002769 => Gls Tokens per Sec:      654 || Batch Translation Loss:   0.698015 => Txt Tokens per Sec:     1926 || Lr: 0.000100
2024-02-04 10:33:33,204 Epoch 177: Total Training Recognition Loss 0.06  Total Training Translation Loss 11.46 
2024-02-04 10:33:33,204 EPOCH 178
2024-02-04 10:33:44,059 Epoch 178: Total Training Recognition Loss 0.05  Total Training Translation Loss 10.85 
2024-02-04 10:33:44,060 EPOCH 179
2024-02-04 10:33:55,111 Epoch 179: Total Training Recognition Loss 0.05  Total Training Translation Loss 10.34 
2024-02-04 10:33:55,111 EPOCH 180
2024-02-04 10:34:06,184 Epoch 180: Total Training Recognition Loss 0.05  Total Training Translation Loss 10.07 
2024-02-04 10:34:06,185 EPOCH 181
2024-02-04 10:34:16,863 Epoch 181: Total Training Recognition Loss 0.05  Total Training Translation Loss 10.19 
2024-02-04 10:34:16,864 EPOCH 182
2024-02-04 10:34:27,458 Epoch 182: Total Training Recognition Loss 0.05  Total Training Translation Loss 9.81 
2024-02-04 10:34:27,460 EPOCH 183
2024-02-04 10:34:34,219 [Epoch: 183 Step: 00003100] Batch Recognition Loss:   0.003854 => Gls Tokens per Sec:      531 || Batch Translation Loss:   0.782876 => Txt Tokens per Sec:     1626 || Lr: 0.000100
2024-02-04 10:34:38,138 Epoch 183: Total Training Recognition Loss 0.05  Total Training Translation Loss 9.60 
2024-02-04 10:34:38,139 EPOCH 184
2024-02-04 10:34:48,946 Epoch 184: Total Training Recognition Loss 0.05  Total Training Translation Loss 9.64 
2024-02-04 10:34:48,946 EPOCH 185
2024-02-04 10:34:59,850 Epoch 185: Total Training Recognition Loss 0.05  Total Training Translation Loss 9.53 
2024-02-04 10:34:59,851 EPOCH 186
2024-02-04 10:35:11,056 Epoch 186: Total Training Recognition Loss 0.05  Total Training Translation Loss 9.51 
2024-02-04 10:35:11,057 EPOCH 187
2024-02-04 10:35:22,216 Epoch 187: Total Training Recognition Loss 0.05  Total Training Translation Loss 9.50 
2024-02-04 10:35:22,218 EPOCH 188
2024-02-04 10:35:33,014 Epoch 188: Total Training Recognition Loss 0.04  Total Training Translation Loss 9.31 
2024-02-04 10:35:33,016 EPOCH 189
2024-02-04 10:35:35,638 [Epoch: 189 Step: 00003200] Batch Recognition Loss:   0.002330 => Gls Tokens per Sec:      977 || Batch Translation Loss:   0.443942 => Txt Tokens per Sec:     2607 || Lr: 0.000100
2024-02-04 10:35:43,802 Epoch 189: Total Training Recognition Loss 0.04  Total Training Translation Loss 9.20 
2024-02-04 10:35:43,804 EPOCH 190
2024-02-04 10:35:54,472 Epoch 190: Total Training Recognition Loss 0.05  Total Training Translation Loss 9.61 
2024-02-04 10:35:54,473 EPOCH 191
2024-02-04 10:36:05,505 Epoch 191: Total Training Recognition Loss 0.05  Total Training Translation Loss 9.12 
2024-02-04 10:36:05,507 EPOCH 192
2024-02-04 10:36:16,071 Epoch 192: Total Training Recognition Loss 0.05  Total Training Translation Loss 9.67 
2024-02-04 10:36:16,071 EPOCH 193
2024-02-04 10:36:26,745 Epoch 193: Total Training Recognition Loss 0.05  Total Training Translation Loss 9.52 
2024-02-04 10:36:26,746 EPOCH 194
2024-02-04 10:36:37,183 Epoch 194: Total Training Recognition Loss 0.05  Total Training Translation Loss 9.62 
2024-02-04 10:36:37,184 EPOCH 195
2024-02-04 10:36:37,497 [Epoch: 195 Step: 00003300] Batch Recognition Loss:   0.002142 => Gls Tokens per Sec:     4098 || Batch Translation Loss:   0.477497 => Txt Tokens per Sec:    10039 || Lr: 0.000100
2024-02-04 10:36:47,899 Epoch 195: Total Training Recognition Loss 0.05  Total Training Translation Loss 9.62 
2024-02-04 10:36:47,900 EPOCH 196
2024-02-04 10:36:58,958 Epoch 196: Total Training Recognition Loss 0.05  Total Training Translation Loss 8.88 
2024-02-04 10:36:58,959 EPOCH 197
2024-02-04 10:37:09,870 Epoch 197: Total Training Recognition Loss 0.04  Total Training Translation Loss 8.78 
2024-02-04 10:37:09,872 EPOCH 198
2024-02-04 10:37:20,806 Epoch 198: Total Training Recognition Loss 0.05  Total Training Translation Loss 9.12 
2024-02-04 10:37:20,806 EPOCH 199
2024-02-04 10:37:30,984 Epoch 199: Total Training Recognition Loss 0.05  Total Training Translation Loss 8.84 
2024-02-04 10:37:30,985 EPOCH 200
2024-02-04 10:37:41,733 [Epoch: 200 Step: 00003400] Batch Recognition Loss:   0.002551 => Gls Tokens per Sec:      989 || Batch Translation Loss:   0.528909 => Txt Tokens per Sec:     2746 || Lr: 0.000100
2024-02-04 10:37:41,734 Epoch 200: Total Training Recognition Loss 0.05  Total Training Translation Loss 8.33 
2024-02-04 10:37:41,734 EPOCH 201
2024-02-04 10:37:52,428 Epoch 201: Total Training Recognition Loss 0.05  Total Training Translation Loss 8.13 
2024-02-04 10:37:52,429 EPOCH 202
2024-02-04 10:38:03,318 Epoch 202: Total Training Recognition Loss 0.05  Total Training Translation Loss 7.70 
2024-02-04 10:38:03,320 EPOCH 203
2024-02-04 10:38:13,998 Epoch 203: Total Training Recognition Loss 0.04  Total Training Translation Loss 7.54 
2024-02-04 10:38:13,998 EPOCH 204
2024-02-04 10:38:24,513 Epoch 204: Total Training Recognition Loss 0.04  Total Training Translation Loss 7.05 
2024-02-04 10:38:24,514 EPOCH 205
2024-02-04 10:38:35,124 Epoch 205: Total Training Recognition Loss 0.04  Total Training Translation Loss 7.11 
2024-02-04 10:38:35,124 EPOCH 206
2024-02-04 10:38:45,006 [Epoch: 206 Step: 00003500] Batch Recognition Loss:   0.002748 => Gls Tokens per Sec:      946 || Batch Translation Loss:   0.563364 => Txt Tokens per Sec:     2620 || Lr: 0.000100
2024-02-04 10:38:45,571 Epoch 206: Total Training Recognition Loss 0.04  Total Training Translation Loss 7.77 
2024-02-04 10:38:45,572 EPOCH 207
2024-02-04 10:38:56,102 Epoch 207: Total Training Recognition Loss 0.04  Total Training Translation Loss 8.47 
2024-02-04 10:38:56,103 EPOCH 208
2024-02-04 10:39:06,718 Epoch 208: Total Training Recognition Loss 0.04  Total Training Translation Loss 7.93 
2024-02-04 10:39:06,718 EPOCH 209
2024-02-04 10:39:17,307 Epoch 209: Total Training Recognition Loss 0.04  Total Training Translation Loss 7.32 
2024-02-04 10:39:17,307 EPOCH 210
2024-02-04 10:39:28,176 Epoch 210: Total Training Recognition Loss 0.04  Total Training Translation Loss 7.15 
2024-02-04 10:39:28,177 EPOCH 211
2024-02-04 10:39:38,917 Epoch 211: Total Training Recognition Loss 0.04  Total Training Translation Loss 7.27 
2024-02-04 10:39:38,918 EPOCH 212
2024-02-04 10:39:46,164 [Epoch: 212 Step: 00003600] Batch Recognition Loss:   0.002282 => Gls Tokens per Sec:     1148 || Batch Translation Loss:   0.508220 => Txt Tokens per Sec:     3158 || Lr: 0.000100
2024-02-04 10:39:49,527 Epoch 212: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.85 
2024-02-04 10:39:49,528 EPOCH 213
2024-02-04 10:40:00,293 Epoch 213: Total Training Recognition Loss 0.04  Total Training Translation Loss 6.86 
2024-02-04 10:40:00,294 EPOCH 214
2024-02-04 10:40:11,058 Epoch 214: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.21 
2024-02-04 10:40:11,059 EPOCH 215
2024-02-04 10:40:21,595 Epoch 215: Total Training Recognition Loss 0.04  Total Training Translation Loss 7.32 
2024-02-04 10:40:21,596 EPOCH 216
2024-02-04 10:40:32,133 Epoch 216: Total Training Recognition Loss 0.04  Total Training Translation Loss 7.11 
2024-02-04 10:40:32,133 EPOCH 217
2024-02-04 10:40:42,630 Epoch 217: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.26 
2024-02-04 10:40:42,630 EPOCH 218
2024-02-04 10:40:48,730 [Epoch: 218 Step: 00003700] Batch Recognition Loss:   0.002151 => Gls Tokens per Sec:     1113 || Batch Translation Loss:   0.545890 => Txt Tokens per Sec:     3034 || Lr: 0.000100
2024-02-04 10:40:53,203 Epoch 218: Total Training Recognition Loss 0.04  Total Training Translation Loss 6.80 
2024-02-04 10:40:53,204 EPOCH 219
2024-02-04 10:41:03,907 Epoch 219: Total Training Recognition Loss 0.04  Total Training Translation Loss 6.39 
2024-02-04 10:41:03,908 EPOCH 220
2024-02-04 10:41:14,538 Epoch 220: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.00 
2024-02-04 10:41:14,540 EPOCH 221
2024-02-04 10:41:25,295 Epoch 221: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.92 
2024-02-04 10:41:25,296 EPOCH 222
2024-02-04 10:41:35,792 Epoch 222: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.97 
2024-02-04 10:41:35,793 EPOCH 223
2024-02-04 10:41:46,377 Epoch 223: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.99 
2024-02-04 10:41:46,378 EPOCH 224
2024-02-04 10:41:52,256 [Epoch: 224 Step: 00003800] Batch Recognition Loss:   0.001222 => Gls Tokens per Sec:      937 || Batch Translation Loss:   0.256978 => Txt Tokens per Sec:     2577 || Lr: 0.000100
2024-02-04 10:41:56,955 Epoch 224: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.71 
2024-02-04 10:41:56,955 EPOCH 225
2024-02-04 10:42:07,845 Epoch 225: Total Training Recognition Loss 0.04  Total Training Translation Loss 6.05 
2024-02-04 10:42:07,847 EPOCH 226
2024-02-04 10:42:18,275 Epoch 226: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.82 
2024-02-04 10:42:18,275 EPOCH 227
2024-02-04 10:42:29,056 Epoch 227: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.35 
2024-02-04 10:42:29,057 EPOCH 228
2024-02-04 10:42:40,111 Epoch 228: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.28 
2024-02-04 10:42:40,112 EPOCH 229
2024-02-04 10:42:51,905 Epoch 229: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.27 
2024-02-04 10:42:51,906 EPOCH 230
2024-02-04 10:42:56,470 [Epoch: 230 Step: 00003900] Batch Recognition Loss:   0.001956 => Gls Tokens per Sec:      982 || Batch Translation Loss:   0.139467 => Txt Tokens per Sec:     2667 || Lr: 0.000100
2024-02-04 10:43:02,688 Epoch 230: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.21 
2024-02-04 10:43:02,688 EPOCH 231
2024-02-04 10:43:13,691 Epoch 231: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.58 
2024-02-04 10:43:13,692 EPOCH 232
2024-02-04 10:43:24,145 Epoch 232: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.91 
2024-02-04 10:43:24,147 EPOCH 233
2024-02-04 10:43:34,773 Epoch 233: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.51 
2024-02-04 10:43:34,774 EPOCH 234
2024-02-04 10:43:45,275 Epoch 234: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.19 
2024-02-04 10:43:45,276 EPOCH 235
2024-02-04 10:43:56,057 Epoch 235: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.83 
2024-02-04 10:43:56,059 EPOCH 236
2024-02-04 10:43:57,068 [Epoch: 236 Step: 00004000] Batch Recognition Loss:   0.001285 => Gls Tokens per Sec:     3174 || Batch Translation Loss:   0.215372 => Txt Tokens per Sec:     8301 || Lr: 0.000100
2024-02-04 10:44:35,046 Hooray! New best validation result [eval_metric]!
2024-02-04 10:44:35,048 Saving new checkpoint.
2024-02-04 10:44:35,344 Validation result at epoch 236, step     4000: duration: 38.2751s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00195	Translation Loss: 82366.17188	PPL: 3798.59619
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.96	(BLEU-1: 12.88,	BLEU-2: 4.33,	BLEU-3: 1.89,	BLEU-4: 0.96)
	CHRF 18.10	ROUGE 10.71
2024-02-04 10:44:35,346 Logging Recognition and Translation Outputs
2024-02-04 10:44:35,346 ========================================================================================================================
2024-02-04 10:44:35,346 Logging Sequence: 133_173.00
2024-02-04 10:44:35,347 	Gloss Reference :	A B+C+D+E
2024-02-04 10:44:35,347 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 10:44:35,347 	Gloss Alignment :	         
2024-02-04 10:44:35,347 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 10:44:35,350 	Text Reference  :	** according to       sources the   leaders of the two   countries are   set to      join the ******* *** ** **** *** commentary panel as  well 
2024-02-04 10:44:35,350 	Text Hypothesis :	on 12th      february 2023    there was     a  t20 match between   india and gujarat at   the stadium let me tell you can        watch the world
2024-02-04 10:44:35,350 	Text Alignment  :	I  S         S        S       S     S       S  S   S     S         S     S   S       S        I       I   I  I    I   S          S     S   S    
2024-02-04 10:44:35,350 ========================================================================================================================
2024-02-04 10:44:35,350 Logging Sequence: 83_33.00
2024-02-04 10:44:35,350 	Gloss Reference :	A B+C+D+E
2024-02-04 10:44:35,350 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 10:44:35,351 	Gloss Alignment :	         
2024-02-04 10:44:35,351 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 10:44:35,352 	Text Reference  :	*** ******* ****** ** a ** **** *** ***** ** football match   lasts for  two    equal halves of  45      minutes
2024-02-04 10:44:35,352 	Text Hypothesis :	the denmark person is a 29 year old staff to the      denmark team  with people seem  to     the denmark team   
2024-02-04 10:44:35,352 	Text Alignment  :	I   I       I      I    I  I    I   I     I  S        S       S     S    S      S     S      S   S       S      
2024-02-04 10:44:35,352 ========================================================================================================================
2024-02-04 10:44:35,352 Logging Sequence: 68_147.00
2024-02-04 10:44:35,353 	Gloss Reference :	A B+C+D+E
2024-02-04 10:44:35,353 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 10:44:35,353 	Gloss Alignment :	         
2024-02-04 10:44:35,353 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 10:44:35,354 	Text Reference  :	remember the 2007 t20 world cup    amid  a    lot   of  sledging by     english players
2024-02-04 10:44:35,354 	Text Hypothesis :	******** *** **** *** while stuart broad gave dhoni for the      bowler smashed sixes  
2024-02-04 10:44:35,354 	Text Alignment  :	D        D   D    D   S     S      S     S    S     S   S        S      S       S      
2024-02-04 10:44:35,355 ========================================================================================================================
2024-02-04 10:44:35,355 Logging Sequence: 165_8.00
2024-02-04 10:44:35,355 	Gloss Reference :	A B+C+D+E
2024-02-04 10:44:35,355 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 10:44:35,355 	Gloss Alignment :	         
2024-02-04 10:44:35,355 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 10:44:35,356 	Text Reference  :	however many don't believe in     it   it    varies among people
2024-02-04 10:44:35,356 	Text Hypothesis :	******* **** they  are     caught that after his    bag   medals
2024-02-04 10:44:35,356 	Text Alignment  :	D       D    S     S       S      S    S     S      S     S     
2024-02-04 10:44:35,356 ========================================================================================================================
2024-02-04 10:44:35,356 Logging Sequence: 119_71.00
2024-02-04 10:44:35,357 	Gloss Reference :	A B+C+D+E
2024-02-04 10:44:35,357 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 10:44:35,357 	Gloss Alignment :	         
2024-02-04 10:44:35,357 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 10:44:35,358 	Text Reference  :	the special gold devices have each player' names   and ***** jersey numbers next   to the     camera
2024-02-04 10:44:35,358 	Text Hypothesis :	*** idesign gold ******* is   a    violent clashes and messi got    the     reason of idesign gold  
2024-02-04 10:44:35,358 	Text Alignment  :	D   S            D       S    S    S       S           I     S      S       S      S  S       S     
2024-02-04 10:44:35,359 ========================================================================================================================
2024-02-04 10:44:45,532 Epoch 236: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.69 
2024-02-04 10:44:45,533 EPOCH 237
2024-02-04 10:44:56,213 Epoch 237: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.80 
2024-02-04 10:44:56,214 EPOCH 238
2024-02-04 10:45:06,620 Epoch 238: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.88 
2024-02-04 10:45:06,620 EPOCH 239
2024-02-04 10:45:17,335 Epoch 239: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.88 
2024-02-04 10:45:17,336 EPOCH 240
2024-02-04 10:45:28,004 Epoch 240: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.77 
2024-02-04 10:45:28,004 EPOCH 241
2024-02-04 10:45:38,537 Epoch 241: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.55 
2024-02-04 10:45:38,538 EPOCH 242
2024-02-04 10:45:39,042 [Epoch: 242 Step: 00004100] Batch Recognition Loss:   0.001017 => Gls Tokens per Sec:     3817 || Batch Translation Loss:   0.229453 => Txt Tokens per Sec:     9402 || Lr: 0.000100
2024-02-04 10:45:49,067 Epoch 242: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.30 
2024-02-04 10:45:49,068 EPOCH 243
2024-02-04 10:45:59,746 Epoch 243: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.57 
2024-02-04 10:45:59,746 EPOCH 244
2024-02-04 10:46:10,262 Epoch 244: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.28 
2024-02-04 10:46:10,263 EPOCH 245
2024-02-04 10:46:20,710 Epoch 245: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.30 
2024-02-04 10:46:20,711 EPOCH 246
2024-02-04 10:46:31,033 Epoch 246: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.51 
2024-02-04 10:46:31,034 EPOCH 247
2024-02-04 10:46:41,500 Epoch 247: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.19 
2024-02-04 10:46:41,501 EPOCH 248
2024-02-04 10:46:43,263 [Epoch: 248 Step: 00004200] Batch Recognition Loss:   0.002619 => Gls Tokens per Sec:      363 || Batch Translation Loss:   0.411332 => Txt Tokens per Sec:     1286 || Lr: 0.000100
2024-02-04 10:46:52,396 Epoch 248: Total Training Recognition Loss 0.05  Total Training Translation Loss 9.95 
2024-02-04 10:46:52,397 EPOCH 249
2024-02-04 10:47:02,852 Epoch 249: Total Training Recognition Loss 0.08  Total Training Translation Loss 20.24 
2024-02-04 10:47:02,853 EPOCH 250
2024-02-04 10:47:13,198 Epoch 250: Total Training Recognition Loss 0.07  Total Training Translation Loss 13.68 
2024-02-04 10:47:13,199 EPOCH 251
2024-02-04 10:47:23,643 Epoch 251: Total Training Recognition Loss 0.06  Total Training Translation Loss 11.64 
2024-02-04 10:47:23,643 EPOCH 252
2024-02-04 10:47:34,010 Epoch 252: Total Training Recognition Loss 0.07  Total Training Translation Loss 8.83 
2024-02-04 10:47:34,011 EPOCH 253
2024-02-04 10:47:44,363 [Epoch: 253 Step: 00004300] Batch Recognition Loss:   0.002694 => Gls Tokens per Sec:      965 || Batch Translation Loss:   0.449053 => Txt Tokens per Sec:     2681 || Lr: 0.000100
2024-02-04 10:47:44,580 Epoch 253: Total Training Recognition Loss 0.04  Total Training Translation Loss 6.39 
2024-02-04 10:47:44,580 EPOCH 254
2024-02-04 10:47:55,060 Epoch 254: Total Training Recognition Loss 0.04  Total Training Translation Loss 4.73 
2024-02-04 10:47:55,061 EPOCH 255
2024-02-04 10:48:05,656 Epoch 255: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.30 
2024-02-04 10:48:05,657 EPOCH 256
2024-02-04 10:48:16,287 Epoch 256: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.66 
2024-02-04 10:48:16,288 EPOCH 257
2024-02-04 10:48:26,678 Epoch 257: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.44 
2024-02-04 10:48:26,678 EPOCH 258
2024-02-04 10:48:37,191 Epoch 258: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.27 
2024-02-04 10:48:37,192 EPOCH 259
2024-02-04 10:48:45,519 [Epoch: 259 Step: 00004400] Batch Recognition Loss:   0.001368 => Gls Tokens per Sec:     1046 || Batch Translation Loss:   0.258118 => Txt Tokens per Sec:     2838 || Lr: 0.000100
2024-02-04 10:48:47,714 Epoch 259: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.28 
2024-02-04 10:48:47,714 EPOCH 260
2024-02-04 10:48:58,420 Epoch 260: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.04 
2024-02-04 10:48:58,420 EPOCH 261
2024-02-04 10:49:08,699 Epoch 261: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.78 
2024-02-04 10:49:08,700 EPOCH 262
2024-02-04 10:49:19,307 Epoch 262: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.89 
2024-02-04 10:49:19,308 EPOCH 263
2024-02-04 10:49:29,844 Epoch 263: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.92 
2024-02-04 10:49:29,845 EPOCH 264
2024-02-04 10:49:40,299 Epoch 264: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.81 
2024-02-04 10:49:40,300 EPOCH 265
2024-02-04 10:49:47,508 [Epoch: 265 Step: 00004500] Batch Recognition Loss:   0.001099 => Gls Tokens per Sec:     1066 || Batch Translation Loss:   0.188761 => Txt Tokens per Sec:     3122 || Lr: 0.000100
2024-02-04 10:49:50,765 Epoch 265: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.70 
2024-02-04 10:49:50,765 EPOCH 266
2024-02-04 10:50:01,563 Epoch 266: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.53 
2024-02-04 10:50:01,564 EPOCH 267
2024-02-04 10:50:12,219 Epoch 267: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.47 
2024-02-04 10:50:12,220 EPOCH 268
2024-02-04 10:50:22,615 Epoch 268: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.58 
2024-02-04 10:50:22,616 EPOCH 269
2024-02-04 10:50:32,803 Epoch 269: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.38 
2024-02-04 10:50:32,804 EPOCH 270
2024-02-04 10:50:43,378 Epoch 270: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.46 
2024-02-04 10:50:43,379 EPOCH 271
2024-02-04 10:50:49,413 [Epoch: 271 Step: 00004600] Batch Recognition Loss:   0.001876 => Gls Tokens per Sec:     1019 || Batch Translation Loss:   0.086897 => Txt Tokens per Sec:     2782 || Lr: 0.000100
2024-02-04 10:50:53,887 Epoch 271: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.40 
2024-02-04 10:50:53,887 EPOCH 272
2024-02-04 10:51:04,294 Epoch 272: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.52 
2024-02-04 10:51:04,295 EPOCH 273
2024-02-04 10:51:14,837 Epoch 273: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.00 
2024-02-04 10:51:14,838 EPOCH 274
2024-02-04 10:51:25,596 Epoch 274: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.46 
2024-02-04 10:51:25,597 EPOCH 275
2024-02-04 10:51:36,025 Epoch 275: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.10 
2024-02-04 10:51:36,026 EPOCH 276
2024-02-04 10:51:46,804 Epoch 276: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.07 
2024-02-04 10:51:46,805 EPOCH 277
2024-02-04 10:51:49,968 [Epoch: 277 Step: 00004700] Batch Recognition Loss:   0.000857 => Gls Tokens per Sec:     1619 || Batch Translation Loss:   0.117618 => Txt Tokens per Sec:     4439 || Lr: 0.000100
2024-02-04 10:51:57,544 Epoch 277: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.90 
2024-02-04 10:51:57,544 EPOCH 278
2024-02-04 10:52:08,119 Epoch 278: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.05 
2024-02-04 10:52:08,119 EPOCH 279
2024-02-04 10:52:18,630 Epoch 279: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.76 
2024-02-04 10:52:18,631 EPOCH 280
2024-02-04 10:52:29,273 Epoch 280: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.96 
2024-02-04 10:52:29,274 EPOCH 281
2024-02-04 10:52:39,843 Epoch 281: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.90 
2024-02-04 10:52:39,843 EPOCH 282
2024-02-04 10:52:50,357 Epoch 282: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.14 
2024-02-04 10:52:50,358 EPOCH 283
2024-02-04 10:52:51,396 [Epoch: 283 Step: 00004800] Batch Recognition Loss:   0.000776 => Gls Tokens per Sec:     3700 || Batch Translation Loss:   0.174599 => Txt Tokens per Sec:     9562 || Lr: 0.000100
2024-02-04 10:53:00,876 Epoch 283: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.42 
2024-02-04 10:53:00,876 EPOCH 284
2024-02-04 10:53:11,588 Epoch 284: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.99 
2024-02-04 10:53:11,589 EPOCH 285
2024-02-04 10:53:22,009 Epoch 285: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.83 
2024-02-04 10:53:22,010 EPOCH 286
2024-02-04 10:53:32,688 Epoch 286: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.91 
2024-02-04 10:53:32,688 EPOCH 287
2024-02-04 10:53:43,176 Epoch 287: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.04 
2024-02-04 10:53:43,177 EPOCH 288
2024-02-04 10:53:53,794 Epoch 288: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.99 
2024-02-04 10:53:53,794 EPOCH 289
2024-02-04 10:53:56,296 [Epoch: 289 Step: 00004900] Batch Recognition Loss:   0.000769 => Gls Tokens per Sec:     1023 || Batch Translation Loss:   0.164114 => Txt Tokens per Sec:     2987 || Lr: 0.000100
2024-02-04 10:54:04,370 Epoch 289: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.33 
2024-02-04 10:54:04,371 EPOCH 290
2024-02-04 10:54:14,756 Epoch 290: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.13 
2024-02-04 10:54:14,756 EPOCH 291
2024-02-04 10:54:25,600 Epoch 291: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.93 
2024-02-04 10:54:25,600 EPOCH 292
2024-02-04 10:54:37,316 Epoch 292: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.94 
2024-02-04 10:54:37,317 EPOCH 293
2024-02-04 10:54:48,138 Epoch 293: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.29 
2024-02-04 10:54:48,139 EPOCH 294
2024-02-04 10:54:58,521 Epoch 294: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.94 
2024-02-04 10:54:58,522 EPOCH 295
2024-02-04 10:54:59,043 [Epoch: 295 Step: 00005000] Batch Recognition Loss:   0.001010 => Gls Tokens per Sec:     2462 || Batch Translation Loss:   0.181288 => Txt Tokens per Sec:     7227 || Lr: 0.000100
2024-02-04 10:55:09,259 Epoch 295: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.60 
2024-02-04 10:55:09,260 EPOCH 296
2024-02-04 10:55:20,161 Epoch 296: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.37 
2024-02-04 10:55:20,162 EPOCH 297
2024-02-04 10:55:31,228 Epoch 297: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.54 
2024-02-04 10:55:31,229 EPOCH 298
2024-02-04 10:55:42,070 Epoch 298: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.61 
2024-02-04 10:55:42,071 EPOCH 299
2024-02-04 10:55:52,577 Epoch 299: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.48 
2024-02-04 10:55:52,578 EPOCH 300
2024-02-04 10:56:03,290 [Epoch: 300 Step: 00005100] Batch Recognition Loss:   0.000815 => Gls Tokens per Sec:      993 || Batch Translation Loss:   0.116123 => Txt Tokens per Sec:     2755 || Lr: 0.000100
2024-02-04 10:56:03,291 Epoch 300: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.57 
2024-02-04 10:56:03,291 EPOCH 301
2024-02-04 10:56:13,858 Epoch 301: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.90 
2024-02-04 10:56:13,859 EPOCH 302
2024-02-04 10:56:24,604 Epoch 302: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.18 
2024-02-04 10:56:24,605 EPOCH 303
2024-02-04 10:56:35,142 Epoch 303: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.34 
2024-02-04 10:56:35,142 EPOCH 304
2024-02-04 10:56:45,750 Epoch 304: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.56 
2024-02-04 10:56:45,751 EPOCH 305
2024-02-04 10:56:56,471 Epoch 305: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.72 
2024-02-04 10:56:56,472 EPOCH 306
2024-02-04 10:57:06,617 [Epoch: 306 Step: 00005200] Batch Recognition Loss:   0.000929 => Gls Tokens per Sec:      922 || Batch Translation Loss:   0.141444 => Txt Tokens per Sec:     2572 || Lr: 0.000100
2024-02-04 10:57:07,005 Epoch 306: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.90 
2024-02-04 10:57:07,005 EPOCH 307
2024-02-04 10:57:17,454 Epoch 307: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.93 
2024-02-04 10:57:17,454 EPOCH 308
2024-02-04 10:57:27,984 Epoch 308: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.74 
2024-02-04 10:57:27,985 EPOCH 309
2024-02-04 10:57:38,600 Epoch 309: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.84 
2024-02-04 10:57:38,601 EPOCH 310
2024-02-04 10:57:49,321 Epoch 310: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.77 
2024-02-04 10:57:49,322 EPOCH 311
2024-02-04 10:57:59,695 Epoch 311: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.14 
2024-02-04 10:57:59,695 EPOCH 312
2024-02-04 10:58:05,531 [Epoch: 312 Step: 00005300] Batch Recognition Loss:   0.001154 => Gls Tokens per Sec:     1426 || Batch Translation Loss:   0.394829 => Txt Tokens per Sec:     3880 || Lr: 0.000100
2024-02-04 10:58:10,075 Epoch 312: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.39 
2024-02-04 10:58:10,075 EPOCH 313
2024-02-04 10:58:20,959 Epoch 313: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.18 
2024-02-04 10:58:20,961 EPOCH 314
2024-02-04 10:58:31,566 Epoch 314: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.92 
2024-02-04 10:58:31,566 EPOCH 315
2024-02-04 10:58:42,236 Epoch 315: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.95 
2024-02-04 10:58:42,237 EPOCH 316
2024-02-04 10:58:52,921 Epoch 316: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.58 
2024-02-04 10:58:52,921 EPOCH 317
2024-02-04 10:59:03,361 Epoch 317: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.04 
2024-02-04 10:59:03,362 EPOCH 318
2024-02-04 10:59:11,083 [Epoch: 318 Step: 00005400] Batch Recognition Loss:   0.001096 => Gls Tokens per Sec:      880 || Batch Translation Loss:   0.180741 => Txt Tokens per Sec:     2420 || Lr: 0.000100
2024-02-04 10:59:13,875 Epoch 318: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.99 
2024-02-04 10:59:13,875 EPOCH 319
2024-02-04 10:59:24,351 Epoch 319: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.56 
2024-02-04 10:59:24,352 EPOCH 320
2024-02-04 10:59:35,565 Epoch 320: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.25 
2024-02-04 10:59:35,566 EPOCH 321
2024-02-04 10:59:46,631 Epoch 321: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.45 
2024-02-04 10:59:46,632 EPOCH 322
2024-02-04 10:59:57,365 Epoch 322: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.31 
2024-02-04 10:59:57,366 EPOCH 323
2024-02-04 11:00:08,269 Epoch 323: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.44 
2024-02-04 11:00:08,269 EPOCH 324
2024-02-04 11:00:11,506 [Epoch: 324 Step: 00005500] Batch Recognition Loss:   0.000697 => Gls Tokens per Sec:     1780 || Batch Translation Loss:   0.164021 => Txt Tokens per Sec:     4718 || Lr: 0.000100
2024-02-04 11:00:18,706 Epoch 324: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.78 
2024-02-04 11:00:18,706 EPOCH 325
2024-02-04 11:00:29,179 Epoch 325: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.04 
2024-02-04 11:00:29,181 EPOCH 326
2024-02-04 11:00:39,836 Epoch 326: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.95 
2024-02-04 11:00:39,837 EPOCH 327
2024-02-04 11:00:50,514 Epoch 327: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.00 
2024-02-04 11:00:50,516 EPOCH 328
2024-02-04 11:01:00,929 Epoch 328: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.74 
2024-02-04 11:01:00,929 EPOCH 329
2024-02-04 11:01:11,577 Epoch 329: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.83 
2024-02-04 11:01:11,578 EPOCH 330
2024-02-04 11:01:14,510 [Epoch: 330 Step: 00005600] Batch Recognition Loss:   0.000759 => Gls Tokens per Sec:     1528 || Batch Translation Loss:   0.160821 => Txt Tokens per Sec:     4389 || Lr: 0.000100
2024-02-04 11:01:21,941 Epoch 330: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.73 
2024-02-04 11:01:21,941 EPOCH 331
2024-02-04 11:01:32,809 Epoch 331: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.37 
2024-02-04 11:01:32,809 EPOCH 332
2024-02-04 11:01:43,577 Epoch 332: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.34 
2024-02-04 11:01:43,578 EPOCH 333
2024-02-04 11:01:54,172 Epoch 333: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.55 
2024-02-04 11:01:54,172 EPOCH 334
2024-02-04 11:02:04,653 Epoch 334: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.77 
2024-02-04 11:02:04,653 EPOCH 335
2024-02-04 11:02:15,377 Epoch 335: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.01 
2024-02-04 11:02:15,377 EPOCH 336
2024-02-04 11:02:19,562 [Epoch: 336 Step: 00005700] Batch Recognition Loss:   0.000648 => Gls Tokens per Sec:      765 || Batch Translation Loss:   0.181509 => Txt Tokens per Sec:     2317 || Lr: 0.000100
2024-02-04 11:02:26,223 Epoch 336: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.17 
2024-02-04 11:02:26,224 EPOCH 337
2024-02-04 11:02:36,839 Epoch 337: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.00 
2024-02-04 11:02:36,840 EPOCH 338
2024-02-04 11:02:47,520 Epoch 338: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.59 
2024-02-04 11:02:47,521 EPOCH 339
2024-02-04 11:02:58,350 Epoch 339: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.23 
2024-02-04 11:02:58,351 EPOCH 340
2024-02-04 11:03:08,836 Epoch 340: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.86 
2024-02-04 11:03:08,837 EPOCH 341
2024-02-04 11:03:19,568 Epoch 341: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.80 
2024-02-04 11:03:19,569 EPOCH 342
2024-02-04 11:03:22,499 [Epoch: 342 Step: 00005800] Batch Recognition Loss:   0.000485 => Gls Tokens per Sec:      570 || Batch Translation Loss:   0.065082 => Txt Tokens per Sec:     1316 || Lr: 0.000100
2024-02-04 11:03:30,307 Epoch 342: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.92 
2024-02-04 11:03:30,308 EPOCH 343
2024-02-04 11:03:41,053 Epoch 343: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.07 
2024-02-04 11:03:41,054 EPOCH 344
2024-02-04 11:03:51,393 Epoch 344: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.80 
2024-02-04 11:03:51,394 EPOCH 345
2024-02-04 11:04:01,896 Epoch 345: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.75 
2024-02-04 11:04:01,897 EPOCH 346
2024-02-04 11:04:12,488 Epoch 346: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.94 
2024-02-04 11:04:12,489 EPOCH 347
2024-02-04 11:04:23,074 Epoch 347: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.89 
2024-02-04 11:04:23,075 EPOCH 348
2024-02-04 11:04:24,719 [Epoch: 348 Step: 00005900] Batch Recognition Loss:   0.000639 => Gls Tokens per Sec:      390 || Batch Translation Loss:   0.106512 => Txt Tokens per Sec:     1372 || Lr: 0.000100
2024-02-04 11:04:33,741 Epoch 348: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.91 
2024-02-04 11:04:33,741 EPOCH 349
2024-02-04 11:04:44,345 Epoch 349: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.76 
2024-02-04 11:04:44,345 EPOCH 350
2024-02-04 11:04:54,676 Epoch 350: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.02 
2024-02-04 11:04:54,676 EPOCH 351
2024-02-04 11:05:05,481 Epoch 351: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.92 
2024-02-04 11:05:05,482 EPOCH 352
2024-02-04 11:05:16,090 Epoch 352: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.01 
2024-02-04 11:05:16,091 EPOCH 353
2024-02-04 11:05:24,990 [Epoch: 353 Step: 00006000] Batch Recognition Loss:   0.000631 => Gls Tokens per Sec:     1123 || Batch Translation Loss:   0.082095 => Txt Tokens per Sec:     3062 || Lr: 0.000100
2024-02-04 11:06:02,777 Validation result at epoch 353, step     6000: duration: 37.7872s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00065	Translation Loss: 89157.75000	PPL: 7495.22070
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.81	(BLEU-1: 12.39,	BLEU-2: 4.27,	BLEU-3: 1.64,	BLEU-4: 0.81)
	CHRF 18.00	ROUGE 10.53
2024-02-04 11:06:02,779 Logging Recognition and Translation Outputs
2024-02-04 11:06:02,780 ========================================================================================================================
2024-02-04 11:06:02,780 Logging Sequence: 89_111.00
2024-02-04 11:06:02,780 	Gloss Reference :	A B+C+D+E
2024-02-04 11:06:02,780 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 11:06:02,780 	Gloss Alignment :	         
2024-02-04 11:06:02,780 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 11:06:02,781 	Text Reference  :	** however selectors never selected me for the team *** ***** **** * *****
2024-02-04 11:06:02,781 	Text Hypothesis :	it is      not       known if       a  tie the team was touch with 6 balls
2024-02-04 11:06:02,782 	Text Alignment  :	I  S       S         S     S        S  S            I   I     I    I I    
2024-02-04 11:06:02,782 ========================================================================================================================
2024-02-04 11:06:02,782 Logging Sequence: 137_23.00
2024-02-04 11:06:02,782 	Gloss Reference :	A B+C+D+E
2024-02-04 11:06:02,782 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 11:06:02,782 	Gloss Alignment :	         
2024-02-04 11:06:02,782 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 11:06:02,783 	Text Reference  :	******** fan  from around the world are in      qatar for the fifa world   cup   
2024-02-04 11:06:02,783 	Text Hypothesis :	football fans from around the world are excited to    see the **** current season
2024-02-04 11:06:02,784 	Text Alignment  :	I        S                              S       S     S       D    S       S     
2024-02-04 11:06:02,784 ========================================================================================================================
2024-02-04 11:06:02,784 Logging Sequence: 128_145.00
2024-02-04 11:06:02,784 	Gloss Reference :	A B+C+D+E
2024-02-04 11:06:02,784 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 11:06:02,784 	Gloss Alignment :	         
2024-02-04 11:06:02,784 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 11:06:02,785 	Text Reference  :	icc   also uploaded a   video of  the  same
2024-02-04 11:06:02,785 	Text Hypothesis :	about the  two      men and   met with this
2024-02-04 11:06:02,785 	Text Alignment  :	S     S    S        S   S     S   S    S   
2024-02-04 11:06:02,785 ========================================================================================================================
2024-02-04 11:06:02,785 Logging Sequence: 165_192.00
2024-02-04 11:06:02,786 	Gloss Reference :	A B+C+D+E
2024-02-04 11:06:02,786 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 11:06:02,786 	Gloss Alignment :	         
2024-02-04 11:06:02,786 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 11:06:02,787 	Text Reference  :	3 ravichandran ashwin believes   that **** his bag ** *** ****** **** is lucky
2024-02-04 11:06:02,787 	Text Hypothesis :	* it           is     disgusting that such a   bag of the indian team by this 
2024-02-04 11:06:02,787 	Text Alignment  :	D S            S      S               I    S       I  I   I      I    S  S    
2024-02-04 11:06:02,787 ========================================================================================================================
2024-02-04 11:06:02,787 Logging Sequence: 180_494.00
2024-02-04 11:06:02,787 	Gloss Reference :	A B+C+D+E
2024-02-04 11:06:02,788 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 11:06:02,788 	Gloss Alignment :	         
2024-02-04 11:06:02,788 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 11:06:02,790 	Text Reference  :	******* **** *** the   women  wrestlers ***** spoke angrily against    the       police  and  the     controversy in    front of the    media  
2024-02-04 11:06:02,790 	Text Hypothesis :	however they say seven female wrestlers filed a     sexual  harassment complaint against brij bhushan sharan      singh at    cp police station
2024-02-04 11:06:02,790 	Text Alignment  :	I       I    I   S     S                I     S     S       S          S         S       S    S       S           S     S     S  S      S      
2024-02-04 11:06:02,790 ========================================================================================================================
2024-02-04 11:06:04,633 Epoch 353: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.92 
2024-02-04 11:06:04,633 EPOCH 354
2024-02-04 11:06:15,799 Epoch 354: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.83 
2024-02-04 11:06:15,799 EPOCH 355
2024-02-04 11:06:26,497 Epoch 355: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.73 
2024-02-04 11:06:26,497 EPOCH 356
2024-02-04 11:06:37,006 Epoch 356: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.08 
2024-02-04 11:06:37,006 EPOCH 357
2024-02-04 11:06:47,425 Epoch 357: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.14 
2024-02-04 11:06:47,425 EPOCH 358
2024-02-04 11:06:58,076 Epoch 358: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.12 
2024-02-04 11:06:58,077 EPOCH 359
2024-02-04 11:07:08,014 [Epoch: 359 Step: 00006100] Batch Recognition Loss:   0.000421 => Gls Tokens per Sec:      877 || Batch Translation Loss:   0.129057 => Txt Tokens per Sec:     2458 || Lr: 0.000100
2024-02-04 11:07:08,618 Epoch 359: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.31 
2024-02-04 11:07:08,618 EPOCH 360
2024-02-04 11:07:19,386 Epoch 360: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.40 
2024-02-04 11:07:19,386 EPOCH 361
2024-02-04 11:07:30,009 Epoch 361: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.08 
2024-02-04 11:07:30,010 EPOCH 362
2024-02-04 11:07:40,693 Epoch 362: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.87 
2024-02-04 11:07:40,694 EPOCH 363
2024-02-04 11:07:51,286 Epoch 363: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.13 
2024-02-04 11:07:51,287 EPOCH 364
2024-02-04 11:08:02,065 Epoch 364: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.25 
2024-02-04 11:08:02,066 EPOCH 365
2024-02-04 11:08:08,858 [Epoch: 365 Step: 00006200] Batch Recognition Loss:   0.000617 => Gls Tokens per Sec:     1131 || Batch Translation Loss:   0.061730 => Txt Tokens per Sec:     3120 || Lr: 0.000100
2024-02-04 11:08:12,522 Epoch 365: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.31 
2024-02-04 11:08:12,522 EPOCH 366
2024-02-04 11:08:22,952 Epoch 366: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.63 
2024-02-04 11:08:22,952 EPOCH 367
2024-02-04 11:08:33,359 Epoch 367: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.24 
2024-02-04 11:08:33,360 EPOCH 368
2024-02-04 11:08:44,082 Epoch 368: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.22 
2024-02-04 11:08:44,082 EPOCH 369
2024-02-04 11:08:54,999 Epoch 369: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.69 
2024-02-04 11:08:55,000 EPOCH 370
2024-02-04 11:09:05,779 Epoch 370: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.35 
2024-02-04 11:09:05,780 EPOCH 371
2024-02-04 11:09:10,511 [Epoch: 371 Step: 00006300] Batch Recognition Loss:   0.000743 => Gls Tokens per Sec:     1353 || Batch Translation Loss:   0.162990 => Txt Tokens per Sec:     3656 || Lr: 0.000100
2024-02-04 11:09:16,128 Epoch 371: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.72 
2024-02-04 11:09:16,129 EPOCH 372
2024-02-04 11:09:26,689 Epoch 372: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.26 
2024-02-04 11:09:26,690 EPOCH 373
2024-02-04 11:09:37,185 Epoch 373: Total Training Recognition Loss 0.05  Total Training Translation Loss 14.61 
2024-02-04 11:09:37,185 EPOCH 374
2024-02-04 11:09:47,573 Epoch 374: Total Training Recognition Loss 0.05  Total Training Translation Loss 11.23 
2024-02-04 11:09:47,573 EPOCH 375
2024-02-04 11:09:58,255 Epoch 375: Total Training Recognition Loss 0.04  Total Training Translation Loss 7.04 
2024-02-04 11:09:58,255 EPOCH 376
2024-02-04 11:10:08,736 Epoch 376: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.01 
2024-02-04 11:10:08,737 EPOCH 377
2024-02-04 11:10:12,163 [Epoch: 377 Step: 00006400] Batch Recognition Loss:   0.000990 => Gls Tokens per Sec:     1495 || Batch Translation Loss:   0.188042 => Txt Tokens per Sec:     4215 || Lr: 0.000100
2024-02-04 11:10:19,556 Epoch 377: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.77 
2024-02-04 11:10:19,556 EPOCH 378
2024-02-04 11:10:30,025 Epoch 378: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.23 
2024-02-04 11:10:30,025 EPOCH 379
2024-02-04 11:10:40,408 Epoch 379: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.86 
2024-02-04 11:10:40,409 EPOCH 380
2024-02-04 11:10:51,212 Epoch 380: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.75 
2024-02-04 11:10:51,213 EPOCH 381
2024-02-04 11:11:01,656 Epoch 381: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.62 
2024-02-04 11:11:01,657 EPOCH 382
2024-02-04 11:11:12,291 Epoch 382: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.56 
2024-02-04 11:11:12,292 EPOCH 383
2024-02-04 11:11:13,615 [Epoch: 383 Step: 00006500] Batch Recognition Loss:   0.000547 => Gls Tokens per Sec:     2905 || Batch Translation Loss:   0.127508 => Txt Tokens per Sec:     8064 || Lr: 0.000100
2024-02-04 11:11:22,835 Epoch 383: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.42 
2024-02-04 11:11:22,835 EPOCH 384
2024-02-04 11:11:33,490 Epoch 384: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.34 
2024-02-04 11:11:33,491 EPOCH 385
2024-02-04 11:11:44,014 Epoch 385: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.25 
2024-02-04 11:11:44,015 EPOCH 386
2024-02-04 11:11:54,784 Epoch 386: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.22 
2024-02-04 11:11:54,785 EPOCH 387
2024-02-04 11:12:05,210 Epoch 387: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.38 
2024-02-04 11:12:05,211 EPOCH 388
2024-02-04 11:12:15,844 Epoch 388: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.41 
2024-02-04 11:12:15,845 EPOCH 389
2024-02-04 11:12:18,138 [Epoch: 389 Step: 00006600] Batch Recognition Loss:   0.000573 => Gls Tokens per Sec:     1117 || Batch Translation Loss:   0.064758 => Txt Tokens per Sec:     3268 || Lr: 0.000100
2024-02-04 11:12:26,274 Epoch 389: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.17 
2024-02-04 11:12:26,275 EPOCH 390
2024-02-04 11:12:38,062 Epoch 390: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.24 
2024-02-04 11:12:38,062 EPOCH 391
2024-02-04 11:12:48,587 Epoch 391: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.33 
2024-02-04 11:12:48,588 EPOCH 392
2024-02-04 11:12:59,250 Epoch 392: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.99 
2024-02-04 11:12:59,250 EPOCH 393
2024-02-04 11:13:09,811 Epoch 393: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.01 
2024-02-04 11:13:09,812 EPOCH 394
2024-02-04 11:13:20,333 Epoch 394: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.04 
2024-02-04 11:13:20,334 EPOCH 395
2024-02-04 11:13:20,659 [Epoch: 395 Step: 00006700] Batch Recognition Loss:   0.000429 => Gls Tokens per Sec:     3951 || Batch Translation Loss:   0.034483 => Txt Tokens per Sec:     9904 || Lr: 0.000100
2024-02-04 11:13:30,640 Epoch 395: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.02 
2024-02-04 11:13:30,640 EPOCH 396
2024-02-04 11:13:41,508 Epoch 396: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.00 
2024-02-04 11:13:41,509 EPOCH 397
2024-02-04 11:13:52,031 Epoch 397: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.13 
2024-02-04 11:13:52,032 EPOCH 398
2024-02-04 11:14:02,667 Epoch 398: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.04 
2024-02-04 11:14:02,668 EPOCH 399
2024-02-04 11:14:13,265 Epoch 399: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.19 
2024-02-04 11:14:13,266 EPOCH 400
2024-02-04 11:14:23,709 [Epoch: 400 Step: 00006800] Batch Recognition Loss:   0.000340 => Gls Tokens per Sec:     1018 || Batch Translation Loss:   0.048013 => Txt Tokens per Sec:     2826 || Lr: 0.000100
2024-02-04 11:14:23,709 Epoch 400: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.20 
2024-02-04 11:14:23,709 EPOCH 401
2024-02-04 11:14:34,145 Epoch 401: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.37 
2024-02-04 11:14:34,145 EPOCH 402
2024-02-04 11:14:44,943 Epoch 402: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.44 
2024-02-04 11:14:44,944 EPOCH 403
2024-02-04 11:14:55,347 Epoch 403: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.55 
2024-02-04 11:14:55,347 EPOCH 404
2024-02-04 11:15:05,997 Epoch 404: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.40 
2024-02-04 11:15:05,998 EPOCH 405
2024-02-04 11:15:16,554 Epoch 405: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.17 
2024-02-04 11:15:16,555 EPOCH 406
2024-02-04 11:15:26,568 [Epoch: 406 Step: 00006900] Batch Recognition Loss:   0.000437 => Gls Tokens per Sec:      934 || Batch Translation Loss:   0.061252 => Txt Tokens per Sec:     2558 || Lr: 0.000100
2024-02-04 11:15:27,155 Epoch 406: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.15 
2024-02-04 11:15:27,155 EPOCH 407
2024-02-04 11:15:37,596 Epoch 407: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.25 
2024-02-04 11:15:37,597 EPOCH 408
2024-02-04 11:15:48,075 Epoch 408: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.23 
2024-02-04 11:15:48,076 EPOCH 409
2024-02-04 11:15:58,807 Epoch 409: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.40 
2024-02-04 11:15:58,807 EPOCH 410
2024-02-04 11:16:09,404 Epoch 410: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.24 
2024-02-04 11:16:09,404 EPOCH 411
2024-02-04 11:16:20,134 Epoch 411: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.36 
2024-02-04 11:16:20,134 EPOCH 412
2024-02-04 11:16:28,451 [Epoch: 412 Step: 00007000] Batch Recognition Loss:   0.000273 => Gls Tokens per Sec:      971 || Batch Translation Loss:   0.034171 => Txt Tokens per Sec:     2712 || Lr: 0.000100
2024-02-04 11:16:30,610 Epoch 412: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.27 
2024-02-04 11:16:30,610 EPOCH 413
2024-02-04 11:16:41,005 Epoch 413: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.51 
2024-02-04 11:16:41,006 EPOCH 414
2024-02-04 11:16:51,524 Epoch 414: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.34 
2024-02-04 11:16:51,524 EPOCH 415
2024-02-04 11:17:01,959 Epoch 415: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.11 
2024-02-04 11:17:01,960 EPOCH 416
2024-02-04 11:17:12,414 Epoch 416: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.40 
2024-02-04 11:17:12,415 EPOCH 417
2024-02-04 11:17:23,100 Epoch 417: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.12 
2024-02-04 11:17:23,100 EPOCH 418
2024-02-04 11:17:29,448 [Epoch: 418 Step: 00007100] Batch Recognition Loss:   0.000428 => Gls Tokens per Sec:     1070 || Batch Translation Loss:   0.132729 => Txt Tokens per Sec:     2915 || Lr: 0.000100
2024-02-04 11:17:33,806 Epoch 418: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.32 
2024-02-04 11:17:33,806 EPOCH 419
2024-02-04 11:17:44,534 Epoch 419: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.25 
2024-02-04 11:17:44,535 EPOCH 420
2024-02-04 11:17:55,089 Epoch 420: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.52 
2024-02-04 11:17:55,089 EPOCH 421
2024-02-04 11:18:05,548 Epoch 421: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.35 
2024-02-04 11:18:05,549 EPOCH 422
2024-02-04 11:18:16,205 Epoch 422: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.45 
2024-02-04 11:18:16,205 EPOCH 423
2024-02-04 11:18:26,985 Epoch 423: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.16 
2024-02-04 11:18:26,985 EPOCH 424
2024-02-04 11:18:30,221 [Epoch: 424 Step: 00007200] Batch Recognition Loss:   0.000348 => Gls Tokens per Sec:     1781 || Batch Translation Loss:   0.112091 => Txt Tokens per Sec:     4882 || Lr: 0.000100
2024-02-04 11:18:37,400 Epoch 424: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.78 
2024-02-04 11:18:37,400 EPOCH 425
2024-02-04 11:18:47,884 Epoch 425: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.72 
2024-02-04 11:18:47,885 EPOCH 426
2024-02-04 11:18:58,291 Epoch 426: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.69 
2024-02-04 11:18:58,291 EPOCH 427
2024-02-04 11:19:08,935 Epoch 427: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.31 
2024-02-04 11:19:08,935 EPOCH 428
2024-02-04 11:19:19,454 Epoch 428: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.31 
2024-02-04 11:19:19,455 EPOCH 429
2024-02-04 11:19:29,874 Epoch 429: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.22 
2024-02-04 11:19:29,875 EPOCH 430
2024-02-04 11:19:35,217 [Epoch: 430 Step: 00007300] Batch Recognition Loss:   0.000397 => Gls Tokens per Sec:      792 || Batch Translation Loss:   0.113325 => Txt Tokens per Sec:     2228 || Lr: 0.000100
2024-02-04 11:19:40,546 Epoch 430: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.28 
2024-02-04 11:19:40,546 EPOCH 431
2024-02-04 11:19:50,968 Epoch 431: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.30 
2024-02-04 11:19:50,969 EPOCH 432
2024-02-04 11:20:01,480 Epoch 432: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.48 
2024-02-04 11:20:01,481 EPOCH 433
2024-02-04 11:20:12,109 Epoch 433: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.76 
2024-02-04 11:20:12,109 EPOCH 434
2024-02-04 11:20:22,883 Epoch 434: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.83 
2024-02-04 11:20:22,884 EPOCH 435
2024-02-04 11:20:33,429 Epoch 435: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.95 
2024-02-04 11:20:33,430 EPOCH 436
2024-02-04 11:20:34,322 [Epoch: 436 Step: 00007400] Batch Recognition Loss:   0.000322 => Gls Tokens per Sec:     3591 || Batch Translation Loss:   0.089727 => Txt Tokens per Sec:     8750 || Lr: 0.000100
2024-02-04 11:20:43,898 Epoch 436: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.91 
2024-02-04 11:20:43,899 EPOCH 437
2024-02-04 11:20:54,442 Epoch 437: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.60 
2024-02-04 11:20:54,442 EPOCH 438
2024-02-04 11:21:04,846 Epoch 438: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.49 
2024-02-04 11:21:04,847 EPOCH 439
2024-02-04 11:21:15,609 Epoch 439: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.66 
2024-02-04 11:21:15,610 EPOCH 440
2024-02-04 11:21:26,345 Epoch 440: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.83 
2024-02-04 11:21:26,345 EPOCH 441
2024-02-04 11:21:37,044 Epoch 441: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.78 
2024-02-04 11:21:37,044 EPOCH 442
2024-02-04 11:21:40,152 [Epoch: 442 Step: 00007500] Batch Recognition Loss:   0.000375 => Gls Tokens per Sec:      538 || Batch Translation Loss:   0.067654 => Txt Tokens per Sec:     1644 || Lr: 0.000100
2024-02-04 11:21:47,774 Epoch 442: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.78 
2024-02-04 11:21:47,775 EPOCH 443
2024-02-04 11:21:58,096 Epoch 443: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.55 
2024-02-04 11:21:58,096 EPOCH 444
2024-02-04 11:22:08,814 Epoch 444: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.57 
2024-02-04 11:22:08,815 EPOCH 445
2024-02-04 11:22:19,229 Epoch 445: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.52 
2024-02-04 11:22:19,230 EPOCH 446
2024-02-04 11:22:29,619 Epoch 446: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.51 
2024-02-04 11:22:29,619 EPOCH 447
2024-02-04 11:22:40,418 Epoch 447: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.37 
2024-02-04 11:22:40,418 EPOCH 448
2024-02-04 11:22:40,668 [Epoch: 448 Step: 00007600] Batch Recognition Loss:   0.000277 => Gls Tokens per Sec:     2577 || Batch Translation Loss:   0.040442 => Txt Tokens per Sec:     6176 || Lr: 0.000100
2024-02-04 11:22:51,035 Epoch 448: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.44 
2024-02-04 11:22:51,035 EPOCH 449
2024-02-04 11:23:01,767 Epoch 449: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.76 
2024-02-04 11:23:01,767 EPOCH 450
2024-02-04 11:23:12,340 Epoch 450: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.53 
2024-02-04 11:23:12,340 EPOCH 451
2024-02-04 11:23:22,831 Epoch 451: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.33 
2024-02-04 11:23:22,832 EPOCH 452
2024-02-04 11:23:33,324 Epoch 452: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.42 
2024-02-04 11:23:33,325 EPOCH 453
2024-02-04 11:23:41,221 [Epoch: 453 Step: 00007700] Batch Recognition Loss:   0.000325 => Gls Tokens per Sec:     1297 || Batch Translation Loss:   0.051515 => Txt Tokens per Sec:     3560 || Lr: 0.000100
2024-02-04 11:23:43,791 Epoch 453: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.77 
2024-02-04 11:23:43,791 EPOCH 454
2024-02-04 11:23:54,049 Epoch 454: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.58 
2024-02-04 11:23:54,050 EPOCH 455
2024-02-04 11:24:04,568 Epoch 455: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.83 
2024-02-04 11:24:04,568 EPOCH 456
2024-02-04 11:24:14,879 Epoch 456: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.98 
2024-02-04 11:24:14,879 EPOCH 457
2024-02-04 11:24:25,389 Epoch 457: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.89 
2024-02-04 11:24:25,389 EPOCH 458
2024-02-04 11:24:35,774 Epoch 458: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.03 
2024-02-04 11:24:35,775 EPOCH 459
2024-02-04 11:24:44,508 [Epoch: 459 Step: 00007800] Batch Recognition Loss:   0.000614 => Gls Tokens per Sec:      997 || Batch Translation Loss:   0.193054 => Txt Tokens per Sec:     2748 || Lr: 0.000100
2024-02-04 11:24:46,444 Epoch 459: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.74 
2024-02-04 11:24:46,444 EPOCH 460
2024-02-04 11:24:56,928 Epoch 460: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.68 
2024-02-04 11:24:56,929 EPOCH 461
2024-02-04 11:25:07,427 Epoch 461: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.60 
2024-02-04 11:25:07,428 EPOCH 462
2024-02-04 11:25:17,833 Epoch 462: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.50 
2024-02-04 11:25:17,833 EPOCH 463
2024-02-04 11:25:28,475 Epoch 463: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.24 
2024-02-04 11:25:28,475 EPOCH 464
2024-02-04 11:25:38,992 Epoch 464: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.44 
2024-02-04 11:25:38,993 EPOCH 465
2024-02-04 11:25:44,333 [Epoch: 465 Step: 00007900] Batch Recognition Loss:   0.000181 => Gls Tokens per Sec:     1438 || Batch Translation Loss:   0.054320 => Txt Tokens per Sec:     3933 || Lr: 0.000100
2024-02-04 11:25:49,403 Epoch 465: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.76 
2024-02-04 11:25:49,404 EPOCH 466
2024-02-04 11:25:59,917 Epoch 466: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.66 
2024-02-04 11:25:59,918 EPOCH 467
2024-02-04 11:26:10,506 Epoch 467: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.87 
2024-02-04 11:26:10,507 EPOCH 468
2024-02-04 11:26:20,980 Epoch 468: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.42 
2024-02-04 11:26:20,981 EPOCH 469
2024-02-04 11:26:31,414 Epoch 469: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.28 
2024-02-04 11:26:31,414 EPOCH 470
2024-02-04 11:26:42,107 Epoch 470: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.17 
2024-02-04 11:26:42,108 EPOCH 471
2024-02-04 11:26:49,854 [Epoch: 471 Step: 00008000] Batch Recognition Loss:   0.000223 => Gls Tokens per Sec:      794 || Batch Translation Loss:   0.059231 => Txt Tokens per Sec:     2227 || Lr: 0.000100
2024-02-04 11:27:27,600 Hooray! New best validation result [eval_metric]!
2024-02-04 11:27:27,602 Saving new checkpoint.
2024-02-04 11:27:27,918 Validation result at epoch 471, step     8000: duration: 38.0637s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00030	Translation Loss: 92136.25781	PPL: 10097.87598
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.96	(BLEU-1: 11.87,	BLEU-2: 3.98,	BLEU-3: 1.72,	BLEU-4: 0.96)
	CHRF 17.76	ROUGE 10.10
2024-02-04 11:27:27,921 Logging Recognition and Translation Outputs
2024-02-04 11:27:27,921 ========================================================================================================================
2024-02-04 11:27:27,921 Logging Sequence: 88_57.00
2024-02-04 11:27:27,922 	Gloss Reference :	A B+C+D+E
2024-02-04 11:27:27,922 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 11:27:27,922 	Gloss Alignment :	         
2024-02-04 11:27:27,922 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 11:27:27,925 	Text Reference  :	which   stated  messi we're  waiting for  you     to   come here      you  will be finished when    you come
2024-02-04 11:27:27,925 	Text Hypothesis :	notably rosario has   become the     most violent city in   argentina with 250  to 300      murders in  2022
2024-02-04 11:27:27,925 	Text Alignment  :	S       S       S     S      S       S    S       S    S    S         S    S    S  S        S       S   S   
2024-02-04 11:27:27,925 ========================================================================================================================
2024-02-04 11:27:27,925 Logging Sequence: 171_142.00
2024-02-04 11:27:27,926 	Gloss Reference :	A B+C+D+E
2024-02-04 11:27:27,926 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 11:27:27,926 	Gloss Alignment :	         
2024-02-04 11:27:27,926 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 11:27:27,927 	Text Reference  :	this  decision on     dhoni made      a  significant impact as     pathirana claimed two   tough wickets 
2024-02-04 11:27:27,927 	Text Hypothesis :	since the      couple were  residents of mumbai      the    finals against   each    other for   covid-19
2024-02-04 11:27:27,928 	Text Alignment  :	S     S        S      S     S         S  S           S      S      S         S       S     S     S       
2024-02-04 11:27:27,928 ========================================================================================================================
2024-02-04 11:27:27,928 Logging Sequence: 125_207.00
2024-02-04 11:27:27,928 	Gloss Reference :	A B+C+D+E
2024-02-04 11:27:27,928 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 11:27:27,928 	Gloss Alignment :	         
2024-02-04 11:27:27,928 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 11:27:27,929 	Text Reference  :	he  had not  practised since he   returned and he        had  also fallen sick
2024-02-04 11:27:27,930 	Text Hypothesis :	you all know the       bcci  were supposed to  australia with the  first  time
2024-02-04 11:27:27,930 	Text Alignment  :	S   S   S    S         S     S    S        S   S         S    S    S      S   
2024-02-04 11:27:27,930 ========================================================================================================================
2024-02-04 11:27:27,930 Logging Sequence: 68_230.00
2024-02-04 11:27:27,930 	Gloss Reference :	A B+C+D+E
2024-02-04 11:27:27,930 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 11:27:27,930 	Gloss Alignment :	         
2024-02-04 11:27:27,931 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 11:27:27,932 	Text Reference  :	*** let     us        know   what  you think in   the ***** ********** *** **** comments below
2024-02-04 11:27:27,932 	Text Hypothesis :	new zealand cricketer suresh raina did not   find the brand ambassador and also called   dhoni
2024-02-04 11:27:27,932 	Text Alignment  :	I   S       S         S      S     S   S     S        I     I          I   I    S        S    
2024-02-04 11:27:27,932 ========================================================================================================================
2024-02-04 11:27:27,932 Logging Sequence: 126_82.00
2024-02-04 11:27:27,932 	Gloss Reference :	A B+C+D+E
2024-02-04 11:27:27,933 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 11:27:27,933 	Gloss Alignment :	         
2024-02-04 11:27:27,933 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 11:27:27,934 	Text Reference  :	neeraj also dedicated his gold medal to former indian olympians who     came  close to  winning medals  
2024-02-04 11:27:27,934 	Text Hypothesis :	****** he   won       a   gold medal ** ****** ****** in        javelin throw at    the 2012    olympics
2024-02-04 11:27:27,934 	Text Alignment  :	D      S    S         S              D  D      D      S         S       S     S     S   S       S       
2024-02-04 11:27:27,934 ========================================================================================================================
2024-02-04 11:27:31,337 Epoch 471: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.16 
2024-02-04 11:27:31,338 EPOCH 472
2024-02-04 11:27:42,378 Epoch 472: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.35 
2024-02-04 11:27:42,379 EPOCH 473
2024-02-04 11:27:53,006 Epoch 473: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.14 
2024-02-04 11:27:53,007 EPOCH 474
2024-02-04 11:28:03,478 Epoch 474: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.41 
2024-02-04 11:28:03,478 EPOCH 475
2024-02-04 11:28:14,136 Epoch 475: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.29 
2024-02-04 11:28:14,136 EPOCH 476
2024-02-04 11:28:24,776 Epoch 476: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.41 
2024-02-04 11:28:24,776 EPOCH 477
2024-02-04 11:28:30,863 [Epoch: 477 Step: 00008100] Batch Recognition Loss:   0.000318 => Gls Tokens per Sec:      841 || Batch Translation Loss:   0.054520 => Txt Tokens per Sec:     2520 || Lr: 0.000100
2024-02-04 11:28:35,164 Epoch 477: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.20 
2024-02-04 11:28:35,165 EPOCH 478
2024-02-04 11:28:45,655 Epoch 478: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.01 
2024-02-04 11:28:45,655 EPOCH 479
2024-02-04 11:28:56,181 Epoch 479: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.04 
2024-02-04 11:28:56,181 EPOCH 480
2024-02-04 11:29:06,634 Epoch 480: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.68 
2024-02-04 11:29:06,635 EPOCH 481
2024-02-04 11:29:17,291 Epoch 481: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.51 
2024-02-04 11:29:17,291 EPOCH 482
2024-02-04 11:29:28,116 Epoch 482: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.46 
2024-02-04 11:29:28,117 EPOCH 483
2024-02-04 11:29:32,136 [Epoch: 483 Step: 00008200] Batch Recognition Loss:   0.000658 => Gls Tokens per Sec:      956 || Batch Translation Loss:   0.213823 => Txt Tokens per Sec:     2572 || Lr: 0.000100
2024-02-04 11:29:38,602 Epoch 483: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.40 
2024-02-04 11:29:38,603 EPOCH 484
2024-02-04 11:29:49,191 Epoch 484: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.56 
2024-02-04 11:29:49,192 EPOCH 485
2024-02-04 11:29:59,886 Epoch 485: Total Training Recognition Loss 0.01  Total Training Translation Loss 6.89 
2024-02-04 11:29:59,887 EPOCH 486
2024-02-04 11:30:10,664 Epoch 486: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.03 
2024-02-04 11:30:10,665 EPOCH 487
2024-02-04 11:30:21,203 Epoch 487: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.19 
2024-02-04 11:30:21,204 EPOCH 488
2024-02-04 11:30:31,965 Epoch 488: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.96 
2024-02-04 11:30:31,966 EPOCH 489
2024-02-04 11:30:32,765 [Epoch: 489 Step: 00008300] Batch Recognition Loss:   0.000837 => Gls Tokens per Sec:     3208 || Batch Translation Loss:   0.138547 => Txt Tokens per Sec:     8203 || Lr: 0.000100
2024-02-04 11:30:42,377 Epoch 489: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.95 
2024-02-04 11:30:42,378 EPOCH 490
2024-02-04 11:30:53,093 Epoch 490: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.85 
2024-02-04 11:30:53,094 EPOCH 491
2024-02-04 11:31:03,835 Epoch 491: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.38 
2024-02-04 11:31:03,835 EPOCH 492
2024-02-04 11:31:14,085 Epoch 492: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.33 
2024-02-04 11:31:14,086 EPOCH 493
2024-02-04 11:31:25,118 Epoch 493: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.08 
2024-02-04 11:31:25,118 EPOCH 494
2024-02-04 11:31:35,685 Epoch 494: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.86 
2024-02-04 11:31:35,686 EPOCH 495
2024-02-04 11:31:36,202 [Epoch: 495 Step: 00008400] Batch Recognition Loss:   0.000340 => Gls Tokens per Sec:     2481 || Batch Translation Loss:   0.050310 => Txt Tokens per Sec:     6798 || Lr: 0.000100
2024-02-04 11:31:46,302 Epoch 495: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.83 
2024-02-04 11:31:46,302 EPOCH 496
2024-02-04 11:31:57,166 Epoch 496: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.02 
2024-02-04 11:31:57,166 EPOCH 497
2024-02-04 11:32:07,967 Epoch 497: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.89 
2024-02-04 11:32:07,968 EPOCH 498
2024-02-04 11:32:18,751 Epoch 498: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.94 
2024-02-04 11:32:18,751 EPOCH 499
2024-02-04 11:32:29,345 Epoch 499: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.77 
2024-02-04 11:32:29,345 EPOCH 500
2024-02-04 11:32:40,008 [Epoch: 500 Step: 00008500] Batch Recognition Loss:   0.000142 => Gls Tokens per Sec:      997 || Batch Translation Loss:   0.049710 => Txt Tokens per Sec:     2768 || Lr: 0.000100
2024-02-04 11:32:40,009 Epoch 500: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.13 
2024-02-04 11:32:40,009 EPOCH 501
2024-02-04 11:32:50,416 Epoch 501: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.16 
2024-02-04 11:32:50,416 EPOCH 502
2024-02-04 11:33:00,800 Epoch 502: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.03 
2024-02-04 11:33:00,801 EPOCH 503
2024-02-04 11:33:11,416 Epoch 503: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.15 
2024-02-04 11:33:11,416 EPOCH 504
2024-02-04 11:33:21,973 Epoch 504: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.98 
2024-02-04 11:33:21,973 EPOCH 505
2024-02-04 11:33:32,672 Epoch 505: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.16 
2024-02-04 11:33:32,672 EPOCH 506
2024-02-04 11:33:38,937 [Epoch: 506 Step: 00008600] Batch Recognition Loss:   0.000265 => Gls Tokens per Sec:     1533 || Batch Translation Loss:   0.029422 => Txt Tokens per Sec:     4126 || Lr: 0.000100
2024-02-04 11:33:43,109 Epoch 506: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.03 
2024-02-04 11:33:43,109 EPOCH 507
2024-02-04 11:33:53,668 Epoch 507: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.98 
2024-02-04 11:33:53,669 EPOCH 508
2024-02-04 11:34:04,300 Epoch 508: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.03 
2024-02-04 11:34:04,301 EPOCH 509
2024-02-04 11:34:14,943 Epoch 509: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.87 
2024-02-04 11:34:14,944 EPOCH 510
2024-02-04 11:34:25,692 Epoch 510: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-04 11:34:25,692 EPOCH 511
2024-02-04 11:34:36,398 Epoch 511: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.71 
2024-02-04 11:34:36,399 EPOCH 512
2024-02-04 11:34:43,652 [Epoch: 512 Step: 00008700] Batch Recognition Loss:   0.000160 => Gls Tokens per Sec:     1147 || Batch Translation Loss:   0.097201 => Txt Tokens per Sec:     3179 || Lr: 0.000100
2024-02-04 11:34:47,051 Epoch 512: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.80 
2024-02-04 11:34:47,051 EPOCH 513
2024-02-04 11:34:57,676 Epoch 513: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-04 11:34:57,677 EPOCH 514
2024-02-04 11:35:08,238 Epoch 514: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-04 11:35:08,238 EPOCH 515
2024-02-04 11:35:19,004 Epoch 515: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-04 11:35:19,005 EPOCH 516
2024-02-04 11:35:29,554 Epoch 516: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.55 
2024-02-04 11:35:29,555 EPOCH 517
2024-02-04 11:35:40,314 Epoch 517: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.57 
2024-02-04 11:35:40,314 EPOCH 518
2024-02-04 11:35:46,309 [Epoch: 518 Step: 00008800] Batch Recognition Loss:   0.000194 => Gls Tokens per Sec:     1133 || Batch Translation Loss:   0.028677 => Txt Tokens per Sec:     2943 || Lr: 0.000100
2024-02-04 11:35:50,876 Epoch 518: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.64 
2024-02-04 11:35:50,877 EPOCH 519
2024-02-04 11:36:01,532 Epoch 519: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.78 
2024-02-04 11:36:01,532 EPOCH 520
2024-02-04 11:36:12,033 Epoch 520: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.97 
2024-02-04 11:36:12,034 EPOCH 521
2024-02-04 11:36:22,734 Epoch 521: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.75 
2024-02-04 11:36:22,735 EPOCH 522
2024-02-04 11:36:33,323 Epoch 522: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.73 
2024-02-04 11:36:33,324 EPOCH 523
2024-02-04 11:36:43,863 Epoch 523: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.89 
2024-02-04 11:36:43,864 EPOCH 524
2024-02-04 11:36:52,628 [Epoch: 524 Step: 00008900] Batch Recognition Loss:   0.000191 => Gls Tokens per Sec:      629 || Batch Translation Loss:   0.130639 => Txt Tokens per Sec:     1880 || Lr: 0.000100
2024-02-04 11:36:54,278 Epoch 524: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.90 
2024-02-04 11:36:54,278 EPOCH 525
2024-02-04 11:37:04,593 Epoch 525: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.08 
2024-02-04 11:37:04,594 EPOCH 526
2024-02-04 11:37:15,585 Epoch 526: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.99 
2024-02-04 11:37:15,586 EPOCH 527
2024-02-04 11:37:26,327 Epoch 527: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.84 
2024-02-04 11:37:26,328 EPOCH 528
2024-02-04 11:37:36,848 Epoch 528: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.14 
2024-02-04 11:37:36,848 EPOCH 529
2024-02-04 11:37:47,193 Epoch 529: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.30 
2024-02-04 11:37:47,194 EPOCH 530
2024-02-04 11:37:52,352 [Epoch: 530 Step: 00009000] Batch Recognition Loss:   0.000213 => Gls Tokens per Sec:      820 || Batch Translation Loss:   0.135948 => Txt Tokens per Sec:     2274 || Lr: 0.000100
2024-02-04 11:37:57,621 Epoch 530: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.50 
2024-02-04 11:37:57,621 EPOCH 531
2024-02-04 11:38:08,011 Epoch 531: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.57 
2024-02-04 11:38:08,012 EPOCH 532
2024-02-04 11:38:18,911 Epoch 532: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.83 
2024-02-04 11:38:18,911 EPOCH 533
2024-02-04 11:38:29,552 Epoch 533: Total Training Recognition Loss 0.00  Total Training Translation Loss 2.20 
2024-02-04 11:38:29,553 EPOCH 534
2024-02-04 11:38:40,019 Epoch 534: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.31 
2024-02-04 11:38:40,019 EPOCH 535
2024-02-04 11:38:50,674 Epoch 535: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.73 
2024-02-04 11:38:50,674 EPOCH 536
2024-02-04 11:38:54,926 [Epoch: 536 Step: 00009100] Batch Recognition Loss:   0.000365 => Gls Tokens per Sec:      753 || Batch Translation Loss:   0.088477 => Txt Tokens per Sec:     2295 || Lr: 0.000100
2024-02-04 11:39:01,162 Epoch 536: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.66 
2024-02-04 11:39:01,162 EPOCH 537
2024-02-04 11:39:11,809 Epoch 537: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.25 
2024-02-04 11:39:11,809 EPOCH 538
2024-02-04 11:39:22,561 Epoch 538: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.40 
2024-02-04 11:39:22,561 EPOCH 539
2024-02-04 11:39:33,383 Epoch 539: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.29 
2024-02-04 11:39:33,384 EPOCH 540
2024-02-04 11:39:44,031 Epoch 540: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.92 
2024-02-04 11:39:44,032 EPOCH 541
2024-02-04 11:39:54,575 Epoch 541: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.59 
2024-02-04 11:39:54,576 EPOCH 542
2024-02-04 11:40:00,616 [Epoch: 542 Step: 00009200] Batch Recognition Loss:   0.000245 => Gls Tokens per Sec:      277 || Batch Translation Loss:   0.034615 => Txt Tokens per Sec:      972 || Lr: 0.000100
2024-02-04 11:40:04,979 Epoch 542: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.37 
2024-02-04 11:40:04,979 EPOCH 543
2024-02-04 11:40:15,617 Epoch 543: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.08 
2024-02-04 11:40:15,618 EPOCH 544
2024-02-04 11:40:26,353 Epoch 544: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-04 11:40:26,353 EPOCH 545
2024-02-04 11:40:36,825 Epoch 545: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-04 11:40:36,826 EPOCH 546
2024-02-04 11:40:47,380 Epoch 546: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.31 
2024-02-04 11:40:47,380 EPOCH 547
2024-02-04 11:40:57,819 Epoch 547: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.28 
2024-02-04 11:40:57,819 EPOCH 548
2024-02-04 11:40:57,971 [Epoch: 548 Step: 00009300] Batch Recognition Loss:   0.000317 => Gls Tokens per Sec:     4257 || Batch Translation Loss:   0.061438 => Txt Tokens per Sec:     9731 || Lr: 0.000100
2024-02-04 11:41:08,411 Epoch 548: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.30 
2024-02-04 11:41:08,412 EPOCH 549
2024-02-04 11:41:18,808 Epoch 549: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.05 
2024-02-04 11:41:18,808 EPOCH 550
2024-02-04 11:41:29,671 Epoch 550: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.05 
2024-02-04 11:41:29,672 EPOCH 551
2024-02-04 11:41:40,198 Epoch 551: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.96 
2024-02-04 11:41:40,198 EPOCH 552
2024-02-04 11:41:50,797 Epoch 552: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.02 
2024-02-04 11:41:50,797 EPOCH 553
2024-02-04 11:41:59,746 [Epoch: 553 Step: 00009400] Batch Recognition Loss:   0.000199 => Gls Tokens per Sec:     1117 || Batch Translation Loss:   0.093959 => Txt Tokens per Sec:     3068 || Lr: 0.000100
2024-02-04 11:42:01,300 Epoch 553: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.07 
2024-02-04 11:42:01,300 EPOCH 554
2024-02-04 11:42:12,057 Epoch 554: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.89 
2024-02-04 11:42:12,058 EPOCH 555
2024-02-04 11:42:22,953 Epoch 555: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.99 
2024-02-04 11:42:22,954 EPOCH 556
2024-02-04 11:42:33,475 Epoch 556: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.79 
2024-02-04 11:42:33,476 EPOCH 557
2024-02-04 11:42:44,365 Epoch 557: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.79 
2024-02-04 11:42:44,366 EPOCH 558
2024-02-04 11:42:55,389 Epoch 558: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.60 
2024-02-04 11:42:55,390 EPOCH 559
2024-02-04 11:43:04,292 [Epoch: 559 Step: 00009500] Batch Recognition Loss:   0.000226 => Gls Tokens per Sec:      978 || Batch Translation Loss:   0.047877 => Txt Tokens per Sec:     2637 || Lr: 0.000100
2024-02-04 11:43:06,433 Epoch 559: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.77 
2024-02-04 11:43:06,434 EPOCH 560
2024-02-04 11:43:17,302 Epoch 560: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.73 
2024-02-04 11:43:17,302 EPOCH 561
2024-02-04 11:43:27,765 Epoch 561: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.93 
2024-02-04 11:43:27,766 EPOCH 562
2024-02-04 11:43:38,372 Epoch 562: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.05 
2024-02-04 11:43:38,373 EPOCH 563
2024-02-04 11:43:48,762 Epoch 563: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.98 
2024-02-04 11:43:48,763 EPOCH 564
2024-02-04 11:43:59,409 Epoch 564: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.83 
2024-02-04 11:43:59,410 EPOCH 565
2024-02-04 11:44:05,051 [Epoch: 565 Step: 00009600] Batch Recognition Loss:   0.000132 => Gls Tokens per Sec:     1362 || Batch Translation Loss:   0.031693 => Txt Tokens per Sec:     3808 || Lr: 0.000100
2024-02-04 11:44:09,926 Epoch 565: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.01 
2024-02-04 11:44:09,927 EPOCH 566
2024-02-04 11:44:20,748 Epoch 566: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.11 
2024-02-04 11:44:20,749 EPOCH 567
2024-02-04 11:44:31,405 Epoch 567: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.17 
2024-02-04 11:44:31,406 EPOCH 568
2024-02-04 11:44:41,785 Epoch 568: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.29 
2024-02-04 11:44:41,785 EPOCH 569
2024-02-04 11:44:52,356 Epoch 569: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.56 
2024-02-04 11:44:52,357 EPOCH 570
2024-02-04 11:45:02,963 Epoch 570: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.56 
2024-02-04 11:45:02,963 EPOCH 571
2024-02-04 11:45:12,302 [Epoch: 571 Step: 00009700] Batch Recognition Loss:   0.000515 => Gls Tokens per Sec:      659 || Batch Translation Loss:   0.089149 => Txt Tokens per Sec:     2054 || Lr: 0.000100
2024-02-04 11:45:13,502 Epoch 571: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.40 
2024-02-04 11:45:13,502 EPOCH 572
2024-02-04 11:45:24,011 Epoch 572: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.63 
2024-02-04 11:45:24,012 EPOCH 573
2024-02-04 11:45:34,684 Epoch 573: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.36 
2024-02-04 11:45:34,685 EPOCH 574
2024-02-04 11:45:45,159 Epoch 574: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.56 
2024-02-04 11:45:45,159 EPOCH 575
2024-02-04 11:45:55,773 Epoch 575: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.38 
2024-02-04 11:45:55,774 EPOCH 576
2024-02-04 11:46:06,344 Epoch 576: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.54 
2024-02-04 11:46:06,344 EPOCH 577
2024-02-04 11:46:09,167 [Epoch: 577 Step: 00009800] Batch Recognition Loss:   0.000254 => Gls Tokens per Sec:     1815 || Batch Translation Loss:   0.078486 => Txt Tokens per Sec:     4559 || Lr: 0.000100
2024-02-04 11:46:16,842 Epoch 577: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.23 
2024-02-04 11:46:16,842 EPOCH 578
2024-02-04 11:46:27,324 Epoch 578: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.44 
2024-02-04 11:46:27,325 EPOCH 579
2024-02-04 11:46:38,165 Epoch 579: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.45 
2024-02-04 11:46:38,166 EPOCH 580
2024-02-04 11:46:48,719 Epoch 580: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.49 
2024-02-04 11:46:48,720 EPOCH 581
2024-02-04 11:46:59,537 Epoch 581: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.44 
2024-02-04 11:46:59,538 EPOCH 582
2024-02-04 11:47:10,107 Epoch 582: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.29 
2024-02-04 11:47:10,107 EPOCH 583
2024-02-04 11:47:15,000 [Epoch: 583 Step: 00009900] Batch Recognition Loss:   0.000303 => Gls Tokens per Sec:      734 || Batch Translation Loss:   0.057311 => Txt Tokens per Sec:     1949 || Lr: 0.000100
2024-02-04 11:47:20,590 Epoch 583: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.72 
2024-02-04 11:47:20,591 EPOCH 584
2024-02-04 11:47:31,159 Epoch 584: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.61 
2024-02-04 11:47:31,160 EPOCH 585
2024-02-04 11:47:41,770 Epoch 585: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.54 
2024-02-04 11:47:41,771 EPOCH 586
2024-02-04 11:47:52,184 Epoch 586: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.28 
2024-02-04 11:47:52,185 EPOCH 587
2024-02-04 11:48:02,666 Epoch 587: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.10 
2024-02-04 11:48:02,667 EPOCH 588
2024-02-04 11:48:13,241 Epoch 588: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.02 
2024-02-04 11:48:13,242 EPOCH 589
2024-02-04 11:48:14,036 [Epoch: 589 Step: 00010000] Batch Recognition Loss:   0.000242 => Gls Tokens per Sec:     3227 || Batch Translation Loss:   0.033036 => Txt Tokens per Sec:     8121 || Lr: 0.000100
2024-02-04 11:48:51,794 Hooray! New best validation result [eval_metric]!
2024-02-04 11:48:51,796 Saving new checkpoint.
2024-02-04 11:48:52,078 Validation result at epoch 589, step    10000: duration: 38.0408s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00026	Translation Loss: 94945.04688	PPL: 13375.17773
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.99	(BLEU-1: 11.21,	BLEU-2: 3.83,	BLEU-3: 1.78,	BLEU-4: 0.99)
	CHRF 17.26	ROUGE 9.58
2024-02-04 11:48:52,079 Logging Recognition and Translation Outputs
2024-02-04 11:48:52,080 ========================================================================================================================
2024-02-04 11:48:52,080 Logging Sequence: 159_139.00
2024-02-04 11:48:52,080 	Gloss Reference :	A B+C+D+E
2024-02-04 11:48:52,080 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 11:48:52,080 	Gloss Alignment :	         
2024-02-04 11:48:52,080 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 11:48:52,082 	Text Reference  :	***** he  took time and finally was ready for the asia cup where he   scored the ****** century   
2024-02-04 11:48:52,082 	Text Hypothesis :	dhoni are all  out  and ******* *** ***** *** did she  get a     duck in     the sports tournament
2024-02-04 11:48:52,082 	Text Alignment  :	I     S   S    S        D       D   D     D   S   S    S   S     S    S          I      S         
2024-02-04 11:48:52,082 ========================================================================================================================
2024-02-04 11:48:52,082 Logging Sequence: 159_159.00
2024-02-04 11:48:52,082 	Gloss Reference :	A B+C+D+E
2024-02-04 11:48:52,083 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 11:48:52,083 	Gloss Alignment :	         
2024-02-04 11:48:52,083 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 11:48:52,085 	Text Reference  :	he said  it  wasn't easy the mind has to be    focussed and he is glad that he is back in form with the asia cup century
2024-02-04 11:48:52,085 	Text Hypothesis :	** kohli was out    of   the **** *** ** first time     and ** ** **** **** ** ** **** ** **** **** csk will be  played 
2024-02-04 11:48:52,085 	Text Alignment  :	D  S     S   S      S        D    D   D  S     S            D  D  D    D    D  D  D    D  D    D    S   S    S   S      
2024-02-04 11:48:52,085 ========================================================================================================================
2024-02-04 11:48:52,085 Logging Sequence: 103_8.00
2024-02-04 11:48:52,085 	Gloss Reference :	A B+C+D+E
2024-02-04 11:48:52,085 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 11:48:52,086 	Gloss Alignment :	         
2024-02-04 11:48:52,086 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 11:48:52,087 	Text Reference  :	************ ***** ********* were         going on  in      birmingham england from 28th july to 8th  august 2022 
2024-02-04 11:48:52,087 	Text Hypothesis :	commonwealth games encourage independence from  the british empire     games   and  will have to wait for    india
2024-02-04 11:48:52,087 	Text Alignment  :	I            I     I         S            S     S   S       S          S       S    S    S       S    S      S    
2024-02-04 11:48:52,087 ========================================================================================================================
2024-02-04 11:48:52,088 Logging Sequence: 164_546.00
2024-02-04 11:48:52,088 	Gloss Reference :	A B+C+D+E
2024-02-04 11:48:52,088 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 11:48:52,088 	Gloss Alignment :	         
2024-02-04 11:48:52,088 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 11:48:52,089 	Text Reference  :	***** *** reliance has    turned out      to **** **** ****** be  the strongest company
2024-02-04 11:48:52,089 	Text Hypothesis :	after the match    people are    expected to have been rights for the next      season 
2024-02-04 11:48:52,089 	Text Alignment  :	I     I   S        S      S      S           I    I    I      S       S         S      
2024-02-04 11:48:52,089 ========================================================================================================================
2024-02-04 11:48:52,089 Logging Sequence: 132_173.00
2024-02-04 11:48:52,090 	Gloss Reference :	A B+C+D+E
2024-02-04 11:48:52,090 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 11:48:52,090 	Gloss Alignment :	         
2024-02-04 11:48:52,090 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 11:48:52,091 	Text Reference  :	** **** ******** ** *** *** ** ***** ** ** usman is   australia' first muslim player 
2024-02-04 11:48:52,091 	Text Hypothesis :	he will continue at the age of kohli as he will  have to         wait  and    special
2024-02-04 11:48:52,091 	Text Alignment  :	I  I    I        I  I   I   I  I     I  I  S     S    S          S     S      S      
2024-02-04 11:48:52,091 ========================================================================================================================
2024-02-04 11:49:02,557 Epoch 589: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.94 
2024-02-04 11:49:02,558 EPOCH 590
2024-02-04 11:49:13,141 Epoch 590: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.89 
2024-02-04 11:49:13,141 EPOCH 591
2024-02-04 11:49:23,829 Epoch 591: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.88 
2024-02-04 11:49:23,829 EPOCH 592
2024-02-04 11:49:34,435 Epoch 592: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.90 
2024-02-04 11:49:34,436 EPOCH 593
2024-02-04 11:49:45,080 Epoch 593: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.06 
2024-02-04 11:49:45,081 EPOCH 594
2024-02-04 11:49:55,468 Epoch 594: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.24 
2024-02-04 11:49:55,469 EPOCH 595
2024-02-04 11:49:57,639 [Epoch: 595 Step: 00010100] Batch Recognition Loss:   0.000553 => Gls Tokens per Sec:      590 || Batch Translation Loss:   0.081340 => Txt Tokens per Sec:     1805 || Lr: 0.000100
2024-02-04 11:50:06,225 Epoch 595: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.15 
2024-02-04 11:50:06,226 EPOCH 596
2024-02-04 11:50:16,556 Epoch 596: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.13 
2024-02-04 11:50:16,557 EPOCH 597
2024-02-04 11:50:27,040 Epoch 597: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.97 
2024-02-04 11:50:27,040 EPOCH 598
2024-02-04 11:50:37,521 Epoch 598: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.98 
2024-02-04 11:50:37,521 EPOCH 599
2024-02-04 11:50:48,285 Epoch 599: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.35 
2024-02-04 11:50:48,286 EPOCH 600
2024-02-04 11:50:59,016 [Epoch: 600 Step: 00010200] Batch Recognition Loss:   0.000285 => Gls Tokens per Sec:      991 || Batch Translation Loss:   0.048343 => Txt Tokens per Sec:     2750 || Lr: 0.000100
2024-02-04 11:50:59,017 Epoch 600: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.44 
2024-02-04 11:50:59,017 EPOCH 601
2024-02-04 11:51:09,387 Epoch 601: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.39 
2024-02-04 11:51:09,388 EPOCH 602
2024-02-04 11:51:19,515 Epoch 602: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.60 
2024-02-04 11:51:19,515 EPOCH 603
2024-02-04 11:51:29,923 Epoch 603: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.08 
2024-02-04 11:51:29,924 EPOCH 604
2024-02-04 11:51:40,661 Epoch 604: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.81 
2024-02-04 11:51:40,661 EPOCH 605
2024-02-04 11:51:51,319 Epoch 605: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.71 
2024-02-04 11:51:51,320 EPOCH 606
2024-02-04 11:52:01,463 [Epoch: 606 Step: 00010300] Batch Recognition Loss:   0.000215 => Gls Tokens per Sec:      922 || Batch Translation Loss:   0.043104 => Txt Tokens per Sec:     2541 || Lr: 0.000100
2024-02-04 11:52:01,993 Epoch 606: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.60 
2024-02-04 11:52:01,993 EPOCH 607
2024-02-04 11:52:12,484 Epoch 607: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.87 
2024-02-04 11:52:12,485 EPOCH 608
2024-02-04 11:52:23,241 Epoch 608: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.01 
2024-02-04 11:52:23,241 EPOCH 609
2024-02-04 11:52:33,744 Epoch 609: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.96 
2024-02-04 11:52:33,745 EPOCH 610
2024-02-04 11:52:44,428 Epoch 610: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.79 
2024-02-04 11:52:44,428 EPOCH 611
2024-02-04 11:52:55,143 Epoch 611: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.96 
2024-02-04 11:52:55,144 EPOCH 612
2024-02-04 11:53:01,886 [Epoch: 612 Step: 00010400] Batch Recognition Loss:   0.000288 => Gls Tokens per Sec:     1197 || Batch Translation Loss:   0.022598 => Txt Tokens per Sec:     3252 || Lr: 0.000100
2024-02-04 11:53:05,796 Epoch 612: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.78 
2024-02-04 11:53:05,796 EPOCH 613
2024-02-04 11:53:16,299 Epoch 613: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.67 
2024-02-04 11:53:16,299 EPOCH 614
2024-02-04 11:53:27,169 Epoch 614: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.75 
2024-02-04 11:53:27,169 EPOCH 615
2024-02-04 11:53:37,693 Epoch 615: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.74 
2024-02-04 11:53:37,694 EPOCH 616
2024-02-04 11:53:48,061 Epoch 616: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.83 
2024-02-04 11:53:48,062 EPOCH 617
2024-02-04 11:53:58,943 Epoch 617: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.01 
2024-02-04 11:53:58,944 EPOCH 618
2024-02-04 11:54:02,481 [Epoch: 618 Step: 00010500] Batch Recognition Loss:   0.000291 => Gls Tokens per Sec:     1991 || Batch Translation Loss:   0.017422 => Txt Tokens per Sec:     5224 || Lr: 0.000100
2024-02-04 11:54:09,380 Epoch 618: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.10 
2024-02-04 11:54:09,381 EPOCH 619
2024-02-04 11:54:19,920 Epoch 619: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.14 
2024-02-04 11:54:19,920 EPOCH 620
2024-02-04 11:54:30,523 Epoch 620: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.01 
2024-02-04 11:54:30,524 EPOCH 621
2024-02-04 11:54:40,983 Epoch 621: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.28 
2024-02-04 11:54:40,983 EPOCH 622
2024-02-04 11:54:51,543 Epoch 622: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.31 
2024-02-04 11:54:51,544 EPOCH 623
2024-02-04 11:55:02,416 Epoch 623: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.51 
2024-02-04 11:55:02,417 EPOCH 624
2024-02-04 11:55:05,959 [Epoch: 624 Step: 00010600] Batch Recognition Loss:   0.000284 => Gls Tokens per Sec:     1627 || Batch Translation Loss:   0.094084 => Txt Tokens per Sec:     4378 || Lr: 0.000100
2024-02-04 11:55:12,947 Epoch 624: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.71 
2024-02-04 11:55:12,947 EPOCH 625
2024-02-04 11:55:23,524 Epoch 625: Total Training Recognition Loss 0.00  Total Training Translation Loss 2.35 
2024-02-04 11:55:23,524 EPOCH 626
2024-02-04 11:55:33,896 Epoch 626: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.39 
2024-02-04 11:55:33,897 EPOCH 627
2024-02-04 11:55:44,564 Epoch 627: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.90 
2024-02-04 11:55:44,564 EPOCH 628
2024-02-04 11:55:55,191 Epoch 628: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.70 
2024-02-04 11:55:55,192 EPOCH 629
2024-02-04 11:56:05,977 Epoch 629: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.91 
2024-02-04 11:56:05,978 EPOCH 630
2024-02-04 11:56:11,132 [Epoch: 630 Step: 00010700] Batch Recognition Loss:   0.000338 => Gls Tokens per Sec:      821 || Batch Translation Loss:   0.095992 => Txt Tokens per Sec:     2258 || Lr: 0.000100
2024-02-04 11:56:16,526 Epoch 630: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.51 
2024-02-04 11:56:16,527 EPOCH 631
2024-02-04 11:56:27,021 Epoch 631: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.70 
2024-02-04 11:56:27,022 EPOCH 632
2024-02-04 11:56:37,371 Epoch 632: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.55 
2024-02-04 11:56:37,372 EPOCH 633
2024-02-04 11:56:48,069 Epoch 633: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.45 
2024-02-04 11:56:48,070 EPOCH 634
2024-02-04 11:56:58,429 Epoch 634: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.01 
2024-02-04 11:56:58,429 EPOCH 635
2024-02-04 11:57:09,011 Epoch 635: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.17 
2024-02-04 11:57:09,011 EPOCH 636
2024-02-04 11:57:12,238 [Epoch: 636 Step: 00010800] Batch Recognition Loss:   0.000419 => Gls Tokens per Sec:      915 || Batch Translation Loss:   0.039150 => Txt Tokens per Sec:     2265 || Lr: 0.000100
2024-02-04 11:57:19,459 Epoch 636: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.26 
2024-02-04 11:57:19,459 EPOCH 637
2024-02-04 11:57:29,863 Epoch 637: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.09 
2024-02-04 11:57:29,864 EPOCH 638
2024-02-04 11:57:40,012 Epoch 638: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.92 
2024-02-04 11:57:40,013 EPOCH 639
2024-02-04 11:57:50,556 Epoch 639: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.78 
2024-02-04 11:57:50,556 EPOCH 640
2024-02-04 11:58:01,217 Epoch 640: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.93 
2024-02-04 11:58:01,218 EPOCH 641
2024-02-04 11:58:11,461 Epoch 641: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.73 
2024-02-04 11:58:11,461 EPOCH 642
2024-02-04 11:58:13,261 [Epoch: 642 Step: 00010900] Batch Recognition Loss:   0.000267 => Gls Tokens per Sec:     1067 || Batch Translation Loss:   0.047046 => Txt Tokens per Sec:     3009 || Lr: 0.000100
2024-02-04 11:58:21,990 Epoch 642: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.85 
2024-02-04 11:58:21,991 EPOCH 643
2024-02-04 11:58:32,582 Epoch 643: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.86 
2024-02-04 11:58:32,583 EPOCH 644
2024-02-04 11:58:43,122 Epoch 644: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.77 
2024-02-04 11:58:43,123 EPOCH 645
2024-02-04 11:58:53,808 Epoch 645: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.80 
2024-02-04 11:58:53,809 EPOCH 646
2024-02-04 11:59:04,469 Epoch 646: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.91 
2024-02-04 11:59:04,470 EPOCH 647
2024-02-04 11:59:15,183 Epoch 647: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.50 
2024-02-04 11:59:15,183 EPOCH 648
2024-02-04 11:59:15,463 [Epoch: 648 Step: 00011000] Batch Recognition Loss:   0.000206 => Gls Tokens per Sec:     2294 || Batch Translation Loss:   0.061805 => Txt Tokens per Sec:     6692 || Lr: 0.000100
2024-02-04 11:59:25,766 Epoch 648: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.13 
2024-02-04 11:59:25,767 EPOCH 649
2024-02-04 11:59:36,379 Epoch 649: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.26 
2024-02-04 11:59:36,380 EPOCH 650
2024-02-04 11:59:46,996 Epoch 650: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.44 
2024-02-04 11:59:46,997 EPOCH 651
2024-02-04 11:59:57,670 Epoch 651: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.48 
2024-02-04 11:59:57,670 EPOCH 652
2024-02-04 12:00:08,393 Epoch 652: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.99 
2024-02-04 12:00:08,394 EPOCH 653
2024-02-04 12:00:18,702 [Epoch: 653 Step: 00011100] Batch Recognition Loss:   0.000781 => Gls Tokens per Sec:      969 || Batch Translation Loss:   0.861705 => Txt Tokens per Sec:     2686 || Lr: 0.000100
2024-02-04 12:00:19,002 Epoch 653: Total Training Recognition Loss 0.01  Total Training Translation Loss 5.13 
2024-02-04 12:00:19,002 EPOCH 654
2024-02-04 12:00:29,677 Epoch 654: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.18 
2024-02-04 12:00:29,678 EPOCH 655
2024-02-04 12:00:40,366 Epoch 655: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.21 
2024-02-04 12:00:40,366 EPOCH 656
2024-02-04 12:00:51,032 Epoch 656: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.77 
2024-02-04 12:00:51,033 EPOCH 657
2024-02-04 12:01:01,379 Epoch 657: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.28 
2024-02-04 12:01:01,380 EPOCH 658
2024-02-04 12:01:11,899 Epoch 658: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.35 
2024-02-04 12:01:11,899 EPOCH 659
2024-02-04 12:01:20,554 [Epoch: 659 Step: 00011200] Batch Recognition Loss:   0.000236 => Gls Tokens per Sec:     1007 || Batch Translation Loss:   0.044502 => Txt Tokens per Sec:     2809 || Lr: 0.000100
2024-02-04 12:01:22,455 Epoch 659: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.93 
2024-02-04 12:01:22,456 EPOCH 660
2024-02-04 12:01:33,123 Epoch 660: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-04 12:01:33,124 EPOCH 661
2024-02-04 12:01:43,412 Epoch 661: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-04 12:01:43,412 EPOCH 662
2024-02-04 12:01:54,276 Epoch 662: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.71 
2024-02-04 12:01:54,276 EPOCH 663
2024-02-04 12:02:04,885 Epoch 663: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.66 
2024-02-04 12:02:04,885 EPOCH 664
2024-02-04 12:02:15,160 Epoch 664: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.57 
2024-02-04 12:02:15,160 EPOCH 665
2024-02-04 12:02:23,225 [Epoch: 665 Step: 00011300] Batch Recognition Loss:   0.000217 => Gls Tokens per Sec:      921 || Batch Translation Loss:   0.024808 => Txt Tokens per Sec:     2627 || Lr: 0.000100
2024-02-04 12:02:25,977 Epoch 665: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-04 12:02:25,977 EPOCH 666
2024-02-04 12:02:36,775 Epoch 666: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-04 12:02:36,775 EPOCH 667
2024-02-04 12:02:47,517 Epoch 667: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-04 12:02:47,518 EPOCH 668
2024-02-04 12:02:57,940 Epoch 668: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-04 12:02:57,940 EPOCH 669
2024-02-04 12:03:08,341 Epoch 669: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-04 12:03:08,342 EPOCH 670
2024-02-04 12:03:19,008 Epoch 670: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-04 12:03:19,009 EPOCH 671
2024-02-04 12:03:26,762 [Epoch: 671 Step: 00011400] Batch Recognition Loss:   0.000237 => Gls Tokens per Sec:      793 || Batch Translation Loss:   0.026586 => Txt Tokens per Sec:     2270 || Lr: 0.000100
2024-02-04 12:03:29,837 Epoch 671: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-04 12:03:29,837 EPOCH 672
2024-02-04 12:03:40,280 Epoch 672: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-04 12:03:40,280 EPOCH 673
2024-02-04 12:03:50,908 Epoch 673: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.58 
2024-02-04 12:03:50,909 EPOCH 674
2024-02-04 12:04:01,234 Epoch 674: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.75 
2024-02-04 12:04:01,234 EPOCH 675
2024-02-04 12:04:12,031 Epoch 675: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.86 
2024-02-04 12:04:12,032 EPOCH 676
2024-02-04 12:04:22,467 Epoch 676: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.86 
2024-02-04 12:04:22,467 EPOCH 677
2024-02-04 12:04:26,784 [Epoch: 677 Step: 00011500] Batch Recognition Loss:   0.000222 => Gls Tokens per Sec:     1186 || Batch Translation Loss:   0.077776 => Txt Tokens per Sec:     3143 || Lr: 0.000100
2024-02-04 12:04:33,192 Epoch 677: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.97 
2024-02-04 12:04:33,193 EPOCH 678
2024-02-04 12:04:43,763 Epoch 678: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.36 
2024-02-04 12:04:43,763 EPOCH 679
2024-02-04 12:04:54,277 Epoch 679: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.45 
2024-02-04 12:04:54,278 EPOCH 680
2024-02-04 12:05:04,882 Epoch 680: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.32 
2024-02-04 12:05:04,882 EPOCH 681
2024-02-04 12:05:15,424 Epoch 681: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.10 
2024-02-04 12:05:15,425 EPOCH 682
2024-02-04 12:05:26,043 Epoch 682: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.16 
2024-02-04 12:05:26,044 EPOCH 683
2024-02-04 12:05:28,788 [Epoch: 683 Step: 00011600] Batch Recognition Loss:   0.000227 => Gls Tokens per Sec:     1399 || Batch Translation Loss:   0.158524 => Txt Tokens per Sec:     4003 || Lr: 0.000100
2024-02-04 12:05:36,615 Epoch 683: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.31 
2024-02-04 12:05:36,615 EPOCH 684
2024-02-04 12:05:47,299 Epoch 684: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.48 
2024-02-04 12:05:47,300 EPOCH 685
2024-02-04 12:05:57,864 Epoch 685: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.55 
2024-02-04 12:05:57,865 EPOCH 686
2024-02-04 12:06:08,565 Epoch 686: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.05 
2024-02-04 12:06:08,566 EPOCH 687
2024-02-04 12:06:18,915 Epoch 687: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.15 
2024-02-04 12:06:18,915 EPOCH 688
2024-02-04 12:06:29,658 Epoch 688: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.18 
2024-02-04 12:06:29,659 EPOCH 689
2024-02-04 12:06:34,150 [Epoch: 689 Step: 00011700] Batch Recognition Loss:   0.000238 => Gls Tokens per Sec:      515 || Batch Translation Loss:   0.045426 => Txt Tokens per Sec:     1564 || Lr: 0.000100
2024-02-04 12:06:40,092 Epoch 689: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.40 
2024-02-04 12:06:40,093 EPOCH 690
2024-02-04 12:06:50,792 Epoch 690: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-04 12:06:50,792 EPOCH 691
2024-02-04 12:07:01,465 Epoch 691: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.21 
2024-02-04 12:07:01,466 EPOCH 692
2024-02-04 12:07:11,881 Epoch 692: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.17 
2024-02-04 12:07:11,882 EPOCH 693
2024-02-04 12:07:22,326 Epoch 693: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.14 
2024-02-04 12:07:22,327 EPOCH 694
2024-02-04 12:07:32,808 Epoch 694: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.11 
2024-02-04 12:07:32,808 EPOCH 695
2024-02-04 12:07:34,364 [Epoch: 695 Step: 00011800] Batch Recognition Loss:   0.000178 => Gls Tokens per Sec:      824 || Batch Translation Loss:   0.023937 => Txt Tokens per Sec:     2160 || Lr: 0.000100
2024-02-04 12:07:43,194 Epoch 695: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.93 
2024-02-04 12:07:43,194 EPOCH 696
2024-02-04 12:07:53,642 Epoch 696: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.69 
2024-02-04 12:07:53,643 EPOCH 697
2024-02-04 12:08:04,246 Epoch 697: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.76 
2024-02-04 12:08:04,247 EPOCH 698
2024-02-04 12:08:15,019 Epoch 698: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.56 
2024-02-04 12:08:15,020 EPOCH 699
2024-02-04 12:08:25,437 Epoch 699: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-04 12:08:25,437 EPOCH 700
2024-02-04 12:08:36,006 [Epoch: 700 Step: 00011900] Batch Recognition Loss:   0.000178 => Gls Tokens per Sec:     1006 || Batch Translation Loss:   0.019793 => Txt Tokens per Sec:     2792 || Lr: 0.000100
2024-02-04 12:08:36,007 Epoch 700: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-04 12:08:36,007 EPOCH 701
2024-02-04 12:08:46,532 Epoch 701: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-04 12:08:46,533 EPOCH 702
2024-02-04 12:08:57,263 Epoch 702: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.59 
2024-02-04 12:08:57,263 EPOCH 703
2024-02-04 12:09:07,916 Epoch 703: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.66 
2024-02-04 12:09:07,917 EPOCH 704
2024-02-04 12:09:18,486 Epoch 704: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-04 12:09:18,487 EPOCH 705
2024-02-04 12:09:28,862 Epoch 705: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.55 
2024-02-04 12:09:28,863 EPOCH 706
2024-02-04 12:09:39,002 [Epoch: 706 Step: 00012000] Batch Recognition Loss:   0.000225 => Gls Tokens per Sec:      922 || Batch Translation Loss:   0.086286 => Txt Tokens per Sec:     2583 || Lr: 0.000100
2024-02-04 12:10:16,854 Hooray! New best validation result [eval_metric]!
2024-02-04 12:10:16,856 Saving new checkpoint.
2024-02-04 12:10:17,149 Validation result at epoch 706, step    12000: duration: 38.1455s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00021	Translation Loss: 95682.77344	PPL: 14399.94629
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 1.06	(BLEU-1: 11.73,	BLEU-2: 3.85,	BLEU-3: 1.85,	BLEU-4: 1.06)
	CHRF 17.62	ROUGE 10.14
2024-02-04 12:10:17,150 Logging Recognition and Translation Outputs
2024-02-04 12:10:17,150 ========================================================================================================================
2024-02-04 12:10:17,150 Logging Sequence: 177_50.00
2024-02-04 12:10:17,151 	Gloss Reference :	A B+C+D+E
2024-02-04 12:10:17,151 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 12:10:17,151 	Gloss Alignment :	         
2024-02-04 12:10:17,151 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 12:10:17,152 	Text Reference  :	a similar reward    of rs      50000 was announced for information against his    associate ajay kumar   
2024-02-04 12:10:17,153 	Text Hypothesis :	* but     according to rumours the   ipl may       be  held        in      search of        the  wrestler
2024-02-04 12:10:17,153 	Text Alignment  :	D S       S         S  S       S     S   S         S   S           S       S      S         S    S       
2024-02-04 12:10:17,153 ========================================================================================================================
2024-02-04 12:10:17,153 Logging Sequence: 122_86.00
2024-02-04 12:10:17,153 	Gloss Reference :	A B+C+D+E
2024-02-04 12:10:17,153 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 12:10:17,154 	Gloss Alignment :	         
2024-02-04 12:10:17,154 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 12:10:17,155 	Text Reference  :	** **** after winning chanu spoke to the **** ***** ** ***** *** media and said    
2024-02-04 12:10:17,155 	Text Hypothesis :	he said that  he      is    part  in the 2012 going on chanu are now   and handsome
2024-02-04 12:10:17,155 	Text Alignment  :	I  I    S     S       S     S     S      I    I     I  I     I   S         S       
2024-02-04 12:10:17,155 ========================================================================================================================
2024-02-04 12:10:17,155 Logging Sequence: 165_27.00
2024-02-04 12:10:17,155 	Gloss Reference :	A B+C+D+E
2024-02-04 12:10:17,155 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 12:10:17,156 	Gloss Alignment :	         
2024-02-04 12:10:17,156 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 12:10:17,157 	Text Reference  :	so then they change their routes some people     believe in  this while some don't
2024-02-04 12:10:17,157 	Text Hypothesis :	** **** **** ****** ***** it     is   disgusting that    his bag  did   not  agree
2024-02-04 12:10:17,157 	Text Alignment  :	D  D    D    D      D     S      S    S          S       S   S    S     S    S    
2024-02-04 12:10:17,157 ========================================================================================================================
2024-02-04 12:10:17,157 Logging Sequence: 70_65.00
2024-02-04 12:10:17,157 	Gloss Reference :	A B+C+D+E
2024-02-04 12:10:17,157 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 12:10:17,158 	Gloss Alignment :	         
2024-02-04 12:10:17,158 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 12:10:17,159 	Text Reference  :	during the press   conference a   table was  placed in front of the   media   
2024-02-04 12:10:17,159 	Text Hypothesis :	this   is  because of         the euro  2020 is     in ***** ** tokyo olympics
2024-02-04 12:10:17,159 	Text Alignment  :	S      S   S       S          S   S     S    S         D     D  S     S       
2024-02-04 12:10:17,159 ========================================================================================================================
2024-02-04 12:10:17,159 Logging Sequence: 149_65.00
2024-02-04 12:10:17,159 	Gloss Reference :	A B+C+D+E
2024-02-04 12:10:17,159 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 12:10:17,160 	Gloss Alignment :	         
2024-02-04 12:10:17,160 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 12:10:17,162 	Text Reference  :	at 6am on 6th november 2022  the     police reached  sri lankan team's      hotel in sydney  australia's central business district cbd 
2024-02-04 12:10:17,162 	Text Hypothesis :	** *** ** *** the      woman alleged that   danushka had sexual intercourse with  a  without his         consent which    means    rape
2024-02-04 12:10:17,162 	Text Alignment  :	D  D   D  D   S        S     S       S      S        S   S      S           S     S  S       S           S       S        S        S   
2024-02-04 12:10:17,162 ========================================================================================================================
2024-02-04 12:10:17,718 Epoch 706: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.59 
2024-02-04 12:10:17,718 EPOCH 707
2024-02-04 12:10:28,850 Epoch 707: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.56 
2024-02-04 12:10:28,851 EPOCH 708
2024-02-04 12:10:39,309 Epoch 708: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.74 
2024-02-04 12:10:39,309 EPOCH 709
2024-02-04 12:10:49,734 Epoch 709: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.69 
2024-02-04 12:10:49,735 EPOCH 710
2024-02-04 12:11:00,524 Epoch 710: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.80 
2024-02-04 12:11:00,525 EPOCH 711
2024-02-04 12:11:11,157 Epoch 711: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.82 
2024-02-04 12:11:11,158 EPOCH 712
2024-02-04 12:11:19,836 [Epoch: 712 Step: 00012100] Batch Recognition Loss:   0.000153 => Gls Tokens per Sec:      930 || Batch Translation Loss:   0.023603 => Txt Tokens per Sec:     2689 || Lr: 0.000100
2024-02-04 12:11:21,763 Epoch 712: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.79 
2024-02-04 12:11:21,764 EPOCH 713
2024-02-04 12:11:32,214 Epoch 713: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.85 
2024-02-04 12:11:32,215 EPOCH 714
2024-02-04 12:11:42,925 Epoch 714: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.23 
2024-02-04 12:11:42,925 EPOCH 715
2024-02-04 12:11:53,388 Epoch 715: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.22 
2024-02-04 12:11:53,388 EPOCH 716
2024-02-04 12:12:03,923 Epoch 716: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.06 
2024-02-04 12:12:03,924 EPOCH 717
2024-02-04 12:12:14,404 Epoch 717: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.88 
2024-02-04 12:12:14,404 EPOCH 718
2024-02-04 12:12:23,202 [Epoch: 718 Step: 00012200] Batch Recognition Loss:   0.000279 => Gls Tokens per Sec:      772 || Batch Translation Loss:   0.033015 => Txt Tokens per Sec:     2286 || Lr: 0.000100
2024-02-04 12:12:24,589 Epoch 718: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.98 
2024-02-04 12:12:24,590 EPOCH 719
2024-02-04 12:12:34,996 Epoch 719: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.28 
2024-02-04 12:12:34,996 EPOCH 720
2024-02-04 12:12:46,928 Epoch 720: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.49 
2024-02-04 12:12:46,929 EPOCH 721
2024-02-04 12:12:57,712 Epoch 721: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.55 
2024-02-04 12:12:57,713 EPOCH 722
2024-02-04 12:13:08,224 Epoch 722: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.44 
2024-02-04 12:13:08,225 EPOCH 723
2024-02-04 12:13:18,801 Epoch 723: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.31 
2024-02-04 12:13:18,801 EPOCH 724
2024-02-04 12:13:27,612 [Epoch: 724 Step: 00012300] Batch Recognition Loss:   0.000295 => Gls Tokens per Sec:      625 || Batch Translation Loss:   0.073008 => Txt Tokens per Sec:     1749 || Lr: 0.000100
2024-02-04 12:13:29,693 Epoch 724: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-04 12:13:29,694 EPOCH 725
2024-02-04 12:13:40,200 Epoch 725: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.81 
2024-02-04 12:13:40,201 EPOCH 726
2024-02-04 12:13:51,062 Epoch 726: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.75 
2024-02-04 12:13:51,063 EPOCH 727
2024-02-04 12:14:01,547 Epoch 727: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.68 
2024-02-04 12:14:01,548 EPOCH 728
2024-02-04 12:14:12,044 Epoch 728: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.59 
2024-02-04 12:14:12,045 EPOCH 729
2024-02-04 12:14:22,532 Epoch 729: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.79 
2024-02-04 12:14:22,532 EPOCH 730
2024-02-04 12:14:26,617 [Epoch: 730 Step: 00012400] Batch Recognition Loss:   0.000282 => Gls Tokens per Sec:     1036 || Batch Translation Loss:   0.025999 => Txt Tokens per Sec:     2881 || Lr: 0.000100
2024-02-04 12:14:32,974 Epoch 730: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.76 
2024-02-04 12:14:32,974 EPOCH 731
2024-02-04 12:14:43,641 Epoch 731: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.03 
2024-02-04 12:14:43,643 EPOCH 732
2024-02-04 12:14:54,589 Epoch 732: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.17 
2024-02-04 12:14:54,589 EPOCH 733
2024-02-04 12:15:05,118 Epoch 733: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.69 
2024-02-04 12:15:05,119 EPOCH 734
2024-02-04 12:15:15,721 Epoch 734: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.25 
2024-02-04 12:15:15,721 EPOCH 735
2024-02-04 12:15:26,582 Epoch 735: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.14 
2024-02-04 12:15:26,583 EPOCH 736
2024-02-04 12:15:29,021 [Epoch: 736 Step: 00012500] Batch Recognition Loss:   0.000365 => Gls Tokens per Sec:     1313 || Batch Translation Loss:   0.078626 => Txt Tokens per Sec:     3492 || Lr: 0.000100
2024-02-04 12:15:37,063 Epoch 736: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.96 
2024-02-04 12:15:37,064 EPOCH 737
2024-02-04 12:15:47,636 Epoch 737: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.13 
2024-02-04 12:15:47,637 EPOCH 738
2024-02-04 12:15:58,373 Epoch 738: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.15 
2024-02-04 12:15:58,374 EPOCH 739
2024-02-04 12:16:09,143 Epoch 739: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.14 
2024-02-04 12:16:09,144 EPOCH 740
2024-02-04 12:16:19,863 Epoch 740: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.50 
2024-02-04 12:16:19,864 EPOCH 741
2024-02-04 12:16:30,177 Epoch 741: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.47 
2024-02-04 12:16:30,177 EPOCH 742
2024-02-04 12:16:30,788 [Epoch: 742 Step: 00012600] Batch Recognition Loss:   0.000587 => Gls Tokens per Sec:     3153 || Batch Translation Loss:   0.332020 => Txt Tokens per Sec:     7998 || Lr: 0.000100
2024-02-04 12:16:40,635 Epoch 742: Total Training Recognition Loss 0.01  Total Training Translation Loss 11.65 
2024-02-04 12:16:40,636 EPOCH 743
2024-02-04 12:16:51,568 Epoch 743: Total Training Recognition Loss 0.05  Total Training Translation Loss 7.42 
2024-02-04 12:16:51,569 EPOCH 744
2024-02-04 12:17:02,171 Epoch 744: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.31 
2024-02-04 12:17:02,172 EPOCH 745
2024-02-04 12:17:12,801 Epoch 745: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.63 
2024-02-04 12:17:12,801 EPOCH 746
2024-02-04 12:17:23,364 Epoch 746: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.14 
2024-02-04 12:17:23,365 EPOCH 747
2024-02-04 12:17:34,150 Epoch 747: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-04 12:17:34,151 EPOCH 748
2024-02-04 12:17:34,301 [Epoch: 748 Step: 00012700] Batch Recognition Loss:   0.000305 => Gls Tokens per Sec:     4324 || Batch Translation Loss:   0.052753 => Txt Tokens per Sec:    10007 || Lr: 0.000100
2024-02-04 12:17:44,683 Epoch 748: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.85 
2024-02-04 12:17:44,683 EPOCH 749
2024-02-04 12:17:55,219 Epoch 749: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-04 12:17:55,220 EPOCH 750
2024-02-04 12:18:05,880 Epoch 750: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-04 12:18:05,881 EPOCH 751
2024-02-04 12:18:16,697 Epoch 751: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-04 12:18:16,697 EPOCH 752
2024-02-04 12:18:27,073 Epoch 752: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-04 12:18:27,073 EPOCH 753
2024-02-04 12:18:37,294 [Epoch: 753 Step: 00012800] Batch Recognition Loss:   0.000188 => Gls Tokens per Sec:      978 || Batch Translation Loss:   0.018270 => Txt Tokens per Sec:     2690 || Lr: 0.000100
2024-02-04 12:18:37,660 Epoch 753: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-04 12:18:37,660 EPOCH 754
2024-02-04 12:18:48,191 Epoch 754: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-04 12:18:48,191 EPOCH 755
2024-02-04 12:18:58,656 Epoch 755: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-04 12:18:58,656 EPOCH 756
2024-02-04 12:19:09,212 Epoch 756: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-04 12:19:09,213 EPOCH 757
2024-02-04 12:19:19,906 Epoch 757: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-04 12:19:19,907 EPOCH 758
2024-02-04 12:19:30,540 Epoch 758: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-04 12:19:30,541 EPOCH 759
2024-02-04 12:19:39,001 [Epoch: 759 Step: 00012900] Batch Recognition Loss:   0.000221 => Gls Tokens per Sec:     1030 || Batch Translation Loss:   0.023018 => Txt Tokens per Sec:     2836 || Lr: 0.000100
2024-02-04 12:19:41,050 Epoch 759: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-04 12:19:41,051 EPOCH 760
2024-02-04 12:19:51,699 Epoch 760: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-04 12:19:51,699 EPOCH 761
2024-02-04 12:20:02,327 Epoch 761: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-04 12:20:02,328 EPOCH 762
2024-02-04 12:20:12,801 Epoch 762: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-04 12:20:12,802 EPOCH 763
2024-02-04 12:20:23,265 Epoch 763: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-04 12:20:23,266 EPOCH 764
2024-02-04 12:20:33,801 Epoch 764: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-04 12:20:33,802 EPOCH 765
2024-02-04 12:20:41,628 [Epoch: 765 Step: 00013000] Batch Recognition Loss:   0.000142 => Gls Tokens per Sec:      950 || Batch Translation Loss:   0.019432 => Txt Tokens per Sec:     2619 || Lr: 0.000100
2024-02-04 12:20:44,518 Epoch 765: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-04 12:20:44,519 EPOCH 766
2024-02-04 12:20:55,470 Epoch 766: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-04 12:20:55,471 EPOCH 767
2024-02-04 12:21:06,018 Epoch 767: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-04 12:21:06,018 EPOCH 768
2024-02-04 12:21:16,511 Epoch 768: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-04 12:21:16,512 EPOCH 769
2024-02-04 12:21:27,147 Epoch 769: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-04 12:21:27,148 EPOCH 770
2024-02-04 12:21:37,791 Epoch 770: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-04 12:21:37,791 EPOCH 771
2024-02-04 12:21:45,341 [Epoch: 771 Step: 00013100] Batch Recognition Loss:   0.000151 => Gls Tokens per Sec:      815 || Batch Translation Loss:   0.020584 => Txt Tokens per Sec:     2260 || Lr: 0.000100
2024-02-04 12:21:48,577 Epoch 771: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.62 
2024-02-04 12:21:48,577 EPOCH 772
2024-02-04 12:21:59,544 Epoch 772: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.82 
2024-02-04 12:21:59,544 EPOCH 773
2024-02-04 12:22:10,257 Epoch 773: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.84 
2024-02-04 12:22:10,257 EPOCH 774
2024-02-04 12:22:20,834 Epoch 774: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.03 
2024-02-04 12:22:20,834 EPOCH 775
2024-02-04 12:22:31,410 Epoch 775: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.40 
2024-02-04 12:22:31,411 EPOCH 776
2024-02-04 12:22:41,913 Epoch 776: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.08 
2024-02-04 12:22:41,913 EPOCH 777
2024-02-04 12:22:48,617 [Epoch: 777 Step: 00013200] Batch Recognition Loss:   0.000237 => Gls Tokens per Sec:      726 || Batch Translation Loss:   0.028385 => Txt Tokens per Sec:     1970 || Lr: 0.000100
2024-02-04 12:22:52,476 Epoch 777: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.10 
2024-02-04 12:22:52,476 EPOCH 778
2024-02-04 12:23:03,222 Epoch 778: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.10 
2024-02-04 12:23:03,223 EPOCH 779
2024-02-04 12:23:13,837 Epoch 779: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.75 
2024-02-04 12:23:13,838 EPOCH 780
2024-02-04 12:23:24,196 Epoch 780: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.91 
2024-02-04 12:23:24,196 EPOCH 781
2024-02-04 12:23:34,909 Epoch 781: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.87 
2024-02-04 12:23:34,910 EPOCH 782
2024-02-04 12:23:45,374 Epoch 782: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.67 
2024-02-04 12:23:45,374 EPOCH 783
2024-02-04 12:23:46,783 [Epoch: 783 Step: 00013300] Batch Recognition Loss:   0.000168 => Gls Tokens per Sec:     2726 || Batch Translation Loss:   0.057557 => Txt Tokens per Sec:     7489 || Lr: 0.000100
2024-02-04 12:23:56,046 Epoch 783: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.74 
2024-02-04 12:23:56,046 EPOCH 784
2024-02-04 12:24:06,685 Epoch 784: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-04 12:24:06,685 EPOCH 785
2024-02-04 12:24:17,489 Epoch 785: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-04 12:24:17,489 EPOCH 786
2024-02-04 12:24:27,993 Epoch 786: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.58 
2024-02-04 12:24:27,993 EPOCH 787
2024-02-04 12:24:38,569 Epoch 787: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.64 
2024-02-04 12:24:38,569 EPOCH 788
2024-02-04 12:24:49,052 Epoch 788: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.62 
2024-02-04 12:24:49,053 EPOCH 789
2024-02-04 12:24:49,724 [Epoch: 789 Step: 00013400] Batch Recognition Loss:   0.000180 => Gls Tokens per Sec:     3821 || Batch Translation Loss:   0.030370 => Txt Tokens per Sec:     9145 || Lr: 0.000100
2024-02-04 12:24:59,747 Epoch 789: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.58 
2024-02-04 12:24:59,748 EPOCH 790
2024-02-04 12:25:10,421 Epoch 790: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.60 
2024-02-04 12:25:10,422 EPOCH 791
2024-02-04 12:25:20,926 Epoch 791: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.71 
2024-02-04 12:25:20,926 EPOCH 792
2024-02-04 12:25:31,505 Epoch 792: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-04 12:25:31,505 EPOCH 793
2024-02-04 12:25:41,946 Epoch 793: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-04 12:25:41,947 EPOCH 794
2024-02-04 12:25:52,706 Epoch 794: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.96 
2024-02-04 12:25:52,707 EPOCH 795
2024-02-04 12:25:53,094 [Epoch: 795 Step: 00013500] Batch Recognition Loss:   0.000169 => Gls Tokens per Sec:     3325 || Batch Translation Loss:   0.079458 => Txt Tokens per Sec:     9086 || Lr: 0.000100
2024-02-04 12:26:03,140 Epoch 795: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.28 
2024-02-04 12:26:03,141 EPOCH 796
2024-02-04 12:26:13,677 Epoch 796: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.25 
2024-02-04 12:26:13,678 EPOCH 797
2024-02-04 12:26:24,332 Epoch 797: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.03 
2024-02-04 12:26:24,332 EPOCH 798
2024-02-04 12:26:35,043 Epoch 798: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.05 
2024-02-04 12:26:35,043 EPOCH 799
2024-02-04 12:26:45,430 Epoch 799: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.01 
2024-02-04 12:26:45,431 EPOCH 800
2024-02-04 12:26:55,916 [Epoch: 800 Step: 00013600] Batch Recognition Loss:   0.000199 => Gls Tokens per Sec:     1014 || Batch Translation Loss:   0.130436 => Txt Tokens per Sec:     2815 || Lr: 0.000100
2024-02-04 12:26:55,916 Epoch 800: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.25 
2024-02-04 12:26:55,916 EPOCH 801
2024-02-04 12:27:06,559 Epoch 801: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.62 
2024-02-04 12:27:06,560 EPOCH 802
2024-02-04 12:27:17,070 Epoch 802: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.25 
2024-02-04 12:27:17,071 EPOCH 803
2024-02-04 12:27:27,551 Epoch 803: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.45 
2024-02-04 12:27:27,552 EPOCH 804
2024-02-04 12:27:38,287 Epoch 804: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.35 
2024-02-04 12:27:38,288 EPOCH 805
2024-02-04 12:27:49,108 Epoch 805: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.98 
2024-02-04 12:27:49,108 EPOCH 806
2024-02-04 12:27:59,377 [Epoch: 806 Step: 00013700] Batch Recognition Loss:   0.000307 => Gls Tokens per Sec:      911 || Batch Translation Loss:   0.046850 => Txt Tokens per Sec:     2533 || Lr: 0.000100
2024-02-04 12:27:59,877 Epoch 806: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.24 
2024-02-04 12:27:59,878 EPOCH 807
2024-02-04 12:28:10,552 Epoch 807: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.34 
2024-02-04 12:28:10,553 EPOCH 808
2024-02-04 12:28:21,122 Epoch 808: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.12 
2024-02-04 12:28:21,123 EPOCH 809
2024-02-04 12:28:31,572 Epoch 809: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.91 
2024-02-04 12:28:31,573 EPOCH 810
2024-02-04 12:28:42,216 Epoch 810: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.92 
2024-02-04 12:28:42,217 EPOCH 811
2024-02-04 12:28:52,860 Epoch 811: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.09 
2024-02-04 12:28:52,860 EPOCH 812
2024-02-04 12:28:59,206 [Epoch: 812 Step: 00013800] Batch Recognition Loss:   0.000274 => Gls Tokens per Sec:     1272 || Batch Translation Loss:   0.124334 => Txt Tokens per Sec:     3522 || Lr: 0.000100
2024-02-04 12:29:03,025 Epoch 812: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.86 
2024-02-04 12:29:03,025 EPOCH 813
2024-02-04 12:29:13,833 Epoch 813: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.56 
2024-02-04 12:29:13,834 EPOCH 814
2024-02-04 12:29:24,441 Epoch 814: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.47 
2024-02-04 12:29:24,442 EPOCH 815
2024-02-04 12:29:34,859 Epoch 815: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.73 
2024-02-04 12:29:34,860 EPOCH 816
2024-02-04 12:29:45,746 Epoch 816: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.97 
2024-02-04 12:29:45,747 EPOCH 817
2024-02-04 12:29:56,266 Epoch 817: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.81 
2024-02-04 12:29:56,267 EPOCH 818
2024-02-04 12:30:01,786 [Epoch: 818 Step: 00013900] Batch Recognition Loss:   0.000217 => Gls Tokens per Sec:     1276 || Batch Translation Loss:   0.033906 => Txt Tokens per Sec:     3506 || Lr: 0.000100
2024-02-04 12:30:07,015 Epoch 818: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.73 
2024-02-04 12:30:07,015 EPOCH 819
2024-02-04 12:30:17,398 Epoch 819: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-04 12:30:17,399 EPOCH 820
2024-02-04 12:30:28,180 Epoch 820: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.68 
2024-02-04 12:30:28,181 EPOCH 821
2024-02-04 12:30:38,697 Epoch 821: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.07 
2024-02-04 12:30:38,697 EPOCH 822
2024-02-04 12:30:49,155 Epoch 822: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.91 
2024-02-04 12:30:49,156 EPOCH 823
2024-02-04 12:30:59,741 Epoch 823: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.14 
2024-02-04 12:30:59,742 EPOCH 824
2024-02-04 12:31:04,625 [Epoch: 824 Step: 00014000] Batch Recognition Loss:   0.000191 => Gls Tokens per Sec:     1180 || Batch Translation Loss:   0.085916 => Txt Tokens per Sec:     3386 || Lr: 0.000100
2024-02-04 12:31:42,470 Validation result at epoch 824, step    14000: duration: 37.8437s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00025	Translation Loss: 94254.34375	PPL: 12481.93164
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.70	(BLEU-1: 10.79,	BLEU-2: 3.48,	BLEU-3: 1.41,	BLEU-4: 0.70)
	CHRF 17.28	ROUGE 9.23
2024-02-04 12:31:42,472 Logging Recognition and Translation Outputs
2024-02-04 12:31:42,472 ========================================================================================================================
2024-02-04 12:31:42,473 Logging Sequence: 141_40.00
2024-02-04 12:31:42,473 	Gloss Reference :	A B+C+D+E
2024-02-04 12:31:42,473 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 12:31:42,473 	Gloss Alignment :	         
2024-02-04 12:31:42,473 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 12:31:42,476 	Text Reference  :	got infected with  covid-19 he  was quarantined and could not take part in      the warmup match
2024-02-04 12:31:42,476 	Text Hypothesis :	*** ******** india had      won the toss        and ***** *** **** **** decided to  bowl   first
2024-02-04 12:31:42,476 	Text Alignment  :	D   D        S     S        S   S   S               D     D   D    D    S       S   S      S    
2024-02-04 12:31:42,477 ========================================================================================================================
2024-02-04 12:31:42,477 Logging Sequence: 117_37.00
2024-02-04 12:31:42,477 	Gloss Reference :	A B+C+D+E
2024-02-04 12:31:42,477 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 12:31:42,477 	Gloss Alignment :	         
2024-02-04 12:31:42,477 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 12:31:42,478 	Text Reference  :	shikhar dhawan put up a    wonderful performance scoring 98     runs
2024-02-04 12:31:42,478 	Text Hypothesis :	******* ****** *** on 23rd september 2022        he      scored 3175
2024-02-04 12:31:42,478 	Text Alignment  :	D       D      D   S  S    S         S           S       S      S   
2024-02-04 12:31:42,478 ========================================================================================================================
2024-02-04 12:31:42,479 Logging Sequence: 64_13.00
2024-02-04 12:31:42,479 	Gloss Reference :	A B+C+D+E
2024-02-04 12:31:42,479 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 12:31:42,479 	Gloss Alignment :	         
2024-02-04 12:31:42,479 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 12:31:42,480 	Text Reference  :	*** arrangements were made to  move all the ipl matches to the ******* *** wankhede stadium in     mumbai
2024-02-04 12:31:42,481 	Text Hypothesis :	for the          bcci has  not move *** *** ipl ******* to the country has banned   all     indian team  
2024-02-04 12:31:42,481 	Text Alignment  :	I   S            S    S    S        D   D       D              I       I   S        S       S      S     
2024-02-04 12:31:42,481 ========================================================================================================================
2024-02-04 12:31:42,481 Logging Sequence: 98_121.00
2024-02-04 12:31:42,481 	Gloss Reference :	A B+C+D+E
2024-02-04 12:31:42,481 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 12:31:42,481 	Gloss Alignment :	         
2024-02-04 12:31:42,481 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 12:31:42,482 	Text Reference  :	so then england legends and bangladesh legends were added to    the     tournament
2024-02-04 12:31:42,482 	Text Hypothesis :	** **** out     of      the tournament still   a    huge  fight between india     
2024-02-04 12:31:42,483 	Text Alignment  :	D  D    S       S       S   S          S       S    S     S     S       S         
2024-02-04 12:31:42,483 ========================================================================================================================
2024-02-04 12:31:42,483 Logging Sequence: 179_414.00
2024-02-04 12:31:42,483 	Gloss Reference :	A B+C+D+E
2024-02-04 12:31:42,483 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 12:31:42,483 	Gloss Alignment :	         
2024-02-04 12:31:42,483 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 12:31:42,485 	Text Reference  :	we could not    travel to      delhi as    there     was a lockdown in our home town     haryana
2024-02-04 12:31:42,485 	Text Hypothesis :	** ***** people were   stunned over  their behaviour for a ******** ** *** sai  official twitter
2024-02-04 12:31:42,485 	Text Alignment  :	D  D     S      S      S       S     S     S         S     D        D  D   S    S        S      
2024-02-04 12:31:42,485 ========================================================================================================================
2024-02-04 12:31:48,381 Epoch 824: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.23 
2024-02-04 12:31:48,382 EPOCH 825
2024-02-04 12:31:59,591 Epoch 825: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.21 
2024-02-04 12:31:59,591 EPOCH 826
2024-02-04 12:32:10,137 Epoch 826: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.92 
2024-02-04 12:32:10,137 EPOCH 827
2024-02-04 12:32:20,880 Epoch 827: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.85 
2024-02-04 12:32:20,881 EPOCH 828
2024-02-04 12:32:31,073 Epoch 828: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.85 
2024-02-04 12:32:31,073 EPOCH 829
2024-02-04 12:32:41,951 Epoch 829: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.63 
2024-02-04 12:32:41,951 EPOCH 830
2024-02-04 12:32:44,703 [Epoch: 830 Step: 00014100] Batch Recognition Loss:   0.000162 => Gls Tokens per Sec:     1629 || Batch Translation Loss:   0.033703 => Txt Tokens per Sec:     4230 || Lr: 0.000100
2024-02-04 12:32:52,467 Epoch 830: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.63 
2024-02-04 12:32:52,468 EPOCH 831
2024-02-04 12:33:02,895 Epoch 831: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.59 
2024-02-04 12:33:02,896 EPOCH 832
2024-02-04 12:33:13,413 Epoch 832: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-04 12:33:13,413 EPOCH 833
2024-02-04 12:33:24,009 Epoch 833: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.55 
2024-02-04 12:33:24,009 EPOCH 834
2024-02-04 12:33:34,536 Epoch 834: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.14 
2024-02-04 12:33:34,536 EPOCH 835
2024-02-04 12:33:45,263 Epoch 835: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.01 
2024-02-04 12:33:45,264 EPOCH 836
2024-02-04 12:33:48,606 [Epoch: 836 Step: 00014200] Batch Recognition Loss:   0.000197 => Gls Tokens per Sec:      883 || Batch Translation Loss:   0.041770 => Txt Tokens per Sec:     2213 || Lr: 0.000100
2024-02-04 12:33:55,708 Epoch 836: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.22 
2024-02-04 12:33:55,708 EPOCH 837
2024-02-04 12:34:06,365 Epoch 837: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.30 
2024-02-04 12:34:06,366 EPOCH 838
2024-02-04 12:34:16,896 Epoch 838: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.90 
2024-02-04 12:34:16,897 EPOCH 839
2024-02-04 12:34:27,363 Epoch 839: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.97 
2024-02-04 12:34:27,364 EPOCH 840
2024-02-04 12:34:37,914 Epoch 840: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.00 
2024-02-04 12:34:37,915 EPOCH 841
2024-02-04 12:34:48,601 Epoch 841: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.22 
2024-02-04 12:34:48,601 EPOCH 842
2024-02-04 12:34:50,613 [Epoch: 842 Step: 00014300] Batch Recognition Loss:   0.000314 => Gls Tokens per Sec:      955 || Batch Translation Loss:   0.085147 => Txt Tokens per Sec:     2659 || Lr: 0.000100
2024-02-04 12:34:59,100 Epoch 842: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.18 
2024-02-04 12:34:59,100 EPOCH 843
2024-02-04 12:35:09,668 Epoch 843: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.03 
2024-02-04 12:35:09,669 EPOCH 844
2024-02-04 12:35:20,308 Epoch 844: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.07 
2024-02-04 12:35:20,309 EPOCH 845
2024-02-04 12:35:30,797 Epoch 845: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.79 
2024-02-04 12:35:30,798 EPOCH 846
2024-02-04 12:35:41,388 Epoch 846: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.11 
2024-02-04 12:35:41,388 EPOCH 847
2024-02-04 12:35:51,623 Epoch 847: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.91 
2024-02-04 12:35:51,623 EPOCH 848
2024-02-04 12:35:53,224 [Epoch: 848 Step: 00014400] Batch Recognition Loss:   0.000373 => Gls Tokens per Sec:      400 || Batch Translation Loss:   0.050442 => Txt Tokens per Sec:     1296 || Lr: 0.000100
2024-02-04 12:36:02,308 Epoch 848: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.34 
2024-02-04 12:36:02,309 EPOCH 849
2024-02-04 12:36:13,329 Epoch 849: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.47 
2024-02-04 12:36:13,330 EPOCH 850
2024-02-04 12:36:24,101 Epoch 850: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.22 
2024-02-04 12:36:24,102 EPOCH 851
2024-02-04 12:36:34,656 Epoch 851: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.30 
2024-02-04 12:36:34,656 EPOCH 852
2024-02-04 12:36:45,303 Epoch 852: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.32 
2024-02-04 12:36:45,304 EPOCH 853
2024-02-04 12:36:54,042 [Epoch: 853 Step: 00014500] Batch Recognition Loss:   0.000268 => Gls Tokens per Sec:     1143 || Batch Translation Loss:   0.174139 => Txt Tokens per Sec:     3123 || Lr: 0.000100
2024-02-04 12:36:55,940 Epoch 853: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.00 
2024-02-04 12:36:55,940 EPOCH 854
2024-02-04 12:37:06,931 Epoch 854: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.90 
2024-02-04 12:37:06,932 EPOCH 855
2024-02-04 12:37:17,353 Epoch 855: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.12 
2024-02-04 12:37:17,354 EPOCH 856
2024-02-04 12:37:28,053 Epoch 856: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.05 
2024-02-04 12:37:28,054 EPOCH 857
2024-02-04 12:37:38,693 Epoch 857: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.70 
2024-02-04 12:37:38,693 EPOCH 858
2024-02-04 12:37:49,107 Epoch 858: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.63 
2024-02-04 12:37:49,107 EPOCH 859
2024-02-04 12:37:58,853 [Epoch: 859 Step: 00014600] Batch Recognition Loss:   0.000228 => Gls Tokens per Sec:      894 || Batch Translation Loss:   0.024340 => Txt Tokens per Sec:     2496 || Lr: 0.000100
2024-02-04 12:37:59,545 Epoch 859: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-04 12:37:59,546 EPOCH 860
2024-02-04 12:38:10,018 Epoch 860: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-04 12:38:10,018 EPOCH 861
2024-02-04 12:38:20,754 Epoch 861: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.62 
2024-02-04 12:38:20,754 EPOCH 862
2024-02-04 12:38:31,453 Epoch 862: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.60 
2024-02-04 12:38:31,454 EPOCH 863
2024-02-04 12:38:42,222 Epoch 863: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.58 
2024-02-04 12:38:42,222 EPOCH 864
2024-02-04 12:38:52,769 Epoch 864: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-04 12:38:52,769 EPOCH 865
2024-02-04 12:39:02,347 [Epoch: 865 Step: 00014700] Batch Recognition Loss:   0.000222 => Gls Tokens per Sec:      776 || Batch Translation Loss:   0.047416 => Txt Tokens per Sec:     2198 || Lr: 0.000100
2024-02-04 12:39:03,455 Epoch 865: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-04 12:39:03,455 EPOCH 866
2024-02-04 12:39:13,851 Epoch 866: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-04 12:39:13,851 EPOCH 867
2024-02-04 12:39:24,389 Epoch 867: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.48 
2024-02-04 12:39:24,390 EPOCH 868
2024-02-04 12:39:34,724 Epoch 868: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.66 
2024-02-04 12:39:34,725 EPOCH 869
2024-02-04 12:39:45,218 Epoch 869: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.65 
2024-02-04 12:39:45,218 EPOCH 870
2024-02-04 12:39:55,941 Epoch 870: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.75 
2024-02-04 12:39:55,942 EPOCH 871
2024-02-04 12:39:59,661 [Epoch: 871 Step: 00014800] Batch Recognition Loss:   0.000304 => Gls Tokens per Sec:     1721 || Batch Translation Loss:   0.054144 => Txt Tokens per Sec:     4809 || Lr: 0.000100
2024-02-04 12:40:06,411 Epoch 871: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.62 
2024-02-04 12:40:06,412 EPOCH 872
2024-02-04 12:40:17,030 Epoch 872: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.81 
2024-02-04 12:40:17,031 EPOCH 873
2024-02-04 12:40:27,537 Epoch 873: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.00 
2024-02-04 12:40:27,537 EPOCH 874
2024-02-04 12:40:37,878 Epoch 874: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.33 
2024-02-04 12:40:37,878 EPOCH 875
2024-02-04 12:40:48,303 Epoch 875: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.40 
2024-02-04 12:40:48,304 EPOCH 876
2024-02-04 12:40:58,622 Epoch 876: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.15 
2024-02-04 12:40:58,623 EPOCH 877
2024-02-04 12:41:05,965 [Epoch: 877 Step: 00014900] Batch Recognition Loss:   0.000358 => Gls Tokens per Sec:      663 || Batch Translation Loss:   0.047218 => Txt Tokens per Sec:     2002 || Lr: 0.000100
2024-02-04 12:41:09,190 Epoch 877: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.91 
2024-02-04 12:41:09,190 EPOCH 878
2024-02-04 12:41:19,620 Epoch 878: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.79 
2024-02-04 12:41:19,621 EPOCH 879
2024-02-04 12:41:30,025 Epoch 879: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.80 
2024-02-04 12:41:30,025 EPOCH 880
2024-02-04 12:41:40,690 Epoch 880: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.80 
2024-02-04 12:41:40,691 EPOCH 881
2024-02-04 12:41:51,025 Epoch 881: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.77 
2024-02-04 12:41:51,026 EPOCH 882
2024-02-04 12:42:01,620 Epoch 882: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.87 
2024-02-04 12:42:01,620 EPOCH 883
2024-02-04 12:42:04,132 [Epoch: 883 Step: 00015000] Batch Recognition Loss:   0.000147 => Gls Tokens per Sec:     1530 || Batch Translation Loss:   0.037096 => Txt Tokens per Sec:     4314 || Lr: 0.000100
2024-02-04 12:42:12,119 Epoch 883: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.21 
2024-02-04 12:42:12,120 EPOCH 884
2024-02-04 12:42:22,697 Epoch 884: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.47 
2024-02-04 12:42:22,698 EPOCH 885
2024-02-04 12:42:33,436 Epoch 885: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.20 
2024-02-04 12:42:33,437 EPOCH 886
2024-02-04 12:42:43,986 Epoch 886: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.30 
2024-02-04 12:42:43,987 EPOCH 887
2024-02-04 12:42:54,635 Epoch 887: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.93 
2024-02-04 12:42:54,635 EPOCH 888
2024-02-04 12:43:06,170 Epoch 888: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.01 
2024-02-04 12:43:06,170 EPOCH 889
2024-02-04 12:43:07,140 [Epoch: 889 Step: 00015100] Batch Recognition Loss:   0.000260 => Gls Tokens per Sec:     2644 || Batch Translation Loss:   0.038137 => Txt Tokens per Sec:     7442 || Lr: 0.000100
2024-02-04 12:43:16,762 Epoch 889: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.05 
2024-02-04 12:43:16,764 EPOCH 890
2024-02-04 12:43:27,270 Epoch 890: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.87 
2024-02-04 12:43:27,271 EPOCH 891
2024-02-04 12:43:37,899 Epoch 891: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.03 
2024-02-04 12:43:37,900 EPOCH 892
2024-02-04 12:43:48,563 Epoch 892: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.18 
2024-02-04 12:43:48,563 EPOCH 893
2024-02-04 12:43:59,435 Epoch 893: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.03 
2024-02-04 12:43:59,436 EPOCH 894
2024-02-04 12:44:12,262 Epoch 894: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.83 
2024-02-04 12:44:12,263 EPOCH 895
2024-02-04 12:44:15,903 [Epoch: 895 Step: 00015200] Batch Recognition Loss:   0.000258 => Gls Tokens per Sec:      283 || Batch Translation Loss:   0.066406 => Txt Tokens per Sec:      950 || Lr: 0.000100
2024-02-04 12:44:24,450 Epoch 895: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.91 
2024-02-04 12:44:24,450 EPOCH 896
2024-02-04 12:44:37,928 Epoch 896: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.66 
2024-02-04 12:44:37,929 EPOCH 897
2024-02-04 12:44:49,134 Epoch 897: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.60 
2024-02-04 12:44:49,134 EPOCH 898
2024-02-04 12:45:00,165 Epoch 898: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.80 
2024-02-04 12:45:00,166 EPOCH 899
2024-02-04 12:45:10,846 Epoch 899: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.85 
2024-02-04 12:45:10,847 EPOCH 900
2024-02-04 12:45:21,449 [Epoch: 900 Step: 00015300] Batch Recognition Loss:   0.000167 => Gls Tokens per Sec:     1003 || Batch Translation Loss:   0.027534 => Txt Tokens per Sec:     2784 || Lr: 0.000100
2024-02-04 12:45:21,450 Epoch 900: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.02 
2024-02-04 12:45:21,450 EPOCH 901
2024-02-04 12:45:32,174 Epoch 901: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.75 
2024-02-04 12:45:32,175 EPOCH 902
2024-02-04 12:45:42,807 Epoch 902: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.59 
2024-02-04 12:45:42,808 EPOCH 903
2024-02-04 12:45:53,594 Epoch 903: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-04 12:45:53,594 EPOCH 904
2024-02-04 12:46:04,408 Epoch 904: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-04 12:46:04,409 EPOCH 905
2024-02-04 12:46:15,107 Epoch 905: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-04 12:46:15,108 EPOCH 906
2024-02-04 12:46:22,238 [Epoch: 906 Step: 00015400] Batch Recognition Loss:   0.000198 => Gls Tokens per Sec:     1312 || Batch Translation Loss:   0.019712 => Txt Tokens per Sec:     3536 || Lr: 0.000100
2024-02-04 12:46:25,521 Epoch 906: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.55 
2024-02-04 12:46:25,522 EPOCH 907
2024-02-04 12:46:36,363 Epoch 907: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-04 12:46:36,364 EPOCH 908
2024-02-04 12:46:47,326 Epoch 908: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.70 
2024-02-04 12:46:47,327 EPOCH 909
2024-02-04 12:46:57,795 Epoch 909: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-04 12:46:57,796 EPOCH 910
2024-02-04 12:47:08,346 Epoch 910: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.74 
2024-02-04 12:47:08,347 EPOCH 911
2024-02-04 12:47:18,707 Epoch 911: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.80 
2024-02-04 12:47:18,708 EPOCH 912
2024-02-04 12:47:28,535 [Epoch: 912 Step: 00015500] Batch Recognition Loss:   0.000193 => Gls Tokens per Sec:      821 || Batch Translation Loss:   0.032184 => Txt Tokens per Sec:     2292 || Lr: 0.000100
2024-02-04 12:47:29,376 Epoch 912: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.98 
2024-02-04 12:47:29,377 EPOCH 913
2024-02-04 12:47:40,073 Epoch 913: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.92 
2024-02-04 12:47:40,074 EPOCH 914
2024-02-04 12:47:51,011 Epoch 914: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.98 
2024-02-04 12:47:51,011 EPOCH 915
2024-02-04 12:48:01,480 Epoch 915: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.74 
2024-02-04 12:48:01,481 EPOCH 916
2024-02-04 12:48:12,032 Epoch 916: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.88 
2024-02-04 12:48:12,033 EPOCH 917
2024-02-04 12:48:22,665 Epoch 917: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.88 
2024-02-04 12:48:22,665 EPOCH 918
2024-02-04 12:48:30,376 [Epoch: 918 Step: 00015600] Batch Recognition Loss:   0.000236 => Gls Tokens per Sec:      881 || Batch Translation Loss:   0.062169 => Txt Tokens per Sec:     2447 || Lr: 0.000100
2024-02-04 12:48:33,282 Epoch 918: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.85 
2024-02-04 12:48:33,282 EPOCH 919
2024-02-04 12:48:44,277 Epoch 919: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.71 
2024-02-04 12:48:44,278 EPOCH 920
2024-02-04 12:48:55,180 Epoch 920: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.57 
2024-02-04 12:48:55,181 EPOCH 921
2024-02-04 12:49:05,973 Epoch 921: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-04 12:49:05,974 EPOCH 922
2024-02-04 12:49:16,557 Epoch 922: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.72 
2024-02-04 12:49:16,558 EPOCH 923
2024-02-04 12:49:27,051 Epoch 923: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.76 
2024-02-04 12:49:27,052 EPOCH 924
2024-02-04 12:49:32,345 [Epoch: 924 Step: 00015700] Batch Recognition Loss:   0.000200 => Gls Tokens per Sec:     1089 || Batch Translation Loss:   0.033063 => Txt Tokens per Sec:     3178 || Lr: 0.000100
2024-02-04 12:49:37,997 Epoch 924: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.30 
2024-02-04 12:49:37,998 EPOCH 925
2024-02-04 12:49:48,840 Epoch 925: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.47 
2024-02-04 12:49:48,841 EPOCH 926
2024-02-04 12:49:59,426 Epoch 926: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.08 
2024-02-04 12:49:59,427 EPOCH 927
2024-02-04 12:50:10,050 Epoch 927: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.17 
2024-02-04 12:50:10,051 EPOCH 928
2024-02-04 12:50:20,905 Epoch 928: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.43 
2024-02-04 12:50:20,906 EPOCH 929
2024-02-04 12:50:31,780 Epoch 929: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.47 
2024-02-04 12:50:31,780 EPOCH 930
2024-02-04 12:50:34,665 [Epoch: 930 Step: 00015800] Batch Recognition Loss:   0.000272 => Gls Tokens per Sec:     1554 || Batch Translation Loss:   0.056569 => Txt Tokens per Sec:     4126 || Lr: 0.000100
2024-02-04 12:50:42,524 Epoch 930: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.58 
2024-02-04 12:50:42,524 EPOCH 931
2024-02-04 12:50:53,397 Epoch 931: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.38 
2024-02-04 12:50:53,398 EPOCH 932
2024-02-04 12:51:03,826 Epoch 932: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.92 
2024-02-04 12:51:03,826 EPOCH 933
2024-02-04 12:51:14,290 Epoch 933: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.73 
2024-02-04 12:51:14,291 EPOCH 934
2024-02-04 12:51:25,192 Epoch 934: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.57 
2024-02-04 12:51:25,192 EPOCH 935
2024-02-04 12:51:35,767 Epoch 935: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.58 
2024-02-04 12:51:35,768 EPOCH 936
2024-02-04 12:51:40,858 [Epoch: 936 Step: 00015900] Batch Recognition Loss:   0.000171 => Gls Tokens per Sec:      580 || Batch Translation Loss:   0.017220 => Txt Tokens per Sec:     1717 || Lr: 0.000100
2024-02-04 12:51:46,582 Epoch 936: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.68 
2024-02-04 12:51:46,583 EPOCH 937
2024-02-04 12:51:57,167 Epoch 937: Total Training Recognition Loss 0.00  Total Training Translation Loss 5.23 
2024-02-04 12:51:57,168 EPOCH 938
2024-02-04 12:52:07,751 Epoch 938: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.61 
2024-02-04 12:52:07,752 EPOCH 939
2024-02-04 12:52:18,344 Epoch 939: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.10 
2024-02-04 12:52:18,345 EPOCH 940
2024-02-04 12:52:29,013 Epoch 940: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.35 
2024-02-04 12:52:29,014 EPOCH 941
2024-02-04 12:52:39,714 Epoch 941: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.12 
2024-02-04 12:52:39,715 EPOCH 942
2024-02-04 12:52:40,526 [Epoch: 942 Step: 00016000] Batch Recognition Loss:   0.000506 => Gls Tokens per Sec:     2372 || Batch Translation Loss:   0.071849 => Txt Tokens per Sec:     6730 || Lr: 0.000100
2024-02-04 12:53:18,639 Validation result at epoch 942, step    16000: duration: 38.1135s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00044	Translation Loss: 94963.22656	PPL: 13399.52441
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.77	(BLEU-1: 11.34,	BLEU-2: 3.53,	BLEU-3: 1.45,	BLEU-4: 0.77)
	CHRF 17.19	ROUGE 9.65
2024-02-04 12:53:18,642 Logging Recognition and Translation Outputs
2024-02-04 12:53:18,642 ========================================================================================================================
2024-02-04 12:53:18,642 Logging Sequence: 147_132.00
2024-02-04 12:53:18,642 	Gloss Reference :	A B+C+D+E
2024-02-04 12:53:18,642 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 12:53:18,642 	Gloss Alignment :	         
2024-02-04 12:53:18,643 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 12:53:18,645 	Text Reference  :	** i  can not ***** ** *** **** *** earlier i     used to ** have    fun in   gymnastics
2024-02-04 12:53:18,645 	Text Hypothesis :	if it is  not known as the same but she     would have to be stopped her from them      
2024-02-04 12:53:18,645 	Text Alignment  :	I  S  S       I     I  I   I    I   S       S     S       I  S       S   S    S         
2024-02-04 12:53:18,646 ========================================================================================================================
2024-02-04 12:53:18,646 Logging Sequence: 116_162.00
2024-02-04 12:53:18,646 	Gloss Reference :	A B+C+D+E
2024-02-04 12:53:18,646 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 12:53:18,646 	Gloss Alignment :	         
2024-02-04 12:53:18,646 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 12:53:18,648 	Text Reference  :	turned out  the     video was shared on social media by   a staff   at   the  hotel
2024-02-04 12:53:18,648 	Text Hypothesis :	on     31st october 2022  he  shared a  video  along with a caption that went viral
2024-02-04 12:53:18,648 	Text Alignment  :	S      S    S       S     S          S  S      S     S      S       S    S    S    
2024-02-04 12:53:18,648 ========================================================================================================================
2024-02-04 12:53:18,648 Logging Sequence: 73_79.00
2024-02-04 12:53:18,648 	Gloss Reference :	A B+C+D+E
2024-02-04 12:53:18,648 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 12:53:18,649 	Gloss Alignment :	         
2024-02-04 12:53:18,649 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 12:53:18,650 	Text Reference  :	raina resturant has food from the rich spices    of   north india to     the aromatic curries of south india     
2024-02-04 12:53:18,650 	Text Hypothesis :	***** ********* *** **** **** on  23rd september 2023 raina has   opened the ******** ******* ** ***** restaurant
2024-02-04 12:53:18,650 	Text Alignment  :	D     D         D   D    D    S   S    S         S    S     S     S          D        D       D  D     S         
2024-02-04 12:53:18,650 ========================================================================================================================
2024-02-04 12:53:18,650 Logging Sequence: 165_523.00
2024-02-04 12:53:18,650 	Gloss Reference :	A B+C+D+E
2024-02-04 12:53:18,651 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 12:53:18,651 	Gloss Alignment :	         
2024-02-04 12:53:18,651 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 12:53:18,652 	Text Reference  :	***** *** *** *** **** as  he     believed that his    team might lose if he   takes off  his  batting pads
2024-02-04 12:53:18,653 	Text Hypothesis :	india had won the toss and choose to       bowl indian team ***** with 5  boys and   they have left    pad 
2024-02-04 12:53:18,653 	Text Alignment  :	I     I   I   I   I    S   S      S        S    S           D     S    S  S    S     S    S    S       S   
2024-02-04 12:53:18,653 ========================================================================================================================
2024-02-04 12:53:18,653 Logging Sequence: 125_72.00
2024-02-04 12:53:18,653 	Gloss Reference :	A B+C+D+E
2024-02-04 12:53:18,653 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 12:53:18,653 	Gloss Alignment :	         
2024-02-04 12:53:18,653 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 12:53:18,654 	Text Reference  :	some said the pakistani javelineer had milicious intentions of  tampering  with      the javelin out of   jealousy
2024-02-04 12:53:18,654 	Text Hypothesis :	**** **** *** ********* ********** *** ********* india      has completely dominated the ******* *** gold medal   
2024-02-04 12:53:18,655 	Text Alignment  :	D    D    D   D         D          D   D         S          S   S          S             D       D   S    S       
2024-02-04 12:53:18,655 ========================================================================================================================
2024-02-04 12:53:29,321 Epoch 942: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.82 
2024-02-04 12:53:29,321 EPOCH 943
2024-02-04 12:53:40,018 Epoch 943: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.94 
2024-02-04 12:53:40,019 EPOCH 944
2024-02-04 12:53:50,729 Epoch 944: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-04 12:53:50,729 EPOCH 945
2024-02-04 12:54:01,506 Epoch 945: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-04 12:54:01,507 EPOCH 946
2024-02-04 12:54:12,168 Epoch 946: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-04 12:54:12,168 EPOCH 947
2024-02-04 12:54:22,843 Epoch 947: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-04 12:54:22,843 EPOCH 948
2024-02-04 12:54:23,204 [Epoch: 948 Step: 00016100] Batch Recognition Loss:   0.000226 => Gls Tokens per Sec:     1783 || Batch Translation Loss:   0.018606 => Txt Tokens per Sec:     5733 || Lr: 0.000100
2024-02-04 12:54:33,460 Epoch 948: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-04 12:54:33,460 EPOCH 949
2024-02-04 12:54:43,980 Epoch 949: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-04 12:54:43,981 EPOCH 950
2024-02-04 12:54:54,562 Epoch 950: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-04 12:54:54,562 EPOCH 951
2024-02-04 12:55:05,286 Epoch 951: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-04 12:55:05,287 EPOCH 952
2024-02-04 12:55:16,181 Epoch 952: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-04 12:55:16,181 EPOCH 953
2024-02-04 12:55:26,364 [Epoch: 953 Step: 00016200] Batch Recognition Loss:   0.000156 => Gls Tokens per Sec:      981 || Batch Translation Loss:   0.025491 => Txt Tokens per Sec:     2744 || Lr: 0.000100
2024-02-04 12:55:26,627 Epoch 953: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-04 12:55:26,627 EPOCH 954
2024-02-04 12:55:37,629 Epoch 954: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-04 12:55:37,630 EPOCH 955
2024-02-04 12:55:48,338 Epoch 955: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-04 12:55:48,339 EPOCH 956
2024-02-04 12:55:59,185 Epoch 956: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-04 12:55:59,186 EPOCH 957
2024-02-04 12:56:10,103 Epoch 957: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-04 12:56:10,103 EPOCH 958
2024-02-04 12:56:20,761 Epoch 958: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-04 12:56:20,761 EPOCH 959
2024-02-04 12:56:26,808 [Epoch: 959 Step: 00016300] Batch Recognition Loss:   0.000226 => Gls Tokens per Sec:     1482 || Batch Translation Loss:   0.010287 => Txt Tokens per Sec:     3960 || Lr: 0.000100
2024-02-04 12:56:31,503 Epoch 959: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-04 12:56:31,503 EPOCH 960
2024-02-04 12:56:42,257 Epoch 960: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-04 12:56:42,257 EPOCH 961
2024-02-04 12:56:52,950 Epoch 961: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-04 12:56:52,951 EPOCH 962
2024-02-04 12:57:03,501 Epoch 962: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.65 
2024-02-04 12:57:03,501 EPOCH 963
2024-02-04 12:57:13,985 Epoch 963: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.55 
2024-02-04 12:57:13,986 EPOCH 964
2024-02-04 12:57:24,845 Epoch 964: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-04 12:57:24,846 EPOCH 965
2024-02-04 12:57:34,400 [Epoch: 965 Step: 00016400] Batch Recognition Loss:   0.000162 => Gls Tokens per Sec:      778 || Batch Translation Loss:   0.023707 => Txt Tokens per Sec:     2205 || Lr: 0.000100
2024-02-04 12:57:35,758 Epoch 965: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-04 12:57:35,758 EPOCH 966
2024-02-04 12:57:46,512 Epoch 966: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-04 12:57:46,513 EPOCH 967
2024-02-04 12:57:57,277 Epoch 967: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-04 12:57:57,278 EPOCH 968
2024-02-04 12:58:07,900 Epoch 968: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.60 
2024-02-04 12:58:07,901 EPOCH 969
2024-02-04 12:58:18,457 Epoch 969: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.69 
2024-02-04 12:58:18,457 EPOCH 970
2024-02-04 12:58:29,218 Epoch 970: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.72 
2024-02-04 12:58:29,218 EPOCH 971
2024-02-04 12:58:32,828 [Epoch: 971 Step: 00016500] Batch Recognition Loss:   0.000160 => Gls Tokens per Sec:     1773 || Batch Translation Loss:   0.022961 => Txt Tokens per Sec:     4797 || Lr: 0.000100
2024-02-04 12:58:39,952 Epoch 971: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.67 
2024-02-04 12:58:39,953 EPOCH 972
2024-02-04 12:58:50,620 Epoch 972: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.25 
2024-02-04 12:58:50,620 EPOCH 973
2024-02-04 12:59:01,364 Epoch 973: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.87 
2024-02-04 12:59:01,365 EPOCH 974
2024-02-04 12:59:11,900 Epoch 974: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.70 
2024-02-04 12:59:11,900 EPOCH 975
2024-02-04 12:59:22,393 Epoch 975: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.74 
2024-02-04 12:59:22,394 EPOCH 976
2024-02-04 12:59:33,045 Epoch 976: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.06 
2024-02-04 12:59:33,045 EPOCH 977
2024-02-04 12:59:40,372 [Epoch: 977 Step: 00016600] Batch Recognition Loss:   0.000154 => Gls Tokens per Sec:      665 || Batch Translation Loss:   0.044949 => Txt Tokens per Sec:     1935 || Lr: 0.000100
2024-02-04 12:59:43,773 Epoch 977: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.03 
2024-02-04 12:59:43,774 EPOCH 978
2024-02-04 12:59:54,386 Epoch 978: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.21 
2024-02-04 12:59:54,386 EPOCH 979
2024-02-04 13:00:05,023 Epoch 979: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.97 
2024-02-04 13:00:05,023 EPOCH 980
2024-02-04 13:00:15,962 Epoch 980: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.12 
2024-02-04 13:00:15,962 EPOCH 981
2024-02-04 13:00:26,910 Epoch 981: Total Training Recognition Loss 0.00  Total Training Translation Loss 2.71 
2024-02-04 13:00:26,910 EPOCH 982
2024-02-04 13:00:37,592 Epoch 982: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.53 
2024-02-04 13:00:37,592 EPOCH 983
2024-02-04 13:00:38,893 [Epoch: 983 Step: 00016700] Batch Recognition Loss:   0.000952 => Gls Tokens per Sec:     2956 || Batch Translation Loss:   0.149254 => Txt Tokens per Sec:     8351 || Lr: 0.000100
2024-02-04 13:00:48,008 Epoch 983: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.03 
2024-02-04 13:00:48,009 EPOCH 984
2024-02-04 13:00:58,565 Epoch 984: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.17 
2024-02-04 13:00:58,566 EPOCH 985
2024-02-04 13:01:09,098 Epoch 985: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.46 
2024-02-04 13:01:09,099 EPOCH 986
2024-02-04 13:01:20,293 Epoch 986: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.19 
2024-02-04 13:01:20,294 EPOCH 987
2024-02-04 13:01:31,094 Epoch 987: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.87 
2024-02-04 13:01:31,095 EPOCH 988
2024-02-04 13:01:41,737 Epoch 988: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.73 
2024-02-04 13:01:41,738 EPOCH 989
2024-02-04 13:01:42,481 [Epoch: 989 Step: 00016800] Batch Recognition Loss:   0.000206 => Gls Tokens per Sec:     3449 || Batch Translation Loss:   0.045789 => Txt Tokens per Sec:     8855 || Lr: 0.000100
2024-02-04 13:01:52,311 Epoch 989: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.65 
2024-02-04 13:01:52,312 EPOCH 990
2024-02-04 13:02:02,815 Epoch 990: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-04 13:02:02,815 EPOCH 991
2024-02-04 13:02:13,643 Epoch 991: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-04 13:02:13,644 EPOCH 992
2024-02-04 13:02:24,494 Epoch 992: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-04 13:02:24,495 EPOCH 993
2024-02-04 13:02:35,355 Epoch 993: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-04 13:02:35,355 EPOCH 994
2024-02-04 13:02:46,114 Epoch 994: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.55 
2024-02-04 13:02:46,114 EPOCH 995
2024-02-04 13:02:48,987 [Epoch: 995 Step: 00016900] Batch Recognition Loss:   0.000189 => Gls Tokens per Sec:      359 || Batch Translation Loss:   0.025664 => Txt Tokens per Sec:     1167 || Lr: 0.000100
2024-02-04 13:02:56,855 Epoch 995: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-04 13:02:56,856 EPOCH 996
2024-02-04 13:03:07,793 Epoch 996: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-04 13:03:07,793 EPOCH 997
2024-02-04 13:03:18,530 Epoch 997: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.48 
2024-02-04 13:03:18,530 EPOCH 998
2024-02-04 13:03:29,017 Epoch 998: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-04 13:03:29,018 EPOCH 999
2024-02-04 13:03:39,744 Epoch 999: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-04 13:03:39,744 EPOCH 1000
2024-02-04 13:03:50,351 [Epoch: 1000 Step: 00017000] Batch Recognition Loss:   0.000158 => Gls Tokens per Sec:     1002 || Batch Translation Loss:   0.019776 => Txt Tokens per Sec:     2782 || Lr: 0.000100
2024-02-04 13:03:50,352 Epoch 1000: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-04 13:03:50,352 EPOCH 1001
2024-02-04 13:04:01,100 Epoch 1001: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-04 13:04:01,101 EPOCH 1002
2024-02-04 13:04:11,850 Epoch 1002: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-04 13:04:11,851 EPOCH 1003
2024-02-04 13:04:22,604 Epoch 1003: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-04 13:04:22,605 EPOCH 1004
2024-02-04 13:04:33,291 Epoch 1004: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-04 13:04:33,291 EPOCH 1005
2024-02-04 13:04:43,851 Epoch 1005: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-04 13:04:43,851 EPOCH 1006
2024-02-04 13:04:52,492 [Epoch: 1006 Step: 00017100] Batch Recognition Loss:   0.000137 => Gls Tokens per Sec:     1082 || Batch Translation Loss:   0.010178 => Txt Tokens per Sec:     2936 || Lr: 0.000100
2024-02-04 13:04:54,638 Epoch 1006: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-04 13:04:54,639 EPOCH 1007
2024-02-04 13:05:05,059 Epoch 1007: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-04 13:05:05,059 EPOCH 1008
2024-02-04 13:05:15,928 Epoch 1008: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-04 13:05:15,929 EPOCH 1009
2024-02-04 13:05:26,854 Epoch 1009: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-04 13:05:26,854 EPOCH 1010
2024-02-04 13:05:37,715 Epoch 1010: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-04 13:05:37,715 EPOCH 1011
2024-02-04 13:05:48,403 Epoch 1011: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-04 13:05:48,403 EPOCH 1012
2024-02-04 13:05:53,581 [Epoch: 1012 Step: 00017200] Batch Recognition Loss:   0.000139 => Gls Tokens per Sec:     1607 || Batch Translation Loss:   0.009780 => Txt Tokens per Sec:     4383 || Lr: 0.000100
2024-02-04 13:05:58,665 Epoch 1012: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-04 13:05:58,666 EPOCH 1013
2024-02-04 13:06:09,221 Epoch 1013: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.60 
2024-02-04 13:06:09,222 EPOCH 1014
2024-02-04 13:06:20,126 Epoch 1014: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.58 
2024-02-04 13:06:20,127 EPOCH 1015
2024-02-04 13:06:30,660 Epoch 1015: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.74 
2024-02-04 13:06:30,660 EPOCH 1016
2024-02-04 13:06:41,435 Epoch 1016: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.62 
2024-02-04 13:06:41,436 EPOCH 1017
2024-02-04 13:06:52,041 Epoch 1017: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.76 
2024-02-04 13:06:52,042 EPOCH 1018
2024-02-04 13:06:58,453 [Epoch: 1018 Step: 00017300] Batch Recognition Loss:   0.000176 => Gls Tokens per Sec:     1059 || Batch Translation Loss:   0.041386 => Txt Tokens per Sec:     2958 || Lr: 0.000100
2024-02-04 13:07:02,831 Epoch 1018: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.73 
2024-02-04 13:07:02,831 EPOCH 1019
2024-02-04 13:07:13,449 Epoch 1019: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.22 
2024-02-04 13:07:13,450 EPOCH 1020
2024-02-04 13:07:24,161 Epoch 1020: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.16 
2024-02-04 13:07:24,162 EPOCH 1021
2024-02-04 13:07:34,947 Epoch 1021: Total Training Recognition Loss 0.03  Total Training Translation Loss 13.44 
2024-02-04 13:07:34,947 EPOCH 1022
2024-02-04 13:07:45,659 Epoch 1022: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.66 
2024-02-04 13:07:45,659 EPOCH 1023
2024-02-04 13:07:56,302 Epoch 1023: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.66 
2024-02-04 13:07:56,302 EPOCH 1024
2024-02-04 13:08:01,378 [Epoch: 1024 Step: 00017400] Batch Recognition Loss:   0.000641 => Gls Tokens per Sec:     1135 || Batch Translation Loss:   0.057524 => Txt Tokens per Sec:     3178 || Lr: 0.000100
2024-02-04 13:08:06,981 Epoch 1024: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.52 
2024-02-04 13:08:06,982 EPOCH 1025
2024-02-04 13:08:17,877 Epoch 1025: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.04 
2024-02-04 13:08:17,877 EPOCH 1026
2024-02-04 13:08:28,686 Epoch 1026: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.84 
2024-02-04 13:08:28,687 EPOCH 1027
2024-02-04 13:08:39,347 Epoch 1027: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.70 
2024-02-04 13:08:39,347 EPOCH 1028
2024-02-04 13:08:50,101 Epoch 1028: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-04 13:08:50,102 EPOCH 1029
2024-02-04 13:09:00,774 Epoch 1029: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-04 13:09:00,774 EPOCH 1030
2024-02-04 13:09:05,429 [Epoch: 1030 Step: 00017500] Batch Recognition Loss:   0.000262 => Gls Tokens per Sec:      963 || Batch Translation Loss:   0.021754 => Txt Tokens per Sec:     2628 || Lr: 0.000100
2024-02-04 13:09:11,558 Epoch 1030: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-04 13:09:11,558 EPOCH 1031
2024-02-04 13:09:22,288 Epoch 1031: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-04 13:09:22,288 EPOCH 1032
2024-02-04 13:09:33,069 Epoch 1032: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-04 13:09:33,070 EPOCH 1033
2024-02-04 13:09:43,579 Epoch 1033: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-04 13:09:43,580 EPOCH 1034
2024-02-04 13:09:54,438 Epoch 1034: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-04 13:09:54,439 EPOCH 1035
2024-02-04 13:10:05,244 Epoch 1035: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-04 13:10:05,244 EPOCH 1036
2024-02-04 13:10:11,595 [Epoch: 1036 Step: 00017600] Batch Recognition Loss:   0.000170 => Gls Tokens per Sec:      465 || Batch Translation Loss:   0.011964 => Txt Tokens per Sec:     1425 || Lr: 0.000100
2024-02-04 13:10:16,073 Epoch 1036: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-04 13:10:16,073 EPOCH 1037
2024-02-04 13:10:26,738 Epoch 1037: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-04 13:10:26,739 EPOCH 1038
2024-02-04 13:10:37,568 Epoch 1038: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-04 13:10:37,569 EPOCH 1039
2024-02-04 13:10:48,472 Epoch 1039: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-04 13:10:48,472 EPOCH 1040
2024-02-04 13:10:59,151 Epoch 1040: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-04 13:10:59,152 EPOCH 1041
2024-02-04 13:11:09,941 Epoch 1041: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-04 13:11:09,942 EPOCH 1042
2024-02-04 13:11:12,302 [Epoch: 1042 Step: 00017700] Batch Recognition Loss:   0.000157 => Gls Tokens per Sec:      813 || Batch Translation Loss:   0.019777 => Txt Tokens per Sec:     2369 || Lr: 0.000100
2024-02-04 13:11:20,937 Epoch 1042: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-04 13:11:20,937 EPOCH 1043
2024-02-04 13:11:31,487 Epoch 1043: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-04 13:11:31,487 EPOCH 1044
2024-02-04 13:11:42,302 Epoch 1044: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-04 13:11:42,302 EPOCH 1045
2024-02-04 13:11:52,981 Epoch 1045: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-04 13:11:52,981 EPOCH 1046
2024-02-04 13:12:03,583 Epoch 1046: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-04 13:12:03,584 EPOCH 1047
2024-02-04 13:12:14,061 Epoch 1047: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-04 13:12:14,062 EPOCH 1048
2024-02-04 13:12:15,770 [Epoch: 1048 Step: 00017800] Batch Recognition Loss:   0.000183 => Gls Tokens per Sec:      375 || Batch Translation Loss:   0.019729 => Txt Tokens per Sec:     1327 || Lr: 0.000100
2024-02-04 13:12:24,954 Epoch 1048: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-04 13:12:24,954 EPOCH 1049
2024-02-04 13:12:35,776 Epoch 1049: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-04 13:12:35,777 EPOCH 1050
2024-02-04 13:12:46,304 Epoch 1050: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-04 13:12:46,305 EPOCH 1051
2024-02-04 13:12:57,093 Epoch 1051: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-04 13:12:57,093 EPOCH 1052
2024-02-04 13:13:08,103 Epoch 1052: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-04 13:13:08,104 EPOCH 1053
2024-02-04 13:13:18,748 [Epoch: 1053 Step: 00017900] Batch Recognition Loss:   0.000121 => Gls Tokens per Sec:      939 || Batch Translation Loss:   0.015697 => Txt Tokens per Sec:     2639 || Lr: 0.000100
2024-02-04 13:13:18,898 Epoch 1053: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-04 13:13:18,898 EPOCH 1054
2024-02-04 13:13:29,806 Epoch 1054: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-04 13:13:29,807 EPOCH 1055
2024-02-04 13:13:40,634 Epoch 1055: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-04 13:13:40,635 EPOCH 1056
2024-02-04 13:13:51,202 Epoch 1056: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-04 13:13:51,203 EPOCH 1057
2024-02-04 13:14:02,064 Epoch 1057: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-04 13:14:02,065 EPOCH 1058
2024-02-04 13:14:12,737 Epoch 1058: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-04 13:14:12,738 EPOCH 1059
2024-02-04 13:14:23,029 [Epoch: 1059 Step: 00018000] Batch Recognition Loss:   0.000126 => Gls Tokens per Sec:      846 || Batch Translation Loss:   0.031489 => Txt Tokens per Sec:     2417 || Lr: 0.000100
2024-02-04 13:15:01,190 Validation result at epoch 1059, step    18000: duration: 38.1605s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00020	Translation Loss: 93988.82812	PPL: 12154.64941
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.83	(BLEU-1: 11.47,	BLEU-2: 3.49,	BLEU-3: 1.55,	BLEU-4: 0.83)
	CHRF 17.33	ROUGE 9.79
2024-02-04 13:15:01,193 Logging Recognition and Translation Outputs
2024-02-04 13:15:01,193 ========================================================================================================================
2024-02-04 13:15:01,193 Logging Sequence: 155_119.00
2024-02-04 13:15:01,193 	Gloss Reference :	A B+C+D+E
2024-02-04 13:15:01,193 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 13:15:01,193 	Gloss Alignment :	         
2024-02-04 13:15:01,195 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 13:15:01,197 	Text Reference  :	a    report said   that the **** taliban wanted icc     to   replace the ***** ******* *** ***** afghan flag    with its own 
2024-02-04 13:15:01,197 	Text Hypothesis :	they would  decide if   the bcci to      play   against they lost    the match however icc would 8      wickets in   the game
2024-02-04 13:15:01,197 	Text Alignment  :	S    S      S      S        I    S       S      S       S    S           I     I       I   I     S      S       S    S   S   
2024-02-04 13:15:01,197 ========================================================================================================================
2024-02-04 13:15:01,197 Logging Sequence: 153_43.00
2024-02-04 13:15:01,197 	Gloss Reference :	A B+C+D+E
2024-02-04 13:15:01,197 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 13:15:01,198 	Gloss Alignment :	         
2024-02-04 13:15:01,198 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 13:15:01,199 	Text Reference  :	these   runs were all because of      hardik pandya and    virat  kohli
2024-02-04 13:15:01,199 	Text Hypothesis :	however they lost the match   leaving on     the    caught indian team 
2024-02-04 13:15:01,199 	Text Alignment  :	S       S    S    S   S       S       S      S      S      S      S    
2024-02-04 13:15:01,199 ========================================================================================================================
2024-02-04 13:15:01,199 Logging Sequence: 150_35.00
2024-02-04 13:15:01,199 	Gloss Reference :	A B+C+D+E
2024-02-04 13:15:01,200 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 13:15:01,200 	Gloss Alignment :	         
2024-02-04 13:15:01,200 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 13:15:01,201 	Text Reference  :	********* ** * *** ** ****** wow    india   football team   is   really strong
2024-02-04 13:15:01,201 	Text Hypothesis :	according to a lot of hardik pandya because he       looked like a      lot   
2024-02-04 13:15:01,201 	Text Alignment  :	I         I  I I   I  I      S      S       S        S      S    S      S     
2024-02-04 13:15:01,201 ========================================================================================================================
2024-02-04 13:15:01,201 Logging Sequence: 146_154.00
2024-02-04 13:15:01,201 	Gloss Reference :	A B+C+D+E
2024-02-04 13:15:01,201 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 13:15:01,202 	Gloss Alignment :	         
2024-02-04 13:15:01,202 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 13:15:01,203 	Text Reference  :	bwf said that testing protocols have been implemented to       ensure the      health  and safety of     all participants
2024-02-04 13:15:01,203 	Text Hypothesis :	*** **** **** ******* ********* **** and  was         consoled by     denmark' captain and ****** scored 95  runs        
2024-02-04 13:15:01,203 	Text Alignment  :	D   D    D    D       D         D    S    S           S        S      S        S           D      S      S   S           
2024-02-04 13:15:01,203 ========================================================================================================================
2024-02-04 13:15:01,203 Logging Sequence: 76_79.00
2024-02-04 13:15:01,203 	Gloss Reference :	A B+C+D+E
2024-02-04 13:15:01,204 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 13:15:01,204 	Gloss Alignment :	         
2024-02-04 13:15:01,204 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 13:15:01,204 	Text Reference  :	*** speaking to       ani csk       ceo   kasi viswanathan said
2024-02-04 13:15:01,205 	Text Hypothesis :	the t20      worldcup is  currently going in   dubai       oman
2024-02-04 13:15:01,205 	Text Alignment  :	I   S        S        S   S         S     S    S           S   
2024-02-04 13:15:01,205 ========================================================================================================================
2024-02-04 13:15:01,856 Epoch 1059: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-04 13:15:01,856 EPOCH 1060
2024-02-04 13:15:13,325 Epoch 1060: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.65 
2024-02-04 13:15:13,325 EPOCH 1061
2024-02-04 13:15:23,991 Epoch 1061: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.06 
2024-02-04 13:15:23,991 EPOCH 1062
2024-02-04 13:15:34,759 Epoch 1062: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.52 
2024-02-04 13:15:34,760 EPOCH 1063
2024-02-04 13:15:45,356 Epoch 1063: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.58 
2024-02-04 13:15:45,357 EPOCH 1064
2024-02-04 13:15:56,192 Epoch 1064: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.48 
2024-02-04 13:15:56,192 EPOCH 1065
2024-02-04 13:16:04,548 [Epoch: 1065 Step: 00018100] Batch Recognition Loss:   0.000775 => Gls Tokens per Sec:      889 || Batch Translation Loss:   0.195314 => Txt Tokens per Sec:     2530 || Lr: 0.000100
2024-02-04 13:16:06,957 Epoch 1065: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.80 
2024-02-04 13:16:06,958 EPOCH 1066
2024-02-04 13:16:17,648 Epoch 1066: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.35 
2024-02-04 13:16:17,648 EPOCH 1067
2024-02-04 13:16:28,504 Epoch 1067: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.38 
2024-02-04 13:16:28,505 EPOCH 1068
2024-02-04 13:16:39,248 Epoch 1068: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.40 
2024-02-04 13:16:39,248 EPOCH 1069
2024-02-04 13:16:50,221 Epoch 1069: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.12 
2024-02-04 13:16:50,222 EPOCH 1070
2024-02-04 13:17:00,803 Epoch 1070: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.19 
2024-02-04 13:17:00,803 EPOCH 1071
2024-02-04 13:17:08,870 [Epoch: 1071 Step: 00018200] Batch Recognition Loss:   0.000297 => Gls Tokens per Sec:      763 || Batch Translation Loss:   0.061755 => Txt Tokens per Sec:     2246 || Lr: 0.000100
2024-02-04 13:17:11,460 Epoch 1071: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.95 
2024-02-04 13:17:11,461 EPOCH 1072
2024-02-04 13:17:22,021 Epoch 1072: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.89 
2024-02-04 13:17:22,022 EPOCH 1073
2024-02-04 13:17:32,625 Epoch 1073: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.73 
2024-02-04 13:17:32,626 EPOCH 1074
2024-02-04 13:17:43,387 Epoch 1074: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.67 
2024-02-04 13:17:43,388 EPOCH 1075
2024-02-04 13:17:54,187 Epoch 1075: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-04 13:17:54,187 EPOCH 1076
2024-02-04 13:18:04,848 Epoch 1076: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.56 
2024-02-04 13:18:04,849 EPOCH 1077
2024-02-04 13:18:06,436 [Epoch: 1077 Step: 00018300] Batch Recognition Loss:   0.000252 => Gls Tokens per Sec:     3228 || Batch Translation Loss:   0.029483 => Txt Tokens per Sec:     7936 || Lr: 0.000100
2024-02-04 13:18:15,457 Epoch 1077: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.96 
2024-02-04 13:18:15,457 EPOCH 1078
2024-02-04 13:18:26,215 Epoch 1078: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.35 
2024-02-04 13:18:26,216 EPOCH 1079
2024-02-04 13:18:36,818 Epoch 1079: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.82 
2024-02-04 13:18:36,819 EPOCH 1080
2024-02-04 13:18:47,553 Epoch 1080: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.00 
2024-02-04 13:18:47,553 EPOCH 1081
2024-02-04 13:18:58,664 Epoch 1081: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.78 
2024-02-04 13:18:58,665 EPOCH 1082
2024-02-04 13:19:09,508 Epoch 1082: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.61 
2024-02-04 13:19:09,508 EPOCH 1083
2024-02-04 13:19:12,551 [Epoch: 1083 Step: 00018400] Batch Recognition Loss:   0.000166 => Gls Tokens per Sec:     1263 || Batch Translation Loss:   0.016983 => Txt Tokens per Sec:     3633 || Lr: 0.000100
2024-02-04 13:19:20,270 Epoch 1083: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.64 
2024-02-04 13:19:20,270 EPOCH 1084
2024-02-04 13:19:31,190 Epoch 1084: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.56 
2024-02-04 13:19:31,191 EPOCH 1085
2024-02-04 13:19:41,988 Epoch 1085: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-04 13:19:41,988 EPOCH 1086
2024-02-04 13:19:52,953 Epoch 1086: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-04 13:19:52,954 EPOCH 1087
2024-02-04 13:20:03,783 Epoch 1087: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-04 13:20:03,784 EPOCH 1088
2024-02-04 13:20:14,426 Epoch 1088: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-04 13:20:14,426 EPOCH 1089
2024-02-04 13:20:17,776 [Epoch: 1089 Step: 00018500] Batch Recognition Loss:   0.000200 => Gls Tokens per Sec:      690 || Batch Translation Loss:   0.015661 => Txt Tokens per Sec:     1767 || Lr: 0.000100
2024-02-04 13:20:24,971 Epoch 1089: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.63 
2024-02-04 13:20:24,972 EPOCH 1090
2024-02-04 13:20:35,820 Epoch 1090: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-04 13:20:35,820 EPOCH 1091
2024-02-04 13:20:46,422 Epoch 1091: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-04 13:20:46,422 EPOCH 1092
2024-02-04 13:20:57,278 Epoch 1092: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-04 13:20:57,279 EPOCH 1093
2024-02-04 13:21:07,954 Epoch 1093: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-04 13:21:07,954 EPOCH 1094
2024-02-04 13:21:18,670 Epoch 1094: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-04 13:21:18,670 EPOCH 1095
2024-02-04 13:21:19,064 [Epoch: 1095 Step: 00018600] Batch Recognition Loss:   0.000105 => Gls Tokens per Sec:     3251 || Batch Translation Loss:   0.015408 => Txt Tokens per Sec:     8941 || Lr: 0.000100
2024-02-04 13:21:29,180 Epoch 1095: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-04 13:21:29,181 EPOCH 1096
2024-02-04 13:21:39,832 Epoch 1096: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-04 13:21:39,833 EPOCH 1097
2024-02-04 13:21:50,803 Epoch 1097: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-04 13:21:50,804 EPOCH 1098
2024-02-04 13:22:01,488 Epoch 1098: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-04 13:22:01,488 EPOCH 1099
2024-02-04 13:22:12,473 Epoch 1099: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-04 13:22:12,474 EPOCH 1100
2024-02-04 13:22:22,906 [Epoch: 1100 Step: 00018700] Batch Recognition Loss:   0.000139 => Gls Tokens per Sec:     1019 || Batch Translation Loss:   0.017866 => Txt Tokens per Sec:     2829 || Lr: 0.000100
2024-02-04 13:22:22,906 Epoch 1100: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-04 13:22:22,907 EPOCH 1101
2024-02-04 13:22:33,578 Epoch 1101: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-04 13:22:33,579 EPOCH 1102
2024-02-04 13:22:44,224 Epoch 1102: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-04 13:22:44,224 EPOCH 1103
2024-02-04 13:22:54,886 Epoch 1103: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-04 13:22:54,887 EPOCH 1104
2024-02-04 13:23:05,736 Epoch 1104: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-04 13:23:05,737 EPOCH 1105
2024-02-04 13:23:16,548 Epoch 1105: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-04 13:23:16,548 EPOCH 1106
2024-02-04 13:23:24,239 [Epoch: 1106 Step: 00018800] Batch Recognition Loss:   0.000130 => Gls Tokens per Sec:     1248 || Batch Translation Loss:   0.009128 => Txt Tokens per Sec:     3459 || Lr: 0.000100
2024-02-04 13:23:27,140 Epoch 1106: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-04 13:23:27,141 EPOCH 1107
2024-02-04 13:23:37,843 Epoch 1107: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-04 13:23:37,843 EPOCH 1108
2024-02-04 13:23:48,501 Epoch 1108: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-04 13:23:48,501 EPOCH 1109
2024-02-04 13:23:59,196 Epoch 1109: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-04 13:23:59,197 EPOCH 1110
2024-02-04 13:24:10,008 Epoch 1110: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.55 
2024-02-04 13:24:10,008 EPOCH 1111
2024-02-04 13:24:20,663 Epoch 1111: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.67 
2024-02-04 13:24:20,664 EPOCH 1112
2024-02-04 13:24:26,357 [Epoch: 1112 Step: 00018900] Batch Recognition Loss:   0.000174 => Gls Tokens per Sec:     1462 || Batch Translation Loss:   0.044264 => Txt Tokens per Sec:     3932 || Lr: 0.000100
2024-02-04 13:24:31,220 Epoch 1112: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.89 
2024-02-04 13:24:31,220 EPOCH 1113
2024-02-04 13:24:41,887 Epoch 1113: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.05 
2024-02-04 13:24:41,888 EPOCH 1114
2024-02-04 13:24:52,659 Epoch 1114: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.31 
2024-02-04 13:24:52,660 EPOCH 1115
2024-02-04 13:25:03,070 Epoch 1115: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.34 
2024-02-04 13:25:03,070 EPOCH 1116
2024-02-04 13:25:14,032 Epoch 1116: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.57 
2024-02-04 13:25:14,032 EPOCH 1117
2024-02-04 13:25:24,686 Epoch 1117: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.65 
2024-02-04 13:25:24,687 EPOCH 1118
2024-02-04 13:25:32,609 [Epoch: 1118 Step: 00019000] Batch Recognition Loss:   0.000369 => Gls Tokens per Sec:      857 || Batch Translation Loss:   0.095604 => Txt Tokens per Sec:     2313 || Lr: 0.000100
2024-02-04 13:25:35,633 Epoch 1118: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.24 
2024-02-04 13:25:35,634 EPOCH 1119
2024-02-04 13:25:46,390 Epoch 1119: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.31 
2024-02-04 13:25:46,391 EPOCH 1120
2024-02-04 13:25:57,165 Epoch 1120: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.93 
2024-02-04 13:25:57,165 EPOCH 1121
2024-02-04 13:26:07,460 Epoch 1121: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.51 
2024-02-04 13:26:07,461 EPOCH 1122
2024-02-04 13:26:18,370 Epoch 1122: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.07 
2024-02-04 13:26:18,371 EPOCH 1123
2024-02-04 13:26:29,102 Epoch 1123: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.94 
2024-02-04 13:26:29,103 EPOCH 1124
2024-02-04 13:26:32,468 [Epoch: 1124 Step: 00019100] Batch Recognition Loss:   0.000185 => Gls Tokens per Sec:     1712 || Batch Translation Loss:   0.050309 => Txt Tokens per Sec:     4575 || Lr: 0.000100
2024-02-04 13:26:39,644 Epoch 1124: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.82 
2024-02-04 13:26:39,645 EPOCH 1125
2024-02-04 13:26:50,519 Epoch 1125: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.91 
2024-02-04 13:26:50,520 EPOCH 1126
2024-02-04 13:27:01,240 Epoch 1126: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.73 
2024-02-04 13:27:01,240 EPOCH 1127
2024-02-04 13:27:11,867 Epoch 1127: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.75 
2024-02-04 13:27:11,868 EPOCH 1128
2024-02-04 13:27:22,903 Epoch 1128: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.85 
2024-02-04 13:27:22,904 EPOCH 1129
2024-02-04 13:27:33,802 Epoch 1129: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.78 
2024-02-04 13:27:33,803 EPOCH 1130
2024-02-04 13:27:36,616 [Epoch: 1130 Step: 00019200] Batch Recognition Loss:   0.000196 => Gls Tokens per Sec:     1593 || Batch Translation Loss:   0.012674 => Txt Tokens per Sec:     4161 || Lr: 0.000100
2024-02-04 13:27:44,466 Epoch 1130: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.10 
2024-02-04 13:27:44,467 EPOCH 1131
2024-02-04 13:27:55,328 Epoch 1131: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.61 
2024-02-04 13:27:55,329 EPOCH 1132
2024-02-04 13:28:05,901 Epoch 1132: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.73 
2024-02-04 13:28:05,902 EPOCH 1133
2024-02-04 13:28:16,687 Epoch 1133: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.55 
2024-02-04 13:28:16,688 EPOCH 1134
2024-02-04 13:28:27,280 Epoch 1134: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-04 13:28:27,280 EPOCH 1135
2024-02-04 13:28:37,726 Epoch 1135: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-04 13:28:37,726 EPOCH 1136
2024-02-04 13:28:41,984 [Epoch: 1136 Step: 00019300] Batch Recognition Loss:   0.000204 => Gls Tokens per Sec:      752 || Batch Translation Loss:   0.025726 => Txt Tokens per Sec:     2197 || Lr: 0.000100
2024-02-04 13:28:48,509 Epoch 1136: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-04 13:28:48,510 EPOCH 1137
2024-02-04 13:28:59,402 Epoch 1137: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-04 13:28:59,403 EPOCH 1138
2024-02-04 13:29:09,980 Epoch 1138: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-04 13:29:09,981 EPOCH 1139
2024-02-04 13:29:20,822 Epoch 1139: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-04 13:29:20,823 EPOCH 1140
2024-02-04 13:29:31,341 Epoch 1140: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-04 13:29:31,341 EPOCH 1141
2024-02-04 13:29:41,885 Epoch 1141: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-04 13:29:41,886 EPOCH 1142
2024-02-04 13:29:42,433 [Epoch: 1142 Step: 00019400] Batch Recognition Loss:   0.000140 => Gls Tokens per Sec:     3523 || Batch Translation Loss:   0.017871 => Txt Tokens per Sec:     8763 || Lr: 0.000100
2024-02-04 13:29:52,648 Epoch 1142: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-04 13:29:52,649 EPOCH 1143
2024-02-04 13:30:03,510 Epoch 1143: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-04 13:30:03,510 EPOCH 1144
2024-02-04 13:30:14,008 Epoch 1144: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-04 13:30:14,009 EPOCH 1145
2024-02-04 13:30:24,721 Epoch 1145: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-04 13:30:24,722 EPOCH 1146
2024-02-04 13:30:35,722 Epoch 1146: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.60 
2024-02-04 13:30:35,722 EPOCH 1147
2024-02-04 13:30:46,697 Epoch 1147: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-04 13:30:46,697 EPOCH 1148
2024-02-04 13:30:46,847 [Epoch: 1148 Step: 00019500] Batch Recognition Loss:   0.000124 => Gls Tokens per Sec:     4324 || Batch Translation Loss:   0.015637 => Txt Tokens per Sec:    11115 || Lr: 0.000100
2024-02-04 13:30:57,273 Epoch 1148: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.64 
2024-02-04 13:30:57,274 EPOCH 1149
2024-02-04 13:31:08,071 Epoch 1149: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.76 
2024-02-04 13:31:08,071 EPOCH 1150
2024-02-04 13:31:18,593 Epoch 1150: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.80 
2024-02-04 13:31:18,594 EPOCH 1151
2024-02-04 13:31:29,151 Epoch 1151: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-04 13:31:29,152 EPOCH 1152
2024-02-04 13:31:39,764 Epoch 1152: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.57 
2024-02-04 13:31:39,765 EPOCH 1153
2024-02-04 13:31:50,559 [Epoch: 1153 Step: 00019600] Batch Recognition Loss:   0.000215 => Gls Tokens per Sec:      926 || Batch Translation Loss:   0.084457 => Txt Tokens per Sec:     2643 || Lr: 0.000100
2024-02-04 13:31:50,670 Epoch 1153: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.77 
2024-02-04 13:31:50,671 EPOCH 1154
2024-02-04 13:32:01,573 Epoch 1154: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.23 
2024-02-04 13:32:01,573 EPOCH 1155
2024-02-04 13:32:12,225 Epoch 1155: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.44 
2024-02-04 13:32:12,226 EPOCH 1156
2024-02-04 13:32:23,118 Epoch 1156: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.11 
2024-02-04 13:32:23,118 EPOCH 1157
2024-02-04 13:32:34,082 Epoch 1157: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.93 
2024-02-04 13:32:34,083 EPOCH 1158
2024-02-04 13:32:44,916 Epoch 1158: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.01 
2024-02-04 13:32:44,916 EPOCH 1159
2024-02-04 13:32:53,647 [Epoch: 1159 Step: 00019700] Batch Recognition Loss:   0.000254 => Gls Tokens per Sec:      998 || Batch Translation Loss:   0.120590 => Txt Tokens per Sec:     2788 || Lr: 0.000100
2024-02-04 13:32:55,674 Epoch 1159: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.30 
2024-02-04 13:32:55,674 EPOCH 1160
2024-02-04 13:33:06,330 Epoch 1160: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.28 
2024-02-04 13:33:06,331 EPOCH 1161
2024-02-04 13:33:16,858 Epoch 1161: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.36 
2024-02-04 13:33:16,858 EPOCH 1162
2024-02-04 13:33:27,633 Epoch 1162: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.31 
2024-02-04 13:33:27,633 EPOCH 1163
2024-02-04 13:33:38,606 Epoch 1163: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.05 
2024-02-04 13:33:38,607 EPOCH 1164
2024-02-04 13:33:49,301 Epoch 1164: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.77 
2024-02-04 13:33:49,302 EPOCH 1165
2024-02-04 13:33:59,055 [Epoch: 1165 Step: 00019800] Batch Recognition Loss:   0.000216 => Gls Tokens per Sec:      762 || Batch Translation Loss:   0.034888 => Txt Tokens per Sec:     2219 || Lr: 0.000100
2024-02-04 13:33:59,988 Epoch 1165: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.68 
2024-02-04 13:33:59,989 EPOCH 1166
2024-02-04 13:34:10,613 Epoch 1166: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.76 
2024-02-04 13:34:10,613 EPOCH 1167
2024-02-04 13:34:21,319 Epoch 1167: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.03 
2024-02-04 13:34:21,320 EPOCH 1168
2024-02-04 13:34:31,833 Epoch 1168: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.99 
2024-02-04 13:34:31,833 EPOCH 1169
2024-02-04 13:34:42,619 Epoch 1169: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.05 
2024-02-04 13:34:42,619 EPOCH 1170
2024-02-04 13:34:53,549 Epoch 1170: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.19 
2024-02-04 13:34:53,550 EPOCH 1171
2024-02-04 13:35:00,215 [Epoch: 1171 Step: 00019900] Batch Recognition Loss:   0.000165 => Gls Tokens per Sec:      960 || Batch Translation Loss:   0.036719 => Txt Tokens per Sec:     2675 || Lr: 0.000100
2024-02-04 13:35:04,344 Epoch 1171: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.70 
2024-02-04 13:35:04,345 EPOCH 1172
2024-02-04 13:35:15,239 Epoch 1172: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.71 
2024-02-04 13:35:15,240 EPOCH 1173
2024-02-04 13:35:25,321 Epoch 1173: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.59 
2024-02-04 13:35:25,322 EPOCH 1174
2024-02-04 13:35:36,000 Epoch 1174: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-04 13:35:36,000 EPOCH 1175
2024-02-04 13:35:46,908 Epoch 1175: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.60 
2024-02-04 13:35:46,908 EPOCH 1176
2024-02-04 13:35:57,570 Epoch 1176: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.57 
2024-02-04 13:35:57,570 EPOCH 1177
2024-02-04 13:36:02,136 [Epoch: 1177 Step: 00020000] Batch Recognition Loss:   0.000215 => Gls Tokens per Sec:     1122 || Batch Translation Loss:   0.099952 => Txt Tokens per Sec:     2885 || Lr: 0.000100
2024-02-04 13:36:40,840 Validation result at epoch 1177, step    20000: duration: 38.7016s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00036	Translation Loss: 94831.17188	PPL: 13223.62695
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.64	(BLEU-1: 11.62,	BLEU-2: 3.51,	BLEU-3: 1.33,	BLEU-4: 0.64)
	CHRF 17.22	ROUGE 9.72
2024-02-04 13:36:40,842 Logging Recognition and Translation Outputs
2024-02-04 13:36:40,842 ========================================================================================================================
2024-02-04 13:36:40,842 Logging Sequence: 174_121.00
2024-02-04 13:36:40,843 	Gloss Reference :	A B+C+D+E
2024-02-04 13:36:40,843 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 13:36:40,843 	Gloss Alignment :	         
2024-02-04 13:36:40,843 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 13:36:40,845 	Text Reference  :	*** ****** *** ******** * **** *** ********* ** ****** there was  a  strong   competition and   a     difficult auction for the    5     franchise owners
2024-02-04 13:36:40,846 	Text Hypothesis :	the couple are enjoying a huge fan following in sports there were an argument between     their world cup       match   and forget about rs        250   
2024-02-04 13:36:40,846 	Text Alignment  :	I   I      I   I        I I    I   I         I  I            S    S  S        S           S     S     S         S       S   S      S     S         S     
2024-02-04 13:36:40,846 ========================================================================================================================
2024-02-04 13:36:40,846 Logging Sequence: 170_24.00
2024-02-04 13:36:40,846 	Gloss Reference :	A B+C+D+E
2024-02-04 13:36:40,846 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 13:36:40,847 	Gloss Alignment :	         
2024-02-04 13:36:40,847 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 13:36:40,847 	Text Reference  :	let me tell you about it
2024-02-04 13:36:40,847 	Text Hypothesis :	let me tell you about it
2024-02-04 13:36:40,847 	Text Alignment  :	                        
2024-02-04 13:36:40,847 ========================================================================================================================
2024-02-04 13:36:40,848 Logging Sequence: 73_79.00
2024-02-04 13:36:40,848 	Gloss Reference :	A B+C+D+E
2024-02-04 13:36:40,848 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 13:36:40,848 	Gloss Alignment :	         
2024-02-04 13:36:40,848 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 13:36:40,849 	Text Reference  :	raina resturant has food from   the rich spices of north india      to the aromatic curries of south     india
2024-02-04 13:36:40,849 	Text Hypothesis :	when  it        has **** opened the **** ****** ** ***** restaurant in the ******** ******* ** beautiful city 
2024-02-04 13:36:40,850 	Text Alignment  :	S     S             D    S          D    D      D  D     S          S      D        D       D  S         S    
2024-02-04 13:36:40,850 ========================================================================================================================
2024-02-04 13:36:40,850 Logging Sequence: 140_2.00
2024-02-04 13:36:40,850 	Gloss Reference :	A B+C+D+E
2024-02-04 13:36:40,850 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 13:36:40,850 	Gloss Alignment :	         
2024-02-04 13:36:40,850 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 13:36:40,851 	Text Reference  :	indian batsman-wicket keeper rishabh pant has    outstanding skills in     cricket
2024-02-04 13:36:40,851 	Text Hypothesis :	****** ************** for    india   had  become viral       on     social media  
2024-02-04 13:36:40,851 	Text Alignment  :	D      D              S      S       S    S      S           S      S      S      
2024-02-04 13:36:40,851 ========================================================================================================================
2024-02-04 13:36:40,852 Logging Sequence: 81_470.00
2024-02-04 13:36:40,852 	Gloss Reference :	A B+C+D+E
2024-02-04 13:36:40,852 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 13:36:40,852 	Gloss Alignment :	         
2024-02-04 13:36:40,852 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 13:36:40,853 	Text Reference  :	********** or you don't  know if   you    do let  us    know      in the comments
2024-02-04 13:36:40,853 	Text Hypothesis :	arbitrator is a   person who  will listen to each other countries in the team    
2024-02-04 13:36:40,853 	Text Alignment  :	I          S  S   S      S    S    S      S  S    S     S                S       
2024-02-04 13:36:40,854 ========================================================================================================================
2024-02-04 13:36:47,469 Epoch 1177: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.69 
2024-02-04 13:36:47,469 EPOCH 1178
2024-02-04 13:36:58,406 Epoch 1178: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.72 
2024-02-04 13:36:58,406 EPOCH 1179
2024-02-04 13:37:09,004 Epoch 1179: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.87 
2024-02-04 13:37:09,004 EPOCH 1180
2024-02-04 13:37:19,795 Epoch 1180: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.92 
2024-02-04 13:37:19,796 EPOCH 1181
2024-02-04 13:37:30,469 Epoch 1181: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.10 
2024-02-04 13:37:30,470 EPOCH 1182
2024-02-04 13:37:41,295 Epoch 1182: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.38 
2024-02-04 13:37:41,295 EPOCH 1183
2024-02-04 13:37:47,324 [Epoch: 1183 Step: 00020100] Batch Recognition Loss:   0.000292 => Gls Tokens per Sec:      637 || Batch Translation Loss:   0.045048 => Txt Tokens per Sec:     2009 || Lr: 0.000100
2024-02-04 13:37:52,209 Epoch 1183: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.79 
2024-02-04 13:37:52,210 EPOCH 1184
2024-02-04 13:38:02,736 Epoch 1184: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.77 
2024-02-04 13:38:02,737 EPOCH 1185
2024-02-04 13:38:13,462 Epoch 1185: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.94 
2024-02-04 13:38:13,462 EPOCH 1186
2024-02-04 13:38:24,297 Epoch 1186: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.63 
2024-02-04 13:38:24,298 EPOCH 1187
2024-02-04 13:38:34,855 Epoch 1187: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-04 13:38:34,856 EPOCH 1188
2024-02-04 13:38:45,496 Epoch 1188: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-04 13:38:45,496 EPOCH 1189
2024-02-04 13:38:48,877 [Epoch: 1189 Step: 00020200] Batch Recognition Loss:   0.000168 => Gls Tokens per Sec:      684 || Batch Translation Loss:   0.017471 => Txt Tokens per Sec:     1812 || Lr: 0.000100
2024-02-04 13:38:56,507 Epoch 1189: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-04 13:38:56,507 EPOCH 1190
2024-02-04 13:39:07,405 Epoch 1190: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-04 13:39:07,406 EPOCH 1191
2024-02-04 13:39:18,198 Epoch 1191: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-04 13:39:18,198 EPOCH 1192
2024-02-04 13:39:28,675 Epoch 1192: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-04 13:39:28,675 EPOCH 1193
2024-02-04 13:39:39,434 Epoch 1193: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-04 13:39:39,435 EPOCH 1194
2024-02-04 13:39:50,248 Epoch 1194: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-04 13:39:50,248 EPOCH 1195
2024-02-04 13:39:52,412 [Epoch: 1195 Step: 00020300] Batch Recognition Loss:   0.000206 => Gls Tokens per Sec:      592 || Batch Translation Loss:   0.028070 => Txt Tokens per Sec:     1744 || Lr: 0.000100
2024-02-04 13:40:01,086 Epoch 1195: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-04 13:40:01,087 EPOCH 1196
2024-02-04 13:40:12,117 Epoch 1196: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-04 13:40:12,118 EPOCH 1197
2024-02-04 13:40:22,794 Epoch 1197: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-04 13:40:22,795 EPOCH 1198
2024-02-04 13:40:33,588 Epoch 1198: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-04 13:40:33,589 EPOCH 1199
2024-02-04 13:40:44,361 Epoch 1199: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-04 13:40:44,362 EPOCH 1200
2024-02-04 13:40:54,947 [Epoch: 1200 Step: 00020400] Batch Recognition Loss:   0.000160 => Gls Tokens per Sec:     1004 || Batch Translation Loss:   0.021654 => Txt Tokens per Sec:     2788 || Lr: 0.000100
2024-02-04 13:40:54,947 Epoch 1200: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-04 13:40:54,948 EPOCH 1201
2024-02-04 13:41:05,607 Epoch 1201: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.56 
2024-02-04 13:41:05,608 EPOCH 1202
2024-02-04 13:41:16,230 Epoch 1202: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.57 
2024-02-04 13:41:16,230 EPOCH 1203
2024-02-04 13:41:26,759 Epoch 1203: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-04 13:41:26,760 EPOCH 1204
2024-02-04 13:41:37,380 Epoch 1204: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-04 13:41:37,381 EPOCH 1205
2024-02-04 13:41:48,158 Epoch 1205: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.30 
2024-02-04 13:41:48,159 EPOCH 1206
2024-02-04 13:41:58,392 [Epoch: 1206 Step: 00020500] Batch Recognition Loss:   0.000221 => Gls Tokens per Sec:      914 || Batch Translation Loss:   0.020837 => Txt Tokens per Sec:     2554 || Lr: 0.000100
2024-02-04 13:41:58,862 Epoch 1206: Total Training Recognition Loss 0.00  Total Training Translation Loss 3.02 
2024-02-04 13:41:58,862 EPOCH 1207
2024-02-04 13:42:09,338 Epoch 1207: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.03 
2024-02-04 13:42:09,339 EPOCH 1208
2024-02-04 13:42:20,070 Epoch 1208: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.40 
2024-02-04 13:42:20,071 EPOCH 1209
2024-02-04 13:42:30,746 Epoch 1209: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.57 
2024-02-04 13:42:30,746 EPOCH 1210
2024-02-04 13:42:41,347 Epoch 1210: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.53 
2024-02-04 13:42:41,347 EPOCH 1211
2024-02-04 13:42:51,959 Epoch 1211: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.36 
2024-02-04 13:42:51,959 EPOCH 1212
2024-02-04 13:43:00,054 [Epoch: 1212 Step: 00020600] Batch Recognition Loss:   0.000970 => Gls Tokens per Sec:      997 || Batch Translation Loss:   0.082412 => Txt Tokens per Sec:     2776 || Lr: 0.000100
2024-02-04 13:43:02,631 Epoch 1212: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.90 
2024-02-04 13:43:02,631 EPOCH 1213
2024-02-04 13:43:13,767 Epoch 1213: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.00 
2024-02-04 13:43:13,768 EPOCH 1214
2024-02-04 13:43:24,672 Epoch 1214: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-04 13:43:24,672 EPOCH 1215
2024-02-04 13:43:35,758 Epoch 1215: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-04 13:43:35,759 EPOCH 1216
2024-02-04 13:43:46,505 Epoch 1216: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-04 13:43:46,505 EPOCH 1217
2024-02-04 13:43:57,172 Epoch 1217: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-04 13:43:57,172 EPOCH 1218
2024-02-04 13:44:03,366 [Epoch: 1218 Step: 00020700] Batch Recognition Loss:   0.000327 => Gls Tokens per Sec:     1097 || Batch Translation Loss:   0.029728 => Txt Tokens per Sec:     2891 || Lr: 0.000100
2024-02-04 13:44:07,954 Epoch 1218: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-04 13:44:07,954 EPOCH 1219
2024-02-04 13:44:18,534 Epoch 1219: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-04 13:44:18,535 EPOCH 1220
2024-02-04 13:44:29,167 Epoch 1220: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-04 13:44:29,167 EPOCH 1221
2024-02-04 13:44:39,891 Epoch 1221: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-04 13:44:39,892 EPOCH 1222
2024-02-04 13:44:50,650 Epoch 1222: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-04 13:44:50,651 EPOCH 1223
2024-02-04 13:45:01,684 Epoch 1223: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-04 13:45:01,685 EPOCH 1224
2024-02-04 13:45:05,540 [Epoch: 1224 Step: 00020800] Batch Recognition Loss:   0.000185 => Gls Tokens per Sec:     1495 || Batch Translation Loss:   0.016819 => Txt Tokens per Sec:     3919 || Lr: 0.000100
2024-02-04 13:45:12,488 Epoch 1224: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-04 13:45:12,489 EPOCH 1225
2024-02-04 13:45:23,444 Epoch 1225: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-04 13:45:23,445 EPOCH 1226
2024-02-04 13:45:34,158 Epoch 1226: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-04 13:45:34,158 EPOCH 1227
2024-02-04 13:45:44,792 Epoch 1227: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-04 13:45:44,793 EPOCH 1228
2024-02-04 13:45:55,390 Epoch 1228: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-04 13:45:55,390 EPOCH 1229
2024-02-04 13:46:06,156 Epoch 1229: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-04 13:46:06,156 EPOCH 1230
2024-02-04 13:46:09,131 [Epoch: 1230 Step: 00020900] Batch Recognition Loss:   0.000153 => Gls Tokens per Sec:     1506 || Batch Translation Loss:   0.013931 => Txt Tokens per Sec:     4179 || Lr: 0.000100
2024-02-04 13:46:16,930 Epoch 1230: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-04 13:46:16,930 EPOCH 1231
2024-02-04 13:46:27,640 Epoch 1231: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-04 13:46:27,641 EPOCH 1232
2024-02-04 13:46:38,321 Epoch 1232: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-04 13:46:38,322 EPOCH 1233
2024-02-04 13:46:48,798 Epoch 1233: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-04 13:46:48,798 EPOCH 1234
2024-02-04 13:46:59,225 Epoch 1234: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-04 13:46:59,226 EPOCH 1235
2024-02-04 13:47:10,387 Epoch 1235: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-04 13:47:10,388 EPOCH 1236
2024-02-04 13:47:11,210 [Epoch: 1236 Step: 00021000] Batch Recognition Loss:   0.000117 => Gls Tokens per Sec:     3899 || Batch Translation Loss:   0.013462 => Txt Tokens per Sec:     9096 || Lr: 0.000100
2024-02-04 13:47:20,924 Epoch 1236: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-04 13:47:20,925 EPOCH 1237
2024-02-04 13:47:31,667 Epoch 1237: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-04 13:47:31,667 EPOCH 1238
2024-02-04 13:47:42,587 Epoch 1238: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-04 13:47:42,588 EPOCH 1239
2024-02-04 13:47:53,129 Epoch 1239: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-04 13:47:53,130 EPOCH 1240
2024-02-04 13:48:04,074 Epoch 1240: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-04 13:48:04,075 EPOCH 1241
2024-02-04 13:48:14,812 Epoch 1241: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-04 13:48:14,812 EPOCH 1242
2024-02-04 13:48:15,331 [Epoch: 1242 Step: 00021100] Batch Recognition Loss:   0.000166 => Gls Tokens per Sec:     3706 || Batch Translation Loss:   0.010645 => Txt Tokens per Sec:     9044 || Lr: 0.000100
2024-02-04 13:48:25,786 Epoch 1242: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-04 13:48:25,787 EPOCH 1243
2024-02-04 13:48:36,447 Epoch 1243: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-04 13:48:36,448 EPOCH 1244
2024-02-04 13:48:47,242 Epoch 1244: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-04 13:48:47,243 EPOCH 1245
2024-02-04 13:48:58,333 Epoch 1245: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.58 
2024-02-04 13:48:58,335 EPOCH 1246
2024-02-04 13:49:09,095 Epoch 1246: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.63 
2024-02-04 13:49:09,095 EPOCH 1247
2024-02-04 13:49:20,133 Epoch 1247: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.71 
2024-02-04 13:49:20,133 EPOCH 1248
2024-02-04 13:49:22,832 [Epoch: 1248 Step: 00021200] Batch Recognition Loss:   0.000296 => Gls Tokens per Sec:      145 || Batch Translation Loss:   0.033306 => Txt Tokens per Sec:      518 || Lr: 0.000100
2024-02-04 13:49:30,943 Epoch 1248: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.87 
2024-02-04 13:49:30,944 EPOCH 1249
2024-02-04 13:49:41,866 Epoch 1249: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-04 13:49:41,868 EPOCH 1250
2024-02-04 13:49:52,780 Epoch 1250: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.62 
2024-02-04 13:49:52,782 EPOCH 1251
2024-02-04 13:50:03,734 Epoch 1251: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.57 
2024-02-04 13:50:03,734 EPOCH 1252
2024-02-04 13:50:14,554 Epoch 1252: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-04 13:50:14,556 EPOCH 1253
2024-02-04 13:50:25,317 [Epoch: 1253 Step: 00021300] Batch Recognition Loss:   0.000195 => Gls Tokens per Sec:      929 || Batch Translation Loss:   0.051750 => Txt Tokens per Sec:     2574 || Lr: 0.000100
2024-02-04 13:50:25,620 Epoch 1253: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-04 13:50:25,621 EPOCH 1254
2024-02-04 13:50:36,531 Epoch 1254: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-04 13:50:36,533 EPOCH 1255
2024-02-04 13:50:47,226 Epoch 1255: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-04 13:50:47,227 EPOCH 1256
2024-02-04 13:50:58,292 Epoch 1256: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.66 
2024-02-04 13:50:58,293 EPOCH 1257
2024-02-04 13:51:09,013 Epoch 1257: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-04 13:51:09,014 EPOCH 1258
2024-02-04 13:51:19,979 Epoch 1258: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-04 13:51:19,979 EPOCH 1259
2024-02-04 13:51:29,795 [Epoch: 1259 Step: 00021400] Batch Recognition Loss:   0.000167 => Gls Tokens per Sec:      887 || Batch Translation Loss:   0.024268 => Txt Tokens per Sec:     2438 || Lr: 0.000100
2024-02-04 13:51:30,667 Epoch 1259: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-04 13:51:30,667 EPOCH 1260
2024-02-04 13:51:41,442 Epoch 1260: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-04 13:51:41,444 EPOCH 1261
2024-02-04 13:51:51,719 Epoch 1261: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.62 
2024-02-04 13:51:51,720 EPOCH 1262
2024-02-04 13:52:02,642 Epoch 1262: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-04 13:52:02,642 EPOCH 1263
2024-02-04 13:52:13,457 Epoch 1263: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.48 
2024-02-04 13:52:13,458 EPOCH 1264
2024-02-04 13:52:24,288 Epoch 1264: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-04 13:52:24,289 EPOCH 1265
2024-02-04 13:52:32,745 [Epoch: 1265 Step: 00021500] Batch Recognition Loss:   0.000181 => Gls Tokens per Sec:      879 || Batch Translation Loss:   0.021110 => Txt Tokens per Sec:     2426 || Lr: 0.000100
2024-02-04 13:52:35,128 Epoch 1265: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.80 
2024-02-04 13:52:35,128 EPOCH 1266
2024-02-04 13:52:45,879 Epoch 1266: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.99 
2024-02-04 13:52:45,880 EPOCH 1267
2024-02-04 13:52:56,999 Epoch 1267: Total Training Recognition Loss 0.01  Total Training Translation Loss 5.66 
2024-02-04 13:52:57,001 EPOCH 1268
2024-02-04 13:53:08,064 Epoch 1268: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.64 
2024-02-04 13:53:08,064 EPOCH 1269
2024-02-04 13:53:18,873 Epoch 1269: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.63 
2024-02-04 13:53:18,873 EPOCH 1270
2024-02-04 13:53:29,701 Epoch 1270: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.68 
2024-02-04 13:53:29,701 EPOCH 1271
2024-02-04 13:53:33,057 [Epoch: 1271 Step: 00021600] Batch Recognition Loss:   0.000195 => Gls Tokens per Sec:     1908 || Batch Translation Loss:   0.034720 => Txt Tokens per Sec:     5058 || Lr: 0.000100
2024-02-04 13:53:40,326 Epoch 1271: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.02 
2024-02-04 13:53:40,327 EPOCH 1272
2024-02-04 13:53:51,144 Epoch 1272: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.90 
2024-02-04 13:53:51,145 EPOCH 1273
2024-02-04 13:54:01,879 Epoch 1273: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.81 
2024-02-04 13:54:01,880 EPOCH 1274
2024-02-04 13:54:12,897 Epoch 1274: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.63 
2024-02-04 13:54:12,897 EPOCH 1275
2024-02-04 13:54:23,439 Epoch 1275: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.61 
2024-02-04 13:54:23,440 EPOCH 1276
2024-02-04 13:54:34,176 Epoch 1276: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.48 
2024-02-04 13:54:34,178 EPOCH 1277
2024-02-04 13:54:41,238 [Epoch: 1277 Step: 00021700] Batch Recognition Loss:   0.000232 => Gls Tokens per Sec:      690 || Batch Translation Loss:   0.022669 => Txt Tokens per Sec:     1938 || Lr: 0.000100
2024-02-04 13:54:44,834 Epoch 1277: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-04 13:54:44,835 EPOCH 1278
2024-02-04 13:54:55,611 Epoch 1278: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-04 13:54:55,611 EPOCH 1279
2024-02-04 13:55:06,845 Epoch 1279: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.69 
2024-02-04 13:55:06,846 EPOCH 1280
2024-02-04 13:55:17,435 Epoch 1280: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.58 
2024-02-04 13:55:17,436 EPOCH 1281
2024-02-04 13:55:28,238 Epoch 1281: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-04 13:55:28,238 EPOCH 1282
2024-02-04 13:55:38,860 Epoch 1282: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-04 13:55:38,861 EPOCH 1283
2024-02-04 13:55:44,335 [Epoch: 1283 Step: 00021800] Batch Recognition Loss:   0.000246 => Gls Tokens per Sec:      656 || Batch Translation Loss:   0.015049 => Txt Tokens per Sec:     1908 || Lr: 0.000100
2024-02-04 13:55:49,642 Epoch 1283: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-04 13:55:49,642 EPOCH 1284
2024-02-04 13:56:00,102 Epoch 1284: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-04 13:56:00,102 EPOCH 1285
2024-02-04 13:56:10,935 Epoch 1285: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-04 13:56:10,936 EPOCH 1286
2024-02-04 13:56:21,894 Epoch 1286: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-04 13:56:21,894 EPOCH 1287
2024-02-04 13:56:32,733 Epoch 1287: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-04 13:56:32,734 EPOCH 1288
2024-02-04 13:56:43,593 Epoch 1288: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-04 13:56:43,594 EPOCH 1289
2024-02-04 13:56:45,640 [Epoch: 1289 Step: 00021900] Batch Recognition Loss:   0.000124 => Gls Tokens per Sec:     1252 || Batch Translation Loss:   0.019032 => Txt Tokens per Sec:     3373 || Lr: 0.000100
2024-02-04 13:56:54,237 Epoch 1289: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-04 13:56:54,238 EPOCH 1290
2024-02-04 13:57:04,962 Epoch 1290: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.58 
2024-02-04 13:57:04,963 EPOCH 1291
2024-02-04 13:57:16,127 Epoch 1291: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-04 13:57:16,128 EPOCH 1292
2024-02-04 13:57:26,656 Epoch 1292: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-04 13:57:26,657 EPOCH 1293
2024-02-04 13:57:37,727 Epoch 1293: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.78 
2024-02-04 13:57:37,727 EPOCH 1294
2024-02-04 13:57:48,338 Epoch 1294: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.62 
2024-02-04 13:57:48,339 EPOCH 1295
2024-02-04 13:57:51,298 [Epoch: 1295 Step: 00022000] Batch Recognition Loss:   0.000174 => Gls Tokens per Sec:      348 || Batch Translation Loss:   0.023812 => Txt Tokens per Sec:     1046 || Lr: 0.000100
2024-02-04 13:58:29,870 Validation result at epoch 1295, step    22000: duration: 38.5711s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00034	Translation Loss: 95237.64062	PPL: 13772.58789
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.57	(BLEU-1: 12.02,	BLEU-2: 3.64,	BLEU-3: 1.36,	BLEU-4: 0.57)
	CHRF 17.61	ROUGE 9.95
2024-02-04 13:58:29,872 Logging Recognition and Translation Outputs
2024-02-04 13:58:29,872 ========================================================================================================================
2024-02-04 13:58:29,873 Logging Sequence: 146_56.00
2024-02-04 13:58:29,873 	Gloss Reference :	A B+C+D+E
2024-02-04 13:58:29,873 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 13:58:29,873 	Gloss Alignment :	         
2024-02-04 13:58:29,873 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 13:58:29,876 	Text Reference  :	when the players go back to the hotel as per rules all          of    them have to undergo rtpcr test  for covid-19 ** ** ******* everyday
2024-02-04 13:58:29,876 	Text Hypothesis :	**** *** ******* ** **** ** the ***** ** *** team  representing india is   set  to ******* ***** leave for covid-19 on 18 january 2022    
2024-02-04 13:58:29,876 	Text Alignment  :	D    D   D       D  D    D      D     D  D   S     S            S     S    S       D       D     S                  I  I  I       S       
2024-02-04 13:58:29,876 ========================================================================================================================
2024-02-04 13:58:29,877 Logging Sequence: 118_338.00
2024-02-04 13:58:29,877 	Gloss Reference :	A B+C+D+E
2024-02-04 13:58:29,877 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 13:58:29,877 	Gloss Alignment :	         
2024-02-04 13:58:29,877 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 13:58:29,878 	Text Reference  :	this is why *** ***** **** *** ****** ******* ** even  messi wore it     
2024-02-04 13:58:29,878 	Text Hypothesis :	**** so why are aware that the entire support as there was   no   problem
2024-02-04 13:58:29,878 	Text Alignment  :	D    S      I   I     I    I   I      I       I  S     S     S    S      
2024-02-04 13:58:29,878 ========================================================================================================================
2024-02-04 13:58:29,878 Logging Sequence: 66_61.00
2024-02-04 13:58:29,878 	Gloss Reference :	A B+C+D+E
2024-02-04 13:58:29,879 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 13:58:29,879 	Gloss Alignment :	         
2024-02-04 13:58:29,879 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 13:58:29,880 	Text Reference  :	* ** **** instead of  returning back to      his homeland because of *** *** ***** ** **** *** his    injury
2024-02-04 13:58:29,880 	Text Hypothesis :	i am sure you     all must      have enjoyed the current  season  of ipl and would be from the afghan team  
2024-02-04 13:58:29,881 	Text Alignment  :	I I  I    S       S   S         S    S       S   S        S          I   I   I     I  I    I   S      S     
2024-02-04 13:58:29,881 ========================================================================================================================
2024-02-04 13:58:29,881 Logging Sequence: 81_278.00
2024-02-04 13:58:29,881 	Gloss Reference :	A B+C+D+E
2024-02-04 13:58:29,881 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 13:58:29,881 	Gloss Alignment :	         
2024-02-04 13:58:29,881 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 13:58:29,884 	Text Reference  :	of this amrapali group paid rs 3570 crore the remaining rs    652  crore was   paid by      amrapali sapphire developers a  subsidiary of   amrapali group
2024-02-04 13:58:29,884 	Text Hypothesis :	** **** ******** ***** **** ** now  with  the ********* first time a     court then lifting the      supreme  court      of his        wife sakshi   dhoni
2024-02-04 13:58:29,884 	Text Alignment  :	D  D    D        D     D    D  S    S         D         S     S    S     S     S    S       S        S        S          S  S          S    S        S    
2024-02-04 13:58:29,884 ========================================================================================================================
2024-02-04 13:58:29,884 Logging Sequence: 162_125.00
2024-02-04 13:58:29,884 	Gloss Reference :	A B+C+D+E
2024-02-04 13:58:29,884 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 13:58:29,885 	Gloss Alignment :	         
2024-02-04 13:58:29,885 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 13:58:29,886 	Text Reference  :	**** **** ********* **** ***** ** in response  to this    kohli received many hate comments on   social        media
2024-02-04 13:58:29,886 	Text Hypothesis :	fans have dedicated this match as a  pakistani to chasing from  the      team was  very     hard unfortunately they 
2024-02-04 13:58:29,886 	Text Alignment  :	I    I    I         I    I     I  S  S            S       S     S        S    S    S        S    S             S    
2024-02-04 13:58:29,886 ========================================================================================================================
2024-02-04 13:58:38,431 Epoch 1295: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.60 
2024-02-04 13:58:38,431 EPOCH 1296
2024-02-04 13:58:49,381 Epoch 1296: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.70 
2024-02-04 13:58:49,382 EPOCH 1297
2024-02-04 13:58:59,923 Epoch 1297: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.56 
2024-02-04 13:58:59,923 EPOCH 1298
2024-02-04 13:59:10,463 Epoch 1298: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-04 13:59:10,464 EPOCH 1299
2024-02-04 13:59:20,893 Epoch 1299: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-04 13:59:20,893 EPOCH 1300
2024-02-04 13:59:31,750 [Epoch: 1300 Step: 00022100] Batch Recognition Loss:   0.000115 => Gls Tokens per Sec:      979 || Batch Translation Loss:   0.011328 => Txt Tokens per Sec:     2719 || Lr: 0.000100
2024-02-04 13:59:31,751 Epoch 1300: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-04 13:59:31,751 EPOCH 1301
2024-02-04 13:59:42,458 Epoch 1301: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-04 13:59:42,458 EPOCH 1302
2024-02-04 13:59:53,700 Epoch 1302: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-04 13:59:53,701 EPOCH 1303
2024-02-04 14:00:04,249 Epoch 1303: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.66 
2024-02-04 14:00:04,250 EPOCH 1304
2024-02-04 14:00:15,828 Epoch 1304: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.58 
2024-02-04 14:00:15,828 EPOCH 1305
2024-02-04 14:00:30,538 Epoch 1305: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.55 
2024-02-04 14:00:30,539 EPOCH 1306
2024-02-04 14:00:44,652 [Epoch: 1306 Step: 00022200] Batch Recognition Loss:   0.000230 => Gls Tokens per Sec:      663 || Batch Translation Loss:   0.024865 => Txt Tokens per Sec:     1856 || Lr: 0.000100
2024-02-04 14:00:45,278 Epoch 1306: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.59 
2024-02-04 14:00:45,278 EPOCH 1307
2024-02-04 14:01:02,221 Epoch 1307: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.62 
2024-02-04 14:01:02,222 EPOCH 1308
2024-02-04 14:01:15,596 Epoch 1308: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.73 
2024-02-04 14:01:15,597 EPOCH 1309
2024-02-04 14:01:26,526 Epoch 1309: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.83 
2024-02-04 14:01:26,527 EPOCH 1310
2024-02-04 14:01:37,377 Epoch 1310: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.92 
2024-02-04 14:01:37,378 EPOCH 1311
2024-02-04 14:01:48,431 Epoch 1311: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.20 
2024-02-04 14:01:48,432 EPOCH 1312
2024-02-04 14:01:55,257 [Epoch: 1312 Step: 00022300] Batch Recognition Loss:   0.000184 => Gls Tokens per Sec:     1183 || Batch Translation Loss:   0.043025 => Txt Tokens per Sec:     3182 || Lr: 0.000100
2024-02-04 14:01:59,197 Epoch 1312: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.88 
2024-02-04 14:01:59,198 EPOCH 1313
2024-02-04 14:02:10,079 Epoch 1313: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.50 
2024-02-04 14:02:10,080 EPOCH 1314
2024-02-04 14:02:20,737 Epoch 1314: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.30 
2024-02-04 14:02:20,738 EPOCH 1315
2024-02-04 14:02:31,490 Epoch 1315: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.25 
2024-02-04 14:02:31,492 EPOCH 1316
2024-02-04 14:02:42,415 Epoch 1316: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.27 
2024-02-04 14:02:42,416 EPOCH 1317
2024-02-04 14:02:53,497 Epoch 1317: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.08 
2024-02-04 14:02:53,497 EPOCH 1318
2024-02-04 14:03:00,409 [Epoch: 1318 Step: 00022400] Batch Recognition Loss:   0.000324 => Gls Tokens per Sec:     1019 || Batch Translation Loss:   0.112321 => Txt Tokens per Sec:     2810 || Lr: 0.000100
2024-02-04 14:03:04,287 Epoch 1318: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.33 
2024-02-04 14:03:04,287 EPOCH 1319
2024-02-04 14:03:15,108 Epoch 1319: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.39 
2024-02-04 14:03:15,109 EPOCH 1320
2024-02-04 14:03:25,875 Epoch 1320: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.50 
2024-02-04 14:03:25,876 EPOCH 1321
2024-02-04 14:03:36,507 Epoch 1321: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.49 
2024-02-04 14:03:36,508 EPOCH 1322
2024-02-04 14:03:47,734 Epoch 1322: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.12 
2024-02-04 14:03:47,735 EPOCH 1323
2024-02-04 14:03:58,571 Epoch 1323: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.92 
2024-02-04 14:03:58,571 EPOCH 1324
2024-02-04 14:04:01,791 [Epoch: 1324 Step: 00022500] Batch Recognition Loss:   0.000316 => Gls Tokens per Sec:     1790 || Batch Translation Loss:   0.057790 => Txt Tokens per Sec:     4877 || Lr: 0.000100
2024-02-04 14:04:09,239 Epoch 1324: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.00 
2024-02-04 14:04:09,240 EPOCH 1325
2024-02-04 14:04:20,723 Epoch 1325: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.21 
2024-02-04 14:04:20,724 EPOCH 1326
2024-02-04 14:04:38,432 Epoch 1326: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.30 
2024-02-04 14:04:38,434 EPOCH 1327
2024-02-04 14:04:55,118 Epoch 1327: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.00 
2024-02-04 14:04:55,118 EPOCH 1328
2024-02-04 14:05:13,767 Epoch 1328: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.66 
2024-02-04 14:05:13,769 EPOCH 1329
2024-02-04 14:05:27,186 Epoch 1329: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.57 
2024-02-04 14:05:27,187 EPOCH 1330
2024-02-04 14:05:34,531 [Epoch: 1330 Step: 00022600] Batch Recognition Loss:   0.000380 => Gls Tokens per Sec:      576 || Batch Translation Loss:   0.036364 => Txt Tokens per Sec:     1625 || Lr: 0.000100
2024-02-04 14:05:38,366 Epoch 1330: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.60 
2024-02-04 14:05:38,366 EPOCH 1331
2024-02-04 14:05:52,128 Epoch 1331: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-04 14:05:52,129 EPOCH 1332
2024-02-04 14:06:08,654 Epoch 1332: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.48 
2024-02-04 14:06:08,656 EPOCH 1333
2024-02-04 14:06:34,946 Epoch 1333: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-04 14:06:34,947 EPOCH 1334
2024-02-04 14:07:01,868 Epoch 1334: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-04 14:07:01,870 EPOCH 1335
2024-02-04 14:07:28,493 Epoch 1335: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-04 14:07:28,495 EPOCH 1336
2024-02-04 14:07:43,471 [Epoch: 1336 Step: 00022700] Batch Recognition Loss:   0.000348 => Gls Tokens per Sec:      214 || Batch Translation Loss:   0.035250 => Txt Tokens per Sec:      699 || Lr: 0.000100
2024-02-04 14:07:55,003 Epoch 1336: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-04 14:07:55,004 EPOCH 1337
2024-02-04 14:08:22,257 Epoch 1337: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-04 14:08:22,259 EPOCH 1338
2024-02-04 14:08:49,330 Epoch 1338: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.57 
2024-02-04 14:08:49,332 EPOCH 1339
2024-02-04 14:09:17,327 Epoch 1339: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.99 
2024-02-04 14:09:17,329 EPOCH 1340
2024-02-04 14:09:44,034 Epoch 1340: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.66 
2024-02-04 14:09:44,035 EPOCH 1341
2024-02-04 14:10:10,579 Epoch 1341: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.18 
2024-02-04 14:10:10,581 EPOCH 1342
2024-02-04 14:10:15,875 [Epoch: 1342 Step: 00022800] Batch Recognition Loss:   0.000307 => Gls Tokens per Sec:      363 || Batch Translation Loss:   0.061312 => Txt Tokens per Sec:     1093 || Lr: 0.000100
2024-02-04 14:10:37,883 Epoch 1342: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.59 
2024-02-04 14:10:37,884 EPOCH 1343
2024-02-04 14:11:04,165 Epoch 1343: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-04 14:11:04,167 EPOCH 1344
2024-02-04 14:11:30,859 Epoch 1344: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-04 14:11:30,860 EPOCH 1345
2024-02-04 14:11:57,075 Epoch 1345: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.63 
2024-02-04 14:11:57,076 EPOCH 1346
2024-02-04 14:12:24,286 Epoch 1346: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-04 14:12:24,288 EPOCH 1347
2024-02-04 14:12:51,497 Epoch 1347: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-04 14:12:51,498 EPOCH 1348
2024-02-04 14:12:51,785 [Epoch: 1348 Step: 00022900] Batch Recognition Loss:   0.000155 => Gls Tokens per Sec:     2240 || Batch Translation Loss:   0.012609 => Txt Tokens per Sec:     5723 || Lr: 0.000100
2024-02-04 14:13:22,424 Epoch 1348: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-04 14:13:22,426 EPOCH 1349
2024-02-04 14:13:49,059 Epoch 1349: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-04 14:13:49,061 EPOCH 1350
2024-02-04 14:14:16,608 Epoch 1350: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-04 14:14:16,610 EPOCH 1351
2024-02-04 14:14:43,500 Epoch 1351: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-04 14:14:43,502 EPOCH 1352
2024-02-04 14:15:10,122 Epoch 1352: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-04 14:15:10,123 EPOCH 1353
2024-02-04 14:15:36,562 [Epoch: 1353 Step: 00023000] Batch Recognition Loss:   0.000170 => Gls Tokens per Sec:      378 || Batch Translation Loss:   0.016819 => Txt Tokens per Sec:     1054 || Lr: 0.000100
2024-02-04 14:15:36,925 Epoch 1353: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-04 14:15:36,925 EPOCH 1354
2024-02-04 14:16:03,208 Epoch 1354: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-04 14:16:03,210 EPOCH 1355
2024-02-04 14:16:30,444 Epoch 1355: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-04 14:16:30,445 EPOCH 1356
2024-02-04 14:16:57,706 Epoch 1356: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.87 
2024-02-04 14:16:57,707 EPOCH 1357
2024-02-04 14:17:24,951 Epoch 1357: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.63 
2024-02-04 14:17:24,952 EPOCH 1358
2024-02-04 14:17:52,448 Epoch 1358: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.71 
2024-02-04 14:17:52,450 EPOCH 1359
2024-02-04 14:18:13,220 [Epoch: 1359 Step: 00023100] Batch Recognition Loss:   0.000140 => Gls Tokens per Sec:      419 || Batch Translation Loss:   0.034578 => Txt Tokens per Sec:     1134 || Lr: 0.000100
2024-02-04 14:18:18,552 Epoch 1359: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.94 
2024-02-04 14:18:18,552 EPOCH 1360
2024-02-04 14:18:42,949 Epoch 1360: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.04 
2024-02-04 14:18:42,949 EPOCH 1361
2024-02-04 14:19:09,214 Epoch 1361: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.92 
2024-02-04 14:19:09,216 EPOCH 1362
2024-02-04 14:19:36,175 Epoch 1362: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.86 
2024-02-04 14:19:36,176 EPOCH 1363
2024-02-04 14:20:03,436 Epoch 1363: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.79 
2024-02-04 14:20:03,437 EPOCH 1364
2024-02-04 14:20:29,731 Epoch 1364: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.76 
2024-02-04 14:20:29,733 EPOCH 1365
2024-02-04 14:20:45,753 [Epoch: 1365 Step: 00023200] Batch Recognition Loss:   0.000279 => Gls Tokens per Sec:      464 || Batch Translation Loss:   0.041752 => Txt Tokens per Sec:     1259 || Lr: 0.000100
2024-02-04 14:20:56,104 Epoch 1365: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.75 
2024-02-04 14:20:56,104 EPOCH 1366
2024-02-04 14:21:22,860 Epoch 1366: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.74 
2024-02-04 14:21:22,862 EPOCH 1367
2024-02-04 14:21:35,436 Epoch 1367: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.72 
2024-02-04 14:21:35,437 EPOCH 1368
2024-02-04 14:21:46,188 Epoch 1368: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.68 
2024-02-04 14:21:46,190 EPOCH 1369
2024-02-04 14:21:57,444 Epoch 1369: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.98 
2024-02-04 14:21:57,444 EPOCH 1370
2024-02-04 14:22:08,389 Epoch 1370: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.91 
2024-02-04 14:22:08,390 EPOCH 1371
2024-02-04 14:22:13,422 [Epoch: 1371 Step: 00023300] Batch Recognition Loss:   0.000224 => Gls Tokens per Sec:     1272 || Batch Translation Loss:   0.057234 => Txt Tokens per Sec:     3534 || Lr: 0.000100
2024-02-04 14:22:18,891 Epoch 1371: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.69 
2024-02-04 14:22:18,891 EPOCH 1372
2024-02-04 14:22:30,116 Epoch 1372: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.68 
2024-02-04 14:22:30,117 EPOCH 1373
2024-02-04 14:22:40,583 Epoch 1373: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.99 
2024-02-04 14:22:40,584 EPOCH 1374
2024-02-04 14:22:52,060 Epoch 1374: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.04 
2024-02-04 14:22:52,061 EPOCH 1375
2024-02-04 14:23:03,008 Epoch 1375: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.96 
2024-02-04 14:23:03,010 EPOCH 1376
2024-02-04 14:23:14,007 Epoch 1376: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.99 
2024-02-04 14:23:14,008 EPOCH 1377
2024-02-04 14:23:19,198 [Epoch: 1377 Step: 00023400] Batch Recognition Loss:   0.000213 => Gls Tokens per Sec:      987 || Batch Translation Loss:   0.044677 => Txt Tokens per Sec:     2881 || Lr: 0.000100
2024-02-04 14:23:25,202 Epoch 1377: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.70 
2024-02-04 14:23:25,203 EPOCH 1378
2024-02-04 14:23:36,020 Epoch 1378: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.60 
2024-02-04 14:23:36,021 EPOCH 1379
2024-02-04 14:23:47,140 Epoch 1379: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-04 14:23:47,141 EPOCH 1380
2024-02-04 14:23:58,138 Epoch 1380: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.75 
2024-02-04 14:23:58,139 EPOCH 1381
2024-02-04 14:24:09,339 Epoch 1381: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.82 
2024-02-04 14:24:09,340 EPOCH 1382
2024-02-04 14:24:20,477 Epoch 1382: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.81 
2024-02-04 14:24:20,478 EPOCH 1383
2024-02-04 14:24:27,705 [Epoch: 1383 Step: 00023500] Batch Recognition Loss:   0.000270 => Gls Tokens per Sec:      497 || Batch Translation Loss:   0.043453 => Txt Tokens per Sec:     1546 || Lr: 0.000100
2024-02-04 14:24:31,537 Epoch 1383: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.91 
2024-02-04 14:24:31,538 EPOCH 1384
2024-02-04 14:24:42,630 Epoch 1384: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.91 
2024-02-04 14:24:42,632 EPOCH 1385
2024-02-04 14:24:53,473 Epoch 1385: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.95 
2024-02-04 14:24:53,473 EPOCH 1386
2024-02-04 14:25:04,178 Epoch 1386: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.64 
2024-02-04 14:25:04,178 EPOCH 1387
2024-02-04 14:25:15,081 Epoch 1387: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.65 
2024-02-04 14:25:15,082 EPOCH 1388
2024-02-04 14:25:26,319 Epoch 1388: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.60 
2024-02-04 14:25:26,320 EPOCH 1389
2024-02-04 14:25:28,868 [Epoch: 1389 Step: 00023600] Batch Recognition Loss:   0.000195 => Gls Tokens per Sec:     1005 || Batch Translation Loss:   0.025355 => Txt Tokens per Sec:     2803 || Lr: 0.000100
2024-02-04 14:25:37,254 Epoch 1389: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-04 14:25:37,255 EPOCH 1390
2024-02-04 14:25:48,154 Epoch 1390: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-04 14:25:48,155 EPOCH 1391
2024-02-04 14:25:59,438 Epoch 1391: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-04 14:25:59,439 EPOCH 1392
2024-02-04 14:26:10,247 Epoch 1392: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-04 14:26:10,248 EPOCH 1393
2024-02-04 14:26:21,193 Epoch 1393: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-04 14:26:21,194 EPOCH 1394
2024-02-04 14:26:32,128 Epoch 1394: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-04 14:26:32,129 EPOCH 1395
2024-02-04 14:26:35,217 [Epoch: 1395 Step: 00023700] Batch Recognition Loss:   0.000158 => Gls Tokens per Sec:      334 || Batch Translation Loss:   0.032034 => Txt Tokens per Sec:     1060 || Lr: 0.000100
2024-02-04 14:26:43,132 Epoch 1395: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.48 
2024-02-04 14:26:43,132 EPOCH 1396
2024-02-04 14:26:53,979 Epoch 1396: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.66 
2024-02-04 14:26:53,980 EPOCH 1397
2024-02-04 14:27:04,949 Epoch 1397: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-04 14:27:04,950 EPOCH 1398
2024-02-04 14:27:16,079 Epoch 1398: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-04 14:27:16,080 EPOCH 1399
2024-02-04 14:27:26,784 Epoch 1399: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.62 
2024-02-04 14:27:26,786 EPOCH 1400
2024-02-04 14:27:37,906 [Epoch: 1400 Step: 00023800] Batch Recognition Loss:   0.000206 => Gls Tokens per Sec:      956 || Batch Translation Loss:   0.029794 => Txt Tokens per Sec:     2654 || Lr: 0.000100
2024-02-04 14:27:37,906 Epoch 1400: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.59 
2024-02-04 14:27:37,906 EPOCH 1401
2024-02-04 14:27:48,899 Epoch 1401: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-04 14:27:48,900 EPOCH 1402
2024-02-04 14:27:59,958 Epoch 1402: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-04 14:27:59,958 EPOCH 1403
2024-02-04 14:28:11,100 Epoch 1403: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.64 
2024-02-04 14:28:11,100 EPOCH 1404
2024-02-04 14:28:22,109 Epoch 1404: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.16 
2024-02-04 14:28:22,111 EPOCH 1405
2024-02-04 14:28:33,130 Epoch 1405: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.25 
2024-02-04 14:28:33,130 EPOCH 1406
2024-02-04 14:28:40,953 [Epoch: 1406 Step: 00023900] Batch Recognition Loss:   0.000830 => Gls Tokens per Sec:     1227 || Batch Translation Loss:   0.120002 => Txt Tokens per Sec:     3401 || Lr: 0.000100
2024-02-04 14:28:43,974 Epoch 1406: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.78 
2024-02-04 14:28:43,974 EPOCH 1407
2024-02-04 14:28:54,881 Epoch 1407: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.78 
2024-02-04 14:28:54,882 EPOCH 1408
2024-02-04 14:29:05,936 Epoch 1408: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.52 
2024-02-04 14:29:05,937 EPOCH 1409
2024-02-04 14:29:16,744 Epoch 1409: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.49 
2024-02-04 14:29:16,745 EPOCH 1410
2024-02-04 14:29:27,690 Epoch 1410: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.98 
2024-02-04 14:29:27,691 EPOCH 1411
2024-02-04 14:29:38,581 Epoch 1411: Total Training Recognition Loss 0.01  Total Training Translation Loss 5.06 
2024-02-04 14:29:38,583 EPOCH 1412
2024-02-04 14:29:46,118 [Epoch: 1412 Step: 00024000] Batch Recognition Loss:   0.000807 => Gls Tokens per Sec:     1104 || Batch Translation Loss:   0.232331 => Txt Tokens per Sec:     3036 || Lr: 0.000100
2024-02-04 14:30:25,964 Validation result at epoch 1412, step    24000: duration: 39.8439s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00126	Translation Loss: 92963.03125	PPL: 10968.85742
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.59	(BLEU-1: 10.43,	BLEU-2: 3.18,	BLEU-3: 1.19,	BLEU-4: 0.59)
	CHRF 16.94	ROUGE 8.80
2024-02-04 14:30:25,966 Logging Recognition and Translation Outputs
2024-02-04 14:30:25,966 ========================================================================================================================
2024-02-04 14:30:25,966 Logging Sequence: 169_165.00
2024-02-04 14:30:25,966 	Gloss Reference :	A B+C+D+E
2024-02-04 14:30:25,967 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 14:30:25,967 	Gloss Alignment :	         
2024-02-04 14:30:25,967 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 14:30:25,969 	Text Reference  :	** the indian government was       outraged by          the incident and  these changes were undone by     wikipedia
2024-02-04 14:30:25,969 	Text Hypothesis :	do you know   that       wikipedia provides information on  celebs   like their height  age  must   ensure male     
2024-02-04 14:30:25,969 	Text Alignment  :	I  S   S      S          S         S        S           S   S        S    S     S       S    S      S      S        
2024-02-04 14:30:25,970 ========================================================================================================================
2024-02-04 14:30:25,970 Logging Sequence: 175_60.00
2024-02-04 14:30:25,970 	Gloss Reference :	A B+C+D+E
2024-02-04 14:30:25,970 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 14:30:25,971 	Gloss Alignment :	         
2024-02-04 14:30:25,971 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 14:30:25,973 	Text Reference  :	that is    how       india bagged 9   medals in the ****** ****** *** ***** *** **** youth tournament
2024-02-04 14:30:25,973 	Text Hypothesis :	**** hence pakistan' ended at     the fall   of the eighth wicket and india has left the   match     
2024-02-04 14:30:25,973 	Text Alignment  :	D    S     S         S     S      S   S      S      I      I      I   I     I   I    S     S         
2024-02-04 14:30:25,973 ========================================================================================================================
2024-02-04 14:30:25,974 Logging Sequence: 61_255.00
2024-02-04 14:30:25,974 	Gloss Reference :	A B+C+D+E
2024-02-04 14:30:25,974 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 14:30:25,974 	Gloss Alignment :	         
2024-02-04 14:30:25,975 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 14:30:25,976 	Text Reference  :	*** *** **** *** in       2011 we  decided to     marry and      informed our families
2024-02-04 14:30:25,976 	Text Hypothesis :	she has said her marriage is   not majorly during the   business class    on  twitter 
2024-02-04 14:30:25,977 	Text Alignment  :	I   I   I    I   S        S    S   S       S      S     S        S        S   S       
2024-02-04 14:30:25,977 ========================================================================================================================
2024-02-04 14:30:25,977 Logging Sequence: 173_39.00
2024-02-04 14:30:25,977 	Gloss Reference :	A B+C+D+E
2024-02-04 14:30:25,978 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 14:30:25,978 	Gloss Alignment :	         
2024-02-04 14:30:25,978 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 14:30:25,979 	Text Reference  :	**** ****** *** kohli will step        down as     india' captain 
2024-02-04 14:30:25,979 	Text Hypothesis :	when taylor too was   not  comfortable and  joined pune   warriors
2024-02-04 14:30:25,979 	Text Alignment  :	I    I      I   S     S    S           S    S      S      S       
2024-02-04 14:30:25,980 ========================================================================================================================
2024-02-04 14:30:25,980 Logging Sequence: 172_82.00
2024-02-04 14:30:25,980 	Gloss Reference :	A B+C+D+E
2024-02-04 14:30:25,980 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 14:30:25,981 	Gloss Alignment :	         
2024-02-04 14:30:25,981 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 14:30:25,983 	Text Reference  :	you all know     that the toss was about to start   at  700 pm  but  it started raining at around 630 pm     
2024-02-04 14:30:25,983 	Text Hypothesis :	*** *** remember that *** **** *** ***** ** amazing win you can find a  tickets will    be runner an  amazing
2024-02-04 14:30:25,984 	Text Alignment  :	D   D   S             D   D    D   D     D  S       S   S   S   S    S  S       S       S  S      S   S      
2024-02-04 14:30:25,984 ========================================================================================================================
2024-02-04 14:30:29,915 Epoch 1412: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.11 
2024-02-04 14:30:29,915 EPOCH 1413
2024-02-04 14:30:41,625 Epoch 1413: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.48 
2024-02-04 14:30:41,625 EPOCH 1414
2024-02-04 14:30:52,616 Epoch 1414: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.89 
2024-02-04 14:30:52,617 EPOCH 1415
2024-02-04 14:31:03,499 Epoch 1415: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-04 14:31:03,499 EPOCH 1416
2024-02-04 14:31:13,953 Epoch 1416: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-04 14:31:13,953 EPOCH 1417
2024-02-04 14:31:24,894 Epoch 1417: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-04 14:31:24,895 EPOCH 1418
2024-02-04 14:31:33,516 [Epoch: 1418 Step: 00024100] Batch Recognition Loss:   0.000241 => Gls Tokens per Sec:      788 || Batch Translation Loss:   0.020614 => Txt Tokens per Sec:     2402 || Lr: 0.000100
2024-02-04 14:31:35,901 Epoch 1418: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-04 14:31:35,902 EPOCH 1419
2024-02-04 14:31:47,119 Epoch 1419: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-04 14:31:47,120 EPOCH 1420
2024-02-04 14:31:57,945 Epoch 1420: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-04 14:31:57,945 EPOCH 1421
2024-02-04 14:32:08,706 Epoch 1421: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-04 14:32:08,706 EPOCH 1422
2024-02-04 14:32:19,484 Epoch 1422: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-04 14:32:19,484 EPOCH 1423
2024-02-04 14:32:30,321 Epoch 1423: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-04 14:32:30,321 EPOCH 1424
2024-02-04 14:32:37,575 [Epoch: 1424 Step: 00024200] Batch Recognition Loss:   0.000205 => Gls Tokens per Sec:      760 || Batch Translation Loss:   0.029340 => Txt Tokens per Sec:     2069 || Lr: 0.000100
2024-02-04 14:32:41,357 Epoch 1424: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-04 14:32:41,357 EPOCH 1425
2024-02-04 14:32:52,259 Epoch 1425: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-04 14:32:52,260 EPOCH 1426
2024-02-04 14:33:03,258 Epoch 1426: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-04 14:33:03,259 EPOCH 1427
2024-02-04 14:33:14,354 Epoch 1427: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.58 
2024-02-04 14:33:14,355 EPOCH 1428
2024-02-04 14:33:25,314 Epoch 1428: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-04 14:33:25,315 EPOCH 1429
2024-02-04 14:33:36,264 Epoch 1429: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-04 14:33:36,265 EPOCH 1430
2024-02-04 14:33:37,542 [Epoch: 1430 Step: 00024300] Batch Recognition Loss:   0.000141 => Gls Tokens per Sec:     3510 || Batch Translation Loss:   0.010309 => Txt Tokens per Sec:     9018 || Lr: 0.000100
2024-02-04 14:33:47,109 Epoch 1430: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-04 14:33:47,110 EPOCH 1431
2024-02-04 14:33:57,975 Epoch 1431: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-04 14:33:57,975 EPOCH 1432
2024-02-04 14:34:08,851 Epoch 1432: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-04 14:34:08,852 EPOCH 1433
2024-02-04 14:34:19,833 Epoch 1433: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-04 14:34:19,834 EPOCH 1434
2024-02-04 14:34:30,335 Epoch 1434: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-04 14:34:30,336 EPOCH 1435
2024-02-04 14:34:41,255 Epoch 1435: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-04 14:34:41,256 EPOCH 1436
2024-02-04 14:34:43,616 [Epoch: 1436 Step: 00024400] Batch Recognition Loss:   0.000143 => Gls Tokens per Sec:     1357 || Batch Translation Loss:   0.012789 => Txt Tokens per Sec:     3630 || Lr: 0.000100
2024-02-04 14:34:52,237 Epoch 1436: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-04 14:34:52,237 EPOCH 1437
2024-02-04 14:35:03,549 Epoch 1437: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-04 14:35:03,550 EPOCH 1438
2024-02-04 14:35:14,475 Epoch 1438: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-04 14:35:14,475 EPOCH 1439
2024-02-04 14:35:26,406 Epoch 1439: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.59 
2024-02-04 14:35:26,407 EPOCH 1440
2024-02-04 14:35:42,253 Epoch 1440: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-04 14:35:42,255 EPOCH 1441
2024-02-04 14:35:55,582 Epoch 1441: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-04 14:35:55,583 EPOCH 1442
2024-02-04 14:35:57,755 [Epoch: 1442 Step: 00024500] Batch Recognition Loss:   0.000187 => Gls Tokens per Sec:      884 || Batch Translation Loss:   0.021610 => Txt Tokens per Sec:     2490 || Lr: 0.000100
2024-02-04 14:36:11,063 Epoch 1442: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-04 14:36:11,063 EPOCH 1443
2024-02-04 14:36:23,238 Epoch 1443: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-04 14:36:23,239 EPOCH 1444
2024-02-04 14:36:34,840 Epoch 1444: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-04 14:36:34,840 EPOCH 1445
2024-02-04 14:36:46,016 Epoch 1445: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-04 14:36:46,017 EPOCH 1446
2024-02-04 14:36:57,444 Epoch 1446: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-04 14:36:57,445 EPOCH 1447
2024-02-04 14:37:08,818 Epoch 1447: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.88 
2024-02-04 14:37:08,818 EPOCH 1448
2024-02-04 14:37:08,923 [Epoch: 1448 Step: 00024600] Batch Recognition Loss:   0.000147 => Gls Tokens per Sec:     6154 || Batch Translation Loss:   0.030712 => Txt Tokens per Sec:     9500 || Lr: 0.000100
2024-02-04 14:37:20,226 Epoch 1448: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.17 
2024-02-04 14:37:20,226 EPOCH 1449
2024-02-04 14:37:31,409 Epoch 1449: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.37 
2024-02-04 14:37:31,410 EPOCH 1450
2024-02-04 14:37:42,473 Epoch 1450: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.06 
2024-02-04 14:37:42,474 EPOCH 1451
2024-02-04 14:37:53,429 Epoch 1451: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.10 
2024-02-04 14:37:53,430 EPOCH 1452
2024-02-04 14:38:04,551 Epoch 1452: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.18 
2024-02-04 14:38:04,551 EPOCH 1453
2024-02-04 14:38:13,301 [Epoch: 1453 Step: 00024700] Batch Recognition Loss:   0.000209 => Gls Tokens per Sec:     1170 || Batch Translation Loss:   0.038890 => Txt Tokens per Sec:     3213 || Lr: 0.000100
2024-02-04 14:38:17,017 Epoch 1453: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.95 
2024-02-04 14:38:17,017 EPOCH 1454
2024-02-04 14:38:29,824 Epoch 1454: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.64 
2024-02-04 14:38:29,824 EPOCH 1455
2024-02-04 14:38:41,283 Epoch 1455: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.60 
2024-02-04 14:38:41,283 EPOCH 1456
2024-02-04 14:38:52,618 Epoch 1456: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.69 
2024-02-04 14:38:52,619 EPOCH 1457
2024-02-04 14:39:04,845 Epoch 1457: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.69 
2024-02-04 14:39:04,847 EPOCH 1458
2024-02-04 14:39:16,962 Epoch 1458: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.57 
2024-02-04 14:39:16,963 EPOCH 1459
2024-02-04 14:39:25,287 [Epoch: 1459 Step: 00024800] Batch Recognition Loss:   0.000217 => Gls Tokens per Sec:     1077 || Batch Translation Loss:   0.038393 => Txt Tokens per Sec:     3047 || Lr: 0.000100
2024-02-04 14:39:28,667 Epoch 1459: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.94 
2024-02-04 14:39:28,668 EPOCH 1460
2024-02-04 14:39:40,458 Epoch 1460: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.85 
2024-02-04 14:39:40,459 EPOCH 1461
2024-02-04 14:39:51,428 Epoch 1461: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.87 
2024-02-04 14:39:51,430 EPOCH 1462
2024-02-04 14:40:04,326 Epoch 1462: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.80 
2024-02-04 14:40:04,327 EPOCH 1463
2024-02-04 14:40:15,827 Epoch 1463: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.67 
2024-02-04 14:40:15,828 EPOCH 1464
2024-02-04 14:40:27,536 Epoch 1464: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-04 14:40:27,536 EPOCH 1465
2024-02-04 14:40:33,274 [Epoch: 1465 Step: 00024900] Batch Recognition Loss:   0.000147 => Gls Tokens per Sec:     1339 || Batch Translation Loss:   0.009292 => Txt Tokens per Sec:     3692 || Lr: 0.000100
2024-02-04 14:40:38,651 Epoch 1465: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.86 
2024-02-04 14:40:38,652 EPOCH 1466
2024-02-04 14:40:49,752 Epoch 1466: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.73 
2024-02-04 14:40:49,753 EPOCH 1467
2024-02-04 14:41:01,101 Epoch 1467: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.96 
2024-02-04 14:41:01,102 EPOCH 1468
2024-02-04 14:41:12,933 Epoch 1468: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.99 
2024-02-04 14:41:12,934 EPOCH 1469
2024-02-04 14:41:24,684 Epoch 1469: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.00 
2024-02-04 14:41:24,685 EPOCH 1470
2024-02-04 14:41:36,617 Epoch 1470: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.32 
2024-02-04 14:41:36,618 EPOCH 1471
2024-02-04 14:41:45,428 [Epoch: 1471 Step: 00025000] Batch Recognition Loss:   0.000194 => Gls Tokens per Sec:      698 || Batch Translation Loss:   0.040214 => Txt Tokens per Sec:     2060 || Lr: 0.000100
2024-02-04 14:41:48,497 Epoch 1471: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.54 
2024-02-04 14:41:48,497 EPOCH 1472
2024-02-04 14:42:00,119 Epoch 1472: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.51 
2024-02-04 14:42:00,121 EPOCH 1473
2024-02-04 14:42:11,342 Epoch 1473: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.37 
2024-02-04 14:42:11,343 EPOCH 1474
2024-02-04 14:42:23,401 Epoch 1474: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.78 
2024-02-04 14:42:23,402 EPOCH 1475
2024-02-04 14:42:34,764 Epoch 1475: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.75 
2024-02-04 14:42:34,765 EPOCH 1476
2024-02-04 14:42:46,148 Epoch 1476: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-04 14:42:46,149 EPOCH 1477
2024-02-04 14:42:53,305 [Epoch: 1477 Step: 00025100] Batch Recognition Loss:   0.000260 => Gls Tokens per Sec:      681 || Batch Translation Loss:   0.033858 => Txt Tokens per Sec:     1822 || Lr: 0.000100
2024-02-04 14:42:57,428 Epoch 1477: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.55 
2024-02-04 14:42:57,428 EPOCH 1478
2024-02-04 14:43:09,109 Epoch 1478: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-04 14:43:09,110 EPOCH 1479
2024-02-04 14:43:21,062 Epoch 1479: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-04 14:43:21,062 EPOCH 1480
2024-02-04 14:43:32,741 Epoch 1480: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-04 14:43:32,741 EPOCH 1481
2024-02-04 14:43:44,881 Epoch 1481: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-04 14:43:44,882 EPOCH 1482
2024-02-04 14:43:55,742 Epoch 1482: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-04 14:43:55,742 EPOCH 1483
2024-02-04 14:44:02,968 [Epoch: 1483 Step: 00025200] Batch Recognition Loss:   0.000293 => Gls Tokens per Sec:      497 || Batch Translation Loss:   0.012448 => Txt Tokens per Sec:     1499 || Lr: 0.000100
2024-02-04 14:44:06,987 Epoch 1483: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-04 14:44:06,988 EPOCH 1484
2024-02-04 14:44:17,962 Epoch 1484: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-04 14:44:17,963 EPOCH 1485
2024-02-04 14:44:28,910 Epoch 1485: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-04 14:44:28,911 EPOCH 1486
2024-02-04 14:44:39,500 Epoch 1486: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-04 14:44:39,501 EPOCH 1487
2024-02-04 14:44:50,249 Epoch 1487: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-04 14:44:50,251 EPOCH 1488
2024-02-04 14:45:01,112 Epoch 1488: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-04 14:45:01,112 EPOCH 1489
2024-02-04 14:45:03,400 [Epoch: 1489 Step: 00025300] Batch Recognition Loss:   0.000148 => Gls Tokens per Sec:     1120 || Batch Translation Loss:   0.015406 => Txt Tokens per Sec:     3367 || Lr: 0.000100
2024-02-04 14:45:12,051 Epoch 1489: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-04 14:45:12,051 EPOCH 1490
2024-02-04 14:45:22,975 Epoch 1490: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-04 14:45:22,976 EPOCH 1491
2024-02-04 14:45:33,621 Epoch 1491: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-04 14:45:33,622 EPOCH 1492
2024-02-04 14:45:44,518 Epoch 1492: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-04 14:45:44,518 EPOCH 1493
2024-02-04 14:45:55,353 Epoch 1493: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-04 14:45:55,354 EPOCH 1494
2024-02-04 14:46:05,929 Epoch 1494: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.60 
2024-02-04 14:46:05,929 EPOCH 1495
2024-02-04 14:46:07,691 [Epoch: 1495 Step: 00025400] Batch Recognition Loss:   0.000189 => Gls Tokens per Sec:      727 || Batch Translation Loss:   0.020534 => Txt Tokens per Sec:     2181 || Lr: 0.000100
2024-02-04 14:46:16,781 Epoch 1495: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.64 
2024-02-04 14:46:16,782 EPOCH 1496
2024-02-04 14:46:27,868 Epoch 1496: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.78 
2024-02-04 14:46:27,869 EPOCH 1497
2024-02-04 14:46:38,816 Epoch 1497: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.16 
2024-02-04 14:46:38,817 EPOCH 1498
2024-02-04 14:46:49,474 Epoch 1498: Total Training Recognition Loss 0.01  Total Training Translation Loss 5.85 
2024-02-04 14:46:49,475 EPOCH 1499
2024-02-04 14:47:00,103 Epoch 1499: Total Training Recognition Loss 0.06  Total Training Translation Loss 5.96 
2024-02-04 14:47:00,103 EPOCH 1500
2024-02-04 14:47:10,750 [Epoch: 1500 Step: 00025500] Batch Recognition Loss:   0.000349 => Gls Tokens per Sec:      999 || Batch Translation Loss:   0.083977 => Txt Tokens per Sec:     2772 || Lr: 0.000100
2024-02-04 14:47:10,750 Epoch 1500: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.40 
2024-02-04 14:47:10,751 EPOCH 1501
2024-02-04 14:47:21,618 Epoch 1501: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.54 
2024-02-04 14:47:21,618 EPOCH 1502
2024-02-04 14:47:32,473 Epoch 1502: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.12 
2024-02-04 14:47:32,474 EPOCH 1503
2024-02-04 14:47:43,398 Epoch 1503: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.94 
2024-02-04 14:47:43,399 EPOCH 1504
2024-02-04 14:47:54,513 Epoch 1504: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.82 
2024-02-04 14:47:54,514 EPOCH 1505
2024-02-04 14:48:05,353 Epoch 1505: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-04 14:48:05,355 EPOCH 1506
2024-02-04 14:48:15,677 [Epoch: 1506 Step: 00025600] Batch Recognition Loss:   0.000300 => Gls Tokens per Sec:      906 || Batch Translation Loss:   0.018154 => Txt Tokens per Sec:     2513 || Lr: 0.000100
2024-02-04 14:48:16,235 Epoch 1506: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-04 14:48:16,235 EPOCH 1507
2024-02-04 14:48:27,121 Epoch 1507: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-04 14:48:27,121 EPOCH 1508
2024-02-04 14:48:38,052 Epoch 1508: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-04 14:48:38,053 EPOCH 1509
2024-02-04 14:48:48,903 Epoch 1509: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-04 14:48:48,904 EPOCH 1510
2024-02-04 14:48:59,798 Epoch 1510: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-04 14:48:59,798 EPOCH 1511
2024-02-04 14:49:10,701 Epoch 1511: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-04 14:49:10,701 EPOCH 1512
2024-02-04 14:49:18,858 [Epoch: 1512 Step: 00025700] Batch Recognition Loss:   0.000246 => Gls Tokens per Sec:      990 || Batch Translation Loss:   0.019088 => Txt Tokens per Sec:     2758 || Lr: 0.000100
2024-02-04 14:49:21,336 Epoch 1512: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-04 14:49:21,336 EPOCH 1513
2024-02-04 14:49:31,841 Epoch 1513: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-04 14:49:31,842 EPOCH 1514
2024-02-04 14:49:42,531 Epoch 1514: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-04 14:49:42,531 EPOCH 1515
2024-02-04 14:49:53,627 Epoch 1515: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-04 14:49:53,628 EPOCH 1516
2024-02-04 14:50:04,442 Epoch 1516: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-04 14:50:04,443 EPOCH 1517
2024-02-04 14:50:15,266 Epoch 1517: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-04 14:50:15,266 EPOCH 1518
2024-02-04 14:50:22,016 [Epoch: 1518 Step: 00025800] Batch Recognition Loss:   0.000140 => Gls Tokens per Sec:     1043 || Batch Translation Loss:   0.010533 => Txt Tokens per Sec:     3031 || Lr: 0.000100
2024-02-04 14:50:25,704 Epoch 1518: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-04 14:50:25,704 EPOCH 1519
2024-02-04 14:50:36,393 Epoch 1519: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-04 14:50:36,393 EPOCH 1520
2024-02-04 14:50:47,254 Epoch 1520: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-04 14:50:47,254 EPOCH 1521
2024-02-04 14:50:57,961 Epoch 1521: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-04 14:50:57,962 EPOCH 1522
2024-02-04 14:51:08,792 Epoch 1522: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-04 14:51:08,792 EPOCH 1523
2024-02-04 14:51:19,402 Epoch 1523: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-04 14:51:19,403 EPOCH 1524
2024-02-04 14:51:24,456 [Epoch: 1524 Step: 00025900] Batch Recognition Loss:   0.000160 => Gls Tokens per Sec:     1140 || Batch Translation Loss:   0.015459 => Txt Tokens per Sec:     3212 || Lr: 0.000100
2024-02-04 14:51:30,022 Epoch 1524: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-04 14:51:30,022 EPOCH 1525
2024-02-04 14:51:41,114 Epoch 1525: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-04 14:51:41,114 EPOCH 1526
2024-02-04 14:51:51,891 Epoch 1526: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-04 14:51:51,892 EPOCH 1527
2024-02-04 14:52:02,601 Epoch 1527: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-04 14:52:02,602 EPOCH 1528
2024-02-04 14:52:13,359 Epoch 1528: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-04 14:52:13,360 EPOCH 1529
2024-02-04 14:52:24,088 Epoch 1529: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-04 14:52:24,088 EPOCH 1530
2024-02-04 14:52:27,955 [Epoch: 1530 Step: 00026000] Batch Recognition Loss:   0.000172 => Gls Tokens per Sec:     1094 || Batch Translation Loss:   0.011250 => Txt Tokens per Sec:     2994 || Lr: 0.000100
2024-02-04 14:53:07,194 Validation result at epoch 1530, step    26000: duration: 39.2373s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00032	Translation Loss: 94322.03906	PPL: 12566.77051
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.86	(BLEU-1: 10.99,	BLEU-2: 3.47,	BLEU-3: 1.52,	BLEU-4: 0.86)
	CHRF 17.47	ROUGE 9.32
2024-02-04 14:53:07,196 Logging Recognition and Translation Outputs
2024-02-04 14:53:07,196 ========================================================================================================================
2024-02-04 14:53:07,196 Logging Sequence: 130_139.00
2024-02-04 14:53:07,197 	Gloss Reference :	A B+C+D+E
2024-02-04 14:53:07,197 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 14:53:07,197 	Gloss Alignment :	         
2024-02-04 14:53:07,197 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 14:53:07,200 	Text Reference  :	he shared a picture of     a little pouch he knit for his olympic gold medal with uk   flag on one side     and **** japanese flag on      the     other
2024-02-04 14:53:07,200 	Text Hypothesis :	he ****** * ******* played a ****** ***** ** **** *** *** ******* **** diver and  took part in the audience and 2016 rio      de   janeiro olympic games
2024-02-04 14:53:07,200 	Text Alignment  :	   D      D D       S        D      D     D  D    D   D   D       D    S     S    S    S    S  S   S            I    S        S    S       S       S    
2024-02-04 14:53:07,200 ========================================================================================================================
2024-02-04 14:53:07,200 Logging Sequence: 72_194.00
2024-02-04 14:53:07,201 	Gloss Reference :	A B+C+D+E
2024-02-04 14:53:07,201 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 14:53:07,201 	Gloss Alignment :	         
2024-02-04 14:53:07,201 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 14:53:07,203 	Text Reference  :	** shah told her      to **** do  what she    wants and filed a   police complaint against her    
2024-02-04 14:53:07,203 	Text Hypothesis :	as they were supposed to book the same colony he    was from  the video  of        9       minutes
2024-02-04 14:53:07,203 	Text Alignment  :	I  S    S    S           I    S   S    S      S     S   S     S   S      S         S       S      
2024-02-04 14:53:07,203 ========================================================================================================================
2024-02-04 14:53:07,203 Logging Sequence: 69_177.00
2024-02-04 14:53:07,204 	Gloss Reference :	A B+C+D+E
2024-02-04 14:53:07,204 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 14:53:07,204 	Gloss Alignment :	         
2024-02-04 14:53:07,204 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 14:53:07,205 	Text Reference  :	he said 'i will continue playing i know it's about time i   retire   i  also have a   knee condition
2024-02-04 14:53:07,206 	Text Hypothesis :	** **** ** **** ******** ******* * **** **** it    was  not selected to play the  ipl next season   
2024-02-04 14:53:07,206 	Text Alignment  :	D  D    D  D    D        D       D D    D    S     S    S   S        S  S    S    S   S    S        
2024-02-04 14:53:07,206 ========================================================================================================================
2024-02-04 14:53:07,206 Logging Sequence: 95_118.00
2024-02-04 14:53:07,206 	Gloss Reference :	A B+C+D+E
2024-02-04 14:53:07,206 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 14:53:07,206 	Gloss Alignment :	         
2024-02-04 14:53:07,207 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 14:53:07,207 	Text Reference  :	**** **** the      game    was stopped strangely due to   excessive sunlight
2024-02-04 14:53:07,208 	Text Hypothesis :	have been shocking ronaldo for the     pitch     and play a         lot     
2024-02-04 14:53:07,208 	Text Alignment  :	I    I    S        S       S   S       S         S   S    S         S       
2024-02-04 14:53:07,208 ========================================================================================================================
2024-02-04 14:53:07,208 Logging Sequence: 112_8.00
2024-02-04 14:53:07,208 	Gloss Reference :	A B+C+D+E
2024-02-04 14:53:07,208 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 14:53:07,208 	Gloss Alignment :	         
2024-02-04 14:53:07,208 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 14:53:07,211 	Text Reference  :	before there were 8 teams such as      mumbai indians delhi capitals punjab kings etc  and      now  there   will be 10   teams in 2022
2024-02-04 14:53:07,211 	Text Hypothesis :	****** ***** **** * ***** **** however the    bcci    has   been     made   the   same narendra modi stadium will ** home for   3  days
2024-02-04 14:53:07,211 	Text Alignment  :	D      D     D    D D     D    S       S      S       S     S        S      S     S    S        S    S            D  S    S     S  S   
2024-02-04 14:53:07,211 ========================================================================================================================
2024-02-04 14:53:14,607 Epoch 1530: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-04 14:53:14,608 EPOCH 1531
2024-02-04 14:53:26,552 Epoch 1531: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-04 14:53:26,553 EPOCH 1532
2024-02-04 14:53:37,537 Epoch 1532: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-04 14:53:37,537 EPOCH 1533
2024-02-04 14:53:48,442 Epoch 1533: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-04 14:53:48,442 EPOCH 1534
2024-02-04 14:53:59,107 Epoch 1534: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.48 
2024-02-04 14:53:59,107 EPOCH 1535
2024-02-04 14:54:10,345 Epoch 1535: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-04 14:54:10,346 EPOCH 1536
2024-02-04 14:54:13,203 [Epoch: 1536 Step: 00026100] Batch Recognition Loss:   0.000183 => Gls Tokens per Sec:     1120 || Batch Translation Loss:   0.015612 => Txt Tokens per Sec:     3164 || Lr: 0.000100
2024-02-04 14:54:21,735 Epoch 1536: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-04 14:54:21,736 EPOCH 1537
2024-02-04 14:54:33,158 Epoch 1537: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-04 14:54:33,158 EPOCH 1538
2024-02-04 14:54:44,505 Epoch 1538: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-04 14:54:44,506 EPOCH 1539
2024-02-04 14:54:55,280 Epoch 1539: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-04 14:54:55,280 EPOCH 1540
2024-02-04 14:55:06,184 Epoch 1540: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-04 14:55:06,185 EPOCH 1541
2024-02-04 14:55:17,214 Epoch 1541: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-04 14:55:17,216 EPOCH 1542
2024-02-04 14:55:17,777 [Epoch: 1542 Step: 00026200] Batch Recognition Loss:   0.000135 => Gls Tokens per Sec:     3428 || Batch Translation Loss:   0.008529 => Txt Tokens per Sec:     7919 || Lr: 0.000100
2024-02-04 14:55:28,446 Epoch 1542: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.70 
2024-02-04 14:55:28,446 EPOCH 1543
2024-02-04 14:55:40,402 Epoch 1543: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.56 
2024-02-04 14:55:40,403 EPOCH 1544
2024-02-04 14:55:51,518 Epoch 1544: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.63 
2024-02-04 14:55:51,518 EPOCH 1545
2024-02-04 14:56:02,246 Epoch 1545: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.09 
2024-02-04 14:56:02,246 EPOCH 1546
2024-02-04 14:58:09,201 Epoch 1546: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.99 
2024-02-04 14:58:09,203 EPOCH 1547
2024-02-04 14:58:21,336 Epoch 1547: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.51 
2024-02-04 14:58:21,336 EPOCH 1548
2024-02-04 14:58:21,485 [Epoch: 1548 Step: 00026300] Batch Recognition Loss:   0.000177 => Gls Tokens per Sec:     4308 || Batch Translation Loss:   0.030705 => Txt Tokens per Sec:     9600 || Lr: 0.000100
2024-02-04 14:58:32,243 Epoch 1548: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.27 
2024-02-04 14:58:32,244 EPOCH 1549
2024-02-04 14:58:43,253 Epoch 1549: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.46 
2024-02-04 14:58:43,253 EPOCH 1550
2024-02-04 14:58:54,703 Epoch 1550: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.45 
2024-02-04 14:58:54,703 EPOCH 1551
2024-02-04 14:59:06,070 Epoch 1551: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.31 
2024-02-04 14:59:06,070 EPOCH 1552
2024-02-04 14:59:17,047 Epoch 1552: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.17 
2024-02-04 14:59:17,047 EPOCH 1553
2024-02-04 14:59:27,973 [Epoch: 1553 Step: 00026400] Batch Recognition Loss:   0.000367 => Gls Tokens per Sec:      915 || Batch Translation Loss:   0.052980 => Txt Tokens per Sec:     2570 || Lr: 0.000100
2024-02-04 14:59:28,113 Epoch 1553: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.30 
2024-02-04 14:59:28,113 EPOCH 1554
2024-02-04 14:59:39,093 Epoch 1554: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.94 
2024-02-04 14:59:39,094 EPOCH 1555
2024-02-04 14:59:50,230 Epoch 1555: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.97 
2024-02-04 14:59:50,231 EPOCH 1556
2024-02-04 15:00:01,509 Epoch 1556: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.82 
2024-02-04 15:00:01,510 EPOCH 1557
2024-02-04 15:49:12,403 Epoch 1557: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.75 
2024-02-04 15:49:12,404 EPOCH 1558
2024-02-04 15:49:25,200 Epoch 1558: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.61 
2024-02-04 15:49:25,200 EPOCH 1559
2024-02-04 15:49:37,469 [Epoch: 1559 Step: 00026500] Batch Recognition Loss:   0.000205 => Gls Tokens per Sec:      710 || Batch Translation Loss:   0.024428 => Txt Tokens per Sec:     1971 || Lr: 0.000100
2024-02-04 15:49:38,326 Epoch 1559: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.59 
2024-02-04 15:49:38,326 EPOCH 1560
2024-02-04 15:49:52,703 Epoch 1560: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.66 
2024-02-04 15:49:52,704 EPOCH 1561
2024-02-04 15:50:05,288 Epoch 1561: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.90 
2024-02-04 15:50:05,289 EPOCH 1562
2024-02-04 15:50:17,698 Epoch 1562: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.64 
2024-02-04 15:50:17,699 EPOCH 1563
2024-02-04 15:50:29,878 Epoch 1563: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.72 
2024-02-04 15:50:29,880 EPOCH 1564
2024-02-04 15:50:41,645 Epoch 1564: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-04 15:50:41,646 EPOCH 1565
2024-02-04 15:50:50,559 [Epoch: 1565 Step: 00026600] Batch Recognition Loss:   0.000217 => Gls Tokens per Sec:      834 || Batch Translation Loss:   0.021569 => Txt Tokens per Sec:     2314 || Lr: 0.000100
2024-02-04 15:50:53,362 Epoch 1565: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-04 15:50:53,362 EPOCH 1566
2024-02-04 15:51:05,086 Epoch 1566: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-04 15:51:05,087 EPOCH 1567
2024-02-04 15:51:16,817 Epoch 1567: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-04 15:51:16,818 EPOCH 1568
2024-02-04 15:51:28,838 Epoch 1568: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-04 15:51:28,840 EPOCH 1569
2024-02-04 15:51:40,599 Epoch 1569: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-04 15:51:40,600 EPOCH 1570
2024-02-04 15:51:52,505 Epoch 1570: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.64 
2024-02-04 15:51:52,506 EPOCH 1571
