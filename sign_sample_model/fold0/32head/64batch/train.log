2024-02-02 17:54:44,420 Hello! This is Joey-NMT.
2024-02-02 17:54:44,436 Total params: 25639944
2024-02-02 17:54:44,437 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'gloss_output_layer.bias', 'gloss_output_layer.weight', 'sgn_embed.ln.bias', 'sgn_embed.ln.weight', 'sgn_embed.norm.norm.bias', 'sgn_embed.norm.norm.weight', 'txt_embed.norm.norm.bias', 'txt_embed.norm.norm.weight']
2024-02-02 17:54:45,631 cfg.name                           : sign_experiment
2024-02-02 17:54:45,631 cfg.data.data_path                 : ./data/Sports_dataset/0/
2024-02-02 17:54:45,631 cfg.data.version                   : phoenix_2014_trans
2024-02-02 17:54:45,631 cfg.data.sgn                       : sign
2024-02-02 17:54:45,631 cfg.data.txt                       : text
2024-02-02 17:54:45,631 cfg.data.gls                       : gloss
2024-02-02 17:54:45,631 cfg.data.train                     : excel_data.train
2024-02-02 17:54:45,632 cfg.data.dev                       : excel_data.dev
2024-02-02 17:54:45,632 cfg.data.test                      : excel_data.test
2024-02-02 17:54:45,632 cfg.data.feature_size              : 2560
2024-02-02 17:54:45,632 cfg.data.level                     : word
2024-02-02 17:54:45,632 cfg.data.txt_lowercase             : True
2024-02-02 17:54:45,632 cfg.data.max_sent_length           : 500
2024-02-02 17:54:45,632 cfg.data.random_train_subset       : -1
2024-02-02 17:54:45,632 cfg.data.random_dev_subset         : -1
2024-02-02 17:54:45,632 cfg.testing.recognition_beam_sizes : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
2024-02-02 17:54:45,633 cfg.testing.translation_beam_sizes : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
2024-02-02 17:54:45,633 cfg.testing.translation_beam_alphas : [-1, 0, 1, 2, 3, 4, 5]
2024-02-02 17:54:45,633 cfg.training.reset_best_ckpt       : False
2024-02-02 17:54:45,633 cfg.training.reset_scheduler       : False
2024-02-02 17:54:45,633 cfg.training.reset_optimizer       : False
2024-02-02 17:54:45,633 cfg.training.random_seed           : 42
2024-02-02 17:54:45,633 cfg.training.model_dir             : ./sign_sample_model/fold0/32head/64batch
2024-02-02 17:54:45,633 cfg.training.recognition_loss_weight : 1.0
2024-02-02 17:54:45,634 cfg.training.translation_loss_weight : 1.0
2024-02-02 17:54:45,634 cfg.training.eval_metric           : bleu
2024-02-02 17:54:45,634 cfg.training.optimizer             : adam
2024-02-02 17:54:45,634 cfg.training.learning_rate         : 0.0001
2024-02-02 17:54:45,634 cfg.training.batch_size            : 64
2024-02-02 17:54:45,634 cfg.training.num_valid_log         : 5
2024-02-02 17:54:45,634 cfg.training.epochs                : 50000
2024-02-02 17:54:45,634 cfg.training.early_stopping_metric : eval_metric
2024-02-02 17:54:45,634 cfg.training.batch_type            : sentence
2024-02-02 17:54:45,635 cfg.training.translation_normalization : batch
2024-02-02 17:54:45,635 cfg.training.eval_recognition_beam_size : 1
2024-02-02 17:54:45,635 cfg.training.eval_translation_beam_size : 1
2024-02-02 17:54:45,635 cfg.training.eval_translation_beam_alpha : -1
2024-02-02 17:54:45,635 cfg.training.overwrite             : True
2024-02-02 17:54:45,635 cfg.training.shuffle               : True
2024-02-02 17:54:45,635 cfg.training.use_cuda              : True
2024-02-02 17:54:45,635 cfg.training.translation_max_output_length : 40
2024-02-02 17:54:45,635 cfg.training.keep_last_ckpts       : 1
2024-02-02 17:54:45,636 cfg.training.batch_multiplier      : 1
2024-02-02 17:54:45,636 cfg.training.logging_freq          : 100
2024-02-02 17:54:45,636 cfg.training.validation_freq       : 2000
2024-02-02 17:54:45,636 cfg.training.betas                 : [0.9, 0.998]
2024-02-02 17:54:45,636 cfg.training.scheduling            : plateau
2024-02-02 17:54:45,636 cfg.training.learning_rate_min     : 1e-08
2024-02-02 17:54:45,636 cfg.training.weight_decay          : 0.0001
2024-02-02 17:54:45,636 cfg.training.patience              : 12
2024-02-02 17:54:45,636 cfg.training.decrease_factor       : 0.5
2024-02-02 17:54:45,637 cfg.training.label_smoothing       : 0.0
2024-02-02 17:54:45,637 cfg.model.initializer              : xavier
2024-02-02 17:54:45,637 cfg.model.bias_initializer         : zeros
2024-02-02 17:54:45,637 cfg.model.init_gain                : 1.0
2024-02-02 17:54:45,637 cfg.model.embed_initializer        : xavier
2024-02-02 17:54:45,637 cfg.model.embed_init_gain          : 1.0
2024-02-02 17:54:45,637 cfg.model.tied_softmax             : True
2024-02-02 17:54:45,637 cfg.model.encoder.type             : transformer
2024-02-02 17:54:45,637 cfg.model.encoder.num_layers       : 3
2024-02-02 17:54:45,638 cfg.model.encoder.num_heads        : 32
2024-02-02 17:54:45,638 cfg.model.encoder.embeddings.embedding_dim : 512
2024-02-02 17:54:45,638 cfg.model.encoder.embeddings.scale : False
2024-02-02 17:54:45,638 cfg.model.encoder.embeddings.dropout : 0.1
2024-02-02 17:54:45,638 cfg.model.encoder.embeddings.norm_type : batch
2024-02-02 17:54:45,638 cfg.model.encoder.embeddings.activation_type : softsign
2024-02-02 17:54:45,638 cfg.model.encoder.hidden_size      : 512
2024-02-02 17:54:45,638 cfg.model.encoder.ff_size          : 2048
2024-02-02 17:54:45,638 cfg.model.encoder.dropout          : 0.1
2024-02-02 17:54:45,639 cfg.model.decoder.type             : transformer
2024-02-02 17:54:45,639 cfg.model.decoder.num_layers       : 3
2024-02-02 17:54:45,639 cfg.model.decoder.num_heads        : 32
2024-02-02 17:54:45,639 cfg.model.decoder.embeddings.embedding_dim : 512
2024-02-02 17:54:45,639 cfg.model.decoder.embeddings.scale : False
2024-02-02 17:54:45,639 cfg.model.decoder.embeddings.dropout : 0.1
2024-02-02 17:54:45,639 cfg.model.decoder.embeddings.norm_type : batch
2024-02-02 17:54:45,639 cfg.model.decoder.embeddings.activation_type : softsign
2024-02-02 17:54:45,639 cfg.model.decoder.hidden_size      : 512
2024-02-02 17:54:45,640 cfg.model.decoder.ff_size          : 2048
2024-02-02 17:54:45,640 cfg.model.decoder.dropout          : 0.1
2024-02-02 17:54:45,640 Data set sizes: 
	train 2126,
	valid 708,
	test 706
2024-02-02 17:54:45,640 First training example:
	[GLS] A B C D E
	[TXT] although new zealand was disappointed to faltered at the finals against australia they did well throughout the tournament
2024-02-02 17:54:45,640 First 10 words (gls): (0) <si> (1) <unk> (2) <pad> (3) A (4) B (5) C (6) D (7) E
2024-02-02 17:54:45,640 First 10 words (txt): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) the (5) and (6) to (7) a (8) in (9) of
2024-02-02 17:54:45,640 Number of unique glosses (types): 8
2024-02-02 17:54:45,640 Number of unique words (types): 4397
2024-02-02 17:54:45,641 SignModel(
	encoder=TransformerEncoder(num_layers=3, num_heads=32),
	decoder=TransformerDecoder(num_layers=3, num_heads=32),
	sgn_embed=SpatialEmbeddings(embedding_dim=512, input_size=2560),
	txt_embed=Embeddings(embedding_dim=512, vocab_size=4397))
2024-02-02 17:54:45,652 EPOCH 1
2024-02-02 17:54:51,336 Epoch   1: Total Training Recognition Loss 147.87  Total Training Translation Loss 3460.04 
2024-02-02 17:54:51,336 EPOCH 2
2024-02-02 17:54:55,338 Epoch   2: Total Training Recognition Loss 21.36  Total Training Translation Loss 3099.74 
2024-02-02 17:54:55,338 EPOCH 3
2024-02-02 17:54:59,262 [Epoch: 003 Step: 00000100] Batch Recognition Loss:   0.104173 => Gls Tokens per Sec:     2547 || Batch Translation Loss:  66.278923 => Txt Tokens per Sec:     7029 || Lr: 0.000100
2024-02-02 17:54:59,575 Epoch   3: Total Training Recognition Loss 5.65  Total Training Translation Loss 3014.45 
2024-02-02 17:54:59,576 EPOCH 4
2024-02-02 17:55:04,416 Epoch   4: Total Training Recognition Loss 1.52  Total Training Translation Loss 2980.93 
2024-02-02 17:55:04,416 EPOCH 5
2024-02-02 17:55:09,243 Epoch   5: Total Training Recognition Loss 0.72  Total Training Translation Loss 2927.88 
2024-02-02 17:55:09,244 EPOCH 6
2024-02-02 17:55:13,025 [Epoch: 006 Step: 00000200] Batch Recognition Loss:   0.008316 => Gls Tokens per Sec:     2474 || Batch Translation Loss:  77.712624 => Txt Tokens per Sec:     6901 || Lr: 0.000100
2024-02-02 17:55:13,494 Epoch   6: Total Training Recognition Loss 0.45  Total Training Translation Loss 2848.58 
2024-02-02 17:55:13,495 EPOCH 7
2024-02-02 17:55:18,237 Epoch   7: Total Training Recognition Loss 0.35  Total Training Translation Loss 2763.02 
2024-02-02 17:55:18,237 EPOCH 8
2024-02-02 17:55:22,567 Epoch   8: Total Training Recognition Loss 0.32  Total Training Translation Loss 2695.15 
2024-02-02 17:55:22,567 EPOCH 9
2024-02-02 17:55:26,453 [Epoch: 009 Step: 00000300] Batch Recognition Loss:   0.009363 => Gls Tokens per Sec:     2306 || Batch Translation Loss:  48.849171 => Txt Tokens per Sec:     6327 || Lr: 0.000100
2024-02-02 17:55:27,522 Epoch   9: Total Training Recognition Loss 0.28  Total Training Translation Loss 2600.96 
2024-02-02 17:55:27,522 EPOCH 10
2024-02-02 17:55:31,869 Epoch  10: Total Training Recognition Loss 0.27  Total Training Translation Loss 2521.89 
2024-02-02 17:55:31,869 EPOCH 11
2024-02-02 17:55:36,701 Epoch  11: Total Training Recognition Loss 0.26  Total Training Translation Loss 2434.19 
2024-02-02 17:55:36,701 EPOCH 12
2024-02-02 17:55:39,689 [Epoch: 012 Step: 00000400] Batch Recognition Loss:   0.005877 => Gls Tokens per Sec:     2701 || Batch Translation Loss:  65.686958 => Txt Tokens per Sec:     7350 || Lr: 0.000100
2024-02-02 17:55:40,945 Epoch  12: Total Training Recognition Loss 0.24  Total Training Translation Loss 2363.63 
2024-02-02 17:55:40,945 EPOCH 13
2024-02-02 17:55:45,832 Epoch  13: Total Training Recognition Loss 0.24  Total Training Translation Loss 2288.89 
2024-02-02 17:55:45,833 EPOCH 14
2024-02-02 17:55:50,115 Epoch  14: Total Training Recognition Loss 0.27  Total Training Translation Loss 2214.64 
2024-02-02 17:55:50,115 EPOCH 15
2024-02-02 17:55:53,512 [Epoch: 015 Step: 00000500] Batch Recognition Loss:   0.005315 => Gls Tokens per Sec:     2262 || Batch Translation Loss:  56.101425 => Txt Tokens per Sec:     6194 || Lr: 0.000100
2024-02-02 17:55:55,021 Epoch  15: Total Training Recognition Loss 0.28  Total Training Translation Loss 2144.75 
2024-02-02 17:55:55,021 EPOCH 16
2024-02-02 17:55:59,224 Epoch  16: Total Training Recognition Loss 0.25  Total Training Translation Loss 2087.27 
2024-02-02 17:55:59,224 EPOCH 17
2024-02-02 17:56:04,015 Epoch  17: Total Training Recognition Loss 0.33  Total Training Translation Loss 2023.11 
2024-02-02 17:56:04,016 EPOCH 18
2024-02-02 17:56:07,034 [Epoch: 018 Step: 00000600] Batch Recognition Loss:   0.018723 => Gls Tokens per Sec:     2252 || Batch Translation Loss:  81.908287 => Txt Tokens per Sec:     6444 || Lr: 0.000100
2024-02-02 17:56:08,680 Epoch  18: Total Training Recognition Loss 0.35  Total Training Translation Loss 1972.92 
2024-02-02 17:56:08,681 EPOCH 19
2024-02-02 17:56:13,360 Epoch  19: Total Training Recognition Loss 0.36  Total Training Translation Loss 1903.10 
2024-02-02 17:56:13,361 EPOCH 20
2024-02-02 17:56:17,931 Epoch  20: Total Training Recognition Loss 0.38  Total Training Translation Loss 1843.85 
2024-02-02 17:56:17,932 EPOCH 21
2024-02-02 17:56:20,634 [Epoch: 021 Step: 00000700] Batch Recognition Loss:   0.010045 => Gls Tokens per Sec:     2277 || Batch Translation Loss:  38.808155 => Txt Tokens per Sec:     6521 || Lr: 0.000100
2024-02-02 17:56:22,406 Epoch  21: Total Training Recognition Loss 0.40  Total Training Translation Loss 1782.81 
2024-02-02 17:56:22,407 EPOCH 22
2024-02-02 17:56:27,074 Epoch  22: Total Training Recognition Loss 0.43  Total Training Translation Loss 1733.28 
2024-02-02 17:56:27,074 EPOCH 23
2024-02-02 17:56:31,546 Epoch  23: Total Training Recognition Loss 0.43  Total Training Translation Loss 1682.19 
2024-02-02 17:56:31,546 EPOCH 24
2024-02-02 17:56:34,082 [Epoch: 024 Step: 00000800] Batch Recognition Loss:   0.005965 => Gls Tokens per Sec:     2174 || Batch Translation Loss:  51.508545 => Txt Tokens per Sec:     6013 || Lr: 0.000100
2024-02-02 17:56:36,316 Epoch  24: Total Training Recognition Loss 0.47  Total Training Translation Loss 1620.23 
2024-02-02 17:56:36,316 EPOCH 25
2024-02-02 17:56:40,686 Epoch  25: Total Training Recognition Loss 0.50  Total Training Translation Loss 1562.56 
2024-02-02 17:56:40,687 EPOCH 26
2024-02-02 17:56:45,422 Epoch  26: Total Training Recognition Loss 0.54  Total Training Translation Loss 1518.01 
2024-02-02 17:56:45,422 EPOCH 27
2024-02-02 17:56:47,493 [Epoch: 027 Step: 00000900] Batch Recognition Loss:   0.013489 => Gls Tokens per Sec:     2353 || Batch Translation Loss:  42.443436 => Txt Tokens per Sec:     6631 || Lr: 0.000100
2024-02-02 17:56:49,799 Epoch  27: Total Training Recognition Loss 0.53  Total Training Translation Loss 1473.98 
2024-02-02 17:56:49,799 EPOCH 28
2024-02-02 17:56:54,674 Epoch  28: Total Training Recognition Loss 0.55  Total Training Translation Loss 1426.44 
2024-02-02 17:56:54,674 EPOCH 29
2024-02-02 17:56:59,003 Epoch  29: Total Training Recognition Loss 0.55  Total Training Translation Loss 1370.52 
2024-02-02 17:56:59,003 EPOCH 30
2024-02-02 17:57:00,933 [Epoch: 030 Step: 00001000] Batch Recognition Loss:   0.023104 => Gls Tokens per Sec:     2323 || Batch Translation Loss:  16.354052 => Txt Tokens per Sec:     6239 || Lr: 0.000100
2024-02-02 17:57:03,875 Epoch  30: Total Training Recognition Loss 0.63  Total Training Translation Loss 1313.77 
2024-02-02 17:57:03,875 EPOCH 31
2024-02-02 17:57:08,132 Epoch  31: Total Training Recognition Loss 0.59  Total Training Translation Loss 1265.50 
2024-02-02 17:57:08,132 EPOCH 32
2024-02-02 17:57:13,019 Epoch  32: Total Training Recognition Loss 0.64  Total Training Translation Loss 1217.81 
2024-02-02 17:57:13,019 EPOCH 33
2024-02-02 17:57:14,316 [Epoch: 033 Step: 00001100] Batch Recognition Loss:   0.020683 => Gls Tokens per Sec:     2770 || Batch Translation Loss:  46.974258 => Txt Tokens per Sec:     7429 || Lr: 0.000100
2024-02-02 17:57:17,321 Epoch  33: Total Training Recognition Loss 0.62  Total Training Translation Loss 1172.79 
2024-02-02 17:57:17,321 EPOCH 34
2024-02-02 17:57:22,230 Epoch  34: Total Training Recognition Loss 0.65  Total Training Translation Loss 1133.89 
2024-02-02 17:57:22,230 EPOCH 35
2024-02-02 17:57:26,503 Epoch  35: Total Training Recognition Loss 0.74  Total Training Translation Loss 1100.27 
2024-02-02 17:57:26,504 EPOCH 36
2024-02-02 17:57:28,175 [Epoch: 036 Step: 00001200] Batch Recognition Loss:   0.025542 => Gls Tokens per Sec:     1768 || Batch Translation Loss:  37.407822 => Txt Tokens per Sec:     5034 || Lr: 0.000100
2024-02-02 17:57:31,402 Epoch  36: Total Training Recognition Loss 0.70  Total Training Translation Loss 1057.16 
2024-02-02 17:57:31,402 EPOCH 37
2024-02-02 17:57:36,277 Epoch  37: Total Training Recognition Loss 0.64  Total Training Translation Loss 999.14 
2024-02-02 17:57:36,278 EPOCH 38
2024-02-02 17:57:40,493 Epoch  38: Total Training Recognition Loss 0.66  Total Training Translation Loss 955.14 
2024-02-02 17:57:40,493 EPOCH 39
2024-02-02 17:57:41,399 [Epoch: 039 Step: 00001300] Batch Recognition Loss:   0.017941 => Gls Tokens per Sec:     2831 || Batch Translation Loss:  26.978214 => Txt Tokens per Sec:     7680 || Lr: 0.000100
2024-02-02 17:57:45,217 Epoch  39: Total Training Recognition Loss 0.66  Total Training Translation Loss 910.41 
2024-02-02 17:57:45,217 EPOCH 40
2024-02-02 17:57:49,627 Epoch  40: Total Training Recognition Loss 0.74  Total Training Translation Loss 897.68 
2024-02-02 17:57:49,627 EPOCH 41
2024-02-02 17:57:54,496 Epoch  41: Total Training Recognition Loss 0.69  Total Training Translation Loss 840.61 
2024-02-02 17:57:54,497 EPOCH 42
2024-02-02 17:57:55,497 [Epoch: 042 Step: 00001400] Batch Recognition Loss:   0.016090 => Gls Tokens per Sec:     1920 || Batch Translation Loss:  17.032543 => Txt Tokens per Sec:     4877 || Lr: 0.000100
2024-02-02 17:57:58,848 Epoch  42: Total Training Recognition Loss 0.67  Total Training Translation Loss 790.03 
2024-02-02 17:57:58,848 EPOCH 43
2024-02-02 17:58:03,809 Epoch  43: Total Training Recognition Loss 0.64  Total Training Translation Loss 751.79 
2024-02-02 17:58:03,809 EPOCH 44
2024-02-02 17:58:08,113 Epoch  44: Total Training Recognition Loss 0.65  Total Training Translation Loss 725.11 
2024-02-02 17:58:08,113 EPOCH 45
2024-02-02 17:58:08,608 [Epoch: 045 Step: 00001500] Batch Recognition Loss:   0.029316 => Gls Tokens per Sec:     2085 || Batch Translation Loss:  23.276892 => Txt Tokens per Sec:     5540 || Lr: 0.000100
2024-02-02 17:58:12,641 Epoch  45: Total Training Recognition Loss 0.74  Total Training Translation Loss 693.76 
2024-02-02 17:58:12,642 EPOCH 46
2024-02-02 17:58:17,283 Epoch  46: Total Training Recognition Loss 0.67  Total Training Translation Loss 648.54 
2024-02-02 17:58:17,284 EPOCH 47
2024-02-02 17:58:22,163 Epoch  47: Total Training Recognition Loss 0.64  Total Training Translation Loss 618.99 
2024-02-02 17:58:22,164 EPOCH 48
2024-02-02 17:58:22,393 [Epoch: 048 Step: 00001600] Batch Recognition Loss:   0.018253 => Gls Tokens per Sec:     2795 || Batch Translation Loss:  13.151278 => Txt Tokens per Sec:     7638 || Lr: 0.000100
2024-02-02 17:58:26,446 Epoch  48: Total Training Recognition Loss 0.65  Total Training Translation Loss 586.77 
2024-02-02 17:58:26,447 EPOCH 49
2024-02-02 17:58:31,314 Epoch  49: Total Training Recognition Loss 0.64  Total Training Translation Loss 550.73 
2024-02-02 17:58:31,314 EPOCH 50
2024-02-02 17:58:35,395 [Epoch: 050 Step: 00001700] Batch Recognition Loss:   0.016756 => Gls Tokens per Sec:     2606 || Batch Translation Loss:  19.043316 => Txt Tokens per Sec:     7234 || Lr: 0.000100
2024-02-02 17:58:35,395 Epoch  50: Total Training Recognition Loss 0.61  Total Training Translation Loss 523.31 
2024-02-02 17:58:35,395 EPOCH 51
2024-02-02 17:58:39,451 Epoch  51: Total Training Recognition Loss 0.63  Total Training Translation Loss 503.84 
2024-02-02 17:58:39,451 EPOCH 52
2024-02-02 17:58:44,428 Epoch  52: Total Training Recognition Loss 0.59  Total Training Translation Loss 464.70 
2024-02-02 17:58:44,429 EPOCH 53
2024-02-02 17:58:48,500 [Epoch: 053 Step: 00001800] Batch Recognition Loss:   0.013881 => Gls Tokens per Sec:     2454 || Batch Translation Loss:   7.896895 => Txt Tokens per Sec:     6833 || Lr: 0.000100
2024-02-02 17:58:48,727 Epoch  53: Total Training Recognition Loss 0.59  Total Training Translation Loss 443.46 
2024-02-02 17:58:48,728 EPOCH 54
2024-02-02 17:58:53,617 Epoch  54: Total Training Recognition Loss 0.62  Total Training Translation Loss 422.43 
2024-02-02 17:58:53,618 EPOCH 55
2024-02-02 17:58:58,036 Epoch  55: Total Training Recognition Loss 0.59  Total Training Translation Loss 399.28 
2024-02-02 17:58:58,036 EPOCH 56
2024-02-02 17:59:02,443 [Epoch: 056 Step: 00001900] Batch Recognition Loss:   0.014878 => Gls Tokens per Sec:     2122 || Batch Translation Loss:  10.049990 => Txt Tokens per Sec:     6035 || Lr: 0.000100
2024-02-02 17:59:02,851 Epoch  56: Total Training Recognition Loss 0.56  Total Training Translation Loss 374.00 
2024-02-02 17:59:02,851 EPOCH 57
2024-02-02 17:59:07,289 Epoch  57: Total Training Recognition Loss 0.54  Total Training Translation Loss 351.18 
2024-02-02 17:59:07,290 EPOCH 58
2024-02-02 17:59:11,963 Epoch  58: Total Training Recognition Loss 0.53  Total Training Translation Loss 331.81 
2024-02-02 17:59:11,963 EPOCH 59
2024-02-02 17:59:15,359 [Epoch: 059 Step: 00002000] Batch Recognition Loss:   0.011722 => Gls Tokens per Sec:     2640 || Batch Translation Loss:  10.888443 => Txt Tokens per Sec:     7243 || Lr: 0.000100
2024-02-02 17:59:24,462 Hooray! New best validation result [eval_metric]!
2024-02-02 17:59:24,463 Saving new checkpoint.
2024-02-02 17:59:24,718 Validation result at epoch  59, step     2000: duration: 9.3586s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.04326	Translation Loss: 66587.10938	PPL: 783.19000
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.98	(BLEU-1: 13.32,	BLEU-2: 4.80,	BLEU-3: 2.02,	BLEU-4: 0.98)
	CHRF 17.56	ROUGE 11.25
2024-02-02 17:59:24,720 Logging Recognition and Translation Outputs
2024-02-02 17:59:24,720 ========================================================================================================================
2024-02-02 17:59:24,720 Logging Sequence: 182_115.00
2024-02-02 17:59:24,720 	Gloss Reference :	A B+C+D+E
2024-02-02 17:59:24,720 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 17:59:24,720 	Gloss Alignment :	         
2024-02-02 17:59:24,721 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 17:59:24,723 	Text Reference  :	fans are unclear whether yuvraj will  be   returning to    play  test match odi     or     in  t20   leagues from february 2022 
2024-02-02 17:59:24,723 	Text Hypothesis :	you  are ******* ******* ****** aware that lionel    messi messi made the   matches across the world cup     on   the      world
2024-02-02 17:59:24,723 	Text Alignment  :	S        D       D       D      S     S    S         S     S     S    S     S       S      S   S     S       S    S        S    
2024-02-02 17:59:24,723 ========================================================================================================================
2024-02-02 17:59:24,723 Logging Sequence: 140_120.00
2024-02-02 17:59:24,723 	Gloss Reference :	A B+C+D+E
2024-02-02 17:59:24,723 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 17:59:24,724 	Gloss Alignment :	         
2024-02-02 17:59:24,724 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 17:59:24,726 	Text Reference  :	but why so       it   is  because pant is      a   talented player and it    will help      encouraging the youth of uttarakhand toward sports
2024-02-02 17:59:24,726 	Text Hypothesis :	*** he  believed that his bag     will qualify for the      world  cup which is   currently at          the ***** ** most        loved  old   
2024-02-02 17:59:24,727 	Text Alignment  :	D   S   S        S    S   S       S    S       S   S        S      S   S     S    S         S               D     D  S           S      S     
2024-02-02 17:59:24,727 ========================================================================================================================
2024-02-02 17:59:24,727 Logging Sequence: 85_36.00
2024-02-02 17:59:24,727 	Gloss Reference :	A B+C+D+E
2024-02-02 17:59:24,727 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 17:59:24,727 	Gloss Alignment :	         
2024-02-02 17:59:24,727 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 17:59:24,728 	Text Reference  :	symonds has scored 2 centuries in 26   tests that he *** **** *** played for his country
2024-02-02 17:59:24,729 	Text Hypothesis :	******* *** ****** * i         am very sad   that he has made the game   for 8   crores 
2024-02-02 17:59:24,729 	Text Alignment  :	D       D   D      D S         S  S    S             I   I    I   S          S   S      
2024-02-02 17:59:24,729 ========================================================================================================================
2024-02-02 17:59:24,729 Logging Sequence: 164_100.00
2024-02-02 17:59:24,729 	Gloss Reference :	A B+C+D+E
2024-02-02 17:59:24,729 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 17:59:24,729 	Gloss Alignment :	         
2024-02-02 17:59:24,729 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 17:59:24,732 	Text Reference  :	* the   tv rights for broadcasting ipl  matches in  india  for the next 5 years went to star  india for   rs 23575 crore
2024-02-02 17:59:24,732 	Text Hypothesis :	a total of our    our indian       team beat    its rights for the **** * ***** **** ** world cup   match in the   world
2024-02-02 17:59:24,732 	Text Alignment  :	I S     S  S      S   S            S    S       S   S              D    D D     D    D  S     S     S     S  S     S    
2024-02-02 17:59:24,732 ========================================================================================================================
2024-02-02 17:59:24,732 Logging Sequence: 76_79.00
2024-02-02 17:59:24,732 	Gloss Reference :	A B+C+D+E
2024-02-02 17:59:24,733 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 17:59:24,733 	Gloss Alignment :	         
2024-02-02 17:59:24,733 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 17:59:24,734 	Text Reference  :	** ** ****** speaking to   ani csk   ceo kasi viswanathan said   
2024-02-02 17:59:24,734 	Text Hypothesis :	it is played by       this ipl there was a    huge        problem
2024-02-02 17:59:24,734 	Text Alignment  :	I  I  I      S        S    S   S     S   S    S           S      
2024-02-02 17:59:24,734 ========================================================================================================================
2024-02-02 17:59:25,754 Epoch  59: Total Training Recognition Loss 0.52  Total Training Translation Loss 305.33 
2024-02-02 17:59:25,754 EPOCH 60
2024-02-02 17:59:30,353 Epoch  60: Total Training Recognition Loss 0.49  Total Training Translation Loss 284.94 
2024-02-02 17:59:30,353 EPOCH 61
2024-02-02 17:59:34,722 Epoch  61: Total Training Recognition Loss 0.46  Total Training Translation Loss 268.17 
2024-02-02 17:59:34,723 EPOCH 62
2024-02-02 17:59:38,474 [Epoch: 062 Step: 00002100] Batch Recognition Loss:   0.011180 => Gls Tokens per Sec:     2152 || Batch Translation Loss:   5.218978 => Txt Tokens per Sec:     5900 || Lr: 0.000100
2024-02-02 17:59:39,515 Epoch  62: Total Training Recognition Loss 0.45  Total Training Translation Loss 251.62 
2024-02-02 17:59:39,516 EPOCH 63
2024-02-02 17:59:44,274 Epoch  63: Total Training Recognition Loss 0.43  Total Training Translation Loss 231.10 
2024-02-02 17:59:44,275 EPOCH 64
2024-02-02 17:59:48,929 Epoch  64: Total Training Recognition Loss 0.42  Total Training Translation Loss 219.79 
2024-02-02 17:59:48,930 EPOCH 65
2024-02-02 17:59:52,174 [Epoch: 065 Step: 00002200] Batch Recognition Loss:   0.014414 => Gls Tokens per Sec:     2291 || Batch Translation Loss:   1.075178 => Txt Tokens per Sec:     6606 || Lr: 0.000100
2024-02-02 17:59:53,399 Epoch  65: Total Training Recognition Loss 0.43  Total Training Translation Loss 215.92 
2024-02-02 17:59:53,399 EPOCH 66
2024-02-02 17:59:58,110 Epoch  66: Total Training Recognition Loss 0.42  Total Training Translation Loss 202.98 
2024-02-02 17:59:58,110 EPOCH 67
2024-02-02 18:00:03,049 Epoch  67: Total Training Recognition Loss 0.40  Total Training Translation Loss 184.86 
2024-02-02 18:00:03,050 EPOCH 68
2024-02-02 18:00:05,910 [Epoch: 068 Step: 00002300] Batch Recognition Loss:   0.010312 => Gls Tokens per Sec:     2374 || Batch Translation Loss:   3.398639 => Txt Tokens per Sec:     6644 || Lr: 0.000100
2024-02-02 18:00:07,301 Epoch  68: Total Training Recognition Loss 0.38  Total Training Translation Loss 168.56 
2024-02-02 18:00:07,302 EPOCH 69
2024-02-02 18:00:12,146 Epoch  69: Total Training Recognition Loss 0.36  Total Training Translation Loss 161.70 
2024-02-02 18:00:12,146 EPOCH 70
2024-02-02 18:00:16,292 Epoch  70: Total Training Recognition Loss 0.35  Total Training Translation Loss 151.09 
2024-02-02 18:00:16,292 EPOCH 71
2024-02-02 18:00:18,750 [Epoch: 071 Step: 00002400] Batch Recognition Loss:   0.010983 => Gls Tokens per Sec:     2504 || Batch Translation Loss:   5.572219 => Txt Tokens per Sec:     6902 || Lr: 0.000100
2024-02-02 18:00:20,931 Epoch  71: Total Training Recognition Loss 0.31  Total Training Translation Loss 140.83 
2024-02-02 18:00:20,931 EPOCH 72
2024-02-02 18:00:25,682 Epoch  72: Total Training Recognition Loss 0.33  Total Training Translation Loss 132.25 
2024-02-02 18:00:25,682 EPOCH 73
2024-02-02 18:00:30,447 Epoch  73: Total Training Recognition Loss 0.30  Total Training Translation Loss 123.12 
2024-02-02 18:00:30,447 EPOCH 74
2024-02-02 18:00:32,630 [Epoch: 074 Step: 00002500] Batch Recognition Loss:   0.006599 => Gls Tokens per Sec:     2526 || Batch Translation Loss:   3.404981 => Txt Tokens per Sec:     7054 || Lr: 0.000100
2024-02-02 18:00:34,885 Epoch  74: Total Training Recognition Loss 0.27  Total Training Translation Loss 117.62 
2024-02-02 18:00:34,886 EPOCH 75
2024-02-02 18:00:39,587 Epoch  75: Total Training Recognition Loss 0.30  Total Training Translation Loss 116.42 
2024-02-02 18:00:39,588 EPOCH 76
2024-02-02 18:00:44,104 Epoch  76: Total Training Recognition Loss 0.28  Total Training Translation Loss 107.27 
2024-02-02 18:00:44,105 EPOCH 77
2024-02-02 18:00:46,239 [Epoch: 077 Step: 00002600] Batch Recognition Loss:   0.005704 => Gls Tokens per Sec:     2284 || Batch Translation Loss:   2.567907 => Txt Tokens per Sec:     6239 || Lr: 0.000100
2024-02-02 18:00:48,744 Epoch  77: Total Training Recognition Loss 0.26  Total Training Translation Loss 98.74 
2024-02-02 18:00:48,744 EPOCH 78
2024-02-02 18:00:53,304 Epoch  78: Total Training Recognition Loss 0.24  Total Training Translation Loss 93.83 
2024-02-02 18:00:53,305 EPOCH 79
2024-02-02 18:00:57,856 Epoch  79: Total Training Recognition Loss 0.24  Total Training Translation Loss 90.16 
2024-02-02 18:00:57,856 EPOCH 80
2024-02-02 18:00:59,529 [Epoch: 080 Step: 00002700] Batch Recognition Loss:   0.006687 => Gls Tokens per Sec:     2529 || Batch Translation Loss:   2.113507 => Txt Tokens per Sec:     6837 || Lr: 0.000100
2024-02-02 18:01:02,476 Epoch  80: Total Training Recognition Loss 0.22  Total Training Translation Loss 83.68 
2024-02-02 18:01:02,476 EPOCH 81
2024-02-02 18:01:07,043 Epoch  81: Total Training Recognition Loss 0.22  Total Training Translation Loss 79.94 
2024-02-02 18:01:07,043 EPOCH 82
2024-02-02 18:01:11,707 Epoch  82: Total Training Recognition Loss 0.21  Total Training Translation Loss 76.63 
2024-02-02 18:01:11,708 EPOCH 83
2024-02-02 18:01:13,352 [Epoch: 083 Step: 00002800] Batch Recognition Loss:   0.007660 => Gls Tokens per Sec:     2337 || Batch Translation Loss:   2.648875 => Txt Tokens per Sec:     6726 || Lr: 0.000100
2024-02-02 18:01:16,184 Epoch  83: Total Training Recognition Loss 0.20  Total Training Translation Loss 72.95 
2024-02-02 18:01:16,185 EPOCH 84
2024-02-02 18:01:20,877 Epoch  84: Total Training Recognition Loss 0.19  Total Training Translation Loss 70.50 
2024-02-02 18:01:20,877 EPOCH 85
2024-02-02 18:01:25,339 Epoch  85: Total Training Recognition Loss 0.20  Total Training Translation Loss 69.65 
2024-02-02 18:01:25,339 EPOCH 86
2024-02-02 18:01:26,987 [Epoch: 086 Step: 00002900] Batch Recognition Loss:   0.004776 => Gls Tokens per Sec:     1943 || Batch Translation Loss:   1.678555 => Txt Tokens per Sec:     5835 || Lr: 0.000100
2024-02-02 18:01:30,107 Epoch  86: Total Training Recognition Loss 0.20  Total Training Translation Loss 66.65 
2024-02-02 18:01:30,108 EPOCH 87
2024-02-02 18:01:34,513 Epoch  87: Total Training Recognition Loss 0.17  Total Training Translation Loss 63.02 
2024-02-02 18:01:34,513 EPOCH 88
2024-02-02 18:01:39,267 Epoch  88: Total Training Recognition Loss 0.17  Total Training Translation Loss 58.70 
2024-02-02 18:01:39,267 EPOCH 89
2024-02-02 18:01:40,182 [Epoch: 089 Step: 00003000] Batch Recognition Loss:   0.003708 => Gls Tokens per Sec:     2529 || Batch Translation Loss:   1.910485 => Txt Tokens per Sec:     6601 || Lr: 0.000100
2024-02-02 18:01:43,622 Epoch  89: Total Training Recognition Loss 0.17  Total Training Translation Loss 56.40 
2024-02-02 18:01:43,622 EPOCH 90
2024-02-02 18:01:48,477 Epoch  90: Total Training Recognition Loss 0.17  Total Training Translation Loss 55.39 
2024-02-02 18:01:48,478 EPOCH 91
2024-02-02 18:01:52,912 Epoch  91: Total Training Recognition Loss 0.15  Total Training Translation Loss 52.31 
2024-02-02 18:01:52,912 EPOCH 92
2024-02-02 18:01:53,573 [Epoch: 092 Step: 00003100] Batch Recognition Loss:   0.004132 => Gls Tokens per Sec:     2914 || Batch Translation Loss:   1.670498 => Txt Tokens per Sec:     6837 || Lr: 0.000100
2024-02-02 18:01:57,793 Epoch  92: Total Training Recognition Loss 0.15  Total Training Translation Loss 50.41 
2024-02-02 18:01:57,794 EPOCH 93
2024-02-02 18:02:02,241 Epoch  93: Total Training Recognition Loss 0.15  Total Training Translation Loss 47.82 
2024-02-02 18:02:02,241 EPOCH 94
2024-02-02 18:02:06,938 Epoch  94: Total Training Recognition Loss 0.15  Total Training Translation Loss 45.58 
2024-02-02 18:02:06,939 EPOCH 95
2024-02-02 18:02:07,627 [Epoch: 095 Step: 00003200] Batch Recognition Loss:   0.003369 => Gls Tokens per Sec:     1863 || Batch Translation Loss:   1.428355 => Txt Tokens per Sec:     5856 || Lr: 0.000100
2024-02-02 18:02:11,549 Epoch  95: Total Training Recognition Loss 0.15  Total Training Translation Loss 45.98 
2024-02-02 18:02:11,550 EPOCH 96
2024-02-02 18:02:16,057 Epoch  96: Total Training Recognition Loss 0.15  Total Training Translation Loss 50.15 
2024-02-02 18:02:16,058 EPOCH 97
2024-02-02 18:02:20,684 Epoch  97: Total Training Recognition Loss 0.15  Total Training Translation Loss 46.44 
2024-02-02 18:02:20,685 EPOCH 98
2024-02-02 18:02:20,937 [Epoch: 098 Step: 00003300] Batch Recognition Loss:   0.003792 => Gls Tokens per Sec:     2550 || Batch Translation Loss:   1.110312 => Txt Tokens per Sec:     6968 || Lr: 0.000100
2024-02-02 18:02:25,179 Epoch  98: Total Training Recognition Loss 0.14  Total Training Translation Loss 43.37 
2024-02-02 18:02:25,179 EPOCH 99
2024-02-02 18:02:29,876 Epoch  99: Total Training Recognition Loss 0.13  Total Training Translation Loss 41.56 
2024-02-02 18:02:29,877 EPOCH 100
2024-02-02 18:02:34,346 [Epoch: 100 Step: 00003400] Batch Recognition Loss:   0.006435 => Gls Tokens per Sec:     2379 || Batch Translation Loss:   1.555442 => Txt Tokens per Sec:     6605 || Lr: 0.000100
2024-02-02 18:02:34,346 Epoch 100: Total Training Recognition Loss 0.13  Total Training Translation Loss 37.68 
2024-02-02 18:02:34,346 EPOCH 101
2024-02-02 18:02:38,621 Epoch 101: Total Training Recognition Loss 0.13  Total Training Translation Loss 36.70 
2024-02-02 18:02:38,622 EPOCH 102
2024-02-02 18:02:43,454 Epoch 102: Total Training Recognition Loss 0.12  Total Training Translation Loss 36.50 
2024-02-02 18:02:43,455 EPOCH 103
2024-02-02 18:02:47,959 [Epoch: 103 Step: 00003500] Batch Recognition Loss:   0.005017 => Gls Tokens per Sec:     2218 || Batch Translation Loss:   1.155789 => Txt Tokens per Sec:     6095 || Lr: 0.000100
2024-02-02 18:02:48,294 Epoch 103: Total Training Recognition Loss 0.11  Total Training Translation Loss 34.31 
2024-02-02 18:02:48,294 EPOCH 104
2024-02-02 18:02:52,663 Epoch 104: Total Training Recognition Loss 0.13  Total Training Translation Loss 33.99 
2024-02-02 18:02:52,664 EPOCH 105
2024-02-02 18:02:57,521 Epoch 105: Total Training Recognition Loss 0.12  Total Training Translation Loss 31.96 
2024-02-02 18:02:57,522 EPOCH 106
2024-02-02 18:03:01,148 [Epoch: 106 Step: 00003600] Batch Recognition Loss:   0.002102 => Gls Tokens per Sec:     2579 || Batch Translation Loss:   0.763301 => Txt Tokens per Sec:     7143 || Lr: 0.000100
2024-02-02 18:03:01,835 Epoch 106: Total Training Recognition Loss 0.12  Total Training Translation Loss 32.15 
2024-02-02 18:03:01,835 EPOCH 107
2024-02-02 18:03:06,871 Epoch 107: Total Training Recognition Loss 0.10  Total Training Translation Loss 28.88 
2024-02-02 18:03:06,871 EPOCH 108
2024-02-02 18:03:11,850 Epoch 108: Total Training Recognition Loss 0.11  Total Training Translation Loss 29.13 
2024-02-02 18:03:11,851 EPOCH 109
2024-02-02 18:03:15,672 [Epoch: 109 Step: 00003700] Batch Recognition Loss:   0.002151 => Gls Tokens per Sec:     2280 || Batch Translation Loss:   0.750402 => Txt Tokens per Sec:     6383 || Lr: 0.000100
2024-02-02 18:03:16,627 Epoch 109: Total Training Recognition Loss 0.09  Total Training Translation Loss 28.05 
2024-02-02 18:03:16,628 EPOCH 110
2024-02-02 18:03:21,138 Epoch 110: Total Training Recognition Loss 0.09  Total Training Translation Loss 28.05 
2024-02-02 18:03:21,138 EPOCH 111
2024-02-02 18:03:25,777 Epoch 111: Total Training Recognition Loss 0.10  Total Training Translation Loss 31.04 
2024-02-02 18:03:25,777 EPOCH 112
2024-02-02 18:03:29,455 [Epoch: 112 Step: 00003800] Batch Recognition Loss:   0.002336 => Gls Tokens per Sec:     2195 || Batch Translation Loss:   0.793339 => Txt Tokens per Sec:     6112 || Lr: 0.000100
2024-02-02 18:03:30,473 Epoch 112: Total Training Recognition Loss 0.09  Total Training Translation Loss 27.95 
2024-02-02 18:03:30,473 EPOCH 113
2024-02-02 18:03:34,893 Epoch 113: Total Training Recognition Loss 0.09  Total Training Translation Loss 25.25 
2024-02-02 18:03:34,893 EPOCH 114
2024-02-02 18:03:39,743 Epoch 114: Total Training Recognition Loss 0.09  Total Training Translation Loss 25.25 
2024-02-02 18:03:39,744 EPOCH 115
2024-02-02 18:03:42,987 [Epoch: 115 Step: 00003900] Batch Recognition Loss:   0.002340 => Gls Tokens per Sec:     2291 || Batch Translation Loss:   1.039244 => Txt Tokens per Sec:     6635 || Lr: 0.000100
2024-02-02 18:03:44,045 Epoch 115: Total Training Recognition Loss 0.08  Total Training Translation Loss 24.81 
2024-02-02 18:03:44,045 EPOCH 116
2024-02-02 18:03:48,940 Epoch 116: Total Training Recognition Loss 0.07  Total Training Translation Loss 24.34 
2024-02-02 18:03:48,941 EPOCH 117
2024-02-02 18:03:53,183 Epoch 117: Total Training Recognition Loss 0.08  Total Training Translation Loss 22.96 
2024-02-02 18:03:53,183 EPOCH 118
2024-02-02 18:03:56,523 [Epoch: 118 Step: 00004000] Batch Recognition Loss:   0.001677 => Gls Tokens per Sec:     2108 || Batch Translation Loss:   0.621000 => Txt Tokens per Sec:     5769 || Lr: 0.000100
2024-02-02 18:04:05,453 Validation result at epoch 118, step     4000: duration: 8.9286s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00486	Translation Loss: 78728.27344	PPL: 2639.49316
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.87	(BLEU-1: 13.19,	BLEU-2: 4.32,	BLEU-3: 1.79,	BLEU-4: 0.87)
	CHRF 17.61	ROUGE 11.33
2024-02-02 18:04:05,454 Logging Recognition and Translation Outputs
2024-02-02 18:04:05,454 ========================================================================================================================
2024-02-02 18:04:05,454 Logging Sequence: 133_173.00
2024-02-02 18:04:05,454 	Gloss Reference :	A B+C+D+E
2024-02-02 18:04:05,454 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 18:04:05,455 	Gloss Alignment :	         
2024-02-02 18:04:05,455 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 18:04:05,456 	Text Reference  :	according to sources the  leaders of   the two countries are set to  join  the commentary panel as  well   
2024-02-02 18:04:05,456 	Text Hypothesis :	********* ** pm      modi along   with the *** ********* *** *** ipl there is  one        of    his arrival
2024-02-02 18:04:05,456 	Text Alignment  :	D         D  S       S    S       S        D   D         D   D   S   S     S   S          S     S   S      
2024-02-02 18:04:05,456 ========================================================================================================================
2024-02-02 18:04:05,456 Logging Sequence: 83_33.00
2024-02-02 18:04:05,457 	Gloss Reference :	A B+C+D+E
2024-02-02 18:04:05,457 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 18:04:05,457 	Gloss Alignment :	         
2024-02-02 18:04:05,457 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 18:04:05,458 	Text Reference  :	*** a       football *********** ******* ** match lasts   for two equal halves of *** 45      minutes
2024-02-02 18:04:05,458 	Text Hypothesis :	the denmark football association tweeted by 15    minutes and at  the   reason of the denmark team   
2024-02-02 18:04:05,458 	Text Alignment  :	I   S                I           I       I  S     S       S   S   S     S         I   S       S      
2024-02-02 18:04:05,458 ========================================================================================================================
2024-02-02 18:04:05,458 Logging Sequence: 68_147.00
2024-02-02 18:04:05,458 	Gloss Reference :	A B+C+D+E
2024-02-02 18:04:05,459 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 18:04:05,459 	Gloss Alignment :	         
2024-02-02 18:04:05,459 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 18:04:05,460 	Text Reference  :	***** ****** remember the 2007 t20     world cup amid a     lot    of    sledging by   english players
2024-02-02 18:04:05,460 	Text Hypothesis :	while bumrah scored   29  runs himself in    the over while stuart broad gave     away 6       balls  
2024-02-02 18:04:05,460 	Text Alignment  :	I     I      S        S   S    S       S     S   S    S     S      S     S        S    S       S      
2024-02-02 18:04:05,460 ========================================================================================================================
2024-02-02 18:04:05,461 Logging Sequence: 165_8.00
2024-02-02 18:04:05,461 	Gloss Reference :	A B+C+D+E
2024-02-02 18:04:05,461 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 18:04:05,461 	Gloss Alignment :	         
2024-02-02 18:04:05,461 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 18:04:05,462 	Text Reference  :	** ** however many        don't   believe in  it    it         varies among people
2024-02-02 18:04:05,462 	Text Hypothesis :	he is an      interesting history of      his guru' photograph in     the   world 
2024-02-02 18:04:05,462 	Text Alignment  :	I  I  S       S           S       S       S   S     S          S      S     S     
2024-02-02 18:04:05,462 ========================================================================================================================
2024-02-02 18:04:05,462 Logging Sequence: 119_71.00
2024-02-02 18:04:05,463 	Gloss Reference :	A B+C+D+E
2024-02-02 18:04:05,463 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 18:04:05,463 	Gloss Alignment :	         
2024-02-02 18:04:05,463 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 18:04:05,464 	Text Reference  :	the special gold devices  have     each  player' names and    jersey numbers next    to ** the camera
2024-02-02 18:04:05,464 	Text Hypothesis :	the ******* **** iphones' combined worth is      eur   175000 which  roughly amounts to rs 173 crore 
2024-02-02 18:04:05,464 	Text Alignment  :	    D       D    S        S        S     S       S     S      S      S       S          I  S   S     
2024-02-02 18:04:05,465 ========================================================================================================================
2024-02-02 18:04:07,167 Epoch 118: Total Training Recognition Loss 0.08  Total Training Translation Loss 21.94 
2024-02-02 18:04:07,167 EPOCH 119
2024-02-02 18:04:11,690 Epoch 119: Total Training Recognition Loss 0.08  Total Training Translation Loss 20.22 
2024-02-02 18:04:11,690 EPOCH 120
2024-02-02 18:04:16,628 Epoch 120: Total Training Recognition Loss 0.08  Total Training Translation Loss 25.16 
2024-02-02 18:04:16,629 EPOCH 121
2024-02-02 18:04:19,306 [Epoch: 121 Step: 00004100] Batch Recognition Loss:   0.003598 => Gls Tokens per Sec:     2392 || Batch Translation Loss:   0.641467 => Txt Tokens per Sec:     6919 || Lr: 0.000100
2024-02-02 18:04:20,845 Epoch 121: Total Training Recognition Loss 0.08  Total Training Translation Loss 23.26 
2024-02-02 18:04:20,845 EPOCH 122
2024-02-02 18:04:25,662 Epoch 122: Total Training Recognition Loss 0.09  Total Training Translation Loss 24.18 
2024-02-02 18:04:25,662 EPOCH 123
2024-02-02 18:04:29,924 Epoch 123: Total Training Recognition Loss 0.08  Total Training Translation Loss 25.02 
2024-02-02 18:04:29,924 EPOCH 124
2024-02-02 18:04:32,285 [Epoch: 124 Step: 00004200] Batch Recognition Loss:   0.002028 => Gls Tokens per Sec:     2335 || Batch Translation Loss:   0.690389 => Txt Tokens per Sec:     6137 || Lr: 0.000100
2024-02-02 18:04:34,828 Epoch 124: Total Training Recognition Loss 0.07  Total Training Translation Loss 23.68 
2024-02-02 18:04:34,828 EPOCH 125
2024-02-02 18:04:39,128 Epoch 125: Total Training Recognition Loss 0.08  Total Training Translation Loss 22.60 
2024-02-02 18:04:39,128 EPOCH 126
2024-02-02 18:04:44,045 Epoch 126: Total Training Recognition Loss 0.08  Total Training Translation Loss 21.99 
2024-02-02 18:04:44,045 EPOCH 127
2024-02-02 18:04:46,268 [Epoch: 127 Step: 00004300] Batch Recognition Loss:   0.001554 => Gls Tokens per Sec:     2192 || Batch Translation Loss:   0.421406 => Txt Tokens per Sec:     6462 || Lr: 0.000100
2024-02-02 18:04:48,289 Epoch 127: Total Training Recognition Loss 0.08  Total Training Translation Loss 20.78 
2024-02-02 18:04:48,290 EPOCH 128
2024-02-02 18:04:53,139 Epoch 128: Total Training Recognition Loss 0.07  Total Training Translation Loss 19.77 
2024-02-02 18:04:53,139 EPOCH 129
2024-02-02 18:04:57,462 Epoch 129: Total Training Recognition Loss 0.07  Total Training Translation Loss 18.09 
2024-02-02 18:04:57,462 EPOCH 130
2024-02-02 18:04:59,568 [Epoch: 130 Step: 00004400] Batch Recognition Loss:   0.001847 => Gls Tokens per Sec:     2128 || Batch Translation Loss:   0.582094 => Txt Tokens per Sec:     6083 || Lr: 0.000100
2024-02-02 18:05:02,320 Epoch 130: Total Training Recognition Loss 0.06  Total Training Translation Loss 17.43 
2024-02-02 18:05:02,320 EPOCH 131
2024-02-02 18:05:06,657 Epoch 131: Total Training Recognition Loss 0.06  Total Training Translation Loss 17.06 
2024-02-02 18:05:06,658 EPOCH 132
2024-02-02 18:05:11,486 Epoch 132: Total Training Recognition Loss 0.06  Total Training Translation Loss 15.88 
2024-02-02 18:05:11,486 EPOCH 133
2024-02-02 18:05:12,867 [Epoch: 133 Step: 00004500] Batch Recognition Loss:   0.001373 => Gls Tokens per Sec:     2785 || Batch Translation Loss:   0.307354 => Txt Tokens per Sec:     7690 || Lr: 0.000100
2024-02-02 18:05:15,962 Epoch 133: Total Training Recognition Loss 0.06  Total Training Translation Loss 15.54 
2024-02-02 18:05:15,963 EPOCH 134
2024-02-02 18:05:20,757 Epoch 134: Total Training Recognition Loss 0.06  Total Training Translation Loss 14.76 
2024-02-02 18:05:20,757 EPOCH 135
2024-02-02 18:05:25,252 Epoch 135: Total Training Recognition Loss 0.06  Total Training Translation Loss 14.74 
2024-02-02 18:05:25,252 EPOCH 136
2024-02-02 18:05:26,735 [Epoch: 136 Step: 00004600] Batch Recognition Loss:   0.001363 => Gls Tokens per Sec:     2158 || Batch Translation Loss:   0.397804 => Txt Tokens per Sec:     6362 || Lr: 0.000100
2024-02-02 18:05:30,003 Epoch 136: Total Training Recognition Loss 0.06  Total Training Translation Loss 15.43 
2024-02-02 18:05:30,003 EPOCH 137
2024-02-02 18:05:34,429 Epoch 137: Total Training Recognition Loss 0.05  Total Training Translation Loss 15.57 
2024-02-02 18:05:34,430 EPOCH 138
2024-02-02 18:05:39,112 Epoch 138: Total Training Recognition Loss 0.06  Total Training Translation Loss 16.13 
2024-02-02 18:05:39,112 EPOCH 139
2024-02-02 18:05:40,328 [Epoch: 139 Step: 00004700] Batch Recognition Loss:   0.001190 => Gls Tokens per Sec:     2107 || Batch Translation Loss:   0.379198 => Txt Tokens per Sec:     6128 || Lr: 0.000100
2024-02-02 18:05:43,664 Epoch 139: Total Training Recognition Loss 0.05  Total Training Translation Loss 15.47 
2024-02-02 18:05:43,665 EPOCH 140
2024-02-02 18:05:48,297 Epoch 140: Total Training Recognition Loss 0.05  Total Training Translation Loss 19.10 
2024-02-02 18:05:48,298 EPOCH 141
2024-02-02 18:05:52,847 Epoch 141: Total Training Recognition Loss 0.06  Total Training Translation Loss 20.24 
2024-02-02 18:05:52,848 EPOCH 142
2024-02-02 18:05:53,818 [Epoch: 142 Step: 00004800] Batch Recognition Loss:   0.002785 => Gls Tokens per Sec:     1981 || Batch Translation Loss:   0.704378 => Txt Tokens per Sec:     5889 || Lr: 0.000100
2024-02-02 18:05:57,446 Epoch 142: Total Training Recognition Loss 0.07  Total Training Translation Loss 20.88 
2024-02-02 18:05:57,447 EPOCH 143
2024-02-02 18:06:01,672 Epoch 143: Total Training Recognition Loss 0.07  Total Training Translation Loss 17.94 
2024-02-02 18:06:01,673 EPOCH 144
2024-02-02 18:06:06,573 Epoch 144: Total Training Recognition Loss 0.07  Total Training Translation Loss 19.85 
2024-02-02 18:06:06,574 EPOCH 145
2024-02-02 18:06:07,130 [Epoch: 145 Step: 00004900] Batch Recognition Loss:   0.002018 => Gls Tokens per Sec:     2306 || Batch Translation Loss:   0.386585 => Txt Tokens per Sec:     6611 || Lr: 0.000100
2024-02-02 18:06:11,212 Epoch 145: Total Training Recognition Loss 0.07  Total Training Translation Loss 19.79 
2024-02-02 18:06:11,213 EPOCH 146
2024-02-02 18:06:15,656 Epoch 146: Total Training Recognition Loss 0.06  Total Training Translation Loss 16.43 
2024-02-02 18:06:15,656 EPOCH 147
2024-02-02 18:06:20,374 Epoch 147: Total Training Recognition Loss 0.05  Total Training Translation Loss 13.08 
2024-02-02 18:06:20,374 EPOCH 148
2024-02-02 18:06:20,733 [Epoch: 148 Step: 00005000] Batch Recognition Loss:   0.001253 => Gls Tokens per Sec:     1793 || Batch Translation Loss:   0.478046 => Txt Tokens per Sec:     5739 || Lr: 0.000100
2024-02-02 18:06:24,820 Epoch 148: Total Training Recognition Loss 0.05  Total Training Translation Loss 11.04 
2024-02-02 18:06:24,820 EPOCH 149
2024-02-02 18:06:29,623 Epoch 149: Total Training Recognition Loss 0.05  Total Training Translation Loss 11.08 
2024-02-02 18:06:29,624 EPOCH 150
2024-02-02 18:06:33,983 [Epoch: 150 Step: 00005100] Batch Recognition Loss:   0.001091 => Gls Tokens per Sec:     2439 || Batch Translation Loss:   0.144646 => Txt Tokens per Sec:     6772 || Lr: 0.000100
2024-02-02 18:06:33,983 Epoch 150: Total Training Recognition Loss 0.04  Total Training Translation Loss 11.02 
2024-02-02 18:06:33,984 EPOCH 151
2024-02-02 18:06:38,797 Epoch 151: Total Training Recognition Loss 0.04  Total Training Translation Loss 9.95 
2024-02-02 18:06:38,798 EPOCH 152
2024-02-02 18:06:43,147 Epoch 152: Total Training Recognition Loss 0.04  Total Training Translation Loss 9.98 
2024-02-02 18:06:43,147 EPOCH 153
2024-02-02 18:06:47,744 [Epoch: 153 Step: 00005200] Batch Recognition Loss:   0.001554 => Gls Tokens per Sec:     2174 || Batch Translation Loss:   0.335881 => Txt Tokens per Sec:     6069 || Lr: 0.000100
2024-02-02 18:06:47,929 Epoch 153: Total Training Recognition Loss 0.04  Total Training Translation Loss 10.63 
2024-02-02 18:06:47,929 EPOCH 154
2024-02-02 18:06:52,283 Epoch 154: Total Training Recognition Loss 0.04  Total Training Translation Loss 12.19 
2024-02-02 18:06:52,283 EPOCH 155
2024-02-02 18:06:56,841 Epoch 155: Total Training Recognition Loss 0.04  Total Training Translation Loss 13.26 
2024-02-02 18:06:56,842 EPOCH 156
2024-02-02 18:07:01,021 [Epoch: 156 Step: 00005300] Batch Recognition Loss:   0.000836 => Gls Tokens per Sec:     2237 || Batch Translation Loss:   0.411376 => Txt Tokens per Sec:     6216 || Lr: 0.000100
2024-02-02 18:07:01,621 Epoch 156: Total Training Recognition Loss 0.04  Total Training Translation Loss 11.00 
2024-02-02 18:07:01,622 EPOCH 157
2024-02-02 18:07:06,311 Epoch 157: Total Training Recognition Loss 0.04  Total Training Translation Loss 10.14 
2024-02-02 18:07:06,311 EPOCH 158
2024-02-02 18:07:10,767 Epoch 158: Total Training Recognition Loss 0.04  Total Training Translation Loss 9.67 
2024-02-02 18:07:10,768 EPOCH 159
2024-02-02 18:07:14,711 [Epoch: 159 Step: 00005400] Batch Recognition Loss:   0.001293 => Gls Tokens per Sec:     2210 || Batch Translation Loss:   0.296963 => Txt Tokens per Sec:     6120 || Lr: 0.000100
2024-02-02 18:07:15,455 Epoch 159: Total Training Recognition Loss 0.04  Total Training Translation Loss 9.18 
2024-02-02 18:07:15,455 EPOCH 160
2024-02-02 18:07:19,981 Epoch 160: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.74 
2024-02-02 18:07:19,982 EPOCH 161
2024-02-02 18:07:24,540 Epoch 161: Total Training Recognition Loss 0.04  Total Training Translation Loss 8.57 
2024-02-02 18:07:24,540 EPOCH 162
2024-02-02 18:07:28,042 [Epoch: 162 Step: 00005500] Batch Recognition Loss:   0.001095 => Gls Tokens per Sec:     2377 || Batch Translation Loss:   0.363213 => Txt Tokens per Sec:     6649 || Lr: 0.000100
2024-02-02 18:07:29,097 Epoch 162: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.95 
2024-02-02 18:07:29,098 EPOCH 163
2024-02-02 18:07:33,824 Epoch 163: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.78 
2024-02-02 18:07:33,825 EPOCH 164
2024-02-02 18:07:38,716 Epoch 164: Total Training Recognition Loss 0.04  Total Training Translation Loss 12.94 
2024-02-02 18:07:38,717 EPOCH 165
2024-02-02 18:07:41,910 [Epoch: 165 Step: 00005600] Batch Recognition Loss:   0.001425 => Gls Tokens per Sec:     2328 || Batch Translation Loss:   1.313636 => Txt Tokens per Sec:     6421 || Lr: 0.000100
2024-02-02 18:07:43,067 Epoch 165: Total Training Recognition Loss 0.08  Total Training Translation Loss 28.45 
2024-02-02 18:07:43,068 EPOCH 166
2024-02-02 18:07:47,765 Epoch 166: Total Training Recognition Loss 0.09  Total Training Translation Loss 24.70 
2024-02-02 18:07:47,765 EPOCH 167
2024-02-02 18:07:52,231 Epoch 167: Total Training Recognition Loss 0.05  Total Training Translation Loss 14.27 
2024-02-02 18:07:52,232 EPOCH 168
2024-02-02 18:07:55,330 [Epoch: 168 Step: 00005700] Batch Recognition Loss:   0.002230 => Gls Tokens per Sec:     2192 || Batch Translation Loss:   0.320123 => Txt Tokens per Sec:     6109 || Lr: 0.000100
2024-02-02 18:07:57,050 Epoch 168: Total Training Recognition Loss 0.05  Total Training Translation Loss 10.70 
2024-02-02 18:07:57,050 EPOCH 169
2024-02-02 18:08:01,401 Epoch 169: Total Training Recognition Loss 0.04  Total Training Translation Loss 9.27 
2024-02-02 18:08:01,401 EPOCH 170
2024-02-02 18:08:06,162 Epoch 170: Total Training Recognition Loss 0.04  Total Training Translation Loss 12.26 
2024-02-02 18:08:06,162 EPOCH 171
2024-02-02 18:08:08,415 [Epoch: 171 Step: 00005800] Batch Recognition Loss:   0.000502 => Gls Tokens per Sec:     2843 || Batch Translation Loss:   0.194207 => Txt Tokens per Sec:     7558 || Lr: 0.000100
2024-02-02 18:08:10,479 Epoch 171: Total Training Recognition Loss 0.04  Total Training Translation Loss 10.36 
2024-02-02 18:08:10,480 EPOCH 172
2024-02-02 18:08:15,319 Epoch 172: Total Training Recognition Loss 0.04  Total Training Translation Loss 8.90 
2024-02-02 18:08:15,319 EPOCH 173
2024-02-02 18:08:19,643 Epoch 173: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.06 
2024-02-02 18:08:19,643 EPOCH 174
2024-02-02 18:08:22,448 [Epoch: 174 Step: 00005900] Batch Recognition Loss:   0.000597 => Gls Tokens per Sec:     2055 || Batch Translation Loss:   0.271188 => Txt Tokens per Sec:     5819 || Lr: 0.000100
2024-02-02 18:08:24,469 Epoch 174: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.29 
2024-02-02 18:08:24,470 EPOCH 175
2024-02-02 18:08:29,064 Epoch 175: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.06 
2024-02-02 18:08:29,065 EPOCH 176
2024-02-02 18:08:33,777 Epoch 176: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.95 
2024-02-02 18:08:33,778 EPOCH 177
2024-02-02 18:08:35,471 [Epoch: 177 Step: 00006000] Batch Recognition Loss:   0.000484 => Gls Tokens per Sec:     3026 || Batch Translation Loss:   0.179204 => Txt Tokens per Sec:     7822 || Lr: 0.000100
2024-02-02 18:08:43,791 Validation result at epoch 177, step     6000: duration: 8.3192s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00132	Translation Loss: 85519.70312	PPL: 5208.05371
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.81	(BLEU-1: 11.89,	BLEU-2: 3.91,	BLEU-3: 1.70,	BLEU-4: 0.81)
	CHRF 17.49	ROUGE 10.58
2024-02-02 18:08:43,792 Logging Recognition and Translation Outputs
2024-02-02 18:08:43,792 ========================================================================================================================
2024-02-02 18:08:43,792 Logging Sequence: 89_111.00
2024-02-02 18:08:43,792 	Gloss Reference :	A B+C+D+E
2024-02-02 18:08:43,792 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 18:08:43,792 	Gloss Alignment :	         
2024-02-02 18:08:43,793 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 18:08:43,794 	Text Reference  :	however selectors never   selected me  for   the *** ***** *** *** ** ****** *** ** ** ***** ** ** team  
2024-02-02 18:08:43,794 	Text Hypothesis :	******* the       spinner is       not touch the t20 world cup due to retire but he is going on to retire
2024-02-02 18:08:43,794 	Text Alignment  :	D       S         S       S        S   S         I   I     I   I   I  I      I   I  I  I     I  I  S     
2024-02-02 18:08:43,794 ========================================================================================================================
2024-02-02 18:08:43,794 Logging Sequence: 137_23.00
2024-02-02 18:08:43,794 	Gloss Reference :	A B+C+D+E
2024-02-02 18:08:43,794 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 18:08:43,795 	Gloss Alignment :	         
2024-02-02 18:08:43,795 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 18:08:43,796 	Text Reference  :	fan from around the world are *** in      qatar for the fifa world cup    
2024-02-02 18:08:43,796 	Text Hypothesis :	the fans across the world are not allowed to    see the **** ***** stadium
2024-02-02 18:08:43,796 	Text Alignment  :	S   S    S                    I   S       S     S       D    D     S      
2024-02-02 18:08:43,796 ========================================================================================================================
2024-02-02 18:08:43,796 Logging Sequence: 128_145.00
2024-02-02 18:08:43,796 	Gloss Reference :	A B+C+D+E
2024-02-02 18:08:43,797 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 18:08:43,797 	Gloss Alignment :	         
2024-02-02 18:08:43,797 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 18:08:43,798 	Text Reference  :	icc also uploaded a   video of the same *** ** ** ***** *** ****** ***
2024-02-02 18:08:43,798 	Text Hypothesis :	*** the  ban      was not   in the same but he is about the entire man
2024-02-02 18:08:43,798 	Text Alignment  :	D   S    S        S   S     S           I   I  I  I     I   I      I  
2024-02-02 18:08:43,798 ========================================================================================================================
2024-02-02 18:08:43,798 Logging Sequence: 165_192.00
2024-02-02 18:08:43,798 	Gloss Reference :	A B+C+D+E
2024-02-02 18:08:43,798 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 18:08:43,799 	Gloss Alignment :	         
2024-02-02 18:08:43,799 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 18:08:43,800 	Text Reference  :	** ** ***** **** *** ****** 3    ravichandran ashwin believes that his   bag is   lucky
2024-02-02 18:08:43,800 	Text Hypothesis :	he is happy with the indian team won          the    toss     and  chose to  give it   
2024-02-02 18:08:43,800 	Text Alignment  :	I  I  I     I    I   I      S    S            S      S        S    S     S   S    S    
2024-02-02 18:08:43,800 ========================================================================================================================
2024-02-02 18:08:43,800 Logging Sequence: 180_494.00
2024-02-02 18:08:43,800 	Gloss Reference :	A B+C+D+E
2024-02-02 18:08:43,800 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 18:08:43,801 	Gloss Alignment :	         
2024-02-02 18:08:43,801 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 18:08:43,802 	Text Reference  :	the     women wrestlers spoke angrily against the  police and the controversy in front of     the        media    
2024-02-02 18:08:43,802 	Text Hypothesis :	however an    wrestlers ***** ******* ******* said it     is  not permitted   in ***** sexual harassment complaint
2024-02-02 18:08:43,802 	Text Alignment  :	S       S               D     D       D       S    S      S   S   S              D     S      S          S        
2024-02-02 18:08:43,802 ========================================================================================================================
2024-02-02 18:08:46,832 Epoch 177: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.01 
2024-02-02 18:08:46,833 EPOCH 178
2024-02-02 18:08:51,436 Epoch 178: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.08 
2024-02-02 18:08:51,436 EPOCH 179
2024-02-02 18:08:55,469 Epoch 179: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.55 
2024-02-02 18:08:55,469 EPOCH 180
2024-02-02 18:08:57,498 [Epoch: 180 Step: 00006100] Batch Recognition Loss:   0.000444 => Gls Tokens per Sec:     2086 || Batch Translation Loss:   0.196149 => Txt Tokens per Sec:     5697 || Lr: 0.000100
2024-02-02 18:09:00,356 Epoch 180: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.15 
2024-02-02 18:09:00,356 EPOCH 181
2024-02-02 18:09:04,587 Epoch 181: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.10 
2024-02-02 18:09:04,587 EPOCH 182
2024-02-02 18:09:08,733 Epoch 182: Total Training Recognition Loss 0.02  Total Training Translation Loss 8.25 
2024-02-02 18:09:08,733 EPOCH 183
2024-02-02 18:09:10,479 [Epoch: 183 Step: 00006200] Batch Recognition Loss:   0.000504 => Gls Tokens per Sec:     2059 || Batch Translation Loss:   0.375396 => Txt Tokens per Sec:     5810 || Lr: 0.000100
2024-02-02 18:09:13,644 Epoch 183: Total Training Recognition Loss 0.02  Total Training Translation Loss 8.11 
2024-02-02 18:09:13,644 EPOCH 184
2024-02-02 18:09:18,031 Epoch 184: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.72 
2024-02-02 18:09:18,032 EPOCH 185
2024-02-02 18:09:22,784 Epoch 185: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.40 
2024-02-02 18:09:22,784 EPOCH 186
2024-02-02 18:09:24,199 [Epoch: 186 Step: 00006300] Batch Recognition Loss:   0.000840 => Gls Tokens per Sec:     2263 || Batch Translation Loss:   0.259017 => Txt Tokens per Sec:     6564 || Lr: 0.000100
2024-02-02 18:09:26,795 Epoch 186: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.46 
2024-02-02 18:09:26,795 EPOCH 187
2024-02-02 18:09:31,524 Epoch 187: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.98 
2024-02-02 18:09:31,525 EPOCH 188
2024-02-02 18:09:35,946 Epoch 188: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.39 
2024-02-02 18:09:35,946 EPOCH 189
2024-02-02 18:09:37,057 [Epoch: 189 Step: 00006400] Batch Recognition Loss:   0.000721 => Gls Tokens per Sec:     2305 || Batch Translation Loss:   0.213899 => Txt Tokens per Sec:     6143 || Lr: 0.000100
2024-02-02 18:09:40,758 Epoch 189: Total Training Recognition Loss 0.02  Total Training Translation Loss 8.48 
2024-02-02 18:09:40,759 EPOCH 190
2024-02-02 18:09:45,185 Epoch 190: Total Training Recognition Loss 0.02  Total Training Translation Loss 8.14 
2024-02-02 18:09:45,186 EPOCH 191
2024-02-02 18:09:49,997 Epoch 191: Total Training Recognition Loss 0.02  Total Training Translation Loss 9.05 
2024-02-02 18:09:49,998 EPOCH 192
2024-02-02 18:09:50,737 [Epoch: 192 Step: 00006500] Batch Recognition Loss:   0.000608 => Gls Tokens per Sec:     2605 || Batch Translation Loss:   0.157399 => Txt Tokens per Sec:     7787 || Lr: 0.000100
2024-02-02 18:09:54,479 Epoch 192: Total Training Recognition Loss 0.02  Total Training Translation Loss 8.15 
2024-02-02 18:09:54,480 EPOCH 193
2024-02-02 18:09:59,153 Epoch 193: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.26 
2024-02-02 18:09:59,153 EPOCH 194
2024-02-02 18:10:03,194 Epoch 194: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.62 
2024-02-02 18:10:03,194 EPOCH 195
2024-02-02 18:10:03,837 [Epoch: 195 Step: 00006600] Batch Recognition Loss:   0.000471 => Gls Tokens per Sec:     1996 || Batch Translation Loss:   0.324087 => Txt Tokens per Sec:     5691 || Lr: 0.000100
2024-02-02 18:10:07,314 Epoch 195: Total Training Recognition Loss 0.02  Total Training Translation Loss 8.00 
2024-02-02 18:10:07,314 EPOCH 196
2024-02-02 18:10:12,148 Epoch 196: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.02 
2024-02-02 18:10:12,149 EPOCH 197
2024-02-02 18:10:16,513 Epoch 197: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.00 
2024-02-02 18:10:16,514 EPOCH 198
2024-02-02 18:10:16,763 [Epoch: 198 Step: 00006700] Batch Recognition Loss:   0.000586 => Gls Tokens per Sec:     2581 || Batch Translation Loss:   0.233555 => Txt Tokens per Sec:     6605 || Lr: 0.000100
2024-02-02 18:10:21,331 Epoch 198: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.65 
2024-02-02 18:10:21,331 EPOCH 199
2024-02-02 18:10:25,908 Epoch 199: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.25 
2024-02-02 18:10:25,909 EPOCH 200
2024-02-02 18:10:30,486 [Epoch: 200 Step: 00006800] Batch Recognition Loss:   0.001257 => Gls Tokens per Sec:     2322 || Batch Translation Loss:   0.188765 => Txt Tokens per Sec:     6446 || Lr: 0.000100
2024-02-02 18:10:30,486 Epoch 200: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.10 
2024-02-02 18:10:30,487 EPOCH 201
2024-02-02 18:10:35,133 Epoch 201: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.49 
2024-02-02 18:10:35,134 EPOCH 202
2024-02-02 18:10:39,596 Epoch 202: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.42 
2024-02-02 18:10:39,596 EPOCH 203
2024-02-02 18:10:43,959 [Epoch: 203 Step: 00006900] Batch Recognition Loss:   0.000283 => Gls Tokens per Sec:     2291 || Batch Translation Loss:   0.205044 => Txt Tokens per Sec:     6257 || Lr: 0.000100
2024-02-02 18:10:44,328 Epoch 203: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.63 
2024-02-02 18:10:44,328 EPOCH 204
2024-02-02 18:10:48,709 Epoch 204: Total Training Recognition Loss 0.02  Total Training Translation Loss 8.24 
2024-02-02 18:10:48,709 EPOCH 205
2024-02-02 18:10:53,480 Epoch 205: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.84 
2024-02-02 18:10:53,481 EPOCH 206
2024-02-02 18:10:57,415 [Epoch: 206 Step: 00007000] Batch Recognition Loss:   0.001108 => Gls Tokens per Sec:     2376 || Batch Translation Loss:   0.282624 => Txt Tokens per Sec:     6621 || Lr: 0.000100
2024-02-02 18:10:57,822 Epoch 206: Total Training Recognition Loss 0.03  Total Training Translation Loss 10.02 
2024-02-02 18:10:57,823 EPOCH 207
2024-02-02 18:11:02,646 Epoch 207: Total Training Recognition Loss 0.02  Total Training Translation Loss 9.84 
2024-02-02 18:11:02,646 EPOCH 208
2024-02-02 18:11:06,988 Epoch 208: Total Training Recognition Loss 0.02  Total Training Translation Loss 8.21 
2024-02-02 18:11:06,988 EPOCH 209
2024-02-02 18:11:10,941 [Epoch: 209 Step: 00007100] Batch Recognition Loss:   0.000646 => Gls Tokens per Sec:     2204 || Batch Translation Loss:   0.243974 => Txt Tokens per Sec:     6050 || Lr: 0.000100
2024-02-02 18:11:11,860 Epoch 209: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.34 
2024-02-02 18:11:11,860 EPOCH 210
2024-02-02 18:11:16,104 Epoch 210: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.83 
2024-02-02 18:11:16,104 EPOCH 211
2024-02-02 18:11:20,987 Epoch 211: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.24 
2024-02-02 18:11:20,987 EPOCH 212
2024-02-02 18:11:24,271 [Epoch: 212 Step: 00007200] Batch Recognition Loss:   0.000428 => Gls Tokens per Sec:     2458 || Batch Translation Loss:   0.318888 => Txt Tokens per Sec:     6841 || Lr: 0.000100
2024-02-02 18:11:25,190 Epoch 212: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.89 
2024-02-02 18:11:25,190 EPOCH 213
2024-02-02 18:11:29,251 Epoch 213: Total Training Recognition Loss 0.03  Total Training Translation Loss 10.30 
2024-02-02 18:11:29,251 EPOCH 214
2024-02-02 18:11:33,334 Epoch 214: Total Training Recognition Loss 0.02  Total Training Translation Loss 8.31 
2024-02-02 18:11:33,335 EPOCH 215
2024-02-02 18:11:36,242 [Epoch: 215 Step: 00007300] Batch Recognition Loss:   0.000968 => Gls Tokens per Sec:     2555 || Batch Translation Loss:   0.224741 => Txt Tokens per Sec:     6909 || Lr: 0.000100
2024-02-02 18:11:37,980 Epoch 215: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.80 
2024-02-02 18:11:37,980 EPOCH 216
2024-02-02 18:11:42,696 Epoch 216: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.49 
2024-02-02 18:11:42,697 EPOCH 217
2024-02-02 18:11:47,431 Epoch 217: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.61 
2024-02-02 18:11:47,431 EPOCH 218
2024-02-02 18:11:49,885 [Epoch: 218 Step: 00007400] Batch Recognition Loss:   0.000276 => Gls Tokens per Sec:     2768 || Batch Translation Loss:   0.104782 => Txt Tokens per Sec:     7627 || Lr: 0.000100
2024-02-02 18:11:51,733 Epoch 218: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.64 
2024-02-02 18:11:51,734 EPOCH 219
2024-02-02 18:11:56,532 Epoch 219: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.27 
2024-02-02 18:11:56,532 EPOCH 220
2024-02-02 18:12:01,037 Epoch 220: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.10 
2024-02-02 18:12:01,038 EPOCH 221
2024-02-02 18:12:03,879 [Epoch: 221 Step: 00007500] Batch Recognition Loss:   0.000753 => Gls Tokens per Sec:     2166 || Batch Translation Loss:   0.140823 => Txt Tokens per Sec:     5953 || Lr: 0.000100
2024-02-02 18:12:05,701 Epoch 221: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.32 
2024-02-02 18:12:05,701 EPOCH 222
2024-02-02 18:12:10,358 Epoch 222: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.34 
2024-02-02 18:12:10,359 EPOCH 223
2024-02-02 18:12:14,801 Epoch 223: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.74 
2024-02-02 18:12:14,801 EPOCH 224
2024-02-02 18:12:17,621 [Epoch: 224 Step: 00007600] Batch Recognition Loss:   0.000327 => Gls Tokens per Sec:     2043 || Batch Translation Loss:   0.072251 => Txt Tokens per Sec:     5726 || Lr: 0.000100
2024-02-02 18:12:19,676 Epoch 224: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.87 
2024-02-02 18:12:19,676 EPOCH 225
2024-02-02 18:12:24,115 Epoch 225: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.62 
2024-02-02 18:12:24,116 EPOCH 226
2024-02-02 18:12:29,005 Epoch 226: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.83 
2024-02-02 18:12:29,006 EPOCH 227
2024-02-02 18:12:30,824 [Epoch: 227 Step: 00007700] Batch Recognition Loss:   0.000340 => Gls Tokens per Sec:     2815 || Batch Translation Loss:   0.151813 => Txt Tokens per Sec:     7804 || Lr: 0.000100
2024-02-02 18:12:33,293 Epoch 227: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.18 
2024-02-02 18:12:33,293 EPOCH 228
2024-02-02 18:12:38,147 Epoch 228: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.73 
2024-02-02 18:12:38,148 EPOCH 229
2024-02-02 18:12:42,435 Epoch 229: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.76 
2024-02-02 18:12:42,436 EPOCH 230
2024-02-02 18:12:44,590 [Epoch: 230 Step: 00007800] Batch Recognition Loss:   0.000455 => Gls Tokens per Sec:     2082 || Batch Translation Loss:   0.457464 => Txt Tokens per Sec:     6000 || Lr: 0.000100
2024-02-02 18:12:47,252 Epoch 230: Total Training Recognition Loss 0.02  Total Training Translation Loss 11.34 
2024-02-02 18:12:47,252 EPOCH 231
2024-02-02 18:12:51,693 Epoch 231: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.85 
2024-02-02 18:12:51,693 EPOCH 232
2024-02-02 18:12:56,449 Epoch 232: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.51 
2024-02-02 18:12:56,449 EPOCH 233
2024-02-02 18:12:58,032 [Epoch: 233 Step: 00007900] Batch Recognition Loss:   0.001049 => Gls Tokens per Sec:     2269 || Batch Translation Loss:   0.111574 => Txt Tokens per Sec:     6671 || Lr: 0.000100
2024-02-02 18:13:00,990 Epoch 233: Total Training Recognition Loss 0.02  Total Training Translation Loss 8.66 
2024-02-02 18:13:00,991 EPOCH 234
2024-02-02 18:13:05,580 Epoch 234: Total Training Recognition Loss 0.02  Total Training Translation Loss 8.63 
2024-02-02 18:13:05,580 EPOCH 235
2024-02-02 18:13:10,316 Epoch 235: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.26 
2024-02-02 18:13:10,317 EPOCH 236
2024-02-02 18:13:11,513 [Epoch: 236 Step: 00008000] Batch Recognition Loss:   0.001098 => Gls Tokens per Sec:     2678 || Batch Translation Loss:   0.419294 => Txt Tokens per Sec:     6895 || Lr: 0.000100
2024-02-02 18:13:19,608 Validation result at epoch 236, step     8000: duration: 8.0950s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00202	Translation Loss: 90072.08594	PPL: 8213.36816
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.92	(BLEU-1: 10.74,	BLEU-2: 3.83,	BLEU-3: 1.70,	BLEU-4: 0.92)
	CHRF 16.59	ROUGE 9.76
2024-02-02 18:13:19,609 Logging Recognition and Translation Outputs
2024-02-02 18:13:19,609 ========================================================================================================================
2024-02-02 18:13:19,609 Logging Sequence: 88_57.00
2024-02-02 18:13:19,610 	Gloss Reference :	A B+C+D+E
2024-02-02 18:13:19,610 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 18:13:19,610 	Gloss Alignment :	         
2024-02-02 18:13:19,610 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 18:13:19,612 	Text Reference  :	which   stated  messi we're  waiting for  you     to   come here      you  will be finished when    you come
2024-02-02 18:13:19,612 	Text Hypothesis :	notably rosario has   become the     most violent city in   argentina with 250  to 300      murders in  2022
2024-02-02 18:13:19,612 	Text Alignment  :	S       S       S     S      S       S    S       S    S    S         S    S    S  S        S       S   S   
2024-02-02 18:13:19,612 ========================================================================================================================
2024-02-02 18:13:19,612 Logging Sequence: 171_142.00
2024-02-02 18:13:19,612 	Gloss Reference :	A B+C+D+E
2024-02-02 18:13:19,613 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 18:13:19,613 	Gloss Alignment :	         
2024-02-02 18:13:19,613 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 18:13:19,614 	Text Reference  :	***** **** *** ********* this decision on   dhoni made     a *** ******** significant impact  as      pathirana claimed two tough  wickets
2024-02-02 18:13:19,615 	Text Hypothesis :	since then the wrestlers left the      same time  spending a new agencies were        playing against each      other   at  jantar mantar 
2024-02-02 18:13:19,615 	Text Alignment  :	I     I    I   I         S    S        S    S     S          I   I        S           S       S       S         S       S   S      S      
2024-02-02 18:13:19,615 ========================================================================================================================
2024-02-02 18:13:19,615 Logging Sequence: 125_207.00
2024-02-02 18:13:19,615 	Gloss Reference :	A B+C+D+E
2024-02-02 18:13:19,615 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 18:13:19,615 	Gloss Alignment :	         
2024-02-02 18:13:19,616 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 18:13:19,616 	Text Reference  :	he had not practised since he returned and he   had          also fallen sick   
2024-02-02 18:13:19,616 	Text Hypothesis :	** *** *** ********* ***** ** neeraj   was very disappointed by   his    brother
2024-02-02 18:13:19,617 	Text Alignment  :	D  D   D   D         D     D  S        S   S    S            S    S      S      
2024-02-02 18:13:19,617 ========================================================================================================================
2024-02-02 18:13:19,617 Logging Sequence: 68_230.00
2024-02-02 18:13:19,617 	Gloss Reference :	A B+C+D+E
2024-02-02 18:13:19,617 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 18:13:19,617 	Gloss Alignment :	         
2024-02-02 18:13:19,617 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 18:13:19,618 	Text Reference  :	let us know what you  think in   the comments below
2024-02-02 18:13:19,618 	Text Hypothesis :	*** ** they also sent to    pick up  against  pant 
2024-02-02 18:13:19,618 	Text Alignment  :	D   D  S    S    S    S     S    S   S        S    
2024-02-02 18:13:19,618 ========================================================================================================================
2024-02-02 18:13:19,618 Logging Sequence: 126_82.00
2024-02-02 18:13:19,619 	Gloss Reference :	A B+C+D+E
2024-02-02 18:13:19,619 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 18:13:19,619 	Gloss Alignment :	         
2024-02-02 18:13:19,619 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 18:13:19,621 	Text Reference  :	neeraj also dedicated his gold medal  to ********* **** ** former indian olympians who came close to winning *** medals
2024-02-02 18:13:19,621 	Text Hypothesis :	he     also dedicated *** the  medals to olympians like pt usha   and    others    who came close to winning the medals
2024-02-02 18:13:19,621 	Text Alignment  :	S                     D   S    S         I         I    I  S      S      S                                   I         
2024-02-02 18:13:19,621 ========================================================================================================================
2024-02-02 18:13:23,175 Epoch 236: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.33 
2024-02-02 18:13:23,176 EPOCH 237
2024-02-02 18:13:28,046 Epoch 237: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.30 
2024-02-02 18:13:28,047 EPOCH 238
2024-02-02 18:13:32,432 Epoch 238: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.46 
2024-02-02 18:13:32,432 EPOCH 239
2024-02-02 18:13:33,552 [Epoch: 239 Step: 00008100] Batch Recognition Loss:   0.000382 => Gls Tokens per Sec:     2287 || Batch Translation Loss:   0.079129 => Txt Tokens per Sec:     6060 || Lr: 0.000100
2024-02-02 18:13:36,511 Epoch 239: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.51 
2024-02-02 18:13:36,511 EPOCH 240
2024-02-02 18:13:41,434 Epoch 240: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.28 
2024-02-02 18:13:41,435 EPOCH 241
2024-02-02 18:13:45,670 Epoch 241: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.29 
2024-02-02 18:13:45,671 EPOCH 242
2024-02-02 18:13:46,438 [Epoch: 242 Step: 00008200] Batch Recognition Loss:   0.000507 => Gls Tokens per Sec:     2506 || Batch Translation Loss:   0.175211 => Txt Tokens per Sec:     6030 || Lr: 0.000100
2024-02-02 18:13:50,897 Epoch 242: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.20 
2024-02-02 18:13:50,898 EPOCH 243
2024-02-02 18:13:55,707 Epoch 243: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.66 
2024-02-02 18:13:55,707 EPOCH 244
2024-02-02 18:14:00,522 Epoch 244: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.07 
2024-02-02 18:14:00,522 EPOCH 245
2024-02-02 18:14:01,143 [Epoch: 245 Step: 00008300] Batch Recognition Loss:   0.000581 => Gls Tokens per Sec:     2065 || Batch Translation Loss:   0.119350 => Txt Tokens per Sec:     6248 || Lr: 0.000100
2024-02-02 18:14:04,819 Epoch 245: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.23 
2024-02-02 18:14:04,820 EPOCH 246
2024-02-02 18:14:09,728 Epoch 246: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.12 
2024-02-02 18:14:09,729 EPOCH 247
2024-02-02 18:14:14,019 Epoch 247: Total Training Recognition Loss 0.01  Total Training Translation Loss 5.44 
2024-02-02 18:14:14,019 EPOCH 248
2024-02-02 18:14:14,266 [Epoch: 248 Step: 00008400] Batch Recognition Loss:   0.000429 => Gls Tokens per Sec:     2602 || Batch Translation Loss:   0.090073 => Txt Tokens per Sec:     8150 || Lr: 0.000100
2024-02-02 18:14:18,885 Epoch 248: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.41 
2024-02-02 18:14:18,886 EPOCH 249
2024-02-02 18:14:23,594 Epoch 249: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.94 
2024-02-02 18:14:23,594 EPOCH 250
2024-02-02 18:14:28,331 [Epoch: 250 Step: 00008500] Batch Recognition Loss:   0.000293 => Gls Tokens per Sec:     2245 || Batch Translation Loss:   0.030954 => Txt Tokens per Sec:     6231 || Lr: 0.000100
2024-02-02 18:14:28,332 Epoch 250: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.32 
2024-02-02 18:14:28,332 EPOCH 251
2024-02-02 18:14:32,394 Epoch 251: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.30 
2024-02-02 18:14:32,395 EPOCH 252
2024-02-02 18:14:37,307 Epoch 252: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.09 
2024-02-02 18:14:37,307 EPOCH 253
2024-02-02 18:14:41,659 [Epoch: 253 Step: 00008600] Batch Recognition Loss:   0.000369 => Gls Tokens per Sec:     2296 || Batch Translation Loss:   0.213785 => Txt Tokens per Sec:     6381 || Lr: 0.000100
2024-02-02 18:14:41,962 Epoch 253: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.63 
2024-02-02 18:14:41,962 EPOCH 254
2024-02-02 18:14:46,531 Epoch 254: Total Training Recognition Loss 0.03  Total Training Translation Loss 10.99 
2024-02-02 18:14:46,532 EPOCH 255
2024-02-02 18:14:51,181 Epoch 255: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.87 
2024-02-02 18:14:51,182 EPOCH 256
2024-02-02 18:14:54,964 [Epoch: 256 Step: 00008700] Batch Recognition Loss:   0.000620 => Gls Tokens per Sec:     2539 || Batch Translation Loss:   0.150347 => Txt Tokens per Sec:     6926 || Lr: 0.000100
2024-02-02 18:14:55,697 Epoch 256: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.99 
2024-02-02 18:14:55,697 EPOCH 257
2024-02-02 18:15:00,415 Epoch 257: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.87 
2024-02-02 18:15:00,416 EPOCH 258
2024-02-02 18:15:05,389 Epoch 258: Total Training Recognition Loss 0.01  Total Training Translation Loss 5.68 
2024-02-02 18:15:05,390 EPOCH 259
2024-02-02 18:15:09,369 [Epoch: 259 Step: 00008800] Batch Recognition Loss:   0.000200 => Gls Tokens per Sec:     2190 || Batch Translation Loss:   0.109291 => Txt Tokens per Sec:     6185 || Lr: 0.000100
2024-02-02 18:15:10,134 Epoch 259: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.25 
2024-02-02 18:15:10,134 EPOCH 260
2024-02-02 18:15:14,822 Epoch 260: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.80 
2024-02-02 18:15:14,823 EPOCH 261
2024-02-02 18:15:19,766 Epoch 261: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.72 
2024-02-02 18:15:19,767 EPOCH 262
2024-02-02 18:15:23,025 [Epoch: 262 Step: 00008900] Batch Recognition Loss:   0.000247 => Gls Tokens per Sec:     2477 || Batch Translation Loss:   0.061088 => Txt Tokens per Sec:     6918 || Lr: 0.000100
2024-02-02 18:15:24,029 Epoch 262: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.89 
2024-02-02 18:15:24,029 EPOCH 263
2024-02-02 18:15:28,898 Epoch 263: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.07 
2024-02-02 18:15:28,899 EPOCH 264
2024-02-02 18:15:33,220 Epoch 264: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.74 
2024-02-02 18:15:33,221 EPOCH 265
2024-02-02 18:15:36,594 [Epoch: 265 Step: 00009000] Batch Recognition Loss:   0.000326 => Gls Tokens per Sec:     2204 || Batch Translation Loss:   0.226491 => Txt Tokens per Sec:     6060 || Lr: 0.000100
2024-02-02 18:15:38,060 Epoch 265: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.57 
2024-02-02 18:15:38,061 EPOCH 266
2024-02-02 18:15:42,247 Epoch 266: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.00 
2024-02-02 18:15:42,248 EPOCH 267
2024-02-02 18:15:47,117 Epoch 267: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.11 
2024-02-02 18:15:47,118 EPOCH 268
2024-02-02 18:15:50,142 [Epoch: 268 Step: 00009100] Batch Recognition Loss:   0.000210 => Gls Tokens per Sec:     2247 || Batch Translation Loss:   0.101374 => Txt Tokens per Sec:     6396 || Lr: 0.000100
2024-02-02 18:15:51,746 Epoch 268: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.49 
2024-02-02 18:15:51,747 EPOCH 269
2024-02-02 18:15:56,274 Epoch 269: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.48 
2024-02-02 18:15:56,274 EPOCH 270
2024-02-02 18:16:00,882 Epoch 270: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.42 
2024-02-02 18:16:00,883 EPOCH 271
2024-02-02 18:16:03,752 [Epoch: 271 Step: 00009200] Batch Recognition Loss:   0.001253 => Gls Tokens per Sec:     2144 || Batch Translation Loss:   0.145108 => Txt Tokens per Sec:     6082 || Lr: 0.000100
2024-02-02 18:16:05,494 Epoch 271: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.77 
2024-02-02 18:16:05,494 EPOCH 272
2024-02-02 18:16:10,062 Epoch 272: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.59 
2024-02-02 18:16:10,063 EPOCH 273
2024-02-02 18:16:14,575 Epoch 273: Total Training Recognition Loss 0.02  Total Training Translation Loss 10.74 
2024-02-02 18:16:14,575 EPOCH 274
2024-02-02 18:16:16,991 [Epoch: 274 Step: 00009300] Batch Recognition Loss:   0.000533 => Gls Tokens per Sec:     2282 || Batch Translation Loss:   0.463026 => Txt Tokens per Sec:     6448 || Lr: 0.000100
2024-02-02 18:16:19,210 Epoch 274: Total Training Recognition Loss 0.03  Total Training Translation Loss 11.65 
2024-02-02 18:16:19,211 EPOCH 275
2024-02-02 18:16:23,744 Epoch 275: Total Training Recognition Loss 0.04  Total Training Translation Loss 9.32 
2024-02-02 18:16:23,744 EPOCH 276
2024-02-02 18:16:28,441 Epoch 276: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.57 
2024-02-02 18:16:28,442 EPOCH 277
2024-02-02 18:16:30,443 [Epoch: 277 Step: 00009400] Batch Recognition Loss:   0.000566 => Gls Tokens per Sec:     2561 || Batch Translation Loss:   0.163891 => Txt Tokens per Sec:     7183 || Lr: 0.000100
2024-02-02 18:16:33,132 Epoch 277: Total Training Recognition Loss 0.03  Total Training Translation Loss 10.59 
2024-02-02 18:16:33,133 EPOCH 278
2024-02-02 18:16:37,932 Epoch 278: Total Training Recognition Loss 0.09  Total Training Translation Loss 8.67 
2024-02-02 18:16:37,932 EPOCH 279
2024-02-02 18:16:42,162 Epoch 279: Total Training Recognition Loss 0.02  Total Training Translation Loss 8.69 
2024-02-02 18:16:42,163 EPOCH 280
2024-02-02 18:16:44,295 [Epoch: 280 Step: 00009500] Batch Recognition Loss:   0.000515 => Gls Tokens per Sec:     1984 || Batch Translation Loss:   0.130096 => Txt Tokens per Sec:     5831 || Lr: 0.000100
2024-02-02 18:16:46,946 Epoch 280: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.22 
2024-02-02 18:16:46,946 EPOCH 281
2024-02-02 18:16:51,298 Epoch 281: Total Training Recognition Loss 0.01  Total Training Translation Loss 6.81 
2024-02-02 18:16:51,298 EPOCH 282
2024-02-02 18:16:55,327 Epoch 282: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.02 
2024-02-02 18:16:55,328 EPOCH 283
2024-02-02 18:16:56,693 [Epoch: 283 Step: 00009600] Batch Recognition Loss:   0.000622 => Gls Tokens per Sec:     2630 || Batch Translation Loss:   0.330013 => Txt Tokens per Sec:     7152 || Lr: 0.000100
2024-02-02 18:16:59,364 Epoch 283: Total Training Recognition Loss 0.01  Total Training Translation Loss 7.39 
2024-02-02 18:16:59,365 EPOCH 284
2024-02-02 18:17:04,158 Epoch 284: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.93 
2024-02-02 18:17:04,158 EPOCH 285
2024-02-02 18:17:08,548 Epoch 285: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.96 
2024-02-02 18:17:08,548 EPOCH 286
2024-02-02 18:17:10,416 [Epoch: 286 Step: 00009700] Batch Recognition Loss:   0.000619 => Gls Tokens per Sec:     1582 || Batch Translation Loss:   0.231258 => Txt Tokens per Sec:     4471 || Lr: 0.000100
2024-02-02 18:17:13,560 Epoch 286: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.26 
2024-02-02 18:17:13,561 EPOCH 287
2024-02-02 18:17:17,857 Epoch 287: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.93 
2024-02-02 18:17:17,857 EPOCH 288
2024-02-02 18:17:22,745 Epoch 288: Total Training Recognition Loss 0.10  Total Training Translation Loss 13.36 
2024-02-02 18:17:22,745 EPOCH 289
2024-02-02 18:17:24,063 [Epoch: 289 Step: 00009800] Batch Recognition Loss:   0.000374 => Gls Tokens per Sec:     1943 || Batch Translation Loss:   0.319923 => Txt Tokens per Sec:     5924 || Lr: 0.000100
2024-02-02 18:17:27,070 Epoch 289: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.70 
2024-02-02 18:17:27,071 EPOCH 290
2024-02-02 18:17:31,909 Epoch 290: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.93 
2024-02-02 18:17:31,909 EPOCH 291
2024-02-02 18:17:36,175 Epoch 291: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.93 
2024-02-02 18:17:36,176 EPOCH 292
2024-02-02 18:17:37,234 [Epoch: 292 Step: 00009900] Batch Recognition Loss:   0.000608 => Gls Tokens per Sec:     1580 || Batch Translation Loss:   0.229777 => Txt Tokens per Sec:     4805 || Lr: 0.000100
2024-02-02 18:17:41,070 Epoch 292: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.14 
2024-02-02 18:17:41,071 EPOCH 293
2024-02-02 18:17:45,451 Epoch 293: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.98 
2024-02-02 18:17:45,452 EPOCH 294
2024-02-02 18:17:50,222 Epoch 294: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.64 
2024-02-02 18:17:50,222 EPOCH 295
2024-02-02 18:17:50,662 [Epoch: 295 Step: 00010000] Batch Recognition Loss:   0.000336 => Gls Tokens per Sec:     2916 || Batch Translation Loss:   0.055200 => Txt Tokens per Sec:     8360 || Lr: 0.000100
2024-02-02 18:17:59,063 Hooray! New best validation result [eval_metric]!
2024-02-02 18:17:59,064 Saving new checkpoint.
2024-02-02 18:17:59,354 Validation result at epoch 295, step    10000: duration: 8.6914s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00089	Translation Loss: 91787.36719	PPL: 9751.40234
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 1.17	(BLEU-1: 11.33,	BLEU-2: 4.18,	BLEU-3: 2.06,	BLEU-4: 1.17)
	CHRF 17.52	ROUGE 9.83
2024-02-02 18:17:59,355 Logging Recognition and Translation Outputs
2024-02-02 18:17:59,355 ========================================================================================================================
2024-02-02 18:17:59,355 Logging Sequence: 159_139.00
2024-02-02 18:17:59,356 	Gloss Reference :	A B+C+D+E
2024-02-02 18:17:59,356 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 18:17:59,356 	Gloss Alignment :	         
2024-02-02 18:17:59,356 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 18:17:59,357 	Text Reference  :	***** **** **** he took time and     finally was ready for the asia cup where he      scored the century
2024-02-02 18:17:59,358 	Text Hypothesis :	dhoni said that he **** felt playing cricket was ***** *** *** **** *** not   present at     the man    
2024-02-02 18:17:59,358 	Text Alignment  :	I     I    I       D    S    S       S           D     D   D   D    D   S     S       S          S      
2024-02-02 18:17:59,358 ========================================================================================================================
2024-02-02 18:17:59,358 Logging Sequence: 159_159.00
2024-02-02 18:17:59,358 	Gloss Reference :	A B+C+D+E
2024-02-02 18:17:59,358 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 18:17:59,359 	Gloss Alignment :	         
2024-02-02 18:17:59,359 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 18:17:59,361 	Text Reference  :	he    said it       wasn't easy   the mind has to be focussed and he is glad that he is   back in form with   the asia     cup  century
2024-02-02 18:17:59,361 	Text Hypothesis :	kohli had  revealed that   before the **** *** ** ** ******** *** ** ** **** **** ** ball for  a  sai  before the incident went viral  
2024-02-02 18:17:59,361 	Text Alignment  :	S     S    S        S      S          D    D   D  D  D        D   D  D  D    D    D  S    S    S  S    S          S        S    S      
2024-02-02 18:17:59,361 ========================================================================================================================
2024-02-02 18:17:59,361 Logging Sequence: 103_8.00
2024-02-02 18:17:59,361 	Gloss Reference :	A B+C+D+E
2024-02-02 18:17:59,362 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 18:17:59,362 	Gloss Alignment :	         
2024-02-02 18:17:59,362 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 18:17:59,363 	Text Reference  :	were         going on  in birmingham england from 28th july  to        8th   august 2022 
2024-02-02 18:17:59,363 	Text Hypothesis :	commonwealth games cwg in ********** ******* **** **** which australia every 4      years
2024-02-02 18:17:59,363 	Text Alignment  :	S            S     S      D          D       D    D    S     S         S     S      S    
2024-02-02 18:17:59,363 ========================================================================================================================
2024-02-02 18:17:59,363 Logging Sequence: 164_546.00
2024-02-02 18:17:59,363 	Gloss Reference :	A B+C+D+E
2024-02-02 18:17:59,364 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 18:17:59,364 	Gloss Alignment :	         
2024-02-02 18:17:59,364 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 18:17:59,365 	Text Reference  :	******* * ** * ******* reliance has turned out     to be the ******** *** strongest company
2024-02-02 18:17:59,365 	Text Hypothesis :	package c is a special category of  mumbai indians to ** the audience was for       drinks 
2024-02-02 18:17:59,365 	Text Alignment  :	I       I I  I I       S        S   S      S          D      I        I   S         S      
2024-02-02 18:17:59,365 ========================================================================================================================
2024-02-02 18:17:59,365 Logging Sequence: 132_173.00
2024-02-02 18:17:59,365 	Gloss Reference :	A B+C+D+E
2024-02-02 18:17:59,366 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 18:17:59,366 	Gloss Alignment :	         
2024-02-02 18:17:59,366 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 18:17:59,367 	Text Reference  :	**** ********* ****** ******* *** ** ***** *** **** **** ** **** ****** *** ******* usman is   australia' first muslim player
2024-02-02 18:17:59,367 	Text Hypothesis :	bcci president sourav ganguly and kl rahul had said that he then asking the captain and   then going      on    his    face  
2024-02-02 18:17:59,367 	Text Alignment  :	I    I         I      I       I   I  I     I   I    I    I  I    I      I   I       S     S    S          S     S      S     
2024-02-02 18:17:59,367 ========================================================================================================================
2024-02-02 18:18:03,866 Epoch 295: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.94 
2024-02-02 18:18:03,867 EPOCH 296
2024-02-02 18:18:08,484 Epoch 296: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.98 
2024-02-02 18:18:08,484 EPOCH 297
2024-02-02 18:18:12,964 Epoch 297: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.01 
2024-02-02 18:18:12,964 EPOCH 298
2024-02-02 18:18:13,172 [Epoch: 298 Step: 00010100] Batch Recognition Loss:   0.000481 => Gls Tokens per Sec:     1893 || Batch Translation Loss:   0.012475 => Txt Tokens per Sec:     5039 || Lr: 0.000100
2024-02-02 18:18:17,572 Epoch 298: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.19 
2024-02-02 18:18:17,572 EPOCH 299
2024-02-02 18:18:22,088 Epoch 299: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.82 
2024-02-02 18:18:22,089 EPOCH 300
2024-02-02 18:18:26,690 [Epoch: 300 Step: 00010200] Batch Recognition Loss:   0.000209 => Gls Tokens per Sec:     2311 || Batch Translation Loss:   0.045971 => Txt Tokens per Sec:     6414 || Lr: 0.000100
2024-02-02 18:18:26,690 Epoch 300: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.99 
2024-02-02 18:18:26,690 EPOCH 301
2024-02-02 18:18:31,375 Epoch 301: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.59 
2024-02-02 18:18:31,375 EPOCH 302
2024-02-02 18:18:35,867 Epoch 302: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.19 
2024-02-02 18:18:35,868 EPOCH 303
2024-02-02 18:18:40,336 [Epoch: 303 Step: 00010300] Batch Recognition Loss:   0.000267 => Gls Tokens per Sec:     2236 || Batch Translation Loss:   0.060948 => Txt Tokens per Sec:     6215 || Lr: 0.000100
2024-02-02 18:18:40,556 Epoch 303: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.25 
2024-02-02 18:18:40,556 EPOCH 304
2024-02-02 18:18:45,064 Epoch 304: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.54 
2024-02-02 18:18:45,064 EPOCH 305
2024-02-02 18:18:49,243 Epoch 305: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.81 
2024-02-02 18:18:49,244 EPOCH 306
2024-02-02 18:18:53,551 [Epoch: 306 Step: 00010400] Batch Recognition Loss:   0.000401 => Gls Tokens per Sec:     2171 || Batch Translation Loss:   0.053449 => Txt Tokens per Sec:     5988 || Lr: 0.000100
2024-02-02 18:18:54,134 Epoch 306: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.94 
2024-02-02 18:18:54,135 EPOCH 307
2024-02-02 18:18:58,998 Epoch 307: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.21 
2024-02-02 18:18:58,998 EPOCH 308
2024-02-02 18:19:03,270 Epoch 308: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.51 
2024-02-02 18:19:03,270 EPOCH 309
2024-02-02 18:19:06,935 [Epoch: 309 Step: 00010500] Batch Recognition Loss:   0.000291 => Gls Tokens per Sec:     2377 || Batch Translation Loss:   0.215559 => Txt Tokens per Sec:     6791 || Lr: 0.000100
2024-02-02 18:19:07,665 Epoch 309: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.03 
2024-02-02 18:19:07,665 EPOCH 310
2024-02-02 18:19:12,443 Epoch 310: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.43 
2024-02-02 18:19:12,444 EPOCH 311
2024-02-02 18:19:17,281 Epoch 311: Total Training Recognition Loss 0.01  Total Training Translation Loss 7.10 
2024-02-02 18:19:17,282 EPOCH 312
2024-02-02 18:19:20,690 [Epoch: 312 Step: 00010600] Batch Recognition Loss:   0.000460 => Gls Tokens per Sec:     2369 || Batch Translation Loss:   0.553466 => Txt Tokens per Sec:     6681 || Lr: 0.000100
2024-02-02 18:19:21,698 Epoch 312: Total Training Recognition Loss 0.02  Total Training Translation Loss 8.15 
2024-02-02 18:19:21,698 EPOCH 313
2024-02-02 18:19:26,814 Epoch 313: Total Training Recognition Loss 0.02  Total Training Translation Loss 9.81 
2024-02-02 18:19:26,814 EPOCH 314
2024-02-02 18:19:31,546 Epoch 314: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.52 
2024-02-02 18:19:31,547 EPOCH 315
2024-02-02 18:19:34,799 [Epoch: 315 Step: 00010700] Batch Recognition Loss:   0.000420 => Gls Tokens per Sec:     2285 || Batch Translation Loss:   0.149969 => Txt Tokens per Sec:     6245 || Lr: 0.000100
2024-02-02 18:19:36,154 Epoch 315: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.60 
2024-02-02 18:19:36,154 EPOCH 316
2024-02-02 18:19:40,787 Epoch 316: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.68 
2024-02-02 18:19:40,788 EPOCH 317
2024-02-02 18:19:45,247 Epoch 317: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.83 
2024-02-02 18:19:45,247 EPOCH 318
2024-02-02 18:19:48,282 [Epoch: 318 Step: 00010800] Batch Recognition Loss:   0.000300 => Gls Tokens per Sec:     2238 || Batch Translation Loss:   0.076557 => Txt Tokens per Sec:     6041 || Lr: 0.000100
2024-02-02 18:19:49,992 Epoch 318: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.35 
2024-02-02 18:19:49,992 EPOCH 319
2024-02-02 18:19:54,380 Epoch 319: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.49 
2024-02-02 18:19:54,380 EPOCH 320
2024-02-02 18:19:59,256 Epoch 320: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.30 
2024-02-02 18:19:59,257 EPOCH 321
2024-02-02 18:20:01,840 [Epoch: 321 Step: 00010900] Batch Recognition Loss:   0.000290 => Gls Tokens per Sec:     2382 || Batch Translation Loss:   0.120389 => Txt Tokens per Sec:     6734 || Lr: 0.000100
2024-02-02 18:20:03,520 Epoch 321: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.17 
2024-02-02 18:20:03,520 EPOCH 322
2024-02-02 18:20:08,429 Epoch 322: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.04 
2024-02-02 18:20:08,430 EPOCH 323
2024-02-02 18:20:13,263 Epoch 323: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.08 
2024-02-02 18:20:13,263 EPOCH 324
2024-02-02 18:20:16,158 [Epoch: 324 Step: 00011000] Batch Recognition Loss:   0.000400 => Gls Tokens per Sec:     1905 || Batch Translation Loss:   0.087606 => Txt Tokens per Sec:     5572 || Lr: 0.000100
2024-02-02 18:20:18,015 Epoch 324: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.24 
2024-02-02 18:20:18,015 EPOCH 325
2024-02-02 18:20:22,800 Epoch 325: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.17 
2024-02-02 18:20:22,801 EPOCH 326
2024-02-02 18:20:27,408 Epoch 326: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.97 
2024-02-02 18:20:27,408 EPOCH 327
2024-02-02 18:20:29,370 [Epoch: 327 Step: 00011100] Batch Recognition Loss:   0.000206 => Gls Tokens per Sec:     2610 || Batch Translation Loss:   0.040313 => Txt Tokens per Sec:     7358 || Lr: 0.000100
2024-02-02 18:20:31,724 Epoch 327: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.23 
2024-02-02 18:20:31,725 EPOCH 328
2024-02-02 18:20:36,536 Epoch 328: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.22 
2024-02-02 18:20:36,536 EPOCH 329
2024-02-02 18:20:40,956 Epoch 329: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.81 
2024-02-02 18:20:40,956 EPOCH 330
2024-02-02 18:20:43,233 [Epoch: 330 Step: 00011200] Batch Recognition Loss:   0.000283 => Gls Tokens per Sec:     1969 || Batch Translation Loss:   0.076971 => Txt Tokens per Sec:     5636 || Lr: 0.000100
2024-02-02 18:20:45,635 Epoch 330: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.66 
2024-02-02 18:20:45,636 EPOCH 331
2024-02-02 18:20:50,071 Epoch 331: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.82 
2024-02-02 18:20:50,072 EPOCH 332
2024-02-02 18:20:54,760 Epoch 332: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.17 
2024-02-02 18:20:54,760 EPOCH 333
2024-02-02 18:20:55,985 [Epoch: 333 Step: 00011300] Batch Recognition Loss:   0.000331 => Gls Tokens per Sec:     3136 || Batch Translation Loss:   0.050949 => Txt Tokens per Sec:     8160 || Lr: 0.000100
2024-02-02 18:20:59,202 Epoch 333: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.15 
2024-02-02 18:20:59,203 EPOCH 334
2024-02-02 18:21:03,868 Epoch 334: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.18 
2024-02-02 18:21:03,868 EPOCH 335
2024-02-02 18:21:08,354 Epoch 335: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.73 
2024-02-02 18:21:08,355 EPOCH 336
2024-02-02 18:21:09,533 [Epoch: 336 Step: 00011400] Batch Recognition Loss:   0.000203 => Gls Tokens per Sec:     2506 || Batch Translation Loss:   0.308856 => Txt Tokens per Sec:     6414 || Lr: 0.000100
2024-02-02 18:21:12,942 Epoch 336: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.95 
2024-02-02 18:21:12,942 EPOCH 337
2024-02-02 18:21:17,486 Epoch 337: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.69 
2024-02-02 18:21:17,487 EPOCH 338
2024-02-02 18:21:22,042 Epoch 338: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.21 
2024-02-02 18:21:22,043 EPOCH 339
2024-02-02 18:21:22,933 [Epoch: 339 Step: 00011500] Batch Recognition Loss:   0.000408 => Gls Tokens per Sec:     2880 || Batch Translation Loss:   0.103807 => Txt Tokens per Sec:     8135 || Lr: 0.000100
2024-02-02 18:21:26,587 Epoch 339: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.05 
2024-02-02 18:21:26,587 EPOCH 340
2024-02-02 18:21:31,107 Epoch 340: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.12 
2024-02-02 18:21:31,107 EPOCH 341
2024-02-02 18:21:35,642 Epoch 341: Total Training Recognition Loss 0.01  Total Training Translation Loss 5.80 
2024-02-02 18:21:35,643 EPOCH 342
2024-02-02 18:21:36,338 [Epoch: 342 Step: 00011600] Batch Recognition Loss:   0.000620 => Gls Tokens per Sec:     2765 || Batch Translation Loss:   0.231398 => Txt Tokens per Sec:     7337 || Lr: 0.000100
2024-02-02 18:21:40,169 Epoch 342: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.77 
2024-02-02 18:21:40,169 EPOCH 343
2024-02-02 18:21:44,848 Epoch 343: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.60 
2024-02-02 18:21:44,848 EPOCH 344
2024-02-02 18:21:49,300 Epoch 344: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.03 
2024-02-02 18:21:49,300 EPOCH 345
2024-02-02 18:21:49,729 [Epoch: 345 Step: 00011700] Batch Recognition Loss:   0.000278 => Gls Tokens per Sec:     2989 || Batch Translation Loss:   0.123845 => Txt Tokens per Sec:     8513 || Lr: 0.000100
2024-02-02 18:21:54,041 Epoch 345: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.01 
2024-02-02 18:21:54,042 EPOCH 346
2024-02-02 18:21:58,443 Epoch 346: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.11 
2024-02-02 18:21:58,443 EPOCH 347
2024-02-02 18:22:03,201 Epoch 347: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.59 
2024-02-02 18:22:03,202 EPOCH 348
2024-02-02 18:22:03,367 [Epoch: 348 Step: 00011800] Batch Recognition Loss:   0.000227 => Gls Tokens per Sec:     3894 || Batch Translation Loss:   0.124470 => Txt Tokens per Sec:    10301 || Lr: 0.000100
2024-02-02 18:22:07,594 Epoch 348: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.62 
2024-02-02 18:22:07,594 EPOCH 349
2024-02-02 18:22:12,340 Epoch 349: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.87 
2024-02-02 18:22:12,340 EPOCH 350
2024-02-02 18:22:16,640 [Epoch: 350 Step: 00011900] Batch Recognition Loss:   0.000343 => Gls Tokens per Sec:     2473 || Batch Translation Loss:   0.076486 => Txt Tokens per Sec:     6865 || Lr: 0.000100
2024-02-02 18:22:16,640 Epoch 350: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.94 
2024-02-02 18:22:16,640 EPOCH 351
2024-02-02 18:22:21,326 Epoch 351: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.79 
2024-02-02 18:22:21,327 EPOCH 352
2024-02-02 18:22:25,950 Epoch 352: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.08 
2024-02-02 18:22:25,951 EPOCH 353
2024-02-02 18:22:30,393 [Epoch: 353 Step: 00012000] Batch Recognition Loss:   0.000220 => Gls Tokens per Sec:     2250 || Batch Translation Loss:   0.033403 => Txt Tokens per Sec:     6184 || Lr: 0.000100
2024-02-02 18:22:38,933 Validation result at epoch 353, step    12000: duration: 8.5398s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00060	Translation Loss: 93047.62500	PPL: 11062.10156
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.88	(BLEU-1: 10.78,	BLEU-2: 3.71,	BLEU-3: 1.66,	BLEU-4: 0.88)
	CHRF 17.40	ROUGE 9.44
2024-02-02 18:22:38,934 Logging Recognition and Translation Outputs
2024-02-02 18:22:38,934 ========================================================================================================================
2024-02-02 18:22:38,934 Logging Sequence: 177_50.00
2024-02-02 18:22:38,935 	Gloss Reference :	A B+C+D+E
2024-02-02 18:22:38,935 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 18:22:38,935 	Gloss Alignment :	         
2024-02-02 18:22:38,935 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 18:22:38,936 	Text Reference  :	a similar reward of  rs        50000  was    announced for          information against his associate ajay kumar 
2024-02-02 18:22:38,936 	Text Hypothesis :	* ******* ****** but rajasthan royals issued a         non-bailable warrant     against *** ********* **** sushil
2024-02-02 18:22:38,936 	Text Alignment  :	D D       D      S   S         S      S      S         S            S                   D   D         D    S     
2024-02-02 18:22:38,937 ========================================================================================================================
2024-02-02 18:22:38,937 Logging Sequence: 122_86.00
2024-02-02 18:22:38,937 	Gloss Reference :	A B+C+D+E
2024-02-02 18:22:38,937 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 18:22:38,937 	Gloss Alignment :	         
2024-02-02 18:22:38,937 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 18:22:38,938 	Text Reference  :	after winning chanu spoke to the media and ****** said
2024-02-02 18:22:38,938 	Text Hypothesis :	i     am      very  hard  to *** train and scored 3175
2024-02-02 18:22:38,938 	Text Alignment  :	S     S       S     S        D   S         I      S   
2024-02-02 18:22:38,938 ========================================================================================================================
2024-02-02 18:22:38,938 Logging Sequence: 165_27.00
2024-02-02 18:22:38,939 	Gloss Reference :	A B+C+D+E
2024-02-02 18:22:38,939 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 18:22:38,939 	Gloss Alignment :	         
2024-02-02 18:22:38,939 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 18:22:38,940 	Text Reference  :	so then they change their      routes some people believe in  this while some don't    
2024-02-02 18:22:38,940 	Text Hypothesis :	** **** it   is     disgusting that   if   a      huge    fan has  been  very difficult
2024-02-02 18:22:38,940 	Text Alignment  :	D  D    S    S      S          S      S    S      S       S   S    S     S    S        
2024-02-02 18:22:38,940 ========================================================================================================================
2024-02-02 18:22:38,940 Logging Sequence: 70_65.00
2024-02-02 18:22:38,941 	Gloss Reference :	A B+C+D+E
2024-02-02 18:22:38,941 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 18:22:38,941 	Gloss Alignment :	         
2024-02-02 18:22:38,941 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 18:22:38,942 	Text Reference  :	during the press conference a table was placed in front   of the ******* ******** *** media    
2024-02-02 18:22:38,942 	Text Hypothesis :	****** *** ***** ********** * ***** *** this   is because of the various sponsors for marketing
2024-02-02 18:22:38,942 	Text Alignment  :	D      D   D     D          D D     D   S      S  S              I       I        I   S        
2024-02-02 18:22:38,942 ========================================================================================================================
2024-02-02 18:22:38,942 Logging Sequence: 149_65.00
2024-02-02 18:22:38,942 	Gloss Reference :	A B+C+D+E
2024-02-02 18:22:38,943 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 18:22:38,943 	Gloss Alignment :	         
2024-02-02 18:22:38,943 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 18:22:38,945 	Text Reference  :	at 6am on 6th november 2022  the     police reached  sri lankan team's      hotel in  sydney  australia's central business district cbd 
2024-02-02 18:22:38,945 	Text Hypothesis :	** *** ** *** the      woman alleged that   danushka had sexual intercourse with  her without her         consent which    means    rape
2024-02-02 18:22:38,945 	Text Alignment  :	D  D   D  D   S        S     S       S      S        S   S      S           S     S   S       S           S       S        S        S   
2024-02-02 18:22:38,945 ========================================================================================================================
2024-02-02 18:22:39,347 Epoch 353: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.61 
2024-02-02 18:22:39,348 EPOCH 354
2024-02-02 18:22:44,411 Epoch 354: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.48 
2024-02-02 18:22:44,411 EPOCH 355
2024-02-02 18:22:48,954 Epoch 355: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.13 
2024-02-02 18:22:48,954 EPOCH 356
2024-02-02 18:22:52,776 [Epoch: 356 Step: 00012100] Batch Recognition Loss:   0.000337 => Gls Tokens per Sec:     2447 || Batch Translation Loss:   0.071550 => Txt Tokens per Sec:     6769 || Lr: 0.000100
2024-02-02 18:22:53,421 Epoch 356: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.46 
2024-02-02 18:22:53,422 EPOCH 357
2024-02-02 18:22:58,142 Epoch 357: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.95 
2024-02-02 18:22:58,142 EPOCH 358
2024-02-02 18:23:02,617 Epoch 358: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.94 
2024-02-02 18:23:02,618 EPOCH 359
2024-02-02 18:23:06,471 [Epoch: 359 Step: 00012200] Batch Recognition Loss:   0.000195 => Gls Tokens per Sec:     2261 || Batch Translation Loss:   0.033091 => Txt Tokens per Sec:     6326 || Lr: 0.000100
2024-02-02 18:23:07,234 Epoch 359: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.73 
2024-02-02 18:23:07,234 EPOCH 360
2024-02-02 18:23:11,266 Epoch 360: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.32 
2024-02-02 18:23:11,266 EPOCH 361
2024-02-02 18:23:16,108 Epoch 361: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.57 
2024-02-02 18:23:16,109 EPOCH 362
2024-02-02 18:23:19,757 [Epoch: 362 Step: 00012300] Batch Recognition Loss:   0.000259 => Gls Tokens per Sec:     2281 || Batch Translation Loss:   0.208105 => Txt Tokens per Sec:     6284 || Lr: 0.000100
2024-02-02 18:23:20,862 Epoch 362: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.30 
2024-02-02 18:23:20,862 EPOCH 363
2024-02-02 18:23:25,387 Epoch 363: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.29 
2024-02-02 18:23:25,387 EPOCH 364
2024-02-02 18:23:30,045 Epoch 364: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.50 
2024-02-02 18:23:30,046 EPOCH 365
2024-02-02 18:23:33,185 [Epoch: 365 Step: 00012400] Batch Recognition Loss:   0.000325 => Gls Tokens per Sec:     2368 || Batch Translation Loss:   0.093925 => Txt Tokens per Sec:     6341 || Lr: 0.000100
2024-02-02 18:23:34,559 Epoch 365: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.89 
2024-02-02 18:23:34,560 EPOCH 366
2024-02-02 18:23:39,245 Epoch 366: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.71 
2024-02-02 18:23:39,245 EPOCH 367
2024-02-02 18:23:43,696 Epoch 367: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.11 
2024-02-02 18:23:43,696 EPOCH 368
2024-02-02 18:23:46,506 [Epoch: 368 Step: 00012500] Batch Recognition Loss:   0.000491 => Gls Tokens per Sec:     2417 || Batch Translation Loss:   0.065190 => Txt Tokens per Sec:     6628 || Lr: 0.000100
2024-02-02 18:23:48,425 Epoch 368: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.07 
2024-02-02 18:23:48,425 EPOCH 369
2024-02-02 18:23:52,761 Epoch 369: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.50 
2024-02-02 18:23:52,761 EPOCH 370
2024-02-02 18:23:56,818 Epoch 370: Total Training Recognition Loss 0.01  Total Training Translation Loss 5.03 
2024-02-02 18:23:56,818 EPOCH 371
2024-02-02 18:23:59,403 [Epoch: 371 Step: 00012600] Batch Recognition Loss:   0.000251 => Gls Tokens per Sec:     2380 || Batch Translation Loss:   0.118819 => Txt Tokens per Sec:     6790 || Lr: 0.000100
2024-02-02 18:24:01,135 Epoch 371: Total Training Recognition Loss 0.01  Total Training Translation Loss 5.52 
2024-02-02 18:24:01,136 EPOCH 372
2024-02-02 18:24:05,968 Epoch 372: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.68 
2024-02-02 18:24:05,968 EPOCH 373
2024-02-02 18:24:10,232 Epoch 373: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.48 
2024-02-02 18:24:10,232 EPOCH 374
2024-02-02 18:24:12,649 [Epoch: 374 Step: 00012700] Batch Recognition Loss:   0.000259 => Gls Tokens per Sec:     2281 || Batch Translation Loss:   0.105803 => Txt Tokens per Sec:     6158 || Lr: 0.000100
2024-02-02 18:24:15,106 Epoch 374: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.79 
2024-02-02 18:24:15,106 EPOCH 375
2024-02-02 18:24:19,657 Epoch 375: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.10 
2024-02-02 18:24:19,658 EPOCH 376
2024-02-02 18:24:24,274 Epoch 376: Total Training Recognition Loss 0.02  Total Training Translation Loss 9.55 
2024-02-02 18:24:24,274 EPOCH 377
2024-02-02 18:24:26,962 [Epoch: 377 Step: 00012800] Batch Recognition Loss:   0.001049 => Gls Tokens per Sec:     1813 || Batch Translation Loss:   0.305884 => Txt Tokens per Sec:     5414 || Lr: 0.000100
2024-02-02 18:24:29,147 Epoch 377: Total Training Recognition Loss 0.03  Total Training Translation Loss 10.50 
2024-02-02 18:24:29,147 EPOCH 378
2024-02-02 18:24:33,424 Epoch 378: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.38 
2024-02-02 18:24:33,424 EPOCH 379
2024-02-02 18:24:38,280 Epoch 379: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.32 
2024-02-02 18:24:38,280 EPOCH 380
2024-02-02 18:24:40,205 [Epoch: 380 Step: 00012900] Batch Recognition Loss:   0.000891 => Gls Tokens per Sec:     2200 || Batch Translation Loss:   0.166611 => Txt Tokens per Sec:     6416 || Lr: 0.000100
2024-02-02 18:24:42,963 Epoch 380: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.42 
2024-02-02 18:24:42,964 EPOCH 381
2024-02-02 18:24:47,597 Epoch 381: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.88 
2024-02-02 18:24:47,598 EPOCH 382
2024-02-02 18:24:52,154 Epoch 382: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.54 
2024-02-02 18:24:52,155 EPOCH 383
2024-02-02 18:24:53,426 [Epoch: 383 Step: 00013000] Batch Recognition Loss:   0.000266 => Gls Tokens per Sec:     3024 || Batch Translation Loss:   0.030539 => Txt Tokens per Sec:     7560 || Lr: 0.000100
2024-02-02 18:24:56,763 Epoch 383: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.29 
2024-02-02 18:24:56,763 EPOCH 384
2024-02-02 18:25:01,301 Epoch 384: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.10 
2024-02-02 18:25:01,302 EPOCH 385
2024-02-02 18:25:05,920 Epoch 385: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.58 
2024-02-02 18:25:05,920 EPOCH 386
2024-02-02 18:25:07,617 [Epoch: 386 Step: 00013100] Batch Recognition Loss:   0.000433 => Gls Tokens per Sec:     1887 || Batch Translation Loss:   0.129882 => Txt Tokens per Sec:     5648 || Lr: 0.000100
2024-02-02 18:25:10,605 Epoch 386: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.16 
2024-02-02 18:25:10,605 EPOCH 387
2024-02-02 18:25:15,064 Epoch 387: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.07 
2024-02-02 18:25:15,064 EPOCH 388
2024-02-02 18:25:19,833 Epoch 388: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.89 
2024-02-02 18:25:19,833 EPOCH 389
2024-02-02 18:25:21,212 [Epoch: 389 Step: 00013200] Batch Recognition Loss:   0.000500 => Gls Tokens per Sec:     1858 || Batch Translation Loss:   0.093834 => Txt Tokens per Sec:     5464 || Lr: 0.000100
2024-02-02 18:25:24,194 Epoch 389: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.61 
2024-02-02 18:25:24,194 EPOCH 390
2024-02-02 18:25:28,957 Epoch 390: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.32 
2024-02-02 18:25:28,958 EPOCH 391
2024-02-02 18:25:33,326 Epoch 391: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.14 
2024-02-02 18:25:33,326 EPOCH 392
2024-02-02 18:25:33,940 [Epoch: 392 Step: 00013300] Batch Recognition Loss:   0.000260 => Gls Tokens per Sec:     3132 || Batch Translation Loss:   0.055241 => Txt Tokens per Sec:     8295 || Lr: 0.000100
2024-02-02 18:25:38,138 Epoch 392: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.79 
2024-02-02 18:25:38,139 EPOCH 393
2024-02-02 18:25:42,448 Epoch 393: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.78 
2024-02-02 18:25:42,449 EPOCH 394
2024-02-02 18:25:47,538 Epoch 394: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.97 
2024-02-02 18:25:47,539 EPOCH 395
2024-02-02 18:25:48,278 [Epoch: 395 Step: 00013400] Batch Recognition Loss:   0.000233 => Gls Tokens per Sec:     1734 || Batch Translation Loss:   0.050061 => Txt Tokens per Sec:     4596 || Lr: 0.000100
2024-02-02 18:25:52,212 Epoch 395: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.69 
2024-02-02 18:25:52,212 EPOCH 396
2024-02-02 18:25:57,119 Epoch 396: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.09 
2024-02-02 18:25:57,119 EPOCH 397
2024-02-02 18:26:01,316 Epoch 397: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.61 
2024-02-02 18:26:01,317 EPOCH 398
2024-02-02 18:26:01,930 [Epoch: 398 Step: 00013500] Batch Recognition Loss:   0.000390 => Gls Tokens per Sec:     1047 || Batch Translation Loss:   0.040828 => Txt Tokens per Sec:     3642 || Lr: 0.000100
2024-02-02 18:26:06,242 Epoch 398: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.17 
2024-02-02 18:26:06,242 EPOCH 399
2024-02-02 18:26:10,479 Epoch 399: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.29 
2024-02-02 18:26:10,480 EPOCH 400
2024-02-02 18:26:15,428 [Epoch: 400 Step: 00013600] Batch Recognition Loss:   0.000595 => Gls Tokens per Sec:     2149 || Batch Translation Loss:   0.011950 => Txt Tokens per Sec:     5966 || Lr: 0.000100
2024-02-02 18:26:15,428 Epoch 400: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.78 
2024-02-02 18:26:15,428 EPOCH 401
2024-02-02 18:26:19,771 Epoch 401: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.26 
2024-02-02 18:26:19,772 EPOCH 402
2024-02-02 18:26:24,637 Epoch 402: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.56 
2024-02-02 18:26:24,637 EPOCH 403
2024-02-02 18:26:28,485 [Epoch: 403 Step: 00013700] Batch Recognition Loss:   0.000261 => Gls Tokens per Sec:     2662 || Batch Translation Loss:   0.052609 => Txt Tokens per Sec:     7316 || Lr: 0.000100
2024-02-02 18:26:28,866 Epoch 403: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.69 
2024-02-02 18:26:28,866 EPOCH 404
2024-02-02 18:26:33,754 Epoch 404: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.87 
2024-02-02 18:26:33,755 EPOCH 405
2024-02-02 18:26:38,130 Epoch 405: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.68 
2024-02-02 18:26:38,131 EPOCH 406
2024-02-02 18:26:42,495 [Epoch: 406 Step: 00013800] Batch Recognition Loss:   0.000262 => Gls Tokens per Sec:     2144 || Batch Translation Loss:   0.032082 => Txt Tokens per Sec:     5981 || Lr: 0.000100
2024-02-02 18:26:42,931 Epoch 406: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.65 
2024-02-02 18:26:42,931 EPOCH 407
2024-02-02 18:26:47,298 Epoch 407: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.19 
2024-02-02 18:26:47,299 EPOCH 408
2024-02-02 18:26:52,056 Epoch 408: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.74 
2024-02-02 18:26:52,057 EPOCH 409
2024-02-02 18:26:55,526 [Epoch: 409 Step: 00013900] Batch Recognition Loss:   0.000413 => Gls Tokens per Sec:     2510 || Batch Translation Loss:   0.103959 => Txt Tokens per Sec:     6895 || Lr: 0.000100
2024-02-02 18:26:56,456 Epoch 409: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.35 
2024-02-02 18:26:56,457 EPOCH 410
2024-02-02 18:27:01,157 Epoch 410: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.25 
2024-02-02 18:27:01,158 EPOCH 411
2024-02-02 18:27:05,667 Epoch 411: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.85 
2024-02-02 18:27:05,668 EPOCH 412
2024-02-02 18:27:09,438 [Epoch: 412 Step: 00014000] Batch Recognition Loss:   0.000270 => Gls Tokens per Sec:     2141 || Batch Translation Loss:   0.049406 => Txt Tokens per Sec:     6020 || Lr: 0.000100
2024-02-02 18:27:17,914 Validation result at epoch 412, step    14000: duration: 8.4760s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00062	Translation Loss: 94365.78906	PPL: 12621.91211
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.77	(BLEU-1: 10.75,	BLEU-2: 3.35,	BLEU-3: 1.45,	BLEU-4: 0.77)
	CHRF 16.67	ROUGE 9.34
2024-02-02 18:27:17,915 Logging Recognition and Translation Outputs
2024-02-02 18:27:17,915 ========================================================================================================================
2024-02-02 18:27:17,915 Logging Sequence: 141_40.00
2024-02-02 18:27:17,916 	Gloss Reference :	A B+C+D+E
2024-02-02 18:27:17,916 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 18:27:17,916 	Gloss Alignment :	         
2024-02-02 18:27:17,916 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 18:27:17,917 	Text Reference  :	got infected with covid-19 he   was     quarantined and  could not        take part in the warmup match   
2024-02-02 18:27:17,918 	Text Hypothesis :	*** and      told him      that mirabai who         gave a     five-match the  2000 on the ****** olympics
2024-02-02 18:27:17,918 	Text Alignment  :	D   S        S    S        S    S       S           S    S     S          S    S    S      D      S       
2024-02-02 18:27:17,918 ========================================================================================================================
2024-02-02 18:27:17,918 Logging Sequence: 117_37.00
2024-02-02 18:27:17,918 	Gloss Reference :	A B+C+D+E
2024-02-02 18:27:17,918 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 18:27:17,919 	Gloss Alignment :	         
2024-02-02 18:27:17,919 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 18:27:17,919 	Text Reference  :	****** shikhar dhawan put   up   a    wonderful performance scoring 98     runs  
2024-02-02 18:27:17,920 	Text Hypothesis :	krunal pandya  and    rahul took part of        pant        and     hardik pandya
2024-02-02 18:27:17,920 	Text Alignment  :	I      S       S      S     S    S    S         S           S       S      S     
2024-02-02 18:27:17,920 ========================================================================================================================
2024-02-02 18:27:17,920 Logging Sequence: 64_13.00
2024-02-02 18:27:17,920 	Gloss Reference :	A B+C+D+E
2024-02-02 18:27:17,920 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 18:27:17,920 	Gloss Alignment :	         
2024-02-02 18:27:17,920 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 18:27:17,921 	Text Reference  :	arrangements were made to move all  the    ipl    matches to the wankhede stadium in   mumbai
2024-02-02 18:27:17,921 	Text Hypothesis :	************ **** **** ** and  then deepak chahar not     to *** ******** play    very well  
2024-02-02 18:27:17,922 	Text Alignment  :	D            D    D    D  S    S    S      S      S          D   D        S       S    S     
2024-02-02 18:27:17,922 ========================================================================================================================
2024-02-02 18:27:17,922 Logging Sequence: 98_121.00
2024-02-02 18:27:17,922 	Gloss Reference :	A B+C+D+E
2024-02-02 18:27:17,922 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 18:27:17,922 	Gloss Alignment :	         
2024-02-02 18:27:17,922 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 18:27:17,923 	Text Reference  :	so then england   legends and     bangladesh legends were added to    the tournament
2024-02-02 18:27:17,923 	Text Hypothesis :	** **** according to      various news       reports were ***** taken the tickets   
2024-02-02 18:27:17,923 	Text Alignment  :	D  D    S         S       S       S          S            D     S         S         
2024-02-02 18:27:17,923 ========================================================================================================================
2024-02-02 18:27:17,924 Logging Sequence: 179_414.00
2024-02-02 18:27:17,924 	Gloss Reference :	A B+C+D+E
2024-02-02 18:27:17,924 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 18:27:17,924 	Gloss Alignment :	         
2024-02-02 18:27:17,924 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 18:27:17,926 	Text Reference  :	we      could not        travel to   delhi as there was  a     lockdown in   our home town haryana 
2024-02-02 18:27:17,926 	Text Hypothesis :	however zabka federation said   that he    is a     fine other vehicles that she is   the  passport
2024-02-02 18:27:17,926 	Text Alignment  :	S       S     S          S      S    S     S  S     S    S     S        S    S   S    S    S       
2024-02-02 18:27:17,926 ========================================================================================================================
2024-02-02 18:27:18,861 Epoch 412: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.98 
2024-02-02 18:27:18,861 EPOCH 413
2024-02-02 18:27:23,728 Epoch 413: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.79 
2024-02-02 18:27:23,729 EPOCH 414
2024-02-02 18:27:28,354 Epoch 414: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.35 
2024-02-02 18:27:28,355 EPOCH 415
2024-02-02 18:27:31,528 [Epoch: 415 Step: 00014100] Batch Recognition Loss:   0.000450 => Gls Tokens per Sec:     2342 || Batch Translation Loss:   0.121707 => Txt Tokens per Sec:     6428 || Lr: 0.000100
2024-02-02 18:27:32,953 Epoch 415: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.93 
2024-02-02 18:27:32,953 EPOCH 416
2024-02-02 18:27:37,544 Epoch 416: Total Training Recognition Loss 0.01  Total Training Translation Loss 5.92 
2024-02-02 18:27:37,544 EPOCH 417
2024-02-02 18:27:42,185 Epoch 417: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.49 
2024-02-02 18:27:42,185 EPOCH 418
2024-02-02 18:27:45,325 [Epoch: 418 Step: 00014200] Batch Recognition Loss:   0.000633 => Gls Tokens per Sec:     2243 || Batch Translation Loss:   0.155702 => Txt Tokens per Sec:     6286 || Lr: 0.000100
2024-02-02 18:27:46,721 Epoch 418: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.36 
2024-02-02 18:27:46,721 EPOCH 419
2024-02-02 18:27:51,411 Epoch 419: Total Training Recognition Loss 0.01  Total Training Translation Loss 5.29 
2024-02-02 18:27:51,411 EPOCH 420
2024-02-02 18:27:55,888 Epoch 420: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.29 
2024-02-02 18:27:55,888 EPOCH 421
2024-02-02 18:27:58,749 [Epoch: 421 Step: 00014300] Batch Recognition Loss:   0.000555 => Gls Tokens per Sec:     2151 || Batch Translation Loss:   0.392785 => Txt Tokens per Sec:     6039 || Lr: 0.000100
2024-02-02 18:28:00,628 Epoch 421: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.20 
2024-02-02 18:28:00,628 EPOCH 422
2024-02-02 18:28:05,317 Epoch 422: Total Training Recognition Loss 0.01  Total Training Translation Loss 5.15 
2024-02-02 18:28:05,317 EPOCH 423
2024-02-02 18:28:10,157 Epoch 423: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.91 
2024-02-02 18:28:10,157 EPOCH 424
2024-02-02 18:28:12,340 [Epoch: 424 Step: 00014400] Batch Recognition Loss:   0.000680 => Gls Tokens per Sec:     2639 || Batch Translation Loss:   0.057762 => Txt Tokens per Sec:     7406 || Lr: 0.000100
2024-02-02 18:28:14,363 Epoch 424: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.74 
2024-02-02 18:28:14,363 EPOCH 425
2024-02-02 18:28:19,161 Epoch 425: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.35 
2024-02-02 18:28:19,162 EPOCH 426
2024-02-02 18:28:23,497 Epoch 426: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.85 
2024-02-02 18:28:23,497 EPOCH 427
2024-02-02 18:28:25,660 [Epoch: 427 Step: 00014500] Batch Recognition Loss:   0.000230 => Gls Tokens per Sec:     2369 || Batch Translation Loss:   0.083314 => Txt Tokens per Sec:     6853 || Lr: 0.000100
2024-02-02 18:28:28,358 Epoch 427: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.76 
2024-02-02 18:28:28,359 EPOCH 428
2024-02-02 18:28:32,596 Epoch 428: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.30 
2024-02-02 18:28:32,596 EPOCH 429
2024-02-02 18:28:37,498 Epoch 429: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.44 
2024-02-02 18:28:37,498 EPOCH 430
2024-02-02 18:28:39,187 [Epoch: 430 Step: 00014600] Batch Recognition Loss:   0.000263 => Gls Tokens per Sec:     2655 || Batch Translation Loss:   0.024820 => Txt Tokens per Sec:     7344 || Lr: 0.000100
2024-02-02 18:28:41,732 Epoch 430: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.56 
2024-02-02 18:28:41,732 EPOCH 431
2024-02-02 18:28:46,664 Epoch 431: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.98 
2024-02-02 18:28:46,664 EPOCH 432
2024-02-02 18:28:50,900 Epoch 432: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.49 
2024-02-02 18:28:50,901 EPOCH 433
2024-02-02 18:28:52,979 [Epoch: 433 Step: 00014700] Batch Recognition Loss:   0.000258 => Gls Tokens per Sec:     1849 || Batch Translation Loss:   0.073308 => Txt Tokens per Sec:     5405 || Lr: 0.000100
2024-02-02 18:28:55,819 Epoch 433: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.37 
2024-02-02 18:28:55,819 EPOCH 434
2024-02-02 18:29:00,069 Epoch 434: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.41 
2024-02-02 18:29:00,069 EPOCH 435
2024-02-02 18:29:04,942 Epoch 435: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.18 
2024-02-02 18:29:04,943 EPOCH 436
2024-02-02 18:29:06,305 [Epoch: 436 Step: 00014800] Batch Recognition Loss:   0.000222 => Gls Tokens per Sec:     2351 || Batch Translation Loss:   0.088242 => Txt Tokens per Sec:     6509 || Lr: 0.000100
2024-02-02 18:29:09,260 Epoch 436: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.53 
2024-02-02 18:29:09,260 EPOCH 437
2024-02-02 18:29:14,115 Epoch 437: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.42 
2024-02-02 18:29:14,116 EPOCH 438
2024-02-02 18:29:18,356 Epoch 438: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.41 
2024-02-02 18:29:18,357 EPOCH 439
2024-02-02 18:29:19,415 [Epoch: 439 Step: 00014900] Batch Recognition Loss:   0.000255 => Gls Tokens per Sec:     2423 || Batch Translation Loss:   0.057359 => Txt Tokens per Sec:     6377 || Lr: 0.000100
2024-02-02 18:29:23,296 Epoch 439: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.31 
2024-02-02 18:29:23,296 EPOCH 440
2024-02-02 18:29:27,970 Epoch 440: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.15 
2024-02-02 18:29:27,971 EPOCH 441
2024-02-02 18:29:32,466 Epoch 441: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.01 
2024-02-02 18:29:32,466 EPOCH 442
2024-02-02 18:29:33,041 [Epoch: 442 Step: 00015000] Batch Recognition Loss:   0.000241 => Gls Tokens per Sec:     3345 || Batch Translation Loss:   0.043436 => Txt Tokens per Sec:     8591 || Lr: 0.000100
2024-02-02 18:29:36,950 Epoch 442: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.92 
2024-02-02 18:29:36,951 EPOCH 443
2024-02-02 18:29:41,534 Epoch 443: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.06 
2024-02-02 18:29:41,534 EPOCH 444
2024-02-02 18:29:45,976 Epoch 444: Total Training Recognition Loss 0.01  Total Training Translation Loss 7.73 
2024-02-02 18:29:45,976 EPOCH 445
2024-02-02 18:29:46,391 [Epoch: 445 Step: 00015100] Batch Recognition Loss:   0.000360 => Gls Tokens per Sec:     3092 || Batch Translation Loss:   0.170931 => Txt Tokens per Sec:     7483 || Lr: 0.000100
2024-02-02 18:29:50,959 Epoch 445: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.55 
2024-02-02 18:29:50,960 EPOCH 446
2024-02-02 18:29:55,647 Epoch 446: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.07 
2024-02-02 18:29:55,647 EPOCH 447
2024-02-02 18:30:00,230 Epoch 447: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.45 
2024-02-02 18:30:00,230 EPOCH 448
2024-02-02 18:30:00,546 [Epoch: 448 Step: 00015200] Batch Recognition Loss:   0.000412 => Gls Tokens per Sec:     2030 || Batch Translation Loss:   0.044069 => Txt Tokens per Sec:     6451 || Lr: 0.000100
2024-02-02 18:30:04,294 Epoch 448: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.31 
2024-02-02 18:30:04,294 EPOCH 449
2024-02-02 18:30:09,117 Epoch 449: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.41 
2024-02-02 18:30:09,118 EPOCH 450
2024-02-02 18:30:13,783 [Epoch: 450 Step: 00015300] Batch Recognition Loss:   0.000320 => Gls Tokens per Sec:     2279 || Batch Translation Loss:   0.032065 => Txt Tokens per Sec:     6327 || Lr: 0.000100
2024-02-02 18:30:13,784 Epoch 450: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.10 
2024-02-02 18:30:13,784 EPOCH 451
2024-02-02 18:30:18,442 Epoch 451: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.09 
2024-02-02 18:30:18,442 EPOCH 452
2024-02-02 18:30:22,493 Epoch 452: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.56 
2024-02-02 18:30:22,494 EPOCH 453
2024-02-02 18:30:27,046 [Epoch: 453 Step: 00015400] Batch Recognition Loss:   0.000173 => Gls Tokens per Sec:     2196 || Batch Translation Loss:   0.017670 => Txt Tokens per Sec:     6069 || Lr: 0.000100
2024-02-02 18:30:27,361 Epoch 453: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.63 
2024-02-02 18:30:27,361 EPOCH 454
2024-02-02 18:30:32,058 Epoch 454: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.68 
2024-02-02 18:30:32,058 EPOCH 455
2024-02-02 18:30:36,660 Epoch 455: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.75 
2024-02-02 18:30:36,660 EPOCH 456
2024-02-02 18:30:40,560 [Epoch: 456 Step: 00015500] Batch Recognition Loss:   0.000198 => Gls Tokens per Sec:     2398 || Batch Translation Loss:   0.037628 => Txt Tokens per Sec:     6599 || Lr: 0.000100
2024-02-02 18:30:41,251 Epoch 456: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.70 
2024-02-02 18:30:41,251 EPOCH 457
2024-02-02 18:30:45,784 Epoch 457: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.90 
2024-02-02 18:30:45,784 EPOCH 458
2024-02-02 18:30:50,453 Epoch 458: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.36 
2024-02-02 18:30:50,453 EPOCH 459
2024-02-02 18:30:53,922 [Epoch: 459 Step: 00015600] Batch Recognition Loss:   0.000265 => Gls Tokens per Sec:     2583 || Batch Translation Loss:   0.045313 => Txt Tokens per Sec:     7048 || Lr: 0.000100
2024-02-02 18:30:54,958 Epoch 459: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.06 
2024-02-02 18:30:54,958 EPOCH 460
2024-02-02 18:30:59,518 Epoch 460: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.27 
2024-02-02 18:30:59,519 EPOCH 461
2024-02-02 18:31:04,113 Epoch 461: Total Training Recognition Loss 0.01  Total Training Translation Loss 6.67 
2024-02-02 18:31:04,113 EPOCH 462
2024-02-02 18:31:07,812 [Epoch: 462 Step: 00015700] Batch Recognition Loss:   0.000422 => Gls Tokens per Sec:     2250 || Batch Translation Loss:   0.079226 => Txt Tokens per Sec:     6167 || Lr: 0.000100
2024-02-02 18:31:08,948 Epoch 462: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.81 
2024-02-02 18:31:08,948 EPOCH 463
2024-02-02 18:31:13,328 Epoch 463: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.51 
2024-02-02 18:31:13,328 EPOCH 464
2024-02-02 18:31:18,258 Epoch 464: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.24 
2024-02-02 18:31:18,259 EPOCH 465
2024-02-02 18:31:21,031 [Epoch: 465 Step: 00015800] Batch Recognition Loss:   0.000496 => Gls Tokens per Sec:     2682 || Batch Translation Loss:   0.084826 => Txt Tokens per Sec:     7326 || Lr: 0.000100
2024-02-02 18:31:22,495 Epoch 465: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.98 
2024-02-02 18:31:22,496 EPOCH 466
2024-02-02 18:31:27,373 Epoch 466: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.70 
2024-02-02 18:31:27,374 EPOCH 467
2024-02-02 18:31:31,696 Epoch 467: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.07 
2024-02-02 18:31:31,696 EPOCH 468
2024-02-02 18:31:35,441 [Epoch: 468 Step: 00015900] Batch Recognition Loss:   0.000251 => Gls Tokens per Sec:     1813 || Batch Translation Loss:   0.023604 => Txt Tokens per Sec:     5278 || Lr: 0.000100
2024-02-02 18:31:36,772 Epoch 468: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.54 
2024-02-02 18:31:36,772 EPOCH 469
2024-02-02 18:31:41,535 Epoch 469: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.08 
2024-02-02 18:31:41,536 EPOCH 470
2024-02-02 18:31:46,264 Epoch 470: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.06 
2024-02-02 18:31:46,265 EPOCH 471
2024-02-02 18:31:48,524 [Epoch: 471 Step: 00016000] Batch Recognition Loss:   0.000162 => Gls Tokens per Sec:     2833 || Batch Translation Loss:   0.017090 => Txt Tokens per Sec:     7517 || Lr: 0.000100
2024-02-02 18:31:57,052 Validation result at epoch 471, step    16000: duration: 8.5265s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00047	Translation Loss: 94308.07031	PPL: 12549.21289
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.79	(BLEU-1: 11.52,	BLEU-2: 3.67,	BLEU-3: 1.51,	BLEU-4: 0.79)
	CHRF 17.45	ROUGE 9.56
2024-02-02 18:31:57,053 Logging Recognition and Translation Outputs
2024-02-02 18:31:57,053 ========================================================================================================================
2024-02-02 18:31:57,053 Logging Sequence: 147_132.00
2024-02-02 18:31:57,053 	Gloss Reference :	A B+C+D+E
2024-02-02 18:31:57,053 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 18:31:57,053 	Gloss Alignment :	         
2024-02-02 18:31:57,054 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 18:31:57,055 	Text Reference  :	i    can not **** ** **** earlier i   used   to ***** have fun in     gymnastics
2024-02-02 18:31:57,055 	Text Hypothesis :	they do  not want to risk this    and wanted to focus on   her mental health    
2024-02-02 18:31:57,055 	Text Alignment  :	S    S       I    I  I    S       S   S         I     S    S   S      S         
2024-02-02 18:31:57,055 ========================================================================================================================
2024-02-02 18:31:57,055 Logging Sequence: 116_162.00
2024-02-02 18:31:57,055 	Gloss Reference :	A B+C+D+E
2024-02-02 18:31:57,055 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 18:31:57,056 	Gloss Alignment :	         
2024-02-02 18:31:57,056 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 18:31:57,058 	Text Reference  :	***** turned out   the     video  was shared on    social media by   a         staff at the ******* *** ******** **** * hotel
2024-02-02 18:31:57,058 	Text Hypothesis :	after this   match gambhir issued a   video  along with   two   12th september 2023  at the stadium and pakistan what a man  
2024-02-02 18:31:57,058 	Text Alignment  :	I     S      S     S       S      S   S      S     S      S     S    S         S            I       I   I        I    I S    
2024-02-02 18:31:57,058 ========================================================================================================================
2024-02-02 18:31:57,058 Logging Sequence: 73_79.00
2024-02-02 18:31:57,058 	Gloss Reference :	A B+C+D+E
2024-02-02 18:31:57,058 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 18:31:57,058 	Gloss Alignment :	         
2024-02-02 18:31:57,059 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 18:31:57,060 	Text Reference  :	raina resturant has food from the  rich  spices of    north india   to  the aromatic curries of south  india  
2024-02-02 18:31:57,060 	Text Hypothesis :	***** ********* *** **** on   23rd march 2023   raina loves playing for the ******** ******* ** police station
2024-02-02 18:31:57,060 	Text Alignment  :	D     D         D   D    S    S    S     S      S     S     S       S       D        D       D  S      S      
2024-02-02 18:31:57,060 ========================================================================================================================
2024-02-02 18:31:57,060 Logging Sequence: 165_523.00
2024-02-02 18:31:57,060 	Gloss Reference :	A B+C+D+E
2024-02-02 18:31:57,061 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 18:31:57,061 	Gloss Alignment :	         
2024-02-02 18:31:57,061 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 18:31:57,062 	Text Reference  :	**** as   he   believed that his  team might     lose if   he   takes off  his batting pads 
2024-02-02 18:31:57,062 	Text Hypothesis :	when they were batting  and  well to   celebrate the  2011 when the   team won the     match
2024-02-02 18:31:57,063 	Text Alignment  :	I    S    S    S        S    S    S    S         S    S    S    S     S    S   S       S    
2024-02-02 18:31:57,063 ========================================================================================================================
2024-02-02 18:31:57,063 Logging Sequence: 125_72.00
2024-02-02 18:31:57,063 	Gloss Reference :	A B+C+D+E
2024-02-02 18:31:57,063 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 18:31:57,063 	Gloss Alignment :	         
2024-02-02 18:31:57,063 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 18:31:57,064 	Text Reference  :	some said the pakistani javelineer had milicious intentions of tampering with the  javelin      out of    jealousy  
2024-02-02 18:31:57,064 	Text Hypothesis :	**** **** *** ********* ********** *** ********* ********** ** neeraj    was  very disappointed by  these statements
2024-02-02 18:31:57,064 	Text Alignment  :	D    D    D   D         D          D   D         D          D  S         S    S    S            S   S     S         
2024-02-02 18:31:57,064 ========================================================================================================================
2024-02-02 18:31:59,390 Epoch 471: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.95 
2024-02-02 18:31:59,391 EPOCH 472
2024-02-02 18:32:04,187 Epoch 472: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.87 
2024-02-02 18:32:04,187 EPOCH 473
2024-02-02 18:32:08,638 Epoch 473: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.98 
2024-02-02 18:32:08,639 EPOCH 474
2024-02-02 18:32:11,133 [Epoch: 474 Step: 00016100] Batch Recognition Loss:   0.000184 => Gls Tokens per Sec:     2310 || Batch Translation Loss:   0.064946 => Txt Tokens per Sec:     6318 || Lr: 0.000100
2024-02-02 18:32:13,524 Epoch 474: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.57 
2024-02-02 18:32:13,524 EPOCH 475
2024-02-02 18:32:18,291 Epoch 475: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.89 
2024-02-02 18:32:18,291 EPOCH 476
2024-02-02 18:32:22,469 Epoch 476: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.38 
2024-02-02 18:32:22,469 EPOCH 477
2024-02-02 18:32:24,878 [Epoch: 477 Step: 00016200] Batch Recognition Loss:   0.000318 => Gls Tokens per Sec:     2126 || Batch Translation Loss:   0.035686 => Txt Tokens per Sec:     5914 || Lr: 0.000100
2024-02-02 18:32:27,328 Epoch 477: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.75 
2024-02-02 18:32:27,328 EPOCH 478
2024-02-02 18:32:31,810 Epoch 478: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.60 
2024-02-02 18:32:31,811 EPOCH 479
2024-02-02 18:32:36,507 Epoch 479: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.20 
2024-02-02 18:32:36,507 EPOCH 480
2024-02-02 18:32:38,411 [Epoch: 480 Step: 00016300] Batch Recognition Loss:   0.000445 => Gls Tokens per Sec:     2355 || Batch Translation Loss:   0.212164 => Txt Tokens per Sec:     6602 || Lr: 0.000100
2024-02-02 18:32:41,058 Epoch 480: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.01 
2024-02-02 18:32:41,058 EPOCH 481
2024-02-02 18:32:45,694 Epoch 481: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.37 
2024-02-02 18:32:45,694 EPOCH 482
2024-02-02 18:32:50,213 Epoch 482: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.53 
2024-02-02 18:32:50,214 EPOCH 483
2024-02-02 18:32:51,499 [Epoch: 483 Step: 00016400] Batch Recognition Loss:   0.000187 => Gls Tokens per Sec:     2796 || Batch Translation Loss:   0.105821 => Txt Tokens per Sec:     7318 || Lr: 0.000100
2024-02-02 18:32:54,822 Epoch 483: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.33 
2024-02-02 18:32:54,823 EPOCH 484
2024-02-02 18:32:59,378 Epoch 484: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.90 
2024-02-02 18:32:59,378 EPOCH 485
2024-02-02 18:33:03,932 Epoch 485: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.11 
2024-02-02 18:33:03,932 EPOCH 486
2024-02-02 18:33:05,212 [Epoch: 486 Step: 00016500] Batch Recognition Loss:   0.000278 => Gls Tokens per Sec:     2503 || Batch Translation Loss:   0.143452 => Txt Tokens per Sec:     7003 || Lr: 0.000100
2024-02-02 18:33:08,520 Epoch 486: Total Training Recognition Loss 0.01  Total Training Translation Loss 6.14 
2024-02-02 18:33:08,521 EPOCH 487
2024-02-02 18:33:13,109 Epoch 487: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.51 
2024-02-02 18:33:13,109 EPOCH 488
2024-02-02 18:33:17,895 Epoch 488: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.56 
2024-02-02 18:33:17,896 EPOCH 489
2024-02-02 18:33:18,847 [Epoch: 489 Step: 00016600] Batch Recognition Loss:   0.000361 => Gls Tokens per Sec:     2696 || Batch Translation Loss:   0.093457 => Txt Tokens per Sec:     7320 || Lr: 0.000100
2024-02-02 18:33:22,367 Epoch 489: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.74 
2024-02-02 18:33:22,367 EPOCH 490
2024-02-02 18:33:27,018 Epoch 490: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.97 
2024-02-02 18:33:27,018 EPOCH 491
2024-02-02 18:33:31,412 Epoch 491: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.84 
2024-02-02 18:33:31,412 EPOCH 492
2024-02-02 18:33:32,327 [Epoch: 492 Step: 00016700] Batch Recognition Loss:   0.000337 => Gls Tokens per Sec:     2100 || Batch Translation Loss:   0.076990 => Txt Tokens per Sec:     5692 || Lr: 0.000100
2024-02-02 18:33:35,598 Epoch 492: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.91 
2024-02-02 18:33:35,598 EPOCH 493
2024-02-02 18:33:40,437 Epoch 493: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.80 
2024-02-02 18:33:40,438 EPOCH 494
2024-02-02 18:33:45,289 Epoch 494: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.44 
2024-02-02 18:33:45,289 EPOCH 495
2024-02-02 18:33:45,684 [Epoch: 495 Step: 00016800] Batch Recognition Loss:   0.000351 => Gls Tokens per Sec:     3249 || Batch Translation Loss:   0.036186 => Txt Tokens per Sec:     9029 || Lr: 0.000100
2024-02-02 18:33:49,591 Epoch 495: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.92 
2024-02-02 18:33:49,591 EPOCH 496
2024-02-02 18:33:54,472 Epoch 496: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.20 
2024-02-02 18:33:54,473 EPOCH 497
2024-02-02 18:33:58,832 Epoch 497: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.17 
2024-02-02 18:33:58,832 EPOCH 498
2024-02-02 18:33:59,156 [Epoch: 498 Step: 00016900] Batch Recognition Loss:   0.000391 => Gls Tokens per Sec:     1981 || Batch Translation Loss:   0.051554 => Txt Tokens per Sec:     6949 || Lr: 0.000100
2024-02-02 18:34:03,721 Epoch 498: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.54 
2024-02-02 18:34:03,722 EPOCH 499
2024-02-02 18:34:07,936 Epoch 499: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.44 
2024-02-02 18:34:07,936 EPOCH 500
2024-02-02 18:34:12,740 [Epoch: 500 Step: 00017000] Batch Recognition Loss:   0.000247 => Gls Tokens per Sec:     2213 || Batch Translation Loss:   0.034771 => Txt Tokens per Sec:     6144 || Lr: 0.000100
2024-02-02 18:34:12,741 Epoch 500: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.54 
2024-02-02 18:34:12,741 EPOCH 501
2024-02-02 18:34:17,067 Epoch 501: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.52 
2024-02-02 18:34:17,067 EPOCH 502
2024-02-02 18:34:21,892 Epoch 502: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.36 
2024-02-02 18:34:21,893 EPOCH 503
2024-02-02 18:34:26,447 [Epoch: 503 Step: 00017100] Batch Recognition Loss:   0.000316 => Gls Tokens per Sec:     2194 || Batch Translation Loss:   0.056226 => Txt Tokens per Sec:     6108 || Lr: 0.000100
2024-02-02 18:34:26,686 Epoch 503: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.28 
2024-02-02 18:34:26,686 EPOCH 504
2024-02-02 18:34:31,467 Epoch 504: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.77 
2024-02-02 18:34:31,468 EPOCH 505
2024-02-02 18:34:35,809 Epoch 505: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.60 
2024-02-02 18:34:35,809 EPOCH 506
2024-02-02 18:34:40,139 [Epoch: 506 Step: 00017200] Batch Recognition Loss:   0.000143 => Gls Tokens per Sec:     2160 || Batch Translation Loss:   0.084116 => Txt Tokens per Sec:     5918 || Lr: 0.000100
2024-02-02 18:34:40,744 Epoch 506: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.86 
2024-02-02 18:34:40,744 EPOCH 507
2024-02-02 18:34:45,134 Epoch 507: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.92 
2024-02-02 18:34:45,135 EPOCH 508
2024-02-02 18:34:50,004 Epoch 508: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.38 
2024-02-02 18:34:50,004 EPOCH 509
2024-02-02 18:34:53,386 [Epoch: 509 Step: 00017300] Batch Recognition Loss:   0.000222 => Gls Tokens per Sec:     2576 || Batch Translation Loss:   0.024373 => Txt Tokens per Sec:     6981 || Lr: 0.000100
2024-02-02 18:34:54,373 Epoch 509: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.73 
2024-02-02 18:34:54,373 EPOCH 510
2024-02-02 18:34:59,115 Epoch 510: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.19 
2024-02-02 18:34:59,115 EPOCH 511
2024-02-02 18:35:03,607 Epoch 511: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.62 
2024-02-02 18:35:03,608 EPOCH 512
2024-02-02 18:35:07,158 [Epoch: 512 Step: 00017400] Batch Recognition Loss:   0.000194 => Gls Tokens per Sec:     2274 || Batch Translation Loss:   0.058082 => Txt Tokens per Sec:     6271 || Lr: 0.000100
2024-02-02 18:35:08,282 Epoch 512: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.18 
2024-02-02 18:35:08,282 EPOCH 513
2024-02-02 18:35:12,735 Epoch 513: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.36 
2024-02-02 18:35:12,736 EPOCH 514
2024-02-02 18:35:17,377 Epoch 514: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.53 
2024-02-02 18:35:17,378 EPOCH 515
2024-02-02 18:35:20,181 [Epoch: 515 Step: 00017500] Batch Recognition Loss:   0.000241 => Gls Tokens per Sec:     2650 || Batch Translation Loss:   0.018347 => Txt Tokens per Sec:     7387 || Lr: 0.000100
2024-02-02 18:35:21,418 Epoch 515: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.69 
2024-02-02 18:35:21,418 EPOCH 516
2024-02-02 18:35:26,241 Epoch 516: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.88 
2024-02-02 18:35:26,242 EPOCH 517
2024-02-02 18:35:31,062 Epoch 517: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.35 
2024-02-02 18:35:31,062 EPOCH 518
2024-02-02 18:35:33,716 [Epoch: 518 Step: 00017600] Batch Recognition Loss:   0.000182 => Gls Tokens per Sec:     2561 || Batch Translation Loss:   0.020468 => Txt Tokens per Sec:     6835 || Lr: 0.000100
2024-02-02 18:35:35,622 Epoch 518: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.91 
2024-02-02 18:35:35,622 EPOCH 519
2024-02-02 18:35:40,261 Epoch 519: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.96 
2024-02-02 18:35:40,262 EPOCH 520
2024-02-02 18:35:45,021 Epoch 520: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.81 
2024-02-02 18:35:45,022 EPOCH 521
2024-02-02 18:35:48,103 [Epoch: 521 Step: 00017700] Batch Recognition Loss:   0.000393 => Gls Tokens per Sec:     1997 || Batch Translation Loss:   0.075475 => Txt Tokens per Sec:     5755 || Lr: 0.000100
2024-02-02 18:35:49,881 Epoch 521: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.12 
2024-02-02 18:35:49,881 EPOCH 522
2024-02-02 18:35:54,204 Epoch 522: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.05 
2024-02-02 18:35:54,204 EPOCH 523
2024-02-02 18:35:58,940 Epoch 523: Total Training Recognition Loss 0.03  Total Training Translation Loss 19.10 
2024-02-02 18:35:58,941 EPOCH 524
2024-02-02 18:36:01,247 [Epoch: 524 Step: 00017800] Batch Recognition Loss:   0.000667 => Gls Tokens per Sec:     2500 || Batch Translation Loss:   0.357947 => Txt Tokens per Sec:     6523 || Lr: 0.000100
2024-02-02 18:36:03,924 Epoch 524: Total Training Recognition Loss 0.04  Total Training Translation Loss 7.24 
2024-02-02 18:36:03,925 EPOCH 525
2024-02-02 18:36:08,735 Epoch 525: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.31 
2024-02-02 18:36:08,735 EPOCH 526
2024-02-02 18:36:12,911 Epoch 526: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.89 
2024-02-02 18:36:12,911 EPOCH 527
2024-02-02 18:36:15,154 [Epoch: 527 Step: 00017900] Batch Recognition Loss:   0.000290 => Gls Tokens per Sec:     2172 || Batch Translation Loss:   0.048030 => Txt Tokens per Sec:     6067 || Lr: 0.000100
2024-02-02 18:36:17,825 Epoch 527: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.97 
2024-02-02 18:36:17,825 EPOCH 528
2024-02-02 18:36:22,125 Epoch 528: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.79 
2024-02-02 18:36:22,126 EPOCH 529
2024-02-02 18:36:26,980 Epoch 529: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.84 
2024-02-02 18:36:26,981 EPOCH 530
2024-02-02 18:36:28,780 [Epoch: 530 Step: 00018000] Batch Recognition Loss:   0.000257 => Gls Tokens per Sec:     2491 || Batch Translation Loss:   0.079618 => Txt Tokens per Sec:     7272 || Lr: 0.000100
2024-02-02 18:36:37,243 Validation result at epoch 530, step    18000: duration: 8.4630s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00059	Translation Loss: 94243.93750	PPL: 12468.93945
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.79	(BLEU-1: 11.26,	BLEU-2: 3.89,	BLEU-3: 1.65,	BLEU-4: 0.79)
	CHRF 17.36	ROUGE 10.00
2024-02-02 18:36:37,244 Logging Recognition and Translation Outputs
2024-02-02 18:36:37,244 ========================================================================================================================
2024-02-02 18:36:37,244 Logging Sequence: 155_119.00
2024-02-02 18:36:37,244 	Gloss Reference :	A B+C+D+E
2024-02-02 18:36:37,245 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 18:36:37,245 	Gloss Alignment :	         
2024-02-02 18:36:37,245 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 18:36:37,247 	Text Reference  :	*** a       report    said   that the ***** ******* ***** ***** ** taliban wanted icc    to ** replace the afghan flag with its own 
2024-02-02 18:36:37,247 	Text Hypothesis :	and taliban considers itself as   the match between their match as they    would  decide to go on      the afghan **** **** *** team
2024-02-02 18:36:37,247 	Text Alignment  :	I   S       S         S      S        I     I       I     I     I  S       S      S         I  S                  D    D    D   S   
2024-02-02 18:36:37,247 ========================================================================================================================
2024-02-02 18:36:37,248 Logging Sequence: 153_43.00
2024-02-02 18:36:37,248 	Gloss Reference :	A B+C+D+E
2024-02-02 18:36:37,248 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 18:36:37,248 	Gloss Alignment :	         
2024-02-02 18:36:37,248 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 18:36:37,249 	Text Reference  :	******* these runs   were all because of     hardik  pandya    and  virat kohli
2024-02-02 18:36:37,249 	Text Hypothesis :	however the   venues took now were    played against rajasthan lost the   match
2024-02-02 18:36:37,249 	Text Alignment  :	I       S     S      S    S   S       S      S       S         S    S     S    
2024-02-02 18:36:37,249 ========================================================================================================================
2024-02-02 18:36:37,249 Logging Sequence: 150_35.00
2024-02-02 18:36:37,250 	Gloss Reference :	A B+C+D+E
2024-02-02 18:36:37,250 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 18:36:37,250 	Gloss Alignment :	         
2024-02-02 18:36:37,250 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 18:36:37,251 	Text Reference  :	*** ***** ** ** wow india football team is      really  strong
2024-02-02 18:36:37,251 	Text Hypothesis :	the match is no one to    keep     with another similar way   
2024-02-02 18:36:37,251 	Text Alignment  :	I   I     I  I  S   S     S        S    S       S       S     
2024-02-02 18:36:37,251 ========================================================================================================================
2024-02-02 18:36:37,251 Logging Sequence: 146_154.00
2024-02-02 18:36:37,252 	Gloss Reference :	A B+C+D+E
2024-02-02 18:36:37,252 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 18:36:37,252 	Gloss Alignment :	         
2024-02-02 18:36:37,252 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 18:36:37,254 	Text Reference  :	bwf said  that testing protocols have   been implemented to  ensure the health and ** safety of all participants
2024-02-02 18:36:37,254 	Text Hypothesis :	the ashes is   a       special   series of   afghanistan and won    the toss   and is going  on 9th of          
2024-02-02 18:36:37,254 	Text Alignment  :	S   S     S    S       S         S      S    S           S   S          S          I  S      S  S   S           
2024-02-02 18:36:37,254 ========================================================================================================================
2024-02-02 18:36:37,254 Logging Sequence: 76_79.00
2024-02-02 18:36:37,254 	Gloss Reference :	A B+C+D+E
2024-02-02 18:36:37,254 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 18:36:37,255 	Gloss Alignment :	         
2024-02-02 18:36:37,255 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 18:36:37,255 	Text Reference  :	** **** speaking to   ani   csk ceo kasi  viswanathan said  
2024-02-02 18:36:37,255 	Text Hypothesis :	on 13th february 2023 there was a   match between     mumbai
2024-02-02 18:36:37,255 	Text Alignment  :	I  I    S        S    S     S   S   S     S           S     
2024-02-02 18:36:37,256 ========================================================================================================================
2024-02-02 18:36:40,042 Epoch 530: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.53 
2024-02-02 18:36:40,042 EPOCH 531
2024-02-02 18:36:44,906 Epoch 531: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.49 
2024-02-02 18:36:44,907 EPOCH 532
2024-02-02 18:36:49,165 Epoch 532: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.12 
2024-02-02 18:36:49,166 EPOCH 533
2024-02-02 18:36:50,724 [Epoch: 533 Step: 00018100] Batch Recognition Loss:   0.000215 => Gls Tokens per Sec:     2466 || Batch Translation Loss:   0.035071 => Txt Tokens per Sec:     6584 || Lr: 0.000100
2024-02-02 18:36:54,033 Epoch 533: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.35 
2024-02-02 18:36:54,033 EPOCH 534
2024-02-02 18:36:58,380 Epoch 534: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.86 
2024-02-02 18:36:58,381 EPOCH 535
2024-02-02 18:37:03,150 Epoch 535: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.24 
2024-02-02 18:37:03,150 EPOCH 536
2024-02-02 18:37:04,133 [Epoch: 536 Step: 00018200] Batch Recognition Loss:   0.000278 => Gls Tokens per Sec:     3259 || Batch Translation Loss:   0.020118 => Txt Tokens per Sec:     8392 || Lr: 0.000100
2024-02-02 18:37:07,602 Epoch 536: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.15 
2024-02-02 18:37:07,603 EPOCH 537
2024-02-02 18:37:12,325 Epoch 537: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.54 
2024-02-02 18:37:12,325 EPOCH 538
2024-02-02 18:37:16,378 Epoch 538: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.10 
2024-02-02 18:37:16,379 EPOCH 539
2024-02-02 18:37:17,084 [Epoch: 539 Step: 00018300] Batch Recognition Loss:   0.000143 => Gls Tokens per Sec:     3635 || Batch Translation Loss:   0.081076 => Txt Tokens per Sec:     8357 || Lr: 0.000100
2024-02-02 18:37:20,471 Epoch 539: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.11 
2024-02-02 18:37:20,471 EPOCH 540
2024-02-02 18:37:24,871 Epoch 540: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.01 
2024-02-02 18:37:24,871 EPOCH 541
2024-02-02 18:37:30,022 Epoch 541: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.12 
2024-02-02 18:37:30,023 EPOCH 542
2024-02-02 18:37:30,812 [Epoch: 542 Step: 00018400] Batch Recognition Loss:   0.000313 => Gls Tokens per Sec:     2437 || Batch Translation Loss:   0.167130 => Txt Tokens per Sec:     6637 || Lr: 0.000100
2024-02-02 18:37:34,878 Epoch 542: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.29 
2024-02-02 18:37:34,879 EPOCH 543
2024-02-02 18:37:39,198 Epoch 543: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.86 
2024-02-02 18:37:39,199 EPOCH 544
2024-02-02 18:37:43,842 Epoch 544: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.51 
2024-02-02 18:37:43,843 EPOCH 545
2024-02-02 18:37:44,409 [Epoch: 545 Step: 00018500] Batch Recognition Loss:   0.000272 => Gls Tokens per Sec:     2264 || Batch Translation Loss:   0.053622 => Txt Tokens per Sec:     6594 || Lr: 0.000100
2024-02-02 18:37:48,454 Epoch 545: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.79 
2024-02-02 18:37:48,454 EPOCH 546
2024-02-02 18:37:53,201 Epoch 546: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.25 
2024-02-02 18:37:53,201 EPOCH 547
2024-02-02 18:37:57,834 Epoch 547: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.34 
2024-02-02 18:37:57,835 EPOCH 548
2024-02-02 18:37:58,003 [Epoch: 548 Step: 00018600] Batch Recognition Loss:   0.000280 => Gls Tokens per Sec:     3855 || Batch Translation Loss:   0.090858 => Txt Tokens per Sec:     8000 || Lr: 0.000100
2024-02-02 18:38:02,742 Epoch 548: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.99 
2024-02-02 18:38:02,743 EPOCH 549
2024-02-02 18:38:07,380 Epoch 549: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.10 
2024-02-02 18:38:07,381 EPOCH 550
2024-02-02 18:38:12,293 [Epoch: 550 Step: 00018700] Batch Recognition Loss:   0.000325 => Gls Tokens per Sec:     2164 || Batch Translation Loss:   0.074239 => Txt Tokens per Sec:     6008 || Lr: 0.000100
2024-02-02 18:38:12,294 Epoch 550: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.04 
2024-02-02 18:38:12,294 EPOCH 551
2024-02-02 18:38:16,994 Epoch 551: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.17 
2024-02-02 18:38:16,995 EPOCH 552
2024-02-02 18:38:21,793 Epoch 552: Total Training Recognition Loss 0.01  Total Training Translation Loss 6.59 
2024-02-02 18:38:21,793 EPOCH 553
2024-02-02 18:38:26,307 [Epoch: 553 Step: 00018800] Batch Recognition Loss:   0.000501 => Gls Tokens per Sec:     2214 || Batch Translation Loss:   0.093363 => Txt Tokens per Sec:     6186 || Lr: 0.000100
2024-02-02 18:38:26,593 Epoch 553: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.27 
2024-02-02 18:38:26,593 EPOCH 554
2024-02-02 18:38:31,280 Epoch 554: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.24 
2024-02-02 18:38:31,281 EPOCH 555
2024-02-02 18:38:36,180 Epoch 555: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.74 
2024-02-02 18:38:36,180 EPOCH 556
2024-02-02 18:38:40,338 [Epoch: 556 Step: 00018900] Batch Recognition Loss:   0.000238 => Gls Tokens per Sec:     2310 || Batch Translation Loss:   0.042502 => Txt Tokens per Sec:     6495 || Lr: 0.000100
2024-02-02 18:38:40,759 Epoch 556: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.61 
2024-02-02 18:38:40,759 EPOCH 557
2024-02-02 18:38:44,803 Epoch 557: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.43 
2024-02-02 18:38:44,804 EPOCH 558
2024-02-02 18:38:49,791 Epoch 558: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.61 
2024-02-02 18:38:49,791 EPOCH 559
2024-02-02 18:38:53,683 [Epoch: 559 Step: 00019000] Batch Recognition Loss:   0.000212 => Gls Tokens per Sec:     2238 || Batch Translation Loss:   0.035588 => Txt Tokens per Sec:     6230 || Lr: 0.000100
2024-02-02 18:38:54,477 Epoch 559: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.50 
2024-02-02 18:38:54,477 EPOCH 560
2024-02-02 18:38:59,319 Epoch 560: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.07 
2024-02-02 18:38:59,320 EPOCH 561
2024-02-02 18:39:03,537 Epoch 561: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.37 
2024-02-02 18:39:03,537 EPOCH 562
2024-02-02 18:39:06,769 [Epoch: 562 Step: 00019100] Batch Recognition Loss:   0.000267 => Gls Tokens per Sec:     2497 || Batch Translation Loss:   0.086170 => Txt Tokens per Sec:     6974 || Lr: 0.000100
2024-02-02 18:39:07,749 Epoch 562: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.77 
2024-02-02 18:39:07,749 EPOCH 563
2024-02-02 18:39:12,616 Epoch 563: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.01 
2024-02-02 18:39:12,616 EPOCH 564
2024-02-02 18:39:16,968 Epoch 564: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.81 
2024-02-02 18:39:16,969 EPOCH 565
2024-02-02 18:39:20,362 [Epoch: 565 Step: 00019200] Batch Recognition Loss:   0.000129 => Gls Tokens per Sec:     2190 || Batch Translation Loss:   0.038697 => Txt Tokens per Sec:     6043 || Lr: 0.000100
2024-02-02 18:39:21,835 Epoch 565: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.01 
2024-02-02 18:39:21,835 EPOCH 566
2024-02-02 18:39:26,263 Epoch 566: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.69 
2024-02-02 18:39:26,264 EPOCH 567
2024-02-02 18:39:30,973 Epoch 567: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.35 
2024-02-02 18:39:30,973 EPOCH 568
2024-02-02 18:39:33,540 [Epoch: 568 Step: 00019300] Batch Recognition Loss:   0.000239 => Gls Tokens per Sec:     2743 || Batch Translation Loss:   0.037699 => Txt Tokens per Sec:     7425 || Lr: 0.000100
2024-02-02 18:39:35,598 Epoch 568: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.62 
2024-02-02 18:39:35,599 EPOCH 569
2024-02-02 18:39:40,184 Epoch 569: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.16 
2024-02-02 18:39:40,184 EPOCH 570
2024-02-02 18:39:44,910 Epoch 570: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.59 
2024-02-02 18:39:44,910 EPOCH 571
2024-02-02 18:39:47,426 [Epoch: 571 Step: 00019400] Batch Recognition Loss:   0.000169 => Gls Tokens per Sec:     2445 || Batch Translation Loss:   0.025725 => Txt Tokens per Sec:     6608 || Lr: 0.000100
2024-02-02 18:39:49,349 Epoch 571: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.28 
2024-02-02 18:39:49,350 EPOCH 572
2024-02-02 18:39:54,056 Epoch 572: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.03 
2024-02-02 18:39:54,056 EPOCH 573
2024-02-02 18:39:58,532 Epoch 573: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.90 
2024-02-02 18:39:58,532 EPOCH 574
2024-02-02 18:40:01,074 [Epoch: 574 Step: 00019500] Batch Recognition Loss:   0.000133 => Gls Tokens per Sec:     2168 || Batch Translation Loss:   0.013275 => Txt Tokens per Sec:     5906 || Lr: 0.000100
2024-02-02 18:40:03,335 Epoch 574: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.91 
2024-02-02 18:40:03,335 EPOCH 575
2024-02-02 18:40:07,737 Epoch 575: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.12 
2024-02-02 18:40:07,737 EPOCH 576
2024-02-02 18:40:12,546 Epoch 576: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.01 
2024-02-02 18:40:12,546 EPOCH 577
2024-02-02 18:40:14,304 [Epoch: 577 Step: 00019600] Batch Recognition Loss:   0.000292 => Gls Tokens per Sec:     2772 || Batch Translation Loss:   0.117192 => Txt Tokens per Sec:     7415 || Lr: 0.000100
2024-02-02 18:40:16,857 Epoch 577: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.82 
2024-02-02 18:40:16,857 EPOCH 578
2024-02-02 18:40:21,695 Epoch 578: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.68 
2024-02-02 18:40:21,695 EPOCH 579
2024-02-02 18:40:26,048 Epoch 579: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.95 
2024-02-02 18:40:26,048 EPOCH 580
2024-02-02 18:40:28,437 [Epoch: 580 Step: 00019700] Batch Recognition Loss:   0.000330 => Gls Tokens per Sec:     1876 || Batch Translation Loss:   0.053208 => Txt Tokens per Sec:     5746 || Lr: 0.000100
2024-02-02 18:40:30,919 Epoch 580: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.40 
2024-02-02 18:40:30,920 EPOCH 581
2024-02-02 18:40:35,241 Epoch 581: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.02 
2024-02-02 18:40:35,241 EPOCH 582
2024-02-02 18:40:40,116 Epoch 582: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.43 
2024-02-02 18:40:40,116 EPOCH 583
2024-02-02 18:40:41,728 [Epoch: 583 Step: 00019800] Batch Recognition Loss:   0.000493 => Gls Tokens per Sec:     2385 || Batch Translation Loss:   0.275981 => Txt Tokens per Sec:     6890 || Lr: 0.000100
2024-02-02 18:40:44,377 Epoch 583: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.89 
2024-02-02 18:40:44,377 EPOCH 584
2024-02-02 18:40:49,335 Epoch 584: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.72 
2024-02-02 18:40:49,335 EPOCH 585
2024-02-02 18:40:53,584 Epoch 585: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.89 
2024-02-02 18:40:53,584 EPOCH 586
2024-02-02 18:40:55,158 [Epoch: 586 Step: 00019900] Batch Recognition Loss:   0.000383 => Gls Tokens per Sec:     2035 || Batch Translation Loss:   0.119780 => Txt Tokens per Sec:     5828 || Lr: 0.000100
2024-02-02 18:40:58,523 Epoch 586: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.55 
2024-02-02 18:40:58,523 EPOCH 587
2024-02-02 18:41:02,783 Epoch 587: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.26 
2024-02-02 18:41:02,783 EPOCH 588
2024-02-02 18:41:07,687 Epoch 588: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.53 
2024-02-02 18:41:07,688 EPOCH 589
2024-02-02 18:41:08,488 [Epoch: 589 Step: 00020000] Batch Recognition Loss:   0.000214 => Gls Tokens per Sec:     3206 || Batch Translation Loss:   0.026481 => Txt Tokens per Sec:     8555 || Lr: 0.000100
2024-02-02 18:41:17,013 Validation result at epoch 589, step    20000: duration: 8.5254s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00070	Translation Loss: 94814.90625	PPL: 13202.11719
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.90	(BLEU-1: 10.69,	BLEU-2: 3.43,	BLEU-3: 1.62,	BLEU-4: 0.90)
	CHRF 17.06	ROUGE 9.31
2024-02-02 18:41:17,014 Logging Recognition and Translation Outputs
2024-02-02 18:41:17,014 ========================================================================================================================
2024-02-02 18:41:17,014 Logging Sequence: 174_121.00
2024-02-02 18:41:17,015 	Gloss Reference :	A B+C+D+E
2024-02-02 18:41:17,015 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 18:41:17,015 	Gloss Alignment :	         
2024-02-02 18:41:17,015 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 18:41:17,016 	Text Reference  :	******* **** there was a strong competition and a difficult auction for       the 5      franchise owners
2024-02-02 18:41:17,016 	Text Hypothesis :	however here there was * ****** *********** *** a huge      fan     following in  mumbai and       shobit
2024-02-02 18:41:17,016 	Text Alignment  :	I       I              D D      D           D     S         S       S         S   S      S         S     
2024-02-02 18:41:17,017 ========================================================================================================================
2024-02-02 18:41:17,017 Logging Sequence: 170_24.00
2024-02-02 18:41:17,017 	Gloss Reference :	A B+C+D+E
2024-02-02 18:41:17,017 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 18:41:17,017 	Gloss Alignment :	         
2024-02-02 18:41:17,017 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 18:41:17,018 	Text Reference  :	let me tell you about it
2024-02-02 18:41:17,018 	Text Hypothesis :	let me tell you about it
2024-02-02 18:41:17,018 	Text Alignment  :	                        
2024-02-02 18:41:17,018 ========================================================================================================================
2024-02-02 18:41:17,018 Logging Sequence: 73_79.00
2024-02-02 18:41:17,018 	Gloss Reference :	A B+C+D+E
2024-02-02 18:41:17,018 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 18:41:17,019 	Gloss Alignment :	         
2024-02-02 18:41:17,019 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 18:41:17,020 	Text Reference  :	raina resturant has food from the  rich  spices of    north india   to the aromatic curries of south india  
2024-02-02 18:41:17,020 	Text Hypothesis :	***** ********* *** **** on   23rd march 2023   raina loves playing in the ******** ******* ** ***** stadium
2024-02-02 18:41:17,020 	Text Alignment  :	D     D         D   D    S    S    S     S      S     S     S       S      D        D       D  D     S      
2024-02-02 18:41:17,020 ========================================================================================================================
2024-02-02 18:41:17,020 Logging Sequence: 140_2.00
2024-02-02 18:41:17,021 	Gloss Reference :	A B+C+D+E
2024-02-02 18:41:17,021 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 18:41:17,021 	Gloss Alignment :	         
2024-02-02 18:41:17,021 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 18:41:17,022 	Text Reference  :	***** *** **** indian batsman-wicket keeper rishabh pant has   outstanding skills in  cricket
2024-02-02 18:41:17,022 	Text Hypothesis :	dhoni has also become the            first  time    when kohli was         given  the reason 
2024-02-02 18:41:17,022 	Text Alignment  :	I     I   I    S      S              S      S       S    S     S           S      S   S      
2024-02-02 18:41:17,022 ========================================================================================================================
2024-02-02 18:41:17,022 Logging Sequence: 81_470.00
2024-02-02 18:41:17,022 	Gloss Reference :	A B+C+D+E
2024-02-02 18:41:17,023 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 18:41:17,023 	Gloss Alignment :	         
2024-02-02 18:41:17,023 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 18:41:17,024 	Text Reference  :	or you don't know       if you do      let   us know in         the comments
2024-02-02 18:41:17,024 	Text Hypothesis :	** *** ***** arbitrator is a   supreme court of the  tournament was said    
2024-02-02 18:41:17,024 	Text Alignment  :	D  D   D     S          S  S   S       S     S  S    S          S   S       
2024-02-02 18:41:17,024 ========================================================================================================================
2024-02-02 18:41:21,042 Epoch 589: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.24 
2024-02-02 18:41:21,043 EPOCH 590
2024-02-02 18:41:25,925 Epoch 590: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.12 
2024-02-02 18:41:25,926 EPOCH 591
2024-02-02 18:41:30,235 Epoch 591: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.96 
2024-02-02 18:41:30,235 EPOCH 592
2024-02-02 18:41:31,126 [Epoch: 592 Step: 00020100] Batch Recognition Loss:   0.000216 => Gls Tokens per Sec:     2157 || Batch Translation Loss:   0.029243 => Txt Tokens per Sec:     6206 || Lr: 0.000100
2024-02-02 18:41:35,037 Epoch 592: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.98 
2024-02-02 18:41:35,038 EPOCH 593
2024-02-02 18:41:39,424 Epoch 593: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.23 
2024-02-02 18:41:39,425 EPOCH 594
2024-02-02 18:41:44,217 Epoch 594: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.23 
2024-02-02 18:41:44,217 EPOCH 595
2024-02-02 18:41:44,868 [Epoch: 595 Step: 00020200] Batch Recognition Loss:   0.000232 => Gls Tokens per Sec:     1969 || Batch Translation Loss:   0.108399 => Txt Tokens per Sec:     6358 || Lr: 0.000100
2024-02-02 18:41:48,679 Epoch 595: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.42 
2024-02-02 18:41:48,679 EPOCH 596
2024-02-02 18:41:53,345 Epoch 596: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.99 
2024-02-02 18:41:53,345 EPOCH 597
2024-02-02 18:41:57,724 Epoch 597: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.76 
2024-02-02 18:41:57,724 EPOCH 598
2024-02-02 18:41:58,017 [Epoch: 598 Step: 00020300] Batch Recognition Loss:   0.000397 => Gls Tokens per Sec:     2192 || Batch Translation Loss:   0.090315 => Txt Tokens per Sec:     6449 || Lr: 0.000100
2024-02-02 18:42:02,589 Epoch 598: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.89 
2024-02-02 18:42:02,590 EPOCH 599
2024-02-02 18:42:07,355 Epoch 599: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.22 
2024-02-02 18:42:07,356 EPOCH 600
2024-02-02 18:42:12,266 [Epoch: 600 Step: 00020400] Batch Recognition Loss:   0.000214 => Gls Tokens per Sec:     2165 || Batch Translation Loss:   0.102966 => Txt Tokens per Sec:     6011 || Lr: 0.000100
2024-02-02 18:42:12,266 Epoch 600: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.01 
2024-02-02 18:42:12,267 EPOCH 601
2024-02-02 18:42:16,520 Epoch 601: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.98 
2024-02-02 18:42:16,520 EPOCH 602
2024-02-02 18:42:21,368 Epoch 602: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.34 
2024-02-02 18:42:21,369 EPOCH 603
2024-02-02 18:42:25,420 [Epoch: 603 Step: 00020500] Batch Recognition Loss:   0.000422 => Gls Tokens per Sec:     2529 || Batch Translation Loss:   0.070940 => Txt Tokens per Sec:     7012 || Lr: 0.000100
2024-02-02 18:42:25,668 Epoch 603: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.24 
2024-02-02 18:42:25,668 EPOCH 604
2024-02-02 18:42:30,500 Epoch 604: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.37 
2024-02-02 18:42:30,501 EPOCH 605
2024-02-02 18:42:34,833 Epoch 605: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.01 
2024-02-02 18:42:34,834 EPOCH 606
2024-02-02 18:42:39,136 [Epoch: 606 Step: 00020600] Batch Recognition Loss:   0.000285 => Gls Tokens per Sec:     2175 || Batch Translation Loss:   0.152807 => Txt Tokens per Sec:     6095 || Lr: 0.000100
2024-02-02 18:42:39,567 Epoch 606: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.98 
2024-02-02 18:42:39,567 EPOCH 607
2024-02-02 18:42:44,534 Epoch 607: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.32 
2024-02-02 18:42:44,535 EPOCH 608
2024-02-02 18:42:49,050 Epoch 608: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.37 
2024-02-02 18:42:49,050 EPOCH 609
2024-02-02 18:42:52,703 [Epoch: 609 Step: 00020700] Batch Recognition Loss:   0.000374 => Gls Tokens per Sec:     2385 || Batch Translation Loss:   0.068227 => Txt Tokens per Sec:     6491 || Lr: 0.000100
2024-02-02 18:42:53,671 Epoch 609: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.29 
2024-02-02 18:42:53,672 EPOCH 610
2024-02-02 18:42:58,153 Epoch 610: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.12 
2024-02-02 18:42:58,153 EPOCH 611
2024-02-02 18:43:02,700 Epoch 611: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.28 
2024-02-02 18:43:02,701 EPOCH 612
2024-02-02 18:43:06,486 [Epoch: 612 Step: 00020800] Batch Recognition Loss:   0.000474 => Gls Tokens per Sec:     2199 || Batch Translation Loss:   0.085922 => Txt Tokens per Sec:     6265 || Lr: 0.000100
2024-02-02 18:43:07,321 Epoch 612: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.95 
2024-02-02 18:43:07,321 EPOCH 613
2024-02-02 18:43:11,796 Epoch 613: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.71 
2024-02-02 18:43:11,797 EPOCH 614
2024-02-02 18:43:16,451 Epoch 614: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.66 
2024-02-02 18:43:16,451 EPOCH 615
2024-02-02 18:43:19,694 [Epoch: 615 Step: 00020900] Batch Recognition Loss:   0.000221 => Gls Tokens per Sec:     2293 || Batch Translation Loss:   0.062222 => Txt Tokens per Sec:     6572 || Lr: 0.000100
2024-02-02 18:43:20,951 Epoch 615: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.34 
2024-02-02 18:43:20,951 EPOCH 616
2024-02-02 18:43:25,578 Epoch 616: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.37 
2024-02-02 18:43:25,578 EPOCH 617
2024-02-02 18:43:30,138 Epoch 617: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.96 
2024-02-02 18:43:30,139 EPOCH 618
2024-02-02 18:43:32,896 [Epoch: 618 Step: 00021000] Batch Recognition Loss:   0.000190 => Gls Tokens per Sec:     2555 || Batch Translation Loss:   0.044988 => Txt Tokens per Sec:     6956 || Lr: 0.000100
2024-02-02 18:43:34,762 Epoch 618: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.75 
2024-02-02 18:43:34,762 EPOCH 619
2024-02-02 18:43:39,310 Epoch 619: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.21 
2024-02-02 18:43:39,310 EPOCH 620
2024-02-02 18:43:43,908 Epoch 620: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.17 
2024-02-02 18:43:43,908 EPOCH 621
2024-02-02 18:43:46,267 [Epoch: 621 Step: 00021100] Batch Recognition Loss:   0.000139 => Gls Tokens per Sec:     2609 || Batch Translation Loss:   0.033400 => Txt Tokens per Sec:     7219 || Lr: 0.000100
2024-02-02 18:43:47,964 Epoch 621: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.10 
2024-02-02 18:43:47,964 EPOCH 622
2024-02-02 18:43:52,028 Epoch 622: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.07 
2024-02-02 18:43:52,028 EPOCH 623
2024-02-02 18:43:56,457 Epoch 623: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.25 
2024-02-02 18:43:56,458 EPOCH 624
2024-02-02 18:43:58,810 [Epoch: 624 Step: 00021200] Batch Recognition Loss:   0.000215 => Gls Tokens per Sec:     2451 || Batch Translation Loss:   0.043136 => Txt Tokens per Sec:     6557 || Lr: 0.000100
2024-02-02 18:44:01,174 Epoch 624: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.49 
2024-02-02 18:44:01,174 EPOCH 625
2024-02-02 18:44:06,034 Epoch 625: Total Training Recognition Loss 0.06  Total Training Translation Loss 12.66 
2024-02-02 18:44:06,034 EPOCH 626
2024-02-02 18:44:10,455 Epoch 626: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.42 
2024-02-02 18:44:10,456 EPOCH 627
2024-02-02 18:44:12,544 [Epoch: 627 Step: 00021300] Batch Recognition Loss:   0.000350 => Gls Tokens per Sec:     2453 || Batch Translation Loss:   0.084459 => Txt Tokens per Sec:     6552 || Lr: 0.000100
2024-02-02 18:44:15,345 Epoch 627: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.25 
2024-02-02 18:44:15,345 EPOCH 628
2024-02-02 18:44:19,453 Epoch 628: Total Training Recognition Loss 0.02  Total Training Translation Loss 11.26 
2024-02-02 18:44:19,453 EPOCH 629
2024-02-02 18:44:24,259 Epoch 629: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.91 
2024-02-02 18:44:24,260 EPOCH 630
2024-02-02 18:44:25,647 [Epoch: 630 Step: 00021400] Batch Recognition Loss:   0.000672 => Gls Tokens per Sec:     3230 || Batch Translation Loss:   0.073058 => Txt Tokens per Sec:     8303 || Lr: 0.000100
2024-02-02 18:44:28,939 Epoch 630: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.11 
2024-02-02 18:44:28,940 EPOCH 631
2024-02-02 18:44:33,443 Epoch 631: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.18 
2024-02-02 18:44:33,444 EPOCH 632
2024-02-02 18:44:37,985 Epoch 632: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.50 
2024-02-02 18:44:37,985 EPOCH 633
2024-02-02 18:44:39,791 [Epoch: 633 Step: 00021500] Batch Recognition Loss:   0.000304 => Gls Tokens per Sec:     2127 || Batch Translation Loss:   0.037853 => Txt Tokens per Sec:     6410 || Lr: 0.000100
2024-02-02 18:44:42,657 Epoch 633: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.57 
2024-02-02 18:44:42,657 EPOCH 634
2024-02-02 18:44:47,551 Epoch 634: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.82 
2024-02-02 18:44:47,551 EPOCH 635
2024-02-02 18:44:51,887 Epoch 635: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.32 
2024-02-02 18:44:51,888 EPOCH 636
2024-02-02 18:44:53,581 [Epoch: 636 Step: 00021600] Batch Recognition Loss:   0.000243 => Gls Tokens per Sec:     1893 || Batch Translation Loss:   0.027313 => Txt Tokens per Sec:     5491 || Lr: 0.000100
2024-02-02 18:44:56,786 Epoch 636: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.00 
2024-02-02 18:44:56,786 EPOCH 637
2024-02-02 18:45:01,600 Epoch 637: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.96 
2024-02-02 18:45:01,600 EPOCH 638
2024-02-02 18:45:06,229 Epoch 638: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.09 
2024-02-02 18:45:06,229 EPOCH 639
2024-02-02 18:45:07,026 [Epoch: 639 Step: 00021700] Batch Recognition Loss:   0.000233 => Gls Tokens per Sec:     3216 || Batch Translation Loss:   0.026720 => Txt Tokens per Sec:     8265 || Lr: 0.000100
2024-02-02 18:45:10,752 Epoch 639: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.86 
2024-02-02 18:45:10,752 EPOCH 640
2024-02-02 18:45:15,349 Epoch 640: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-02 18:45:15,349 EPOCH 641
2024-02-02 18:45:19,927 Epoch 641: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.70 
2024-02-02 18:45:19,927 EPOCH 642
2024-02-02 18:45:20,661 [Epoch: 642 Step: 00021800] Batch Recognition Loss:   0.000198 => Gls Tokens per Sec:     2619 || Batch Translation Loss:   0.019125 => Txt Tokens per Sec:     7060 || Lr: 0.000100
2024-02-02 18:45:24,503 Epoch 642: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.73 
2024-02-02 18:45:24,504 EPOCH 643
2024-02-02 18:45:29,080 Epoch 643: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-02 18:45:29,080 EPOCH 644
2024-02-02 18:45:33,604 Epoch 644: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-02 18:45:33,604 EPOCH 645
2024-02-02 18:45:34,056 [Epoch: 645 Step: 00021900] Batch Recognition Loss:   0.000172 => Gls Tokens per Sec:     2839 || Batch Translation Loss:   0.012566 => Txt Tokens per Sec:     8256 || Lr: 0.000100
2024-02-02 18:45:38,258 Epoch 645: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-02 18:45:38,259 EPOCH 646
2024-02-02 18:45:42,822 Epoch 646: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-02 18:45:42,822 EPOCH 647
2024-02-02 18:45:47,629 Epoch 647: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.01 
2024-02-02 18:45:47,629 EPOCH 648
2024-02-02 18:45:47,866 [Epoch: 648 Step: 00022000] Batch Recognition Loss:   0.000176 => Gls Tokens per Sec:     2708 || Batch Translation Loss:   0.069469 => Txt Tokens per Sec:     8040 || Lr: 0.000100
2024-02-02 18:45:56,370 Validation result at epoch 648, step    22000: duration: 8.5033s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00044	Translation Loss: 93554.91406	PPL: 11638.16797
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 1.10	(BLEU-1: 10.91,	BLEU-2: 3.85,	BLEU-3: 1.86,	BLEU-4: 1.10)
	CHRF 17.06	ROUGE 9.63
2024-02-02 18:45:56,371 Logging Recognition and Translation Outputs
2024-02-02 18:45:56,371 ========================================================================================================================
2024-02-02 18:45:56,372 Logging Sequence: 146_56.00
2024-02-02 18:45:56,372 	Gloss Reference :	A B+C+D+E
2024-02-02 18:45:56,372 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 18:45:56,372 	Gloss Alignment :	         
2024-02-02 18:45:56,372 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 18:45:56,374 	Text Reference  :	when the players go back to the hotel  as   per     rules all          of them have to     undergo rtpcr test for    covid-19 everyday
2024-02-02 18:45:56,374 	Text Hypothesis :	**** *** ******* ** **** ** the indian team follows a     superstition of **** **** saying we      will  lose before any      match   
2024-02-02 18:45:56,374 	Text Alignment  :	D    D   D       D  D    D      S      S    S       S     S               D    D    S      S       S     S    S      S        S       
2024-02-02 18:45:56,374 ========================================================================================================================
2024-02-02 18:45:56,375 Logging Sequence: 118_338.00
2024-02-02 18:45:56,375 	Gloss Reference :	A B+C+D+E
2024-02-02 18:45:56,375 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 18:45:56,375 	Gloss Alignment :	         
2024-02-02 18:45:56,375 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 18:45:56,376 	Text Reference  :	this is    why    even    messi wore it  
2024-02-02 18:45:56,376 	Text Hypothesis :	**** their maiden victory was   in   1978
2024-02-02 18:45:56,376 	Text Alignment  :	D    S     S      S       S     S    S   
2024-02-02 18:45:56,376 ========================================================================================================================
2024-02-02 18:45:56,376 Logging Sequence: 66_61.00
2024-02-02 18:45:56,376 	Gloss Reference :	A B+C+D+E
2024-02-02 18:45:56,377 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 18:45:56,377 	Gloss Alignment :	         
2024-02-02 18:45:56,377 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 18:45:56,377 	Text Reference  :	instead of returning back to his homeland because of   his  injury
2024-02-02 18:45:56,378 	Text Hypothesis :	******* ** she       led  to *** ******** a       slow over rate  
2024-02-02 18:45:56,378 	Text Alignment  :	D       D  S         S       D   D        S       S    S    S     
2024-02-02 18:45:56,378 ========================================================================================================================
2024-02-02 18:45:56,378 Logging Sequence: 81_278.00
2024-02-02 18:45:56,378 	Gloss Reference :	A B+C+D+E
2024-02-02 18:45:56,378 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 18:45:56,378 	Gloss Alignment :	         
2024-02-02 18:45:56,379 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 18:45:56,381 	Text Reference  :	of this amrapali group paid rs    3570 crore the   remaining rs 652  crore was paid  by amrapali sapphire developers a subsidiary of amrapali group
2024-02-02 18:45:56,381 	Text Hypothesis :	** **** ******** then  in   april 2021 ms    dhoni scored    35 runs so    far total of amrapali mahi     developers a subsidiary of amrapali group
2024-02-02 18:45:56,381 	Text Alignment  :	D  D    D        S     S    S     S    S     S     S         S  S    S     S   S     S           S                                                 
2024-02-02 18:45:56,382 ========================================================================================================================
2024-02-02 18:45:56,382 Logging Sequence: 162_125.00
2024-02-02 18:45:56,382 	Gloss Reference :	A B+C+D+E
2024-02-02 18:45:56,382 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 18:45:56,382 	Gloss Alignment :	         
2024-02-02 18:45:56,382 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 18:45:56,383 	Text Reference  :	in response to      this kohli received many hate    comments on social    media    
2024-02-02 18:45:56,383 	Text Hypothesis :	** ******** ronaldo has  also  become   a    violent city     in ahmedabad wonderful
2024-02-02 18:45:56,383 	Text Alignment  :	D  D        S       S    S     S        S    S       S        S  S         S        
2024-02-02 18:45:56,384 ========================================================================================================================
2024-02-02 18:46:01,075 Epoch 648: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.36 
2024-02-02 18:46:01,075 EPOCH 649
2024-02-02 18:46:05,946 Epoch 649: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.11 
2024-02-02 18:46:05,947 EPOCH 650
2024-02-02 18:46:10,229 [Epoch: 650 Step: 00022100] Batch Recognition Loss:   0.000157 => Gls Tokens per Sec:     2483 || Batch Translation Loss:   0.051350 => Txt Tokens per Sec:     6894 || Lr: 0.000100
2024-02-02 18:46:10,230 Epoch 650: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.16 
2024-02-02 18:46:10,230 EPOCH 651
2024-02-02 18:46:15,087 Epoch 651: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-02 18:46:15,088 EPOCH 652
2024-02-02 18:46:19,496 Epoch 652: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.49 
2024-02-02 18:46:19,497 EPOCH 653
2024-02-02 18:46:24,000 [Epoch: 653 Step: 00022200] Batch Recognition Loss:   0.000241 => Gls Tokens per Sec:     2219 || Batch Translation Loss:   0.069217 => Txt Tokens per Sec:     6137 || Lr: 0.000100
2024-02-02 18:46:24,284 Epoch 653: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.34 
2024-02-02 18:46:24,284 EPOCH 654
2024-02-02 18:46:28,368 Epoch 654: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.22 
2024-02-02 18:46:28,368 EPOCH 655
2024-02-02 18:46:32,411 Epoch 655: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.17 
2024-02-02 18:46:32,411 EPOCH 656
2024-02-02 18:46:36,114 [Epoch: 656 Step: 00022300] Batch Recognition Loss:   0.000228 => Gls Tokens per Sec:     2593 || Batch Translation Loss:   0.080434 => Txt Tokens per Sec:     7217 || Lr: 0.000100
2024-02-02 18:46:36,815 Epoch 656: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.00 
2024-02-02 18:46:36,815 EPOCH 657
2024-02-02 18:46:41,591 Epoch 657: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.67 
2024-02-02 18:46:41,592 EPOCH 658
2024-02-02 18:46:46,432 Epoch 658: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.92 
2024-02-02 18:46:46,433 EPOCH 659
2024-02-02 18:46:50,081 [Epoch: 659 Step: 00022400] Batch Recognition Loss:   0.000173 => Gls Tokens per Sec:     2457 || Batch Translation Loss:   0.033885 => Txt Tokens per Sec:     6759 || Lr: 0.000100
2024-02-02 18:46:50,990 Epoch 659: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.32 
2024-02-02 18:46:50,991 EPOCH 660
2024-02-02 18:46:55,694 Epoch 660: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.56 
2024-02-02 18:46:55,694 EPOCH 661
2024-02-02 18:46:59,775 Epoch 661: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.58 
2024-02-02 18:46:59,775 EPOCH 662
2024-02-02 18:47:03,577 [Epoch: 662 Step: 00022500] Batch Recognition Loss:   0.000466 => Gls Tokens per Sec:     2189 || Batch Translation Loss:   0.093980 => Txt Tokens per Sec:     5996 || Lr: 0.000100
2024-02-02 18:47:04,633 Epoch 662: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.37 
2024-02-02 18:47:04,633 EPOCH 663
2024-02-02 18:47:09,349 Epoch 663: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.85 
2024-02-02 18:47:09,350 EPOCH 664
2024-02-02 18:47:13,905 Epoch 664: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.60 
2024-02-02 18:47:13,905 EPOCH 665
2024-02-02 18:47:16,840 [Epoch: 665 Step: 00022600] Batch Recognition Loss:   0.000195 => Gls Tokens per Sec:     2532 || Batch Translation Loss:   0.066966 => Txt Tokens per Sec:     7025 || Lr: 0.000100
2024-02-02 18:47:18,537 Epoch 665: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.36 
2024-02-02 18:47:18,538 EPOCH 666
2024-02-02 18:47:23,140 Epoch 666: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.18 
2024-02-02 18:47:23,141 EPOCH 667
2024-02-02 18:47:28,050 Epoch 667: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.97 
2024-02-02 18:47:28,051 EPOCH 668
2024-02-02 18:47:30,676 [Epoch: 668 Step: 00022700] Batch Recognition Loss:   0.000236 => Gls Tokens per Sec:     2588 || Batch Translation Loss:   0.028996 => Txt Tokens per Sec:     6749 || Lr: 0.000100
2024-02-02 18:47:32,860 Epoch 668: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.57 
2024-02-02 18:47:32,860 EPOCH 669
2024-02-02 18:47:37,687 Epoch 669: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.72 
2024-02-02 18:47:37,688 EPOCH 670
2024-02-02 18:47:41,975 Epoch 670: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.71 
2024-02-02 18:47:41,976 EPOCH 671
2024-02-02 18:47:44,797 [Epoch: 671 Step: 00022800] Batch Recognition Loss:   0.000227 => Gls Tokens per Sec:     2181 || Batch Translation Loss:   0.040408 => Txt Tokens per Sec:     6145 || Lr: 0.000100
2024-02-02 18:47:46,815 Epoch 671: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.80 
2024-02-02 18:47:46,816 EPOCH 672
2024-02-02 18:47:51,095 Epoch 672: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.51 
2024-02-02 18:47:51,096 EPOCH 673
2024-02-02 18:47:55,998 Epoch 673: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.06 
2024-02-02 18:47:55,999 EPOCH 674
2024-02-02 18:47:58,203 [Epoch: 674 Step: 00022900] Batch Recognition Loss:   0.000322 => Gls Tokens per Sec:     2502 || Batch Translation Loss:   0.076619 => Txt Tokens per Sec:     6868 || Lr: 0.000100
2024-02-02 18:48:00,205 Epoch 674: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.67 
2024-02-02 18:48:00,205 EPOCH 675
2024-02-02 18:48:04,293 Epoch 675: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.21 
2024-02-02 18:48:04,293 EPOCH 676
2024-02-02 18:48:08,391 Epoch 676: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.31 
2024-02-02 18:48:08,391 EPOCH 677
2024-02-02 18:48:10,836 [Epoch: 677 Step: 00023000] Batch Recognition Loss:   0.000301 => Gls Tokens per Sec:     1993 || Batch Translation Loss:   0.149342 => Txt Tokens per Sec:     5645 || Lr: 0.000100
2024-02-02 18:48:13,378 Epoch 677: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.82 
2024-02-02 18:48:13,378 EPOCH 678
2024-02-02 18:48:18,246 Epoch 678: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.60 
2024-02-02 18:48:18,247 EPOCH 679
2024-02-02 18:48:22,839 Epoch 679: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.06 
2024-02-02 18:48:22,839 EPOCH 680
2024-02-02 18:48:24,721 [Epoch: 680 Step: 00023100] Batch Recognition Loss:   0.000245 => Gls Tokens per Sec:     2383 || Batch Translation Loss:   0.023545 => Txt Tokens per Sec:     6594 || Lr: 0.000100
2024-02-02 18:48:27,651 Epoch 680: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.55 
2024-02-02 18:48:27,651 EPOCH 681
2024-02-02 18:48:32,123 Epoch 681: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.05 
2024-02-02 18:48:32,124 EPOCH 682
2024-02-02 18:48:36,825 Epoch 682: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.83 
2024-02-02 18:48:36,825 EPOCH 683
2024-02-02 18:48:38,262 [Epoch: 683 Step: 00023200] Batch Recognition Loss:   0.000185 => Gls Tokens per Sec:     2675 || Batch Translation Loss:   0.044937 => Txt Tokens per Sec:     7394 || Lr: 0.000100
2024-02-02 18:48:41,255 Epoch 683: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.80 
2024-02-02 18:48:41,256 EPOCH 684
2024-02-02 18:48:45,963 Epoch 684: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.64 
2024-02-02 18:48:45,963 EPOCH 685
2024-02-02 18:48:50,470 Epoch 685: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.56 
2024-02-02 18:48:50,471 EPOCH 686
2024-02-02 18:48:51,606 [Epoch: 686 Step: 00023300] Batch Recognition Loss:   0.000244 => Gls Tokens per Sec:     2819 || Batch Translation Loss:   0.110859 => Txt Tokens per Sec:     7421 || Lr: 0.000100
2024-02-02 18:48:55,108 Epoch 686: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.00 
2024-02-02 18:48:55,108 EPOCH 687
2024-02-02 18:48:59,570 Epoch 687: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.78 
2024-02-02 18:48:59,570 EPOCH 688
2024-02-02 18:49:04,193 Epoch 688: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.33 
2024-02-02 18:49:04,193 EPOCH 689
2024-02-02 18:49:05,145 [Epoch: 689 Step: 00023400] Batch Recognition Loss:   0.000206 => Gls Tokens per Sec:     2692 || Batch Translation Loss:   0.060317 => Txt Tokens per Sec:     7481 || Lr: 0.000100
2024-02-02 18:49:08,760 Epoch 689: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.38 
2024-02-02 18:49:08,761 EPOCH 690
2024-02-02 18:49:13,367 Epoch 690: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.11 
2024-02-02 18:49:13,367 EPOCH 691
2024-02-02 18:49:17,794 Epoch 691: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.26 
2024-02-02 18:49:17,795 EPOCH 692
2024-02-02 18:49:18,775 [Epoch: 692 Step: 00023500] Batch Recognition Loss:   0.000170 => Gls Tokens per Sec:     1707 || Batch Translation Loss:   0.004598 => Txt Tokens per Sec:     4840 || Lr: 0.000100
2024-02-02 18:49:22,414 Epoch 692: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.85 
2024-02-02 18:49:22,415 EPOCH 693
2024-02-02 18:49:27,112 Epoch 693: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.07 
2024-02-02 18:49:27,113 EPOCH 694
2024-02-02 18:49:31,573 Epoch 694: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.92 
2024-02-02 18:49:31,573 EPOCH 695
2024-02-02 18:49:31,987 [Epoch: 695 Step: 00023600] Batch Recognition Loss:   0.000291 => Gls Tokens per Sec:     3099 || Batch Translation Loss:   0.019044 => Txt Tokens per Sec:     8271 || Lr: 0.000100
2024-02-02 18:49:36,380 Epoch 695: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.99 
2024-02-02 18:49:36,381 EPOCH 696
2024-02-02 18:49:40,754 Epoch 696: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.96 
2024-02-02 18:49:40,754 EPOCH 697
2024-02-02 18:49:45,591 Epoch 697: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.30 
2024-02-02 18:49:45,591 EPOCH 698
2024-02-02 18:49:45,967 [Epoch: 698 Step: 00023700] Batch Recognition Loss:   0.000195 => Gls Tokens per Sec:     1707 || Batch Translation Loss:   0.029708 => Txt Tokens per Sec:     5693 || Lr: 0.000100
2024-02-02 18:49:49,933 Epoch 698: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.06 
2024-02-02 18:49:49,933 EPOCH 699
2024-02-02 18:49:54,818 Epoch 699: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.76 
2024-02-02 18:49:54,818 EPOCH 700
2024-02-02 18:49:59,133 [Epoch: 700 Step: 00023800] Batch Recognition Loss:   0.000387 => Gls Tokens per Sec:     2464 || Batch Translation Loss:   0.070325 => Txt Tokens per Sec:     6840 || Lr: 0.000100
2024-02-02 18:49:59,134 Epoch 700: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.88 
2024-02-02 18:49:59,134 EPOCH 701
2024-02-02 18:50:03,991 Epoch 701: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.47 
2024-02-02 18:50:03,991 EPOCH 702
2024-02-02 18:50:08,274 Epoch 702: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.34 
2024-02-02 18:50:08,274 EPOCH 703
2024-02-02 18:50:12,817 [Epoch: 703 Step: 00023900] Batch Recognition Loss:   0.000471 => Gls Tokens per Sec:     2200 || Batch Translation Loss:   0.080362 => Txt Tokens per Sec:     6153 || Lr: 0.000100
2024-02-02 18:50:13,097 Epoch 703: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.32 
2024-02-02 18:50:13,097 EPOCH 704
2024-02-02 18:50:17,726 Epoch 704: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.74 
2024-02-02 18:50:17,727 EPOCH 705
2024-02-02 18:50:22,463 Epoch 705: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.67 
2024-02-02 18:50:22,464 EPOCH 706
2024-02-02 18:50:26,072 [Epoch: 706 Step: 00024000] Batch Recognition Loss:   0.000179 => Gls Tokens per Sec:     2592 || Batch Translation Loss:   0.024879 => Txt Tokens per Sec:     7031 || Lr: 0.000100
2024-02-02 18:50:34,389 Validation result at epoch 706, step    24000: duration: 8.3166s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00074	Translation Loss: 94128.24219	PPL: 12325.41113
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.74	(BLEU-1: 10.19,	BLEU-2: 3.15,	BLEU-3: 1.39,	BLEU-4: 0.74)
	CHRF 17.07	ROUGE 9.04
2024-02-02 18:50:34,390 Logging Recognition and Translation Outputs
2024-02-02 18:50:34,390 ========================================================================================================================
2024-02-02 18:50:34,391 Logging Sequence: 169_165.00
2024-02-02 18:50:34,391 	Gloss Reference :	A B+C+D+E
2024-02-02 18:50:34,391 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 18:50:34,391 	Gloss Alignment :	         
2024-02-02 18:50:34,391 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 18:50:34,392 	Text Reference  :	the indian government was   outraged by       the  incident and   these changes were undone by    wikipedia 
2024-02-02 18:50:34,393 	Text Hypothesis :	*** ****** ********** kohli has      revealed that mahendra singh dhoni was     the  most   loved footballer
2024-02-02 18:50:34,393 	Text Alignment  :	D   D      D          S     S        S        S    S        S     S     S       S    S      S     S         
2024-02-02 18:50:34,393 ========================================================================================================================
2024-02-02 18:50:34,393 Logging Sequence: 175_60.00
2024-02-02 18:50:34,393 	Gloss Reference :	A B+C+D+E
2024-02-02 18:50:34,393 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 18:50:34,393 	Gloss Alignment :	         
2024-02-02 18:50:34,394 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 18:50:34,394 	Text Reference  :	that is how india bagged 9   medals in the youth    tournament
2024-02-02 18:50:34,394 	Text Hypothesis :	**** ** *** as    per    the rules  of the original way       
2024-02-02 18:50:34,394 	Text Alignment  :	D    D  D   S     S      S   S      S      S        S         
2024-02-02 18:50:34,395 ========================================================================================================================
2024-02-02 18:50:34,395 Logging Sequence: 61_255.00
2024-02-02 18:50:34,395 	Gloss Reference :	A B+C+D+E
2024-02-02 18:50:34,395 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 18:50:34,395 	Gloss Alignment :	         
2024-02-02 18:50:34,395 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 18:50:34,396 	Text Reference  :	** in   2011    we  decided to    marry  and informed our families
2024-02-02 18:50:34,396 	Text Hypothesis :	so many tickets can watch   other number on  3rd      may 2023    
2024-02-02 18:50:34,396 	Text Alignment  :	I  S    S       S   S       S     S      S   S        S   S       
2024-02-02 18:50:34,397 ========================================================================================================================
2024-02-02 18:50:34,397 Logging Sequence: 173_39.00
2024-02-02 18:50:34,397 	Gloss Reference :	A B+C+D+E
2024-02-02 18:50:34,397 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 18:50:34,397 	Gloss Alignment :	         
2024-02-02 18:50:34,397 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 18:50:34,398 	Text Reference  :	***** ****** *** *** kohli   will step down  as india' captain
2024-02-02 18:50:34,398 	Text Hypothesis :	since sharma was the captain he   was  fined rs 12     lakh   
2024-02-02 18:50:34,398 	Text Alignment  :	I     I      I   I   S       S    S    S     S  S      S      
2024-02-02 18:50:34,398 ========================================================================================================================
2024-02-02 18:50:34,398 Logging Sequence: 172_82.00
2024-02-02 18:50:34,399 	Gloss Reference :	A B+C+D+E
2024-02-02 18:50:34,399 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 18:50:34,399 	Gloss Alignment :	         
2024-02-02 18:50:34,399 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 18:50:34,400 	Text Reference  :	you all know that the toss was about to start at 700 pm but it started raining at around 630 pm      
2024-02-02 18:50:34,400 	Text Hypothesis :	*** she said that *** **** *** ***** ** ***** ** *** ** *** ** ******* ******* he has    5   children
2024-02-02 18:50:34,400 	Text Alignment  :	D   S   S         D   D    D   D     D  D     D  D   D  D   D  D       D       S  S      S   S       
2024-02-02 18:50:34,400 ========================================================================================================================
2024-02-02 18:50:35,183 Epoch 706: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.66 
2024-02-02 18:50:35,183 EPOCH 707
2024-02-02 18:50:40,142 Epoch 707: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.94 
2024-02-02 18:50:40,143 EPOCH 708
2024-02-02 18:50:44,832 Epoch 708: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.62 
2024-02-02 18:50:44,833 EPOCH 709
2024-02-02 18:50:48,472 [Epoch: 709 Step: 00024100] Batch Recognition Loss:   0.000195 => Gls Tokens per Sec:     2394 || Batch Translation Loss:   0.020491 => Txt Tokens per Sec:     6516 || Lr: 0.000100
2024-02-02 18:50:49,461 Epoch 709: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.09 
2024-02-02 18:50:49,461 EPOCH 710
2024-02-02 18:50:54,040 Epoch 710: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.91 
2024-02-02 18:50:54,041 EPOCH 711
2024-02-02 18:50:58,640 Epoch 711: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.16 
2024-02-02 18:50:58,640 EPOCH 712
2024-02-02 18:51:01,839 [Epoch: 712 Step: 00024200] Batch Recognition Loss:   0.000250 => Gls Tokens per Sec:     2525 || Batch Translation Loss:   0.030996 => Txt Tokens per Sec:     7073 || Lr: 0.000100
2024-02-02 18:51:03,035 Epoch 712: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.99 
2024-02-02 18:51:03,035 EPOCH 713
2024-02-02 18:51:07,735 Epoch 713: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.95 
2024-02-02 18:51:07,735 EPOCH 714
2024-02-02 18:51:12,569 Epoch 714: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.19 
2024-02-02 18:51:12,569 EPOCH 715
2024-02-02 18:51:15,274 [Epoch: 715 Step: 00024300] Batch Recognition Loss:   0.000232 => Gls Tokens per Sec:     2748 || Batch Translation Loss:   0.086004 => Txt Tokens per Sec:     7442 || Lr: 0.000100
2024-02-02 18:51:16,854 Epoch 715: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.89 
2024-02-02 18:51:16,855 EPOCH 716
2024-02-02 18:51:21,716 Epoch 716: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.75 
2024-02-02 18:51:21,717 EPOCH 717
2024-02-02 18:51:25,984 Epoch 717: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.09 
2024-02-02 18:51:25,984 EPOCH 718
2024-02-02 18:51:29,114 [Epoch: 718 Step: 00024400] Batch Recognition Loss:   0.000113 => Gls Tokens per Sec:     2170 || Batch Translation Loss:   0.080639 => Txt Tokens per Sec:     6167 || Lr: 0.000100
2024-02-02 18:51:30,858 Epoch 718: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.24 
2024-02-02 18:51:30,858 EPOCH 719
2024-02-02 18:51:35,198 Epoch 719: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.04 
2024-02-02 18:51:35,198 EPOCH 720
2024-02-02 18:51:40,112 Epoch 720: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.21 
2024-02-02 18:51:40,113 EPOCH 721
2024-02-02 18:51:42,902 [Epoch: 721 Step: 00024500] Batch Recognition Loss:   0.000253 => Gls Tokens per Sec:     2206 || Batch Translation Loss:   0.044478 => Txt Tokens per Sec:     6585 || Lr: 0.000100
2024-02-02 18:51:44,356 Epoch 721: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.12 
2024-02-02 18:51:44,356 EPOCH 722
2024-02-02 18:51:49,537 Epoch 722: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.94 
2024-02-02 18:51:49,537 EPOCH 723
2024-02-02 18:51:54,399 Epoch 723: Total Training Recognition Loss 0.01  Total Training Translation Loss 5.18 
2024-02-02 18:51:54,399 EPOCH 724
2024-02-02 18:51:57,014 [Epoch: 724 Step: 00024600] Batch Recognition Loss:   0.000196 => Gls Tokens per Sec:     2204 || Batch Translation Loss:   0.067970 => Txt Tokens per Sec:     6055 || Lr: 0.000100
2024-02-02 18:51:59,348 Epoch 724: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.30 
2024-02-02 18:51:59,349 EPOCH 725
2024-02-02 18:52:03,861 Epoch 725: Total Training Recognition Loss 0.01  Total Training Translation Loss 5.07 
2024-02-02 18:52:03,861 EPOCH 726
2024-02-02 18:52:08,484 Epoch 726: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.45 
2024-02-02 18:52:08,484 EPOCH 727
2024-02-02 18:52:10,408 [Epoch: 727 Step: 00024700] Batch Recognition Loss:   0.000272 => Gls Tokens per Sec:     2663 || Batch Translation Loss:   0.211821 => Txt Tokens per Sec:     7254 || Lr: 0.000100
2024-02-02 18:52:13,193 Epoch 727: Total Training Recognition Loss 0.01  Total Training Translation Loss 7.32 
2024-02-02 18:52:13,194 EPOCH 728
2024-02-02 18:52:17,655 Epoch 728: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.78 
2024-02-02 18:52:17,655 EPOCH 729
2024-02-02 18:52:22,424 Epoch 729: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.78 
2024-02-02 18:52:22,425 EPOCH 730
2024-02-02 18:52:24,390 [Epoch: 730 Step: 00024800] Batch Recognition Loss:   0.000323 => Gls Tokens per Sec:     2281 || Batch Translation Loss:   0.023488 => Txt Tokens per Sec:     6480 || Lr: 0.000100
2024-02-02 18:52:26,867 Epoch 730: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.38 
2024-02-02 18:52:26,867 EPOCH 731
2024-02-02 18:52:30,890 Epoch 731: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.70 
2024-02-02 18:52:30,890 EPOCH 732
2024-02-02 18:52:35,773 Epoch 732: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.93 
2024-02-02 18:52:35,773 EPOCH 733
2024-02-02 18:52:37,258 [Epoch: 733 Step: 00024900] Batch Recognition Loss:   0.000276 => Gls Tokens per Sec:     2590 || Batch Translation Loss:   0.041069 => Txt Tokens per Sec:     7249 || Lr: 0.000100
2024-02-02 18:52:40,510 Epoch 733: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.95 
2024-02-02 18:52:40,510 EPOCH 734
2024-02-02 18:52:44,980 Epoch 734: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.63 
2024-02-02 18:52:44,980 EPOCH 735
2024-02-02 18:52:49,692 Epoch 735: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.35 
2024-02-02 18:52:49,693 EPOCH 736
2024-02-02 18:52:50,821 [Epoch: 736 Step: 00025000] Batch Recognition Loss:   0.000244 => Gls Tokens per Sec:     2839 || Batch Translation Loss:   0.050040 => Txt Tokens per Sec:     7566 || Lr: 0.000100
2024-02-02 18:52:54,126 Epoch 736: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.92 
2024-02-02 18:52:54,126 EPOCH 737
2024-02-02 18:52:58,839 Epoch 737: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-02 18:52:58,840 EPOCH 738
2024-02-02 18:53:03,277 Epoch 738: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.78 
2024-02-02 18:53:03,278 EPOCH 739
2024-02-02 18:53:04,174 [Epoch: 739 Step: 00025100] Batch Recognition Loss:   0.000165 => Gls Tokens per Sec:     2578 || Batch Translation Loss:   0.017567 => Txt Tokens per Sec:     7048 || Lr: 0.000100
2024-02-02 18:53:08,115 Epoch 739: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.92 
2024-02-02 18:53:08,115 EPOCH 740
2024-02-02 18:53:12,422 Epoch 740: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.01 
2024-02-02 18:53:12,422 EPOCH 741
2024-02-02 18:53:17,122 Epoch 741: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.94 
2024-02-02 18:53:17,122 EPOCH 742
2024-02-02 18:53:17,667 [Epoch: 742 Step: 00025200] Batch Recognition Loss:   0.000236 => Gls Tokens per Sec:     3529 || Batch Translation Loss:   0.017237 => Txt Tokens per Sec:     8237 || Lr: 0.000100
2024-02-02 18:53:21,745 Epoch 742: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.82 
2024-02-02 18:53:21,745 EPOCH 743
2024-02-02 18:53:26,593 Epoch 743: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.43 
2024-02-02 18:53:26,594 EPOCH 744
2024-02-02 18:53:30,958 Epoch 744: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.29 
2024-02-02 18:53:30,959 EPOCH 745
2024-02-02 18:53:31,672 [Epoch: 745 Step: 00025300] Batch Recognition Loss:   0.000327 => Gls Tokens per Sec:     1798 || Batch Translation Loss:   0.030134 => Txt Tokens per Sec:     4697 || Lr: 0.000100
2024-02-02 18:53:35,733 Epoch 745: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.37 
2024-02-02 18:53:35,734 EPOCH 746
2024-02-02 18:53:40,062 Epoch 746: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.49 
2024-02-02 18:53:40,062 EPOCH 747
2024-02-02 18:53:44,828 Epoch 747: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.91 
2024-02-02 18:53:44,828 EPOCH 748
2024-02-02 18:53:45,067 [Epoch: 748 Step: 00025400] Batch Recognition Loss:   0.000242 => Gls Tokens per Sec:     2696 || Batch Translation Loss:   0.101854 => Txt Tokens per Sec:     7047 || Lr: 0.000100
2024-02-02 18:53:49,197 Epoch 748: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.99 
2024-02-02 18:53:49,197 EPOCH 749
2024-02-02 18:53:53,996 Epoch 749: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.42 
2024-02-02 18:53:53,998 EPOCH 750
2024-02-02 18:53:58,449 [Epoch: 750 Step: 00025500] Batch Recognition Loss:   0.000204 => Gls Tokens per Sec:     2389 || Batch Translation Loss:   0.034553 => Txt Tokens per Sec:     6631 || Lr: 0.000100
2024-02-02 18:53:58,450 Epoch 750: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.45 
2024-02-02 18:53:58,450 EPOCH 751
2024-02-02 18:54:03,136 Epoch 751: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.39 
2024-02-02 18:54:03,136 EPOCH 752
2024-02-02 18:54:07,635 Epoch 752: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.61 
2024-02-02 18:54:07,636 EPOCH 753
2024-02-02 18:54:11,892 [Epoch: 753 Step: 00025600] Batch Recognition Loss:   0.000188 => Gls Tokens per Sec:     2348 || Batch Translation Loss:   0.038776 => Txt Tokens per Sec:     6434 || Lr: 0.000100
2024-02-02 18:54:12,280 Epoch 753: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.66 
2024-02-02 18:54:12,281 EPOCH 754
2024-02-02 18:54:16,782 Epoch 754: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.85 
2024-02-02 18:54:16,783 EPOCH 755
2024-02-02 18:54:21,432 Epoch 755: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.33 
2024-02-02 18:54:21,432 EPOCH 756
2024-02-02 18:54:25,634 [Epoch: 756 Step: 00025700] Batch Recognition Loss:   0.000160 => Gls Tokens per Sec:     2285 || Batch Translation Loss:   0.041930 => Txt Tokens per Sec:     6435 || Lr: 0.000100
2024-02-02 18:54:26,047 Epoch 756: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.99 
2024-02-02 18:54:26,048 EPOCH 757
2024-02-02 18:54:30,688 Epoch 757: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.53 
2024-02-02 18:54:30,688 EPOCH 758
2024-02-02 18:54:34,794 Epoch 758: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.96 
2024-02-02 18:54:34,794 EPOCH 759
2024-02-02 18:54:38,334 [Epoch: 759 Step: 00025800] Batch Recognition Loss:   0.000205 => Gls Tokens per Sec:     2533 || Batch Translation Loss:   0.032939 => Txt Tokens per Sec:     7138 || Lr: 0.000100
2024-02-02 18:54:38,942 Epoch 759: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.59 
2024-02-02 18:54:38,942 EPOCH 760
2024-02-02 18:54:43,837 Epoch 760: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.17 
2024-02-02 18:54:43,837 EPOCH 761
2024-02-02 18:54:47,988 Epoch 761: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.95 
2024-02-02 18:54:47,989 EPOCH 762
2024-02-02 18:54:51,143 [Epoch: 762 Step: 00025900] Batch Recognition Loss:   0.000293 => Gls Tokens per Sec:     2559 || Batch Translation Loss:   0.068062 => Txt Tokens per Sec:     7058 || Lr: 0.000100
2024-02-02 18:54:52,226 Epoch 762: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.20 
2024-02-02 18:54:52,226 EPOCH 763
2024-02-02 18:54:57,126 Epoch 763: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.80 
2024-02-02 18:54:57,127 EPOCH 764
2024-02-02 18:55:01,878 Epoch 764: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.90 
2024-02-02 18:55:01,879 EPOCH 765
2024-02-02 18:55:05,092 [Epoch: 765 Step: 00026000] Batch Recognition Loss:   0.000244 => Gls Tokens per Sec:     2314 || Batch Translation Loss:   0.691252 => Txt Tokens per Sec:     6458 || Lr: 0.000100
2024-02-02 18:55:13,642 Validation result at epoch 765, step    26000: duration: 8.5505s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00079	Translation Loss: 93606.57812	PPL: 11698.49219
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.65	(BLEU-1: 10.76,	BLEU-2: 3.27,	BLEU-3: 1.37,	BLEU-4: 0.65)
	CHRF 17.49	ROUGE 9.20
2024-02-02 18:55:13,643 Logging Recognition and Translation Outputs
2024-02-02 18:55:13,643 ========================================================================================================================
2024-02-02 18:55:13,643 Logging Sequence: 130_139.00
2024-02-02 18:55:13,644 	Gloss Reference :	A B+C+D+E
2024-02-02 18:55:13,644 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 18:55:13,644 	Gloss Alignment :	         
2024-02-02 18:55:13,644 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 18:55:13,646 	Text Reference  :	he shared a picture of  a little pouch he knit for his olympic gold   medal with uk flag on  one  side   and **** japanese flag on      the     other
2024-02-02 18:55:13,646 	Text Hypothesis :	he ****** * ******* won a ****** ***** ** **** *** *** ******* bronze medal **** ** at   the 2012 london and 2016 rio      de   janeiro olympic games
2024-02-02 18:55:13,647 	Text Alignment  :	   D      D D       S     D      D     D  D    D   D   D       S            D    D  S    S   S    S          I    S        S    S       S       S    
2024-02-02 18:55:13,647 ========================================================================================================================
2024-02-02 18:55:13,647 Logging Sequence: 72_194.00
2024-02-02 18:55:13,647 	Gloss Reference :	A B+C+D+E
2024-02-02 18:55:13,647 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 18:55:13,647 	Gloss Alignment :	         
2024-02-02 18:55:13,647 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 18:55:13,648 	Text Reference  :	shah told her to do what she  wants and      filed a police complaint against her  
2024-02-02 18:55:13,648 	Text Hypothesis :	**** **** *** ** ** they have been  shocking for   5 or     going     on      there
2024-02-02 18:55:13,649 	Text Alignment  :	D    D    D   D  D  S    S    S     S        S     S S      S         S       S    
2024-02-02 18:55:13,649 ========================================================================================================================
2024-02-02 18:55:13,649 Logging Sequence: 69_177.00
2024-02-02 18:55:13,649 	Gloss Reference :	A B+C+D+E
2024-02-02 18:55:13,649 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 18:55:13,649 	Gloss Alignment :	         
2024-02-02 18:55:13,649 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 18:55:13,651 	Text Reference  :	he said 'i will continue playing i   know   it's about   time i    retire i     also    have   a       knee    condition
2024-02-02 18:55:13,651 	Text Hypothesis :	** **** ** **** ******** when    the stumps were waiting to   play the    match between mumbai indians indians mi       
2024-02-02 18:55:13,651 	Text Alignment  :	D  D    D  D    D        S       S   S      S    S       S    S    S      S     S       S      S       S       S        
2024-02-02 18:55:13,651 ========================================================================================================================
2024-02-02 18:55:13,651 Logging Sequence: 95_118.00
2024-02-02 18:55:13,652 	Gloss Reference :	A B+C+D+E
2024-02-02 18:55:13,652 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 18:55:13,652 	Gloss Alignment :	         
2024-02-02 18:55:13,652 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 18:55:13,653 	Text Reference  :	*** **** ** **** *** *** the game  was stopped strangely due    to excessive sunlight
2024-02-02 18:55:13,653 	Text Hypothesis :	and post my cool are all the pitch and hope    he        failed to ********* bat     
2024-02-02 18:55:13,653 	Text Alignment  :	I   I    I  I    I   I       S     S   S       S         S         D         S       
2024-02-02 18:55:13,653 ========================================================================================================================
2024-02-02 18:55:13,653 Logging Sequence: 112_8.00
2024-02-02 18:55:13,654 	Gloss Reference :	A B+C+D+E
2024-02-02 18:55:13,654 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 18:55:13,654 	Gloss Alignment :	         
2024-02-02 18:55:13,654 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 18:55:13,656 	Text Reference  :	before there were 8   teams such  as         mumbai indians delhi  capitals punjab     kings etc  and now there will be 10 teams in 2022
2024-02-02 18:55:13,656 	Text Hypothesis :	****** ***** **** the rpsg  group previously owned  the     rising pune     supergiant in    2016 and *** ***** **** ** ** ***** ** 2017
2024-02-02 18:55:13,656 	Text Alignment  :	D      D     D    S   S     S     S          S      S       S      S        S          S     S        D   D     D    D  D  D     D  S   
2024-02-02 18:55:13,656 ========================================================================================================================
2024-02-02 18:55:14,944 Epoch 765: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.29 
2024-02-02 18:55:14,944 EPOCH 766
2024-02-02 18:55:19,944 Epoch 766: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.73 
2024-02-02 18:55:19,945 EPOCH 767
2024-02-02 18:55:24,299 Epoch 767: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.24 
2024-02-02 18:55:24,299 EPOCH 768
2024-02-02 18:55:27,531 [Epoch: 768 Step: 00026100] Batch Recognition Loss:   0.000266 => Gls Tokens per Sec:     2179 || Batch Translation Loss:   0.032617 => Txt Tokens per Sec:     5930 || Lr: 0.000100
2024-02-02 18:55:29,186 Epoch 768: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.14 
2024-02-02 18:55:29,186 EPOCH 769
2024-02-02 18:55:33,439 Epoch 769: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.41 
2024-02-02 18:55:33,440 EPOCH 770
2024-02-02 18:55:38,274 Epoch 770: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.01 
2024-02-02 18:55:38,275 EPOCH 771
2024-02-02 18:55:40,932 [Epoch: 771 Step: 00026200] Batch Recognition Loss:   0.000488 => Gls Tokens per Sec:     2315 || Batch Translation Loss:   0.406856 => Txt Tokens per Sec:     6560 || Lr: 0.000100
2024-02-02 18:55:42,455 Epoch 771: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.58 
2024-02-02 18:55:42,456 EPOCH 772
2024-02-02 18:55:47,376 Epoch 772: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.52 
2024-02-02 18:55:47,376 EPOCH 773
2024-02-02 18:55:51,938 Epoch 773: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.77 
2024-02-02 18:55:51,939 EPOCH 774
2024-02-02 18:55:54,400 [Epoch: 774 Step: 00026300] Batch Recognition Loss:   0.000227 => Gls Tokens per Sec:     2240 || Batch Translation Loss:   0.036375 => Txt Tokens per Sec:     6216 || Lr: 0.000100
2024-02-02 18:55:56,596 Epoch 774: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.12 
2024-02-02 18:55:56,596 EPOCH 775
2024-02-02 18:56:01,292 Epoch 775: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.44 
2024-02-02 18:56:01,292 EPOCH 776
2024-02-02 18:56:05,767 Epoch 776: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.72 
2024-02-02 18:56:05,767 EPOCH 777
2024-02-02 18:56:07,988 [Epoch: 777 Step: 00026400] Batch Recognition Loss:   0.000304 => Gls Tokens per Sec:     2193 || Batch Translation Loss:   0.050428 => Txt Tokens per Sec:     6320 || Lr: 0.000100
2024-02-02 18:56:10,500 Epoch 777: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.38 
2024-02-02 18:56:10,500 EPOCH 778
2024-02-02 18:56:14,935 Epoch 778: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.24 
2024-02-02 18:56:14,935 EPOCH 779
2024-02-02 18:56:19,671 Epoch 779: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.26 
2024-02-02 18:56:19,672 EPOCH 780
2024-02-02 18:56:21,712 [Epoch: 780 Step: 00026500] Batch Recognition Loss:   0.000244 => Gls Tokens per Sec:     2075 || Batch Translation Loss:   0.032166 => Txt Tokens per Sec:     5969 || Lr: 0.000100
2024-02-02 18:56:24,071 Epoch 780: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.30 
2024-02-02 18:56:24,071 EPOCH 781
2024-02-02 18:56:28,859 Epoch 781: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.23 
2024-02-02 18:56:28,860 EPOCH 782
2024-02-02 18:56:33,257 Epoch 782: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.15 
2024-02-02 18:56:33,258 EPOCH 783
2024-02-02 18:56:34,804 [Epoch: 783 Step: 00026600] Batch Recognition Loss:   0.000146 => Gls Tokens per Sec:     2485 || Batch Translation Loss:   0.023431 => Txt Tokens per Sec:     7045 || Lr: 0.000100
2024-02-02 18:56:38,030 Epoch 783: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.35 
2024-02-02 18:56:38,030 EPOCH 784
2024-02-02 18:56:42,366 Epoch 784: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.02 
2024-02-02 18:56:42,366 EPOCH 785
2024-02-02 18:56:47,204 Epoch 785: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.54 
2024-02-02 18:56:47,205 EPOCH 786
2024-02-02 18:56:48,306 [Epoch: 786 Step: 00026700] Batch Recognition Loss:   0.000323 => Gls Tokens per Sec:     2682 || Batch Translation Loss:   0.029749 => Txt Tokens per Sec:     7033 || Lr: 0.000100
2024-02-02 18:56:51,529 Epoch 786: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.50 
2024-02-02 18:56:51,529 EPOCH 787
2024-02-02 18:56:56,370 Epoch 787: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.36 
2024-02-02 18:56:56,370 EPOCH 788
2024-02-02 18:57:00,618 Epoch 788: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.90 
2024-02-02 18:57:00,618 EPOCH 789
2024-02-02 18:57:01,671 [Epoch: 789 Step: 00026800] Batch Recognition Loss:   0.000331 => Gls Tokens per Sec:     2435 || Batch Translation Loss:   0.054392 => Txt Tokens per Sec:     6891 || Lr: 0.000100
2024-02-02 18:57:05,513 Epoch 789: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.33 
2024-02-02 18:57:05,513 EPOCH 790
2024-02-02 18:57:09,770 Epoch 790: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.02 
2024-02-02 18:57:09,770 EPOCH 791
2024-02-02 18:57:14,657 Epoch 791: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.02 
2024-02-02 18:57:14,658 EPOCH 792
2024-02-02 18:57:15,268 [Epoch: 792 Step: 00026900] Batch Recognition Loss:   0.000222 => Gls Tokens per Sec:     3151 || Batch Translation Loss:   0.015748 => Txt Tokens per Sec:     7713 || Lr: 0.000100
2024-02-02 18:57:18,895 Epoch 792: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.73 
2024-02-02 18:57:18,895 EPOCH 793
2024-02-02 18:57:23,820 Epoch 793: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.29 
2024-02-02 18:57:23,820 EPOCH 794
2024-02-02 18:57:27,998 Epoch 794: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.54 
2024-02-02 18:57:27,998 EPOCH 795
2024-02-02 18:57:28,369 [Epoch: 795 Step: 00027000] Batch Recognition Loss:   0.000181 => Gls Tokens per Sec:     3459 || Batch Translation Loss:   0.048285 => Txt Tokens per Sec:     8424 || Lr: 0.000100
2024-02-02 18:57:32,128 Epoch 795: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.71 
2024-02-02 18:57:32,128 EPOCH 796
2024-02-02 18:57:36,243 Epoch 796: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.61 
2024-02-02 18:57:36,244 EPOCH 797
2024-02-02 18:57:41,400 Epoch 797: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.71 
2024-02-02 18:57:41,401 EPOCH 798
2024-02-02 18:57:41,602 [Epoch: 798 Step: 00027100] Batch Recognition Loss:   0.000239 => Gls Tokens per Sec:     3200 || Batch Translation Loss:   0.054436 => Txt Tokens per Sec:     8250 || Lr: 0.000100
2024-02-02 18:57:46,414 Epoch 798: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.43 
2024-02-02 18:57:46,414 EPOCH 799
2024-02-02 18:57:51,068 Epoch 799: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.34 
2024-02-02 18:57:51,068 EPOCH 800
2024-02-02 18:57:55,431 [Epoch: 800 Step: 00027200] Batch Recognition Loss:   0.000408 => Gls Tokens per Sec:     2436 || Batch Translation Loss:   0.114465 => Txt Tokens per Sec:     6763 || Lr: 0.000100
2024-02-02 18:57:55,432 Epoch 800: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.66 
2024-02-02 18:57:55,432 EPOCH 801
2024-02-02 18:58:00,270 Epoch 801: Total Training Recognition Loss 0.05  Total Training Translation Loss 13.22 
2024-02-02 18:58:00,271 EPOCH 802
2024-02-02 18:58:05,051 Epoch 802: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.65 
2024-02-02 18:58:05,052 EPOCH 803
2024-02-02 18:58:09,228 [Epoch: 803 Step: 00027300] Batch Recognition Loss:   0.000343 => Gls Tokens per Sec:     2393 || Batch Translation Loss:   0.075315 => Txt Tokens per Sec:     6684 || Lr: 0.000100
2024-02-02 18:58:09,403 Epoch 803: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.40 
2024-02-02 18:58:09,403 EPOCH 804
2024-02-02 18:58:14,169 Epoch 804: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.80 
2024-02-02 18:58:14,170 EPOCH 805
2024-02-02 18:58:18,475 Epoch 805: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.88 
2024-02-02 18:58:18,475 EPOCH 806
2024-02-02 18:58:22,760 [Epoch: 806 Step: 00027400] Batch Recognition Loss:   0.000289 => Gls Tokens per Sec:     2183 || Batch Translation Loss:   0.014520 => Txt Tokens per Sec:     6086 || Lr: 0.000100
2024-02-02 18:58:23,320 Epoch 806: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.98 
2024-02-02 18:58:23,320 EPOCH 807
2024-02-02 18:58:27,925 Epoch 807: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.93 
2024-02-02 18:58:27,925 EPOCH 808
2024-02-02 18:58:32,767 Epoch 808: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-02 18:58:32,767 EPOCH 809
2024-02-02 18:58:36,224 [Epoch: 809 Step: 00027500] Batch Recognition Loss:   0.000229 => Gls Tokens per Sec:     2592 || Batch Translation Loss:   0.019383 => Txt Tokens per Sec:     7263 || Lr: 0.000100
2024-02-02 18:58:36,915 Epoch 809: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-02 18:58:36,915 EPOCH 810
2024-02-02 18:58:41,812 Epoch 810: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.85 
2024-02-02 18:58:41,813 EPOCH 811
2024-02-02 18:58:46,047 Epoch 811: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.96 
2024-02-02 18:58:46,047 EPOCH 812
2024-02-02 18:58:49,848 [Epoch: 812 Step: 00027600] Batch Recognition Loss:   0.000133 => Gls Tokens per Sec:     2124 || Batch Translation Loss:   0.014855 => Txt Tokens per Sec:     5822 || Lr: 0.000100
2024-02-02 18:58:51,009 Epoch 812: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-02 18:58:51,009 EPOCH 813
2024-02-02 18:58:55,207 Epoch 813: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-02 18:58:55,208 EPOCH 814
2024-02-02 18:59:00,169 Epoch 814: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-02 18:59:00,170 EPOCH 815
2024-02-02 18:59:03,008 [Epoch: 815 Step: 00027700] Batch Recognition Loss:   0.000145 => Gls Tokens per Sec:     2619 || Batch Translation Loss:   0.012601 => Txt Tokens per Sec:     7244 || Lr: 0.000100
2024-02-02 18:59:04,475 Epoch 815: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-02 18:59:04,476 EPOCH 816
2024-02-02 18:59:09,357 Epoch 816: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.72 
2024-02-02 18:59:09,358 EPOCH 817
2024-02-02 18:59:13,627 Epoch 817: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.04 
2024-02-02 18:59:13,627 EPOCH 818
2024-02-02 18:59:16,775 [Epoch: 818 Step: 00027800] Batch Recognition Loss:   0.000206 => Gls Tokens per Sec:     2158 || Batch Translation Loss:   0.015921 => Txt Tokens per Sec:     6061 || Lr: 0.000100
2024-02-02 18:59:18,459 Epoch 818: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-02 18:59:18,459 EPOCH 819
2024-02-02 18:59:22,682 Epoch 819: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-02 18:59:22,683 EPOCH 820
2024-02-02 18:59:27,574 Epoch 820: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.48 
2024-02-02 18:59:27,575 EPOCH 821
2024-02-02 18:59:30,205 [Epoch: 821 Step: 00027900] Batch Recognition Loss:   0.000618 => Gls Tokens per Sec:     2435 || Batch Translation Loss:   0.125289 => Txt Tokens per Sec:     6832 || Lr: 0.000100
2024-02-02 18:59:31,926 Epoch 821: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.34 
2024-02-02 18:59:31,927 EPOCH 822
2024-02-02 18:59:36,753 Epoch 822: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.15 
2024-02-02 18:59:36,753 EPOCH 823
2024-02-02 18:59:40,908 Epoch 823: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.04 
2024-02-02 18:59:40,909 EPOCH 824
2024-02-02 18:59:43,175 [Epoch: 824 Step: 00028000] Batch Recognition Loss:   0.000226 => Gls Tokens per Sec:     2432 || Batch Translation Loss:   0.056396 => Txt Tokens per Sec:     6374 || Lr: 0.000100
2024-02-02 18:59:51,871 Validation result at epoch 824, step    28000: duration: 8.6946s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00077	Translation Loss: 91738.74219	PPL: 9704.07031
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.74	(BLEU-1: 10.16,	BLEU-2: 3.18,	BLEU-3: 1.39,	BLEU-4: 0.74)
	CHRF 16.97	ROUGE 8.86
2024-02-02 18:59:51,872 Logging Recognition and Translation Outputs
2024-02-02 18:59:51,873 ========================================================================================================================
2024-02-02 18:59:51,873 Logging Sequence: 67_98.00
2024-02-02 18:59:51,873 	Gloss Reference :	A B+C+D+E
2024-02-02 18:59:51,873 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 18:59:51,873 	Gloss Alignment :	         
2024-02-02 18:59:51,873 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 18:59:51,875 	Text Reference  :	it saddens me to   see people suffering and  dying   due to      lack   of ****** ******** *** ***** ** ****** ** *** oxygen     
2024-02-02 18:59:51,875 	Text Hypothesis :	** i       am sure you all    must      have enjoyed the current season of indian wrestler but would be played in the semi-finals
2024-02-02 18:59:51,875 	Text Alignment  :	D  S       S  S    S   S      S         S    S       S   S       S         I      I        I   I     I  I      I  I   S          
2024-02-02 18:59:51,875 ========================================================================================================================
2024-02-02 18:59:51,876 Logging Sequence: 157_83.00
2024-02-02 18:59:51,876 	Gloss Reference :	A B+C+D+E
2024-02-02 18:59:51,876 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 18:59:51,876 	Gloss Alignment :	         
2024-02-02 18:59:51,876 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 18:59:51,878 	Text Reference  :	also when you eat sandwich at        a  streetside hawker or   stall the   sandwich maker     will      first apply     butter with a       knife 
2024-02-02 18:59:51,878 	Text Hypothesis :	**** **** *** *** however  yesterday on 28th       june   2023 virat kohli said     ganguly's statement is    extremely fit    and  gujarat giants
2024-02-02 18:59:51,878 	Text Alignment  :	D    D    D   D   S        S         S  S          S      S    S     S     S        S         S         S     S         S      S    S       S     
2024-02-02 18:59:51,878 ========================================================================================================================
2024-02-02 18:59:51,879 Logging Sequence: 76_35.00
2024-02-02 18:59:51,879 	Gloss Reference :	A B+C+D+E
2024-02-02 18:59:51,879 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 18:59:51,879 	Gloss Alignment :	         
2024-02-02 18:59:51,879 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 18:59:51,880 	Text Reference  :	*** bcci   president sourav ganguly along with   board secretary jay     shah
2024-02-02 18:59:51,880 	Text Hypothesis :	the indian premiere  league ipl     the   indian team  is        leading 2-1 
2024-02-02 18:59:51,880 	Text Alignment  :	I   S      S         S      S       S     S      S     S         S       S   
2024-02-02 18:59:51,880 ========================================================================================================================
2024-02-02 18:59:51,880 Logging Sequence: 139_180.00
2024-02-02 18:59:51,881 	Gloss Reference :	A B+C+D+E
2024-02-02 18:59:51,881 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 18:59:51,881 	Gloss Alignment :	         
2024-02-02 18:59:51,881 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 18:59:51,881 	Text Reference  :	* *** **** netherlands also  faced similar riots 
2024-02-02 18:59:51,881 	Text Hypothesis :	a few days went        viral on    the     matter
2024-02-02 18:59:51,882 	Text Alignment  :	I I   I    S           S     S     S       S     
2024-02-02 18:59:51,882 ========================================================================================================================
2024-02-02 18:59:51,882 Logging Sequence: 98_87.00
2024-02-02 18:59:51,882 	Gloss Reference :	A B+C+D+E
2024-02-02 18:59:51,882 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 18:59:51,882 	Gloss Alignment :	         
2024-02-02 18:59:51,882 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 18:59:51,884 	Text Reference  :	*** instead of starting afresh in  2021 the  organizers opted to ** resume     with the previous edition   
2024-02-02 18:59:51,884 	Text Hypothesis :	the coach   of ******** ****** the 10   team will       have  to be applicable in   the ******** tournament
2024-02-02 18:59:51,884 	Text Alignment  :	I   S          D        D      S   S    S    S          S        I  S          S        D        S         
2024-02-02 18:59:51,884 ========================================================================================================================
2024-02-02 18:59:54,553 Epoch 824: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.92 
2024-02-02 18:59:54,553 EPOCH 825
2024-02-02 18:59:59,383 Epoch 825: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.79 
2024-02-02 18:59:59,383 EPOCH 826
2024-02-02 19:00:03,962 Epoch 826: Total Training Recognition Loss 0.01  Total Training Translation Loss 5.40 
2024-02-02 19:00:03,962 EPOCH 827
2024-02-02 19:00:06,341 [Epoch: 827 Step: 00028100] Batch Recognition Loss:   0.000328 => Gls Tokens per Sec:     2049 || Batch Translation Loss:   0.069688 => Txt Tokens per Sec:     6074 || Lr: 0.000100
2024-02-02 19:00:08,593 Epoch 827: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.44 
2024-02-02 19:00:08,594 EPOCH 828
2024-02-02 19:00:13,687 Epoch 828: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.61 
2024-02-02 19:00:13,688 EPOCH 829
2024-02-02 19:00:18,563 Epoch 829: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.00 
2024-02-02 19:00:18,563 EPOCH 830
2024-02-02 19:00:20,114 [Epoch: 830 Step: 00028200] Batch Recognition Loss:   0.000361 => Gls Tokens per Sec:     2729 || Batch Translation Loss:   0.162608 => Txt Tokens per Sec:     7488 || Lr: 0.000100
2024-02-02 19:00:22,781 Epoch 830: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.77 
2024-02-02 19:00:22,781 EPOCH 831
2024-02-02 19:00:27,258 Epoch 831: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.23 
2024-02-02 19:00:27,259 EPOCH 832
2024-02-02 19:00:31,852 Epoch 832: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.98 
2024-02-02 19:00:31,852 EPOCH 833
2024-02-02 19:00:33,018 [Epoch: 833 Step: 00028300] Batch Recognition Loss:   0.000213 => Gls Tokens per Sec:     3297 || Batch Translation Loss:   0.015839 => Txt Tokens per Sec:     8787 || Lr: 0.000100
2024-02-02 19:00:36,387 Epoch 833: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.79 
2024-02-02 19:00:36,388 EPOCH 834
2024-02-02 19:00:41,001 Epoch 834: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.72 
2024-02-02 19:00:41,002 EPOCH 835
2024-02-02 19:00:45,633 Epoch 835: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.94 
2024-02-02 19:00:45,634 EPOCH 836
2024-02-02 19:00:46,973 [Epoch: 836 Step: 00028400] Batch Recognition Loss:   0.000115 => Gls Tokens per Sec:     2393 || Batch Translation Loss:   0.016583 => Txt Tokens per Sec:     6368 || Lr: 0.000100
2024-02-02 19:00:50,503 Epoch 836: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.92 
2024-02-02 19:00:50,503 EPOCH 837
2024-02-02 19:00:55,305 Epoch 837: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-02 19:00:55,306 EPOCH 838
2024-02-02 19:00:59,631 Epoch 838: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-02 19:00:59,632 EPOCH 839
2024-02-02 19:01:00,562 [Epoch: 839 Step: 00028500] Batch Recognition Loss:   0.000223 => Gls Tokens per Sec:     2759 || Batch Translation Loss:   0.014824 => Txt Tokens per Sec:     7557 || Lr: 0.000100
2024-02-02 19:01:04,504 Epoch 839: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.98 
2024-02-02 19:01:04,504 EPOCH 840
2024-02-02 19:01:08,843 Epoch 840: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.56 
2024-02-02 19:01:08,843 EPOCH 841
2024-02-02 19:01:13,674 Epoch 841: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.46 
2024-02-02 19:01:13,675 EPOCH 842
2024-02-02 19:01:14,767 [Epoch: 842 Step: 00028600] Batch Recognition Loss:   0.000152 => Gls Tokens per Sec:     1761 || Batch Translation Loss:   0.023346 => Txt Tokens per Sec:     5388 || Lr: 0.000100
2024-02-02 19:01:17,935 Epoch 842: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.20 
2024-02-02 19:01:17,935 EPOCH 843
2024-02-02 19:01:22,825 Epoch 843: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.55 
2024-02-02 19:01:22,825 EPOCH 844
2024-02-02 19:01:27,093 Epoch 844: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.61 
2024-02-02 19:01:27,094 EPOCH 845
2024-02-02 19:01:27,616 [Epoch: 845 Step: 00028700] Batch Recognition Loss:   0.000676 => Gls Tokens per Sec:     2457 || Batch Translation Loss:   0.112870 => Txt Tokens per Sec:     6392 || Lr: 0.000100
2024-02-02 19:01:32,213 Epoch 845: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.19 
2024-02-02 19:01:32,214 EPOCH 846
2024-02-02 19:01:36,822 Epoch 846: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.98 
2024-02-02 19:01:36,822 EPOCH 847
2024-02-02 19:01:41,706 Epoch 847: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.71 
2024-02-02 19:01:41,707 EPOCH 848
2024-02-02 19:01:41,928 [Epoch: 848 Step: 00028800] Batch Recognition Loss:   0.000323 => Gls Tokens per Sec:     2909 || Batch Translation Loss:   0.116725 => Txt Tokens per Sec:     8082 || Lr: 0.000100
2024-02-02 19:01:45,973 Epoch 848: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.83 
2024-02-02 19:01:45,973 EPOCH 849
2024-02-02 19:01:50,956 Epoch 849: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.78 
2024-02-02 19:01:50,957 EPOCH 850
2024-02-02 19:01:55,142 [Epoch: 850 Step: 00028900] Batch Recognition Loss:   0.000304 => Gls Tokens per Sec:     2540 || Batch Translation Loss:   0.054911 => Txt Tokens per Sec:     7052 || Lr: 0.000100
2024-02-02 19:01:55,143 Epoch 850: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.61 
2024-02-02 19:01:55,143 EPOCH 851
2024-02-02 19:02:00,001 Epoch 851: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.87 
2024-02-02 19:02:00,002 EPOCH 852
2024-02-02 19:02:04,712 Epoch 852: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.07 
2024-02-02 19:02:04,712 EPOCH 853
2024-02-02 19:02:09,160 [Epoch: 853 Step: 00029000] Batch Recognition Loss:   0.000257 => Gls Tokens per Sec:     2247 || Batch Translation Loss:   0.026804 => Txt Tokens per Sec:     6262 || Lr: 0.000100
2024-02-02 19:02:09,368 Epoch 853: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.93 
2024-02-02 19:02:09,368 EPOCH 854
2024-02-02 19:02:13,990 Epoch 854: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.89 
2024-02-02 19:02:13,991 EPOCH 855
2024-02-02 19:02:18,562 Epoch 855: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.13 
2024-02-02 19:02:18,563 EPOCH 856
2024-02-02 19:02:22,196 [Epoch: 856 Step: 00029100] Batch Recognition Loss:   0.000215 => Gls Tokens per Sec:     2573 || Batch Translation Loss:   0.093337 => Txt Tokens per Sec:     7126 || Lr: 0.000100
2024-02-02 19:02:22,616 Epoch 856: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.46 
2024-02-02 19:02:22,616 EPOCH 857
2024-02-02 19:02:27,525 Epoch 857: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.77 
2024-02-02 19:02:27,526 EPOCH 858
2024-02-02 19:02:31,768 Epoch 858: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.71 
2024-02-02 19:02:31,768 EPOCH 859
2024-02-02 19:02:35,728 [Epoch: 859 Step: 00029200] Batch Recognition Loss:   0.000206 => Gls Tokens per Sec:     2201 || Batch Translation Loss:   0.041486 => Txt Tokens per Sec:     5953 || Lr: 0.000100
2024-02-02 19:02:36,691 Epoch 859: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.16 
2024-02-02 19:02:36,692 EPOCH 860
2024-02-02 19:02:41,525 Epoch 860: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.46 
2024-02-02 19:02:41,526 EPOCH 861
2024-02-02 19:02:45,952 Epoch 861: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.50 
2024-02-02 19:02:45,952 EPOCH 862
2024-02-02 19:02:49,855 [Epoch: 862 Step: 00029300] Batch Recognition Loss:   0.000143 => Gls Tokens per Sec:     2068 || Batch Translation Loss:   0.150684 => Txt Tokens per Sec:     5852 || Lr: 0.000100
2024-02-02 19:02:50,834 Epoch 862: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.62 
2024-02-02 19:02:50,835 EPOCH 863
2024-02-02 19:02:55,268 Epoch 863: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.94 
2024-02-02 19:02:55,268 EPOCH 864
2024-02-02 19:03:00,170 Epoch 864: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.30 
2024-02-02 19:03:00,170 EPOCH 865
2024-02-02 19:03:02,857 [Epoch: 865 Step: 00029400] Batch Recognition Loss:   0.000174 => Gls Tokens per Sec:     2766 || Batch Translation Loss:   0.027050 => Txt Tokens per Sec:     7395 || Lr: 0.000100
2024-02-02 19:03:04,516 Epoch 865: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.68 
2024-02-02 19:03:04,517 EPOCH 866
2024-02-02 19:03:09,362 Epoch 866: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.24 
2024-02-02 19:03:09,363 EPOCH 867
2024-02-02 19:03:13,596 Epoch 867: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.00 
2024-02-02 19:03:13,597 EPOCH 868
2024-02-02 19:03:16,714 [Epoch: 868 Step: 00029500] Batch Recognition Loss:   0.000252 => Gls Tokens per Sec:     2259 || Batch Translation Loss:   0.054760 => Txt Tokens per Sec:     6177 || Lr: 0.000100
2024-02-02 19:03:18,444 Epoch 868: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.06 
2024-02-02 19:03:18,444 EPOCH 869
2024-02-02 19:03:22,882 Epoch 869: Total Training Recognition Loss 0.01  Total Training Translation Loss 8.54 
2024-02-02 19:03:22,882 EPOCH 870
2024-02-02 19:03:27,920 Epoch 870: Total Training Recognition Loss 0.02  Total Training Translation Loss 9.73 
2024-02-02 19:03:27,921 EPOCH 871
2024-02-02 19:03:30,584 [Epoch: 871 Step: 00029600] Batch Recognition Loss:   0.000547 => Gls Tokens per Sec:     2404 || Batch Translation Loss:   0.098532 => Txt Tokens per Sec:     6787 || Lr: 0.000100
2024-02-02 19:03:32,541 Epoch 871: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.58 
2024-02-02 19:03:32,541 EPOCH 872
2024-02-02 19:03:37,203 Epoch 872: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.71 
2024-02-02 19:03:37,203 EPOCH 873
2024-02-02 19:03:41,712 Epoch 873: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.82 
2024-02-02 19:03:41,713 EPOCH 874
2024-02-02 19:03:44,228 [Epoch: 874 Step: 00029700] Batch Recognition Loss:   0.000260 => Gls Tokens per Sec:     2191 || Batch Translation Loss:   0.175917 => Txt Tokens per Sec:     5911 || Lr: 0.000100
2024-02-02 19:03:46,337 Epoch 874: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.74 
2024-02-02 19:03:46,337 EPOCH 875
2024-02-02 19:03:50,818 Epoch 875: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.62 
2024-02-02 19:03:50,818 EPOCH 876
2024-02-02 19:03:55,452 Epoch 876: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.19 
2024-02-02 19:03:55,452 EPOCH 877
2024-02-02 19:03:57,446 [Epoch: 877 Step: 00029800] Batch Recognition Loss:   0.000399 => Gls Tokens per Sec:     2444 || Batch Translation Loss:   0.029155 => Txt Tokens per Sec:     6774 || Lr: 0.000100
2024-02-02 19:04:00,026 Epoch 877: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.87 
2024-02-02 19:04:00,027 EPOCH 878
2024-02-02 19:04:04,611 Epoch 878: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.89 
2024-02-02 19:04:04,612 EPOCH 879
2024-02-02 19:04:09,259 Epoch 879: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.20 
2024-02-02 19:04:09,260 EPOCH 880
2024-02-02 19:04:10,753 [Epoch: 880 Step: 00029900] Batch Recognition Loss:   0.000202 => Gls Tokens per Sec:     3001 || Batch Translation Loss:   0.033173 => Txt Tokens per Sec:     7741 || Lr: 0.000100
2024-02-02 19:04:13,769 Epoch 880: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.94 
2024-02-02 19:04:13,769 EPOCH 881
2024-02-02 19:04:18,291 Epoch 881: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.77 
2024-02-02 19:04:18,292 EPOCH 882
2024-02-02 19:04:23,163 Epoch 882: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.70 
2024-02-02 19:04:23,163 EPOCH 883
2024-02-02 19:04:24,613 [Epoch: 883 Step: 00030000] Batch Recognition Loss:   0.000152 => Gls Tokens per Sec:     2651 || Batch Translation Loss:   0.014770 => Txt Tokens per Sec:     7071 || Lr: 0.000100
2024-02-02 19:04:33,114 Validation result at epoch 883, step    30000: duration: 8.4997s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00060	Translation Loss: 94254.43750	PPL: 12482.03809
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.81	(BLEU-1: 10.37,	BLEU-2: 3.47,	BLEU-3: 1.55,	BLEU-4: 0.81)
	CHRF 16.73	ROUGE 8.99
2024-02-02 19:04:33,115 Logging Recognition and Translation Outputs
2024-02-02 19:04:33,115 ========================================================================================================================
2024-02-02 19:04:33,115 Logging Sequence: 165_502.00
2024-02-02 19:04:33,115 	Gloss Reference :	A B+C+D+E
2024-02-02 19:04:33,115 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 19:04:33,116 	Gloss Alignment :	         
2024-02-02 19:04:33,116 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 19:04:33,117 	Text Reference  :	tendulkar would sit in the pavilion wearing both his   batting    pads even  after he got out   
2024-02-02 19:04:33,117 	Text Hypothesis :	********* ***** *** ** but many     people  use  their birthdates in   their email id and finals
2024-02-02 19:04:33,117 	Text Alignment  :	D         D     D   D  S   S        S       S    S     S          S    S     S     S  S   S     
2024-02-02 19:04:33,117 ========================================================================================================================
2024-02-02 19:04:33,117 Logging Sequence: 127_57.00
2024-02-02 19:04:33,118 	Gloss Reference :	A B+C+D+E
2024-02-02 19:04:33,118 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 19:04:33,118 	Gloss Alignment :	         
2024-02-02 19:04:33,118 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 19:04:33,120 	Text Reference  :	till date india had won only 2  medals    at the championships which like the    olympics is   the highest   level championship
2024-02-02 19:04:33,120 	Text Hypothesis :	**** **** ***** the an  odi  or scheduled in the ************* ***** same colony he       held in  australia new   zealand     
2024-02-02 19:04:33,120 	Text Alignment  :	D    D    D     S   S   S    S  S         S      D             D     S    S      S        S    S   S         S     S           
2024-02-02 19:04:33,120 ========================================================================================================================
2024-02-02 19:04:33,120 Logging Sequence: 169_10.00
2024-02-02 19:04:33,120 	Gloss Reference :	A B+C+D+E
2024-02-02 19:04:33,120 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 19:04:33,120 	Gloss Alignment :	         
2024-02-02 19:04:33,121 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 19:04:33,122 	Text Reference  :	the  18th     over was bowled by      ravi    bishnoi with   khushdil shah       and asif ali on the ******* crease    
2024-02-02 19:04:33,122 	Text Hypothesis :	when arshdeep lost the ball   leaving skipper rohit   sharma the      spectators and **** *** ** the viewers frustrated
2024-02-02 19:04:33,122 	Text Alignment  :	S    S        S    S   S      S       S       S       S      S        S              D    D   D      I       S         
2024-02-02 19:04:33,123 ========================================================================================================================
2024-02-02 19:04:33,123 Logging Sequence: 64_89.00
2024-02-02 19:04:33,123 	Gloss Reference :	A B+C+D+E
2024-02-02 19:04:33,123 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 19:04:33,123 	Gloss Alignment :	         
2024-02-02 19:04:33,123 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 19:04:33,124 	Text Reference  :	but this can  not go on amidst the rising cases human lives    need to   be safeguarded
2024-02-02 19:04:33,124 	Text Hypothesis :	*** ipl  will not ** ** ****** *** ****** ***** be    possible in   june as well       
2024-02-02 19:04:33,124 	Text Alignment  :	D   S    S        D  D  D      D   D      D     S     S        S    S    S  S          
2024-02-02 19:04:33,124 ========================================================================================================================
2024-02-02 19:04:33,125 Logging Sequence: 166_261.00
2024-02-02 19:04:33,125 	Gloss Reference :	A B+C+D+E
2024-02-02 19:04:33,125 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 19:04:33,125 	Gloss Alignment :	         
2024-02-02 19:04:33,125 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 19:04:33,126 	Text Reference  :	***** ** *** ****** ** for     all organizational matters   and the schedule
2024-02-02 19:04:33,126 	Text Hypothesis :	based on the number of matches we  are            spreading out of  them    
2024-02-02 19:04:33,126 	Text Alignment  :	I     I  I   I      I  S       S   S              S         S   S   S       
2024-02-02 19:04:33,126 ========================================================================================================================
2024-02-02 19:04:36,639 Epoch 883: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-02 19:04:36,640 EPOCH 884
2024-02-02 19:04:41,396 Epoch 884: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-02 19:04:41,396 EPOCH 885
2024-02-02 19:04:46,244 Epoch 885: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.07 
2024-02-02 19:04:46,244 EPOCH 886
2024-02-02 19:04:47,314 [Epoch: 886 Step: 00030100] Batch Recognition Loss:   0.000173 => Gls Tokens per Sec:     2762 || Batch Translation Loss:   0.015892 => Txt Tokens per Sec:     7244 || Lr: 0.000100
2024-02-02 19:04:50,441 Epoch 886: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.94 
2024-02-02 19:04:50,442 EPOCH 887
2024-02-02 19:04:55,363 Epoch 887: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.75 
2024-02-02 19:04:55,364 EPOCH 888
2024-02-02 19:04:59,590 Epoch 888: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.87 
2024-02-02 19:04:59,591 EPOCH 889
2024-02-02 19:05:01,128 [Epoch: 889 Step: 00030200] Batch Recognition Loss:   0.000114 => Gls Tokens per Sec:     1667 || Batch Translation Loss:   0.018879 => Txt Tokens per Sec:     4953 || Lr: 0.000100
2024-02-02 19:05:04,530 Epoch 889: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.04 
2024-02-02 19:05:04,531 EPOCH 890
2024-02-02 19:05:08,768 Epoch 890: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-02 19:05:08,769 EPOCH 891
2024-02-02 19:05:13,692 Epoch 891: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.75 
2024-02-02 19:05:13,692 EPOCH 892
2024-02-02 19:05:14,285 [Epoch: 892 Step: 00030300] Batch Recognition Loss:   0.000126 => Gls Tokens per Sec:     3241 || Batch Translation Loss:   0.016242 => Txt Tokens per Sec:     8560 || Lr: 0.000100
2024-02-02 19:05:17,931 Epoch 892: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.83 
2024-02-02 19:05:17,932 EPOCH 893
2024-02-02 19:05:23,057 Epoch 893: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.83 
2024-02-02 19:05:23,058 EPOCH 894
2024-02-02 19:05:27,673 Epoch 894: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-02 19:05:27,673 EPOCH 895
2024-02-02 19:05:28,264 [Epoch: 895 Step: 00030400] Batch Recognition Loss:   0.000199 => Gls Tokens per Sec:     1749 || Batch Translation Loss:   0.043696 => Txt Tokens per Sec:     4952 || Lr: 0.000100
2024-02-02 19:05:32,533 Epoch 895: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.93 
2024-02-02 19:05:32,533 EPOCH 896
2024-02-02 19:05:37,070 Epoch 896: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.12 
2024-02-02 19:05:37,070 EPOCH 897
2024-02-02 19:05:41,741 Epoch 897: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.48 
2024-02-02 19:05:41,741 EPOCH 898
2024-02-02 19:05:41,965 [Epoch: 898 Step: 00030500] Batch Recognition Loss:   0.000206 => Gls Tokens per Sec:     2870 || Batch Translation Loss:   0.027977 => Txt Tokens per Sec:     8574 || Lr: 0.000100
2024-02-02 19:05:46,428 Epoch 898: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.91 
2024-02-02 19:05:46,428 EPOCH 899
2024-02-02 19:05:50,969 Epoch 899: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.20 
2024-02-02 19:05:50,969 EPOCH 900
2024-02-02 19:05:55,657 [Epoch: 900 Step: 00030600] Batch Recognition Loss:   0.000432 => Gls Tokens per Sec:     2268 || Batch Translation Loss:   0.159497 => Txt Tokens per Sec:     6295 || Lr: 0.000100
2024-02-02 19:05:55,658 Epoch 900: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.44 
2024-02-02 19:05:55,658 EPOCH 901
2024-02-02 19:06:00,313 Epoch 901: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.55 
2024-02-02 19:06:00,313 EPOCH 902
2024-02-02 19:06:05,118 Epoch 902: Total Training Recognition Loss 0.02  Total Training Translation Loss 11.43 
2024-02-02 19:06:05,118 EPOCH 903
2024-02-02 19:06:09,238 [Epoch: 903 Step: 00030700] Batch Recognition Loss:   0.000420 => Gls Tokens per Sec:     2425 || Batch Translation Loss:   0.190574 => Txt Tokens per Sec:     6687 || Lr: 0.000100
2024-02-02 19:06:09,541 Epoch 903: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.12 
2024-02-02 19:06:09,541 EPOCH 904
2024-02-02 19:06:14,227 Epoch 904: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.93 
2024-02-02 19:06:14,227 EPOCH 905
2024-02-02 19:06:18,266 Epoch 905: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.43 
2024-02-02 19:06:18,266 EPOCH 906
2024-02-02 19:06:21,938 [Epoch: 906 Step: 00030800] Batch Recognition Loss:   0.000425 => Gls Tokens per Sec:     2614 || Batch Translation Loss:   0.062522 => Txt Tokens per Sec:     7402 || Lr: 0.000100
2024-02-02 19:06:22,322 Epoch 906: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.27 
2024-02-02 19:06:22,322 EPOCH 907
2024-02-02 19:06:27,258 Epoch 907: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.43 
2024-02-02 19:06:27,258 EPOCH 908
2024-02-02 19:06:31,645 Epoch 908: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.27 
2024-02-02 19:06:31,645 EPOCH 909
2024-02-02 19:06:35,603 [Epoch: 909 Step: 00030900] Batch Recognition Loss:   0.000367 => Gls Tokens per Sec:     2265 || Batch Translation Loss:   0.038209 => Txt Tokens per Sec:     6171 || Lr: 0.000100
2024-02-02 19:06:36,487 Epoch 909: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.96 
2024-02-02 19:06:36,487 EPOCH 910
2024-02-02 19:06:40,792 Epoch 910: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.88 
2024-02-02 19:06:40,793 EPOCH 911
2024-02-02 19:06:45,645 Epoch 911: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.85 
2024-02-02 19:06:45,645 EPOCH 912
2024-02-02 19:06:48,647 [Epoch: 912 Step: 00031000] Batch Recognition Loss:   0.000354 => Gls Tokens per Sec:     2689 || Batch Translation Loss:   0.020313 => Txt Tokens per Sec:     7409 || Lr: 0.000100
2024-02-02 19:06:49,761 Epoch 912: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.72 
2024-02-02 19:06:49,762 EPOCH 913
2024-02-02 19:06:54,388 Epoch 913: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-02 19:06:54,388 EPOCH 914
2024-02-02 19:06:59,057 Epoch 914: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-02 19:06:59,057 EPOCH 915
2024-02-02 19:07:02,714 [Epoch: 915 Step: 00031100] Batch Recognition Loss:   0.000201 => Gls Tokens per Sec:     2032 || Batch Translation Loss:   0.018051 => Txt Tokens per Sec:     5677 || Lr: 0.000100
2024-02-02 19:07:04,048 Epoch 915: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-02 19:07:04,048 EPOCH 916
2024-02-02 19:07:08,262 Epoch 916: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.89 
2024-02-02 19:07:08,263 EPOCH 917
2024-02-02 19:07:12,359 Epoch 917: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.77 
2024-02-02 19:07:12,359 EPOCH 918
2024-02-02 19:07:14,931 [Epoch: 918 Step: 00031200] Batch Recognition Loss:   0.000142 => Gls Tokens per Sec:     2641 || Batch Translation Loss:   0.023792 => Txt Tokens per Sec:     6898 || Lr: 0.000100
2024-02-02 19:07:17,117 Epoch 918: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.92 
2024-02-02 19:07:17,117 EPOCH 919
2024-02-02 19:07:21,481 Epoch 919: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.26 
2024-02-02 19:07:21,481 EPOCH 920
2024-02-02 19:07:26,285 Epoch 920: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-02 19:07:26,286 EPOCH 921
2024-02-02 19:07:28,744 [Epoch: 921 Step: 00031300] Batch Recognition Loss:   0.000121 => Gls Tokens per Sec:     2606 || Batch Translation Loss:   0.027279 => Txt Tokens per Sec:     7178 || Lr: 0.000100
2024-02-02 19:07:30,774 Epoch 921: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.41 
2024-02-02 19:07:30,774 EPOCH 922
2024-02-02 19:07:35,684 Epoch 922: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.16 
2024-02-02 19:07:35,685 EPOCH 923
2024-02-02 19:07:40,104 Epoch 923: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.52 
2024-02-02 19:07:40,105 EPOCH 924
2024-02-02 19:07:42,596 [Epoch: 924 Step: 00031400] Batch Recognition Loss:   0.000293 => Gls Tokens per Sec:     2313 || Batch Translation Loss:   0.054924 => Txt Tokens per Sec:     6386 || Lr: 0.000100
2024-02-02 19:07:44,875 Epoch 924: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.80 
2024-02-02 19:07:44,875 EPOCH 925
2024-02-02 19:07:49,273 Epoch 925: Total Training Recognition Loss 0.05  Total Training Translation Loss 6.86 
2024-02-02 19:07:49,273 EPOCH 926
2024-02-02 19:07:54,013 Epoch 926: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.53 
2024-02-02 19:07:54,013 EPOCH 927
2024-02-02 19:07:56,082 [Epoch: 927 Step: 00031500] Batch Recognition Loss:   0.000413 => Gls Tokens per Sec:     2356 || Batch Translation Loss:   0.077042 => Txt Tokens per Sec:     6565 || Lr: 0.000100
2024-02-02 19:07:58,159 Epoch 927: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.81 
2024-02-02 19:07:58,159 EPOCH 928
2024-02-02 19:08:02,834 Epoch 928: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.98 
2024-02-02 19:08:02,834 EPOCH 929
2024-02-02 19:08:07,484 Epoch 929: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.85 
2024-02-02 19:08:07,484 EPOCH 930
2024-02-02 19:08:09,622 [Epoch: 930 Step: 00031600] Batch Recognition Loss:   0.000236 => Gls Tokens per Sec:     1980 || Batch Translation Loss:   0.033707 => Txt Tokens per Sec:     5732 || Lr: 0.000100
2024-02-02 19:08:12,275 Epoch 930: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.76 
2024-02-02 19:08:12,275 EPOCH 931
2024-02-02 19:08:16,878 Epoch 931: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.18 
2024-02-02 19:08:16,879 EPOCH 932
2024-02-02 19:08:21,476 Epoch 932: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.11 
2024-02-02 19:08:21,477 EPOCH 933
2024-02-02 19:08:22,943 [Epoch: 933 Step: 00031700] Batch Recognition Loss:   0.000167 => Gls Tokens per Sec:     2450 || Batch Translation Loss:   0.022789 => Txt Tokens per Sec:     6852 || Lr: 0.000100
2024-02-02 19:08:26,155 Epoch 933: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.87 
2024-02-02 19:08:26,155 EPOCH 934
2024-02-02 19:08:30,580 Epoch 934: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.81 
2024-02-02 19:08:30,580 EPOCH 935
2024-02-02 19:08:35,276 Epoch 935: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.82 
2024-02-02 19:08:35,277 EPOCH 936
2024-02-02 19:08:36,538 [Epoch: 936 Step: 00031800] Batch Recognition Loss:   0.000192 => Gls Tokens per Sec:     2540 || Batch Translation Loss:   0.032787 => Txt Tokens per Sec:     7163 || Lr: 0.000100
2024-02-02 19:08:39,721 Epoch 936: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.78 
2024-02-02 19:08:39,721 EPOCH 937
2024-02-02 19:08:44,477 Epoch 937: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.73 
2024-02-02 19:08:44,478 EPOCH 938
2024-02-02 19:08:48,843 Epoch 938: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.08 
2024-02-02 19:08:48,844 EPOCH 939
2024-02-02 19:08:49,684 [Epoch: 939 Step: 00031900] Batch Recognition Loss:   0.000203 => Gls Tokens per Sec:     2750 || Batch Translation Loss:   0.022004 => Txt Tokens per Sec:     7337 || Lr: 0.000100
2024-02-02 19:08:53,663 Epoch 939: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.58 
2024-02-02 19:08:53,664 EPOCH 940
2024-02-02 19:08:58,018 Epoch 940: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.24 
2024-02-02 19:08:58,018 EPOCH 941
2024-02-02 19:09:02,868 Epoch 941: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.35 
2024-02-02 19:09:02,869 EPOCH 942
2024-02-02 19:09:03,800 [Epoch: 942 Step: 00032000] Batch Recognition Loss:   0.000200 => Gls Tokens per Sec:     2061 || Batch Translation Loss:   0.051339 => Txt Tokens per Sec:     6434 || Lr: 0.000100
2024-02-02 19:09:12,424 Validation result at epoch 942, step    32000: duration: 8.6234s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00065	Translation Loss: 94033.11719	PPL: 12208.63477
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.57	(BLEU-1: 10.58,	BLEU-2: 3.11,	BLEU-3: 1.23,	BLEU-4: 0.57)
	CHRF 16.85	ROUGE 8.94
2024-02-02 19:09:12,426 Logging Recognition and Translation Outputs
2024-02-02 19:09:12,426 ========================================================================================================================
2024-02-02 19:09:12,426 Logging Sequence: 86_11.00
2024-02-02 19:09:12,426 	Gloss Reference :	A B+C+D+E
2024-02-02 19:09:12,426 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 19:09:12,426 	Gloss Alignment :	         
2024-02-02 19:09:12,426 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 19:09:12,427 	Text Reference  :	he was 66 years     old   
2024-02-02 19:09:12,427 	Text Hypothesis :	he was a  brilliant player
2024-02-02 19:09:12,427 	Text Alignment  :	       S  S         S     
2024-02-02 19:09:12,427 ========================================================================================================================
2024-02-02 19:09:12,427 Logging Sequence: 67_16.00
2024-02-02 19:09:12,428 	Gloss Reference :	A B+C+D+E
2024-02-02 19:09:12,428 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 19:09:12,428 	Gloss Alignment :	         
2024-02-02 19:09:12,428 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 19:09:12,429 	Text Reference  :	***** *** **** to      help   india's fight against  the     covid-19 pandemic
2024-02-02 19:09:12,429 	Text Hypothesis :	there are many batsmen posted a       huge  argument between their    dhoni   
2024-02-02 19:09:12,429 	Text Alignment  :	I     I   I    S       S      S       S     S        S       S        S       
2024-02-02 19:09:12,429 ========================================================================================================================
2024-02-02 19:09:12,429 Logging Sequence: 69_177.00
2024-02-02 19:09:12,429 	Gloss Reference :	A B+C+D+E
2024-02-02 19:09:12,430 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 19:09:12,430 	Gloss Alignment :	         
2024-02-02 19:09:12,430 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 19:09:12,431 	Text Reference  :	he said 'i will continue playing i know it's about time i   retire i   also have a     knee    condition
2024-02-02 19:09:12,431 	Text Hypothesis :	** **** ** **** ******** ******* * **** when csk   came out to     bat ipl  the  first against england  
2024-02-02 19:09:12,431 	Text Alignment  :	D  D    D  D    D        D       D D    S    S     S    S   S      S   S    S    S     S       S        
2024-02-02 19:09:12,431 ========================================================================================================================
2024-02-02 19:09:12,432 Logging Sequence: 165_615.00
2024-02-02 19:09:12,432 	Gloss Reference :	A B+C+D+E
2024-02-02 19:09:12,432 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 19:09:12,432 	Gloss Alignment :	         
2024-02-02 19:09:12,432 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 19:09:12,432 	Text Reference  :	** ** ********* ***** *** we  defeated pakistan too   
2024-02-02 19:09:12,433 	Text Hypothesis :	it is currently known for the age      of       people
2024-02-02 19:09:12,433 	Text Alignment  :	I  I  I         I     I   S   S        S        S     
2024-02-02 19:09:12,433 ========================================================================================================================
2024-02-02 19:09:12,433 Logging Sequence: 61_5.00
2024-02-02 19:09:12,433 	Gloss Reference :	A B+C+D+E
2024-02-02 19:09:12,433 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 19:09:12,433 	Gloss Alignment :	         
2024-02-02 19:09:12,433 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 19:09:12,434 	Text Reference  :	they    rivalry is ** *** seen the **** most   during india pakistan cricket matches
2024-02-02 19:09:12,435 	Text Hypothesis :	however there   is no one day  the huge demand to     see   him      as      well   
2024-02-02 19:09:12,435 	Text Alignment  :	S       S          I  I   S        I    S      S      S     S        S       S      
2024-02-02 19:09:12,435 ========================================================================================================================
2024-02-02 19:09:16,209 Epoch 942: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.38 
2024-02-02 19:09:16,209 EPOCH 943
2024-02-02 19:09:21,050 Epoch 943: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.34 
2024-02-02 19:09:21,051 EPOCH 944
2024-02-02 19:09:25,364 Epoch 944: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.20 
2024-02-02 19:09:25,364 EPOCH 945
2024-02-02 19:09:25,807 [Epoch: 945 Step: 00032100] Batch Recognition Loss:   0.000297 => Gls Tokens per Sec:     2893 || Batch Translation Loss:   0.034466 => Txt Tokens per Sec:     8131 || Lr: 0.000100
2024-02-02 19:09:30,249 Epoch 945: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.42 
2024-02-02 19:09:30,249 EPOCH 946
2024-02-02 19:09:34,524 Epoch 946: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.21 
2024-02-02 19:09:34,524 EPOCH 947
2024-02-02 19:09:39,000 Epoch 947: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.98 
2024-02-02 19:09:39,001 EPOCH 948
2024-02-02 19:09:39,395 [Epoch: 948 Step: 00032200] Batch Recognition Loss:   0.000215 => Gls Tokens per Sec:      994 || Batch Translation Loss:   0.007364 => Txt Tokens per Sec:     3536 || Lr: 0.000100
2024-02-02 19:09:43,668 Epoch 948: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.20 
2024-02-02 19:09:43,668 EPOCH 949
2024-02-02 19:09:48,307 Epoch 949: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.15 
2024-02-02 19:09:48,307 EPOCH 950
2024-02-02 19:09:52,866 [Epoch: 950 Step: 00032300] Batch Recognition Loss:   0.000452 => Gls Tokens per Sec:     2333 || Batch Translation Loss:   0.112594 => Txt Tokens per Sec:     6476 || Lr: 0.000100
2024-02-02 19:09:52,866 Epoch 950: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.24 
2024-02-02 19:09:52,866 EPOCH 951
2024-02-02 19:09:57,343 Epoch 951: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.88 
2024-02-02 19:09:57,344 EPOCH 952
2024-02-02 19:10:02,014 Epoch 952: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.16 
2024-02-02 19:10:02,015 EPOCH 953
2024-02-02 19:10:06,401 [Epoch: 953 Step: 00032400] Batch Recognition Loss:   0.000632 => Gls Tokens per Sec:     2278 || Batch Translation Loss:   0.144703 => Txt Tokens per Sec:     6396 || Lr: 0.000100
2024-02-02 19:10:06,569 Epoch 953: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.42 
2024-02-02 19:10:06,570 EPOCH 954
2024-02-02 19:10:11,217 Epoch 954: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.76 
2024-02-02 19:10:11,217 EPOCH 955
2024-02-02 19:10:15,780 Epoch 955: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.36 
2024-02-02 19:10:15,780 EPOCH 956
2024-02-02 19:10:19,596 [Epoch: 956 Step: 00032500] Batch Recognition Loss:   0.000234 => Gls Tokens per Sec:     2451 || Batch Translation Loss:   0.049563 => Txt Tokens per Sec:     6708 || Lr: 0.000100
2024-02-02 19:10:20,393 Epoch 956: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.24 
2024-02-02 19:10:20,394 EPOCH 957
2024-02-02 19:10:25,033 Epoch 957: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.08 
2024-02-02 19:10:25,033 EPOCH 958
2024-02-02 19:10:29,479 Epoch 958: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.07 
2024-02-02 19:10:29,479 EPOCH 959
2024-02-02 19:10:33,359 [Epoch: 959 Step: 00032600] Batch Recognition Loss:   0.000179 => Gls Tokens per Sec:     2310 || Batch Translation Loss:   0.021562 => Txt Tokens per Sec:     6321 || Lr: 0.000100
2024-02-02 19:10:34,199 Epoch 959: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.88 
2024-02-02 19:10:34,199 EPOCH 960
2024-02-02 19:10:38,645 Epoch 960: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.80 
2024-02-02 19:10:38,645 EPOCH 961
2024-02-02 19:10:43,393 Epoch 961: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.94 
2024-02-02 19:10:43,393 EPOCH 962
2024-02-02 19:10:46,754 [Epoch: 962 Step: 00032700] Batch Recognition Loss:   0.000216 => Gls Tokens per Sec:     2477 || Batch Translation Loss:   0.015324 => Txt Tokens per Sec:     7029 || Lr: 0.000100
2024-02-02 19:10:47,765 Epoch 962: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.84 
2024-02-02 19:10:47,766 EPOCH 963
2024-02-02 19:10:52,481 Epoch 963: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.79 
2024-02-02 19:10:52,482 EPOCH 964
2024-02-02 19:10:56,930 Epoch 964: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.91 
2024-02-02 19:10:56,930 EPOCH 965
2024-02-02 19:11:00,161 [Epoch: 965 Step: 00032800] Batch Recognition Loss:   0.000209 => Gls Tokens per Sec:     2301 || Batch Translation Loss:   0.015949 => Txt Tokens per Sec:     6304 || Lr: 0.000100
2024-02-02 19:11:01,645 Epoch 965: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-02 19:11:01,645 EPOCH 966
2024-02-02 19:11:06,125 Epoch 966: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.77 
2024-02-02 19:11:06,125 EPOCH 967
2024-02-02 19:11:10,956 Epoch 967: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.05 
2024-02-02 19:11:10,956 EPOCH 968
2024-02-02 19:11:13,974 [Epoch: 968 Step: 00032900] Batch Recognition Loss:   0.000165 => Gls Tokens per Sec:     2250 || Batch Translation Loss:   0.015222 => Txt Tokens per Sec:     6391 || Lr: 0.000100
2024-02-02 19:11:15,235 Epoch 968: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.81 
2024-02-02 19:11:15,236 EPOCH 969
2024-02-02 19:11:20,213 Epoch 969: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.09 
2024-02-02 19:11:20,214 EPOCH 970
2024-02-02 19:11:24,476 Epoch 970: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.43 
2024-02-02 19:11:24,477 EPOCH 971
2024-02-02 19:11:27,355 [Epoch: 971 Step: 00033000] Batch Recognition Loss:   0.000232 => Gls Tokens per Sec:     2138 || Batch Translation Loss:   0.045884 => Txt Tokens per Sec:     6127 || Lr: 0.000100
2024-02-02 19:11:29,316 Epoch 971: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.09 
2024-02-02 19:11:29,316 EPOCH 972
2024-02-02 19:11:33,597 Epoch 972: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.99 
2024-02-02 19:11:33,598 EPOCH 973
2024-02-02 19:11:38,460 Epoch 973: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.55 
2024-02-02 19:11:38,460 EPOCH 974
2024-02-02 19:11:40,776 [Epoch: 974 Step: 00033100] Batch Recognition Loss:   0.000211 => Gls Tokens per Sec:     2380 || Batch Translation Loss:   0.118450 => Txt Tokens per Sec:     6586 || Lr: 0.000100
2024-02-02 19:11:42,759 Epoch 974: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.81 
2024-02-02 19:11:42,759 EPOCH 975
2024-02-02 19:11:47,665 Epoch 975: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.37 
2024-02-02 19:11:47,666 EPOCH 976
2024-02-02 19:11:51,938 Epoch 976: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.00 
2024-02-02 19:11:51,939 EPOCH 977
2024-02-02 19:11:53,894 [Epoch: 977 Step: 00033200] Batch Recognition Loss:   0.000271 => Gls Tokens per Sec:     2621 || Batch Translation Loss:   0.038014 => Txt Tokens per Sec:     6918 || Lr: 0.000100
2024-02-02 19:11:56,763 Epoch 977: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.44 
2024-02-02 19:11:56,764 EPOCH 978
2024-02-02 19:12:01,088 Epoch 978: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.21 
2024-02-02 19:12:01,089 EPOCH 979
2024-02-02 19:12:05,956 Epoch 979: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.13 
2024-02-02 19:12:05,956 EPOCH 980
2024-02-02 19:12:07,559 [Epoch: 980 Step: 00033300] Batch Recognition Loss:   0.000331 => Gls Tokens per Sec:     2643 || Batch Translation Loss:   0.089218 => Txt Tokens per Sec:     7324 || Lr: 0.000100
2024-02-02 19:12:10,330 Epoch 980: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.07 
2024-02-02 19:12:10,330 EPOCH 981
2024-02-02 19:12:15,130 Epoch 981: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.94 
2024-02-02 19:12:15,130 EPOCH 982
2024-02-02 19:12:19,480 Epoch 982: Total Training Recognition Loss 0.01  Total Training Translation Loss 6.84 
2024-02-02 19:12:19,481 EPOCH 983
2024-02-02 19:12:21,225 [Epoch: 983 Step: 00033400] Batch Recognition Loss:   0.000640 => Gls Tokens per Sec:     2060 || Batch Translation Loss:   0.116355 => Txt Tokens per Sec:     5494 || Lr: 0.000100
2024-02-02 19:12:24,265 Epoch 983: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.64 
2024-02-02 19:12:24,266 EPOCH 984
2024-02-02 19:12:28,868 Epoch 984: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.56 
2024-02-02 19:12:28,868 EPOCH 985
2024-02-02 19:12:33,414 Epoch 985: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.13 
2024-02-02 19:12:33,415 EPOCH 986
2024-02-02 19:12:34,626 [Epoch: 986 Step: 00033500] Batch Recognition Loss:   0.000489 => Gls Tokens per Sec:     2644 || Batch Translation Loss:   0.056707 => Txt Tokens per Sec:     7283 || Lr: 0.000100
2024-02-02 19:12:38,122 Epoch 986: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.52 
2024-02-02 19:12:38,122 EPOCH 987
2024-02-02 19:12:42,555 Epoch 987: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.98 
2024-02-02 19:12:42,556 EPOCH 988
2024-02-02 19:12:47,376 Epoch 988: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.89 
2024-02-02 19:12:47,377 EPOCH 989
2024-02-02 19:12:48,544 [Epoch: 989 Step: 00033600] Batch Recognition Loss:   0.000194 => Gls Tokens per Sec:     2196 || Batch Translation Loss:   0.018092 => Txt Tokens per Sec:     6591 || Lr: 0.000100
2024-02-02 19:12:51,708 Epoch 989: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-02 19:12:51,708 EPOCH 990
2024-02-02 19:12:56,557 Epoch 990: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-02 19:12:56,558 EPOCH 991
2024-02-02 19:13:00,818 Epoch 991: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-02 19:13:00,819 EPOCH 992
2024-02-02 19:13:01,423 [Epoch: 992 Step: 00033700] Batch Recognition Loss:   0.000171 => Gls Tokens per Sec:     3184 || Batch Translation Loss:   0.015112 => Txt Tokens per Sec:     8756 || Lr: 0.000100
2024-02-02 19:13:05,024 Epoch 992: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-02 19:13:05,025 EPOCH 993
2024-02-02 19:13:09,932 Epoch 993: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-02 19:13:09,933 EPOCH 994
2024-02-02 19:13:14,085 Epoch 994: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-02 19:13:14,086 EPOCH 995
2024-02-02 19:13:14,466 [Epoch: 995 Step: 00033800] Batch Recognition Loss:   0.000125 => Gls Tokens per Sec:     3377 || Batch Translation Loss:   0.010706 => Txt Tokens per Sec:     8090 || Lr: 0.000100
2024-02-02 19:13:18,226 Epoch 995: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-02 19:13:18,227 EPOCH 996
2024-02-02 19:13:23,012 Epoch 996: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-02 19:13:23,013 EPOCH 997
2024-02-02 19:13:27,723 Epoch 997: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.73 
2024-02-02 19:13:27,723 EPOCH 998
2024-02-02 19:13:27,955 [Epoch: 998 Step: 00033900] Batch Recognition Loss:   0.000114 => Gls Tokens per Sec:     2771 || Batch Translation Loss:   0.393584 => Txt Tokens per Sec:     6610 || Lr: 0.000100
2024-02-02 19:13:32,385 Epoch 998: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.13 
2024-02-02 19:13:32,386 EPOCH 999
2024-02-02 19:13:37,249 Epoch 999: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.31 
2024-02-02 19:13:37,249 EPOCH 1000
2024-02-02 19:13:41,507 [Epoch: 1000 Step: 00034000] Batch Recognition Loss:   0.000416 => Gls Tokens per Sec:     2497 || Batch Translation Loss:   0.011191 => Txt Tokens per Sec:     6933 || Lr: 0.000100
2024-02-02 19:13:49,906 Validation result at epoch 1000, step    34000: duration: 8.3981s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00076	Translation Loss: 94438.60938	PPL: 12714.22461
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.83	(BLEU-1: 10.57,	BLEU-2: 3.24,	BLEU-3: 1.48,	BLEU-4: 0.83)
	CHRF 16.68	ROUGE 9.17
2024-02-02 19:13:49,907 Logging Recognition and Translation Outputs
2024-02-02 19:13:49,907 ========================================================================================================================
2024-02-02 19:13:49,907 Logging Sequence: 92_199.00
2024-02-02 19:13:49,907 	Gloss Reference :	A B+C+D+E
2024-02-02 19:13:49,907 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 19:13:49,907 	Gloss Alignment :	         
2024-02-02 19:13:49,907 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 19:13:49,908 	Text Reference  :	*** ******** *** people on     social media said that 
2024-02-02 19:13:49,908 	Text Hypothesis :	and shocking are the    second team   won   the  match
2024-02-02 19:13:49,908 	Text Alignment  :	I   I        I   S      S      S      S     S    S    
2024-02-02 19:13:49,908 ========================================================================================================================
2024-02-02 19:13:49,908 Logging Sequence: 109_64.00
2024-02-02 19:13:49,909 	Gloss Reference :	A B+C+D+E
2024-02-02 19:13:49,909 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 19:13:49,909 	Gloss Alignment :	         
2024-02-02 19:13:49,909 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 19:13:49,910 	Text Reference  :	** the **** 2 players as well as    the ***** entire kkr team have        been    quarantined
2024-02-02 19:13:49,910 	Text Hypothesis :	in the next 9 months  of the  match the super kings  ceo kasi viswanathan bowling coach      
2024-02-02 19:13:49,910 	Text Alignment  :	I      I    S S       S  S    S         I     S      S   S    S           S       S          
2024-02-02 19:13:49,910 ========================================================================================================================
2024-02-02 19:13:49,911 Logging Sequence: 84_108.00
2024-02-02 19:13:49,911 	Gloss Reference :	A B+C+D+E
2024-02-02 19:13:49,911 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 19:13:49,911 	Gloss Alignment :	         
2024-02-02 19:13:49,911 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 19:13:49,912 	Text Reference  :	so in order to show their protest they covered their mouth in the photos which then went viral
2024-02-02 19:13:49,912 	Text Hypothesis :	** ** ***** ** **** ***** ******* **** ******* ***** ***** ** the ****** ***** news went viral
2024-02-02 19:13:49,912 	Text Alignment  :	D  D  D     D  D    D     D       D    D       D     D     D      D      D     S              
2024-02-02 19:13:49,912 ========================================================================================================================
2024-02-02 19:13:49,912 Logging Sequence: 115_24.00
2024-02-02 19:13:49,912 	Gloss Reference :	A B+C+D+E
2024-02-02 19:13:49,913 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 19:13:49,913 	Gloss Alignment :	         
2024-02-02 19:13:49,913 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 19:13:49,914 	Text Reference  :	** bumrah  also did not   participate in   the 5      match t20 series  
2024-02-02 19:13:49,914 	Text Hypothesis :	as neither of   the teams could       when he  always at    the ceremony
2024-02-02 19:13:49,914 	Text Alignment  :	I  S       S    S   S     S           S    S   S      S     S   S       
2024-02-02 19:13:49,914 ========================================================================================================================
2024-02-02 19:13:49,914 Logging Sequence: 96_129.00
2024-02-02 19:13:49,914 	Gloss Reference :	A B+C+D+E
2024-02-02 19:13:49,914 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 19:13:49,914 	Gloss Alignment :	         
2024-02-02 19:13:49,915 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 19:13:49,915 	Text Reference  :	******** ***** viewers were    very  stressed
2024-02-02 19:13:49,915 	Text Hypothesis :	everyone hoped that    england would win     
2024-02-02 19:13:49,916 	Text Alignment  :	I        I     S       S       S     S       
2024-02-02 19:13:49,916 ========================================================================================================================
2024-02-02 19:13:49,919 Epoch 1000: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.80 
2024-02-02 19:13:49,919 EPOCH 1001
2024-02-02 19:13:54,957 Epoch 1001: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.29 
2024-02-02 19:13:54,958 EPOCH 1002
2024-02-02 19:13:59,283 Epoch 1002: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.70 
2024-02-02 19:13:59,283 EPOCH 1003
2024-02-02 19:14:03,812 [Epoch: 1003 Step: 00034100] Batch Recognition Loss:   0.000319 => Gls Tokens per Sec:     2206 || Batch Translation Loss:   0.030676 => Txt Tokens per Sec:     6074 || Lr: 0.000100
2024-02-02 19:14:04,140 Epoch 1003: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.34 
2024-02-02 19:14:04,141 EPOCH 1004
2024-02-02 19:14:08,466 Epoch 1004: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.15 
2024-02-02 19:14:08,467 EPOCH 1005
2024-02-02 19:14:13,300 Epoch 1005: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.48 
2024-02-02 19:14:13,301 EPOCH 1006
2024-02-02 19:14:16,964 [Epoch: 1006 Step: 00034200] Batch Recognition Loss:   0.000309 => Gls Tokens per Sec:     2554 || Batch Translation Loss:   0.193701 => Txt Tokens per Sec:     7162 || Lr: 0.000100
2024-02-02 19:14:17,608 Epoch 1006: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.26 
2024-02-02 19:14:17,608 EPOCH 1007
2024-02-02 19:14:22,541 Epoch 1007: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.51 
2024-02-02 19:14:22,542 EPOCH 1008
2024-02-02 19:14:27,422 Epoch 1008: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.25 
2024-02-02 19:14:27,422 EPOCH 1009
2024-02-02 19:14:31,131 [Epoch: 1009 Step: 00034300] Batch Recognition Loss:   0.000280 => Gls Tokens per Sec:     2417 || Batch Translation Loss:   0.126895 => Txt Tokens per Sec:     6959 || Lr: 0.000100
2024-02-02 19:14:31,672 Epoch 1009: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.44 
2024-02-02 19:14:31,672 EPOCH 1010
2024-02-02 19:14:36,569 Epoch 1010: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.42 
2024-02-02 19:14:36,570 EPOCH 1011
2024-02-02 19:14:41,259 Epoch 1011: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.36 
2024-02-02 19:14:41,259 EPOCH 1012
2024-02-02 19:14:44,697 [Epoch: 1012 Step: 00034400] Batch Recognition Loss:   0.000323 => Gls Tokens per Sec:     2348 || Batch Translation Loss:   0.056600 => Txt Tokens per Sec:     6397 || Lr: 0.000100
2024-02-02 19:14:45,892 Epoch 1012: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.26 
2024-02-02 19:14:45,892 EPOCH 1013
2024-02-02 19:14:50,465 Epoch 1013: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.49 
2024-02-02 19:14:50,466 EPOCH 1014
2024-02-02 19:14:55,110 Epoch 1014: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.78 
2024-02-02 19:14:55,110 EPOCH 1015
2024-02-02 19:14:58,309 [Epoch: 1015 Step: 00034500] Batch Recognition Loss:   0.000615 => Gls Tokens per Sec:     2401 || Batch Translation Loss:   0.257035 => Txt Tokens per Sec:     6703 || Lr: 0.000100
2024-02-02 19:14:59,699 Epoch 1015: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.35 
2024-02-02 19:14:59,699 EPOCH 1016
2024-02-02 19:15:04,268 Epoch 1016: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.57 
2024-02-02 19:15:04,268 EPOCH 1017
2024-02-02 19:15:08,350 Epoch 1017: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.27 
2024-02-02 19:15:08,350 EPOCH 1018
2024-02-02 19:15:11,439 [Epoch: 1018 Step: 00034600] Batch Recognition Loss:   0.000364 => Gls Tokens per Sec:     2280 || Batch Translation Loss:   0.025360 => Txt Tokens per Sec:     6247 || Lr: 0.000100
2024-02-02 19:15:13,145 Epoch 1018: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.98 
2024-02-02 19:15:13,145 EPOCH 1019
2024-02-02 19:15:17,775 Epoch 1019: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.79 
2024-02-02 19:15:17,776 EPOCH 1020
2024-02-02 19:15:22,398 Epoch 1020: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-02 19:15:22,398 EPOCH 1021
2024-02-02 19:15:25,232 [Epoch: 1021 Step: 00034700] Batch Recognition Loss:   0.000171 => Gls Tokens per Sec:     2172 || Batch Translation Loss:   0.031250 => Txt Tokens per Sec:     6014 || Lr: 0.000100
2024-02-02 19:15:27,161 Epoch 1021: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-02 19:15:27,161 EPOCH 1022
2024-02-02 19:15:31,545 Epoch 1022: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-02 19:15:31,545 EPOCH 1023
2024-02-02 19:15:36,352 Epoch 1023: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-02 19:15:36,353 EPOCH 1024
2024-02-02 19:15:38,849 [Epoch: 1024 Step: 00034800] Batch Recognition Loss:   0.000157 => Gls Tokens per Sec:     2208 || Batch Translation Loss:   0.017335 => Txt Tokens per Sec:     6548 || Lr: 0.000100
2024-02-02 19:15:40,694 Epoch 1024: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-02 19:15:40,694 EPOCH 1025
2024-02-02 19:15:45,474 Epoch 1025: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-02 19:15:45,475 EPOCH 1026
2024-02-02 19:15:49,843 Epoch 1026: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-02 19:15:49,843 EPOCH 1027
2024-02-02 19:15:51,902 [Epoch: 1027 Step: 00034900] Batch Recognition Loss:   0.000159 => Gls Tokens per Sec:     2366 || Batch Translation Loss:   0.014527 => Txt Tokens per Sec:     6294 || Lr: 0.000100
2024-02-02 19:15:54,843 Epoch 1027: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-02 19:15:54,844 EPOCH 1028
2024-02-02 19:15:59,590 Epoch 1028: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.88 
2024-02-02 19:15:59,590 EPOCH 1029
2024-02-02 19:16:04,498 Epoch 1029: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.51 
2024-02-02 19:16:04,498 EPOCH 1030
2024-02-02 19:16:05,928 [Epoch: 1030 Step: 00035000] Batch Recognition Loss:   0.000205 => Gls Tokens per Sec:     2960 || Batch Translation Loss:   0.012478 => Txt Tokens per Sec:     7733 || Lr: 0.000100
2024-02-02 19:16:08,869 Epoch 1030: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.65 
2024-02-02 19:16:08,869 EPOCH 1031
2024-02-02 19:16:13,739 Epoch 1031: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.34 
2024-02-02 19:16:13,740 EPOCH 1032
2024-02-02 19:16:18,294 Epoch 1032: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.05 
2024-02-02 19:16:18,295 EPOCH 1033
2024-02-02 19:16:19,954 [Epoch: 1033 Step: 00035100] Batch Recognition Loss:   0.000212 => Gls Tokens per Sec:     2167 || Batch Translation Loss:   0.025281 => Txt Tokens per Sec:     6117 || Lr: 0.000100
2024-02-02 19:16:22,971 Epoch 1033: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.11 
2024-02-02 19:16:22,972 EPOCH 1034
2024-02-02 19:16:27,488 Epoch 1034: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.69 
2024-02-02 19:16:27,489 EPOCH 1035
2024-02-02 19:16:32,134 Epoch 1035: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.40 
2024-02-02 19:16:32,134 EPOCH 1036
2024-02-02 19:16:33,270 [Epoch: 1036 Step: 00035200] Batch Recognition Loss:   0.000214 => Gls Tokens per Sec:     2820 || Batch Translation Loss:   0.110828 => Txt Tokens per Sec:     7319 || Lr: 0.000100
2024-02-02 19:16:36,634 Epoch 1036: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.39 
2024-02-02 19:16:36,634 EPOCH 1037
2024-02-02 19:16:41,291 Epoch 1037: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.20 
2024-02-02 19:16:41,291 EPOCH 1038
2024-02-02 19:16:45,825 Epoch 1038: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.26 
2024-02-02 19:16:45,826 EPOCH 1039
2024-02-02 19:16:46,850 [Epoch: 1039 Step: 00035300] Batch Recognition Loss:   0.000191 => Gls Tokens per Sec:     2258 || Batch Translation Loss:   0.014781 => Txt Tokens per Sec:     6157 || Lr: 0.000100
2024-02-02 19:16:50,449 Epoch 1039: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.84 
2024-02-02 19:16:50,449 EPOCH 1040
2024-02-02 19:16:54,959 Epoch 1040: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.13 
2024-02-02 19:16:54,960 EPOCH 1041
2024-02-02 19:16:59,570 Epoch 1041: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.51 
2024-02-02 19:16:59,570 EPOCH 1042
2024-02-02 19:17:00,208 [Epoch: 1042 Step: 00035400] Batch Recognition Loss:   0.000185 => Gls Tokens per Sec:     3012 || Batch Translation Loss:   0.101372 => Txt Tokens per Sec:     8096 || Lr: 0.000100
2024-02-02 19:17:04,227 Epoch 1042: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.45 
2024-02-02 19:17:04,228 EPOCH 1043
2024-02-02 19:17:08,835 Epoch 1043: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.80 
2024-02-02 19:17:08,835 EPOCH 1044
2024-02-02 19:17:13,421 Epoch 1044: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.76 
2024-02-02 19:17:13,421 EPOCH 1045
2024-02-02 19:17:13,809 [Epoch: 1045 Step: 00035500] Batch Recognition Loss:   0.000246 => Gls Tokens per Sec:     3308 || Batch Translation Loss:   0.066583 => Txt Tokens per Sec:     8083 || Lr: 0.000100
2024-02-02 19:17:18,150 Epoch 1045: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.47 
2024-02-02 19:17:18,150 EPOCH 1046
2024-02-02 19:17:22,995 Epoch 1046: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.06 
2024-02-02 19:17:22,996 EPOCH 1047
2024-02-02 19:17:27,285 Epoch 1047: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.46 
2024-02-02 19:17:27,285 EPOCH 1048
2024-02-02 19:17:27,434 [Epoch: 1048 Step: 00035600] Batch Recognition Loss:   0.000451 => Gls Tokens per Sec:     4384 || Batch Translation Loss:   0.288279 => Txt Tokens per Sec:    10199 || Lr: 0.000100
2024-02-02 19:17:31,979 Epoch 1048: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.59 
2024-02-02 19:17:31,979 EPOCH 1049
2024-02-02 19:17:36,402 Epoch 1049: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.92 
2024-02-02 19:17:36,402 EPOCH 1050
2024-02-02 19:17:41,126 [Epoch: 1050 Step: 00035700] Batch Recognition Loss:   0.000234 => Gls Tokens per Sec:     2251 || Batch Translation Loss:   0.044146 => Txt Tokens per Sec:     6248 || Lr: 0.000100
2024-02-02 19:17:41,127 Epoch 1050: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.54 
2024-02-02 19:17:41,127 EPOCH 1051
2024-02-02 19:17:45,601 Epoch 1051: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.90 
2024-02-02 19:17:45,601 EPOCH 1052
2024-02-02 19:17:50,397 Epoch 1052: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.26 
2024-02-02 19:17:50,398 EPOCH 1053
2024-02-02 19:17:54,327 [Epoch: 1053 Step: 00035800] Batch Recognition Loss:   0.000188 => Gls Tokens per Sec:     2544 || Batch Translation Loss:   0.019336 => Txt Tokens per Sec:     7058 || Lr: 0.000100
2024-02-02 19:17:54,780 Epoch 1053: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.26 
2024-02-02 19:17:54,780 EPOCH 1054
2024-02-02 19:17:59,592 Epoch 1054: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-02 19:17:59,592 EPOCH 1055
2024-02-02 19:18:03,982 Epoch 1055: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.23 
2024-02-02 19:18:03,982 EPOCH 1056
2024-02-02 19:18:07,666 [Epoch: 1056 Step: 00035900] Batch Recognition Loss:   0.000214 => Gls Tokens per Sec:     2539 || Batch Translation Loss:   0.025433 => Txt Tokens per Sec:     7088 || Lr: 0.000100
2024-02-02 19:18:08,153 Epoch 1056: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.48 
2024-02-02 19:18:08,154 EPOCH 1057
2024-02-02 19:18:13,011 Epoch 1057: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.12 
2024-02-02 19:18:13,012 EPOCH 1058
2024-02-02 19:18:17,788 Epoch 1058: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.12 
2024-02-02 19:18:17,788 EPOCH 1059
2024-02-02 19:18:21,727 [Epoch: 1059 Step: 00036000] Batch Recognition Loss:   0.000210 => Gls Tokens per Sec:     2212 || Batch Translation Loss:   0.023091 => Txt Tokens per Sec:     6341 || Lr: 0.000100
2024-02-02 19:18:30,210 Validation result at epoch 1059, step    36000: duration: 8.4833s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00088	Translation Loss: 94737.71094	PPL: 13100.52734
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.84	(BLEU-1: 11.58,	BLEU-2: 3.71,	BLEU-3: 1.59,	BLEU-4: 0.84)
	CHRF 17.25	ROUGE 9.71
2024-02-02 19:18:30,211 Logging Recognition and Translation Outputs
2024-02-02 19:18:30,211 ========================================================================================================================
2024-02-02 19:18:30,211 Logging Sequence: 78_198.00
2024-02-02 19:18:30,212 	Gloss Reference :	A B+C+D+E
2024-02-02 19:18:30,212 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 19:18:30,212 	Gloss Alignment :	         
2024-02-02 19:18:30,212 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 19:18:30,213 	Text Reference  :	******* **** ** **** *** ****** ** ****** *** they    have been         flooded with congratulations comments
2024-02-02 19:18:30,213 	Text Hypothesis :	england were to host the finals of cities for players who  disassociate himself from the             builder 
2024-02-02 19:18:30,213 	Text Alignment  :	I       I    I  I    I   I      I  I      I   S       S    S            S       S    S               S       
2024-02-02 19:18:30,214 ========================================================================================================================
2024-02-02 19:18:30,214 Logging Sequence: 145_216.00
2024-02-02 19:18:30,214 	Gloss Reference :	A B+C+D+E
2024-02-02 19:18:30,214 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 19:18:30,215 	Gloss Alignment :	         
2024-02-02 19:18:30,215 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 19:18:30,216 	Text Reference  :	asking him to include sameeha in *** the    world championship as  she was a talented athlete    
2024-02-02 19:18:30,216 	Text Hypothesis :	****** *** ** ******* ******* in his series went  viral        and t20 was a severe   competition
2024-02-02 19:18:30,216 	Text Alignment  :	D      D   D  D       D          I   S      S     S            S   S         S        S          
2024-02-02 19:18:30,216 ========================================================================================================================
2024-02-02 19:18:30,217 Logging Sequence: 70_137.00
2024-02-02 19:18:30,217 	Gloss Reference :	A B+C+D+E
2024-02-02 19:18:30,217 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 19:18:30,217 	Gloss Alignment :	         
2024-02-02 19:18:30,217 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 19:18:30,218 	Text Reference  :	the     small gesture appeared to    encourage people to    drink water  instead of   aerated drinks 
2024-02-02 19:18:30,219 	Text Hypothesis :	however even  though  the      tokyo olympics  are    angry at    jantar mantar  with thier   demands
2024-02-02 19:18:30,219 	Text Alignment  :	S       S     S       S        S     S         S      S     S     S      S       S    S       S      
2024-02-02 19:18:30,219 ========================================================================================================================
2024-02-02 19:18:30,219 Logging Sequence: 119_20.00
2024-02-02 19:18:30,219 	Gloss Reference :	A B+C+D+E
2024-02-02 19:18:30,219 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 19:18:30,219 	Gloss Alignment :	         
2024-02-02 19:18:30,220 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 19:18:30,221 	Text Reference  :	messi intended to gift something to       all      the   players and the    staff to      special to celebrate the moment
2024-02-02 19:18:30,221 	Text Hypothesis :	***** ******** ** **** the       iphones' combined worth is      eur 175000 which roughly amounts to rs        173 crore 
2024-02-02 19:18:30,221 	Text Alignment  :	D     D        D  D    S         S        S        S     S       S   S      S     S       S          S         S   S     
2024-02-02 19:18:30,222 ========================================================================================================================
2024-02-02 19:18:30,222 Logging Sequence: 106_15.00
2024-02-02 19:18:30,222 	Gloss Reference :	A B+C+D+E
2024-02-02 19:18:30,222 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 19:18:30,222 	Gloss Alignment :	         
2024-02-02 19:18:30,222 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 19:18:30,224 	Text Reference  :	but what about women's cricket   earlier we   never spoke about it **** **** ** ******
2024-02-02 19:18:30,224 	Text Hypothesis :	*** and  then  the     wrestlers were    left the   field and   it will help of defeat
2024-02-02 19:18:30,224 	Text Alignment  :	D   S    S     S       S         S       S    S     S     S        I    I    I  I     
2024-02-02 19:18:30,224 ========================================================================================================================
2024-02-02 19:18:30,930 Epoch 1059: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.22 
2024-02-02 19:18:30,931 EPOCH 1060
2024-02-02 19:18:35,712 Epoch 1060: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.82 
2024-02-02 19:18:35,712 EPOCH 1061
2024-02-02 19:18:40,036 Epoch 1061: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-02 19:18:40,036 EPOCH 1062
2024-02-02 19:18:43,650 [Epoch: 1062 Step: 00036100] Batch Recognition Loss:   0.000182 => Gls Tokens per Sec:     2303 || Batch Translation Loss:   0.013879 => Txt Tokens per Sec:     6467 || Lr: 0.000050
2024-02-02 19:18:44,834 Epoch 1062: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-02 19:18:44,835 EPOCH 1063
2024-02-02 19:18:49,155 Epoch 1063: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-02 19:18:49,156 EPOCH 1064
2024-02-02 19:18:53,974 Epoch 1064: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-02 19:18:53,974 EPOCH 1065
2024-02-02 19:18:56,639 [Epoch: 1065 Step: 00036200] Batch Recognition Loss:   0.000162 => Gls Tokens per Sec:     2883 || Batch Translation Loss:   0.010008 => Txt Tokens per Sec:     7987 || Lr: 0.000050
2024-02-02 19:18:58,327 Epoch 1065: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-02 19:18:58,327 EPOCH 1066
2024-02-02 19:19:03,051 Epoch 1066: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-02 19:19:03,051 EPOCH 1067
2024-02-02 19:19:07,124 Epoch 1067: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-02 19:19:07,124 EPOCH 1068
2024-02-02 19:19:09,993 [Epoch: 1068 Step: 00036300] Batch Recognition Loss:   0.000163 => Gls Tokens per Sec:     2367 || Batch Translation Loss:   0.008750 => Txt Tokens per Sec:     6558 || Lr: 0.000050
2024-02-02 19:19:11,757 Epoch 1068: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-02 19:19:11,758 EPOCH 1069
2024-02-02 19:19:16,496 Epoch 1069: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-02 19:19:16,496 EPOCH 1070
2024-02-02 19:19:21,216 Epoch 1070: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-02 19:19:21,216 EPOCH 1071
2024-02-02 19:19:23,288 [Epoch: 1071 Step: 00036400] Batch Recognition Loss:   0.000215 => Gls Tokens per Sec:     2970 || Batch Translation Loss:   0.013037 => Txt Tokens per Sec:     7662 || Lr: 0.000050
2024-02-02 19:19:25,909 Epoch 1071: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-02 19:19:25,910 EPOCH 1072
2024-02-02 19:19:30,430 Epoch 1072: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-02 19:19:30,431 EPOCH 1073
2024-02-02 19:19:34,642 Epoch 1073: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-02 19:19:34,642 EPOCH 1074
2024-02-02 19:19:37,228 [Epoch: 1074 Step: 00036500] Batch Recognition Loss:   0.000130 => Gls Tokens per Sec:     2132 || Batch Translation Loss:   0.009360 => Txt Tokens per Sec:     5732 || Lr: 0.000050
2024-02-02 19:19:39,495 Epoch 1074: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-02 19:19:39,496 EPOCH 1075
2024-02-02 19:19:43,701 Epoch 1075: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-02 19:19:43,702 EPOCH 1076
2024-02-02 19:19:48,555 Epoch 1076: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-02 19:19:48,556 EPOCH 1077
2024-02-02 19:19:50,984 [Epoch: 1077 Step: 00036600] Batch Recognition Loss:   0.000216 => Gls Tokens per Sec:     2109 || Batch Translation Loss:   0.016637 => Txt Tokens per Sec:     6015 || Lr: 0.000050
2024-02-02 19:19:53,297 Epoch 1077: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-02 19:19:53,297 EPOCH 1078
2024-02-02 19:19:57,676 Epoch 1078: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-02 19:19:57,676 EPOCH 1079
2024-02-02 19:20:02,599 Epoch 1079: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-02 19:20:02,600 EPOCH 1080
2024-02-02 19:20:04,057 [Epoch: 1080 Step: 00036700] Batch Recognition Loss:   0.000165 => Gls Tokens per Sec:     2905 || Batch Translation Loss:   0.008503 => Txt Tokens per Sec:     7460 || Lr: 0.000050
2024-02-02 19:20:07,028 Epoch 1080: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-02 19:20:07,029 EPOCH 1081
2024-02-02 19:20:11,887 Epoch 1081: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-02 19:20:11,888 EPOCH 1082
2024-02-02 19:20:16,230 Epoch 1082: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-02 19:20:16,230 EPOCH 1083
2024-02-02 19:20:18,060 [Epoch: 1083 Step: 00036800] Batch Recognition Loss:   0.000126 => Gls Tokens per Sec:     1962 || Batch Translation Loss:   0.010214 => Txt Tokens per Sec:     5554 || Lr: 0.000050
2024-02-02 19:20:21,104 Epoch 1083: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-02 19:20:21,104 EPOCH 1084
2024-02-02 19:20:25,370 Epoch 1084: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-02 19:20:25,371 EPOCH 1085
2024-02-02 19:20:30,523 Epoch 1085: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-02 19:20:30,524 EPOCH 1086
2024-02-02 19:20:32,060 [Epoch: 1086 Step: 00036900] Batch Recognition Loss:   0.000104 => Gls Tokens per Sec:     2086 || Batch Translation Loss:   0.006529 => Txt Tokens per Sec:     5882 || Lr: 0.000050
2024-02-02 19:20:35,143 Epoch 1086: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-02 19:20:35,143 EPOCH 1087
2024-02-02 19:20:39,999 Epoch 1087: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-02 19:20:40,000 EPOCH 1088
2024-02-02 19:20:44,130 Epoch 1088: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.98 
2024-02-02 19:20:44,131 EPOCH 1089
2024-02-02 19:20:45,338 [Epoch: 1089 Step: 00037000] Batch Recognition Loss:   0.000122 => Gls Tokens per Sec:     2122 || Batch Translation Loss:   0.012683 => Txt Tokens per Sec:     5712 || Lr: 0.000050
2024-02-02 19:20:49,044 Epoch 1089: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-02 19:20:49,045 EPOCH 1090
2024-02-02 19:20:53,389 Epoch 1090: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-02 19:20:53,389 EPOCH 1091
2024-02-02 19:20:58,240 Epoch 1091: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.89 
2024-02-02 19:20:58,240 EPOCH 1092
2024-02-02 19:20:59,042 [Epoch: 1092 Step: 00037100] Batch Recognition Loss:   0.000269 => Gls Tokens per Sec:     2397 || Batch Translation Loss:   0.024768 => Txt Tokens per Sec:     6851 || Lr: 0.000050
2024-02-02 19:21:02,484 Epoch 1092: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.17 
2024-02-02 19:21:02,485 EPOCH 1093
2024-02-02 19:21:07,341 Epoch 1093: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.41 
2024-02-02 19:21:07,341 EPOCH 1094
2024-02-02 19:21:11,684 Epoch 1094: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.19 
2024-02-02 19:21:11,684 EPOCH 1095
2024-02-02 19:21:12,407 [Epoch: 1095 Step: 00037200] Batch Recognition Loss:   0.000149 => Gls Tokens per Sec:     1773 || Batch Translation Loss:   0.018629 => Txt Tokens per Sec:     5097 || Lr: 0.000050
2024-02-02 19:21:16,529 Epoch 1095: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.94 
2024-02-02 19:21:16,530 EPOCH 1096
2024-02-02 19:21:20,906 Epoch 1096: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-02 19:21:20,907 EPOCH 1097
2024-02-02 19:21:25,701 Epoch 1097: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.72 
2024-02-02 19:21:25,701 EPOCH 1098
2024-02-02 19:21:25,958 [Epoch: 1098 Step: 00037300] Batch Recognition Loss:   0.000199 => Gls Tokens per Sec:     1523 || Batch Translation Loss:   0.003894 => Txt Tokens per Sec:     4332 || Lr: 0.000050
2024-02-02 19:21:30,023 Epoch 1098: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.58 
2024-02-02 19:21:30,024 EPOCH 1099
2024-02-02 19:21:34,829 Epoch 1099: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-02 19:21:34,829 EPOCH 1100
2024-02-02 19:21:39,198 [Epoch: 1100 Step: 00037400] Batch Recognition Loss:   0.000130 => Gls Tokens per Sec:     2434 || Batch Translation Loss:   0.012602 => Txt Tokens per Sec:     6758 || Lr: 0.000050
2024-02-02 19:21:39,198 Epoch 1100: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-02 19:21:39,198 EPOCH 1101
2024-02-02 19:21:43,946 Epoch 1101: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-02 19:21:43,946 EPOCH 1102
2024-02-02 19:21:48,046 Epoch 1102: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-02 19:21:48,046 EPOCH 1103
2024-02-02 19:21:52,331 [Epoch: 1103 Step: 00037500] Batch Recognition Loss:   0.000134 => Gls Tokens per Sec:     2332 || Batch Translation Loss:   0.009914 => Txt Tokens per Sec:     6425 || Lr: 0.000050
2024-02-02 19:21:52,643 Epoch 1103: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-02 19:21:52,643 EPOCH 1104
2024-02-02 19:21:57,643 Epoch 1104: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-02 19:21:57,644 EPOCH 1105
2024-02-02 19:22:02,419 Epoch 1105: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-02 19:22:02,420 EPOCH 1106
2024-02-02 19:22:06,721 [Epoch: 1106 Step: 00037600] Batch Recognition Loss:   0.000132 => Gls Tokens per Sec:     2174 || Batch Translation Loss:   0.010401 => Txt Tokens per Sec:     6048 || Lr: 0.000050
2024-02-02 19:22:07,339 Epoch 1106: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-02 19:22:07,339 EPOCH 1107
2024-02-02 19:22:11,667 Epoch 1107: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-02 19:22:11,667 EPOCH 1108
2024-02-02 19:22:16,061 Epoch 1108: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-02 19:22:16,062 EPOCH 1109
2024-02-02 19:22:20,252 [Epoch: 1109 Step: 00037700] Batch Recognition Loss:   0.000111 => Gls Tokens per Sec:     2079 || Batch Translation Loss:   0.010265 => Txt Tokens per Sec:     5858 || Lr: 0.000050
2024-02-02 19:22:20,924 Epoch 1109: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-02 19:22:20,924 EPOCH 1110
2024-02-02 19:22:25,752 Epoch 1110: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-02 19:22:25,753 EPOCH 1111
2024-02-02 19:22:30,428 Epoch 1111: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.76 
2024-02-02 19:22:30,429 EPOCH 1112
2024-02-02 19:22:33,386 [Epoch: 1112 Step: 00037800] Batch Recognition Loss:   0.000127 => Gls Tokens per Sec:     2814 || Batch Translation Loss:   0.022362 => Txt Tokens per Sec:     7736 || Lr: 0.000050
2024-02-02 19:22:34,573 Epoch 1112: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.65 
2024-02-02 19:22:34,573 EPOCH 1113
2024-02-02 19:22:39,464 Epoch 1113: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-02 19:22:39,465 EPOCH 1114
2024-02-02 19:22:43,746 Epoch 1114: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-02 19:22:43,747 EPOCH 1115
2024-02-02 19:22:47,359 [Epoch: 1115 Step: 00037900] Batch Recognition Loss:   0.000229 => Gls Tokens per Sec:     2058 || Batch Translation Loss:   0.029551 => Txt Tokens per Sec:     5935 || Lr: 0.000050
2024-02-02 19:22:48,643 Epoch 1115: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.09 
2024-02-02 19:22:48,643 EPOCH 1116
2024-02-02 19:22:52,947 Epoch 1116: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.88 
2024-02-02 19:22:52,947 EPOCH 1117
2024-02-02 19:22:57,796 Epoch 1117: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.64 
2024-02-02 19:22:57,796 EPOCH 1118
2024-02-02 19:23:00,270 [Epoch: 1118 Step: 00038000] Batch Recognition Loss:   0.000396 => Gls Tokens per Sec:     2745 || Batch Translation Loss:   0.032659 => Txt Tokens per Sec:     7550 || Lr: 0.000050
2024-02-02 19:23:08,606 Validation result at epoch 1118, step    38000: duration: 8.3360s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00125	Translation Loss: 93647.77344	PPL: 11746.80957
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.69	(BLEU-1: 10.66,	BLEU-2: 3.02,	BLEU-3: 1.31,	BLEU-4: 0.69)
	CHRF 16.66	ROUGE 9.27
2024-02-02 19:23:08,607 Logging Recognition and Translation Outputs
2024-02-02 19:23:08,607 ========================================================================================================================
2024-02-02 19:23:08,607 Logging Sequence: 72_194.00
2024-02-02 19:23:08,608 	Gloss Reference :	A B+C+D+E
2024-02-02 19:23:08,608 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 19:23:08,608 	Gloss Alignment :	         
2024-02-02 19:23:08,608 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 19:23:08,609 	Text Reference  :	***** shah told  her   to   do     what she  wants and filed a police complaint against her   
2024-02-02 19:23:08,609 	Text Hypothesis :	babar kept their video call people are  very well  and ***** * ****** for       his     defeat
2024-02-02 19:23:08,609 	Text Alignment  :	I     S    S     S     S    S      S    S    S         D     D D      S         S       S     
2024-02-02 19:23:08,609 ========================================================================================================================
2024-02-02 19:23:08,610 Logging Sequence: 108_59.00
2024-02-02 19:23:08,610 	Gloss Reference :	A B+C+D+E
2024-02-02 19:23:08,610 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 19:23:08,610 	Gloss Alignment :	         
2024-02-02 19:23:08,610 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 19:23:08,612 	Text Reference  :	ishan kishan remained the biggest buy of     ipl  as    mumbai indians paid  a  whopping rs  1525 crore to   keep    him 
2024-02-02 19:23:08,612 	Text Hypothesis :	***** ****** ******** *** ******* he  gifted each other before the     games to play     was held on    15th october 2023
2024-02-02 19:23:08,612 	Text Alignment  :	D     D      D        D   D       S   S      S    S     S      S       S     S  S        S   S    S     S    S       S   
2024-02-02 19:23:08,612 ========================================================================================================================
2024-02-02 19:23:08,612 Logging Sequence: 109_10.00
2024-02-02 19:23:08,613 	Gloss Reference :	A B+C+D+E
2024-02-02 19:23:08,613 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 19:23:08,613 	Gloss Alignment :	         
2024-02-02 19:23:08,613 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 19:23:08,614 	Text Reference  :	was scheduled to be   played at the ***** narendra modi stadium in  ahmedabad
2024-02-02 19:23:08,614 	Text Hypothesis :	*** ********* ** this went   on the first time     if   a       t20 match    
2024-02-02 19:23:08,614 	Text Alignment  :	D   D         D  S    S      S      I     S        S    S       S   S        
2024-02-02 19:23:08,614 ========================================================================================================================
2024-02-02 19:23:08,614 Logging Sequence: 103_202.00
2024-02-02 19:23:08,614 	Gloss Reference :	A B+C+D+E
2024-02-02 19:23:08,615 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 19:23:08,615 	Gloss Alignment :	         
2024-02-02 19:23:08,615 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 19:23:08,616 	Text Reference  :	india in total has won 61           medals including 22           gold medals 16      silver    medals 23     bronze      medals
2024-02-02 19:23:08,616 	Text Hypothesis :	***** ** ***** *** *** commonwealth games  encourage independence from the    british democracy human  rights development etc   
2024-02-02 19:23:08,616 	Text Alignment  :	D     D  D     D   D   S            S      S         S            S    S      S       S         S      S      S           S     
2024-02-02 19:23:08,616 ========================================================================================================================
2024-02-02 19:23:08,617 Logging Sequence: 149_77.00
2024-02-02 19:23:08,617 	Gloss Reference :	A B+C+D+E
2024-02-02 19:23:08,617 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 19:23:08,617 	Gloss Alignment :	         
2024-02-02 19:23:08,617 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 19:23:08,618 	Text Reference  :	and arrested danushka for alleged sexual assault of a       29    year       old      woman whose  name has    not been  disclosed
2024-02-02 19:23:08,618 	Text Hypothesis :	*** ******** ******** *** ******* ****** ******* ** however zabka gracefully declined to    accept her  family and found out      
2024-02-02 19:23:08,619 	Text Alignment  :	D   D        D        D   D       D      D       D  S       S     S          S        S     S      S    S      S   S     S        
2024-02-02 19:23:08,619 ========================================================================================================================
2024-02-02 19:23:10,596 Epoch 1118: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.79 
2024-02-02 19:23:10,597 EPOCH 1119
2024-02-02 19:23:15,504 Epoch 1119: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.88 
2024-02-02 19:23:15,505 EPOCH 1120
2024-02-02 19:23:19,840 Epoch 1120: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.77 
2024-02-02 19:23:19,840 EPOCH 1121
2024-02-02 19:23:22,922 [Epoch: 1121 Step: 00038100] Batch Recognition Loss:   0.000162 => Gls Tokens per Sec:     1997 || Batch Translation Loss:   0.014970 => Txt Tokens per Sec:     5680 || Lr: 0.000050
2024-02-02 19:23:24,676 Epoch 1121: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-02 19:23:24,676 EPOCH 1122
2024-02-02 19:23:28,928 Epoch 1122: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-02 19:23:28,929 EPOCH 1123
2024-02-02 19:23:33,839 Epoch 1123: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-02 19:23:33,839 EPOCH 1124
2024-02-02 19:23:35,862 [Epoch: 1124 Step: 00038200] Batch Recognition Loss:   0.000200 => Gls Tokens per Sec:     2849 || Batch Translation Loss:   0.013226 => Txt Tokens per Sec:     7573 || Lr: 0.000050
2024-02-02 19:23:38,703 Epoch 1124: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-02 19:23:38,703 EPOCH 1125
2024-02-02 19:23:43,316 Epoch 1125: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-02 19:23:43,316 EPOCH 1126
2024-02-02 19:23:47,352 Epoch 1126: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-02 19:23:47,352 EPOCH 1127
2024-02-02 19:23:49,029 [Epoch: 1127 Step: 00038300] Batch Recognition Loss:   0.000170 => Gls Tokens per Sec:     3053 || Batch Translation Loss:   0.012030 => Txt Tokens per Sec:     8326 || Lr: 0.000050
2024-02-02 19:23:51,398 Epoch 1127: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-02 19:23:51,398 EPOCH 1128
2024-02-02 19:23:55,466 Epoch 1128: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-02 19:23:55,466 EPOCH 1129
2024-02-02 19:23:59,741 Epoch 1129: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-02 19:23:59,742 EPOCH 1130
2024-02-02 19:24:01,838 [Epoch: 1130 Step: 00038400] Batch Recognition Loss:   0.000166 => Gls Tokens per Sec:     2137 || Batch Translation Loss:   0.013051 => Txt Tokens per Sec:     6102 || Lr: 0.000050
2024-02-02 19:24:04,616 Epoch 1130: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-02 19:24:04,616 EPOCH 1131
2024-02-02 19:24:09,453 Epoch 1131: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-02 19:24:09,454 EPOCH 1132
2024-02-02 19:24:13,772 Epoch 1132: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-02 19:24:13,772 EPOCH 1133
2024-02-02 19:24:15,208 [Epoch: 1133 Step: 00038500] Batch Recognition Loss:   0.000157 => Gls Tokens per Sec:     2676 || Batch Translation Loss:   0.014361 => Txt Tokens per Sec:     7442 || Lr: 0.000050
2024-02-02 19:24:18,631 Epoch 1133: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.86 
2024-02-02 19:24:18,632 EPOCH 1134
2024-02-02 19:24:22,965 Epoch 1134: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.81 
2024-02-02 19:24:22,965 EPOCH 1135
2024-02-02 19:24:27,864 Epoch 1135: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.77 
2024-02-02 19:24:27,865 EPOCH 1136
2024-02-02 19:24:29,346 [Epoch: 1136 Step: 00038600] Batch Recognition Loss:   0.000137 => Gls Tokens per Sec:     2164 || Batch Translation Loss:   0.016237 => Txt Tokens per Sec:     6333 || Lr: 0.000050
2024-02-02 19:24:32,190 Epoch 1136: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-02 19:24:32,191 EPOCH 1137
2024-02-02 19:24:37,037 Epoch 1137: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-02 19:24:37,038 EPOCH 1138
2024-02-02 19:24:41,335 Epoch 1138: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-02 19:24:41,335 EPOCH 1139
2024-02-02 19:24:42,451 [Epoch: 1139 Step: 00038700] Batch Recognition Loss:   0.000126 => Gls Tokens per Sec:     2296 || Batch Translation Loss:   0.014162 => Txt Tokens per Sec:     6763 || Lr: 0.000050
2024-02-02 19:24:46,278 Epoch 1139: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-02 19:24:46,279 EPOCH 1140
2024-02-02 19:24:50,518 Epoch 1140: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.73 
2024-02-02 19:24:50,518 EPOCH 1141
2024-02-02 19:24:55,065 Epoch 1141: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-02 19:24:55,065 EPOCH 1142
2024-02-02 19:24:55,669 [Epoch: 1142 Step: 00038800] Batch Recognition Loss:   0.000153 => Gls Tokens per Sec:     3190 || Batch Translation Loss:   0.018519 => Txt Tokens per Sec:     8528 || Lr: 0.000050
2024-02-02 19:24:59,617 Epoch 1142: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.98 
2024-02-02 19:24:59,618 EPOCH 1143
2024-02-02 19:25:04,607 Epoch 1143: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.82 
2024-02-02 19:25:04,608 EPOCH 1144
2024-02-02 19:25:08,969 Epoch 1144: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.78 
2024-02-02 19:25:08,970 EPOCH 1145
2024-02-02 19:25:09,531 [Epoch: 1145 Step: 00038900] Batch Recognition Loss:   0.000207 => Gls Tokens per Sec:     2288 || Batch Translation Loss:   0.041298 => Txt Tokens per Sec:     6542 || Lr: 0.000050
2024-02-02 19:25:13,843 Epoch 1145: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.80 
2024-02-02 19:25:13,843 EPOCH 1146
2024-02-02 19:25:18,091 Epoch 1146: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.02 
2024-02-02 19:25:18,092 EPOCH 1147
2024-02-02 19:25:22,985 Epoch 1147: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.09 
2024-02-02 19:25:22,986 EPOCH 1148
2024-02-02 19:25:23,345 [Epoch: 1148 Step: 00039000] Batch Recognition Loss:   0.000263 => Gls Tokens per Sec:     1788 || Batch Translation Loss:   0.026890 => Txt Tokens per Sec:     5796 || Lr: 0.000050
2024-02-02 19:25:27,332 Epoch 1148: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.95 
2024-02-02 19:25:27,333 EPOCH 1149
2024-02-02 19:25:32,245 Epoch 1149: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.70 
2024-02-02 19:25:32,246 EPOCH 1150
2024-02-02 19:25:36,615 [Epoch: 1150 Step: 00039100] Batch Recognition Loss:   0.000208 => Gls Tokens per Sec:     2434 || Batch Translation Loss:   0.021257 => Txt Tokens per Sec:     6756 || Lr: 0.000050
2024-02-02 19:25:36,616 Epoch 1150: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-02 19:25:36,616 EPOCH 1151
2024-02-02 19:25:41,467 Epoch 1151: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-02 19:25:41,468 EPOCH 1152
2024-02-02 19:25:45,528 Epoch 1152: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-02 19:25:45,528 EPOCH 1153
2024-02-02 19:25:50,258 [Epoch: 1153 Step: 00039200] Batch Recognition Loss:   0.000123 => Gls Tokens per Sec:     2113 || Batch Translation Loss:   0.017397 => Txt Tokens per Sec:     5884 || Lr: 0.000050
2024-02-02 19:25:50,476 Epoch 1153: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-02 19:25:50,476 EPOCH 1154
2024-02-02 19:25:55,294 Epoch 1154: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-02 19:25:55,295 EPOCH 1155
2024-02-02 19:26:00,036 Epoch 1155: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-02 19:26:00,036 EPOCH 1156
2024-02-02 19:26:04,723 [Epoch: 1156 Step: 00039300] Batch Recognition Loss:   0.000142 => Gls Tokens per Sec:     1995 || Batch Translation Loss:   0.011269 => Txt Tokens per Sec:     5540 || Lr: 0.000050
2024-02-02 19:26:05,237 Epoch 1156: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-02 19:26:05,237 EPOCH 1157
2024-02-02 19:26:09,875 Epoch 1157: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.72 
2024-02-02 19:26:09,875 EPOCH 1158
2024-02-02 19:26:14,431 Epoch 1158: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.96 
2024-02-02 19:26:14,432 EPOCH 1159
2024-02-02 19:26:18,523 [Epoch: 1159 Step: 00039400] Batch Recognition Loss:   0.000129 => Gls Tokens per Sec:     2130 || Batch Translation Loss:   0.021007 => Txt Tokens per Sec:     5908 || Lr: 0.000050
2024-02-02 19:26:19,435 Epoch 1159: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.02 
2024-02-02 19:26:19,435 EPOCH 1160
2024-02-02 19:26:24,194 Epoch 1160: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.08 
2024-02-02 19:26:24,195 EPOCH 1161
2024-02-02 19:26:28,838 Epoch 1161: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.19 
2024-02-02 19:26:28,838 EPOCH 1162
2024-02-02 19:26:32,604 [Epoch: 1162 Step: 00039500] Batch Recognition Loss:   0.000190 => Gls Tokens per Sec:     2143 || Batch Translation Loss:   0.013847 => Txt Tokens per Sec:     5970 || Lr: 0.000050
2024-02-02 19:26:33,630 Epoch 1162: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.82 
2024-02-02 19:26:33,631 EPOCH 1163
2024-02-02 19:26:38,042 Epoch 1163: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.66 
2024-02-02 19:26:38,042 EPOCH 1164
2024-02-02 19:26:42,367 Epoch 1164: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.61 
2024-02-02 19:26:42,368 EPOCH 1165
2024-02-02 19:26:45,912 [Epoch: 1165 Step: 00039600] Batch Recognition Loss:   0.000233 => Gls Tokens per Sec:     2168 || Batch Translation Loss:   0.035956 => Txt Tokens per Sec:     6032 || Lr: 0.000050
2024-02-02 19:26:47,246 Epoch 1165: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.45 
2024-02-02 19:26:47,247 EPOCH 1166
2024-02-02 19:26:52,191 Epoch 1166: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.46 
2024-02-02 19:26:52,192 EPOCH 1167
2024-02-02 19:26:56,940 Epoch 1167: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.25 
2024-02-02 19:26:56,941 EPOCH 1168
2024-02-02 19:26:59,713 [Epoch: 1168 Step: 00039700] Batch Recognition Loss:   0.000277 => Gls Tokens per Sec:     2451 || Batch Translation Loss:   0.018802 => Txt Tokens per Sec:     6488 || Lr: 0.000050
2024-02-02 19:27:01,650 Epoch 1168: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.13 
2024-02-02 19:27:01,651 EPOCH 1169
2024-02-02 19:27:05,716 Epoch 1169: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.12 
2024-02-02 19:27:05,717 EPOCH 1170
2024-02-02 19:27:09,832 Epoch 1170: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.85 
2024-02-02 19:27:09,832 EPOCH 1171
2024-02-02 19:27:12,783 [Epoch: 1171 Step: 00039800] Batch Recognition Loss:   0.000225 => Gls Tokens per Sec:     2170 || Batch Translation Loss:   0.016575 => Txt Tokens per Sec:     5989 || Lr: 0.000050
2024-02-02 19:27:14,748 Epoch 1171: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-02 19:27:14,749 EPOCH 1172
2024-02-02 19:27:19,199 Epoch 1172: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-02 19:27:19,199 EPOCH 1173
2024-02-02 19:27:24,066 Epoch 1173: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-02 19:27:24,067 EPOCH 1174
2024-02-02 19:27:26,329 [Epoch: 1174 Step: 00039900] Batch Recognition Loss:   0.000164 => Gls Tokens per Sec:     2437 || Batch Translation Loss:   0.007367 => Txt Tokens per Sec:     6644 || Lr: 0.000050
2024-02-02 19:27:28,276 Epoch 1174: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-02 19:27:28,276 EPOCH 1175
2024-02-02 19:27:33,206 Epoch 1175: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-02 19:27:33,206 EPOCH 1176
2024-02-02 19:27:37,568 Epoch 1176: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.77 
2024-02-02 19:27:37,568 EPOCH 1177
2024-02-02 19:27:39,798 [Epoch: 1177 Step: 00040000] Batch Recognition Loss:   0.000178 => Gls Tokens per Sec:     2185 || Batch Translation Loss:   0.014257 => Txt Tokens per Sec:     5892 || Lr: 0.000050
2024-02-02 19:27:48,351 Validation result at epoch 1177, step    40000: duration: 8.5530s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00064	Translation Loss: 92624.41406	PPL: 10603.39453
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.68	(BLEU-1: 10.69,	BLEU-2: 3.10,	BLEU-3: 1.30,	BLEU-4: 0.68)
	CHRF 16.78	ROUGE 8.92
2024-02-02 19:27:48,352 Logging Recognition and Translation Outputs
2024-02-02 19:27:48,352 ========================================================================================================================
2024-02-02 19:27:48,352 Logging Sequence: 123_104.00
2024-02-02 19:27:48,352 	Gloss Reference :	A B+C+D+E
2024-02-02 19:27:48,352 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 19:27:48,353 	Gloss Alignment :	         
2024-02-02 19:27:48,353 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 19:27:48,354 	Text Reference  :	the car was presented to the  former india      cricketer from an unknown person
2024-02-02 19:27:48,354 	Text Hypothesis :	*** now i   am        my cool and    shockingly they      won  a  gold    medals
2024-02-02 19:27:48,354 	Text Alignment  :	D   S   S   S         S  S    S      S          S         S    S  S       S     
2024-02-02 19:27:48,354 ========================================================================================================================
2024-02-02 19:27:48,354 Logging Sequence: 107_23.00
2024-02-02 19:27:48,354 	Gloss Reference :	A B+C+D+E
2024-02-02 19:27:48,354 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 19:27:48,355 	Gloss Alignment :	         
2024-02-02 19:27:48,355 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 19:27:48,355 	Text Reference  :	and viktor lilov who  is also from the  usa    
2024-02-02 19:27:48,355 	Text Hypothesis :	*** hence  the   bcci is **** **** very worried
2024-02-02 19:27:48,355 	Text Alignment  :	D   S      S     S       D    D    S    S      
2024-02-02 19:27:48,356 ========================================================================================================================
2024-02-02 19:27:48,356 Logging Sequence: 134_212.00
2024-02-02 19:27:48,356 	Gloss Reference :	A B+C+D+E
2024-02-02 19:27:48,356 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 19:27:48,356 	Gloss Alignment :	         
2024-02-02 19:27:48,356 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 19:27:48,357 	Text Reference  :	*** ******* ** *** ******** **** ** dhanush said   that  he   practises little yoga
2024-02-02 19:27:48,357 	Text Hypothesis :	and because of the athletes held in the     league stage with 15th      may    2022
2024-02-02 19:27:48,357 	Text Alignment  :	I   I       I  I   I        I    I  S       S      S     S    S         S      S   
2024-02-02 19:27:48,357 ========================================================================================================================
2024-02-02 19:27:48,357 Logging Sequence: 165_577.00
2024-02-02 19:27:48,358 	Gloss Reference :	A B+C+D+E
2024-02-02 19:27:48,358 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 19:27:48,358 	Gloss Alignment :	         
2024-02-02 19:27:48,358 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 19:27:48,359 	Text Reference  :	then after 28 years india won the world cup again  in   2011   
2024-02-02 19:27:48,359 	Text Hypothesis :	**** ***** ** dhoni said  'i  am  going to  retire from cricket
2024-02-02 19:27:48,359 	Text Alignment  :	D    D     D  S     S     S   S   S     S   S      S    S      
2024-02-02 19:27:48,359 ========================================================================================================================
2024-02-02 19:27:48,359 Logging Sequence: 88_142.00
2024-02-02 19:27:48,359 	Gloss Reference :	A B+C+D+E
2024-02-02 19:27:48,359 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 19:27:48,360 	Gloss Alignment :	         
2024-02-02 19:27:48,360 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 19:27:48,361 	Text Reference  :	*** this  is    because the police ***** does ******** ** ***** ******** *** **** ** *** *** ******** not  do     anything
2024-02-02 19:27:48,361 	Text Hypothesis :	the mayor added that    the police never does anything to catch culprits and that is why the culprits have become bold    
2024-02-02 19:27:48,361 	Text Alignment  :	I   S     S     S                  I          I        I  I     I        I   I    I  I   I   I        S    S      S       
2024-02-02 19:27:48,361 ========================================================================================================================
2024-02-02 19:27:51,044 Epoch 1177: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.09 
2024-02-02 19:27:51,045 EPOCH 1178
2024-02-02 19:27:55,578 Epoch 1178: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.97 
2024-02-02 19:27:55,579 EPOCH 1179
2024-02-02 19:28:00,435 Epoch 1179: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.82 
2024-02-02 19:28:00,436 EPOCH 1180
2024-02-02 19:28:01,911 [Epoch: 1180 Step: 00040100] Batch Recognition Loss:   0.000136 => Gls Tokens per Sec:     2870 || Batch Translation Loss:   0.016260 => Txt Tokens per Sec:     7919 || Lr: 0.000050
2024-02-02 19:28:04,670 Epoch 1180: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.72 
2024-02-02 19:28:04,671 EPOCH 1181
2024-02-02 19:28:09,561 Epoch 1181: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-02 19:28:09,562 EPOCH 1182
2024-02-02 19:28:13,943 Epoch 1182: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-02 19:28:13,943 EPOCH 1183
2024-02-02 19:28:15,705 [Epoch: 1183 Step: 00040200] Batch Recognition Loss:   0.000188 => Gls Tokens per Sec:     2181 || Batch Translation Loss:   0.016683 => Txt Tokens per Sec:     5998 || Lr: 0.000050
2024-02-02 19:28:18,765 Epoch 1183: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-02 19:28:18,765 EPOCH 1184
2024-02-02 19:28:22,834 Epoch 1184: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-02 19:28:22,834 EPOCH 1185
2024-02-02 19:28:27,548 Epoch 1185: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-02 19:28:27,548 EPOCH 1186
2024-02-02 19:28:28,970 [Epoch: 1186 Step: 00040300] Batch Recognition Loss:   0.000150 => Gls Tokens per Sec:     2252 || Batch Translation Loss:   0.013289 => Txt Tokens per Sec:     6534 || Lr: 0.000050
2024-02-02 19:28:32,280 Epoch 1186: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-02 19:28:32,281 EPOCH 1187
2024-02-02 19:28:36,901 Epoch 1187: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-02 19:28:36,901 EPOCH 1188
2024-02-02 19:28:41,515 Epoch 1188: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-02 19:28:41,516 EPOCH 1189
2024-02-02 19:28:42,612 [Epoch: 1189 Step: 00040400] Batch Recognition Loss:   0.000129 => Gls Tokens per Sec:     2106 || Batch Translation Loss:   0.011625 => Txt Tokens per Sec:     6117 || Lr: 0.000050
2024-02-02 19:28:46,035 Epoch 1189: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-02 19:28:46,036 EPOCH 1190
2024-02-02 19:28:50,777 Epoch 1190: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.59 
2024-02-02 19:28:50,777 EPOCH 1191
2024-02-02 19:28:55,243 Epoch 1191: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.00 
2024-02-02 19:28:55,243 EPOCH 1192
2024-02-02 19:28:55,827 [Epoch: 1192 Step: 00040500] Batch Recognition Loss:   0.000153 => Gls Tokens per Sec:     3293 || Batch Translation Loss:   0.012851 => Txt Tokens per Sec:     8350 || Lr: 0.000050
2024-02-02 19:29:00,042 Epoch 1192: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.43 
2024-02-02 19:29:00,043 EPOCH 1193
2024-02-02 19:29:04,465 Epoch 1193: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.05 
2024-02-02 19:29:04,466 EPOCH 1194
2024-02-02 19:29:09,207 Epoch 1194: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.14 
2024-02-02 19:29:09,208 EPOCH 1195
2024-02-02 19:29:09,921 [Epoch: 1195 Step: 00040600] Batch Recognition Loss:   0.000184 => Gls Tokens per Sec:     1798 || Batch Translation Loss:   0.048601 => Txt Tokens per Sec:     5465 || Lr: 0.000050
2024-02-02 19:29:13,713 Epoch 1195: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.00 
2024-02-02 19:29:13,714 EPOCH 1196
2024-02-02 19:29:18,534 Epoch 1196: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.56 
2024-02-02 19:29:18,535 EPOCH 1197
2024-02-02 19:29:22,927 Epoch 1197: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.63 
2024-02-02 19:29:22,928 EPOCH 1198
2024-02-02 19:29:23,190 [Epoch: 1198 Step: 00040700] Batch Recognition Loss:   0.000228 => Gls Tokens per Sec:     2452 || Batch Translation Loss:   0.279779 => Txt Tokens per Sec:     7552 || Lr: 0.000050
2024-02-02 19:29:27,759 Epoch 1198: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.62 
2024-02-02 19:29:27,759 EPOCH 1199
2024-02-02 19:29:32,094 Epoch 1199: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.40 
2024-02-02 19:29:32,095 EPOCH 1200
2024-02-02 19:29:37,010 [Epoch: 1200 Step: 00040800] Batch Recognition Loss:   0.000209 => Gls Tokens per Sec:     2163 || Batch Translation Loss:   0.009363 => Txt Tokens per Sec:     6005 || Lr: 0.000050
2024-02-02 19:29:37,011 Epoch 1200: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.89 
2024-02-02 19:29:37,011 EPOCH 1201
2024-02-02 19:29:41,382 Epoch 1201: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.75 
2024-02-02 19:29:41,382 EPOCH 1202
2024-02-02 19:29:46,255 Epoch 1202: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-02 19:29:46,256 EPOCH 1203
2024-02-02 19:29:50,182 [Epoch: 1203 Step: 00040900] Batch Recognition Loss:   0.000155 => Gls Tokens per Sec:     2545 || Batch Translation Loss:   0.013039 => Txt Tokens per Sec:     6954 || Lr: 0.000050
2024-02-02 19:29:50,639 Epoch 1203: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.83 
2024-02-02 19:29:50,639 EPOCH 1204
2024-02-02 19:29:55,536 Epoch 1204: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-02 19:29:55,537 EPOCH 1205
2024-02-02 19:29:59,715 Epoch 1205: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-02 19:29:59,716 EPOCH 1206
2024-02-02 19:30:03,903 [Epoch: 1206 Step: 00041000] Batch Recognition Loss:   0.000137 => Gls Tokens per Sec:     2233 || Batch Translation Loss:   0.015226 => Txt Tokens per Sec:     6240 || Lr: 0.000050
2024-02-02 19:30:04,405 Epoch 1206: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-02 19:30:04,405 EPOCH 1207
2024-02-02 19:30:09,082 Epoch 1207: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-02 19:30:09,083 EPOCH 1208
2024-02-02 19:30:13,913 Epoch 1208: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-02 19:30:13,913 EPOCH 1209
2024-02-02 19:30:17,150 [Epoch: 1209 Step: 00041100] Batch Recognition Loss:   0.000141 => Gls Tokens per Sec:     2692 || Batch Translation Loss:   0.012372 => Txt Tokens per Sec:     7393 || Lr: 0.000050
2024-02-02 19:30:18,380 Epoch 1209: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-02 19:30:18,381 EPOCH 1210
2024-02-02 19:30:23,189 Epoch 1210: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-02 19:30:23,189 EPOCH 1211
2024-02-02 19:30:27,576 Epoch 1211: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-02 19:30:27,577 EPOCH 1212
2024-02-02 19:30:31,135 [Epoch: 1212 Step: 00041200] Batch Recognition Loss:   0.000139 => Gls Tokens per Sec:     2269 || Batch Translation Loss:   0.023450 => Txt Tokens per Sec:     6201 || Lr: 0.000050
2024-02-02 19:30:32,295 Epoch 1212: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.82 
2024-02-02 19:30:32,295 EPOCH 1213
2024-02-02 19:30:36,667 Epoch 1213: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.01 
2024-02-02 19:30:36,668 EPOCH 1214
2024-02-02 19:30:41,408 Epoch 1214: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.88 
2024-02-02 19:30:41,408 EPOCH 1215
2024-02-02 19:30:44,544 [Epoch: 1215 Step: 00041300] Batch Recognition Loss:   0.000634 => Gls Tokens per Sec:     2371 || Batch Translation Loss:   0.035005 => Txt Tokens per Sec:     6620 || Lr: 0.000050
2024-02-02 19:30:45,874 Epoch 1215: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.16 
2024-02-02 19:30:45,875 EPOCH 1216
2024-02-02 19:30:50,562 Epoch 1216: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.90 
2024-02-02 19:30:50,562 EPOCH 1217
2024-02-02 19:30:55,019 Epoch 1217: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.40 
2024-02-02 19:30:55,020 EPOCH 1218
2024-02-02 19:30:58,206 [Epoch: 1218 Step: 00041400] Batch Recognition Loss:   0.000221 => Gls Tokens per Sec:     2131 || Batch Translation Loss:   0.025113 => Txt Tokens per Sec:     5965 || Lr: 0.000050
2024-02-02 19:30:59,702 Epoch 1218: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.72 
2024-02-02 19:30:59,703 EPOCH 1219
2024-02-02 19:31:04,210 Epoch 1219: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.16 
2024-02-02 19:31:04,211 EPOCH 1220
2024-02-02 19:31:08,945 Epoch 1220: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.03 
2024-02-02 19:31:08,946 EPOCH 1221
2024-02-02 19:31:11,041 [Epoch: 1221 Step: 00041500] Batch Recognition Loss:   0.000210 => Gls Tokens per Sec:     2935 || Batch Translation Loss:   0.018485 => Txt Tokens per Sec:     7772 || Lr: 0.000050
2024-02-02 19:31:13,472 Epoch 1221: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-02 19:31:13,472 EPOCH 1222
2024-02-02 19:31:18,078 Epoch 1222: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-02 19:31:18,078 EPOCH 1223
2024-02-02 19:31:22,559 Epoch 1223: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.78 
2024-02-02 19:31:22,560 EPOCH 1224
2024-02-02 19:31:25,099 [Epoch: 1224 Step: 00041600] Batch Recognition Loss:   0.000163 => Gls Tokens per Sec:     2268 || Batch Translation Loss:   0.023298 => Txt Tokens per Sec:     6493 || Lr: 0.000050
2024-02-02 19:31:27,228 Epoch 1224: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.23 
2024-02-02 19:31:27,228 EPOCH 1225
2024-02-02 19:31:31,797 Epoch 1225: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.04 
2024-02-02 19:31:31,798 EPOCH 1226
2024-02-02 19:31:36,611 Epoch 1226: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.82 
2024-02-02 19:31:36,612 EPOCH 1227
2024-02-02 19:31:38,619 [Epoch: 1227 Step: 00041700] Batch Recognition Loss:   0.000183 => Gls Tokens per Sec:     2430 || Batch Translation Loss:   0.035910 => Txt Tokens per Sec:     6489 || Lr: 0.000050
2024-02-02 19:31:41,427 Epoch 1227: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.83 
2024-02-02 19:31:41,427 EPOCH 1228
2024-02-02 19:31:45,481 Epoch 1228: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-02 19:31:45,482 EPOCH 1229
2024-02-02 19:31:49,824 Epoch 1229: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-02 19:31:49,825 EPOCH 1230
2024-02-02 19:31:52,055 [Epoch: 1230 Step: 00041800] Batch Recognition Loss:   0.000187 => Gls Tokens per Sec:     1898 || Batch Translation Loss:   0.014218 => Txt Tokens per Sec:     5334 || Lr: 0.000050
2024-02-02 19:31:54,643 Epoch 1230: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-02 19:31:54,643 EPOCH 1231
2024-02-02 19:31:59,160 Epoch 1231: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-02 19:31:59,160 EPOCH 1232
2024-02-02 19:32:03,825 Epoch 1232: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-02 19:32:03,825 EPOCH 1233
2024-02-02 19:32:05,350 [Epoch: 1233 Step: 00041900] Batch Recognition Loss:   0.000149 => Gls Tokens per Sec:     2520 || Batch Translation Loss:   0.015264 => Txt Tokens per Sec:     7136 || Lr: 0.000050
2024-02-02 19:32:08,502 Epoch 1233: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-02 19:32:08,503 EPOCH 1234
2024-02-02 19:32:13,075 Epoch 1234: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-02 19:32:13,076 EPOCH 1235
2024-02-02 19:32:17,329 Epoch 1235: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-02 19:32:17,329 EPOCH 1236
2024-02-02 19:32:18,819 [Epoch: 1236 Step: 00042000] Batch Recognition Loss:   0.000132 => Gls Tokens per Sec:     1982 || Batch Translation Loss:   0.010462 => Txt Tokens per Sec:     5180 || Lr: 0.000050
2024-02-02 19:32:27,262 Validation result at epoch 1236, step    42000: duration: 8.4422s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00054	Translation Loss: 93852.93750	PPL: 11990.48340
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.83	(BLEU-1: 10.46,	BLEU-2: 3.17,	BLEU-3: 1.44,	BLEU-4: 0.83)
	CHRF 16.99	ROUGE 9.11
2024-02-02 19:32:27,263 Logging Recognition and Translation Outputs
2024-02-02 19:32:27,263 ========================================================================================================================
2024-02-02 19:32:27,264 Logging Sequence: 81_8.00
2024-02-02 19:32:27,264 	Gloss Reference :	A B+C+D+E
2024-02-02 19:32:27,264 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 19:32:27,264 	Gloss Alignment :	         
2024-02-02 19:32:27,265 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 19:32:27,267 	Text Reference  :	have been involved in       a    huge    controversy in  connection     to    real estate developer amrapali group since  last 7  years 
2024-02-02 19:32:27,267 	Text Hypothesis :	**** the  venue    narendra modi stadium for         the india-pakistan match has  been   kept      the      same  people can  be played
2024-02-02 19:32:27,267 	Text Alignment  :	D    S    S        S        S    S       S           S   S              S     S    S      S         S        S     S      S    S  S     
2024-02-02 19:32:27,267 ========================================================================================================================
2024-02-02 19:32:27,267 Logging Sequence: 148_239.00
2024-02-02 19:32:27,268 	Gloss Reference :	A B+C+D+E
2024-02-02 19:32:27,268 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 19:32:27,268 	Gloss Alignment :	         
2024-02-02 19:32:27,268 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 19:32:27,269 	Text Reference  :	******* ***** ***** the ground staff were very happy  and    thanked the bowler for  his     kind gesture
2024-02-02 19:32:27,269 	Text Hypothesis :	however virat kohli is  one    of    the  most widely shared a       lot of     this becomes very first  
2024-02-02 19:32:27,270 	Text Alignment  :	I       I     I     S   S      S     S    S    S      S      S       S   S      S    S       S    S      
2024-02-02 19:32:27,270 ========================================================================================================================
2024-02-02 19:32:27,270 Logging Sequence: 165_8.00
2024-02-02 19:32:27,270 	Gloss Reference :	A B+C+D+E
2024-02-02 19:32:27,270 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 19:32:27,270 	Gloss Alignment :	         
2024-02-02 19:32:27,270 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 19:32:27,271 	Text Reference  :	however many don't believe in          it  it       varies among people 
2024-02-02 19:32:27,271 	Text Hypothesis :	******* they are   caught  interogated and expelled from   the   stadium
2024-02-02 19:32:27,271 	Text Alignment  :	D       S    S     S       S           S   S        S      S     S      
2024-02-02 19:32:27,271 ========================================================================================================================
2024-02-02 19:32:27,271 Logging Sequence: 93_93.00
2024-02-02 19:32:27,272 	Gloss Reference :	A B+C+D+E
2024-02-02 19:32:27,272 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 19:32:27,272 	Gloss Alignment :	         
2024-02-02 19:32:27,272 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 19:32:27,273 	Text Reference  :	*** rooney was *** at the club as      well   
2024-02-02 19:32:27,273 	Text Hypothesis :	his mother was out to a   loss against england
2024-02-02 19:32:27,273 	Text Alignment  :	I   S          I   S  S   S    S       S      
2024-02-02 19:32:27,273 ========================================================================================================================
2024-02-02 19:32:27,273 Logging Sequence: 96_129.00
2024-02-02 19:32:27,273 	Gloss Reference :	A B+C+D+E
2024-02-02 19:32:27,273 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 19:32:27,273 	Gloss Alignment :	         
2024-02-02 19:32:27,273 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 19:32:27,274 	Text Reference  :	***** *** ***** ******** *********** ********** *** ***** viewers were   very stressed
2024-02-02 19:32:27,274 	Text Hypothesis :	while the other includes afghanistan bangladesh and india had     scored 91   runs    
2024-02-02 19:32:27,274 	Text Alignment  :	I     I   I     I        I           I          I   I     S       S      S    S       
2024-02-02 19:32:27,274 ========================================================================================================================
2024-02-02 19:32:30,651 Epoch 1236: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-02 19:32:30,652 EPOCH 1237
2024-02-02 19:32:35,355 Epoch 1237: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-02 19:32:35,356 EPOCH 1238
2024-02-02 19:32:39,838 Epoch 1238: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-02 19:32:39,839 EPOCH 1239
2024-02-02 19:32:41,031 [Epoch: 1239 Step: 00042100] Batch Recognition Loss:   0.000159 => Gls Tokens per Sec:     2149 || Batch Translation Loss:   0.017569 => Txt Tokens per Sec:     6603 || Lr: 0.000050
2024-02-02 19:32:44,494 Epoch 1239: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.94 
2024-02-02 19:32:44,494 EPOCH 1240
2024-02-02 19:32:49,007 Epoch 1240: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.07 
2024-02-02 19:32:49,008 EPOCH 1241
2024-02-02 19:32:53,695 Epoch 1241: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.04 
2024-02-02 19:32:53,696 EPOCH 1242
2024-02-02 19:32:54,281 [Epoch: 1242 Step: 00042200] Batch Recognition Loss:   0.000159 => Gls Tokens per Sec:     3288 || Batch Translation Loss:   0.010901 => Txt Tokens per Sec:     8288 || Lr: 0.000050
2024-02-02 19:32:58,364 Epoch 1242: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.28 
2024-02-02 19:32:58,365 EPOCH 1243
2024-02-02 19:33:03,234 Epoch 1243: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.19 
2024-02-02 19:33:03,234 EPOCH 1244
2024-02-02 19:33:07,750 Epoch 1244: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.46 
2024-02-02 19:33:07,750 EPOCH 1245
2024-02-02 19:33:08,296 [Epoch: 1245 Step: 00042300] Batch Recognition Loss:   0.000280 => Gls Tokens per Sec:     1890 || Batch Translation Loss:   0.051461 => Txt Tokens per Sec:     5299 || Lr: 0.000050
2024-02-02 19:33:12,438 Epoch 1245: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.02 
2024-02-02 19:33:12,438 EPOCH 1246
2024-02-02 19:33:17,443 Epoch 1246: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.91 
2024-02-02 19:33:17,444 EPOCH 1247
2024-02-02 19:33:21,903 Epoch 1247: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.91 
2024-02-02 19:33:21,903 EPOCH 1248
2024-02-02 19:33:22,120 [Epoch: 1248 Step: 00042400] Batch Recognition Loss:   0.000154 => Gls Tokens per Sec:     2977 || Batch Translation Loss:   0.018892 => Txt Tokens per Sec:     9112 || Lr: 0.000050
2024-02-02 19:33:26,410 Epoch 1248: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.73 
2024-02-02 19:33:26,410 EPOCH 1249
2024-02-02 19:33:31,150 Epoch 1249: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.89 
2024-02-02 19:33:31,150 EPOCH 1250
2024-02-02 19:33:36,012 [Epoch: 1250 Step: 00042500] Batch Recognition Loss:   0.000176 => Gls Tokens per Sec:     2187 || Batch Translation Loss:   0.031904 => Txt Tokens per Sec:     6071 || Lr: 0.000050
2024-02-02 19:33:36,013 Epoch 1250: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.78 
2024-02-02 19:33:36,013 EPOCH 1251
2024-02-02 19:33:40,307 Epoch 1251: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-02 19:33:40,307 EPOCH 1252
2024-02-02 19:33:45,106 Epoch 1252: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.73 
2024-02-02 19:33:45,107 EPOCH 1253
2024-02-02 19:33:49,193 [Epoch: 1253 Step: 00042600] Batch Recognition Loss:   0.000258 => Gls Tokens per Sec:     2445 || Batch Translation Loss:   0.021866 => Txt Tokens per Sec:     6796 || Lr: 0.000050
2024-02-02 19:33:49,451 Epoch 1253: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.84 
2024-02-02 19:33:49,451 EPOCH 1254
2024-02-02 19:33:53,819 Epoch 1254: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-02 19:33:53,820 EPOCH 1255
2024-02-02 19:33:58,620 Epoch 1255: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-02 19:33:58,621 EPOCH 1256
2024-02-02 19:34:03,042 [Epoch: 1256 Step: 00042700] Batch Recognition Loss:   0.000131 => Gls Tokens per Sec:     2115 || Batch Translation Loss:   0.007365 => Txt Tokens per Sec:     5859 || Lr: 0.000050
2024-02-02 19:34:03,574 Epoch 1256: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.81 
2024-02-02 19:34:03,574 EPOCH 1257
2024-02-02 19:34:07,766 Epoch 1257: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.91 
2024-02-02 19:34:07,766 EPOCH 1258
2024-02-02 19:34:11,828 Epoch 1258: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.97 
2024-02-02 19:34:11,828 EPOCH 1259
2024-02-02 19:34:15,523 [Epoch: 1259 Step: 00042800] Batch Recognition Loss:   0.000219 => Gls Tokens per Sec:     2358 || Batch Translation Loss:   0.027012 => Txt Tokens per Sec:     6419 || Lr: 0.000050
2024-02-02 19:34:16,691 Epoch 1259: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.16 
2024-02-02 19:34:16,691 EPOCH 1260
2024-02-02 19:34:21,510 Epoch 1260: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.92 
2024-02-02 19:34:21,511 EPOCH 1261
2024-02-02 19:34:25,924 Epoch 1261: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.95 
2024-02-02 19:34:25,924 EPOCH 1262
2024-02-02 19:34:29,909 [Epoch: 1262 Step: 00042900] Batch Recognition Loss:   0.000174 => Gls Tokens per Sec:     2025 || Batch Translation Loss:   0.024287 => Txt Tokens per Sec:     5765 || Lr: 0.000050
2024-02-02 19:34:30,767 Epoch 1262: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.82 
2024-02-02 19:34:30,767 EPOCH 1263
2024-02-02 19:34:35,062 Epoch 1263: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.08 
2024-02-02 19:34:35,063 EPOCH 1264
2024-02-02 19:34:39,991 Epoch 1264: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.24 
2024-02-02 19:34:39,992 EPOCH 1265
2024-02-02 19:34:42,859 [Epoch: 1265 Step: 00043000] Batch Recognition Loss:   0.000514 => Gls Tokens per Sec:     2679 || Batch Translation Loss:   0.035006 => Txt Tokens per Sec:     7307 || Lr: 0.000050
2024-02-02 19:34:44,195 Epoch 1265: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.50 
2024-02-02 19:34:44,195 EPOCH 1266
2024-02-02 19:34:49,321 Epoch 1266: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.35 
2024-02-02 19:34:49,321 EPOCH 1267
2024-02-02 19:34:53,982 Epoch 1267: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.29 
2024-02-02 19:34:53,983 EPOCH 1268
2024-02-02 19:34:57,539 [Epoch: 1268 Step: 00043100] Batch Recognition Loss:   0.000231 => Gls Tokens per Sec:     1910 || Batch Translation Loss:   0.031693 => Txt Tokens per Sec:     5583 || Lr: 0.000050
2024-02-02 19:34:58,852 Epoch 1268: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.96 
2024-02-02 19:34:58,852 EPOCH 1269
2024-02-02 19:35:03,216 Epoch 1269: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.38 
2024-02-02 19:35:03,217 EPOCH 1270
2024-02-02 19:35:08,039 Epoch 1270: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.10 
2024-02-02 19:35:08,039 EPOCH 1271
2024-02-02 19:35:10,475 [Epoch: 1271 Step: 00043200] Batch Recognition Loss:   0.000175 => Gls Tokens per Sec:     2628 || Batch Translation Loss:   0.012062 => Txt Tokens per Sec:     7200 || Lr: 0.000050
2024-02-02 19:35:12,403 Epoch 1271: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.73 
2024-02-02 19:35:12,404 EPOCH 1272
2024-02-02 19:35:17,164 Epoch 1272: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-02 19:35:17,165 EPOCH 1273
2024-02-02 19:35:21,507 Epoch 1273: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.70 
2024-02-02 19:35:21,508 EPOCH 1274
2024-02-02 19:35:24,055 [Epoch: 1274 Step: 00043300] Batch Recognition Loss:   0.000218 => Gls Tokens per Sec:     2263 || Batch Translation Loss:   0.016811 => Txt Tokens per Sec:     6214 || Lr: 0.000050
2024-02-02 19:35:26,318 Epoch 1274: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-02 19:35:26,318 EPOCH 1275
2024-02-02 19:35:30,724 Epoch 1275: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-02 19:35:30,724 EPOCH 1276
2024-02-02 19:35:35,423 Epoch 1276: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-02 19:35:35,423 EPOCH 1277
2024-02-02 19:35:37,322 [Epoch: 1277 Step: 00043400] Batch Recognition Loss:   0.000165 => Gls Tokens per Sec:     2698 || Batch Translation Loss:   0.017091 => Txt Tokens per Sec:     7499 || Lr: 0.000050
2024-02-02 19:35:39,863 Epoch 1277: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.82 
2024-02-02 19:35:39,864 EPOCH 1278
2024-02-02 19:35:44,860 Epoch 1278: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.85 
2024-02-02 19:35:44,861 EPOCH 1279
2024-02-02 19:35:49,408 Epoch 1279: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-02 19:35:49,408 EPOCH 1280
2024-02-02 19:35:51,598 [Epoch: 1280 Step: 00043500] Batch Recognition Loss:   0.000123 => Gls Tokens per Sec:     2048 || Batch Translation Loss:   0.013004 => Txt Tokens per Sec:     5903 || Lr: 0.000050
2024-02-02 19:35:54,220 Epoch 1280: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-02 19:35:54,221 EPOCH 1281
2024-02-02 19:35:58,572 Epoch 1281: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.76 
2024-02-02 19:35:58,573 EPOCH 1282
2024-02-02 19:36:03,283 Epoch 1282: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.81 
2024-02-02 19:36:03,283 EPOCH 1283
2024-02-02 19:36:04,544 [Epoch: 1283 Step: 00043600] Batch Recognition Loss:   0.000153 => Gls Tokens per Sec:     2848 || Batch Translation Loss:   0.015837 => Txt Tokens per Sec:     7714 || Lr: 0.000050
2024-02-02 19:36:07,810 Epoch 1283: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-02 19:36:07,811 EPOCH 1284
2024-02-02 19:36:12,458 Epoch 1284: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-02 19:36:12,458 EPOCH 1285
2024-02-02 19:36:16,543 Epoch 1285: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-02 19:36:16,543 EPOCH 1286
2024-02-02 19:36:18,065 [Epoch: 1286 Step: 00043700] Batch Recognition Loss:   0.000181 => Gls Tokens per Sec:     1941 || Batch Translation Loss:   0.017972 => Txt Tokens per Sec:     5501 || Lr: 0.000050
2024-02-02 19:36:21,328 Epoch 1286: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-02 19:36:21,329 EPOCH 1287
2024-02-02 19:36:25,989 Epoch 1287: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.55 
2024-02-02 19:36:25,989 EPOCH 1288
2024-02-02 19:36:30,631 Epoch 1288: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-02 19:36:30,632 EPOCH 1289
2024-02-02 19:36:31,550 [Epoch: 1289 Step: 00043800] Batch Recognition Loss:   0.000129 => Gls Tokens per Sec:     2518 || Batch Translation Loss:   0.014990 => Txt Tokens per Sec:     6835 || Lr: 0.000050
2024-02-02 19:36:35,134 Epoch 1289: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.57 
2024-02-02 19:36:35,135 EPOCH 1290
2024-02-02 19:36:39,719 Epoch 1290: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.73 
2024-02-02 19:36:39,720 EPOCH 1291
2024-02-02 19:36:44,276 Epoch 1291: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.47 
2024-02-02 19:36:44,276 EPOCH 1292
2024-02-02 19:36:45,101 [Epoch: 1292 Step: 00043900] Batch Recognition Loss:   0.000308 => Gls Tokens per Sec:     2330 || Batch Translation Loss:   0.055082 => Txt Tokens per Sec:     6223 || Lr: 0.000050
2024-02-02 19:36:48,877 Epoch 1292: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.96 
2024-02-02 19:36:48,877 EPOCH 1293
2024-02-02 19:36:53,406 Epoch 1293: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.07 
2024-02-02 19:36:53,407 EPOCH 1294
2024-02-02 19:36:57,995 Epoch 1294: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.75 
2024-02-02 19:36:57,995 EPOCH 1295
2024-02-02 19:36:58,468 [Epoch: 1295 Step: 00044000] Batch Recognition Loss:   0.000183 => Gls Tokens per Sec:     2182 || Batch Translation Loss:   0.019691 => Txt Tokens per Sec:     6013 || Lr: 0.000050
2024-02-02 19:37:06,848 Validation result at epoch 1295, step    44000: duration: 8.3805s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00073	Translation Loss: 94441.53906	PPL: 12717.94727
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.81	(BLEU-1: 10.51,	BLEU-2: 3.42,	BLEU-3: 1.49,	BLEU-4: 0.81)
	CHRF 17.07	ROUGE 9.17
2024-02-02 19:37:06,849 Logging Recognition and Translation Outputs
2024-02-02 19:37:06,849 ========================================================================================================================
2024-02-02 19:37:06,849 Logging Sequence: 117_29.00
2024-02-02 19:37:06,850 	Gloss Reference :	A B+C+D+E
2024-02-02 19:37:06,850 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 19:37:06,850 	Gloss Alignment :	         
2024-02-02 19:37:06,850 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 19:37:06,851 	Text Reference  :	however england was unable to reach the target they were all  out lost  by  66     runs
2024-02-02 19:37:06,851 	Text Hypothesis :	******* ******* *** ****** ** ***** *** sadly  they **** lost the match and scored 3175
2024-02-02 19:37:06,851 	Text Alignment  :	D       D       D   D      D  D     D   S           D    S    S   S     S   S      S   
2024-02-02 19:37:06,852 ========================================================================================================================
2024-02-02 19:37:06,852 Logging Sequence: 84_176.00
2024-02-02 19:37:06,852 	Gloss Reference :	A B+C+D+E
2024-02-02 19:37:06,852 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 19:37:06,852 	Gloss Alignment :	         
2024-02-02 19:37:06,852 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 19:37:06,853 	Text Reference  :	germany's nancy faeser who attended the game  in        doha   against  japan  said 
2024-02-02 19:37:06,853 	Text Hypothesis :	********* ***** ****** *** ******** and named prominent indian wrestler sushil kumar
2024-02-02 19:37:06,853 	Text Alignment  :	D         D     D      D   D        S   S     S         S      S        S      S    
2024-02-02 19:37:06,853 ========================================================================================================================
2024-02-02 19:37:06,853 Logging Sequence: 172_98.00
2024-02-02 19:37:06,854 	Gloss Reference :	A B+C+D+E
2024-02-02 19:37:06,854 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 19:37:06,854 	Gloss Alignment :	         
2024-02-02 19:37:06,854 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 19:37:06,855 	Text Reference  :	*** ***** since 700 pm ****** it  kept    raining the  intensity plunged around 915        pm    
2024-02-02 19:37:06,855 	Text Hypothesis :	the rains at    630 pm centre was covered with    with two       layers  of     protective sheets
2024-02-02 19:37:06,855 	Text Alignment  :	I   I     S     S      I      S   S       S       S    S         S       S      S          S     
2024-02-02 19:37:06,855 ========================================================================================================================
2024-02-02 19:37:06,855 Logging Sequence: 135_92.00
2024-02-02 19:37:06,856 	Gloss Reference :	A B+C+D+E
2024-02-02 19:37:06,856 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 19:37:06,856 	Gloss Alignment :	         
2024-02-02 19:37:06,856 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 19:37:06,857 	Text Reference  :	she wrote that half had already been raised by      the family's online fundraiser
2024-02-02 19:37:06,857 	Text Hypothesis :	she ***** **** **** *** ******* **** would  auction the ******** same   medal     
2024-02-02 19:37:06,857 	Text Alignment  :	    D     D    D    D   D       D    S      S           D        S      S         
2024-02-02 19:37:06,857 ========================================================================================================================
2024-02-02 19:37:06,857 Logging Sequence: 180_332.00
2024-02-02 19:37:06,857 	Gloss Reference :	A B+C+D+E
2024-02-02 19:37:06,857 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 19:37:06,857 	Gloss Alignment :	         
2024-02-02 19:37:06,858 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 19:37:06,859 	Text Reference  :	did i eat roti made of   shilajit that i        got     energy to  assault   so      many  girls 
2024-02-02 19:37:06,859 	Text Hypothesis :	*** * *** and  is   also the      most followed support and    the wrestlers through their tweets
2024-02-02 19:37:06,859 	Text Alignment  :	D   D D   S    S    S    S        S    S        S       S      S   S         S       S     S     
2024-02-02 19:37:06,859 ========================================================================================================================
2024-02-02 19:37:11,239 Epoch 1295: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.87 
2024-02-02 19:37:11,239 EPOCH 1296
2024-02-02 19:37:15,606 Epoch 1296: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-02 19:37:15,606 EPOCH 1297
2024-02-02 19:37:20,380 Epoch 1297: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.96 
2024-02-02 19:37:20,381 EPOCH 1298
2024-02-02 19:37:20,601 [Epoch: 1298 Step: 00044100] Batch Recognition Loss:   0.000174 => Gls Tokens per Sec:     2922 || Batch Translation Loss:   0.017765 => Txt Tokens per Sec:     7023 || Lr: 0.000050
2024-02-02 19:37:24,713 Epoch 1298: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.90 
2024-02-02 19:37:24,713 EPOCH 1299
2024-02-02 19:37:29,103 Epoch 1299: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.20 
2024-02-02 19:37:29,104 EPOCH 1300
2024-02-02 19:37:33,793 [Epoch: 1300 Step: 00044200] Batch Recognition Loss:   0.000280 => Gls Tokens per Sec:     2268 || Batch Translation Loss:   0.074790 => Txt Tokens per Sec:     6295 || Lr: 0.000050
2024-02-02 19:37:33,793 Epoch 1300: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.08 
2024-02-02 19:37:33,794 EPOCH 1301
2024-02-02 19:37:38,610 Epoch 1301: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.03 
2024-02-02 19:37:38,611 EPOCH 1302
2024-02-02 19:37:42,861 Epoch 1302: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.00 
2024-02-02 19:37:42,861 EPOCH 1303
2024-02-02 19:37:47,364 [Epoch: 1303 Step: 00044300] Batch Recognition Loss:   0.000192 => Gls Tokens per Sec:     2219 || Batch Translation Loss:   0.024303 => Txt Tokens per Sec:     6118 || Lr: 0.000050
2024-02-02 19:37:47,702 Epoch 1303: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.29 
2024-02-02 19:37:47,702 EPOCH 1304
2024-02-02 19:37:52,338 Epoch 1304: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.05 
2024-02-02 19:37:52,339 EPOCH 1305
2024-02-02 19:37:56,800 Epoch 1305: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.85 
2024-02-02 19:37:56,800 EPOCH 1306
2024-02-02 19:38:00,460 [Epoch: 1306 Step: 00044400] Batch Recognition Loss:   0.000185 => Gls Tokens per Sec:     2555 || Batch Translation Loss:   0.039561 => Txt Tokens per Sec:     6966 || Lr: 0.000050
2024-02-02 19:38:01,187 Epoch 1306: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.32 
2024-02-02 19:38:01,187 EPOCH 1307
2024-02-02 19:38:05,878 Epoch 1307: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.95 
2024-02-02 19:38:05,878 EPOCH 1308
2024-02-02 19:38:10,333 Epoch 1308: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.76 
2024-02-02 19:38:10,333 EPOCH 1309
2024-02-02 19:38:14,261 [Epoch: 1309 Step: 00044500] Batch Recognition Loss:   0.000194 => Gls Tokens per Sec:     2218 || Batch Translation Loss:   0.028487 => Txt Tokens per Sec:     6130 || Lr: 0.000050
2024-02-02 19:38:14,995 Epoch 1309: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.77 
2024-02-02 19:38:14,995 EPOCH 1310
2024-02-02 19:38:19,625 Epoch 1310: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.79 
2024-02-02 19:38:19,626 EPOCH 1311
2024-02-02 19:38:24,049 Epoch 1311: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.91 
2024-02-02 19:38:24,049 EPOCH 1312
2024-02-02 19:38:27,306 [Epoch: 1312 Step: 00044600] Batch Recognition Loss:   0.000213 => Gls Tokens per Sec:     2478 || Batch Translation Loss:   0.026482 => Txt Tokens per Sec:     6570 || Lr: 0.000050
2024-02-02 19:38:28,794 Epoch 1312: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.99 
2024-02-02 19:38:28,794 EPOCH 1313
2024-02-02 19:38:33,218 Epoch 1313: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.83 
2024-02-02 19:38:33,218 EPOCH 1314
2024-02-02 19:38:37,866 Epoch 1314: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.09 
2024-02-02 19:38:37,867 EPOCH 1315
2024-02-02 19:38:41,170 [Epoch: 1315 Step: 00044700] Batch Recognition Loss:   0.000425 => Gls Tokens per Sec:     2250 || Batch Translation Loss:   0.005245 => Txt Tokens per Sec:     6345 || Lr: 0.000050
2024-02-02 19:38:42,285 Epoch 1315: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.83 
2024-02-02 19:38:42,286 EPOCH 1316
2024-02-02 19:38:46,311 Epoch 1316: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-02 19:38:46,311 EPOCH 1317
2024-02-02 19:38:50,376 Epoch 1317: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.73 
2024-02-02 19:38:50,376 EPOCH 1318
2024-02-02 19:38:53,209 [Epoch: 1318 Step: 00044800] Batch Recognition Loss:   0.000137 => Gls Tokens per Sec:     2485 || Batch Translation Loss:   0.014638 => Txt Tokens per Sec:     6937 || Lr: 0.000050
2024-02-02 19:38:54,831 Epoch 1318: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-02 19:38:54,832 EPOCH 1319
2024-02-02 19:38:59,459 Epoch 1319: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.95 
2024-02-02 19:38:59,459 EPOCH 1320
2024-02-02 19:39:04,186 Epoch 1320: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.43 
2024-02-02 19:39:04,187 EPOCH 1321
2024-02-02 19:39:06,828 [Epoch: 1321 Step: 00044900] Batch Recognition Loss:   0.000351 => Gls Tokens per Sec:     2424 || Batch Translation Loss:   0.023275 => Txt Tokens per Sec:     6562 || Lr: 0.000050
2024-02-02 19:39:08,635 Epoch 1321: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.03 
2024-02-02 19:39:08,635 EPOCH 1322
2024-02-02 19:39:13,450 Epoch 1322: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.93 
2024-02-02 19:39:13,450 EPOCH 1323
2024-02-02 19:39:17,790 Epoch 1323: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.84 
2024-02-02 19:39:17,791 EPOCH 1324
2024-02-02 19:39:20,421 [Epoch: 1324 Step: 00045000] Batch Recognition Loss:   0.000196 => Gls Tokens per Sec:     2191 || Batch Translation Loss:   0.021821 => Txt Tokens per Sec:     6061 || Lr: 0.000050
2024-02-02 19:39:22,521 Epoch 1324: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.84 
2024-02-02 19:39:22,521 EPOCH 1325
2024-02-02 19:39:26,857 Epoch 1325: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.84 
2024-02-02 19:39:26,857 EPOCH 1326
2024-02-02 19:39:31,726 Epoch 1326: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.19 
2024-02-02 19:39:31,726 EPOCH 1327
2024-02-02 19:39:34,017 [Epoch: 1327 Step: 00045100] Batch Recognition Loss:   0.000176 => Gls Tokens per Sec:     2235 || Batch Translation Loss:   0.028210 => Txt Tokens per Sec:     6464 || Lr: 0.000050
2024-02-02 19:39:36,448 Epoch 1327: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.95 
2024-02-02 19:39:36,449 EPOCH 1328
2024-02-02 19:39:41,292 Epoch 1328: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-02 19:39:41,292 EPOCH 1329
2024-02-02 19:39:45,859 Epoch 1329: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-02 19:39:45,860 EPOCH 1330
2024-02-02 19:39:47,831 [Epoch: 1330 Step: 00045200] Batch Recognition Loss:   0.000145 => Gls Tokens per Sec:     2148 || Batch Translation Loss:   0.012588 => Txt Tokens per Sec:     5866 || Lr: 0.000050
2024-02-02 19:39:50,761 Epoch 1330: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-02 19:39:50,761 EPOCH 1331
2024-02-02 19:39:55,249 Epoch 1331: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-02 19:39:55,249 EPOCH 1332
2024-02-02 19:40:00,112 Epoch 1332: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-02 19:40:00,112 EPOCH 1333
2024-02-02 19:40:01,552 [Epoch: 1333 Step: 00045300] Batch Recognition Loss:   0.000099 => Gls Tokens per Sec:     2669 || Batch Translation Loss:   0.005064 => Txt Tokens per Sec:     6905 || Lr: 0.000050
2024-02-02 19:40:05,022 Epoch 1333: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-02 19:40:05,023 EPOCH 1334
2024-02-02 19:40:09,458 Epoch 1334: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.70 
2024-02-02 19:40:09,458 EPOCH 1335
2024-02-02 19:40:13,950 Epoch 1335: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-02 19:40:13,951 EPOCH 1336
2024-02-02 19:40:15,725 [Epoch: 1336 Step: 00045400] Batch Recognition Loss:   0.000132 => Gls Tokens per Sec:     1805 || Batch Translation Loss:   0.014562 => Txt Tokens per Sec:     5364 || Lr: 0.000050
2024-02-02 19:40:18,918 Epoch 1336: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-02 19:40:18,918 EPOCH 1337
2024-02-02 19:40:23,740 Epoch 1337: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-02 19:40:23,740 EPOCH 1338
2024-02-02 19:40:28,308 Epoch 1338: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-02 19:40:28,309 EPOCH 1339
2024-02-02 19:40:29,802 [Epoch: 1339 Step: 00045500] Batch Recognition Loss:   0.000114 => Gls Tokens per Sec:     1548 || Batch Translation Loss:   0.012702 => Txt Tokens per Sec:     4488 || Lr: 0.000050
2024-02-02 19:40:33,162 Epoch 1339: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.95 
2024-02-02 19:40:33,163 EPOCH 1340
2024-02-02 19:40:37,790 Epoch 1340: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.41 
2024-02-02 19:40:37,791 EPOCH 1341
2024-02-02 19:40:42,639 Epoch 1341: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.74 
2024-02-02 19:40:42,640 EPOCH 1342
2024-02-02 19:40:43,587 [Epoch: 1342 Step: 00045600] Batch Recognition Loss:   0.000239 => Gls Tokens per Sec:     2031 || Batch Translation Loss:   0.024392 => Txt Tokens per Sec:     5952 || Lr: 0.000050
2024-02-02 19:40:47,306 Epoch 1342: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.58 
2024-02-02 19:40:47,307 EPOCH 1343
2024-02-02 19:40:51,950 Epoch 1343: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.07 
2024-02-02 19:40:51,950 EPOCH 1344
2024-02-02 19:40:56,659 Epoch 1344: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.53 
2024-02-02 19:40:56,660 EPOCH 1345
2024-02-02 19:40:57,396 [Epoch: 1345 Step: 00045700] Batch Recognition Loss:   0.000281 => Gls Tokens per Sec:     1741 || Batch Translation Loss:   0.026014 => Txt Tokens per Sec:     5235 || Lr: 0.000050
2024-02-02 19:41:01,428 Epoch 1345: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.85 
2024-02-02 19:41:01,428 EPOCH 1346
2024-02-02 19:41:05,476 Epoch 1346: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.76 
2024-02-02 19:41:05,476 EPOCH 1347
2024-02-02 19:41:09,548 Epoch 1347: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.87 
2024-02-02 19:41:09,548 EPOCH 1348
2024-02-02 19:41:09,843 [Epoch: 1348 Step: 00045800] Batch Recognition Loss:   0.000246 => Gls Tokens per Sec:     2182 || Batch Translation Loss:   0.057370 => Txt Tokens per Sec:     6440 || Lr: 0.000050
2024-02-02 19:41:14,360 Epoch 1348: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.84 
2024-02-02 19:41:14,360 EPOCH 1349
2024-02-02 19:41:18,896 Epoch 1349: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.04 
2024-02-02 19:41:18,896 EPOCH 1350
2024-02-02 19:41:23,646 [Epoch: 1350 Step: 00045900] Batch Recognition Loss:   0.000143 => Gls Tokens per Sec:     2238 || Batch Translation Loss:   0.014781 => Txt Tokens per Sec:     6214 || Lr: 0.000050
2024-02-02 19:41:23,646 Epoch 1350: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.77 
2024-02-02 19:41:23,646 EPOCH 1351
2024-02-02 19:41:28,508 Epoch 1351: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-02 19:41:28,508 EPOCH 1352
2024-02-02 19:41:33,047 Epoch 1352: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-02 19:41:33,047 EPOCH 1353
2024-02-02 19:41:37,348 [Epoch: 1353 Step: 00046000] Batch Recognition Loss:   0.000104 => Gls Tokens per Sec:     2323 || Batch Translation Loss:   0.007987 => Txt Tokens per Sec:     6420 || Lr: 0.000050
2024-02-02 19:41:45,606 Validation result at epoch 1353, step    46000: duration: 8.2573s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00063	Translation Loss: 95228.14062	PPL: 13759.49902
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.59	(BLEU-1: 10.67,	BLEU-2: 3.12,	BLEU-3: 1.21,	BLEU-4: 0.59)
	CHRF 16.63	ROUGE 9.45
2024-02-02 19:41:45,607 Logging Recognition and Translation Outputs
2024-02-02 19:41:45,607 ========================================================================================================================
2024-02-02 19:41:45,607 Logging Sequence: 126_121.00
2024-02-02 19:41:45,607 	Gloss Reference :	A B+C+D+E
2024-02-02 19:41:45,607 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 19:41:45,607 	Gloss Alignment :	         
2024-02-02 19:41:45,607 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 19:41:45,608 	Text Reference  :	everyone was ****** ** ** very happy by his victory
2024-02-02 19:41:45,608 	Text Hypothesis :	why      was always to be fit  and   do you -      
2024-02-02 19:41:45,608 	Text Alignment  :	S            I      I  I  S    S     S  S   S      
2024-02-02 19:41:45,608 ========================================================================================================================
2024-02-02 19:41:45,608 Logging Sequence: 73_79.00
2024-02-02 19:41:45,609 	Gloss Reference :	A B+C+D+E
2024-02-02 19:41:45,609 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 19:41:45,609 	Gloss Alignment :	         
2024-02-02 19:41:45,609 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 19:41:45,610 	Text Reference  :	raina resturant has food from the rich spices of   north india to   the aromatic curries of     south india
2024-02-02 19:41:45,610 	Text Hypothesis :	***** ********* *** **** **** he  was  pm     modi ji    had   said she was      very    humble and   down 
2024-02-02 19:41:45,611 	Text Alignment  :	D     D         D   D    D    S   S    S      S    S     S     S    S   S        S       S      S     S    
2024-02-02 19:41:45,611 ========================================================================================================================
2024-02-02 19:41:45,611 Logging Sequence: 95_152.00
2024-02-02 19:41:45,611 	Gloss Reference :	A B+C+D+E
2024-02-02 19:41:45,611 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 19:41:45,611 	Gloss Alignment :	         
2024-02-02 19:41:45,611 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 19:41:45,612 	Text Reference  :	******** how strange
2024-02-02 19:41:45,612 	Text Hypothesis :	everyone is  excited
2024-02-02 19:41:45,612 	Text Alignment  :	I        S   S      
2024-02-02 19:41:45,612 ========================================================================================================================
2024-02-02 19:41:45,612 Logging Sequence: 135_39.00
2024-02-02 19:41:45,612 	Gloss Reference :	A B+C+D+E
2024-02-02 19:41:45,612 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 19:41:45,612 	Gloss Alignment :	         
2024-02-02 19:41:45,613 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 19:41:45,613 	Text Reference  :	who needs to ** * ****** travel from poland  to    stanford university in    california
2024-02-02 19:41:45,614 	Text Hypothesis :	and needs to be a silver medal  in   javelin throw at       the        tokyo olympics  
2024-02-02 19:41:45,614 	Text Alignment  :	S            I  I I      S      S    S       S     S        S          S     S         
2024-02-02 19:41:45,614 ========================================================================================================================
2024-02-02 19:41:45,614 Logging Sequence: 87_2.00
2024-02-02 19:41:45,614 	Gloss Reference :	A B+C+D+E
2024-02-02 19:41:45,614 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 19:41:45,614 	Gloss Alignment :	         
2024-02-02 19:41:45,614 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 19:41:45,616 	Text Reference  :	cricketer gautam gambhir's jealousy against ms  dhoni and      virat kohli has       been increasing day     by  day       
2024-02-02 19:41:45,616 	Text Hypothesis :	********* ****** however   due      to      the covid pandemic they  were  postponed and  started    without any spectators
2024-02-02 19:41:45,616 	Text Alignment  :	D         D      S         S        S       S   S     S        S     S     S         S    S          S       S   S         
2024-02-02 19:41:45,616 ========================================================================================================================
2024-02-02 19:41:45,959 Epoch 1353: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-02 19:41:45,959 EPOCH 1354
2024-02-02 19:41:50,862 Epoch 1354: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-02 19:41:50,863 EPOCH 1355
2024-02-02 19:41:55,628 Epoch 1355: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.91 
2024-02-02 19:41:55,629 EPOCH 1356
2024-02-02 19:41:59,703 [Epoch: 1356 Step: 00046100] Batch Recognition Loss:   0.000154 => Gls Tokens per Sec:     2296 || Batch Translation Loss:   0.019369 => Txt Tokens per Sec:     6536 || Lr: 0.000050
2024-02-02 19:42:00,063 Epoch 1356: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.43 
2024-02-02 19:42:00,063 EPOCH 1357
2024-02-02 19:42:04,717 Epoch 1357: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.05 
2024-02-02 19:42:04,717 EPOCH 1358
2024-02-02 19:42:09,117 Epoch 1358: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.89 
2024-02-02 19:42:09,118 EPOCH 1359
2024-02-02 19:42:12,987 [Epoch: 1359 Step: 00046200] Batch Recognition Loss:   0.000217 => Gls Tokens per Sec:     2252 || Batch Translation Loss:   0.022061 => Txt Tokens per Sec:     6219 || Lr: 0.000050
2024-02-02 19:42:13,857 Epoch 1359: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.84 
2024-02-02 19:42:13,857 EPOCH 1360
2024-02-02 19:42:18,242 Epoch 1360: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.16 
2024-02-02 19:42:18,242 EPOCH 1361
2024-02-02 19:42:23,014 Epoch 1361: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-02 19:42:23,014 EPOCH 1362
2024-02-02 19:42:26,591 [Epoch: 1362 Step: 00046300] Batch Recognition Loss:   0.000128 => Gls Tokens per Sec:     2327 || Batch Translation Loss:   0.010860 => Txt Tokens per Sec:     6616 || Lr: 0.000050
2024-02-02 19:42:27,390 Epoch 1362: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-02 19:42:27,390 EPOCH 1363
2024-02-02 19:42:32,091 Epoch 1363: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.80 
2024-02-02 19:42:32,091 EPOCH 1364
2024-02-02 19:42:36,504 Epoch 1364: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.90 
2024-02-02 19:42:36,504 EPOCH 1365
2024-02-02 19:42:39,287 [Epoch: 1365 Step: 00046400] Batch Recognition Loss:   0.000199 => Gls Tokens per Sec:     2672 || Batch Translation Loss:   0.018890 => Txt Tokens per Sec:     7418 || Lr: 0.000050
2024-02-02 19:42:40,625 Epoch 1365: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-02 19:42:40,625 EPOCH 1366
2024-02-02 19:42:45,525 Epoch 1366: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.76 
2024-02-02 19:42:45,526 EPOCH 1367
2024-02-02 19:42:50,317 Epoch 1367: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-02 19:42:50,317 EPOCH 1368
2024-02-02 19:42:53,454 [Epoch: 1368 Step: 00046500] Batch Recognition Loss:   0.000201 => Gls Tokens per Sec:     2245 || Batch Translation Loss:   0.026727 => Txt Tokens per Sec:     6523 || Lr: 0.000050
2024-02-02 19:42:54,671 Epoch 1368: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.87 
2024-02-02 19:42:54,671 EPOCH 1369
2024-02-02 19:42:59,539 Epoch 1369: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.15 
2024-02-02 19:42:59,540 EPOCH 1370
2024-02-02 19:43:03,887 Epoch 1370: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.03 
2024-02-02 19:43:03,887 EPOCH 1371
2024-02-02 19:43:06,759 [Epoch: 1371 Step: 00046600] Batch Recognition Loss:   0.000144 => Gls Tokens per Sec:     2143 || Batch Translation Loss:   0.034687 => Txt Tokens per Sec:     5834 || Lr: 0.000050
2024-02-02 19:43:08,740 Epoch 1371: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.17 
2024-02-02 19:43:08,741 EPOCH 1372
2024-02-02 19:43:12,937 Epoch 1372: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.31 
2024-02-02 19:43:12,937 EPOCH 1373
2024-02-02 19:43:17,802 Epoch 1373: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.01 
2024-02-02 19:43:17,803 EPOCH 1374
2024-02-02 19:43:19,695 [Epoch: 1374 Step: 00046700] Batch Recognition Loss:   0.000173 => Gls Tokens per Sec:     2915 || Batch Translation Loss:   0.015929 => Txt Tokens per Sec:     7929 || Lr: 0.000050
2024-02-02 19:43:21,892 Epoch 1374: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-02 19:43:21,893 EPOCH 1375
2024-02-02 19:43:26,444 Epoch 1375: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-02 19:43:26,445 EPOCH 1376
2024-02-02 19:43:31,086 Epoch 1376: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-02 19:43:31,087 EPOCH 1377
2024-02-02 19:43:33,667 [Epoch: 1377 Step: 00046800] Batch Recognition Loss:   0.000115 => Gls Tokens per Sec:     1985 || Batch Translation Loss:   0.013854 => Txt Tokens per Sec:     5566 || Lr: 0.000050
2024-02-02 19:43:35,908 Epoch 1377: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-02 19:43:35,908 EPOCH 1378
2024-02-02 19:43:40,124 Epoch 1378: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.82 
2024-02-02 19:43:40,125 EPOCH 1379
2024-02-02 19:43:45,006 Epoch 1379: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.43 
2024-02-02 19:43:45,006 EPOCH 1380
2024-02-02 19:43:46,657 [Epoch: 1380 Step: 00046900] Batch Recognition Loss:   0.000272 => Gls Tokens per Sec:     2564 || Batch Translation Loss:   0.023365 => Txt Tokens per Sec:     7157 || Lr: 0.000050
2024-02-02 19:43:49,271 Epoch 1380: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.33 
2024-02-02 19:43:49,272 EPOCH 1381
2024-02-02 19:43:54,096 Epoch 1381: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.05 
2024-02-02 19:43:54,096 EPOCH 1382
2024-02-02 19:43:58,370 Epoch 1382: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.94 
2024-02-02 19:43:58,370 EPOCH 1383
2024-02-02 19:44:00,122 [Epoch: 1383 Step: 00047000] Batch Recognition Loss:   0.000285 => Gls Tokens per Sec:     2194 || Batch Translation Loss:   0.024462 => Txt Tokens per Sec:     6115 || Lr: 0.000050
2024-02-02 19:44:03,203 Epoch 1383: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.84 
2024-02-02 19:44:03,204 EPOCH 1384
2024-02-02 19:44:07,434 Epoch 1384: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.75 
2024-02-02 19:44:07,435 EPOCH 1385
2024-02-02 19:44:12,238 Epoch 1385: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-02 19:44:12,239 EPOCH 1386
2024-02-02 19:44:13,309 [Epoch: 1386 Step: 00047100] Batch Recognition Loss:   0.000172 => Gls Tokens per Sec:     2988 || Batch Translation Loss:   0.018161 => Txt Tokens per Sec:     7916 || Lr: 0.000050
2024-02-02 19:44:16,416 Epoch 1386: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-02 19:44:16,417 EPOCH 1387
2024-02-02 19:44:21,273 Epoch 1387: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-02 19:44:21,274 EPOCH 1388
2024-02-02 19:44:25,552 Epoch 1388: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-02 19:44:25,552 EPOCH 1389
2024-02-02 19:44:26,829 [Epoch: 1389 Step: 00047200] Batch Recognition Loss:   0.000177 => Gls Tokens per Sec:     1809 || Batch Translation Loss:   0.018339 => Txt Tokens per Sec:     5053 || Lr: 0.000050
2024-02-02 19:44:30,359 Epoch 1389: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-02 19:44:30,359 EPOCH 1390
2024-02-02 19:44:34,846 Epoch 1390: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-02 19:44:34,847 EPOCH 1391
2024-02-02 19:44:39,480 Epoch 1391: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-02 19:44:39,481 EPOCH 1392
2024-02-02 19:44:40,061 [Epoch: 1392 Step: 00047300] Batch Recognition Loss:   0.000202 => Gls Tokens per Sec:     3314 || Batch Translation Loss:   0.015346 => Txt Tokens per Sec:     8701 || Lr: 0.000050
2024-02-02 19:44:44,090 Epoch 1392: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.77 
2024-02-02 19:44:44,090 EPOCH 1393
2024-02-02 19:44:48,657 Epoch 1393: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.73 
2024-02-02 19:44:48,657 EPOCH 1394
2024-02-02 19:44:52,753 Epoch 1394: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-02 19:44:52,754 EPOCH 1395
2024-02-02 19:44:53,348 [Epoch: 1395 Step: 00047400] Batch Recognition Loss:   0.000192 => Gls Tokens per Sec:     2157 || Batch Translation Loss:   0.020089 => Txt Tokens per Sec:     6529 || Lr: 0.000050
2024-02-02 19:44:57,578 Epoch 1395: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-02 19:44:57,578 EPOCH 1396
2024-02-02 19:45:01,835 Epoch 1396: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-02 19:45:01,835 EPOCH 1397
2024-02-02 19:45:06,770 Epoch 1397: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-02 19:45:06,770 EPOCH 1398
2024-02-02 19:45:07,264 [Epoch: 1398 Step: 00047500] Batch Recognition Loss:   0.000181 => Gls Tokens per Sec:     1297 || Batch Translation Loss:   0.021281 => Txt Tokens per Sec:     3936 || Lr: 0.000050
2024-02-02 19:45:11,007 Epoch 1398: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-02 19:45:11,007 EPOCH 1399
2024-02-02 19:45:15,971 Epoch 1399: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.82 
2024-02-02 19:45:15,971 EPOCH 1400
2024-02-02 19:45:20,104 [Epoch: 1400 Step: 00047600] Batch Recognition Loss:   0.000273 => Gls Tokens per Sec:     2573 || Batch Translation Loss:   0.037296 => Txt Tokens per Sec:     7142 || Lr: 0.000050
2024-02-02 19:45:20,105 Epoch 1400: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.02 
2024-02-02 19:45:20,106 EPOCH 1401
2024-02-02 19:45:25,069 Epoch 1401: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.56 
2024-02-02 19:45:25,069 EPOCH 1402
2024-02-02 19:45:29,247 Epoch 1402: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.97 
2024-02-02 19:45:29,247 EPOCH 1403
2024-02-02 19:45:33,176 [Epoch: 1403 Step: 00047700] Batch Recognition Loss:   0.000408 => Gls Tokens per Sec:     2544 || Batch Translation Loss:   0.045453 => Txt Tokens per Sec:     7122 || Lr: 0.000050
2024-02-02 19:45:33,341 Epoch 1403: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.26 
2024-02-02 19:45:33,341 EPOCH 1404
2024-02-02 19:45:37,799 Epoch 1404: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.35 
2024-02-02 19:45:37,799 EPOCH 1405
2024-02-02 19:45:42,436 Epoch 1405: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.41 
2024-02-02 19:45:42,436 EPOCH 1406
2024-02-02 19:45:46,770 [Epoch: 1406 Step: 00047800] Batch Recognition Loss:   0.000116 => Gls Tokens per Sec:     2158 || Batch Translation Loss:   0.024416 => Txt Tokens per Sec:     6083 || Lr: 0.000050
2024-02-02 19:45:47,181 Epoch 1406: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.18 
2024-02-02 19:45:47,181 EPOCH 1407
2024-02-02 19:45:51,549 Epoch 1407: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.97 
2024-02-02 19:45:51,549 EPOCH 1408
2024-02-02 19:45:56,407 Epoch 1408: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.73 
2024-02-02 19:45:56,408 EPOCH 1409
2024-02-02 19:45:59,715 [Epoch: 1409 Step: 00047900] Batch Recognition Loss:   0.000206 => Gls Tokens per Sec:     2635 || Batch Translation Loss:   0.024086 => Txt Tokens per Sec:     7229 || Lr: 0.000050
2024-02-02 19:46:00,641 Epoch 1409: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-02 19:46:00,641 EPOCH 1410
2024-02-02 19:46:05,442 Epoch 1410: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-02 19:46:05,443 EPOCH 1411
2024-02-02 19:46:09,702 Epoch 1411: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-02 19:46:09,702 EPOCH 1412
2024-02-02 19:46:13,360 [Epoch: 1412 Step: 00048000] Batch Recognition Loss:   0.000115 => Gls Tokens per Sec:     2207 || Batch Translation Loss:   0.013084 => Txt Tokens per Sec:     6129 || Lr: 0.000050
2024-02-02 19:46:21,885 Validation result at epoch 1412, step    48000: duration: 8.5234s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00063	Translation Loss: 95194.59375	PPL: 13713.38770
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.83	(BLEU-1: 10.69,	BLEU-2: 3.35,	BLEU-3: 1.55,	BLEU-4: 0.83)
	CHRF 16.94	ROUGE 9.32
2024-02-02 19:46:21,886 Logging Recognition and Translation Outputs
2024-02-02 19:46:21,886 ========================================================================================================================
2024-02-02 19:46:21,886 Logging Sequence: 88_159.00
2024-02-02 19:46:21,886 	Gloss Reference :	A B+C+D+E
2024-02-02 19:46:21,886 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 19:46:21,887 	Gloss Alignment :	         
2024-02-02 19:46:21,887 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 19:46:21,888 	Text Reference  :	**** ** *** however he       often     comes to        the town to  meet   his relatives
2024-02-02 19:46:21,888 	Text Hypothesis :	that is the most    followed sportsman on    instagram and rcb  has become the police   
2024-02-02 19:46:21,888 	Text Alignment  :	I    I  I   S       S        S         S     S         S   S    S   S      S   S        
2024-02-02 19:46:21,889 ========================================================================================================================
2024-02-02 19:46:21,889 Logging Sequence: 180_53.00
2024-02-02 19:46:21,889 	Gloss Reference :	A B+C+D+E
2024-02-02 19:46:21,889 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 19:46:21,889 	Gloss Alignment :	         
2024-02-02 19:46:21,889 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 19:46:21,890 	Text Reference  :	**** ** **** the protest is    against singh again  
2024-02-02 19:46:21,890 	Text Hypothesis :	this is also the ******* first time    that  cricket
2024-02-02 19:46:21,890 	Text Alignment  :	I    I  I        D       S     S       S     S      
2024-02-02 19:46:21,890 ========================================================================================================================
2024-02-02 19:46:21,890 Logging Sequence: 163_30.00
2024-02-02 19:46:21,890 	Gloss Reference :	A B+C+D+E
2024-02-02 19:46:21,890 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 19:46:21,891 	Gloss Alignment :	         
2024-02-02 19:46:21,891 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 19:46:21,891 	Text Reference  :	they never permitted anyone    to  reveal her face  
2024-02-02 19:46:21,891 	Text Hypothesis :	the  group then      announced for the    8   crores
2024-02-02 19:46:21,891 	Text Alignment  :	S    S     S         S         S   S      S   S     
2024-02-02 19:46:21,892 ========================================================================================================================
2024-02-02 19:46:21,892 Logging Sequence: 51_110.00
2024-02-02 19:46:21,892 	Gloss Reference :	A B+C+D+E
2024-02-02 19:46:21,892 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 19:46:21,892 	Gloss Alignment :	         
2024-02-02 19:46:21,892 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 19:46:21,893 	Text Reference  :	the aussies were very happy with their     victory
2024-02-02 19:46:21,893 	Text Hypothesis :	*** ******* **** he   is    a    fantastic diver  
2024-02-02 19:46:21,893 	Text Alignment  :	D   D       D    S    S     S    S         S      
2024-02-02 19:46:21,893 ========================================================================================================================
2024-02-02 19:46:21,893 Logging Sequence: 70_249.00
2024-02-02 19:46:21,893 	Gloss Reference :	A B+C+D+E
2024-02-02 19:46:21,893 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 19:46:21,894 	Gloss Alignment :	         
2024-02-02 19:46:21,894 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 19:46:21,894 	Text Reference  :	** *** *** *** ***** *** have a    look at        this video
2024-02-02 19:46:21,894 	Text Hypothesis :	on 5th may the court and met  with top  ministers of   ipl  
2024-02-02 19:46:21,894 	Text Alignment  :	I  I   I   I   I     I   S    S    S    S         S    S    
2024-02-02 19:46:21,895 ========================================================================================================================
2024-02-02 19:46:23,078 Epoch 1412: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.72 
2024-02-02 19:46:23,079 EPOCH 1413
2024-02-02 19:46:27,684 Epoch 1413: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.75 
2024-02-02 19:46:27,684 EPOCH 1414
2024-02-02 19:46:32,501 Epoch 1414: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-02 19:46:32,502 EPOCH 1415
2024-02-02 19:46:35,577 [Epoch: 1415 Step: 00048100] Batch Recognition Loss:   0.000138 => Gls Tokens per Sec:     2497 || Batch Translation Loss:   0.058103 => Txt Tokens per Sec:     6983 || Lr: 0.000050
2024-02-02 19:46:36,772 Epoch 1415: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.70 
2024-02-02 19:46:36,773 EPOCH 1416
2024-02-02 19:46:41,824 Epoch 1416: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.76 
2024-02-02 19:46:41,825 EPOCH 1417
2024-02-02 19:46:46,495 Epoch 1417: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-02 19:46:46,495 EPOCH 1418
2024-02-02 19:46:49,954 [Epoch: 1418 Step: 00048200] Batch Recognition Loss:   0.000167 => Gls Tokens per Sec:     1963 || Batch Translation Loss:   0.015588 => Txt Tokens per Sec:     5555 || Lr: 0.000050
2024-02-02 19:46:51,350 Epoch 1418: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.93 
2024-02-02 19:46:51,350 EPOCH 1419
2024-02-02 19:46:55,924 Epoch 1419: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.04 
2024-02-02 19:46:55,925 EPOCH 1420
2024-02-02 19:47:00,398 Epoch 1420: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.90 
2024-02-02 19:47:00,398 EPOCH 1421
2024-02-02 19:47:02,937 [Epoch: 1421 Step: 00048300] Batch Recognition Loss:   0.000180 => Gls Tokens per Sec:     2423 || Batch Translation Loss:   0.052889 => Txt Tokens per Sec:     6850 || Lr: 0.000050
2024-02-02 19:47:05,074 Epoch 1421: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.35 
2024-02-02 19:47:05,074 EPOCH 1422
2024-02-02 19:47:09,466 Epoch 1422: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.54 
2024-02-02 19:47:09,466 EPOCH 1423
2024-02-02 19:47:14,113 Epoch 1423: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.12 
2024-02-02 19:47:14,113 EPOCH 1424
2024-02-02 19:47:16,644 [Epoch: 1424 Step: 00048400] Batch Recognition Loss:   0.000351 => Gls Tokens per Sec:     2178 || Batch Translation Loss:   0.034732 => Txt Tokens per Sec:     6037 || Lr: 0.000050
2024-02-02 19:47:18,491 Epoch 1424: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.83 
2024-02-02 19:47:18,492 EPOCH 1425
2024-02-02 19:47:22,515 Epoch 1425: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-02 19:47:22,515 EPOCH 1426
2024-02-02 19:47:27,296 Epoch 1426: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.86 
2024-02-02 19:47:27,296 EPOCH 1427
2024-02-02 19:47:29,187 [Epoch: 1427 Step: 00048500] Batch Recognition Loss:   0.000231 => Gls Tokens per Sec:     2710 || Batch Translation Loss:   0.026148 => Txt Tokens per Sec:     7356 || Lr: 0.000050
2024-02-02 19:47:31,582 Epoch 1427: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.94 
2024-02-02 19:47:31,583 EPOCH 1428
2024-02-02 19:47:36,544 Epoch 1428: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.88 
2024-02-02 19:47:36,545 EPOCH 1429
2024-02-02 19:47:41,149 Epoch 1429: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.81 
2024-02-02 19:47:41,149 EPOCH 1430
2024-02-02 19:47:43,518 [Epoch: 1430 Step: 00048600] Batch Recognition Loss:   0.000165 => Gls Tokens per Sec:     1893 || Batch Translation Loss:   0.025707 => Txt Tokens per Sec:     5361 || Lr: 0.000050
2024-02-02 19:47:46,058 Epoch 1430: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.56 
2024-02-02 19:47:46,059 EPOCH 1431
2024-02-02 19:47:50,204 Epoch 1431: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.53 
2024-02-02 19:47:50,204 EPOCH 1432
2024-02-02 19:47:54,747 Epoch 1432: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.99 
2024-02-02 19:47:54,747 EPOCH 1433
2024-02-02 19:47:56,572 [Epoch: 1433 Step: 00048700] Batch Recognition Loss:   0.000231 => Gls Tokens per Sec:     2106 || Batch Translation Loss:   0.046475 => Txt Tokens per Sec:     5850 || Lr: 0.000050
2024-02-02 19:47:59,317 Epoch 1433: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.85 
2024-02-02 19:47:59,318 EPOCH 1434
2024-02-02 19:48:03,583 Epoch 1434: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.44 
2024-02-02 19:48:03,584 EPOCH 1435
2024-02-02 19:48:08,193 Epoch 1435: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.89 
2024-02-02 19:48:08,193 EPOCH 1436
2024-02-02 19:48:09,331 [Epoch: 1436 Step: 00048800] Batch Recognition Loss:   0.000157 => Gls Tokens per Sec:     2814 || Batch Translation Loss:   0.016634 => Txt Tokens per Sec:     7807 || Lr: 0.000050
2024-02-02 19:48:12,211 Epoch 1436: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.81 
2024-02-02 19:48:12,211 EPOCH 1437
2024-02-02 19:48:16,267 Epoch 1437: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.95 
2024-02-02 19:48:16,268 EPOCH 1438
2024-02-02 19:48:20,279 Epoch 1438: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.34 
2024-02-02 19:48:20,279 EPOCH 1439
2024-02-02 19:48:20,952 [Epoch: 1439 Step: 00048900] Batch Recognition Loss:   0.000189 => Gls Tokens per Sec:     3809 || Batch Translation Loss:   0.011475 => Txt Tokens per Sec:     8559 || Lr: 0.000050
2024-02-02 19:48:24,857 Epoch 1439: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.99 
2024-02-02 19:48:24,858 EPOCH 1440
2024-02-02 19:48:29,354 Epoch 1440: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.93 
2024-02-02 19:48:29,354 EPOCH 1441
2024-02-02 19:48:33,979 Epoch 1441: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-02 19:48:33,980 EPOCH 1442
2024-02-02 19:48:34,831 [Epoch: 1442 Step: 00049000] Batch Recognition Loss:   0.000162 => Gls Tokens per Sec:     1965 || Batch Translation Loss:   0.017162 => Txt Tokens per Sec:     5344 || Lr: 0.000050
2024-02-02 19:48:38,862 Epoch 1442: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-02 19:48:38,862 EPOCH 1443
2024-02-02 19:48:43,655 Epoch 1443: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-02 19:48:43,656 EPOCH 1444
2024-02-02 19:48:48,446 Epoch 1444: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-02 19:48:48,447 EPOCH 1445
2024-02-02 19:48:48,973 [Epoch: 1445 Step: 00049100] Batch Recognition Loss:   0.000189 => Gls Tokens per Sec:     1968 || Batch Translation Loss:   0.016790 => Txt Tokens per Sec:     5605 || Lr: 0.000050
2024-02-02 19:48:53,411 Epoch 1445: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.81 
2024-02-02 19:48:53,412 EPOCH 1446
2024-02-02 19:48:58,088 Epoch 1446: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-02 19:48:58,088 EPOCH 1447
2024-02-02 19:49:02,962 Epoch 1447: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-02 19:49:02,962 EPOCH 1448
2024-02-02 19:49:03,178 [Epoch: 1448 Step: 00049200] Batch Recognition Loss:   0.000178 => Gls Tokens per Sec:     1814 || Batch Translation Loss:   0.004597 => Txt Tokens per Sec:     4293 || Lr: 0.000050
2024-02-02 19:49:07,607 Epoch 1448: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.48 
2024-02-02 19:49:07,608 EPOCH 1449
2024-02-02 19:49:12,391 Epoch 1449: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-02 19:49:12,392 EPOCH 1450
2024-02-02 19:49:17,304 [Epoch: 1450 Step: 00049300] Batch Recognition Loss:   0.000103 => Gls Tokens per Sec:     2164 || Batch Translation Loss:   0.010922 => Txt Tokens per Sec:     6009 || Lr: 0.000050
2024-02-02 19:49:17,305 Epoch 1450: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-02 19:49:17,305 EPOCH 1451
2024-02-02 19:49:21,847 Epoch 1451: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-02 19:49:21,847 EPOCH 1452
2024-02-02 19:49:26,790 Epoch 1452: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.56 
2024-02-02 19:49:26,791 EPOCH 1453
2024-02-02 19:49:30,965 [Epoch: 1453 Step: 00049400] Batch Recognition Loss:   0.000187 => Gls Tokens per Sec:     2394 || Batch Translation Loss:   0.018753 => Txt Tokens per Sec:     6626 || Lr: 0.000050
2024-02-02 19:49:31,350 Epoch 1453: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-02 19:49:31,350 EPOCH 1454
2024-02-02 19:49:36,348 Epoch 1454: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-02 19:49:36,349 EPOCH 1455
2024-02-02 19:49:40,798 Epoch 1455: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.25 
2024-02-02 19:49:40,798 EPOCH 1456
2024-02-02 19:49:44,232 [Epoch: 1456 Step: 00049500] Batch Recognition Loss:   0.000210 => Gls Tokens per Sec:     2723 || Batch Translation Loss:   0.046144 => Txt Tokens per Sec:     7549 || Lr: 0.000050
2024-02-02 19:49:44,860 Epoch 1456: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.86 
2024-02-02 19:49:44,861 EPOCH 1457
2024-02-02 19:49:49,499 Epoch 1457: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.87 
2024-02-02 19:49:49,499 EPOCH 1458
2024-02-02 19:49:54,211 Epoch 1458: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.03 
2024-02-02 19:49:54,211 EPOCH 1459
2024-02-02 19:49:58,151 [Epoch: 1459 Step: 00049600] Batch Recognition Loss:   0.000218 => Gls Tokens per Sec:     2211 || Batch Translation Loss:   0.017606 => Txt Tokens per Sec:     6227 || Lr: 0.000050
2024-02-02 19:49:58,858 Epoch 1459: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.34 
2024-02-02 19:49:58,858 EPOCH 1460
2024-02-02 19:50:03,361 Epoch 1460: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.40 
2024-02-02 19:50:03,362 EPOCH 1461
2024-02-02 19:50:07,891 Epoch 1461: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.67 
2024-02-02 19:50:07,891 EPOCH 1462
2024-02-02 19:50:11,706 [Epoch: 1462 Step: 00049700] Batch Recognition Loss:   0.000296 => Gls Tokens per Sec:     2182 || Batch Translation Loss:   0.045515 => Txt Tokens per Sec:     6174 || Lr: 0.000050
2024-02-02 19:50:12,593 Epoch 1462: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.80 
2024-02-02 19:50:12,593 EPOCH 1463
2024-02-02 19:50:16,990 Epoch 1463: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.38 
2024-02-02 19:50:16,990 EPOCH 1464
2024-02-02 19:50:21,697 Epoch 1464: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.86 
2024-02-02 19:50:21,698 EPOCH 1465
2024-02-02 19:50:24,667 [Epoch: 1465 Step: 00049800] Batch Recognition Loss:   0.000540 => Gls Tokens per Sec:     2588 || Batch Translation Loss:   0.041233 => Txt Tokens per Sec:     7092 || Lr: 0.000050
2024-02-02 19:50:26,145 Epoch 1465: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.50 
2024-02-02 19:50:26,146 EPOCH 1466
2024-02-02 19:50:30,193 Epoch 1466: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.02 
2024-02-02 19:50:30,193 EPOCH 1467
2024-02-02 19:50:34,255 Epoch 1467: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.72 
2024-02-02 19:50:34,255 EPOCH 1468
2024-02-02 19:50:36,539 [Epoch: 1468 Step: 00049900] Batch Recognition Loss:   0.000170 => Gls Tokens per Sec:     2975 || Batch Translation Loss:   0.015732 => Txt Tokens per Sec:     7717 || Lr: 0.000050
2024-02-02 19:50:38,697 Epoch 1468: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-02 19:50:38,698 EPOCH 1469
2024-02-02 19:50:43,305 Epoch 1469: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.73 
2024-02-02 19:50:43,306 EPOCH 1470
2024-02-02 19:50:47,795 Epoch 1470: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-02 19:50:47,795 EPOCH 1471
2024-02-02 19:50:50,257 [Epoch: 1471 Step: 00050000] Batch Recognition Loss:   0.000098 => Gls Tokens per Sec:     2601 || Batch Translation Loss:   0.010488 => Txt Tokens per Sec:     7008 || Lr: 0.000050
2024-02-02 19:50:58,735 Validation result at epoch 1471, step    50000: duration: 8.4780s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00071	Translation Loss: 95595.14844	PPL: 14274.23828
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.78	(BLEU-1: 10.31,	BLEU-2: 3.35,	BLEU-3: 1.47,	BLEU-4: 0.78)
	CHRF 16.67	ROUGE 9.05
2024-02-02 19:50:58,736 Logging Recognition and Translation Outputs
2024-02-02 19:50:58,736 ========================================================================================================================
2024-02-02 19:50:58,736 Logging Sequence: 59_58.00
2024-02-02 19:50:58,736 	Gloss Reference :	A B+C+D+E
2024-02-02 19:50:58,737 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 19:50:58,737 	Gloss Alignment :	         
2024-02-02 19:50:58,737 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 19:50:58,738 	Text Reference  :	to fix the damage they  did not     have a lot   of *** ***** time  
2024-02-02 19:50:58,738 	Text Hypothesis :	** *** *** when   rahul was working as   a video of the wrong jersey
2024-02-02 19:50:58,738 	Text Alignment  :	D  D   D   S      S     S   S       S      S        I   I     S     
2024-02-02 19:50:58,739 ========================================================================================================================
2024-02-02 19:50:58,739 Logging Sequence: 165_2.00
2024-02-02 19:50:58,739 	Gloss Reference :	A B+C+D+E
2024-02-02 19:50:58,739 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 19:50:58,739 	Gloss Alignment :	         
2024-02-02 19:50:58,739 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 19:50:58,740 	Text Reference  :	many people believe in  superstitions and  think    it brings good luck and bad   luck
2024-02-02 19:50:58,741 	Text Hypothesis :	**** and    do      not know          that everyone is a      bag  ipl  to  india won 
2024-02-02 19:50:58,741 	Text Alignment  :	D    S      S       S   S             S    S        S  S      S    S    S   S     S   
2024-02-02 19:50:58,741 ========================================================================================================================
2024-02-02 19:50:58,741 Logging Sequence: 58_147.00
2024-02-02 19:50:58,741 	Gloss Reference :	A B+C+D+E
2024-02-02 19:50:58,741 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 19:50:58,741 	Gloss Alignment :	         
2024-02-02 19:50:58,741 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 19:50:58,742 	Text Reference  :	the women's cricket team grabbed gold by beating sri lanka in the  finals what a   historic win      
2024-02-02 19:50:58,743 	Text Hypothesis :	*** ******* ******* **** ******* **** ** ******* i   want  to tell you    what has been     postponed
2024-02-02 19:50:58,743 	Text Alignment  :	D   D       D       D    D       D    D  D       S   S     S  S    S           S   S        S        
2024-02-02 19:50:58,743 ========================================================================================================================
2024-02-02 19:50:58,743 Logging Sequence: 81_139.00
2024-02-02 19:50:58,743 	Gloss Reference :	A B+C+D+E
2024-02-02 19:50:58,743 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 19:50:58,743 	Gloss Alignment :	         
2024-02-02 19:50:58,743 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 19:50:58,745 	Text Reference  :	in 2017 the case was filed first in delhi high    court by rhiti sports   management on       behalf of     dhoni       
2024-02-02 19:50:58,745 	Text Hypothesis :	** when the **** *** ***** ***** ** ***** supreme court ** ***** canceled the        builder' real   estate registration
2024-02-02 19:50:58,745 	Text Alignment  :	D  S        D    D   D     D     D  D     S             D  D     S        S          S        S      S      S           
2024-02-02 19:50:58,745 ========================================================================================================================
2024-02-02 19:50:58,745 Logging Sequence: 125_72.00
2024-02-02 19:50:58,745 	Gloss Reference :	A B+C+D+E
2024-02-02 19:50:58,745 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 19:50:58,745 	Gloss Alignment :	         
2024-02-02 19:50:58,746 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 19:50:58,747 	Text Reference  :	some      said the pakistani javelineer had milicious intentions of    tampering with the javelin out   of jealousy
2024-02-02 19:50:58,747 	Text Hypothesis :	yesterday on   the ********* ********** *** october   chennai    super kings     csk  are very    proud of india   
2024-02-02 19:50:58,747 	Text Alignment  :	S         S        D         D          D   S         S          S     S         S    S   S       S        S       
2024-02-02 19:50:58,747 ========================================================================================================================
2024-02-02 19:51:01,011 Epoch 1471: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-02 19:51:01,011 EPOCH 1472
2024-02-02 19:51:05,741 Epoch 1472: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.78 
2024-02-02 19:51:05,742 EPOCH 1473
2024-02-02 19:51:10,355 Epoch 1473: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.97 
2024-02-02 19:51:10,355 EPOCH 1474
2024-02-02 19:51:12,311 [Epoch: 1474 Step: 00050100] Batch Recognition Loss:   0.000136 => Gls Tokens per Sec:     2946 || Batch Translation Loss:   0.012090 => Txt Tokens per Sec:     7974 || Lr: 0.000050
2024-02-02 19:51:14,852 Epoch 1474: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.96 
2024-02-02 19:51:14,853 EPOCH 1475
2024-02-02 19:51:19,439 Epoch 1475: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.98 
2024-02-02 19:51:19,439 EPOCH 1476
2024-02-02 19:51:23,967 Epoch 1476: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.83 
2024-02-02 19:51:23,968 EPOCH 1477
2024-02-02 19:51:26,450 [Epoch: 1477 Step: 00050200] Batch Recognition Loss:   0.000131 => Gls Tokens per Sec:     1964 || Batch Translation Loss:   0.012648 => Txt Tokens per Sec:     5799 || Lr: 0.000050
2024-02-02 19:51:28,671 Epoch 1477: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.77 
2024-02-02 19:51:28,672 EPOCH 1478
2024-02-02 19:51:33,583 Epoch 1478: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-02 19:51:33,584 EPOCH 1479
2024-02-02 19:51:38,564 Epoch 1479: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.77 
2024-02-02 19:51:38,565 EPOCH 1480
2024-02-02 19:51:40,557 [Epoch: 1480 Step: 00050300] Batch Recognition Loss:   0.000142 => Gls Tokens per Sec:     2125 || Batch Translation Loss:   0.012223 => Txt Tokens per Sec:     6240 || Lr: 0.000050
2024-02-02 19:51:43,044 Epoch 1480: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.03 
2024-02-02 19:51:43,044 EPOCH 1481
2024-02-02 19:51:47,933 Epoch 1481: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.30 
2024-02-02 19:51:47,934 EPOCH 1482
2024-02-02 19:51:52,572 Epoch 1482: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.35 
2024-02-02 19:51:52,573 EPOCH 1483
2024-02-02 19:51:54,126 [Epoch: 1483 Step: 00050400] Batch Recognition Loss:   0.000139 => Gls Tokens per Sec:     2474 || Batch Translation Loss:   0.037986 => Txt Tokens per Sec:     6639 || Lr: 0.000050
2024-02-02 19:51:57,446 Epoch 1483: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.50 
2024-02-02 19:51:57,446 EPOCH 1484
2024-02-02 19:52:02,339 Epoch 1484: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.41 
2024-02-02 19:52:02,339 EPOCH 1485
2024-02-02 19:52:07,104 Epoch 1485: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.90 
2024-02-02 19:52:07,105 EPOCH 1486
2024-02-02 19:52:08,546 [Epoch: 1486 Step: 00050500] Batch Recognition Loss:   0.000178 => Gls Tokens per Sec:     2221 || Batch Translation Loss:   0.019110 => Txt Tokens per Sec:     6334 || Lr: 0.000050
2024-02-02 19:52:12,067 Epoch 1486: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.77 
2024-02-02 19:52:12,068 EPOCH 1487
2024-02-02 19:52:16,573 Epoch 1487: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-02 19:52:16,573 EPOCH 1488
2024-02-02 19:52:21,023 Epoch 1488: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-02 19:52:21,023 EPOCH 1489
2024-02-02 19:52:22,144 [Epoch: 1489 Step: 00050600] Batch Recognition Loss:   0.000164 => Gls Tokens per Sec:     2285 || Batch Translation Loss:   0.015724 => Txt Tokens per Sec:     6764 || Lr: 0.000050
2024-02-02 19:52:25,734 Epoch 1489: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-02 19:52:25,734 EPOCH 1490
2024-02-02 19:52:30,199 Epoch 1490: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-02 19:52:30,200 EPOCH 1491
2024-02-02 19:52:34,883 Epoch 1491: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-02 19:52:34,883 EPOCH 1492
2024-02-02 19:52:35,775 [Epoch: 1492 Step: 00050700] Batch Recognition Loss:   0.000183 => Gls Tokens per Sec:     2154 || Batch Translation Loss:   0.029934 => Txt Tokens per Sec:     6781 || Lr: 0.000050
2024-02-02 19:52:39,381 Epoch 1492: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.70 
2024-02-02 19:52:39,382 EPOCH 1493
2024-02-02 19:52:43,937 Epoch 1493: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.74 
2024-02-02 19:52:43,937 EPOCH 1494
2024-02-02 19:52:47,967 Epoch 1494: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.70 
2024-02-02 19:52:47,967 EPOCH 1495
2024-02-02 19:52:48,528 [Epoch: 1495 Step: 00050800] Batch Recognition Loss:   0.000166 => Gls Tokens per Sec:     2286 || Batch Translation Loss:   0.019036 => Txt Tokens per Sec:     7070 || Lr: 0.000050
2024-02-02 19:52:52,751 Epoch 1495: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.08 
2024-02-02 19:52:52,751 EPOCH 1496
2024-02-02 19:52:57,315 Epoch 1496: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.82 
2024-02-02 19:52:57,316 EPOCH 1497
2024-02-02 19:53:02,007 Epoch 1497: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.14 
2024-02-02 19:53:02,008 EPOCH 1498
2024-02-02 19:53:02,130 [Epoch: 1498 Step: 00050900] Batch Recognition Loss:   0.000138 => Gls Tokens per Sec:     5289 || Batch Translation Loss:   0.011190 => Txt Tokens per Sec:     9660 || Lr: 0.000050
2024-02-02 19:53:06,779 Epoch 1498: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.11 
2024-02-02 19:53:06,780 EPOCH 1499
2024-02-02 19:53:11,151 Epoch 1499: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.94 
2024-02-02 19:53:11,151 EPOCH 1500
2024-02-02 19:53:15,980 [Epoch: 1500 Step: 00051000] Batch Recognition Loss:   0.000270 => Gls Tokens per Sec:     2202 || Batch Translation Loss:   0.025531 => Txt Tokens per Sec:     6111 || Lr: 0.000050
2024-02-02 19:53:15,981 Epoch 1500: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.29 
2024-02-02 19:53:15,981 EPOCH 1501
2024-02-02 19:53:20,218 Epoch 1501: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.22 
2024-02-02 19:53:20,218 EPOCH 1502
2024-02-02 19:53:24,244 Epoch 1502: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.91 
2024-02-02 19:53:24,244 EPOCH 1503
2024-02-02 19:53:28,973 [Epoch: 1503 Step: 00051100] Batch Recognition Loss:   0.000227 => Gls Tokens per Sec:     2114 || Batch Translation Loss:   0.063098 => Txt Tokens per Sec:     5938 || Lr: 0.000050
2024-02-02 19:53:29,137 Epoch 1503: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.85 
2024-02-02 19:53:29,137 EPOCH 1504
2024-02-02 19:53:33,817 Epoch 1504: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-02 19:53:33,818 EPOCH 1505
2024-02-02 19:53:38,528 Epoch 1505: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-02 19:53:38,529 EPOCH 1506
2024-02-02 19:53:42,984 [Epoch: 1506 Step: 00051200] Batch Recognition Loss:   0.000149 => Gls Tokens per Sec:     2099 || Batch Translation Loss:   0.016869 => Txt Tokens per Sec:     5939 || Lr: 0.000050
2024-02-02 19:53:43,404 Epoch 1506: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.86 
2024-02-02 19:53:43,404 EPOCH 1507
2024-02-02 19:53:48,067 Epoch 1507: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-02 19:53:48,067 EPOCH 1508
2024-02-02 19:53:52,690 Epoch 1508: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.89 
2024-02-02 19:53:52,691 EPOCH 1509
2024-02-02 19:53:56,370 [Epoch: 1509 Step: 00051300] Batch Recognition Loss:   0.000244 => Gls Tokens per Sec:     2368 || Batch Translation Loss:   0.019405 => Txt Tokens per Sec:     6604 || Lr: 0.000050
2024-02-02 19:53:57,042 Epoch 1509: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-02 19:53:57,042 EPOCH 1510
2024-02-02 19:54:01,085 Epoch 1510: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.88 
2024-02-02 19:54:01,086 EPOCH 1511
2024-02-02 19:54:05,112 Epoch 1511: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.99 
2024-02-02 19:54:05,112 EPOCH 1512
2024-02-02 19:54:08,883 [Epoch: 1512 Step: 00051400] Batch Recognition Loss:   0.000274 => Gls Tokens per Sec:     2141 || Batch Translation Loss:   0.007536 => Txt Tokens per Sec:     5852 || Lr: 0.000050
2024-02-02 19:54:09,991 Epoch 1512: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.13 
2024-02-02 19:54:09,992 EPOCH 1513
2024-02-02 19:54:14,835 Epoch 1513: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.54 
2024-02-02 19:54:14,835 EPOCH 1514
2024-02-02 19:54:19,622 Epoch 1514: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.06 
2024-02-02 19:54:19,623 EPOCH 1515
2024-02-02 19:54:23,067 [Epoch: 1515 Step: 00051500] Batch Recognition Loss:   0.000295 => Gls Tokens per Sec:     2158 || Batch Translation Loss:   0.025952 => Txt Tokens per Sec:     6025 || Lr: 0.000050
2024-02-02 19:54:24,449 Epoch 1515: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.11 
2024-02-02 19:54:24,450 EPOCH 1516
2024-02-02 19:54:29,078 Epoch 1516: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.75 
2024-02-02 19:54:29,078 EPOCH 1517
2024-02-02 19:54:33,856 Epoch 1517: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-02 19:54:33,857 EPOCH 1518
2024-02-02 19:54:36,738 [Epoch: 1518 Step: 00051600] Batch Recognition Loss:   0.000162 => Gls Tokens per Sec:     2358 || Batch Translation Loss:   0.016874 => Txt Tokens per Sec:     6360 || Lr: 0.000050
2024-02-02 19:54:38,474 Epoch 1518: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-02 19:54:38,474 EPOCH 1519
2024-02-02 19:54:42,670 Epoch 1519: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-02 19:54:42,670 EPOCH 1520
2024-02-02 19:54:47,503 Epoch 1520: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-02 19:54:47,503 EPOCH 1521
2024-02-02 19:54:49,948 [Epoch: 1521 Step: 00051700] Batch Recognition Loss:   0.000222 => Gls Tokens per Sec:     2619 || Batch Translation Loss:   0.019141 => Txt Tokens per Sec:     6979 || Lr: 0.000050
2024-02-02 19:54:52,344 Epoch 1521: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-02 19:54:52,345 EPOCH 1522
2024-02-02 19:54:56,923 Epoch 1522: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-02 19:54:56,923 EPOCH 1523
2024-02-02 19:55:01,012 Epoch 1523: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-02 19:55:01,012 EPOCH 1524
2024-02-02 19:55:03,022 [Epoch: 1524 Step: 00051800] Batch Recognition Loss:   0.000157 => Gls Tokens per Sec:     2866 || Batch Translation Loss:   0.017527 => Txt Tokens per Sec:     7768 || Lr: 0.000050
2024-02-02 19:55:05,276 Epoch 1524: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-02 19:55:05,277 EPOCH 1525
2024-02-02 19:55:10,097 Epoch 1525: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-02 19:55:10,097 EPOCH 1526
2024-02-02 19:55:14,702 Epoch 1526: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-02 19:55:14,702 EPOCH 1527
2024-02-02 19:55:16,968 [Epoch: 1527 Step: 00051900] Batch Recognition Loss:   0.000137 => Gls Tokens per Sec:     2261 || Batch Translation Loss:   0.015573 => Txt Tokens per Sec:     6206 || Lr: 0.000050
2024-02-02 19:55:19,572 Epoch 1527: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.96 
2024-02-02 19:55:19,572 EPOCH 1528
2024-02-02 19:55:24,270 Epoch 1528: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.14 
2024-02-02 19:55:24,271 EPOCH 1529
2024-02-02 19:55:29,039 Epoch 1529: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.96 
2024-02-02 19:55:29,039 EPOCH 1530
2024-02-02 19:55:30,934 [Epoch: 1530 Step: 00052000] Batch Recognition Loss:   0.000249 => Gls Tokens per Sec:     2235 || Batch Translation Loss:   0.005278 => Txt Tokens per Sec:     6593 || Lr: 0.000050
2024-02-02 19:55:39,156 Validation result at epoch 1530, step    52000: duration: 8.2221s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00077	Translation Loss: 94726.99219	PPL: 13086.47949
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.75	(BLEU-1: 10.36,	BLEU-2: 3.32,	BLEU-3: 1.46,	BLEU-4: 0.75)
	CHRF 16.70	ROUGE 8.96
2024-02-02 19:55:39,158 Logging Recognition and Translation Outputs
2024-02-02 19:55:39,158 ========================================================================================================================
2024-02-02 19:55:39,158 Logging Sequence: 87_229.00
2024-02-02 19:55:39,159 	Gloss Reference :	A B+C+D+E
2024-02-02 19:55:39,159 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 19:55:39,159 	Gloss Alignment :	         
2024-02-02 19:55:39,159 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 19:55:39,160 	Text Reference  :	**** ******** ** ***** it was  not against dhoni or  kohli        
2024-02-02 19:55:39,160 	Text Hypothesis :	then returned to india he came so  much    did   not misunderstand
2024-02-02 19:55:39,160 	Text Alignment  :	I    I        I  I     S  S    S   S       S     S   S            
2024-02-02 19:55:39,160 ========================================================================================================================
2024-02-02 19:55:39,161 Logging Sequence: 134_153.00
2024-02-02 19:55:39,161 	Gloss Reference :	A B+C+D+E
2024-02-02 19:55:39,161 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 19:55:39,161 	Gloss Alignment :	         
2024-02-02 19:55:39,161 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 19:55:39,163 	Text Reference  :	pm modi in his interaction said that deaf athletes must fight for         their goals   and never give up  despite the       losses
2024-02-02 19:55:39,163 	Text Hypothesis :	** **** ** *** *********** **** **** **** ******** **** while interacting with  dhanush and will  help him a       practises yoga  
2024-02-02 19:55:39,163 	Text Alignment  :	D  D    D  D   D           D    D    D    D        D    S     S           S     S           S     S    S   S       S         S     
2024-02-02 19:55:39,163 ========================================================================================================================
2024-02-02 19:55:39,163 Logging Sequence: 137_155.00
2024-02-02 19:55:39,164 	Gloss Reference :	A B+C+D+E
2024-02-02 19:55:39,164 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 19:55:39,164 	Gloss Alignment :	         
2024-02-02 19:55:39,164 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 19:55:39,165 	Text Reference  :	** *** ******** **** an  extremely high tax named  as sin   tax will be applied
2024-02-02 19:55:39,165 	Text Hypothesis :	on 2nd november 2023 the team's    win  a   finger in qatar he  is   in 2022   
2024-02-02 19:55:39,165 	Text Alignment  :	I  I   I        I    S   S         S    S   S      S  S     S   S    S  S      
2024-02-02 19:55:39,166 ========================================================================================================================
2024-02-02 19:55:39,166 Logging Sequence: 59_18.00
2024-02-02 19:55:39,166 	Gloss Reference :	A B+C+D+E
2024-02-02 19:55:39,166 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 19:55:39,166 	Gloss Alignment :	         
2024-02-02 19:55:39,166 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 19:55:39,168 	Text Reference  :	27-year-old jessica fox from australia won     a bronze ***** a  gold medal    in ***** ****** *** ***** ******* ** *** *** canoeing
2024-02-02 19:55:39,168 	Text Hypothesis :	*********** ******* *** **** ********* however a bronze medal at the  olympics in tokyo handed out 60000 condoms to all the athletes
2024-02-02 19:55:39,168 	Text Alignment  :	D           D       D   D    D         S                I     S  S    S           I     I      I   I     I       I  I   I   S       
2024-02-02 19:55:39,168 ========================================================================================================================
2024-02-02 19:55:39,168 Logging Sequence: 173_103.00
2024-02-02 19:55:39,169 	Gloss Reference :	A B+C+D+E
2024-02-02 19:55:39,169 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 19:55:39,169 	Gloss Alignment :	         
2024-02-02 19:55:39,169 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 19:55:39,170 	Text Reference  :	*** **** **** *** these rumours are  absolutely rubbish
2024-02-02 19:55:39,170 	Text Hypothesis :	the bcci will not want  to      live the        team   
2024-02-02 19:55:39,170 	Text Alignment  :	I   I    I    I   S     S       S    S          S      
2024-02-02 19:55:39,170 ========================================================================================================================
2024-02-02 19:55:41,987 Epoch 1530: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.85 
2024-02-02 19:55:41,988 EPOCH 1531
2024-02-02 19:55:46,791 Epoch 1531: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.88 
2024-02-02 19:55:46,792 EPOCH 1532
2024-02-02 19:55:50,833 Epoch 1532: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.47 
2024-02-02 19:55:50,833 EPOCH 1533
2024-02-02 19:55:52,153 [Epoch: 1533 Step: 00052100] Batch Recognition Loss:   0.000173 => Gls Tokens per Sec:     2914 || Batch Translation Loss:   0.017454 => Txt Tokens per Sec:     7442 || Lr: 0.000050
2024-02-02 19:55:55,727 Epoch 1533: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.90 
2024-02-02 19:55:55,728 EPOCH 1534
2024-02-02 19:55:59,981 Epoch 1534: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.51 
2024-02-02 19:55:59,981 EPOCH 1535
2024-02-02 19:56:04,908 Epoch 1535: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.73 
2024-02-02 19:56:04,908 EPOCH 1536
2024-02-02 19:56:06,092 [Epoch: 1536 Step: 00052200] Batch Recognition Loss:   0.000113 => Gls Tokens per Sec:     2496 || Batch Translation Loss:   0.140068 => Txt Tokens per Sec:     6554 || Lr: 0.000050
2024-02-02 19:56:09,086 Epoch 1536: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.34 
2024-02-02 19:56:09,086 EPOCH 1537
2024-02-02 19:56:13,954 Epoch 1537: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.09 
2024-02-02 19:56:13,954 EPOCH 1538
2024-02-02 19:56:18,193 Epoch 1538: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.21 
2024-02-02 19:56:18,193 EPOCH 1539
2024-02-02 19:56:19,399 [Epoch: 1539 Step: 00052300] Batch Recognition Loss:   0.000172 => Gls Tokens per Sec:     1917 || Batch Translation Loss:   0.021122 => Txt Tokens per Sec:     5451 || Lr: 0.000050
2024-02-02 19:56:23,079 Epoch 1539: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.71 
2024-02-02 19:56:23,080 EPOCH 1540
2024-02-02 19:56:27,274 Epoch 1540: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.38 
2024-02-02 19:56:27,275 EPOCH 1541
2024-02-02 19:56:31,682 Epoch 1541: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.08 
2024-02-02 19:56:31,682 EPOCH 1542
2024-02-02 19:56:32,519 [Epoch: 1542 Step: 00052400] Batch Recognition Loss:   0.000255 => Gls Tokens per Sec:     2299 || Batch Translation Loss:   0.027814 => Txt Tokens per Sec:     6848 || Lr: 0.000050
2024-02-02 19:56:36,331 Epoch 1542: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.95 
2024-02-02 19:56:36,331 EPOCH 1543
2024-02-02 19:56:41,224 Epoch 1543: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.80 
2024-02-02 19:56:41,224 EPOCH 1544
2024-02-02 19:56:45,379 Epoch 1544: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.88 
2024-02-02 19:56:45,380 EPOCH 1545
2024-02-02 19:56:45,957 [Epoch: 1545 Step: 00052500] Batch Recognition Loss:   0.000283 => Gls Tokens per Sec:     2222 || Batch Translation Loss:   0.022296 => Txt Tokens per Sec:     6115 || Lr: 0.000050
2024-02-02 19:56:50,268 Epoch 1545: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.94 
2024-02-02 19:56:50,269 EPOCH 1546
2024-02-02 19:56:54,475 Epoch 1546: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.81 
2024-02-02 19:56:54,475 EPOCH 1547
2024-02-02 19:56:59,332 Epoch 1547: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.07 
2024-02-02 19:56:59,333 EPOCH 1548
2024-02-02 19:56:59,586 [Epoch: 1548 Step: 00052600] Batch Recognition Loss:   0.000228 => Gls Tokens per Sec:     2540 || Batch Translation Loss:   0.025246 => Txt Tokens per Sec:     7774 || Lr: 0.000050
2024-02-02 19:57:03,693 Epoch 1548: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-02 19:57:03,693 EPOCH 1549
2024-02-02 19:57:08,692 Epoch 1549: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.70 
2024-02-02 19:57:08,692 EPOCH 1550
2024-02-02 19:57:13,508 [Epoch: 1550 Step: 00052700] Batch Recognition Loss:   0.000173 => Gls Tokens per Sec:     2208 || Batch Translation Loss:   0.028859 => Txt Tokens per Sec:     6128 || Lr: 0.000050
2024-02-02 19:57:13,509 Epoch 1550: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-02 19:57:13,509 EPOCH 1551
2024-02-02 19:57:17,696 Epoch 1551: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.79 
2024-02-02 19:57:17,697 EPOCH 1552
2024-02-02 19:57:22,406 Epoch 1552: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-02 19:57:22,406 EPOCH 1553
2024-02-02 19:57:26,453 [Epoch: 1553 Step: 00052800] Batch Recognition Loss:   0.000147 => Gls Tokens per Sec:     2469 || Batch Translation Loss:   0.011395 => Txt Tokens per Sec:     6853 || Lr: 0.000050
2024-02-02 19:57:26,738 Epoch 1553: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-02 19:57:26,738 EPOCH 1554
2024-02-02 19:57:31,637 Epoch 1554: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-02 19:57:31,637 EPOCH 1555
2024-02-02 19:57:35,800 Epoch 1555: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.78 
2024-02-02 19:57:35,800 EPOCH 1556
2024-02-02 19:57:39,425 [Epoch: 1556 Step: 00052900] Batch Recognition Loss:   0.000111 => Gls Tokens per Sec:     2580 || Batch Translation Loss:   0.010542 => Txt Tokens per Sec:     7181 || Lr: 0.000050
2024-02-02 19:57:39,825 Epoch 1556: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.04 
2024-02-02 19:57:39,825 EPOCH 1557
2024-02-02 19:57:43,910 Epoch 1557: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-02 19:57:43,910 EPOCH 1558
2024-02-02 19:57:48,634 Epoch 1558: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.79 
2024-02-02 19:57:48,634 EPOCH 1559
2024-02-02 19:57:52,339 [Epoch: 1559 Step: 00053000] Batch Recognition Loss:   0.000207 => Gls Tokens per Sec:     2351 || Batch Translation Loss:   0.151450 => Txt Tokens per Sec:     6471 || Lr: 0.000050
2024-02-02 19:57:53,287 Epoch 1559: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.89 
2024-02-02 19:57:53,288 EPOCH 1560
2024-02-02 19:57:58,035 Epoch 1560: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.97 
2024-02-02 19:57:58,036 EPOCH 1561
2024-02-02 19:58:02,941 Epoch 1561: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-02 19:58:02,942 EPOCH 1562
2024-02-02 19:58:06,443 [Epoch: 1562 Step: 00053100] Batch Recognition Loss:   0.000171 => Gls Tokens per Sec:     2306 || Batch Translation Loss:   0.020036 => Txt Tokens per Sec:     6475 || Lr: 0.000050
2024-02-02 19:58:07,489 Epoch 1562: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-02 19:58:07,490 EPOCH 1563
2024-02-02 19:58:12,375 Epoch 1563: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.91 
2024-02-02 19:58:12,376 EPOCH 1564
2024-02-02 19:58:16,886 Epoch 1564: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-02 19:58:16,886 EPOCH 1565
2024-02-02 19:58:20,120 [Epoch: 1565 Step: 00053200] Batch Recognition Loss:   0.000165 => Gls Tokens per Sec:     2298 || Batch Translation Loss:   0.016317 => Txt Tokens per Sec:     6282 || Lr: 0.000050
2024-02-02 19:58:21,746 Epoch 1565: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-02 19:58:21,746 EPOCH 1566
2024-02-02 19:58:26,246 Epoch 1566: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-02 19:58:26,246 EPOCH 1567
2024-02-02 19:58:31,090 Epoch 1567: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-02 19:58:31,091 EPOCH 1568
2024-02-02 19:58:33,585 [Epoch: 1568 Step: 00053300] Batch Recognition Loss:   0.000140 => Gls Tokens per Sec:     2724 || Batch Translation Loss:   0.013165 => Txt Tokens per Sec:     7547 || Lr: 0.000050
2024-02-02 19:58:35,553 Epoch 1568: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-02 19:58:35,554 EPOCH 1569
2024-02-02 19:58:40,367 Epoch 1569: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-02 19:58:40,367 EPOCH 1570
2024-02-02 19:58:44,487 Epoch 1570: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.59 
2024-02-02 19:58:44,487 EPOCH 1571
2024-02-02 19:58:47,307 [Epoch: 1571 Step: 00053400] Batch Recognition Loss:   0.000403 => Gls Tokens per Sec:     2183 || Batch Translation Loss:   0.042863 => Txt Tokens per Sec:     6033 || Lr: 0.000050
2024-02-02 19:58:49,329 Epoch 1571: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.00 
2024-02-02 19:58:49,330 EPOCH 1572
2024-02-02 19:58:53,610 Epoch 1572: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.69 
2024-02-02 19:58:53,610 EPOCH 1573
2024-02-02 19:58:58,418 Epoch 1573: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.14 
2024-02-02 19:58:58,419 EPOCH 1574
2024-02-02 19:59:00,355 [Epoch: 1574 Step: 00053500] Batch Recognition Loss:   0.000354 => Gls Tokens per Sec:     2977 || Batch Translation Loss:   0.021670 => Txt Tokens per Sec:     8076 || Lr: 0.000050
2024-02-02 19:59:02,645 Epoch 1574: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.90 
2024-02-02 19:59:02,645 EPOCH 1575
2024-02-02 19:59:07,456 Epoch 1575: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.84 
2024-02-02 19:59:07,456 EPOCH 1576
2024-02-02 19:59:12,056 Epoch 1576: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-02 19:59:12,057 EPOCH 1577
2024-02-02 19:59:14,667 [Epoch: 1577 Step: 00053600] Batch Recognition Loss:   0.000229 => Gls Tokens per Sec:     1963 || Batch Translation Loss:   0.019862 => Txt Tokens per Sec:     5562 || Lr: 0.000050
2024-02-02 19:59:16,938 Epoch 1577: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-02 19:59:16,938 EPOCH 1578
2024-02-02 19:59:21,510 Epoch 1578: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-02 19:59:21,511 EPOCH 1579
2024-02-02 19:59:26,365 Epoch 1579: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-02 19:59:26,365 EPOCH 1580
2024-02-02 19:59:28,418 [Epoch: 1580 Step: 00053700] Batch Recognition Loss:   0.000187 => Gls Tokens per Sec:     2063 || Batch Translation Loss:   0.012011 => Txt Tokens per Sec:     6148 || Lr: 0.000050
2024-02-02 19:59:31,069 Epoch 1580: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-02 19:59:31,069 EPOCH 1581
2024-02-02 19:59:35,835 Epoch 1581: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-02 19:59:35,835 EPOCH 1582
2024-02-02 19:59:40,643 Epoch 1582: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-02 19:59:40,644 EPOCH 1583
2024-02-02 19:59:42,087 [Epoch: 1583 Step: 00053800] Batch Recognition Loss:   0.000204 => Gls Tokens per Sec:     2489 || Batch Translation Loss:   0.023123 => Txt Tokens per Sec:     6441 || Lr: 0.000050
2024-02-02 19:59:45,284 Epoch 1583: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-02 19:59:45,284 EPOCH 1584
2024-02-02 19:59:50,105 Epoch 1584: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.84 
2024-02-02 19:59:50,105 EPOCH 1585
2024-02-02 19:59:54,711 Epoch 1585: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-02 19:59:54,711 EPOCH 1586
2024-02-02 19:59:55,906 [Epoch: 1586 Step: 00053900] Batch Recognition Loss:   0.000195 => Gls Tokens per Sec:     2681 || Batch Translation Loss:   0.023452 => Txt Tokens per Sec:     7585 || Lr: 0.000050
2024-02-02 19:59:58,937 Epoch 1586: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-02 19:59:58,937 EPOCH 1587
2024-02-02 20:00:03,755 Epoch 1587: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-02 20:00:03,755 EPOCH 1588
2024-02-02 20:00:08,015 Epoch 1588: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-02 20:00:08,016 EPOCH 1589
2024-02-02 20:00:09,301 [Epoch: 1589 Step: 00054000] Batch Recognition Loss:   0.000112 => Gls Tokens per Sec:     1994 || Batch Translation Loss:   0.007573 => Txt Tokens per Sec:     5760 || Lr: 0.000050
2024-02-02 20:00:17,907 Validation result at epoch 1589, step    54000: duration: 8.6060s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00078	Translation Loss: 95057.98438	PPL: 13527.18848
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.84	(BLEU-1: 10.67,	BLEU-2: 3.37,	BLEU-3: 1.57,	BLEU-4: 0.84)
	CHRF 17.26	ROUGE 9.14
2024-02-02 20:00:17,908 Logging Recognition and Translation Outputs
2024-02-02 20:00:17,909 ========================================================================================================================
2024-02-02 20:00:17,909 Logging Sequence: 130_139.00
2024-02-02 20:00:17,909 	Gloss Reference :	A B+C+D+E
2024-02-02 20:00:17,909 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 20:00:17,910 	Gloss Alignment :	         
2024-02-02 20:00:17,910 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 20:00:17,912 	Text Reference  :	he shared a picture of a little pouch he knit for his olympic gold medal with uk     flag on    one side and japanese flag on the other         
2024-02-02 20:00:17,912 	Text Hypothesis :	** ****** * ******* ** * ****** ***** he **** *** *** ******* **** ***** **** played 18   tests 226 odis and ******** **** 78 t20 internationals
2024-02-02 20:00:17,912 	Text Alignment  :	D  D      D D       D  D D      D        D    D   D   D       D    D     D    S      S    S     S   S        D        D    S  S   S             
2024-02-02 20:00:17,912 ========================================================================================================================
2024-02-02 20:00:17,913 Logging Sequence: 148_155.00
2024-02-02 20:00:17,913 	Gloss Reference :	A B+C+D+E
2024-02-02 20:00:17,913 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 20:00:17,913 	Gloss Alignment :	         
2024-02-02 20:00:17,913 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 20:00:17,914 	Text Reference  :	india won the match with 263 balls remaining and without losing  any   wicket  
2024-02-02 20:00:17,914 	Text Hypothesis :	india *** *** ***** did  not have  a         t20 match   against india pakistan
2024-02-02 20:00:17,914 	Text Alignment  :	      D   D   D     S    S   S     S         S   S       S       S     S       
2024-02-02 20:00:17,914 ========================================================================================================================
2024-02-02 20:00:17,915 Logging Sequence: 126_99.00
2024-02-02 20:00:17,915 	Gloss Reference :	A B+C+D+E
2024-02-02 20:00:17,915 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 20:00:17,915 	Gloss Alignment :	         
2024-02-02 20:00:17,915 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 20:00:17,916 	Text Reference  :	*** he    dedicated the medal   to sprinter milkha singh
2024-02-02 20:00:17,916 	Text Hypothesis :	now let's wait      for updates to become   the    match
2024-02-02 20:00:17,916 	Text Alignment  :	I   S     S         S   S          S        S      S    
2024-02-02 20:00:17,916 ========================================================================================================================
2024-02-02 20:00:17,916 Logging Sequence: 149_77.00
2024-02-02 20:00:17,916 	Gloss Reference :	A B+C+D+E
2024-02-02 20:00:17,916 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 20:00:17,916 	Gloss Alignment :	         
2024-02-02 20:00:17,917 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 20:00:17,918 	Text Reference  :	and arrested danushka for  alleged sexual assault  of a 29     year       old  woman whose name  has not  been   disclosed
2024-02-02 20:00:17,919 	Text Hypothesis :	*** woman    from     rose bay     a      suburban of * sydney complained that she   was   raped by  this fierce 2022     
2024-02-02 20:00:17,919 	Text Alignment  :	D   S        S        S    S       S      S           D S      S          S    S     S     S     S   S    S      S        
2024-02-02 20:00:17,919 ========================================================================================================================
2024-02-02 20:00:17,919 Logging Sequence: 168_15.00
2024-02-02 20:00:17,919 	Gloss Reference :	A B+C+D+E
2024-02-02 20:00:17,919 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 20:00:17,919 	Gloss Alignment :	         
2024-02-02 20:00:17,920 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 20:00:17,920 	Text Reference  :	when in public the couple are    always approached for     photographys and autographs
2024-02-02 20:00:17,920 	Text Hypothesis :	**** ** ****** the ****** indian team   was        present at           the league    
2024-02-02 20:00:17,921 	Text Alignment  :	D    D  D          D      S      S      S          S       S            S   S         
2024-02-02 20:00:17,921 ========================================================================================================================
2024-02-02 20:00:21,535 Epoch 1589: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-02 20:00:21,535 EPOCH 1590
2024-02-02 20:00:26,140 Epoch 1590: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-02 20:00:26,140 EPOCH 1591
2024-02-02 20:00:30,739 Epoch 1591: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-02 20:00:30,739 EPOCH 1592
2024-02-02 20:00:31,478 [Epoch: 1592 Step: 00054100] Batch Recognition Loss:   0.000196 => Gls Tokens per Sec:     2603 || Batch Translation Loss:   0.021222 => Txt Tokens per Sec:     7304 || Lr: 0.000050
2024-02-02 20:00:34,748 Epoch 1592: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-02 20:00:34,748 EPOCH 1593
2024-02-02 20:00:38,772 Epoch 1593: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-02 20:00:38,772 EPOCH 1594
2024-02-02 20:00:43,648 Epoch 1594: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-02 20:00:43,648 EPOCH 1595
2024-02-02 20:00:44,243 [Epoch: 1595 Step: 00054200] Batch Recognition Loss:   0.000184 => Gls Tokens per Sec:     2158 || Batch Translation Loss:   0.021489 => Txt Tokens per Sec:     6718 || Lr: 0.000050
2024-02-02 20:00:47,806 Epoch 1595: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-02 20:00:47,806 EPOCH 1596
2024-02-02 20:00:52,713 Epoch 1596: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-02 20:00:52,713 EPOCH 1597
2024-02-02 20:00:57,174 Epoch 1597: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.58 
2024-02-02 20:00:57,174 EPOCH 1598
2024-02-02 20:00:57,424 [Epoch: 1598 Step: 00054300] Batch Recognition Loss:   0.000187 => Gls Tokens per Sec:     2570 || Batch Translation Loss:   0.021530 => Txt Tokens per Sec:     6602 || Lr: 0.000050
2024-02-02 20:01:01,835 Epoch 1598: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.61 
2024-02-02 20:01:01,836 EPOCH 1599
2024-02-02 20:01:06,234 Epoch 1599: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-02 20:01:06,235 EPOCH 1600
2024-02-02 20:01:10,942 [Epoch: 1600 Step: 00054400] Batch Recognition Loss:   0.000151 => Gls Tokens per Sec:     2259 || Batch Translation Loss:   0.014773 => Txt Tokens per Sec:     6271 || Lr: 0.000050
2024-02-02 20:01:10,943 Epoch 1600: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.92 
2024-02-02 20:01:10,943 EPOCH 1601
2024-02-02 20:01:15,314 Epoch 1601: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.56 
2024-02-02 20:01:15,315 EPOCH 1602
2024-02-02 20:01:20,013 Epoch 1602: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.44 
2024-02-02 20:01:20,014 EPOCH 1603
2024-02-02 20:01:24,742 [Epoch: 1603 Step: 00054500] Batch Recognition Loss:   0.000292 => Gls Tokens per Sec:     2113 || Batch Translation Loss:   0.033450 => Txt Tokens per Sec:     5882 || Lr: 0.000050
2024-02-02 20:01:24,939 Epoch 1603: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.46 
2024-02-02 20:01:24,940 EPOCH 1604
2024-02-02 20:01:29,456 Epoch 1604: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.15 
2024-02-02 20:01:29,457 EPOCH 1605
2024-02-02 20:01:34,443 Epoch 1605: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.10 
2024-02-02 20:01:34,444 EPOCH 1606
2024-02-02 20:01:38,502 [Epoch: 1606 Step: 00054600] Batch Recognition Loss:   0.000401 => Gls Tokens per Sec:     2305 || Batch Translation Loss:   0.033571 => Txt Tokens per Sec:     6451 || Lr: 0.000050
2024-02-02 20:01:38,889 Epoch 1606: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.90 
2024-02-02 20:01:38,889 EPOCH 1607
2024-02-02 20:01:43,378 Epoch 1607: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.84 
2024-02-02 20:01:43,379 EPOCH 1608
2024-02-02 20:01:48,098 Epoch 1608: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.75 
2024-02-02 20:01:48,099 EPOCH 1609
2024-02-02 20:01:52,296 [Epoch: 1609 Step: 00054700] Batch Recognition Loss:   0.000123 => Gls Tokens per Sec:     2076 || Batch Translation Loss:   0.015282 => Txt Tokens per Sec:     5719 || Lr: 0.000050
2024-02-02 20:01:53,050 Epoch 1609: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.91 
2024-02-02 20:01:53,051 EPOCH 1610
2024-02-02 20:01:57,465 Epoch 1610: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.21 
2024-02-02 20:01:57,465 EPOCH 1611
2024-02-02 20:02:02,218 Epoch 1611: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.92 
2024-02-02 20:02:02,219 EPOCH 1612
2024-02-02 20:02:05,294 [Epoch: 1612 Step: 00054800] Batch Recognition Loss:   0.000118 => Gls Tokens per Sec:     2624 || Batch Translation Loss:   0.017052 => Txt Tokens per Sec:     7052 || Lr: 0.000050
2024-02-02 20:02:06,503 Epoch 1612: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.05 
2024-02-02 20:02:06,504 EPOCH 1613
2024-02-02 20:02:11,422 Epoch 1613: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.88 
2024-02-02 20:02:11,422 EPOCH 1614
2024-02-02 20:02:15,769 Epoch 1614: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.02 
2024-02-02 20:02:15,769 EPOCH 1615
2024-02-02 20:02:19,083 [Epoch: 1615 Step: 00054900] Batch Recognition Loss:   0.000149 => Gls Tokens per Sec:     2318 || Batch Translation Loss:   0.016301 => Txt Tokens per Sec:     6406 || Lr: 0.000050
2024-02-02 20:02:20,597 Epoch 1615: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.95 
2024-02-02 20:02:20,597 EPOCH 1616
2024-02-02 20:02:24,783 Epoch 1616: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.78 
2024-02-02 20:02:24,783 EPOCH 1617
2024-02-02 20:02:29,624 Epoch 1617: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.96 
2024-02-02 20:02:29,624 EPOCH 1618
2024-02-02 20:02:32,186 [Epoch: 1618 Step: 00055000] Batch Recognition Loss:   0.000133 => Gls Tokens per Sec:     2749 || Batch Translation Loss:   0.024729 => Txt Tokens per Sec:     7369 || Lr: 0.000050
2024-02-02 20:02:33,984 Epoch 1618: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.03 
2024-02-02 20:02:33,984 EPOCH 1619
2024-02-02 20:02:38,718 Epoch 1619: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.58 
2024-02-02 20:02:38,718 EPOCH 1620
2024-02-02 20:02:43,545 Epoch 1620: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.40 
2024-02-02 20:02:43,546 EPOCH 1621
2024-02-02 20:02:46,094 [Epoch: 1621 Step: 00055100] Batch Recognition Loss:   0.000307 => Gls Tokens per Sec:     2415 || Batch Translation Loss:   0.040062 => Txt Tokens per Sec:     6646 || Lr: 0.000050
2024-02-02 20:02:47,816 Epoch 1621: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.45 
2024-02-02 20:02:47,816 EPOCH 1622
2024-02-02 20:02:52,611 Epoch 1622: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.26 
2024-02-02 20:02:52,612 EPOCH 1623
2024-02-02 20:02:57,408 Epoch 1623: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.98 
2024-02-02 20:02:57,409 EPOCH 1624
2024-02-02 20:02:59,868 [Epoch: 1624 Step: 00055200] Batch Recognition Loss:   0.000179 => Gls Tokens per Sec:     2345 || Batch Translation Loss:   0.023750 => Txt Tokens per Sec:     6632 || Lr: 0.000050
2024-02-02 20:03:01,693 Epoch 1624: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-02 20:03:01,693 EPOCH 1625
2024-02-02 20:03:06,129 Epoch 1625: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.70 
2024-02-02 20:03:06,130 EPOCH 1626
2024-02-02 20:03:10,714 Epoch 1626: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.84 
2024-02-02 20:03:10,714 EPOCH 1627
2024-02-02 20:03:12,568 [Epoch: 1627 Step: 00055300] Batch Recognition Loss:   0.000164 => Gls Tokens per Sec:     2763 || Batch Translation Loss:   0.030684 => Txt Tokens per Sec:     7705 || Lr: 0.000050
2024-02-02 20:03:15,126 Epoch 1627: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.73 
2024-02-02 20:03:15,127 EPOCH 1628
2024-02-02 20:03:19,784 Epoch 1628: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.07 
2024-02-02 20:03:19,784 EPOCH 1629
2024-02-02 20:03:24,255 Epoch 1629: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.07 
2024-02-02 20:03:24,256 EPOCH 1630
2024-02-02 20:03:25,971 [Epoch: 1630 Step: 00055400] Batch Recognition Loss:   0.000123 => Gls Tokens per Sec:     2614 || Batch Translation Loss:   0.034130 => Txt Tokens per Sec:     7114 || Lr: 0.000050
2024-02-02 20:03:28,920 Epoch 1630: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.81 
2024-02-02 20:03:28,920 EPOCH 1631
2024-02-02 20:03:32,929 Epoch 1631: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.77 
2024-02-02 20:03:32,929 EPOCH 1632
2024-02-02 20:03:37,654 Epoch 1632: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-02 20:03:37,655 EPOCH 1633
2024-02-02 20:03:39,783 [Epoch: 1633 Step: 00055500] Batch Recognition Loss:   0.000195 => Gls Tokens per Sec:     1688 || Batch Translation Loss:   0.020535 => Txt Tokens per Sec:     4988 || Lr: 0.000050
2024-02-02 20:03:42,678 Epoch 1633: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-02 20:03:42,679 EPOCH 1634
2024-02-02 20:03:47,235 Epoch 1634: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-02 20:03:47,236 EPOCH 1635
2024-02-02 20:03:51,408 Epoch 1635: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-02 20:03:51,408 EPOCH 1636
2024-02-02 20:03:52,708 [Epoch: 1636 Step: 00055600] Batch Recognition Loss:   0.000119 => Gls Tokens per Sec:     2462 || Batch Translation Loss:   0.010967 => Txt Tokens per Sec:     6487 || Lr: 0.000050
2024-02-02 20:03:56,279 Epoch 1636: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-02 20:03:56,279 EPOCH 1637
2024-02-02 20:04:00,762 Epoch 1637: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-02 20:04:00,762 EPOCH 1638
2024-02-02 20:04:05,377 Epoch 1638: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.67 
2024-02-02 20:04:05,377 EPOCH 1639
2024-02-02 20:04:06,249 [Epoch: 1639 Step: 00055700] Batch Recognition Loss:   0.000133 => Gls Tokens per Sec:     2939 || Batch Translation Loss:   0.009259 => Txt Tokens per Sec:     7610 || Lr: 0.000050
2024-02-02 20:04:09,846 Epoch 1639: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.01 
2024-02-02 20:04:09,847 EPOCH 1640
2024-02-02 20:04:14,425 Epoch 1640: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-02 20:04:14,426 EPOCH 1641
2024-02-02 20:04:18,866 Epoch 1641: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.88 
2024-02-02 20:04:18,866 EPOCH 1642
2024-02-02 20:04:19,454 [Epoch: 1642 Step: 00055800] Batch Recognition Loss:   0.000138 => Gls Tokens per Sec:     3271 || Batch Translation Loss:   0.030620 => Txt Tokens per Sec:     7971 || Lr: 0.000050
2024-02-02 20:04:23,511 Epoch 1642: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.91 
2024-02-02 20:04:23,511 EPOCH 1643
2024-02-02 20:04:27,538 Epoch 1643: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.82 
2024-02-02 20:04:27,538 EPOCH 1644
2024-02-02 20:04:32,366 Epoch 1644: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.48 
2024-02-02 20:04:32,367 EPOCH 1645
2024-02-02 20:04:32,687 [Epoch: 1645 Step: 00055900] Batch Recognition Loss:   0.000258 => Gls Tokens per Sec:     4008 || Batch Translation Loss:   0.090981 => Txt Tokens per Sec:     9447 || Lr: 0.000050
2024-02-02 20:04:37,051 Epoch 1645: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.93 
2024-02-02 20:04:37,052 EPOCH 1646
2024-02-02 20:04:41,592 Epoch 1646: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.58 
2024-02-02 20:04:41,592 EPOCH 1647
2024-02-02 20:04:46,057 Epoch 1647: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.55 
2024-02-02 20:04:46,058 EPOCH 1648
2024-02-02 20:04:46,325 [Epoch: 1648 Step: 00056000] Batch Recognition Loss:   0.000234 => Gls Tokens per Sec:     2398 || Batch Translation Loss:   0.116668 => Txt Tokens per Sec:     6443 || Lr: 0.000050
2024-02-02 20:04:54,667 Validation result at epoch 1648, step    56000: duration: 8.3419s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00143	Translation Loss: 97012.47656	PPL: 16449.38086
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.52	(BLEU-1: 9.44,	BLEU-2: 2.64,	BLEU-3: 1.08,	BLEU-4: 0.52)
	CHRF 16.46	ROUGE 8.32
2024-02-02 20:04:54,668 Logging Recognition and Translation Outputs
2024-02-02 20:04:54,668 ========================================================================================================================
2024-02-02 20:04:54,668 Logging Sequence: 122_110.00
2024-02-02 20:04:54,669 	Gloss Reference :	A B+C+D+E
2024-02-02 20:04:54,669 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 20:04:54,669 	Gloss Alignment :	         
2024-02-02 20:04:54,669 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 20:04:54,670 	Text Reference  :	** *** **** now that    i    achieved my     dream  and  secured a    silver medal
2024-02-02 20:04:54,670 	Text Hypothesis :	if she does not respond then the      police learnt from the     play is     in   
2024-02-02 20:04:54,670 	Text Alignment  :	I  I   I    S   S       S    S        S      S      S    S       S    S      S    
2024-02-02 20:04:54,670 ========================================================================================================================
2024-02-02 20:04:54,670 Logging Sequence: 161_111.00
2024-02-02 20:04:54,671 	Gloss Reference :	A B+C+D+E
2024-02-02 20:04:54,671 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 20:04:54,671 	Gloss Alignment :	         
2024-02-02 20:04:54,671 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 20:04:54,673 	Text Reference  :	*** **** ******** ********* his  last      game   as  captain  was the cape  town test  in south africa in jan 2022
2024-02-02 20:04:54,673 	Text Hypothesis :	the bcci recently announced that rajasthan royals and continue to  be  given a    total of anger but    it was done
2024-02-02 20:04:54,673 	Text Alignment  :	I   I    I        I         S    S         S      S   S        S   S   S     S    S     S  S     S      S  S   S   
2024-02-02 20:04:54,673 ========================================================================================================================
2024-02-02 20:04:54,673 Logging Sequence: 136_79.00
2024-02-02 20:04:54,673 	Gloss Reference :	A B+C+D+E
2024-02-02 20:04:54,674 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 20:04:54,674 	Gloss Alignment :	         
2024-02-02 20:04:54,674 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 20:04:54,675 	Text Reference  :	with this win sindhu became the first indian woman to win two    individual olympic medals     
2024-02-02 20:04:54,675 	Text Hypothesis :	**** **** it  had    won    the ***** ****** ***** ** *** silver medal      in      afghanistan
2024-02-02 20:04:54,675 	Text Alignment  :	D    D    S   S      S          D     D      D     D  D   S      S          S       S          
2024-02-02 20:04:54,675 ========================================================================================================================
2024-02-02 20:04:54,675 Logging Sequence: 166_335.00
2024-02-02 20:04:54,675 	Gloss Reference :	A B+C+D+E
2024-02-02 20:04:54,675 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 20:04:54,676 	Gloss Alignment :	         
2024-02-02 20:04:54,676 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 20:04:54,676 	Text Reference  :	the second world test championship is     scheduled from june 2021 to        30  april 2023  
2024-02-02 20:04:54,677 	Text Hypothesis :	*** ****** ***** **** ************ dahiya won       and  has  now  qualified for the   finals
2024-02-02 20:04:54,677 	Text Alignment  :	D   D      D     D    D            S      S         S    S    S    S         S   S     S     
2024-02-02 20:04:54,677 ========================================================================================================================
2024-02-02 20:04:54,677 Logging Sequence: 95_152.00
2024-02-02 20:04:54,677 	Gloss Reference :	A B+C+D+E
2024-02-02 20:04:54,677 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 20:04:54,677 	Gloss Alignment :	         
2024-02-02 20:04:54,677 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 20:04:54,678 	Text Reference  :	**** ***** ** ***** ******* *** how  strange
2024-02-02 20:04:54,678 	Text Hypothesis :	they began pm boris johnson and went viral  
2024-02-02 20:04:54,678 	Text Alignment  :	I    I     I  I     I       I   S    S      
2024-02-02 20:04:54,678 ========================================================================================================================
2024-02-02 20:04:59,415 Epoch 1648: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.71 
2024-02-02 20:04:59,416 EPOCH 1649
2024-02-02 20:05:04,103 Epoch 1649: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.20 
2024-02-02 20:05:04,104 EPOCH 1650
2024-02-02 20:05:08,724 [Epoch: 1650 Step: 00056100] Batch Recognition Loss:   0.000234 => Gls Tokens per Sec:     2302 || Batch Translation Loss:   0.036018 => Txt Tokens per Sec:     6390 || Lr: 0.000050
2024-02-02 20:05:08,724 Epoch 1650: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.25 
2024-02-02 20:05:08,724 EPOCH 1651
2024-02-02 20:05:13,150 Epoch 1651: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.94 
2024-02-02 20:05:13,151 EPOCH 1652
2024-02-02 20:05:17,779 Epoch 1652: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.77 
2024-02-02 20:05:17,779 EPOCH 1653
2024-02-02 20:05:21,900 [Epoch: 1653 Step: 00056200] Batch Recognition Loss:   0.000150 => Gls Tokens per Sec:     2424 || Batch Translation Loss:   0.020300 => Txt Tokens per Sec:     6778 || Lr: 0.000050
2024-02-02 20:05:22,267 Epoch 1653: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.89 
2024-02-02 20:05:22,267 EPOCH 1654
2024-02-02 20:05:26,896 Epoch 1654: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-02 20:05:26,896 EPOCH 1655
2024-02-02 20:05:30,908 Epoch 1655: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-02 20:05:30,908 EPOCH 1656
2024-02-02 20:05:35,226 [Epoch: 1656 Step: 00056300] Batch Recognition Loss:   0.000140 => Gls Tokens per Sec:     2166 || Batch Translation Loss:   0.013268 => Txt Tokens per Sec:     6049 || Lr: 0.000050
2024-02-02 20:05:35,638 Epoch 1656: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-02 20:05:35,638 EPOCH 1657
2024-02-02 20:05:40,276 Epoch 1657: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-02 20:05:40,277 EPOCH 1658
2024-02-02 20:05:44,913 Epoch 1658: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.73 
2024-02-02 20:05:44,913 EPOCH 1659
2024-02-02 20:05:48,261 [Epoch: 1659 Step: 00056400] Batch Recognition Loss:   0.000198 => Gls Tokens per Sec:     2604 || Batch Translation Loss:   0.028379 => Txt Tokens per Sec:     7150 || Lr: 0.000050
2024-02-02 20:05:49,297 Epoch 1659: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.81 
2024-02-02 20:05:49,297 EPOCH 1660
2024-02-02 20:05:54,124 Epoch 1660: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-02 20:05:54,124 EPOCH 1661
2024-02-02 20:05:58,769 Epoch 1661: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-02 20:05:58,770 EPOCH 1662
2024-02-02 20:06:02,343 [Epoch: 1662 Step: 00056500] Batch Recognition Loss:   0.000117 => Gls Tokens per Sec:     2259 || Batch Translation Loss:   0.012869 => Txt Tokens per Sec:     6262 || Lr: 0.000050
2024-02-02 20:06:03,293 Epoch 1662: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-02 20:06:03,293 EPOCH 1663
2024-02-02 20:06:07,895 Epoch 1663: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.76 
2024-02-02 20:06:07,896 EPOCH 1664
2024-02-02 20:06:12,367 Epoch 1664: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-02 20:06:12,367 EPOCH 1665
2024-02-02 20:06:15,326 [Epoch: 1665 Step: 00056600] Batch Recognition Loss:   0.000151 => Gls Tokens per Sec:     2513 || Batch Translation Loss:   0.017667 => Txt Tokens per Sec:     6728 || Lr: 0.000050
2024-02-02 20:06:16,951 Epoch 1665: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.77 
2024-02-02 20:06:16,952 EPOCH 1666
2024-02-02 20:06:21,455 Epoch 1666: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-02 20:06:21,455 EPOCH 1667
2024-02-02 20:06:26,077 Epoch 1667: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.86 
2024-02-02 20:06:26,078 EPOCH 1668
2024-02-02 20:06:28,993 [Epoch: 1668 Step: 00056700] Batch Recognition Loss:   0.000191 => Gls Tokens per Sec:     2416 || Batch Translation Loss:   0.024615 => Txt Tokens per Sec:     6647 || Lr: 0.000050
2024-02-02 20:06:30,572 Epoch 1668: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-02 20:06:30,572 EPOCH 1669
2024-02-02 20:06:35,154 Epoch 1669: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.76 
2024-02-02 20:06:35,154 EPOCH 1670
2024-02-02 20:06:39,655 Epoch 1670: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.98 
2024-02-02 20:06:39,656 EPOCH 1671
2024-02-02 20:06:42,199 [Epoch: 1671 Step: 00056800] Batch Recognition Loss:   0.000182 => Gls Tokens per Sec:     2418 || Batch Translation Loss:   0.024244 => Txt Tokens per Sec:     6775 || Lr: 0.000050
2024-02-02 20:06:44,197 Epoch 1671: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.67 
2024-02-02 20:06:44,197 EPOCH 1672
2024-02-02 20:06:48,814 Epoch 1672: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.18 
2024-02-02 20:06:48,815 EPOCH 1673
2024-02-02 20:06:53,704 Epoch 1673: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.79 
2024-02-02 20:06:53,705 EPOCH 1674
2024-02-02 20:06:56,231 [Epoch: 1674 Step: 00056900] Batch Recognition Loss:   0.000232 => Gls Tokens per Sec:     2280 || Batch Translation Loss:   0.021911 => Txt Tokens per Sec:     6444 || Lr: 0.000050
2024-02-02 20:06:58,255 Epoch 1674: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.86 
2024-02-02 20:06:58,256 EPOCH 1675
2024-02-02 20:07:03,221 Epoch 1675: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.52 
2024-02-02 20:07:03,222 EPOCH 1676
2024-02-02 20:07:07,549 Epoch 1676: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.49 
2024-02-02 20:07:07,550 EPOCH 1677
2024-02-02 20:07:09,996 [Epoch: 1677 Step: 00057000] Batch Recognition Loss:   0.000242 => Gls Tokens per Sec:     2093 || Batch Translation Loss:   0.047540 => Txt Tokens per Sec:     6202 || Lr: 0.000050
2024-02-02 20:07:12,196 Epoch 1677: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.63 
2024-02-02 20:07:12,197 EPOCH 1678
2024-02-02 20:07:16,640 Epoch 1678: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.22 
2024-02-02 20:07:16,641 EPOCH 1679
2024-02-02 20:07:21,272 Epoch 1679: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.04 
2024-02-02 20:07:21,273 EPOCH 1680
2024-02-02 20:07:23,168 [Epoch: 1680 Step: 00057100] Batch Recognition Loss:   0.000431 => Gls Tokens per Sec:     2365 || Batch Translation Loss:   0.064679 => Txt Tokens per Sec:     6390 || Lr: 0.000050
2024-02-02 20:07:25,673 Epoch 1680: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.79 
2024-02-02 20:07:25,673 EPOCH 1681
2024-02-02 20:07:30,324 Epoch 1681: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.26 
2024-02-02 20:07:30,325 EPOCH 1682
2024-02-02 20:07:34,828 Epoch 1682: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.16 
2024-02-02 20:07:34,829 EPOCH 1683
2024-02-02 20:07:36,094 [Epoch: 1683 Step: 00057200] Batch Recognition Loss:   0.000125 => Gls Tokens per Sec:     2839 || Batch Translation Loss:   0.014565 => Txt Tokens per Sec:     7549 || Lr: 0.000050
2024-02-02 20:07:39,462 Epoch 1683: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.96 
2024-02-02 20:07:39,463 EPOCH 1684
2024-02-02 20:07:43,899 Epoch 1684: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.89 
2024-02-02 20:07:43,900 EPOCH 1685
2024-02-02 20:07:48,538 Epoch 1685: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.86 
2024-02-02 20:07:48,538 EPOCH 1686
2024-02-02 20:07:49,781 [Epoch: 1686 Step: 00057300] Batch Recognition Loss:   0.000174 => Gls Tokens per Sec:     2578 || Batch Translation Loss:   0.024091 => Txt Tokens per Sec:     7105 || Lr: 0.000050
2024-02-02 20:07:52,983 Epoch 1686: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-02 20:07:52,983 EPOCH 1687
2024-02-02 20:07:57,633 Epoch 1687: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.90 
2024-02-02 20:07:57,634 EPOCH 1688
2024-02-02 20:08:02,593 Epoch 1688: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-02 20:08:02,594 EPOCH 1689
2024-02-02 20:08:03,473 [Epoch: 1689 Step: 00057400] Batch Recognition Loss:   0.000131 => Gls Tokens per Sec:     2914 || Batch Translation Loss:   0.015988 => Txt Tokens per Sec:     8143 || Lr: 0.000050
2024-02-02 20:08:07,346 Epoch 1689: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-02 20:08:07,346 EPOCH 1690
2024-02-02 20:08:11,756 Epoch 1690: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.76 
2024-02-02 20:08:11,756 EPOCH 1691
2024-02-02 20:08:16,416 Epoch 1691: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.82 
2024-02-02 20:08:16,416 EPOCH 1692
2024-02-02 20:08:17,472 [Epoch: 1692 Step: 00057500] Batch Recognition Loss:   0.000167 => Gls Tokens per Sec:     1821 || Batch Translation Loss:   0.017929 => Txt Tokens per Sec:     5544 || Lr: 0.000050
2024-02-02 20:08:20,819 Epoch 1692: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.78 
2024-02-02 20:08:20,819 EPOCH 1693
2024-02-02 20:08:25,695 Epoch 1693: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.78 
2024-02-02 20:08:25,695 EPOCH 1694
2024-02-02 20:08:29,885 Epoch 1694: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.87 
2024-02-02 20:08:29,885 EPOCH 1695
2024-02-02 20:08:30,369 [Epoch: 1695 Step: 00057600] Batch Recognition Loss:   0.000239 => Gls Tokens per Sec:     2645 || Batch Translation Loss:   0.021832 => Txt Tokens per Sec:     7399 || Lr: 0.000050
2024-02-02 20:08:34,709 Epoch 1695: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.80 
2024-02-02 20:08:34,709 EPOCH 1696
2024-02-02 20:08:38,941 Epoch 1696: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.25 
2024-02-02 20:08:38,941 EPOCH 1697
2024-02-02 20:08:43,758 Epoch 1697: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.97 
2024-02-02 20:08:43,758 EPOCH 1698
2024-02-02 20:08:44,070 [Epoch: 1698 Step: 00057700] Batch Recognition Loss:   0.000292 => Gls Tokens per Sec:     2058 || Batch Translation Loss:   0.021800 => Txt Tokens per Sec:     6305 || Lr: 0.000050
2024-02-02 20:08:48,410 Epoch 1698: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-02 20:08:48,410 EPOCH 1699
2024-02-02 20:08:53,216 Epoch 1699: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-02 20:08:53,216 EPOCH 1700
2024-02-02 20:08:57,350 [Epoch: 1700 Step: 00057800] Batch Recognition Loss:   0.000100 => Gls Tokens per Sec:     2572 || Batch Translation Loss:   0.013213 => Txt Tokens per Sec:     7141 || Lr: 0.000050
2024-02-02 20:08:57,350 Epoch 1700: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.10 
2024-02-02 20:08:57,350 EPOCH 1701
2024-02-02 20:09:02,253 Epoch 1701: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-02 20:09:02,254 EPOCH 1702
2024-02-02 20:09:06,443 Epoch 1702: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.70 
2024-02-02 20:09:06,443 EPOCH 1703
2024-02-02 20:09:11,069 [Epoch: 1703 Step: 00057900] Batch Recognition Loss:   0.000205 => Gls Tokens per Sec:     2161 || Batch Translation Loss:   0.024057 => Txt Tokens per Sec:     5986 || Lr: 0.000050
2024-02-02 20:09:11,320 Epoch 1703: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-02 20:09:11,320 EPOCH 1704
2024-02-02 20:09:15,564 Epoch 1704: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.75 
2024-02-02 20:09:15,564 EPOCH 1705
2024-02-02 20:09:20,447 Epoch 1705: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-02 20:09:20,448 EPOCH 1706
2024-02-02 20:09:24,234 [Epoch: 1706 Step: 00058000] Batch Recognition Loss:   0.000114 => Gls Tokens per Sec:     2470 || Batch Translation Loss:   0.011740 => Txt Tokens per Sec:     6846 || Lr: 0.000050
2024-02-02 20:09:32,610 Validation result at epoch 1706, step    58000: duration: 8.3758s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00109	Translation Loss: 97677.93750	PPL: 17582.09375
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.64	(BLEU-1: 10.16,	BLEU-2: 2.97,	BLEU-3: 1.24,	BLEU-4: 0.64)
	CHRF 16.41	ROUGE 8.82
2024-02-02 20:09:32,611 Logging Recognition and Translation Outputs
2024-02-02 20:09:32,611 ========================================================================================================================
2024-02-02 20:09:32,611 Logging Sequence: 180_138.00
2024-02-02 20:09:32,611 	Gloss Reference :	A B+C+D+E
2024-02-02 20:09:32,612 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 20:09:32,612 	Gloss Alignment :	         
2024-02-02 20:09:32,612 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 20:09:32,614 	Text Reference  :	ioa president p t usha constituted a    seven-member panel     which included world champions from various sports    to  inquire into   the allegations
2024-02-02 20:09:32,614 	Text Hypothesis :	*** ********* * * **** *********** they also         demanding that  the      full  faith     in   the     judiciary and will    accpet the verdict    
2024-02-02 20:09:32,614 	Text Alignment  :	D   D         D D D    D           S    S            S         S     S        S     S         S    S       S         S   S       S          S          
2024-02-02 20:09:32,615 ========================================================================================================================
2024-02-02 20:09:32,615 Logging Sequence: 128_189.00
2024-02-02 20:09:32,615 	Gloss Reference :	A B+C+D+E
2024-02-02 20:09:32,615 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 20:09:32,615 	Gloss Alignment :	         
2024-02-02 20:09:32,615 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 20:09:32,616 	Text Reference  :	*** meanwhile some funny incidents happened during the   match
2024-02-02 20:09:32,616 	Text Hypothesis :	now is        the  most  followed  to       t20    world cup  
2024-02-02 20:09:32,616 	Text Alignment  :	I   S         S    S     S         S        S      S     S    
2024-02-02 20:09:32,616 ========================================================================================================================
2024-02-02 20:09:32,616 Logging Sequence: 165_523.00
2024-02-02 20:09:32,616 	Gloss Reference :	A B+C+D+E
2024-02-02 20:09:32,617 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 20:09:32,617 	Gloss Alignment :	         
2024-02-02 20:09:32,617 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 20:09:32,618 	Text Reference  :	as he believed that his  team might lose    if he  takes off his       batting pads   
2024-02-02 20:09:32,618 	Text Hypothesis :	** ** ******** **** when they were  waiting to see him   to  celebrate their   victory
2024-02-02 20:09:32,618 	Text Alignment  :	D  D  D        D    S    S    S     S       S  S   S     S   S         S       S      
2024-02-02 20:09:32,618 ========================================================================================================================
2024-02-02 20:09:32,618 Logging Sequence: 145_168.00
2024-02-02 20:09:32,618 	Gloss Reference :	A B+C+D+E
2024-02-02 20:09:32,619 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 20:09:32,619 	Gloss Alignment :	         
2024-02-02 20:09:32,619 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 20:09:32,620 	Text Reference  :	**** ** ******* *** *** ********* the  decision has       devastated sameeha   and her parents 
2024-02-02 20:09:32,620 	Text Hypothesis :	this is because she was diagnosed with an       inflammed million    followers on  his personal
2024-02-02 20:09:32,620 	Text Alignment  :	I    I  I       I   I   I         S    S        S         S          S         S   S   S       
2024-02-02 20:09:32,620 ========================================================================================================================
2024-02-02 20:09:32,620 Logging Sequence: 92_123.00
2024-02-02 20:09:32,620 	Gloss Reference :	A B+C+D+E
2024-02-02 20:09:32,621 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 20:09:32,621 	Gloss Alignment :	         
2024-02-02 20:09:32,621 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 20:09:32,621 	Text Reference  :	a heated argument also took place between members of the family and the   two       men 
2024-02-02 20:09:32,622 	Text Hypothesis :	* ****** ******** **** **** ***** very    few     of *** ****** *** these wrestlers said
2024-02-02 20:09:32,622 	Text Alignment  :	D D      D        D    D    D     S       S          D   D      D   S     S         S   
2024-02-02 20:09:32,622 ========================================================================================================================
2024-02-02 20:09:33,160 Epoch 1706: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-02 20:09:33,160 EPOCH 1707
2024-02-02 20:09:38,115 Epoch 1707: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.81 
2024-02-02 20:09:38,115 EPOCH 1708
2024-02-02 20:09:42,370 Epoch 1708: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.53 
2024-02-02 20:09:42,370 EPOCH 1709
2024-02-02 20:09:45,955 [Epoch: 1709 Step: 00058100] Batch Recognition Loss:   0.000767 => Gls Tokens per Sec:     2430 || Batch Translation Loss:   0.074603 => Txt Tokens per Sec:     6864 || Lr: 0.000050
2024-02-02 20:09:46,620 Epoch 1709: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.94 
2024-02-02 20:09:46,621 EPOCH 1710
2024-02-02 20:09:51,405 Epoch 1710: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.57 
2024-02-02 20:09:51,405 EPOCH 1711
2024-02-02 20:09:55,445 Epoch 1711: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.48 
2024-02-02 20:09:55,445 EPOCH 1712
2024-02-02 20:09:58,321 [Epoch: 1712 Step: 00058200] Batch Recognition Loss:   0.000232 => Gls Tokens per Sec:     2807 || Batch Translation Loss:   0.021008 => Txt Tokens per Sec:     7549 || Lr: 0.000050
2024-02-02 20:09:59,893 Epoch 1712: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.70 
2024-02-02 20:09:59,894 EPOCH 1713
2024-02-02 20:10:04,514 Epoch 1713: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.67 
2024-02-02 20:10:04,514 EPOCH 1714
2024-02-02 20:10:09,145 Epoch 1714: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.86 
2024-02-02 20:10:09,146 EPOCH 1715
2024-02-02 20:10:12,551 [Epoch: 1715 Step: 00058300] Batch Recognition Loss:   0.000206 => Gls Tokens per Sec:     2256 || Batch Translation Loss:   0.030223 => Txt Tokens per Sec:     6375 || Lr: 0.000050
2024-02-02 20:10:13,831 Epoch 1715: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.90 
2024-02-02 20:10:13,831 EPOCH 1716
2024-02-02 20:10:18,547 Epoch 1716: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.22 
2024-02-02 20:10:18,547 EPOCH 1717
2024-02-02 20:10:22,780 Epoch 1717: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.75 
2024-02-02 20:10:22,780 EPOCH 1718
2024-02-02 20:10:26,104 [Epoch: 1718 Step: 00058400] Batch Recognition Loss:   0.000175 => Gls Tokens per Sec:     2119 || Batch Translation Loss:   0.018698 => Txt Tokens per Sec:     5889 || Lr: 0.000050
2024-02-02 20:10:27,608 Epoch 1718: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-02 20:10:27,608 EPOCH 1719
2024-02-02 20:10:31,623 Epoch 1719: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-02 20:10:31,624 EPOCH 1720
2024-02-02 20:10:36,328 Epoch 1720: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-02 20:10:36,329 EPOCH 1721
2024-02-02 20:10:39,259 [Epoch: 1721 Step: 00058500] Batch Recognition Loss:   0.000136 => Gls Tokens per Sec:     2100 || Batch Translation Loss:   0.026268 => Txt Tokens per Sec:     5789 || Lr: 0.000050
2024-02-02 20:10:41,181 Epoch 1721: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-02 20:10:41,182 EPOCH 1722
2024-02-02 20:10:45,965 Epoch 1722: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-02 20:10:45,965 EPOCH 1723
2024-02-02 20:10:50,240 Epoch 1723: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-02 20:10:50,240 EPOCH 1724
2024-02-02 20:10:53,043 [Epoch: 1724 Step: 00058600] Batch Recognition Loss:   0.000130 => Gls Tokens per Sec:     1967 || Batch Translation Loss:   0.012198 => Txt Tokens per Sec:     5564 || Lr: 0.000050
2024-02-02 20:10:55,149 Epoch 1724: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-02 20:10:55,149 EPOCH 1725
2024-02-02 20:10:59,404 Epoch 1725: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-02 20:10:59,405 EPOCH 1726
2024-02-02 20:11:04,193 Epoch 1726: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-02 20:11:04,194 EPOCH 1727
2024-02-02 20:11:06,266 [Epoch: 1727 Step: 00058700] Batch Recognition Loss:   0.000120 => Gls Tokens per Sec:     2472 || Batch Translation Loss:   0.009273 => Txt Tokens per Sec:     7099 || Lr: 0.000050
2024-02-02 20:11:08,434 Epoch 1727: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-02 20:11:08,435 EPOCH 1728
2024-02-02 20:11:13,272 Epoch 1728: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-02 20:11:13,272 EPOCH 1729
2024-02-02 20:11:17,558 Epoch 1729: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-02 20:11:17,559 EPOCH 1730
2024-02-02 20:11:19,462 [Epoch: 1730 Step: 00058800] Batch Recognition Loss:   0.000122 => Gls Tokens per Sec:     2355 || Batch Translation Loss:   0.006649 => Txt Tokens per Sec:     6142 || Lr: 0.000050
2024-02-02 20:11:22,372 Epoch 1730: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-02 20:11:22,373 EPOCH 1731
2024-02-02 20:11:26,515 Epoch 1731: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-02 20:11:26,516 EPOCH 1732
2024-02-02 20:11:31,387 Epoch 1732: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-02 20:11:31,388 EPOCH 1733
2024-02-02 20:11:32,841 [Epoch: 1733 Step: 00058900] Batch Recognition Loss:   0.000184 => Gls Tokens per Sec:     2473 || Batch Translation Loss:   0.004375 => Txt Tokens per Sec:     6898 || Lr: 0.000050
2024-02-02 20:11:35,588 Epoch 1733: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-02 20:11:35,589 EPOCH 1734
2024-02-02 20:11:40,479 Epoch 1734: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-02 20:11:40,480 EPOCH 1735
2024-02-02 20:11:44,730 Epoch 1735: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-02 20:11:44,730 EPOCH 1736
2024-02-02 20:11:46,410 [Epoch: 1736 Step: 00059000] Batch Recognition Loss:   0.000163 => Gls Tokens per Sec:     1757 || Batch Translation Loss:   0.013958 => Txt Tokens per Sec:     4987 || Lr: 0.000050
2024-02-02 20:11:49,623 Epoch 1736: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.48 
2024-02-02 20:11:49,623 EPOCH 1737
2024-02-02 20:11:53,713 Epoch 1737: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-02 20:11:53,713 EPOCH 1738
2024-02-02 20:11:58,179 Epoch 1738: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-02 20:11:58,179 EPOCH 1739
2024-02-02 20:11:59,308 [Epoch: 1739 Step: 00059100] Batch Recognition Loss:   0.000152 => Gls Tokens per Sec:     2268 || Batch Translation Loss:   0.011887 => Txt Tokens per Sec:     5929 || Lr: 0.000050
2024-02-02 20:12:02,717 Epoch 1739: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.55 
2024-02-02 20:12:02,717 EPOCH 1740
2024-02-02 20:12:07,407 Epoch 1740: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.55 
2024-02-02 20:12:07,408 EPOCH 1741
2024-02-02 20:12:11,798 Epoch 1741: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.90 
2024-02-02 20:12:11,798 EPOCH 1742
2024-02-02 20:12:12,304 [Epoch: 1742 Step: 00059200] Batch Recognition Loss:   0.000163 => Gls Tokens per Sec:     3799 || Batch Translation Loss:   0.037596 => Txt Tokens per Sec:     8890 || Lr: 0.000050
2024-02-02 20:12:16,495 Epoch 1742: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.41 
2024-02-02 20:12:16,495 EPOCH 1743
2024-02-02 20:12:20,780 Epoch 1743: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.39 
2024-02-02 20:12:20,780 EPOCH 1744
2024-02-02 20:12:25,558 Epoch 1744: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.29 
2024-02-02 20:12:25,558 EPOCH 1745
2024-02-02 20:12:25,949 [Epoch: 1745 Step: 00059300] Batch Recognition Loss:   0.000261 => Gls Tokens per Sec:     3282 || Batch Translation Loss:   0.041292 => Txt Tokens per Sec:     8597 || Lr: 0.000050
2024-02-02 20:12:29,881 Epoch 1745: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.75 
2024-02-02 20:12:29,881 EPOCH 1746
2024-02-02 20:12:34,568 Epoch 1746: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.31 
2024-02-02 20:12:34,569 EPOCH 1747
2024-02-02 20:12:38,941 Epoch 1747: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.26 
2024-02-02 20:12:38,941 EPOCH 1748
2024-02-02 20:12:39,089 [Epoch: 1748 Step: 00059400] Batch Recognition Loss:   0.000164 => Gls Tokens per Sec:     4354 || Batch Translation Loss:   0.035573 => Txt Tokens per Sec:    10116 || Lr: 0.000050
2024-02-02 20:12:43,669 Epoch 1748: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.97 
2024-02-02 20:12:43,669 EPOCH 1749
2024-02-02 20:12:48,029 Epoch 1749: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.78 
2024-02-02 20:12:48,029 EPOCH 1750
2024-02-02 20:12:52,722 [Epoch: 1750 Step: 00059500] Batch Recognition Loss:   0.000220 => Gls Tokens per Sec:     2266 || Batch Translation Loss:   0.036479 => Txt Tokens per Sec:     6289 || Lr: 0.000050
2024-02-02 20:12:52,722 Epoch 1750: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.79 
2024-02-02 20:12:52,723 EPOCH 1751
2024-02-02 20:12:57,068 Epoch 1751: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-02 20:12:57,068 EPOCH 1752
2024-02-02 20:13:01,698 Epoch 1752: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.83 
2024-02-02 20:13:01,699 EPOCH 1753
2024-02-02 20:13:05,918 [Epoch: 1753 Step: 00059600] Batch Recognition Loss:   0.000166 => Gls Tokens per Sec:     2369 || Batch Translation Loss:   0.020675 => Txt Tokens per Sec:     6551 || Lr: 0.000050
2024-02-02 20:13:06,140 Epoch 1753: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.79 
2024-02-02 20:13:06,140 EPOCH 1754
2024-02-02 20:13:10,853 Epoch 1754: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-02 20:13:10,854 EPOCH 1755
2024-02-02 20:13:15,251 Epoch 1755: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-02 20:13:15,251 EPOCH 1756
2024-02-02 20:13:19,437 [Epoch: 1756 Step: 00059700] Batch Recognition Loss:   0.000121 => Gls Tokens per Sec:     2234 || Batch Translation Loss:   0.008514 => Txt Tokens per Sec:     6326 || Lr: 0.000050
2024-02-02 20:13:19,942 Epoch 1756: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-02 20:13:19,942 EPOCH 1757
2024-02-02 20:13:24,291 Epoch 1757: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-02 20:13:24,292 EPOCH 1758
2024-02-02 20:13:28,933 Epoch 1758: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.82 
2024-02-02 20:13:28,933 EPOCH 1759
2024-02-02 20:13:32,701 [Epoch: 1759 Step: 00059800] Batch Recognition Loss:   0.000140 => Gls Tokens per Sec:     2312 || Batch Translation Loss:   0.016061 => Txt Tokens per Sec:     6503 || Lr: 0.000050
2024-02-02 20:13:33,347 Epoch 1759: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.92 
2024-02-02 20:13:33,347 EPOCH 1760
2024-02-02 20:13:37,593 Epoch 1760: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-02 20:13:37,594 EPOCH 1761
2024-02-02 20:13:42,447 Epoch 1761: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-02 20:13:42,447 EPOCH 1762
2024-02-02 20:13:45,845 [Epoch: 1762 Step: 00059900] Batch Recognition Loss:   0.000117 => Gls Tokens per Sec:     2376 || Batch Translation Loss:   0.008345 => Txt Tokens per Sec:     6326 || Lr: 0.000050
2024-02-02 20:13:47,235 Epoch 1762: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.77 
2024-02-02 20:13:47,235 EPOCH 1763
2024-02-02 20:13:51,472 Epoch 1763: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.84 
2024-02-02 20:13:51,472 EPOCH 1764
2024-02-02 20:13:56,168 Epoch 1764: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-02 20:13:56,168 EPOCH 1765
2024-02-02 20:13:59,153 [Epoch: 1765 Step: 00060000] Batch Recognition Loss:   0.000159 => Gls Tokens per Sec:     2490 || Batch Translation Loss:   0.031636 => Txt Tokens per Sec:     6800 || Lr: 0.000050
2024-02-02 20:14:07,654 Validation result at epoch 1765, step    60000: duration: 8.5013s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00108	Translation Loss: 97531.07031	PPL: 17325.57812
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.77	(BLEU-1: 10.64,	BLEU-2: 3.29,	BLEU-3: 1.45,	BLEU-4: 0.77)
	CHRF 16.96	ROUGE 9.42
2024-02-02 20:14:07,655 Logging Recognition and Translation Outputs
2024-02-02 20:14:07,655 ========================================================================================================================
2024-02-02 20:14:07,655 Logging Sequence: 179_269.00
2024-02-02 20:14:07,655 	Gloss Reference :	A B+C+D+E
2024-02-02 20:14:07,656 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 20:14:07,656 	Gloss Alignment :	         
2024-02-02 20:14:07,656 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 20:14:07,657 	Text Reference  :	the ban would mean    she  can't compete   in      any  national or    other domestic events
2024-02-02 20:14:07,657 	Text Hypothesis :	*** *** ***** several year old   christian eriksen from wfi      takes a     same     time  
2024-02-02 20:14:07,657 	Text Alignment  :	D   D   D     S       S    S     S         S       S    S        S     S     S        S     
2024-02-02 20:14:07,657 ========================================================================================================================
2024-02-02 20:14:07,657 Logging Sequence: 94_253.00
2024-02-02 20:14:07,658 	Gloss Reference :	A B+C+D+E
2024-02-02 20:14:07,658 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 20:14:07,658 	Gloss Alignment :	         
2024-02-02 20:14:07,658 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 20:14:07,660 	Text Reference  :	however some tickets will be   kept    aside for  physical sale  at the stadiums a few days prior to the  match 
2024-02-02 20:14:07,660 	Text Hypothesis :	however when they    were many debated in    this super    kings on the ******** * *** **** ***** ** many medals
2024-02-02 20:14:07,660 	Text Alignment  :	        S    S       S    S    S       S     S    S        S     S      D        D D   D    D     D  S    S     
2024-02-02 20:14:07,660 ========================================================================================================================
2024-02-02 20:14:07,660 Logging Sequence: 114_201.00
2024-02-02 20:14:07,660 	Gloss Reference :	A B+C+D+E
2024-02-02 20:14:07,660 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 20:14:07,661 	Gloss Alignment :	         
2024-02-02 20:14:07,661 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 20:14:07,661 	Text Reference  :	*** **** this is   his first time *** **** winning the copa      
2024-02-02 20:14:07,661 	Text Hypothesis :	the game was  tied the first time and save with    the goalkeeper
2024-02-02 20:14:07,662 	Text Alignment  :	I   I    S    S    S              I   I    S           S         
2024-02-02 20:14:07,662 ========================================================================================================================
2024-02-02 20:14:07,662 Logging Sequence: 118_104.00
2024-02-02 20:14:07,662 	Gloss Reference :	A B+C+D+E
2024-02-02 20:14:07,662 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 20:14:07,662 	Gloss Alignment :	         
2024-02-02 20:14:07,662 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 20:14:07,663 	Text Reference  :	kylian mbappã strong performance in the ****** *** match ** * was    greatly appreciated
2024-02-02 20:14:07,663 	Text Hypothesis :	****** ****** ****** *********** in the second odi match as a played 3       balls      
2024-02-02 20:14:07,663 	Text Alignment  :	D      D      D      D                  I      I         I  I S      S       S          
2024-02-02 20:14:07,663 ========================================================================================================================
2024-02-02 20:14:07,664 Logging Sequence: 144_74.00
2024-02-02 20:14:07,664 	Gloss Reference :	A B+C+D+E
2024-02-02 20:14:07,664 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 20:14:07,664 	Gloss Alignment :	         
2024-02-02 20:14:07,664 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 20:14:07,664 	Text Reference  :	** **** *** * *** ** isn't        that amazing    
2024-02-02 20:14:07,665 	Text Hypothesis :	so what was a lot of appreciation and  participate
2024-02-02 20:14:07,665 	Text Alignment  :	I  I    I   I I   I  S            S    S          
2024-02-02 20:14:07,665 ========================================================================================================================
2024-02-02 20:14:09,352 Epoch 1765: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-02 20:14:09,353 EPOCH 1766
2024-02-02 20:14:14,302 Epoch 1766: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.25 
2024-02-02 20:14:14,303 EPOCH 1767
2024-02-02 20:14:18,889 Epoch 1767: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.85 
2024-02-02 20:14:18,889 EPOCH 1768
2024-02-02 20:14:22,363 [Epoch: 1768 Step: 00060100] Batch Recognition Loss:   0.000176 => Gls Tokens per Sec:     1956 || Batch Translation Loss:   0.021085 => Txt Tokens per Sec:     5642 || Lr: 0.000050
2024-02-02 20:14:23,767 Epoch 1768: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.35 
2024-02-02 20:14:23,768 EPOCH 1769
2024-02-02 20:14:27,998 Epoch 1769: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.32 
2024-02-02 20:14:27,999 EPOCH 1770
2024-02-02 20:14:32,660 Epoch 1770: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.74 
2024-02-02 20:14:32,660 EPOCH 1771
2024-02-02 20:14:35,148 [Epoch: 1771 Step: 00060200] Batch Recognition Loss:   0.000118 => Gls Tokens per Sec:     2473 || Batch Translation Loss:   0.009777 => Txt Tokens per Sec:     6934 || Lr: 0.000050
2024-02-02 20:14:37,344 Epoch 1771: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.08 
2024-02-02 20:14:37,344 EPOCH 1772
2024-02-02 20:14:42,167 Epoch 1772: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.09 
2024-02-02 20:14:42,168 EPOCH 1773
2024-02-02 20:14:46,382 Epoch 1773: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.95 
2024-02-02 20:14:46,382 EPOCH 1774
2024-02-02 20:14:49,151 [Epoch: 1774 Step: 00060300] Batch Recognition Loss:   0.000497 => Gls Tokens per Sec:     2082 || Batch Translation Loss:   0.035906 => Txt Tokens per Sec:     5896 || Lr: 0.000050
2024-02-02 20:14:51,296 Epoch 1774: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.34 
2024-02-02 20:14:51,296 EPOCH 1775
2024-02-02 20:14:56,066 Epoch 1775: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.01 
2024-02-02 20:14:56,067 EPOCH 1776
2024-02-02 20:15:00,916 Epoch 1776: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.87 
2024-02-02 20:15:00,917 EPOCH 1777
2024-02-02 20:15:02,501 [Epoch: 1777 Step: 00060400] Batch Recognition Loss:   0.000219 => Gls Tokens per Sec:     3075 || Batch Translation Loss:   0.041872 => Txt Tokens per Sec:     8042 || Lr: 0.000050
2024-02-02 20:15:05,505 Epoch 1777: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.83 
2024-02-02 20:15:05,506 EPOCH 1778
2024-02-02 20:15:10,374 Epoch 1778: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-02 20:15:10,374 EPOCH 1779
2024-02-02 20:15:14,588 Epoch 1779: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-02 20:15:14,589 EPOCH 1780
2024-02-02 20:15:16,477 [Epoch: 1780 Step: 00060500] Batch Recognition Loss:   0.000138 => Gls Tokens per Sec:     2242 || Batch Translation Loss:   0.012170 => Txt Tokens per Sec:     6119 || Lr: 0.000050
2024-02-02 20:15:19,508 Epoch 1780: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-02 20:15:19,509 EPOCH 1781
2024-02-02 20:15:24,245 Epoch 1781: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.75 
2024-02-02 20:15:24,246 EPOCH 1782
2024-02-02 20:15:29,137 Epoch 1782: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.03 
2024-02-02 20:15:29,138 EPOCH 1783
2024-02-02 20:15:30,303 [Epoch: 1783 Step: 00060600] Batch Recognition Loss:   0.000161 => Gls Tokens per Sec:     3294 || Batch Translation Loss:   0.021411 => Txt Tokens per Sec:     8348 || Lr: 0.000050
2024-02-02 20:15:33,330 Epoch 1783: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-02 20:15:33,330 EPOCH 1784
2024-02-02 20:15:38,205 Epoch 1784: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.70 
2024-02-02 20:15:38,206 EPOCH 1785
2024-02-02 20:15:43,058 Epoch 1785: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-02 20:15:43,058 EPOCH 1786
2024-02-02 20:15:44,603 [Epoch: 1786 Step: 00060700] Batch Recognition Loss:   0.000143 => Gls Tokens per Sec:     2072 || Batch Translation Loss:   0.014687 => Txt Tokens per Sec:     5712 || Lr: 0.000050
2024-02-02 20:15:47,979 Epoch 1786: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.89 
2024-02-02 20:15:47,980 EPOCH 1787
2024-02-02 20:15:52,300 Epoch 1787: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.95 
2024-02-02 20:15:52,300 EPOCH 1788
2024-02-02 20:15:56,923 Epoch 1788: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.72 
2024-02-02 20:15:56,924 EPOCH 1789
2024-02-02 20:15:58,602 [Epoch: 1789 Step: 00060800] Batch Recognition Loss:   0.000179 => Gls Tokens per Sec:     1526 || Batch Translation Loss:   0.016267 => Txt Tokens per Sec:     4955 || Lr: 0.000050
2024-02-02 20:16:01,344 Epoch 1789: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-02 20:16:01,345 EPOCH 1790
2024-02-02 20:16:05,366 Epoch 1790: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-02 20:16:05,366 EPOCH 1791
2024-02-02 20:16:10,287 Epoch 1791: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-02 20:16:10,287 EPOCH 1792
2024-02-02 20:16:11,374 [Epoch: 1792 Step: 00060900] Batch Recognition Loss:   0.000176 => Gls Tokens per Sec:     1768 || Batch Translation Loss:   0.026791 => Txt Tokens per Sec:     5683 || Lr: 0.000050
2024-02-02 20:16:14,995 Epoch 1792: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-02 20:16:14,996 EPOCH 1793
2024-02-02 20:16:19,388 Epoch 1793: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-02 20:16:19,388 EPOCH 1794
2024-02-02 20:16:23,788 Epoch 1794: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.48 
2024-02-02 20:16:23,788 EPOCH 1795
2024-02-02 20:16:24,510 [Epoch: 1795 Step: 00061000] Batch Recognition Loss:   0.000131 => Gls Tokens per Sec:     1776 || Batch Translation Loss:   0.013264 => Txt Tokens per Sec:     5613 || Lr: 0.000050
2024-02-02 20:16:28,340 Epoch 1795: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.75 
2024-02-02 20:16:28,340 EPOCH 1796
2024-02-02 20:16:32,961 Epoch 1796: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.90 
2024-02-02 20:16:32,961 EPOCH 1797
2024-02-02 20:16:37,464 Epoch 1797: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.92 
2024-02-02 20:16:37,464 EPOCH 1798
2024-02-02 20:16:37,686 [Epoch: 1798 Step: 00061100] Batch Recognition Loss:   0.000229 => Gls Tokens per Sec:     2891 || Batch Translation Loss:   0.022600 => Txt Tokens per Sec:     7255 || Lr: 0.000050
2024-02-02 20:16:42,202 Epoch 1798: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.82 
2024-02-02 20:16:42,203 EPOCH 1799
2024-02-02 20:16:46,522 Epoch 1799: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.07 
2024-02-02 20:16:46,522 EPOCH 1800
2024-02-02 20:16:51,381 [Epoch: 1800 Step: 00061200] Batch Recognition Loss:   0.000227 => Gls Tokens per Sec:     2188 || Batch Translation Loss:   0.026794 => Txt Tokens per Sec:     6075 || Lr: 0.000050
2024-02-02 20:16:51,381 Epoch 1800: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.22 
2024-02-02 20:16:51,382 EPOCH 1801
2024-02-02 20:16:55,773 Epoch 1801: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.85 
2024-02-02 20:16:55,774 EPOCH 1802
2024-02-02 20:17:00,671 Epoch 1802: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.79 
2024-02-02 20:17:00,672 EPOCH 1803
2024-02-02 20:17:05,043 [Epoch: 1803 Step: 00061300] Batch Recognition Loss:   0.000160 => Gls Tokens per Sec:     2344 || Batch Translation Loss:   0.013069 => Txt Tokens per Sec:     6552 || Lr: 0.000050
2024-02-02 20:17:05,262 Epoch 1803: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-02 20:17:05,262 EPOCH 1804
2024-02-02 20:17:09,945 Epoch 1804: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-02 20:17:09,945 EPOCH 1805
2024-02-02 20:17:13,968 Epoch 1805: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.83 
2024-02-02 20:17:13,969 EPOCH 1806
2024-02-02 20:17:18,310 [Epoch: 1806 Step: 00061400] Batch Recognition Loss:   0.000122 => Gls Tokens per Sec:     2154 || Batch Translation Loss:   0.012508 => Txt Tokens per Sec:     6096 || Lr: 0.000050
2024-02-02 20:17:18,717 Epoch 1806: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-02 20:17:18,717 EPOCH 1807
2024-02-02 20:17:23,328 Epoch 1807: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-02 20:17:23,329 EPOCH 1808
2024-02-02 20:17:27,990 Epoch 1808: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-02 20:17:27,990 EPOCH 1809
2024-02-02 20:17:31,613 [Epoch: 1809 Step: 00061500] Batch Recognition Loss:   0.000146 => Gls Tokens per Sec:     2405 || Batch Translation Loss:   0.025474 => Txt Tokens per Sec:     6747 || Lr: 0.000050
2024-02-02 20:17:32,463 Epoch 1809: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.72 
2024-02-02 20:17:32,463 EPOCH 1810
2024-02-02 20:17:37,118 Epoch 1810: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.12 
2024-02-02 20:17:37,118 EPOCH 1811
2024-02-02 20:17:41,605 Epoch 1811: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.26 
2024-02-02 20:17:41,605 EPOCH 1812
2024-02-02 20:17:45,177 [Epoch: 1812 Step: 00061600] Batch Recognition Loss:   0.000377 => Gls Tokens per Sec:     2330 || Batch Translation Loss:   0.033673 => Txt Tokens per Sec:     6456 || Lr: 0.000050
2024-02-02 20:17:46,258 Epoch 1812: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.55 
2024-02-02 20:17:46,258 EPOCH 1813
2024-02-02 20:17:50,659 Epoch 1813: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.24 
2024-02-02 20:17:50,660 EPOCH 1814
2024-02-02 20:17:55,329 Epoch 1814: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.23 
2024-02-02 20:17:55,329 EPOCH 1815
2024-02-02 20:17:57,904 [Epoch: 1815 Step: 00061700] Batch Recognition Loss:   0.000255 => Gls Tokens per Sec:     2886 || Batch Translation Loss:   0.049083 => Txt Tokens per Sec:     7851 || Lr: 0.000050
2024-02-02 20:17:59,695 Epoch 1815: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.61 
2024-02-02 20:17:59,696 EPOCH 1816
2024-02-02 20:18:04,387 Epoch 1816: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.77 
2024-02-02 20:18:04,387 EPOCH 1817
2024-02-02 20:18:08,842 Epoch 1817: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.51 
2024-02-02 20:18:08,843 EPOCH 1818
2024-02-02 20:18:11,860 [Epoch: 1818 Step: 00061800] Batch Recognition Loss:   0.000264 => Gls Tokens per Sec:     2252 || Batch Translation Loss:   0.080022 => Txt Tokens per Sec:     6206 || Lr: 0.000050
2024-02-02 20:18:13,535 Epoch 1818: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.81 
2024-02-02 20:18:13,535 EPOCH 1819
2024-02-02 20:18:17,907 Epoch 1819: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.18 
2024-02-02 20:18:17,907 EPOCH 1820
2024-02-02 20:18:22,638 Epoch 1820: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.96 
2024-02-02 20:18:22,638 EPOCH 1821
2024-02-02 20:18:24,828 [Epoch: 1821 Step: 00061900] Batch Recognition Loss:   0.000166 => Gls Tokens per Sec:     2810 || Batch Translation Loss:   0.017462 => Txt Tokens per Sec:     7795 || Lr: 0.000050
2024-02-02 20:18:27,032 Epoch 1821: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.73 
2024-02-02 20:18:27,033 EPOCH 1822
2024-02-02 20:18:31,720 Epoch 1822: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.76 
2024-02-02 20:18:31,720 EPOCH 1823
2024-02-02 20:18:36,093 Epoch 1823: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-02 20:18:36,094 EPOCH 1824
2024-02-02 20:18:38,416 [Epoch: 1824 Step: 00062000] Batch Recognition Loss:   0.000122 => Gls Tokens per Sec:     2373 || Batch Translation Loss:   0.011252 => Txt Tokens per Sec:     6395 || Lr: 0.000050
2024-02-02 20:18:46,666 Validation result at epoch 1824, step    62000: duration: 8.2504s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00106	Translation Loss: 97158.92969	PPL: 16692.23828
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.66	(BLEU-1: 10.65,	BLEU-2: 3.33,	BLEU-3: 1.31,	BLEU-4: 0.66)
	CHRF 17.06	ROUGE 9.17
2024-02-02 20:18:46,667 Logging Recognition and Translation Outputs
2024-02-02 20:18:46,667 ========================================================================================================================
2024-02-02 20:18:46,667 Logging Sequence: 87_52.00
2024-02-02 20:18:46,667 	Gloss Reference :	A B+C+D+E
2024-02-02 20:18:46,668 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 20:18:46,668 	Gloss Alignment :	         
2024-02-02 20:18:46,668 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 20:18:46,669 	Text Reference  :	that is   when  gambhir walked into     bat   and rescued india        with his brilliant 97 runs     
2024-02-02 20:18:46,669 	Text Hypothesis :	**** also kohli with    an     accident about the are     wicketkeeper of   his video     on instagram
2024-02-02 20:18:46,670 	Text Alignment  :	D    S    S     S       S      S        S     S   S       S            S        S         S  S        
2024-02-02 20:18:46,670 ========================================================================================================================
2024-02-02 20:18:46,670 Logging Sequence: 85_2.00
2024-02-02 20:18:46,670 	Gloss Reference :	A B+C+D+E
2024-02-02 20:18:46,670 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 20:18:46,670 	Gloss Alignment :	         
2024-02-02 20:18:46,670 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 20:18:46,671 	Text Reference  :	andrew symonds is one of the finest all rounders in   the    history of  australian cricket       
2024-02-02 20:18:46,671 	Text Hypothesis :	****** he      is *** ** *** ****** *** ******** also played 198     one day        internationals
2024-02-02 20:18:46,671 	Text Alignment  :	D      S          D   D  D   D      D   D        S    S      S       S   S          S             
2024-02-02 20:18:46,672 ========================================================================================================================
2024-02-02 20:18:46,672 Logging Sequence: 51_110.00
2024-02-02 20:18:46,672 	Gloss Reference :	A B+C+D+E
2024-02-02 20:18:46,672 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 20:18:46,672 	Gloss Alignment :	         
2024-02-02 20:18:46,672 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 20:18:46,673 	Text Reference  :	**** *** **** the aussies were very happy with their victory
2024-02-02 20:18:46,673 	Text Hypothesis :	they had been the ******* same time that  they had   lost   
2024-02-02 20:18:46,673 	Text Alignment  :	I    I   I        D       S    S    S     S    S     S      
2024-02-02 20:18:46,673 ========================================================================================================================
2024-02-02 20:18:46,673 Logging Sequence: 72_59.00
2024-02-02 20:18:46,675 	Gloss Reference :	A B+C+D+E
2024-02-02 20:18:46,675 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 20:18:46,675 	Gloss Alignment :	         
2024-02-02 20:18:46,675 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 20:18:46,675 	Text Reference  :	**** after that sapna and   shobit started arguing and ******** misbehaving with the   cricketer
2024-02-02 20:18:46,675 	Text Hypothesis :	this is    not  the   first time   that    kohli   and pakistan was         a    small wedding  
2024-02-02 20:18:46,675 	Text Alignment  :	I    S     S    S     S     S      S       S           I        S           S    S     S        
2024-02-02 20:18:46,675 ========================================================================================================================
2024-02-02 20:18:46,675 Logging Sequence: 122_184.00
2024-02-02 20:18:46,676 	Gloss Reference :	A B+C+D+E
2024-02-02 20:18:46,676 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 20:18:46,676 	Gloss Alignment :	         
2024-02-02 20:18:46,676 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 20:18:46,677 	Text Reference  :	are playing exceptionally well  and  keeping hopes of  further olympic medals alive
2024-02-02 20:18:46,677 	Text Hypothesis :	*** ******* the           kiwis said what    about the cup     indian  womens team 
2024-02-02 20:18:46,677 	Text Alignment  :	D   D       S             S     S    S       S     S   S       S       S      S    
2024-02-02 20:18:46,677 ========================================================================================================================
2024-02-02 20:18:46,681 Training ended since there were no improvements inthe last learning rate step: 0.000050
2024-02-02 20:18:46,681 Best validation result at step    10000:   1.17 eval_metric.
2024-02-02 20:19:13,068 ------------------------------------------------------------
2024-02-02 20:19:13,069 [DEV] partition [RECOGNITION] experiment [BW]: 1
2024-02-02 20:19:21,325 finished in 8.2552s 
2024-02-02 20:19:21,325 ************************************************************
2024-02-02 20:19:21,325 [DEV] partition [RECOGNITION] results:
	New Best CTC Decode Beam Size: 1
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
2024-02-02 20:19:21,325 ************************************************************
2024-02-02 20:19:21,326 ------------------------------------------------------------
2024-02-02 20:19:21,326 [DEV] partition [RECOGNITION] experiment [BW]: 2
2024-02-02 20:19:29,544 finished in 8.2182s 
2024-02-02 20:19:29,544 ------------------------------------------------------------
2024-02-02 20:19:29,544 [DEV] partition [RECOGNITION] experiment [BW]: 3
2024-02-02 20:19:37,761 finished in 8.2177s 
2024-02-02 20:19:37,761 ------------------------------------------------------------
2024-02-02 20:19:37,762 [DEV] partition [RECOGNITION] experiment [BW]: 4
2024-02-02 20:19:45,969 finished in 8.2069s 
2024-02-02 20:19:45,969 ------------------------------------------------------------
2024-02-02 20:19:45,970 [DEV] partition [RECOGNITION] experiment [BW]: 5
2024-02-02 20:19:54,340 finished in 8.3699s 
2024-02-02 20:19:54,340 ------------------------------------------------------------
2024-02-02 20:19:54,340 [DEV] partition [RECOGNITION] experiment [BW]: 6
2024-02-02 20:20:02,611 finished in 8.2707s 
2024-02-02 20:20:02,611 ------------------------------------------------------------
2024-02-02 20:20:02,612 [DEV] partition [RECOGNITION] experiment [BW]: 7
2024-02-02 20:20:10,953 finished in 8.3411s 
2024-02-02 20:20:10,954 ------------------------------------------------------------
2024-02-02 20:20:10,954 [DEV] partition [RECOGNITION] experiment [BW]: 8
2024-02-02 20:20:19,316 finished in 8.3616s 
2024-02-02 20:20:19,317 ------------------------------------------------------------
2024-02-02 20:20:19,317 [DEV] partition [RECOGNITION] experiment [BW]: 9
2024-02-02 20:20:27,630 finished in 8.3127s 
2024-02-02 20:20:27,631 ------------------------------------------------------------
2024-02-02 20:20:27,631 [DEV] partition [RECOGNITION] experiment [BW]: 10
2024-02-02 20:20:35,974 finished in 8.3438s 
2024-02-02 20:20:35,974 ============================================================
2024-02-02 20:20:44,043 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 1 and Alpha: -1
	BLEU-4 1.17	(BLEU-1: 11.33,	BLEU-2: 4.18,	BLEU-3: 2.06,	BLEU-4: 1.17)
	CHRF 17.52	ROUGE 9.83
2024-02-02 20:20:44,044 ------------------------------------------------------------
2024-02-02 20:22:01,784 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 2 and Alpha: 1
	BLEU-4 1.17	(BLEU-1: 11.45,	BLEU-2: 4.17,	BLEU-3: 2.02,	BLEU-4: 1.17)
	CHRF 17.23	ROUGE 9.94
2024-02-02 20:22:01,785 ------------------------------------------------------------
2024-02-02 20:22:11,695 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 2 and Alpha: 2
	BLEU-4 1.23	(BLEU-1: 11.68,	BLEU-2: 4.34,	BLEU-3: 2.13,	BLEU-4: 1.23)
	CHRF 17.36	ROUGE 9.93
2024-02-02 20:22:11,696 ------------------------------------------------------------
2024-02-02 20:22:21,485 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 2 and Alpha: 3
	BLEU-4 1.27	(BLEU-1: 11.80,	BLEU-2: 4.39,	BLEU-3: 2.18,	BLEU-4: 1.27)
	CHRF 17.38	ROUGE 9.94
2024-02-02 20:22:21,485 ------------------------------------------------------------
2024-02-02 20:25:01,286 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 4 and Alpha: 3
	BLEU-4 1.29	(BLEU-1: 11.53,	BLEU-2: 4.32,	BLEU-3: 2.19,	BLEU-4: 1.29)
	CHRF 17.32	ROUGE 9.72
2024-02-02 20:25:01,287 ------------------------------------------------------------
2024-02-02 20:25:13,774 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 4 and Alpha: 4
	BLEU-4 1.29	(BLEU-1: 11.56,	BLEU-2: 4.34,	BLEU-3: 2.19,	BLEU-4: 1.29)
	CHRF 17.34	ROUGE 9.71
2024-02-02 20:25:13,775 ------------------------------------------------------------
2024-02-02 20:37:31,498 ************************************************************
2024-02-02 20:37:31,498 [DEV] partition [Recognition & Translation] results:
	Best CTC Decode Beam Size: 1
	Best Translation Beam Size: 4 and Alpha: 4
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 1.29	(BLEU-1: 11.56,	BLEU-2: 4.34,	BLEU-3: 2.19,	BLEU-4: 1.29)
	CHRF 17.34	ROUGE 9.71
2024-02-02 20:37:31,499 ************************************************************
2024-02-02 20:37:44,181 [TEST] partition [Recognition & Translation] results:
	Best CTC Decode Beam Size: 1
	Best Translation Beam Size: 4 and Alpha: 4
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.47	(BLEU-1: 10.45,	BLEU-2: 3.18,	BLEU-3: 1.22,	BLEU-4: 0.47)
	CHRF 17.01	ROUGE 8.93
2024-02-02 20:37:44,181 ************************************************************
