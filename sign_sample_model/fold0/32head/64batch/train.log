2024-01-31 17:02:37,527 Hello! This is Joey-NMT.
2024-01-31 17:02:37,533 Total params: 25639944
2024-01-31 17:02:37,534 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'gloss_output_layer.bias', 'gloss_output_layer.weight', 'sgn_embed.ln.bias', 'sgn_embed.ln.weight', 'sgn_embed.norm.norm.bias', 'sgn_embed.norm.norm.weight', 'txt_embed.norm.norm.bias', 'txt_embed.norm.norm.weight']
2024-01-31 17:02:37,701 cfg.name                           : sign_experiment
2024-01-31 17:02:37,701 cfg.data.data_path                 : ./data/Sports_dataset/0/
2024-01-31 17:02:37,701 cfg.data.version                   : phoenix_2014_trans
2024-01-31 17:02:37,701 cfg.data.sgn                       : sign
2024-01-31 17:02:37,702 cfg.data.txt                       : text
2024-01-31 17:02:37,702 cfg.data.gls                       : gloss
2024-01-31 17:02:37,702 cfg.data.train                     : excel_data.train
2024-01-31 17:02:37,702 cfg.data.dev                       : excel_data.dev
2024-01-31 17:02:37,702 cfg.data.test                      : excel_data.test
2024-01-31 17:02:37,702 cfg.data.feature_size              : 2560
2024-01-31 17:02:37,702 cfg.data.level                     : word
2024-01-31 17:02:37,703 cfg.data.txt_lowercase             : True
2024-01-31 17:02:37,703 cfg.data.max_sent_length           : 500
2024-01-31 17:02:37,703 cfg.data.random_train_subset       : -1
2024-01-31 17:02:37,703 cfg.data.random_dev_subset         : -1
2024-01-31 17:02:37,703 cfg.testing.recognition_beam_sizes : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
2024-01-31 17:02:37,703 cfg.testing.translation_beam_sizes : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
2024-01-31 17:02:37,703 cfg.testing.translation_beam_alphas : [-1, 0, 1, 2, 3, 4, 5]
2024-01-31 17:02:37,703 cfg.training.reset_best_ckpt       : False
2024-01-31 17:02:37,703 cfg.training.reset_scheduler       : False
2024-01-31 17:02:37,704 cfg.training.reset_optimizer       : False
2024-01-31 17:02:37,704 cfg.training.random_seed           : 42
2024-01-31 17:02:37,704 cfg.training.model_dir             : ./sign_sample_model/fold0/32head/64batch
2024-01-31 17:02:37,704 cfg.training.recognition_loss_weight : 1.0
2024-01-31 17:02:37,704 cfg.training.translation_loss_weight : 1.0
2024-01-31 17:02:37,704 cfg.training.eval_metric           : bleu
2024-01-31 17:02:37,704 cfg.training.optimizer             : adam
2024-01-31 17:02:37,704 cfg.training.learning_rate         : 0.0001
2024-01-31 17:02:37,704 cfg.training.batch_size            : 64
2024-01-31 17:02:37,705 cfg.training.num_valid_log         : 5
2024-01-31 17:02:37,705 cfg.training.epochs                : 50000
2024-01-31 17:02:37,705 cfg.training.early_stopping_metric : eval_metric
2024-01-31 17:02:37,705 cfg.training.batch_type            : sentence
2024-01-31 17:02:37,705 cfg.training.translation_normalization : batch
2024-01-31 17:02:37,705 cfg.training.eval_recognition_beam_size : 1
2024-01-31 17:02:37,705 cfg.training.eval_translation_beam_size : 1
2024-01-31 17:02:37,705 cfg.training.eval_translation_beam_alpha : -1
2024-01-31 17:02:37,706 cfg.training.overwrite             : True
2024-01-31 17:02:37,706 cfg.training.shuffle               : True
2024-01-31 17:02:37,706 cfg.training.use_cuda              : True
2024-01-31 17:02:37,706 cfg.training.translation_max_output_length : 40
2024-01-31 17:02:37,706 cfg.training.keep_last_ckpts       : 1
2024-01-31 17:02:37,706 cfg.training.batch_multiplier      : 1
2024-01-31 17:02:37,706 cfg.training.logging_freq          : 100
2024-01-31 17:02:37,706 cfg.training.validation_freq       : 2000
2024-01-31 17:02:37,706 cfg.training.betas                 : [0.9, 0.998]
2024-01-31 17:02:37,707 cfg.training.scheduling            : plateau
2024-01-31 17:02:37,707 cfg.training.learning_rate_min     : 1e-08
2024-01-31 17:02:37,707 cfg.training.weight_decay          : 0.0001
2024-01-31 17:02:37,707 cfg.training.patience              : 12
2024-01-31 17:02:37,707 cfg.training.decrease_factor       : 0.5
2024-01-31 17:02:37,707 cfg.training.label_smoothing       : 0.1
2024-01-31 17:02:37,707 cfg.model.initializer              : xavier
2024-01-31 17:02:37,707 cfg.model.bias_initializer         : zeros
2024-01-31 17:02:37,707 cfg.model.init_gain                : 1.0
2024-01-31 17:02:37,708 cfg.model.embed_initializer        : xavier
2024-01-31 17:02:37,708 cfg.model.embed_init_gain          : 1.0
2024-01-31 17:02:37,708 cfg.model.tied_softmax             : True
2024-01-31 17:02:37,708 cfg.model.encoder.type             : transformer
2024-01-31 17:02:37,708 cfg.model.encoder.num_layers       : 3
2024-01-31 17:02:37,708 cfg.model.encoder.num_heads        : 32
2024-01-31 17:02:37,708 cfg.model.encoder.embeddings.embedding_dim : 512
2024-01-31 17:02:37,708 cfg.model.encoder.embeddings.scale : False
2024-01-31 17:02:37,708 cfg.model.encoder.embeddings.dropout : 0.1
2024-01-31 17:02:37,709 cfg.model.encoder.embeddings.norm_type : batch
2024-01-31 17:02:37,709 cfg.model.encoder.embeddings.activation_type : softsign
2024-01-31 17:02:37,709 cfg.model.encoder.hidden_size      : 512
2024-01-31 17:02:37,709 cfg.model.encoder.ff_size          : 2048
2024-01-31 17:02:37,709 cfg.model.encoder.dropout          : 0.1
2024-01-31 17:02:37,709 cfg.model.decoder.type             : transformer
2024-01-31 17:02:37,709 cfg.model.decoder.num_layers       : 3
2024-01-31 17:02:37,709 cfg.model.decoder.num_heads        : 32
2024-01-31 17:02:37,709 cfg.model.decoder.embeddings.embedding_dim : 512
2024-01-31 17:02:37,710 cfg.model.decoder.embeddings.scale : False
2024-01-31 17:02:37,710 cfg.model.decoder.embeddings.dropout : 0.1
2024-01-31 17:02:37,710 cfg.model.decoder.embeddings.norm_type : batch
2024-01-31 17:02:37,710 cfg.model.decoder.embeddings.activation_type : softsign
2024-01-31 17:02:37,710 cfg.model.decoder.hidden_size      : 512
2024-01-31 17:02:37,710 cfg.model.decoder.ff_size          : 2048
2024-01-31 17:02:37,710 cfg.model.decoder.dropout          : 0.1
2024-01-31 17:02:37,710 Data set sizes: 
	train 2126,
	valid 708,
	test 706
2024-01-31 17:02:37,711 First training example:
	[GLS] A B C D E
	[TXT] although new zealand was disappointed to faltered at the finals against australia they did well throughout the tournament
2024-01-31 17:02:37,711 First 10 words (gls): (0) <si> (1) <unk> (2) <pad> (3) A (4) B (5) C (6) D (7) E
2024-01-31 17:02:37,711 First 10 words (txt): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) the (5) and (6) to (7) a (8) in (9) of
2024-01-31 17:02:37,711 Number of unique glosses (types): 8
2024-01-31 17:02:37,711 Number of unique words (types): 4397
2024-01-31 17:02:37,711 SignModel(
	encoder=TransformerEncoder(num_layers=3, num_heads=32),
	decoder=TransformerDecoder(num_layers=3, num_heads=32),
	sgn_embed=SpatialEmbeddings(embedding_dim=512, input_size=2560),
	txt_embed=Embeddings(embedding_dim=512, vocab_size=4397))
2024-01-31 17:02:37,715 EPOCH 1
2024-01-31 17:03:01,989 Epoch   1: Total Training Recognition Loss 1100.09  Total Training Translation Loss 3487.49 
2024-01-31 17:03:01,990 EPOCH 2
2024-01-31 17:03:21,481 Epoch   2: Total Training Recognition Loss 1100.04  Total Training Translation Loss 3487.65 
2024-01-31 17:03:21,481 EPOCH 3
2024-01-31 17:03:38,276 [Epoch: 003 Step: 00000100] Batch Recognition Loss:  15.367375 => Gls Tokens per Sec:      595 || Batch Translation Loss:  79.936142 => Txt Tokens per Sec:     1642 || Lr: 0.000100
2024-01-31 17:03:39,431 Epoch   3: Total Training Recognition Loss 1100.57  Total Training Translation Loss 3487.97 
2024-01-31 17:03:39,431 EPOCH 4
2024-01-31 17:03:57,486 Epoch   4: Total Training Recognition Loss 1101.23  Total Training Translation Loss 3486.51 
2024-01-31 17:03:57,486 EPOCH 5
2024-01-31 17:04:15,676 Epoch   5: Total Training Recognition Loss 1102.33  Total Training Translation Loss 3487.49 
2024-01-31 17:04:15,676 EPOCH 6
2024-01-31 17:04:31,465 [Epoch: 006 Step: 00000200] Batch Recognition Loss:  28.130493 => Gls Tokens per Sec:      592 || Batch Translation Loss:  96.146042 => Txt Tokens per Sec:     1652 || Lr: 0.000100
2024-01-31 17:04:33,588 Epoch   6: Total Training Recognition Loss 1100.43  Total Training Translation Loss 3486.99 
2024-01-31 17:04:33,588 EPOCH 7
2024-01-31 17:04:51,605 Epoch   7: Total Training Recognition Loss 1097.56  Total Training Translation Loss 3487.26 
2024-01-31 17:04:51,605 EPOCH 8
2024-01-31 17:05:09,394 Epoch   8: Total Training Recognition Loss 1099.50  Total Training Translation Loss 3488.43 
2024-01-31 17:05:09,394 EPOCH 9
2024-01-31 17:05:23,011 [Epoch: 009 Step: 00000300] Batch Recognition Loss:  13.098934 => Gls Tokens per Sec:      658 || Batch Translation Loss:  71.814728 => Txt Tokens per Sec:     1805 || Lr: 0.000100
2024-01-31 17:05:27,364 Epoch   9: Total Training Recognition Loss 1100.80  Total Training Translation Loss 3487.18 
2024-01-31 17:05:27,364 EPOCH 10
2024-01-31 17:05:45,388 Epoch  10: Total Training Recognition Loss 1097.99  Total Training Translation Loss 3486.97 
2024-01-31 17:05:45,388 EPOCH 11
2024-01-31 17:06:03,191 Epoch  11: Total Training Recognition Loss 1100.58  Total Training Translation Loss 3486.53 
2024-01-31 17:06:03,191 EPOCH 12
2024-01-31 17:06:16,387 [Epoch: 012 Step: 00000400] Batch Recognition Loss:  27.385498 => Gls Tokens per Sec:      612 || Batch Translation Loss:  96.582748 => Txt Tokens per Sec:     1664 || Lr: 0.000100
2024-01-31 17:06:21,168 Epoch  12: Total Training Recognition Loss 1100.58  Total Training Translation Loss 3487.42 
2024-01-31 17:06:21,168 EPOCH 13
2024-01-31 17:06:39,246 Epoch  13: Total Training Recognition Loss 1101.43  Total Training Translation Loss 3487.08 
2024-01-31 17:06:39,246 EPOCH 14
2024-01-31 17:06:57,129 Epoch  14: Total Training Recognition Loss 1100.47  Total Training Translation Loss 3487.08 
2024-01-31 17:06:57,129 EPOCH 15
2024-01-31 17:07:09,299 [Epoch: 015 Step: 00000500] Batch Recognition Loss:  27.421129 => Gls Tokens per Sec:      631 || Batch Translation Loss:  91.506897 => Txt Tokens per Sec:     1728 || Lr: 0.000100
2024-01-31 17:07:15,121 Epoch  15: Total Training Recognition Loss 1100.24  Total Training Translation Loss 3486.46 
2024-01-31 17:07:15,121 EPOCH 16
2024-01-31 17:07:33,049 Epoch  16: Total Training Recognition Loss 1100.94  Total Training Translation Loss 3487.48 
2024-01-31 17:07:33,050 EPOCH 17
2024-01-31 17:07:50,990 Epoch  17: Total Training Recognition Loss 1101.44  Total Training Translation Loss 3487.88 
2024-01-31 17:07:50,990 EPOCH 18
2024-01-31 17:08:03,667 [Epoch: 018 Step: 00000600] Batch Recognition Loss:  57.641747 => Gls Tokens per Sec:      536 || Batch Translation Loss: 132.222839 => Txt Tokens per Sec:     1533 || Lr: 0.000100
2024-01-31 17:08:09,073 Epoch  18: Total Training Recognition Loss 1100.91  Total Training Translation Loss 3486.79 
2024-01-31 17:08:09,073 EPOCH 19
2024-01-31 17:08:26,869 Epoch  19: Total Training Recognition Loss 1098.87  Total Training Translation Loss 3487.64 
2024-01-31 17:08:26,869 EPOCH 20
2024-01-31 17:08:45,023 Epoch  20: Total Training Recognition Loss 1100.51  Total Training Translation Loss 3487.51 
2024-01-31 17:08:45,024 EPOCH 21
2024-01-31 17:08:58,391 [Epoch: 021 Step: 00000700] Batch Recognition Loss:  15.230978 => Gls Tokens per Sec:      460 || Batch Translation Loss:  76.888260 => Txt Tokens per Sec:     1318 || Lr: 0.000100
2024-01-31 17:09:07,692 Epoch  21: Total Training Recognition Loss 1100.35  Total Training Translation Loss 3488.26 
2024-01-31 17:09:07,692 EPOCH 22
2024-01-31 17:09:25,969 Epoch  22: Total Training Recognition Loss 1101.09  Total Training Translation Loss 3486.81 
2024-01-31 17:09:25,969 EPOCH 23
2024-01-31 17:09:43,424 Epoch  23: Total Training Recognition Loss 1100.65  Total Training Translation Loss 3486.97 
2024-01-31 17:09:43,425 EPOCH 24
2024-01-31 17:09:53,267 [Epoch: 024 Step: 00000800] Batch Recognition Loss:  32.858727 => Gls Tokens per Sec:      560 || Batch Translation Loss: 107.875961 => Txt Tokens per Sec:     1549 || Lr: 0.000100
2024-01-31 17:10:01,013 Epoch  24: Total Training Recognition Loss 1100.02  Total Training Translation Loss 3487.42 
2024-01-31 17:10:01,014 EPOCH 25
2024-01-31 17:10:18,574 Epoch  25: Total Training Recognition Loss 1101.91  Total Training Translation Loss 3486.91 
2024-01-31 17:10:18,575 EPOCH 26
2024-01-31 17:10:36,101 Epoch  26: Total Training Recognition Loss 1099.96  Total Training Translation Loss 3487.21 
2024-01-31 17:10:36,101 EPOCH 27
2024-01-31 17:10:44,211 [Epoch: 027 Step: 00000900] Batch Recognition Loss:  33.259140 => Gls Tokens per Sec:      601 || Batch Translation Loss: 100.384758 => Txt Tokens per Sec:     1693 || Lr: 0.000100
2024-01-31 17:10:53,712 Epoch  27: Total Training Recognition Loss 1100.80  Total Training Translation Loss 3487.35 
2024-01-31 17:10:53,713 EPOCH 28
2024-01-31 17:11:11,257 Epoch  28: Total Training Recognition Loss 1100.26  Total Training Translation Loss 3487.32 
2024-01-31 17:11:11,257 EPOCH 29
2024-01-31 17:11:28,816 Epoch  29: Total Training Recognition Loss 1101.00  Total Training Translation Loss 3487.26 
2024-01-31 17:11:28,816 EPOCH 30
2024-01-31 17:11:35,476 [Epoch: 030 Step: 00001000] Batch Recognition Loss:   6.221476 => Gls Tokens per Sec:      673 || Batch Translation Loss:  50.872231 => Txt Tokens per Sec:     1807 || Lr: 0.000100
2024-01-31 17:11:46,370 Epoch  30: Total Training Recognition Loss 1098.01  Total Training Translation Loss 3487.01 
2024-01-31 17:11:46,370 EPOCH 31
2024-01-31 17:12:03,807 Epoch  31: Total Training Recognition Loss 1098.29  Total Training Translation Loss 3488.19 
2024-01-31 17:12:03,807 EPOCH 32
2024-01-31 17:12:21,445 Epoch  32: Total Training Recognition Loss 1102.82  Total Training Translation Loss 3487.82 
2024-01-31 17:12:21,445 EPOCH 33
2024-01-31 17:12:26,636 [Epoch: 033 Step: 00001100] Batch Recognition Loss:  42.285065 => Gls Tokens per Sec:      692 || Batch Translation Loss: 130.478027 => Txt Tokens per Sec:     1855 || Lr: 0.000100
2024-01-31 17:12:39,034 Epoch  33: Total Training Recognition Loss 1101.63  Total Training Translation Loss 3486.63 
2024-01-31 17:12:39,035 EPOCH 34
2024-01-31 17:12:56,629 Epoch  34: Total Training Recognition Loss 1100.10  Total Training Translation Loss 3488.00 
2024-01-31 17:12:56,630 EPOCH 35
2024-01-31 17:13:14,059 Epoch  35: Total Training Recognition Loss 1102.01  Total Training Translation Loss 3487.48 
2024-01-31 17:13:14,060 EPOCH 36
2024-01-31 17:13:19,918 [Epoch: 036 Step: 00001200] Batch Recognition Loss:  38.346375 => Gls Tokens per Sec:      504 || Batch Translation Loss: 120.823242 => Txt Tokens per Sec:     1435 || Lr: 0.000100
2024-01-31 17:13:31,414 Epoch  36: Total Training Recognition Loss 1099.60  Total Training Translation Loss 3487.70 
2024-01-31 17:13:31,415 EPOCH 37
2024-01-31 17:13:48,868 Epoch  37: Total Training Recognition Loss 1098.79  Total Training Translation Loss 3487.47 
2024-01-31 17:13:48,868 EPOCH 38
2024-01-31 17:14:06,207 Epoch  38: Total Training Recognition Loss 1099.31  Total Training Translation Loss 3487.69 
2024-01-31 17:14:06,208 EPOCH 39
2024-01-31 17:14:10,087 [Epoch: 039 Step: 00001300] Batch Recognition Loss:  30.044140 => Gls Tokens per Sec:      660 || Batch Translation Loss: 100.800201 => Txt Tokens per Sec:     1791 || Lr: 0.000100
2024-01-31 17:14:27,870 Epoch  39: Total Training Recognition Loss 1098.78  Total Training Translation Loss 3487.84 
2024-01-31 17:14:27,871 EPOCH 40
2024-01-31 17:14:46,189 Epoch  40: Total Training Recognition Loss 1099.64  Total Training Translation Loss 3487.20 
2024-01-31 17:14:46,190 EPOCH 41
2024-01-31 17:15:03,616 Epoch  41: Total Training Recognition Loss 1098.94  Total Training Translation Loss 3487.52 
2024-01-31 17:15:03,616 EPOCH 42
2024-01-31 17:15:07,689 [Epoch: 042 Step: 00001400] Batch Recognition Loss:  17.232054 => Gls Tokens per Sec:      472 || Batch Translation Loss:  75.645844 => Txt Tokens per Sec:     1198 || Lr: 0.000100
2024-01-31 17:15:21,140 Epoch  42: Total Training Recognition Loss 1100.36  Total Training Translation Loss 3487.17 
2024-01-31 17:15:21,140 EPOCH 43
2024-01-31 17:15:39,759 Epoch  43: Total Training Recognition Loss 1099.59  Total Training Translation Loss 3486.92 
2024-01-31 17:15:39,759 EPOCH 44
2024-01-31 17:15:58,343 Epoch  44: Total Training Recognition Loss 1103.42  Total Training Translation Loss 3487.21 
2024-01-31 17:15:58,343 EPOCH 45
2024-01-31 17:16:00,582 [Epoch: 045 Step: 00001500] Batch Recognition Loss:  39.513618 => Gls Tokens per Sec:      460 || Batch Translation Loss: 114.597916 => Txt Tokens per Sec:     1223 || Lr: 0.000100
2024-01-31 17:16:15,945 Epoch  45: Total Training Recognition Loss 1099.89  Total Training Translation Loss 3486.97 
2024-01-31 17:16:15,945 EPOCH 46
2024-01-31 17:16:33,453 Epoch  46: Total Training Recognition Loss 1101.33  Total Training Translation Loss 3487.52 
2024-01-31 17:16:33,453 EPOCH 47
2024-01-31 17:16:51,030 Epoch  47: Total Training Recognition Loss 1099.94  Total Training Translation Loss 3486.74 
2024-01-31 17:16:51,031 EPOCH 48
2024-01-31 17:16:51,992 [Epoch: 048 Step: 00001600] Batch Recognition Loss:  22.794886 => Gls Tokens per Sec:      667 || Batch Translation Loss:  81.075577 => Txt Tokens per Sec:     1822 || Lr: 0.000100
2024-01-31 17:17:08,627 Epoch  48: Total Training Recognition Loss 1100.95  Total Training Translation Loss 3488.13 
2024-01-31 17:17:08,627 EPOCH 49
2024-01-31 17:17:26,182 Epoch  49: Total Training Recognition Loss 1100.45  Total Training Translation Loss 3486.99 
2024-01-31 17:17:26,182 EPOCH 50
2024-01-31 17:17:43,628 [Epoch: 050 Step: 00001700] Batch Recognition Loss:  36.487049 => Gls Tokens per Sec:      609 || Batch Translation Loss: 120.080956 => Txt Tokens per Sec:     1691 || Lr: 0.000100
2024-01-31 17:17:43,629 Epoch  50: Total Training Recognition Loss 1101.58  Total Training Translation Loss 3486.82 
2024-01-31 17:17:43,629 EPOCH 51
2024-01-31 17:18:03,332 Epoch  51: Total Training Recognition Loss 1099.73  Total Training Translation Loss 3487.05 
2024-01-31 17:18:03,333 EPOCH 52
2024-01-31 17:18:21,894 Epoch  52: Total Training Recognition Loss 1101.10  Total Training Translation Loss 3487.04 
2024-01-31 17:18:21,894 EPOCH 53
2024-01-31 17:18:38,789 [Epoch: 053 Step: 00001800] Batch Recognition Loss:  13.163236 => Gls Tokens per Sec:      591 || Batch Translation Loss:  69.913727 => Txt Tokens per Sec:     1646 || Lr: 0.000100
2024-01-31 17:18:39,573 Epoch  53: Total Training Recognition Loss 1099.53  Total Training Translation Loss 3487.25 
2024-01-31 17:18:39,574 EPOCH 54
2024-01-31 17:18:56,981 Epoch  54: Total Training Recognition Loss 1101.28  Total Training Translation Loss 3486.92 
2024-01-31 17:18:56,981 EPOCH 55
2024-01-31 17:19:14,432 Epoch  55: Total Training Recognition Loss 1099.54  Total Training Translation Loss 3487.84 
2024-01-31 17:19:14,432 EPOCH 56
2024-01-31 17:19:30,524 [Epoch: 056 Step: 00001900] Batch Recognition Loss:  20.777441 => Gls Tokens per Sec:      581 || Batch Translation Loss:  88.503021 => Txt Tokens per Sec:     1653 || Lr: 0.000100
2024-01-31 17:19:31,891 Epoch  56: Total Training Recognition Loss 1101.32  Total Training Translation Loss 3487.37 
2024-01-31 17:19:31,892 EPOCH 57
2024-01-31 17:19:49,320 Epoch  57: Total Training Recognition Loss 1101.15  Total Training Translation Loss 3487.10 
2024-01-31 17:19:49,320 EPOCH 58
2024-01-31 17:20:06,837 Epoch  58: Total Training Recognition Loss 1103.72  Total Training Translation Loss 3487.25 
2024-01-31 17:20:06,838 EPOCH 59
2024-01-31 17:20:20,759 [Epoch: 059 Step: 00002000] Batch Recognition Loss:  37.317482 => Gls Tokens per Sec:      644 || Batch Translation Loss: 116.084404 => Txt Tokens per Sec:     1766 || Lr: 0.000100
2024-01-31 17:20:46,095 Hooray! New best validation result [eval_metric]!
2024-01-31 17:20:46,103 Saving new checkpoint.
2024-01-31 17:20:47,140 Validation result at epoch  59, step     2000: duration: 26.3808s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 348.47968	Translation Loss: 73466.89844	PPL: 1559.05676
	Eval Metric: BLEU
	WER 533.83	(DEL: 4.38,	INS: 444.56,	SUB: 84.89)
	BLEU-4 0.00	(BLEU-1: 0.01,	BLEU-2: 0.00,	BLEU-3: 0.00,	BLEU-4: 0.00)
	CHRF 3.51	ROUGE 0.02
2024-01-31 17:20:47,142 Logging Recognition and Translation Outputs
2024-01-31 17:20:47,142 ========================================================================================================================
2024-01-31 17:20:47,142 Logging Sequence: 182_115.00
2024-01-31 17:20:47,143 	Gloss Reference :	A B+C+D+E
2024-01-31 17:20:47,143 	Gloss Hypothesis:	E C+E+B+E
2024-01-31 17:20:47,143 	Gloss Alignment :	S S      
2024-01-31 17:20:47,143 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 17:20:47,147 	Text Reference  :	*** *** *** *** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** fans   are    unclear whether yuvraj will   be     returning to     play   test   match  odi    or     in     t20    leagues from   february 2022  
2024-01-31 17:20:47,147 	Text Hypothesis :	<s> <s> <s> <s> chosen chosen chosen chosen chosen chosen chosen chosen chosen chosen chosen chosen chosen chosen chosen chosen chosen chosen chosen  chosen  chosen chosen chosen chosen    chosen chosen chosen chosen chosen chosen chosen chosen chosen  chosen chosen   chosen
2024-01-31 17:20:47,147 	Text Alignment  :	I   I   I   I   I      I      I      I      I      I      I      I      I      I      I      I      I      I      I      I      S      S      S       S       S      S      S      S         S      S      S      S      S      S      S      S      S       S      S        S     
2024-01-31 17:20:47,148 ========================================================================================================================
2024-01-31 17:20:47,148 Logging Sequence: 140_120.00
2024-01-31 17:20:47,148 	Gloss Reference :	A B+C+D+E
2024-01-31 17:20:47,148 	Gloss Hypothesis:	* E+C+E  
2024-01-31 17:20:47,148 	Gloss Alignment :	D S      
2024-01-31 17:20:47,148 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 17:20:47,153 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** but why so  it  is  because pant is  a   talented player and it  will help encouraging the youth of  uttarakhand toward sports
2024-01-31 17:20:47,153 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s>     <s>  <s> <s> <s>      <s>    <s> <s> <s>  <s>  <s>         <s> <s>   <s> <s>         <s>    <s>   
2024-01-31 17:20:47,153 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   S   S   S   S   S   S       S    S   S   S        S      S   S   S    S    S           S   S     S   S           S      S     
2024-01-31 17:20:47,153 ========================================================================================================================
2024-01-31 17:20:47,153 Logging Sequence: 85_36.00
2024-01-31 17:20:47,154 	Gloss Reference :	***** * ***** ***** A ***** * ***** * ***** *** ***** ********* ***** * ***** * ***** B+C+D+E
2024-01-31 17:20:47,154 	Gloss Hypothesis:	<unk> E <pad> <unk> A <unk> C <unk> E <unk> E+C <pad> C+E+C+E+C <unk> C <unk> E <unk> E      
2024-01-31 17:20:47,154 	Gloss Alignment :	I     I I     I       I     I I     I I     I   I     I         I     I I     I I     S      
2024-01-31 17:20:47,154 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 17:20:47,157 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ******* ******* ******* ******* ******* ******* ******* ******* symonds has     scored  2       centuries in      26      tests   that    he      played  for     his     country
2024-01-31 17:20:47,157 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing   nothing nothing nothing nothing nothing nothing nothing nothing nothing
2024-01-31 17:20:47,158 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I       I       I       I       I       I       I       I       S       S       S       S       S         S       S       S       S       S       S       S       S       S      
2024-01-31 17:20:47,158 ========================================================================================================================
2024-01-31 17:20:47,158 Logging Sequence: 164_100.00
2024-01-31 17:20:47,158 	Gloss Reference :	A B+C+D+E                
2024-01-31 17:20:47,158 	Gloss Hypothesis:	E D+E+D+E+C+E+D+E+C+E+C+E
2024-01-31 17:20:47,158 	Gloss Alignment :	S S                      
2024-01-31 17:20:47,158 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 17:20:47,163 	Text Reference  :	*** *** *** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** the    tv     rights for    broadcasting ipl    matches in     india  for    the    next   5      years  went   to     star   india  for    rs     23575  crore 
2024-01-31 17:20:47,163 	Text Hypothesis :	<s> <s> <s> boards boards boards boards boards boards boards boards boards boards boards boards boards boards boards boards boards boards boards boards       boards boards  boards boards boards boards boards boards boards boards boards boards boards boards boards boards boards
2024-01-31 17:20:47,163 	Text Alignment  :	I   I   I   I      I      I      I      I      I      I      I      I      I      I      I      I      I      I      S      S      S      S      S            S      S       S      S      S      S      S      S      S      S      S      S      S      S      S      S      S     
2024-01-31 17:20:47,163 ========================================================================================================================
2024-01-31 17:20:47,164 Logging Sequence: 76_79.00
2024-01-31 17:20:47,164 	Gloss Reference :	***** ********* ***** * A     B+C+D+E    
2024-01-31 17:20:47,164 	Gloss Hypothesis:	<unk> E+A+E+A+E <unk> C <unk> E+B+C+E+B+E
2024-01-31 17:20:47,164 	Gloss Alignment :	I     I         I     I S     S          
2024-01-31 17:20:47,164 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 17:20:47,166 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* speaking to      ani     csk     ceo     kasi    viswanathan said   
2024-01-31 17:20:47,166 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing  nothing nothing nothing nothing nothing nothing     nothing
2024-01-31 17:20:47,166 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       S        S       S       S       S       S       S           S      
2024-01-31 17:20:47,166 ========================================================================================================================
2024-01-31 17:20:51,132 Epoch  59: Total Training Recognition Loss 1099.50  Total Training Translation Loss 3487.17 
2024-01-31 17:20:51,132 EPOCH 60
2024-01-31 17:21:09,925 Epoch  60: Total Training Recognition Loss 1101.12  Total Training Translation Loss 3487.18 
2024-01-31 17:21:09,925 EPOCH 61
2024-01-31 17:21:27,528 Epoch  61: Total Training Recognition Loss 1099.53  Total Training Translation Loss 3487.12 
2024-01-31 17:21:27,529 EPOCH 62
2024-01-31 17:21:41,423 [Epoch: 062 Step: 00002100] Batch Recognition Loss:  17.329357 => Gls Tokens per Sec:      581 || Batch Translation Loss:  76.363342 => Txt Tokens per Sec:     1592 || Lr: 0.000100
2024-01-31 17:21:45,247 Epoch  62: Total Training Recognition Loss 1101.31  Total Training Translation Loss 3487.67 
2024-01-31 17:21:45,248 EPOCH 63
2024-01-31 17:22:02,730 Epoch  63: Total Training Recognition Loss 1099.55  Total Training Translation Loss 3486.76 
2024-01-31 17:22:02,730 EPOCH 64
2024-01-31 17:22:20,271 Epoch  64: Total Training Recognition Loss 1099.44  Total Training Translation Loss 3487.42 
2024-01-31 17:22:20,271 EPOCH 65
2024-01-31 17:22:33,989 [Epoch: 065 Step: 00002200] Batch Recognition Loss:  72.696007 => Gls Tokens per Sec:      542 || Batch Translation Loss: 126.603645 => Txt Tokens per Sec:     1562 || Lr: 0.000100
2024-01-31 17:22:37,767 Epoch  65: Total Training Recognition Loss 1100.67  Total Training Translation Loss 3488.44 
2024-01-31 17:22:37,767 EPOCH 66
2024-01-31 17:22:55,206 Epoch  66: Total Training Recognition Loss 1102.40  Total Training Translation Loss 3487.38 
2024-01-31 17:22:55,206 EPOCH 67
2024-01-31 17:23:12,747 Epoch  67: Total Training Recognition Loss 1100.10  Total Training Translation Loss 3487.64 
2024-01-31 17:23:12,748 EPOCH 68
2024-01-31 17:23:25,088 [Epoch: 068 Step: 00002300] Batch Recognition Loss:  15.634825 => Gls Tokens per Sec:      550 || Batch Translation Loss:  74.053146 => Txt Tokens per Sec:     1540 || Lr: 0.000100
2024-01-31 17:23:30,218 Epoch  68: Total Training Recognition Loss 1102.02  Total Training Translation Loss 3487.48 
2024-01-31 17:23:30,218 EPOCH 69
2024-01-31 17:23:47,729 Epoch  69: Total Training Recognition Loss 1099.83  Total Training Translation Loss 3487.68 
2024-01-31 17:23:47,729 EPOCH 70
2024-01-31 17:24:05,139 Epoch  70: Total Training Recognition Loss 1101.57  Total Training Translation Loss 3487.34 
2024-01-31 17:24:05,139 EPOCH 71
2024-01-31 17:24:14,967 [Epoch: 071 Step: 00002400] Batch Recognition Loss:  46.577591 => Gls Tokens per Sec:      626 || Batch Translation Loss: 124.952011 => Txt Tokens per Sec:     1725 || Lr: 0.000100
2024-01-31 17:24:22,614 Epoch  71: Total Training Recognition Loss 1101.84  Total Training Translation Loss 3487.58 
2024-01-31 17:24:22,615 EPOCH 72
2024-01-31 17:24:40,129 Epoch  72: Total Training Recognition Loss 1100.97  Total Training Translation Loss 3486.87 
2024-01-31 17:24:40,129 EPOCH 73
2024-01-31 17:24:57,592 Epoch  73: Total Training Recognition Loss 1101.29  Total Training Translation Loss 3487.56 
2024-01-31 17:24:57,593 EPOCH 74
2024-01-31 17:25:06,953 [Epoch: 074 Step: 00002500] Batch Recognition Loss:  18.867859 => Gls Tokens per Sec:      589 || Batch Translation Loss:  91.975822 => Txt Tokens per Sec:     1645 || Lr: 0.000100
2024-01-31 17:25:14,957 Epoch  74: Total Training Recognition Loss 1099.54  Total Training Translation Loss 3486.96 
2024-01-31 17:25:14,957 EPOCH 75
2024-01-31 17:25:32,426 Epoch  75: Total Training Recognition Loss 1099.95  Total Training Translation Loss 3487.50 
2024-01-31 17:25:32,426 EPOCH 76
2024-01-31 17:25:49,972 Epoch  76: Total Training Recognition Loss 1100.56  Total Training Translation Loss 3487.49 
2024-01-31 17:25:49,973 EPOCH 77
2024-01-31 17:25:57,713 [Epoch: 077 Step: 00002600] Batch Recognition Loss:  28.302353 => Gls Tokens per Sec:      629 || Batch Translation Loss:  89.090897 => Txt Tokens per Sec:     1719 || Lr: 0.000100
2024-01-31 17:26:07,400 Epoch  77: Total Training Recognition Loss 1101.23  Total Training Translation Loss 3487.56 
2024-01-31 17:26:07,401 EPOCH 78
2024-01-31 17:26:24,763 Epoch  78: Total Training Recognition Loss 1100.85  Total Training Translation Loss 3487.17 
2024-01-31 17:26:24,763 EPOCH 79
2024-01-31 17:26:42,157 Epoch  79: Total Training Recognition Loss 1101.53  Total Training Translation Loss 3486.96 
2024-01-31 17:26:42,157 EPOCH 80
2024-01-31 17:26:49,681 [Epoch: 080 Step: 00002700] Batch Recognition Loss:  28.512310 => Gls Tokens per Sec:      562 || Batch Translation Loss:  88.333511 => Txt Tokens per Sec:     1520 || Lr: 0.000100
2024-01-31 17:26:59,751 Epoch  80: Total Training Recognition Loss 1100.52  Total Training Translation Loss 3487.35 
2024-01-31 17:26:59,751 EPOCH 81
2024-01-31 17:27:17,191 Epoch  81: Total Training Recognition Loss 1101.11  Total Training Translation Loss 3486.88 
2024-01-31 17:27:17,191 EPOCH 82
2024-01-31 17:27:34,741 Epoch  82: Total Training Recognition Loss 1100.14  Total Training Translation Loss 3487.82 
2024-01-31 17:27:34,741 EPOCH 83
2024-01-31 17:27:40,915 [Epoch: 083 Step: 00002800] Batch Recognition Loss:  46.054035 => Gls Tokens per Sec:      622 || Batch Translation Loss: 124.661575 => Txt Tokens per Sec:     1791 || Lr: 0.000100
2024-01-31 17:27:52,121 Epoch  83: Total Training Recognition Loss 1102.95  Total Training Translation Loss 3487.83 
2024-01-31 17:27:52,121 EPOCH 84
2024-01-31 17:28:09,678 Epoch  84: Total Training Recognition Loss 1102.35  Total Training Translation Loss 3488.08 
2024-01-31 17:28:09,678 EPOCH 85
2024-01-31 17:28:27,241 Epoch  85: Total Training Recognition Loss 1101.73  Total Training Translation Loss 3487.46 
2024-01-31 17:28:27,242 EPOCH 86
2024-01-31 17:28:34,241 [Epoch: 086 Step: 00002900] Batch Recognition Loss:  22.725616 => Gls Tokens per Sec:      457 || Batch Translation Loss:  87.793892 => Txt Tokens per Sec:     1373 || Lr: 0.000100
2024-01-31 17:28:44,766 Epoch  86: Total Training Recognition Loss 1099.86  Total Training Translation Loss 3487.96 
2024-01-31 17:28:44,766 EPOCH 87
2024-01-31 17:29:02,196 Epoch  87: Total Training Recognition Loss 1102.62  Total Training Translation Loss 3486.24 
2024-01-31 17:29:02,197 EPOCH 88
2024-01-31 17:29:19,786 Epoch  88: Total Training Recognition Loss 1099.32  Total Training Translation Loss 3488.06 
2024-01-31 17:29:19,786 EPOCH 89
2024-01-31 17:29:23,343 [Epoch: 089 Step: 00003000] Batch Recognition Loss:  33.130955 => Gls Tokens per Sec:      650 || Batch Translation Loss: 111.257904 => Txt Tokens per Sec:     1696 || Lr: 0.000100
2024-01-31 17:29:37,246 Epoch  89: Total Training Recognition Loss 1101.49  Total Training Translation Loss 3487.65 
2024-01-31 17:29:37,246 EPOCH 90
2024-01-31 17:29:54,794 Epoch  90: Total Training Recognition Loss 1099.16  Total Training Translation Loss 3485.61 
2024-01-31 17:29:54,794 EPOCH 91
2024-01-31 17:30:12,255 Epoch  91: Total Training Recognition Loss 1100.82  Total Training Translation Loss 3486.83 
2024-01-31 17:30:12,255 EPOCH 92
2024-01-31 17:30:14,324 [Epoch: 092 Step: 00003100] Batch Recognition Loss:  35.196095 => Gls Tokens per Sec:      929 || Batch Translation Loss: 107.897446 => Txt Tokens per Sec:     2180 || Lr: 0.000100
2024-01-31 17:30:29,679 Epoch  92: Total Training Recognition Loss 1101.75  Total Training Translation Loss 3488.03 
2024-01-31 17:30:29,679 EPOCH 93
2024-01-31 17:30:47,147 Epoch  93: Total Training Recognition Loss 1102.18  Total Training Translation Loss 3486.66 
2024-01-31 17:30:47,148 EPOCH 94
2024-01-31 17:31:04,539 Epoch  94: Total Training Recognition Loss 1100.46  Total Training Translation Loss 3487.69 
2024-01-31 17:31:04,540 EPOCH 95
2024-01-31 17:31:07,573 [Epoch: 095 Step: 00003200] Batch Recognition Loss:  31.472326 => Gls Tokens per Sec:      422 || Batch Translation Loss:  97.580055 => Txt Tokens per Sec:     1327 || Lr: 0.000100
2024-01-31 17:31:22,122 Epoch  95: Total Training Recognition Loss 1101.10  Total Training Translation Loss 3487.33 
2024-01-31 17:31:22,122 EPOCH 96
2024-01-31 17:31:39,607 Epoch  96: Total Training Recognition Loss 1101.06  Total Training Translation Loss 3487.65 
2024-01-31 17:31:39,607 EPOCH 97
2024-01-31 17:31:57,178 Epoch  97: Total Training Recognition Loss 1099.39  Total Training Translation Loss 3487.23 
2024-01-31 17:31:57,178 EPOCH 98
2024-01-31 17:31:58,047 [Epoch: 098 Step: 00003300] Batch Recognition Loss:  18.827513 => Gls Tokens per Sec:      738 || Batch Translation Loss:  91.551376 => Txt Tokens per Sec:     2016 || Lr: 0.000100
2024-01-31 17:32:14,702 Epoch  98: Total Training Recognition Loss 1100.34  Total Training Translation Loss 3487.52 
2024-01-31 17:32:14,702 EPOCH 99
2024-01-31 17:32:32,214 Epoch  99: Total Training Recognition Loss 1100.84  Total Training Translation Loss 3487.27 
2024-01-31 17:32:32,215 EPOCH 100
2024-01-31 17:32:50,003 [Epoch: 100 Step: 00003400] Batch Recognition Loss:  44.833347 => Gls Tokens per Sec:      598 || Batch Translation Loss: 132.035675 => Txt Tokens per Sec:     1659 || Lr: 0.000100
2024-01-31 17:32:50,003 Epoch 100: Total Training Recognition Loss 1099.86  Total Training Translation Loss 3487.10 
2024-01-31 17:32:50,003 EPOCH 101
2024-01-31 17:33:08,016 Epoch 101: Total Training Recognition Loss 1099.17  Total Training Translation Loss 3487.39 
2024-01-31 17:33:08,016 EPOCH 102
2024-01-31 17:33:25,557 Epoch 102: Total Training Recognition Loss 1100.05  Total Training Translation Loss 3487.03 
2024-01-31 17:33:25,557 EPOCH 103
2024-01-31 17:33:41,933 [Epoch: 103 Step: 00003500] Batch Recognition Loss:  44.221107 => Gls Tokens per Sec:      610 || Batch Translation Loss: 128.296051 => Txt Tokens per Sec:     1677 || Lr: 0.000100
2024-01-31 17:33:43,123 Epoch 103: Total Training Recognition Loss 1101.53  Total Training Translation Loss 3487.39 
2024-01-31 17:33:43,123 EPOCH 104
2024-01-31 17:34:00,617 Epoch 104: Total Training Recognition Loss 1100.01  Total Training Translation Loss 3487.75 
2024-01-31 17:34:00,618 EPOCH 105
2024-01-31 17:34:18,092 Epoch 105: Total Training Recognition Loss 1099.16  Total Training Translation Loss 3487.34 
2024-01-31 17:34:18,093 EPOCH 106
2024-01-31 17:34:32,566 [Epoch: 106 Step: 00003600] Batch Recognition Loss:  23.148514 => Gls Tokens per Sec:      646 || Batch Translation Loss:  95.033066 => Txt Tokens per Sec:     1789 || Lr: 0.000100
2024-01-31 17:34:35,493 Epoch 106: Total Training Recognition Loss 1100.80  Total Training Translation Loss 3486.90 
2024-01-31 17:34:35,493 EPOCH 107
2024-01-31 17:34:52,906 Epoch 107: Total Training Recognition Loss 1101.24  Total Training Translation Loss 3487.34 
2024-01-31 17:34:52,906 EPOCH 108
2024-01-31 17:35:10,316 Epoch 108: Total Training Recognition Loss 1101.80  Total Training Translation Loss 3487.46 
2024-01-31 17:35:10,316 EPOCH 109
2024-01-31 17:35:23,917 [Epoch: 109 Step: 00003700] Batch Recognition Loss:  23.187778 => Gls Tokens per Sec:      641 || Batch Translation Loss:  94.004868 => Txt Tokens per Sec:     1793 || Lr: 0.000100
2024-01-31 17:35:27,687 Epoch 109: Total Training Recognition Loss 1100.87  Total Training Translation Loss 3487.53 
2024-01-31 17:35:27,688 EPOCH 110
2024-01-31 17:35:45,136 Epoch 110: Total Training Recognition Loss 1101.90  Total Training Translation Loss 3486.60 
2024-01-31 17:35:45,136 EPOCH 111
2024-01-31 17:36:02,635 Epoch 111: Total Training Recognition Loss 1099.17  Total Training Translation Loss 3487.33 
2024-01-31 17:36:02,635 EPOCH 112
2024-01-31 17:36:16,630 [Epoch: 112 Step: 00003800] Batch Recognition Loss:  30.895981 => Gls Tokens per Sec:      577 || Batch Translation Loss: 103.472137 => Txt Tokens per Sec:     1606 || Lr: 0.000100
2024-01-31 17:36:20,233 Epoch 112: Total Training Recognition Loss 1101.15  Total Training Translation Loss 3487.24 
2024-01-31 17:36:20,233 EPOCH 113
2024-01-31 17:36:37,722 Epoch 113: Total Training Recognition Loss 1100.19  Total Training Translation Loss 3486.64 
2024-01-31 17:36:37,722 EPOCH 114
2024-01-31 17:36:55,138 Epoch 114: Total Training Recognition Loss 1100.05  Total Training Translation Loss 3486.27 
2024-01-31 17:36:55,138 EPOCH 115
2024-01-31 17:37:08,650 [Epoch: 115 Step: 00003900] Batch Recognition Loss:  38.184860 => Gls Tokens per Sec:      550 || Batch Translation Loss: 123.402931 => Txt Tokens per Sec:     1593 || Lr: 0.000100
2024-01-31 17:37:12,712 Epoch 115: Total Training Recognition Loss 1097.60  Total Training Translation Loss 3486.78 
2024-01-31 17:37:12,712 EPOCH 116
2024-01-31 17:37:30,179 Epoch 116: Total Training Recognition Loss 1101.87  Total Training Translation Loss 3487.59 
2024-01-31 17:37:30,179 EPOCH 117
2024-01-31 17:37:47,775 Epoch 117: Total Training Recognition Loss 1100.86  Total Training Translation Loss 3486.75 
2024-01-31 17:37:47,775 EPOCH 118
2024-01-31 17:37:59,139 [Epoch: 118 Step: 00004000] Batch Recognition Loss:  30.289402 => Gls Tokens per Sec:      620 || Batch Translation Loss: 100.540581 => Txt Tokens per Sec:     1695 || Lr: 0.000100
2024-01-31 17:38:23,229 Validation result at epoch 118, step     4000: duration: 24.0892s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 351.66663	Translation Loss: 73469.18750	PPL: 1559.41345
	Eval Metric: BLEU
	WER 537.01	(DEL: 3.95,	INS: 447.88,	SUB: 85.17)
	BLEU-4 0.00	(BLEU-1: 0.01,	BLEU-2: 0.00,	BLEU-3: 0.00,	BLEU-4: 0.00)
	CHRF 3.50	ROUGE 0.02
2024-01-31 17:38:23,231 Logging Recognition and Translation Outputs
2024-01-31 17:38:23,231 ========================================================================================================================
2024-01-31 17:38:23,231 Logging Sequence: 133_173.00
2024-01-31 17:38:23,231 	Gloss Reference :	A B+C+D+E
2024-01-31 17:38:23,231 	Gloss Hypothesis:	* E      
2024-01-31 17:38:23,231 	Gloss Alignment :	D S      
2024-01-31 17:38:23,232 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 17:38:23,235 	Text Reference  :	*** *** *** *** ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* according to      sources the     leaders of      the     two     countries are     set     to      join    the     commentary panel   as      well   
2024-01-31 17:38:23,236 	Text Hypothesis :	<s> <s> <s> <s> ashwini ashwini ashwini ashwini ashwini ashwini ashwini ashwini ashwini ashwini ashwini ashwini ashwini ashwini ashwini ashwini ashwini ashwini ashwini   ashwini ashwini ashwini ashwini ashwini ashwini ashwini ashwini   ashwini ashwini ashwini ashwini ashwini ashwini    ashwini ashwini ashwini
2024-01-31 17:38:23,236 	Text Alignment  :	I   I   I   I   I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       S         S       S       S       S       S       S       S       S         S       S       S       S       S       S          S       S       S      
2024-01-31 17:38:23,236 ========================================================================================================================
2024-01-31 17:38:23,236 Logging Sequence: 83_33.00
2024-01-31 17:38:23,237 	Gloss Reference :	***** *** ***** * ***** * ***** ************* ***** ************************* ***** ***** * ***** ***** * ***** ***** A     B+C+D+E                      
2024-01-31 17:38:23,237 	Gloss Hypothesis:	<pad> B+E <pad> B <unk> C <unk> C+D+E+B+E+C+B <unk> C+B+C+E+C+E+D+C+E+C+E+B+E <pad> <unk> E <pad> <unk> C <unk> <pad> <unk> E+C+E+C+E+B+E+C+E+C+E+C+E+C+E
2024-01-31 17:38:23,237 	Gloss Alignment :	I     I   I     I I     I I     I             I     I                         I     I     I I     I     I I     I     S     S                            
2024-01-31 17:38:23,237 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 17:38:23,239 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** a   football match lasts for two equal halves of  45  minutes
2024-01-31 17:38:23,239 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s>      <s>   <s>   <s> <s> <s>   <s>    <s> <s> <s>    
2024-01-31 17:38:23,240 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   S   S        S     S     S   S   S     S      S   S   S      
2024-01-31 17:38:23,240 ========================================================================================================================
2024-01-31 17:38:23,240 Logging Sequence: 68_147.00
2024-01-31 17:38:23,240 	Gloss Reference :	***** * ***** ******* ***** ******* ***** ************* ***** * ***** * ***** ********* ***** ************* ***** * A     B+C+D+E
2024-01-31 17:38:23,240 	Gloss Hypothesis:	<unk> C <pad> C+E+D+C <unk> C+E+B+E <unk> B+E+C+E+C+E+D <unk> E <unk> E <unk> E+C+E+C+B <unk> E+C+E+C+E+C+E <unk> E <unk> E      
2024-01-31 17:38:23,241 	Gloss Alignment :	I     I I     I       I     I       I     I             I     I I     I I     I         I     I             I     I S     S      
2024-01-31 17:38:23,241 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 17:38:23,244 	Text Reference  :	*** *** *** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** remember the   2007  t20   world cup   amid  a     lot   of    sledging by    english players
2024-01-31 17:38:23,244 	Text Hypothesis :	<s> <s> <s> sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha    sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha    sabha sabha   sabha  
2024-01-31 17:38:23,244 	Text Alignment  :	I   I   I   I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     S        S     S     S     S     S     S     S     S     S     S        S     S       S      
2024-01-31 17:38:23,244 ========================================================================================================================
2024-01-31 17:38:23,244 Logging Sequence: 165_8.00
2024-01-31 17:38:23,244 	Gloss Reference :	A     B+C+D+E
2024-01-31 17:38:23,244 	Gloss Hypothesis:	<unk> E      
2024-01-31 17:38:23,245 	Gloss Alignment :	S     S      
2024-01-31 17:38:23,245 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 17:38:23,247 	Text Reference  :	*** *** *** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** however many  don't believe in    it    it    varies among people
2024-01-31 17:38:23,247 	Text Hypothesis :	<s> <s> <s> sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha   sabha sabha sabha   sabha sabha sabha sabha  sabha sabha 
2024-01-31 17:38:23,247 	Text Alignment  :	I   I   I   I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     S       S     S     S       S     S     S     S      S     S     
2024-01-31 17:38:23,247 ========================================================================================================================
2024-01-31 17:38:23,247 Logging Sequence: 119_71.00
2024-01-31 17:38:23,248 	Gloss Reference :	* ***** * ***** * A     B+C+D+E
2024-01-31 17:38:23,248 	Gloss Hypothesis:	B <pad> B <pad> B <pad> B+E+B  
2024-01-31 17:38:23,248 	Gloss Alignment :	I I     I I     I S     S      
2024-01-31 17:38:23,248 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 17:38:23,251 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** *** *** *** ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* the           special       gold          devices       have          each          player'       names         and           jersey        numbers       next          to            the           camera       
2024-01-31 17:38:23,251 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions
2024-01-31 17:38:23,252 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I   I   I   I             I             I             I             I             I             I             I             I             I             I             S             S             S             S             S             S             S             S             S             S             S             S             S             S             S            
2024-01-31 17:38:23,252 ========================================================================================================================
2024-01-31 17:38:29,368 Epoch 118: Total Training Recognition Loss 1098.71  Total Training Translation Loss 3487.44 
2024-01-31 17:38:29,368 EPOCH 119
2024-01-31 17:38:46,796 Epoch 119: Total Training Recognition Loss 1100.37  Total Training Translation Loss 3487.74 
2024-01-31 17:38:46,797 EPOCH 120
2024-01-31 17:39:04,317 Epoch 120: Total Training Recognition Loss 1102.45  Total Training Translation Loss 3486.94 
2024-01-31 17:39:04,317 EPOCH 121
2024-01-31 17:39:15,555 [Epoch: 121 Step: 00004100] Batch Recognition Loss:  32.611061 => Gls Tokens per Sec:      570 || Batch Translation Loss: 105.499664 => Txt Tokens per Sec:     1648 || Lr: 0.000100
2024-01-31 17:39:21,780 Epoch 121: Total Training Recognition Loss 1098.91  Total Training Translation Loss 3485.93 
2024-01-31 17:39:21,780 EPOCH 122
2024-01-31 17:39:39,278 Epoch 122: Total Training Recognition Loss 1099.53  Total Training Translation Loss 3486.03 
2024-01-31 17:39:39,278 EPOCH 123
2024-01-31 17:39:56,774 Epoch 123: Total Training Recognition Loss 1101.61  Total Training Translation Loss 3486.58 
2024-01-31 17:39:56,774 EPOCH 124
2024-01-31 17:40:04,865 [Epoch: 124 Step: 00004200] Batch Recognition Loss:  74.044373 => Gls Tokens per Sec:      681 || Batch Translation Loss: 126.672653 => Txt Tokens per Sec:     1790 || Lr: 0.000100
2024-01-31 17:40:14,252 Epoch 124: Total Training Recognition Loss 1100.78  Total Training Translation Loss 3487.10 
2024-01-31 17:40:14,252 EPOCH 125
2024-01-31 17:40:31,833 Epoch 125: Total Training Recognition Loss 1099.25  Total Training Translation Loss 3487.49 
2024-01-31 17:40:31,833 EPOCH 126
2024-01-31 17:40:49,226 Epoch 126: Total Training Recognition Loss 1100.43  Total Training Translation Loss 3486.57 
2024-01-31 17:40:49,226 EPOCH 127
2024-01-31 17:40:58,822 [Epoch: 127 Step: 00004300] Batch Recognition Loss:  15.214746 => Gls Tokens per Sec:      508 || Batch Translation Loss:  75.087227 => Txt Tokens per Sec:     1496 || Lr: 0.000100
2024-01-31 17:41:06,659 Epoch 127: Total Training Recognition Loss 1100.56  Total Training Translation Loss 3486.57 
2024-01-31 17:41:06,659 EPOCH 128
2024-01-31 17:41:24,238 Epoch 128: Total Training Recognition Loss 1101.44  Total Training Translation Loss 3486.59 
2024-01-31 17:41:24,238 EPOCH 129
2024-01-31 17:41:41,754 Epoch 129: Total Training Recognition Loss 1100.53  Total Training Translation Loss 3486.85 
2024-01-31 17:41:41,755 EPOCH 130
2024-01-31 17:41:49,083 [Epoch: 130 Step: 00004400] Batch Recognition Loss:  43.262875 => Gls Tokens per Sec:      611 || Batch Translation Loss: 130.117752 => Txt Tokens per Sec:     1748 || Lr: 0.000100
2024-01-31 17:41:59,222 Epoch 130: Total Training Recognition Loss 1101.76  Total Training Translation Loss 3486.38 
2024-01-31 17:41:59,223 EPOCH 131
2024-01-31 17:42:16,691 Epoch 131: Total Training Recognition Loss 1101.05  Total Training Translation Loss 3486.91 
2024-01-31 17:42:16,691 EPOCH 132
2024-01-31 17:42:34,113 Epoch 132: Total Training Recognition Loss 1101.05  Total Training Translation Loss 3486.32 
2024-01-31 17:42:34,113 EPOCH 133
2024-01-31 17:42:39,889 [Epoch: 133 Step: 00004500] Batch Recognition Loss:  22.998865 => Gls Tokens per Sec:      665 || Batch Translation Loss:  87.497765 => Txt Tokens per Sec:     1837 || Lr: 0.000100
2024-01-31 17:42:51,555 Epoch 133: Total Training Recognition Loss 1099.36  Total Training Translation Loss 3486.18 
2024-01-31 17:42:51,555 EPOCH 134
2024-01-31 17:43:08,995 Epoch 134: Total Training Recognition Loss 1100.26  Total Training Translation Loss 3486.94 
2024-01-31 17:43:08,995 EPOCH 135
2024-01-31 17:43:26,410 Epoch 135: Total Training Recognition Loss 1102.29  Total Training Translation Loss 3487.67 
2024-01-31 17:43:26,411 EPOCH 136
2024-01-31 17:43:31,565 [Epoch: 136 Step: 00004600] Batch Recognition Loss:  17.425175 => Gls Tokens per Sec:      621 || Batch Translation Loss:  78.223526 => Txt Tokens per Sec:     1830 || Lr: 0.000100
2024-01-31 17:43:43,988 Epoch 136: Total Training Recognition Loss 1100.03  Total Training Translation Loss 3487.62 
2024-01-31 17:43:43,989 EPOCH 137
2024-01-31 17:44:01,444 Epoch 137: Total Training Recognition Loss 1099.38  Total Training Translation Loss 3487.63 
2024-01-31 17:44:01,445 EPOCH 138
2024-01-31 17:44:19,011 Epoch 138: Total Training Recognition Loss 1101.37  Total Training Translation Loss 3487.92 
2024-01-31 17:44:19,011 EPOCH 139
2024-01-31 17:44:24,448 [Epoch: 139 Step: 00004700] Batch Recognition Loss:  27.922993 => Gls Tokens per Sec:      471 || Batch Translation Loss:  97.991623 => Txt Tokens per Sec:     1370 || Lr: 0.000100
2024-01-31 17:44:36,448 Epoch 139: Total Training Recognition Loss 1101.49  Total Training Translation Loss 3487.04 
2024-01-31 17:44:36,448 EPOCH 140
2024-01-31 17:44:54,026 Epoch 140: Total Training Recognition Loss 1102.70  Total Training Translation Loss 3487.98 
2024-01-31 17:44:54,026 EPOCH 141
2024-01-31 17:45:11,647 Epoch 141: Total Training Recognition Loss 1102.59  Total Training Translation Loss 3486.76 
2024-01-31 17:45:11,647 EPOCH 142
2024-01-31 17:45:15,186 [Epoch: 142 Step: 00004800] Batch Recognition Loss:  51.462036 => Gls Tokens per Sec:      543 || Batch Translation Loss: 128.879761 => Txt Tokens per Sec:     1613 || Lr: 0.000100
2024-01-31 17:45:29,092 Epoch 142: Total Training Recognition Loss 1098.59  Total Training Translation Loss 3486.61 
2024-01-31 17:45:29,093 EPOCH 143
2024-01-31 17:45:46,632 Epoch 143: Total Training Recognition Loss 1100.00  Total Training Translation Loss 3486.25 
2024-01-31 17:45:46,632 EPOCH 144
2024-01-31 17:46:04,210 Epoch 144: Total Training Recognition Loss 1098.89  Total Training Translation Loss 3486.78 
2024-01-31 17:46:04,211 EPOCH 145
2024-01-31 17:46:06,728 [Epoch: 145 Step: 00004900] Batch Recognition Loss:  11.014538 => Gls Tokens per Sec:      508 || Batch Translation Loss:  62.653801 => Txt Tokens per Sec:     1457 || Lr: 0.000100
2024-01-31 17:46:21,847 Epoch 145: Total Training Recognition Loss 1100.29  Total Training Translation Loss 3487.02 
2024-01-31 17:46:21,847 EPOCH 146
2024-01-31 17:46:39,257 Epoch 146: Total Training Recognition Loss 1102.95  Total Training Translation Loss 3487.13 
2024-01-31 17:46:39,257 EPOCH 147
2024-01-31 17:46:56,686 Epoch 147: Total Training Recognition Loss 1102.79  Total Training Translation Loss 3488.25 
2024-01-31 17:46:56,686 EPOCH 148
2024-01-31 17:46:58,067 [Epoch: 148 Step: 00005000] Batch Recognition Loss:  40.092262 => Gls Tokens per Sec:      464 || Batch Translation Loss: 117.118584 => Txt Tokens per Sec:     1484 || Lr: 0.000100
2024-01-31 17:47:14,071 Epoch 148: Total Training Recognition Loss 1100.86  Total Training Translation Loss 3487.89 
2024-01-31 17:47:14,071 EPOCH 149
2024-01-31 17:47:31,510 Epoch 149: Total Training Recognition Loss 1101.44  Total Training Translation Loss 3487.78 
2024-01-31 17:47:31,510 EPOCH 150
2024-01-31 17:47:49,075 [Epoch: 150 Step: 00005100] Batch Recognition Loss:  10.844518 => Gls Tokens per Sec:      605 || Batch Translation Loss:  62.369545 => Txt Tokens per Sec:     1680 || Lr: 0.000100
2024-01-31 17:47:49,076 Epoch 150: Total Training Recognition Loss 1102.03  Total Training Translation Loss 3486.90 
2024-01-31 17:47:49,076 EPOCH 151
2024-01-31 17:48:06,630 Epoch 151: Total Training Recognition Loss 1101.34  Total Training Translation Loss 3487.16 
2024-01-31 17:48:06,630 EPOCH 152
2024-01-31 17:48:24,114 Epoch 152: Total Training Recognition Loss 1098.23  Total Training Translation Loss 3486.94 
2024-01-31 17:48:24,114 EPOCH 153
2024-01-31 17:48:40,913 [Epoch: 153 Step: 00005200] Batch Recognition Loss:  36.916088 => Gls Tokens per Sec:      595 || Batch Translation Loss: 104.347923 => Txt Tokens per Sec:     1660 || Lr: 0.000100
2024-01-31 17:48:41,546 Epoch 153: Total Training Recognition Loss 1100.20  Total Training Translation Loss 3487.43 
2024-01-31 17:48:41,546 EPOCH 154
2024-01-31 17:48:58,935 Epoch 154: Total Training Recognition Loss 1103.61  Total Training Translation Loss 3486.49 
2024-01-31 17:48:58,936 EPOCH 155
2024-01-31 17:49:16,375 Epoch 155: Total Training Recognition Loss 1101.45  Total Training Translation Loss 3488.53 
2024-01-31 17:49:16,375 EPOCH 156
2024-01-31 17:49:31,890 [Epoch: 156 Step: 00005300] Batch Recognition Loss:  37.401436 => Gls Tokens per Sec:      603 || Batch Translation Loss: 118.132668 => Txt Tokens per Sec:     1674 || Lr: 0.000100
2024-01-31 17:49:33,708 Epoch 156: Total Training Recognition Loss 1099.54  Total Training Translation Loss 3487.35 
2024-01-31 17:49:33,708 EPOCH 157
2024-01-31 17:49:51,048 Epoch 157: Total Training Recognition Loss 1099.56  Total Training Translation Loss 3487.33 
2024-01-31 17:49:51,049 EPOCH 158
2024-01-31 17:50:08,440 Epoch 158: Total Training Recognition Loss 1101.56  Total Training Translation Loss 3485.79 
2024-01-31 17:50:08,440 EPOCH 159
2024-01-31 17:50:23,137 [Epoch: 159 Step: 00005400] Batch Recognition Loss:  45.903114 => Gls Tokens per Sec:      593 || Batch Translation Loss: 123.176163 => Txt Tokens per Sec:     1641 || Lr: 0.000100
2024-01-31 17:50:25,789 Epoch 159: Total Training Recognition Loss 1100.14  Total Training Translation Loss 3487.18 
2024-01-31 17:50:25,789 EPOCH 160
2024-01-31 17:50:44,405 Epoch 160: Total Training Recognition Loss 1100.97  Total Training Translation Loss 3487.36 
2024-01-31 17:50:44,406 EPOCH 161
2024-01-31 17:51:03,457 Epoch 161: Total Training Recognition Loss 1100.70  Total Training Translation Loss 3487.20 
2024-01-31 17:51:03,458 EPOCH 162
2024-01-31 17:51:17,107 [Epoch: 162 Step: 00005500] Batch Recognition Loss:  38.772038 => Gls Tokens per Sec:      610 || Batch Translation Loss: 120.632812 => Txt Tokens per Sec:     1705 || Lr: 0.000100
2024-01-31 17:51:20,875 Epoch 162: Total Training Recognition Loss 1100.92  Total Training Translation Loss 3487.58 
2024-01-31 17:51:20,876 EPOCH 163
2024-01-31 17:51:38,415 Epoch 163: Total Training Recognition Loss 1100.15  Total Training Translation Loss 3487.55 
2024-01-31 17:51:38,415 EPOCH 164
2024-01-31 17:51:56,075 Epoch 164: Total Training Recognition Loss 1102.27  Total Training Translation Loss 3487.80 
2024-01-31 17:51:56,075 EPOCH 165
2024-01-31 17:52:08,938 [Epoch: 165 Step: 00005600] Batch Recognition Loss:  36.577278 => Gls Tokens per Sec:      578 || Batch Translation Loss: 110.217682 => Txt Tokens per Sec:     1593 || Lr: 0.000100
2024-01-31 17:52:13,722 Epoch 165: Total Training Recognition Loss 1102.33  Total Training Translation Loss 3487.45 
2024-01-31 17:52:13,722 EPOCH 166
2024-01-31 17:52:31,329 Epoch 166: Total Training Recognition Loss 1099.29  Total Training Translation Loss 3487.81 
2024-01-31 17:52:31,329 EPOCH 167
2024-01-31 17:52:48,873 Epoch 167: Total Training Recognition Loss 1100.60  Total Training Translation Loss 3487.46 
2024-01-31 17:52:48,873 EPOCH 168
2024-01-31 17:53:00,106 [Epoch: 168 Step: 00005700] Batch Recognition Loss:  45.816734 => Gls Tokens per Sec:      605 || Batch Translation Loss: 124.551674 => Txt Tokens per Sec:     1685 || Lr: 0.000100
2024-01-31 17:53:06,441 Epoch 168: Total Training Recognition Loss 1099.84  Total Training Translation Loss 3487.73 
2024-01-31 17:53:06,441 EPOCH 169
2024-01-31 17:53:24,039 Epoch 169: Total Training Recognition Loss 1100.06  Total Training Translation Loss 3486.56 
2024-01-31 17:53:24,039 EPOCH 170
2024-01-31 17:53:41,521 Epoch 170: Total Training Recognition Loss 1099.03  Total Training Translation Loss 3486.15 
2024-01-31 17:53:41,521 EPOCH 171
2024-01-31 17:53:50,216 [Epoch: 171 Step: 00005800] Batch Recognition Loss:  27.499285 => Gls Tokens per Sec:      736 || Batch Translation Loss:  98.286621 => Txt Tokens per Sec:     1957 || Lr: 0.000100
2024-01-31 17:53:59,029 Epoch 171: Total Training Recognition Loss 1099.08  Total Training Translation Loss 3486.62 
2024-01-31 17:53:59,029 EPOCH 172
2024-01-31 17:54:16,519 Epoch 172: Total Training Recognition Loss 1100.15  Total Training Translation Loss 3487.41 
2024-01-31 17:54:16,519 EPOCH 173
2024-01-31 17:54:34,159 Epoch 173: Total Training Recognition Loss 1099.79  Total Training Translation Loss 3487.81 
2024-01-31 17:54:34,159 EPOCH 174
2024-01-31 17:54:44,647 [Epoch: 174 Step: 00005900] Batch Recognition Loss:  23.306623 => Gls Tokens per Sec:      549 || Batch Translation Loss:  94.178009 => Txt Tokens per Sec:     1556 || Lr: 0.000100
2024-01-31 17:54:51,729 Epoch 174: Total Training Recognition Loss 1098.67  Total Training Translation Loss 3486.71 
2024-01-31 17:54:51,729 EPOCH 175
2024-01-31 17:55:09,323 Epoch 175: Total Training Recognition Loss 1101.16  Total Training Translation Loss 3487.80 
2024-01-31 17:55:09,324 EPOCH 176
2024-01-31 17:55:26,885 Epoch 176: Total Training Recognition Loss 1099.20  Total Training Translation Loss 3487.59 
2024-01-31 17:55:26,885 EPOCH 177
2024-01-31 17:55:33,889 [Epoch: 177 Step: 00006000] Batch Recognition Loss:  33.241058 => Gls Tokens per Sec:      731 || Batch Translation Loss: 102.295578 => Txt Tokens per Sec:     1890 || Lr: 0.000100
2024-01-31 17:55:58,076 Validation result at epoch 177, step     6000: duration: 24.1872s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 346.78860	Translation Loss: 73500.24219	PPL: 1564.26794
	Eval Metric: BLEU
	WER 523.45	(DEL: 4.45,	INS: 434.75,	SUB: 84.25)
	BLEU-4 0.00	(BLEU-1: 0.01,	BLEU-2: 0.00,	BLEU-3: 0.00,	BLEU-4: 0.00)
	CHRF 3.49	ROUGE 0.02
2024-01-31 17:55:58,077 Logging Recognition and Translation Outputs
2024-01-31 17:55:58,077 ========================================================================================================================
2024-01-31 17:55:58,077 Logging Sequence: 89_111.00
2024-01-31 17:55:58,078 	Gloss Reference :	***** * ***** *** ***** * ***** * ***** A B+C+D+E
2024-01-31 17:55:58,078 	Gloss Hypothesis:	<unk> C <unk> E+C <unk> E <unk> C <unk> E <unk>  
2024-01-31 17:55:58,078 	Gloss Alignment :	I     I I     I   I     I I     I I     S S      
2024-01-31 17:55:58,078 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 17:55:58,080 	Text Reference  :	***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** however selectors never selected me    for   the   team 
2024-01-31 17:55:58,080 	Text Hypothesis :	sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha   sabha     sabha sabha    sabha sabha sabha sabha
2024-01-31 17:55:58,080 	Text Alignment  :	I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     S       S         S     S        S     S     S     S    
2024-01-31 17:55:58,081 ========================================================================================================================
2024-01-31 17:55:58,081 Logging Sequence: 137_23.00
2024-01-31 17:55:58,081 	Gloss Reference :	* ***** ***** * ***** * ***** ***** ***** ***** ***** * ***** A B+C+D+E
2024-01-31 17:55:58,081 	Gloss Hypothesis:	A <unk> <pad> A <unk> A <unk> <pad> <unk> <pad> <unk> A <unk> A <unk>  
2024-01-31 17:55:58,081 	Gloss Alignment :	I I     I     I I     I I     I     I     I     I     I I       S      
2024-01-31 17:55:58,081 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 17:55:58,084 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** *** *** ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* fan           from          around        the           world         are           in            qatar         for           the           fifa          world         cup          
2024-01-31 17:55:58,084 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions
2024-01-31 17:55:58,085 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I   I   I             I             I             I             I             I             I             I             I             I             I             I             I             I             S             S             S             S             S             S             S             S             S             S             S             S             S            
2024-01-31 17:55:58,085 ========================================================================================================================
2024-01-31 17:55:58,085 Logging Sequence: 128_145.00
2024-01-31 17:55:58,085 	Gloss Reference :	***** A ***** B+C+D+E
2024-01-31 17:55:58,085 	Gloss Hypothesis:	<unk> A <unk> <pad>  
2024-01-31 17:55:58,085 	Gloss Alignment :	I       I     S      
2024-01-31 17:55:58,085 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 17:55:58,087 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** icc also uploaded a   video of  the same
2024-01-31 17:55:58,087 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s>  <s>      <s> <s>   <s> <s> <s> 
2024-01-31 17:55:58,088 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   S   S    S        S   S     S   S   S   
2024-01-31 17:55:58,088 ========================================================================================================================
2024-01-31 17:55:58,088 Logging Sequence: 165_192.00
2024-01-31 17:55:58,088 	Gloss Reference :	A ********* ***** B+C+D+E            
2024-01-31 17:55:58,088 	Gloss Hypothesis:	A E+A+E+A+E <unk> E+B+E+C+E+A+E+C+E+C
2024-01-31 17:55:58,088 	Gloss Alignment :	  I         I     S                  
2024-01-31 17:55:58,088 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 17:55:58,090 	Text Reference  :	*** *** ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* 3             ravichandran  ashwin        believes      that          his           bag           is            lucky        
2024-01-31 17:55:58,091 	Text Hypothesis :	<s> <s> misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand
2024-01-31 17:55:58,091 	Text Alignment  :	I   I   I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             S             S             S             S             S             S             S             S             S            
2024-01-31 17:55:58,091 ========================================================================================================================
2024-01-31 17:55:58,091 Logging Sequence: 180_494.00
2024-01-31 17:55:58,091 	Gloss Reference :	* *************************** A     B+C+D+E          
2024-01-31 17:55:58,091 	Gloss Hypothesis:	C E+C+B+E+C+B+C+B+C+B+C+E+B+C <unk> C+E+C+E+C+B+C+B+C
2024-01-31 17:55:58,091 	Gloss Alignment :	I I                           S     S                
2024-01-31 17:55:58,092 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 17:55:58,095 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** the women wrestlers spoke angrily against the police and the controversy in  front of  the media
2024-01-31 17:55:58,095 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s>   <s>       <s>   <s>     <s>     <s> <s>    <s> <s> <s>         <s> <s>   <s> <s> <s>  
2024-01-31 17:55:58,095 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   S   S     S         S     S       S       S   S      S   S   S           S   S     S   S   S    
2024-01-31 17:55:58,095 ========================================================================================================================
2024-01-31 17:56:08,571 Epoch 177: Total Training Recognition Loss 1102.00  Total Training Translation Loss 3486.83 
2024-01-31 17:56:08,571 EPOCH 178
2024-01-31 17:56:26,049 Epoch 178: Total Training Recognition Loss 1099.31  Total Training Translation Loss 3487.85 
2024-01-31 17:56:26,050 EPOCH 179
2024-01-31 17:56:43,594 Epoch 179: Total Training Recognition Loss 1100.82  Total Training Translation Loss 3487.83 
2024-01-31 17:56:43,595 EPOCH 180
2024-01-31 17:56:50,787 [Epoch: 180 Step: 00006100] Batch Recognition Loss:  33.414726 => Gls Tokens per Sec:      588 || Batch Translation Loss:  99.511421 => Txt Tokens per Sec:     1606 || Lr: 0.000100
2024-01-31 17:57:01,152 Epoch 180: Total Training Recognition Loss 1098.39  Total Training Translation Loss 3487.43 
2024-01-31 17:57:01,152 EPOCH 181
2024-01-31 17:57:18,666 Epoch 181: Total Training Recognition Loss 1101.69  Total Training Translation Loss 3487.05 
2024-01-31 17:57:18,666 EPOCH 182
2024-01-31 17:57:36,116 Epoch 182: Total Training Recognition Loss 1099.75  Total Training Translation Loss 3487.75 
2024-01-31 17:57:36,116 EPOCH 183
2024-01-31 17:57:42,062 [Epoch: 183 Step: 00006200] Batch Recognition Loss:  42.981335 => Gls Tokens per Sec:      604 || Batch Translation Loss: 128.610245 => Txt Tokens per Sec:     1704 || Lr: 0.000100
2024-01-31 17:57:53,728 Epoch 183: Total Training Recognition Loss 1102.65  Total Training Translation Loss 3486.24 
2024-01-31 17:57:53,729 EPOCH 184
2024-01-31 17:58:11,272 Epoch 184: Total Training Recognition Loss 1100.47  Total Training Translation Loss 3487.24 
2024-01-31 17:58:11,273 EPOCH 185
2024-01-31 17:58:28,948 Epoch 185: Total Training Recognition Loss 1100.66  Total Training Translation Loss 3487.24 
2024-01-31 17:58:28,948 EPOCH 186
2024-01-31 17:58:35,360 [Epoch: 186 Step: 00006300] Batch Recognition Loss:  42.343567 => Gls Tokens per Sec:      499 || Batch Translation Loss: 128.794296 => Txt Tokens per Sec:     1448 || Lr: 0.000100
2024-01-31 17:58:46,464 Epoch 186: Total Training Recognition Loss 1100.01  Total Training Translation Loss 3488.14 
2024-01-31 17:58:46,465 EPOCH 187
2024-01-31 17:59:03,810 Epoch 187: Total Training Recognition Loss 1099.57  Total Training Translation Loss 3486.80 
2024-01-31 17:59:03,810 EPOCH 188
2024-01-31 17:59:21,303 Epoch 188: Total Training Recognition Loss 1101.06  Total Training Translation Loss 3486.94 
2024-01-31 17:59:21,304 EPOCH 189
2024-01-31 17:59:25,787 [Epoch: 189 Step: 00006400] Batch Recognition Loss:  36.798721 => Gls Tokens per Sec:      571 || Batch Translation Loss: 105.545692 => Txt Tokens per Sec:     1522 || Lr: 0.000100
2024-01-31 17:59:38,757 Epoch 189: Total Training Recognition Loss 1100.12  Total Training Translation Loss 3487.86 
2024-01-31 17:59:38,757 EPOCH 190
2024-01-31 17:59:56,216 Epoch 190: Total Training Recognition Loss 1100.39  Total Training Translation Loss 3486.64 
2024-01-31 17:59:56,216 EPOCH 191
2024-01-31 18:00:13,758 Epoch 191: Total Training Recognition Loss 1101.10  Total Training Translation Loss 3487.15 
2024-01-31 18:00:13,759 EPOCH 192
2024-01-31 18:00:16,844 [Epoch: 192 Step: 00006500] Batch Recognition Loss:  22.692953 => Gls Tokens per Sec:      622 || Batch Translation Loss:  83.217033 => Txt Tokens per Sec:     1860 || Lr: 0.000100
2024-01-31 18:00:31,122 Epoch 192: Total Training Recognition Loss 1101.39  Total Training Translation Loss 3486.68 
2024-01-31 18:00:31,122 EPOCH 193
2024-01-31 18:00:48,761 Epoch 193: Total Training Recognition Loss 1101.79  Total Training Translation Loss 3487.00 
2024-01-31 18:00:48,761 EPOCH 194
2024-01-31 18:01:06,267 Epoch 194: Total Training Recognition Loss 1100.09  Total Training Translation Loss 3487.26 
2024-01-31 18:01:06,267 EPOCH 195
2024-01-31 18:01:09,257 [Epoch: 195 Step: 00006600] Batch Recognition Loss:  37.052338 => Gls Tokens per Sec:      428 || Batch Translation Loss: 107.691895 => Txt Tokens per Sec:     1221 || Lr: 0.000100
2024-01-31 18:01:23,835 Epoch 195: Total Training Recognition Loss 1100.07  Total Training Translation Loss 3488.09 
2024-01-31 18:01:23,835 EPOCH 196
2024-01-31 18:01:41,284 Epoch 196: Total Training Recognition Loss 1100.70  Total Training Translation Loss 3487.16 
2024-01-31 18:01:41,284 EPOCH 197
2024-01-31 18:01:58,732 Epoch 197: Total Training Recognition Loss 1100.21  Total Training Translation Loss 3487.57 
2024-01-31 18:01:58,732 EPOCH 198
2024-01-31 18:01:59,378 [Epoch: 198 Step: 00006700] Batch Recognition Loss:  20.746029 => Gls Tokens per Sec:      992 || Batch Translation Loss:  93.868080 => Txt Tokens per Sec:     2540 || Lr: 0.000100
2024-01-31 18:02:16,157 Epoch 198: Total Training Recognition Loss 1102.18  Total Training Translation Loss 3487.20 
2024-01-31 18:02:16,157 EPOCH 199
2024-01-31 18:02:33,609 Epoch 199: Total Training Recognition Loss 1099.65  Total Training Translation Loss 3487.33 
2024-01-31 18:02:33,609 EPOCH 200
2024-01-31 18:02:51,024 [Epoch: 200 Step: 00006800] Batch Recognition Loss:  58.119911 => Gls Tokens per Sec:      610 || Batch Translation Loss: 132.339081 => Txt Tokens per Sec:     1695 || Lr: 0.000100
2024-01-31 18:02:51,024 Epoch 200: Total Training Recognition Loss 1100.29  Total Training Translation Loss 3486.25 
2024-01-31 18:02:51,024 EPOCH 201
2024-01-31 18:03:08,314 Epoch 201: Total Training Recognition Loss 1099.67  Total Training Translation Loss 3486.48 
2024-01-31 18:03:08,315 EPOCH 202
2024-01-31 18:03:25,825 Epoch 202: Total Training Recognition Loss 1100.40  Total Training Translation Loss 3487.67 
2024-01-31 18:03:25,826 EPOCH 203
2024-01-31 18:03:41,922 [Epoch: 203 Step: 00006900] Batch Recognition Loss:  33.060356 => Gls Tokens per Sec:      621 || Batch Translation Loss: 107.288818 => Txt Tokens per Sec:     1695 || Lr: 0.000100
2024-01-31 18:03:43,232 Epoch 203: Total Training Recognition Loss 1098.52  Total Training Translation Loss 3487.20 
2024-01-31 18:03:43,232 EPOCH 204
2024-01-31 18:04:00,773 Epoch 204: Total Training Recognition Loss 1098.18  Total Training Translation Loss 3487.24 
2024-01-31 18:04:00,773 EPOCH 205
2024-01-31 18:04:18,404 Epoch 205: Total Training Recognition Loss 1101.87  Total Training Translation Loss 3487.48 
2024-01-31 18:04:18,404 EPOCH 206
2024-01-31 18:04:34,433 [Epoch: 206 Step: 00007000] Batch Recognition Loss:  58.033909 => Gls Tokens per Sec:      583 || Batch Translation Loss: 132.220612 => Txt Tokens per Sec:     1625 || Lr: 0.000100
2024-01-31 18:04:35,931 Epoch 206: Total Training Recognition Loss 1102.84  Total Training Translation Loss 3487.46 
2024-01-31 18:04:35,931 EPOCH 207
2024-01-31 18:04:53,379 Epoch 207: Total Training Recognition Loss 1102.04  Total Training Translation Loss 3487.31 
2024-01-31 18:04:53,379 EPOCH 208
2024-01-31 18:05:10,802 Epoch 208: Total Training Recognition Loss 1104.51  Total Training Translation Loss 3487.55 
2024-01-31 18:05:10,802 EPOCH 209
2024-01-31 18:05:24,958 [Epoch: 209 Step: 00007100] Batch Recognition Loss:  37.195526 => Gls Tokens per Sec:      615 || Batch Translation Loss: 118.086288 => Txt Tokens per Sec:     1689 || Lr: 0.000100
2024-01-31 18:05:28,259 Epoch 209: Total Training Recognition Loss 1099.15  Total Training Translation Loss 3487.47 
2024-01-31 18:05:28,259 EPOCH 210
2024-01-31 18:05:45,651 Epoch 210: Total Training Recognition Loss 1101.98  Total Training Translation Loss 3487.19 
2024-01-31 18:05:45,651 EPOCH 211
2024-01-31 18:06:03,065 Epoch 211: Total Training Recognition Loss 1099.94  Total Training Translation Loss 3487.03 
2024-01-31 18:06:03,065 EPOCH 212
2024-01-31 18:06:16,681 [Epoch: 212 Step: 00007200] Batch Recognition Loss:  36.380249 => Gls Tokens per Sec:      593 || Batch Translation Loss: 115.704987 => Txt Tokens per Sec:     1649 || Lr: 0.000100
2024-01-31 18:06:20,554 Epoch 212: Total Training Recognition Loss 1099.09  Total Training Translation Loss 3487.06 
2024-01-31 18:06:20,554 EPOCH 213
2024-01-31 18:06:38,168 Epoch 213: Total Training Recognition Loss 1100.19  Total Training Translation Loss 3486.61 
2024-01-31 18:06:38,169 EPOCH 214
2024-01-31 18:06:55,628 Epoch 214: Total Training Recognition Loss 1100.83  Total Training Translation Loss 3486.51 
2024-01-31 18:06:55,628 EPOCH 215
2024-01-31 18:07:06,641 [Epoch: 215 Step: 00007300] Batch Recognition Loss:  36.339432 => Gls Tokens per Sec:      675 || Batch Translation Loss: 110.990997 => Txt Tokens per Sec:     1824 || Lr: 0.000100
2024-01-31 18:07:13,138 Epoch 215: Total Training Recognition Loss 1098.80  Total Training Translation Loss 3487.75 
2024-01-31 18:07:13,138 EPOCH 216
2024-01-31 18:07:30,667 Epoch 216: Total Training Recognition Loss 1099.91  Total Training Translation Loss 3486.92 
2024-01-31 18:07:30,667 EPOCH 217
2024-01-31 18:07:47,932 Epoch 217: Total Training Recognition Loss 1101.66  Total Training Translation Loss 3487.09 
2024-01-31 18:07:47,933 EPOCH 218
2024-01-31 18:07:58,215 [Epoch: 218 Step: 00007400] Batch Recognition Loss:  27.421032 => Gls Tokens per Sec:      661 || Batch Translation Loss:  97.675339 => Txt Tokens per Sec:     1820 || Lr: 0.000100
2024-01-31 18:08:05,419 Epoch 218: Total Training Recognition Loss 1101.60  Total Training Translation Loss 3487.29 
2024-01-31 18:08:05,419 EPOCH 219
2024-01-31 18:08:22,930 Epoch 219: Total Training Recognition Loss 1100.28  Total Training Translation Loss 3485.93 
2024-01-31 18:08:22,930 EPOCH 220
2024-01-31 18:08:40,325 Epoch 220: Total Training Recognition Loss 1102.69  Total Training Translation Loss 3487.64 
2024-01-31 18:08:40,325 EPOCH 221
2024-01-31 18:08:51,316 [Epoch: 221 Step: 00007500] Batch Recognition Loss:  43.172371 => Gls Tokens per Sec:      560 || Batch Translation Loss: 129.217346 => Txt Tokens per Sec:     1538 || Lr: 0.000100
2024-01-31 18:08:57,947 Epoch 221: Total Training Recognition Loss 1098.93  Total Training Translation Loss 3486.66 
2024-01-31 18:08:57,947 EPOCH 222
2024-01-31 18:09:15,304 Epoch 222: Total Training Recognition Loss 1102.42  Total Training Translation Loss 3486.35 
2024-01-31 18:09:15,304 EPOCH 223
2024-01-31 18:09:32,707 Epoch 223: Total Training Recognition Loss 1099.27  Total Training Translation Loss 3487.04 
2024-01-31 18:09:32,708 EPOCH 224
2024-01-31 18:09:42,764 [Epoch: 224 Step: 00007600] Batch Recognition Loss:  22.741684 => Gls Tokens per Sec:      573 || Batch Translation Loss:  82.540588 => Txt Tokens per Sec:     1605 || Lr: 0.000100
2024-01-31 18:09:50,150 Epoch 224: Total Training Recognition Loss 1099.78  Total Training Translation Loss 3487.38 
2024-01-31 18:09:50,150 EPOCH 225
2024-01-31 18:10:07,585 Epoch 225: Total Training Recognition Loss 1100.90  Total Training Translation Loss 3486.68 
2024-01-31 18:10:07,586 EPOCH 226
2024-01-31 18:10:25,166 Epoch 226: Total Training Recognition Loss 1100.12  Total Training Translation Loss 3486.77 
2024-01-31 18:10:25,166 EPOCH 227
2024-01-31 18:10:32,745 [Epoch: 227 Step: 00007700] Batch Recognition Loss:  20.838966 => Gls Tokens per Sec:      676 || Batch Translation Loss:  88.072144 => Txt Tokens per Sec:     1873 || Lr: 0.000100
2024-01-31 18:10:42,682 Epoch 227: Total Training Recognition Loss 1101.00  Total Training Translation Loss 3487.09 
2024-01-31 18:10:42,682 EPOCH 228
2024-01-31 18:11:00,285 Epoch 228: Total Training Recognition Loss 1098.51  Total Training Translation Loss 3486.88 
2024-01-31 18:11:00,285 EPOCH 229
2024-01-31 18:11:17,911 Epoch 229: Total Training Recognition Loss 1099.51  Total Training Translation Loss 3487.57 
2024-01-31 18:11:17,912 EPOCH 230
2024-01-31 18:11:25,431 [Epoch: 230 Step: 00007800] Batch Recognition Loss:  36.954834 => Gls Tokens per Sec:      596 || Batch Translation Loss: 117.248566 => Txt Tokens per Sec:     1717 || Lr: 0.000100
2024-01-31 18:11:35,448 Epoch 230: Total Training Recognition Loss 1098.59  Total Training Translation Loss 3486.92 
2024-01-31 18:11:35,448 EPOCH 231
2024-01-31 18:11:52,962 Epoch 231: Total Training Recognition Loss 1101.95  Total Training Translation Loss 3487.18 
2024-01-31 18:11:52,962 EPOCH 232
2024-01-31 18:12:10,458 Epoch 232: Total Training Recognition Loss 1102.31  Total Training Translation Loss 3487.46 
2024-01-31 18:12:10,458 EPOCH 233
2024-01-31 18:12:17,469 [Epoch: 233 Step: 00007900] Batch Recognition Loss:  73.604431 => Gls Tokens per Sec:      512 || Batch Translation Loss: 126.814041 => Txt Tokens per Sec:     1506 || Lr: 0.000100
2024-01-31 18:12:27,949 Epoch 233: Total Training Recognition Loss 1101.77  Total Training Translation Loss 3487.02 
2024-01-31 18:12:27,949 EPOCH 234
2024-01-31 18:12:46,728 Epoch 234: Total Training Recognition Loss 1099.07  Total Training Translation Loss 3486.44 
2024-01-31 18:12:46,729 EPOCH 235
2024-01-31 18:13:05,879 Epoch 235: Total Training Recognition Loss 1102.00  Total Training Translation Loss 3487.60 
2024-01-31 18:13:05,879 EPOCH 236
2024-01-31 18:13:10,330 [Epoch: 236 Step: 00008000] Batch Recognition Loss:  38.418423 => Gls Tokens per Sec:      719 || Batch Translation Loss: 110.765633 => Txt Tokens per Sec:     1852 || Lr: 0.000100
2024-01-31 18:13:34,420 Validation result at epoch 236, step     8000: duration: 24.0900s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 345.12662	Translation Loss: 73491.54688	PPL: 1562.90662
	Eval Metric: BLEU
	WER 521.12	(DEL: 4.73,	INS: 432.63,	SUB: 83.76)
	BLEU-4 0.00	(BLEU-1: 0.01,	BLEU-2: 0.00,	BLEU-3: 0.00,	BLEU-4: 0.00)
	CHRF 3.56	ROUGE 0.02
2024-01-31 18:13:34,421 Logging Recognition and Translation Outputs
2024-01-31 18:13:34,421 ========================================================================================================================
2024-01-31 18:13:34,421 Logging Sequence: 88_57.00
2024-01-31 18:13:34,422 	Gloss Reference :	***** * ***** * ***** * ***** * ***** * ***** ***** * A     B+C+D+E
2024-01-31 18:13:34,422 	Gloss Hypothesis:	<pad> B <pad> B <unk> B <unk> B <unk> B <pad> <unk> B <pad> B      
2024-01-31 18:13:34,422 	Gloss Alignment :	I     I I     I I     I I     I I     I I     I     I S     S      
2024-01-31 18:13:34,422 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 18:13:34,426 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** which stated messi we're waiting for you to  come here you will be  finished when you come
2024-01-31 18:13:34,426 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s>   <s>    <s>   <s>   <s>     <s> <s> <s> <s>  <s>  <s> <s>  <s> <s>      <s>  <s> <s> 
2024-01-31 18:13:34,426 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   S     S      S     S     S       S   S   S   S    S    S   S    S   S        S    S   S   
2024-01-31 18:13:34,426 ========================================================================================================================
2024-01-31 18:13:34,426 Logging Sequence: 171_142.00
2024-01-31 18:13:34,427 	Gloss Reference :	***** * ***** * A     B+C+D+E                                                              
2024-01-31 18:13:34,427 	Gloss Hypothesis:	<unk> E <unk> E <unk> E+B+E+C+E+C+E+C+E+C+E+C+E+C+E+C+E+C+E+B+E+B+E+C+E+C+E+C+E+C+E+C+E+C+E
2024-01-31 18:13:34,427 	Gloss Alignment :	I     I I     I S     S                                                                    
2024-01-31 18:13:34,427 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 18:13:34,430 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** this   decision on     dhoni  made   a      significant impact as     pathirana claimed two    tough  wickets
2024-01-31 18:13:34,430 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> chosen chosen chosen chosen chosen chosen chosen chosen chosen chosen chosen chosen chosen chosen chosen chosen chosen   chosen chosen chosen chosen chosen      chosen chosen chosen    chosen  chosen chosen chosen 
2024-01-31 18:13:34,430 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I      I      I      I      I      I      I      I      I      I      I      I      I      I      I      S      S        S      S      S      S      S           S      S      S         S       S      S      S      
2024-01-31 18:13:34,431 ========================================================================================================================
2024-01-31 18:13:34,431 Logging Sequence: 125_207.00
2024-01-31 18:13:34,431 	Gloss Reference :	***** A ***** ***** *** ***** * ***** * ***** * ***** ********* ***** ***** ***** * ***** ***** ***** ***** ***** ***** * ***** ***************** ***** B+C+D+E
2024-01-31 18:13:34,431 	Gloss Hypothesis:	<unk> A <unk> <pad> A+B <unk> E <unk> B <unk> E <unk> E+B+E+C+B <unk> B+E+C <unk> B <unk> B+C+B <unk> C+B+E <pad> <unk> E <unk> B+C+B+E+B+E+B+E+B <pad> C+E+C  
2024-01-31 18:13:34,432 	Gloss Alignment :	I       I     I     I   I     I I     I I     I I     I         I     I     I     I I     I     I     I     I     I     I I     I                 I     S      
2024-01-31 18:13:34,432 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 18:13:34,435 	Text Reference  :	*** *** *** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** he    had   not   practised since he    returned and   he    had   also  fallen sick 
2024-01-31 18:13:34,435 	Text Hypothesis :	<s> <s> <s> sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha     sabha sabha sabha    sabha sabha sabha sabha sabha  sabha
2024-01-31 18:13:34,435 	Text Alignment  :	I   I   I   I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     S     S     S     S         S     S     S        S     S     S     S     S      S    
2024-01-31 18:13:34,435 ========================================================================================================================
2024-01-31 18:13:34,435 Logging Sequence: 68_230.00
2024-01-31 18:13:34,435 	Gloss Reference :	* ********* A     B+C+D+E      
2024-01-31 18:13:34,435 	Gloss Hypothesis:	E C+E+D+E+D <unk> E+C+E+C+E+C+E
2024-01-31 18:13:34,435 	Gloss Alignment :	I I         S     S            
2024-01-31 18:13:34,436 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 18:13:34,438 	Text Reference  :	*** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** let   us    know  what  you   think in    the   comments below
2024-01-31 18:13:34,438 	Text Hypothesis :	<s> sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha    sabha
2024-01-31 18:13:34,438 	Text Alignment  :	I   I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     S     S     S     S     S     S     S     S     S        S    
2024-01-31 18:13:34,438 ========================================================================================================================
2024-01-31 18:13:34,438 Logging Sequence: 126_82.00
2024-01-31 18:13:34,439 	Gloss Reference :	***** *** ***** * ***** * ***** ********* A     B+C+D+E          
2024-01-31 18:13:34,439 	Gloss Hypothesis:	<unk> E+D <pad> E <unk> E <unk> E+C+E+C+E <unk> E+C+E+C+E+C+E+C+E
2024-01-31 18:13:34,439 	Gloss Alignment :	I     I   I     I I     I I     I         S     S                
2024-01-31 18:13:34,439 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 18:13:34,442 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ****** ****** ****** ****** ****** ****** neeraj also   dedicated his    gold   medal  to     former indian olympians who    came   close  to     winning medals
2024-01-31 18:13:34,443 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> boards boards boards boards boards boards boards boards boards    boards boards boards boards boards boards boards    boards boards boards boards boards  boards
2024-01-31 18:13:34,443 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I      I      I      I      I      I      S      S      S         S      S      S      S      S      S      S         S      S      S      S      S       S     
2024-01-31 18:13:34,443 ========================================================================================================================
2024-01-31 18:13:47,418 Epoch 236: Total Training Recognition Loss 1101.15  Total Training Translation Loss 3487.65 
2024-01-31 18:13:47,418 EPOCH 237
2024-01-31 18:14:04,724 Epoch 237: Total Training Recognition Loss 1102.07  Total Training Translation Loss 3487.21 
2024-01-31 18:14:04,724 EPOCH 238
2024-01-31 18:14:22,195 Epoch 238: Total Training Recognition Loss 1100.43  Total Training Translation Loss 3487.86 
2024-01-31 18:14:22,196 EPOCH 239
2024-01-31 18:14:27,162 [Epoch: 239 Step: 00008100] Batch Recognition Loss:  17.182518 => Gls Tokens per Sec:      516 || Batch Translation Loss:  75.407379 => Txt Tokens per Sec:     1366 || Lr: 0.000100
2024-01-31 18:14:39,586 Epoch 239: Total Training Recognition Loss 1100.59  Total Training Translation Loss 3486.97 
2024-01-31 18:14:39,586 EPOCH 240
2024-01-31 18:14:56,857 Epoch 240: Total Training Recognition Loss 1099.70  Total Training Translation Loss 3487.31 
2024-01-31 18:14:56,858 EPOCH 241
2024-01-31 18:15:14,262 Epoch 241: Total Training Recognition Loss 1102.21  Total Training Translation Loss 3487.03 
2024-01-31 18:15:14,263 EPOCH 242
2024-01-31 18:15:16,695 [Epoch: 242 Step: 00008200] Batch Recognition Loss:  14.996105 => Gls Tokens per Sec:      790 || Batch Translation Loss:  75.203903 => Txt Tokens per Sec:     1900 || Lr: 0.000100
2024-01-31 18:15:31,654 Epoch 242: Total Training Recognition Loss 1100.92  Total Training Translation Loss 3486.54 
2024-01-31 18:15:31,655 EPOCH 243
2024-01-31 18:15:49,047 Epoch 243: Total Training Recognition Loss 1100.99  Total Training Translation Loss 3487.66 
2024-01-31 18:15:49,047 EPOCH 244
2024-01-31 18:16:06,434 Epoch 244: Total Training Recognition Loss 1101.99  Total Training Translation Loss 3487.12 
2024-01-31 18:16:06,434 EPOCH 245
2024-01-31 18:16:09,030 [Epoch: 245 Step: 00008300] Batch Recognition Loss:  42.357738 => Gls Tokens per Sec:      493 || Batch Translation Loss: 129.060181 => Txt Tokens per Sec:     1493 || Lr: 0.000100
2024-01-31 18:16:23,887 Epoch 245: Total Training Recognition Loss 1100.91  Total Training Translation Loss 3487.21 
2024-01-31 18:16:23,887 EPOCH 246
2024-01-31 18:16:41,356 Epoch 246: Total Training Recognition Loss 1101.47  Total Training Translation Loss 3486.85 
2024-01-31 18:16:41,356 EPOCH 247
2024-01-31 18:16:58,876 Epoch 247: Total Training Recognition Loss 1101.78  Total Training Translation Loss 3486.53 
2024-01-31 18:16:58,876 EPOCH 248
2024-01-31 18:16:59,880 [Epoch: 248 Step: 00008400] Batch Recognition Loss:  37.306198 => Gls Tokens per Sec:      638 || Batch Translation Loss: 122.752396 => Txt Tokens per Sec:     1998 || Lr: 0.000100
2024-01-31 18:17:16,502 Epoch 248: Total Training Recognition Loss 1101.79  Total Training Translation Loss 3487.30 
2024-01-31 18:17:16,502 EPOCH 249
2024-01-31 18:17:33,998 Epoch 249: Total Training Recognition Loss 1099.84  Total Training Translation Loss 3487.83 
2024-01-31 18:17:33,998 EPOCH 250
2024-01-31 18:17:51,573 [Epoch: 250 Step: 00008500] Batch Recognition Loss:  72.875626 => Gls Tokens per Sec:      605 || Batch Translation Loss: 126.068222 => Txt Tokens per Sec:     1679 || Lr: 0.000100
2024-01-31 18:17:51,573 Epoch 250: Total Training Recognition Loss 1098.67  Total Training Translation Loss 3486.89 
2024-01-31 18:17:51,573 EPOCH 251
2024-01-31 18:18:09,251 Epoch 251: Total Training Recognition Loss 1100.23  Total Training Translation Loss 3488.01 
2024-01-31 18:18:09,251 EPOCH 252
2024-01-31 18:18:27,203 Epoch 252: Total Training Recognition Loss 1102.74  Total Training Translation Loss 3487.06 
2024-01-31 18:18:27,203 EPOCH 253
2024-01-31 18:18:43,840 [Epoch: 253 Step: 00008600] Batch Recognition Loss:  18.769325 => Gls Tokens per Sec:      601 || Batch Translation Loss:  87.813095 => Txt Tokens per Sec:     1669 || Lr: 0.000100
2024-01-31 18:18:44,904 Epoch 253: Total Training Recognition Loss 1100.58  Total Training Translation Loss 3487.26 
2024-01-31 18:18:44,905 EPOCH 254
2024-01-31 18:19:02,816 Epoch 254: Total Training Recognition Loss 1099.92  Total Training Translation Loss 3486.84 
2024-01-31 18:19:02,816 EPOCH 255
2024-01-31 18:19:20,585 Epoch 255: Total Training Recognition Loss 1102.19  Total Training Translation Loss 3488.25 
2024-01-31 18:19:20,585 EPOCH 256
2024-01-31 18:19:34,969 [Epoch: 256 Step: 00008700] Batch Recognition Loss:  38.120148 => Gls Tokens per Sec:      668 || Batch Translation Loss: 121.127296 => Txt Tokens per Sec:     1821 || Lr: 0.000100
2024-01-31 18:19:38,461 Epoch 256: Total Training Recognition Loss 1102.25  Total Training Translation Loss 3487.38 
2024-01-31 18:19:38,461 EPOCH 257
2024-01-31 18:19:56,406 Epoch 257: Total Training Recognition Loss 1098.60  Total Training Translation Loss 3487.60 
2024-01-31 18:19:56,406 EPOCH 258
2024-01-31 18:20:14,319 Epoch 258: Total Training Recognition Loss 1100.00  Total Training Translation Loss 3487.99 
2024-01-31 18:20:14,319 EPOCH 259
2024-01-31 18:20:29,860 [Epoch: 259 Step: 00008800] Batch Recognition Loss:  18.535679 => Gls Tokens per Sec:      561 || Batch Translation Loss:  88.650345 => Txt Tokens per Sec:     1583 || Lr: 0.000100
2024-01-31 18:20:32,236 Epoch 259: Total Training Recognition Loss 1102.13  Total Training Translation Loss 3487.58 
2024-01-31 18:20:32,237 EPOCH 260
2024-01-31 18:20:50,567 Epoch 260: Total Training Recognition Loss 1102.21  Total Training Translation Loss 3487.06 
2024-01-31 18:20:50,568 EPOCH 261
2024-01-31 18:21:10,336 Epoch 261: Total Training Recognition Loss 1101.03  Total Training Translation Loss 3487.05 
2024-01-31 18:21:10,336 EPOCH 262
2024-01-31 18:21:24,325 [Epoch: 262 Step: 00008900] Batch Recognition Loss:  21.051542 => Gls Tokens per Sec:      577 || Batch Translation Loss:  89.195656 => Txt Tokens per Sec:     1611 || Lr: 0.000100
2024-01-31 18:21:27,973 Epoch 262: Total Training Recognition Loss 1099.26  Total Training Translation Loss 3487.67 
2024-01-31 18:21:27,974 EPOCH 263
2024-01-31 18:21:45,460 Epoch 263: Total Training Recognition Loss 1102.58  Total Training Translation Loss 3486.81 
2024-01-31 18:21:45,460 EPOCH 264
2024-01-31 18:22:03,049 Epoch 264: Total Training Recognition Loss 1103.00  Total Training Translation Loss 3487.21 
2024-01-31 18:22:03,050 EPOCH 265
2024-01-31 18:22:15,306 [Epoch: 265 Step: 00009000] Batch Recognition Loss:  52.045258 => Gls Tokens per Sec:      606 || Batch Translation Loss: 129.809052 => Txt Tokens per Sec:     1667 || Lr: 0.000100
2024-01-31 18:22:20,494 Epoch 265: Total Training Recognition Loss 1101.69  Total Training Translation Loss 3487.11 
2024-01-31 18:22:20,494 EPOCH 266
2024-01-31 18:22:38,189 Epoch 266: Total Training Recognition Loss 1100.46  Total Training Translation Loss 3487.63 
2024-01-31 18:22:38,190 EPOCH 267
2024-01-31 18:22:55,730 Epoch 267: Total Training Recognition Loss 1099.33  Total Training Translation Loss 3487.94 
2024-01-31 18:22:55,731 EPOCH 268
2024-01-31 18:23:07,697 [Epoch: 268 Step: 00009100] Batch Recognition Loss:  37.598553 => Gls Tokens per Sec:      567 || Batch Translation Loss: 110.835281 => Txt Tokens per Sec:     1615 || Lr: 0.000100
2024-01-31 18:23:13,284 Epoch 268: Total Training Recognition Loss 1102.08  Total Training Translation Loss 3488.80 
2024-01-31 18:23:13,285 EPOCH 269
2024-01-31 18:23:30,948 Epoch 269: Total Training Recognition Loss 1101.68  Total Training Translation Loss 3487.03 
2024-01-31 18:23:30,948 EPOCH 270
2024-01-31 18:23:48,564 Epoch 270: Total Training Recognition Loss 1102.88  Total Training Translation Loss 3487.35 
2024-01-31 18:23:48,564 EPOCH 271
2024-01-31 18:23:59,442 [Epoch: 271 Step: 00009200] Batch Recognition Loss:  30.579601 => Gls Tokens per Sec:      565 || Batch Translation Loss: 104.919342 => Txt Tokens per Sec:     1604 || Lr: 0.000100
2024-01-31 18:24:06,085 Epoch 271: Total Training Recognition Loss 1098.81  Total Training Translation Loss 3487.40 
2024-01-31 18:24:06,085 EPOCH 272
2024-01-31 18:24:23,704 Epoch 272: Total Training Recognition Loss 1100.73  Total Training Translation Loss 3488.17 
2024-01-31 18:24:23,705 EPOCH 273
2024-01-31 18:24:41,331 Epoch 273: Total Training Recognition Loss 1102.85  Total Training Translation Loss 3488.05 
2024-01-31 18:24:41,331 EPOCH 274
2024-01-31 18:24:51,286 [Epoch: 274 Step: 00009300] Batch Recognition Loss:  15.290411 => Gls Tokens per Sec:      554 || Batch Translation Loss:  81.903992 => Txt Tokens per Sec:     1564 || Lr: 0.000100
2024-01-31 18:24:58,992 Epoch 274: Total Training Recognition Loss 1098.84  Total Training Translation Loss 3487.24 
2024-01-31 18:24:58,992 EPOCH 275
2024-01-31 18:25:16,568 Epoch 275: Total Training Recognition Loss 1100.79  Total Training Translation Loss 3486.54 
2024-01-31 18:25:16,568 EPOCH 276
2024-01-31 18:25:34,157 Epoch 276: Total Training Recognition Loss 1099.92  Total Training Translation Loss 3488.23 
2024-01-31 18:25:34,157 EPOCH 277
2024-01-31 18:25:41,889 [Epoch: 277 Step: 00009400] Batch Recognition Loss:  42.444527 => Gls Tokens per Sec:      662 || Batch Translation Loss: 128.261200 => Txt Tokens per Sec:     1857 || Lr: 0.000100
2024-01-31 18:25:51,690 Epoch 277: Total Training Recognition Loss 1100.77  Total Training Translation Loss 3487.31 
2024-01-31 18:25:51,691 EPOCH 278
2024-01-31 18:26:09,332 Epoch 278: Total Training Recognition Loss 1102.66  Total Training Translation Loss 3486.77 
2024-01-31 18:26:09,332 EPOCH 279
2024-01-31 18:26:26,856 Epoch 279: Total Training Recognition Loss 1100.68  Total Training Translation Loss 3487.12 
2024-01-31 18:26:26,856 EPOCH 280
2024-01-31 18:26:35,006 [Epoch: 280 Step: 00009500] Batch Recognition Loss:  15.232782 => Gls Tokens per Sec:      519 || Batch Translation Loss:  79.821190 => Txt Tokens per Sec:     1526 || Lr: 0.000100
2024-01-31 18:26:44,415 Epoch 280: Total Training Recognition Loss 1100.87  Total Training Translation Loss 3487.39 
2024-01-31 18:26:44,416 EPOCH 281
2024-01-31 18:27:02,105 Epoch 281: Total Training Recognition Loss 1102.71  Total Training Translation Loss 3487.19 
2024-01-31 18:27:02,105 EPOCH 282
2024-01-31 18:27:19,629 Epoch 282: Total Training Recognition Loss 1101.13  Total Training Translation Loss 3486.95 
2024-01-31 18:27:19,629 EPOCH 283
2024-01-31 18:27:25,624 [Epoch: 283 Step: 00009600] Batch Recognition Loss:  42.631554 => Gls Tokens per Sec:      599 || Batch Translation Loss: 131.954269 => Txt Tokens per Sec:     1629 || Lr: 0.000100
2024-01-31 18:27:37,190 Epoch 283: Total Training Recognition Loss 1100.49  Total Training Translation Loss 3486.99 
2024-01-31 18:27:37,190 EPOCH 284
2024-01-31 18:27:54,805 Epoch 284: Total Training Recognition Loss 1101.72  Total Training Translation Loss 3486.37 
2024-01-31 18:27:54,805 EPOCH 285
2024-01-31 18:28:12,394 Epoch 285: Total Training Recognition Loss 1101.15  Total Training Translation Loss 3486.83 
2024-01-31 18:28:12,394 EPOCH 286
2024-01-31 18:28:18,867 [Epoch: 286 Step: 00009700] Batch Recognition Loss:  36.767075 => Gls Tokens per Sec:      456 || Batch Translation Loss: 122.391220 => Txt Tokens per Sec:     1289 || Lr: 0.000100
2024-01-31 18:28:29,894 Epoch 286: Total Training Recognition Loss 1100.29  Total Training Translation Loss 3487.40 
2024-01-31 18:28:29,895 EPOCH 287
2024-01-31 18:28:47,514 Epoch 287: Total Training Recognition Loss 1103.15  Total Training Translation Loss 3486.04 
2024-01-31 18:28:47,514 EPOCH 288
2024-01-31 18:29:05,162 Epoch 288: Total Training Recognition Loss 1101.77  Total Training Translation Loss 3487.83 
2024-01-31 18:29:05,162 EPOCH 289
2024-01-31 18:29:10,988 [Epoch: 289 Step: 00009800] Batch Recognition Loss:  51.543732 => Gls Tokens per Sec:      440 || Batch Translation Loss: 130.084549 => Txt Tokens per Sec:     1340 || Lr: 0.000100
2024-01-31 18:29:22,692 Epoch 289: Total Training Recognition Loss 1100.40  Total Training Translation Loss 3486.32 
2024-01-31 18:29:22,692 EPOCH 290
2024-01-31 18:29:40,217 Epoch 290: Total Training Recognition Loss 1099.91  Total Training Translation Loss 3487.20 
2024-01-31 18:29:40,218 EPOCH 291
2024-01-31 18:29:57,866 Epoch 291: Total Training Recognition Loss 1100.79  Total Training Translation Loss 3487.80 
2024-01-31 18:29:57,867 EPOCH 292
2024-01-31 18:30:01,597 [Epoch: 292 Step: 00009900] Batch Recognition Loss:  35.804176 => Gls Tokens per Sec:      448 || Batch Translation Loss: 111.952057 => Txt Tokens per Sec:     1362 || Lr: 0.000100
2024-01-31 18:30:15,514 Epoch 292: Total Training Recognition Loss 1100.56  Total Training Translation Loss 3486.65 
2024-01-31 18:30:15,514 EPOCH 293
2024-01-31 18:30:33,120 Epoch 293: Total Training Recognition Loss 1098.45  Total Training Translation Loss 3487.38 
2024-01-31 18:30:33,121 EPOCH 294
2024-01-31 18:30:50,762 Epoch 294: Total Training Recognition Loss 1099.74  Total Training Translation Loss 3486.86 
2024-01-31 18:30:50,762 EPOCH 295
2024-01-31 18:30:52,544 [Epoch: 295 Step: 00010000] Batch Recognition Loss:  27.701202 => Gls Tokens per Sec:      719 || Batch Translation Loss: 103.874374 => Txt Tokens per Sec:     2061 || Lr: 0.000100
2024-01-31 18:31:16,922 Validation result at epoch 295, step    10000: duration: 24.3784s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 352.51721	Translation Loss: 73476.28125	PPL: 1560.52087
	Eval Metric: BLEU
	WER 541.17	(DEL: 4.17,	INS: 452.05,	SUB: 84.96)
	BLEU-4 0.00	(BLEU-1: 0.01,	BLEU-2: 0.00,	BLEU-3: 0.00,	BLEU-4: 0.00)
	CHRF 3.48	ROUGE 0.02
2024-01-31 18:31:16,923 Logging Recognition and Translation Outputs
2024-01-31 18:31:16,923 ========================================================================================================================
2024-01-31 18:31:16,923 Logging Sequence: 159_139.00
2024-01-31 18:31:16,924 	Gloss Reference :	***** * ***** A ***** * ***** ********* ***** ***** ***** * ***** B+C+D+E
2024-01-31 18:31:16,924 	Gloss Hypothesis:	<unk> A <unk> A <unk> E <unk> E+B+E+B+E <unk> D+A+E <unk> E <unk> E      
2024-01-31 18:31:16,924 	Gloss Alignment :	I     I I       I     I I     I         I     I     I     I I     S      
2024-01-31 18:31:16,924 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 18:31:16,927 	Text Reference  :	*** *** *** *** *** *** *** *** *** ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* he      took    time    and     finally was     ready   for     the     asia    cup     where   he      scored  the     century
2024-01-31 18:31:16,928 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing
2024-01-31 18:31:16,928 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       S       S       S       S       S       S       S       S       S       S       S       S       S       S       S       S      
2024-01-31 18:31:16,928 ========================================================================================================================
2024-01-31 18:31:16,928 Logging Sequence: 159_159.00
2024-01-31 18:31:16,928 	Gloss Reference :	A B+C+D+E        
2024-01-31 18:31:16,928 	Gloss Hypothesis:	E A+E+A+E+A+E+C+E
2024-01-31 18:31:16,928 	Gloss Alignment :	S S              
2024-01-31 18:31:16,929 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 18:31:16,934 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** *** *** *** he  said it  wasn't easy the mind has to  be  focussed and he  is  glad that he  is  back in  form with the asia cup century
2024-01-31 18:31:16,934 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s>  <s> <s>    <s>  <s> <s>  <s> <s> <s> <s>      <s> <s> <s> <s>  <s>  <s> <s> <s>  <s> <s>  <s>  <s> <s>  <s> <s>    
2024-01-31 18:31:16,934 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I   I   I   S   S    S   S      S    S   S    S   S   S   S        S   S   S   S    S    S   S   S    S   S    S    S   S    S   S      
2024-01-31 18:31:16,934 ========================================================================================================================
2024-01-31 18:31:16,934 Logging Sequence: 103_8.00
2024-01-31 18:31:16,935 	Gloss Reference :	***** * ***** * ***** * ***** * ***** * ***** * ***** * ***** * A     B+C+D+E
2024-01-31 18:31:16,935 	Gloss Hypothesis:	<unk> E <unk> E <unk> E <unk> E <unk> E <unk> E <unk> E <unk> E <unk> E      
2024-01-31 18:31:16,935 	Gloss Alignment :	I     I I     I I     I I     I I     I I     I I     I I     I S     S      
2024-01-31 18:31:16,935 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 18:31:16,938 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* were    going   on      in      birmingham england from    28th    july    to      8th     august  2022   
2024-01-31 18:31:16,938 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing    nothing nothing nothing nothing nothing nothing nothing nothing
2024-01-31 18:31:16,938 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I       I       I       I       I       I       I       I       I       I       S       S       S       S       S          S       S       S       S       S       S       S       S      
2024-01-31 18:31:16,938 ========================================================================================================================
2024-01-31 18:31:16,939 Logging Sequence: 164_546.00
2024-01-31 18:31:16,939 	Gloss Reference :	***** * ***** A B+C+D+E
2024-01-31 18:31:16,939 	Gloss Hypothesis:	<unk> B <unk> C <unk>  
2024-01-31 18:31:16,939 	Gloss Alignment :	I     I I     S S      
2024-01-31 18:31:16,939 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 18:31:16,941 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** reliance has turned out to  be  the strongest company
2024-01-31 18:31:16,941 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s>      <s> <s>    <s> <s> <s> <s> <s>       <s>    
2024-01-31 18:31:16,941 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   S        S   S      S   S   S   S   S         S      
2024-01-31 18:31:16,942 ========================================================================================================================
2024-01-31 18:31:16,942 Logging Sequence: 132_173.00
2024-01-31 18:31:16,942 	Gloss Reference :	* ***** *** ***** ***** ***** ********************* ***** * ***** ***************************************************** ***** * A     B+C+D+E                  
2024-01-31 18:31:16,942 	Gloss Hypothesis:	E <unk> E+C <pad> C+E+C <unk> E+B+E+C+E+C+E+C+B+C+E <unk> E <unk> C+E+B+E+C+B+E+C+B+E+C+E+C+E+C+E+C+B+C+E+C+E+C+E+C+E+C <pad> E <pad> C+E+C+E+C+E+C+E+C+E+C+B+E
2024-01-31 18:31:16,942 	Gloss Alignment :	I I     I   I     I     I     I                     I     I I     I                                                     I     I S     S                        
2024-01-31 18:31:16,942 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 18:31:16,944 	Text Reference  :	************ ************ ************ ************ ************ ************ ************ ************ ************ ************ ************ ************ ************ ************ ************ ************ ************ ************ ************ ************ ************ ************ ************ ************ ************ ************ ************ ************ ************ ************ ************ ************ ************ ************ usman        is           australia'   first        muslim       player      
2024-01-31 18:31:16,944 	Text Hypothesis :	competitions competitions competitions competitions competitions competitions competitions competitions competitions competitions competitions competitions competitions competitions competitions competitions competitions competitions competitions competitions competitions competitions competitions competitions competitions competitions competitions competitions competitions competitions competitions competitions competitions competitions competitions competitions competitions competitions competitions competitions
2024-01-31 18:31:16,944 	Text Alignment  :	I            I            I            I            I            I            I            I            I            I            I            I            I            I            I            I            I            I            I            I            I            I            I            I            I            I            I            I            I            I            I            I            I            I            S            S            S            S            S            S           
2024-01-31 18:31:16,944 ========================================================================================================================
2024-01-31 18:31:32,725 Epoch 295: Total Training Recognition Loss 1101.97  Total Training Translation Loss 3486.86 
2024-01-31 18:31:32,726 EPOCH 296
2024-01-31 18:31:50,311 Epoch 296: Total Training Recognition Loss 1100.92  Total Training Translation Loss 3487.70 
2024-01-31 18:31:50,311 EPOCH 297
2024-01-31 18:32:07,907 Epoch 297: Total Training Recognition Loss 1102.78  Total Training Translation Loss 3486.46 
2024-01-31 18:32:07,908 EPOCH 298
2024-01-31 18:32:08,885 [Epoch: 298 Step: 00010100] Batch Recognition Loss:  74.636093 => Gls Tokens per Sec:      400 || Batch Translation Loss: 126.785408 => Txt Tokens per Sec:     1065 || Lr: 0.000100
2024-01-31 18:32:25,526 Epoch 298: Total Training Recognition Loss 1100.59  Total Training Translation Loss 3487.45 
2024-01-31 18:32:25,527 EPOCH 299
2024-01-31 18:32:43,096 Epoch 299: Total Training Recognition Loss 1102.16  Total Training Translation Loss 3487.10 
2024-01-31 18:32:43,096 EPOCH 300
2024-01-31 18:33:00,630 [Epoch: 300 Step: 00010200] Batch Recognition Loss:  36.593170 => Gls Tokens per Sec:      606 || Batch Translation Loss: 110.877945 => Txt Tokens per Sec:     1683 || Lr: 0.000100
2024-01-31 18:33:00,631 Epoch 300: Total Training Recognition Loss 1099.41  Total Training Translation Loss 3486.28 
2024-01-31 18:33:00,631 EPOCH 301
2024-01-31 18:33:18,336 Epoch 301: Total Training Recognition Loss 1100.06  Total Training Translation Loss 3487.56 
2024-01-31 18:33:18,336 EPOCH 302
2024-01-31 18:33:35,923 Epoch 302: Total Training Recognition Loss 1099.54  Total Training Translation Loss 3487.04 
2024-01-31 18:33:35,923 EPOCH 303
2024-01-31 18:33:52,860 [Epoch: 303 Step: 00010300] Batch Recognition Loss:  43.266624 => Gls Tokens per Sec:      590 || Batch Translation Loss: 127.413849 => Txt Tokens per Sec:     1639 || Lr: 0.000100
2024-01-31 18:33:53,598 Epoch 303: Total Training Recognition Loss 1102.75  Total Training Translation Loss 3486.99 
2024-01-31 18:33:53,598 EPOCH 304
2024-01-31 18:34:11,189 Epoch 304: Total Training Recognition Loss 1099.95  Total Training Translation Loss 3486.68 
2024-01-31 18:34:11,190 EPOCH 305
2024-01-31 18:34:28,797 Epoch 305: Total Training Recognition Loss 1100.18  Total Training Translation Loss 3487.00 
2024-01-31 18:34:28,797 EPOCH 306
2024-01-31 18:34:44,119 [Epoch: 306 Step: 00010400] Batch Recognition Loss:  22.644295 => Gls Tokens per Sec:      610 || Batch Translation Loss:  83.133026 => Txt Tokens per Sec:     1683 || Lr: 0.000100
2024-01-31 18:34:46,224 Epoch 306: Total Training Recognition Loss 1102.03  Total Training Translation Loss 3487.55 
2024-01-31 18:34:46,224 EPOCH 307
2024-01-31 18:35:03,779 Epoch 307: Total Training Recognition Loss 1100.69  Total Training Translation Loss 3487.10 
2024-01-31 18:35:03,779 EPOCH 308
2024-01-31 18:35:21,253 Epoch 308: Total Training Recognition Loss 1098.84  Total Training Translation Loss 3488.76 
2024-01-31 18:35:21,253 EPOCH 309
2024-01-31 18:35:36,396 [Epoch: 309 Step: 00010500] Batch Recognition Loss:  36.594494 => Gls Tokens per Sec:      575 || Batch Translation Loss: 101.532219 => Txt Tokens per Sec:     1643 || Lr: 0.000100
2024-01-31 18:35:38,735 Epoch 309: Total Training Recognition Loss 1100.72  Total Training Translation Loss 3487.67 
2024-01-31 18:35:38,735 EPOCH 310
2024-01-31 18:35:56,324 Epoch 310: Total Training Recognition Loss 1101.18  Total Training Translation Loss 3487.89 
2024-01-31 18:35:56,324 EPOCH 311
2024-01-31 18:36:13,858 Epoch 311: Total Training Recognition Loss 1098.47  Total Training Translation Loss 3487.50 
2024-01-31 18:36:13,858 EPOCH 312
2024-01-31 18:36:28,030 [Epoch: 312 Step: 00010600] Batch Recognition Loss:  51.309639 => Gls Tokens per Sec:      569 || Batch Translation Loss: 129.285522 => Txt Tokens per Sec:     1606 || Lr: 0.000100
2024-01-31 18:36:31,399 Epoch 312: Total Training Recognition Loss 1100.83  Total Training Translation Loss 3487.24 
2024-01-31 18:36:31,399 EPOCH 313
2024-01-31 18:36:48,938 Epoch 313: Total Training Recognition Loss 1102.54  Total Training Translation Loss 3487.53 
2024-01-31 18:36:48,938 EPOCH 314
2024-01-31 18:37:06,525 Epoch 314: Total Training Recognition Loss 1102.30  Total Training Translation Loss 3487.10 
2024-01-31 18:37:06,525 EPOCH 315
2024-01-31 18:37:19,073 [Epoch: 315 Step: 00010700] Batch Recognition Loss:  32.949699 => Gls Tokens per Sec:      592 || Batch Translation Loss: 108.689911 => Txt Tokens per Sec:     1619 || Lr: 0.000100
2024-01-31 18:37:24,273 Epoch 315: Total Training Recognition Loss 1100.51  Total Training Translation Loss 3487.45 
2024-01-31 18:37:24,273 EPOCH 316
2024-01-31 18:37:41,914 Epoch 316: Total Training Recognition Loss 1099.50  Total Training Translation Loss 3487.55 
2024-01-31 18:37:41,914 EPOCH 317
2024-01-31 18:37:59,393 Epoch 317: Total Training Recognition Loss 1102.13  Total Training Translation Loss 3486.98 
2024-01-31 18:37:59,394 EPOCH 318
2024-01-31 18:38:10,796 [Epoch: 318 Step: 00010800] Batch Recognition Loss:  22.551815 => Gls Tokens per Sec:      596 || Batch Translation Loss:  81.645966 => Txt Tokens per Sec:     1608 || Lr: 0.000100
2024-01-31 18:38:16,950 Epoch 318: Total Training Recognition Loss 1099.86  Total Training Translation Loss 3487.37 
2024-01-31 18:38:16,950 EPOCH 319
2024-01-31 18:38:34,550 Epoch 319: Total Training Recognition Loss 1099.12  Total Training Translation Loss 3486.64 
2024-01-31 18:38:34,550 EPOCH 320
2024-01-31 18:38:52,086 Epoch 320: Total Training Recognition Loss 1101.53  Total Training Translation Loss 3488.02 
2024-01-31 18:38:52,086 EPOCH 321
2024-01-31 18:39:02,895 [Epoch: 321 Step: 00010900] Batch Recognition Loss:  37.235710 => Gls Tokens per Sec:      569 || Batch Translation Loss: 121.964203 => Txt Tokens per Sec:     1609 || Lr: 0.000100
2024-01-31 18:39:09,698 Epoch 321: Total Training Recognition Loss 1101.41  Total Training Translation Loss 3487.26 
2024-01-31 18:39:09,698 EPOCH 322
2024-01-31 18:39:27,252 Epoch 322: Total Training Recognition Loss 1100.00  Total Training Translation Loss 3487.97 
2024-01-31 18:39:27,252 EPOCH 323
2024-01-31 18:39:44,831 Epoch 323: Total Training Recognition Loss 1100.16  Total Training Translation Loss 3486.98 
2024-01-31 18:39:44,832 EPOCH 324
2024-01-31 18:39:55,647 [Epoch: 324 Step: 00011000] Batch Recognition Loss:  23.089146 => Gls Tokens per Sec:      510 || Batch Translation Loss:  93.022202 => Txt Tokens per Sec:     1491 || Lr: 0.000100
2024-01-31 18:40:02,446 Epoch 324: Total Training Recognition Loss 1101.37  Total Training Translation Loss 3487.90 
2024-01-31 18:40:02,446 EPOCH 325
2024-01-31 18:40:20,076 Epoch 325: Total Training Recognition Loss 1099.87  Total Training Translation Loss 3487.95 
2024-01-31 18:40:20,076 EPOCH 326
2024-01-31 18:40:37,589 Epoch 326: Total Training Recognition Loss 1101.18  Total Training Translation Loss 3487.63 
2024-01-31 18:40:37,589 EPOCH 327
2024-01-31 18:40:46,101 [Epoch: 327 Step: 00011100] Batch Recognition Loss:  30.688765 => Gls Tokens per Sec:      602 || Batch Translation Loss:  96.110237 => Txt Tokens per Sec:     1696 || Lr: 0.000100
2024-01-31 18:40:55,176 Epoch 327: Total Training Recognition Loss 1101.55  Total Training Translation Loss 3488.15 
2024-01-31 18:40:55,177 EPOCH 328
2024-01-31 18:41:12,754 Epoch 328: Total Training Recognition Loss 1101.59  Total Training Translation Loss 3487.72 
2024-01-31 18:41:12,755 EPOCH 329
2024-01-31 18:41:30,326 Epoch 329: Total Training Recognition Loss 1100.96  Total Training Translation Loss 3486.74 
2024-01-31 18:41:30,326 EPOCH 330
2024-01-31 18:41:38,828 [Epoch: 330 Step: 00011200] Batch Recognition Loss:  58.034409 => Gls Tokens per Sec:      527 || Batch Translation Loss: 132.379150 => Txt Tokens per Sec:     1509 || Lr: 0.000100
2024-01-31 18:41:47,836 Epoch 330: Total Training Recognition Loss 1100.10  Total Training Translation Loss 3486.95 
2024-01-31 18:41:47,837 EPOCH 331
2024-01-31 18:42:05,531 Epoch 331: Total Training Recognition Loss 1101.61  Total Training Translation Loss 3487.37 
2024-01-31 18:42:05,531 EPOCH 332
2024-01-31 18:42:23,165 Epoch 332: Total Training Recognition Loss 1100.16  Total Training Translation Loss 3486.40 
2024-01-31 18:42:23,166 EPOCH 333
2024-01-31 18:42:28,323 [Epoch: 333 Step: 00011300] Batch Recognition Loss:  27.793755 => Gls Tokens per Sec:      745 || Batch Translation Loss:  84.497116 => Txt Tokens per Sec:     1938 || Lr: 0.000100
2024-01-31 18:42:40,834 Epoch 333: Total Training Recognition Loss 1101.60  Total Training Translation Loss 3487.55 
2024-01-31 18:42:40,834 EPOCH 334
2024-01-31 18:42:58,495 Epoch 334: Total Training Recognition Loss 1101.19  Total Training Translation Loss 3487.61 
2024-01-31 18:42:58,495 EPOCH 335
2024-01-31 18:43:16,209 Epoch 335: Total Training Recognition Loss 1100.64  Total Training Translation Loss 3486.91 
2024-01-31 18:43:16,210 EPOCH 336
2024-01-31 18:43:20,563 [Epoch: 336 Step: 00011400] Batch Recognition Loss:  37.187744 => Gls Tokens per Sec:      678 || Batch Translation Loss: 126.036629 => Txt Tokens per Sec:     1735 || Lr: 0.000100
2024-01-31 18:43:33,742 Epoch 336: Total Training Recognition Loss 1100.25  Total Training Translation Loss 3487.14 
2024-01-31 18:43:33,742 EPOCH 337
2024-01-31 18:43:51,394 Epoch 337: Total Training Recognition Loss 1099.99  Total Training Translation Loss 3487.17 
2024-01-31 18:43:51,394 EPOCH 338
2024-01-31 18:44:08,939 Epoch 338: Total Training Recognition Loss 1101.50  Total Training Translation Loss 3487.90 
2024-01-31 18:44:08,940 EPOCH 339
2024-01-31 18:44:12,723 [Epoch: 339 Step: 00011500] Batch Recognition Loss:  38.182877 => Gls Tokens per Sec:      677 || Batch Translation Loss: 115.988289 => Txt Tokens per Sec:     1912 || Lr: 0.000100
2024-01-31 18:44:26,562 Epoch 339: Total Training Recognition Loss 1101.62  Total Training Translation Loss 3487.93 
2024-01-31 18:44:26,563 EPOCH 340
2024-01-31 18:44:44,247 Epoch 340: Total Training Recognition Loss 1099.89  Total Training Translation Loss 3487.81 
2024-01-31 18:44:44,247 EPOCH 341
2024-01-31 18:45:01,951 Epoch 341: Total Training Recognition Loss 1102.06  Total Training Translation Loss 3487.24 
2024-01-31 18:45:01,951 EPOCH 342
2024-01-31 18:45:04,466 [Epoch: 342 Step: 00011600] Batch Recognition Loss:  30.583473 => Gls Tokens per Sec:      764 || Batch Translation Loss: 103.942291 => Txt Tokens per Sec:     2026 || Lr: 0.000100
2024-01-31 18:45:19,704 Epoch 342: Total Training Recognition Loss 1101.35  Total Training Translation Loss 3487.59 
2024-01-31 18:45:19,705 EPOCH 343
2024-01-31 18:45:37,203 Epoch 343: Total Training Recognition Loss 1098.46  Total Training Translation Loss 3486.79 
2024-01-31 18:45:37,203 EPOCH 344
2024-01-31 18:45:54,782 Epoch 344: Total Training Recognition Loss 1099.76  Total Training Translation Loss 3486.78 
2024-01-31 18:45:54,782 EPOCH 345
2024-01-31 18:45:56,606 [Epoch: 345 Step: 00011700] Batch Recognition Loss:  22.588230 => Gls Tokens per Sec:      702 || Batch Translation Loss:  84.708832 => Txt Tokens per Sec:     2001 || Lr: 0.000100
2024-01-31 18:46:12,410 Epoch 345: Total Training Recognition Loss 1100.59  Total Training Translation Loss 3487.15 
2024-01-31 18:46:12,410 EPOCH 346
2024-01-31 18:46:30,090 Epoch 346: Total Training Recognition Loss 1100.55  Total Training Translation Loss 3486.72 
2024-01-31 18:46:30,091 EPOCH 347
2024-01-31 18:46:47,627 Epoch 347: Total Training Recognition Loss 1098.53  Total Training Translation Loss 3487.10 
2024-01-31 18:46:47,627 EPOCH 348
2024-01-31 18:46:48,208 [Epoch: 348 Step: 00011800] Batch Recognition Loss:  23.844112 => Gls Tokens per Sec:     1103 || Batch Translation Loss:  92.873947 => Txt Tokens per Sec:     2919 || Lr: 0.000100
2024-01-31 18:47:05,147 Epoch 348: Total Training Recognition Loss 1099.91  Total Training Translation Loss 3487.35 
2024-01-31 18:47:05,147 EPOCH 349
2024-01-31 18:47:22,768 Epoch 349: Total Training Recognition Loss 1099.10  Total Training Translation Loss 3488.11 
2024-01-31 18:47:22,768 EPOCH 350
2024-01-31 18:47:40,326 [Epoch: 350 Step: 00011900] Batch Recognition Loss:  37.523140 => Gls Tokens per Sec:      605 || Batch Translation Loss: 115.810486 => Txt Tokens per Sec:     1681 || Lr: 0.000100
2024-01-31 18:47:40,326 Epoch 350: Total Training Recognition Loss 1101.37  Total Training Translation Loss 3487.33 
2024-01-31 18:47:40,326 EPOCH 351
2024-01-31 18:47:57,820 Epoch 351: Total Training Recognition Loss 1098.54  Total Training Translation Loss 3486.67 
2024-01-31 18:47:57,820 EPOCH 352
2024-01-31 18:48:15,462 Epoch 352: Total Training Recognition Loss 1099.87  Total Training Translation Loss 3486.33 
2024-01-31 18:48:15,463 EPOCH 353
2024-01-31 18:48:31,431 [Epoch: 353 Step: 00012000] Batch Recognition Loss:  22.826012 => Gls Tokens per Sec:      626 || Batch Translation Loss:  86.424179 => Txt Tokens per Sec:     1720 || Lr: 0.000100
2024-01-31 18:48:55,341 Validation result at epoch 353, step    12000: duration: 23.9104s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 349.92886	Translation Loss: 73481.18750	PPL: 1561.28760
	Eval Metric: BLEU
	WER 534.32	(DEL: 4.45,	INS: 444.92,	SUB: 84.96)
	BLEU-4 0.00	(BLEU-1: 0.01,	BLEU-2: 0.00,	BLEU-3: 0.00,	BLEU-4: 0.00)
	CHRF 3.50	ROUGE 0.02
2024-01-31 18:48:55,342 Logging Recognition and Translation Outputs
2024-01-31 18:48:55,342 ========================================================================================================================
2024-01-31 18:48:55,343 Logging Sequence: 177_50.00
2024-01-31 18:48:55,343 	Gloss Reference :	***** * ***** ***** * ***** *********** ***** ******* ***** ******* ***** *** ***** * ***** * ***** * ***** ********************************* ***** * ***** * ***** * ***** A         B+C+D+E
2024-01-31 18:48:55,343 	Gloss Hypothesis:	<pad> E <pad> <unk> C <pad> B+C+E+C+B+E <pad> C+B+E+B <pad> C+B+E+C <pad> E+C <pad> E <pad> E <unk> E <pad> C+E+C+B+C+E+B+E+C+E+C+E+B+E+C+B+E <pad> C <pad> E <pad> E <pad> C+E+C+E+C <pad>  
2024-01-31 18:48:55,343 	Gloss Alignment :	I     I I     I     I I     I           I     I       I     I       I     I   I     I I     I I     I I     I                                 I     I I     I I     I I     S         S      
2024-01-31 18:48:55,344 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 18:48:55,347 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** a   similar reward of  rs  50000 was announced for information against his associate ajay kumar
2024-01-31 18:48:55,347 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s>     <s>    <s> <s> <s>   <s> <s>       <s> <s>         <s>     <s> <s>       <s>  <s>  
2024-01-31 18:48:55,347 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   S   S       S      S   S   S     S   S         S   S           S       S   S         S    S    
2024-01-31 18:48:55,347 ========================================================================================================================
2024-01-31 18:48:55,347 Logging Sequence: 122_86.00
2024-01-31 18:48:55,348 	Gloss Reference :	***** * ***** * ***** * ***** A B+C+D+E
2024-01-31 18:48:55,348 	Gloss Hypothesis:	<unk> E <unk> E <unk> E <unk> E <unk>  
2024-01-31 18:48:55,348 	Gloss Alignment :	I     I I     I I     I I     S S      
2024-01-31 18:48:55,348 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 18:48:55,350 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** after winning chanu spoke to  the media and said
2024-01-31 18:48:55,350 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s>   <s>     <s>   <s>   <s> <s> <s>   <s> <s> 
2024-01-31 18:48:55,350 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   S     S       S     S     S   S   S     S   S   
2024-01-31 18:48:55,351 ========================================================================================================================
2024-01-31 18:48:55,351 Logging Sequence: 165_27.00
2024-01-31 18:48:55,351 	Gloss Reference :	* *** A     B+C+D+E      
2024-01-31 18:48:55,351 	Gloss Hypothesis:	E A+E <pad> E+A+E+C+E+C+E
2024-01-31 18:48:55,351 	Gloss Alignment :	I I   S     S            
2024-01-31 18:48:55,351 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 18:48:55,354 	Text Reference  :	*** *** *** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** so    then  they  change their routes some  people believe in    this  while some  don't
2024-01-31 18:48:55,354 	Text Hypothesis :	<s> <s> <s> sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha  sabha sabha  sabha sabha  sabha   sabha sabha sabha sabha sabha
2024-01-31 18:48:55,355 	Text Alignment  :	I   I   I   I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     S     S     S     S      S     S      S     S      S       S     S     S     S     S    
2024-01-31 18:48:55,355 ========================================================================================================================
2024-01-31 18:48:55,355 Logging Sequence: 70_65.00
2024-01-31 18:48:55,355 	Gloss Reference :	***** A   B+C+D+E
2024-01-31 18:48:55,355 	Gloss Hypothesis:	<unk> C+B <unk>  
2024-01-31 18:48:55,355 	Gloss Alignment :	I     S   S      
2024-01-31 18:48:55,355 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 18:48:55,358 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** during the press conference a   table was placed in  front of  the media
2024-01-31 18:48:55,358 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s>    <s> <s>   <s>        <s> <s>   <s> <s>    <s> <s>   <s> <s> <s>  
2024-01-31 18:48:55,358 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   S      S   S     S          S   S     S   S      S   S     S   S   S    
2024-01-31 18:48:55,359 ========================================================================================================================
2024-01-31 18:48:55,359 Logging Sequence: 149_65.00
2024-01-31 18:48:55,359 	Gloss Reference :	A     B+C+D+E                                        
2024-01-31 18:48:55,359 	Gloss Hypothesis:	<unk> E+C+E+C+E+C+E+C+E+C+E+C+D+C+E+C+E+C+E+C+E+C+E+C
2024-01-31 18:48:55,359 	Gloss Alignment :	S     S                                              
2024-01-31 18:48:55,359 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 18:48:55,364 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** ***** ***** ***** ***** ***** ***** ***** ***** ***** at    6am   on    6th   november 2022  the   police reached sri   lankan team's hotel in    sydney australia's central business district cbd  
2024-01-31 18:48:55,364 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha    sabha sabha sabha  sabha   sabha sabha  sabha  sabha sabha sabha  sabha       sabha   sabha    sabha    sabha
2024-01-31 18:48:55,364 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I     I     I     I     I     I     I     I     I     S     S     S     S     S        S     S     S      S       S     S      S      S     S     S      S           S       S        S        S    
2024-01-31 18:48:55,364 ========================================================================================================================
2024-01-31 18:48:56,961 Epoch 353: Total Training Recognition Loss 1101.96  Total Training Translation Loss 3487.37 
2024-01-31 18:48:56,962 EPOCH 354
2024-01-31 18:49:14,568 Epoch 354: Total Training Recognition Loss 1100.54  Total Training Translation Loss 3487.75 
2024-01-31 18:49:14,568 EPOCH 355
2024-01-31 18:49:32,126 Epoch 355: Total Training Recognition Loss 1099.92  Total Training Translation Loss 3487.50 
2024-01-31 18:49:32,126 EPOCH 356
2024-01-31 18:49:47,604 [Epoch: 356 Step: 00012100] Batch Recognition Loss:  58.144241 => Gls Tokens per Sec:      604 || Batch Translation Loss: 132.286041 => Txt Tokens per Sec:     1671 || Lr: 0.000100
2024-01-31 18:49:49,709 Epoch 356: Total Training Recognition Loss 1103.14  Total Training Translation Loss 3487.70 
2024-01-31 18:49:49,709 EPOCH 357
2024-01-31 18:50:07,177 Epoch 357: Total Training Recognition Loss 1100.60  Total Training Translation Loss 3487.01 
2024-01-31 18:50:07,178 EPOCH 358
2024-01-31 18:50:24,728 Epoch 358: Total Training Recognition Loss 1099.12  Total Training Translation Loss 3487.05 
2024-01-31 18:50:24,728 EPOCH 359
2024-01-31 18:50:39,431 [Epoch: 359 Step: 00012200] Batch Recognition Loss:  28.319183 => Gls Tokens per Sec:      592 || Batch Translation Loss:  90.624069 => Txt Tokens per Sec:     1658 || Lr: 0.000100
2024-01-31 18:50:42,254 Epoch 359: Total Training Recognition Loss 1103.46  Total Training Translation Loss 3488.11 
2024-01-31 18:50:42,255 EPOCH 360
2024-01-31 18:51:00,612 Epoch 360: Total Training Recognition Loss 1100.57  Total Training Translation Loss 3487.30 
2024-01-31 18:51:00,613 EPOCH 361
2024-01-31 18:51:20,191 Epoch 361: Total Training Recognition Loss 1100.99  Total Training Translation Loss 3487.94 
2024-01-31 18:51:20,191 EPOCH 362
2024-01-31 18:51:33,716 [Epoch: 362 Step: 00012300] Batch Recognition Loss:  18.818798 => Gls Tokens per Sec:      615 || Batch Translation Loss:  89.988327 => Txt Tokens per Sec:     1695 || Lr: 0.000100
2024-01-31 18:51:37,837 Epoch 362: Total Training Recognition Loss 1100.54  Total Training Translation Loss 3486.60 
2024-01-31 18:51:37,837 EPOCH 363
2024-01-31 18:51:55,399 Epoch 363: Total Training Recognition Loss 1102.06  Total Training Translation Loss 3487.01 
2024-01-31 18:51:55,399 EPOCH 364
2024-01-31 18:52:13,106 Epoch 364: Total Training Recognition Loss 1101.87  Total Training Translation Loss 3486.12 
2024-01-31 18:52:13,106 EPOCH 365
2024-01-31 18:52:25,466 [Epoch: 365 Step: 00012400] Batch Recognition Loss:  33.499630 => Gls Tokens per Sec:      601 || Batch Translation Loss: 107.383194 => Txt Tokens per Sec:     1610 || Lr: 0.000100
2024-01-31 18:52:30,780 Epoch 365: Total Training Recognition Loss 1100.21  Total Training Translation Loss 3486.85 
2024-01-31 18:52:30,780 EPOCH 366
2024-01-31 18:52:48,333 Epoch 366: Total Training Recognition Loss 1100.31  Total Training Translation Loss 3487.39 
2024-01-31 18:52:48,333 EPOCH 367
2024-01-31 18:53:05,909 Epoch 367: Total Training Recognition Loss 1101.31  Total Training Translation Loss 3487.67 
2024-01-31 18:53:05,910 EPOCH 368
2024-01-31 18:53:16,425 [Epoch: 368 Step: 00012500] Batch Recognition Loss:  36.301369 => Gls Tokens per Sec:      646 || Batch Translation Loss: 104.290192 => Txt Tokens per Sec:     1771 || Lr: 0.000100
2024-01-31 18:53:23,486 Epoch 368: Total Training Recognition Loss 1099.56  Total Training Translation Loss 3487.08 
2024-01-31 18:53:23,486 EPOCH 369
2024-01-31 18:53:41,149 Epoch 369: Total Training Recognition Loss 1099.26  Total Training Translation Loss 3487.39 
2024-01-31 18:53:41,149 EPOCH 370
2024-01-31 18:53:58,682 Epoch 370: Total Training Recognition Loss 1100.79  Total Training Translation Loss 3487.36 
2024-01-31 18:53:58,682 EPOCH 371
2024-01-31 18:54:09,998 [Epoch: 371 Step: 00012600] Batch Recognition Loss:  28.103172 => Gls Tokens per Sec:      544 || Batch Translation Loss:  88.923500 => Txt Tokens per Sec:     1551 || Lr: 0.000100
2024-01-31 18:54:16,318 Epoch 371: Total Training Recognition Loss 1099.76  Total Training Translation Loss 3487.12 
2024-01-31 18:54:16,318 EPOCH 372
2024-01-31 18:54:33,918 Epoch 372: Total Training Recognition Loss 1102.15  Total Training Translation Loss 3487.15 
2024-01-31 18:54:33,919 EPOCH 373
2024-01-31 18:54:51,542 Epoch 373: Total Training Recognition Loss 1100.93  Total Training Translation Loss 3487.15 
2024-01-31 18:54:51,542 EPOCH 374
2024-01-31 18:54:59,894 [Epoch: 374 Step: 00012700] Batch Recognition Loss:  12.975487 => Gls Tokens per Sec:      660 || Batch Translation Loss:  72.188126 => Txt Tokens per Sec:     1782 || Lr: 0.000100
2024-01-31 18:55:09,209 Epoch 374: Total Training Recognition Loss 1102.15  Total Training Translation Loss 3487.57 
2024-01-31 18:55:09,209 EPOCH 375
2024-01-31 18:55:26,888 Epoch 375: Total Training Recognition Loss 1102.98  Total Training Translation Loss 3486.56 
2024-01-31 18:55:26,888 EPOCH 376
2024-01-31 18:55:44,560 Epoch 376: Total Training Recognition Loss 1100.72  Total Training Translation Loss 3486.20 
2024-01-31 18:55:44,560 EPOCH 377
2024-01-31 18:55:54,609 [Epoch: 377 Step: 00012800] Batch Recognition Loss:  51.687801 => Gls Tokens per Sec:      485 || Batch Translation Loss: 129.751785 => Txt Tokens per Sec:     1447 || Lr: 0.000100
2024-01-31 18:56:02,108 Epoch 377: Total Training Recognition Loss 1101.98  Total Training Translation Loss 3488.18 
2024-01-31 18:56:02,108 EPOCH 378
2024-01-31 18:56:19,710 Epoch 378: Total Training Recognition Loss 1100.50  Total Training Translation Loss 3487.42 
2024-01-31 18:56:19,710 EPOCH 379
2024-01-31 18:56:37,357 Epoch 379: Total Training Recognition Loss 1101.75  Total Training Translation Loss 3487.66 
2024-01-31 18:56:37,357 EPOCH 380
2024-01-31 18:56:45,091 [Epoch: 380 Step: 00012900] Batch Recognition Loss:  33.039852 => Gls Tokens per Sec:      547 || Batch Translation Loss: 112.816574 => Txt Tokens per Sec:     1595 || Lr: 0.000100
2024-01-31 18:56:54,982 Epoch 380: Total Training Recognition Loss 1100.93  Total Training Translation Loss 3487.48 
2024-01-31 18:56:54,982 EPOCH 381
2024-01-31 18:57:12,732 Epoch 381: Total Training Recognition Loss 1102.64  Total Training Translation Loss 3487.50 
2024-01-31 18:57:12,732 EPOCH 382
2024-01-31 18:57:30,384 Epoch 382: Total Training Recognition Loss 1101.53  Total Training Translation Loss 3487.62 
2024-01-31 18:57:30,384 EPOCH 383
2024-01-31 18:57:34,860 [Epoch: 383 Step: 00013000] Batch Recognition Loss:  23.171963 => Gls Tokens per Sec:      858 || Batch Translation Loss:  89.777298 => Txt Tokens per Sec:     2146 || Lr: 0.000100
2024-01-31 18:57:48,101 Epoch 383: Total Training Recognition Loss 1102.93  Total Training Translation Loss 3486.45 
2024-01-31 18:57:48,101 EPOCH 384
2024-01-31 18:58:05,787 Epoch 384: Total Training Recognition Loss 1101.99  Total Training Translation Loss 3485.84 
2024-01-31 18:58:05,787 EPOCH 385
2024-01-31 18:58:23,381 Epoch 385: Total Training Recognition Loss 1101.40  Total Training Translation Loss 3486.96 
2024-01-31 18:58:23,382 EPOCH 386
2024-01-31 18:58:30,609 [Epoch: 386 Step: 00013100] Batch Recognition Loss:  44.119545 => Gls Tokens per Sec:      443 || Batch Translation Loss: 129.202332 => Txt Tokens per Sec:     1325 || Lr: 0.000100
2024-01-31 18:58:41,016 Epoch 386: Total Training Recognition Loss 1098.95  Total Training Translation Loss 3487.43 
2024-01-31 18:58:41,016 EPOCH 387
2024-01-31 18:59:00,865 Epoch 387: Total Training Recognition Loss 1101.15  Total Training Translation Loss 3487.10 
2024-01-31 18:59:00,865 EPOCH 388
2024-01-31 18:59:18,551 Epoch 388: Total Training Recognition Loss 1099.68  Total Training Translation Loss 3487.64 
2024-01-31 18:59:18,552 EPOCH 389
2024-01-31 18:59:24,697 [Epoch: 389 Step: 00013200] Batch Recognition Loss:  45.801151 => Gls Tokens per Sec:      417 || Batch Translation Loss: 125.238571 => Txt Tokens per Sec:     1225 || Lr: 0.000100
2024-01-31 18:59:36,177 Epoch 389: Total Training Recognition Loss 1102.14  Total Training Translation Loss 3486.49 
2024-01-31 18:59:36,177 EPOCH 390
2024-01-31 18:59:53,757 Epoch 390: Total Training Recognition Loss 1100.02  Total Training Translation Loss 3486.99 
2024-01-31 18:59:53,757 EPOCH 391
2024-01-31 19:00:11,319 Epoch 391: Total Training Recognition Loss 1102.54  Total Training Translation Loss 3487.17 
2024-01-31 19:00:11,320 EPOCH 392
2024-01-31 19:00:13,861 [Epoch: 392 Step: 00013300] Batch Recognition Loss:  36.746727 => Gls Tokens per Sec:      756 || Batch Translation Loss: 110.191978 => Txt Tokens per Sec:     2003 || Lr: 0.000100
2024-01-31 19:00:28,879 Epoch 392: Total Training Recognition Loss 1100.01  Total Training Translation Loss 3487.14 
2024-01-31 19:00:28,880 EPOCH 393
2024-01-31 19:00:46,410 Epoch 393: Total Training Recognition Loss 1099.79  Total Training Translation Loss 3487.71 
2024-01-31 19:00:46,411 EPOCH 394
2024-01-31 19:01:04,139 Epoch 394: Total Training Recognition Loss 1100.97  Total Training Translation Loss 3487.24 
2024-01-31 19:01:04,139 EPOCH 395
2024-01-31 19:01:06,926 [Epoch: 395 Step: 00013400] Batch Recognition Loss:  35.716969 => Gls Tokens per Sec:      460 || Batch Translation Loss: 108.193840 => Txt Tokens per Sec:     1218 || Lr: 0.000100
2024-01-31 19:01:21,613 Epoch 395: Total Training Recognition Loss 1099.80  Total Training Translation Loss 3487.65 
2024-01-31 19:01:21,614 EPOCH 396
2024-01-31 19:01:39,149 Epoch 396: Total Training Recognition Loss 1099.83  Total Training Translation Loss 3487.29 
2024-01-31 19:01:39,149 EPOCH 397
2024-01-31 19:01:56,614 Epoch 397: Total Training Recognition Loss 1101.71  Total Training Translation Loss 3487.59 
2024-01-31 19:01:56,615 EPOCH 398
2024-01-31 19:01:58,942 [Epoch: 398 Step: 00013500] Batch Recognition Loss:  57.555576 => Gls Tokens per Sec:      275 || Batch Translation Loss: 132.994415 => Txt Tokens per Sec:      957 || Lr: 0.000100
2024-01-31 19:02:14,081 Epoch 398: Total Training Recognition Loss 1099.53  Total Training Translation Loss 3486.96 
2024-01-31 19:02:14,081 EPOCH 399
2024-01-31 19:02:31,598 Epoch 399: Total Training Recognition Loss 1101.49  Total Training Translation Loss 3487.33 
2024-01-31 19:02:31,598 EPOCH 400
2024-01-31 19:02:49,204 [Epoch: 400 Step: 00013600] Batch Recognition Loss:   6.241632 => Gls Tokens per Sec:      604 || Batch Translation Loss:  50.701988 => Txt Tokens per Sec:     1676 || Lr: 0.000100
2024-01-31 19:02:49,204 Epoch 400: Total Training Recognition Loss 1102.40  Total Training Translation Loss 3487.15 
2024-01-31 19:02:49,204 EPOCH 401
2024-01-31 19:03:06,903 Epoch 401: Total Training Recognition Loss 1100.02  Total Training Translation Loss 3486.86 
2024-01-31 19:03:06,903 EPOCH 402
2024-01-31 19:03:24,417 Epoch 402: Total Training Recognition Loss 1097.04  Total Training Translation Loss 3487.51 
2024-01-31 19:03:24,417 EPOCH 403
2024-01-31 19:03:40,605 [Epoch: 403 Step: 00013700] Batch Recognition Loss:  36.634666 => Gls Tokens per Sec:      633 || Batch Translation Loss: 121.870163 => Txt Tokens per Sec:     1739 || Lr: 0.000100
2024-01-31 19:03:42,072 Epoch 403: Total Training Recognition Loss 1102.34  Total Training Translation Loss 3488.16 
2024-01-31 19:03:42,072 EPOCH 404
2024-01-31 19:03:59,733 Epoch 404: Total Training Recognition Loss 1097.92  Total Training Translation Loss 3487.95 
2024-01-31 19:03:59,734 EPOCH 405
2024-01-31 19:04:17,278 Epoch 405: Total Training Recognition Loss 1101.79  Total Training Translation Loss 3486.15 
2024-01-31 19:04:17,279 EPOCH 406
2024-01-31 19:04:33,508 [Epoch: 406 Step: 00013800] Batch Recognition Loss:  27.331104 => Gls Tokens per Sec:      576 || Batch Translation Loss:  95.763451 => Txt Tokens per Sec:     1608 || Lr: 0.000100
2024-01-31 19:04:35,071 Epoch 406: Total Training Recognition Loss 1099.53  Total Training Translation Loss 3486.63 
2024-01-31 19:04:35,071 EPOCH 407
2024-01-31 19:04:52,799 Epoch 407: Total Training Recognition Loss 1103.14  Total Training Translation Loss 3487.89 
2024-01-31 19:04:52,800 EPOCH 408
2024-01-31 19:05:10,375 Epoch 408: Total Training Recognition Loss 1099.90  Total Training Translation Loss 3487.19 
2024-01-31 19:05:10,375 EPOCH 409
2024-01-31 19:05:24,860 [Epoch: 409 Step: 00013900] Batch Recognition Loss:  10.915457 => Gls Tokens per Sec:      601 || Batch Translation Loss:  62.575855 => Txt Tokens per Sec:     1652 || Lr: 0.000100
2024-01-31 19:05:27,945 Epoch 409: Total Training Recognition Loss 1100.41  Total Training Translation Loss 3486.65 
2024-01-31 19:05:27,946 EPOCH 410
2024-01-31 19:05:45,581 Epoch 410: Total Training Recognition Loss 1098.54  Total Training Translation Loss 3487.20 
2024-01-31 19:05:45,581 EPOCH 411
2024-01-31 19:06:03,107 Epoch 411: Total Training Recognition Loss 1099.35  Total Training Translation Loss 3487.13 
2024-01-31 19:06:03,107 EPOCH 412
2024-01-31 19:06:17,501 [Epoch: 412 Step: 00014000] Batch Recognition Loss:  12.927511 => Gls Tokens per Sec:      561 || Batch Translation Loss:  73.816452 => Txt Tokens per Sec:     1576 || Lr: 0.000100
2024-01-31 19:06:41,663 Validation result at epoch 412, step    14000: duration: 24.1616s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 348.52365	Translation Loss: 73486.73438	PPL: 1562.15479
	Eval Metric: BLEU
	WER 527.54	(DEL: 4.45,	INS: 438.35,	SUB: 84.75)
	BLEU-4 0.00	(BLEU-1: 0.01,	BLEU-2: 0.00,	BLEU-3: 0.00,	BLEU-4: 0.00)
	CHRF 3.53	ROUGE 0.02
2024-01-31 19:06:41,664 Logging Recognition and Translation Outputs
2024-01-31 19:06:41,664 ========================================================================================================================
2024-01-31 19:06:41,664 Logging Sequence: 141_40.00
2024-01-31 19:06:41,665 	Gloss Reference :	***** *** ***** * ***** * ***** ********* ***** * ***** * ***** * A     B+C+D+E  
2024-01-31 19:06:41,665 	Gloss Hypothesis:	<unk> D+E <unk> E <unk> E <unk> E+D+E+D+E <unk> D <unk> E <unk> E <unk> E+D+E+C+E
2024-01-31 19:06:41,665 	Gloss Alignment :	I     I   I     I I     I I     I         I     I I     I I     I S     S        
2024-01-31 19:06:41,665 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 19:06:41,669 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ********* ********* ********* ********* ********* ********* ********* got       infected  with      covid-19  he        was       quarantined and       could     not       take      part      in        the       warmup    match    
2024-01-31 19:06:41,669 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> patiently patiently patiently patiently patiently patiently patiently patiently patiently patiently patiently patiently patiently patiently   patiently patiently patiently patiently patiently patiently patiently patiently patiently
2024-01-31 19:06:41,669 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I         I         I         I         I         I         I         S         S         S         S         S         S         S           S         S         S         S         S         S         S         S         S        
2024-01-31 19:06:41,669 ========================================================================================================================
2024-01-31 19:06:41,669 Logging Sequence: 117_37.00
2024-01-31 19:06:41,670 	Gloss Reference :	***** ***** ***** * ***** * ***** * ***** * ***** *** ***** ***** ***** * ***** *** ***** A     B+C+D+E
2024-01-31 19:06:41,670 	Gloss Hypothesis:	<unk> B+A+E <unk> E <unk> E <unk> E <unk> C <unk> C+E <unk> E+B+C <unk> C <unk> C+B <unk> C+E+C <unk>  
2024-01-31 19:06:41,670 	Gloss Alignment :	I     I     I     I I     I I     I I     I I     I   I     I     I     I I     I   I     S     S      
2024-01-31 19:06:41,670 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 19:06:41,672 	Text Reference  :	*** *** *** *** *** *** *** *** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** shikhar  dhawan   put      up       a        wonderful performance scoring  98       runs    
2024-01-31 19:06:41,673 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas  casillas    casillas casillas casillas
2024-01-31 19:06:41,673 	Text Alignment  :	I   I   I   I   I   I   I   I   I        I        I        I        I        I        I        I        I        I        I        I        I        I        I        I        I        I        I        I        I        I        S        S        S        S        S        S         S           S        S        S       
2024-01-31 19:06:41,673 ========================================================================================================================
2024-01-31 19:06:41,673 Logging Sequence: 64_13.00
2024-01-31 19:06:41,674 	Gloss Reference :	***** ***** ***** A ***** ***** ***** ***** ***** ***** * ***** ***** B+C+D+E
2024-01-31 19:06:41,674 	Gloss Hypothesis:	<unk> <pad> <unk> A <pad> <unk> <pad> <unk> <pad> <unk> E <unk> <pad> <unk>  
2024-01-31 19:06:41,674 	Gloss Alignment :	I     I     I       I     I     I     I     I     I     I I     I     S      
2024-01-31 19:06:41,674 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 19:06:41,677 	Text Reference  :	*** *** *** ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* arrangements  were          made          to            move          all           the           ipl           matches       to            the           wankhede      stadium       in            mumbai       
2024-01-31 19:06:41,677 	Text Hypothesis :	<s> <s> <s> misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand
2024-01-31 19:06:41,678 	Text Alignment  :	I   I   I   I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             S             S             S             S             S             S             S             S             S             S             S             S             S             S             S            
2024-01-31 19:06:41,678 ========================================================================================================================
2024-01-31 19:06:41,678 Logging Sequence: 98_121.00
2024-01-31 19:06:41,678 	Gloss Reference :	***** * ***** * ***** *** ***** * ***** * ***** A B+C+D+E
2024-01-31 19:06:41,678 	Gloss Hypothesis:	<unk> C <unk> C <unk> C+B <unk> C <unk> C <unk> C <unk>  
2024-01-31 19:06:41,678 	Gloss Alignment :	I     I I     I I     I   I     I I     I I     S S      
2024-01-31 19:06:41,679 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 19:06:41,681 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* so      then    england legends and     bangladesh legends were    added   to      the     tournament
2024-01-31 19:06:41,681 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing    nothing nothing nothing nothing nothing nothing   
2024-01-31 19:06:41,681 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       S       S       S       S       S       S          S       S       S       S       S       S         
2024-01-31 19:06:41,682 ========================================================================================================================
2024-01-31 19:06:41,682 Logging Sequence: 179_414.00
2024-01-31 19:06:41,682 	Gloss Reference :	***** ***** ***** * ***** * ***** A     B+C+D+E
2024-01-31 19:06:41,682 	Gloss Hypothesis:	<unk> B+E+B <unk> C <unk> C <unk> B+E+C <unk>  
2024-01-31 19:06:41,682 	Gloss Alignment :	I     I     I     I I     I I     S     S      
2024-01-31 19:06:41,682 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 19:06:41,686 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** we  could not travel to  delhi as  there was a   lockdown in  our home town haryana
2024-01-31 19:06:41,686 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s>   <s> <s>    <s> <s>   <s> <s>   <s> <s> <s>      <s> <s> <s>  <s>  <s>    
2024-01-31 19:06:41,686 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   S   S     S   S      S   S     S   S     S   S   S        S   S   S    S    S      
2024-01-31 19:06:41,686 ========================================================================================================================
2024-01-31 19:06:44,791 Epoch 412: Total Training Recognition Loss 1100.36  Total Training Translation Loss 3486.84 
2024-01-31 19:06:44,791 EPOCH 413
2024-01-31 19:07:02,273 Epoch 413: Total Training Recognition Loss 1101.85  Total Training Translation Loss 3487.31 
2024-01-31 19:07:02,273 EPOCH 414
2024-01-31 19:07:19,912 Epoch 414: Total Training Recognition Loss 1098.69  Total Training Translation Loss 3487.34 
2024-01-31 19:07:19,913 EPOCH 415
2024-01-31 19:07:32,747 [Epoch: 415 Step: 00014100] Batch Recognition Loss:  57.874172 => Gls Tokens per Sec:      579 || Batch Translation Loss: 132.331390 => Txt Tokens per Sec:     1589 || Lr: 0.000100
2024-01-31 19:07:37,490 Epoch 415: Total Training Recognition Loss 1100.67  Total Training Translation Loss 3487.43 
2024-01-31 19:07:37,491 EPOCH 416
2024-01-31 19:07:55,172 Epoch 416: Total Training Recognition Loss 1102.09  Total Training Translation Loss 3486.26 
2024-01-31 19:07:55,173 EPOCH 417
2024-01-31 19:08:12,796 Epoch 417: Total Training Recognition Loss 1101.95  Total Training Translation Loss 3486.60 
2024-01-31 19:08:12,796 EPOCH 418
2024-01-31 19:08:25,103 [Epoch: 418 Step: 00014200] Batch Recognition Loss:   6.193639 => Gls Tokens per Sec:      572 || Batch Translation Loss:  50.735413 => Txt Tokens per Sec:     1603 || Lr: 0.000100
2024-01-31 19:08:30,457 Epoch 418: Total Training Recognition Loss 1100.39  Total Training Translation Loss 3486.82 
2024-01-31 19:08:30,457 EPOCH 419
2024-01-31 19:08:48,239 Epoch 419: Total Training Recognition Loss 1101.01  Total Training Translation Loss 3487.49 
2024-01-31 19:08:48,239 EPOCH 420
2024-01-31 19:09:05,761 Epoch 420: Total Training Recognition Loss 1102.99  Total Training Translation Loss 3487.50 
2024-01-31 19:09:05,762 EPOCH 421
2024-01-31 19:09:16,602 [Epoch: 421 Step: 00014300] Batch Recognition Loss:  38.655651 => Gls Tokens per Sec:      567 || Batch Translation Loss: 122.514206 => Txt Tokens per Sec:     1593 || Lr: 0.000100
2024-01-31 19:09:23,304 Epoch 421: Total Training Recognition Loss 1103.09  Total Training Translation Loss 3487.31 
2024-01-31 19:09:23,304 EPOCH 422
2024-01-31 19:09:41,051 Epoch 422: Total Training Recognition Loss 1101.63  Total Training Translation Loss 3486.70 
2024-01-31 19:09:41,052 EPOCH 423
2024-01-31 19:09:58,732 Epoch 423: Total Training Recognition Loss 1101.21  Total Training Translation Loss 3488.17 
2024-01-31 19:09:58,732 EPOCH 424
2024-01-31 19:10:07,665 [Epoch: 424 Step: 00014400] Batch Recognition Loss:  37.916965 => Gls Tokens per Sec:      645 || Batch Translation Loss: 119.638062 => Txt Tokens per Sec:     1810 || Lr: 0.000100
2024-01-31 19:10:16,248 Epoch 424: Total Training Recognition Loss 1101.63  Total Training Translation Loss 3487.53 
2024-01-31 19:10:16,248 EPOCH 425
2024-01-31 19:10:33,772 Epoch 425: Total Training Recognition Loss 1100.39  Total Training Translation Loss 3487.75 
2024-01-31 19:10:33,773 EPOCH 426
2024-01-31 19:10:51,350 Epoch 426: Total Training Recognition Loss 1100.59  Total Training Translation Loss 3487.21 
2024-01-31 19:10:51,351 EPOCH 427
2024-01-31 19:10:58,765 [Epoch: 427 Step: 00014500] Batch Recognition Loss:  36.349205 => Gls Tokens per Sec:      691 || Batch Translation Loss: 103.579681 => Txt Tokens per Sec:     1998 || Lr: 0.000100
2024-01-31 19:11:08,948 Epoch 427: Total Training Recognition Loss 1099.26  Total Training Translation Loss 3487.57 
2024-01-31 19:11:08,948 EPOCH 428
2024-01-31 19:11:26,464 Epoch 428: Total Training Recognition Loss 1100.48  Total Training Translation Loss 3487.58 
2024-01-31 19:11:26,464 EPOCH 429
2024-01-31 19:11:44,062 Epoch 429: Total Training Recognition Loss 1099.62  Total Training Translation Loss 3487.49 
2024-01-31 19:11:44,062 EPOCH 430
2024-01-31 19:11:50,969 [Epoch: 430 Step: 00014600] Batch Recognition Loss:  20.766071 => Gls Tokens per Sec:      649 || Batch Translation Loss:  89.546959 => Txt Tokens per Sec:     1795 || Lr: 0.000100
2024-01-31 19:12:01,590 Epoch 430: Total Training Recognition Loss 1100.37  Total Training Translation Loss 3487.60 
2024-01-31 19:12:01,590 EPOCH 431
2024-01-31 19:12:19,131 Epoch 431: Total Training Recognition Loss 1101.39  Total Training Translation Loss 3486.69 
2024-01-31 19:12:19,131 EPOCH 432
2024-01-31 19:12:36,711 Epoch 432: Total Training Recognition Loss 1102.46  Total Training Translation Loss 3487.69 
2024-01-31 19:12:36,711 EPOCH 433
2024-01-31 19:12:43,872 [Epoch: 433 Step: 00014700] Batch Recognition Loss:  42.871632 => Gls Tokens per Sec:      536 || Batch Translation Loss: 129.199509 => Txt Tokens per Sec:     1568 || Lr: 0.000100
2024-01-31 19:12:54,335 Epoch 433: Total Training Recognition Loss 1100.56  Total Training Translation Loss 3486.56 
2024-01-31 19:12:54,335 EPOCH 434
2024-01-31 19:13:11,931 Epoch 434: Total Training Recognition Loss 1099.95  Total Training Translation Loss 3487.27 
2024-01-31 19:13:11,931 EPOCH 435
2024-01-31 19:13:29,466 Epoch 435: Total Training Recognition Loss 1100.60  Total Training Translation Loss 3487.55 
2024-01-31 19:13:29,466 EPOCH 436
2024-01-31 19:13:35,178 [Epoch: 436 Step: 00014800] Batch Recognition Loss:  18.523508 => Gls Tokens per Sec:      560 || Batch Translation Loss:  89.685081 => Txt Tokens per Sec:     1552 || Lr: 0.000100
2024-01-31 19:13:47,093 Epoch 436: Total Training Recognition Loss 1100.30  Total Training Translation Loss 3487.03 
2024-01-31 19:13:47,093 EPOCH 437
2024-01-31 19:14:04,779 Epoch 437: Total Training Recognition Loss 1098.51  Total Training Translation Loss 3488.09 
2024-01-31 19:14:04,779 EPOCH 438
2024-01-31 19:14:22,435 Epoch 438: Total Training Recognition Loss 1102.30  Total Training Translation Loss 3487.18 
2024-01-31 19:14:22,435 EPOCH 439
2024-01-31 19:14:25,658 [Epoch: 439 Step: 00014900] Batch Recognition Loss:  30.399731 => Gls Tokens per Sec:      794 || Batch Translation Loss: 102.901382 => Txt Tokens per Sec:     2091 || Lr: 0.000100
2024-01-31 19:14:40,067 Epoch 439: Total Training Recognition Loss 1100.90  Total Training Translation Loss 3487.41 
2024-01-31 19:14:40,067 EPOCH 440
2024-01-31 19:14:57,581 Epoch 440: Total Training Recognition Loss 1099.39  Total Training Translation Loss 3486.35 
2024-01-31 19:14:57,581 EPOCH 441
2024-01-31 19:15:15,122 Epoch 441: Total Training Recognition Loss 1100.85  Total Training Translation Loss 3487.63 
2024-01-31 19:15:15,122 EPOCH 442
2024-01-31 19:15:17,514 [Epoch: 442 Step: 00015000] Batch Recognition Loss:  10.785696 => Gls Tokens per Sec:      803 || Batch Translation Loss:  62.994087 => Txt Tokens per Sec:     2063 || Lr: 0.000100
2024-01-31 19:15:32,800 Epoch 442: Total Training Recognition Loss 1100.45  Total Training Translation Loss 3487.49 
2024-01-31 19:15:32,800 EPOCH 443
2024-01-31 19:15:50,392 Epoch 443: Total Training Recognition Loss 1101.00  Total Training Translation Loss 3486.86 
2024-01-31 19:15:50,392 EPOCH 444
2024-01-31 19:16:08,025 Epoch 444: Total Training Recognition Loss 1100.36  Total Training Translation Loss 3487.83 
2024-01-31 19:16:08,025 EPOCH 445
2024-01-31 19:16:09,605 [Epoch: 445 Step: 00015100] Batch Recognition Loss:  32.641628 => Gls Tokens per Sec:      810 || Batch Translation Loss: 110.805061 => Txt Tokens per Sec:     1961 || Lr: 0.000100
2024-01-31 19:16:25,753 Epoch 445: Total Training Recognition Loss 1100.52  Total Training Translation Loss 3486.29 
2024-01-31 19:16:25,754 EPOCH 446
2024-01-31 19:16:43,372 Epoch 446: Total Training Recognition Loss 1101.68  Total Training Translation Loss 3487.21 
2024-01-31 19:16:43,372 EPOCH 447
2024-01-31 19:17:00,953 Epoch 447: Total Training Recognition Loss 1098.11  Total Training Translation Loss 3487.29 
2024-01-31 19:17:00,954 EPOCH 448
2024-01-31 19:17:02,306 [Epoch: 448 Step: 00015200] Batch Recognition Loss:  40.044228 => Gls Tokens per Sec:      474 || Batch Translation Loss: 113.368408 => Txt Tokens per Sec:     1507 || Lr: 0.000100
2024-01-31 19:17:18,509 Epoch 448: Total Training Recognition Loss 1098.26  Total Training Translation Loss 3487.14 
2024-01-31 19:17:18,509 EPOCH 449
2024-01-31 19:17:36,182 Epoch 449: Total Training Recognition Loss 1100.05  Total Training Translation Loss 3487.02 
2024-01-31 19:17:36,182 EPOCH 450
2024-01-31 19:17:53,846 [Epoch: 450 Step: 00015300] Batch Recognition Loss:  38.906143 => Gls Tokens per Sec:      602 || Batch Translation Loss: 123.638733 => Txt Tokens per Sec:     1671 || Lr: 0.000100
2024-01-31 19:17:53,847 Epoch 450: Total Training Recognition Loss 1100.77  Total Training Translation Loss 3486.56 
2024-01-31 19:17:53,847 EPOCH 451
2024-01-31 19:18:11,414 Epoch 451: Total Training Recognition Loss 1102.31  Total Training Translation Loss 3486.79 
2024-01-31 19:18:11,414 EPOCH 452
2024-01-31 19:18:28,989 Epoch 452: Total Training Recognition Loss 1098.19  Total Training Translation Loss 3487.29 
2024-01-31 19:18:28,990 EPOCH 453
2024-01-31 19:18:45,446 [Epoch: 453 Step: 00015400] Batch Recognition Loss:  17.463638 => Gls Tokens per Sec:      607 || Batch Translation Loss:  75.506058 => Txt Tokens per Sec:     1678 || Lr: 0.000100
2024-01-31 19:18:46,612 Epoch 453: Total Training Recognition Loss 1101.80  Total Training Translation Loss 3487.82 
2024-01-31 19:18:46,613 EPOCH 454
2024-01-31 19:19:04,197 Epoch 454: Total Training Recognition Loss 1100.88  Total Training Translation Loss 3488.24 
2024-01-31 19:19:04,197 EPOCH 455
2024-01-31 19:19:21,809 Epoch 455: Total Training Recognition Loss 1100.72  Total Training Translation Loss 3488.07 
2024-01-31 19:19:21,809 EPOCH 456
2024-01-31 19:19:36,924 [Epoch: 456 Step: 00015500] Batch Recognition Loss:  22.908882 => Gls Tokens per Sec:      619 || Batch Translation Loss:  84.783127 => Txt Tokens per Sec:     1702 || Lr: 0.000100
2024-01-31 19:19:39,442 Epoch 456: Total Training Recognition Loss 1099.54  Total Training Translation Loss 3487.23 
2024-01-31 19:19:39,442 EPOCH 457
2024-01-31 19:19:56,851 Epoch 457: Total Training Recognition Loss 1100.38  Total Training Translation Loss 3487.78 
2024-01-31 19:19:56,851 EPOCH 458
2024-01-31 19:20:14,396 Epoch 458: Total Training Recognition Loss 1100.90  Total Training Translation Loss 3487.04 
2024-01-31 19:20:14,396 EPOCH 459
2024-01-31 19:20:27,368 [Epoch: 459 Step: 00015600] Batch Recognition Loss:  27.421816 => Gls Tokens per Sec:      691 || Batch Translation Loss:  94.926086 => Txt Tokens per Sec:     1884 || Lr: 0.000100
2024-01-31 19:20:31,913 Epoch 459: Total Training Recognition Loss 1099.48  Total Training Translation Loss 3486.78 
2024-01-31 19:20:31,913 EPOCH 460
2024-01-31 19:20:49,555 Epoch 460: Total Training Recognition Loss 1099.75  Total Training Translation Loss 3487.21 
2024-01-31 19:20:49,555 EPOCH 461
2024-01-31 19:21:07,373 Epoch 461: Total Training Recognition Loss 1098.83  Total Training Translation Loss 3486.49 
2024-01-31 19:21:07,373 EPOCH 462
2024-01-31 19:21:22,948 [Epoch: 462 Step: 00015700] Batch Recognition Loss:  10.819509 => Gls Tokens per Sec:      534 || Batch Translation Loss:  62.301529 => Txt Tokens per Sec:     1464 || Lr: 0.000100
2024-01-31 19:21:27,428 Epoch 462: Total Training Recognition Loss 1100.01  Total Training Translation Loss 3487.93 
2024-01-31 19:21:27,428 EPOCH 463
2024-01-31 19:21:45,324 Epoch 463: Total Training Recognition Loss 1101.35  Total Training Translation Loss 3487.58 
2024-01-31 19:21:45,324 EPOCH 464
2024-01-31 19:22:02,974 Epoch 464: Total Training Recognition Loss 1102.56  Total Training Translation Loss 3487.21 
2024-01-31 19:22:02,974 EPOCH 465
2024-01-31 19:22:14,773 [Epoch: 465 Step: 00015800] Batch Recognition Loss:  37.384884 => Gls Tokens per Sec:      630 || Batch Translation Loss: 113.982750 => Txt Tokens per Sec:     1720 || Lr: 0.000100
2024-01-31 19:22:20,685 Epoch 465: Total Training Recognition Loss 1100.26  Total Training Translation Loss 3487.09 
2024-01-31 19:22:20,685 EPOCH 466
2024-01-31 19:22:38,434 Epoch 466: Total Training Recognition Loss 1100.09  Total Training Translation Loss 3487.33 
2024-01-31 19:22:38,434 EPOCH 467
2024-01-31 19:22:56,027 Epoch 467: Total Training Recognition Loss 1101.70  Total Training Translation Loss 3487.73 
2024-01-31 19:22:56,028 EPOCH 468
2024-01-31 19:23:09,115 [Epoch: 468 Step: 00015900] Batch Recognition Loss:  21.006563 => Gls Tokens per Sec:      519 || Batch Translation Loss:  87.885521 => Txt Tokens per Sec:     1510 || Lr: 0.000100
2024-01-31 19:23:13,613 Epoch 468: Total Training Recognition Loss 1101.87  Total Training Translation Loss 3487.56 
2024-01-31 19:23:13,614 EPOCH 469
2024-01-31 19:23:31,336 Epoch 469: Total Training Recognition Loss 1098.85  Total Training Translation Loss 3487.17 
2024-01-31 19:23:31,337 EPOCH 470
2024-01-31 19:23:49,025 Epoch 470: Total Training Recognition Loss 1101.63  Total Training Translation Loss 3488.00 
2024-01-31 19:23:49,025 EPOCH 471
2024-01-31 19:23:58,715 [Epoch: 471 Step: 00016000] Batch Recognition Loss:  15.384055 => Gls Tokens per Sec:      661 || Batch Translation Loss:  76.939140 => Txt Tokens per Sec:     1753 || Lr: 0.000100
2024-01-31 19:24:22,917 Validation result at epoch 471, step    16000: duration: 24.2023s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 349.23984	Translation Loss: 73485.63281	PPL: 1561.98230
	Eval Metric: BLEU
	WER 532.42	(DEL: 4.52,	INS: 443.15,	SUB: 84.75)
	BLEU-4 0.00	(BLEU-1: 0.01,	BLEU-2: 0.00,	BLEU-3: 0.00,	BLEU-4: 0.00)
	CHRF 3.52	ROUGE 0.02
2024-01-31 19:24:22,918 Logging Recognition and Translation Outputs
2024-01-31 19:24:22,918 ========================================================================================================================
2024-01-31 19:24:22,919 Logging Sequence: 147_132.00
2024-01-31 19:24:22,919 	Gloss Reference :	* ***** *** ***** A ***** ***** ***** ******************* ***** ***** ***** ******* ***** B+C+D+E      
2024-01-31 19:24:22,919 	Gloss Hypothesis:	E <unk> B+E <unk> A <pad> B+E+B <unk> E+B+E+B+E+B+E+C+B+C <unk> C+E+B <unk> B+C+B+E <unk> B+E+B+C+E+B+C
2024-01-31 19:24:22,919 	Gloss Alignment :	I I     I   I       I     I     I     I                   I     I     I     I       I     S            
2024-01-31 19:24:22,919 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 19:24:22,922 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** i   can not earlier i   used to  have fun in  gymnastics
2024-01-31 19:24:22,922 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s>     <s> <s>  <s> <s>  <s> <s> <s>       
2024-01-31 19:24:22,922 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   S   S   S   S       S   S    S   S    S   S   S         
2024-01-31 19:24:22,922 ========================================================================================================================
2024-01-31 19:24:22,922 Logging Sequence: 116_162.00
2024-01-31 19:24:22,923 	Gloss Reference :	A B+C+D+E
2024-01-31 19:24:22,923 	Gloss Hypothesis:	E C+E+A  
2024-01-31 19:24:22,923 	Gloss Alignment :	S S      
2024-01-31 19:24:22,923 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 19:24:22,926 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* turned        out           the           video         was           shared        on            social        media         by            a             staff         at            the           hotel        
2024-01-31 19:24:22,926 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions
2024-01-31 19:24:22,926 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I             I             I             I             I             I             I             I             I             I             I             I             I             I             S             S             S             S             S             S             S             S             S             S             S             S             S             S             S            
2024-01-31 19:24:22,927 ========================================================================================================================
2024-01-31 19:24:22,927 Logging Sequence: 73_79.00
2024-01-31 19:24:22,927 	Gloss Reference :	***** ***** ***** * ***** * ***** ********* ***** ***** ***** * ***** A       B+C+D+E
2024-01-31 19:24:22,927 	Gloss Hypothesis:	<unk> B+C+A <unk> B <unk> B <unk> C+B+C+B+C <unk> C+B+A <unk> C <unk> B+C+B+C <unk>  
2024-01-31 19:24:22,927 	Gloss Alignment :	I     I     I     I I     I I     I         I     I     I     I I     S       S      
2024-01-31 19:24:22,927 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 19:24:22,931 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***** ***** ***** ***** raina resturant has   food  from  the   rich  spices of    north india to    the   aromatic curries of    south india
2024-01-31 19:24:22,931 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> mateo mateo mateo mateo mateo mateo     mateo mateo mateo mateo mateo mateo  mateo mateo mateo mateo mateo mateo    mateo   mateo mateo mateo
2024-01-31 19:24:22,932 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I     I     I     I     S     S         S     S     S     S     S     S      S     S     S     S     S     S        S       S     S     S    
2024-01-31 19:24:22,932 ========================================================================================================================
2024-01-31 19:24:22,932 Logging Sequence: 165_523.00
2024-01-31 19:24:22,932 	Gloss Reference :	***** ***** *** ***** ***** ***** A ***** * ***** ********* ***** ***** B+C+D+E
2024-01-31 19:24:22,932 	Gloss Hypothesis:	<pad> <unk> B+C <unk> <pad> <unk> A <unk> E <unk> E+B+E+B+E <unk> E+C+E <unk>  
2024-01-31 19:24:22,932 	Gloss Alignment :	I     I     I   I     I     I       I     I I     I         I     I     S      
2024-01-31 19:24:22,933 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 19:24:22,936 	Text Reference  :	******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** as       he       believed that     his      team     might    lose     if       he       takes    off      his      batting  pads    
2024-01-31 19:24:22,936 	Text Hypothesis :	reshared reshared reshared reshared reshared reshared reshared reshared reshared reshared reshared reshared reshared reshared reshared reshared reshared reshared reshared reshared reshared reshared reshared reshared reshared reshared reshared reshared reshared reshared reshared reshared reshared reshared reshared reshared reshared reshared reshared reshared
2024-01-31 19:24:22,936 	Text Alignment  :	I        I        I        I        I        I        I        I        I        I        I        I        I        I        I        I        I        I        I        I        I        I        I        I        I        S        S        S        S        S        S        S        S        S        S        S        S        S        S        S       
2024-01-31 19:24:22,936 ========================================================================================================================
2024-01-31 19:24:22,936 Logging Sequence: 125_72.00
2024-01-31 19:24:22,937 	Gloss Reference :	***** ***** A ***** * ***** *** ***** * ***** ***** ***** * ***** *** ***** * ***** ***** ******* ***** * ***** * ***** * ***** * ***** ***************** ***** ***** B+C+D+E
2024-01-31 19:24:22,937 	Gloss Hypothesis:	<unk> <pad> A <pad> E <unk> B+C <unk> C <unk> E+B+E <pad> E <pad> E+B <unk> E <pad> <unk> C+E+B+E <pad> E <unk> E <unk> E <unk> E <unk> B+E+B+E+B+C+E+C+B <unk> <pad> C+E    
2024-01-31 19:24:22,937 	Gloss Alignment :	I     I       I     I I     I   I     I I     I     I     I I     I   I     I I     I     I       I     I I     I I     I I     I I     I                 I     I     S      
2024-01-31 19:24:22,937 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 19:24:22,941 	Text Reference  :	*** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** some  said  the   pakistani javelineer had   milicious intentions of    tampering with  the   javelin out   of    jealousy
2024-01-31 19:24:22,941 	Text Hypothesis :	<s> sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha     sabha      sabha sabha     sabha      sabha sabha     sabha sabha sabha   sabha sabha sabha   
2024-01-31 19:24:22,941 	Text Alignment  :	I   I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     S     S     S     S         S          S     S         S          S     S         S     S     S       S     S     S       
2024-01-31 19:24:22,941 ========================================================================================================================
2024-01-31 19:24:30,893 Epoch 471: Total Training Recognition Loss 1100.59  Total Training Translation Loss 3486.62 
2024-01-31 19:24:30,893 EPOCH 472
2024-01-31 19:24:48,486 Epoch 472: Total Training Recognition Loss 1099.10  Total Training Translation Loss 3486.37 
2024-01-31 19:24:48,487 EPOCH 473
2024-01-31 19:25:06,175 Epoch 473: Total Training Recognition Loss 1099.64  Total Training Translation Loss 3486.47 
2024-01-31 19:25:06,175 EPOCH 474
2024-01-31 19:25:15,362 [Epoch: 474 Step: 00016100] Batch Recognition Loss:  57.580894 => Gls Tokens per Sec:      627 || Batch Translation Loss: 133.110474 => Txt Tokens per Sec:     1715 || Lr: 0.000100
2024-01-31 19:25:23,753 Epoch 474: Total Training Recognition Loss 1100.24  Total Training Translation Loss 3487.90 
2024-01-31 19:25:23,753 EPOCH 475
2024-01-31 19:25:41,253 Epoch 475: Total Training Recognition Loss 1101.34  Total Training Translation Loss 3487.15 
2024-01-31 19:25:41,254 EPOCH 476
2024-01-31 19:25:58,855 Epoch 476: Total Training Recognition Loss 1098.63  Total Training Translation Loss 3486.87 
2024-01-31 19:25:58,856 EPOCH 477
2024-01-31 19:26:07,315 [Epoch: 477 Step: 00016200] Batch Recognition Loss:  10.810306 => Gls Tokens per Sec:      605 || Batch Translation Loss:  62.897743 => Txt Tokens per Sec:     1684 || Lr: 0.000100
2024-01-31 19:26:16,328 Epoch 477: Total Training Recognition Loss 1100.16  Total Training Translation Loss 3487.43 
2024-01-31 19:26:16,328 EPOCH 478
2024-01-31 19:26:34,011 Epoch 478: Total Training Recognition Loss 1099.70  Total Training Translation Loss 3487.25 
2024-01-31 19:26:34,012 EPOCH 479
2024-01-31 19:26:51,585 Epoch 479: Total Training Recognition Loss 1099.85  Total Training Translation Loss 3487.24 
2024-01-31 19:26:51,586 EPOCH 480
2024-01-31 19:26:59,948 [Epoch: 480 Step: 00016300] Batch Recognition Loss:  46.135719 => Gls Tokens per Sec:      536 || Batch Translation Loss: 125.202568 => Txt Tokens per Sec:     1502 || Lr: 0.000100
2024-01-31 19:27:09,180 Epoch 480: Total Training Recognition Loss 1099.57  Total Training Translation Loss 3486.91 
2024-01-31 19:27:09,180 EPOCH 481
2024-01-31 19:27:26,789 Epoch 481: Total Training Recognition Loss 1102.52  Total Training Translation Loss 3487.29 
2024-01-31 19:27:26,789 EPOCH 482
2024-01-31 19:27:44,481 Epoch 482: Total Training Recognition Loss 1102.19  Total Training Translation Loss 3486.91 
2024-01-31 19:27:44,481 EPOCH 483
2024-01-31 19:27:49,177 [Epoch: 483 Step: 00016400] Batch Recognition Loss:  17.009159 => Gls Tokens per Sec:      765 || Batch Translation Loss:  75.587936 => Txt Tokens per Sec:     2002 || Lr: 0.000100
2024-01-31 19:28:02,116 Epoch 483: Total Training Recognition Loss 1101.93  Total Training Translation Loss 3486.29 
2024-01-31 19:28:02,117 EPOCH 484
2024-01-31 19:28:19,641 Epoch 484: Total Training Recognition Loss 1101.92  Total Training Translation Loss 3487.09 
2024-01-31 19:28:19,641 EPOCH 485
2024-01-31 19:28:37,272 Epoch 485: Total Training Recognition Loss 1099.49  Total Training Translation Loss 3486.42 
2024-01-31 19:28:37,272 EPOCH 486
2024-01-31 19:28:42,868 [Epoch: 486 Step: 00016500] Batch Recognition Loss:  37.951061 => Gls Tokens per Sec:      572 || Batch Translation Loss: 112.735886 => Txt Tokens per Sec:     1601 || Lr: 0.000100
2024-01-31 19:28:54,901 Epoch 486: Total Training Recognition Loss 1099.18  Total Training Translation Loss 3486.68 
2024-01-31 19:28:54,901 EPOCH 487
2024-01-31 19:29:12,613 Epoch 487: Total Training Recognition Loss 1100.01  Total Training Translation Loss 3487.18 
2024-01-31 19:29:12,613 EPOCH 488
2024-01-31 19:29:30,246 Epoch 488: Total Training Recognition Loss 1101.30  Total Training Translation Loss 3487.33 
2024-01-31 19:29:30,246 EPOCH 489
2024-01-31 19:29:33,749 [Epoch: 489 Step: 00016600] Batch Recognition Loss:  39.424034 => Gls Tokens per Sec:      731 || Batch Translation Loss: 122.447548 => Txt Tokens per Sec:     1985 || Lr: 0.000100
2024-01-31 19:29:47,902 Epoch 489: Total Training Recognition Loss 1101.74  Total Training Translation Loss 3486.59 
2024-01-31 19:29:47,902 EPOCH 490
2024-01-31 19:30:05,497 Epoch 490: Total Training Recognition Loss 1099.70  Total Training Translation Loss 3487.57 
2024-01-31 19:30:05,498 EPOCH 491
2024-01-31 19:30:23,009 Epoch 491: Total Training Recognition Loss 1103.12  Total Training Translation Loss 3487.22 
2024-01-31 19:30:23,009 EPOCH 492
2024-01-31 19:30:27,278 [Epoch: 492 Step: 00016700] Batch Recognition Loss:  50.880833 => Gls Tokens per Sec:      450 || Batch Translation Loss: 130.507935 => Txt Tokens per Sec:     1219 || Lr: 0.000100
2024-01-31 19:30:40,591 Epoch 492: Total Training Recognition Loss 1100.35  Total Training Translation Loss 3487.75 
2024-01-31 19:30:40,592 EPOCH 493
2024-01-31 19:30:58,294 Epoch 493: Total Training Recognition Loss 1098.25  Total Training Translation Loss 3486.47 
2024-01-31 19:30:58,294 EPOCH 494
2024-01-31 19:31:15,842 Epoch 494: Total Training Recognition Loss 1099.30  Total Training Translation Loss 3487.79 
2024-01-31 19:31:15,842 EPOCH 495
2024-01-31 19:31:17,465 [Epoch: 495 Step: 00016800] Batch Recognition Loss:  27.746367 => Gls Tokens per Sec:      790 || Batch Translation Loss:  99.777481 => Txt Tokens per Sec:     2194 || Lr: 0.000100
2024-01-31 19:31:33,519 Epoch 495: Total Training Recognition Loss 1099.95  Total Training Translation Loss 3488.22 
2024-01-31 19:31:33,520 EPOCH 496
2024-01-31 19:31:50,988 Epoch 496: Total Training Recognition Loss 1100.98  Total Training Translation Loss 3486.58 
2024-01-31 19:31:50,988 EPOCH 497
2024-01-31 19:32:08,637 Epoch 497: Total Training Recognition Loss 1100.44  Total Training Translation Loss 3487.77 
2024-01-31 19:32:08,637 EPOCH 498
2024-01-31 19:32:10,035 [Epoch: 498 Step: 00016900] Batch Recognition Loss:  36.832615 => Gls Tokens per Sec:      458 || Batch Translation Loss: 127.613441 => Txt Tokens per Sec:     1607 || Lr: 0.000100
2024-01-31 19:32:26,226 Epoch 498: Total Training Recognition Loss 1101.78  Total Training Translation Loss 3486.89 
2024-01-31 19:32:26,226 EPOCH 499
2024-01-31 19:32:43,908 Epoch 499: Total Training Recognition Loss 1101.11  Total Training Translation Loss 3486.72 
2024-01-31 19:32:43,908 EPOCH 500
2024-01-31 19:33:01,626 [Epoch: 500 Step: 00017000] Batch Recognition Loss:  33.056038 => Gls Tokens per Sec:      600 || Batch Translation Loss: 111.286331 => Txt Tokens per Sec:     1666 || Lr: 0.000100
2024-01-31 19:33:01,626 Epoch 500: Total Training Recognition Loss 1102.93  Total Training Translation Loss 3487.79 
2024-01-31 19:33:01,626 EPOCH 501
2024-01-31 19:33:19,264 Epoch 501: Total Training Recognition Loss 1097.11  Total Training Translation Loss 3486.58 
2024-01-31 19:33:19,264 EPOCH 502
2024-01-31 19:33:36,835 Epoch 502: Total Training Recognition Loss 1100.72  Total Training Translation Loss 3486.60 
2024-01-31 19:33:36,835 EPOCH 503
2024-01-31 19:33:53,672 [Epoch: 503 Step: 00017100] Batch Recognition Loss:  37.750801 => Gls Tokens per Sec:      593 || Batch Translation Loss: 117.738388 => Txt Tokens per Sec:     1652 || Lr: 0.000100
2024-01-31 19:33:54,477 Epoch 503: Total Training Recognition Loss 1100.42  Total Training Translation Loss 3487.51 
2024-01-31 19:33:54,477 EPOCH 504
2024-01-31 19:34:12,161 Epoch 504: Total Training Recognition Loss 1098.59  Total Training Translation Loss 3487.14 
2024-01-31 19:34:12,161 EPOCH 505
2024-01-31 19:34:29,865 Epoch 505: Total Training Recognition Loss 1099.79  Total Training Translation Loss 3487.48 
2024-01-31 19:34:29,865 EPOCH 506
2024-01-31 19:34:45,169 [Epoch: 506 Step: 00017200] Batch Recognition Loss:  31.010031 => Gls Tokens per Sec:      611 || Batch Translation Loss:  93.762947 => Txt Tokens per Sec:     1674 || Lr: 0.000100
2024-01-31 19:34:47,328 Epoch 506: Total Training Recognition Loss 1101.72  Total Training Translation Loss 3487.97 
2024-01-31 19:34:47,329 EPOCH 507
2024-01-31 19:35:05,014 Epoch 507: Total Training Recognition Loss 1101.50  Total Training Translation Loss 3487.01 
2024-01-31 19:35:05,014 EPOCH 508
2024-01-31 19:35:22,697 Epoch 508: Total Training Recognition Loss 1100.66  Total Training Translation Loss 3487.88 
2024-01-31 19:35:22,698 EPOCH 509
2024-01-31 19:35:36,954 [Epoch: 509 Step: 00017300] Batch Recognition Loss:  15.462563 => Gls Tokens per Sec:      611 || Batch Translation Loss:  79.440903 => Txt Tokens per Sec:     1656 || Lr: 0.000100
2024-01-31 19:35:40,301 Epoch 509: Total Training Recognition Loss 1099.62  Total Training Translation Loss 3487.09 
2024-01-31 19:35:40,301 EPOCH 510
2024-01-31 19:35:57,816 Epoch 510: Total Training Recognition Loss 1100.52  Total Training Translation Loss 3487.18 
2024-01-31 19:35:57,817 EPOCH 511
2024-01-31 19:36:15,361 Epoch 511: Total Training Recognition Loss 1102.60  Total Training Translation Loss 3487.78 
2024-01-31 19:36:15,362 EPOCH 512
2024-01-31 19:36:28,590 [Epoch: 512 Step: 00017400] Batch Recognition Loss:  23.557068 => Gls Tokens per Sec:      610 || Batch Translation Loss:  92.026413 => Txt Tokens per Sec:     1683 || Lr: 0.000100
2024-01-31 19:36:32,842 Epoch 512: Total Training Recognition Loss 1103.63  Total Training Translation Loss 3486.58 
2024-01-31 19:36:32,842 EPOCH 513
2024-01-31 19:36:50,440 Epoch 513: Total Training Recognition Loss 1100.97  Total Training Translation Loss 3488.44 
2024-01-31 19:36:50,440 EPOCH 514
2024-01-31 19:37:07,937 Epoch 514: Total Training Recognition Loss 1101.44  Total Training Translation Loss 3487.40 
2024-01-31 19:37:07,937 EPOCH 515
2024-01-31 19:37:20,216 [Epoch: 515 Step: 00017500] Batch Recognition Loss:  23.056705 => Gls Tokens per Sec:      605 || Batch Translation Loss:  83.119354 => Txt Tokens per Sec:     1687 || Lr: 0.000100
2024-01-31 19:37:25,594 Epoch 515: Total Training Recognition Loss 1102.42  Total Training Translation Loss 3487.00 
2024-01-31 19:37:25,595 EPOCH 516
2024-01-31 19:37:43,167 Epoch 516: Total Training Recognition Loss 1099.13  Total Training Translation Loss 3487.13 
2024-01-31 19:37:43,167 EPOCH 517
2024-01-31 19:38:00,829 Epoch 517: Total Training Recognition Loss 1104.79  Total Training Translation Loss 3487.13 
2024-01-31 19:38:00,829 EPOCH 518
2024-01-31 19:38:10,951 [Epoch: 518 Step: 00017600] Batch Recognition Loss:  73.860191 => Gls Tokens per Sec:      671 || Batch Translation Loss: 126.635117 => Txt Tokens per Sec:     1791 || Lr: 0.000100
2024-01-31 19:38:18,376 Epoch 518: Total Training Recognition Loss 1100.19  Total Training Translation Loss 3487.87 
2024-01-31 19:38:18,376 EPOCH 519
2024-01-31 19:38:36,026 Epoch 519: Total Training Recognition Loss 1100.49  Total Training Translation Loss 3486.45 
2024-01-31 19:38:36,026 EPOCH 520
2024-01-31 19:38:53,594 Epoch 520: Total Training Recognition Loss 1101.20  Total Training Translation Loss 3487.13 
2024-01-31 19:38:53,595 EPOCH 521
2024-01-31 19:39:04,735 [Epoch: 521 Step: 00017700] Batch Recognition Loss:  38.625896 => Gls Tokens per Sec:      552 || Batch Translation Loss: 121.623344 => Txt Tokens per Sec:     1591 || Lr: 0.000100
2024-01-31 19:39:11,162 Epoch 521: Total Training Recognition Loss 1102.36  Total Training Translation Loss 3486.98 
2024-01-31 19:39:11,162 EPOCH 522
2024-01-31 19:39:28,774 Epoch 522: Total Training Recognition Loss 1100.84  Total Training Translation Loss 3486.70 
2024-01-31 19:39:28,774 EPOCH 523
2024-01-31 19:39:46,412 Epoch 523: Total Training Recognition Loss 1100.33  Total Training Translation Loss 3487.72 
2024-01-31 19:39:46,412 EPOCH 524
2024-01-31 19:39:54,171 [Epoch: 524 Step: 00017800] Batch Recognition Loss:  20.607134 => Gls Tokens per Sec:      742 || Batch Translation Loss:  85.464539 => Txt Tokens per Sec:     1937 || Lr: 0.000100
2024-01-31 19:40:04,018 Epoch 524: Total Training Recognition Loss 1097.97  Total Training Translation Loss 3486.85 
2024-01-31 19:40:04,018 EPOCH 525
2024-01-31 19:40:21,657 Epoch 525: Total Training Recognition Loss 1101.21  Total Training Translation Loss 3487.55 
2024-01-31 19:40:21,657 EPOCH 526
2024-01-31 19:40:39,313 Epoch 526: Total Training Recognition Loss 1098.91  Total Training Translation Loss 3487.96 
2024-01-31 19:40:39,313 EPOCH 527
2024-01-31 19:40:46,969 [Epoch: 527 Step: 00017900] Batch Recognition Loss:  37.495697 => Gls Tokens per Sec:      636 || Batch Translation Loss: 116.623405 => Txt Tokens per Sec:     1777 || Lr: 0.000100
2024-01-31 19:40:56,873 Epoch 527: Total Training Recognition Loss 1099.98  Total Training Translation Loss 3486.70 
2024-01-31 19:40:56,874 EPOCH 528
2024-01-31 19:41:14,418 Epoch 528: Total Training Recognition Loss 1100.90  Total Training Translation Loss 3486.82 
2024-01-31 19:41:14,419 EPOCH 529
2024-01-31 19:41:31,974 Epoch 529: Total Training Recognition Loss 1101.08  Total Training Translation Loss 3487.63 
2024-01-31 19:41:31,974 EPOCH 530
2024-01-31 19:41:39,623 [Epoch: 530 Step: 00018000] Batch Recognition Loss:  36.022984 => Gls Tokens per Sec:      586 || Batch Translation Loss: 109.382416 => Txt Tokens per Sec:     1710 || Lr: 0.000100
2024-01-31 19:42:03,548 Validation result at epoch 530, step    18000: duration: 23.9249s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 348.93173	Translation Loss: 73485.51562	PPL: 1561.96375
	Eval Metric: BLEU
	WER 528.74	(DEL: 4.31,	INS: 439.90,	SUB: 84.53)
	BLEU-4 0.00	(BLEU-1: 0.01,	BLEU-2: 0.00,	BLEU-3: 0.00,	BLEU-4: 0.00)
	CHRF 3.47	ROUGE 0.02
2024-01-31 19:42:03,550 Logging Recognition and Translation Outputs
2024-01-31 19:42:03,550 ========================================================================================================================
2024-01-31 19:42:03,550 Logging Sequence: 155_119.00
2024-01-31 19:42:03,550 	Gloss Reference :	A B+C+D+E                          
2024-01-31 19:42:03,550 	Gloss Hypothesis:	C A+E+C+E+C+B+E+C+E+C+E+B+C+B+C+E+C
2024-01-31 19:42:03,550 	Gloss Alignment :	S S                                
2024-01-31 19:42:03,551 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 19:42:03,554 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** a   report said that the taliban wanted icc to  replace the afghan flag with its own
2024-01-31 19:42:03,554 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s>    <s>  <s>  <s> <s>     <s>    <s> <s> <s>     <s> <s>    <s>  <s>  <s> <s>
2024-01-31 19:42:03,554 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   S   S      S    S    S   S       S      S   S   S       S   S      S    S    S   S  
2024-01-31 19:42:03,555 ========================================================================================================================
2024-01-31 19:42:03,555 Logging Sequence: 153_43.00
2024-01-31 19:42:03,555 	Gloss Reference :	A B+C+D+E
2024-01-31 19:42:03,555 	Gloss Hypothesis:	* <unk>  
2024-01-31 19:42:03,555 	Gloss Alignment :	D S      
2024-01-31 19:42:03,555 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 19:42:03,558 	Text Reference  :	*** *** ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* these   runs    were    all     because of      hardik  pandya  and     virat   kohli  
2024-01-31 19:42:03,558 	Text Hypothesis :	<s> <s> nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing
2024-01-31 19:42:03,558 	Text Alignment  :	I   I   I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       S       S       S       S       S       S       S       S       S       S       S      
2024-01-31 19:42:03,558 ========================================================================================================================
2024-01-31 19:42:03,558 Logging Sequence: 150_35.00
2024-01-31 19:42:03,558 	Gloss Reference :	***** * ***** A ***** B+C+D+E
2024-01-31 19:42:03,559 	Gloss Hypothesis:	<unk> A <pad> A <unk> E      
2024-01-31 19:42:03,559 	Gloss Alignment :	I     I I       I     S      
2024-01-31 19:42:03,559 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 19:42:03,561 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** wow india football team is  really strong
2024-01-31 19:42:03,561 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s>   <s>      <s>  <s> <s>    <s>   
2024-01-31 19:42:03,561 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   S   S     S        S    S   S      S     
2024-01-31 19:42:03,561 ========================================================================================================================
2024-01-31 19:42:03,561 Logging Sequence: 146_154.00
2024-01-31 19:42:03,561 	Gloss Reference :	***** *********** ***** ***** ***** * ***** ******* ***** ********************* A     B+C+D+E
2024-01-31 19:42:03,561 	Gloss Hypothesis:	<unk> C+E+B+C+E+B <unk> E+C+E <unk> E <unk> E+C+E+C <unk> E+C+E+B+E+C+E+C+E+C+E <unk> E+C    
2024-01-31 19:42:03,562 	Gloss Alignment :	I     I           I     I     I     I I     I       I     I                     S     S      
2024-01-31 19:42:03,562 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 19:42:03,565 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ****** ****** ****** ****** ****** ****** bwf    said   that   testing protocols have   been   implemented to     ensure the    health and    safety of     all    participants
2024-01-31 19:42:03,565 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> boards boards boards boards boards boards boards boards boards boards  boards    boards boards boards      boards boards boards boards boards boards boards boards boards      
2024-01-31 19:42:03,566 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I      I      I      I      I      I      S      S      S      S       S         S      S      S           S      S      S      S      S      S      S      S      S           
2024-01-31 19:42:03,566 ========================================================================================================================
2024-01-31 19:42:03,566 Logging Sequence: 76_79.00
2024-01-31 19:42:03,566 	Gloss Reference :	***** ********* ***** * A     B+C+D+E    
2024-01-31 19:42:03,566 	Gloss Hypothesis:	<unk> E+A+E+A+E <unk> C <unk> E+B+C+E+B+E
2024-01-31 19:42:03,566 	Gloss Alignment :	I     I         I     I S     S          
2024-01-31 19:42:03,566 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 19:42:03,568 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* speaking to      ani     csk     ceo     kasi    viswanathan said   
2024-01-31 19:42:03,568 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing  nothing nothing nothing nothing nothing nothing     nothing
2024-01-31 19:42:03,568 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       S        S       S       S       S       S       S           S      
2024-01-31 19:42:03,569 ========================================================================================================================
2024-01-31 19:42:13,481 Epoch 530: Total Training Recognition Loss 1098.81  Total Training Translation Loss 3488.14 
2024-01-31 19:42:13,482 EPOCH 531
2024-01-31 19:42:31,089 Epoch 531: Total Training Recognition Loss 1100.90  Total Training Translation Loss 3487.25 
2024-01-31 19:42:31,090 EPOCH 532
2024-01-31 19:42:48,754 Epoch 532: Total Training Recognition Loss 1099.37  Total Training Translation Loss 3487.16 
2024-01-31 19:42:48,754 EPOCH 533
2024-01-31 19:42:53,710 [Epoch: 533 Step: 00018100] Batch Recognition Loss:  42.652248 => Gls Tokens per Sec:      775 || Batch Translation Loss: 129.897156 => Txt Tokens per Sec:     2069 || Lr: 0.000100
2024-01-31 19:43:06,396 Epoch 533: Total Training Recognition Loss 1101.69  Total Training Translation Loss 3486.56 
2024-01-31 19:43:06,397 EPOCH 534
2024-01-31 19:43:23,972 Epoch 534: Total Training Recognition Loss 1099.56  Total Training Translation Loss 3486.72 
2024-01-31 19:43:23,973 EPOCH 535
2024-01-31 19:43:41,527 Epoch 535: Total Training Recognition Loss 1101.67  Total Training Translation Loss 3487.15 
2024-01-31 19:43:41,527 EPOCH 536
2024-01-31 19:43:45,582 [Epoch: 536 Step: 00018200] Batch Recognition Loss:  36.784424 => Gls Tokens per Sec:      789 || Batch Translation Loss: 102.781204 => Txt Tokens per Sec:     2033 || Lr: 0.000100
2024-01-31 19:43:59,053 Epoch 536: Total Training Recognition Loss 1102.39  Total Training Translation Loss 3487.15 
2024-01-31 19:43:59,054 EPOCH 537
2024-01-31 19:44:16,590 Epoch 537: Total Training Recognition Loss 1102.21  Total Training Translation Loss 3487.06 
2024-01-31 19:44:16,590 EPOCH 538
2024-01-31 19:44:34,211 Epoch 538: Total Training Recognition Loss 1100.55  Total Training Translation Loss 3487.68 
2024-01-31 19:44:34,211 EPOCH 539
2024-01-31 19:44:36,940 [Epoch: 539 Step: 00018300] Batch Recognition Loss:  20.466896 => Gls Tokens per Sec:      938 || Batch Translation Loss:  91.899254 => Txt Tokens per Sec:     2157 || Lr: 0.000100
2024-01-31 19:44:51,757 Epoch 539: Total Training Recognition Loss 1099.07  Total Training Translation Loss 3486.49 
2024-01-31 19:44:51,757 EPOCH 540
2024-01-31 19:45:09,302 Epoch 540: Total Training Recognition Loss 1100.96  Total Training Translation Loss 3486.88 
2024-01-31 19:45:09,302 EPOCH 541
2024-01-31 19:45:26,985 Epoch 541: Total Training Recognition Loss 1101.56  Total Training Translation Loss 3486.85 
2024-01-31 19:45:26,985 EPOCH 542
2024-01-31 19:45:29,826 [Epoch: 542 Step: 00018400] Batch Recognition Loss:  27.267576 => Gls Tokens per Sec:      676 || Batch Translation Loss:  93.780411 => Txt Tokens per Sec:     1841 || Lr: 0.000100
2024-01-31 19:45:44,705 Epoch 542: Total Training Recognition Loss 1101.77  Total Training Translation Loss 3487.05 
2024-01-31 19:45:44,705 EPOCH 543
2024-01-31 19:46:02,316 Epoch 543: Total Training Recognition Loss 1098.39  Total Training Translation Loss 3487.63 
2024-01-31 19:46:02,317 EPOCH 544
2024-01-31 19:46:20,032 Epoch 544: Total Training Recognition Loss 1100.61  Total Training Translation Loss 3486.97 
2024-01-31 19:46:20,032 EPOCH 545
2024-01-31 19:46:22,040 [Epoch: 545 Step: 00018500] Batch Recognition Loss:  27.629166 => Gls Tokens per Sec:      638 || Batch Translation Loss: 103.922913 => Txt Tokens per Sec:     1858 || Lr: 0.000100
2024-01-31 19:46:37,826 Epoch 545: Total Training Recognition Loss 1099.43  Total Training Translation Loss 3487.48 
2024-01-31 19:46:37,827 EPOCH 546
2024-01-31 19:46:55,455 Epoch 546: Total Training Recognition Loss 1100.62  Total Training Translation Loss 3486.92 
2024-01-31 19:46:55,455 EPOCH 547
2024-01-31 19:47:13,015 Epoch 547: Total Training Recognition Loss 1098.41  Total Training Translation Loss 3487.46 
2024-01-31 19:47:13,015 EPOCH 548
2024-01-31 19:47:13,560 [Epoch: 548 Step: 00018600] Batch Recognition Loss:  31.020699 => Gls Tokens per Sec:     1176 || Batch Translation Loss: 101.638130 => Txt Tokens per Sec:     2441 || Lr: 0.000100
2024-01-31 19:47:30,641 Epoch 548: Total Training Recognition Loss 1100.85  Total Training Translation Loss 3487.94 
2024-01-31 19:47:30,641 EPOCH 549
2024-01-31 19:47:48,357 Epoch 549: Total Training Recognition Loss 1101.48  Total Training Translation Loss 3487.50 
2024-01-31 19:47:48,357 EPOCH 550
2024-01-31 19:48:05,952 [Epoch: 550 Step: 00018700] Batch Recognition Loss:  30.453613 => Gls Tokens per Sec:      604 || Batch Translation Loss:  98.432190 => Txt Tokens per Sec:     1677 || Lr: 0.000100
2024-01-31 19:48:05,952 Epoch 550: Total Training Recognition Loss 1103.12  Total Training Translation Loss 3486.84 
2024-01-31 19:48:05,952 EPOCH 551
2024-01-31 19:48:23,518 Epoch 551: Total Training Recognition Loss 1101.53  Total Training Translation Loss 3486.83 
2024-01-31 19:48:23,519 EPOCH 552
2024-01-31 19:48:41,106 Epoch 552: Total Training Recognition Loss 1100.48  Total Training Translation Loss 3487.53 
2024-01-31 19:48:41,106 EPOCH 553
2024-01-31 19:48:57,690 [Epoch: 553 Step: 00018800] Batch Recognition Loss:  40.529858 => Gls Tokens per Sec:      602 || Batch Translation Loss: 113.652512 => Txt Tokens per Sec:     1683 || Lr: 0.000100
2024-01-31 19:48:58,685 Epoch 553: Total Training Recognition Loss 1102.72  Total Training Translation Loss 3487.65 
2024-01-31 19:48:58,685 EPOCH 554
2024-01-31 19:49:16,347 Epoch 554: Total Training Recognition Loss 1101.15  Total Training Translation Loss 3487.36 
2024-01-31 19:49:16,347 EPOCH 555
2024-01-31 19:49:34,099 Epoch 555: Total Training Recognition Loss 1101.49  Total Training Translation Loss 3487.60 
2024-01-31 19:49:34,100 EPOCH 556
2024-01-31 19:49:50,027 [Epoch: 556 Step: 00018900] Batch Recognition Loss:  38.297306 => Gls Tokens per Sec:      603 || Batch Translation Loss: 121.774033 => Txt Tokens per Sec:     1695 || Lr: 0.000100
2024-01-31 19:49:51,752 Epoch 556: Total Training Recognition Loss 1100.53  Total Training Translation Loss 3487.06 
2024-01-31 19:49:51,752 EPOCH 557
2024-01-31 19:50:09,344 Epoch 557: Total Training Recognition Loss 1101.68  Total Training Translation Loss 3486.93 
2024-01-31 19:50:09,344 EPOCH 558
2024-01-31 19:50:26,851 Epoch 558: Total Training Recognition Loss 1099.53  Total Training Translation Loss 3487.23 
2024-01-31 19:50:26,851 EPOCH 559
2024-01-31 19:50:41,700 [Epoch: 559 Step: 00019000] Batch Recognition Loss:  17.141514 => Gls Tokens per Sec:      587 || Batch Translation Loss:  78.910332 => Txt Tokens per Sec:     1633 || Lr: 0.000100
2024-01-31 19:50:44,448 Epoch 559: Total Training Recognition Loss 1098.88  Total Training Translation Loss 3487.07 
2024-01-31 19:50:44,449 EPOCH 560
2024-01-31 19:51:02,089 Epoch 560: Total Training Recognition Loss 1099.24  Total Training Translation Loss 3487.65 
2024-01-31 19:51:02,089 EPOCH 561
2024-01-31 19:51:20,509 Epoch 561: Total Training Recognition Loss 1101.41  Total Training Translation Loss 3487.95 
2024-01-31 19:51:20,509 EPOCH 562
2024-01-31 19:51:36,785 [Epoch: 562 Step: 00019100] Batch Recognition Loss:  38.729454 => Gls Tokens per Sec:      496 || Batch Translation Loss: 121.458488 => Txt Tokens per Sec:     1385 || Lr: 0.000100
2024-01-31 19:51:40,426 Epoch 562: Total Training Recognition Loss 1100.15  Total Training Translation Loss 3486.50 
2024-01-31 19:51:40,426 EPOCH 563
2024-01-31 19:51:57,990 Epoch 563: Total Training Recognition Loss 1099.76  Total Training Translation Loss 3487.71 
2024-01-31 19:51:57,990 EPOCH 564
2024-01-31 19:52:15,588 Epoch 564: Total Training Recognition Loss 1101.29  Total Training Translation Loss 3487.16 
2024-01-31 19:52:15,588 EPOCH 565
2024-01-31 19:52:27,680 [Epoch: 565 Step: 00019200] Batch Recognition Loss:  29.439434 => Gls Tokens per Sec:      615 || Batch Translation Loss: 102.884979 => Txt Tokens per Sec:     1696 || Lr: 0.000100
2024-01-31 19:52:33,269 Epoch 565: Total Training Recognition Loss 1099.37  Total Training Translation Loss 3486.46 
2024-01-31 19:52:33,270 EPOCH 566
2024-01-31 19:52:50,954 Epoch 566: Total Training Recognition Loss 1101.14  Total Training Translation Loss 3487.11 
2024-01-31 19:52:50,954 EPOCH 567
2024-01-31 19:53:08,497 Epoch 567: Total Training Recognition Loss 1099.57  Total Training Translation Loss 3487.81 
2024-01-31 19:53:08,497 EPOCH 568
2024-01-31 19:53:18,462 [Epoch: 568 Step: 00019300] Batch Recognition Loss:  29.795431 => Gls Tokens per Sec:      707 || Batch Translation Loss:  99.657959 => Txt Tokens per Sec:     1912 || Lr: 0.000100
2024-01-31 19:53:26,044 Epoch 568: Total Training Recognition Loss 1097.29  Total Training Translation Loss 3486.95 
2024-01-31 19:53:26,044 EPOCH 569
2024-01-31 19:53:43,680 Epoch 569: Total Training Recognition Loss 1101.53  Total Training Translation Loss 3487.44 
2024-01-31 19:53:43,681 EPOCH 570
2024-01-31 19:54:01,362 Epoch 570: Total Training Recognition Loss 1097.87  Total Training Translation Loss 3486.85 
2024-01-31 19:54:01,362 EPOCH 571
2024-01-31 19:54:11,335 [Epoch: 571 Step: 00019400] Batch Recognition Loss:  30.538082 => Gls Tokens per Sec:      617 || Batch Translation Loss:  99.180283 => Txt Tokens per Sec:     1667 || Lr: 0.000100
2024-01-31 19:54:18,905 Epoch 571: Total Training Recognition Loss 1100.46  Total Training Translation Loss 3487.41 
2024-01-31 19:54:18,905 EPOCH 572
2024-01-31 19:54:36,574 Epoch 572: Total Training Recognition Loss 1098.03  Total Training Translation Loss 3486.44 
2024-01-31 19:54:36,575 EPOCH 573
2024-01-31 19:54:54,148 Epoch 573: Total Training Recognition Loss 1101.35  Total Training Translation Loss 3487.93 
2024-01-31 19:54:54,148 EPOCH 574
2024-01-31 19:55:03,732 [Epoch: 574 Step: 00019500] Batch Recognition Loss:  27.990417 => Gls Tokens per Sec:      575 || Batch Translation Loss: 100.055893 => Txt Tokens per Sec:     1566 || Lr: 0.000100
2024-01-31 19:55:11,933 Epoch 574: Total Training Recognition Loss 1100.86  Total Training Translation Loss 3487.70 
2024-01-31 19:55:11,934 EPOCH 575
2024-01-31 19:55:29,478 Epoch 575: Total Training Recognition Loss 1100.22  Total Training Translation Loss 3487.44 
2024-01-31 19:55:29,478 EPOCH 576
2024-01-31 19:55:47,195 Epoch 576: Total Training Recognition Loss 1101.52  Total Training Translation Loss 3488.27 
2024-01-31 19:55:47,196 EPOCH 577
2024-01-31 19:55:54,474 [Epoch: 577 Step: 00019600] Batch Recognition Loss:  37.165241 => Gls Tokens per Sec:      669 || Batch Translation Loss: 118.685318 => Txt Tokens per Sec:     1790 || Lr: 0.000100
2024-01-31 19:56:04,907 Epoch 577: Total Training Recognition Loss 1100.43  Total Training Translation Loss 3487.32 
2024-01-31 19:56:04,908 EPOCH 578
2024-01-31 19:56:22,544 Epoch 578: Total Training Recognition Loss 1099.46  Total Training Translation Loss 3486.71 
2024-01-31 19:56:22,545 EPOCH 579
2024-01-31 19:56:40,196 Epoch 579: Total Training Recognition Loss 1101.94  Total Training Translation Loss 3486.97 
2024-01-31 19:56:40,197 EPOCH 580
2024-01-31 19:56:48,651 [Epoch: 580 Step: 00019700] Batch Recognition Loss:  40.032505 => Gls Tokens per Sec:      530 || Batch Translation Loss: 114.908783 => Txt Tokens per Sec:     1624 || Lr: 0.000100
2024-01-31 19:56:57,770 Epoch 580: Total Training Recognition Loss 1099.60  Total Training Translation Loss 3486.91 
2024-01-31 19:56:57,770 EPOCH 581
2024-01-31 19:57:15,424 Epoch 581: Total Training Recognition Loss 1101.87  Total Training Translation Loss 3487.02 
2024-01-31 19:57:15,424 EPOCH 582
2024-01-31 19:57:33,037 Epoch 582: Total Training Recognition Loss 1100.44  Total Training Translation Loss 3487.72 
2024-01-31 19:57:33,037 EPOCH 583
2024-01-31 19:57:39,921 [Epoch: 583 Step: 00019800] Batch Recognition Loss:  57.774429 => Gls Tokens per Sec:      558 || Batch Translation Loss: 132.247269 => Txt Tokens per Sec:     1612 || Lr: 0.000100
2024-01-31 19:57:50,680 Epoch 583: Total Training Recognition Loss 1099.88  Total Training Translation Loss 3485.69 
2024-01-31 19:57:50,680 EPOCH 584
2024-01-31 19:58:08,488 Epoch 584: Total Training Recognition Loss 1098.64  Total Training Translation Loss 3486.17 
2024-01-31 19:58:08,488 EPOCH 585
2024-01-31 19:58:26,019 Epoch 585: Total Training Recognition Loss 1099.65  Total Training Translation Loss 3487.18 
2024-01-31 19:58:26,019 EPOCH 586
2024-01-31 19:58:31,228 [Epoch: 586 Step: 00019900] Batch Recognition Loss:  36.815262 => Gls Tokens per Sec:      614 || Batch Translation Loss: 109.125763 => Txt Tokens per Sec:     1759 || Lr: 0.000100
2024-01-31 19:58:43,644 Epoch 586: Total Training Recognition Loss 1102.82  Total Training Translation Loss 3486.09 
2024-01-31 19:58:43,645 EPOCH 587
2024-01-31 19:59:01,419 Epoch 587: Total Training Recognition Loss 1099.32  Total Training Translation Loss 3487.99 
2024-01-31 19:59:01,420 EPOCH 588
2024-01-31 19:59:19,146 Epoch 588: Total Training Recognition Loss 1099.03  Total Training Translation Loss 3487.97 
2024-01-31 19:59:19,146 EPOCH 589
2024-01-31 19:59:22,423 [Epoch: 589 Step: 00020000] Batch Recognition Loss:  27.062126 => Gls Tokens per Sec:      781 || Batch Translation Loss:  87.252396 => Txt Tokens per Sec:     2085 || Lr: 0.000100
2024-01-31 19:59:46,800 Validation result at epoch 589, step    20000: duration: 24.3756s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 351.15369	Translation Loss: 73465.82812	PPL: 1558.88879
	Eval Metric: BLEU
	WER 535.38	(DEL: 4.10,	INS: 446.26,	SUB: 85.03)
	BLEU-4 0.00	(BLEU-1: 0.01,	BLEU-2: 0.00,	BLEU-3: 0.00,	BLEU-4: 0.00)
	CHRF 3.46	ROUGE 0.02
2024-01-31 19:59:46,802 Logging Recognition and Translation Outputs
2024-01-31 19:59:46,802 ========================================================================================================================
2024-01-31 19:59:46,802 Logging Sequence: 174_121.00
2024-01-31 19:59:46,802 	Gloss Reference :	***** *** ***** * ***** * ***** * ***** *** ***** * ***** * ***** A ***** * ***** * ***** * ***** B+C+D+E  
2024-01-31 19:59:46,803 	Gloss Hypothesis:	<unk> E+A <unk> A <unk> E <unk> E <unk> C+E <unk> E <unk> C <unk> A <unk> E <unk> E <unk> C <unk> E+A+E+C+E
2024-01-31 19:59:46,803 	Gloss Alignment :	I     I   I     I I     I I     I I     I   I     I I     I I       I     I I     I I     I I     S        
2024-01-31 19:59:46,803 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 19:59:46,806 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** *** *** ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* there         was           a             strong        competition   and           a             difficult     auction       for           the           5             franchise     owners       
2024-01-31 19:59:46,806 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions
2024-01-31 19:59:46,806 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I   I   I             I             I             I             I             I             I             I             I             I             I             I             I             S             S             S             S             S             S             S             S             S             S             S             S             S             S            
2024-01-31 19:59:46,806 ========================================================================================================================
2024-01-31 19:59:46,807 Logging Sequence: 170_24.00
2024-01-31 19:59:46,807 	Gloss Reference :	A B+C+D+E
2024-01-31 19:59:46,807 	Gloss Hypothesis:	* *******
2024-01-31 19:59:46,807 	Gloss Alignment :	D D      
2024-01-31 19:59:46,807 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 19:59:46,809 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** *** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** let      me       tell     you      about    it      
2024-01-31 19:59:46,809 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas
2024-01-31 19:59:46,809 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I   I        I        I        I        I        I        I        I        I        I        I        I        I        I        I        I        I        I        I        I        I        I        S        S        S        S        S        S       
2024-01-31 19:59:46,809 ========================================================================================================================
2024-01-31 19:59:46,809 Logging Sequence: 73_79.00
2024-01-31 19:59:46,809 	Gloss Reference :	***** ***** ***** A ***** * ***** * ***** ********* ***** ***** ***** * ***** ******* B+C+D+E
2024-01-31 19:59:46,810 	Gloss Hypothesis:	<unk> B+C+A <unk> A <unk> B <unk> B <unk> C+B+C+B+C <unk> C+B+A <unk> C <unk> B+C+B+C <unk>  
2024-01-31 19:59:46,810 	Gloss Alignment :	I     I     I       I     I I     I I     I         I     I     I     I I     I       S      
2024-01-31 19:59:46,810 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 19:59:46,814 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** raina resturant has food from the rich spices of  north india to  the aromatic curries of  south india
2024-01-31 19:59:46,814 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s>   <s>       <s> <s>  <s>  <s> <s>  <s>    <s> <s>   <s>   <s> <s> <s>      <s>     <s> <s>   <s>  
2024-01-31 19:59:46,814 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   S     S         S   S    S    S   S    S      S   S     S     S   S   S        S       S   S     S    
2024-01-31 19:59:46,814 ========================================================================================================================
2024-01-31 19:59:46,814 Logging Sequence: 140_2.00
2024-01-31 19:59:46,814 	Gloss Reference :	A ************************* ***** B+C+D+E
2024-01-31 19:59:46,815 	Gloss Hypothesis:	A E+A+E+A+E+B+E+B+E+C+E+A+E <unk> E      
2024-01-31 19:59:46,815 	Gloss Alignment :	  I                         I     S      
2024-01-31 19:59:46,815 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 19:59:46,817 	Text Reference  :	*** *** *** ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* indian        batsman-wicket keeper        rishabh       pant          has           outstanding   skills        in            cricket      
2024-01-31 19:59:46,817 	Text Hypothesis :	<s> <s> <s> misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand  misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand
2024-01-31 19:59:46,817 	Text Alignment  :	I   I   I   I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             S             S              S             S             S             S             S             S             S             S            
2024-01-31 19:59:46,818 ========================================================================================================================
2024-01-31 19:59:46,818 Logging Sequence: 81_470.00
2024-01-31 19:59:46,818 	Gloss Reference :	***** A B+C+D+E
2024-01-31 19:59:46,818 	Gloss Hypothesis:	<unk> C <unk>  
2024-01-31 19:59:46,818 	Gloss Alignment :	I     S S      
2024-01-31 19:59:46,818 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 19:59:46,821 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** or    you   don't know  if    you   do    let   us    know  in    the   comments
2024-01-31 19:59:46,821 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha   
2024-01-31 19:59:46,821 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     S     S     S     S     S     S     S     S     S     S     S     S     S       
2024-01-31 19:59:46,821 ========================================================================================================================
2024-01-31 20:00:01,158 Epoch 589: Total Training Recognition Loss 1101.20  Total Training Translation Loss 3486.69 
2024-01-31 20:00:01,158 EPOCH 590
2024-01-31 20:00:18,658 Epoch 590: Total Training Recognition Loss 1099.83  Total Training Translation Loss 3488.02 
2024-01-31 20:00:18,658 EPOCH 591
2024-01-31 20:00:36,175 Epoch 591: Total Training Recognition Loss 1099.91  Total Training Translation Loss 3486.02 
2024-01-31 20:00:36,176 EPOCH 592
2024-01-31 20:00:39,265 [Epoch: 592 Step: 00020100] Batch Recognition Loss:  30.046688 => Gls Tokens per Sec:      622 || Batch Translation Loss: 103.675354 => Txt Tokens per Sec:     1789 || Lr: 0.000100
2024-01-31 20:00:53,878 Epoch 592: Total Training Recognition Loss 1102.16  Total Training Translation Loss 3487.09 
2024-01-31 20:00:53,878 EPOCH 593
2024-01-31 20:01:11,665 Epoch 593: Total Training Recognition Loss 1099.31  Total Training Translation Loss 3487.84 
2024-01-31 20:01:11,665 EPOCH 594
2024-01-31 20:01:29,349 Epoch 594: Total Training Recognition Loss 1101.06  Total Training Translation Loss 3486.91 
2024-01-31 20:01:29,349 EPOCH 595
2024-01-31 20:01:32,224 [Epoch: 595 Step: 00020200] Batch Recognition Loss:  35.173744 => Gls Tokens per Sec:      445 || Batch Translation Loss: 109.418335 => Txt Tokens per Sec:     1438 || Lr: 0.000100
2024-01-31 20:01:46,902 Epoch 595: Total Training Recognition Loss 1102.17  Total Training Translation Loss 3487.20 
2024-01-31 20:01:46,902 EPOCH 596
2024-01-31 20:02:04,396 Epoch 596: Total Training Recognition Loss 1100.99  Total Training Translation Loss 3487.58 
2024-01-31 20:02:04,396 EPOCH 597
2024-01-31 20:02:21,952 Epoch 597: Total Training Recognition Loss 1101.64  Total Training Translation Loss 3487.61 
2024-01-31 20:02:21,952 EPOCH 598
2024-01-31 20:02:22,986 [Epoch: 598 Step: 00020300] Batch Recognition Loss:  23.661789 => Gls Tokens per Sec:      620 || Batch Translation Loss: 100.822159 => Txt Tokens per Sec:     1825 || Lr: 0.000100
2024-01-31 20:02:39,587 Epoch 598: Total Training Recognition Loss 1098.83  Total Training Translation Loss 3487.38 
2024-01-31 20:02:39,587 EPOCH 599
2024-01-31 20:02:57,280 Epoch 599: Total Training Recognition Loss 1103.31  Total Training Translation Loss 3486.76 
2024-01-31 20:02:57,280 EPOCH 600
2024-01-31 20:03:14,894 [Epoch: 600 Step: 00020400] Batch Recognition Loss:  17.275005 => Gls Tokens per Sec:      604 || Batch Translation Loss:  74.851234 => Txt Tokens per Sec:     1675 || Lr: 0.000100
2024-01-31 20:03:14,894 Epoch 600: Total Training Recognition Loss 1100.18  Total Training Translation Loss 3487.44 
2024-01-31 20:03:14,894 EPOCH 601
2024-01-31 20:03:32,437 Epoch 601: Total Training Recognition Loss 1100.28  Total Training Translation Loss 3487.50 
2024-01-31 20:03:32,437 EPOCH 602
2024-01-31 20:03:49,963 Epoch 602: Total Training Recognition Loss 1103.41  Total Training Translation Loss 3488.40 
2024-01-31 20:03:49,963 EPOCH 603
2024-01-31 20:04:06,467 [Epoch: 603 Step: 00020500] Batch Recognition Loss:  58.093369 => Gls Tokens per Sec:      620 || Batch Translation Loss: 133.045731 => Txt Tokens per Sec:     1721 || Lr: 0.000100
2024-01-31 20:04:07,544 Epoch 603: Total Training Recognition Loss 1101.51  Total Training Translation Loss 3486.36 
2024-01-31 20:04:07,544 EPOCH 604
2024-01-31 20:04:25,159 Epoch 604: Total Training Recognition Loss 1102.08  Total Training Translation Loss 3487.14 
2024-01-31 20:04:25,159 EPOCH 605
2024-01-31 20:04:42,912 Epoch 605: Total Training Recognition Loss 1098.65  Total Training Translation Loss 3486.10 
2024-01-31 20:04:42,912 EPOCH 606
2024-01-31 20:04:59,078 [Epoch: 606 Step: 00020600] Batch Recognition Loss:  32.493111 => Gls Tokens per Sec:      578 || Batch Translation Loss: 107.779045 => Txt Tokens per Sec:     1621 || Lr: 0.000100
2024-01-31 20:05:00,549 Epoch 606: Total Training Recognition Loss 1101.22  Total Training Translation Loss 3487.60 
2024-01-31 20:05:00,549 EPOCH 607
2024-01-31 20:05:18,116 Epoch 607: Total Training Recognition Loss 1102.71  Total Training Translation Loss 3487.89 
2024-01-31 20:05:18,117 EPOCH 608
2024-01-31 20:05:35,633 Epoch 608: Total Training Recognition Loss 1098.40  Total Training Translation Loss 3488.02 
2024-01-31 20:05:35,633 EPOCH 609
2024-01-31 20:05:49,725 [Epoch: 609 Step: 00020700] Batch Recognition Loss:  45.570702 => Gls Tokens per Sec:      618 || Batch Translation Loss: 126.423447 => Txt Tokens per Sec:     1682 || Lr: 0.000100
2024-01-31 20:05:53,282 Epoch 609: Total Training Recognition Loss 1099.11  Total Training Translation Loss 3487.05 
2024-01-31 20:05:53,282 EPOCH 610
2024-01-31 20:06:10,913 Epoch 610: Total Training Recognition Loss 1101.33  Total Training Translation Loss 3487.88 
2024-01-31 20:06:10,913 EPOCH 611
2024-01-31 20:06:28,598 Epoch 611: Total Training Recognition Loss 1098.94  Total Training Translation Loss 3488.00 
2024-01-31 20:06:28,598 EPOCH 612
2024-01-31 20:06:43,130 [Epoch: 612 Step: 00020800] Batch Recognition Loss:  36.883106 => Gls Tokens per Sec:      573 || Batch Translation Loss: 123.425018 => Txt Tokens per Sec:     1631 || Lr: 0.000100
2024-01-31 20:06:46,167 Epoch 612: Total Training Recognition Loss 1100.20  Total Training Translation Loss 3486.85 
2024-01-31 20:06:46,167 EPOCH 613
2024-01-31 20:07:03,708 Epoch 613: Total Training Recognition Loss 1100.92  Total Training Translation Loss 3487.55 
2024-01-31 20:07:03,708 EPOCH 614
2024-01-31 20:07:21,231 Epoch 614: Total Training Recognition Loss 1101.09  Total Training Translation Loss 3487.55 
2024-01-31 20:07:21,231 EPOCH 615
2024-01-31 20:07:34,767 [Epoch: 615 Step: 00020900] Batch Recognition Loss:  23.764572 => Gls Tokens per Sec:      549 || Batch Translation Loss:  96.141037 => Txt Tokens per Sec:     1573 || Lr: 0.000100
2024-01-31 20:07:38,864 Epoch 615: Total Training Recognition Loss 1101.08  Total Training Translation Loss 3487.26 
2024-01-31 20:07:38,864 EPOCH 616
2024-01-31 20:07:56,402 Epoch 616: Total Training Recognition Loss 1101.99  Total Training Translation Loss 3486.91 
2024-01-31 20:07:56,402 EPOCH 617
2024-01-31 20:08:14,005 Epoch 617: Total Training Recognition Loss 1100.76  Total Training Translation Loss 3486.57 
2024-01-31 20:08:14,006 EPOCH 618
2024-01-31 20:08:24,273 [Epoch: 618 Step: 00021000] Batch Recognition Loss:  17.143009 => Gls Tokens per Sec:      686 || Batch Translation Loss:  75.932053 => Txt Tokens per Sec:     1867 || Lr: 0.000100
2024-01-31 20:08:31,734 Epoch 618: Total Training Recognition Loss 1100.21  Total Training Translation Loss 3487.00 
2024-01-31 20:08:31,735 EPOCH 619
2024-01-31 20:08:49,247 Epoch 619: Total Training Recognition Loss 1101.65  Total Training Translation Loss 3487.28 
2024-01-31 20:08:49,248 EPOCH 620
2024-01-31 20:09:06,834 Epoch 620: Total Training Recognition Loss 1101.20  Total Training Translation Loss 3486.67 
2024-01-31 20:09:06,834 EPOCH 621
2024-01-31 20:09:17,007 [Epoch: 621 Step: 00021100] Batch Recognition Loss:  19.175291 => Gls Tokens per Sec:      605 || Batch Translation Loss:  91.057190 => Txt Tokens per Sec:     1673 || Lr: 0.000100
2024-01-31 20:09:24,437 Epoch 621: Total Training Recognition Loss 1099.11  Total Training Translation Loss 3486.91 
2024-01-31 20:09:24,438 EPOCH 622
2024-01-31 20:09:42,132 Epoch 622: Total Training Recognition Loss 1100.67  Total Training Translation Loss 3486.41 
2024-01-31 20:09:42,132 EPOCH 623
2024-01-31 20:09:59,768 Epoch 623: Total Training Recognition Loss 1099.31  Total Training Translation Loss 3486.46 
2024-01-31 20:09:59,769 EPOCH 624
2024-01-31 20:10:08,191 [Epoch: 624 Step: 00021200] Batch Recognition Loss:  44.671516 => Gls Tokens per Sec:      684 || Batch Translation Loss: 128.371246 => Txt Tokens per Sec:     1830 || Lr: 0.000100
2024-01-31 20:10:17,334 Epoch 624: Total Training Recognition Loss 1099.97  Total Training Translation Loss 3488.11 
2024-01-31 20:10:17,334 EPOCH 625
2024-01-31 20:10:34,894 Epoch 625: Total Training Recognition Loss 1101.88  Total Training Translation Loss 3487.28 
2024-01-31 20:10:34,894 EPOCH 626
2024-01-31 20:10:52,449 Epoch 626: Total Training Recognition Loss 1099.60  Total Training Translation Loss 3488.44 
2024-01-31 20:10:52,449 EPOCH 627
2024-01-31 20:10:59,281 [Epoch: 627 Step: 00021300] Batch Recognition Loss:  30.644266 => Gls Tokens per Sec:      750 || Batch Translation Loss: 100.078568 => Txt Tokens per Sec:     2002 || Lr: 0.000100
2024-01-31 20:11:09,977 Epoch 627: Total Training Recognition Loss 1099.63  Total Training Translation Loss 3487.36 
2024-01-31 20:11:09,977 EPOCH 628
2024-01-31 20:11:27,524 Epoch 628: Total Training Recognition Loss 1097.14  Total Training Translation Loss 3486.95 
2024-01-31 20:11:27,525 EPOCH 629
2024-01-31 20:11:45,072 Epoch 629: Total Training Recognition Loss 1100.24  Total Training Translation Loss 3487.01 
2024-01-31 20:11:45,072 EPOCH 630
2024-01-31 20:11:50,438 [Epoch: 630 Step: 00021400] Batch Recognition Loss:  30.645096 => Gls Tokens per Sec:      835 || Batch Translation Loss: 110.450546 => Txt Tokens per Sec:     2147 || Lr: 0.000100
2024-01-31 20:12:02,742 Epoch 630: Total Training Recognition Loss 1099.44  Total Training Translation Loss 3486.76 
2024-01-31 20:12:02,742 EPOCH 631
2024-01-31 20:12:20,298 Epoch 631: Total Training Recognition Loss 1099.05  Total Training Translation Loss 3487.09 
2024-01-31 20:12:20,298 EPOCH 632
2024-01-31 20:12:37,863 Epoch 632: Total Training Recognition Loss 1101.45  Total Training Translation Loss 3487.44 
2024-01-31 20:12:37,863 EPOCH 633
2024-01-31 20:12:44,676 [Epoch: 633 Step: 00021500] Batch Recognition Loss:  33.661629 => Gls Tokens per Sec:      564 || Batch Translation Loss: 102.696945 => Txt Tokens per Sec:     1699 || Lr: 0.000100
2024-01-31 20:12:55,548 Epoch 633: Total Training Recognition Loss 1102.65  Total Training Translation Loss 3487.53 
2024-01-31 20:12:55,548 EPOCH 634
2024-01-31 20:13:13,166 Epoch 634: Total Training Recognition Loss 1102.40  Total Training Translation Loss 3486.22 
2024-01-31 20:13:13,166 EPOCH 635
2024-01-31 20:13:30,800 Epoch 635: Total Training Recognition Loss 1101.03  Total Training Translation Loss 3486.99 
2024-01-31 20:13:30,800 EPOCH 636
2024-01-31 20:13:37,253 [Epoch: 636 Step: 00021600] Batch Recognition Loss:  40.317146 => Gls Tokens per Sec:      496 || Batch Translation Loss: 115.504890 => Txt Tokens per Sec:     1439 || Lr: 0.000100
2024-01-31 20:13:48,418 Epoch 636: Total Training Recognition Loss 1099.24  Total Training Translation Loss 3487.88 
2024-01-31 20:13:48,418 EPOCH 637
2024-01-31 20:14:06,027 Epoch 637: Total Training Recognition Loss 1100.80  Total Training Translation Loss 3487.48 
2024-01-31 20:14:06,027 EPOCH 638
2024-01-31 20:14:23,642 Epoch 638: Total Training Recognition Loss 1100.36  Total Training Translation Loss 3487.88 
2024-01-31 20:14:23,642 EPOCH 639
2024-01-31 20:14:26,945 [Epoch: 639 Step: 00021700] Batch Recognition Loss:  34.840157 => Gls Tokens per Sec:      775 || Batch Translation Loss: 109.388222 => Txt Tokens per Sec:     1992 || Lr: 0.000100
2024-01-31 20:14:41,178 Epoch 639: Total Training Recognition Loss 1103.48  Total Training Translation Loss 3487.32 
2024-01-31 20:14:41,178 EPOCH 640
2024-01-31 20:14:58,722 Epoch 640: Total Training Recognition Loss 1102.49  Total Training Translation Loss 3487.21 
2024-01-31 20:14:58,723 EPOCH 641
2024-01-31 20:15:16,137 Epoch 641: Total Training Recognition Loss 1100.96  Total Training Translation Loss 3487.73 
2024-01-31 20:15:16,138 EPOCH 642
2024-01-31 20:15:18,786 [Epoch: 642 Step: 00021800] Batch Recognition Loss:  45.793755 => Gls Tokens per Sec:      726 || Batch Translation Loss: 125.953445 => Txt Tokens per Sec:     1956 || Lr: 0.000100
2024-01-31 20:15:33,752 Epoch 642: Total Training Recognition Loss 1098.49  Total Training Translation Loss 3486.73 
2024-01-31 20:15:33,752 EPOCH 643
2024-01-31 20:15:51,441 Epoch 643: Total Training Recognition Loss 1099.08  Total Training Translation Loss 3487.40 
2024-01-31 20:15:51,441 EPOCH 644
2024-01-31 20:16:09,013 Epoch 644: Total Training Recognition Loss 1100.80  Total Training Translation Loss 3486.93 
2024-01-31 20:16:09,013 EPOCH 645
2024-01-31 20:16:10,935 [Epoch: 645 Step: 00021900] Batch Recognition Loss:  27.808945 => Gls Tokens per Sec:      666 || Batch Translation Loss:  88.553505 => Txt Tokens per Sec:     1937 || Lr: 0.000100
2024-01-31 20:16:26,603 Epoch 645: Total Training Recognition Loss 1100.73  Total Training Translation Loss 3487.28 
2024-01-31 20:16:26,603 EPOCH 646
2024-01-31 20:16:44,201 Epoch 646: Total Training Recognition Loss 1099.81  Total Training Translation Loss 3487.19 
2024-01-31 20:16:44,201 EPOCH 647
2024-01-31 20:17:01,779 Epoch 647: Total Training Recognition Loss 1101.23  Total Training Translation Loss 3487.48 
2024-01-31 20:17:01,779 EPOCH 648
2024-01-31 20:17:02,635 [Epoch: 648 Step: 00022000] Batch Recognition Loss:  36.974442 => Gls Tokens per Sec:      749 || Batch Translation Loss: 125.251633 => Txt Tokens per Sec:     2224 || Lr: 0.000100
2024-01-31 20:17:26,615 Validation result at epoch 648, step    22000: duration: 23.9789s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 351.10706	Translation Loss: 73469.95312	PPL: 1559.53296
	Eval Metric: BLEU
	WER 537.01	(DEL: 4.17,	INS: 447.88,	SUB: 84.96)
	BLEU-4 0.00	(BLEU-1: 0.01,	BLEU-2: 0.00,	BLEU-3: 0.00,	BLEU-4: 0.00)
	CHRF 3.52	ROUGE 0.02
2024-01-31 20:17:26,617 Logging Recognition and Translation Outputs
2024-01-31 20:17:26,617 ========================================================================================================================
2024-01-31 20:17:26,617 Logging Sequence: 146_56.00
2024-01-31 20:17:26,617 	Gloss Reference :	*** ***** * ***** * ***** * ***** ***** ***** ***************** A     B+C+D+E
2024-01-31 20:17:26,618 	Gloss Hypothesis:	E+C <pad> E <unk> E <unk> E <unk> E+C+E <unk> E+C+E+C+E+C+E+C+E <unk> E+C    
2024-01-31 20:17:26,618 	Gloss Alignment :	I   I     I I     I I     I I     I     I     I                 S     S      
2024-01-31 20:17:26,618 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 20:17:26,622 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** ** ** ** ** ** ** ** ** when the players go back to the hotel as per rules all of them have to undergo rtpcr test for covid-19 everyday
2024-01-31 20:17:26,623 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> kl kl kl kl kl kl kl kl kl   kl  kl      kl kl   kl kl  kl    kl kl  kl    kl  kl kl   kl   kl kl      kl    kl   kl  kl       kl      
2024-01-31 20:17:26,623 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I  I  I  I  I  I  I  I  S    S   S       S  S    S  S   S     S  S   S     S   S  S    S    S  S       S     S    S   S        S       
2024-01-31 20:17:26,623 ========================================================================================================================
2024-01-31 20:17:26,623 Logging Sequence: 118_338.00
2024-01-31 20:17:26,623 	Gloss Reference :	***** *** ***** * ***** * ***** *** ***** *** A     B+C+D+E
2024-01-31 20:17:26,623 	Gloss Hypothesis:	<unk> C+E <unk> E <unk> E <unk> E+C <unk> C+E <unk> C      
2024-01-31 20:17:26,624 	Gloss Alignment :	I     I   I     I I     I I     I   I     I   S     S      
2024-01-31 20:17:26,624 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 20:17:26,625 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** this is  why even messi wore it 
2024-01-31 20:17:26,626 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s>  <s> <s> <s>  <s>   <s>  <s>
2024-01-31 20:17:26,626 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   S    S   S   S    S     S    S  
2024-01-31 20:17:26,626 ========================================================================================================================
2024-01-31 20:17:26,626 Logging Sequence: 66_61.00
2024-01-31 20:17:26,626 	Gloss Reference :	***** ***** ***** *** ***** * ***** * ***** * ***** A B+C+D+E
2024-01-31 20:17:26,626 	Gloss Hypothesis:	<unk> <pad> <unk> E+B <unk> B <unk> E <unk> B <unk> C <unk>  
2024-01-31 20:17:26,627 	Gloss Alignment :	I     I     I     I   I     I I     I I     I I     S S      
2024-01-31 20:17:26,627 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 20:17:26,629 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** instead  of       returning back     to       his      homeland because  of       his      injury  
2024-01-31 20:17:26,629 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas  casillas casillas casillas casillas casillas casillas casillas casillas
2024-01-31 20:17:26,629 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I        I        I        I        I        I        I        I        I        I        I        I        I        I        I        I        I        I        I        S        S        S         S        S        S        S        S        S        S        S       
2024-01-31 20:17:26,630 ========================================================================================================================
2024-01-31 20:17:26,630 Logging Sequence: 81_278.00
2024-01-31 20:17:26,630 	Gloss Reference :	A B+C+D+E                                                          
2024-01-31 20:17:26,630 	Gloss Hypothesis:	E C+E+C+E+C+E+C+E+C+E+D+E+C+E+C+E+C+E+C+E+C+E+C+E+C+E+C+E+C+E+B+E+B
2024-01-31 20:17:26,630 	Gloss Alignment :	S S                                                                
2024-01-31 20:17:26,630 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 20:17:26,635 	Text Reference  :	*** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** of    this  amrapali group paid  rs    3570  crore the   remaining rs    652   crore was   paid  by    amrapali sapphire developers a     subsidiary of    amrapali group
2024-01-31 20:17:26,635 	Text Hypothesis :	<s> sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha    sabha sabha sabha sabha sabha sabha sabha     sabha sabha sabha sabha sabha sabha sabha    sabha    sabha      sabha sabha      sabha sabha    sabha
2024-01-31 20:17:26,635 	Text Alignment  :	I   I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     S     S     S        S     S     S     S     S     S     S         S     S     S     S     S     S     S        S        S          S     S          S     S        S    
2024-01-31 20:17:26,636 ========================================================================================================================
2024-01-31 20:17:26,636 Logging Sequence: 162_125.00
2024-01-31 20:17:26,636 	Gloss Reference :	A     B+C+D+E                
2024-01-31 20:17:26,636 	Gloss Hypothesis:	<pad> B+E+B+E+B+E+B+E+B+E+B+E
2024-01-31 20:17:26,636 	Gloss Alignment :	S     S                      
2024-01-31 20:17:26,636 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 20:17:26,639 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** in  response to  this kohli received many hate comments on  social media
2024-01-31 20:17:26,639 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s>      <s> <s>  <s>   <s>      <s>  <s>  <s>      <s> <s>    <s>  
2024-01-31 20:17:26,639 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   S   S        S   S    S     S        S    S    S        S   S      S    
2024-01-31 20:17:26,639 ========================================================================================================================
2024-01-31 20:17:43,289 Epoch 648: Total Training Recognition Loss 1100.31  Total Training Translation Loss 3487.72 
2024-01-31 20:17:43,289 EPOCH 649
2024-01-31 20:18:01,002 Epoch 649: Total Training Recognition Loss 1101.75  Total Training Translation Loss 3486.70 
2024-01-31 20:18:01,002 EPOCH 650
2024-01-31 20:18:18,548 [Epoch: 650 Step: 00022100] Batch Recognition Loss:  36.569412 => Gls Tokens per Sec:      606 || Batch Translation Loss: 119.938995 => Txt Tokens per Sec:     1682 || Lr: 0.000100
2024-01-31 20:18:18,548 Epoch 650: Total Training Recognition Loss 1100.61  Total Training Translation Loss 3487.85 
2024-01-31 20:18:18,548 EPOCH 651
2024-01-31 20:18:36,208 Epoch 651: Total Training Recognition Loss 1101.90  Total Training Translation Loss 3487.72 
2024-01-31 20:18:36,208 EPOCH 652
2024-01-31 20:18:53,805 Epoch 652: Total Training Recognition Loss 1100.40  Total Training Translation Loss 3487.55 
2024-01-31 20:18:53,805 EPOCH 653
2024-01-31 20:19:10,401 [Epoch: 653 Step: 00022200] Batch Recognition Loss:  20.569418 => Gls Tokens per Sec:      602 || Batch Translation Loss:  85.569298 => Txt Tokens per Sec:     1665 || Lr: 0.000100
2024-01-31 20:19:11,486 Epoch 653: Total Training Recognition Loss 1101.16  Total Training Translation Loss 3486.25 
2024-01-31 20:19:11,486 EPOCH 654
2024-01-31 20:19:29,210 Epoch 654: Total Training Recognition Loss 1099.18  Total Training Translation Loss 3487.55 
2024-01-31 20:19:29,210 EPOCH 655
2024-01-31 20:19:46,769 Epoch 655: Total Training Recognition Loss 1101.05  Total Training Translation Loss 3486.88 
2024-01-31 20:19:46,769 EPOCH 656
2024-01-31 20:20:01,954 [Epoch: 656 Step: 00022300] Batch Recognition Loss:  28.220390 => Gls Tokens per Sec:      632 || Batch Translation Loss:  86.524246 => Txt Tokens per Sec:     1760 || Lr: 0.000100
2024-01-31 20:20:04,495 Epoch 656: Total Training Recognition Loss 1100.49  Total Training Translation Loss 3487.41 
2024-01-31 20:20:04,495 EPOCH 657
2024-01-31 20:20:21,967 Epoch 657: Total Training Recognition Loss 1102.13  Total Training Translation Loss 3487.09 
2024-01-31 20:20:21,968 EPOCH 658
2024-01-31 20:20:39,531 Epoch 658: Total Training Recognition Loss 1102.72  Total Training Translation Loss 3486.02 
2024-01-31 20:20:39,531 EPOCH 659
2024-01-31 20:20:53,898 [Epoch: 659 Step: 00022400] Batch Recognition Loss:  18.896721 => Gls Tokens per Sec:      624 || Batch Translation Loss:  88.695923 => Txt Tokens per Sec:     1716 || Lr: 0.000100
2024-01-31 20:20:57,211 Epoch 659: Total Training Recognition Loss 1103.04  Total Training Translation Loss 3486.87 
2024-01-31 20:20:57,211 EPOCH 660
2024-01-31 20:21:14,785 Epoch 660: Total Training Recognition Loss 1100.39  Total Training Translation Loss 3487.29 
2024-01-31 20:21:14,785 EPOCH 661
2024-01-31 20:21:33,806 Epoch 661: Total Training Recognition Loss 1100.63  Total Training Translation Loss 3487.63 
2024-01-31 20:21:33,806 EPOCH 662
2024-01-31 20:21:49,036 [Epoch: 662 Step: 00022500] Batch Recognition Loss:  52.324417 => Gls Tokens per Sec:      546 || Batch Translation Loss: 128.329651 => Txt Tokens per Sec:     1497 || Lr: 0.000100
2024-01-31 20:21:52,996 Epoch 662: Total Training Recognition Loss 1101.02  Total Training Translation Loss 3486.46 
2024-01-31 20:21:52,996 EPOCH 663
2024-01-31 20:22:10,372 Epoch 663: Total Training Recognition Loss 1099.57  Total Training Translation Loss 3487.25 
2024-01-31 20:22:10,372 EPOCH 664
2024-01-31 20:22:27,922 Epoch 664: Total Training Recognition Loss 1098.39  Total Training Translation Loss 3487.69 
2024-01-31 20:22:27,922 EPOCH 665
2024-01-31 20:22:39,328 [Epoch: 665 Step: 00022600] Batch Recognition Loss:  27.510986 => Gls Tokens per Sec:      652 || Batch Translation Loss:  86.347939 => Txt Tokens per Sec:     1807 || Lr: 0.000100
2024-01-31 20:22:45,388 Epoch 665: Total Training Recognition Loss 1101.31  Total Training Translation Loss 3487.77 
2024-01-31 20:22:45,388 EPOCH 666
2024-01-31 20:23:02,858 Epoch 666: Total Training Recognition Loss 1101.77  Total Training Translation Loss 3487.08 
2024-01-31 20:23:02,858 EPOCH 667
2024-01-31 20:23:20,464 Epoch 667: Total Training Recognition Loss 1101.22  Total Training Translation Loss 3487.21 
2024-01-31 20:23:20,464 EPOCH 668
2024-01-31 20:23:29,474 [Epoch: 668 Step: 00022700] Batch Recognition Loss:  21.193653 => Gls Tokens per Sec:      754 || Batch Translation Loss:  87.404015 => Txt Tokens per Sec:     1966 || Lr: 0.000100
2024-01-31 20:23:37,866 Epoch 668: Total Training Recognition Loss 1101.48  Total Training Translation Loss 3486.68 
2024-01-31 20:23:37,866 EPOCH 669
2024-01-31 20:23:55,331 Epoch 669: Total Training Recognition Loss 1100.91  Total Training Translation Loss 3486.94 
2024-01-31 20:23:55,331 EPOCH 670
2024-01-31 20:24:12,813 Epoch 670: Total Training Recognition Loss 1102.40  Total Training Translation Loss 3486.98 
2024-01-31 20:24:12,813 EPOCH 671
2024-01-31 20:24:23,084 [Epoch: 671 Step: 00022800] Batch Recognition Loss:  10.873281 => Gls Tokens per Sec:      599 || Batch Translation Loss:  61.801804 => Txt Tokens per Sec:     1687 || Lr: 0.000100
2024-01-31 20:24:30,304 Epoch 671: Total Training Recognition Loss 1100.66  Total Training Translation Loss 3486.48 
2024-01-31 20:24:30,304 EPOCH 672
2024-01-31 20:24:47,721 Epoch 672: Total Training Recognition Loss 1098.93  Total Training Translation Loss 3486.83 
2024-01-31 20:24:47,722 EPOCH 673
2024-01-31 20:25:05,196 Epoch 673: Total Training Recognition Loss 1100.06  Total Training Translation Loss 3487.98 
2024-01-31 20:25:05,197 EPOCH 674
2024-01-31 20:25:14,271 [Epoch: 674 Step: 00022900] Batch Recognition Loss:  13.044321 => Gls Tokens per Sec:      607 || Batch Translation Loss:  71.827118 => Txt Tokens per Sec:     1667 || Lr: 0.000100
2024-01-31 20:25:22,671 Epoch 674: Total Training Recognition Loss 1099.92  Total Training Translation Loss 3487.64 
2024-01-31 20:25:22,672 EPOCH 675
2024-01-31 20:25:40,209 Epoch 675: Total Training Recognition Loss 1101.85  Total Training Translation Loss 3487.90 
2024-01-31 20:25:40,210 EPOCH 676
2024-01-31 20:25:57,816 Epoch 676: Total Training Recognition Loss 1099.00  Total Training Translation Loss 3487.19 
2024-01-31 20:25:57,817 EPOCH 677
2024-01-31 20:26:05,948 [Epoch: 677 Step: 00023000] Batch Recognition Loss:  38.237511 => Gls Tokens per Sec:      599 || Batch Translation Loss: 112.944992 => Txt Tokens per Sec:     1696 || Lr: 0.000100
2024-01-31 20:26:15,335 Epoch 677: Total Training Recognition Loss 1100.31  Total Training Translation Loss 3487.50 
2024-01-31 20:26:15,335 EPOCH 678
2024-01-31 20:26:32,839 Epoch 678: Total Training Recognition Loss 1101.99  Total Training Translation Loss 3486.83 
2024-01-31 20:26:32,839 EPOCH 679
2024-01-31 20:26:50,392 Epoch 679: Total Training Recognition Loss 1101.50  Total Training Translation Loss 3487.50 
2024-01-31 20:26:50,393 EPOCH 680
2024-01-31 20:26:56,822 [Epoch: 680 Step: 00023100] Batch Recognition Loss:  30.732967 => Gls Tokens per Sec:      697 || Batch Translation Loss: 111.094559 => Txt Tokens per Sec:     1929 || Lr: 0.000100
2024-01-31 20:27:08,038 Epoch 680: Total Training Recognition Loss 1100.02  Total Training Translation Loss 3487.10 
2024-01-31 20:27:08,038 EPOCH 681
2024-01-31 20:27:25,565 Epoch 681: Total Training Recognition Loss 1099.36  Total Training Translation Loss 3487.66 
2024-01-31 20:27:25,565 EPOCH 682
2024-01-31 20:27:43,045 Epoch 682: Total Training Recognition Loss 1099.65  Total Training Translation Loss 3486.86 
2024-01-31 20:27:43,045 EPOCH 683
2024-01-31 20:27:49,215 [Epoch: 683 Step: 00023200] Batch Recognition Loss:  33.243210 => Gls Tokens per Sec:      622 || Batch Translation Loss:  97.679260 => Txt Tokens per Sec:     1720 || Lr: 0.000100
2024-01-31 20:28:00,496 Epoch 683: Total Training Recognition Loss 1099.01  Total Training Translation Loss 3487.09 
2024-01-31 20:28:00,496 EPOCH 684
2024-01-31 20:28:18,096 Epoch 684: Total Training Recognition Loss 1100.93  Total Training Translation Loss 3487.15 
2024-01-31 20:28:18,096 EPOCH 685
2024-01-31 20:28:35,684 Epoch 685: Total Training Recognition Loss 1101.00  Total Training Translation Loss 3486.82 
2024-01-31 20:28:35,684 EPOCH 686
2024-01-31 20:28:39,643 [Epoch: 686 Step: 00023300] Batch Recognition Loss:  37.646782 => Gls Tokens per Sec:      808 || Batch Translation Loss: 110.493561 => Txt Tokens per Sec:     2128 || Lr: 0.000100
2024-01-31 20:28:53,314 Epoch 686: Total Training Recognition Loss 1099.52  Total Training Translation Loss 3487.40 
2024-01-31 20:28:53,315 EPOCH 687
2024-01-31 20:29:10,998 Epoch 687: Total Training Recognition Loss 1102.01  Total Training Translation Loss 3487.32 
2024-01-31 20:29:10,998 EPOCH 688
2024-01-31 20:29:28,639 Epoch 688: Total Training Recognition Loss 1099.80  Total Training Translation Loss 3487.53 
2024-01-31 20:29:28,640 EPOCH 689
2024-01-31 20:29:32,640 [Epoch: 689 Step: 00023400] Batch Recognition Loss:  37.319855 => Gls Tokens per Sec:      640 || Batch Translation Loss: 121.377785 => Txt Tokens per Sec:     1779 || Lr: 0.000100
2024-01-31 20:29:46,159 Epoch 689: Total Training Recognition Loss 1103.15  Total Training Translation Loss 3487.39 
2024-01-31 20:29:46,160 EPOCH 690
2024-01-31 20:30:03,671 Epoch 690: Total Training Recognition Loss 1099.24  Total Training Translation Loss 3487.46 
2024-01-31 20:30:03,672 EPOCH 691
2024-01-31 20:30:21,164 Epoch 691: Total Training Recognition Loss 1101.92  Total Training Translation Loss 3486.82 
2024-01-31 20:30:21,165 EPOCH 692
2024-01-31 20:30:24,984 [Epoch: 692 Step: 00023500] Batch Recognition Loss:  73.991333 => Gls Tokens per Sec:      438 || Batch Translation Loss: 126.293846 => Txt Tokens per Sec:     1241 || Lr: 0.000100
2024-01-31 20:30:38,655 Epoch 692: Total Training Recognition Loss 1101.22  Total Training Translation Loss 3486.96 
2024-01-31 20:30:38,655 EPOCH 693
2024-01-31 20:30:56,179 Epoch 693: Total Training Recognition Loss 1102.60  Total Training Translation Loss 3486.61 
2024-01-31 20:30:56,179 EPOCH 694
2024-01-31 20:31:13,723 Epoch 694: Total Training Recognition Loss 1099.61  Total Training Translation Loss 3487.43 
2024-01-31 20:31:13,723 EPOCH 695
2024-01-31 20:31:15,374 [Epoch: 695 Step: 00023600] Batch Recognition Loss:  32.780838 => Gls Tokens per Sec:      776 || Batch Translation Loss: 107.890808 => Txt Tokens per Sec:     2070 || Lr: 0.000100
2024-01-31 20:31:31,300 Epoch 695: Total Training Recognition Loss 1100.59  Total Training Translation Loss 3487.23 
2024-01-31 20:31:31,300 EPOCH 696
2024-01-31 20:31:48,930 Epoch 696: Total Training Recognition Loss 1102.09  Total Training Translation Loss 3486.79 
2024-01-31 20:31:48,930 EPOCH 697
2024-01-31 20:32:06,483 Epoch 697: Total Training Recognition Loss 1100.63  Total Training Translation Loss 3486.90 
2024-01-31 20:32:06,484 EPOCH 698
2024-01-31 20:32:07,970 [Epoch: 698 Step: 00023700] Batch Recognition Loss:  43.388603 => Gls Tokens per Sec:      431 || Batch Translation Loss: 131.914246 => Txt Tokens per Sec:     1439 || Lr: 0.000100
2024-01-31 20:32:24,088 Epoch 698: Total Training Recognition Loss 1098.66  Total Training Translation Loss 3486.55 
2024-01-31 20:32:24,089 EPOCH 699
2024-01-31 20:32:41,676 Epoch 699: Total Training Recognition Loss 1100.70  Total Training Translation Loss 3487.38 
2024-01-31 20:32:41,677 EPOCH 700
2024-01-31 20:32:59,289 [Epoch: 700 Step: 00023800] Batch Recognition Loss:  39.911488 => Gls Tokens per Sec:      604 || Batch Translation Loss: 116.825455 => Txt Tokens per Sec:     1676 || Lr: 0.000100
2024-01-31 20:32:59,289 Epoch 700: Total Training Recognition Loss 1101.76  Total Training Translation Loss 3485.97 
2024-01-31 20:32:59,289 EPOCH 701
2024-01-31 20:33:16,709 Epoch 701: Total Training Recognition Loss 1099.47  Total Training Translation Loss 3487.79 
2024-01-31 20:33:16,710 EPOCH 702
2024-01-31 20:33:34,393 Epoch 702: Total Training Recognition Loss 1099.62  Total Training Translation Loss 3487.36 
2024-01-31 20:33:34,394 EPOCH 703
2024-01-31 20:33:51,043 [Epoch: 703 Step: 00023900] Batch Recognition Loss:  32.732384 => Gls Tokens per Sec:      600 || Batch Translation Loss: 109.066635 => Txt Tokens per Sec:     1678 || Lr: 0.000100
2024-01-31 20:33:52,001 Epoch 703: Total Training Recognition Loss 1101.22  Total Training Translation Loss 3487.50 
2024-01-31 20:33:52,001 EPOCH 704
2024-01-31 20:34:09,746 Epoch 704: Total Training Recognition Loss 1100.05  Total Training Translation Loss 3487.09 
2024-01-31 20:34:09,746 EPOCH 705
2024-01-31 20:34:27,292 Epoch 705: Total Training Recognition Loss 1101.95  Total Training Translation Loss 3487.02 
2024-01-31 20:34:27,292 EPOCH 706
2024-01-31 20:34:41,874 [Epoch: 706 Step: 00024000] Batch Recognition Loss:  23.615059 => Gls Tokens per Sec:      641 || Batch Translation Loss: 100.268692 => Txt Tokens per Sec:     1740 || Lr: 0.000100
2024-01-31 20:35:06,146 Validation result at epoch 706, step    24000: duration: 24.2723s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 350.78952	Translation Loss: 73473.65625	PPL: 1560.11145
	Eval Metric: BLEU
	WER 533.40	(DEL: 4.10,	INS: 444.35,	SUB: 84.96)
	BLEU-4 0.00	(BLEU-1: 0.01,	BLEU-2: 0.00,	BLEU-3: 0.00,	BLEU-4: 0.00)
	CHRF 3.49	ROUGE 0.02
2024-01-31 20:35:06,148 Logging Recognition and Translation Outputs
2024-01-31 20:35:06,148 ========================================================================================================================
2024-01-31 20:35:06,148 Logging Sequence: 169_165.00
2024-01-31 20:35:06,148 	Gloss Reference :	A B+C+D+E            
2024-01-31 20:35:06,148 	Gloss Hypothesis:	E D+E+D+E+D+E+D+E+C+E
2024-01-31 20:35:06,149 	Gloss Alignment :	S S                  
2024-01-31 20:35:06,149 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 20:35:06,152 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** the indian government was outraged by  the incident and these changes were undone by  wikipedia
2024-01-31 20:35:06,152 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s>    <s>        <s> <s>      <s> <s> <s>      <s> <s>   <s>     <s>  <s>    <s> <s>      
2024-01-31 20:35:06,152 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   S   S      S          S   S        S   S   S        S   S     S       S    S      S   S        
2024-01-31 20:35:06,152 ========================================================================================================================
2024-01-31 20:35:06,153 Logging Sequence: 175_60.00
2024-01-31 20:35:06,153 	Gloss Reference :	* ***** A ***** B+C+D+E
2024-01-31 20:35:06,153 	Gloss Hypothesis:	A <unk> A <unk> E      
2024-01-31 20:35:06,153 	Gloss Alignment :	I I       I     S      
2024-01-31 20:35:06,153 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 20:35:06,156 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** that     is       how      india    bagged   9        medals   in       the      youth    tournament
2024-01-31 20:35:06,156 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas  
2024-01-31 20:35:06,156 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I        I        I        I        I        I        I        I        I        I        I        I        I        I        I        I        I        I        S        S        S        S        S        S        S        S        S        S        S         
2024-01-31 20:35:06,156 ========================================================================================================================
2024-01-31 20:35:06,156 Logging Sequence: 61_255.00
2024-01-31 20:35:06,157 	Gloss Reference :	* ***** * ***** * ***** * ***** *** ***** A ***** *** ***** * ***** * ***** * ***** ********* ***** B+C+D+E
2024-01-31 20:35:06,157 	Gloss Hypothesis:	E <unk> A <unk> E <unk> E <unk> B+E <unk> A <unk> A+E <unk> E <unk> E <unk> E <unk> E+A+E+C+E <unk> A+C    
2024-01-31 20:35:06,157 	Gloss Alignment :	I I     I I     I I     I I     I   I       I     I   I     I I     I I     I I     I         I     S      
2024-01-31 20:35:06,157 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 20:35:06,160 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** *** *** ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* in            2011          we            decided       to            marry         and           informed      our           families     
2024-01-31 20:35:06,160 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions
2024-01-31 20:35:06,160 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I   I   I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             S             S             S             S             S             S             S             S             S             S            
2024-01-31 20:35:06,160 ========================================================================================================================
2024-01-31 20:35:06,160 Logging Sequence: 173_39.00
2024-01-31 20:35:06,160 	Gloss Reference :	A B+C+D+E
2024-01-31 20:35:06,161 	Gloss Hypothesis:	* E      
2024-01-31 20:35:06,161 	Gloss Alignment :	D S      
2024-01-31 20:35:06,161 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 20:35:06,162 	Text Reference  :	*** *** *** *** *** *** *** *** *** ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* kohli   will    step    down    as      india'  captain
2024-01-31 20:35:06,163 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing
2024-01-31 20:35:06,163 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       S       S       S       S       S       S       S      
2024-01-31 20:35:06,163 ========================================================================================================================
2024-01-31 20:35:06,163 Logging Sequence: 172_82.00
2024-01-31 20:35:06,163 	Gloss Reference :	***** * ***** * ***** * ***** * ***** ***** ***** * ***** A B+C+D+E
2024-01-31 20:35:06,163 	Gloss Hypothesis:	<unk> C <unk> E <unk> C <unk> E <unk> C+E+C <unk> E <unk> E <unk>  
2024-01-31 20:35:06,164 	Gloss Alignment :	I     I I     I I     I I     I I     I     I     I I     S S      
2024-01-31 20:35:06,164 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 20:35:06,168 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** you all know that the toss was about to  start at  700 pm  but it  started raining at  around 630 pm 
2024-01-31 20:35:06,168 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s>  <s>  <s> <s>  <s> <s>   <s> <s>   <s> <s> <s> <s> <s> <s>     <s>     <s> <s>    <s> <s>
2024-01-31 20:35:06,168 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   S   S   S    S    S   S    S   S     S   S     S   S   S   S   S   S       S       S   S      S   S  
2024-01-31 20:35:06,168 ========================================================================================================================
2024-01-31 20:35:09,055 Epoch 706: Total Training Recognition Loss 1100.47  Total Training Translation Loss 3487.60 
2024-01-31 20:35:09,055 EPOCH 707
2024-01-31 20:35:26,590 Epoch 707: Total Training Recognition Loss 1100.94  Total Training Translation Loss 3487.67 
2024-01-31 20:35:26,591 EPOCH 708
2024-01-31 20:35:44,144 Epoch 708: Total Training Recognition Loss 1102.38  Total Training Translation Loss 3486.80 
2024-01-31 20:35:44,144 EPOCH 709
2024-01-31 20:35:57,588 [Epoch: 709 Step: 00024100] Batch Recognition Loss:  15.212389 => Gls Tokens per Sec:      648 || Batch Translation Loss:  76.322525 => Txt Tokens per Sec:     1764 || Lr: 0.000100
2024-01-31 20:36:01,706 Epoch 709: Total Training Recognition Loss 1100.67  Total Training Translation Loss 3486.70 
2024-01-31 20:36:01,706 EPOCH 710
2024-01-31 20:36:19,258 Epoch 710: Total Training Recognition Loss 1101.06  Total Training Translation Loss 3487.03 
2024-01-31 20:36:19,258 EPOCH 711
2024-01-31 20:36:36,759 Epoch 711: Total Training Recognition Loss 1099.91  Total Training Translation Loss 3486.76 
2024-01-31 20:36:36,759 EPOCH 712
2024-01-31 20:36:50,480 [Epoch: 712 Step: 00024200] Batch Recognition Loss:  39.120026 => Gls Tokens per Sec:      588 || Batch Translation Loss: 123.590820 => Txt Tokens per Sec:     1648 || Lr: 0.000100
2024-01-31 20:36:54,451 Epoch 712: Total Training Recognition Loss 1099.30  Total Training Translation Loss 3487.25 
2024-01-31 20:36:54,451 EPOCH 713
2024-01-31 20:37:12,116 Epoch 713: Total Training Recognition Loss 1101.20  Total Training Translation Loss 3487.13 
2024-01-31 20:37:12,116 EPOCH 714
2024-01-31 20:37:29,693 Epoch 714: Total Training Recognition Loss 1100.79  Total Training Translation Loss 3487.65 
2024-01-31 20:37:29,694 EPOCH 715
2024-01-31 20:37:40,402 [Epoch: 715 Step: 00024300] Batch Recognition Loss:  37.486748 => Gls Tokens per Sec:      694 || Batch Translation Loss: 107.141785 => Txt Tokens per Sec:     1879 || Lr: 0.000100
2024-01-31 20:37:47,261 Epoch 715: Total Training Recognition Loss 1101.20  Total Training Translation Loss 3487.60 
2024-01-31 20:37:47,261 EPOCH 716
2024-01-31 20:38:04,927 Epoch 716: Total Training Recognition Loss 1102.18  Total Training Translation Loss 3487.96 
2024-01-31 20:38:04,927 EPOCH 717
2024-01-31 20:38:22,508 Epoch 717: Total Training Recognition Loss 1101.17  Total Training Translation Loss 3486.99 
2024-01-31 20:38:22,508 EPOCH 718
2024-01-31 20:38:33,626 [Epoch: 718 Step: 00024400] Batch Recognition Loss:  27.669817 => Gls Tokens per Sec:      611 || Batch Translation Loss:  97.772903 => Txt Tokens per Sec:     1736 || Lr: 0.000100
2024-01-31 20:38:40,107 Epoch 718: Total Training Recognition Loss 1100.63  Total Training Translation Loss 3486.46 
2024-01-31 20:38:40,107 EPOCH 719
2024-01-31 20:38:57,677 Epoch 719: Total Training Recognition Loss 1102.23  Total Training Translation Loss 3486.75 
2024-01-31 20:38:57,678 EPOCH 720
2024-01-31 20:39:15,189 Epoch 720: Total Training Recognition Loss 1102.04  Total Training Translation Loss 3487.86 
2024-01-31 20:39:15,189 EPOCH 721
2024-01-31 20:39:27,174 [Epoch: 721 Step: 00024500] Batch Recognition Loss:  30.046432 => Gls Tokens per Sec:      513 || Batch Translation Loss: 100.931137 => Txt Tokens per Sec:     1532 || Lr: 0.000100
2024-01-31 20:39:32,784 Epoch 721: Total Training Recognition Loss 1100.79  Total Training Translation Loss 3487.80 
2024-01-31 20:39:32,784 EPOCH 722
2024-01-31 20:39:50,446 Epoch 722: Total Training Recognition Loss 1098.73  Total Training Translation Loss 3487.02 
2024-01-31 20:39:50,446 EPOCH 723
2024-01-31 20:40:08,177 Epoch 723: Total Training Recognition Loss 1099.74  Total Training Translation Loss 3487.97 
2024-01-31 20:40:08,178 EPOCH 724
2024-01-31 20:40:17,300 [Epoch: 724 Step: 00024600] Batch Recognition Loss:  36.289700 => Gls Tokens per Sec:      632 || Batch Translation Loss: 110.726517 => Txt Tokens per Sec:     1735 || Lr: 0.000100
2024-01-31 20:40:25,899 Epoch 724: Total Training Recognition Loss 1100.66  Total Training Translation Loss 3486.58 
2024-01-31 20:40:25,899 EPOCH 725
2024-01-31 20:40:43,455 Epoch 725: Total Training Recognition Loss 1099.97  Total Training Translation Loss 3485.97 
2024-01-31 20:40:43,455 EPOCH 726
2024-01-31 20:41:01,062 Epoch 726: Total Training Recognition Loss 1100.23  Total Training Translation Loss 3487.24 
2024-01-31 20:41:01,062 EPOCH 727
2024-01-31 20:41:08,813 [Epoch: 727 Step: 00024700] Batch Recognition Loss:  30.306061 => Gls Tokens per Sec:      661 || Batch Translation Loss: 109.440193 => Txt Tokens per Sec:     1800 || Lr: 0.000100
2024-01-31 20:41:18,687 Epoch 727: Total Training Recognition Loss 1099.64  Total Training Translation Loss 3486.47 
2024-01-31 20:41:18,688 EPOCH 728
2024-01-31 20:41:36,375 Epoch 728: Total Training Recognition Loss 1099.63  Total Training Translation Loss 3487.27 
2024-01-31 20:41:36,375 EPOCH 729
2024-01-31 20:41:53,932 Epoch 729: Total Training Recognition Loss 1101.01  Total Training Translation Loss 3487.59 
2024-01-31 20:41:53,932 EPOCH 730
2024-01-31 20:42:01,948 [Epoch: 730 Step: 00024800] Batch Recognition Loss:  22.971741 => Gls Tokens per Sec:      559 || Batch Translation Loss:  82.276619 => Txt Tokens per Sec:     1588 || Lr: 0.000100
2024-01-31 20:42:11,531 Epoch 730: Total Training Recognition Loss 1100.29  Total Training Translation Loss 3486.72 
2024-01-31 20:42:11,531 EPOCH 731
2024-01-31 20:42:29,151 Epoch 731: Total Training Recognition Loss 1099.37  Total Training Translation Loss 3487.65 
2024-01-31 20:42:29,151 EPOCH 732
2024-01-31 20:42:46,757 Epoch 732: Total Training Recognition Loss 1100.85  Total Training Translation Loss 3487.80 
2024-01-31 20:42:46,757 EPOCH 733
2024-01-31 20:42:52,801 [Epoch: 733 Step: 00024900] Batch Recognition Loss:  31.013227 => Gls Tokens per Sec:      636 || Batch Translation Loss:  93.327454 => Txt Tokens per Sec:     1778 || Lr: 0.000100
2024-01-31 20:43:04,513 Epoch 733: Total Training Recognition Loss 1100.76  Total Training Translation Loss 3486.99 
2024-01-31 20:43:04,513 EPOCH 734
2024-01-31 20:43:21,924 Epoch 734: Total Training Recognition Loss 1099.88  Total Training Translation Loss 3486.65 
2024-01-31 20:43:21,924 EPOCH 735
2024-01-31 20:43:39,446 Epoch 735: Total Training Recognition Loss 1100.46  Total Training Translation Loss 3487.43 
2024-01-31 20:43:39,446 EPOCH 736
2024-01-31 20:43:43,567 [Epoch: 736 Step: 00025000] Batch Recognition Loss:  37.689438 => Gls Tokens per Sec:      777 || Batch Translation Loss: 119.330780 => Txt Tokens per Sec:     2071 || Lr: 0.000100
2024-01-31 20:43:56,946 Epoch 736: Total Training Recognition Loss 1102.01  Total Training Translation Loss 3486.82 
2024-01-31 20:43:56,946 EPOCH 737
2024-01-31 20:44:14,515 Epoch 737: Total Training Recognition Loss 1101.54  Total Training Translation Loss 3487.30 
2024-01-31 20:44:14,516 EPOCH 738
2024-01-31 20:44:32,239 Epoch 738: Total Training Recognition Loss 1098.97  Total Training Translation Loss 3487.78 
2024-01-31 20:44:32,239 EPOCH 739
2024-01-31 20:44:35,993 [Epoch: 739 Step: 00025100] Batch Recognition Loss:  23.886261 => Gls Tokens per Sec:      616 || Batch Translation Loss:  98.102661 => Txt Tokens per Sec:     1683 || Lr: 0.000100
2024-01-31 20:44:49,648 Epoch 739: Total Training Recognition Loss 1100.23  Total Training Translation Loss 3486.77 
2024-01-31 20:44:49,648 EPOCH 740
2024-01-31 20:45:07,125 Epoch 740: Total Training Recognition Loss 1101.66  Total Training Translation Loss 3486.62 
2024-01-31 20:45:07,125 EPOCH 741
2024-01-31 20:45:24,696 Epoch 741: Total Training Recognition Loss 1100.70  Total Training Translation Loss 3487.76 
2024-01-31 20:45:24,696 EPOCH 742
2024-01-31 20:45:26,712 [Epoch: 742 Step: 00025200] Batch Recognition Loss:  37.382076 => Gls Tokens per Sec:      953 || Batch Translation Loss: 114.084740 => Txt Tokens per Sec:     2224 || Lr: 0.000100
2024-01-31 20:45:42,245 Epoch 742: Total Training Recognition Loss 1102.36  Total Training Translation Loss 3487.48 
2024-01-31 20:45:42,245 EPOCH 743
2024-01-31 20:45:59,748 Epoch 743: Total Training Recognition Loss 1099.75  Total Training Translation Loss 3487.66 
2024-01-31 20:45:59,748 EPOCH 744
2024-01-31 20:46:17,328 Epoch 744: Total Training Recognition Loss 1101.40  Total Training Translation Loss 3486.79 
2024-01-31 20:46:17,328 EPOCH 745
2024-01-31 20:46:19,911 [Epoch: 745 Step: 00025300] Batch Recognition Loss:  58.182842 => Gls Tokens per Sec:      496 || Batch Translation Loss: 132.414490 => Txt Tokens per Sec:     1295 || Lr: 0.000100
2024-01-31 20:46:35,029 Epoch 745: Total Training Recognition Loss 1099.64  Total Training Translation Loss 3486.48 
2024-01-31 20:46:35,029 EPOCH 746
2024-01-31 20:46:52,710 Epoch 746: Total Training Recognition Loss 1102.64  Total Training Translation Loss 3487.70 
2024-01-31 20:46:52,710 EPOCH 747
2024-01-31 20:47:10,265 Epoch 747: Total Training Recognition Loss 1103.00  Total Training Translation Loss 3487.10 
2024-01-31 20:47:10,265 EPOCH 748
2024-01-31 20:47:11,255 [Epoch: 748 Step: 00025400] Batch Recognition Loss:  40.462883 => Gls Tokens per Sec:      647 || Batch Translation Loss: 115.021072 => Txt Tokens per Sec:     1692 || Lr: 0.000100
2024-01-31 20:47:27,800 Epoch 748: Total Training Recognition Loss 1102.06  Total Training Translation Loss 3487.41 
2024-01-31 20:47:27,801 EPOCH 749
2024-01-31 20:47:45,271 Epoch 749: Total Training Recognition Loss 1100.03  Total Training Translation Loss 3486.34 
2024-01-31 20:47:45,272 EPOCH 750
2024-01-31 20:48:02,762 [Epoch: 750 Step: 00025500] Batch Recognition Loss:  32.253746 => Gls Tokens per Sec:      608 || Batch Translation Loss: 114.541580 => Txt Tokens per Sec:     1687 || Lr: 0.000100
2024-01-31 20:48:02,762 Epoch 750: Total Training Recognition Loss 1100.73  Total Training Translation Loss 3487.04 
2024-01-31 20:48:02,762 EPOCH 751
2024-01-31 20:48:20,209 Epoch 751: Total Training Recognition Loss 1099.82  Total Training Translation Loss 3487.08 
2024-01-31 20:48:20,209 EPOCH 752
2024-01-31 20:48:37,836 Epoch 752: Total Training Recognition Loss 1101.86  Total Training Translation Loss 3487.03 
2024-01-31 20:48:37,836 EPOCH 753
2024-01-31 20:48:53,757 [Epoch: 753 Step: 00025600] Batch Recognition Loss:  33.872902 => Gls Tokens per Sec:      628 || Batch Translation Loss: 102.057983 => Txt Tokens per Sec:     1720 || Lr: 0.000100
2024-01-31 20:48:55,363 Epoch 753: Total Training Recognition Loss 1101.55  Total Training Translation Loss 3487.43 
2024-01-31 20:48:55,364 EPOCH 754
2024-01-31 20:49:12,915 Epoch 754: Total Training Recognition Loss 1103.21  Total Training Translation Loss 3487.26 
2024-01-31 20:49:12,916 EPOCH 755
2024-01-31 20:49:30,393 Epoch 755: Total Training Recognition Loss 1101.35  Total Training Translation Loss 3486.78 
2024-01-31 20:49:30,394 EPOCH 756
2024-01-31 20:49:46,397 [Epoch: 756 Step: 00025700] Batch Recognition Loss:  23.485523 => Gls Tokens per Sec:      600 || Batch Translation Loss:  99.967606 => Txt Tokens per Sec:     1690 || Lr: 0.000100
2024-01-31 20:49:48,010 Epoch 756: Total Training Recognition Loss 1102.02  Total Training Translation Loss 3486.87 
2024-01-31 20:49:48,010 EPOCH 757
2024-01-31 20:50:05,534 Epoch 757: Total Training Recognition Loss 1101.56  Total Training Translation Loss 3487.91 
2024-01-31 20:50:05,534 EPOCH 758
2024-01-31 20:50:23,068 Epoch 758: Total Training Recognition Loss 1100.43  Total Training Translation Loss 3487.41 
2024-01-31 20:50:23,069 EPOCH 759
2024-01-31 20:50:38,247 [Epoch: 759 Step: 00025800] Batch Recognition Loss:  36.727921 => Gls Tokens per Sec:      590 || Batch Translation Loss: 118.227219 => Txt Tokens per Sec:     1664 || Lr: 0.000100
2024-01-31 20:50:40,618 Epoch 759: Total Training Recognition Loss 1097.95  Total Training Translation Loss 3487.60 
2024-01-31 20:50:40,618 EPOCH 760
2024-01-31 20:50:58,082 Epoch 760: Total Training Recognition Loss 1102.43  Total Training Translation Loss 3487.06 
2024-01-31 20:50:58,082 EPOCH 761
2024-01-31 20:51:15,606 Epoch 761: Total Training Recognition Loss 1100.34  Total Training Translation Loss 3486.01 
2024-01-31 20:51:15,606 EPOCH 762
2024-01-31 20:51:29,212 [Epoch: 762 Step: 00025900] Batch Recognition Loss:  38.472191 => Gls Tokens per Sec:      593 || Batch Translation Loss: 118.294861 => Txt Tokens per Sec:     1636 || Lr: 0.000100
2024-01-31 20:51:33,161 Epoch 762: Total Training Recognition Loss 1098.79  Total Training Translation Loss 3486.79 
2024-01-31 20:51:33,162 EPOCH 763
2024-01-31 20:51:53,227 Epoch 763: Total Training Recognition Loss 1099.23  Total Training Translation Loss 3487.83 
2024-01-31 20:51:53,228 EPOCH 764
2024-01-31 20:52:10,907 Epoch 764: Total Training Recognition Loss 1101.69  Total Training Translation Loss 3487.09 
2024-01-31 20:52:10,908 EPOCH 765
2024-01-31 20:52:24,068 [Epoch: 765 Step: 00026000] Batch Recognition Loss:  17.278877 => Gls Tokens per Sec:      565 || Batch Translation Loss:  76.872574 => Txt Tokens per Sec:     1576 || Lr: 0.000100
2024-01-31 20:52:48,140 Validation result at epoch 765, step    26000: duration: 24.0725s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 343.99899	Translation Loss: 73505.28125	PPL: 1565.05603
	Eval Metric: BLEU
	WER 511.79	(DEL: 4.80,	INS: 423.23,	SUB: 83.76)
	BLEU-4 0.00	(BLEU-1: 0.01,	BLEU-2: 0.00,	BLEU-3: 0.00,	BLEU-4: 0.00)
	CHRF 3.50	ROUGE 0.02
2024-01-31 20:52:48,142 Logging Recognition and Translation Outputs
2024-01-31 20:52:48,142 ========================================================================================================================
2024-01-31 20:52:48,142 Logging Sequence: 130_139.00
2024-01-31 20:52:48,142 	Gloss Reference :	***** ***** ******* ***** *** ***** * ***** ********************* ***** *** ***** * ***** ******************* ***** ***************** A     B+C+D+E        
2024-01-31 20:52:48,143 	Gloss Hypothesis:	<unk> <pad> B+D+E+B <unk> E+B <unk> D <unk> E+B+E+B+E+D+E+D+C+B+D <unk> B+E <unk> E <unk> B+C+B+E+B+C+B+C+B+E <unk> B+E+B+E+C+E+C+E+B <unk> E+B+E+B+E+B+E+C
2024-01-31 20:52:48,143 	Gloss Alignment :	I     I     I       I     I   I     I I     I                     I     I   I     I I     I                   I     I                 S     S              
2024-01-31 20:52:48,143 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 20:52:48,149 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** *** *** he  shared a   picture of  a   little pouch he  knit for his olympic gold medal with uk  flag on  one side and japanese flag on  the other
2024-01-31 20:52:48,149 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s>    <s> <s>     <s> <s> <s>    <s>   <s> <s>  <s> <s> <s>     <s>  <s>   <s>  <s> <s>  <s> <s> <s>  <s> <s>      <s>  <s> <s> <s>  
2024-01-31 20:52:48,149 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I   I   S   S      S   S       S   S   S      S     S   S    S   S   S       S    S     S    S   S    S   S   S    S   S        S    S   S   S    
2024-01-31 20:52:48,149 ========================================================================================================================
2024-01-31 20:52:48,149 Logging Sequence: 72_194.00
2024-01-31 20:52:48,149 	Gloss Reference :	* ***** * ***** * ***** A ***** B+C+D+E
2024-01-31 20:52:48,149 	Gloss Hypothesis:	A <unk> A <unk> A <unk> A <unk> E+A    
2024-01-31 20:52:48,150 	Gloss Alignment :	I I     I I     I I       I     S      
2024-01-31 20:52:48,150 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 20:52:48,153 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** shah told her to  do  what she wants and filed a   police complaint against her
2024-01-31 20:52:48,153 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s>  <s>  <s> <s> <s> <s>  <s> <s>   <s> <s>   <s> <s>    <s>       <s>     <s>
2024-01-31 20:52:48,153 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   S    S    S   S   S   S    S   S     S   S     S   S      S         S       S  
2024-01-31 20:52:48,153 ========================================================================================================================
2024-01-31 20:52:48,153 Logging Sequence: 69_177.00
2024-01-31 20:52:48,154 	Gloss Reference :	* A B+C+D+E
2024-01-31 20:52:48,154 	Gloss Hypothesis:	E A *******
2024-01-31 20:52:48,154 	Gloss Alignment :	I   D      
2024-01-31 20:52:48,154 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 20:52:48,158 	Text Reference  :	*** *** *** *** *** *** *** *** *** ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* he      said    'i      will    continue playing i       know    it's    about   time    i       retire  i       also    have    a       knee    condition
2024-01-31 20:52:48,158 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing  nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing  
2024-01-31 20:52:48,158 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I       I       I       I       I       I       I       I       I       I       I       I       S       S       S       S       S        S       S       S       S       S       S       S       S       S       S       S       S       S       S        
2024-01-31 20:52:48,158 ========================================================================================================================
2024-01-31 20:52:48,158 Logging Sequence: 95_118.00
2024-01-31 20:52:48,159 	Gloss Reference :	* ***** ***** ***** ***** ***** ***** ***** ***** *********** ***** *** ***** * ***** * ***** A ***** * ***** * ***** ***** * ***** B+C+D+E    
2024-01-31 20:52:48,159 	Gloss Hypothesis:	E <pad> <unk> <pad> <unk> A+C+A <unk> B+C+A <unk> E+C+B+E+C+E <unk> E+A <unk> C <unk> C <unk> A <unk> E <unk> C <pad> <unk> E <unk> B+C+B+E+B+C
2024-01-31 20:52:48,159 	Gloss Alignment :	I I     I     I     I     I     I     I     I     I           I     I   I     I I     I I       I     I I     I I     I     I I     S          
2024-01-31 20:52:48,159 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 20:52:48,161 	Text Reference  :	*** *** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** the  game was  stopped strangely due  to   excessive sunlight
2024-01-31 20:52:48,162 	Text Hypothesis :	<s> <s> iker iker iker iker iker iker iker iker iker iker iker iker iker iker iker iker iker iker iker iker iker iker iker iker iker iker iker iker iker iker iker iker iker    iker      iker iker iker      iker    
2024-01-31 20:52:48,162 	Text Alignment  :	I   I   I    I    I    I    I    I    I    I    I    I    I    I    I    I    I    I    I    I    I    I    I    I    I    I    I    I    I    I    I    S    S    S    S       S         S    S    S         S       
2024-01-31 20:52:48,162 ========================================================================================================================
2024-01-31 20:52:48,162 Logging Sequence: 112_8.00
2024-01-31 20:52:48,162 	Gloss Reference :	A B+C+D+E
2024-01-31 20:52:48,162 	Gloss Hypothesis:	* E      
2024-01-31 20:52:48,162 	Gloss Alignment :	D S      
2024-01-31 20:52:48,162 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 20:52:48,167 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** ****** ****** ****** ****** ****** ****** before there  were   8      teams  such   as     mumbai indians delhi  capitals punjab kings  etc    and    now    there  will   be     10     teams  in     2022  
2024-01-31 20:52:48,167 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> boards boards boards boards boards boards boards boards boards boards boards boards boards boards boards  boards boards   boards boards boards boards boards boards boards boards boards boards boards boards
2024-01-31 20:52:48,167 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I      I      I      I      I      I      S      S      S      S      S      S      S      S      S       S      S        S      S      S      S      S      S      S      S      S      S      S      S     
2024-01-31 20:52:48,167 ========================================================================================================================
2024-01-31 20:52:52,341 Epoch 765: Total Training Recognition Loss 1101.02  Total Training Translation Loss 3486.64 
2024-01-31 20:52:52,341 EPOCH 766
2024-01-31 20:53:09,810 Epoch 766: Total Training Recognition Loss 1099.12  Total Training Translation Loss 3488.32 
2024-01-31 20:53:09,810 EPOCH 767
2024-01-31 20:53:27,075 Epoch 767: Total Training Recognition Loss 1101.53  Total Training Translation Loss 3487.54 
2024-01-31 20:53:27,075 EPOCH 768
2024-01-31 20:53:38,473 [Epoch: 768 Step: 00026100] Batch Recognition Loss:  10.908238 => Gls Tokens per Sec:      618 || Batch Translation Loss:  62.437973 => Txt Tokens per Sec:     1681 || Lr: 0.000100
2024-01-31 20:53:44,442 Epoch 768: Total Training Recognition Loss 1099.84  Total Training Translation Loss 3487.20 
2024-01-31 20:53:44,443 EPOCH 769
2024-01-31 20:54:01,758 Epoch 769: Total Training Recognition Loss 1101.04  Total Training Translation Loss 3486.65 
2024-01-31 20:54:01,758 EPOCH 770
2024-01-31 20:54:19,143 Epoch 770: Total Training Recognition Loss 1100.57  Total Training Translation Loss 3487.70 
2024-01-31 20:54:19,143 EPOCH 771
2024-01-31 20:54:30,658 [Epoch: 771 Step: 00026200] Batch Recognition Loss:  46.233067 => Gls Tokens per Sec:      534 || Batch Translation Loss: 124.829559 => Txt Tokens per Sec:     1514 || Lr: 0.000100
2024-01-31 20:54:36,486 Epoch 771: Total Training Recognition Loss 1103.54  Total Training Translation Loss 3487.34 
2024-01-31 20:54:36,486 EPOCH 772
2024-01-31 20:54:53,891 Epoch 772: Total Training Recognition Loss 1101.67  Total Training Translation Loss 3487.06 
2024-01-31 20:54:53,891 EPOCH 773
2024-01-31 20:55:11,348 Epoch 773: Total Training Recognition Loss 1100.68  Total Training Translation Loss 3486.54 
2024-01-31 20:55:11,348 EPOCH 774
2024-01-31 20:55:20,415 [Epoch: 774 Step: 00026300] Batch Recognition Loss:  37.158463 => Gls Tokens per Sec:      608 || Batch Translation Loss: 101.830330 => Txt Tokens per Sec:     1687 || Lr: 0.000100
2024-01-31 20:55:28,699 Epoch 774: Total Training Recognition Loss 1098.53  Total Training Translation Loss 3487.76 
2024-01-31 20:55:28,699 EPOCH 775
2024-01-31 20:55:46,116 Epoch 775: Total Training Recognition Loss 1098.45  Total Training Translation Loss 3486.93 
2024-01-31 20:55:46,116 EPOCH 776
2024-01-31 20:56:03,633 Epoch 776: Total Training Recognition Loss 1101.02  Total Training Translation Loss 3487.25 
2024-01-31 20:56:03,633 EPOCH 777
2024-01-31 20:56:12,677 [Epoch: 777 Step: 00026400] Batch Recognition Loss:  37.893131 => Gls Tokens per Sec:      539 || Batch Translation Loss: 113.016495 => Txt Tokens per Sec:     1552 || Lr: 0.000100
2024-01-31 20:56:21,037 Epoch 777: Total Training Recognition Loss 1098.66  Total Training Translation Loss 3486.37 
2024-01-31 20:56:21,037 EPOCH 778
2024-01-31 20:56:38,401 Epoch 778: Total Training Recognition Loss 1101.66  Total Training Translation Loss 3487.68 
2024-01-31 20:56:38,401 EPOCH 779
2024-01-31 20:56:55,846 Epoch 779: Total Training Recognition Loss 1102.46  Total Training Translation Loss 3487.01 
2024-01-31 20:56:55,847 EPOCH 780
2024-01-31 20:57:04,059 [Epoch: 780 Step: 00026500] Batch Recognition Loss:  45.908440 => Gls Tokens per Sec:      515 || Batch Translation Loss: 125.420105 => Txt Tokens per Sec:     1482 || Lr: 0.000100
2024-01-31 20:57:13,299 Epoch 780: Total Training Recognition Loss 1102.57  Total Training Translation Loss 3487.11 
2024-01-31 20:57:13,300 EPOCH 781
2024-01-31 20:57:30,717 Epoch 781: Total Training Recognition Loss 1098.96  Total Training Translation Loss 3486.73 
2024-01-31 20:57:30,718 EPOCH 782
2024-01-31 20:57:48,229 Epoch 782: Total Training Recognition Loss 1099.93  Total Training Translation Loss 3486.87 
2024-01-31 20:57:48,229 EPOCH 783
2024-01-31 20:57:54,080 [Epoch: 783 Step: 00026600] Batch Recognition Loss:  18.637011 => Gls Tokens per Sec:      656 || Batch Translation Loss:  91.023376 => Txt Tokens per Sec:     1861 || Lr: 0.000100
2024-01-31 20:58:05,695 Epoch 783: Total Training Recognition Loss 1102.09  Total Training Translation Loss 3487.87 
2024-01-31 20:58:05,695 EPOCH 784
2024-01-31 20:58:23,171 Epoch 784: Total Training Recognition Loss 1100.17  Total Training Translation Loss 3487.30 
2024-01-31 20:58:23,171 EPOCH 785
2024-01-31 20:58:40,666 Epoch 785: Total Training Recognition Loss 1101.72  Total Training Translation Loss 3487.20 
2024-01-31 20:58:40,666 EPOCH 786
2024-01-31 20:58:45,249 [Epoch: 786 Step: 00026700] Batch Recognition Loss:  39.861992 => Gls Tokens per Sec:      644 || Batch Translation Loss: 116.283524 => Txt Tokens per Sec:     1688 || Lr: 0.000100
2024-01-31 20:58:58,270 Epoch 786: Total Training Recognition Loss 1100.15  Total Training Translation Loss 3486.91 
2024-01-31 20:58:58,270 EPOCH 787
2024-01-31 20:59:15,755 Epoch 787: Total Training Recognition Loss 1101.05  Total Training Translation Loss 3487.93 
2024-01-31 20:59:15,756 EPOCH 788
2024-01-31 20:59:33,255 Epoch 788: Total Training Recognition Loss 1100.97  Total Training Translation Loss 3487.43 
2024-01-31 20:59:33,255 EPOCH 789
2024-01-31 20:59:36,916 [Epoch: 789 Step: 00026800] Batch Recognition Loss:  44.398041 => Gls Tokens per Sec:      700 || Batch Translation Loss: 130.687897 => Txt Tokens per Sec:     1980 || Lr: 0.000100
2024-01-31 20:59:50,732 Epoch 789: Total Training Recognition Loss 1101.48  Total Training Translation Loss 3488.26 
2024-01-31 20:59:50,732 EPOCH 790
2024-01-31 21:00:08,319 Epoch 790: Total Training Recognition Loss 1099.06  Total Training Translation Loss 3487.73 
2024-01-31 21:00:08,320 EPOCH 791
2024-01-31 21:00:25,923 Epoch 791: Total Training Recognition Loss 1102.03  Total Training Translation Loss 3487.03 
2024-01-31 21:00:25,923 EPOCH 792
2024-01-31 21:00:28,210 [Epoch: 792 Step: 00026900] Batch Recognition Loss:  23.094227 => Gls Tokens per Sec:      840 || Batch Translation Loss:  82.137337 => Txt Tokens per Sec:     2056 || Lr: 0.000100
2024-01-31 21:00:43,558 Epoch 792: Total Training Recognition Loss 1099.15  Total Training Translation Loss 3486.23 
2024-01-31 21:00:43,559 EPOCH 793
2024-01-31 21:01:01,017 Epoch 793: Total Training Recognition Loss 1101.49  Total Training Translation Loss 3486.56 
2024-01-31 21:01:01,017 EPOCH 794
2024-01-31 21:01:18,635 Epoch 794: Total Training Recognition Loss 1101.47  Total Training Translation Loss 3486.86 
2024-01-31 21:01:18,636 EPOCH 795
2024-01-31 21:01:20,195 [Epoch: 795 Step: 00027000] Batch Recognition Loss:  30.136133 => Gls Tokens per Sec:      822 || Batch Translation Loss: 103.663048 => Txt Tokens per Sec:     2001 || Lr: 0.000100
2024-01-31 21:01:36,222 Epoch 795: Total Training Recognition Loss 1099.45  Total Training Translation Loss 3487.67 
2024-01-31 21:01:36,222 EPOCH 796
2024-01-31 21:01:53,651 Epoch 796: Total Training Recognition Loss 1101.41  Total Training Translation Loss 3488.12 
2024-01-31 21:01:53,652 EPOCH 797
2024-01-31 21:02:11,097 Epoch 797: Total Training Recognition Loss 1099.07  Total Training Translation Loss 3486.66 
2024-01-31 21:02:11,097 EPOCH 798
2024-01-31 21:02:11,773 [Epoch: 798 Step: 00027100] Batch Recognition Loss:  30.430204 => Gls Tokens per Sec:      949 || Batch Translation Loss: 102.695580 => Txt Tokens per Sec:     2445 || Lr: 0.000100
2024-01-31 21:02:28,681 Epoch 798: Total Training Recognition Loss 1101.27  Total Training Translation Loss 3487.21 
2024-01-31 21:02:28,682 EPOCH 799
2024-01-31 21:02:46,174 Epoch 799: Total Training Recognition Loss 1101.23  Total Training Translation Loss 3487.40 
2024-01-31 21:02:46,174 EPOCH 800
2024-01-31 21:03:03,798 [Epoch: 800 Step: 00027200] Batch Recognition Loss:  32.881386 => Gls Tokens per Sec:      603 || Batch Translation Loss: 116.506081 => Txt Tokens per Sec:     1675 || Lr: 0.000100
2024-01-31 21:03:03,798 Epoch 800: Total Training Recognition Loss 1100.26  Total Training Translation Loss 3488.00 
2024-01-31 21:03:03,798 EPOCH 801
2024-01-31 21:03:21,315 Epoch 801: Total Training Recognition Loss 1100.88  Total Training Translation Loss 3487.13 
2024-01-31 21:03:21,315 EPOCH 802
2024-01-31 21:03:38,892 Epoch 802: Total Training Recognition Loss 1101.83  Total Training Translation Loss 3487.41 
2024-01-31 21:03:38,892 EPOCH 803
2024-01-31 21:03:55,659 [Epoch: 803 Step: 00027300] Batch Recognition Loss:  23.675648 => Gls Tokens per Sec:      596 || Batch Translation Loss:  97.343567 => Txt Tokens per Sec:     1664 || Lr: 0.000100
2024-01-31 21:03:56,339 Epoch 803: Total Training Recognition Loss 1100.03  Total Training Translation Loss 3487.82 
2024-01-31 21:03:56,339 EPOCH 804
2024-01-31 21:04:13,909 Epoch 804: Total Training Recognition Loss 1100.34  Total Training Translation Loss 3487.76 
2024-01-31 21:04:13,909 EPOCH 805
2024-01-31 21:04:31,371 Epoch 805: Total Training Recognition Loss 1102.47  Total Training Translation Loss 3487.00 
2024-01-31 21:04:31,371 EPOCH 806
2024-01-31 21:04:47,103 [Epoch: 806 Step: 00027400] Batch Recognition Loss:  11.049453 => Gls Tokens per Sec:      594 || Batch Translation Loss:  62.933670 => Txt Tokens per Sec:     1657 || Lr: 0.000100
2024-01-31 21:04:49,102 Epoch 806: Total Training Recognition Loss 1101.74  Total Training Translation Loss 3486.54 
2024-01-31 21:04:49,102 EPOCH 807
2024-01-31 21:05:06,567 Epoch 807: Total Training Recognition Loss 1101.82  Total Training Translation Loss 3488.17 
2024-01-31 21:05:06,567 EPOCH 808
2024-01-31 21:05:24,313 Epoch 808: Total Training Recognition Loss 1098.72  Total Training Translation Loss 3487.73 
2024-01-31 21:05:24,314 EPOCH 809
2024-01-31 21:05:38,968 [Epoch: 809 Step: 00027500] Batch Recognition Loss:  44.181946 => Gls Tokens per Sec:      611 || Batch Translation Loss: 131.767258 => Txt Tokens per Sec:     1713 || Lr: 0.000100
2024-01-31 21:05:41,917 Epoch 809: Total Training Recognition Loss 1101.14  Total Training Translation Loss 3487.50 
2024-01-31 21:05:41,918 EPOCH 810
2024-01-31 21:05:59,442 Epoch 810: Total Training Recognition Loss 1101.13  Total Training Translation Loss 3486.64 
2024-01-31 21:05:59,443 EPOCH 811
2024-01-31 21:06:17,185 Epoch 811: Total Training Recognition Loss 1101.97  Total Training Translation Loss 3486.87 
2024-01-31 21:06:17,185 EPOCH 812
2024-01-31 21:06:30,566 [Epoch: 812 Step: 00027600] Batch Recognition Loss:  27.195089 => Gls Tokens per Sec:      603 || Batch Translation Loss:  99.026855 => Txt Tokens per Sec:     1653 || Lr: 0.000100
2024-01-31 21:06:34,671 Epoch 812: Total Training Recognition Loss 1102.28  Total Training Translation Loss 3486.65 
2024-01-31 21:06:34,671 EPOCH 813
2024-01-31 21:06:52,239 Epoch 813: Total Training Recognition Loss 1101.75  Total Training Translation Loss 3487.03 
2024-01-31 21:06:52,239 EPOCH 814
2024-01-31 21:07:09,819 Epoch 814: Total Training Recognition Loss 1099.82  Total Training Translation Loss 3487.76 
2024-01-31 21:07:09,819 EPOCH 815
2024-01-31 21:07:21,699 [Epoch: 815 Step: 00027700] Batch Recognition Loss:  29.902084 => Gls Tokens per Sec:      626 || Batch Translation Loss: 100.984756 => Txt Tokens per Sec:     1730 || Lr: 0.000100
2024-01-31 21:07:27,380 Epoch 815: Total Training Recognition Loss 1100.02  Total Training Translation Loss 3487.29 
2024-01-31 21:07:27,380 EPOCH 816
2024-01-31 21:07:44,869 Epoch 816: Total Training Recognition Loss 1101.07  Total Training Translation Loss 3486.95 
2024-01-31 21:07:44,870 EPOCH 817
2024-01-31 21:08:02,451 Epoch 817: Total Training Recognition Loss 1100.98  Total Training Translation Loss 3486.32 
2024-01-31 21:08:02,451 EPOCH 818
2024-01-31 21:08:13,551 [Epoch: 818 Step: 00027800] Batch Recognition Loss:  30.732462 => Gls Tokens per Sec:      612 || Batch Translation Loss: 100.497742 => Txt Tokens per Sec:     1718 || Lr: 0.000100
2024-01-31 21:08:20,065 Epoch 818: Total Training Recognition Loss 1100.50  Total Training Translation Loss 3488.28 
2024-01-31 21:08:20,065 EPOCH 819
2024-01-31 21:08:37,574 Epoch 819: Total Training Recognition Loss 1101.62  Total Training Translation Loss 3487.01 
2024-01-31 21:08:37,574 EPOCH 820
2024-01-31 21:08:55,109 Epoch 820: Total Training Recognition Loss 1101.19  Total Training Translation Loss 3487.91 
2024-01-31 21:08:55,109 EPOCH 821
2024-01-31 21:09:06,154 [Epoch: 821 Step: 00027900] Batch Recognition Loss:  57.797752 => Gls Tokens per Sec:      579 || Batch Translation Loss: 132.231400 => Txt Tokens per Sec:     1626 || Lr: 0.000100
2024-01-31 21:09:12,684 Epoch 821: Total Training Recognition Loss 1101.31  Total Training Translation Loss 3487.28 
2024-01-31 21:09:12,684 EPOCH 822
2024-01-31 21:09:31,640 Epoch 822: Total Training Recognition Loss 1098.46  Total Training Translation Loss 3486.73 
2024-01-31 21:09:31,640 EPOCH 823
2024-01-31 21:09:49,365 Epoch 823: Total Training Recognition Loss 1100.22  Total Training Translation Loss 3488.03 
2024-01-31 21:09:49,365 EPOCH 824
2024-01-31 21:09:56,977 [Epoch: 824 Step: 00028000] Batch Recognition Loss:  21.037176 => Gls Tokens per Sec:      724 || Batch Translation Loss:  90.724548 => Txt Tokens per Sec:     1897 || Lr: 0.000100
2024-01-31 21:10:21,119 Validation result at epoch 824, step    28000: duration: 24.1420s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 350.61285	Translation Loss: 73480.17188	PPL: 1561.12866
	Eval Metric: BLEU
	WER 538.21	(DEL: 4.03,	INS: 449.15,	SUB: 85.03)
	BLEU-4 0.00	(BLEU-1: 0.01,	BLEU-2: 0.00,	BLEU-3: 0.00,	BLEU-4: 0.00)
	CHRF 3.51	ROUGE 0.02
2024-01-31 21:10:21,120 Logging Recognition and Translation Outputs
2024-01-31 21:10:21,120 ========================================================================================================================
2024-01-31 21:10:21,120 Logging Sequence: 67_98.00
2024-01-31 21:10:21,121 	Gloss Reference :	* A     B+C+D+E                                
2024-01-31 21:10:21,121 	Gloss Hypothesis:	E <pad> C+E+B+E+B+E+C+E+C+E+C+E+C+E+C+E+C+E+B+E
2024-01-31 21:10:21,121 	Gloss Alignment :	I S     S                                      
2024-01-31 21:10:21,121 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 21:10:21,124 	Text Reference  :	*** *** *** *** ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* it            saddens       me            to            see           people        suffering     and           dying         due           to            lack          of            oxygen       
2024-01-31 21:10:21,124 	Text Hypothesis :	<s> <s> <s> <s> misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand
2024-01-31 21:10:21,124 	Text Alignment  :	I   I   I   I   I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             S             S             S             S             S             S             S             S             S             S             S             S             S             S            
2024-01-31 21:10:21,124 ========================================================================================================================
2024-01-31 21:10:21,125 Logging Sequence: 157_83.00
2024-01-31 21:10:21,125 	Gloss Reference :	A B+C+D+E
2024-01-31 21:10:21,125 	Gloss Hypothesis:	* E      
2024-01-31 21:10:21,125 	Gloss Alignment :	D S      
2024-01-31 21:10:21,125 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 21:10:21,129 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** ************* ************* ************* ************* ************* ************* ************* ************* ************* also          when          you           eat           sandwich      at            a             streetside    hawker        or            stall         the           sandwich      maker         will          first         apply         butter        with          a             knife        
2024-01-31 21:10:21,130 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions
2024-01-31 21:10:21,130 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I             I             I             I             I             I             I             I             I             S             S             S             S             S             S             S             S             S             S             S             S             S             S             S             S             S             S             S             S             S            
2024-01-31 21:10:21,130 ========================================================================================================================
2024-01-31 21:10:21,130 Logging Sequence: 76_35.00
2024-01-31 21:10:21,130 	Gloss Reference :	***** ***** ***** A ***** * ***** * ***** *** ***** *** ***** * ***** B+C+D+E
2024-01-31 21:10:21,131 	Gloss Hypothesis:	<pad> <unk> <pad> A <unk> E <unk> E <unk> B+E <unk> A+E <unk> E <unk> E      
2024-01-31 21:10:21,131 	Gloss Alignment :	I     I     I       I     I I     I I     I   I     I   I     I I     S      
2024-01-31 21:10:21,131 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 21:10:21,133 	Text Reference  :	*** *** *** ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* bcci          president     sourav        ganguly       along         with          board         secretary     jay           shah         
2024-01-31 21:10:21,133 	Text Hypothesis :	<s> <s> <s> misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand
2024-01-31 21:10:21,133 	Text Alignment  :	I   I   I   I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             S             S             S             S             S             S             S             S             S             S            
2024-01-31 21:10:21,133 ========================================================================================================================
2024-01-31 21:10:21,134 Logging Sequence: 139_180.00
2024-01-31 21:10:21,134 	Gloss Reference :	***** * ***** A ***** *** ***** *** ***** B+C+D+E
2024-01-31 21:10:21,134 	Gloss Hypothesis:	<unk> E <unk> A <unk> A+E <unk> A+E <unk> C      
2024-01-31 21:10:21,134 	Gloss Alignment :	I     I I       I     I   I     I   I     S      
2024-01-31 21:10:21,134 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 21:10:21,136 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* netherlands also    faced   similar riots  
2024-01-31 21:10:21,136 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing     nothing nothing nothing nothing
2024-01-31 21:10:21,136 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       S           S       S       S       S      
2024-01-31 21:10:21,136 ========================================================================================================================
2024-01-31 21:10:21,136 Logging Sequence: 98_87.00
2024-01-31 21:10:21,136 	Gloss Reference :	* ***** ******************* ***** ***** ***** * ***** * ***** * ***** ********* A     B+C+D+E  
2024-01-31 21:10:21,137 	Gloss Hypothesis:	E <unk> C+E+C+E+C+E+B+E+C+E <unk> C+E+C <unk> E <unk> E <unk> E <unk> C+B+E+B+C <unk> C+E+B+C+E
2024-01-31 21:10:21,137 	Gloss Alignment :	I I     I                   I     I     I     I I     I I     I I     I         S     S        
2024-01-31 21:10:21,137 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 21:10:21,140 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** instead of  starting afresh in  2021 the organizers opted to  resume with the previous edition
2024-01-31 21:10:21,140 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s>     <s> <s>      <s>    <s> <s>  <s> <s>        <s>   <s> <s>    <s>  <s> <s>      <s>    
2024-01-31 21:10:21,140 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   S       S   S        S      S   S    S   S          S     S   S      S    S   S        S      
2024-01-31 21:10:21,140 ========================================================================================================================
2024-01-31 21:10:31,266 Epoch 824: Total Training Recognition Loss 1099.38  Total Training Translation Loss 3486.90 
2024-01-31 21:10:31,266 EPOCH 825
2024-01-31 21:10:49,013 Epoch 825: Total Training Recognition Loss 1100.21  Total Training Translation Loss 3486.48 
2024-01-31 21:10:49,014 EPOCH 826
2024-01-31 21:11:06,567 Epoch 826: Total Training Recognition Loss 1100.23  Total Training Translation Loss 3486.35 
2024-01-31 21:11:06,567 EPOCH 827
2024-01-31 21:11:16,512 [Epoch: 827 Step: 00028100] Batch Recognition Loss:  31.279694 => Gls Tokens per Sec:      490 || Batch Translation Loss:  99.189972 => Txt Tokens per Sec:     1452 || Lr: 0.000050
2024-01-31 21:11:24,074 Epoch 827: Total Training Recognition Loss 1102.21  Total Training Translation Loss 3487.84 
2024-01-31 21:11:24,074 EPOCH 828
2024-01-31 21:11:41,983 Epoch 828: Total Training Recognition Loss 1098.27  Total Training Translation Loss 3487.23 
2024-01-31 21:11:41,984 EPOCH 829
2024-01-31 21:11:59,528 Epoch 829: Total Training Recognition Loss 1099.73  Total Training Translation Loss 3487.88 
2024-01-31 21:11:59,529 EPOCH 830
2024-01-31 21:12:05,978 [Epoch: 830 Step: 00028200] Batch Recognition Loss:  23.466816 => Gls Tokens per Sec:      656 || Batch Translation Loss:  96.074478 => Txt Tokens per Sec:     1800 || Lr: 0.000050
2024-01-31 21:12:17,076 Epoch 830: Total Training Recognition Loss 1101.22  Total Training Translation Loss 3487.06 
2024-01-31 21:12:17,077 EPOCH 831
2024-01-31 21:12:34,586 Epoch 831: Total Training Recognition Loss 1102.37  Total Training Translation Loss 3486.98 
2024-01-31 21:12:34,586 EPOCH 832
2024-01-31 21:12:52,224 Epoch 832: Total Training Recognition Loss 1100.94  Total Training Translation Loss 3486.28 
2024-01-31 21:12:52,225 EPOCH 833
2024-01-31 21:12:57,067 [Epoch: 833 Step: 00028300] Batch Recognition Loss:  36.734451 => Gls Tokens per Sec:      793 || Batch Translation Loss: 107.709297 => Txt Tokens per Sec:     2114 || Lr: 0.000050
2024-01-31 21:13:09,933 Epoch 833: Total Training Recognition Loss 1100.42  Total Training Translation Loss 3487.21 
2024-01-31 21:13:09,934 EPOCH 834
2024-01-31 21:13:27,693 Epoch 834: Total Training Recognition Loss 1101.24  Total Training Translation Loss 3486.91 
2024-01-31 21:13:27,693 EPOCH 835
2024-01-31 21:13:45,465 Epoch 835: Total Training Recognition Loss 1099.19  Total Training Translation Loss 3486.89 
2024-01-31 21:13:45,465 EPOCH 836
2024-01-31 21:13:49,865 [Epoch: 836 Step: 00028400] Batch Recognition Loss:  30.282913 => Gls Tokens per Sec:      727 || Batch Translation Loss:  97.363991 => Txt Tokens per Sec:     1936 || Lr: 0.000050
2024-01-31 21:14:03,131 Epoch 836: Total Training Recognition Loss 1098.20  Total Training Translation Loss 3487.93 
2024-01-31 21:14:03,131 EPOCH 837
2024-01-31 21:14:20,867 Epoch 837: Total Training Recognition Loss 1101.85  Total Training Translation Loss 3486.69 
2024-01-31 21:14:20,867 EPOCH 838
2024-01-31 21:14:38,478 Epoch 838: Total Training Recognition Loss 1100.14  Total Training Translation Loss 3487.18 
2024-01-31 21:14:38,479 EPOCH 839
2024-01-31 21:14:41,953 [Epoch: 839 Step: 00028500] Batch Recognition Loss:  33.913635 => Gls Tokens per Sec:      737 || Batch Translation Loss:  98.996613 => Txt Tokens per Sec:     2020 || Lr: 0.000050
2024-01-31 21:14:56,219 Epoch 839: Total Training Recognition Loss 1099.92  Total Training Translation Loss 3487.10 
2024-01-31 21:14:56,219 EPOCH 840
2024-01-31 21:15:13,793 Epoch 840: Total Training Recognition Loss 1101.33  Total Training Translation Loss 3486.37 
2024-01-31 21:15:13,794 EPOCH 841
2024-01-31 21:15:31,367 Epoch 841: Total Training Recognition Loss 1103.34  Total Training Translation Loss 3487.91 
2024-01-31 21:15:31,368 EPOCH 842
2024-01-31 21:15:36,374 [Epoch: 842 Step: 00028600] Batch Recognition Loss:  27.302416 => Gls Tokens per Sec:      384 || Batch Translation Loss:  86.218140 => Txt Tokens per Sec:     1174 || Lr: 0.000050
2024-01-31 21:15:48,990 Epoch 842: Total Training Recognition Loss 1099.31  Total Training Translation Loss 3487.24 
2024-01-31 21:15:48,990 EPOCH 843
2024-01-31 21:16:06,606 Epoch 843: Total Training Recognition Loss 1099.70  Total Training Translation Loss 3487.54 
2024-01-31 21:16:06,606 EPOCH 844
2024-01-31 21:16:24,196 Epoch 844: Total Training Recognition Loss 1100.63  Total Training Translation Loss 3487.50 
2024-01-31 21:16:24,196 EPOCH 845
2024-01-31 21:16:26,101 [Epoch: 845 Step: 00028700] Batch Recognition Loss:  45.547672 => Gls Tokens per Sec:      672 || Batch Translation Loss: 124.838058 => Txt Tokens per Sec:     1748 || Lr: 0.000050
2024-01-31 21:16:41,887 Epoch 845: Total Training Recognition Loss 1100.94  Total Training Translation Loss 3487.20 
2024-01-31 21:16:41,887 EPOCH 846
2024-01-31 21:16:59,469 Epoch 846: Total Training Recognition Loss 1100.70  Total Training Translation Loss 3487.74 
2024-01-31 21:16:59,469 EPOCH 847
2024-01-31 21:17:17,140 Epoch 847: Total Training Recognition Loss 1098.26  Total Training Translation Loss 3487.25 
2024-01-31 21:17:17,141 EPOCH 848
2024-01-31 21:17:18,024 [Epoch: 848 Step: 00028800] Batch Recognition Loss:  38.471054 => Gls Tokens per Sec:      725 || Batch Translation Loss: 108.081665 => Txt Tokens per Sec:     2015 || Lr: 0.000050
2024-01-31 21:17:34,694 Epoch 848: Total Training Recognition Loss 1100.80  Total Training Translation Loss 3486.91 
2024-01-31 21:17:34,694 EPOCH 849
2024-01-31 21:17:52,285 Epoch 849: Total Training Recognition Loss 1101.00  Total Training Translation Loss 3487.22 
2024-01-31 21:17:52,285 EPOCH 850
2024-01-31 21:18:09,924 [Epoch: 850 Step: 00028900] Batch Recognition Loss:  36.730331 => Gls Tokens per Sec:      603 || Batch Translation Loss: 122.536758 => Txt Tokens per Sec:     1673 || Lr: 0.000050
2024-01-31 21:18:09,925 Epoch 850: Total Training Recognition Loss 1101.16  Total Training Translation Loss 3487.28 
2024-01-31 21:18:09,925 EPOCH 851
2024-01-31 21:18:27,567 Epoch 851: Total Training Recognition Loss 1101.30  Total Training Translation Loss 3487.63 
2024-01-31 21:18:27,568 EPOCH 852
2024-01-31 21:18:45,200 Epoch 852: Total Training Recognition Loss 1102.24  Total Training Translation Loss 3486.65 
2024-01-31 21:18:45,200 EPOCH 853
2024-01-31 21:19:02,082 [Epoch: 853 Step: 00029000] Batch Recognition Loss:  44.091908 => Gls Tokens per Sec:      592 || Batch Translation Loss: 130.528046 => Txt Tokens per Sec:     1649 || Lr: 0.000050
2024-01-31 21:19:02,804 Epoch 853: Total Training Recognition Loss 1100.65  Total Training Translation Loss 3487.90 
2024-01-31 21:19:02,805 EPOCH 854
2024-01-31 21:19:20,581 Epoch 854: Total Training Recognition Loss 1101.91  Total Training Translation Loss 3487.38 
2024-01-31 21:19:20,581 EPOCH 855
2024-01-31 21:19:38,252 Epoch 855: Total Training Recognition Loss 1100.49  Total Training Translation Loss 3487.20 
2024-01-31 21:19:38,252 EPOCH 856
2024-01-31 21:19:54,275 [Epoch: 856 Step: 00029100] Batch Recognition Loss:  58.018730 => Gls Tokens per Sec:      584 || Batch Translation Loss: 132.284821 => Txt Tokens per Sec:     1616 || Lr: 0.000050
2024-01-31 21:19:55,986 Epoch 856: Total Training Recognition Loss 1100.74  Total Training Translation Loss 3487.14 
2024-01-31 21:19:55,986 EPOCH 857
2024-01-31 21:20:13,581 Epoch 857: Total Training Recognition Loss 1101.09  Total Training Translation Loss 3486.97 
2024-01-31 21:20:13,581 EPOCH 858
2024-01-31 21:20:31,428 Epoch 858: Total Training Recognition Loss 1099.87  Total Training Translation Loss 3487.37 
2024-01-31 21:20:31,429 EPOCH 859
2024-01-31 21:20:45,768 [Epoch: 859 Step: 00029200] Batch Recognition Loss:  42.563637 => Gls Tokens per Sec:      607 || Batch Translation Loss: 129.001282 => Txt Tokens per Sec:     1643 || Lr: 0.000050
2024-01-31 21:20:49,276 Epoch 859: Total Training Recognition Loss 1101.50  Total Training Translation Loss 3488.02 
2024-01-31 21:20:49,276 EPOCH 860
2024-01-31 21:21:06,896 Epoch 860: Total Training Recognition Loss 1101.23  Total Training Translation Loss 3487.06 
2024-01-31 21:21:06,897 EPOCH 861
2024-01-31 21:21:24,474 Epoch 861: Total Training Recognition Loss 1102.07  Total Training Translation Loss 3487.15 
2024-01-31 21:21:24,474 EPOCH 862
2024-01-31 21:21:38,615 [Epoch: 862 Step: 00029300] Batch Recognition Loss:  33.664707 => Gls Tokens per Sec:      571 || Batch Translation Loss:  95.345200 => Txt Tokens per Sec:     1615 || Lr: 0.000050
2024-01-31 21:21:42,189 Epoch 862: Total Training Recognition Loss 1103.29  Total Training Translation Loss 3487.37 
2024-01-31 21:21:42,190 EPOCH 863
2024-01-31 21:22:02,951 Epoch 863: Total Training Recognition Loss 1099.40  Total Training Translation Loss 3487.21 
2024-01-31 21:22:02,952 EPOCH 864
2024-01-31 21:22:20,758 Epoch 864: Total Training Recognition Loss 1101.46  Total Training Translation Loss 3487.52 
2024-01-31 21:22:20,758 EPOCH 865
2024-01-31 21:22:31,584 [Epoch: 865 Step: 00029400] Batch Recognition Loss:  15.195588 => Gls Tokens per Sec:      686 || Batch Translation Loss:  75.940445 => Txt Tokens per Sec:     1835 || Lr: 0.000050
2024-01-31 21:22:38,273 Epoch 865: Total Training Recognition Loss 1099.11  Total Training Translation Loss 3486.43 
2024-01-31 21:22:38,274 EPOCH 866
2024-01-31 21:22:55,848 Epoch 866: Total Training Recognition Loss 1102.66  Total Training Translation Loss 3487.92 
2024-01-31 21:22:55,848 EPOCH 867
2024-01-31 21:23:13,356 Epoch 867: Total Training Recognition Loss 1100.44  Total Training Translation Loss 3487.22 
2024-01-31 21:23:13,357 EPOCH 868
2024-01-31 21:23:24,558 [Epoch: 868 Step: 00029500] Batch Recognition Loss:  32.863846 => Gls Tokens per Sec:      629 || Batch Translation Loss: 107.833801 => Txt Tokens per Sec:     1719 || Lr: 0.000050
2024-01-31 21:23:30,955 Epoch 868: Total Training Recognition Loss 1102.10  Total Training Translation Loss 3488.39 
2024-01-31 21:23:30,955 EPOCH 869
2024-01-31 21:23:50,847 Epoch 869: Total Training Recognition Loss 1099.85  Total Training Translation Loss 3486.70 
2024-01-31 21:23:50,847 EPOCH 870
2024-01-31 21:24:10,051 Epoch 870: Total Training Recognition Loss 1102.84  Total Training Translation Loss 3487.21 
2024-01-31 21:24:10,052 EPOCH 871
2024-01-31 21:24:21,243 [Epoch: 871 Step: 00029600] Batch Recognition Loss:  36.757309 => Gls Tokens per Sec:      572 || Batch Translation Loss: 124.667976 => Txt Tokens per Sec:     1615 || Lr: 0.000050
2024-01-31 21:24:27,775 Epoch 871: Total Training Recognition Loss 1098.97  Total Training Translation Loss 3487.21 
2024-01-31 21:24:27,776 EPOCH 872
2024-01-31 21:24:45,444 Epoch 872: Total Training Recognition Loss 1101.48  Total Training Translation Loss 3487.20 
2024-01-31 21:24:45,445 EPOCH 873
2024-01-31 21:25:03,182 Epoch 873: Total Training Recognition Loss 1100.86  Total Training Translation Loss 3486.49 
2024-01-31 21:25:03,182 EPOCH 874
2024-01-31 21:25:12,964 [Epoch: 874 Step: 00029700] Batch Recognition Loss:  57.597359 => Gls Tokens per Sec:      563 || Batch Translation Loss: 132.933594 => Txt Tokens per Sec:     1520 || Lr: 0.000050
2024-01-31 21:25:22,456 Epoch 874: Total Training Recognition Loss 1099.84  Total Training Translation Loss 3487.50 
2024-01-31 21:25:22,457 EPOCH 875
2024-01-31 21:25:41,311 Epoch 875: Total Training Recognition Loss 1102.40  Total Training Translation Loss 3487.28 
2024-01-31 21:25:41,312 EPOCH 876
2024-01-31 21:25:58,732 Epoch 876: Total Training Recognition Loss 1101.21  Total Training Translation Loss 3486.66 
2024-01-31 21:25:58,732 EPOCH 877
2024-01-31 21:26:07,197 [Epoch: 877 Step: 00029800] Batch Recognition Loss:  15.364730 => Gls Tokens per Sec:      575 || Batch Translation Loss:  75.490036 => Txt Tokens per Sec:     1595 || Lr: 0.000050
2024-01-31 21:26:16,188 Epoch 877: Total Training Recognition Loss 1100.16  Total Training Translation Loss 3486.63 
2024-01-31 21:26:16,188 EPOCH 878
2024-01-31 21:26:33,763 Epoch 878: Total Training Recognition Loss 1100.58  Total Training Translation Loss 3487.67 
2024-01-31 21:26:33,764 EPOCH 879
2024-01-31 21:26:51,313 Epoch 879: Total Training Recognition Loss 1099.48  Total Training Translation Loss 3487.10 
2024-01-31 21:26:51,313 EPOCH 880
2024-01-31 21:26:56,738 [Epoch: 880 Step: 00029900] Batch Recognition Loss:  27.499367 => Gls Tokens per Sec:      826 || Batch Translation Loss:  90.922096 => Txt Tokens per Sec:     2131 || Lr: 0.000050
2024-01-31 21:27:09,053 Epoch 880: Total Training Recognition Loss 1101.37  Total Training Translation Loss 3487.55 
2024-01-31 21:27:09,053 EPOCH 881
2024-01-31 21:27:26,541 Epoch 881: Total Training Recognition Loss 1100.82  Total Training Translation Loss 3487.30 
2024-01-31 21:27:26,542 EPOCH 882
2024-01-31 21:27:44,222 Epoch 882: Total Training Recognition Loss 1100.33  Total Training Translation Loss 3487.84 
2024-01-31 21:27:44,222 EPOCH 883
2024-01-31 21:27:49,526 [Epoch: 883 Step: 00030000] Batch Recognition Loss:  30.371305 => Gls Tokens per Sec:      724 || Batch Translation Loss: 102.751846 => Txt Tokens per Sec:     1932 || Lr: 0.000050
2024-01-31 21:28:13,802 Validation result at epoch 883, step    30000: duration: 24.2752s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 347.48056	Translation Loss: 73479.57031	PPL: 1561.03467
	Eval Metric: BLEU
	WER 528.46	(DEL: 4.24,	INS: 439.69,	SUB: 84.53)
	BLEU-4 0.00	(BLEU-1: 0.01,	BLEU-2: 0.00,	BLEU-3: 0.00,	BLEU-4: 0.00)
	CHRF 3.48	ROUGE 0.02
2024-01-31 21:28:13,803 Logging Recognition and Translation Outputs
2024-01-31 21:28:13,804 ========================================================================================================================
2024-01-31 21:28:13,804 Logging Sequence: 165_502.00
2024-01-31 21:28:13,804 	Gloss Reference :	A B+C+D+E
2024-01-31 21:28:13,804 	Gloss Hypothesis:	* E+C+E  
2024-01-31 21:28:13,804 	Gloss Alignment :	D S      
2024-01-31 21:28:13,804 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 21:28:13,808 	Text Reference  :	*** *** *** *** ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* tendulkar would   sit     in      the     pavilion wearing both    his     batting pads    even    after   he      got     out    
2024-01-31 21:28:13,808 	Text Hypothesis :	<s> <s> <s> <s> ashwini ashwini ashwini ashwini ashwini ashwini ashwini ashwini ashwini ashwini ashwini ashwini ashwini ashwini ashwini ashwini ashwini ashwini ashwini ashwini ashwini   ashwini ashwini ashwini ashwini ashwini  ashwini ashwini ashwini ashwini ashwini ashwini ashwini ashwini ashwini ashwini
2024-01-31 21:28:13,808 	Text Alignment  :	I   I   I   I   I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       S         S       S       S       S       S        S       S       S       S       S       S       S       S       S       S      
2024-01-31 21:28:13,808 ========================================================================================================================
2024-01-31 21:28:13,808 Logging Sequence: 127_57.00
2024-01-31 21:28:13,809 	Gloss Reference :	* ***** ***** ***** *********** ***** * ***** * ***** * A     B+C+D+E            
2024-01-31 21:28:13,809 	Gloss Hypothesis:	E <unk> E+A+E <unk> E+B+E+C+A+C <unk> E <unk> E <unk> E <unk> E+A+C+A+C+E+A+E+C+A
2024-01-31 21:28:13,809 	Gloss Alignment :	I I     I     I     I           I     I I     I I     I S     S                  
2024-01-31 21:28:13,809 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 21:28:13,813 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** ************* ************* ************* ************* ************* ************* ************* ************* ************* till          date          india         had           won           only          2             medals        at            the           championships which         like          the           olympics      is            the           highest       level         championship 
2024-01-31 21:28:13,813 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions
2024-01-31 21:28:13,814 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I             I             I             I             I             I             I             I             I             S             S             S             S             S             S             S             S             S             S             S             S             S             S             S             S             S             S             S             S            
2024-01-31 21:28:13,814 ========================================================================================================================
2024-01-31 21:28:13,814 Logging Sequence: 169_10.00
2024-01-31 21:28:13,814 	Gloss Reference :	***** ***** ***** ********* ***** ********* A     B+C+D+E            
2024-01-31 21:28:13,814 	Gloss Hypothesis:	<unk> E+D+E <unk> E+D+E+D+E <unk> E+D+E+D+E <unk> E+D+C+E+C+E+D+E+C+E
2024-01-31 21:28:13,814 	Gloss Alignment :	I     I     I     I         I     I         S     S                  
2024-01-31 21:28:13,815 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 21:28:13,818 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** the    18th   over   was    bowled by     ravi   bishnoi with   khushdil shah   and    asif   ali    on     the    crease
2024-01-31 21:28:13,818 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> boards boards boards boards boards boards boards boards boards boards boards boards boards boards boards boards boards boards boards boards  boards boards   boards boards boards boards boards boards boards
2024-01-31 21:28:13,818 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I      I      I      I      I      I      I      I      I      I      I      I      S      S      S      S      S      S      S      S       S      S        S      S      S      S      S      S      S     
2024-01-31 21:28:13,818 ========================================================================================================================
2024-01-31 21:28:13,819 Logging Sequence: 64_89.00
2024-01-31 21:28:13,819 	Gloss Reference :	***** ***** ***** ***** ***** ***** ***** ***** ***** A     B+C+D+E
2024-01-31 21:28:13,819 	Gloss Hypothesis:	<unk> <pad> <unk> <pad> <unk> <pad> <unk> <pad> <unk> <pad> <unk>  
2024-01-31 21:28:13,819 	Gloss Alignment :	I     I     I     I     I     I     I     I     I     S     S      
2024-01-31 21:28:13,819 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 21:28:13,823 	Text Reference  :	*** *** *** *** *** *** *** *** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** but  this can  not  go   on   amidst the  rising cases human lives need to   be   safeguarded
2024-01-31 21:28:13,823 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> fine fine fine fine fine fine fine fine fine fine fine fine fine fine fine fine fine fine fine fine fine fine fine   fine fine   fine  fine  fine  fine fine fine fine       
2024-01-31 21:28:13,823 	Text Alignment  :	I   I   I   I   I   I   I   I   I    I    I    I    I    I    I    I    I    I    I    I    I    I    I    I    S    S    S    S    S    S    S      S    S      S     S     S     S    S    S    S          
2024-01-31 21:28:13,823 ========================================================================================================================
2024-01-31 21:28:13,823 Logging Sequence: 166_261.00
2024-01-31 21:28:13,823 	Gloss Reference :	* A     B+C+D+E
2024-01-31 21:28:13,824 	Gloss Hypothesis:	E <unk> E      
2024-01-31 21:28:13,824 	Gloss Alignment :	I S     S      
2024-01-31 21:28:13,824 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 21:28:13,825 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** *** *** ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* for           all           organizational matters       and           the           schedule     
2024-01-31 21:28:13,826 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions  contributions contributions contributions contributions
2024-01-31 21:28:13,826 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I   I   I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             S             S             S              S             S             S             S            
2024-01-31 21:28:13,826 ========================================================================================================================
2024-01-31 21:28:26,062 Epoch 883: Total Training Recognition Loss 1102.36  Total Training Translation Loss 3487.07 
2024-01-31 21:28:26,062 EPOCH 884
2024-01-31 21:28:43,622 Epoch 884: Total Training Recognition Loss 1099.61  Total Training Translation Loss 3487.26 
2024-01-31 21:28:43,622 EPOCH 885
2024-01-31 21:29:01,187 Epoch 885: Total Training Recognition Loss 1099.92  Total Training Translation Loss 3487.03 
2024-01-31 21:29:01,188 EPOCH 886
2024-01-31 21:29:05,613 [Epoch: 886 Step: 00030100] Batch Recognition Loss:  28.799414 => Gls Tokens per Sec:      667 || Batch Translation Loss:  89.256447 => Txt Tokens per Sec:     1749 || Lr: 0.000050
2024-01-31 21:29:18,778 Epoch 886: Total Training Recognition Loss 1099.57  Total Training Translation Loss 3486.42 
2024-01-31 21:29:18,778 EPOCH 887
2024-01-31 21:29:36,493 Epoch 887: Total Training Recognition Loss 1100.63  Total Training Translation Loss 3487.57 
2024-01-31 21:29:36,493 EPOCH 888
2024-01-31 21:29:54,121 Epoch 888: Total Training Recognition Loss 1101.30  Total Training Translation Loss 3487.15 
2024-01-31 21:29:54,121 EPOCH 889
2024-01-31 21:29:59,405 [Epoch: 889 Step: 00030200] Batch Recognition Loss:  37.944004 => Gls Tokens per Sec:      485 || Batch Translation Loss: 113.498779 => Txt Tokens per Sec:     1440 || Lr: 0.000050
2024-01-31 21:30:11,705 Epoch 889: Total Training Recognition Loss 1098.58  Total Training Translation Loss 3487.01 
2024-01-31 21:30:11,705 EPOCH 890
2024-01-31 21:30:29,149 Epoch 890: Total Training Recognition Loss 1099.54  Total Training Translation Loss 3486.59 
2024-01-31 21:30:29,149 EPOCH 891
2024-01-31 21:30:46,651 Epoch 891: Total Training Recognition Loss 1100.38  Total Training Translation Loss 3486.45 
2024-01-31 21:30:46,651 EPOCH 892
2024-01-31 21:30:49,021 [Epoch: 892 Step: 00030300] Batch Recognition Loss:  28.245127 => Gls Tokens per Sec:      810 || Batch Translation Loss:  91.088043 => Txt Tokens per Sec:     2140 || Lr: 0.000050
2024-01-31 21:31:04,124 Epoch 892: Total Training Recognition Loss 1100.39  Total Training Translation Loss 3487.49 
2024-01-31 21:31:04,124 EPOCH 893
2024-01-31 21:31:21,579 Epoch 893: Total Training Recognition Loss 1098.53  Total Training Translation Loss 3487.83 
2024-01-31 21:31:21,579 EPOCH 894
2024-01-31 21:31:39,041 Epoch 894: Total Training Recognition Loss 1099.17  Total Training Translation Loss 3486.52 
2024-01-31 21:31:39,042 EPOCH 895
2024-01-31 21:31:41,179 [Epoch: 895 Step: 00030400] Batch Recognition Loss:  73.195732 => Gls Tokens per Sec:      482 || Batch Translation Loss: 126.667122 => Txt Tokens per Sec:     1366 || Lr: 0.000050
2024-01-31 21:31:56,600 Epoch 895: Total Training Recognition Loss 1099.66  Total Training Translation Loss 3488.12 
2024-01-31 21:31:56,600 EPOCH 896
2024-01-31 21:32:14,060 Epoch 896: Total Training Recognition Loss 1100.24  Total Training Translation Loss 3486.75 
2024-01-31 21:32:14,060 EPOCH 897
2024-01-31 21:32:31,515 Epoch 897: Total Training Recognition Loss 1102.61  Total Training Translation Loss 3488.02 
2024-01-31 21:32:31,515 EPOCH 898
2024-01-31 21:32:32,432 [Epoch: 898 Step: 00030500] Batch Recognition Loss:  36.907616 => Gls Tokens per Sec:      698 || Batch Translation Loss: 121.864975 => Txt Tokens per Sec:     2086 || Lr: 0.000050
2024-01-31 21:32:49,142 Epoch 898: Total Training Recognition Loss 1100.87  Total Training Translation Loss 3487.64 
2024-01-31 21:32:49,142 EPOCH 899
2024-01-31 21:33:06,712 Epoch 899: Total Training Recognition Loss 1100.16  Total Training Translation Loss 3488.75 
2024-01-31 21:33:06,712 EPOCH 900
2024-01-31 21:33:24,238 [Epoch: 900 Step: 00030600] Batch Recognition Loss:  57.623402 => Gls Tokens per Sec:      607 || Batch Translation Loss: 132.188400 => Txt Tokens per Sec:     1684 || Lr: 0.000050
2024-01-31 21:33:24,238 Epoch 900: Total Training Recognition Loss 1101.05  Total Training Translation Loss 3487.29 
2024-01-31 21:33:24,239 EPOCH 901
2024-01-31 21:33:41,764 Epoch 901: Total Training Recognition Loss 1101.06  Total Training Translation Loss 3487.50 
2024-01-31 21:33:41,764 EPOCH 902
2024-01-31 21:33:59,285 Epoch 902: Total Training Recognition Loss 1101.39  Total Training Translation Loss 3487.26 
2024-01-31 21:33:59,285 EPOCH 903
2024-01-31 21:34:15,787 [Epoch: 903 Step: 00030700] Batch Recognition Loss:  36.900032 => Gls Tokens per Sec:      605 || Batch Translation Loss: 100.937698 => Txt Tokens per Sec:     1669 || Lr: 0.000050
2024-01-31 21:34:16,851 Epoch 903: Total Training Recognition Loss 1100.26  Total Training Translation Loss 3486.48 
2024-01-31 21:34:16,852 EPOCH 904
2024-01-31 21:34:34,404 Epoch 904: Total Training Recognition Loss 1098.75  Total Training Translation Loss 3487.40 
2024-01-31 21:34:34,404 EPOCH 905
2024-01-31 21:34:51,906 Epoch 905: Total Training Recognition Loss 1100.55  Total Training Translation Loss 3487.42 
2024-01-31 21:34:51,907 EPOCH 906
2024-01-31 21:35:07,606 [Epoch: 906 Step: 00030800] Batch Recognition Loss:  32.866600 => Gls Tokens per Sec:      612 || Batch Translation Loss: 108.749969 => Txt Tokens per Sec:     1731 || Lr: 0.000050
2024-01-31 21:35:09,307 Epoch 906: Total Training Recognition Loss 1099.70  Total Training Translation Loss 3487.53 
2024-01-31 21:35:09,308 EPOCH 907
2024-01-31 21:35:26,898 Epoch 907: Total Training Recognition Loss 1101.23  Total Training Translation Loss 3487.30 
2024-01-31 21:35:26,898 EPOCH 908
2024-01-31 21:35:44,576 Epoch 908: Total Training Recognition Loss 1101.86  Total Training Translation Loss 3486.86 
2024-01-31 21:35:44,576 EPOCH 909
2024-01-31 21:35:58,724 [Epoch: 909 Step: 00030900] Batch Recognition Loss:  44.454147 => Gls Tokens per Sec:      633 || Batch Translation Loss: 129.952301 => Txt Tokens per Sec:     1726 || Lr: 0.000050
2024-01-31 21:36:02,147 Epoch 909: Total Training Recognition Loss 1101.73  Total Training Translation Loss 3486.91 
2024-01-31 21:36:02,147 EPOCH 910
2024-01-31 21:36:19,720 Epoch 910: Total Training Recognition Loss 1102.11  Total Training Translation Loss 3486.94 
2024-01-31 21:36:19,720 EPOCH 911
2024-01-31 21:36:37,133 Epoch 911: Total Training Recognition Loss 1100.02  Total Training Translation Loss 3487.94 
2024-01-31 21:36:37,133 EPOCH 912
2024-01-31 21:36:49,849 [Epoch: 912 Step: 00031000] Batch Recognition Loss:  45.933464 => Gls Tokens per Sec:      635 || Batch Translation Loss: 125.938828 => Txt Tokens per Sec:     1749 || Lr: 0.000050
2024-01-31 21:36:54,707 Epoch 912: Total Training Recognition Loss 1098.32  Total Training Translation Loss 3487.45 
2024-01-31 21:36:54,707 EPOCH 913
2024-01-31 21:37:12,269 Epoch 913: Total Training Recognition Loss 1100.97  Total Training Translation Loss 3487.39 
2024-01-31 21:37:12,269 EPOCH 914
2024-01-31 21:37:29,775 Epoch 914: Total Training Recognition Loss 1102.98  Total Training Translation Loss 3488.04 
2024-01-31 21:37:29,775 EPOCH 915
2024-01-31 21:37:42,716 [Epoch: 915 Step: 00031100] Batch Recognition Loss:  37.057575 => Gls Tokens per Sec:      574 || Batch Translation Loss: 116.177818 => Txt Tokens per Sec:     1604 || Lr: 0.000050
2024-01-31 21:37:47,399 Epoch 915: Total Training Recognition Loss 1102.55  Total Training Translation Loss 3487.84 
2024-01-31 21:37:47,400 EPOCH 916
2024-01-31 21:38:05,035 Epoch 916: Total Training Recognition Loss 1101.33  Total Training Translation Loss 3486.77 
2024-01-31 21:38:05,035 EPOCH 917
2024-01-31 21:38:22,545 Epoch 917: Total Training Recognition Loss 1100.36  Total Training Translation Loss 3486.56 
2024-01-31 21:38:22,546 EPOCH 918
2024-01-31 21:38:31,898 [Epoch: 918 Step: 00031200] Batch Recognition Loss:  22.944958 => Gls Tokens per Sec:      726 || Batch Translation Loss:  83.815117 => Txt Tokens per Sec:     1896 || Lr: 0.000050
2024-01-31 21:38:40,129 Epoch 918: Total Training Recognition Loss 1100.63  Total Training Translation Loss 3487.99 
2024-01-31 21:38:40,129 EPOCH 919
2024-01-31 21:38:57,687 Epoch 919: Total Training Recognition Loss 1100.22  Total Training Translation Loss 3487.07 
2024-01-31 21:38:57,687 EPOCH 920
2024-01-31 21:39:15,170 Epoch 920: Total Training Recognition Loss 1100.73  Total Training Translation Loss 3487.41 
2024-01-31 21:39:15,171 EPOCH 921
2024-01-31 21:39:24,841 [Epoch: 921 Step: 00031300] Batch Recognition Loss:  27.699451 => Gls Tokens per Sec:      662 || Batch Translation Loss:  97.219101 => Txt Tokens per Sec:     1823 || Lr: 0.000050
2024-01-31 21:39:32,721 Epoch 921: Total Training Recognition Loss 1098.55  Total Training Translation Loss 3488.35 
2024-01-31 21:39:32,722 EPOCH 922
2024-01-31 21:39:50,350 Epoch 922: Total Training Recognition Loss 1101.64  Total Training Translation Loss 3486.86 
2024-01-31 21:39:50,350 EPOCH 923
2024-01-31 21:40:07,716 Epoch 923: Total Training Recognition Loss 1101.74  Total Training Translation Loss 3486.84 
2024-01-31 21:40:07,716 EPOCH 924
2024-01-31 21:40:16,537 [Epoch: 924 Step: 00031400] Batch Recognition Loss:  30.753670 => Gls Tokens per Sec:      653 || Batch Translation Loss: 103.920341 => Txt Tokens per Sec:     1803 || Lr: 0.000050
2024-01-31 21:40:25,186 Epoch 924: Total Training Recognition Loss 1099.60  Total Training Translation Loss 3486.72 
2024-01-31 21:40:25,187 EPOCH 925
2024-01-31 21:40:42,666 Epoch 925: Total Training Recognition Loss 1100.66  Total Training Translation Loss 3486.94 
2024-01-31 21:40:42,666 EPOCH 926
2024-01-31 21:41:00,300 Epoch 926: Total Training Recognition Loss 1099.50  Total Training Translation Loss 3487.46 
2024-01-31 21:41:00,300 EPOCH 927
2024-01-31 21:41:09,231 [Epoch: 927 Step: 00031500] Batch Recognition Loss:  37.492836 => Gls Tokens per Sec:      545 || Batch Translation Loss: 111.733665 => Txt Tokens per Sec:     1520 || Lr: 0.000050
2024-01-31 21:41:17,857 Epoch 927: Total Training Recognition Loss 1099.59  Total Training Translation Loss 3487.68 
2024-01-31 21:41:17,857 EPOCH 928
2024-01-31 21:41:35,448 Epoch 928: Total Training Recognition Loss 1100.27  Total Training Translation Loss 3488.61 
2024-01-31 21:41:35,448 EPOCH 929
2024-01-31 21:41:53,020 Epoch 929: Total Training Recognition Loss 1101.14  Total Training Translation Loss 3487.29 
2024-01-31 21:41:53,020 EPOCH 930
2024-01-31 21:42:01,040 [Epoch: 930 Step: 00031600] Batch Recognition Loss:  37.205578 => Gls Tokens per Sec:      528 || Batch Translation Loss: 112.591492 => Txt Tokens per Sec:     1528 || Lr: 0.000050
2024-01-31 21:42:10,586 Epoch 930: Total Training Recognition Loss 1102.10  Total Training Translation Loss 3487.33 
2024-01-31 21:42:10,587 EPOCH 931
2024-01-31 21:42:28,215 Epoch 931: Total Training Recognition Loss 1098.41  Total Training Translation Loss 3487.63 
2024-01-31 21:42:28,215 EPOCH 932
2024-01-31 21:42:45,687 Epoch 932: Total Training Recognition Loss 1101.84  Total Training Translation Loss 3487.48 
2024-01-31 21:42:45,687 EPOCH 933
2024-01-31 21:42:52,127 [Epoch: 933 Step: 00031700] Batch Recognition Loss:  34.735249 => Gls Tokens per Sec:      558 || Batch Translation Loss: 108.666733 => Txt Tokens per Sec:     1560 || Lr: 0.000050
2024-01-31 21:43:03,284 Epoch 933: Total Training Recognition Loss 1100.55  Total Training Translation Loss 3487.22 
2024-01-31 21:43:03,284 EPOCH 934
2024-01-31 21:43:20,842 Epoch 934: Total Training Recognition Loss 1098.05  Total Training Translation Loss 3486.88 
2024-01-31 21:43:20,842 EPOCH 935
2024-01-31 21:43:38,367 Epoch 935: Total Training Recognition Loss 1101.63  Total Training Translation Loss 3487.66 
2024-01-31 21:43:38,368 EPOCH 936
2024-01-31 21:43:43,165 [Epoch: 936 Step: 00031800] Batch Recognition Loss:  30.218452 => Gls Tokens per Sec:      667 || Batch Translation Loss: 102.570633 => Txt Tokens per Sec:     1882 || Lr: 0.000050
2024-01-31 21:43:55,996 Epoch 936: Total Training Recognition Loss 1099.69  Total Training Translation Loss 3487.92 
2024-01-31 21:43:55,997 EPOCH 937
2024-01-31 21:44:13,657 Epoch 937: Total Training Recognition Loss 1102.43  Total Training Translation Loss 3486.90 
2024-01-31 21:44:13,658 EPOCH 938
2024-01-31 21:44:31,223 Epoch 938: Total Training Recognition Loss 1102.33  Total Training Translation Loss 3488.35 
2024-01-31 21:44:31,224 EPOCH 939
2024-01-31 21:44:34,830 [Epoch: 939 Step: 00031900] Batch Recognition Loss:  35.140732 => Gls Tokens per Sec:      641 || Batch Translation Loss: 107.965164 => Txt Tokens per Sec:     1709 || Lr: 0.000050
2024-01-31 21:44:48,691 Epoch 939: Total Training Recognition Loss 1100.85  Total Training Translation Loss 3487.31 
2024-01-31 21:44:48,691 EPOCH 940
2024-01-31 21:45:06,179 Epoch 940: Total Training Recognition Loss 1101.09  Total Training Translation Loss 3486.41 
2024-01-31 21:45:06,180 EPOCH 941
2024-01-31 21:45:23,851 Epoch 941: Total Training Recognition Loss 1098.50  Total Training Translation Loss 3486.95 
2024-01-31 21:45:23,851 EPOCH 942
2024-01-31 21:45:27,480 [Epoch: 942 Step: 00032000] Batch Recognition Loss:  37.021618 => Gls Tokens per Sec:      529 || Batch Translation Loss: 121.688614 => Txt Tokens per Sec:     1652 || Lr: 0.000050
2024-01-31 21:45:51,545 Validation result at epoch 942, step    32000: duration: 24.0656s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 353.82468	Translation Loss: 73475.17188	PPL: 1560.34766
	Eval Metric: BLEU
	WER 543.64	(DEL: 4.03,	INS: 454.73,	SUB: 84.89)
	BLEU-4 0.00	(BLEU-1: 0.01,	BLEU-2: 0.00,	BLEU-3: 0.00,	BLEU-4: 0.00)
	CHRF 3.42	ROUGE 0.02
2024-01-31 21:45:51,547 Logging Recognition and Translation Outputs
2024-01-31 21:45:51,547 ========================================================================================================================
2024-01-31 21:45:51,547 Logging Sequence: 86_11.00
2024-01-31 21:45:51,547 	Gloss Reference :	***** ***** A B+C+D+E
2024-01-31 21:45:51,547 	Gloss Hypothesis:	<unk> <pad> A <unk>  
2024-01-31 21:45:51,548 	Gloss Alignment :	I     I       S      
2024-01-31 21:45:51,548 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 21:45:51,549 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** he  was 66  years old
2024-01-31 21:45:51,549 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s>   <s>
2024-01-31 21:45:51,549 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   S   S   S   S     S  
2024-01-31 21:45:51,549 ========================================================================================================================
2024-01-31 21:45:51,550 Logging Sequence: 67_16.00
2024-01-31 21:45:51,550 	Gloss Reference :	A ***** *** ***** * ***** ***** ***** * ***** *************** ***** * ***** B+C+D+E
2024-01-31 21:45:51,550 	Gloss Hypothesis:	A <unk> E+A <unk> E <unk> E+C+E <unk> E <unk> E+C+A+B+A+E+B+E <unk> E <unk> E      
2024-01-31 21:45:51,550 	Gloss Alignment :	  I     I   I     I I     I     I     I I     I               I     I I     S      
2024-01-31 21:45:51,550 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 21:45:51,552 	Text Reference  :	*** *** *** ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* to            help          india's       fight         against       the           covid-19      pandemic     
2024-01-31 21:45:51,552 	Text Hypothesis :	<s> <s> <s> misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand
2024-01-31 21:45:51,553 	Text Alignment  :	I   I   I   I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             S             S             S             S             S             S             S             S            
2024-01-31 21:45:51,553 ========================================================================================================================
2024-01-31 21:45:51,553 Logging Sequence: 69_177.00
2024-01-31 21:45:51,553 	Gloss Reference :	* A B+C+D+E
2024-01-31 21:45:51,553 	Gloss Hypothesis:	E A *******
2024-01-31 21:45:51,553 	Gloss Alignment :	I   D      
2024-01-31 21:45:51,553 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 21:45:51,557 	Text Reference  :	*** *** *** *** *** *** *** *** *** ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* he      said    'i      will    continue playing i       know    it's    about   time    i       retire  i       also    have    a       knee    condition
2024-01-31 21:45:51,557 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing  nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing  
2024-01-31 21:45:51,558 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I       I       I       I       I       I       I       I       I       I       I       I       S       S       S       S       S        S       S       S       S       S       S       S       S       S       S       S       S       S       S        
2024-01-31 21:45:51,558 ========================================================================================================================
2024-01-31 21:45:51,558 Logging Sequence: 165_615.00
2024-01-31 21:45:51,558 	Gloss Reference :	A B+C+D+E
2024-01-31 21:45:51,558 	Gloss Hypothesis:	* E+A+E  
2024-01-31 21:45:51,558 	Gloss Alignment :	D S      
2024-01-31 21:45:51,558 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 21:45:51,559 	Text Reference  :	*** *** *** *** *** *** *** *** *** *********** *********** *********** *********** *********** *********** *********** *********** *********** *********** *********** *********** *********** *********** *********** *********** *********** *********** *********** *********** *********** *********** *********** *********** *********** *********** *********** we          defeated    pakistan    too        
2024-01-31 21:45:51,560 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> unconscious unconscious unconscious unconscious unconscious unconscious unconscious unconscious unconscious unconscious unconscious unconscious unconscious unconscious unconscious unconscious unconscious unconscious unconscious unconscious unconscious unconscious unconscious unconscious unconscious unconscious unconscious unconscious unconscious unconscious unconscious
2024-01-31 21:45:51,560 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I           I           I           I           I           I           I           I           I           I           I           I           I           I           I           I           I           I           I           I           I           I           I           I           I           I           I           S           S           S           S          
2024-01-31 21:45:51,560 ========================================================================================================================
2024-01-31 21:45:51,560 Logging Sequence: 61_5.00
2024-01-31 21:45:51,560 	Gloss Reference :	***** * ***** * ***** * ***** * ***** A B+C+D+E
2024-01-31 21:45:51,560 	Gloss Hypothesis:	<unk> A <unk> A <unk> A <unk> E <unk> A <unk>  
2024-01-31 21:45:51,561 	Gloss Alignment :	I     I I     I I     I I     I I       S      
2024-01-31 21:45:51,561 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 21:45:51,563 	Text Reference  :	*** *** ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* they          rivalry       is            seen          the           most          during        india         pakistan      cricket       matches      
2024-01-31 21:45:51,563 	Text Hypothesis :	<s> <s> misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand
2024-01-31 21:45:51,563 	Text Alignment  :	I   I   I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             S             S             S             S             S             S             S             S             S             S             S            
2024-01-31 21:45:51,564 ========================================================================================================================
2024-01-31 21:46:05,397 Epoch 942: Total Training Recognition Loss 1098.96  Total Training Translation Loss 3486.36 
2024-01-31 21:46:05,398 EPOCH 943
2024-01-31 21:46:22,820 Epoch 943: Total Training Recognition Loss 1099.99  Total Training Translation Loss 3486.93 
2024-01-31 21:46:22,821 EPOCH 944
2024-01-31 21:46:40,410 Epoch 944: Total Training Recognition Loss 1097.67  Total Training Translation Loss 3486.92 
2024-01-31 21:46:40,411 EPOCH 945
2024-01-31 21:46:42,210 [Epoch: 945 Step: 00032100] Batch Recognition Loss:  18.579273 => Gls Tokens per Sec:      712 || Batch Translation Loss:  89.233658 => Txt Tokens per Sec:     2001 || Lr: 0.000050
2024-01-31 21:46:57,961 Epoch 945: Total Training Recognition Loss 1102.83  Total Training Translation Loss 3487.16 
2024-01-31 21:46:57,961 EPOCH 946
2024-01-31 21:47:15,489 Epoch 946: Total Training Recognition Loss 1100.75  Total Training Translation Loss 3487.06 
2024-01-31 21:47:15,489 EPOCH 947
2024-01-31 21:47:33,002 Epoch 947: Total Training Recognition Loss 1100.70  Total Training Translation Loss 3487.49 
2024-01-31 21:47:33,002 EPOCH 948
2024-01-31 21:47:34,593 [Epoch: 948 Step: 00032200] Batch Recognition Loss:  73.126335 => Gls Tokens per Sec:      245 || Batch Translation Loss: 126.498734 => Txt Tokens per Sec:      873 || Lr: 0.000050
2024-01-31 21:47:50,467 Epoch 948: Total Training Recognition Loss 1100.79  Total Training Translation Loss 3487.17 
2024-01-31 21:47:50,467 EPOCH 949
2024-01-31 21:48:08,092 Epoch 949: Total Training Recognition Loss 1100.25  Total Training Translation Loss 3486.37 
2024-01-31 21:48:08,092 EPOCH 950
2024-01-31 21:48:25,817 [Epoch: 950 Step: 00032300] Batch Recognition Loss:  33.159119 => Gls Tokens per Sec:      600 || Batch Translation Loss: 100.040825 => Txt Tokens per Sec:     1665 || Lr: 0.000050
2024-01-31 21:48:25,817 Epoch 950: Total Training Recognition Loss 1101.42  Total Training Translation Loss 3487.46 
2024-01-31 21:48:25,817 EPOCH 951
2024-01-31 21:48:43,331 Epoch 951: Total Training Recognition Loss 1101.34  Total Training Translation Loss 3486.36 
2024-01-31 21:48:43,331 EPOCH 952
2024-01-31 21:49:00,679 Epoch 952: Total Training Recognition Loss 1100.57  Total Training Translation Loss 3487.15 
2024-01-31 21:49:00,680 EPOCH 953
2024-01-31 21:49:17,716 [Epoch: 953 Step: 00032400] Batch Recognition Loss:  51.377914 => Gls Tokens per Sec:      586 || Batch Translation Loss: 128.403229 => Txt Tokens per Sec:     1647 || Lr: 0.000050
2024-01-31 21:49:18,249 Epoch 953: Total Training Recognition Loss 1099.85  Total Training Translation Loss 3488.43 
2024-01-31 21:49:18,250 EPOCH 954
2024-01-31 21:49:35,783 Epoch 954: Total Training Recognition Loss 1098.79  Total Training Translation Loss 3488.00 
2024-01-31 21:49:35,783 EPOCH 955
2024-01-31 21:49:53,256 Epoch 955: Total Training Recognition Loss 1100.71  Total Training Translation Loss 3487.85 
2024-01-31 21:49:53,257 EPOCH 956
2024-01-31 21:50:07,200 [Epoch: 956 Step: 00032500] Batch Recognition Loss:  33.196281 => Gls Tokens per Sec:      671 || Batch Translation Loss: 109.711159 => Txt Tokens per Sec:     1835 || Lr: 0.000050
2024-01-31 21:50:10,798 Epoch 956: Total Training Recognition Loss 1100.82  Total Training Translation Loss 3487.44 
2024-01-31 21:50:10,798 EPOCH 957
2024-01-31 21:50:28,196 Epoch 957: Total Training Recognition Loss 1101.32  Total Training Translation Loss 3487.17 
2024-01-31 21:50:28,196 EPOCH 958
2024-01-31 21:50:45,740 Epoch 958: Total Training Recognition Loss 1100.87  Total Training Translation Loss 3487.66 
2024-01-31 21:50:45,740 EPOCH 959
2024-01-31 21:51:00,064 [Epoch: 959 Step: 00032600] Batch Recognition Loss:  28.038967 => Gls Tokens per Sec:      626 || Batch Translation Loss:  89.666534 => Txt Tokens per Sec:     1712 || Lr: 0.000050
2024-01-31 21:51:03,222 Epoch 959: Total Training Recognition Loss 1101.21  Total Training Translation Loss 3488.37 
2024-01-31 21:51:03,222 EPOCH 960
2024-01-31 21:51:20,804 Epoch 960: Total Training Recognition Loss 1100.50  Total Training Translation Loss 3488.14 
2024-01-31 21:51:20,804 EPOCH 961
2024-01-31 21:51:38,370 Epoch 961: Total Training Recognition Loss 1102.19  Total Training Translation Loss 3488.23 
2024-01-31 21:51:38,371 EPOCH 962
2024-01-31 21:51:51,723 [Epoch: 962 Step: 00032700] Batch Recognition Loss:  36.914654 => Gls Tokens per Sec:      623 || Batch Translation Loss: 102.913063 => Txt Tokens per Sec:     1768 || Lr: 0.000050
2024-01-31 21:51:56,188 Epoch 962: Total Training Recognition Loss 1102.61  Total Training Translation Loss 3487.24 
2024-01-31 21:51:56,188 EPOCH 963
2024-01-31 21:52:16,937 Epoch 963: Total Training Recognition Loss 1100.43  Total Training Translation Loss 3487.42 
2024-01-31 21:52:16,938 EPOCH 964
2024-01-31 21:52:34,750 Epoch 964: Total Training Recognition Loss 1102.31  Total Training Translation Loss 3488.13 
2024-01-31 21:52:34,751 EPOCH 965
2024-01-31 21:52:47,131 [Epoch: 965 Step: 00032800] Batch Recognition Loss:  30.905239 => Gls Tokens per Sec:      600 || Batch Translation Loss: 105.844223 => Txt Tokens per Sec:     1644 || Lr: 0.000050
2024-01-31 21:52:52,373 Epoch 965: Total Training Recognition Loss 1100.77  Total Training Translation Loss 3487.80 
2024-01-31 21:52:52,373 EPOCH 966
2024-01-31 21:53:09,953 Epoch 966: Total Training Recognition Loss 1098.78  Total Training Translation Loss 3486.66 
2024-01-31 21:53:09,953 EPOCH 967
2024-01-31 21:53:27,537 Epoch 967: Total Training Recognition Loss 1101.23  Total Training Translation Loss 3487.60 
2024-01-31 21:53:27,538 EPOCH 968
2024-01-31 21:53:40,290 [Epoch: 968 Step: 00032900] Batch Recognition Loss:  23.287792 => Gls Tokens per Sec:      533 || Batch Translation Loss:  97.031677 => Txt Tokens per Sec:     1513 || Lr: 0.000050
2024-01-31 21:53:45,307 Epoch 968: Total Training Recognition Loss 1100.17  Total Training Translation Loss 3486.92 
2024-01-31 21:53:45,307 EPOCH 969
2024-01-31 21:54:02,817 Epoch 969: Total Training Recognition Loss 1100.75  Total Training Translation Loss 3487.25 
2024-01-31 21:54:02,818 EPOCH 970
2024-01-31 21:54:20,602 Epoch 970: Total Training Recognition Loss 1103.79  Total Training Translation Loss 3486.90 
2024-01-31 21:54:20,603 EPOCH 971
2024-01-31 21:54:30,935 [Epoch: 971 Step: 00033000] Batch Recognition Loss:  36.749714 => Gls Tokens per Sec:      595 || Batch Translation Loss: 121.614075 => Txt Tokens per Sec:     1706 || Lr: 0.000050
2024-01-31 21:54:38,331 Epoch 971: Total Training Recognition Loss 1100.11  Total Training Translation Loss 3487.25 
2024-01-31 21:54:38,331 EPOCH 972
2024-01-31 21:54:55,889 Epoch 972: Total Training Recognition Loss 1101.06  Total Training Translation Loss 3487.01 
2024-01-31 21:54:55,889 EPOCH 973
2024-01-31 21:55:13,654 Epoch 973: Total Training Recognition Loss 1100.11  Total Training Translation Loss 3487.19 
2024-01-31 21:55:13,654 EPOCH 974
2024-01-31 21:55:23,620 [Epoch: 974 Step: 00033100] Batch Recognition Loss:  36.412560 => Gls Tokens per Sec:      553 || Batch Translation Loss: 102.137657 => Txt Tokens per Sec:     1530 || Lr: 0.000050
2024-01-31 21:55:31,099 Epoch 974: Total Training Recognition Loss 1098.47  Total Training Translation Loss 3487.11 
2024-01-31 21:55:31,099 EPOCH 975
2024-01-31 21:55:48,452 Epoch 975: Total Training Recognition Loss 1100.63  Total Training Translation Loss 3486.62 
2024-01-31 21:55:48,453 EPOCH 976
2024-01-31 21:56:06,069 Epoch 976: Total Training Recognition Loss 1101.38  Total Training Translation Loss 3488.17 
2024-01-31 21:56:06,069 EPOCH 977
2024-01-31 21:56:12,516 [Epoch: 977 Step: 00033200] Batch Recognition Loss:  31.939632 => Gls Tokens per Sec:      794 || Batch Translation Loss:  94.291000 => Txt Tokens per Sec:     2097 || Lr: 0.000050
2024-01-31 21:56:23,579 Epoch 977: Total Training Recognition Loss 1099.94  Total Training Translation Loss 3487.27 
2024-01-31 21:56:23,579 EPOCH 978
2024-01-31 21:56:41,323 Epoch 978: Total Training Recognition Loss 1101.21  Total Training Translation Loss 3487.13 
2024-01-31 21:56:41,323 EPOCH 979
2024-01-31 21:56:59,007 Epoch 979: Total Training Recognition Loss 1101.05  Total Training Translation Loss 3486.94 
2024-01-31 21:56:59,007 EPOCH 980
2024-01-31 21:57:05,733 [Epoch: 980 Step: 00033300] Batch Recognition Loss:  36.905746 => Gls Tokens per Sec:      629 || Batch Translation Loss: 105.747314 => Txt Tokens per Sec:     1743 || Lr: 0.000050
2024-01-31 21:57:16,462 Epoch 980: Total Training Recognition Loss 1100.96  Total Training Translation Loss 3487.86 
2024-01-31 21:57:16,463 EPOCH 981
2024-01-31 21:57:33,961 Epoch 981: Total Training Recognition Loss 1100.33  Total Training Translation Loss 3487.21 
2024-01-31 21:57:33,962 EPOCH 982
2024-01-31 21:57:51,655 Epoch 982: Total Training Recognition Loss 1099.46  Total Training Translation Loss 3486.99 
2024-01-31 21:57:51,655 EPOCH 983
2024-01-31 21:57:58,209 [Epoch: 983 Step: 00033400] Batch Recognition Loss:  37.469444 => Gls Tokens per Sec:      548 || Batch Translation Loss: 120.225990 => Txt Tokens per Sec:     1461 || Lr: 0.000050
2024-01-31 21:58:09,273 Epoch 983: Total Training Recognition Loss 1102.56  Total Training Translation Loss 3486.48 
2024-01-31 21:58:09,273 EPOCH 984
2024-01-31 21:58:26,779 Epoch 984: Total Training Recognition Loss 1100.69  Total Training Translation Loss 3487.37 
2024-01-31 21:58:26,780 EPOCH 985
2024-01-31 21:58:44,379 Epoch 985: Total Training Recognition Loss 1101.77  Total Training Translation Loss 3486.69 
2024-01-31 21:58:44,379 EPOCH 986
2024-01-31 21:58:49,638 [Epoch: 986 Step: 00033500] Batch Recognition Loss:  50.902077 => Gls Tokens per Sec:      609 || Batch Translation Loss: 129.052155 => Txt Tokens per Sec:     1676 || Lr: 0.000050
2024-01-31 21:59:01,819 Epoch 986: Total Training Recognition Loss 1100.70  Total Training Translation Loss 3486.97 
2024-01-31 21:59:01,819 EPOCH 987
2024-01-31 21:59:19,373 Epoch 987: Total Training Recognition Loss 1101.86  Total Training Translation Loss 3486.82 
2024-01-31 21:59:19,373 EPOCH 988
2024-01-31 21:59:36,949 Epoch 988: Total Training Recognition Loss 1099.23  Total Training Translation Loss 3488.23 
2024-01-31 21:59:36,949 EPOCH 989
2024-01-31 21:59:41,903 [Epoch: 989 Step: 00033600] Batch Recognition Loss:  30.129040 => Gls Tokens per Sec:      517 || Batch Translation Loss: 103.750603 => Txt Tokens per Sec:     1551 || Lr: 0.000050
2024-01-31 21:59:54,511 Epoch 989: Total Training Recognition Loss 1102.37  Total Training Translation Loss 3486.94 
2024-01-31 21:59:54,512 EPOCH 990
2024-01-31 22:00:12,015 Epoch 990: Total Training Recognition Loss 1099.59  Total Training Translation Loss 3486.47 
2024-01-31 22:00:12,016 EPOCH 991
2024-01-31 22:00:29,566 Epoch 991: Total Training Recognition Loss 1100.32  Total Training Translation Loss 3487.71 
2024-01-31 22:00:29,567 EPOCH 992
2024-01-31 22:00:32,078 [Epoch: 992 Step: 00033700] Batch Recognition Loss:  35.603287 => Gls Tokens per Sec:      765 || Batch Translation Loss: 107.203186 => Txt Tokens per Sec:     2104 || Lr: 0.000050
2024-01-31 22:00:47,183 Epoch 992: Total Training Recognition Loss 1101.04  Total Training Translation Loss 3486.84 
2024-01-31 22:00:47,183 EPOCH 993
2024-01-31 22:01:04,793 Epoch 993: Total Training Recognition Loss 1099.55  Total Training Translation Loss 3486.83 
2024-01-31 22:01:04,793 EPOCH 994
2024-01-31 22:01:22,403 Epoch 994: Total Training Recognition Loss 1100.48  Total Training Translation Loss 3487.37 
2024-01-31 22:01:22,403 EPOCH 995
2024-01-31 22:01:23,876 [Epoch: 995 Step: 00033800] Batch Recognition Loss:  30.332186 => Gls Tokens per Sec:      870 || Batch Translation Loss:  96.765686 => Txt Tokens per Sec:     2085 || Lr: 0.000050
2024-01-31 22:01:40,034 Epoch 995: Total Training Recognition Loss 1102.49  Total Training Translation Loss 3486.96 
2024-01-31 22:01:40,035 EPOCH 996
2024-01-31 22:01:57,561 Epoch 996: Total Training Recognition Loss 1100.26  Total Training Translation Loss 3487.35 
2024-01-31 22:01:57,561 EPOCH 997
2024-01-31 22:02:15,037 Epoch 997: Total Training Recognition Loss 1101.22  Total Training Translation Loss 3487.84 
2024-01-31 22:02:15,037 EPOCH 998
2024-01-31 22:02:15,801 [Epoch: 998 Step: 00033900] Batch Recognition Loss:  17.377697 => Gls Tokens per Sec:      839 || Batch Translation Loss:  74.942139 => Txt Tokens per Sec:     2002 || Lr: 0.000050
2024-01-31 22:02:32,577 Epoch 998: Total Training Recognition Loss 1101.48  Total Training Translation Loss 3487.27 
2024-01-31 22:02:32,577 EPOCH 999
2024-01-31 22:02:49,986 Epoch 999: Total Training Recognition Loss 1100.19  Total Training Translation Loss 3488.12 
2024-01-31 22:02:49,986 EPOCH 1000
2024-01-31 22:03:07,520 [Epoch: 1000 Step: 00034000] Batch Recognition Loss:  74.177902 => Gls Tokens per Sec:      606 || Batch Translation Loss: 126.793343 => Txt Tokens per Sec:     1683 || Lr: 0.000050
2024-01-31 22:03:31,733 Validation result at epoch 1000, step    34000: duration: 24.2121s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 347.44293	Translation Loss: 73487.21875	PPL: 1562.23010
	Eval Metric: BLEU
	WER 525.21	(DEL: 4.45,	INS: 436.16,	SUB: 84.60)
	BLEU-4 0.00	(BLEU-1: 0.01,	BLEU-2: 0.00,	BLEU-3: 0.00,	BLEU-4: 0.00)
	CHRF 3.48	ROUGE 0.02
2024-01-31 22:03:31,734 Logging Recognition and Translation Outputs
2024-01-31 22:03:31,734 ========================================================================================================================
2024-01-31 22:03:31,734 Logging Sequence: 92_199.00
2024-01-31 22:03:31,735 	Gloss Reference :	***** ***** ***** * ***** A ***** ***** ***** ***** ***** ***** ***** B+C+D+E
2024-01-31 22:03:31,735 	Gloss Hypothesis:	<unk> <pad> <unk> A <pad> A <pad> <unk> <pad> <unk> <pad> <unk> <pad> <unk>  
2024-01-31 22:03:31,735 	Gloss Alignment :	I     I     I     I I       I     I     I     I     I     I     I     S      
2024-01-31 22:03:31,735 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 22:03:31,737 	Text Reference  :	***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** people on    social media said  that 
2024-01-31 22:03:31,737 	Text Hypothesis :	sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha  sabha sabha  sabha sabha sabha
2024-01-31 22:03:31,737 	Text Alignment  :	I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     S      S     S      S     S     S    
2024-01-31 22:03:31,737 ========================================================================================================================
2024-01-31 22:03:31,737 Logging Sequence: 109_64.00
2024-01-31 22:03:31,738 	Gloss Reference :	***** ***** * ***** *** ***** * ***** ******* ***** *** ***** *** ***** * ***** * ***** * ***** ******* ***** * A     B+C+D+E
2024-01-31 22:03:31,738 	Gloss Hypothesis:	<unk> <pad> E <unk> E+B <unk> E <unk> E+B+E+C <unk> B+E <unk> E+B <unk> B <unk> C <unk> E <unk> E+B+E+C <unk> C <unk> C+E    
2024-01-31 22:03:31,738 	Gloss Alignment :	I     I     I I     I   I     I I     I       I     I   I     I   I     I I     I I     I I     I       I     I S     S      
2024-01-31 22:03:31,738 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 22:03:31,741 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** the 2   players as  well as  the entire kkr team have been quarantined
2024-01-31 22:03:31,741 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s>     <s> <s>  <s> <s> <s>    <s> <s>  <s>  <s>  <s>        
2024-01-31 22:03:31,741 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   S   S   S       S   S    S   S   S      S   S    S    S    S          
2024-01-31 22:03:31,741 ========================================================================================================================
2024-01-31 22:03:31,741 Logging Sequence: 84_108.00
2024-01-31 22:03:31,742 	Gloss Reference :	***** * ***** * ***** * ***** * ***** A B+C+D+E
2024-01-31 22:03:31,742 	Gloss Hypothesis:	<unk> C <unk> C <unk> C <unk> E <unk> C <unk>  
2024-01-31 22:03:31,742 	Gloss Alignment :	I     I I     I I     I I     I I     S S      
2024-01-31 22:03:31,742 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 22:03:31,746 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ******* ******* ******* ******* ******* so      in      order   to      show    their   protest they    covered their   mouth   in      the     photos  which   then    went    viral  
2024-01-31 22:03:31,746 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing
2024-01-31 22:03:31,746 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I       I       I       I       I       S       S       S       S       S       S       S       S       S       S       S       S       S       S       S       S       S       S      
2024-01-31 22:03:31,746 ========================================================================================================================
2024-01-31 22:03:31,746 Logging Sequence: 115_24.00
2024-01-31 22:03:31,747 	Gloss Reference :	***** * ***** *** ***** ***** A     B+C+D+E
2024-01-31 22:03:31,747 	Gloss Hypothesis:	<unk> B <unk> C+B <unk> E+B+E <unk> E      
2024-01-31 22:03:31,747 	Gloss Alignment :	I     I I     I   I     I     S     S      
2024-01-31 22:03:31,747 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 22:03:31,750 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** bumrah also did not participate in  the 5   match t20 series
2024-01-31 22:03:31,750 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s>    <s>  <s> <s> <s>         <s> <s> <s> <s>   <s> <s>   
2024-01-31 22:03:31,750 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   S      S    S   S   S           S   S   S   S     S   S     
2024-01-31 22:03:31,750 ========================================================================================================================
2024-01-31 22:03:31,750 Logging Sequence: 96_129.00
2024-01-31 22:03:31,750 	Gloss Reference :	A B+C+D+E
2024-01-31 22:03:31,750 	Gloss Hypothesis:	A E+A+E  
2024-01-31 22:03:31,750 	Gloss Alignment :	  S      
2024-01-31 22:03:31,751 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 22:03:31,752 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *********** *********** *********** *********** *********** *********** *********** *********** *********** *********** *********** *********** *********** *********** *********** *********** *********** *********** *********** *********** *********** viewers     were        very        stressed   
2024-01-31 22:03:31,752 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> interacting interacting interacting interacting interacting interacting interacting interacting interacting interacting interacting interacting interacting interacting interacting interacting interacting interacting interacting interacting interacting interacting interacting interacting interacting
2024-01-31 22:03:31,752 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I           I           I           I           I           I           I           I           I           I           I           I           I           I           I           I           I           I           I           I           I           S           S           S           S          
2024-01-31 22:03:31,752 ========================================================================================================================
2024-01-31 22:03:31,756 Epoch 1000: Total Training Recognition Loss 1101.69  Total Training Translation Loss 3486.96 
2024-01-31 22:03:31,756 EPOCH 1001
2024-01-31 22:03:49,226 Epoch 1001: Total Training Recognition Loss 1100.83  Total Training Translation Loss 3487.57 
2024-01-31 22:03:49,226 EPOCH 1002
2024-01-31 22:04:06,762 Epoch 1002: Total Training Recognition Loss 1101.19  Total Training Translation Loss 3486.64 
2024-01-31 22:04:06,762 EPOCH 1003
2024-01-31 22:04:23,198 [Epoch: 1003 Step: 00034100] Batch Recognition Loss:  31.593479 => Gls Tokens per Sec:      608 || Batch Translation Loss:  94.857834 => Txt Tokens per Sec:     1673 || Lr: 0.000050
2024-01-31 22:04:24,387 Epoch 1003: Total Training Recognition Loss 1100.32  Total Training Translation Loss 3487.06 
2024-01-31 22:04:24,387 EPOCH 1004
2024-01-31 22:04:41,980 Epoch 1004: Total Training Recognition Loss 1098.85  Total Training Translation Loss 3487.03 
2024-01-31 22:04:41,980 EPOCH 1005
2024-01-31 22:04:59,584 Epoch 1005: Total Training Recognition Loss 1100.69  Total Training Translation Loss 3487.03 
2024-01-31 22:04:59,584 EPOCH 1006
2024-01-31 22:05:14,054 [Epoch: 1006 Step: 00034200] Batch Recognition Loss:  30.394617 => Gls Tokens per Sec:      646 || Batch Translation Loss: 106.744232 => Txt Tokens per Sec:     1812 || Lr: 0.000050
2024-01-31 22:05:17,066 Epoch 1006: Total Training Recognition Loss 1100.87  Total Training Translation Loss 3488.43 
2024-01-31 22:05:17,066 EPOCH 1007
2024-01-31 22:05:34,527 Epoch 1007: Total Training Recognition Loss 1101.32  Total Training Translation Loss 3485.60 
2024-01-31 22:05:34,527 EPOCH 1008
2024-01-31 22:05:51,941 Epoch 1008: Total Training Recognition Loss 1097.86  Total Training Translation Loss 3486.81 
2024-01-31 22:05:51,941 EPOCH 1009
2024-01-31 22:06:07,323 [Epoch: 1009 Step: 00034300] Batch Recognition Loss:  38.749916 => Gls Tokens per Sec:      583 || Batch Translation Loss: 120.937454 => Txt Tokens per Sec:     1678 || Lr: 0.000050
2024-01-31 22:06:09,523 Epoch 1009: Total Training Recognition Loss 1099.97  Total Training Translation Loss 3487.22 
2024-01-31 22:06:09,523 EPOCH 1010
2024-01-31 22:06:27,096 Epoch 1010: Total Training Recognition Loss 1102.09  Total Training Translation Loss 3487.26 
2024-01-31 22:06:27,096 EPOCH 1011
2024-01-31 22:06:44,581 Epoch 1011: Total Training Recognition Loss 1099.72  Total Training Translation Loss 3487.65 
2024-01-31 22:06:44,582 EPOCH 1012
2024-01-31 22:06:57,504 [Epoch: 1012 Step: 00034400] Batch Recognition Loss:  18.759977 => Gls Tokens per Sec:      625 || Batch Translation Loss:  89.012978 => Txt Tokens per Sec:     1701 || Lr: 0.000050
2024-01-31 22:07:02,169 Epoch 1012: Total Training Recognition Loss 1099.08  Total Training Translation Loss 3487.16 
2024-01-31 22:07:02,169 EPOCH 1013
2024-01-31 22:07:19,841 Epoch 1013: Total Training Recognition Loss 1100.83  Total Training Translation Loss 3486.85 
2024-01-31 22:07:19,841 EPOCH 1014
2024-01-31 22:07:37,347 Epoch 1014: Total Training Recognition Loss 1100.03  Total Training Translation Loss 3487.24 
2024-01-31 22:07:37,348 EPOCH 1015
2024-01-31 22:07:50,147 [Epoch: 1015 Step: 00034500] Batch Recognition Loss:  36.903736 => Gls Tokens per Sec:      600 || Batch Translation Loss: 119.884026 => Txt Tokens per Sec:     1675 || Lr: 0.000050
2024-01-31 22:07:55,007 Epoch 1015: Total Training Recognition Loss 1100.48  Total Training Translation Loss 3487.27 
2024-01-31 22:07:55,007 EPOCH 1016
2024-01-31 22:08:12,688 Epoch 1016: Total Training Recognition Loss 1101.66  Total Training Translation Loss 3486.90 
2024-01-31 22:08:12,688 EPOCH 1017
2024-01-31 22:08:30,224 Epoch 1017: Total Training Recognition Loss 1099.31  Total Training Translation Loss 3486.54 
2024-01-31 22:08:30,224 EPOCH 1018
2024-01-31 22:08:41,340 [Epoch: 1018 Step: 00034600] Batch Recognition Loss:  38.300762 => Gls Tokens per Sec:      633 || Batch Translation Loss: 120.336014 => Txt Tokens per Sec:     1736 || Lr: 0.000050
2024-01-31 22:08:47,744 Epoch 1018: Total Training Recognition Loss 1099.02  Total Training Translation Loss 3487.62 
2024-01-31 22:08:47,745 EPOCH 1019
2024-01-31 22:09:05,360 Epoch 1019: Total Training Recognition Loss 1100.56  Total Training Translation Loss 3487.09 
2024-01-31 22:09:05,360 EPOCH 1020
2024-01-31 22:09:23,039 Epoch 1020: Total Training Recognition Loss 1099.79  Total Training Translation Loss 3486.53 
2024-01-31 22:09:23,039 EPOCH 1021
2024-01-31 22:09:33,634 [Epoch: 1021 Step: 00034700] Batch Recognition Loss:  23.643242 => Gls Tokens per Sec:      581 || Batch Translation Loss:  93.864456 => Txt Tokens per Sec:     1608 || Lr: 0.000050
2024-01-31 22:09:40,622 Epoch 1021: Total Training Recognition Loss 1101.51  Total Training Translation Loss 3487.87 
2024-01-31 22:09:40,623 EPOCH 1022
2024-01-31 22:09:58,084 Epoch 1022: Total Training Recognition Loss 1100.58  Total Training Translation Loss 3487.09 
2024-01-31 22:09:58,084 EPOCH 1023
2024-01-31 22:10:15,612 Epoch 1023: Total Training Recognition Loss 1103.03  Total Training Translation Loss 3486.61 
2024-01-31 22:10:15,612 EPOCH 1024
2024-01-31 22:10:25,894 [Epoch: 1024 Step: 00034800] Batch Recognition Loss:  37.623165 => Gls Tokens per Sec:      536 || Batch Translation Loss: 113.095039 => Txt Tokens per Sec:     1589 || Lr: 0.000050
2024-01-31 22:10:33,227 Epoch 1024: Total Training Recognition Loss 1098.99  Total Training Translation Loss 3486.31 
2024-01-31 22:10:33,227 EPOCH 1025
2024-01-31 22:10:50,857 Epoch 1025: Total Training Recognition Loss 1102.42  Total Training Translation Loss 3486.50 
2024-01-31 22:10:50,857 EPOCH 1026
2024-01-31 22:11:08,262 Epoch 1026: Total Training Recognition Loss 1101.11  Total Training Translation Loss 3487.07 
2024-01-31 22:11:08,262 EPOCH 1027
2024-01-31 22:11:15,705 [Epoch: 1027 Step: 00034900] Batch Recognition Loss:  27.535461 => Gls Tokens per Sec:      654 || Batch Translation Loss:  92.455719 => Txt Tokens per Sec:     1741 || Lr: 0.000050
2024-01-31 22:11:25,984 Epoch 1027: Total Training Recognition Loss 1102.56  Total Training Translation Loss 3487.61 
2024-01-31 22:11:25,984 EPOCH 1028
2024-01-31 22:11:43,442 Epoch 1028: Total Training Recognition Loss 1101.12  Total Training Translation Loss 3487.23 
2024-01-31 22:11:43,442 EPOCH 1029
2024-01-31 22:12:01,055 Epoch 1029: Total Training Recognition Loss 1101.89  Total Training Translation Loss 3486.80 
2024-01-31 22:12:01,055 EPOCH 1030
2024-01-31 22:12:06,649 [Epoch: 1030 Step: 00035000] Batch Recognition Loss:  11.005052 => Gls Tokens per Sec:      757 || Batch Translation Loss:  62.949982 => Txt Tokens per Sec:     1976 || Lr: 0.000050
2024-01-31 22:12:18,560 Epoch 1030: Total Training Recognition Loss 1100.76  Total Training Translation Loss 3486.87 
2024-01-31 22:12:18,560 EPOCH 1031
2024-01-31 22:12:36,208 Epoch 1031: Total Training Recognition Loss 1099.95  Total Training Translation Loss 3487.00 
2024-01-31 22:12:36,208 EPOCH 1032
2024-01-31 22:12:53,790 Epoch 1032: Total Training Recognition Loss 1101.61  Total Training Translation Loss 3487.62 
2024-01-31 22:12:53,790 EPOCH 1033
2024-01-31 22:13:00,137 [Epoch: 1033 Step: 00035100] Batch Recognition Loss:  51.198090 => Gls Tokens per Sec:      566 || Batch Translation Loss: 129.045670 => Txt Tokens per Sec:     1597 || Lr: 0.000050
2024-01-31 22:13:11,495 Epoch 1033: Total Training Recognition Loss 1099.82  Total Training Translation Loss 3486.97 
2024-01-31 22:13:11,495 EPOCH 1034
2024-01-31 22:13:29,187 Epoch 1034: Total Training Recognition Loss 1100.65  Total Training Translation Loss 3487.37 
2024-01-31 22:13:29,187 EPOCH 1035
2024-01-31 22:13:46,794 Epoch 1035: Total Training Recognition Loss 1100.40  Total Training Translation Loss 3485.73 
2024-01-31 22:13:46,795 EPOCH 1036
2024-01-31 22:13:51,857 [Epoch: 1036 Step: 00035200] Batch Recognition Loss:  28.383232 => Gls Tokens per Sec:      632 || Batch Translation Loss:  94.384888 => Txt Tokens per Sec:     1641 || Lr: 0.000050
2024-01-31 22:14:04,483 Epoch 1036: Total Training Recognition Loss 1099.71  Total Training Translation Loss 3487.65 
2024-01-31 22:14:04,483 EPOCH 1037
2024-01-31 22:14:21,995 Epoch 1037: Total Training Recognition Loss 1101.00  Total Training Translation Loss 3487.17 
2024-01-31 22:14:21,995 EPOCH 1038
2024-01-31 22:14:39,561 Epoch 1038: Total Training Recognition Loss 1101.75  Total Training Translation Loss 3487.16 
2024-01-31 22:14:39,561 EPOCH 1039
2024-01-31 22:14:43,292 [Epoch: 1039 Step: 00035300] Batch Recognition Loss:  27.621647 => Gls Tokens per Sec:      619 || Batch Translation Loss:  86.290207 => Txt Tokens per Sec:     1689 || Lr: 0.000050
2024-01-31 22:14:56,955 Epoch 1039: Total Training Recognition Loss 1100.08  Total Training Translation Loss 3486.71 
2024-01-31 22:14:56,955 EPOCH 1040
2024-01-31 22:15:14,562 Epoch 1040: Total Training Recognition Loss 1100.96  Total Training Translation Loss 3487.11 
2024-01-31 22:15:14,562 EPOCH 1041
2024-01-31 22:15:32,168 Epoch 1041: Total Training Recognition Loss 1100.00  Total Training Translation Loss 3487.20 
2024-01-31 22:15:32,168 EPOCH 1042
2024-01-31 22:15:34,781 [Epoch: 1042 Step: 00035400] Batch Recognition Loss:  33.542606 => Gls Tokens per Sec:      735 || Batch Translation Loss:  98.758545 => Txt Tokens per Sec:     1976 || Lr: 0.000050
2024-01-31 22:15:49,840 Epoch 1042: Total Training Recognition Loss 1100.74  Total Training Translation Loss 3487.22 
2024-01-31 22:15:49,840 EPOCH 1043
2024-01-31 22:16:07,371 Epoch 1043: Total Training Recognition Loss 1100.84  Total Training Translation Loss 3487.11 
2024-01-31 22:16:07,371 EPOCH 1044
2024-01-31 22:16:24,996 Epoch 1044: Total Training Recognition Loss 1101.62  Total Training Translation Loss 3486.58 
2024-01-31 22:16:24,996 EPOCH 1045
2024-01-31 22:16:26,362 [Epoch: 1045 Step: 00035500] Batch Recognition Loss:  34.799126 => Gls Tokens per Sec:      938 || Batch Translation Loss: 107.280006 => Txt Tokens per Sec:     2292 || Lr: 0.000050
2024-01-31 22:16:42,576 Epoch 1045: Total Training Recognition Loss 1103.48  Total Training Translation Loss 3487.11 
2024-01-31 22:16:42,577 EPOCH 1046
2024-01-31 22:17:00,250 Epoch 1046: Total Training Recognition Loss 1099.93  Total Training Translation Loss 3488.03 
2024-01-31 22:17:00,250 EPOCH 1047
2024-01-31 22:17:17,718 Epoch 1047: Total Training Recognition Loss 1102.00  Total Training Translation Loss 3486.86 
2024-01-31 22:17:17,718 EPOCH 1048
2024-01-31 22:17:18,241 [Epoch: 1048 Step: 00035600] Batch Recognition Loss:  23.266607 => Gls Tokens per Sec:     1230 || Batch Translation Loss:  97.772186 => Txt Tokens per Sec:     2862 || Lr: 0.000050
2024-01-31 22:17:35,425 Epoch 1048: Total Training Recognition Loss 1100.03  Total Training Translation Loss 3487.52 
2024-01-31 22:17:35,425 EPOCH 1049
2024-01-31 22:17:53,006 Epoch 1049: Total Training Recognition Loss 1103.50  Total Training Translation Loss 3488.07 
2024-01-31 22:17:53,006 EPOCH 1050
2024-01-31 22:18:10,599 [Epoch: 1050 Step: 00035700] Batch Recognition Loss:  23.084141 => Gls Tokens per Sec:      604 || Batch Translation Loss:  84.568359 => Txt Tokens per Sec:     1677 || Lr: 0.000050
2024-01-31 22:18:10,599 Epoch 1050: Total Training Recognition Loss 1102.90  Total Training Translation Loss 3487.19 
2024-01-31 22:18:10,599 EPOCH 1051
2024-01-31 22:18:28,353 Epoch 1051: Total Training Recognition Loss 1100.82  Total Training Translation Loss 3486.93 
2024-01-31 22:18:28,353 EPOCH 1052
2024-01-31 22:18:45,969 Epoch 1052: Total Training Recognition Loss 1101.64  Total Training Translation Loss 3487.64 
2024-01-31 22:18:45,969 EPOCH 1053
2024-01-31 22:19:01,656 [Epoch: 1053 Step: 00035800] Batch Recognition Loss:  27.765606 => Gls Tokens per Sec:      637 || Batch Translation Loss:  89.594521 => Txt Tokens per Sec:     1767 || Lr: 0.000050
2024-01-31 22:19:03,632 Epoch 1053: Total Training Recognition Loss 1102.56  Total Training Translation Loss 3487.38 
2024-01-31 22:19:03,632 EPOCH 1054
2024-01-31 22:19:21,293 Epoch 1054: Total Training Recognition Loss 1102.88  Total Training Translation Loss 3486.99 
2024-01-31 22:19:21,293 EPOCH 1055
2024-01-31 22:19:38,833 Epoch 1055: Total Training Recognition Loss 1099.11  Total Training Translation Loss 3488.41 
2024-01-31 22:19:38,833 EPOCH 1056
2024-01-31 22:19:55,002 [Epoch: 1056 Step: 00035900] Batch Recognition Loss:  35.141983 => Gls Tokens per Sec:      578 || Batch Translation Loss: 114.828125 => Txt Tokens per Sec:     1615 || Lr: 0.000050
2024-01-31 22:19:56,494 Epoch 1056: Total Training Recognition Loss 1100.70  Total Training Translation Loss 3487.55 
2024-01-31 22:19:56,494 EPOCH 1057
2024-01-31 22:20:14,083 Epoch 1057: Total Training Recognition Loss 1100.72  Total Training Translation Loss 3487.29 
2024-01-31 22:20:14,083 EPOCH 1058
2024-01-31 22:20:31,575 Epoch 1058: Total Training Recognition Loss 1100.45  Total Training Translation Loss 3487.19 
2024-01-31 22:20:31,575 EPOCH 1059
2024-01-31 22:20:47,167 [Epoch: 1059 Step: 00036000] Batch Recognition Loss:  23.382881 => Gls Tokens per Sec:      559 || Batch Translation Loss:  98.786499 => Txt Tokens per Sec:     1602 || Lr: 0.000050
2024-01-31 22:21:11,162 Validation result at epoch 1059, step    36000: duration: 23.9938s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 350.84470	Translation Loss: 73486.10938	PPL: 1562.05664
	Eval Metric: BLEU
	WER 534.11	(DEL: 4.31,	INS: 444.99,	SUB: 84.82)
	BLEU-4 0.00	(BLEU-1: 0.01,	BLEU-2: 0.00,	BLEU-3: 0.00,	BLEU-4: 0.00)
	CHRF 3.42	ROUGE 0.02
2024-01-31 22:21:11,163 Logging Recognition and Translation Outputs
2024-01-31 22:21:11,163 ========================================================================================================================
2024-01-31 22:21:11,164 Logging Sequence: 78_198.00
2024-01-31 22:21:11,164 	Gloss Reference :	***** * ***** *** ***** ********* ***** * ***** * ***** * ***** * ***** ***** A     B+C+D+E
2024-01-31 22:21:11,164 	Gloss Hypothesis:	<unk> E <unk> E+B <unk> E+B+E+D+E <unk> E <unk> E <unk> E <unk> E <unk> E+C+E <unk> E+C+E  
2024-01-31 22:21:11,164 	Gloss Alignment :	I     I I     I   I     I         I     I I     I I     I I     I I     I     S     S      
2024-01-31 22:21:11,164 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 22:21:11,166 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** they  have  been  flooded with  congratulations comments
2024-01-31 22:21:11,166 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha   sabha sabha           sabha   
2024-01-31 22:21:11,166 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     S     S     S     S       S     S               S       
2024-01-31 22:21:11,167 ========================================================================================================================
2024-01-31 22:21:11,167 Logging Sequence: 145_216.00
2024-01-31 22:21:11,167 	Gloss Reference :	* *** ***** ***** ***** A B+C+D+E
2024-01-31 22:21:11,167 	Gloss Hypothesis:	E A+E <unk> E+B+E <unk> E <unk>  
2024-01-31 22:21:11,167 	Gloss Alignment :	I I   I     I     I     S S      
2024-01-31 22:21:11,167 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 22:21:11,171 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ******* ******* ******* ******* ******* ******* ******* ******* asking  him     to      include sameeha in      the     world   championship as      she     was     a       talented athlete
2024-01-31 22:21:11,171 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing      nothing nothing nothing nothing nothing  nothing
2024-01-31 22:21:11,171 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I       I       I       I       I       I       I       I       S       S       S       S       S       S       S       S       S            S       S       S       S       S        S      
2024-01-31 22:21:11,171 ========================================================================================================================
2024-01-31 22:21:11,171 Logging Sequence: 70_137.00
2024-01-31 22:21:11,172 	Gloss Reference :	***** * ***** *** ***** * ***** * ***** * ***** * ***** ******* ***** * ***** *** A     B+C+D+E
2024-01-31 22:21:11,172 	Gloss Hypothesis:	<unk> E <unk> E+C <unk> E <unk> E <unk> E <unk> E <unk> E+C+E+C <unk> C <unk> C+E <unk> C+E+C  
2024-01-31 22:21:11,172 	Gloss Alignment :	I     I I     I   I     I I     I I     I I     I I     I       I     I I     I   S     S      
2024-01-31 22:21:11,172 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 22:21:11,175 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** the small gesture appeared to  encourage people to  drink water instead of  aerated drinks
2024-01-31 22:21:11,175 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s>   <s>     <s>      <s> <s>       <s>    <s> <s>   <s>   <s>     <s> <s>     <s>   
2024-01-31 22:21:11,175 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   S   S     S       S        S   S         S      S   S     S     S       S   S       S     
2024-01-31 22:21:11,175 ========================================================================================================================
2024-01-31 22:21:11,176 Logging Sequence: 119_20.00
2024-01-31 22:21:11,176 	Gloss Reference :	* ***** * ***** * ***** ***** A     B+C+D+E
2024-01-31 22:21:11,176 	Gloss Hypothesis:	B <pad> B <pad> B <pad> B+E+B <pad> E+B+E+B
2024-01-31 22:21:11,176 	Gloss Alignment :	I I     I I     I I     I     S     S      
2024-01-31 22:21:11,176 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 22:21:11,180 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** *** ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* messi         intended      to            gift          something     to            all           the           players       and           the           staff         to            special       to            celebrate     the           moment       
2024-01-31 22:21:11,180 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions contributions
2024-01-31 22:21:11,180 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I   I             I             I             I             I             I             I             I             I             I             S             S             S             S             S             S             S             S             S             S             S             S             S             S             S             S             S             S            
2024-01-31 22:21:11,180 ========================================================================================================================
2024-01-31 22:21:11,181 Logging Sequence: 106_15.00
2024-01-31 22:21:11,181 	Gloss Reference :	***** * ***** * ***** A   B+C+D+E
2024-01-31 22:21:11,181 	Gloss Hypothesis:	<unk> C <unk> E <unk> E+C <unk>  
2024-01-31 22:21:11,181 	Gloss Alignment :	I     I I     I I     S   S      
2024-01-31 22:21:11,181 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 22:21:11,184 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** but what about women's cricket earlier we  never spoke about it 
2024-01-31 22:21:11,184 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s>  <s>   <s>     <s>     <s>     <s> <s>   <s>   <s>   <s>
2024-01-31 22:21:11,184 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   S   S    S     S       S       S       S   S     S     S     S  
2024-01-31 22:21:11,184 ========================================================================================================================
2024-01-31 22:21:13,211 Epoch 1059: Total Training Recognition Loss 1099.59  Total Training Translation Loss 3487.87 
2024-01-31 22:21:13,211 EPOCH 1060
2024-01-31 22:21:30,902 Epoch 1060: Total Training Recognition Loss 1099.88  Total Training Translation Loss 3487.14 
2024-01-31 22:21:30,902 EPOCH 1061
2024-01-31 22:21:48,535 Epoch 1061: Total Training Recognition Loss 1100.82  Total Training Translation Loss 3487.40 
2024-01-31 22:21:48,535 EPOCH 1062
2024-01-31 22:22:01,442 [Epoch: 1062 Step: 00036100] Batch Recognition Loss:  18.764347 => Gls Tokens per Sec:      645 || Batch Translation Loss:  91.143417 => Txt Tokens per Sec:     1810 || Lr: 0.000050
2024-01-31 22:22:06,380 Epoch 1062: Total Training Recognition Loss 1100.45  Total Training Translation Loss 3488.26 
2024-01-31 22:22:06,380 EPOCH 1063
2024-01-31 22:22:26,907 Epoch 1063: Total Training Recognition Loss 1102.29  Total Training Translation Loss 3487.14 
2024-01-31 22:22:26,907 EPOCH 1064
2024-01-31 22:22:44,851 Epoch 1064: Total Training Recognition Loss 1100.91  Total Training Translation Loss 3487.65 
2024-01-31 22:22:44,851 EPOCH 1065
2024-01-31 22:22:56,148 [Epoch: 1065 Step: 00036200] Batch Recognition Loss:  27.722572 => Gls Tokens per Sec:      680 || Batch Translation Loss:  90.275116 => Txt Tokens per Sec:     1884 || Lr: 0.000050
2024-01-31 22:23:02,534 Epoch 1065: Total Training Recognition Loss 1097.67  Total Training Translation Loss 3486.23 
2024-01-31 22:23:02,534 EPOCH 1066
2024-01-31 22:23:20,181 Epoch 1066: Total Training Recognition Loss 1099.96  Total Training Translation Loss 3487.25 
2024-01-31 22:23:20,181 EPOCH 1067
2024-01-31 22:23:37,804 Epoch 1067: Total Training Recognition Loss 1099.77  Total Training Translation Loss 3487.21 
2024-01-31 22:23:37,805 EPOCH 1068
2024-01-31 22:23:49,229 [Epoch: 1068 Step: 00036300] Batch Recognition Loss:  23.039534 => Gls Tokens per Sec:      594 || Batch Translation Loss:  87.597122 => Txt Tokens per Sec:     1647 || Lr: 0.000050
2024-01-31 22:23:55,363 Epoch 1068: Total Training Recognition Loss 1100.12  Total Training Translation Loss 3487.03 
2024-01-31 22:23:55,363 EPOCH 1069
2024-01-31 22:24:13,122 Epoch 1069: Total Training Recognition Loss 1100.22  Total Training Translation Loss 3487.58 
2024-01-31 22:24:13,123 EPOCH 1070
2024-01-31 22:24:30,922 Epoch 1070: Total Training Recognition Loss 1101.19  Total Training Translation Loss 3487.62 
2024-01-31 22:24:30,922 EPOCH 1071
2024-01-31 22:24:39,430 [Epoch: 1071 Step: 00036400] Batch Recognition Loss:  46.325684 => Gls Tokens per Sec:      723 || Batch Translation Loss: 124.364288 => Txt Tokens per Sec:     1865 || Lr: 0.000050
2024-01-31 22:24:48,680 Epoch 1071: Total Training Recognition Loss 1098.82  Total Training Translation Loss 3487.81 
2024-01-31 22:24:48,681 EPOCH 1072
2024-01-31 22:25:06,282 Epoch 1072: Total Training Recognition Loss 1101.64  Total Training Translation Loss 3486.49 
2024-01-31 22:25:06,282 EPOCH 1073
2024-01-31 22:25:23,747 Epoch 1073: Total Training Recognition Loss 1100.94  Total Training Translation Loss 3486.73 
2024-01-31 22:25:23,747 EPOCH 1074
2024-01-31 22:25:33,074 [Epoch: 1074 Step: 00036500] Batch Recognition Loss:  17.165726 => Gls Tokens per Sec:      591 || Batch Translation Loss:  77.128738 => Txt Tokens per Sec:     1589 || Lr: 0.000050
2024-01-31 22:25:41,475 Epoch 1074: Total Training Recognition Loss 1101.20  Total Training Translation Loss 3487.04 
2024-01-31 22:25:41,475 EPOCH 1075
2024-01-31 22:25:59,147 Epoch 1075: Total Training Recognition Loss 1105.19  Total Training Translation Loss 3486.81 
2024-01-31 22:25:59,148 EPOCH 1076
2024-01-31 22:26:16,547 Epoch 1076: Total Training Recognition Loss 1102.59  Total Training Translation Loss 3487.96 
2024-01-31 22:26:16,547 EPOCH 1077
2024-01-31 22:26:25,922 [Epoch: 1077 Step: 00036600] Batch Recognition Loss:  57.477646 => Gls Tokens per Sec:      546 || Batch Translation Loss: 132.285080 => Txt Tokens per Sec:     1557 || Lr: 0.000050
2024-01-31 22:26:33,944 Epoch 1077: Total Training Recognition Loss 1100.09  Total Training Translation Loss 3487.01 
2024-01-31 22:26:33,944 EPOCH 1078
2024-01-31 22:26:51,428 Epoch 1078: Total Training Recognition Loss 1101.28  Total Training Translation Loss 3487.58 
2024-01-31 22:26:51,428 EPOCH 1079
2024-01-31 22:27:08,927 Epoch 1079: Total Training Recognition Loss 1098.93  Total Training Translation Loss 3486.39 
2024-01-31 22:27:08,927 EPOCH 1080
2024-01-31 22:27:14,540 [Epoch: 1080 Step: 00036700] Batch Recognition Loss:  23.140162 => Gls Tokens per Sec:      754 || Batch Translation Loss:  86.330788 => Txt Tokens per Sec:     1935 || Lr: 0.000050
2024-01-31 22:27:26,355 Epoch 1080: Total Training Recognition Loss 1102.04  Total Training Translation Loss 3487.23 
2024-01-31 22:27:26,355 EPOCH 1081
2024-01-31 22:27:43,950 Epoch 1081: Total Training Recognition Loss 1099.93  Total Training Translation Loss 3487.26 
2024-01-31 22:27:43,950 EPOCH 1082
2024-01-31 22:28:01,454 Epoch 1082: Total Training Recognition Loss 1100.41  Total Training Translation Loss 3487.46 
2024-01-31 22:28:01,454 EPOCH 1083
2024-01-31 22:28:07,668 [Epoch: 1083 Step: 00036800] Batch Recognition Loss:  27.252329 => Gls Tokens per Sec:      578 || Batch Translation Loss:  96.398773 => Txt Tokens per Sec:     1635 || Lr: 0.000050
2024-01-31 22:28:18,856 Epoch 1083: Total Training Recognition Loss 1101.06  Total Training Translation Loss 3487.01 
2024-01-31 22:28:18,856 EPOCH 1084
2024-01-31 22:28:36,523 Epoch 1084: Total Training Recognition Loss 1100.34  Total Training Translation Loss 3487.07 
2024-01-31 22:28:36,523 EPOCH 1085
2024-01-31 22:28:54,161 Epoch 1085: Total Training Recognition Loss 1101.77  Total Training Translation Loss 3487.35 
2024-01-31 22:28:54,161 EPOCH 1086
2024-01-31 22:29:00,150 [Epoch: 1086 Step: 00036900] Batch Recognition Loss:  15.096872 => Gls Tokens per Sec:      534 || Batch Translation Loss:  77.489258 => Txt Tokens per Sec:     1507 || Lr: 0.000050
2024-01-31 22:29:11,703 Epoch 1086: Total Training Recognition Loss 1101.30  Total Training Translation Loss 3486.48 
2024-01-31 22:29:11,704 EPOCH 1087
2024-01-31 22:29:29,303 Epoch 1087: Total Training Recognition Loss 1098.84  Total Training Translation Loss 3488.00 
2024-01-31 22:29:29,303 EPOCH 1088
2024-01-31 22:29:46,994 Epoch 1088: Total Training Recognition Loss 1101.70  Total Training Translation Loss 3487.75 
2024-01-31 22:29:46,994 EPOCH 1089
2024-01-31 22:29:50,819 [Epoch: 1089 Step: 00037000] Batch Recognition Loss:  18.493250 => Gls Tokens per Sec:      669 || Batch Translation Loss:  87.251648 => Txt Tokens per Sec:     1802 || Lr: 0.000050
2024-01-31 22:30:04,579 Epoch 1089: Total Training Recognition Loss 1101.65  Total Training Translation Loss 3486.25 
2024-01-31 22:30:04,579 EPOCH 1090
2024-01-31 22:30:22,181 Epoch 1090: Total Training Recognition Loss 1100.85  Total Training Translation Loss 3486.82 
2024-01-31 22:30:22,181 EPOCH 1091
2024-01-31 22:30:39,677 Epoch 1091: Total Training Recognition Loss 1100.45  Total Training Translation Loss 3487.15 
2024-01-31 22:30:39,678 EPOCH 1092
2024-01-31 22:30:43,072 [Epoch: 1092 Step: 00037100] Batch Recognition Loss:  44.943211 => Gls Tokens per Sec:      566 || Batch Translation Loss: 129.911591 => Txt Tokens per Sec:     1618 || Lr: 0.000050
2024-01-31 22:30:57,250 Epoch 1092: Total Training Recognition Loss 1099.85  Total Training Translation Loss 3486.36 
2024-01-31 22:30:57,251 EPOCH 1093
2024-01-31 22:31:14,755 Epoch 1093: Total Training Recognition Loss 1102.06  Total Training Translation Loss 3486.98 
2024-01-31 22:31:14,756 EPOCH 1094
2024-01-31 22:31:32,382 Epoch 1094: Total Training Recognition Loss 1101.03  Total Training Translation Loss 3487.28 
2024-01-31 22:31:32,383 EPOCH 1095
2024-01-31 22:31:34,809 [Epoch: 1095 Step: 00037200] Batch Recognition Loss:  31.315632 => Gls Tokens per Sec:      528 || Batch Translation Loss:  92.637581 => Txt Tokens per Sec:     1518 || Lr: 0.000050
2024-01-31 22:31:49,989 Epoch 1095: Total Training Recognition Loss 1102.75  Total Training Translation Loss 3487.83 
2024-01-31 22:31:49,989 EPOCH 1096
2024-01-31 22:32:07,520 Epoch 1096: Total Training Recognition Loss 1103.81  Total Training Translation Loss 3487.26 
2024-01-31 22:32:07,520 EPOCH 1097
2024-01-31 22:32:25,328 Epoch 1097: Total Training Recognition Loss 1099.88  Total Training Translation Loss 3488.07 
2024-01-31 22:32:25,328 EPOCH 1098
2024-01-31 22:32:26,528 [Epoch: 1098 Step: 00037300] Batch Recognition Loss:  74.391350 => Gls Tokens per Sec:      326 || Batch Translation Loss: 126.471634 => Txt Tokens per Sec:      926 || Lr: 0.000050
2024-01-31 22:32:43,035 Epoch 1098: Total Training Recognition Loss 1103.74  Total Training Translation Loss 3486.65 
2024-01-31 22:32:43,035 EPOCH 1099
2024-01-31 22:33:00,669 Epoch 1099: Total Training Recognition Loss 1102.50  Total Training Translation Loss 3486.45 
2024-01-31 22:33:00,669 EPOCH 1100
2024-01-31 22:33:18,313 [Epoch: 1100 Step: 00037400] Batch Recognition Loss:  33.888954 => Gls Tokens per Sec:      603 || Batch Translation Loss:  98.552361 => Txt Tokens per Sec:     1673 || Lr: 0.000050
2024-01-31 22:33:18,314 Epoch 1100: Total Training Recognition Loss 1101.50  Total Training Translation Loss 3486.68 
2024-01-31 22:33:18,314 EPOCH 1101
2024-01-31 22:33:36,052 Epoch 1101: Total Training Recognition Loss 1100.01  Total Training Translation Loss 3487.71 
2024-01-31 22:33:36,052 EPOCH 1102
2024-01-31 22:33:53,804 Epoch 1102: Total Training Recognition Loss 1101.37  Total Training Translation Loss 3487.15 
2024-01-31 22:33:53,804 EPOCH 1103
2024-01-31 22:34:10,319 [Epoch: 1103 Step: 00037500] Batch Recognition Loss:  31.534777 => Gls Tokens per Sec:      605 || Batch Translation Loss:  94.560226 => Txt Tokens per Sec:     1666 || Lr: 0.000050
2024-01-31 22:34:11,399 Epoch 1103: Total Training Recognition Loss 1098.56  Total Training Translation Loss 3487.96 
2024-01-31 22:34:11,400 EPOCH 1104
2024-01-31 22:34:28,850 Epoch 1104: Total Training Recognition Loss 1100.48  Total Training Translation Loss 3488.06 
2024-01-31 22:34:28,850 EPOCH 1105
2024-01-31 22:34:46,431 Epoch 1105: Total Training Recognition Loss 1098.79  Total Training Translation Loss 3487.00 
2024-01-31 22:34:46,431 EPOCH 1106
2024-01-31 22:35:01,695 [Epoch: 1106 Step: 00037600] Batch Recognition Loss:  23.485859 => Gls Tokens per Sec:      613 || Batch Translation Loss:  95.334335 => Txt Tokens per Sec:     1704 || Lr: 0.000050
2024-01-31 22:35:03,980 Epoch 1106: Total Training Recognition Loss 1100.55  Total Training Translation Loss 3487.23 
2024-01-31 22:35:03,981 EPOCH 1107
2024-01-31 22:35:21,472 Epoch 1107: Total Training Recognition Loss 1101.29  Total Training Translation Loss 3486.37 
2024-01-31 22:35:21,472 EPOCH 1108
2024-01-31 22:35:38,872 Epoch 1108: Total Training Recognition Loss 1099.68  Total Training Translation Loss 3488.13 
2024-01-31 22:35:38,873 EPOCH 1109
2024-01-31 22:35:54,152 [Epoch: 1109 Step: 00037700] Batch Recognition Loss:  32.703831 => Gls Tokens per Sec:      570 || Batch Translation Loss: 108.610962 => Txt Tokens per Sec:     1606 || Lr: 0.000050
2024-01-31 22:35:56,401 Epoch 1109: Total Training Recognition Loss 1101.21  Total Training Translation Loss 3487.01 
2024-01-31 22:35:56,401 EPOCH 1110
2024-01-31 22:36:13,878 Epoch 1110: Total Training Recognition Loss 1101.21  Total Training Translation Loss 3487.42 
2024-01-31 22:36:13,878 EPOCH 1111
2024-01-31 22:36:31,476 Epoch 1111: Total Training Recognition Loss 1101.10  Total Training Translation Loss 3486.49 
2024-01-31 22:36:31,477 EPOCH 1112
2024-01-31 22:36:44,048 [Epoch: 1112 Step: 00037800] Batch Recognition Loss:  34.488541 => Gls Tokens per Sec:      662 || Batch Translation Loss: 111.576012 => Txt Tokens per Sec:     1820 || Lr: 0.000050
2024-01-31 22:36:49,106 Epoch 1112: Total Training Recognition Loss 1102.03  Total Training Translation Loss 3486.52 
2024-01-31 22:36:49,107 EPOCH 1113
2024-01-31 22:37:06,754 Epoch 1113: Total Training Recognition Loss 1101.81  Total Training Translation Loss 3487.47 
2024-01-31 22:37:06,754 EPOCH 1114
2024-01-31 22:37:24,471 Epoch 1114: Total Training Recognition Loss 1100.40  Total Training Translation Loss 3486.88 
2024-01-31 22:37:24,471 EPOCH 1115
2024-01-31 22:37:37,357 [Epoch: 1115 Step: 00037900] Batch Recognition Loss:  51.879444 => Gls Tokens per Sec:      577 || Batch Translation Loss: 129.828766 => Txt Tokens per Sec:     1663 || Lr: 0.000050
2024-01-31 22:37:42,071 Epoch 1115: Total Training Recognition Loss 1100.82  Total Training Translation Loss 3486.73 
2024-01-31 22:37:42,071 EPOCH 1116
2024-01-31 22:37:59,439 Epoch 1116: Total Training Recognition Loss 1099.83  Total Training Translation Loss 3487.97 
2024-01-31 22:37:59,439 EPOCH 1117
2024-01-31 22:38:16,914 Epoch 1117: Total Training Recognition Loss 1099.90  Total Training Translation Loss 3486.61 
2024-01-31 22:38:16,914 EPOCH 1118
2024-01-31 22:38:27,372 [Epoch: 1118 Step: 00038000] Batch Recognition Loss:  37.370892 => Gls Tokens per Sec:      649 || Batch Translation Loss: 116.558670 => Txt Tokens per Sec:     1786 || Lr: 0.000050
2024-01-31 22:38:51,803 Validation result at epoch 1118, step    38000: duration: 24.4310s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 348.55981	Translation Loss: 73463.00781	PPL: 1558.44983
	Eval Metric: BLEU
	WER 533.62	(DEL: 4.17,	INS: 444.49,	SUB: 84.96)
	BLEU-4 0.00	(BLEU-1: 0.01,	BLEU-2: 0.00,	BLEU-3: 0.00,	BLEU-4: 0.00)
	CHRF 3.50	ROUGE 0.02
2024-01-31 22:38:51,804 Logging Recognition and Translation Outputs
2024-01-31 22:38:51,804 ========================================================================================================================
2024-01-31 22:38:51,805 Logging Sequence: 72_194.00
2024-01-31 22:38:51,805 	Gloss Reference :	* ***** * ***** * ***** A ***** B+C+D+E
2024-01-31 22:38:51,805 	Gloss Hypothesis:	A <unk> A <unk> A <unk> A <unk> E+A    
2024-01-31 22:38:51,805 	Gloss Alignment :	I I     I I     I I       I     S      
2024-01-31 22:38:51,805 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 22:38:51,809 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** shah told her to  do  what she wants and filed a   police complaint against her
2024-01-31 22:38:51,809 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s>  <s>  <s> <s> <s> <s>  <s> <s>   <s> <s>   <s> <s>    <s>       <s>     <s>
2024-01-31 22:38:51,809 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   S    S    S   S   S   S    S   S     S   S     S   S      S         S       S  
2024-01-31 22:38:51,809 ========================================================================================================================
2024-01-31 22:38:51,809 Logging Sequence: 108_59.00
2024-01-31 22:38:51,809 	Gloss Reference :	A B+C+D+E
2024-01-31 22:38:51,809 	Gloss Hypothesis:	* E      
2024-01-31 22:38:51,810 	Gloss Alignment :	D S      
2024-01-31 22:38:51,810 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 22:38:51,814 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** *** ****** ****** ****** ****** ****** ****** ****** ****** ishan  kishan remained the    biggest buy    of     ipl    as     mumbai indians paid   a      whopping rs     1525   crore  to     keep   him   
2024-01-31 22:38:51,814 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> chosen chosen chosen chosen chosen chosen chosen chosen chosen chosen chosen   chosen chosen  chosen chosen chosen chosen chosen chosen  chosen chosen chosen   chosen chosen chosen chosen chosen chosen
2024-01-31 22:38:51,814 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I   I      I      I      I      I      I      I      I      S      S      S        S      S       S      S      S      S      S      S       S      S      S        S      S      S      S      S      S     
2024-01-31 22:38:51,814 ========================================================================================================================
2024-01-31 22:38:51,814 Logging Sequence: 109_10.00
2024-01-31 22:38:51,815 	Gloss Reference :	***** * ***** A B+C+D+E
2024-01-31 22:38:51,815 	Gloss Hypothesis:	<unk> E <unk> E <unk>  
2024-01-31 22:38:51,815 	Gloss Alignment :	I     I I     S S      
2024-01-31 22:38:51,815 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 22:38:51,818 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** was  scheduled to   be   played at   the  narendra modi stadium in   ahmedabad
2024-01-31 22:38:51,818 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> fine fine fine fine fine fine fine fine fine fine fine fine fine fine fine fine fine fine fine fine      fine fine fine   fine fine fine     fine fine    fine fine     
2024-01-31 22:38:51,818 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I    I    I    I    I    I    I    I    I    I    I    I    I    I    I    I    I    I    S    S         S    S    S      S    S    S        S    S       S    S        
2024-01-31 22:38:51,818 ========================================================================================================================
2024-01-31 22:38:51,818 Logging Sequence: 103_202.00
2024-01-31 22:38:51,819 	Gloss Reference :	***** * ***** * ***** * ***** * ***** ***** ***** ***** ***** *** ***** ***** ***** * ***** *** ***** ***** ***** ***** ***** A   B+C+D+E
2024-01-31 22:38:51,819 	Gloss Hypothesis:	<unk> D <unk> D <unk> E <unk> D <unk> D+E+D <unk> D+E+D <unk> D+E <unk> E+D+E <unk> E <unk> D+E <unk> D+E+D <unk> E+C+E <unk> D+E <unk>  
2024-01-31 22:38:51,819 	Gloss Alignment :	I     I I     I I     I I     I I     I     I     I     I     I   I     I     I     I I     I   I     I     I     I     I     S   S      
2024-01-31 22:38:51,819 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 22:38:51,823 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** *** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** india  in     total  has    won    61     medals including 22     gold   medals 16     silver medals 23     bronze medals
2024-01-31 22:38:51,823 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> boards boards boards boards boards boards boards boards boards boards boards boards boards boards boards boards boards boards boards    boards boards boards boards boards boards boards boards boards
2024-01-31 22:38:51,823 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I   I      I      I      I      I      I      I      I      I      I      I      S      S      S      S      S      S      S      S         S      S      S      S      S      S      S      S      S     
2024-01-31 22:38:51,823 ========================================================================================================================
2024-01-31 22:38:51,823 Logging Sequence: 149_77.00
2024-01-31 22:38:51,823 	Gloss Reference :	***** * ***** * A     B+C+D+E
2024-01-31 22:38:51,823 	Gloss Hypothesis:	<unk> D <unk> C <unk> C      
2024-01-31 22:38:51,824 	Gloss Alignment :	I     I I     I S     S      
2024-01-31 22:38:51,824 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 22:38:51,828 	Text Reference  :	***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** and   arrested danushka for   alleged sexual assault of    a     29    year  old   woman whose name  has   not   been  disclosed
2024-01-31 22:38:51,828 	Text Hypothesis :	sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha    sabha    sabha sabha   sabha  sabha   sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha    
2024-01-31 22:38:51,828 	Text Alignment  :	I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     S     S        S        S     S       S      S       S     S     S     S     S     S     S     S     S     S     S     S        
2024-01-31 22:38:51,828 ========================================================================================================================
2024-01-31 22:38:58,819 Epoch 1118: Total Training Recognition Loss 1101.47  Total Training Translation Loss 3487.60 
2024-01-31 22:38:58,820 EPOCH 1119
2024-01-31 22:39:16,385 Epoch 1119: Total Training Recognition Loss 1099.53  Total Training Translation Loss 3487.29 
2024-01-31 22:39:16,385 EPOCH 1120
2024-01-31 22:39:33,802 Epoch 1120: Total Training Recognition Loss 1100.30  Total Training Translation Loss 3487.28 
2024-01-31 22:39:33,803 EPOCH 1121
2024-01-31 22:39:44,926 [Epoch: 1121 Step: 00038100] Batch Recognition Loss:  15.150876 => Gls Tokens per Sec:      553 || Batch Translation Loss:  74.556320 => Txt Tokens per Sec:     1573 || Lr: 0.000050
2024-01-31 22:39:51,351 Epoch 1121: Total Training Recognition Loss 1099.86  Total Training Translation Loss 3486.74 
2024-01-31 22:39:51,351 EPOCH 1122
2024-01-31 22:40:08,898 Epoch 1122: Total Training Recognition Loss 1101.73  Total Training Translation Loss 3487.41 
2024-01-31 22:40:08,898 EPOCH 1123
2024-01-31 22:40:26,428 Epoch 1123: Total Training Recognition Loss 1098.23  Total Training Translation Loss 3487.17 
2024-01-31 22:40:26,428 EPOCH 1124
2024-01-31 22:40:33,880 [Epoch: 1124 Step: 00038200] Batch Recognition Loss:  27.704273 => Gls Tokens per Sec:      773 || Batch Translation Loss:  85.620506 => Txt Tokens per Sec:     2055 || Lr: 0.000050
2024-01-31 22:40:43,945 Epoch 1124: Total Training Recognition Loss 1098.93  Total Training Translation Loss 3487.44 
2024-01-31 22:40:43,946 EPOCH 1125
2024-01-31 22:41:01,582 Epoch 1125: Total Training Recognition Loss 1101.03  Total Training Translation Loss 3487.45 
2024-01-31 22:41:01,582 EPOCH 1126
2024-01-31 22:41:19,000 Epoch 1126: Total Training Recognition Loss 1099.41  Total Training Translation Loss 3487.12 
2024-01-31 22:41:19,001 EPOCH 1127
2024-01-31 22:41:26,034 [Epoch: 1127 Step: 00038300] Batch Recognition Loss:  30.248268 => Gls Tokens per Sec:      728 || Batch Translation Loss:  98.772736 => Txt Tokens per Sec:     1985 || Lr: 0.000050
2024-01-31 22:41:36,672 Epoch 1127: Total Training Recognition Loss 1101.23  Total Training Translation Loss 3487.01 
2024-01-31 22:41:36,672 EPOCH 1128
2024-01-31 22:41:54,174 Epoch 1128: Total Training Recognition Loss 1100.50  Total Training Translation Loss 3487.44 
2024-01-31 22:41:54,175 EPOCH 1129
2024-01-31 22:42:11,594 Epoch 1129: Total Training Recognition Loss 1102.20  Total Training Translation Loss 3487.51 
2024-01-31 22:42:11,594 EPOCH 1130
2024-01-31 22:42:18,717 [Epoch: 1130 Step: 00038400] Batch Recognition Loss:  33.396629 => Gls Tokens per Sec:      629 || Batch Translation Loss:  98.511551 => Txt Tokens per Sec:     1796 || Lr: 0.000050
2024-01-31 22:42:29,060 Epoch 1130: Total Training Recognition Loss 1101.64  Total Training Translation Loss 3487.45 
2024-01-31 22:42:29,060 EPOCH 1131
2024-01-31 22:42:46,631 Epoch 1131: Total Training Recognition Loss 1099.67  Total Training Translation Loss 3488.18 
2024-01-31 22:42:46,631 EPOCH 1132
2024-01-31 22:43:04,189 Epoch 1132: Total Training Recognition Loss 1098.66  Total Training Translation Loss 3487.84 
2024-01-31 22:43:04,190 EPOCH 1133
2024-01-31 22:43:09,442 [Epoch: 1133 Step: 00038500] Batch Recognition Loss:  23.776302 => Gls Tokens per Sec:      731 || Batch Translation Loss:  95.331215 => Txt Tokens per Sec:     2034 || Lr: 0.000050
2024-01-31 22:43:21,700 Epoch 1133: Total Training Recognition Loss 1101.91  Total Training Translation Loss 3487.15 
2024-01-31 22:43:21,700 EPOCH 1134
2024-01-31 22:43:39,183 Epoch 1134: Total Training Recognition Loss 1100.89  Total Training Translation Loss 3486.44 
2024-01-31 22:43:39,184 EPOCH 1135
2024-01-31 22:43:56,677 Epoch 1135: Total Training Recognition Loss 1098.94  Total Training Translation Loss 3487.56 
2024-01-31 22:43:56,677 EPOCH 1136
2024-01-31 22:44:02,570 [Epoch: 1136 Step: 00038600] Batch Recognition Loss:  27.541702 => Gls Tokens per Sec:      543 || Batch Translation Loss:  97.714973 => Txt Tokens per Sec:     1590 || Lr: 0.000050
2024-01-31 22:44:14,140 Epoch 1136: Total Training Recognition Loss 1100.78  Total Training Translation Loss 3486.89 
2024-01-31 22:44:14,141 EPOCH 1137
2024-01-31 22:44:31,666 Epoch 1137: Total Training Recognition Loss 1099.41  Total Training Translation Loss 3486.87 
2024-01-31 22:44:31,666 EPOCH 1138
2024-01-31 22:44:49,377 Epoch 1138: Total Training Recognition Loss 1098.41  Total Training Translation Loss 3487.64 
2024-01-31 22:44:49,377 EPOCH 1139
2024-01-31 22:44:53,243 [Epoch: 1139 Step: 00038700] Batch Recognition Loss:  27.964893 => Gls Tokens per Sec:      662 || Batch Translation Loss:  96.685333 => Txt Tokens per Sec:     1951 || Lr: 0.000050
2024-01-31 22:45:07,005 Epoch 1139: Total Training Recognition Loss 1102.08  Total Training Translation Loss 3487.42 
2024-01-31 22:45:07,005 EPOCH 1140
2024-01-31 22:45:24,566 Epoch 1140: Total Training Recognition Loss 1102.12  Total Training Translation Loss 3487.46 
2024-01-31 22:45:24,566 EPOCH 1141
2024-01-31 22:45:42,033 Epoch 1141: Total Training Recognition Loss 1100.34  Total Training Translation Loss 3487.67 
2024-01-31 22:45:42,034 EPOCH 1142
2024-01-31 22:45:44,237 [Epoch: 1142 Step: 00038800] Batch Recognition Loss:  23.223686 => Gls Tokens per Sec:      872 || Batch Translation Loss:  86.799042 => Txt Tokens per Sec:     2331 || Lr: 0.000050
2024-01-31 22:45:59,676 Epoch 1142: Total Training Recognition Loss 1101.12  Total Training Translation Loss 3487.47 
2024-01-31 22:45:59,676 EPOCH 1143
2024-01-31 22:46:17,232 Epoch 1143: Total Training Recognition Loss 1101.07  Total Training Translation Loss 3487.75 
2024-01-31 22:46:17,232 EPOCH 1144
2024-01-31 22:46:34,783 Epoch 1144: Total Training Recognition Loss 1098.90  Total Training Translation Loss 3487.72 
2024-01-31 22:46:34,783 EPOCH 1145
2024-01-31 22:46:36,666 [Epoch: 1145 Step: 00038900] Batch Recognition Loss:  36.752960 => Gls Tokens per Sec:      681 || Batch Translation Loss: 102.322105 => Txt Tokens per Sec:     1946 || Lr: 0.000050
2024-01-31 22:46:52,559 Epoch 1145: Total Training Recognition Loss 1101.43  Total Training Translation Loss 3488.21 
2024-01-31 22:46:52,559 EPOCH 1146
2024-01-31 22:47:09,977 Epoch 1146: Total Training Recognition Loss 1098.72  Total Training Translation Loss 3486.93 
2024-01-31 22:47:09,978 EPOCH 1147
2024-01-31 22:47:27,512 Epoch 1147: Total Training Recognition Loss 1100.14  Total Training Translation Loss 3486.45 
2024-01-31 22:47:27,513 EPOCH 1148
2024-01-31 22:47:29,046 [Epoch: 1148 Step: 00039000] Batch Recognition Loss:  45.815483 => Gls Tokens per Sec:      418 || Batch Translation Loss: 124.295456 => Txt Tokens per Sec:     1355 || Lr: 0.000050
2024-01-31 22:47:44,949 Epoch 1148: Total Training Recognition Loss 1100.04  Total Training Translation Loss 3486.71 
2024-01-31 22:47:44,950 EPOCH 1149
2024-01-31 22:48:02,399 Epoch 1149: Total Training Recognition Loss 1100.51  Total Training Translation Loss 3487.89 
2024-01-31 22:48:02,400 EPOCH 1150
2024-01-31 22:48:20,022 [Epoch: 1150 Step: 00039100] Batch Recognition Loss:  37.246967 => Gls Tokens per Sec:      603 || Batch Translation Loss: 103.266228 => Txt Tokens per Sec:     1675 || Lr: 0.000050
2024-01-31 22:48:20,022 Epoch 1150: Total Training Recognition Loss 1101.75  Total Training Translation Loss 3487.59 
2024-01-31 22:48:20,022 EPOCH 1151
2024-01-31 22:48:37,567 Epoch 1151: Total Training Recognition Loss 1099.41  Total Training Translation Loss 3486.90 
2024-01-31 22:48:37,567 EPOCH 1152
2024-01-31 22:48:55,108 Epoch 1152: Total Training Recognition Loss 1099.35  Total Training Translation Loss 3486.70 
2024-01-31 22:48:55,109 EPOCH 1153
2024-01-31 22:49:12,121 [Epoch: 1153 Step: 00039200] Batch Recognition Loss:  30.361008 => Gls Tokens per Sec:      587 || Batch Translation Loss: 104.087105 => Txt Tokens per Sec:     1636 || Lr: 0.000050
2024-01-31 22:49:12,809 Epoch 1153: Total Training Recognition Loss 1102.60  Total Training Translation Loss 3487.30 
2024-01-31 22:49:12,809 EPOCH 1154
2024-01-31 22:49:30,464 Epoch 1154: Total Training Recognition Loss 1101.68  Total Training Translation Loss 3487.68 
2024-01-31 22:49:30,464 EPOCH 1155
2024-01-31 22:49:48,079 Epoch 1155: Total Training Recognition Loss 1100.06  Total Training Translation Loss 3487.40 
2024-01-31 22:49:48,079 EPOCH 1156
2024-01-31 22:50:04,090 [Epoch: 1156 Step: 00039300] Batch Recognition Loss:  28.466896 => Gls Tokens per Sec:      584 || Batch Translation Loss:  89.880203 => Txt Tokens per Sec:     1621 || Lr: 0.000050
2024-01-31 22:50:05,880 Epoch 1156: Total Training Recognition Loss 1100.68  Total Training Translation Loss 3486.95 
2024-01-31 22:50:05,880 EPOCH 1157
2024-01-31 22:50:23,478 Epoch 1157: Total Training Recognition Loss 1101.74  Total Training Translation Loss 3487.08 
2024-01-31 22:50:23,478 EPOCH 1158
2024-01-31 22:50:41,136 Epoch 1158: Total Training Recognition Loss 1100.35  Total Training Translation Loss 3486.39 
2024-01-31 22:50:41,136 EPOCH 1159
2024-01-31 22:50:55,223 [Epoch: 1159 Step: 00039400] Batch Recognition Loss:  19.102333 => Gls Tokens per Sec:      618 || Batch Translation Loss:  90.465485 => Txt Tokens per Sec:     1715 || Lr: 0.000050
2024-01-31 22:50:58,558 Epoch 1159: Total Training Recognition Loss 1099.56  Total Training Translation Loss 3487.40 
2024-01-31 22:50:58,558 EPOCH 1160
2024-01-31 22:51:16,175 Epoch 1160: Total Training Recognition Loss 1102.28  Total Training Translation Loss 3487.11 
2024-01-31 22:51:16,175 EPOCH 1161
2024-01-31 22:51:33,791 Epoch 1161: Total Training Recognition Loss 1103.78  Total Training Translation Loss 3485.86 
2024-01-31 22:51:33,791 EPOCH 1162
2024-01-31 22:51:47,678 [Epoch: 1162 Step: 00039500] Batch Recognition Loss:  13.415310 => Gls Tokens per Sec:      581 || Batch Translation Loss:  73.399124 => Txt Tokens per Sec:     1619 || Lr: 0.000050
2024-01-31 22:51:51,282 Epoch 1162: Total Training Recognition Loss 1101.22  Total Training Translation Loss 3486.80 
2024-01-31 22:51:51,282 EPOCH 1163
2024-01-31 22:52:08,856 Epoch 1163: Total Training Recognition Loss 1099.66  Total Training Translation Loss 3487.21 
2024-01-31 22:52:08,857 EPOCH 1164
2024-01-31 22:52:28,126 Epoch 1164: Total Training Recognition Loss 1100.93  Total Training Translation Loss 3486.91 
2024-01-31 22:52:28,127 EPOCH 1165
2024-01-31 22:52:42,038 [Epoch: 1165 Step: 00039600] Batch Recognition Loss:  32.647659 => Gls Tokens per Sec:      552 || Batch Translation Loss: 111.239326 => Txt Tokens per Sec:     1536 || Lr: 0.000050
2024-01-31 22:52:47,451 Epoch 1165: Total Training Recognition Loss 1100.67  Total Training Translation Loss 3487.62 
2024-01-31 22:52:47,451 EPOCH 1166
2024-01-31 22:53:05,112 Epoch 1166: Total Training Recognition Loss 1101.32  Total Training Translation Loss 3486.81 
2024-01-31 22:53:05,112 EPOCH 1167
2024-01-31 22:53:22,754 Epoch 1167: Total Training Recognition Loss 1103.16  Total Training Translation Loss 3487.56 
2024-01-31 22:53:22,754 EPOCH 1168
2024-01-31 22:53:32,639 [Epoch: 1168 Step: 00039700] Batch Recognition Loss:  36.919106 => Gls Tokens per Sec:      687 || Batch Translation Loss: 110.146492 => Txt Tokens per Sec:     1818 || Lr: 0.000050
2024-01-31 22:53:40,426 Epoch 1168: Total Training Recognition Loss 1099.28  Total Training Translation Loss 3487.37 
2024-01-31 22:53:40,427 EPOCH 1169
2024-01-31 22:53:58,034 Epoch 1169: Total Training Recognition Loss 1100.63  Total Training Translation Loss 3487.62 
2024-01-31 22:53:58,034 EPOCH 1170
2024-01-31 22:54:15,834 Epoch 1170: Total Training Recognition Loss 1102.10  Total Training Translation Loss 3486.91 
2024-01-31 22:54:15,834 EPOCH 1171
2024-01-31 22:54:26,216 [Epoch: 1171 Step: 00039800] Batch Recognition Loss:  37.610138 => Gls Tokens per Sec:      616 || Batch Translation Loss: 113.678818 => Txt Tokens per Sec:     1701 || Lr: 0.000050
2024-01-31 22:54:33,462 Epoch 1171: Total Training Recognition Loss 1099.65  Total Training Translation Loss 3487.79 
2024-01-31 22:54:33,462 EPOCH 1172
2024-01-31 22:54:51,204 Epoch 1172: Total Training Recognition Loss 1101.64  Total Training Translation Loss 3487.81 
2024-01-31 22:54:51,205 EPOCH 1173
2024-01-31 22:55:08,684 Epoch 1173: Total Training Recognition Loss 1096.98  Total Training Translation Loss 3487.61 
2024-01-31 22:55:08,684 EPOCH 1174
2024-01-31 22:55:18,579 [Epoch: 1174 Step: 00039900] Batch Recognition Loss:  13.406434 => Gls Tokens per Sec:      557 || Batch Translation Loss:  69.207771 => Txt Tokens per Sec:     1518 || Lr: 0.000050
2024-01-31 22:55:26,265 Epoch 1174: Total Training Recognition Loss 1100.94  Total Training Translation Loss 3486.96 
2024-01-31 22:55:26,265 EPOCH 1175
2024-01-31 22:55:43,868 Epoch 1175: Total Training Recognition Loss 1101.39  Total Training Translation Loss 3486.68 
2024-01-31 22:55:43,868 EPOCH 1176
2024-01-31 22:56:01,806 Epoch 1176: Total Training Recognition Loss 1102.59  Total Training Translation Loss 3487.77 
2024-01-31 22:56:01,806 EPOCH 1177
2024-01-31 22:56:09,597 [Epoch: 1177 Step: 00040000] Batch Recognition Loss:  27.299446 => Gls Tokens per Sec:      625 || Batch Translation Loss:  90.033432 => Txt Tokens per Sec:     1686 || Lr: 0.000050
2024-01-31 22:56:33,758 Validation result at epoch 1177, step    40000: duration: 24.1601s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 346.46933	Translation Loss: 73488.80469	PPL: 1562.47803
	Eval Metric: BLEU
	WER 527.47	(DEL: 4.59,	INS: 438.70,	SUB: 84.18)
	BLEU-4 0.00	(BLEU-1: 0.01,	BLEU-2: 0.00,	BLEU-3: 0.00,	BLEU-4: 0.00)
	CHRF 3.49	ROUGE 0.02
2024-01-31 22:56:33,760 Logging Recognition and Translation Outputs
2024-01-31 22:56:33,760 ========================================================================================================================
2024-01-31 22:56:33,760 Logging Sequence: 123_104.00
2024-01-31 22:56:33,760 	Gloss Reference :	***** * ***** * ***** * ***** * ***** * ***** * ***** A B+C+D+E
2024-01-31 22:56:33,760 	Gloss Hypothesis:	<unk> D <unk> E <unk> E <unk> E <unk> E <unk> D <pad> D <unk>  
2024-01-31 22:56:33,761 	Gloss Alignment :	I     I I     I I     I I     I I     I I     I I     S S      
2024-01-31 22:56:33,761 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 22:56:33,764 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* the     car     was     presented to      the     former  india   cricketer from    an      unknown person 
2024-01-31 22:56:33,764 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing   nothing nothing nothing nothing nothing   nothing nothing nothing nothing
2024-01-31 22:56:33,764 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I       I       I       I       I       I       I       I       I       I       I       S       S       S       S         S       S       S       S       S         S       S       S       S      
2024-01-31 22:56:33,764 ========================================================================================================================
2024-01-31 22:56:33,764 Logging Sequence: 107_23.00
2024-01-31 22:56:33,765 	Gloss Reference :	***** ***** ***** A     B+C+D+E
2024-01-31 22:56:33,765 	Gloss Hypothesis:	<unk> <pad> <unk> <pad> <unk>  
2024-01-31 22:56:33,765 	Gloss Alignment :	I     I     I     S     S      
2024-01-31 22:56:33,765 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 22:56:33,767 	Text Reference  :	*** *** *** *** *** *** *** *** *** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** and   viktor lilov who   is    also  from  the   usa  
2024-01-31 22:56:33,767 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha  sabha sabha sabha sabha sabha sabha sabha
2024-01-31 22:56:33,767 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     S     S      S     S     S     S     S     S     S    
2024-01-31 22:56:33,767 ========================================================================================================================
2024-01-31 22:56:33,767 Logging Sequence: 134_212.00
2024-01-31 22:56:33,768 	Gloss Reference :	A B+C+D+E                    
2024-01-31 22:56:33,768 	Gloss Hypothesis:	C B+E+B+C+B+C+B+C+B+C+B+C+B+C
2024-01-31 22:56:33,768 	Gloss Alignment :	S S                          
2024-01-31 22:56:33,768 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 22:56:33,770 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** dhanush said that he  practises little yoga
2024-01-31 22:56:33,770 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s>     <s>  <s>  <s> <s>       <s>    <s> 
2024-01-31 22:56:33,770 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   S       S    S    S   S         S      S   
2024-01-31 22:56:33,770 ========================================================================================================================
2024-01-31 22:56:33,770 Logging Sequence: 165_577.00
2024-01-31 22:56:33,770 	Gloss Reference :	* ***** *********** ***** ******* ***** * A     B+C+D+E
2024-01-31 22:56:33,771 	Gloss Hypothesis:	E <unk> E+A+E+B+E+A <unk> A+E+A+E <unk> E <unk> E      
2024-01-31 22:56:33,771 	Gloss Alignment :	I I     I           I     I       I     I S     S      
2024-01-31 22:56:33,771 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 22:56:33,773 	Text Reference  :	*** *** ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* then          after         28            years         india         won           the           world         cup           again         in            2011         
2024-01-31 22:56:33,774 	Text Hypothesis :	<s> <s> misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand
2024-01-31 22:56:33,774 	Text Alignment  :	I   I   I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             S             S             S             S             S             S             S             S             S             S             S             S            
2024-01-31 22:56:33,774 ========================================================================================================================
2024-01-31 22:56:33,774 Logging Sequence: 88_142.00
2024-01-31 22:56:33,774 	Gloss Reference :	***** ***** * ***** * ***** * ***** * A     B+C+D+E
2024-01-31 22:56:33,774 	Gloss Hypothesis:	<unk> <pad> B <pad> B <unk> B <unk> B <pad> B      
2024-01-31 22:56:33,775 	Gloss Alignment :	I     I     I I     I I     I I     I S     S      
2024-01-31 22:56:33,775 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 22:56:33,777 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** this is  because the police does not do  anything
2024-01-31 22:56:33,777 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s>  <s> <s>     <s> <s>    <s>  <s> <s> <s>     
2024-01-31 22:56:33,777 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   S    S   S       S   S      S    S   S   S       
2024-01-31 22:56:33,777 ========================================================================================================================
2024-01-31 22:56:43,623 Epoch 1177: Total Training Recognition Loss 1100.61  Total Training Translation Loss 3487.50 
2024-01-31 22:56:43,624 EPOCH 1178
2024-01-31 22:57:01,314 Epoch 1178: Total Training Recognition Loss 1100.76  Total Training Translation Loss 3487.26 
2024-01-31 22:57:01,314 EPOCH 1179
2024-01-31 22:57:18,946 Epoch 1179: Total Training Recognition Loss 1098.41  Total Training Translation Loss 3487.62 
2024-01-31 22:57:18,947 EPOCH 1180
2024-01-31 22:57:25,277 [Epoch: 1180 Step: 00040100] Batch Recognition Loss:  18.728260 => Gls Tokens per Sec:      668 || Batch Translation Loss:  90.090851 => Txt Tokens per Sec:     1844 || Lr: 0.000050
2024-01-31 22:57:36,437 Epoch 1180: Total Training Recognition Loss 1102.92  Total Training Translation Loss 3486.86 
2024-01-31 22:57:36,437 EPOCH 1181
2024-01-31 22:57:53,952 Epoch 1181: Total Training Recognition Loss 1101.51  Total Training Translation Loss 3486.93 
2024-01-31 22:57:53,952 EPOCH 1182
2024-01-31 22:58:11,486 Epoch 1182: Total Training Recognition Loss 1100.35  Total Training Translation Loss 3486.72 
2024-01-31 22:58:11,486 EPOCH 1183
2024-01-31 22:58:17,689 [Epoch: 1183 Step: 00040200] Batch Recognition Loss:  36.865456 => Gls Tokens per Sec:      619 || Batch Translation Loss: 105.071777 => Txt Tokens per Sec:     1703 || Lr: 0.000050
2024-01-31 22:58:29,159 Epoch 1183: Total Training Recognition Loss 1099.92  Total Training Translation Loss 3487.00 
2024-01-31 22:58:29,160 EPOCH 1184
2024-01-31 22:58:46,872 Epoch 1184: Total Training Recognition Loss 1102.62  Total Training Translation Loss 3487.72 
2024-01-31 22:58:46,873 EPOCH 1185
2024-01-31 22:59:04,513 Epoch 1185: Total Training Recognition Loss 1100.19  Total Training Translation Loss 3487.44 
2024-01-31 22:59:04,513 EPOCH 1186
2024-01-31 22:59:10,400 [Epoch: 1186 Step: 00040300] Batch Recognition Loss:  37.095581 => Gls Tokens per Sec:      544 || Batch Translation Loss: 101.140015 => Txt Tokens per Sec:     1577 || Lr: 0.000050
2024-01-31 22:59:22,285 Epoch 1186: Total Training Recognition Loss 1099.61  Total Training Translation Loss 3488.03 
2024-01-31 22:59:22,285 EPOCH 1187
2024-01-31 22:59:39,932 Epoch 1187: Total Training Recognition Loss 1099.50  Total Training Translation Loss 3487.39 
2024-01-31 22:59:39,932 EPOCH 1188
2024-01-31 22:59:57,627 Epoch 1188: Total Training Recognition Loss 1101.84  Total Training Translation Loss 3487.41 
2024-01-31 22:59:57,627 EPOCH 1189
2024-01-31 23:00:01,966 [Epoch: 1189 Step: 00040400] Batch Recognition Loss:  18.644293 => Gls Tokens per Sec:      533 || Batch Translation Loss:  89.553848 => Txt Tokens per Sec:     1547 || Lr: 0.000050
2024-01-31 23:00:15,285 Epoch 1189: Total Training Recognition Loss 1101.74  Total Training Translation Loss 3487.41 
2024-01-31 23:00:15,286 EPOCH 1190
2024-01-31 23:00:32,920 Epoch 1190: Total Training Recognition Loss 1101.34  Total Training Translation Loss 3487.03 
2024-01-31 23:00:32,920 EPOCH 1191
2024-01-31 23:00:50,554 Epoch 1191: Total Training Recognition Loss 1100.88  Total Training Translation Loss 3487.64 
2024-01-31 23:00:50,554 EPOCH 1192
2024-01-31 23:00:52,890 [Epoch: 1192 Step: 00040500] Batch Recognition Loss:  20.828335 => Gls Tokens per Sec:      822 || Batch Translation Loss:  90.155258 => Txt Tokens per Sec:     2084 || Lr: 0.000050
2024-01-31 23:01:08,142 Epoch 1192: Total Training Recognition Loss 1099.92  Total Training Translation Loss 3487.14 
2024-01-31 23:01:08,142 EPOCH 1193
2024-01-31 23:01:25,710 Epoch 1193: Total Training Recognition Loss 1099.45  Total Training Translation Loss 3487.26 
2024-01-31 23:01:25,710 EPOCH 1194
2024-01-31 23:01:43,399 Epoch 1194: Total Training Recognition Loss 1099.41  Total Training Translation Loss 3486.14 
2024-01-31 23:01:43,399 EPOCH 1195
2024-01-31 23:01:46,024 [Epoch: 1195 Step: 00040600] Batch Recognition Loss:  37.992699 => Gls Tokens per Sec:      488 || Batch Translation Loss: 108.096741 => Txt Tokens per Sec:     1483 || Lr: 0.000050
2024-01-31 23:02:00,874 Epoch 1195: Total Training Recognition Loss 1103.10  Total Training Translation Loss 3486.65 
2024-01-31 23:02:00,875 EPOCH 1196
2024-01-31 23:02:18,554 Epoch 1196: Total Training Recognition Loss 1102.05  Total Training Translation Loss 3487.40 
2024-01-31 23:02:18,554 EPOCH 1197
2024-01-31 23:02:36,091 Epoch 1197: Total Training Recognition Loss 1099.62  Total Training Translation Loss 3487.97 
2024-01-31 23:02:36,092 EPOCH 1198
2024-01-31 23:02:37,214 [Epoch: 1198 Step: 00040700] Batch Recognition Loss:  36.891434 => Gls Tokens per Sec:      571 || Batch Translation Loss: 118.247971 => Txt Tokens per Sec:     1760 || Lr: 0.000050
2024-01-31 23:02:53,655 Epoch 1198: Total Training Recognition Loss 1099.91  Total Training Translation Loss 3487.45 
2024-01-31 23:02:53,655 EPOCH 1199
2024-01-31 23:03:11,085 Epoch 1199: Total Training Recognition Loss 1102.85  Total Training Translation Loss 3488.03 
2024-01-31 23:03:11,085 EPOCH 1200
2024-01-31 23:03:28,609 [Epoch: 1200 Step: 00040800] Batch Recognition Loss:  10.818429 => Gls Tokens per Sec:      607 || Batch Translation Loss:  62.634724 => Txt Tokens per Sec:     1684 || Lr: 0.000050
2024-01-31 23:03:28,609 Epoch 1200: Total Training Recognition Loss 1101.53  Total Training Translation Loss 3487.72 
2024-01-31 23:03:28,610 EPOCH 1201
2024-01-31 23:03:46,244 Epoch 1201: Total Training Recognition Loss 1100.18  Total Training Translation Loss 3487.42 
2024-01-31 23:03:46,244 EPOCH 1202
2024-01-31 23:04:03,774 Epoch 1202: Total Training Recognition Loss 1098.94  Total Training Translation Loss 3487.41 
2024-01-31 23:04:03,774 EPOCH 1203
2024-01-31 23:04:19,735 [Epoch: 1203 Step: 00040900] Batch Recognition Loss:  17.372452 => Gls Tokens per Sec:      626 || Batch Translation Loss:  74.925133 => Txt Tokens per Sec:     1710 || Lr: 0.000050
2024-01-31 23:04:21,193 Epoch 1203: Total Training Recognition Loss 1098.88  Total Training Translation Loss 3487.17 
2024-01-31 23:04:21,193 EPOCH 1204
2024-01-31 23:04:38,603 Epoch 1204: Total Training Recognition Loss 1101.34  Total Training Translation Loss 3488.12 
2024-01-31 23:04:38,603 EPOCH 1205
2024-01-31 23:04:56,363 Epoch 1205: Total Training Recognition Loss 1099.45  Total Training Translation Loss 3487.24 
2024-01-31 23:04:56,363 EPOCH 1206
2024-01-31 23:05:12,308 [Epoch: 1206 Step: 00041000] Batch Recognition Loss:  32.706501 => Gls Tokens per Sec:      586 || Batch Translation Loss: 108.558533 => Txt Tokens per Sec:     1639 || Lr: 0.000050
2024-01-31 23:05:14,017 Epoch 1206: Total Training Recognition Loss 1102.00  Total Training Translation Loss 3487.29 
2024-01-31 23:05:14,017 EPOCH 1207
2024-01-31 23:05:31,404 Epoch 1207: Total Training Recognition Loss 1101.23  Total Training Translation Loss 3486.11 
2024-01-31 23:05:31,405 EPOCH 1208
2024-01-31 23:05:49,074 Epoch 1208: Total Training Recognition Loss 1100.36  Total Training Translation Loss 3487.62 
2024-01-31 23:05:49,074 EPOCH 1209
2024-01-31 23:06:02,696 [Epoch: 1209 Step: 00041100] Batch Recognition Loss:  27.341782 => Gls Tokens per Sec:      639 || Batch Translation Loss:  87.393173 => Txt Tokens per Sec:     1757 || Lr: 0.000050
2024-01-31 23:06:06,715 Epoch 1209: Total Training Recognition Loss 1100.05  Total Training Translation Loss 3488.29 
2024-01-31 23:06:06,715 EPOCH 1210
2024-01-31 23:06:24,268 Epoch 1210: Total Training Recognition Loss 1098.45  Total Training Translation Loss 3487.75 
2024-01-31 23:06:24,268 EPOCH 1211
2024-01-31 23:06:41,938 Epoch 1211: Total Training Recognition Loss 1102.69  Total Training Translation Loss 3487.64 
2024-01-31 23:06:41,938 EPOCH 1212
2024-01-31 23:06:55,075 [Epoch: 1212 Step: 00041200] Batch Recognition Loss:  30.652496 => Gls Tokens per Sec:      614 || Batch Translation Loss: 106.707275 => Txt Tokens per Sec:     1679 || Lr: 0.000050
2024-01-31 23:06:59,433 Epoch 1212: Total Training Recognition Loss 1100.09  Total Training Translation Loss 3487.78 
2024-01-31 23:06:59,433 EPOCH 1213
2024-01-31 23:07:16,922 Epoch 1213: Total Training Recognition Loss 1099.79  Total Training Translation Loss 3487.45 
2024-01-31 23:07:16,922 EPOCH 1214
2024-01-31 23:07:34,526 Epoch 1214: Total Training Recognition Loss 1101.51  Total Training Translation Loss 3487.17 
2024-01-31 23:07:34,526 EPOCH 1215
2024-01-31 23:07:47,864 [Epoch: 1215 Step: 00041300] Batch Recognition Loss:  37.147488 => Gls Tokens per Sec:      557 || Batch Translation Loss: 102.713501 => Txt Tokens per Sec:     1556 || Lr: 0.000050
2024-01-31 23:07:52,130 Epoch 1215: Total Training Recognition Loss 1102.36  Total Training Translation Loss 3486.42 
2024-01-31 23:07:52,130 EPOCH 1216
2024-01-31 23:08:09,715 Epoch 1216: Total Training Recognition Loss 1098.95  Total Training Translation Loss 3487.64 
2024-01-31 23:08:09,715 EPOCH 1217
2024-01-31 23:08:27,413 Epoch 1217: Total Training Recognition Loss 1100.14  Total Training Translation Loss 3487.02 
2024-01-31 23:08:27,413 EPOCH 1218
2024-01-31 23:08:39,468 [Epoch: 1218 Step: 00041400] Batch Recognition Loss:  30.142143 => Gls Tokens per Sec:      563 || Batch Translation Loss: 100.499664 => Txt Tokens per Sec:     1577 || Lr: 0.000050
2024-01-31 23:08:44,981 Epoch 1218: Total Training Recognition Loss 1103.85  Total Training Translation Loss 3487.07 
2024-01-31 23:08:44,982 EPOCH 1219
2024-01-31 23:09:02,464 Epoch 1219: Total Training Recognition Loss 1099.85  Total Training Translation Loss 3486.83 
2024-01-31 23:09:02,464 EPOCH 1220
2024-01-31 23:09:19,936 Epoch 1220: Total Training Recognition Loss 1100.76  Total Training Translation Loss 3487.61 
2024-01-31 23:09:19,936 EPOCH 1221
2024-01-31 23:09:28,772 [Epoch: 1221 Step: 00041500] Batch Recognition Loss:  31.886597 => Gls Tokens per Sec:      696 || Batch Translation Loss:  95.584839 => Txt Tokens per Sec:     1843 || Lr: 0.000050
2024-01-31 23:09:37,358 Epoch 1221: Total Training Recognition Loss 1101.46  Total Training Translation Loss 3486.54 
2024-01-31 23:09:37,359 EPOCH 1222
2024-01-31 23:09:54,854 Epoch 1222: Total Training Recognition Loss 1101.65  Total Training Translation Loss 3487.51 
2024-01-31 23:09:54,855 EPOCH 1223
2024-01-31 23:10:12,176 Epoch 1223: Total Training Recognition Loss 1098.62  Total Training Translation Loss 3487.30 
2024-01-31 23:10:12,177 EPOCH 1224
2024-01-31 23:10:21,614 [Epoch: 1224 Step: 00041600] Batch Recognition Loss:  38.908039 => Gls Tokens per Sec:      610 || Batch Translation Loss: 122.761528 => Txt Tokens per Sec:     1747 || Lr: 0.000050
2024-01-31 23:10:29,640 Epoch 1224: Total Training Recognition Loss 1101.57  Total Training Translation Loss 3487.29 
2024-01-31 23:10:29,641 EPOCH 1225
2024-01-31 23:10:47,197 Epoch 1225: Total Training Recognition Loss 1100.77  Total Training Translation Loss 3487.48 
2024-01-31 23:10:47,198 EPOCH 1226
2024-01-31 23:11:04,688 Epoch 1226: Total Training Recognition Loss 1099.12  Total Training Translation Loss 3487.74 
2024-01-31 23:11:04,688 EPOCH 1227
2024-01-31 23:11:11,604 [Epoch: 1227 Step: 00041700] Batch Recognition Loss:  44.198799 => Gls Tokens per Sec:      704 || Batch Translation Loss: 130.356491 => Txt Tokens per Sec:     1881 || Lr: 0.000050
2024-01-31 23:11:22,277 Epoch 1227: Total Training Recognition Loss 1101.59  Total Training Translation Loss 3487.17 
2024-01-31 23:11:22,277 EPOCH 1228
2024-01-31 23:11:39,692 Epoch 1228: Total Training Recognition Loss 1101.25  Total Training Translation Loss 3486.83 
2024-01-31 23:11:39,692 EPOCH 1229
2024-01-31 23:11:57,296 Epoch 1229: Total Training Recognition Loss 1099.04  Total Training Translation Loss 3487.15 
2024-01-31 23:11:57,296 EPOCH 1230
2024-01-31 23:12:05,662 [Epoch: 1230 Step: 00041800] Batch Recognition Loss:  28.789324 => Gls Tokens per Sec:      506 || Batch Translation Loss:  86.759666 => Txt Tokens per Sec:     1421 || Lr: 0.000050
2024-01-31 23:12:14,875 Epoch 1230: Total Training Recognition Loss 1099.87  Total Training Translation Loss 3487.14 
2024-01-31 23:12:14,875 EPOCH 1231
2024-01-31 23:12:32,408 Epoch 1231: Total Training Recognition Loss 1102.38  Total Training Translation Loss 3487.09 
2024-01-31 23:12:32,408 EPOCH 1232
2024-01-31 23:12:49,950 Epoch 1232: Total Training Recognition Loss 1101.14  Total Training Translation Loss 3486.71 
2024-01-31 23:12:49,950 EPOCH 1233
2024-01-31 23:12:56,686 [Epoch: 1233 Step: 00041900] Batch Recognition Loss:  30.205967 => Gls Tokens per Sec:      570 || Batch Translation Loss: 103.422623 => Txt Tokens per Sec:     1615 || Lr: 0.000050
2024-01-31 23:13:07,505 Epoch 1233: Total Training Recognition Loss 1102.09  Total Training Translation Loss 3486.36 
2024-01-31 23:13:07,505 EPOCH 1234
2024-01-31 23:13:25,071 Epoch 1234: Total Training Recognition Loss 1101.02  Total Training Translation Loss 3486.98 
2024-01-31 23:13:25,071 EPOCH 1235
2024-01-31 23:13:42,594 Epoch 1235: Total Training Recognition Loss 1100.41  Total Training Translation Loss 3487.61 
2024-01-31 23:13:42,595 EPOCH 1236
2024-01-31 23:13:47,931 [Epoch: 1236 Step: 00042000] Batch Recognition Loss:  17.451160 => Gls Tokens per Sec:      553 || Batch Translation Loss:  75.092903 => Txt Tokens per Sec:     1445 || Lr: 0.000050
2024-01-31 23:14:11,770 Validation result at epoch 1236, step    42000: duration: 23.8390s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 346.43622	Translation Loss: 73476.39062	PPL: 1560.53845
	Eval Metric: BLEU
	WER 524.58	(DEL: 4.52,	INS: 435.24,	SUB: 84.82)
	BLEU-4 0.00	(BLEU-1: 0.01,	BLEU-2: 0.00,	BLEU-3: 0.00,	BLEU-4: 0.00)
	CHRF 3.55	ROUGE 0.02
2024-01-31 23:14:11,772 Logging Recognition and Translation Outputs
2024-01-31 23:14:11,772 ========================================================================================================================
2024-01-31 23:14:11,772 Logging Sequence: 81_8.00
2024-01-31 23:14:11,772 	Gloss Reference :	***** * ***** * ***** * ***** * ***** * ***** * ***** ***** A     B+C+D+E
2024-01-31 23:14:11,773 	Gloss Hypothesis:	<unk> E <unk> E <unk> E <unk> E <unk> E <unk> E <unk> E+C+E <unk> E      
2024-01-31 23:14:11,773 	Gloss Alignment :	I     I I     I I     I I     I I     I I     I I     I     S     S      
2024-01-31 23:14:11,773 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 23:14:11,777 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* have    been    involved in      a       huge    controversy in      connection to      real    estate  developer amrapali group   since   last    7       years  
2024-01-31 23:14:11,777 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing  nothing nothing nothing nothing     nothing nothing    nothing nothing nothing nothing   nothing  nothing nothing nothing nothing nothing
2024-01-31 23:14:11,777 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I       I       I       I       I       I       I       I       I       I       I       S       S       S        S       S       S       S           S       S          S       S       S       S         S        S       S       S       S       S      
2024-01-31 23:14:11,777 ========================================================================================================================
2024-01-31 23:14:11,777 Logging Sequence: 148_239.00
2024-01-31 23:14:11,778 	Gloss Reference :	***** A ***** ***** ***** ***** ***** * ***** * ***** ********* ***** B+C+D+E
2024-01-31 23:14:11,778 	Gloss Hypothesis:	<unk> A <unk> E+A+E <unk> E+B+E <unk> E <unk> E <unk> E+B+E+C+B <unk> C+E    
2024-01-31 23:14:11,778 	Gloss Alignment :	I       I     I     I     I     I     I I     I I     I         I     S      
2024-01-31 23:14:11,778 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 23:14:11,781 	Text Reference  :	*** *** ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* the     ground  staff   were    very    happy   and     thanked the     bowler  for     his     kind    gesture
2024-01-31 23:14:11,781 	Text Hypothesis :	<s> <s> nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing
2024-01-31 23:14:11,782 	Text Alignment  :	I   I   I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       S       S       S       S       S       S       S       S       S       S       S       S       S       S      
2024-01-31 23:14:11,782 ========================================================================================================================
2024-01-31 23:14:11,782 Logging Sequence: 165_8.00
2024-01-31 23:14:11,782 	Gloss Reference :	A     B+C+D+E
2024-01-31 23:14:11,782 	Gloss Hypothesis:	<unk> E      
2024-01-31 23:14:11,782 	Gloss Alignment :	S     S      
2024-01-31 23:14:11,782 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 23:14:11,785 	Text Reference  :	*** *** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** however many  don't believe in    it    it    varies among people
2024-01-31 23:14:11,785 	Text Hypothesis :	<s> <s> sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha   sabha sabha sabha   sabha sabha sabha sabha  sabha sabha 
2024-01-31 23:14:11,785 	Text Alignment  :	I   I   I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     S       S     S     S       S     S     S     S      S     S     
2024-01-31 23:14:11,785 ========================================================================================================================
2024-01-31 23:14:11,785 Logging Sequence: 93_93.00
2024-01-31 23:14:11,785 	Gloss Reference :	***** ***** ***** * ***** * ***** ******* ***** *** ***** *** ***** * ***** A B+C+D+E
2024-01-31 23:14:11,786 	Gloss Hypothesis:	<unk> B+C+A <unk> C <unk> C <unk> E+B+C+B <unk> B+E <unk> C+B <unk> C <unk> C <unk>  
2024-01-31 23:14:11,786 	Gloss Alignment :	I     I     I     I I     I I     I       I     I   I     I   I     I I     S S      
2024-01-31 23:14:11,786 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 23:14:11,787 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* rooney  was     at      the     club    as      well   
2024-01-31 23:14:11,788 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing
2024-01-31 23:14:11,788 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       S       S       S       S       S       S       S      
2024-01-31 23:14:11,788 ========================================================================================================================
2024-01-31 23:14:11,788 Logging Sequence: 96_129.00
2024-01-31 23:14:11,788 	Gloss Reference :	A B+C+D+E
2024-01-31 23:14:11,788 	Gloss Hypothesis:	A E+A+E  
2024-01-31 23:14:11,788 	Gloss Alignment :	  S      
2024-01-31 23:14:11,789 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 23:14:11,790 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* viewers were    very    stressed
2024-01-31 23:14:11,790 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing 
2024-01-31 23:14:11,790 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       S       S       S       S       
2024-01-31 23:14:11,790 ========================================================================================================================
2024-01-31 23:14:23,919 Epoch 1236: Total Training Recognition Loss 1099.33  Total Training Translation Loss 3486.75 
2024-01-31 23:14:23,919 EPOCH 1237
2024-01-31 23:14:41,514 Epoch 1237: Total Training Recognition Loss 1098.34  Total Training Translation Loss 3488.03 
2024-01-31 23:14:41,514 EPOCH 1238
2024-01-31 23:14:59,232 Epoch 1238: Total Training Recognition Loss 1101.11  Total Training Translation Loss 3487.16 
2024-01-31 23:14:59,233 EPOCH 1239
2024-01-31 23:15:04,607 [Epoch: 1239 Step: 00042100] Batch Recognition Loss:  32.908131 => Gls Tokens per Sec:      476 || Batch Translation Loss: 113.945862 => Txt Tokens per Sec:     1464 || Lr: 0.000050
2024-01-31 23:15:16,779 Epoch 1239: Total Training Recognition Loss 1099.68  Total Training Translation Loss 3486.47 
2024-01-31 23:15:16,780 EPOCH 1240
2024-01-31 23:15:34,345 Epoch 1240: Total Training Recognition Loss 1101.53  Total Training Translation Loss 3486.59 
2024-01-31 23:15:34,345 EPOCH 1241
2024-01-31 23:15:52,001 Epoch 1241: Total Training Recognition Loss 1100.44  Total Training Translation Loss 3486.86 
2024-01-31 23:15:52,002 EPOCH 1242
2024-01-31 23:15:54,060 [Epoch: 1242 Step: 00042200] Batch Recognition Loss:  13.385737 => Gls Tokens per Sec:      934 || Batch Translation Loss:  72.244774 => Txt Tokens per Sec:     2354 || Lr: 0.000050
2024-01-31 23:16:09,679 Epoch 1242: Total Training Recognition Loss 1101.50  Total Training Translation Loss 3488.19 
2024-01-31 23:16:09,679 EPOCH 1243
2024-01-31 23:16:27,227 Epoch 1243: Total Training Recognition Loss 1099.25  Total Training Translation Loss 3486.90 
2024-01-31 23:16:27,228 EPOCH 1244
2024-01-31 23:16:44,736 Epoch 1244: Total Training Recognition Loss 1102.63  Total Training Translation Loss 3487.67 
2024-01-31 23:16:44,736 EPOCH 1245
2024-01-31 23:16:46,883 [Epoch: 1245 Step: 00042300] Batch Recognition Loss:  42.805573 => Gls Tokens per Sec:      480 || Batch Translation Loss: 131.043549 => Txt Tokens per Sec:     1347 || Lr: 0.000050
2024-01-31 23:17:02,486 Epoch 1245: Total Training Recognition Loss 1102.67  Total Training Translation Loss 3487.94 
2024-01-31 23:17:02,486 EPOCH 1246
2024-01-31 23:17:20,259 Epoch 1246: Total Training Recognition Loss 1100.27  Total Training Translation Loss 3487.16 
2024-01-31 23:17:20,259 EPOCH 1247
2024-01-31 23:17:37,967 Epoch 1247: Total Training Recognition Loss 1099.39  Total Training Translation Loss 3487.01 
2024-01-31 23:17:37,967 EPOCH 1248
2024-01-31 23:17:38,819 [Epoch: 1248 Step: 00042400] Batch Recognition Loss:  23.696018 => Gls Tokens per Sec:      752 || Batch Translation Loss: 102.110214 => Txt Tokens per Sec:     2302 || Lr: 0.000050
2024-01-31 23:17:55,630 Epoch 1248: Total Training Recognition Loss 1101.00  Total Training Translation Loss 3487.41 
2024-01-31 23:17:55,631 EPOCH 1249
2024-01-31 23:18:13,251 Epoch 1249: Total Training Recognition Loss 1101.76  Total Training Translation Loss 3486.63 
2024-01-31 23:18:13,251 EPOCH 1250
2024-01-31 23:18:30,884 [Epoch: 1250 Step: 00042500] Batch Recognition Loss:  43.924118 => Gls Tokens per Sec:      603 || Batch Translation Loss: 130.791626 => Txt Tokens per Sec:     1674 || Lr: 0.000050
2024-01-31 23:18:30,884 Epoch 1250: Total Training Recognition Loss 1102.31  Total Training Translation Loss 3486.70 
2024-01-31 23:18:30,884 EPOCH 1251
2024-01-31 23:18:48,586 Epoch 1251: Total Training Recognition Loss 1100.24  Total Training Translation Loss 3486.67 
2024-01-31 23:18:48,586 EPOCH 1252
2024-01-31 23:19:06,246 Epoch 1252: Total Training Recognition Loss 1101.54  Total Training Translation Loss 3487.40 
2024-01-31 23:19:06,246 EPOCH 1253
2024-01-31 23:19:22,897 [Epoch: 1253 Step: 00042600] Batch Recognition Loss:  37.060112 => Gls Tokens per Sec:      600 || Batch Translation Loss: 122.199715 => Txt Tokens per Sec:     1668 || Lr: 0.000050
2024-01-31 23:19:23,897 Epoch 1253: Total Training Recognition Loss 1099.91  Total Training Translation Loss 3487.45 
2024-01-31 23:19:23,897 EPOCH 1254
2024-01-31 23:19:41,470 Epoch 1254: Total Training Recognition Loss 1101.86  Total Training Translation Loss 3487.09 
2024-01-31 23:19:41,471 EPOCH 1255
2024-01-31 23:19:59,057 Epoch 1255: Total Training Recognition Loss 1100.10  Total Training Translation Loss 3487.98 
2024-01-31 23:19:59,058 EPOCH 1256
2024-01-31 23:20:14,804 [Epoch: 1256 Step: 00042700] Batch Recognition Loss:   6.272036 => Gls Tokens per Sec:      594 || Batch Translation Loss:  50.830528 => Txt Tokens per Sec:     1645 || Lr: 0.000050
2024-01-31 23:20:16,663 Epoch 1256: Total Training Recognition Loss 1099.98  Total Training Translation Loss 3486.70 
2024-01-31 23:20:16,664 EPOCH 1257
2024-01-31 23:20:34,227 Epoch 1257: Total Training Recognition Loss 1100.88  Total Training Translation Loss 3486.84 
2024-01-31 23:20:34,227 EPOCH 1258
2024-01-31 23:20:51,716 Epoch 1258: Total Training Recognition Loss 1101.38  Total Training Translation Loss 3487.63 
2024-01-31 23:20:51,717 EPOCH 1259
2024-01-31 23:21:04,679 [Epoch: 1259 Step: 00042800] Batch Recognition Loss:  33.491436 => Gls Tokens per Sec:      672 || Batch Translation Loss: 107.659203 => Txt Tokens per Sec:     1829 || Lr: 0.000050
2024-01-31 23:21:09,336 Epoch 1259: Total Training Recognition Loss 1102.25  Total Training Translation Loss 3486.73 
2024-01-31 23:21:09,336 EPOCH 1260
2024-01-31 23:21:26,937 Epoch 1260: Total Training Recognition Loss 1099.81  Total Training Translation Loss 3487.86 
2024-01-31 23:21:26,937 EPOCH 1261
2024-01-31 23:21:44,443 Epoch 1261: Total Training Recognition Loss 1102.07  Total Training Translation Loss 3487.74 
2024-01-31 23:21:44,444 EPOCH 1262
2024-01-31 23:21:59,046 [Epoch: 1262 Step: 00042900] Batch Recognition Loss:  23.953007 => Gls Tokens per Sec:      553 || Batch Translation Loss:  97.514008 => Txt Tokens per Sec:     1573 || Lr: 0.000050
2024-01-31 23:22:02,016 Epoch 1262: Total Training Recognition Loss 1100.78  Total Training Translation Loss 3487.25 
2024-01-31 23:22:02,016 EPOCH 1263
2024-01-31 23:22:19,713 Epoch 1263: Total Training Recognition Loss 1101.69  Total Training Translation Loss 3488.23 
2024-01-31 23:22:19,713 EPOCH 1264
2024-01-31 23:22:40,120 Epoch 1264: Total Training Recognition Loss 1101.57  Total Training Translation Loss 3487.66 
2024-01-31 23:22:40,121 EPOCH 1265
2024-01-31 23:22:52,410 [Epoch: 1265 Step: 00043000] Batch Recognition Loss:  45.841141 => Gls Tokens per Sec:      625 || Batch Translation Loss: 125.563545 => Txt Tokens per Sec:     1705 || Lr: 0.000050
2024-01-31 23:22:58,137 Epoch 1265: Total Training Recognition Loss 1100.20  Total Training Translation Loss 3486.89 
2024-01-31 23:22:58,137 EPOCH 1266
2024-01-31 23:23:15,696 Epoch 1266: Total Training Recognition Loss 1101.03  Total Training Translation Loss 3487.47 
2024-01-31 23:23:15,696 EPOCH 1267
2024-01-31 23:23:33,348 Epoch 1267: Total Training Recognition Loss 1101.87  Total Training Translation Loss 3487.75 
2024-01-31 23:23:33,348 EPOCH 1268
2024-01-31 23:23:46,373 [Epoch: 1268 Step: 00043100] Batch Recognition Loss:  36.710030 => Gls Tokens per Sec:      521 || Batch Translation Loss: 105.007317 => Txt Tokens per Sec:     1524 || Lr: 0.000050
2024-01-31 23:23:50,911 Epoch 1268: Total Training Recognition Loss 1100.36  Total Training Translation Loss 3486.58 
2024-01-31 23:23:50,911 EPOCH 1269
2024-01-31 23:24:08,589 Epoch 1269: Total Training Recognition Loss 1099.75  Total Training Translation Loss 3488.64 
2024-01-31 23:24:08,590 EPOCH 1270
2024-01-31 23:24:26,108 Epoch 1270: Total Training Recognition Loss 1100.83  Total Training Translation Loss 3488.30 
2024-01-31 23:24:26,109 EPOCH 1271
2024-01-31 23:24:36,569 [Epoch: 1271 Step: 00043200] Batch Recognition Loss:  10.844610 => Gls Tokens per Sec:      612 || Batch Translation Loss:  62.529449 => Txt Tokens per Sec:     1677 || Lr: 0.000050
2024-01-31 23:24:43,841 Epoch 1271: Total Training Recognition Loss 1100.21  Total Training Translation Loss 3486.38 
2024-01-31 23:24:43,841 EPOCH 1272
2024-01-31 23:25:01,527 Epoch 1272: Total Training Recognition Loss 1098.51  Total Training Translation Loss 3485.57 
2024-01-31 23:25:01,528 EPOCH 1273
2024-01-31 23:25:19,054 Epoch 1273: Total Training Recognition Loss 1100.48  Total Training Translation Loss 3487.58 
2024-01-31 23:25:19,054 EPOCH 1274
2024-01-31 23:25:27,993 [Epoch: 1274 Step: 00043300] Batch Recognition Loss:  36.473450 => Gls Tokens per Sec:      645 || Batch Translation Loss: 103.617897 => Txt Tokens per Sec:     1770 || Lr: 0.000050
2024-01-31 23:25:36,560 Epoch 1274: Total Training Recognition Loss 1097.26  Total Training Translation Loss 3487.60 
2024-01-31 23:25:36,560 EPOCH 1275
2024-01-31 23:25:54,220 Epoch 1275: Total Training Recognition Loss 1098.52  Total Training Translation Loss 3487.71 
2024-01-31 23:25:54,220 EPOCH 1276
2024-01-31 23:26:11,760 Epoch 1276: Total Training Recognition Loss 1100.21  Total Training Translation Loss 3486.77 
2024-01-31 23:26:11,760 EPOCH 1277
2024-01-31 23:26:19,833 [Epoch: 1277 Step: 00043400] Batch Recognition Loss:  40.237022 => Gls Tokens per Sec:      634 || Batch Translation Loss: 115.059433 => Txt Tokens per Sec:     1763 || Lr: 0.000050
2024-01-31 23:26:29,308 Epoch 1277: Total Training Recognition Loss 1101.97  Total Training Translation Loss 3487.99 
2024-01-31 23:26:29,309 EPOCH 1278
2024-01-31 23:26:46,983 Epoch 1278: Total Training Recognition Loss 1101.68  Total Training Translation Loss 3486.68 
2024-01-31 23:26:46,984 EPOCH 1279
2024-01-31 23:27:04,648 Epoch 1279: Total Training Recognition Loss 1099.31  Total Training Translation Loss 3486.86 
2024-01-31 23:27:04,649 EPOCH 1280
2024-01-31 23:27:12,375 [Epoch: 1280 Step: 00043500] Batch Recognition Loss:  27.739727 => Gls Tokens per Sec:      580 || Batch Translation Loss: 100.010941 => Txt Tokens per Sec:     1672 || Lr: 0.000050
2024-01-31 23:27:22,338 Epoch 1280: Total Training Recognition Loss 1100.52  Total Training Translation Loss 3487.32 
2024-01-31 23:27:22,339 EPOCH 1281
2024-01-31 23:27:40,061 Epoch 1281: Total Training Recognition Loss 1101.98  Total Training Translation Loss 3487.00 
2024-01-31 23:27:40,061 EPOCH 1282
2024-01-31 23:27:57,669 Epoch 1282: Total Training Recognition Loss 1101.22  Total Training Translation Loss 3486.35 
2024-01-31 23:27:57,669 EPOCH 1283
2024-01-31 23:28:03,018 [Epoch: 1283 Step: 00043600] Batch Recognition Loss:  35.328915 => Gls Tokens per Sec:      671 || Batch Translation Loss: 107.481133 => Txt Tokens per Sec:     1819 || Lr: 0.000050
2024-01-31 23:28:15,366 Epoch 1283: Total Training Recognition Loss 1101.68  Total Training Translation Loss 3488.96 
2024-01-31 23:28:15,367 EPOCH 1284
2024-01-31 23:28:32,998 Epoch 1284: Total Training Recognition Loss 1100.63  Total Training Translation Loss 3486.48 
2024-01-31 23:28:32,998 EPOCH 1285
2024-01-31 23:28:50,559 Epoch 1285: Total Training Recognition Loss 1099.43  Total Training Translation Loss 3486.53 
2024-01-31 23:28:50,560 EPOCH 1286
2024-01-31 23:28:56,911 [Epoch: 1286 Step: 00043700] Batch Recognition Loss:  45.701065 => Gls Tokens per Sec:      465 || Batch Translation Loss: 122.921738 => Txt Tokens per Sec:     1317 || Lr: 0.000050
2024-01-31 23:29:08,296 Epoch 1286: Total Training Recognition Loss 1100.03  Total Training Translation Loss 3487.89 
2024-01-31 23:29:08,296 EPOCH 1287
2024-01-31 23:29:26,085 Epoch 1287: Total Training Recognition Loss 1099.37  Total Training Translation Loss 3486.66 
2024-01-31 23:29:26,085 EPOCH 1288
2024-01-31 23:29:43,662 Epoch 1288: Total Training Recognition Loss 1099.29  Total Training Translation Loss 3487.08 
2024-01-31 23:29:43,662 EPOCH 1289
2024-01-31 23:29:47,487 [Epoch: 1289 Step: 00043800] Batch Recognition Loss:  33.578476 => Gls Tokens per Sec:      604 || Batch Translation Loss: 101.353920 => Txt Tokens per Sec:     1640 || Lr: 0.000050
2024-01-31 23:30:01,354 Epoch 1289: Total Training Recognition Loss 1099.12  Total Training Translation Loss 3487.70 
2024-01-31 23:30:01,354 EPOCH 1290
2024-01-31 23:30:19,066 Epoch 1290: Total Training Recognition Loss 1101.58  Total Training Translation Loss 3487.51 
2024-01-31 23:30:19,067 EPOCH 1291
2024-01-31 23:30:36,599 Epoch 1291: Total Training Recognition Loss 1101.19  Total Training Translation Loss 3487.91 
2024-01-31 23:30:36,599 EPOCH 1292
2024-01-31 23:30:39,694 [Epoch: 1292 Step: 00043900] Batch Recognition Loss:  50.987957 => Gls Tokens per Sec:      621 || Batch Translation Loss: 130.416458 => Txt Tokens per Sec:     1658 || Lr: 0.000050
2024-01-31 23:30:54,298 Epoch 1292: Total Training Recognition Loss 1099.96  Total Training Translation Loss 3486.57 
2024-01-31 23:30:54,299 EPOCH 1293
2024-01-31 23:31:11,901 Epoch 1293: Total Training Recognition Loss 1097.33  Total Training Translation Loss 3487.71 
2024-01-31 23:31:11,901 EPOCH 1294
2024-01-31 23:31:29,606 Epoch 1294: Total Training Recognition Loss 1100.63  Total Training Translation Loss 3487.41 
2024-01-31 23:31:29,606 EPOCH 1295
2024-01-31 23:31:31,691 [Epoch: 1295 Step: 00044000] Batch Recognition Loss:  37.827499 => Gls Tokens per Sec:      495 || Batch Translation Loss: 113.320221 => Txt Tokens per Sec:     1363 || Lr: 0.000050
2024-01-31 23:31:55,877 Validation result at epoch 1295, step    44000: duration: 24.1856s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 344.80811	Translation Loss: 73505.82812	PPL: 1565.14197
	Eval Metric: BLEU
	WER 514.05	(DEL: 4.52,	INS: 425.21,	SUB: 84.32)
	BLEU-4 0.00	(BLEU-1: 0.01,	BLEU-2: 0.00,	BLEU-3: 0.00,	BLEU-4: 0.00)
	CHRF 3.48	ROUGE 0.02
2024-01-31 23:31:55,879 Logging Recognition and Translation Outputs
2024-01-31 23:31:55,879 ========================================================================================================================
2024-01-31 23:31:55,879 Logging Sequence: 117_29.00
2024-01-31 23:31:55,879 	Gloss Reference :	***** ***** * ***** * ***** *** ***** ******* ***** *** ***** A         B+C+D+E
2024-01-31 23:31:55,880 	Gloss Hypothesis:	<unk> <pad> B <unk> C <unk> E+C <unk> E+C+B+C <unk> C+B <unk> B+E+B+C+E <unk>  
2024-01-31 23:31:55,880 	Gloss Alignment :	I     I     I I     I I     I   I     I       I     I   I     S         S      
2024-01-31 23:31:55,880 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 23:31:55,883 	Text Reference  :	*** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** however england was   unable to    reach the   target they  were  all   out   lost  by    66    runs 
2024-01-31 23:31:55,883 	Text Hypothesis :	<s> sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha   sabha   sabha sabha  sabha sabha sabha sabha  sabha sabha sabha sabha sabha sabha sabha sabha
2024-01-31 23:31:55,883 	Text Alignment  :	I   I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     S       S       S     S      S     S     S     S      S     S     S     S     S     S     S     S    
2024-01-31 23:31:55,884 ========================================================================================================================
2024-01-31 23:31:55,884 Logging Sequence: 84_176.00
2024-01-31 23:31:55,884 	Gloss Reference :	***** *********** ***** * ***** ***** ***** ********* ***** *** ***** * ***** ******* A     B+C+D+E
2024-01-31 23:31:55,884 	Gloss Hypothesis:	<unk> E+C+A+E+C+E <unk> E <unk> E+C+E <unk> C+E+C+E+C <unk> C+E <unk> E <unk> E+C+E+C <unk> E+C    
2024-01-31 23:31:55,884 	Gloss Alignment :	I     I           I     I I     I     I     I         I     I   I     I I     I       S     S      
2024-01-31 23:31:55,884 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 23:31:55,887 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** germany's nancy faeser who attended the game in  doha against japan said
2024-01-31 23:31:55,887 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s>       <s>   <s>    <s> <s>      <s> <s>  <s> <s>  <s>     <s>   <s> 
2024-01-31 23:31:55,887 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   S         S     S      S   S        S   S    S   S    S       S     S   
2024-01-31 23:31:55,887 ========================================================================================================================
2024-01-31 23:31:55,888 Logging Sequence: 172_98.00
2024-01-31 23:31:55,888 	Gloss Reference :	***** A ***** * ***** ******* ***** * ***** * ***** * ***** ***** ***** * ***** ********* ***** B+C+D+E
2024-01-31 23:31:55,888 	Gloss Hypothesis:	<unk> A <unk> E <unk> E+B+E+B <unk> B <unk> E <unk> C <unk> C+B+E <unk> E <unk> B+E+C+B+C <unk> C+B+C  
2024-01-31 23:31:55,888 	Gloss Alignment :	I       I     I I     I       I     I I     I I     I I     I     I     I I     I         I     S      
2024-01-31 23:31:55,888 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 23:31:55,891 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** since 700 pm  it  kept raining the intensity plunged around 915 pm 
2024-01-31 23:31:55,891 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s>   <s> <s> <s> <s>  <s>     <s> <s>       <s>     <s>    <s> <s>
2024-01-31 23:31:55,891 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   S     S   S   S   S    S       S   S         S       S      S   S  
2024-01-31 23:31:55,891 ========================================================================================================================
2024-01-31 23:31:55,892 Logging Sequence: 135_92.00
2024-01-31 23:31:55,892 	Gloss Reference :	A *** ***** ***** ***** ***** ***** *** ***** *** ***** *** ***** *** ***** *** ***** B+C+D+E  
2024-01-31 23:31:55,892 	Gloss Hypothesis:	A E+A <unk> A+B+E <unk> B+E+C <unk> A+C <unk> A+E <unk> A+E <unk> E+A <unk> E+A <unk> A+B+E+B+E
2024-01-31 23:31:55,892 	Gloss Alignment :	  I   I     I     I     I     I     I   I     I   I     I   I     I   I     I   I     S        
2024-01-31 23:31:55,892 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 23:31:55,895 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** she wrote that half had already been raised by  the family's online fundraiser
2024-01-31 23:31:55,895 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s>   <s>  <s>  <s> <s>     <s>  <s>    <s> <s> <s>      <s>    <s>       
2024-01-31 23:31:55,895 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   S   S     S    S    S   S       S    S      S   S   S        S      S         
2024-01-31 23:31:55,896 ========================================================================================================================
2024-01-31 23:31:55,896 Logging Sequence: 180_332.00
2024-01-31 23:31:55,896 	Gloss Reference :	* ************************************************* A     B+C+D+E
2024-01-31 23:31:55,896 	Gloss Hypothesis:	C E+C+A+E+C+E+C+E+C+E+C+E+C+B+E+C+B+C+E+C+E+C+E+C+E <unk> E+C+E+C
2024-01-31 23:31:55,896 	Gloss Alignment :	I I                                                 S     S      
2024-01-31 23:31:55,896 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 23:31:55,900 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** did i   eat roti made of  shilajit that i   got energy to  assault so  many girls
2024-01-31 23:31:55,900 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s>  <s>  <s> <s>      <s>  <s> <s> <s>    <s> <s>     <s> <s>  <s>  
2024-01-31 23:31:55,900 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   S   S   S   S    S    S   S        S    S   S   S      S   S       S   S    S    
2024-01-31 23:31:55,900 ========================================================================================================================
2024-01-31 23:32:11,321 Epoch 1295: Total Training Recognition Loss 1100.35  Total Training Translation Loss 3486.39 
2024-01-31 23:32:11,322 EPOCH 1296
2024-01-31 23:32:28,952 Epoch 1296: Total Training Recognition Loss 1098.30  Total Training Translation Loss 3488.23 
2024-01-31 23:32:28,952 EPOCH 1297
2024-01-31 23:32:46,581 Epoch 1297: Total Training Recognition Loss 1099.97  Total Training Translation Loss 3486.95 
2024-01-31 23:32:46,581 EPOCH 1298
2024-01-31 23:32:47,316 [Epoch: 1298 Step: 00044100] Batch Recognition Loss:  13.141599 => Gls Tokens per Sec:      872 || Batch Translation Loss:  73.130020 => Txt Tokens per Sec:     2095 || Lr: 0.000050
2024-01-31 23:33:04,101 Epoch 1298: Total Training Recognition Loss 1103.99  Total Training Translation Loss 3486.95 
2024-01-31 23:33:04,101 EPOCH 1299
2024-01-31 23:33:21,788 Epoch 1299: Total Training Recognition Loss 1102.73  Total Training Translation Loss 3487.88 
2024-01-31 23:33:21,788 EPOCH 1300
2024-01-31 23:33:39,319 [Epoch: 1300 Step: 00044200] Batch Recognition Loss:  37.024506 => Gls Tokens per Sec:      606 || Batch Translation Loss: 122.581161 => Txt Tokens per Sec:     1683 || Lr: 0.000050
2024-01-31 23:33:39,319 Epoch 1300: Total Training Recognition Loss 1100.10  Total Training Translation Loss 3487.49 
2024-01-31 23:33:39,319 EPOCH 1301
2024-01-31 23:33:56,949 Epoch 1301: Total Training Recognition Loss 1100.18  Total Training Translation Loss 3487.70 
2024-01-31 23:33:56,949 EPOCH 1302
2024-01-31 23:34:14,566 Epoch 1302: Total Training Recognition Loss 1098.43  Total Training Translation Loss 3487.18 
2024-01-31 23:34:14,566 EPOCH 1303
2024-01-31 23:34:31,028 [Epoch: 1303 Step: 00044300] Batch Recognition Loss:  36.583797 => Gls Tokens per Sec:      607 || Batch Translation Loss: 109.869980 => Txt Tokens per Sec:     1673 || Lr: 0.000050
2024-01-31 23:34:32,277 Epoch 1303: Total Training Recognition Loss 1099.45  Total Training Translation Loss 3487.59 
2024-01-31 23:34:32,278 EPOCH 1304
2024-01-31 23:34:49,921 Epoch 1304: Total Training Recognition Loss 1101.42  Total Training Translation Loss 3486.66 
2024-01-31 23:34:49,922 EPOCH 1305
2024-01-31 23:35:07,460 Epoch 1305: Total Training Recognition Loss 1101.35  Total Training Translation Loss 3487.79 
2024-01-31 23:35:07,460 EPOCH 1306
2024-01-31 23:35:22,671 [Epoch: 1306 Step: 00044400] Batch Recognition Loss:  28.152542 => Gls Tokens per Sec:      615 || Batch Translation Loss:  87.009552 => Txt Tokens per Sec:     1676 || Lr: 0.000050
2024-01-31 23:35:25,124 Epoch 1306: Total Training Recognition Loss 1099.08  Total Training Translation Loss 3486.84 
2024-01-31 23:35:25,125 EPOCH 1307
2024-01-31 23:35:42,718 Epoch 1307: Total Training Recognition Loss 1100.97  Total Training Translation Loss 3487.55 
2024-01-31 23:35:42,719 EPOCH 1308
2024-01-31 23:36:00,368 Epoch 1308: Total Training Recognition Loss 1101.66  Total Training Translation Loss 3487.92 
2024-01-31 23:36:00,368 EPOCH 1309
2024-01-31 23:36:15,363 [Epoch: 1309 Step: 00044500] Batch Recognition Loss:  40.019211 => Gls Tokens per Sec:      581 || Batch Translation Loss: 116.036186 => Txt Tokens per Sec:     1605 || Lr: 0.000050
2024-01-31 23:36:18,023 Epoch 1309: Total Training Recognition Loss 1100.66  Total Training Translation Loss 3487.84 
2024-01-31 23:36:18,023 EPOCH 1310
2024-01-31 23:36:35,596 Epoch 1310: Total Training Recognition Loss 1102.26  Total Training Translation Loss 3487.04 
2024-01-31 23:36:35,596 EPOCH 1311
2024-01-31 23:36:53,125 Epoch 1311: Total Training Recognition Loss 1100.39  Total Training Translation Loss 3487.53 
2024-01-31 23:36:53,125 EPOCH 1312
2024-01-31 23:37:05,280 [Epoch: 1312 Step: 00044600] Batch Recognition Loss:  20.798300 => Gls Tokens per Sec:      664 || Batch Translation Loss:  89.137001 => Txt Tokens per Sec:     1760 || Lr: 0.000050
2024-01-31 23:37:10,709 Epoch 1312: Total Training Recognition Loss 1100.73  Total Training Translation Loss 3486.56 
2024-01-31 23:37:10,709 EPOCH 1313
2024-01-31 23:37:28,213 Epoch 1313: Total Training Recognition Loss 1099.93  Total Training Translation Loss 3487.58 
2024-01-31 23:37:28,213 EPOCH 1314
2024-01-31 23:37:45,948 Epoch 1314: Total Training Recognition Loss 1102.59  Total Training Translation Loss 3486.52 
2024-01-31 23:37:45,948 EPOCH 1315
2024-01-31 23:37:59,473 [Epoch: 1315 Step: 00044700] Batch Recognition Loss:  73.959595 => Gls Tokens per Sec:      549 || Batch Translation Loss: 126.730995 => Txt Tokens per Sec:     1549 || Lr: 0.000050
2024-01-31 23:38:03,603 Epoch 1315: Total Training Recognition Loss 1101.05  Total Training Translation Loss 3487.45 
2024-01-31 23:38:03,604 EPOCH 1316
2024-01-31 23:38:21,325 Epoch 1316: Total Training Recognition Loss 1101.50  Total Training Translation Loss 3486.63 
2024-01-31 23:38:21,326 EPOCH 1317
2024-01-31 23:38:38,887 Epoch 1317: Total Training Recognition Loss 1101.52  Total Training Translation Loss 3486.98 
2024-01-31 23:38:38,887 EPOCH 1318
2024-01-31 23:38:51,020 [Epoch: 1318 Step: 00044800] Batch Recognition Loss:  23.669714 => Gls Tokens per Sec:      580 || Batch Translation Loss:  91.454758 => Txt Tokens per Sec:     1620 || Lr: 0.000050
2024-01-31 23:38:56,575 Epoch 1318: Total Training Recognition Loss 1102.41  Total Training Translation Loss 3487.54 
2024-01-31 23:38:56,575 EPOCH 1319
2024-01-31 23:39:14,059 Epoch 1319: Total Training Recognition Loss 1099.75  Total Training Translation Loss 3487.60 
2024-01-31 23:39:14,059 EPOCH 1320
2024-01-31 23:39:31,750 Epoch 1320: Total Training Recognition Loss 1100.57  Total Training Translation Loss 3486.50 
2024-01-31 23:39:31,750 EPOCH 1321
2024-01-31 23:39:41,956 [Epoch: 1321 Step: 00044900] Batch Recognition Loss:  23.425982 => Gls Tokens per Sec:      627 || Batch Translation Loss:  90.784073 => Txt Tokens per Sec:     1698 || Lr: 0.000050
2024-01-31 23:39:49,285 Epoch 1321: Total Training Recognition Loss 1099.03  Total Training Translation Loss 3487.61 
2024-01-31 23:39:49,285 EPOCH 1322
2024-01-31 23:40:06,670 Epoch 1322: Total Training Recognition Loss 1102.87  Total Training Translation Loss 3487.38 
2024-01-31 23:40:06,671 EPOCH 1323
2024-01-31 23:40:24,338 Epoch 1323: Total Training Recognition Loss 1100.60  Total Training Translation Loss 3487.99 
2024-01-31 23:40:24,339 EPOCH 1324
2024-01-31 23:40:34,219 [Epoch: 1324 Step: 00045000] Batch Recognition Loss:  40.483154 => Gls Tokens per Sec:      583 || Batch Translation Loss: 114.037865 => Txt Tokens per Sec:     1613 || Lr: 0.000050
2024-01-31 23:40:42,089 Epoch 1324: Total Training Recognition Loss 1099.59  Total Training Translation Loss 3487.00 
2024-01-31 23:40:42,090 EPOCH 1325
2024-01-31 23:40:59,680 Epoch 1325: Total Training Recognition Loss 1099.46  Total Training Translation Loss 3487.82 
2024-01-31 23:40:59,681 EPOCH 1326
2024-01-31 23:41:17,216 Epoch 1326: Total Training Recognition Loss 1100.10  Total Training Translation Loss 3487.13 
2024-01-31 23:41:17,216 EPOCH 1327
2024-01-31 23:41:26,495 [Epoch: 1327 Step: 00045100] Batch Recognition Loss:  30.573418 => Gls Tokens per Sec:      552 || Batch Translation Loss: 110.469131 => Txt Tokens per Sec:     1596 || Lr: 0.000050
2024-01-31 23:41:34,734 Epoch 1327: Total Training Recognition Loss 1099.66  Total Training Translation Loss 3487.18 
2024-01-31 23:41:34,734 EPOCH 1328
2024-01-31 23:41:52,331 Epoch 1328: Total Training Recognition Loss 1101.90  Total Training Translation Loss 3487.27 
2024-01-31 23:41:52,331 EPOCH 1329
2024-01-31 23:42:10,040 Epoch 1329: Total Training Recognition Loss 1099.46  Total Training Translation Loss 3487.95 
2024-01-31 23:42:10,040 EPOCH 1330
2024-01-31 23:42:16,824 [Epoch: 1330 Step: 00045200] Batch Recognition Loss:  27.553375 => Gls Tokens per Sec:      624 || Batch Translation Loss:  90.837349 => Txt Tokens per Sec:     1703 || Lr: 0.000050
2024-01-31 23:42:27,620 Epoch 1330: Total Training Recognition Loss 1100.80  Total Training Translation Loss 3487.79 
2024-01-31 23:42:27,621 EPOCH 1331
2024-01-31 23:42:45,268 Epoch 1331: Total Training Recognition Loss 1101.03  Total Training Translation Loss 3485.74 
2024-01-31 23:42:45,268 EPOCH 1332
2024-01-31 23:43:02,854 Epoch 1332: Total Training Recognition Loss 1098.77  Total Training Translation Loss 3485.88 
2024-01-31 23:43:02,854 EPOCH 1333
2024-01-31 23:43:07,829 [Epoch: 1333 Step: 00045300] Batch Recognition Loss:   6.216502 => Gls Tokens per Sec:      772 || Batch Translation Loss:  50.836643 => Txt Tokens per Sec:     1998 || Lr: 0.000050
2024-01-31 23:43:20,538 Epoch 1333: Total Training Recognition Loss 1099.51  Total Training Translation Loss 3487.11 
2024-01-31 23:43:20,538 EPOCH 1334
2024-01-31 23:43:38,055 Epoch 1334: Total Training Recognition Loss 1098.54  Total Training Translation Loss 3486.43 
2024-01-31 23:43:38,056 EPOCH 1335
2024-01-31 23:43:55,650 Epoch 1335: Total Training Recognition Loss 1099.77  Total Training Translation Loss 3487.36 
2024-01-31 23:43:55,651 EPOCH 1336
2024-01-31 23:44:02,190 [Epoch: 1336 Step: 00045400] Batch Recognition Loss:  36.968071 => Gls Tokens per Sec:      489 || Batch Translation Loss: 109.114304 => Txt Tokens per Sec:     1455 || Lr: 0.000050
2024-01-31 23:44:13,262 Epoch 1336: Total Training Recognition Loss 1099.20  Total Training Translation Loss 3486.35 
2024-01-31 23:44:13,263 EPOCH 1337
2024-01-31 23:44:31,005 Epoch 1337: Total Training Recognition Loss 1100.38  Total Training Translation Loss 3487.14 
2024-01-31 23:44:31,005 EPOCH 1338
2024-01-31 23:44:48,438 Epoch 1338: Total Training Recognition Loss 1100.65  Total Training Translation Loss 3487.99 
2024-01-31 23:44:48,438 EPOCH 1339
2024-01-31 23:44:53,997 [Epoch: 1339 Step: 00045500] Batch Recognition Loss:  17.266459 => Gls Tokens per Sec:      416 || Batch Translation Loss:  77.639359 => Txt Tokens per Sec:     1205 || Lr: 0.000050
2024-01-31 23:45:06,016 Epoch 1339: Total Training Recognition Loss 1099.75  Total Training Translation Loss 3487.10 
2024-01-31 23:45:06,016 EPOCH 1340
2024-01-31 23:45:23,626 Epoch 1340: Total Training Recognition Loss 1099.24  Total Training Translation Loss 3488.26 
2024-01-31 23:45:23,627 EPOCH 1341
2024-01-31 23:45:41,206 Epoch 1341: Total Training Recognition Loss 1101.44  Total Training Translation Loss 3487.14 
2024-01-31 23:45:41,207 EPOCH 1342
2024-01-31 23:45:45,521 [Epoch: 1342 Step: 00045600] Batch Recognition Loss:  15.351027 => Gls Tokens per Sec:      445 || Batch Translation Loss:  74.043228 => Txt Tokens per Sec:     1305 || Lr: 0.000050
2024-01-31 23:45:58,843 Epoch 1342: Total Training Recognition Loss 1103.04  Total Training Translation Loss 3486.25 
2024-01-31 23:45:58,844 EPOCH 1343
2024-01-31 23:46:16,373 Epoch 1343: Total Training Recognition Loss 1101.58  Total Training Translation Loss 3486.43 
2024-01-31 23:46:16,373 EPOCH 1344
2024-01-31 23:46:34,055 Epoch 1344: Total Training Recognition Loss 1099.30  Total Training Translation Loss 3487.80 
2024-01-31 23:46:34,056 EPOCH 1345
2024-01-31 23:46:36,648 [Epoch: 1345 Step: 00045700] Batch Recognition Loss:  40.547909 => Gls Tokens per Sec:      494 || Batch Translation Loss: 113.625198 => Txt Tokens per Sec:     1485 || Lr: 0.000050
2024-01-31 23:46:51,794 Epoch 1345: Total Training Recognition Loss 1099.81  Total Training Translation Loss 3487.37 
2024-01-31 23:46:51,794 EPOCH 1346
2024-01-31 23:47:09,506 Epoch 1346: Total Training Recognition Loss 1101.69  Total Training Translation Loss 3487.03 
2024-01-31 23:47:09,507 EPOCH 1347
2024-01-31 23:47:27,082 Epoch 1347: Total Training Recognition Loss 1099.24  Total Training Translation Loss 3486.57 
2024-01-31 23:47:27,082 EPOCH 1348
2024-01-31 23:47:28,393 [Epoch: 1348 Step: 00045800] Batch Recognition Loss:  37.087151 => Gls Tokens per Sec:      489 || Batch Translation Loss: 102.023079 => Txt Tokens per Sec:     1443 || Lr: 0.000050
2024-01-31 23:47:44,612 Epoch 1348: Total Training Recognition Loss 1102.29  Total Training Translation Loss 3487.47 
2024-01-31 23:47:44,612 EPOCH 1349
2024-01-31 23:48:02,112 Epoch 1349: Total Training Recognition Loss 1102.72  Total Training Translation Loss 3487.35 
2024-01-31 23:48:02,112 EPOCH 1350
2024-01-31 23:48:19,619 [Epoch: 1350 Step: 00045900] Batch Recognition Loss:  27.239426 => Gls Tokens per Sec:      607 || Batch Translation Loss:  88.089249 => Txt Tokens per Sec:     1686 || Lr: 0.000050
2024-01-31 23:48:19,620 Epoch 1350: Total Training Recognition Loss 1101.51  Total Training Translation Loss 3487.53 
2024-01-31 23:48:19,620 EPOCH 1351
2024-01-31 23:48:37,184 Epoch 1351: Total Training Recognition Loss 1100.71  Total Training Translation Loss 3487.08 
2024-01-31 23:48:37,185 EPOCH 1352
2024-01-31 23:48:54,793 Epoch 1352: Total Training Recognition Loss 1100.20  Total Training Translation Loss 3487.12 
2024-01-31 23:48:54,793 EPOCH 1353
2024-01-31 23:49:11,504 [Epoch: 1353 Step: 00046000] Batch Recognition Loss:   6.085083 => Gls Tokens per Sec:      598 || Batch Translation Loss:  50.757698 => Txt Tokens per Sec:     1652 || Lr: 0.000050
2024-01-31 23:49:35,284 Validation result at epoch 1353, step    46000: duration: 23.7809s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 345.40955	Translation Loss: 73498.46094	PPL: 1563.98877
	Eval Metric: BLEU
	WER 517.16	(DEL: 4.66,	INS: 428.25,	SUB: 84.25)
	BLEU-4 0.00	(BLEU-1: 0.01,	BLEU-2: 0.00,	BLEU-3: 0.00,	BLEU-4: 0.00)
	CHRF 3.52	ROUGE 0.02
2024-01-31 23:49:35,286 Logging Recognition and Translation Outputs
2024-01-31 23:49:35,286 ========================================================================================================================
2024-01-31 23:49:35,286 Logging Sequence: 126_121.00
2024-01-31 23:49:35,286 	Gloss Reference :	***** * ***** * ***** * ***** A B+C+D+E
2024-01-31 23:49:35,286 	Gloss Hypothesis:	<unk> E <unk> E <unk> E <unk> E <unk>  
2024-01-31 23:49:35,286 	Gloss Alignment :	I     I I     I I     I I     S S      
2024-01-31 23:49:35,287 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 23:49:35,288 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** everyone was very happy by  his victory
2024-01-31 23:49:35,288 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s>      <s> <s>  <s>   <s> <s> <s>    
2024-01-31 23:49:35,289 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   S        S   S    S     S   S   S      
2024-01-31 23:49:35,289 ========================================================================================================================
2024-01-31 23:49:35,289 Logging Sequence: 73_79.00
2024-01-31 23:49:35,289 	Gloss Reference :	***** ***** ***** A ***** * ***** * ***** ********* ***** ***** ***** * ***** ******* B+C+D+E
2024-01-31 23:49:35,289 	Gloss Hypothesis:	<unk> B+C+A <unk> A <unk> B <unk> B <unk> C+B+C+B+C <unk> C+B+A <unk> C <unk> B+C+B+C <unk>  
2024-01-31 23:49:35,290 	Gloss Alignment :	I     I     I       I     I I     I I     I         I     I     I     I I     I       S      
2024-01-31 23:49:35,290 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 23:49:35,293 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** raina resturant has   food  from  the   rich  spices of    north india to    the   aromatic curries of    south india
2024-01-31 23:49:35,294 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> mateo mateo mateo mateo mateo mateo mateo mateo mateo mateo mateo mateo mateo     mateo mateo mateo mateo mateo mateo  mateo mateo mateo mateo mateo mateo    mateo   mateo mateo mateo
2024-01-31 23:49:35,294 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I     I     I     I     I     I     I     I     I     I     I     S     S         S     S     S     S     S     S      S     S     S     S     S     S        S       S     S     S    
2024-01-31 23:49:35,294 ========================================================================================================================
2024-01-31 23:49:35,294 Logging Sequence: 95_152.00
2024-01-31 23:49:35,294 	Gloss Reference :	A ***** B+C+D+E
2024-01-31 23:49:35,294 	Gloss Hypothesis:	A <unk> A+E+A+E
2024-01-31 23:49:35,294 	Gloss Alignment :	  I     S      
2024-01-31 23:49:35,295 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 23:49:35,295 	Text Reference  :	*** *** *** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** how      strange 
2024-01-31 23:49:35,295 	Text Hypothesis :	<s> <s> <s> stepping stepping stepping stepping stepping stepping stepping stepping stepping stepping stepping stepping stepping stepping stepping stepping stepping stepping stepping stepping stepping stepping stepping stepping stepping stepping stepping stepping stepping stepping stepping stepping stepping stepping stepping stepping stepping
2024-01-31 23:49:35,296 	Text Alignment  :	I   I   I   I        I        I        I        I        I        I        I        I        I        I        I        I        I        I        I        I        I        I        I        I        I        I        I        I        I        I        I        I        I        I        I        I        I        I        S        S       
2024-01-31 23:49:35,296 ========================================================================================================================
2024-01-31 23:49:35,296 Logging Sequence: 135_39.00
2024-01-31 23:49:35,296 	Gloss Reference :	***** ******* ***** * ***** * ***** * ***** * ***** *************** ***** * ***** * ***** * ***** * ***** ********************* ***** * ***** A   B+C+D+E
2024-01-31 23:49:35,296 	Gloss Hypothesis:	<unk> E+A+B+A <unk> B <unk> B <unk> E <unk> E <unk> E+B+E+B+E+B+E+B <unk> E <unk> B <unk> E <unk> E <unk> B+C+B+E+B+E+A+E+A+C+E <unk> E <unk> E+A <unk>  
2024-01-31 23:49:35,297 	Gloss Alignment :	I     I       I     I I     I I     I I     I I     I               I     I I     I I     I I     I I     I                     I     I I     S   S      
2024-01-31 23:49:35,297 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 23:49:35,299 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** who needs to  travel from poland to  stanford university in  california
2024-01-31 23:49:35,299 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s>   <s> <s>    <s>  <s>    <s> <s>      <s>        <s> <s>       
2024-01-31 23:49:35,299 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   S   S     S   S      S    S      S   S        S          S   S         
2024-01-31 23:49:35,299 ========================================================================================================================
2024-01-31 23:49:35,300 Logging Sequence: 87_2.00
2024-01-31 23:49:35,300 	Gloss Reference :	*** ***** ******* ***** * ***** ***** A     B+C+D+E                
2024-01-31 23:49:35,300 	Gloss Hypothesis:	D+E <unk> E+D+E+D <unk> E <unk> D+E+D <unk> D+E+D+E+D+E+D+E+D+E+D+E
2024-01-31 23:49:35,300 	Gloss Alignment :	I   I     I       I     I I     I     S     S                      
2024-01-31 23:49:35,300 	--------------------------------------------------------------------------------------------------------------------
2024-01-31 23:49:35,304 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** cricketer gautam gambhir's jealousy against ms  dhoni and virat kohli has been increasing day by  day
2024-01-31 23:49:35,304 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s>       <s>    <s>       <s>      <s>     <s> <s>   <s> <s>   <s>   <s> <s>  <s>        <s> <s> <s>
2024-01-31 23:49:35,304 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   S         S      S         S        S       S   S     S   S     S     S   S    S          S   S   S  
2024-01-31 23:49:35,304 ========================================================================================================================
2024-01-31 23:49:36,226 Epoch 1353: Total Training Recognition Loss 1099.26  Total Training Translation Loss 3486.67 
2024-01-31 23:49:36,226 EPOCH 1354
2024-01-31 23:49:53,773 Epoch 1354: Total Training Recognition Loss 1101.38  Total Training Translation Loss 3487.45 
2024-01-31 23:49:53,773 EPOCH 1355
2024-01-31 23:50:11,388 Epoch 1355: Total Training Recognition Loss 1101.53  Total Training Translation Loss 3486.72 
2024-01-31 23:50:11,389 EPOCH 1356
2024-01-31 23:50:27,542 [Epoch: 1356 Step: 00046100] Batch Recognition Loss:  20.801832 => Gls Tokens per Sec:      579 || Batch Translation Loss:  86.998367 => Txt Tokens per Sec:     1648 || Lr: 0.000050
2024-01-31 23:50:28,783 Epoch 1356: Total Training Recognition Loss 1101.67  Total Training Translation Loss 3488.07 
2024-01-31 23:50:28,783 EPOCH 1357
2024-01-31 23:50:46,472 Epoch 1357: Total Training Recognition Loss 1099.87  Total Training Translation Loss 3487.22 
2024-01-31 23:50:46,472 EPOCH 1358
2024-01-31 23:51:03,994 Epoch 1358: Total Training Recognition Loss 1099.09  Total Training Translation Loss 3487.07 
2024-01-31 23:51:03,994 EPOCH 1359
2024-01-31 23:51:18,595 [Epoch: 1359 Step: 00046200] Batch Recognition Loss:  36.965576 => Gls Tokens per Sec:      597 || Batch Translation Loss: 119.740158 => Txt Tokens per Sec:     1647 || Lr: 0.000050
2024-01-31 23:51:21,709 Epoch 1359: Total Training Recognition Loss 1100.14  Total Training Translation Loss 3488.23 
2024-01-31 23:51:21,709 EPOCH 1360
2024-01-31 23:51:39,352 Epoch 1360: Total Training Recognition Loss 1099.32  Total Training Translation Loss 3486.83 
2024-01-31 23:51:39,353 EPOCH 1361
2024-01-31 23:51:56,863 Epoch 1361: Total Training Recognition Loss 1101.22  Total Training Translation Loss 3487.56 
2024-01-31 23:51:56,863 EPOCH 1362
2024-01-31 23:52:11,216 [Epoch: 1362 Step: 00046300] Batch Recognition Loss:  22.966167 => Gls Tokens per Sec:      580 || Batch Translation Loss:  82.690445 => Txt Tokens per Sec:     1648 || Lr: 0.000050
2024-01-31 23:52:14,485 Epoch 1362: Total Training Recognition Loss 1103.15  Total Training Translation Loss 3487.42 
2024-01-31 23:52:14,485 EPOCH 1363
2024-01-31 23:52:32,252 Epoch 1363: Total Training Recognition Loss 1099.75  Total Training Translation Loss 3487.07 
2024-01-31 23:52:32,252 EPOCH 1364
2024-01-31 23:52:51,853 Epoch 1364: Total Training Recognition Loss 1099.82  Total Training Translation Loss 3487.07 
2024-01-31 23:52:51,854 EPOCH 1365
2024-01-31 23:53:03,824 [Epoch: 1365 Step: 00046400] Batch Recognition Loss:  37.087475 => Gls Tokens per Sec:      621 || Batch Translation Loss: 101.789139 => Txt Tokens per Sec:     1724 || Lr: 0.000050
2024-01-31 23:53:10,143 Epoch 1365: Total Training Recognition Loss 1101.64  Total Training Translation Loss 3487.47 
2024-01-31 23:53:10,143 EPOCH 1366
2024-01-31 23:53:27,711 Epoch 1366: Total Training Recognition Loss 1097.96  Total Training Translation Loss 3487.36 
2024-01-31 23:53:27,712 EPOCH 1367
2024-01-31 23:53:45,400 Epoch 1367: Total Training Recognition Loss 1100.56  Total Training Translation Loss 3487.57 
2024-01-31 23:53:45,400 EPOCH 1368
2024-01-31 23:53:58,096 [Epoch: 1368 Step: 00046500] Batch Recognition Loss:  57.829540 => Gls Tokens per Sec:      555 || Batch Translation Loss: 132.976715 => Txt Tokens per Sec:     1611 || Lr: 0.000050
2024-01-31 23:54:02,957 Epoch 1368: Total Training Recognition Loss 1101.21  Total Training Translation Loss 3487.75 
2024-01-31 23:54:02,958 EPOCH 1369
2024-01-31 23:54:20,436 Epoch 1369: Total Training Recognition Loss 1100.80  Total Training Translation Loss 3486.39 
2024-01-31 23:54:20,437 EPOCH 1370
2024-01-31 23:54:38,065 Epoch 1370: Total Training Recognition Loss 1101.63  Total Training Translation Loss 3487.68 
2024-01-31 23:54:38,065 EPOCH 1371
2024-01-31 23:54:48,401 [Epoch: 1371 Step: 00046600] Batch Recognition Loss:  23.251816 => Gls Tokens per Sec:      595 || Batch Translation Loss:  93.735878 => Txt Tokens per Sec:     1620 || Lr: 0.000050
2024-01-31 23:54:55,648 Epoch 1371: Total Training Recognition Loss 1098.60  Total Training Translation Loss 3487.26 
2024-01-31 23:54:55,648 EPOCH 1372
2024-01-31 23:55:13,392 Epoch 1372: Total Training Recognition Loss 1099.11  Total Training Translation Loss 3488.29 
2024-01-31 23:55:13,392 EPOCH 1373
2024-01-31 23:55:30,925 Epoch 1373: Total Training Recognition Loss 1099.92  Total Training Translation Loss 3487.68 
2024-01-31 23:55:30,926 EPOCH 1374
2024-01-31 23:55:38,851 [Epoch: 1374 Step: 00046700] Batch Recognition Loss:  31.533653 => Gls Tokens per Sec:      695 || Batch Translation Loss:  93.952881 => Txt Tokens per Sec:     1891 || Lr: 0.000050
2024-01-31 23:55:48,482 Epoch 1374: Total Training Recognition Loss 1099.30  Total Training Translation Loss 3487.32 
2024-01-31 23:55:48,482 EPOCH 1375
2024-01-31 23:56:06,036 Epoch 1375: Total Training Recognition Loss 1100.93  Total Training Translation Loss 3487.70 
2024-01-31 23:56:06,037 EPOCH 1376
2024-01-31 23:56:23,543 Epoch 1376: Total Training Recognition Loss 1101.68  Total Training Translation Loss 3487.32 
2024-01-31 23:56:23,543 EPOCH 1377
2024-01-31 23:56:32,811 [Epoch: 1377 Step: 00046800] Batch Recognition Loss:  27.943047 => Gls Tokens per Sec:      552 || Batch Translation Loss:  90.958267 => Txt Tokens per Sec:     1549 || Lr: 0.000050
2024-01-31 23:56:41,079 Epoch 1377: Total Training Recognition Loss 1100.08  Total Training Translation Loss 3487.49 
2024-01-31 23:56:41,080 EPOCH 1378
2024-01-31 23:56:58,634 Epoch 1378: Total Training Recognition Loss 1100.88  Total Training Translation Loss 3487.61 
2024-01-31 23:56:58,635 EPOCH 1379
2024-01-31 23:57:16,323 Epoch 1379: Total Training Recognition Loss 1100.14  Total Training Translation Loss 3487.40 
2024-01-31 23:57:16,324 EPOCH 1380
2024-01-31 23:57:23,443 [Epoch: 1380 Step: 00046900] Batch Recognition Loss:  18.472061 => Gls Tokens per Sec:      594 || Batch Translation Loss:  90.799149 => Txt Tokens per Sec:     1659 || Lr: 0.000050
2024-01-31 23:57:33,940 Epoch 1380: Total Training Recognition Loss 1100.91  Total Training Translation Loss 3487.99 
2024-01-31 23:57:33,940 EPOCH 1381
2024-01-31 23:57:51,536 Epoch 1381: Total Training Recognition Loss 1099.30  Total Training Translation Loss 3486.75 
2024-01-31 23:57:51,536 EPOCH 1382
2024-01-31 23:58:09,073 Epoch 1382: Total Training Recognition Loss 1101.17  Total Training Translation Loss 3487.72 
2024-01-31 23:58:09,074 EPOCH 1383
2024-01-31 23:58:14,973 [Epoch: 1383 Step: 00047000] Batch Recognition Loss:  37.529617 => Gls Tokens per Sec:      651 || Batch Translation Loss: 117.690315 => Txt Tokens per Sec:     1815 || Lr: 0.000050
2024-01-31 23:58:26,761 Epoch 1383: Total Training Recognition Loss 1099.79  Total Training Translation Loss 3486.84 
2024-01-31 23:58:26,761 EPOCH 1384
2024-01-31 23:58:44,416 Epoch 1384: Total Training Recognition Loss 1100.18  Total Training Translation Loss 3487.64 
2024-01-31 23:58:44,416 EPOCH 1385
2024-01-31 23:59:02,151 Epoch 1385: Total Training Recognition Loss 1101.61  Total Training Translation Loss 3487.32 
2024-01-31 23:59:02,151 EPOCH 1386
2024-01-31 23:59:06,612 [Epoch: 1386 Step: 00047100] Batch Recognition Loss:  31.104269 => Gls Tokens per Sec:      718 || Batch Translation Loss:  95.060959 => Txt Tokens per Sec:     1901 || Lr: 0.000050
2024-01-31 23:59:19,842 Epoch 1386: Total Training Recognition Loss 1102.26  Total Training Translation Loss 3486.94 
2024-01-31 23:59:19,842 EPOCH 1387
2024-01-31 23:59:37,649 Epoch 1387: Total Training Recognition Loss 1099.86  Total Training Translation Loss 3486.50 
2024-01-31 23:59:37,650 EPOCH 1388
2024-01-31 23:59:55,474 Epoch 1388: Total Training Recognition Loss 1102.01  Total Training Translation Loss 3487.55 
2024-01-31 23:59:55,474 EPOCH 1389
2024-02-01 00:00:00,280 [Epoch: 1389 Step: 00047200] Batch Recognition Loss:  38.033283 => Gls Tokens per Sec:      481 || Batch Translation Loss: 120.037262 => Txt Tokens per Sec:     1343 || Lr: 0.000050
2024-02-01 00:00:13,147 Epoch 1389: Total Training Recognition Loss 1099.82  Total Training Translation Loss 3486.55 
2024-02-01 00:00:13,147 EPOCH 1390
2024-02-01 00:00:30,744 Epoch 1390: Total Training Recognition Loss 1100.04  Total Training Translation Loss 3487.44 
2024-02-01 00:00:30,744 EPOCH 1391
2024-02-01 00:00:48,427 Epoch 1391: Total Training Recognition Loss 1101.25  Total Training Translation Loss 3487.84 
2024-02-01 00:00:48,428 EPOCH 1392
2024-02-01 00:00:50,775 [Epoch: 1392 Step: 00047300] Batch Recognition Loss:  36.831051 => Gls Tokens per Sec:      818 || Batch Translation Loss:  99.061310 => Txt Tokens per Sec:     2149 || Lr: 0.000050
2024-02-01 00:01:06,133 Epoch 1392: Total Training Recognition Loss 1100.57  Total Training Translation Loss 3487.06 
2024-02-01 00:01:06,133 EPOCH 1393
2024-02-01 00:01:23,724 Epoch 1393: Total Training Recognition Loss 1100.06  Total Training Translation Loss 3487.20 
2024-02-01 00:01:23,724 EPOCH 1394
2024-02-01 00:01:41,125 Epoch 1394: Total Training Recognition Loss 1099.97  Total Training Translation Loss 3486.82 
2024-02-01 00:01:41,125 EPOCH 1395
2024-02-01 00:01:43,734 [Epoch: 1395 Step: 00047400] Batch Recognition Loss:  45.846226 => Gls Tokens per Sec:      491 || Batch Translation Loss: 124.753395 => Txt Tokens per Sec:     1485 || Lr: 0.000050
2024-02-01 00:01:58,684 Epoch 1395: Total Training Recognition Loss 1102.17  Total Training Translation Loss 3488.20 
2024-02-01 00:01:58,684 EPOCH 1396
2024-02-01 00:02:16,450 Epoch 1396: Total Training Recognition Loss 1101.23  Total Training Translation Loss 3487.39 
2024-02-01 00:02:16,451 EPOCH 1397
2024-02-01 00:02:34,210 Epoch 1397: Total Training Recognition Loss 1101.39  Total Training Translation Loss 3486.95 
2024-02-01 00:02:34,211 EPOCH 1398
2024-02-01 00:02:36,355 [Epoch: 1398 Step: 00047500] Batch Recognition Loss:  58.056335 => Gls Tokens per Sec:      299 || Batch Translation Loss: 132.199692 => Txt Tokens per Sec:      906 || Lr: 0.000050
2024-02-01 00:02:51,841 Epoch 1398: Total Training Recognition Loss 1099.98  Total Training Translation Loss 3486.95 
2024-02-01 00:02:51,842 EPOCH 1399
2024-02-01 00:03:09,568 Epoch 1399: Total Training Recognition Loss 1101.67  Total Training Translation Loss 3487.67 
2024-02-01 00:03:09,569 EPOCH 1400
2024-02-01 00:03:27,221 [Epoch: 1400 Step: 00047600] Batch Recognition Loss:  32.301674 => Gls Tokens per Sec:      602 || Batch Translation Loss: 105.173973 => Txt Tokens per Sec:     1672 || Lr: 0.000050
2024-02-01 00:03:27,221 Epoch 1400: Total Training Recognition Loss 1099.88  Total Training Translation Loss 3486.92 
2024-02-01 00:03:27,221 EPOCH 1401
2024-02-01 00:03:44,900 Epoch 1401: Total Training Recognition Loss 1100.48  Total Training Translation Loss 3486.78 
2024-02-01 00:03:44,900 EPOCH 1402
2024-02-01 00:04:02,587 Epoch 1402: Total Training Recognition Loss 1100.37  Total Training Translation Loss 3487.77 
2024-02-01 00:04:02,587 EPOCH 1403
2024-02-01 00:04:19,659 [Epoch: 1403 Step: 00047700] Batch Recognition Loss:  38.889439 => Gls Tokens per Sec:      585 || Batch Translation Loss: 123.143898 => Txt Tokens per Sec:     1639 || Lr: 0.000050
2024-02-01 00:04:20,267 Epoch 1403: Total Training Recognition Loss 1100.12  Total Training Translation Loss 3486.94 
2024-02-01 00:04:20,267 EPOCH 1404
2024-02-01 00:04:38,165 Epoch 1404: Total Training Recognition Loss 1099.19  Total Training Translation Loss 3487.67 
2024-02-01 00:04:38,165 EPOCH 1405
2024-02-01 00:04:55,873 Epoch 1405: Total Training Recognition Loss 1101.70  Total Training Translation Loss 3487.82 
2024-02-01 00:04:55,874 EPOCH 1406
2024-02-01 00:05:11,960 [Epoch: 1406 Step: 00047800] Batch Recognition Loss:  15.376400 => Gls Tokens per Sec:      581 || Batch Translation Loss:  73.985458 => Txt Tokens per Sec:     1638 || Lr: 0.000050
2024-02-01 00:05:13,354 Epoch 1406: Total Training Recognition Loss 1099.61  Total Training Translation Loss 3486.69 
2024-02-01 00:05:13,355 EPOCH 1407
2024-02-01 00:05:31,021 Epoch 1407: Total Training Recognition Loss 1101.95  Total Training Translation Loss 3487.01 
2024-02-01 00:05:31,022 EPOCH 1408
2024-02-01 00:05:48,725 Epoch 1408: Total Training Recognition Loss 1101.79  Total Training Translation Loss 3487.77 
2024-02-01 00:05:48,726 EPOCH 1409
2024-02-01 00:06:02,203 [Epoch: 1409 Step: 00047900] Batch Recognition Loss:  37.081055 => Gls Tokens per Sec:      646 || Batch Translation Loss: 120.528526 => Txt Tokens per Sec:     1773 || Lr: 0.000050
2024-02-01 00:06:06,639 Epoch 1409: Total Training Recognition Loss 1101.25  Total Training Translation Loss 3487.42 
2024-02-01 00:06:06,640 EPOCH 1410
2024-02-01 00:06:24,383 Epoch 1410: Total Training Recognition Loss 1098.15  Total Training Translation Loss 3486.97 
2024-02-01 00:06:24,383 EPOCH 1411
2024-02-01 00:06:41,877 Epoch 1411: Total Training Recognition Loss 1099.84  Total Training Translation Loss 3486.49 
2024-02-01 00:06:41,877 EPOCH 1412
2024-02-01 00:06:55,259 [Epoch: 1412 Step: 00048000] Batch Recognition Loss:  27.908405 => Gls Tokens per Sec:      603 || Batch Translation Loss:  91.355881 => Txt Tokens per Sec:     1675 || Lr: 0.000050
2024-02-01 00:07:19,584 Validation result at epoch 1412, step    48000: duration: 24.3247s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 348.15094	Translation Loss: 73481.83594	PPL: 1561.38867
	Eval Metric: BLEU
	WER 532.13	(DEL: 4.52,	INS: 443.08,	SUB: 84.53)
	BLEU-4 0.00	(BLEU-1: 0.01,	BLEU-2: 0.00,	BLEU-3: 0.00,	BLEU-4: 0.00)
	CHRF 3.53	ROUGE 0.02
2024-02-01 00:07:19,586 Logging Recognition and Translation Outputs
2024-02-01 00:07:19,586 ========================================================================================================================
2024-02-01 00:07:19,586 Logging Sequence: 88_159.00
2024-02-01 00:07:19,586 	Gloss Reference :	***** * ***** * ***** * ***** A B+C+D+E
2024-02-01 00:07:19,587 	Gloss Hypothesis:	<unk> B <unk> B <unk> B <unk> B <unk>  
2024-02-01 00:07:19,587 	Gloss Alignment :	I     I I     I I     I I     S S      
2024-02-01 00:07:19,587 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 00:07:19,589 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** however he  often comes to  the town to  meet his relatives
2024-02-01 00:07:19,590 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s>     <s> <s>   <s>   <s> <s> <s>  <s> <s>  <s> <s>      
2024-02-01 00:07:19,590 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   S       S   S     S     S   S   S    S   S    S   S        
2024-02-01 00:07:19,590 ========================================================================================================================
2024-02-01 00:07:19,590 Logging Sequence: 180_53.00
2024-02-01 00:07:19,590 	Gloss Reference :	***** ******* ***** *** A     B+C+D+E
2024-02-01 00:07:19,590 	Gloss Hypothesis:	<unk> E+C+E+A <unk> E+A <unk> E      
2024-02-01 00:07:19,590 	Gloss Alignment :	I     I       I     I   S     S      
2024-02-01 00:07:19,591 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 00:07:19,592 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** the protest is  against singh again
2024-02-01 00:07:19,592 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s>     <s> <s>     <s>   <s>  
2024-02-01 00:07:19,592 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   S   S       S   S       S     S    
2024-02-01 00:07:19,593 ========================================================================================================================
2024-02-01 00:07:19,593 Logging Sequence: 163_30.00
2024-02-01 00:07:19,593 	Gloss Reference :	***** ***** ***** ***** A ***** ***** ***** ***** ***** * ***** ***** ***** * ***** B+C+D+E
2024-02-01 00:07:19,593 	Gloss Hypothesis:	<pad> <unk> <pad> <unk> A <pad> <unk> <pad> B+E+B <unk> E <unk> <pad> <unk> E <unk> <pad>  
2024-02-01 00:07:19,593 	Gloss Alignment :	I     I     I     I       I     I     I     I     I     I I     I     I     I I     S      
2024-02-01 00:07:19,593 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 00:07:19,595 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* they    never   permitted anyone  to      reveal  her     face   
2024-02-01 00:07:19,595 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing   nothing nothing nothing nothing nothing
2024-02-01 00:07:19,596 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       S       S       S         S       S       S       S       S      
2024-02-01 00:07:19,596 ========================================================================================================================
2024-02-01 00:07:19,596 Logging Sequence: 51_110.00
2024-02-01 00:07:19,596 	Gloss Reference :	A B+C+D+E
2024-02-01 00:07:19,596 	Gloss Hypothesis:	* E      
2024-02-01 00:07:19,596 	Gloss Alignment :	D S      
2024-02-01 00:07:19,596 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 00:07:19,598 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* the     aussies were    very    happy   with    their   victory
2024-02-01 00:07:19,598 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing
2024-02-01 00:07:19,598 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I       I       I       I       I       I       I       I       I       I       I       I       I       I       S       S       S       S       S       S       S       S      
2024-02-01 00:07:19,599 ========================================================================================================================
2024-02-01 00:07:19,599 Logging Sequence: 70_249.00
2024-02-01 00:07:19,599 	Gloss Reference :	***** * ***** * ***** * ***** * ***** ***** ***** * ***** * ***** A B+C+D+E
2024-02-01 00:07:19,599 	Gloss Hypothesis:	<unk> E <unk> E <unk> E <unk> E <unk> E+C+E <unk> E <unk> E <unk> C <unk>  
2024-02-01 00:07:19,599 	Gloss Alignment :	I     I I     I I     I I     I I     I     I     I I     I I     S S      
2024-02-01 00:07:19,599 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 00:07:19,601 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** have a   look at  this video
2024-02-01 00:07:19,601 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s>  <s> <s>  <s> <s>  <s>  
2024-02-01 00:07:19,601 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   S    S   S    S   S    S    
2024-02-01 00:07:19,601 ========================================================================================================================
2024-02-01 00:07:23,756 Epoch 1412: Total Training Recognition Loss 1100.54  Total Training Translation Loss 3486.56 
2024-02-01 00:07:23,756 EPOCH 1413
2024-02-01 00:07:41,193 Epoch 1413: Total Training Recognition Loss 1101.25  Total Training Translation Loss 3487.58 
2024-02-01 00:07:41,194 EPOCH 1414
2024-02-01 00:07:58,581 Epoch 1414: Total Training Recognition Loss 1100.32  Total Training Translation Loss 3486.85 
2024-02-01 00:07:58,581 EPOCH 1415
2024-02-01 00:08:10,936 [Epoch: 1415 Step: 00048100] Batch Recognition Loss:  23.718666 => Gls Tokens per Sec:      622 || Batch Translation Loss:  97.499741 => Txt Tokens per Sec:     1738 || Lr: 0.000050
2024-02-01 00:08:15,974 Epoch 1415: Total Training Recognition Loss 1100.34  Total Training Translation Loss 3487.59 
2024-02-01 00:08:15,974 EPOCH 1416
2024-02-01 00:08:33,411 Epoch 1416: Total Training Recognition Loss 1102.61  Total Training Translation Loss 3487.31 
2024-02-01 00:08:33,411 EPOCH 1417
2024-02-01 00:08:50,986 Epoch 1417: Total Training Recognition Loss 1099.85  Total Training Translation Loss 3487.04 
2024-02-01 00:08:50,987 EPOCH 1418
2024-02-01 00:09:03,659 [Epoch: 1418 Step: 00048200] Batch Recognition Loss:  37.052399 => Gls Tokens per Sec:      536 || Batch Translation Loss: 101.779167 => Txt Tokens per Sec:     1516 || Lr: 0.000050
2024-02-01 00:09:08,521 Epoch 1418: Total Training Recognition Loss 1101.08  Total Training Translation Loss 3488.01 
2024-02-01 00:09:08,521 EPOCH 1419
2024-02-01 00:09:26,093 Epoch 1419: Total Training Recognition Loss 1102.43  Total Training Translation Loss 3487.48 
2024-02-01 00:09:26,093 EPOCH 1420
2024-02-01 00:09:43,608 Epoch 1420: Total Training Recognition Loss 1100.17  Total Training Translation Loss 3488.46 
2024-02-01 00:09:43,608 EPOCH 1421
2024-02-01 00:09:53,422 [Epoch: 1421 Step: 00048300] Batch Recognition Loss:  33.353138 => Gls Tokens per Sec:      627 || Batch Translation Loss: 105.726929 => Txt Tokens per Sec:     1772 || Lr: 0.000050
2024-02-01 00:10:01,175 Epoch 1421: Total Training Recognition Loss 1103.03  Total Training Translation Loss 3486.66 
2024-02-01 00:10:01,175 EPOCH 1422
2024-02-01 00:10:18,850 Epoch 1422: Total Training Recognition Loss 1100.00  Total Training Translation Loss 3486.15 
2024-02-01 00:10:18,850 EPOCH 1423
2024-02-01 00:10:36,356 Epoch 1423: Total Training Recognition Loss 1100.85  Total Training Translation Loss 3486.22 
2024-02-01 00:10:36,356 EPOCH 1424
2024-02-01 00:10:47,164 [Epoch: 1424 Step: 00048400] Batch Recognition Loss:  50.432484 => Gls Tokens per Sec:      510 || Batch Translation Loss: 128.317810 => Txt Tokens per Sec:     1413 || Lr: 0.000050
2024-02-01 00:10:53,990 Epoch 1424: Total Training Recognition Loss 1099.84  Total Training Translation Loss 3487.10 
2024-02-01 00:10:53,990 EPOCH 1425
2024-02-01 00:11:11,608 Epoch 1425: Total Training Recognition Loss 1101.96  Total Training Translation Loss 3486.06 
2024-02-01 00:11:11,608 EPOCH 1426
2024-02-01 00:11:29,106 Epoch 1426: Total Training Recognition Loss 1100.82  Total Training Translation Loss 3486.78 
2024-02-01 00:11:29,106 EPOCH 1427
2024-02-01 00:11:36,722 [Epoch: 1427 Step: 00048500] Batch Recognition Loss:  38.062447 => Gls Tokens per Sec:      672 || Batch Translation Loss: 115.676285 => Txt Tokens per Sec:     1825 || Lr: 0.000050
2024-02-01 00:11:46,677 Epoch 1427: Total Training Recognition Loss 1101.45  Total Training Translation Loss 3487.52 
2024-02-01 00:11:46,677 EPOCH 1428
2024-02-01 00:12:04,156 Epoch 1428: Total Training Recognition Loss 1101.13  Total Training Translation Loss 3488.04 
2024-02-01 00:12:04,157 EPOCH 1429
2024-02-01 00:12:21,678 Epoch 1429: Total Training Recognition Loss 1097.27  Total Training Translation Loss 3486.95 
2024-02-01 00:12:21,679 EPOCH 1430
2024-02-01 00:12:29,851 [Epoch: 1430 Step: 00048600] Batch Recognition Loss:  42.532299 => Gls Tokens per Sec:      548 || Batch Translation Loss: 130.399460 => Txt Tokens per Sec:     1553 || Lr: 0.000050
2024-02-01 00:12:39,247 Epoch 1430: Total Training Recognition Loss 1100.67  Total Training Translation Loss 3486.57 
2024-02-01 00:12:39,247 EPOCH 1431
2024-02-01 00:12:56,876 Epoch 1431: Total Training Recognition Loss 1100.75  Total Training Translation Loss 3487.94 
2024-02-01 00:12:56,876 EPOCH 1432
2024-02-01 00:13:14,395 Epoch 1432: Total Training Recognition Loss 1099.87  Total Training Translation Loss 3486.96 
2024-02-01 00:13:14,395 EPOCH 1433
2024-02-01 00:13:21,508 [Epoch: 1433 Step: 00048700] Batch Recognition Loss:  37.786140 => Gls Tokens per Sec:      540 || Batch Translation Loss: 100.857971 => Txt Tokens per Sec:     1500 || Lr: 0.000050
2024-02-01 00:13:31,915 Epoch 1433: Total Training Recognition Loss 1101.40  Total Training Translation Loss 3487.67 
2024-02-01 00:13:31,915 EPOCH 1434
2024-02-01 00:13:49,528 Epoch 1434: Total Training Recognition Loss 1100.28  Total Training Translation Loss 3486.89 
2024-02-01 00:13:49,528 EPOCH 1435
2024-02-01 00:14:07,147 Epoch 1435: Total Training Recognition Loss 1100.01  Total Training Translation Loss 3487.77 
2024-02-01 00:14:07,148 EPOCH 1436
2024-02-01 00:14:12,014 [Epoch: 1436 Step: 00048800] Batch Recognition Loss:  33.564587 => Gls Tokens per Sec:      658 || Batch Translation Loss:  97.021912 => Txt Tokens per Sec:     1825 || Lr: 0.000050
2024-02-01 00:14:24,946 Epoch 1436: Total Training Recognition Loss 1102.02  Total Training Translation Loss 3486.62 
2024-02-01 00:14:24,946 EPOCH 1437
2024-02-01 00:14:42,564 Epoch 1437: Total Training Recognition Loss 1101.03  Total Training Translation Loss 3487.45 
2024-02-01 00:14:42,564 EPOCH 1438
2024-02-01 00:15:00,274 Epoch 1438: Total Training Recognition Loss 1101.62  Total Training Translation Loss 3487.53 
2024-02-01 00:15:00,274 EPOCH 1439
2024-02-01 00:15:03,003 [Epoch: 1439 Step: 00048900] Batch Recognition Loss:  11.074953 => Gls Tokens per Sec:      938 || Batch Translation Loss:  62.331093 => Txt Tokens per Sec:     2108 || Lr: 0.000050
2024-02-01 00:15:17,874 Epoch 1439: Total Training Recognition Loss 1100.03  Total Training Translation Loss 3487.00 
2024-02-01 00:15:17,875 EPOCH 1440
2024-02-01 00:15:35,559 Epoch 1440: Total Training Recognition Loss 1100.58  Total Training Translation Loss 3486.63 
2024-02-01 00:15:35,559 EPOCH 1441
2024-02-01 00:15:53,180 Epoch 1441: Total Training Recognition Loss 1100.97  Total Training Translation Loss 3487.51 
2024-02-01 00:15:53,181 EPOCH 1442
2024-02-01 00:15:55,964 [Epoch: 1442 Step: 00049000] Batch Recognition Loss:  23.901329 => Gls Tokens per Sec:      600 || Batch Translation Loss:  97.611465 => Txt Tokens per Sec:     1633 || Lr: 0.000050
2024-02-01 00:16:10,701 Epoch 1442: Total Training Recognition Loss 1099.41  Total Training Translation Loss 3486.31 
2024-02-01 00:16:10,702 EPOCH 1443
2024-02-01 00:16:28,415 Epoch 1443: Total Training Recognition Loss 1100.93  Total Training Translation Loss 3487.28 
2024-02-01 00:16:28,415 EPOCH 1444
2024-02-01 00:16:46,031 Epoch 1444: Total Training Recognition Loss 1100.94  Total Training Translation Loss 3486.95 
2024-02-01 00:16:46,031 EPOCH 1445
2024-02-01 00:16:47,908 [Epoch: 1445 Step: 00049100] Batch Recognition Loss:  36.800880 => Gls Tokens per Sec:      549 || Batch Translation Loss: 109.311577 => Txt Tokens per Sec:     1564 || Lr: 0.000050
2024-02-01 00:17:03,447 Epoch 1445: Total Training Recognition Loss 1100.11  Total Training Translation Loss 3487.88 
2024-02-01 00:17:03,447 EPOCH 1446
2024-02-01 00:17:20,832 Epoch 1446: Total Training Recognition Loss 1101.68  Total Training Translation Loss 3487.21 
2024-02-01 00:17:20,832 EPOCH 1447
2024-02-01 00:17:38,250 Epoch 1447: Total Training Recognition Loss 1099.29  Total Training Translation Loss 3487.71 
2024-02-01 00:17:38,250 EPOCH 1448
2024-02-01 00:17:39,210 [Epoch: 1448 Step: 00049200] Batch Recognition Loss:  72.412315 => Gls Tokens per Sec:      407 || Batch Translation Loss: 126.455841 => Txt Tokens per Sec:      964 || Lr: 0.000050
2024-02-01 00:17:55,765 Epoch 1448: Total Training Recognition Loss 1099.63  Total Training Translation Loss 3486.54 
2024-02-01 00:17:55,765 EPOCH 1449
2024-02-01 00:18:13,256 Epoch 1449: Total Training Recognition Loss 1099.10  Total Training Translation Loss 3487.60 
2024-02-01 00:18:13,256 EPOCH 1450
2024-02-01 00:18:30,657 [Epoch: 1450 Step: 00049300] Batch Recognition Loss:  22.606285 => Gls Tokens per Sec:      611 || Batch Translation Loss:  83.969666 => Txt Tokens per Sec:     1696 || Lr: 0.000050
2024-02-01 00:18:30,658 Epoch 1450: Total Training Recognition Loss 1098.15  Total Training Translation Loss 3486.53 
2024-02-01 00:18:30,658 EPOCH 1451
2024-02-01 00:18:48,135 Epoch 1451: Total Training Recognition Loss 1101.64  Total Training Translation Loss 3487.25 
2024-02-01 00:18:48,135 EPOCH 1452
2024-02-01 00:19:05,754 Epoch 1452: Total Training Recognition Loss 1102.11  Total Training Translation Loss 3487.46 
2024-02-01 00:19:05,755 EPOCH 1453
2024-02-01 00:19:22,207 [Epoch: 1453 Step: 00049400] Batch Recognition Loss:  45.934048 => Gls Tokens per Sec:      607 || Batch Translation Loss: 124.764923 => Txt Tokens per Sec:     1681 || Lr: 0.000050
2024-02-01 00:19:23,355 Epoch 1453: Total Training Recognition Loss 1102.93  Total Training Translation Loss 3486.54 
2024-02-01 00:19:23,355 EPOCH 1454
2024-02-01 00:19:40,941 Epoch 1454: Total Training Recognition Loss 1099.72  Total Training Translation Loss 3487.58 
2024-02-01 00:19:40,942 EPOCH 1455
2024-02-01 00:19:58,573 Epoch 1455: Total Training Recognition Loss 1104.19  Total Training Translation Loss 3486.61 
2024-02-01 00:19:58,573 EPOCH 1456
2024-02-01 00:20:13,382 [Epoch: 1456 Step: 00049500] Batch Recognition Loss:  38.476334 => Gls Tokens per Sec:      631 || Batch Translation Loss: 111.648689 => Txt Tokens per Sec:     1750 || Lr: 0.000050
2024-02-01 00:20:16,297 Epoch 1456: Total Training Recognition Loss 1100.79  Total Training Translation Loss 3487.22 
2024-02-01 00:20:16,298 EPOCH 1457
2024-02-01 00:20:33,923 Epoch 1457: Total Training Recognition Loss 1100.66  Total Training Translation Loss 3486.85 
2024-02-01 00:20:33,924 EPOCH 1458
2024-02-01 00:20:51,725 Epoch 1458: Total Training Recognition Loss 1100.32  Total Training Translation Loss 3487.17 
2024-02-01 00:20:51,725 EPOCH 1459
2024-02-01 00:21:06,857 [Epoch: 1459 Step: 00049600] Batch Recognition Loss:  28.631281 => Gls Tokens per Sec:      576 || Batch Translation Loss:  90.508766 => Txt Tokens per Sec:     1621 || Lr: 0.000050
2024-02-01 00:21:09,351 Epoch 1459: Total Training Recognition Loss 1102.48  Total Training Translation Loss 3487.64 
2024-02-01 00:21:09,351 EPOCH 1460
2024-02-01 00:21:26,962 Epoch 1460: Total Training Recognition Loss 1098.73  Total Training Translation Loss 3485.70 
2024-02-01 00:21:26,962 EPOCH 1461
2024-02-01 00:21:44,496 Epoch 1461: Total Training Recognition Loss 1100.35  Total Training Translation Loss 3487.94 
2024-02-01 00:21:44,496 EPOCH 1462
2024-02-01 00:21:59,015 [Epoch: 1462 Step: 00049700] Batch Recognition Loss:  37.223473 => Gls Tokens per Sec:      573 || Batch Translation Loss: 107.794632 => Txt Tokens per Sec:     1622 || Lr: 0.000050
2024-02-01 00:22:02,167 Epoch 1462: Total Training Recognition Loss 1100.08  Total Training Translation Loss 3487.93 
2024-02-01 00:22:02,167 EPOCH 1463
2024-02-01 00:22:19,842 Epoch 1463: Total Training Recognition Loss 1101.53  Total Training Translation Loss 3487.53 
2024-02-01 00:22:19,843 EPOCH 1464
2024-02-01 00:22:37,517 Epoch 1464: Total Training Recognition Loss 1101.41  Total Training Translation Loss 3487.63 
2024-02-01 00:22:37,517 EPOCH 1465
2024-02-01 00:22:49,635 [Epoch: 1465 Step: 00049800] Batch Recognition Loss:  51.499626 => Gls Tokens per Sec:      634 || Batch Translation Loss: 128.056305 => Txt Tokens per Sec:     1737 || Lr: 0.000050
2024-02-01 00:22:57,156 Epoch 1465: Total Training Recognition Loss 1099.83  Total Training Translation Loss 3487.53 
2024-02-01 00:22:57,156 EPOCH 1466
2024-02-01 00:23:16,195 Epoch 1466: Total Training Recognition Loss 1102.00  Total Training Translation Loss 3488.16 
2024-02-01 00:23:16,195 EPOCH 1467
2024-02-01 00:23:33,707 Epoch 1467: Total Training Recognition Loss 1099.39  Total Training Translation Loss 3487.34 
2024-02-01 00:23:33,707 EPOCH 1468
2024-02-01 00:23:43,630 [Epoch: 1468 Step: 00049900] Batch Recognition Loss:  34.457687 => Gls Tokens per Sec:      684 || Batch Translation Loss: 108.651176 => Txt Tokens per Sec:     1775 || Lr: 0.000050
2024-02-01 00:23:51,124 Epoch 1468: Total Training Recognition Loss 1100.79  Total Training Translation Loss 3487.99 
2024-02-01 00:23:51,125 EPOCH 1469
2024-02-01 00:24:08,630 Epoch 1469: Total Training Recognition Loss 1100.10  Total Training Translation Loss 3486.80 
2024-02-01 00:24:08,630 EPOCH 1470
2024-02-01 00:24:26,124 Epoch 1470: Total Training Recognition Loss 1103.79  Total Training Translation Loss 3486.42 
2024-02-01 00:24:26,124 EPOCH 1471
2024-02-01 00:24:34,944 [Epoch: 1471 Step: 00050000] Batch Recognition Loss:  15.460611 => Gls Tokens per Sec:      726 || Batch Translation Loss:  73.730377 => Txt Tokens per Sec:     1956 || Lr: 0.000050
2024-02-01 00:24:59,393 Validation result at epoch 1471, step    50000: duration: 24.4484s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 350.57877	Translation Loss: 73465.01562	PPL: 1558.76306
	Eval Metric: BLEU
	WER 535.59	(DEL: 4.17,	INS: 446.68,	SUB: 84.75)
	BLEU-4 0.00	(BLEU-1: 0.01,	BLEU-2: 0.00,	BLEU-3: 0.00,	BLEU-4: 0.00)
	CHRF 3.53	ROUGE 0.02
2024-02-01 00:24:59,395 Logging Recognition and Translation Outputs
2024-02-01 00:24:59,395 ========================================================================================================================
2024-02-01 00:24:59,395 Logging Sequence: 59_58.00
2024-02-01 00:24:59,395 	Gloss Reference :	***** ***** ***** ***** * ***** * ***** A B+C+D+E
2024-02-01 00:24:59,396 	Gloss Hypothesis:	<pad> <unk> <pad> <unk> E <unk> E <unk> E <unk>  
2024-02-01 00:24:59,396 	Gloss Alignment :	I     I     I     I     I I     I I     S S      
2024-02-01 00:24:59,396 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 00:24:59,399 	Text Reference  :	************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* to            fix           the           damage        they          did           not           have          a             lot           of            time         
2024-02-01 00:24:59,399 	Text Hypothesis :	misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand
2024-02-01 00:24:59,399 	Text Alignment  :	I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             S             S             S             S             S             S             S             S             S             S             S             S            
2024-02-01 00:24:59,399 ========================================================================================================================
2024-02-01 00:24:59,399 Logging Sequence: 165_2.00
2024-02-01 00:24:59,399 	Gloss Reference :	A B+C+D+E        
2024-02-01 00:24:59,400 	Gloss Hypothesis:	E C+E+C+E+C+E+C+E
2024-02-01 00:24:59,400 	Gloss Alignment :	S S              
2024-02-01 00:24:59,400 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 00:24:59,403 	Text Reference  :	*** *** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** many  people believe in    superstitions and   think it    brings good  luck  and   bad   luck 
2024-02-01 00:24:59,403 	Text Hypothesis :	<s> <s> sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha  sabha   sabha sabha         sabha sabha sabha sabha  sabha sabha sabha sabha sabha
2024-02-01 00:24:59,403 	Text Alignment  :	I   I   I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     S     S      S       S     S             S     S     S     S      S     S     S     S     S    
2024-02-01 00:24:59,403 ========================================================================================================================
2024-02-01 00:24:59,403 Logging Sequence: 58_147.00
2024-02-01 00:24:59,404 	Gloss Reference :	***** * ***** * ***** A B+C+D+E
2024-02-01 00:24:59,404 	Gloss Hypothesis:	<unk> E <unk> B <unk> B <unk>  
2024-02-01 00:24:59,404 	Gloss Alignment :	I     I I     I I     S S      
2024-02-01 00:24:59,404 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 00:24:59,408 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** the women's cricket team grabbed gold by  beating sri lanka in  the finals what a   historic win
2024-02-01 00:24:59,408 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s>     <s>     <s>  <s>     <s>  <s> <s>     <s> <s>   <s> <s> <s>    <s>  <s> <s>      <s>
2024-02-01 00:24:59,408 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   S   S       S       S    S       S    S   S       S   S     S   S   S      S    S   S        S  
2024-02-01 00:24:59,408 ========================================================================================================================
2024-02-01 00:24:59,408 Logging Sequence: 81_139.00
2024-02-01 00:24:59,408 	Gloss Reference :	***** * ***** *********************************** ***** ***************** A     B+C+D+E        
2024-02-01 00:24:59,408 	Gloss Hypothesis:	<unk> D <unk> D+E+D+E+D+E+D+B+D+B+D+B+C+E+C+D+E+D <unk> E+B+E+C+E+D+E+D+C <unk> C+E+C+E+C+E+C+E
2024-02-01 00:24:59,409 	Gloss Alignment :	I     I I     I                                   I     I                 S     S              
2024-02-01 00:24:59,409 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 00:24:59,413 	Text Reference  :	*** *** *** ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* in            2017          the           case          was           filed         first         in            delhi         high          court         by            rhiti         sports        management    on            behalf        of            dhoni        
2024-02-01 00:24:59,413 	Text Hypothesis :	<s> <s> <s> misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand
2024-02-01 00:24:59,413 	Text Alignment  :	I   I   I   I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             S             S             S             S             S             S             S             S             S             S             S             S             S             S             S             S             S             S             S            
2024-02-01 00:24:59,413 ========================================================================================================================
2024-02-01 00:24:59,413 Logging Sequence: 125_72.00
2024-02-01 00:24:59,414 	Gloss Reference :	***** ***** A ***** * ***** * ***** * ***** ***** ***** * ***** *** ***** * ***** ***** ******* ***** * ***** * ***** * ***** ***************** ***** ***** B+C+D+E
2024-02-01 00:24:59,414 	Gloss Hypothesis:	<unk> <pad> A <pad> E <unk> B <unk> C <unk> E+B+E <pad> E <pad> E+B <unk> E <pad> <unk> C+E+B+E <pad> E <unk> E <unk> E <unk> B+E+B+E+B+C+E+C+B <unk> <pad> C+E    
2024-02-01 00:24:59,414 	Gloss Alignment :	I     I       I     I I     I I     I I     I     I     I I     I   I     I I     I     I       I     I I     I I     I I     I                 I     I     S      
2024-02-01 00:24:59,415 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 00:24:59,418 	Text Reference  :	*** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** some  said  the   pakistani javelineer had   milicious intentions of    tampering with  the   javelin out   of    jealousy
2024-02-01 00:24:59,418 	Text Hypothesis :	<s> sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha     sabha      sabha sabha     sabha      sabha sabha     sabha sabha sabha   sabha sabha sabha   
2024-02-01 00:24:59,418 	Text Alignment  :	I   I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     S     S     S     S         S          S     S         S          S     S         S     S     S       S     S     S       
2024-02-01 00:24:59,418 ========================================================================================================================
2024-02-01 00:25:08,043 Epoch 1471: Total Training Recognition Loss 1102.10  Total Training Translation Loss 3486.56 
2024-02-01 00:25:08,043 EPOCH 1472
2024-02-01 00:25:25,498 Epoch 1472: Total Training Recognition Loss 1100.92  Total Training Translation Loss 3487.03 
2024-02-01 00:25:25,499 EPOCH 1473
2024-02-01 00:25:42,975 Epoch 1473: Total Training Recognition Loss 1101.48  Total Training Translation Loss 3488.27 
2024-02-01 00:25:42,975 EPOCH 1474
2024-02-01 00:25:51,304 [Epoch: 1474 Step: 00050100] Batch Recognition Loss:  22.412912 => Gls Tokens per Sec:      692 || Batch Translation Loss:  83.925995 => Txt Tokens per Sec:     1872 || Lr: 0.000050
2024-02-01 00:26:00,530 Epoch 1474: Total Training Recognition Loss 1104.12  Total Training Translation Loss 3487.09 
2024-02-01 00:26:00,530 EPOCH 1475
2024-02-01 00:26:18,030 Epoch 1475: Total Training Recognition Loss 1099.88  Total Training Translation Loss 3486.89 
2024-02-01 00:26:18,030 EPOCH 1476
2024-02-01 00:26:35,498 Epoch 1476: Total Training Recognition Loss 1102.68  Total Training Translation Loss 3487.61 
2024-02-01 00:26:35,498 EPOCH 1477
2024-02-01 00:26:45,341 [Epoch: 1477 Step: 00050200] Batch Recognition Loss:  15.139676 => Gls Tokens per Sec:      495 || Batch Translation Loss:  76.263016 => Txt Tokens per Sec:     1461 || Lr: 0.000050
2024-02-01 00:26:53,132 Epoch 1477: Total Training Recognition Loss 1100.52  Total Training Translation Loss 3486.11 
2024-02-01 00:26:53,132 EPOCH 1478
2024-02-01 00:27:10,726 Epoch 1478: Total Training Recognition Loss 1101.00  Total Training Translation Loss 3487.24 
2024-02-01 00:27:10,727 EPOCH 1479
2024-02-01 00:27:28,369 Epoch 1479: Total Training Recognition Loss 1100.12  Total Training Translation Loss 3488.21 
2024-02-01 00:27:28,369 EPOCH 1480
2024-02-01 00:27:36,556 [Epoch: 1480 Step: 00050300] Batch Recognition Loss:  27.100498 => Gls Tokens per Sec:      517 || Batch Translation Loss:  86.538727 => Txt Tokens per Sec:     1518 || Lr: 0.000050
2024-02-01 00:27:45,913 Epoch 1480: Total Training Recognition Loss 1100.07  Total Training Translation Loss 3486.29 
2024-02-01 00:27:45,914 EPOCH 1481
2024-02-01 00:28:03,648 Epoch 1481: Total Training Recognition Loss 1100.50  Total Training Translation Loss 3487.40 
2024-02-01 00:28:03,648 EPOCH 1482
2024-02-01 00:28:21,348 Epoch 1482: Total Training Recognition Loss 1099.13  Total Training Translation Loss 3487.74 
2024-02-01 00:28:21,348 EPOCH 1483
2024-02-01 00:28:26,376 [Epoch: 1483 Step: 00050400] Batch Recognition Loss:  15.528498 => Gls Tokens per Sec:      764 || Batch Translation Loss:  75.327690 => Txt Tokens per Sec:     2050 || Lr: 0.000050
2024-02-01 00:28:38,858 Epoch 1483: Total Training Recognition Loss 1101.71  Total Training Translation Loss 3486.90 
2024-02-01 00:28:38,859 EPOCH 1484
2024-02-01 00:28:56,340 Epoch 1484: Total Training Recognition Loss 1100.61  Total Training Translation Loss 3485.77 
2024-02-01 00:28:56,341 EPOCH 1485
2024-02-01 00:29:13,882 Epoch 1485: Total Training Recognition Loss 1100.50  Total Training Translation Loss 3487.26 
2024-02-01 00:29:13,882 EPOCH 1486
2024-02-01 00:29:18,822 [Epoch: 1486 Step: 00050500] Batch Recognition Loss:  23.373539 => Gls Tokens per Sec:      648 || Batch Translation Loss:  97.006172 => Txt Tokens per Sec:     1848 || Lr: 0.000050
2024-02-01 00:29:31,465 Epoch 1486: Total Training Recognition Loss 1100.58  Total Training Translation Loss 3487.91 
2024-02-01 00:29:31,466 EPOCH 1487
2024-02-01 00:29:49,185 Epoch 1487: Total Training Recognition Loss 1099.32  Total Training Translation Loss 3486.87 
2024-02-01 00:29:49,185 EPOCH 1488
2024-02-01 00:30:06,938 Epoch 1488: Total Training Recognition Loss 1101.40  Total Training Translation Loss 3487.50 
2024-02-01 00:30:06,938 EPOCH 1489
2024-02-01 00:30:10,933 [Epoch: 1489 Step: 00050600] Batch Recognition Loss:  37.549171 => Gls Tokens per Sec:      641 || Batch Translation Loss: 101.823898 => Txt Tokens per Sec:     1897 || Lr: 0.000050
2024-02-01 00:30:24,504 Epoch 1489: Total Training Recognition Loss 1099.95  Total Training Translation Loss 3487.48 
2024-02-01 00:30:24,504 EPOCH 1490
2024-02-01 00:30:42,182 Epoch 1490: Total Training Recognition Loss 1099.94  Total Training Translation Loss 3487.75 
2024-02-01 00:30:42,182 EPOCH 1491
2024-02-01 00:30:59,801 Epoch 1491: Total Training Recognition Loss 1100.52  Total Training Translation Loss 3486.44 
2024-02-01 00:30:59,801 EPOCH 1492
2024-02-01 00:31:03,729 [Epoch: 1492 Step: 00050700] Batch Recognition Loss:  51.332687 => Gls Tokens per Sec:      489 || Batch Translation Loss: 128.795242 => Txt Tokens per Sec:     1540 || Lr: 0.000050
2024-02-01 00:31:17,370 Epoch 1492: Total Training Recognition Loss 1100.39  Total Training Translation Loss 3487.30 
2024-02-01 00:31:17,370 EPOCH 1493
2024-02-01 00:31:34,859 Epoch 1493: Total Training Recognition Loss 1100.77  Total Training Translation Loss 3487.11 
2024-02-01 00:31:34,859 EPOCH 1494
2024-02-01 00:31:52,345 Epoch 1494: Total Training Recognition Loss 1099.13  Total Training Translation Loss 3487.81 
2024-02-01 00:31:52,346 EPOCH 1495
2024-02-01 00:31:54,721 [Epoch: 1495 Step: 00050800] Batch Recognition Loss:  40.020576 => Gls Tokens per Sec:      539 || Batch Translation Loss: 112.545387 => Txt Tokens per Sec:     1668 || Lr: 0.000050
2024-02-01 00:32:09,991 Epoch 1495: Total Training Recognition Loss 1100.83  Total Training Translation Loss 3486.69 
2024-02-01 00:32:09,992 EPOCH 1496
2024-02-01 00:32:27,516 Epoch 1496: Total Training Recognition Loss 1098.90  Total Training Translation Loss 3487.16 
2024-02-01 00:32:27,516 EPOCH 1497
2024-02-01 00:32:45,064 Epoch 1497: Total Training Recognition Loss 1100.47  Total Training Translation Loss 3486.68 
2024-02-01 00:32:45,064 EPOCH 1498
2024-02-01 00:32:45,475 [Epoch: 1498 Step: 00050900] Batch Recognition Loss:  10.918974 => Gls Tokens per Sec:     1561 || Batch Translation Loss:  62.809807 => Txt Tokens per Sec:     2851 || Lr: 0.000050
2024-02-01 00:33:02,738 Epoch 1498: Total Training Recognition Loss 1100.42  Total Training Translation Loss 3486.46 
2024-02-01 00:33:02,739 EPOCH 1499
2024-02-01 00:33:20,379 Epoch 1499: Total Training Recognition Loss 1103.21  Total Training Translation Loss 3487.32 
2024-02-01 00:33:20,379 EPOCH 1500
2024-02-01 00:33:37,988 [Epoch: 1500 Step: 00051000] Batch Recognition Loss:  36.549942 => Gls Tokens per Sec:      604 || Batch Translation Loss: 103.247246 => Txt Tokens per Sec:     1676 || Lr: 0.000050
2024-02-01 00:33:37,988 Epoch 1500: Total Training Recognition Loss 1099.07  Total Training Translation Loss 3487.63 
2024-02-01 00:33:37,988 EPOCH 1501
2024-02-01 00:33:55,529 Epoch 1501: Total Training Recognition Loss 1099.16  Total Training Translation Loss 3487.78 
2024-02-01 00:33:55,529 EPOCH 1502
2024-02-01 00:34:13,246 Epoch 1502: Total Training Recognition Loss 1100.32  Total Training Translation Loss 3487.17 
2024-02-01 00:34:13,247 EPOCH 1503
2024-02-01 00:34:30,278 [Epoch: 1503 Step: 00051100] Batch Recognition Loss:  37.206566 => Gls Tokens per Sec:      587 || Batch Translation Loss: 124.082726 => Txt Tokens per Sec:     1648 || Lr: 0.000050
2024-02-01 00:34:30,786 Epoch 1503: Total Training Recognition Loss 1099.93  Total Training Translation Loss 3486.78 
2024-02-01 00:34:30,787 EPOCH 1504
2024-02-01 00:34:48,396 Epoch 1504: Total Training Recognition Loss 1099.90  Total Training Translation Loss 3486.02 
2024-02-01 00:34:48,396 EPOCH 1505
2024-02-01 00:35:06,261 Epoch 1505: Total Training Recognition Loss 1098.84  Total Training Translation Loss 3487.43 
2024-02-01 00:35:06,261 EPOCH 1506
2024-02-01 00:35:22,517 [Epoch: 1506 Step: 00051200] Batch Recognition Loss:  20.852486 => Gls Tokens per Sec:      575 || Batch Translation Loss:  88.832947 => Txt Tokens per Sec:     1628 || Lr: 0.000050
2024-02-01 00:35:23,940 Epoch 1506: Total Training Recognition Loss 1100.95  Total Training Translation Loss 3487.23 
2024-02-01 00:35:23,941 EPOCH 1507
2024-02-01 00:35:41,648 Epoch 1507: Total Training Recognition Loss 1100.67  Total Training Translation Loss 3487.42 
2024-02-01 00:35:41,648 EPOCH 1508
2024-02-01 00:35:59,228 Epoch 1508: Total Training Recognition Loss 1100.88  Total Training Translation Loss 3487.10 
2024-02-01 00:35:59,229 EPOCH 1509
2024-02-01 00:36:14,256 [Epoch: 1509 Step: 00051300] Batch Recognition Loss:  45.752205 => Gls Tokens per Sec:      580 || Batch Translation Loss: 127.121094 => Txt Tokens per Sec:     1617 || Lr: 0.000050
2024-02-01 00:36:16,810 Epoch 1509: Total Training Recognition Loss 1099.78  Total Training Translation Loss 3487.79 
2024-02-01 00:36:16,810 EPOCH 1510
2024-02-01 00:36:34,602 Epoch 1510: Total Training Recognition Loss 1100.06  Total Training Translation Loss 3487.32 
2024-02-01 00:36:34,602 EPOCH 1511
2024-02-01 00:36:52,303 Epoch 1511: Total Training Recognition Loss 1099.57  Total Training Translation Loss 3487.12 
2024-02-01 00:36:52,303 EPOCH 1512
2024-02-01 00:37:06,184 [Epoch: 1512 Step: 00051400] Batch Recognition Loss:  72.422256 => Gls Tokens per Sec:      581 || Batch Translation Loss: 126.509056 => Txt Tokens per Sec:     1589 || Lr: 0.000050
2024-02-01 00:37:10,061 Epoch 1512: Total Training Recognition Loss 1101.61  Total Training Translation Loss 3487.19 
2024-02-01 00:37:10,061 EPOCH 1513
2024-02-01 00:37:27,684 Epoch 1513: Total Training Recognition Loss 1100.15  Total Training Translation Loss 3487.27 
2024-02-01 00:37:27,684 EPOCH 1514
2024-02-01 00:37:45,226 Epoch 1514: Total Training Recognition Loss 1102.86  Total Training Translation Loss 3488.10 
2024-02-01 00:37:45,226 EPOCH 1515
2024-02-01 00:37:58,510 [Epoch: 1515 Step: 00051500] Batch Recognition Loss:  36.712780 => Gls Tokens per Sec:      559 || Batch Translation Loss: 102.793358 => Txt Tokens per Sec:     1562 || Lr: 0.000050
2024-02-01 00:38:02,960 Epoch 1515: Total Training Recognition Loss 1100.66  Total Training Translation Loss 3488.03 
2024-02-01 00:38:02,960 EPOCH 1516
2024-02-01 00:38:20,577 Epoch 1516: Total Training Recognition Loss 1098.42  Total Training Translation Loss 3486.79 
2024-02-01 00:38:20,578 EPOCH 1517
2024-02-01 00:38:38,306 Epoch 1517: Total Training Recognition Loss 1098.21  Total Training Translation Loss 3487.16 
2024-02-01 00:38:38,306 EPOCH 1518
2024-02-01 00:38:49,083 [Epoch: 1518 Step: 00051600] Batch Recognition Loss:  35.125462 => Gls Tokens per Sec:      630 || Batch Translation Loss: 108.663582 => Txt Tokens per Sec:     1699 || Lr: 0.000050
2024-02-01 00:38:55,856 Epoch 1518: Total Training Recognition Loss 1099.43  Total Training Translation Loss 3487.84 
2024-02-01 00:38:55,856 EPOCH 1519
2024-02-01 00:39:13,451 Epoch 1519: Total Training Recognition Loss 1100.61  Total Training Translation Loss 3487.43 
2024-02-01 00:39:13,452 EPOCH 1520
2024-02-01 00:39:31,217 Epoch 1520: Total Training Recognition Loss 1101.40  Total Training Translation Loss 3487.53 
2024-02-01 00:39:31,218 EPOCH 1521
2024-02-01 00:39:39,987 [Epoch: 1521 Step: 00051700] Batch Recognition Loss:  46.144123 => Gls Tokens per Sec:      730 || Batch Translation Loss: 124.650131 => Txt Tokens per Sec:     1945 || Lr: 0.000050
2024-02-01 00:39:48,856 Epoch 1521: Total Training Recognition Loss 1099.60  Total Training Translation Loss 3487.28 
2024-02-01 00:39:48,857 EPOCH 1522
2024-02-01 00:40:06,578 Epoch 1522: Total Training Recognition Loss 1100.30  Total Training Translation Loss 3486.35 
2024-02-01 00:40:06,578 EPOCH 1523
2024-02-01 00:40:24,154 Epoch 1523: Total Training Recognition Loss 1099.82  Total Training Translation Loss 3488.06 
2024-02-01 00:40:24,154 EPOCH 1524
2024-02-01 00:40:32,768 [Epoch: 1524 Step: 00051800] Batch Recognition Loss:  40.108036 => Gls Tokens per Sec:      669 || Batch Translation Loss: 114.630295 => Txt Tokens per Sec:     1813 || Lr: 0.000050
2024-02-01 00:40:41,804 Epoch 1524: Total Training Recognition Loss 1101.40  Total Training Translation Loss 3487.16 
2024-02-01 00:40:41,804 EPOCH 1525
2024-02-01 00:40:59,207 Epoch 1525: Total Training Recognition Loss 1097.71  Total Training Translation Loss 3487.26 
2024-02-01 00:40:59,207 EPOCH 1526
2024-02-01 00:41:16,661 Epoch 1526: Total Training Recognition Loss 1099.32  Total Training Translation Loss 3487.61 
2024-02-01 00:41:16,661 EPOCH 1527
2024-02-01 00:41:24,349 [Epoch: 1527 Step: 00051900] Batch Recognition Loss:  32.923069 => Gls Tokens per Sec:      666 || Batch Translation Loss:  94.657959 => Txt Tokens per Sec:     1829 || Lr: 0.000050
2024-02-01 00:41:34,260 Epoch 1527: Total Training Recognition Loss 1101.91  Total Training Translation Loss 3486.94 
2024-02-01 00:41:34,260 EPOCH 1528
2024-02-01 00:41:51,919 Epoch 1528: Total Training Recognition Loss 1099.64  Total Training Translation Loss 3486.44 
2024-02-01 00:41:51,919 EPOCH 1529
2024-02-01 00:42:09,264 Epoch 1529: Total Training Recognition Loss 1100.85  Total Training Translation Loss 3487.97 
2024-02-01 00:42:09,265 EPOCH 1530
2024-02-01 00:42:16,737 [Epoch: 1530 Step: 00052000] Batch Recognition Loss:  73.987961 => Gls Tokens per Sec:      566 || Batch Translation Loss: 126.463562 => Txt Tokens per Sec:     1670 || Lr: 0.000050
2024-02-01 00:42:40,787 Validation result at epoch 1530, step    52000: duration: 24.0499s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 350.68506	Translation Loss: 73458.72656	PPL: 1557.78247
	Eval Metric: BLEU
	WER 533.55	(DEL: 4.17,	INS: 444.35,	SUB: 85.03)
	BLEU-4 0.00	(BLEU-1: 0.01,	BLEU-2: 0.00,	BLEU-3: 0.00,	BLEU-4: 0.00)
	CHRF 3.48	ROUGE 0.01
2024-02-01 00:42:40,788 Logging Recognition and Translation Outputs
2024-02-01 00:42:40,788 ========================================================================================================================
2024-02-01 00:42:40,789 Logging Sequence: 87_229.00
2024-02-01 00:42:40,789 	Gloss Reference :	***** * ***** * ***** A B+C+D+E
2024-02-01 00:42:40,789 	Gloss Hypothesis:	<unk> E <unk> E <unk> E <unk>  
2024-02-01 00:42:40,789 	Gloss Alignment :	I     I I     I I     S S      
2024-02-01 00:42:40,789 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 00:42:40,791 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** it  was not against dhoni or  kohli
2024-02-01 00:42:40,791 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s>     <s>   <s> <s>  
2024-02-01 00:42:40,792 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   S   S   S   S       S     S   S    
2024-02-01 00:42:40,792 ========================================================================================================================
2024-02-01 00:42:40,792 Logging Sequence: 134_153.00
2024-02-01 00:42:40,792 	Gloss Reference :	A B+C+D+E                                                                        
2024-02-01 00:42:40,792 	Gloss Hypothesis:	C B+C+B+C+E+C+E+B+C+B+C+B+E+C+B+C+B+C+B+E+C+B+C+B+C+B+C+B+C+B+C+B+C+B+C+B+E+C+B+C
2024-02-01 00:42:40,792 	Gloss Alignment :	S S                                                                              
2024-02-01 00:42:40,792 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 00:42:40,797 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** pm  modi in  his interaction said that deaf athletes must fight for their goals and never give up  despite the losses
2024-02-01 00:42:40,797 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s>  <s> <s> <s>         <s>  <s>  <s>  <s>      <s>  <s>   <s> <s>   <s>   <s> <s>   <s>  <s> <s>     <s> <s>   
2024-02-01 00:42:40,797 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   S   S    S   S   S           S    S    S    S        S    S     S   S     S     S   S     S    S   S       S   S     
2024-02-01 00:42:40,797 ========================================================================================================================
2024-02-01 00:42:40,797 Logging Sequence: 137_155.00
2024-02-01 00:42:40,798 	Gloss Reference :	* ***** ***** ******* ***** * ***** ******* ***** ***** ***** ***** ***** ***** * ***** ******* ***** ************* A     B+C+D+E          
2024-02-01 00:42:40,798 	Gloss Hypothesis:	E <unk> <pad> A+E+A+E <unk> E <unk> E+C+E+C <unk> C+A+E <pad> E+A+E <unk> <pad> E <unk> A+E+C+E <pad> A+C+E+C+E+C+A <unk> C+E+A+E+A+E+C+A+E
2024-02-01 00:42:40,798 	Gloss Alignment :	I I     I     I       I     I I     I       I     I     I     I     I     I     I I     I       I     I             S     S                
2024-02-01 00:42:40,798 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 00:42:40,801 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** an  extremely high tax named as  sin tax will be  applied
2024-02-01 00:42:40,801 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s>       <s>  <s> <s>   <s> <s> <s> <s>  <s> <s>    
2024-02-01 00:42:40,801 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   S   S         S    S   S     S   S   S   S    S   S      
2024-02-01 00:42:40,801 ========================================================================================================================
2024-02-01 00:42:40,801 Logging Sequence: 59_18.00
2024-02-01 00:42:40,802 	Gloss Reference :	* ***** ************************* ***** ************* A     B+C+D+E                                                  
2024-02-01 00:42:40,802 	Gloss Hypothesis:	E <pad> D+E+D+E+D+E+C+E+C+E+C+E+D <unk> E+C+E+C+E+C+E <unk> E+C+E+C+E+C+E+C+E+C+E+C+E+C+E+C+E+C+E+C+E+C+E+C+E+C+E+C+E
2024-02-01 00:42:40,802 	Gloss Alignment :	I I     I                         I     I             S     S                                                        
2024-02-01 00:42:40,802 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 00:42:40,805 	Text Reference  :	*** ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* ************* 27-year-old   jessica       fox           from          australia     won           a             bronze        a             gold          medal         in            canoeing     
2024-02-01 00:42:40,805 	Text Hypothesis :	<s> misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand misunderstand
2024-02-01 00:42:40,805 	Text Alignment  :	I   I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             I             S             S             S             S             S             S             S             S             S             S             S             S             S            
2024-02-01 00:42:40,805 ========================================================================================================================
2024-02-01 00:42:40,805 Logging Sequence: 173_103.00
2024-02-01 00:42:40,806 	Gloss Reference :	***** * ***** A ***** ***** B+C+D+E
2024-02-01 00:42:40,806 	Gloss Hypothesis:	<unk> A <unk> A <unk> <pad> <unk>  
2024-02-01 00:42:40,806 	Gloss Alignment :	I     I I       I     I     S      
2024-02-01 00:42:40,806 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 00:42:40,807 	Text Reference  :	*** *** *** *** *** *** *** *** *** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** ******** these    rumours  are      absolutely rubbish 
2024-02-01 00:42:40,808 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas casillas   casillas
2024-02-01 00:42:40,808 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I        I        I        I        I        I        I        I        I        I        I        I        I        I        I        I        I        I        I        I        I        I        I        I        I        I        S        S        S        S          S       
2024-02-01 00:42:40,808 ========================================================================================================================
2024-02-01 00:42:50,888 Epoch 1530: Total Training Recognition Loss 1100.42  Total Training Translation Loss 3487.88 
2024-02-01 00:42:50,888 EPOCH 1531
2024-02-01 00:43:08,346 Epoch 1531: Total Training Recognition Loss 1101.20  Total Training Translation Loss 3486.87 
2024-02-01 00:43:08,347 EPOCH 1532
2024-02-01 00:43:25,957 Epoch 1532: Total Training Recognition Loss 1099.66  Total Training Translation Loss 3486.65 
2024-02-01 00:43:25,957 EPOCH 1533
2024-02-01 00:43:30,335 [Epoch: 1533 Step: 00052100] Batch Recognition Loss:  15.240782 => Gls Tokens per Sec:      878 || Batch Translation Loss:  78.092476 => Txt Tokens per Sec:     2242 || Lr: 0.000050
2024-02-01 00:43:43,583 Epoch 1533: Total Training Recognition Loss 1102.67  Total Training Translation Loss 3487.23 
2024-02-01 00:43:43,583 EPOCH 1534
2024-02-01 00:44:01,168 Epoch 1534: Total Training Recognition Loss 1101.52  Total Training Translation Loss 3487.15 
2024-02-01 00:44:01,168 EPOCH 1535
2024-02-01 00:44:18,789 Epoch 1535: Total Training Recognition Loss 1100.32  Total Training Translation Loss 3487.14 
2024-02-01 00:44:18,789 EPOCH 1536
2024-02-01 00:44:23,971 [Epoch: 1536 Step: 00052200] Batch Recognition Loss:   6.209698 => Gls Tokens per Sec:      569 || Batch Translation Loss:  50.759949 => Txt Tokens per Sec:     1495 || Lr: 0.000050
2024-02-01 00:44:36,455 Epoch 1536: Total Training Recognition Loss 1100.20  Total Training Translation Loss 3487.53 
2024-02-01 00:44:36,455 EPOCH 1537
2024-02-01 00:44:54,119 Epoch 1537: Total Training Recognition Loss 1099.34  Total Training Translation Loss 3486.40 
2024-02-01 00:44:54,120 EPOCH 1538
2024-02-01 00:45:11,767 Epoch 1538: Total Training Recognition Loss 1101.24  Total Training Translation Loss 3487.22 
2024-02-01 00:45:11,767 EPOCH 1539
2024-02-01 00:45:16,189 [Epoch: 1539 Step: 00052300] Batch Recognition Loss:  27.140430 => Gls Tokens per Sec:      523 || Batch Translation Loss:  87.872833 => Txt Tokens per Sec:     1486 || Lr: 0.000050
2024-02-01 00:45:29,411 Epoch 1539: Total Training Recognition Loss 1100.77  Total Training Translation Loss 3486.98 
2024-02-01 00:45:29,411 EPOCH 1540
2024-02-01 00:45:47,065 Epoch 1540: Total Training Recognition Loss 1100.37  Total Training Translation Loss 3487.61 
2024-02-01 00:45:47,066 EPOCH 1541
2024-02-01 00:46:04,577 Epoch 1541: Total Training Recognition Loss 1101.05  Total Training Translation Loss 3488.72 
2024-02-01 00:46:04,577 EPOCH 1542
2024-02-01 00:46:07,652 [Epoch: 1542 Step: 00052400] Batch Recognition Loss:  37.578087 => Gls Tokens per Sec:      625 || Batch Translation Loss: 115.838318 => Txt Tokens per Sec:     1860 || Lr: 0.000050
2024-02-01 00:46:22,182 Epoch 1542: Total Training Recognition Loss 1101.53  Total Training Translation Loss 3487.97 
2024-02-01 00:46:22,182 EPOCH 1543
2024-02-01 00:46:39,713 Epoch 1543: Total Training Recognition Loss 1100.74  Total Training Translation Loss 3488.20 
2024-02-01 00:46:39,713 EPOCH 1544
2024-02-01 00:46:57,276 Epoch 1544: Total Training Recognition Loss 1099.48  Total Training Translation Loss 3486.94 
2024-02-01 00:46:57,277 EPOCH 1545
2024-02-01 00:46:59,406 [Epoch: 1545 Step: 00052500] Batch Recognition Loss:  45.453773 => Gls Tokens per Sec:      602 || Batch Translation Loss: 125.442635 => Txt Tokens per Sec:     1656 || Lr: 0.000050
2024-02-01 00:47:15,011 Epoch 1545: Total Training Recognition Loss 1098.18  Total Training Translation Loss 3487.51 
2024-02-01 00:47:15,011 EPOCH 1546
2024-02-01 00:47:32,636 Epoch 1546: Total Training Recognition Loss 1100.94  Total Training Translation Loss 3487.84 
2024-02-01 00:47:32,637 EPOCH 1547
2024-02-01 00:47:50,189 Epoch 1547: Total Training Recognition Loss 1099.17  Total Training Translation Loss 3487.52 
2024-02-01 00:47:50,189 EPOCH 1548
2024-02-01 00:47:51,262 [Epoch: 1548 Step: 00052600] Batch Recognition Loss:  38.780338 => Gls Tokens per Sec:      597 || Batch Translation Loss: 123.339943 => Txt Tokens per Sec:     1827 || Lr: 0.000050
2024-02-01 00:48:07,802 Epoch 1548: Total Training Recognition Loss 1101.28  Total Training Translation Loss 3487.64 
2024-02-01 00:48:07,803 EPOCH 1549
2024-02-01 00:48:25,477 Epoch 1549: Total Training Recognition Loss 1100.69  Total Training Translation Loss 3486.99 
2024-02-01 00:48:25,477 EPOCH 1550
2024-02-01 00:48:43,014 [Epoch: 1550 Step: 00052700] Batch Recognition Loss:  37.378437 => Gls Tokens per Sec:      606 || Batch Translation Loss: 116.986557 => Txt Tokens per Sec:     1683 || Lr: 0.000050
2024-02-01 00:48:43,015 Epoch 1550: Total Training Recognition Loss 1099.84  Total Training Translation Loss 3487.09 
2024-02-01 00:48:43,015 EPOCH 1551
2024-02-01 00:49:00,708 Epoch 1551: Total Training Recognition Loss 1100.79  Total Training Translation Loss 3487.52 
2024-02-01 00:49:00,708 EPOCH 1552
2024-02-01 00:49:18,407 Epoch 1552: Total Training Recognition Loss 1100.60  Total Training Translation Loss 3488.46 
2024-02-01 00:49:18,408 EPOCH 1553
2024-02-01 00:49:34,985 [Epoch: 1553 Step: 00052800] Batch Recognition Loss:  11.009750 => Gls Tokens per Sec:      603 || Batch Translation Loss:  62.639671 => Txt Tokens per Sec:     1673 || Lr: 0.000050
2024-02-01 00:49:36,055 Epoch 1553: Total Training Recognition Loss 1102.72  Total Training Translation Loss 3486.83 
2024-02-01 00:49:36,055 EPOCH 1554
2024-02-01 00:49:53,567 Epoch 1554: Total Training Recognition Loss 1100.87  Total Training Translation Loss 3487.77 
2024-02-01 00:49:53,567 EPOCH 1555
2024-02-01 00:50:11,291 Epoch 1555: Total Training Recognition Loss 1100.13  Total Training Translation Loss 3488.49 
2024-02-01 00:50:11,291 EPOCH 1556
2024-02-01 00:50:27,255 [Epoch: 1556 Step: 00052900] Batch Recognition Loss:   6.093593 => Gls Tokens per Sec:      586 || Batch Translation Loss:  50.851540 => Txt Tokens per Sec:     1631 || Lr: 0.000050
2024-02-01 00:50:28,989 Epoch 1556: Total Training Recognition Loss 1099.32  Total Training Translation Loss 3487.76 
2024-02-01 00:50:28,990 EPOCH 1557
2024-02-01 00:50:46,439 Epoch 1557: Total Training Recognition Loss 1101.58  Total Training Translation Loss 3487.20 
2024-02-01 00:50:46,439 EPOCH 1558
2024-02-01 00:51:04,156 Epoch 1558: Total Training Recognition Loss 1098.92  Total Training Translation Loss 3487.77 
2024-02-01 00:51:04,156 EPOCH 1559
2024-02-01 00:51:18,758 [Epoch: 1559 Step: 00053000] Batch Recognition Loss:  36.728233 => Gls Tokens per Sec:      597 || Batch Translation Loss: 120.741821 => Txt Tokens per Sec:     1642 || Lr: 0.000050
2024-02-01 00:51:21,753 Epoch 1559: Total Training Recognition Loss 1101.13  Total Training Translation Loss 3486.84 
2024-02-01 00:51:21,754 EPOCH 1560
2024-02-01 00:51:39,361 Epoch 1560: Total Training Recognition Loss 1100.04  Total Training Translation Loss 3487.89 
2024-02-01 00:51:39,362 EPOCH 1561
2024-02-01 00:51:56,916 Epoch 1561: Total Training Recognition Loss 1100.37  Total Training Translation Loss 3487.40 
2024-02-01 00:51:56,916 EPOCH 1562
2024-02-01 00:52:11,230 [Epoch: 1562 Step: 00053100] Batch Recognition Loss:  36.684380 => Gls Tokens per Sec:      564 || Batch Translation Loss: 113.211426 => Txt Tokens per Sec:     1583 || Lr: 0.000050
2024-02-01 00:52:14,613 Epoch 1562: Total Training Recognition Loss 1101.54  Total Training Translation Loss 3486.82 
2024-02-01 00:52:14,613 EPOCH 1563
2024-02-01 00:52:32,389 Epoch 1563: Total Training Recognition Loss 1100.90  Total Training Translation Loss 3486.98 
2024-02-01 00:52:32,390 EPOCH 1564
2024-02-01 00:52:50,036 Epoch 1564: Total Training Recognition Loss 1101.29  Total Training Translation Loss 3488.15 
2024-02-01 00:52:50,036 EPOCH 1565
2024-02-01 00:53:02,577 [Epoch: 1565 Step: 00053200] Batch Recognition Loss:  37.472237 => Gls Tokens per Sec:      593 || Batch Translation Loss: 113.866707 => Txt Tokens per Sec:     1620 || Lr: 0.000050
2024-02-01 00:53:10,667 Epoch 1565: Total Training Recognition Loss 1101.47  Total Training Translation Loss 3486.99 
2024-02-01 00:53:10,667 EPOCH 1566
2024-02-01 00:53:29,027 Epoch 1566: Total Training Recognition Loss 1100.78  Total Training Translation Loss 3486.95 
2024-02-01 00:53:29,028 EPOCH 1567
2024-02-01 00:53:46,642 Epoch 1567: Total Training Recognition Loss 1100.57  Total Training Translation Loss 3487.81 
2024-02-01 00:53:46,642 EPOCH 1568
2024-02-01 00:53:57,155 [Epoch: 1568 Step: 00053300] Batch Recognition Loss:  33.777012 => Gls Tokens per Sec:      646 || Batch Translation Loss:  95.795395 => Txt Tokens per Sec:     1790 || Lr: 0.000050
2024-02-01 00:54:04,295 Epoch 1568: Total Training Recognition Loss 1101.23  Total Training Translation Loss 3487.64 
2024-02-01 00:54:04,295 EPOCH 1569
2024-02-01 00:54:21,895 Epoch 1569: Total Training Recognition Loss 1099.91  Total Training Translation Loss 3487.44 
2024-02-01 00:54:21,896 EPOCH 1570
2024-02-01 00:54:39,453 Epoch 1570: Total Training Recognition Loss 1101.69  Total Training Translation Loss 3487.55 
2024-02-01 00:54:39,453 EPOCH 1571
2024-02-01 00:54:49,488 [Epoch: 1571 Step: 00053400] Batch Recognition Loss:   6.354310 => Gls Tokens per Sec:      613 || Batch Translation Loss:  50.795921 => Txt Tokens per Sec:     1694 || Lr: 0.000050
2024-02-01 00:54:57,037 Epoch 1571: Total Training Recognition Loss 1101.52  Total Training Translation Loss 3486.82 
2024-02-01 00:54:57,037 EPOCH 1572
2024-02-01 00:55:14,537 Epoch 1572: Total Training Recognition Loss 1100.64  Total Training Translation Loss 3487.73 
2024-02-01 00:55:14,537 EPOCH 1573
2024-02-01 00:55:32,145 Epoch 1573: Total Training Recognition Loss 1100.63  Total Training Translation Loss 3487.52 
2024-02-01 00:55:32,146 EPOCH 1574
2024-02-01 00:55:39,994 [Epoch: 1574 Step: 00053500] Batch Recognition Loss:  40.430477 => Gls Tokens per Sec:      734 || Batch Translation Loss: 115.168503 => Txt Tokens per Sec:     1991 || Lr: 0.000050
2024-02-01 00:55:49,726 Epoch 1574: Total Training Recognition Loss 1100.17  Total Training Translation Loss 3488.05 
2024-02-01 00:55:49,726 EPOCH 1575
2024-02-01 00:56:07,107 Epoch 1575: Total Training Recognition Loss 1100.63  Total Training Translation Loss 3487.76 
2024-02-01 00:56:07,107 EPOCH 1576
2024-02-01 00:56:24,472 Epoch 1576: Total Training Recognition Loss 1101.95  Total Training Translation Loss 3488.11 
2024-02-01 00:56:24,472 EPOCH 1577
2024-02-01 00:56:33,771 [Epoch: 1577 Step: 00053600] Batch Recognition Loss:  34.990257 => Gls Tokens per Sec:      551 || Batch Translation Loss: 117.683830 => Txt Tokens per Sec:     1561 || Lr: 0.000050
2024-02-01 00:56:42,063 Epoch 1577: Total Training Recognition Loss 1103.48  Total Training Translation Loss 3487.26 
2024-02-01 00:56:42,064 EPOCH 1578
2024-02-01 00:56:59,697 Epoch 1578: Total Training Recognition Loss 1101.95  Total Training Translation Loss 3487.37 
2024-02-01 00:56:59,697 EPOCH 1579
2024-02-01 00:57:17,280 Epoch 1579: Total Training Recognition Loss 1099.72  Total Training Translation Loss 3487.40 
2024-02-01 00:57:17,280 EPOCH 1580
2024-02-01 00:57:25,497 [Epoch: 1580 Step: 00053700] Batch Recognition Loss:  31.409866 => Gls Tokens per Sec:      515 || Batch Translation Loss:  94.754936 => Txt Tokens per Sec:     1535 || Lr: 0.000050
2024-02-01 00:57:34,973 Epoch 1580: Total Training Recognition Loss 1102.61  Total Training Translation Loss 3487.27 
2024-02-01 00:57:34,973 EPOCH 1581
2024-02-01 00:57:52,616 Epoch 1581: Total Training Recognition Loss 1099.03  Total Training Translation Loss 3487.48 
2024-02-01 00:57:52,616 EPOCH 1582
2024-02-01 00:58:10,236 Epoch 1582: Total Training Recognition Loss 1101.92  Total Training Translation Loss 3486.68 
2024-02-01 00:58:10,237 EPOCH 1583
2024-02-01 00:58:15,619 [Epoch: 1583 Step: 00053800] Batch Recognition Loss:  45.925537 => Gls Tokens per Sec:      667 || Batch Translation Loss: 124.923706 => Txt Tokens per Sec:     1727 || Lr: 0.000050
2024-02-01 00:58:27,918 Epoch 1583: Total Training Recognition Loss 1102.21  Total Training Translation Loss 3486.84 
2024-02-01 00:58:27,919 EPOCH 1584
2024-02-01 00:58:45,358 Epoch 1584: Total Training Recognition Loss 1099.59  Total Training Translation Loss 3486.35 
2024-02-01 00:58:45,358 EPOCH 1585
2024-02-01 00:59:03,012 Epoch 1585: Total Training Recognition Loss 1101.61  Total Training Translation Loss 3486.99 
2024-02-01 00:59:03,012 EPOCH 1586
2024-02-01 00:59:08,136 [Epoch: 1586 Step: 00053900] Batch Recognition Loss:  42.366570 => Gls Tokens per Sec:      625 || Batch Translation Loss: 126.926491 => Txt Tokens per Sec:     1768 || Lr: 0.000050
2024-02-01 00:59:20,660 Epoch 1586: Total Training Recognition Loss 1099.54  Total Training Translation Loss 3486.36 
2024-02-01 00:59:20,660 EPOCH 1587
2024-02-01 00:59:38,384 Epoch 1587: Total Training Recognition Loss 1102.44  Total Training Translation Loss 3486.85 
2024-02-01 00:59:38,384 EPOCH 1588
2024-02-01 00:59:56,123 Epoch 1588: Total Training Recognition Loss 1100.11  Total Training Translation Loss 3488.57 
2024-02-01 00:59:56,123 EPOCH 1589
2024-02-01 01:00:00,325 [Epoch: 1589 Step: 00054000] Batch Recognition Loss:  13.108629 => Gls Tokens per Sec:      609 || Batch Translation Loss:  69.104187 => Txt Tokens per Sec:     1760 || Lr: 0.000050
2024-02-01 01:00:24,724 Validation result at epoch 1589, step    54000: duration: 24.3986s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 347.88422	Translation Loss: 73479.40625	PPL: 1561.00891
	Eval Metric: BLEU
	WER 527.19	(DEL: 4.52,	INS: 438.14,	SUB: 84.53)
	BLEU-4 0.00	(BLEU-1: 0.01,	BLEU-2: 0.00,	BLEU-3: 0.00,	BLEU-4: 0.00)
	CHRF 3.54	ROUGE 0.02
2024-02-01 01:00:24,725 Logging Recognition and Translation Outputs
2024-02-01 01:00:24,725 ========================================================================================================================
2024-02-01 01:00:24,725 Logging Sequence: 130_139.00
2024-02-01 01:00:24,726 	Gloss Reference :	***** ***** ******* ***** *** ***** * ***** ********************* ***** *** ***** * ***** ******************* ***** *************** A     B+C+D+E        
2024-02-01 01:00:24,726 	Gloss Hypothesis:	<unk> <pad> B+D+E+B <unk> E+B <unk> D <unk> E+B+E+B+E+D+E+D+C+B+D <unk> B+E <unk> E <unk> B+C+B+E+B+C+B+C+B+E <unk> C+E+B+E+C+E+C+E <unk> E+B+E+B+E+B+E+C
2024-02-01 01:00:24,726 	Gloss Alignment :	I     I     I       I     I   I     I I     I                     I     I   I     I I     I                   I     I               S     S              
2024-02-01 01:00:24,726 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 01:00:24,732 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** *** *** he  shared a   picture of  a   little pouch he  knit for his olympic gold medal with uk  flag on  one side and japanese flag on  the other
2024-02-01 01:00:24,732 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s>    <s> <s>     <s> <s> <s>    <s>   <s> <s>  <s> <s> <s>     <s>  <s>   <s>  <s> <s>  <s> <s> <s>  <s> <s>      <s>  <s> <s> <s>  
2024-02-01 01:00:24,732 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I   I   S   S      S   S       S   S   S      S     S   S    S   S   S       S    S     S    S   S    S   S   S    S   S        S    S   S   S    
2024-02-01 01:00:24,732 ========================================================================================================================
2024-02-01 01:00:24,732 Logging Sequence: 148_155.00
2024-02-01 01:00:24,733 	Gloss Reference :	* ******************* ***** *** A     B+C+D+E      
2024-02-01 01:00:24,733 	Gloss Hypothesis:	E A+E+B+E+C+B+E+C+E+C <unk> C+E <unk> E+C+E+C+E+A+E
2024-02-01 01:00:24,733 	Gloss Alignment :	I I                   I     I   S     S            
2024-02-01 01:00:24,733 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 01:00:24,736 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** india  won    the    match  with   263    balls  remaining and    without losing any    wicket
2024-02-01 01:00:24,736 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> chosen chosen chosen chosen chosen chosen chosen chosen chosen chosen chosen chosen chosen chosen chosen chosen chosen chosen chosen chosen chosen chosen chosen chosen chosen    chosen chosen  chosen chosen chosen
2024-02-01 01:00:24,736 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I      I      I      I      I      I      I      I      I      I      I      I      I      I      I      I      I      S      S      S      S      S      S      S      S         S      S       S      S      S     
2024-02-01 01:00:24,736 ========================================================================================================================
2024-02-01 01:00:24,736 Logging Sequence: 126_99.00
2024-02-01 01:00:24,737 	Gloss Reference :	A B+C+D+E
2024-02-01 01:00:24,737 	Gloss Hypothesis:	* <unk>  
2024-02-01 01:00:24,737 	Gloss Alignment :	D S      
2024-02-01 01:00:24,737 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 01:00:24,739 	Text Reference  :	*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** he  dedicated the medal to  sprinter milkha singh
2024-02-01 01:00:24,739 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s>       <s> <s>   <s> <s>      <s>    <s>  
2024-02-01 01:00:24,739 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   I   S   S         S   S     S   S        S      S    
2024-02-01 01:00:24,739 ========================================================================================================================
2024-02-01 01:00:24,740 Logging Sequence: 149_77.00
2024-02-01 01:00:24,740 	Gloss Reference :	***** * ***** * A     B+C+D+E
2024-02-01 01:00:24,740 	Gloss Hypothesis:	<unk> D <unk> C <unk> C      
2024-02-01 01:00:24,740 	Gloss Alignment :	I     I I     I S     S      
2024-02-01 01:00:24,740 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 01:00:24,744 	Text Reference  :	***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** and   arrested danushka for   alleged sexual assault of    a     29    year  old   woman whose name  has   not   been  disclosed
2024-02-01 01:00:24,744 	Text Hypothesis :	sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha    sabha    sabha sabha   sabha  sabha   sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha sabha    
2024-02-01 01:00:24,744 	Text Alignment  :	I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     S     S        S        S     S       S      S       S     S     S     S     S     S     S     S     S     S     S     S        
2024-02-01 01:00:24,744 ========================================================================================================================
2024-02-01 01:00:24,745 Logging Sequence: 168_15.00
2024-02-01 01:00:24,745 	Gloss Reference :	***** A ***** *** ***** * ***** ***** ***** *** B+C+D+E
2024-02-01 01:00:24,745 	Gloss Hypothesis:	<unk> A <unk> E+B <unk> E <pad> B+E+B <unk> B+E <unk>  
2024-02-01 01:00:24,745 	Gloss Alignment :	I       I     I   I     I I     I     I     I   S      
2024-02-01 01:00:24,745 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 01:00:24,748 	Text Reference  :	*** *** *** *** *** *** *** *** *** ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* ******* when    in      public  the     couple  are     always  approached for     photographys and     autographs
2024-02-01 01:00:24,748 	Text Hypothesis :	<s> <s> <s> <s> <s> <s> <s> <s> <s> nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing    nothing nothing      nothing nothing   
2024-02-01 01:00:24,748 	Text Alignment  :	I   I   I   I   I   I   I   I   I   I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       I       S       S       S       S       S       S       S       S          S       S            S       S         
2024-02-01 01:00:24,748 ========================================================================================================================
2024-02-01 01:00:24,760 Training ended since there were no improvements inthe last learning rate step: 0.000050
2024-02-01 01:00:24,762 Best validation result at step     2000:   0.00 eval_metric.
2024-02-01 01:00:49,155 ------------------------------------------------------------
2024-02-01 01:00:49,155 [DEV] partition [RECOGNITION] experiment [BW]: 1
2024-02-01 01:01:12,998 finished in 23.8436s 
2024-02-01 01:01:12,998 ************************************************************
2024-02-01 01:01:12,998 [DEV] partition [RECOGNITION] results:
	New Best CTC Decode Beam Size: 1
	WER 533.83	(DEL: 4.38,	INS: 444.56,	SUB: 84.89)
2024-02-01 01:01:12,999 ************************************************************
2024-02-01 01:01:12,999 ------------------------------------------------------------
2024-02-01 01:01:12,999 [DEV] partition [RECOGNITION] experiment [BW]: 2
2024-02-01 01:01:40,195 finished in 27.1959s 
2024-02-01 01:01:40,195 ------------------------------------------------------------
2024-02-01 01:01:40,195 [DEV] partition [RECOGNITION] experiment [BW]: 3
2024-02-01 01:02:11,332 finished in 31.1365s 
2024-02-01 01:02:11,332 ------------------------------------------------------------
2024-02-01 01:02:11,332 [DEV] partition [RECOGNITION] experiment [BW]: 4
2024-02-01 01:02:43,725 finished in 32.3930s 
2024-02-01 01:02:43,725 ------------------------------------------------------------
2024-02-01 01:02:43,725 [DEV] partition [RECOGNITION] experiment [BW]: 5
2024-02-01 01:03:16,885 finished in 33.1600s 
2024-02-01 01:03:16,886 ------------------------------------------------------------
2024-02-01 01:03:16,886 [DEV] partition [RECOGNITION] experiment [BW]: 6
2024-02-01 01:03:50,505 finished in 33.6195s 
2024-02-01 01:03:50,505 ------------------------------------------------------------
2024-02-01 01:03:50,505 [DEV] partition [RECOGNITION] experiment [BW]: 7
2024-02-01 01:04:24,477 finished in 33.9713s 
2024-02-01 01:04:24,477 ------------------------------------------------------------
2024-02-01 01:04:24,477 [DEV] partition [RECOGNITION] experiment [BW]: 8
2024-02-01 01:04:58,579 finished in 34.1025s 
2024-02-01 01:04:58,580 ------------------------------------------------------------
2024-02-01 01:04:58,580 [DEV] partition [RECOGNITION] experiment [BW]: 9
2024-02-01 01:05:32,806 finished in 34.2263s 
2024-02-01 01:05:32,806 ------------------------------------------------------------
2024-02-01 01:05:32,807 [DEV] partition [RECOGNITION] experiment [BW]: 10
2024-02-01 01:06:07,107 finished in 34.2997s 
2024-02-01 01:06:07,107 ============================================================
2024-02-01 01:06:30,649 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 1 and Alpha: -1
	BLEU-4 0.00	(BLEU-1: 0.01,	BLEU-2: 0.00,	BLEU-3: 0.00,	BLEU-4: 0.00)
	CHRF 3.51	ROUGE 0.02
2024-02-01 01:06:30,649 ------------------------------------------------------------
2024-02-01 02:28:05,653 ************************************************************
2024-02-01 02:28:05,656 [DEV] partition [Recognition & Translation] results:
	Best CTC Decode Beam Size: 1
	Best Translation Beam Size: 1 and Alpha: -1
	WER 533.83	(DEL: 4.38,	INS: 444.56,	SUB: 84.89)
	BLEU-4 0.00	(BLEU-1: 0.01,	BLEU-2: 0.00,	BLEU-3: 0.00,	BLEU-4: 0.00)
	CHRF 3.51	ROUGE 0.02
2024-02-01 02:28:05,656 ************************************************************
2024-02-01 02:28:29,034 [TEST] partition [Recognition & Translation] results:
	Best CTC Decode Beam Size: 1
	Best Translation Beam Size: 1 and Alpha: -1
	WER 516.08	(DEL: 4.96,	INS: 426.42,	SUB: 84.70)
	BLEU-4 0.00	(BLEU-1: 0.00,	BLEU-2: 0.00,	BLEU-3: 0.00,	BLEU-4: 0.00)
	CHRF 3.60	ROUGE 0.00
2024-02-01 02:28:29,034 ************************************************************
