2024-02-08 20:39:25,798 Hello! This is Joey-NMT.
2024-02-08 20:39:25,806 Total params: 25642504
2024-02-08 20:39:25,807 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'gloss_output_layer.bias', 'gloss_output_layer.weight', 'sgn_embed.ln.bias', 'sgn_embed.ln.weight', 'sgn_embed.norm.norm.bias', 'sgn_embed.norm.norm.weight', 'txt_embed.norm.norm.bias', 'txt_embed.norm.norm.weight']
2024-02-08 20:39:26,773 cfg.name                           : sign_experiment
2024-02-08 20:39:26,774 cfg.data.data_path                 : ./data/Sports_dataset/7/
2024-02-08 20:39:26,774 cfg.data.version                   : phoenix_2014_trans
2024-02-08 20:39:26,774 cfg.data.sgn                       : sign
2024-02-08 20:39:26,774 cfg.data.txt                       : text
2024-02-08 20:39:26,774 cfg.data.gls                       : gloss
2024-02-08 20:39:26,774 cfg.data.train                     : excel_data.train
2024-02-08 20:39:26,774 cfg.data.dev                       : excel_data.dev
2024-02-08 20:39:26,774 cfg.data.test                      : excel_data.test
2024-02-08 20:39:26,775 cfg.data.feature_size              : 2560
2024-02-08 20:39:26,775 cfg.data.level                     : word
2024-02-08 20:39:26,775 cfg.data.txt_lowercase             : True
2024-02-08 20:39:26,775 cfg.data.max_sent_length           : 500
2024-02-08 20:39:26,775 cfg.data.random_train_subset       : -1
2024-02-08 20:39:26,775 cfg.data.random_dev_subset         : -1
2024-02-08 20:39:26,775 cfg.testing.recognition_beam_sizes : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
2024-02-08 20:39:26,775 cfg.testing.translation_beam_sizes : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
2024-02-08 20:39:26,775 cfg.testing.translation_beam_alphas : [-1, 0, 1, 2, 3, 4, 5]
2024-02-08 20:39:26,776 cfg.training.reset_best_ckpt       : False
2024-02-08 20:39:26,776 cfg.training.reset_scheduler       : False
2024-02-08 20:39:26,776 cfg.training.reset_optimizer       : False
2024-02-08 20:39:26,776 cfg.training.random_seed           : 42
2024-02-08 20:39:26,776 cfg.training.model_dir             : ./sign_sample_model/fold7/32head/128batch
2024-02-08 20:39:26,776 cfg.training.recognition_loss_weight : 1.0
2024-02-08 20:39:26,777 cfg.training.translation_loss_weight : 1.0
2024-02-08 20:39:26,777 cfg.training.eval_metric           : bleu
2024-02-08 20:39:26,777 cfg.training.optimizer             : adam
2024-02-08 20:39:26,777 cfg.training.learning_rate         : 0.0001
2024-02-08 20:39:26,777 cfg.training.batch_size            : 128
2024-02-08 20:39:26,777 cfg.training.num_valid_log         : 5
2024-02-08 20:39:26,777 cfg.training.epochs                : 50000
2024-02-08 20:39:26,777 cfg.training.early_stopping_metric : eval_metric
2024-02-08 20:39:26,778 cfg.training.batch_type            : sentence
2024-02-08 20:39:26,778 cfg.training.translation_normalization : batch
2024-02-08 20:39:26,778 cfg.training.eval_recognition_beam_size : 1
2024-02-08 20:39:26,778 cfg.training.eval_translation_beam_size : 1
2024-02-08 20:39:26,778 cfg.training.eval_translation_beam_alpha : -1
2024-02-08 20:39:26,778 cfg.training.overwrite             : True
2024-02-08 20:39:26,778 cfg.training.shuffle               : True
2024-02-08 20:39:26,779 cfg.training.use_cuda              : True
2024-02-08 20:39:26,779 cfg.training.translation_max_output_length : 40
2024-02-08 20:39:26,779 cfg.training.keep_last_ckpts       : 1
2024-02-08 20:39:26,779 cfg.training.batch_multiplier      : 1
2024-02-08 20:39:26,779 cfg.training.logging_freq          : 100
2024-02-08 20:39:26,779 cfg.training.validation_freq       : 2000
2024-02-08 20:39:26,779 cfg.training.betas                 : [0.9, 0.998]
2024-02-08 20:39:26,779 cfg.training.scheduling            : plateau
2024-02-08 20:39:26,779 cfg.training.learning_rate_min     : 1e-08
2024-02-08 20:39:26,780 cfg.training.weight_decay          : 0.0001
2024-02-08 20:39:26,780 cfg.training.patience              : 12
2024-02-08 20:39:26,780 cfg.training.decrease_factor       : 0.5
2024-02-08 20:39:26,780 cfg.training.label_smoothing       : 0.0
2024-02-08 20:39:26,780 cfg.model.initializer              : xavier
2024-02-08 20:39:26,780 cfg.model.bias_initializer         : zeros
2024-02-08 20:39:26,780 cfg.model.init_gain                : 1.0
2024-02-08 20:39:26,780 cfg.model.embed_initializer        : xavier
2024-02-08 20:39:26,781 cfg.model.embed_init_gain          : 1.0
2024-02-08 20:39:26,781 cfg.model.tied_softmax             : True
2024-02-08 20:39:26,781 cfg.model.encoder.type             : transformer
2024-02-08 20:39:26,781 cfg.model.encoder.num_layers       : 3
2024-02-08 20:39:26,781 cfg.model.encoder.num_heads        : 32
2024-02-08 20:39:26,781 cfg.model.encoder.embeddings.embedding_dim : 512
2024-02-08 20:39:26,781 cfg.model.encoder.embeddings.scale : False
2024-02-08 20:39:26,781 cfg.model.encoder.embeddings.dropout : 0.1
2024-02-08 20:39:26,782 cfg.model.encoder.embeddings.norm_type : batch
2024-02-08 20:39:26,782 cfg.model.encoder.embeddings.activation_type : softsign
2024-02-08 20:39:26,782 cfg.model.encoder.hidden_size      : 512
2024-02-08 20:39:26,782 cfg.model.encoder.ff_size          : 2048
2024-02-08 20:39:26,782 cfg.model.encoder.dropout          : 0.1
2024-02-08 20:39:26,782 cfg.model.decoder.type             : transformer
2024-02-08 20:39:26,782 cfg.model.decoder.num_layers       : 3
2024-02-08 20:39:26,782 cfg.model.decoder.num_heads        : 32
2024-02-08 20:39:26,782 cfg.model.decoder.embeddings.embedding_dim : 512
2024-02-08 20:39:26,783 cfg.model.decoder.embeddings.scale : False
2024-02-08 20:39:26,783 cfg.model.decoder.embeddings.dropout : 0.1
2024-02-08 20:39:26,783 cfg.model.decoder.embeddings.norm_type : batch
2024-02-08 20:39:26,783 cfg.model.decoder.embeddings.activation_type : softsign
2024-02-08 20:39:26,783 cfg.model.decoder.hidden_size      : 512
2024-02-08 20:39:26,783 cfg.model.decoder.ff_size          : 2048
2024-02-08 20:39:26,783 cfg.model.decoder.dropout          : 0.1
2024-02-08 20:39:26,783 Data set sizes: 
	train 2124,
	valid 708,
	test 708
2024-02-08 20:39:26,784 First training example:
	[GLS] A B C D E
	[TXT] how did she become a champion
2024-02-08 20:39:26,784 First 10 words (gls): (0) <si> (1) <unk> (2) <pad> (3) A (4) B (5) C (6) D (7) E
2024-02-08 20:39:26,784 First 10 words (txt): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) the (5) and (6) to (7) in (8) a (9) of
2024-02-08 20:39:26,784 Number of unique glosses (types): 8
2024-02-08 20:39:26,784 Number of unique words (types): 4402
2024-02-08 20:39:26,784 SignModel(
	encoder=TransformerEncoder(num_layers=3, num_heads=32),
	decoder=TransformerDecoder(num_layers=3, num_heads=32),
	sgn_embed=SpatialEmbeddings(embedding_dim=512, input_size=2560),
	txt_embed=Embeddings(embedding_dim=512, vocab_size=4402))
2024-02-08 20:39:26,788 EPOCH 1
2024-02-08 20:39:45,245 Epoch   1: Total Training Recognition Loss 128.04  Total Training Translation Loss 1791.33 
2024-02-08 20:39:45,245 EPOCH 2
2024-02-08 20:39:55,767 Epoch   2: Total Training Recognition Loss 54.18  Total Training Translation Loss 1633.16 
2024-02-08 20:39:55,768 EPOCH 3
2024-02-08 20:40:06,289 Epoch   3: Total Training Recognition Loss 32.20  Total Training Translation Loss 1554.05 
2024-02-08 20:40:06,289 EPOCH 4
2024-02-08 20:40:16,901 Epoch   4: Total Training Recognition Loss 24.27  Total Training Translation Loss 1517.32 
2024-02-08 20:40:16,902 EPOCH 5
2024-02-08 20:40:27,437 Epoch   5: Total Training Recognition Loss 19.52  Total Training Translation Loss 1501.90 
2024-02-08 20:40:27,438 EPOCH 6
2024-02-08 20:40:37,516 [Epoch: 006 Step: 00000100] Batch Recognition Loss:   0.976506 => Gls Tokens per Sec:      927 || Batch Translation Loss:  44.967590 => Txt Tokens per Sec:     2587 || Lr: 0.000100
2024-02-08 20:40:37,924 Epoch   6: Total Training Recognition Loss 16.28  Total Training Translation Loss 1492.68 
2024-02-08 20:40:37,924 EPOCH 7
2024-02-08 20:40:48,296 Epoch   7: Total Training Recognition Loss 14.83  Total Training Translation Loss 1482.00 
2024-02-08 20:40:48,296 EPOCH 8
2024-02-08 20:40:58,887 Epoch   8: Total Training Recognition Loss 13.52  Total Training Translation Loss 1463.30 
2024-02-08 20:40:58,887 EPOCH 9
2024-02-08 20:41:09,373 Epoch   9: Total Training Recognition Loss 12.20  Total Training Translation Loss 1440.38 
2024-02-08 20:41:09,373 EPOCH 10
2024-02-08 20:41:19,968 Epoch  10: Total Training Recognition Loss 20.95  Total Training Translation Loss 1414.55 
2024-02-08 20:41:19,969 EPOCH 11
2024-02-08 20:41:30,507 Epoch  11: Total Training Recognition Loss 17.87  Total Training Translation Loss 1386.50 
2024-02-08 20:41:30,507 EPOCH 12
2024-02-08 20:41:36,670 [Epoch: 012 Step: 00000200] Batch Recognition Loss:   0.527052 => Gls Tokens per Sec:     1350 || Batch Translation Loss:  81.015686 => Txt Tokens per Sec:     3763 || Lr: 0.000100
2024-02-08 20:41:41,206 Epoch  12: Total Training Recognition Loss 18.47  Total Training Translation Loss 1359.20 
2024-02-08 20:41:41,207 EPOCH 13
2024-02-08 20:41:51,955 Epoch  13: Total Training Recognition Loss 36.99  Total Training Translation Loss 1332.63 
2024-02-08 20:41:51,956 EPOCH 14
2024-02-08 20:42:02,598 Epoch  14: Total Training Recognition Loss 20.67  Total Training Translation Loss 1303.79 
2024-02-08 20:42:02,598 EPOCH 15
2024-02-08 20:42:12,831 Epoch  15: Total Training Recognition Loss 12.36  Total Training Translation Loss 1273.04 
2024-02-08 20:42:12,832 EPOCH 16
2024-02-08 20:42:23,392 Epoch  16: Total Training Recognition Loss 10.32  Total Training Translation Loss 1247.23 
2024-02-08 20:42:23,392 EPOCH 17
2024-02-08 20:42:33,952 Epoch  17: Total Training Recognition Loss 8.58  Total Training Translation Loss 1219.95 
2024-02-08 20:42:33,953 EPOCH 18
2024-02-08 20:42:40,485 [Epoch: 018 Step: 00000300] Batch Recognition Loss:   0.232222 => Gls Tokens per Sec:     1038 || Batch Translation Loss:  69.765289 => Txt Tokens per Sec:     2967 || Lr: 0.000100
2024-02-08 20:42:44,764 Epoch  18: Total Training Recognition Loss 7.65  Total Training Translation Loss 1194.55 
2024-02-08 20:42:44,764 EPOCH 19
2024-02-08 20:42:55,433 Epoch  19: Total Training Recognition Loss 7.34  Total Training Translation Loss 1182.65 
2024-02-08 20:42:55,433 EPOCH 20
2024-02-08 20:43:06,131 Epoch  20: Total Training Recognition Loss 6.71  Total Training Translation Loss 1150.37 
2024-02-08 20:43:06,132 EPOCH 21
2024-02-08 20:43:16,718 Epoch  21: Total Training Recognition Loss 6.19  Total Training Translation Loss 1121.88 
2024-02-08 20:43:16,719 EPOCH 22
2024-02-08 20:43:27,353 Epoch  22: Total Training Recognition Loss 5.64  Total Training Translation Loss 1094.96 
2024-02-08 20:43:27,354 EPOCH 23
2024-02-08 20:43:38,076 Epoch  23: Total Training Recognition Loss 4.97  Total Training Translation Loss 1076.65 
2024-02-08 20:43:38,077 EPOCH 24
2024-02-08 20:43:46,661 [Epoch: 024 Step: 00000400] Batch Recognition Loss:   0.265214 => Gls Tokens per Sec:      641 || Batch Translation Loss:  70.726700 => Txt Tokens per Sec:     1800 || Lr: 0.000100
2024-02-08 20:43:48,518 Epoch  24: Total Training Recognition Loss 4.85  Total Training Translation Loss 1066.68 
2024-02-08 20:43:48,518 EPOCH 25
2024-02-08 20:43:59,101 Epoch  25: Total Training Recognition Loss 4.67  Total Training Translation Loss 1041.97 
2024-02-08 20:43:59,102 EPOCH 26
2024-02-08 20:44:09,825 Epoch  26: Total Training Recognition Loss 4.47  Total Training Translation Loss 1026.85 
2024-02-08 20:44:09,826 EPOCH 27
2024-02-08 20:44:20,674 Epoch  27: Total Training Recognition Loss 4.16  Total Training Translation Loss 1001.60 
2024-02-08 20:44:20,674 EPOCH 28
2024-02-08 20:44:31,378 Epoch  28: Total Training Recognition Loss 4.03  Total Training Translation Loss 979.50 
2024-02-08 20:44:31,378 EPOCH 29
2024-02-08 20:44:41,917 Epoch  29: Total Training Recognition Loss 3.68  Total Training Translation Loss 953.94 
2024-02-08 20:44:41,917 EPOCH 30
2024-02-08 20:44:46,533 [Epoch: 030 Step: 00000500] Batch Recognition Loss:   0.508456 => Gls Tokens per Sec:      971 || Batch Translation Loss:  72.281548 => Txt Tokens per Sec:     2649 || Lr: 0.000100
2024-02-08 20:44:52,203 Epoch  30: Total Training Recognition Loss 3.41  Total Training Translation Loss 935.38 
2024-02-08 20:44:52,203 EPOCH 31
2024-02-08 20:45:02,869 Epoch  31: Total Training Recognition Loss 3.33  Total Training Translation Loss 917.75 
2024-02-08 20:45:02,870 EPOCH 32
2024-02-08 20:45:13,721 Epoch  32: Total Training Recognition Loss 3.14  Total Training Translation Loss 899.72 
2024-02-08 20:45:13,721 EPOCH 33
2024-02-08 20:45:24,180 Epoch  33: Total Training Recognition Loss 3.01  Total Training Translation Loss 877.50 
2024-02-08 20:45:24,180 EPOCH 34
2024-02-08 20:45:34,849 Epoch  34: Total Training Recognition Loss 2.93  Total Training Translation Loss 875.53 
2024-02-08 20:45:34,849 EPOCH 35
2024-02-08 20:45:45,631 Epoch  35: Total Training Recognition Loss 2.76  Total Training Translation Loss 860.66 
2024-02-08 20:45:45,632 EPOCH 36
2024-02-08 20:45:48,652 [Epoch: 036 Step: 00000600] Batch Recognition Loss:   0.396364 => Gls Tokens per Sec:      974 || Batch Translation Loss:  55.951893 => Txt Tokens per Sec:     2430 || Lr: 0.000100
2024-02-08 20:45:56,162 Epoch  36: Total Training Recognition Loss 2.80  Total Training Translation Loss 842.58 
2024-02-08 20:45:56,162 EPOCH 37
2024-02-08 20:46:06,793 Epoch  37: Total Training Recognition Loss 2.59  Total Training Translation Loss 819.05 
2024-02-08 20:46:06,794 EPOCH 38
2024-02-08 20:46:17,333 Epoch  38: Total Training Recognition Loss 2.58  Total Training Translation Loss 798.37 
2024-02-08 20:46:17,334 EPOCH 39
2024-02-08 20:46:27,981 Epoch  39: Total Training Recognition Loss 2.52  Total Training Translation Loss 782.46 
2024-02-08 20:46:27,981 EPOCH 40
2024-02-08 20:46:38,599 Epoch  40: Total Training Recognition Loss 2.36  Total Training Translation Loss 765.53 
2024-02-08 20:46:38,600 EPOCH 41
2024-02-08 20:46:49,386 Epoch  41: Total Training Recognition Loss 2.37  Total Training Translation Loss 752.80 
2024-02-08 20:46:49,387 EPOCH 42
2024-02-08 20:46:51,553 [Epoch: 042 Step: 00000700] Batch Recognition Loss:   0.171886 => Gls Tokens per Sec:      888 || Batch Translation Loss:  56.693783 => Txt Tokens per Sec:     2457 || Lr: 0.000100
2024-02-08 20:46:59,907 Epoch  42: Total Training Recognition Loss 2.36  Total Training Translation Loss 729.57 
2024-02-08 20:46:59,908 EPOCH 43
2024-02-08 20:47:10,720 Epoch  43: Total Training Recognition Loss 2.34  Total Training Translation Loss 715.08 
2024-02-08 20:47:10,721 EPOCH 44
2024-02-08 20:47:21,366 Epoch  44: Total Training Recognition Loss 2.27  Total Training Translation Loss 699.41 
2024-02-08 20:47:21,366 EPOCH 45
2024-02-08 20:47:32,082 Epoch  45: Total Training Recognition Loss 2.32  Total Training Translation Loss 682.07 
2024-02-08 20:47:32,083 EPOCH 46
2024-02-08 20:47:42,709 Epoch  46: Total Training Recognition Loss 2.38  Total Training Translation Loss 664.12 
2024-02-08 20:47:42,709 EPOCH 47
2024-02-08 20:47:53,340 Epoch  47: Total Training Recognition Loss 1.99  Total Training Translation Loss 656.77 
2024-02-08 20:47:53,341 EPOCH 48
2024-02-08 20:47:53,476 [Epoch: 048 Step: 00000800] Batch Recognition Loss:   0.101157 => Gls Tokens per Sec:     4776 || Batch Translation Loss:  28.995609 => Txt Tokens per Sec:    10216 || Lr: 0.000100
2024-02-08 20:48:04,060 Epoch  48: Total Training Recognition Loss 2.13  Total Training Translation Loss 633.83 
2024-02-08 20:48:04,061 EPOCH 49
2024-02-08 20:48:14,719 Epoch  49: Total Training Recognition Loss 1.96  Total Training Translation Loss 619.68 
2024-02-08 20:48:14,719 EPOCH 50
2024-02-08 20:48:25,425 Epoch  50: Total Training Recognition Loss 1.92  Total Training Translation Loss 614.49 
2024-02-08 20:48:25,426 EPOCH 51
2024-02-08 20:48:36,005 Epoch  51: Total Training Recognition Loss 1.90  Total Training Translation Loss 597.08 
2024-02-08 20:48:36,006 EPOCH 52
2024-02-08 20:48:46,767 Epoch  52: Total Training Recognition Loss 1.90  Total Training Translation Loss 580.55 
2024-02-08 20:48:46,768 EPOCH 53
2024-02-08 20:48:57,238 [Epoch: 053 Step: 00000900] Batch Recognition Loss:   0.099018 => Gls Tokens per Sec:      953 || Batch Translation Loss:  22.809330 => Txt Tokens per Sec:     2711 || Lr: 0.000100
2024-02-08 20:48:57,352 Epoch  53: Total Training Recognition Loss 1.80  Total Training Translation Loss 562.64 
2024-02-08 20:48:57,353 EPOCH 54
2024-02-08 20:49:08,077 Epoch  54: Total Training Recognition Loss 1.77  Total Training Translation Loss 563.77 
2024-02-08 20:49:08,078 EPOCH 55
2024-02-08 20:49:18,800 Epoch  55: Total Training Recognition Loss 1.78  Total Training Translation Loss 539.79 
2024-02-08 20:49:18,800 EPOCH 56
2024-02-08 20:49:29,574 Epoch  56: Total Training Recognition Loss 1.73  Total Training Translation Loss 532.72 
2024-02-08 20:49:29,575 EPOCH 57
2024-02-08 20:49:40,248 Epoch  57: Total Training Recognition Loss 1.82  Total Training Translation Loss 507.33 
2024-02-08 20:49:40,249 EPOCH 58
2024-02-08 20:49:50,897 Epoch  58: Total Training Recognition Loss 1.72  Total Training Translation Loss 495.33 
2024-02-08 20:49:50,897 EPOCH 59
2024-02-08 20:50:00,943 [Epoch: 059 Step: 00001000] Batch Recognition Loss:   0.050578 => Gls Tokens per Sec:      866 || Batch Translation Loss:  24.988049 => Txt Tokens per Sec:     2418 || Lr: 0.000100
2024-02-08 20:50:01,694 Epoch  59: Total Training Recognition Loss 1.76  Total Training Translation Loss 479.86 
2024-02-08 20:50:01,695 EPOCH 60
2024-02-08 20:50:12,408 Epoch  60: Total Training Recognition Loss 1.72  Total Training Translation Loss 464.95 
2024-02-08 20:50:12,408 EPOCH 61
2024-02-08 20:50:23,125 Epoch  61: Total Training Recognition Loss 1.69  Total Training Translation Loss 447.91 
2024-02-08 20:50:23,126 EPOCH 62
2024-02-08 20:50:33,731 Epoch  62: Total Training Recognition Loss 1.57  Total Training Translation Loss 435.53 
2024-02-08 20:50:33,732 EPOCH 63
2024-02-08 20:50:44,174 Epoch  63: Total Training Recognition Loss 1.58  Total Training Translation Loss 422.47 
2024-02-08 20:50:44,175 EPOCH 64
2024-02-08 20:50:55,046 Epoch  64: Total Training Recognition Loss 1.69  Total Training Translation Loss 416.96 
2024-02-08 20:50:55,047 EPOCH 65
2024-02-08 20:51:01,412 [Epoch: 065 Step: 00001100] Batch Recognition Loss:   0.040921 => Gls Tokens per Sec:     1166 || Batch Translation Loss:  22.917747 => Txt Tokens per Sec:     3196 || Lr: 0.000100
2024-02-08 20:51:05,668 Epoch  65: Total Training Recognition Loss 1.65  Total Training Translation Loss 412.93 
2024-02-08 20:51:05,669 EPOCH 66
2024-02-08 20:51:16,323 Epoch  66: Total Training Recognition Loss 1.60  Total Training Translation Loss 391.90 
2024-02-08 20:51:16,324 EPOCH 67
2024-02-08 20:51:27,105 Epoch  67: Total Training Recognition Loss 1.56  Total Training Translation Loss 378.32 
2024-02-08 20:51:27,105 EPOCH 68
2024-02-08 20:51:37,951 Epoch  68: Total Training Recognition Loss 1.47  Total Training Translation Loss 367.11 
2024-02-08 20:51:37,951 EPOCH 69
2024-02-08 20:51:48,535 Epoch  69: Total Training Recognition Loss 1.44  Total Training Translation Loss 353.96 
2024-02-08 20:51:48,535 EPOCH 70
2024-02-08 20:51:59,350 Epoch  70: Total Training Recognition Loss 1.46  Total Training Translation Loss 347.87 
2024-02-08 20:51:59,351 EPOCH 71
2024-02-08 20:52:04,758 [Epoch: 071 Step: 00001200] Batch Recognition Loss:   0.092804 => Gls Tokens per Sec:     1184 || Batch Translation Loss:  13.493877 => Txt Tokens per Sec:     3132 || Lr: 0.000100
2024-02-08 20:52:09,944 Epoch  71: Total Training Recognition Loss 1.64  Total Training Translation Loss 332.92 
2024-02-08 20:52:09,945 EPOCH 72
2024-02-08 20:52:20,675 Epoch  72: Total Training Recognition Loss 1.60  Total Training Translation Loss 329.00 
2024-02-08 20:52:20,675 EPOCH 73
2024-02-08 20:52:31,361 Epoch  73: Total Training Recognition Loss 1.50  Total Training Translation Loss 312.33 
2024-02-08 20:52:31,362 EPOCH 74
2024-02-08 20:52:42,145 Epoch  74: Total Training Recognition Loss 1.40  Total Training Translation Loss 304.62 
2024-02-08 20:52:42,145 EPOCH 75
2024-02-08 20:52:52,886 Epoch  75: Total Training Recognition Loss 1.39  Total Training Translation Loss 294.42 
2024-02-08 20:52:52,887 EPOCH 76
2024-02-08 20:53:03,687 Epoch  76: Total Training Recognition Loss 1.37  Total Training Translation Loss 278.78 
2024-02-08 20:53:03,688 EPOCH 77
2024-02-08 20:53:06,930 [Epoch: 077 Step: 00001300] Batch Recognition Loss:   0.036468 => Gls Tokens per Sec:     1580 || Batch Translation Loss:  15.212393 => Txt Tokens per Sec:     4051 || Lr: 0.000100
2024-02-08 20:53:14,527 Epoch  77: Total Training Recognition Loss 1.38  Total Training Translation Loss 270.85 
2024-02-08 20:53:14,528 EPOCH 78
2024-02-08 20:53:25,276 Epoch  78: Total Training Recognition Loss 1.42  Total Training Translation Loss 266.97 
2024-02-08 20:53:25,276 EPOCH 79
2024-02-08 20:53:35,854 Epoch  79: Total Training Recognition Loss 1.43  Total Training Translation Loss 255.49 
2024-02-08 20:53:35,854 EPOCH 80
2024-02-08 20:53:46,497 Epoch  80: Total Training Recognition Loss 1.32  Total Training Translation Loss 243.15 
2024-02-08 20:53:46,498 EPOCH 81
2024-02-08 20:53:57,113 Epoch  81: Total Training Recognition Loss 1.32  Total Training Translation Loss 233.86 
2024-02-08 20:53:57,113 EPOCH 82
2024-02-08 20:54:07,628 Epoch  82: Total Training Recognition Loss 1.35  Total Training Translation Loss 225.98 
2024-02-08 20:54:07,629 EPOCH 83
2024-02-08 20:54:10,580 [Epoch: 083 Step: 00001400] Batch Recognition Loss:   0.088154 => Gls Tokens per Sec:     1301 || Batch Translation Loss:  17.645266 => Txt Tokens per Sec:     3811 || Lr: 0.000100
2024-02-08 20:54:18,153 Epoch  83: Total Training Recognition Loss 1.31  Total Training Translation Loss 225.09 
2024-02-08 20:54:18,154 EPOCH 84
2024-02-08 20:54:28,730 Epoch  84: Total Training Recognition Loss 1.32  Total Training Translation Loss 212.89 
2024-02-08 20:54:28,731 EPOCH 85
2024-02-08 20:54:39,521 Epoch  85: Total Training Recognition Loss 1.28  Total Training Translation Loss 200.47 
2024-02-08 20:54:39,522 EPOCH 86
2024-02-08 20:54:50,273 Epoch  86: Total Training Recognition Loss 1.23  Total Training Translation Loss 193.42 
2024-02-08 20:54:50,273 EPOCH 87
2024-02-08 20:55:00,863 Epoch  87: Total Training Recognition Loss 1.19  Total Training Translation Loss 183.20 
2024-02-08 20:55:00,864 EPOCH 88
2024-02-08 20:55:11,547 Epoch  88: Total Training Recognition Loss 1.19  Total Training Translation Loss 176.25 
2024-02-08 20:55:11,547 EPOCH 89
2024-02-08 20:55:14,444 [Epoch: 089 Step: 00001500] Batch Recognition Loss:   0.095236 => Gls Tokens per Sec:      794 || Batch Translation Loss:   4.295588 => Txt Tokens per Sec:     1987 || Lr: 0.000100
2024-02-08 20:55:22,135 Epoch  89: Total Training Recognition Loss 1.20  Total Training Translation Loss 171.32 
2024-02-08 20:55:22,136 EPOCH 90
2024-02-08 20:55:32,853 Epoch  90: Total Training Recognition Loss 1.19  Total Training Translation Loss 161.92 
2024-02-08 20:55:32,854 EPOCH 91
2024-02-08 20:55:43,464 Epoch  91: Total Training Recognition Loss 1.08  Total Training Translation Loss 154.37 
2024-02-08 20:55:43,465 EPOCH 92
2024-02-08 20:55:54,209 Epoch  92: Total Training Recognition Loss 1.09  Total Training Translation Loss 149.51 
2024-02-08 20:55:54,210 EPOCH 93
2024-02-08 20:56:05,009 Epoch  93: Total Training Recognition Loss 1.08  Total Training Translation Loss 144.34 
2024-02-08 20:56:05,009 EPOCH 94
2024-02-08 20:56:15,622 Epoch  94: Total Training Recognition Loss 1.08  Total Training Translation Loss 140.35 
2024-02-08 20:56:15,623 EPOCH 95
2024-02-08 20:56:16,130 [Epoch: 095 Step: 00001600] Batch Recognition Loss:   0.042553 => Gls Tokens per Sec:     2530 || Batch Translation Loss:   8.498411 => Txt Tokens per Sec:     7267 || Lr: 0.000100
2024-02-08 20:56:26,388 Epoch  95: Total Training Recognition Loss 1.05  Total Training Translation Loss 132.26 
2024-02-08 20:56:26,388 EPOCH 96
2024-02-08 20:56:37,292 Epoch  96: Total Training Recognition Loss 1.06  Total Training Translation Loss 124.69 
2024-02-08 20:56:37,292 EPOCH 97
2024-02-08 20:56:47,841 Epoch  97: Total Training Recognition Loss 1.10  Total Training Translation Loss 120.88 
2024-02-08 20:56:47,841 EPOCH 98
2024-02-08 20:56:58,594 Epoch  98: Total Training Recognition Loss 1.10  Total Training Translation Loss 116.44 
2024-02-08 20:56:58,595 EPOCH 99
2024-02-08 20:57:09,318 Epoch  99: Total Training Recognition Loss 1.01  Total Training Translation Loss 112.72 
2024-02-08 20:57:09,319 EPOCH 100
2024-02-08 20:57:20,295 [Epoch: 100 Step: 00001700] Batch Recognition Loss:   0.062749 => Gls Tokens per Sec:      968 || Batch Translation Loss:   7.902446 => Txt Tokens per Sec:     2677 || Lr: 0.000100
2024-02-08 20:57:20,295 Epoch 100: Total Training Recognition Loss 0.98  Total Training Translation Loss 107.68 
2024-02-08 20:57:20,296 EPOCH 101
2024-02-08 20:57:31,406 Epoch 101: Total Training Recognition Loss 0.99  Total Training Translation Loss 102.56 
2024-02-08 20:57:31,406 EPOCH 102
2024-02-08 20:57:42,074 Epoch 102: Total Training Recognition Loss 0.95  Total Training Translation Loss 97.33 
2024-02-08 20:57:42,074 EPOCH 103
2024-02-08 20:57:52,786 Epoch 103: Total Training Recognition Loss 0.95  Total Training Translation Loss 92.55 
2024-02-08 20:57:52,786 EPOCH 104
2024-02-08 20:58:03,250 Epoch 104: Total Training Recognition Loss 0.93  Total Training Translation Loss 89.00 
2024-02-08 20:58:03,251 EPOCH 105
2024-02-08 20:58:13,526 Epoch 105: Total Training Recognition Loss 0.94  Total Training Translation Loss 84.21 
2024-02-08 20:58:13,527 EPOCH 106
2024-02-08 20:58:23,510 [Epoch: 106 Step: 00001800] Batch Recognition Loss:   0.032487 => Gls Tokens per Sec:      936 || Batch Translation Loss:   3.980853 => Txt Tokens per Sec:     2600 || Lr: 0.000100
2024-02-08 20:58:23,985 Epoch 106: Total Training Recognition Loss 0.82  Total Training Translation Loss 82.72 
2024-02-08 20:58:23,986 EPOCH 107
2024-02-08 20:58:34,452 Epoch 107: Total Training Recognition Loss 0.88  Total Training Translation Loss 77.76 
2024-02-08 20:58:34,452 EPOCH 108
2024-02-08 20:58:44,770 Epoch 108: Total Training Recognition Loss 0.84  Total Training Translation Loss 74.66 
2024-02-08 20:58:44,770 EPOCH 109
2024-02-08 20:58:54,974 Epoch 109: Total Training Recognition Loss 0.77  Total Training Translation Loss 71.02 
2024-02-08 20:58:54,975 EPOCH 110
2024-02-08 20:59:05,786 Epoch 110: Total Training Recognition Loss 0.84  Total Training Translation Loss 69.11 
2024-02-08 20:59:05,786 EPOCH 111
2024-02-08 20:59:16,432 Epoch 111: Total Training Recognition Loss 0.82  Total Training Translation Loss 66.55 
2024-02-08 20:59:16,433 EPOCH 112
2024-02-08 20:59:26,125 [Epoch: 112 Step: 00001900] Batch Recognition Loss:   0.069450 => Gls Tokens per Sec:      832 || Batch Translation Loss:   2.746231 => Txt Tokens per Sec:     2317 || Lr: 0.000100
2024-02-08 20:59:27,093 Epoch 112: Total Training Recognition Loss 0.75  Total Training Translation Loss 63.81 
2024-02-08 20:59:27,094 EPOCH 113
2024-02-08 20:59:37,674 Epoch 113: Total Training Recognition Loss 0.78  Total Training Translation Loss 61.61 
2024-02-08 20:59:37,675 EPOCH 114
2024-02-08 20:59:48,342 Epoch 114: Total Training Recognition Loss 0.81  Total Training Translation Loss 59.91 
2024-02-08 20:59:48,342 EPOCH 115
2024-02-08 20:59:59,108 Epoch 115: Total Training Recognition Loss 0.72  Total Training Translation Loss 57.33 
2024-02-08 20:59:59,109 EPOCH 116
2024-02-08 21:00:09,533 Epoch 116: Total Training Recognition Loss 0.68  Total Training Translation Loss 56.47 
2024-02-08 21:00:09,534 EPOCH 117
2024-02-08 21:00:20,068 Epoch 117: Total Training Recognition Loss 0.66  Total Training Translation Loss 53.08 
2024-02-08 21:00:20,068 EPOCH 118
2024-02-08 21:00:27,776 [Epoch: 118 Step: 00002000] Batch Recognition Loss:   0.024295 => Gls Tokens per Sec:      880 || Batch Translation Loss:   3.424243 => Txt Tokens per Sec:     2421 || Lr: 0.000100
2024-02-08 21:01:19,982 Hooray! New best validation result [eval_metric]!
2024-02-08 21:01:19,983 Saving new checkpoint.
2024-02-08 21:01:20,247 Validation result at epoch 118, step     2000: duration: 52.4700s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.95281	Translation Loss: 72664.01562	PPL: 1418.97827
	Eval Metric: BLEU
	WER 7.77	(DEL: 0.00,	INS: 0.00,	SUB: 7.77)
	BLEU-4 0.83	(BLEU-1: 12.21,	BLEU-2: 4.29,	BLEU-3: 1.76,	BLEU-4: 0.83)
	CHRF 17.22	ROUGE 10.22
2024-02-08 21:01:20,248 Logging Recognition and Translation Outputs
2024-02-08 21:01:20,248 ========================================================================================================================
2024-02-08 21:01:20,248 Logging Sequence: 165_414.00
2024-02-08 21:01:20,248 	Gloss Reference :	A B+C+D+E
2024-02-08 21:01:20,248 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 21:01:20,249 	Gloss Alignment :	         
2024-02-08 21:01:20,249 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 21:01:20,250 	Text Reference  :	he felt   sachin  was    lucky so he            always  gave his sweater   to   give it   to the      umpire
2024-02-08 21:01:20,251 	Text Hypothesis :	** bowler jasprit bumrah gave  an extraordinary batsman who  is  currently luck in   such a  talented luck  
2024-02-08 21:01:20,251 	Text Alignment  :	D  S      S       S      S     S  S             S       S    S   S         S    S    S    S  S        S     
2024-02-08 21:01:20,251 ========================================================================================================================
2024-02-08 21:01:20,251 Logging Sequence: 169_268.00
2024-02-08 21:01:20,251 	Gloss Reference :	A B+C+D+E
2024-02-08 21:01:20,251 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 21:01:20,252 	Gloss Alignment :	         
2024-02-08 21:01:20,252 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 21:01:20,253 	Text Reference  :	shami supports arshdeep and  many fans    supported him   as  well      
2024-02-08 21:01:20,253 	Text Hypothesis :	***** a        panel    will be   created to        solve the miscreants
2024-02-08 21:01:20,253 	Text Alignment  :	D     S        S        S    S    S       S         S     S   S         
2024-02-08 21:01:20,253 ========================================================================================================================
2024-02-08 21:01:20,253 Logging Sequence: 172_15.00
2024-02-08 21:01:20,254 	Gloss Reference :	A B+C+D+E
2024-02-08 21:01:20,254 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 21:01:20,254 	Gloss Alignment :	         
2024-02-08 21:01:20,254 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 21:01:20,256 	Text Reference  :	now in the final match on       28 may 2023  the   two teams        were  up against each   other at    the same venue
2024-02-08 21:01:20,256 	Text Hypothesis :	*** ** *** ***** ***** whenever it is  being known as  commonwealth games as they    remove the   world cup as   well 
2024-02-08 21:01:20,256 	Text Alignment  :	D   D  D   D     D     S        S  S   S     S     S   S            S     S  S       S      S     S     S   S    S    
2024-02-08 21:01:20,256 ========================================================================================================================
2024-02-08 21:01:20,256 Logging Sequence: 96_158.00
2024-02-08 21:01:20,257 	Gloss Reference :	A B+C+D+E
2024-02-08 21:01:20,257 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 21:01:20,257 	Gloss Alignment :	         
2024-02-08 21:01:20,257 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 21:01:20,258 	Text Reference  :	****** **** ******* after this pandya fell on   his knees in   disappointment
2024-02-08 21:01:20,258 	Text Hypothesis :	people were shocked to    see  the    next ball for the   next wicket        
2024-02-08 21:01:20,258 	Text Alignment  :	I      I    I       S     S    S      S    S    S   S     S    S             
2024-02-08 21:01:20,258 ========================================================================================================================
2024-02-08 21:01:20,258 Logging Sequence: 152_73.00
2024-02-08 21:01:20,258 	Gloss Reference :	A B+C+D+E
2024-02-08 21:01:20,258 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 21:01:20,259 	Gloss Alignment :	         
2024-02-08 21:01:20,259 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 21:01:20,259 	Text Reference  :	eventually he    too  got      out by    shaheen afridi   
2024-02-08 21:01:20,259 	Text Hypothesis :	********** virat will continue in  india for     captaincy
2024-02-08 21:01:20,259 	Text Alignment  :	D          S     S    S        S   S     S       S        
2024-02-08 21:01:20,260 ========================================================================================================================
2024-02-08 21:01:23,138 Epoch 118: Total Training Recognition Loss 0.66  Total Training Translation Loss 52.15 
2024-02-08 21:01:23,138 EPOCH 119
2024-02-08 21:01:33,763 Epoch 119: Total Training Recognition Loss 0.65  Total Training Translation Loss 50.32 
2024-02-08 21:01:33,764 EPOCH 120
2024-02-08 21:01:44,612 Epoch 120: Total Training Recognition Loss 0.63  Total Training Translation Loss 49.05 
2024-02-08 21:01:44,612 EPOCH 121
2024-02-08 21:01:55,383 Epoch 121: Total Training Recognition Loss 0.68  Total Training Translation Loss 47.18 
2024-02-08 21:01:55,384 EPOCH 122
2024-02-08 21:02:06,056 Epoch 122: Total Training Recognition Loss 0.63  Total Training Translation Loss 46.44 
2024-02-08 21:02:06,057 EPOCH 123
2024-02-08 21:02:16,728 Epoch 123: Total Training Recognition Loss 0.61  Total Training Translation Loss 45.83 
2024-02-08 21:02:16,729 EPOCH 124
2024-02-08 21:02:22,314 [Epoch: 124 Step: 00002100] Batch Recognition Loss:   0.032356 => Gls Tokens per Sec:      985 || Batch Translation Loss:   3.405052 => Txt Tokens per Sec:     2605 || Lr: 0.000100
2024-02-08 21:02:27,461 Epoch 124: Total Training Recognition Loss 0.61  Total Training Translation Loss 44.21 
2024-02-08 21:02:27,462 EPOCH 125
2024-02-08 21:02:37,971 Epoch 125: Total Training Recognition Loss 0.58  Total Training Translation Loss 41.62 
2024-02-08 21:02:37,972 EPOCH 126
2024-02-08 21:02:48,252 Epoch 126: Total Training Recognition Loss 0.61  Total Training Translation Loss 38.92 
2024-02-08 21:02:48,252 EPOCH 127
2024-02-08 21:02:59,229 Epoch 127: Total Training Recognition Loss 0.57  Total Training Translation Loss 37.75 
2024-02-08 21:02:59,229 EPOCH 128
2024-02-08 21:03:09,764 Epoch 128: Total Training Recognition Loss 0.55  Total Training Translation Loss 36.98 
2024-02-08 21:03:09,764 EPOCH 129
2024-02-08 21:03:20,556 Epoch 129: Total Training Recognition Loss 0.59  Total Training Translation Loss 35.32 
2024-02-08 21:03:20,557 EPOCH 130
2024-02-08 21:03:23,891 [Epoch: 130 Step: 00002200] Batch Recognition Loss:   0.018128 => Gls Tokens per Sec:     1344 || Batch Translation Loss:   1.823715 => Txt Tokens per Sec:     3567 || Lr: 0.000100
2024-02-08 21:03:31,267 Epoch 130: Total Training Recognition Loss 0.54  Total Training Translation Loss 34.94 
2024-02-08 21:03:31,267 EPOCH 131
2024-02-08 21:03:41,831 Epoch 131: Total Training Recognition Loss 0.52  Total Training Translation Loss 34.19 
2024-02-08 21:03:41,831 EPOCH 132
2024-02-08 21:03:52,577 Epoch 132: Total Training Recognition Loss 0.53  Total Training Translation Loss 34.60 
2024-02-08 21:03:52,577 EPOCH 133
2024-02-08 21:04:03,477 Epoch 133: Total Training Recognition Loss 0.52  Total Training Translation Loss 34.78 
2024-02-08 21:04:03,478 EPOCH 134
2024-02-08 21:04:14,209 Epoch 134: Total Training Recognition Loss 0.51  Total Training Translation Loss 34.97 
2024-02-08 21:04:14,210 EPOCH 135
2024-02-08 21:04:24,830 Epoch 135: Total Training Recognition Loss 0.50  Total Training Translation Loss 32.16 
2024-02-08 21:04:24,831 EPOCH 136
2024-02-08 21:04:25,762 [Epoch: 136 Step: 00002300] Batch Recognition Loss:   0.019672 => Gls Tokens per Sec:     3441 || Batch Translation Loss:   2.161949 => Txt Tokens per Sec:     7778 || Lr: 0.000100
2024-02-08 21:04:35,679 Epoch 136: Total Training Recognition Loss 0.49  Total Training Translation Loss 30.99 
2024-02-08 21:04:35,680 EPOCH 137
2024-02-08 21:04:46,508 Epoch 137: Total Training Recognition Loss 0.52  Total Training Translation Loss 29.06 
2024-02-08 21:04:46,509 EPOCH 138
2024-02-08 21:04:57,326 Epoch 138: Total Training Recognition Loss 0.51  Total Training Translation Loss 28.01 
2024-02-08 21:04:57,327 EPOCH 139
2024-02-08 21:05:08,279 Epoch 139: Total Training Recognition Loss 0.47  Total Training Translation Loss 26.86 
2024-02-08 21:05:08,280 EPOCH 140
2024-02-08 21:05:19,291 Epoch 140: Total Training Recognition Loss 0.43  Total Training Translation Loss 25.81 
2024-02-08 21:05:19,292 EPOCH 141
2024-02-08 21:05:29,887 Epoch 141: Total Training Recognition Loss 0.45  Total Training Translation Loss 25.56 
2024-02-08 21:05:29,887 EPOCH 142
2024-02-08 21:05:30,632 [Epoch: 142 Step: 00002400] Batch Recognition Loss:   0.020266 => Gls Tokens per Sec:     2584 || Batch Translation Loss:   1.617841 => Txt Tokens per Sec:     7081 || Lr: 0.000100
2024-02-08 21:05:40,637 Epoch 142: Total Training Recognition Loss 0.46  Total Training Translation Loss 24.86 
2024-02-08 21:05:40,638 EPOCH 143
2024-02-08 21:05:51,410 Epoch 143: Total Training Recognition Loss 0.41  Total Training Translation Loss 23.75 
2024-02-08 21:05:51,411 EPOCH 144
2024-02-08 21:06:02,025 Epoch 144: Total Training Recognition Loss 0.43  Total Training Translation Loss 23.00 
2024-02-08 21:06:02,026 EPOCH 145
2024-02-08 21:06:12,936 Epoch 145: Total Training Recognition Loss 0.43  Total Training Translation Loss 22.72 
2024-02-08 21:06:12,937 EPOCH 146
2024-02-08 21:06:23,305 Epoch 146: Total Training Recognition Loss 0.42  Total Training Translation Loss 22.14 
2024-02-08 21:06:23,306 EPOCH 147
2024-02-08 21:06:34,133 Epoch 147: Total Training Recognition Loss 0.41  Total Training Translation Loss 22.62 
2024-02-08 21:06:34,134 EPOCH 148
2024-02-08 21:06:34,389 [Epoch: 148 Step: 00002500] Batch Recognition Loss:   0.020597 => Gls Tokens per Sec:     2530 || Batch Translation Loss:   1.320155 => Txt Tokens per Sec:     7178 || Lr: 0.000100
2024-02-08 21:06:44,877 Epoch 148: Total Training Recognition Loss 0.38  Total Training Translation Loss 22.18 
2024-02-08 21:06:44,877 EPOCH 149
2024-02-08 21:06:55,828 Epoch 149: Total Training Recognition Loss 0.39  Total Training Translation Loss 20.82 
2024-02-08 21:06:55,828 EPOCH 150
2024-02-08 21:07:06,735 Epoch 150: Total Training Recognition Loss 0.37  Total Training Translation Loss 20.46 
2024-02-08 21:07:06,735 EPOCH 151
2024-02-08 21:07:17,390 Epoch 151: Total Training Recognition Loss 0.36  Total Training Translation Loss 19.74 
2024-02-08 21:07:17,391 EPOCH 152
2024-02-08 21:07:28,360 Epoch 152: Total Training Recognition Loss 0.36  Total Training Translation Loss 19.38 
2024-02-08 21:07:28,361 EPOCH 153
2024-02-08 21:07:39,243 [Epoch: 153 Step: 00002600] Batch Recognition Loss:   0.015170 => Gls Tokens per Sec:      917 || Batch Translation Loss:   1.102311 => Txt Tokens per Sec:     2527 || Lr: 0.000100
2024-02-08 21:07:39,491 Epoch 153: Total Training Recognition Loss 0.36  Total Training Translation Loss 18.57 
2024-02-08 21:07:39,491 EPOCH 154
2024-02-08 21:07:50,169 Epoch 154: Total Training Recognition Loss 0.31  Total Training Translation Loss 18.12 
2024-02-08 21:07:50,170 EPOCH 155
2024-02-08 21:08:00,720 Epoch 155: Total Training Recognition Loss 0.35  Total Training Translation Loss 17.33 
2024-02-08 21:08:00,720 EPOCH 156
2024-02-08 21:08:11,495 Epoch 156: Total Training Recognition Loss 0.32  Total Training Translation Loss 17.14 
2024-02-08 21:08:11,496 EPOCH 157
2024-02-08 21:08:22,328 Epoch 157: Total Training Recognition Loss 0.37  Total Training Translation Loss 17.12 
2024-02-08 21:08:22,329 EPOCH 158
2024-02-08 21:08:33,166 Epoch 158: Total Training Recognition Loss 0.33  Total Training Translation Loss 16.58 
2024-02-08 21:08:33,167 EPOCH 159
2024-02-08 21:08:41,808 [Epoch: 159 Step: 00002700] Batch Recognition Loss:   0.017439 => Gls Tokens per Sec:     1007 || Batch Translation Loss:   1.289833 => Txt Tokens per Sec:     2752 || Lr: 0.000100
2024-02-08 21:08:44,020 Epoch 159: Total Training Recognition Loss 0.33  Total Training Translation Loss 16.14 
2024-02-08 21:08:44,021 EPOCH 160
2024-02-08 21:08:54,843 Epoch 160: Total Training Recognition Loss 0.37  Total Training Translation Loss 15.79 
2024-02-08 21:08:54,843 EPOCH 161
2024-02-08 21:09:05,824 Epoch 161: Total Training Recognition Loss 0.33  Total Training Translation Loss 16.04 
2024-02-08 21:09:05,824 EPOCH 162
2024-02-08 21:09:16,633 Epoch 162: Total Training Recognition Loss 0.31  Total Training Translation Loss 16.27 
2024-02-08 21:09:16,634 EPOCH 163
2024-02-08 21:09:27,200 Epoch 163: Total Training Recognition Loss 0.31  Total Training Translation Loss 15.38 
2024-02-08 21:09:27,200 EPOCH 164
2024-02-08 21:09:37,830 Epoch 164: Total Training Recognition Loss 0.29  Total Training Translation Loss 15.35 
2024-02-08 21:09:37,831 EPOCH 165
2024-02-08 21:09:45,727 [Epoch: 165 Step: 00002800] Batch Recognition Loss:   0.013821 => Gls Tokens per Sec:      940 || Batch Translation Loss:   0.946653 => Txt Tokens per Sec:     2593 || Lr: 0.000100
2024-02-08 21:09:48,692 Epoch 165: Total Training Recognition Loss 0.30  Total Training Translation Loss 14.70 
2024-02-08 21:09:48,692 EPOCH 166
2024-02-08 21:09:59,414 Epoch 166: Total Training Recognition Loss 0.31  Total Training Translation Loss 14.38 
2024-02-08 21:09:59,414 EPOCH 167
2024-02-08 21:10:10,232 Epoch 167: Total Training Recognition Loss 0.28  Total Training Translation Loss 13.63 
2024-02-08 21:10:10,233 EPOCH 168
2024-02-08 21:10:20,428 Epoch 168: Total Training Recognition Loss 0.28  Total Training Translation Loss 13.59 
2024-02-08 21:10:20,428 EPOCH 169
2024-02-08 21:10:31,255 Epoch 169: Total Training Recognition Loss 0.31  Total Training Translation Loss 13.11 
2024-02-08 21:10:31,256 EPOCH 170
2024-02-08 21:10:41,650 Epoch 170: Total Training Recognition Loss 0.27  Total Training Translation Loss 12.83 
2024-02-08 21:10:41,651 EPOCH 171
2024-02-08 21:10:45,236 [Epoch: 171 Step: 00002900] Batch Recognition Loss:   0.018920 => Gls Tokens per Sec:     1786 || Batch Translation Loss:   0.470193 => Txt Tokens per Sec:     4676 || Lr: 0.000100
2024-02-08 21:10:52,365 Epoch 171: Total Training Recognition Loss 0.29  Total Training Translation Loss 13.06 
2024-02-08 21:10:52,365 EPOCH 172
2024-02-08 21:11:03,022 Epoch 172: Total Training Recognition Loss 0.32  Total Training Translation Loss 12.98 
2024-02-08 21:11:03,023 EPOCH 173
2024-02-08 21:11:14,135 Epoch 173: Total Training Recognition Loss 0.31  Total Training Translation Loss 12.95 
2024-02-08 21:11:14,136 EPOCH 174
2024-02-08 21:11:24,792 Epoch 174: Total Training Recognition Loss 0.26  Total Training Translation Loss 11.90 
2024-02-08 21:11:24,793 EPOCH 175
2024-02-08 21:11:35,727 Epoch 175: Total Training Recognition Loss 0.25  Total Training Translation Loss 11.45 
2024-02-08 21:11:35,728 EPOCH 176
2024-02-08 21:11:46,628 Epoch 176: Total Training Recognition Loss 0.26  Total Training Translation Loss 11.07 
2024-02-08 21:11:46,628 EPOCH 177
2024-02-08 21:11:55,651 [Epoch: 177 Step: 00003000] Batch Recognition Loss:   0.014843 => Gls Tokens per Sec:      539 || Batch Translation Loss:   0.803281 => Txt Tokens per Sec:     1724 || Lr: 0.000100
2024-02-08 21:11:57,468 Epoch 177: Total Training Recognition Loss 0.23  Total Training Translation Loss 11.10 
2024-02-08 21:11:57,469 EPOCH 178
2024-02-08 21:12:08,411 Epoch 178: Total Training Recognition Loss 0.21  Total Training Translation Loss 11.07 
2024-02-08 21:12:08,411 EPOCH 179
2024-02-08 21:12:19,057 Epoch 179: Total Training Recognition Loss 0.25  Total Training Translation Loss 10.36 
2024-02-08 21:12:19,058 EPOCH 180
2024-02-08 21:12:30,024 Epoch 180: Total Training Recognition Loss 0.26  Total Training Translation Loss 10.22 
2024-02-08 21:12:30,024 EPOCH 181
2024-02-08 21:12:40,727 Epoch 181: Total Training Recognition Loss 0.22  Total Training Translation Loss 10.45 
2024-02-08 21:12:40,728 EPOCH 182
2024-02-08 21:12:51,018 Epoch 182: Total Training Recognition Loss 0.21  Total Training Translation Loss 10.27 
2024-02-08 21:12:51,019 EPOCH 183
2024-02-08 21:12:53,788 [Epoch: 183 Step: 00003100] Batch Recognition Loss:   0.008844 => Gls Tokens per Sec:     1387 || Batch Translation Loss:   0.693063 => Txt Tokens per Sec:     3698 || Lr: 0.000100
2024-02-08 21:13:01,585 Epoch 183: Total Training Recognition Loss 0.19  Total Training Translation Loss 10.06 
2024-02-08 21:13:01,586 EPOCH 184
2024-02-08 21:13:12,157 Epoch 184: Total Training Recognition Loss 0.21  Total Training Translation Loss 9.66 
2024-02-08 21:13:12,158 EPOCH 185
2024-02-08 21:13:22,891 Epoch 185: Total Training Recognition Loss 0.20  Total Training Translation Loss 10.04 
2024-02-08 21:13:22,892 EPOCH 186
2024-02-08 21:13:33,500 Epoch 186: Total Training Recognition Loss 0.20  Total Training Translation Loss 10.24 
2024-02-08 21:13:33,501 EPOCH 187
2024-02-08 21:13:44,312 Epoch 187: Total Training Recognition Loss 0.21  Total Training Translation Loss 10.18 
2024-02-08 21:13:44,313 EPOCH 188
2024-02-08 21:13:54,924 Epoch 188: Total Training Recognition Loss 0.19  Total Training Translation Loss 9.75 
2024-02-08 21:13:54,925 EPOCH 189
2024-02-08 21:13:57,775 [Epoch: 189 Step: 00003200] Batch Recognition Loss:   0.007159 => Gls Tokens per Sec:      807 || Batch Translation Loss:   0.604762 => Txt Tokens per Sec:     2197 || Lr: 0.000100
2024-02-08 21:14:05,529 Epoch 189: Total Training Recognition Loss 0.18  Total Training Translation Loss 9.60 
2024-02-08 21:14:05,530 EPOCH 190
2024-02-08 21:14:16,204 Epoch 190: Total Training Recognition Loss 0.22  Total Training Translation Loss 9.39 
2024-02-08 21:14:16,205 EPOCH 191
2024-02-08 21:14:26,904 Epoch 191: Total Training Recognition Loss 0.18  Total Training Translation Loss 9.31 
2024-02-08 21:14:26,905 EPOCH 192
2024-02-08 21:14:37,623 Epoch 192: Total Training Recognition Loss 0.19  Total Training Translation Loss 9.30 
2024-02-08 21:14:37,624 EPOCH 193
2024-02-08 21:14:48,260 Epoch 193: Total Training Recognition Loss 0.20  Total Training Translation Loss 9.66 
2024-02-08 21:14:48,261 EPOCH 194
2024-02-08 21:14:58,965 Epoch 194: Total Training Recognition Loss 0.20  Total Training Translation Loss 9.28 
2024-02-08 21:14:58,965 EPOCH 195
2024-02-08 21:15:00,893 [Epoch: 195 Step: 00003300] Batch Recognition Loss:   0.010328 => Gls Tokens per Sec:      664 || Batch Translation Loss:   0.504697 => Txt Tokens per Sec:     2051 || Lr: 0.000100
2024-02-08 21:15:09,810 Epoch 195: Total Training Recognition Loss 0.19  Total Training Translation Loss 9.56 
2024-02-08 21:15:09,810 EPOCH 196
2024-02-08 21:15:20,730 Epoch 196: Total Training Recognition Loss 0.20  Total Training Translation Loss 9.12 
2024-02-08 21:15:20,731 EPOCH 197
2024-02-08 21:15:32,970 Epoch 197: Total Training Recognition Loss 0.20  Total Training Translation Loss 8.75 
2024-02-08 21:15:32,970 EPOCH 198
2024-02-08 21:15:43,684 Epoch 198: Total Training Recognition Loss 0.23  Total Training Translation Loss 8.67 
2024-02-08 21:15:43,684 EPOCH 199
2024-02-08 21:15:54,320 Epoch 199: Total Training Recognition Loss 0.19  Total Training Translation Loss 8.37 
2024-02-08 21:15:54,321 EPOCH 200
2024-02-08 21:16:05,209 [Epoch: 200 Step: 00003400] Batch Recognition Loss:   0.006579 => Gls Tokens per Sec:      975 || Batch Translation Loss:   0.479568 => Txt Tokens per Sec:     2699 || Lr: 0.000100
2024-02-08 21:16:05,210 Epoch 200: Total Training Recognition Loss 0.15  Total Training Translation Loss 8.61 
2024-02-08 21:16:05,210 EPOCH 201
2024-02-08 21:16:16,252 Epoch 201: Total Training Recognition Loss 0.20  Total Training Translation Loss 8.50 
2024-02-08 21:16:16,253 EPOCH 202
2024-02-08 21:16:26,966 Epoch 202: Total Training Recognition Loss 0.18  Total Training Translation Loss 8.83 
2024-02-08 21:16:26,967 EPOCH 203
2024-02-08 21:16:37,812 Epoch 203: Total Training Recognition Loss 0.18  Total Training Translation Loss 8.15 
2024-02-08 21:16:37,812 EPOCH 204
2024-02-08 21:16:48,339 Epoch 204: Total Training Recognition Loss 0.17  Total Training Translation Loss 7.86 
2024-02-08 21:16:48,339 EPOCH 205
2024-02-08 21:16:59,036 Epoch 205: Total Training Recognition Loss 0.18  Total Training Translation Loss 7.59 
2024-02-08 21:16:59,036 EPOCH 206
2024-02-08 21:17:09,358 [Epoch: 206 Step: 00003500] Batch Recognition Loss:   0.011088 => Gls Tokens per Sec:      905 || Batch Translation Loss:   0.548032 => Txt Tokens per Sec:     2513 || Lr: 0.000100
2024-02-08 21:17:09,839 Epoch 206: Total Training Recognition Loss 0.22  Total Training Translation Loss 7.35 
2024-02-08 21:17:09,839 EPOCH 207
2024-02-08 21:17:20,594 Epoch 207: Total Training Recognition Loss 0.20  Total Training Translation Loss 7.23 
2024-02-08 21:17:20,595 EPOCH 208
2024-02-08 21:17:31,113 Epoch 208: Total Training Recognition Loss 0.16  Total Training Translation Loss 6.71 
2024-02-08 21:17:31,114 EPOCH 209
2024-02-08 21:17:41,710 Epoch 209: Total Training Recognition Loss 0.15  Total Training Translation Loss 6.54 
2024-02-08 21:17:41,711 EPOCH 210
2024-02-08 21:17:52,512 Epoch 210: Total Training Recognition Loss 0.17  Total Training Translation Loss 6.69 
2024-02-08 21:17:52,512 EPOCH 211
2024-02-08 21:18:02,820 Epoch 211: Total Training Recognition Loss 0.17  Total Training Translation Loss 6.71 
2024-02-08 21:18:02,821 EPOCH 212
2024-02-08 21:18:12,749 [Epoch: 212 Step: 00003600] Batch Recognition Loss:   0.020047 => Gls Tokens per Sec:      812 || Batch Translation Loss:   0.539104 => Txt Tokens per Sec:     2258 || Lr: 0.000100
2024-02-08 21:18:13,710 Epoch 212: Total Training Recognition Loss 0.18  Total Training Translation Loss 6.53 
2024-02-08 21:18:13,711 EPOCH 213
2024-02-08 21:18:24,442 Epoch 213: Total Training Recognition Loss 0.16  Total Training Translation Loss 6.15 
2024-02-08 21:18:24,443 EPOCH 214
2024-02-08 21:18:35,179 Epoch 214: Total Training Recognition Loss 0.13  Total Training Translation Loss 6.25 
2024-02-08 21:18:35,180 EPOCH 215
2024-02-08 21:18:46,026 Epoch 215: Total Training Recognition Loss 0.15  Total Training Translation Loss 5.84 
2024-02-08 21:18:46,027 EPOCH 216
2024-02-08 21:18:56,823 Epoch 216: Total Training Recognition Loss 0.14  Total Training Translation Loss 5.77 
2024-02-08 21:18:56,824 EPOCH 217
2024-02-08 21:19:07,532 Epoch 217: Total Training Recognition Loss 0.14  Total Training Translation Loss 5.82 
2024-02-08 21:19:07,533 EPOCH 218
2024-02-08 21:19:14,733 [Epoch: 218 Step: 00003700] Batch Recognition Loss:   0.013974 => Gls Tokens per Sec:      942 || Batch Translation Loss:   0.260702 => Txt Tokens per Sec:     2489 || Lr: 0.000100
2024-02-08 21:19:18,316 Epoch 218: Total Training Recognition Loss 0.13  Total Training Translation Loss 5.70 
2024-02-08 21:19:18,316 EPOCH 219
2024-02-08 21:19:29,213 Epoch 219: Total Training Recognition Loss 0.16  Total Training Translation Loss 5.81 
2024-02-08 21:19:29,213 EPOCH 220
2024-02-08 21:19:40,123 Epoch 220: Total Training Recognition Loss 0.13  Total Training Translation Loss 5.47 
2024-02-08 21:19:40,124 EPOCH 221
2024-02-08 21:19:50,887 Epoch 221: Total Training Recognition Loss 0.13  Total Training Translation Loss 5.41 
2024-02-08 21:19:50,888 EPOCH 222
2024-02-08 21:20:01,608 Epoch 222: Total Training Recognition Loss 0.13  Total Training Translation Loss 5.54 
2024-02-08 21:20:01,609 EPOCH 223
2024-02-08 21:20:12,172 Epoch 223: Total Training Recognition Loss 0.16  Total Training Translation Loss 6.01 
2024-02-08 21:20:12,173 EPOCH 224
2024-02-08 21:20:19,725 [Epoch: 224 Step: 00003800] Batch Recognition Loss:   0.012053 => Gls Tokens per Sec:      728 || Batch Translation Loss:   0.445773 => Txt Tokens per Sec:     2127 || Lr: 0.000100
2024-02-08 21:20:22,958 Epoch 224: Total Training Recognition Loss 0.12  Total Training Translation Loss 5.92 
2024-02-08 21:20:22,959 EPOCH 225
2024-02-08 21:20:33,709 Epoch 225: Total Training Recognition Loss 0.15  Total Training Translation Loss 6.24 
2024-02-08 21:20:33,710 EPOCH 226
2024-02-08 21:20:44,163 Epoch 226: Total Training Recognition Loss 0.13  Total Training Translation Loss 6.52 
2024-02-08 21:20:44,164 EPOCH 227
2024-02-08 21:20:54,842 Epoch 227: Total Training Recognition Loss 0.16  Total Training Translation Loss 7.82 
2024-02-08 21:20:54,842 EPOCH 228
2024-02-08 21:21:05,821 Epoch 228: Total Training Recognition Loss 0.14  Total Training Translation Loss 8.22 
2024-02-08 21:21:05,822 EPOCH 229
2024-02-08 21:21:16,652 Epoch 229: Total Training Recognition Loss 0.13  Total Training Translation Loss 7.34 
2024-02-08 21:21:16,653 EPOCH 230
2024-02-08 21:21:21,206 [Epoch: 230 Step: 00003900] Batch Recognition Loss:   0.003539 => Gls Tokens per Sec:      984 || Batch Translation Loss:   0.364005 => Txt Tokens per Sec:     2686 || Lr: 0.000100
2024-02-08 21:21:27,454 Epoch 230: Total Training Recognition Loss 0.14  Total Training Translation Loss 6.60 
2024-02-08 21:21:27,454 EPOCH 231
2024-02-08 21:21:38,461 Epoch 231: Total Training Recognition Loss 0.13  Total Training Translation Loss 6.33 
2024-02-08 21:21:38,461 EPOCH 232
2024-02-08 21:21:49,513 Epoch 232: Total Training Recognition Loss 0.14  Total Training Translation Loss 6.35 
2024-02-08 21:21:49,513 EPOCH 233
2024-02-08 21:22:00,240 Epoch 233: Total Training Recognition Loss 0.15  Total Training Translation Loss 6.09 
2024-02-08 21:22:00,240 EPOCH 234
2024-02-08 21:22:11,040 Epoch 234: Total Training Recognition Loss 0.15  Total Training Translation Loss 5.55 
2024-02-08 21:22:11,040 EPOCH 235
2024-02-08 21:22:21,887 Epoch 235: Total Training Recognition Loss 0.13  Total Training Translation Loss 5.23 
2024-02-08 21:22:21,887 EPOCH 236
2024-02-08 21:22:26,654 [Epoch: 236 Step: 00004000] Batch Recognition Loss:   0.008531 => Gls Tokens per Sec:      617 || Batch Translation Loss:   0.318206 => Txt Tokens per Sec:     1722 || Lr: 0.000100
2024-02-08 21:23:07,087 Validation result at epoch 236, step     4000: duration: 40.4313s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.03647	Translation Loss: 81619.95312	PPL: 3471.04932
	Eval Metric: BLEU
	WER 5.44	(DEL: 0.00,	INS: 0.00,	SUB: 5.44)
	BLEU-4 0.78	(BLEU-1: 12.32,	BLEU-2: 4.14,	BLEU-3: 1.64,	BLEU-4: 0.78)
	CHRF 17.50	ROUGE 10.29
2024-02-08 21:23:07,088 Logging Recognition and Translation Outputs
2024-02-08 21:23:07,088 ========================================================================================================================
2024-02-08 21:23:07,088 Logging Sequence: 112_165.00
2024-02-08 21:23:07,089 	Gloss Reference :	A B+C+D+E
2024-02-08 21:23:07,089 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 21:23:07,089 	Gloss Alignment :	         
2024-02-08 21:23:07,089 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 21:23:07,090 	Text Reference  :	** *** **** **** the       narendra modi        stadium will   be   the   home   for the    ahmedabad-based franchise
2024-02-08 21:23:07,090 	Text Hypothesis :	do you know that wikipedia provides information on      celebs like their height age family background      etc      
2024-02-08 21:23:07,090 	Text Alignment  :	I  I   I    I    S         S        S           S       S      S    S     S      S   S      S               S        
2024-02-08 21:23:07,091 ========================================================================================================================
2024-02-08 21:23:07,091 Logging Sequence: 176_154.00
2024-02-08 21:23:07,091 	Gloss Reference :	A B+C+D+E
2024-02-08 21:23:07,091 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 21:23:07,091 	Gloss Alignment :	         
2024-02-08 21:23:07,091 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 21:23:07,092 	Text Reference  :	******* ****** dahiya could potentially bring home india's second gold   medal
2024-02-08 21:23:07,092 	Text Hypothesis :	abhinav bindra was    the   first       time  to   win     a      silver medal
2024-02-08 21:23:07,093 	Text Alignment  :	I       I      S      S     S           S     S    S       S      S           
2024-02-08 21:23:07,093 ========================================================================================================================
2024-02-08 21:23:07,093 Logging Sequence: 94_2.00
2024-02-08 21:23:07,093 	Gloss Reference :	A B+C+D+E
2024-02-08 21:23:07,093 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 21:23:07,093 	Gloss Alignment :	         
2024-02-08 21:23:07,093 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 21:23:07,095 	Text Reference  :	****** ******* **** ******* the   icc   odi    men' world cup 2023 will be hosted by      india on 5th october 2023    
2024-02-08 21:23:07,095 	Text Hypothesis :	indian cricket team chennai super kings during the  world cup **** **** ** match  between india ** *** and     pakistan
2024-02-08 21:23:07,095 	Text Alignment  :	I      I       I    I       S     S     S      S              D    D    D  S      S             D  D   S       S       
2024-02-08 21:23:07,095 ========================================================================================================================
2024-02-08 21:23:07,096 Logging Sequence: 165_453.00
2024-02-08 21:23:07,096 	Gloss Reference :	A B+C+D+E
2024-02-08 21:23:07,096 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 21:23:07,096 	Gloss Alignment :	         
2024-02-08 21:23:07,096 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 21:23:07,098 	Text Reference  :	*** **** ******* icc  did not     agree to **** ** *** ******** sehwag' decision of  wearing a  numberless jersey
2024-02-08 21:23:07,098 	Text Hypothesis :	his team members have a   strange due   to this is his accident and     did      not want    it is         more  
2024-02-08 21:23:07,098 	Text Alignment  :	I   I    I       S    S   S       S        I    I  I   I        S       S        S   S       S  S          S     
2024-02-08 21:23:07,098 ========================================================================================================================
2024-02-08 21:23:07,098 Logging Sequence: 139_46.00
2024-02-08 21:23:07,099 	Gloss Reference :	A B+C+D+E
2024-02-08 21:23:07,099 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 21:23:07,099 	Gloss Alignment :	         
2024-02-08 21:23:07,099 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 21:23:07,101 	Text Reference  :	everyone thought it would be a       one sided  match because morocco is an amateur team  and ****** belgium ranks 2nd in the world
2024-02-08 21:23:07,101 	Text Hypothesis :	******** ******* ** ***** ** however the second match ******* ******* ** ** between india and kuwait at      the   end of the qatar
2024-02-08 21:23:07,101 	Text Alignment  :	D        D       D  D     D  S       S   S            D       D       D  D  S       S         I      S       S     S   S      S    
2024-02-08 21:23:07,101 ========================================================================================================================
2024-02-08 21:23:13,339 Epoch 236: Total Training Recognition Loss 0.11  Total Training Translation Loss 4.76 
2024-02-08 21:23:13,339 EPOCH 237
2024-02-08 21:23:24,163 Epoch 237: Total Training Recognition Loss 0.11  Total Training Translation Loss 5.23 
2024-02-08 21:23:24,164 EPOCH 238
2024-02-08 21:23:34,619 Epoch 238: Total Training Recognition Loss 0.14  Total Training Translation Loss 6.83 
2024-02-08 21:23:34,620 EPOCH 239
2024-02-08 21:23:45,113 Epoch 239: Total Training Recognition Loss 0.13  Total Training Translation Loss 12.22 
2024-02-08 21:23:45,113 EPOCH 240
2024-02-08 21:23:56,074 Epoch 240: Total Training Recognition Loss 0.20  Total Training Translation Loss 12.20 
2024-02-08 21:23:56,074 EPOCH 241
2024-02-08 21:24:06,624 Epoch 241: Total Training Recognition Loss 0.17  Total Training Translation Loss 8.43 
2024-02-08 21:24:06,624 EPOCH 242
2024-02-08 21:24:07,232 [Epoch: 242 Step: 00004100] Batch Recognition Loss:   0.004099 => Gls Tokens per Sec:     3168 || Batch Translation Loss:   0.453553 => Txt Tokens per Sec:     8127 || Lr: 0.000100
2024-02-08 21:24:17,111 Epoch 242: Total Training Recognition Loss 0.15  Total Training Translation Loss 6.72 
2024-02-08 21:24:17,112 EPOCH 243
2024-02-08 21:24:27,913 Epoch 243: Total Training Recognition Loss 0.12  Total Training Translation Loss 5.81 
2024-02-08 21:24:27,914 EPOCH 244
2024-02-08 21:24:38,654 Epoch 244: Total Training Recognition Loss 0.12  Total Training Translation Loss 5.67 
2024-02-08 21:24:38,655 EPOCH 245
2024-02-08 21:24:49,363 Epoch 245: Total Training Recognition Loss 0.14  Total Training Translation Loss 5.26 
2024-02-08 21:24:49,363 EPOCH 246
2024-02-08 21:25:00,167 Epoch 246: Total Training Recognition Loss 0.16  Total Training Translation Loss 4.48 
2024-02-08 21:25:00,168 EPOCH 247
2024-02-08 21:25:10,737 Epoch 247: Total Training Recognition Loss 0.12  Total Training Translation Loss 4.19 
2024-02-08 21:25:10,738 EPOCH 248
2024-02-08 21:25:10,831 [Epoch: 248 Step: 00004200] Batch Recognition Loss:   0.011423 => Gls Tokens per Sec:     7033 || Batch Translation Loss:   0.090866 => Txt Tokens per Sec:    11001 || Lr: 0.000100
2024-02-08 21:25:21,627 Epoch 248: Total Training Recognition Loss 0.13  Total Training Translation Loss 3.88 
2024-02-08 21:25:21,628 EPOCH 249
2024-02-08 21:25:32,290 Epoch 249: Total Training Recognition Loss 0.11  Total Training Translation Loss 3.83 
2024-02-08 21:25:32,290 EPOCH 250
2024-02-08 21:25:43,081 Epoch 250: Total Training Recognition Loss 0.10  Total Training Translation Loss 3.72 
2024-02-08 21:25:43,082 EPOCH 251
2024-02-08 21:25:53,836 Epoch 251: Total Training Recognition Loss 0.11  Total Training Translation Loss 3.78 
2024-02-08 21:25:53,837 EPOCH 252
2024-02-08 21:26:04,527 Epoch 252: Total Training Recognition Loss 0.11  Total Training Translation Loss 3.50 
2024-02-08 21:26:04,528 EPOCH 253
2024-02-08 21:26:13,308 [Epoch: 253 Step: 00004300] Batch Recognition Loss:   0.006292 => Gls Tokens per Sec:     1137 || Batch Translation Loss:   0.191218 => Txt Tokens per Sec:     3098 || Lr: 0.000100
2024-02-08 21:26:15,076 Epoch 253: Total Training Recognition Loss 0.10  Total Training Translation Loss 3.47 
2024-02-08 21:26:15,077 EPOCH 254
2024-02-08 21:26:27,019 Epoch 254: Total Training Recognition Loss 0.10  Total Training Translation Loss 3.55 
2024-02-08 21:26:27,020 EPOCH 255
2024-02-08 21:26:37,768 Epoch 255: Total Training Recognition Loss 0.09  Total Training Translation Loss 3.46 
2024-02-08 21:26:37,769 EPOCH 256
2024-02-08 21:26:48,881 Epoch 256: Total Training Recognition Loss 0.10  Total Training Translation Loss 3.42 
2024-02-08 21:26:48,881 EPOCH 257
2024-02-08 21:26:59,732 Epoch 257: Total Training Recognition Loss 0.11  Total Training Translation Loss 3.43 
2024-02-08 21:26:59,732 EPOCH 258
2024-02-08 21:27:10,191 Epoch 258: Total Training Recognition Loss 0.10  Total Training Translation Loss 3.63 
2024-02-08 21:27:10,191 EPOCH 259
2024-02-08 21:27:16,368 [Epoch: 259 Step: 00004400] Batch Recognition Loss:   0.003307 => Gls Tokens per Sec:     1451 || Batch Translation Loss:   0.149649 => Txt Tokens per Sec:     3921 || Lr: 0.000100
2024-02-08 21:27:20,916 Epoch 259: Total Training Recognition Loss 0.08  Total Training Translation Loss 3.63 
2024-02-08 21:27:20,917 EPOCH 260
2024-02-08 21:27:31,846 Epoch 260: Total Training Recognition Loss 0.10  Total Training Translation Loss 3.43 
2024-02-08 21:27:31,847 EPOCH 261
2024-02-08 21:27:42,741 Epoch 261: Total Training Recognition Loss 0.10  Total Training Translation Loss 3.46 
2024-02-08 21:27:42,742 EPOCH 262
2024-02-08 21:27:53,502 Epoch 262: Total Training Recognition Loss 0.10  Total Training Translation Loss 3.51 
2024-02-08 21:27:53,503 EPOCH 263
2024-02-08 21:28:04,337 Epoch 263: Total Training Recognition Loss 0.11  Total Training Translation Loss 3.62 
2024-02-08 21:28:04,338 EPOCH 264
2024-02-08 21:28:14,964 Epoch 264: Total Training Recognition Loss 0.09  Total Training Translation Loss 3.40 
2024-02-08 21:28:14,965 EPOCH 265
2024-02-08 21:28:22,108 [Epoch: 265 Step: 00004500] Batch Recognition Loss:   0.011530 => Gls Tokens per Sec:     1075 || Batch Translation Loss:   0.091004 => Txt Tokens per Sec:     2876 || Lr: 0.000100
2024-02-08 21:28:25,619 Epoch 265: Total Training Recognition Loss 0.09  Total Training Translation Loss 3.39 
2024-02-08 21:28:25,620 EPOCH 266
2024-02-08 21:28:36,110 Epoch 266: Total Training Recognition Loss 0.08  Total Training Translation Loss 3.47 
2024-02-08 21:28:36,110 EPOCH 267
2024-02-08 21:28:46,795 Epoch 267: Total Training Recognition Loss 0.10  Total Training Translation Loss 3.57 
2024-02-08 21:28:46,795 EPOCH 268
2024-02-08 21:28:57,334 Epoch 268: Total Training Recognition Loss 0.09  Total Training Translation Loss 5.32 
2024-02-08 21:28:57,335 EPOCH 269
2024-02-08 21:29:07,924 Epoch 269: Total Training Recognition Loss 0.21  Total Training Translation Loss 31.65 
2024-02-08 21:29:07,925 EPOCH 270
2024-02-08 21:29:18,330 Epoch 270: Total Training Recognition Loss 0.37  Total Training Translation Loss 26.46 
2024-02-08 21:29:18,331 EPOCH 271
2024-02-08 21:29:24,297 [Epoch: 271 Step: 00004600] Batch Recognition Loss:   0.011001 => Gls Tokens per Sec:     1029 || Batch Translation Loss:   0.698493 => Txt Tokens per Sec:     2824 || Lr: 0.000100
2024-02-08 21:29:29,186 Epoch 271: Total Training Recognition Loss 0.27  Total Training Translation Loss 14.49 
2024-02-08 21:29:29,187 EPOCH 272
2024-02-08 21:29:39,987 Epoch 272: Total Training Recognition Loss 0.21  Total Training Translation Loss 8.32 
2024-02-08 21:29:39,987 EPOCH 273
2024-02-08 21:29:50,444 Epoch 273: Total Training Recognition Loss 0.17  Total Training Translation Loss 5.04 
2024-02-08 21:29:50,445 EPOCH 274
2024-02-08 21:30:01,455 Epoch 274: Total Training Recognition Loss 0.16  Total Training Translation Loss 4.30 
2024-02-08 21:30:01,456 EPOCH 275
2024-02-08 21:30:12,302 Epoch 275: Total Training Recognition Loss 0.14  Total Training Translation Loss 3.74 
2024-02-08 21:30:12,303 EPOCH 276
2024-02-08 21:30:23,098 Epoch 276: Total Training Recognition Loss 0.12  Total Training Translation Loss 3.43 
2024-02-08 21:30:23,098 EPOCH 277
2024-02-08 21:30:26,232 [Epoch: 277 Step: 00004700] Batch Recognition Loss:   0.005576 => Gls Tokens per Sec:     1634 || Batch Translation Loss:   0.160332 => Txt Tokens per Sec:     4260 || Lr: 0.000100
2024-02-08 21:30:33,620 Epoch 277: Total Training Recognition Loss 0.13  Total Training Translation Loss 2.97 
2024-02-08 21:30:33,621 EPOCH 278
2024-02-08 21:30:44,270 Epoch 278: Total Training Recognition Loss 0.10  Total Training Translation Loss 2.89 
2024-02-08 21:30:44,271 EPOCH 279
2024-02-08 21:30:54,991 Epoch 279: Total Training Recognition Loss 0.10  Total Training Translation Loss 2.70 
2024-02-08 21:30:54,991 EPOCH 280
2024-02-08 21:31:05,869 Epoch 280: Total Training Recognition Loss 0.09  Total Training Translation Loss 2.67 
2024-02-08 21:31:05,869 EPOCH 281
2024-02-08 21:31:16,455 Epoch 281: Total Training Recognition Loss 0.09  Total Training Translation Loss 2.50 
2024-02-08 21:31:16,456 EPOCH 282
2024-02-08 21:31:27,261 Epoch 282: Total Training Recognition Loss 0.09  Total Training Translation Loss 2.79 
2024-02-08 21:31:27,262 EPOCH 283
2024-02-08 21:31:31,773 [Epoch: 283 Step: 00004800] Batch Recognition Loss:   0.002122 => Gls Tokens per Sec:      851 || Batch Translation Loss:   0.189623 => Txt Tokens per Sec:     2532 || Lr: 0.000100
2024-02-08 21:31:38,055 Epoch 283: Total Training Recognition Loss 0.09  Total Training Translation Loss 2.34 
2024-02-08 21:31:38,055 EPOCH 284
2024-02-08 21:31:48,868 Epoch 284: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.24 
2024-02-08 21:31:48,868 EPOCH 285
2024-02-08 21:31:59,555 Epoch 285: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.16 
2024-02-08 21:31:59,556 EPOCH 286
2024-02-08 21:32:10,356 Epoch 286: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.10 
2024-02-08 21:32:10,357 EPOCH 287
2024-02-08 21:32:20,985 Epoch 287: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.09 
2024-02-08 21:32:20,986 EPOCH 288
2024-02-08 21:32:31,674 Epoch 288: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.95 
2024-02-08 21:32:31,675 EPOCH 289
2024-02-08 21:32:34,464 [Epoch: 289 Step: 00004900] Batch Recognition Loss:   0.001454 => Gls Tokens per Sec:      918 || Batch Translation Loss:   0.118257 => Txt Tokens per Sec:     2751 || Lr: 0.000100
2024-02-08 21:32:42,386 Epoch 289: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.95 
2024-02-08 21:32:42,387 EPOCH 290
2024-02-08 21:32:53,204 Epoch 290: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.00 
2024-02-08 21:32:53,205 EPOCH 291
2024-02-08 21:33:03,671 Epoch 291: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.04 
2024-02-08 21:33:03,671 EPOCH 292
2024-02-08 21:33:13,974 Epoch 292: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.96 
2024-02-08 21:33:13,975 EPOCH 293
2024-02-08 21:33:24,777 Epoch 293: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.93 
2024-02-08 21:33:24,778 EPOCH 294
2024-02-08 21:33:35,515 Epoch 294: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.98 
2024-02-08 21:33:35,515 EPOCH 295
2024-02-08 21:33:35,982 [Epoch: 295 Step: 00005000] Batch Recognition Loss:   0.002629 => Gls Tokens per Sec:     2747 || Batch Translation Loss:   0.132091 => Txt Tokens per Sec:     6586 || Lr: 0.000100
2024-02-08 21:33:46,272 Epoch 295: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.37 
2024-02-08 21:33:46,272 EPOCH 296
2024-02-08 21:33:57,198 Epoch 296: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.29 
2024-02-08 21:33:57,199 EPOCH 297
2024-02-08 21:34:08,071 Epoch 297: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.52 
2024-02-08 21:34:08,071 EPOCH 298
2024-02-08 21:34:18,814 Epoch 298: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.18 
2024-02-08 21:34:18,815 EPOCH 299
2024-02-08 21:34:29,703 Epoch 299: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.10 
2024-02-08 21:34:29,703 EPOCH 300
2024-02-08 21:34:40,335 [Epoch: 300 Step: 00005100] Batch Recognition Loss:   0.002032 => Gls Tokens per Sec:      999 || Batch Translation Loss:   0.131769 => Txt Tokens per Sec:     2764 || Lr: 0.000100
2024-02-08 21:34:40,336 Epoch 300: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.02 
2024-02-08 21:34:40,336 EPOCH 301
2024-02-08 21:34:50,895 Epoch 301: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.92 
2024-02-08 21:34:50,896 EPOCH 302
2024-02-08 21:35:01,725 Epoch 302: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.99 
2024-02-08 21:35:01,725 EPOCH 303
2024-02-08 21:35:12,459 Epoch 303: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.01 
2024-02-08 21:35:12,460 EPOCH 304
2024-02-08 21:35:22,919 Epoch 304: Total Training Recognition Loss 0.09  Total Training Translation Loss 2.25 
2024-02-08 21:35:22,920 EPOCH 305
2024-02-08 21:35:33,819 Epoch 305: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.11 
2024-02-08 21:35:33,819 EPOCH 306
2024-02-08 21:35:42,623 [Epoch: 306 Step: 00005200] Batch Recognition Loss:   0.001761 => Gls Tokens per Sec:     1061 || Batch Translation Loss:   0.088789 => Txt Tokens per Sec:     2894 || Lr: 0.000100
2024-02-08 21:35:44,734 Epoch 306: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.13 
2024-02-08 21:35:44,734 EPOCH 307
2024-02-08 21:35:55,406 Epoch 307: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.93 
2024-02-08 21:35:55,407 EPOCH 308
2024-02-08 21:36:06,167 Epoch 308: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.98 
2024-02-08 21:36:06,168 EPOCH 309
2024-02-08 21:36:16,790 Epoch 309: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.99 
2024-02-08 21:36:16,790 EPOCH 310
2024-02-08 21:36:27,455 Epoch 310: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.16 
2024-02-08 21:36:27,455 EPOCH 311
2024-02-08 21:36:38,150 Epoch 311: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.20 
2024-02-08 21:36:38,150 EPOCH 312
2024-02-08 21:36:44,327 [Epoch: 312 Step: 00005300] Batch Recognition Loss:   0.002229 => Gls Tokens per Sec:     1347 || Batch Translation Loss:   0.104442 => Txt Tokens per Sec:     3550 || Lr: 0.000100
2024-02-08 21:36:49,065 Epoch 312: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.05 
2024-02-08 21:36:49,065 EPOCH 313
2024-02-08 21:36:59,880 Epoch 313: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.99 
2024-02-08 21:36:59,881 EPOCH 314
2024-02-08 21:37:10,328 Epoch 314: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.98 
2024-02-08 21:37:10,328 EPOCH 315
2024-02-08 21:37:20,931 Epoch 315: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.24 
2024-02-08 21:37:20,931 EPOCH 316
2024-02-08 21:37:31,767 Epoch 316: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.00 
2024-02-08 21:37:31,768 EPOCH 317
2024-02-08 21:37:42,771 Epoch 317: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.99 
2024-02-08 21:37:42,772 EPOCH 318
2024-02-08 21:37:50,652 [Epoch: 318 Step: 00005400] Batch Recognition Loss:   0.003124 => Gls Tokens per Sec:      860 || Batch Translation Loss:   0.182480 => Txt Tokens per Sec:     2430 || Lr: 0.000100
2024-02-08 21:37:53,381 Epoch 318: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.08 
2024-02-08 21:37:53,382 EPOCH 319
2024-02-08 21:38:04,103 Epoch 319: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.28 
2024-02-08 21:38:04,104 EPOCH 320
2024-02-08 21:38:15,133 Epoch 320: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.61 
2024-02-08 21:38:15,133 EPOCH 321
2024-02-08 21:38:25,895 Epoch 321: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.55 
2024-02-08 21:38:25,895 EPOCH 322
2024-02-08 21:38:36,742 Epoch 322: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.13 
2024-02-08 21:38:36,743 EPOCH 323
2024-02-08 21:38:47,361 Epoch 323: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.47 
2024-02-08 21:38:47,361 EPOCH 324
2024-02-08 21:38:54,588 [Epoch: 324 Step: 00005500] Batch Recognition Loss:   0.005168 => Gls Tokens per Sec:      761 || Batch Translation Loss:   0.212042 => Txt Tokens per Sec:     2195 || Lr: 0.000100
2024-02-08 21:38:57,910 Epoch 324: Total Training Recognition Loss 0.08  Total Training Translation Loss 3.61 
2024-02-08 21:38:57,911 EPOCH 325
2024-02-08 21:39:08,541 Epoch 325: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.62 
2024-02-08 21:39:08,541 EPOCH 326
2024-02-08 21:39:19,136 Epoch 326: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.67 
2024-02-08 21:39:19,137 EPOCH 327
2024-02-08 21:39:30,123 Epoch 327: Total Training Recognition Loss 0.08  Total Training Translation Loss 3.76 
2024-02-08 21:39:30,123 EPOCH 328
2024-02-08 21:39:40,937 Epoch 328: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.81 
2024-02-08 21:39:40,937 EPOCH 329
2024-02-08 21:39:51,677 Epoch 329: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.55 
2024-02-08 21:39:51,678 EPOCH 330
2024-02-08 21:39:53,306 [Epoch: 330 Step: 00005600] Batch Recognition Loss:   0.002424 => Gls Tokens per Sec:     2754 || Batch Translation Loss:   0.191378 => Txt Tokens per Sec:     7132 || Lr: 0.000100
2024-02-08 21:40:02,547 Epoch 330: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.05 
2024-02-08 21:40:02,547 EPOCH 331
2024-02-08 21:40:13,515 Epoch 331: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.62 
2024-02-08 21:40:13,516 EPOCH 332
2024-02-08 21:40:24,214 Epoch 332: Total Training Recognition Loss 0.09  Total Training Translation Loss 2.81 
2024-02-08 21:40:24,215 EPOCH 333
2024-02-08 21:40:34,892 Epoch 333: Total Training Recognition Loss 0.08  Total Training Translation Loss 3.75 
2024-02-08 21:40:34,892 EPOCH 334
2024-02-08 21:40:45,560 Epoch 334: Total Training Recognition Loss 0.07  Total Training Translation Loss 4.38 
2024-02-08 21:40:45,560 EPOCH 335
2024-02-08 21:40:56,214 Epoch 335: Total Training Recognition Loss 0.10  Total Training Translation Loss 4.58 
2024-02-08 21:40:56,215 EPOCH 336
2024-02-08 21:40:59,470 [Epoch: 336 Step: 00005700] Batch Recognition Loss:   0.006698 => Gls Tokens per Sec:      904 || Batch Translation Loss:   0.311026 => Txt Tokens per Sec:     2658 || Lr: 0.000100
2024-02-08 21:41:06,725 Epoch 336: Total Training Recognition Loss 0.14  Total Training Translation Loss 4.52 
2024-02-08 21:41:06,726 EPOCH 337
2024-02-08 21:41:17,515 Epoch 337: Total Training Recognition Loss 0.10  Total Training Translation Loss 3.83 
2024-02-08 21:41:17,515 EPOCH 338
2024-02-08 21:41:28,469 Epoch 338: Total Training Recognition Loss 0.13  Total Training Translation Loss 3.54 
2024-02-08 21:41:28,470 EPOCH 339
2024-02-08 21:41:39,376 Epoch 339: Total Training Recognition Loss 0.13  Total Training Translation Loss 2.87 
2024-02-08 21:41:39,377 EPOCH 340
2024-02-08 21:41:50,086 Epoch 340: Total Training Recognition Loss 0.09  Total Training Translation Loss 2.70 
2024-02-08 21:41:50,086 EPOCH 341
2024-02-08 21:42:00,931 Epoch 341: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.08 
2024-02-08 21:42:00,931 EPOCH 342
2024-02-08 21:42:01,618 [Epoch: 342 Step: 00005800] Batch Recognition Loss:   0.001794 => Gls Tokens per Sec:     2803 || Batch Translation Loss:   0.113814 => Txt Tokens per Sec:     7615 || Lr: 0.000100
2024-02-08 21:42:11,641 Epoch 342: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.96 
2024-02-08 21:42:11,642 EPOCH 343
2024-02-08 21:42:22,474 Epoch 343: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.15 
2024-02-08 21:42:22,475 EPOCH 344
2024-02-08 21:42:33,313 Epoch 344: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.83 
2024-02-08 21:42:33,314 EPOCH 345
2024-02-08 21:42:44,195 Epoch 345: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.66 
2024-02-08 21:42:44,196 EPOCH 346
2024-02-08 21:42:54,763 Epoch 346: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.76 
2024-02-08 21:42:54,764 EPOCH 347
2024-02-08 21:43:05,396 Epoch 347: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.64 
2024-02-08 21:43:05,397 EPOCH 348
2024-02-08 21:43:05,635 [Epoch: 348 Step: 00005900] Batch Recognition Loss:   0.001400 => Gls Tokens per Sec:     2700 || Batch Translation Loss:   0.086409 => Txt Tokens per Sec:     8089 || Lr: 0.000100
2024-02-08 21:43:16,336 Epoch 348: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.61 
2024-02-08 21:43:16,336 EPOCH 349
2024-02-08 21:43:27,146 Epoch 349: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.59 
2024-02-08 21:43:27,147 EPOCH 350
2024-02-08 21:43:37,809 Epoch 350: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.81 
2024-02-08 21:43:37,810 EPOCH 351
2024-02-08 21:43:48,630 Epoch 351: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.85 
2024-02-08 21:43:48,631 EPOCH 352
2024-02-08 21:43:59,202 Epoch 352: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.69 
2024-02-08 21:43:59,202 EPOCH 353
2024-02-08 21:44:09,725 [Epoch: 353 Step: 00006000] Batch Recognition Loss:   0.001530 => Gls Tokens per Sec:      948 || Batch Translation Loss:   0.104467 => Txt Tokens per Sec:     2628 || Lr: 0.000100
2024-02-08 21:44:50,399 Validation result at epoch 353, step     6000: duration: 40.6730s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.12260	Translation Loss: 86180.67188	PPL: 5473.85010
	Eval Metric: BLEU
	WER 5.72	(DEL: 0.00,	INS: 0.00,	SUB: 5.72)
	BLEU-4 0.45	(BLEU-1: 10.74,	BLEU-2: 3.26,	BLEU-3: 1.14,	BLEU-4: 0.45)
	CHRF 17.88	ROUGE 8.86
2024-02-08 21:44:50,400 Logging Recognition and Translation Outputs
2024-02-08 21:44:50,401 ========================================================================================================================
2024-02-08 21:44:50,401 Logging Sequence: 160_153.00
2024-02-08 21:44:50,401 	Gloss Reference :	A B+C+D+E
2024-02-08 21:44:50,401 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 21:44:50,401 	Gloss Alignment :	         
2024-02-08 21:44:50,401 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 21:44:50,404 	Text Reference  :	*** i         have no  hard feelings towards rohit   sharma and  he    will     always have my  full support as he    is  my    teammate
2024-02-08 21:44:50,404 	Text Hypothesis :	now according to   the bcci if       the     liberty to     pick their decision as     you  can not  allowed to watch the right pad     
2024-02-08 21:44:50,404 	Text Alignment  :	I   S         S    S   S    S        S       S       S      S    S     S        S      S    S   S    S       S  S     S   S     S       
2024-02-08 21:44:50,404 ========================================================================================================================
2024-02-08 21:44:50,405 Logging Sequence: 103_253.00
2024-02-08 21:44:50,405 	Gloss Reference :	A B+C+D+E
2024-02-08 21:44:50,405 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 21:44:50,405 	Gloss Alignment :	         
2024-02-08 21:44:50,405 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 21:44:50,406 	Text Reference  :	canada is 3rd with 92      medals 
2024-02-08 21:44:50,406 	Text Hypothesis :	this   is why the  various winners
2024-02-08 21:44:50,406 	Text Alignment  :	S         S   S    S       S      
2024-02-08 21:44:50,406 ========================================================================================================================
2024-02-08 21:44:50,406 Logging Sequence: 155_25.00
2024-02-08 21:44:50,406 	Gloss Reference :	A B+C+D+E
2024-02-08 21:44:50,407 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 21:44:50,407 	Gloss Alignment :	         
2024-02-08 21:44:50,407 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 21:44:50,408 	Text Reference  :	* this is   because  taliban overthrew the    afghan government and took   over the  country
2024-02-08 21:44:50,408 	Text Hypothesis :	i am   very grateful for     my        family as     well       so  sports or   into tv     
2024-02-08 21:44:50,408 	Text Alignment  :	I S    S    S        S       S         S      S      S          S   S      S    S    S      
2024-02-08 21:44:50,408 ========================================================================================================================
2024-02-08 21:44:50,408 Logging Sequence: 81_105.00
2024-02-08 21:44:50,409 	Gloss Reference :	A B+C+D+E
2024-02-08 21:44:50,409 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 21:44:50,409 	Gloss Alignment :	         
2024-02-08 21:44:50,409 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 21:44:50,410 	Text Reference  :	****** ***** ** ***** dhoni was     tagged      in multiple such     posts as he was the brand ambassador
2024-02-08 21:44:50,410 	Text Hypothesis :	people loved to learn about dhoni's invovlement in ******** amrapali group as ** *** the ***** builder   
2024-02-08 21:44:50,410 	Text Alignment  :	I      I     I  I     S     S       S              D        S        S        D  D       D     S         
2024-02-08 21:44:50,410 ========================================================================================================================
2024-02-08 21:44:50,411 Logging Sequence: 105_136.00
2024-02-08 21:44:50,411 	Gloss Reference :	A B+C+D+E
2024-02-08 21:44:50,411 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 21:44:50,411 	Gloss Alignment :	         
2024-02-08 21:44:50,411 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 21:44:50,412 	Text Reference  :	beating him  once is  my      biggest dream  
2024-02-08 21:44:50,412 	Text Hypothesis :	so      they go   for careful the     morning
2024-02-08 21:44:50,412 	Text Alignment  :	S       S    S    S   S       S       S      
2024-02-08 21:44:50,412 ========================================================================================================================
2024-02-08 21:44:50,726 Epoch 353: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.01 
2024-02-08 21:44:50,726 EPOCH 354
2024-02-08 21:45:01,580 Epoch 354: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.01 
2024-02-08 21:45:01,581 EPOCH 355
2024-02-08 21:45:12,529 Epoch 355: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.70 
2024-02-08 21:45:12,529 EPOCH 356
2024-02-08 21:45:23,580 Epoch 356: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.48 
2024-02-08 21:45:23,581 EPOCH 357
2024-02-08 21:45:34,273 Epoch 357: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.69 
2024-02-08 21:45:34,274 EPOCH 358
2024-02-08 21:45:45,028 Epoch 358: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.49 
2024-02-08 21:45:45,029 EPOCH 359
2024-02-08 21:45:51,429 [Epoch: 359 Step: 00006100] Batch Recognition Loss:   0.002848 => Gls Tokens per Sec:     1360 || Batch Translation Loss:   0.065763 => Txt Tokens per Sec:     3602 || Lr: 0.000100
2024-02-08 21:45:55,490 Epoch 359: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.01 
2024-02-08 21:45:55,490 EPOCH 360
2024-02-08 21:46:06,197 Epoch 360: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.88 
2024-02-08 21:46:06,197 EPOCH 361
2024-02-08 21:46:16,776 Epoch 361: Total Training Recognition Loss 0.08  Total Training Translation Loss 3.25 
2024-02-08 21:46:16,777 EPOCH 362
2024-02-08 21:46:27,474 Epoch 362: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.08 
2024-02-08 21:46:27,474 EPOCH 363
2024-02-08 21:46:38,362 Epoch 363: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.64 
2024-02-08 21:46:38,363 EPOCH 364
2024-02-08 21:46:49,118 Epoch 364: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.51 
2024-02-08 21:46:49,119 EPOCH 365
2024-02-08 21:46:56,524 [Epoch: 365 Step: 00006200] Batch Recognition Loss:   0.001384 => Gls Tokens per Sec:     1037 || Batch Translation Loss:   0.195676 => Txt Tokens per Sec:     2955 || Lr: 0.000100
2024-02-08 21:46:59,733 Epoch 365: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.60 
2024-02-08 21:46:59,733 EPOCH 366
2024-02-08 21:47:10,478 Epoch 366: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.15 
2024-02-08 21:47:10,479 EPOCH 367
2024-02-08 21:47:21,206 Epoch 367: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.20 
2024-02-08 21:47:21,207 EPOCH 368
2024-02-08 21:47:31,982 Epoch 368: Total Training Recognition Loss 0.08  Total Training Translation Loss 3.48 
2024-02-08 21:47:31,983 EPOCH 369
2024-02-08 21:47:42,737 Epoch 369: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.98 
2024-02-08 21:47:42,738 EPOCH 370
2024-02-08 21:47:53,220 Epoch 370: Total Training Recognition Loss 0.08  Total Training Translation Loss 3.03 
2024-02-08 21:47:53,221 EPOCH 371
2024-02-08 21:47:55,315 [Epoch: 371 Step: 00006300] Batch Recognition Loss:   0.001334 => Gls Tokens per Sec:     3058 || Batch Translation Loss:   0.098036 => Txt Tokens per Sec:     8046 || Lr: 0.000100
2024-02-08 21:48:03,798 Epoch 371: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.39 
2024-02-08 21:48:03,799 EPOCH 372
2024-02-08 21:48:14,730 Epoch 372: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.12 
2024-02-08 21:48:14,730 EPOCH 373
2024-02-08 21:48:25,406 Epoch 373: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.77 
2024-02-08 21:48:25,407 EPOCH 374
2024-02-08 21:48:36,073 Epoch 374: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.78 
2024-02-08 21:48:36,073 EPOCH 375
2024-02-08 21:48:46,891 Epoch 375: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.83 
2024-02-08 21:48:46,891 EPOCH 376
2024-02-08 21:48:57,539 Epoch 376: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.70 
2024-02-08 21:48:57,540 EPOCH 377
2024-02-08 21:49:04,798 [Epoch: 377 Step: 00006400] Batch Recognition Loss:   0.011066 => Gls Tokens per Sec:      670 || Batch Translation Loss:   0.077324 => Txt Tokens per Sec:     1910 || Lr: 0.000100
2024-02-08 21:49:08,345 Epoch 377: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.48 
2024-02-08 21:49:08,345 EPOCH 378
2024-02-08 21:49:18,811 Epoch 378: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.69 
2024-02-08 21:49:18,811 EPOCH 379
2024-02-08 21:49:29,704 Epoch 379: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.36 
2024-02-08 21:49:29,705 EPOCH 380
2024-02-08 21:49:40,566 Epoch 380: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.33 
2024-02-08 21:49:40,567 EPOCH 381
2024-02-08 21:49:51,181 Epoch 381: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.26 
2024-02-08 21:49:51,182 EPOCH 382
2024-02-08 21:50:02,119 Epoch 382: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.09 
2024-02-08 21:50:02,119 EPOCH 383
2024-02-08 21:50:06,572 [Epoch: 383 Step: 00006500] Batch Recognition Loss:   0.006246 => Gls Tokens per Sec:      863 || Batch Translation Loss:   0.074073 => Txt Tokens per Sec:     2419 || Lr: 0.000100
2024-02-08 21:50:12,565 Epoch 383: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.28 
2024-02-08 21:50:12,566 EPOCH 384
2024-02-08 21:50:23,482 Epoch 384: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.59 
2024-02-08 21:50:23,483 EPOCH 385
2024-02-08 21:50:34,129 Epoch 385: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.02 
2024-02-08 21:50:34,130 EPOCH 386
2024-02-08 21:50:44,729 Epoch 386: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.06 
2024-02-08 21:50:44,730 EPOCH 387
2024-02-08 21:50:55,398 Epoch 387: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.60 
2024-02-08 21:50:55,398 EPOCH 388
2024-02-08 21:51:06,226 Epoch 388: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.45 
2024-02-08 21:51:06,227 EPOCH 389
2024-02-08 21:51:06,994 [Epoch: 389 Step: 00006600] Batch Recognition Loss:   0.002860 => Gls Tokens per Sec:     3342 || Batch Translation Loss:   0.141823 => Txt Tokens per Sec:     8375 || Lr: 0.000100
2024-02-08 21:51:16,729 Epoch 389: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.51 
2024-02-08 21:51:16,730 EPOCH 390
2024-02-08 21:51:27,360 Epoch 390: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.83 
2024-02-08 21:51:27,361 EPOCH 391
2024-02-08 21:51:38,301 Epoch 391: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.92 
2024-02-08 21:51:38,301 EPOCH 392
2024-02-08 21:51:49,256 Epoch 392: Total Training Recognition Loss 0.06  Total Training Translation Loss 6.12 
2024-02-08 21:51:49,257 EPOCH 393
2024-02-08 21:51:59,892 Epoch 393: Total Training Recognition Loss 0.10  Total Training Translation Loss 10.56 
2024-02-08 21:51:59,893 EPOCH 394
2024-02-08 21:52:10,751 Epoch 394: Total Training Recognition Loss 0.23  Total Training Translation Loss 25.90 
2024-02-08 21:52:10,752 EPOCH 395
2024-02-08 21:52:11,163 [Epoch: 395 Step: 00006700] Batch Recognition Loss:   0.015546 => Gls Tokens per Sec:     3130 || Batch Translation Loss:   0.450522 => Txt Tokens per Sec:     7462 || Lr: 0.000100
2024-02-08 21:52:21,613 Epoch 395: Total Training Recognition Loss 0.18  Total Training Translation Loss 13.30 
2024-02-08 21:52:21,614 EPOCH 396
2024-02-08 21:52:32,404 Epoch 396: Total Training Recognition Loss 0.19  Total Training Translation Loss 7.06 
2024-02-08 21:52:32,404 EPOCH 397
2024-02-08 21:52:43,021 Epoch 397: Total Training Recognition Loss 0.12  Total Training Translation Loss 4.86 
2024-02-08 21:52:43,022 EPOCH 398
2024-02-08 21:52:53,731 Epoch 398: Total Training Recognition Loss 0.11  Total Training Translation Loss 2.82 
2024-02-08 21:52:53,732 EPOCH 399
2024-02-08 21:53:04,574 Epoch 399: Total Training Recognition Loss 0.09  Total Training Translation Loss 2.12 
2024-02-08 21:53:04,574 EPOCH 400
2024-02-08 21:53:15,305 [Epoch: 400 Step: 00006800] Batch Recognition Loss:   0.001481 => Gls Tokens per Sec:      990 || Batch Translation Loss:   0.161181 => Txt Tokens per Sec:     2738 || Lr: 0.000100
2024-02-08 21:53:15,305 Epoch 400: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.75 
2024-02-08 21:53:15,305 EPOCH 401
2024-02-08 21:53:25,992 Epoch 401: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.54 
2024-02-08 21:53:25,993 EPOCH 402
2024-02-08 21:53:36,700 Epoch 402: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.34 
2024-02-08 21:53:36,700 EPOCH 403
2024-02-08 21:53:47,378 Epoch 403: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.12 
2024-02-08 21:53:47,379 EPOCH 404
2024-02-08 21:53:57,976 Epoch 404: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.04 
2024-02-08 21:53:57,976 EPOCH 405
2024-02-08 21:54:09,026 Epoch 405: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.02 
2024-02-08 21:54:09,027 EPOCH 406
2024-02-08 21:54:17,864 [Epoch: 406 Step: 00006900] Batch Recognition Loss:   0.008211 => Gls Tokens per Sec:     1057 || Batch Translation Loss:   0.025959 => Txt Tokens per Sec:     2868 || Lr: 0.000100
2024-02-08 21:54:19,776 Epoch 406: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.01 
2024-02-08 21:54:19,776 EPOCH 407
2024-02-08 21:54:30,589 Epoch 407: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.91 
2024-02-08 21:54:30,589 EPOCH 408
2024-02-08 21:54:41,105 Epoch 408: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.02 
2024-02-08 21:54:41,106 EPOCH 409
2024-02-08 21:54:51,857 Epoch 409: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.79 
2024-02-08 21:54:51,858 EPOCH 410
2024-02-08 21:55:02,737 Epoch 410: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.89 
2024-02-08 21:55:02,738 EPOCH 411
2024-02-08 21:55:13,144 Epoch 411: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.79 
2024-02-08 21:55:13,144 EPOCH 412
2024-02-08 21:55:21,344 [Epoch: 412 Step: 00007000] Batch Recognition Loss:   0.001481 => Gls Tokens per Sec:      983 || Batch Translation Loss:   0.040749 => Txt Tokens per Sec:     2736 || Lr: 0.000100
2024-02-08 21:55:23,804 Epoch 412: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.74 
2024-02-08 21:55:23,804 EPOCH 413
2024-02-08 21:55:34,404 Epoch 413: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.75 
2024-02-08 21:55:34,405 EPOCH 414
2024-02-08 21:55:45,222 Epoch 414: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.83 
2024-02-08 21:55:45,223 EPOCH 415
2024-02-08 21:55:55,959 Epoch 415: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.85 
2024-02-08 21:55:55,959 EPOCH 416
2024-02-08 21:56:06,835 Epoch 416: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.82 
2024-02-08 21:56:06,835 EPOCH 417
2024-02-08 21:56:17,597 Epoch 417: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.83 
2024-02-08 21:56:17,598 EPOCH 418
2024-02-08 21:56:21,845 [Epoch: 418 Step: 00007100] Batch Recognition Loss:   0.002757 => Gls Tokens per Sec:     1658 || Batch Translation Loss:   0.017540 => Txt Tokens per Sec:     4281 || Lr: 0.000100
2024-02-08 21:56:28,720 Epoch 418: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.69 
2024-02-08 21:56:28,720 EPOCH 419
2024-02-08 21:56:39,472 Epoch 419: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.76 
2024-02-08 21:56:39,473 EPOCH 420
2024-02-08 21:56:50,019 Epoch 420: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.69 
2024-02-08 21:56:50,020 EPOCH 421
2024-02-08 21:57:00,853 Epoch 421: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.69 
2024-02-08 21:57:00,853 EPOCH 422
2024-02-08 21:57:11,529 Epoch 422: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.62 
2024-02-08 21:57:11,530 EPOCH 423
2024-02-08 21:57:22,465 Epoch 423: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.62 
2024-02-08 21:57:22,466 EPOCH 424
2024-02-08 21:57:26,260 [Epoch: 424 Step: 00007200] Batch Recognition Loss:   0.000947 => Gls Tokens per Sec:     1519 || Batch Translation Loss:   0.037741 => Txt Tokens per Sec:     4309 || Lr: 0.000100
2024-02-08 21:57:33,103 Epoch 424: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.61 
2024-02-08 21:57:33,103 EPOCH 425
2024-02-08 21:57:43,807 Epoch 425: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.71 
2024-02-08 21:57:43,808 EPOCH 426
2024-02-08 21:57:54,176 Epoch 426: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.68 
2024-02-08 21:57:54,176 EPOCH 427
2024-02-08 21:58:04,695 Epoch 427: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.71 
2024-02-08 21:58:04,695 EPOCH 428
2024-02-08 21:58:15,445 Epoch 428: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.58 
2024-02-08 21:58:15,446 EPOCH 429
2024-02-08 21:58:26,359 Epoch 429: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.64 
2024-02-08 21:58:26,360 EPOCH 430
2024-02-08 21:58:30,083 [Epoch: 430 Step: 00007300] Batch Recognition Loss:   0.003890 => Gls Tokens per Sec:     1134 || Batch Translation Loss:   0.038580 => Txt Tokens per Sec:     3069 || Lr: 0.000100
2024-02-08 21:58:37,029 Epoch 430: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.72 
2024-02-08 21:58:37,029 EPOCH 431
2024-02-08 21:58:47,765 Epoch 431: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.84 
2024-02-08 21:58:47,765 EPOCH 432
2024-02-08 21:58:58,356 Epoch 432: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.02 
2024-02-08 21:58:58,357 EPOCH 433
2024-02-08 21:59:08,998 Epoch 433: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.14 
2024-02-08 21:59:08,998 EPOCH 434
2024-02-08 21:59:19,597 Epoch 434: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.18 
2024-02-08 21:59:19,598 EPOCH 435
2024-02-08 21:59:30,463 Epoch 435: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.04 
2024-02-08 21:59:30,463 EPOCH 436
2024-02-08 21:59:31,698 [Epoch: 436 Step: 00007400] Batch Recognition Loss:   0.001094 => Gls Tokens per Sec:     2593 || Batch Translation Loss:   0.057400 => Txt Tokens per Sec:     7100 || Lr: 0.000100
2024-02-08 21:59:41,501 Epoch 436: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.99 
2024-02-08 21:59:41,501 EPOCH 437
2024-02-08 21:59:52,439 Epoch 437: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.22 
2024-02-08 21:59:52,440 EPOCH 438
2024-02-08 22:00:03,291 Epoch 438: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.93 
2024-02-08 22:00:03,291 EPOCH 439
2024-02-08 22:00:13,956 Epoch 439: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.88 
2024-02-08 22:00:13,956 EPOCH 440
2024-02-08 22:00:24,775 Epoch 440: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.86 
2024-02-08 22:00:24,776 EPOCH 441
2024-02-08 22:00:35,786 Epoch 441: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.99 
2024-02-08 22:00:35,787 EPOCH 442
2024-02-08 22:00:39,376 [Epoch: 442 Step: 00007500] Batch Recognition Loss:   0.001149 => Gls Tokens per Sec:      535 || Batch Translation Loss:   0.051488 => Txt Tokens per Sec:     1651 || Lr: 0.000100
2024-02-08 22:00:46,335 Epoch 442: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.09 
2024-02-08 22:00:46,335 EPOCH 443
2024-02-08 22:00:57,051 Epoch 443: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.21 
2024-02-08 22:00:57,052 EPOCH 444
2024-02-08 22:01:07,860 Epoch 444: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.04 
2024-02-08 22:01:07,861 EPOCH 445
2024-02-08 22:01:18,779 Epoch 445: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.96 
2024-02-08 22:01:18,780 EPOCH 446
2024-02-08 22:01:29,116 Epoch 446: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.08 
2024-02-08 22:01:29,117 EPOCH 447
2024-02-08 22:01:39,993 Epoch 447: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.30 
2024-02-08 22:01:39,993 EPOCH 448
2024-02-08 22:01:42,330 [Epoch: 448 Step: 00007600] Batch Recognition Loss:   0.038985 => Gls Tokens per Sec:      163 || Batch Translation Loss:   0.059825 => Txt Tokens per Sec:      581 || Lr: 0.000100
2024-02-08 22:01:50,621 Epoch 448: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.64 
2024-02-08 22:01:50,621 EPOCH 449
2024-02-08 22:02:01,268 Epoch 449: Total Training Recognition Loss 0.11  Total Training Translation Loss 1.67 
2024-02-08 22:02:01,269 EPOCH 450
2024-02-08 22:02:12,120 Epoch 450: Total Training Recognition Loss 0.10  Total Training Translation Loss 2.52 
2024-02-08 22:02:12,120 EPOCH 451
2024-02-08 22:02:22,635 Epoch 451: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.75 
2024-02-08 22:02:22,635 EPOCH 452
2024-02-08 22:02:33,443 Epoch 452: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.56 
2024-02-08 22:02:33,444 EPOCH 453
2024-02-08 22:02:44,111 [Epoch: 453 Step: 00007700] Batch Recognition Loss:   0.003955 => Gls Tokens per Sec:      936 || Batch Translation Loss:   0.243777 => Txt Tokens per Sec:     2601 || Lr: 0.000100
2024-02-08 22:02:44,333 Epoch 453: Total Training Recognition Loss 0.08  Total Training Translation Loss 4.02 
2024-02-08 22:02:44,333 EPOCH 454
2024-02-08 22:02:55,249 Epoch 454: Total Training Recognition Loss 0.07  Total Training Translation Loss 4.18 
2024-02-08 22:02:55,250 EPOCH 455
2024-02-08 22:03:05,436 Epoch 455: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.71 
2024-02-08 22:03:05,436 EPOCH 456
2024-02-08 22:03:15,971 Epoch 456: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.86 
2024-02-08 22:03:15,972 EPOCH 457
2024-02-08 22:03:26,699 Epoch 457: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.39 
2024-02-08 22:03:26,700 EPOCH 458
2024-02-08 22:03:37,454 Epoch 458: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.10 
2024-02-08 22:03:37,454 EPOCH 459
2024-02-08 22:03:47,804 [Epoch: 459 Step: 00007800] Batch Recognition Loss:   0.000941 => Gls Tokens per Sec:      841 || Batch Translation Loss:   0.192846 => Txt Tokens per Sec:     2381 || Lr: 0.000100
2024-02-08 22:03:48,345 Epoch 459: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.73 
2024-02-08 22:03:48,345 EPOCH 460
2024-02-08 22:03:59,049 Epoch 460: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.74 
2024-02-08 22:03:59,050 EPOCH 461
2024-02-08 22:04:09,542 Epoch 461: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.60 
2024-02-08 22:04:09,543 EPOCH 462
2024-02-08 22:04:20,304 Epoch 462: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.47 
2024-02-08 22:04:20,305 EPOCH 463
2024-02-08 22:04:31,134 Epoch 463: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.25 
2024-02-08 22:04:31,134 EPOCH 464
2024-02-08 22:04:41,854 Epoch 464: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.48 
2024-02-08 22:04:41,855 EPOCH 465
2024-02-08 22:04:48,920 [Epoch: 465 Step: 00007900] Batch Recognition Loss:   0.003109 => Gls Tokens per Sec:     1087 || Batch Translation Loss:   0.072630 => Txt Tokens per Sec:     2961 || Lr: 0.000100
2024-02-08 22:04:52,486 Epoch 465: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.24 
2024-02-08 22:04:52,486 EPOCH 466
2024-02-08 22:05:03,237 Epoch 466: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.16 
2024-02-08 22:05:03,238 EPOCH 467
2024-02-08 22:05:13,974 Epoch 467: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.18 
2024-02-08 22:05:13,975 EPOCH 468
2024-02-08 22:05:24,782 Epoch 468: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.19 
2024-02-08 22:05:24,782 EPOCH 469
2024-02-08 22:05:35,487 Epoch 469: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.29 
2024-02-08 22:05:35,488 EPOCH 470
2024-02-08 22:05:46,528 Epoch 470: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.32 
2024-02-08 22:05:46,529 EPOCH 471
2024-02-08 22:05:50,859 [Epoch: 471 Step: 00008000] Batch Recognition Loss:   0.001463 => Gls Tokens per Sec:     1418 || Batch Translation Loss:   0.114679 => Txt Tokens per Sec:     3667 || Lr: 0.000100
2024-02-08 22:06:30,111 Validation result at epoch 471, step     8000: duration: 39.2513s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.01267	Translation Loss: 90590.53125	PPL: 8503.17480
	Eval Metric: BLEU
	WER 5.30	(DEL: 0.00,	INS: 0.00,	SUB: 5.30)
	BLEU-4 0.65	(BLEU-1: 11.78,	BLEU-2: 3.85,	BLEU-3: 1.45,	BLEU-4: 0.65)
	CHRF 17.76	ROUGE 10.22
2024-02-08 22:06:30,112 Logging Recognition and Translation Outputs
2024-02-08 22:06:30,113 ========================================================================================================================
2024-02-08 22:06:30,113 Logging Sequence: 180_236.00
2024-02-08 22:06:30,113 	Gloss Reference :	A B+C+D+E
2024-02-08 22:06:30,113 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 22:06:30,113 	Gloss Alignment :	         
2024-02-08 22:06:30,114 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 22:06:30,115 	Text Reference  :	however the wrestlers returned to        the protest *** site at  jantar  mantar with     thier demands
2024-02-08 22:06:30,115 	Text Hypothesis :	however the wrestlers ******** continued the protest and has  not allowed to     complete the   panel  
2024-02-08 22:06:30,115 	Text Alignment  :	                      D        S                     I   S    S   S       S      S        S     S      
2024-02-08 22:06:30,115 ========================================================================================================================
2024-02-08 22:06:30,115 Logging Sequence: 111_154.00
2024-02-08 22:06:30,116 	Gloss Reference :	A B+C+D+E  
2024-02-08 22:06:30,116 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-08 22:06:30,116 	Gloss Alignment :	  S        
2024-02-08 22:06:30,116 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 22:06:30,117 	Text Reference  :	********* ** ********** ** *** *** due to  csk's   slow over rate    dhoni was fined rs   12  lakh   
2024-02-08 22:06:30,118 	Text Hypothesis :	over-rate is calculated at the end of  the captain by   the  umpires and   if  they  find the captain
2024-02-08 22:06:30,118 	Text Alignment  :	I         I  I          I  I   I   S   S   S       S    S    S       S     S   S     S    S   S      
2024-02-08 22:06:30,118 ========================================================================================================================
2024-02-08 22:06:30,118 Logging Sequence: 118_314.00
2024-02-08 22:06:30,118 	Gloss Reference :	A B+C+D+E
2024-02-08 22:06:30,118 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 22:06:30,119 	Gloss Alignment :	         
2024-02-08 22:06:30,119 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 22:06:30,119 	Text Reference  :	** wow  even the president had come to   watch
2024-02-08 22:06:30,119 	Text Hypothesis :	so here are  the ********* *** **** same time 
2024-02-08 22:06:30,119 	Text Alignment  :	I  S    S        D         D   D    S    S    
2024-02-08 22:06:30,119 ========================================================================================================================
2024-02-08 22:06:30,120 Logging Sequence: 156_197.00
2024-02-08 22:06:30,120 	Gloss Reference :	A B+C+D+E
2024-02-08 22:06:30,120 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 22:06:30,120 	Gloss Alignment :	         
2024-02-08 22:06:30,120 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 22:06:30,121 	Text Reference  :	seattle orcas sor ****** is   owned       by  many   investors including satya nadella microsoft ceo    
2024-02-08 22:06:30,121 	Text Hypothesis :	******* ***** sor bowler from afghanistan and others have      invested  in    the     trent     rockets
2024-02-08 22:06:30,121 	Text Alignment  :	D       D         I      S    S           S   S      S         S         S     S       S         S      
2024-02-08 22:06:30,122 ========================================================================================================================
2024-02-08 22:06:30,122 Logging Sequence: 183_159.00
2024-02-08 22:06:30,122 	Gloss Reference :	A B+C+D+E
2024-02-08 22:06:30,122 	Gloss Hypothesis:	A B+C+D  
2024-02-08 22:06:30,122 	Gloss Alignment :	  S      
2024-02-08 22:06:30,122 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 22:06:30,124 	Text Reference  :	however an exception to this is virat kohli  and  his     wife anushka sharma who refuse to      share images of     their daughter
2024-02-08 22:06:30,124 	Text Hypothesis :	******* ** ********* ** **** ** later rooney then invited the  first   time   a   new    zealand and   then   joined the   umpire  
2024-02-08 22:06:30,124 	Text Alignment  :	D       D  D         D  D    D  S     S      S    S       S    S       S      S   S      S       S     S      S      S     S       
2024-02-08 22:06:30,125 ========================================================================================================================
2024-02-08 22:06:36,537 Epoch 471: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.19 
2024-02-08 22:06:36,537 EPOCH 472
2024-02-08 22:06:47,289 Epoch 472: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.51 
2024-02-08 22:06:47,290 EPOCH 473
2024-02-08 22:06:58,117 Epoch 473: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.47 
2024-02-08 22:06:58,118 EPOCH 474
2024-02-08 22:07:08,925 Epoch 474: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.29 
2024-02-08 22:07:08,926 EPOCH 475
2024-02-08 22:07:19,672 Epoch 475: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.16 
2024-02-08 22:07:19,673 EPOCH 476
2024-02-08 22:07:30,472 Epoch 476: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.12 
2024-02-08 22:07:30,473 EPOCH 477
2024-02-08 22:07:32,125 [Epoch: 477 Step: 00008100] Batch Recognition Loss:   0.001015 => Gls Tokens per Sec:     3099 || Batch Translation Loss:   0.081984 => Txt Tokens per Sec:     8095 || Lr: 0.000100
2024-02-08 22:07:40,936 Epoch 477: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.26 
2024-02-08 22:07:40,937 EPOCH 478
2024-02-08 22:07:51,862 Epoch 478: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.48 
2024-02-08 22:07:51,863 EPOCH 479
2024-02-08 22:08:02,562 Epoch 479: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.30 
2024-02-08 22:08:02,563 EPOCH 480
2024-02-08 22:08:13,318 Epoch 480: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.31 
2024-02-08 22:08:13,319 EPOCH 481
2024-02-08 22:08:24,222 Epoch 481: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.52 
2024-02-08 22:08:24,222 EPOCH 482
2024-02-08 22:08:34,978 Epoch 482: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.69 
2024-02-08 22:08:34,979 EPOCH 483
2024-02-08 22:08:39,704 [Epoch: 483 Step: 00008200] Batch Recognition Loss:   0.002368 => Gls Tokens per Sec:      813 || Batch Translation Loss:   0.136474 => Txt Tokens per Sec:     2457 || Lr: 0.000100
2024-02-08 22:08:45,587 Epoch 483: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.52 
2024-02-08 22:08:45,588 EPOCH 484
2024-02-08 22:08:56,424 Epoch 484: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.53 
2024-02-08 22:08:56,425 EPOCH 485
2024-02-08 22:09:07,259 Epoch 485: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.52 
2024-02-08 22:09:07,260 EPOCH 486
2024-02-08 22:09:17,791 Epoch 486: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.57 
2024-02-08 22:09:17,791 EPOCH 487
2024-02-08 22:09:28,892 Epoch 487: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.23 
2024-02-08 22:09:28,893 EPOCH 488
2024-02-08 22:09:39,576 Epoch 488: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.20 
2024-02-08 22:09:39,577 EPOCH 489
2024-02-08 22:09:41,831 [Epoch: 489 Step: 00008300] Batch Recognition Loss:   0.000559 => Gls Tokens per Sec:     1136 || Batch Translation Loss:   0.070079 => Txt Tokens per Sec:     3122 || Lr: 0.000100
2024-02-08 22:09:50,297 Epoch 489: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.12 
2024-02-08 22:09:50,297 EPOCH 490
2024-02-08 22:10:01,009 Epoch 490: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.32 
2024-02-08 22:10:01,009 EPOCH 491
2024-02-08 22:10:11,841 Epoch 491: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.55 
2024-02-08 22:10:11,842 EPOCH 492
2024-02-08 22:10:22,540 Epoch 492: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.22 
2024-02-08 22:10:22,541 EPOCH 493
2024-02-08 22:10:33,295 Epoch 493: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.29 
2024-02-08 22:10:33,295 EPOCH 494
2024-02-08 22:10:43,737 Epoch 494: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.16 
2024-02-08 22:10:43,738 EPOCH 495
2024-02-08 22:10:44,004 [Epoch: 495 Step: 00008400] Batch Recognition Loss:   0.002988 => Gls Tokens per Sec:     4830 || Batch Translation Loss:   0.040512 => Txt Tokens per Sec:     9185 || Lr: 0.000100
2024-02-08 22:10:54,575 Epoch 495: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.13 
2024-02-08 22:10:54,576 EPOCH 496
2024-02-08 22:11:05,144 Epoch 496: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.05 
2024-02-08 22:11:05,145 EPOCH 497
2024-02-08 22:11:15,861 Epoch 497: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.18 
2024-02-08 22:11:15,861 EPOCH 498
2024-02-08 22:11:26,602 Epoch 498: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.54 
2024-02-08 22:11:26,602 EPOCH 499
2024-02-08 22:11:37,427 Epoch 499: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.88 
2024-02-08 22:11:37,427 EPOCH 500
2024-02-08 22:11:48,049 [Epoch: 500 Step: 00008500] Batch Recognition Loss:   0.001085 => Gls Tokens per Sec:     1000 || Batch Translation Loss:   0.168239 => Txt Tokens per Sec:     2766 || Lr: 0.000100
2024-02-08 22:11:48,050 Epoch 500: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.42 
2024-02-08 22:11:48,050 EPOCH 501
2024-02-08 22:11:58,654 Epoch 501: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.02 
2024-02-08 22:11:58,655 EPOCH 502
2024-02-08 22:12:09,355 Epoch 502: Total Training Recognition Loss 0.08  Total Training Translation Loss 3.07 
2024-02-08 22:12:09,356 EPOCH 503
2024-02-08 22:12:20,009 Epoch 503: Total Training Recognition Loss 0.16  Total Training Translation Loss 2.35 
2024-02-08 22:12:20,009 EPOCH 504
2024-02-08 22:12:30,728 Epoch 504: Total Training Recognition Loss 0.25  Total Training Translation Loss 1.90 
2024-02-08 22:12:30,728 EPOCH 505
2024-02-08 22:12:41,560 Epoch 505: Total Training Recognition Loss 0.91  Total Training Translation Loss 3.57 
2024-02-08 22:12:41,560 EPOCH 506
2024-02-08 22:12:51,952 [Epoch: 506 Step: 00008600] Batch Recognition Loss:   2.665514 => Gls Tokens per Sec:      899 || Batch Translation Loss:   1.306985 => Txt Tokens per Sec:     2557 || Lr: 0.000100
2024-02-08 22:12:52,343 Epoch 506: Total Training Recognition Loss 6.02  Total Training Translation Loss 6.16 
2024-02-08 22:12:52,343 EPOCH 507
2024-02-08 22:13:03,111 Epoch 507: Total Training Recognition Loss 8.72  Total Training Translation Loss 11.86 
2024-02-08 22:13:03,111 EPOCH 508
2024-02-08 22:13:13,813 Epoch 508: Total Training Recognition Loss 8.33  Total Training Translation Loss 15.57 
2024-02-08 22:13:13,814 EPOCH 509
2024-02-08 22:13:24,535 Epoch 509: Total Training Recognition Loss 3.33  Total Training Translation Loss 7.60 
2024-02-08 22:13:24,536 EPOCH 510
2024-02-08 22:13:35,379 Epoch 510: Total Training Recognition Loss 3.32  Total Training Translation Loss 3.75 
2024-02-08 22:13:35,379 EPOCH 511
2024-02-08 22:13:46,189 Epoch 511: Total Training Recognition Loss 1.12  Total Training Translation Loss 2.46 
2024-02-08 22:13:46,190 EPOCH 512
2024-02-08 22:13:56,196 [Epoch: 512 Step: 00008700] Batch Recognition Loss:   0.000538 => Gls Tokens per Sec:      806 || Batch Translation Loss:   0.070160 => Txt Tokens per Sec:     2254 || Lr: 0.000100
2024-02-08 22:13:57,198 Epoch 512: Total Training Recognition Loss 0.20  Total Training Translation Loss 1.59 
2024-02-08 22:13:57,198 EPOCH 513
2024-02-08 22:14:07,723 Epoch 513: Total Training Recognition Loss 0.12  Total Training Translation Loss 1.09 
2024-02-08 22:14:07,723 EPOCH 514
2024-02-08 22:14:18,333 Epoch 514: Total Training Recognition Loss 0.07  Total Training Translation Loss 0.96 
2024-02-08 22:14:18,334 EPOCH 515
2024-02-08 22:14:29,294 Epoch 515: Total Training Recognition Loss 0.07  Total Training Translation Loss 0.82 
2024-02-08 22:14:29,295 EPOCH 516
2024-02-08 22:14:40,115 Epoch 516: Total Training Recognition Loss 0.07  Total Training Translation Loss 0.79 
2024-02-08 22:14:40,116 EPOCH 517
2024-02-08 22:14:50,757 Epoch 517: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.76 
2024-02-08 22:14:50,758 EPOCH 518
2024-02-08 22:14:54,851 [Epoch: 518 Step: 00008800] Batch Recognition Loss:   0.001574 => Gls Tokens per Sec:     1720 || Batch Translation Loss:   0.025872 => Txt Tokens per Sec:     4613 || Lr: 0.000100
2024-02-08 22:15:01,352 Epoch 518: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.71 
2024-02-08 22:15:01,353 EPOCH 519
2024-02-08 22:15:12,323 Epoch 519: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.61 
2024-02-08 22:15:12,323 EPOCH 520
2024-02-08 22:15:23,016 Epoch 520: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.65 
2024-02-08 22:15:23,017 EPOCH 521
2024-02-08 22:15:33,952 Epoch 521: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.74 
2024-02-08 22:15:33,952 EPOCH 522
2024-02-08 22:15:45,681 Epoch 522: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.65 
2024-02-08 22:15:45,681 EPOCH 523
2024-02-08 22:15:56,404 Epoch 523: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.55 
2024-02-08 22:15:56,404 EPOCH 524
2024-02-08 22:16:03,447 [Epoch: 524 Step: 00008900] Batch Recognition Loss:   0.004848 => Gls Tokens per Sec:      781 || Batch Translation Loss:   0.042534 => Txt Tokens per Sec:     2150 || Lr: 0.000100
2024-02-08 22:16:07,015 Epoch 524: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.55 
2024-02-08 22:16:07,015 EPOCH 525
2024-02-08 22:16:17,703 Epoch 525: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.55 
2024-02-08 22:16:17,704 EPOCH 526
2024-02-08 22:16:28,468 Epoch 526: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.57 
2024-02-08 22:16:28,468 EPOCH 527
2024-02-08 22:16:39,303 Epoch 527: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.61 
2024-02-08 22:16:39,304 EPOCH 528
2024-02-08 22:16:49,759 Epoch 528: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.78 
2024-02-08 22:16:49,759 EPOCH 529
2024-02-08 22:17:00,395 Epoch 529: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.72 
2024-02-08 22:17:00,396 EPOCH 530
2024-02-08 22:17:03,454 [Epoch: 530 Step: 00009000] Batch Recognition Loss:   0.005575 => Gls Tokens per Sec:     1465 || Batch Translation Loss:   0.046296 => Txt Tokens per Sec:     3940 || Lr: 0.000100
2024-02-08 22:17:10,982 Epoch 530: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.64 
2024-02-08 22:17:10,983 EPOCH 531
2024-02-08 22:17:21,468 Epoch 531: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.56 
2024-02-08 22:17:21,468 EPOCH 532
2024-02-08 22:17:32,434 Epoch 532: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.56 
2024-02-08 22:17:32,435 EPOCH 533
2024-02-08 22:17:43,196 Epoch 533: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.55 
2024-02-08 22:17:43,197 EPOCH 534
2024-02-08 22:17:54,012 Epoch 534: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.50 
2024-02-08 22:17:54,013 EPOCH 535
2024-02-08 22:18:04,940 Epoch 535: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.48 
2024-02-08 22:18:04,941 EPOCH 536
2024-02-08 22:18:05,864 [Epoch: 536 Step: 00009100] Batch Recognition Loss:   0.000265 => Gls Tokens per Sec:     3467 || Batch Translation Loss:   0.022961 => Txt Tokens per Sec:     8371 || Lr: 0.000100
2024-02-08 22:18:15,661 Epoch 536: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.53 
2024-02-08 22:18:15,661 EPOCH 537
2024-02-08 22:18:26,330 Epoch 537: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.50 
2024-02-08 22:18:26,330 EPOCH 538
2024-02-08 22:18:37,212 Epoch 538: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.49 
2024-02-08 22:18:37,213 EPOCH 539
2024-02-08 22:18:47,938 Epoch 539: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.40 
2024-02-08 22:18:47,939 EPOCH 540
2024-02-08 22:18:58,658 Epoch 540: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.40 
2024-02-08 22:18:58,659 EPOCH 541
2024-02-08 22:19:09,342 Epoch 541: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.43 
2024-02-08 22:19:09,343 EPOCH 542
2024-02-08 22:19:09,922 [Epoch: 542 Step: 00009200] Batch Recognition Loss:   0.000167 => Gls Tokens per Sec:     3322 || Batch Translation Loss:   0.025226 => Txt Tokens per Sec:     8657 || Lr: 0.000100
2024-02-08 22:19:19,866 Epoch 542: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.47 
2024-02-08 22:19:19,866 EPOCH 543
2024-02-08 22:19:30,608 Epoch 543: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.51 
2024-02-08 22:19:30,609 EPOCH 544
2024-02-08 22:19:41,494 Epoch 544: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.53 
2024-02-08 22:19:41,495 EPOCH 545
2024-02-08 22:19:52,319 Epoch 545: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.57 
2024-02-08 22:19:52,320 EPOCH 546
2024-02-08 22:20:03,223 Epoch 546: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.52 
2024-02-08 22:20:03,223 EPOCH 547
2024-02-08 22:20:14,209 Epoch 547: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.48 
2024-02-08 22:20:14,209 EPOCH 548
2024-02-08 22:20:14,376 [Epoch: 548 Step: 00009300] Batch Recognition Loss:   0.000285 => Gls Tokens per Sec:     3855 || Batch Translation Loss:   0.037852 => Txt Tokens per Sec:     9572 || Lr: 0.000100
2024-02-08 22:20:24,849 Epoch 548: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-08 22:20:24,850 EPOCH 549
2024-02-08 22:20:35,515 Epoch 549: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.83 
2024-02-08 22:20:35,515 EPOCH 550
2024-02-08 22:20:46,126 Epoch 550: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.85 
2024-02-08 22:20:46,127 EPOCH 551
2024-02-08 22:20:56,800 Epoch 551: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.70 
2024-02-08 22:20:56,800 EPOCH 552
2024-02-08 22:21:07,522 Epoch 552: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.05 
2024-02-08 22:21:07,522 EPOCH 553
2024-02-08 22:21:16,601 [Epoch: 553 Step: 00009400] Batch Recognition Loss:   0.000839 => Gls Tokens per Sec:     1099 || Batch Translation Loss:   0.030775 => Txt Tokens per Sec:     3009 || Lr: 0.000100
2024-02-08 22:21:18,382 Epoch 553: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.95 
2024-02-08 22:21:18,382 EPOCH 554
2024-02-08 22:21:29,196 Epoch 554: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.93 
2024-02-08 22:21:29,197 EPOCH 555
2024-02-08 22:21:39,715 Epoch 555: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.79 
2024-02-08 22:21:39,716 EPOCH 556
2024-02-08 22:21:50,547 Epoch 556: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.82 
2024-02-08 22:21:50,548 EPOCH 557
2024-02-08 22:22:01,115 Epoch 557: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.76 
2024-02-08 22:22:01,115 EPOCH 558
2024-02-08 22:22:11,791 Epoch 558: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.69 
2024-02-08 22:22:11,792 EPOCH 559
2024-02-08 22:22:19,512 [Epoch: 559 Step: 00009500] Batch Recognition Loss:   0.000661 => Gls Tokens per Sec:     1161 || Batch Translation Loss:   0.017574 => Txt Tokens per Sec:     3161 || Lr: 0.000100
2024-02-08 22:22:22,428 Epoch 559: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.72 
2024-02-08 22:22:22,428 EPOCH 560
2024-02-08 22:22:33,061 Epoch 560: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-08 22:22:33,062 EPOCH 561
2024-02-08 22:22:43,773 Epoch 561: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.64 
2024-02-08 22:22:43,773 EPOCH 562
2024-02-08 22:22:54,667 Epoch 562: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-08 22:22:54,667 EPOCH 563
2024-02-08 22:23:05,369 Epoch 563: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-08 22:23:05,369 EPOCH 564
2024-02-08 22:23:16,141 Epoch 564: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.94 
2024-02-08 22:23:16,141 EPOCH 565
2024-02-08 22:23:22,171 [Epoch: 565 Step: 00009600] Batch Recognition Loss:   0.000321 => Gls Tokens per Sec:     1274 || Batch Translation Loss:   0.076142 => Txt Tokens per Sec:     3520 || Lr: 0.000100
2024-02-08 22:23:26,941 Epoch 565: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.13 
2024-02-08 22:23:26,941 EPOCH 566
2024-02-08 22:23:37,867 Epoch 566: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.35 
2024-02-08 22:23:37,868 EPOCH 567
2024-02-08 22:23:48,431 Epoch 567: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.72 
2024-02-08 22:23:48,431 EPOCH 568
2024-02-08 22:23:59,068 Epoch 568: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.89 
2024-02-08 22:23:59,069 EPOCH 569
2024-02-08 22:24:09,552 Epoch 569: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.24 
2024-02-08 22:24:09,552 EPOCH 570
2024-02-08 22:24:20,437 Epoch 570: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.14 
2024-02-08 22:24:20,437 EPOCH 571
2024-02-08 22:24:26,300 [Epoch: 571 Step: 00009700] Batch Recognition Loss:   0.000303 => Gls Tokens per Sec:     1092 || Batch Translation Loss:   0.208302 => Txt Tokens per Sec:     3084 || Lr: 0.000100
2024-02-08 22:24:31,334 Epoch 571: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.90 
2024-02-08 22:24:31,334 EPOCH 572
2024-02-08 22:24:42,266 Epoch 572: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.74 
2024-02-08 22:24:42,266 EPOCH 573
2024-02-08 22:24:53,066 Epoch 573: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.65 
2024-02-08 22:24:53,066 EPOCH 574
2024-02-08 22:25:03,876 Epoch 574: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.32 
2024-02-08 22:25:03,877 EPOCH 575
2024-02-08 22:25:14,692 Epoch 575: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.30 
2024-02-08 22:25:14,692 EPOCH 576
2024-02-08 22:25:25,645 Epoch 576: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.45 
2024-02-08 22:25:25,645 EPOCH 577
2024-02-08 22:25:27,720 [Epoch: 577 Step: 00009800] Batch Recognition Loss:   0.000716 => Gls Tokens per Sec:     2470 || Batch Translation Loss:   0.068694 => Txt Tokens per Sec:     6918 || Lr: 0.000100
2024-02-08 22:25:36,468 Epoch 577: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.45 
2024-02-08 22:25:36,469 EPOCH 578
2024-02-08 22:25:47,360 Epoch 578: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.33 
2024-02-08 22:25:47,361 EPOCH 579
2024-02-08 22:25:57,963 Epoch 579: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.53 
2024-02-08 22:25:57,964 EPOCH 580
2024-02-08 22:26:08,878 Epoch 580: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.70 
2024-02-08 22:26:08,878 EPOCH 581
2024-02-08 22:26:19,700 Epoch 581: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.65 
2024-02-08 22:26:19,701 EPOCH 582
2024-02-08 22:26:31,154 Epoch 582: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.59 
2024-02-08 22:26:31,155 EPOCH 583
2024-02-08 22:26:36,292 [Epoch: 583 Step: 00009900] Batch Recognition Loss:   0.000738 => Gls Tokens per Sec:      748 || Batch Translation Loss:   0.073380 => Txt Tokens per Sec:     2188 || Lr: 0.000100
2024-02-08 22:26:42,845 Epoch 583: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.48 
2024-02-08 22:26:42,845 EPOCH 584
2024-02-08 22:26:53,824 Epoch 584: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.26 
2024-02-08 22:26:53,825 EPOCH 585
2024-02-08 22:27:04,439 Epoch 585: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.80 
2024-02-08 22:27:04,439 EPOCH 586
2024-02-08 22:27:15,401 Epoch 586: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.66 
2024-02-08 22:27:15,402 EPOCH 587
2024-02-08 22:27:26,298 Epoch 587: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.66 
2024-02-08 22:27:26,299 EPOCH 588
2024-02-08 22:27:37,035 Epoch 588: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.59 
2024-02-08 22:27:37,035 EPOCH 589
2024-02-08 22:27:39,689 [Epoch: 589 Step: 00010000] Batch Recognition Loss:   0.000509 => Gls Tokens per Sec:      965 || Batch Translation Loss:   0.030087 => Txt Tokens per Sec:     2861 || Lr: 0.000100
2024-02-08 22:28:19,092 Hooray! New best validation result [eval_metric]!
2024-02-08 22:28:19,094 Saving new checkpoint.
2024-02-08 22:28:19,390 Validation result at epoch 589, step    10000: duration: 39.6996s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.81317	Translation Loss: 91838.87500	PPL: 9632.32715
	Eval Metric: BLEU
	WER 4.38	(DEL: 0.00,	INS: 0.00,	SUB: 4.38)
	BLEU-4 1.00	(BLEU-1: 11.60,	BLEU-2: 3.96,	BLEU-3: 1.80,	BLEU-4: 1.00)
	CHRF 17.36	ROUGE 9.91
2024-02-08 22:28:19,391 Logging Recognition and Translation Outputs
2024-02-08 22:28:19,391 ========================================================================================================================
2024-02-08 22:28:19,392 Logging Sequence: 123_147.00
2024-02-08 22:28:19,392 	Gloss Reference :	A B+C+D+E
2024-02-08 22:28:19,392 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 22:28:19,393 	Gloss Alignment :	         
2024-02-08 22:28:19,393 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 22:28:19,394 	Text Reference  :	the former captain also owns the pontiac firebird trans am  car worth    rs         68 lakh    
2024-02-08 22:28:19,394 	Text Hypothesis :	*** ****** ******* **** **** *** ******* ******** dhoni has a   stunning collection of vehicles
2024-02-08 22:28:19,394 	Text Alignment  :	D   D      D       D    D    D   D       D        S     S   S   S        S          S  S       
2024-02-08 22:28:19,394 ========================================================================================================================
2024-02-08 22:28:19,394 Logging Sequence: 58_196.00
2024-02-08 22:28:19,394 	Gloss Reference :	A B+C+D+E
2024-02-08 22:28:19,395 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 22:28:19,395 	Gloss Alignment :	         
2024-02-08 22:28:19,395 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 22:28:19,396 	Text Reference  :	** the talents and  skills of our  athletes knows   no     bounds
2024-02-08 22:28:19,396 	Text Hypothesis :	so we  have    been set    to know in       olympic medals wow   
2024-02-08 22:28:19,396 	Text Alignment  :	I  S   S       S    S      S  S    S        S       S      S     
2024-02-08 22:28:19,396 ========================================================================================================================
2024-02-08 22:28:19,396 Logging Sequence: 168_184.00
2024-02-08 22:28:19,396 	Gloss Reference :	A B+C+D+E
2024-02-08 22:28:19,396 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 22:28:19,397 	Gloss Alignment :	         
2024-02-08 22:28:19,397 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 22:28:19,398 	Text Reference  :	people say that we  may  get     a  true glimpse of vamika in february 2022 when she turns  1    year          old    
2024-02-08 22:28:19,399 	Text Hypothesis :	****** who is   now been brought to bag  because of ****** ** ******** **** csk  to  retire from international matches
2024-02-08 22:28:19,399 	Text Alignment  :	D      S   S    S   S    S       S  S    S          D      D  D        D    S    S   S      S    S             S      
2024-02-08 22:28:19,399 ========================================================================================================================
2024-02-08 22:28:19,399 Logging Sequence: 87_123.00
2024-02-08 22:28:19,399 	Gloss Reference :	A B+C+D+E
2024-02-08 22:28:19,399 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 22:28:19,399 	Gloss Alignment :	         
2024-02-08 22:28:19,400 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 22:28:19,401 	Text Reference  :	****** *** ***** he  said that  he  hoped    kl   rahul would      be  fit for  the upcoming world   cup   
2024-02-08 22:28:19,401 	Text Hypothesis :	during the world cup 2022 india and pakistan have faced footballer due to  play the ******** british empire
2024-02-08 22:28:19,401 	Text Alignment  :	I      I   I     S   S    S     S   S        S    S     S          S   S   S        D        S       S     
2024-02-08 22:28:19,401 ========================================================================================================================
2024-02-08 22:28:19,402 Logging Sequence: 144_154.00
2024-02-08 22:28:19,402 	Gloss Reference :	A B+C+D+E
2024-02-08 22:28:19,402 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 22:28:19,402 	Gloss Alignment :	         
2024-02-08 22:28:19,402 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 22:28:19,403 	Text Reference  :	****** **** ****** she also participated in the rural  olympic games organised in   rajasthan a few     months 
2024-02-08 22:28:19,404 	Text Hypothesis :	people were amazed by  her  way          in *** india' world   cup   for       this was       a similar message
2024-02-08 22:28:19,404 	Text Alignment  :	I      I    I      S   S    S               D   S      S       S     S         S    S           S       S      
2024-02-08 22:28:19,404 ========================================================================================================================
2024-02-08 22:28:27,713 Epoch 589: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.61 
2024-02-08 22:28:27,714 EPOCH 590
2024-02-08 22:28:38,479 Epoch 590: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.62 
2024-02-08 22:28:38,480 EPOCH 591
2024-02-08 22:28:49,217 Epoch 591: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.65 
2024-02-08 22:28:49,218 EPOCH 592
2024-02-08 22:28:59,894 Epoch 592: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.78 
2024-02-08 22:28:59,895 EPOCH 593
2024-02-08 22:29:10,528 Epoch 593: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.80 
2024-02-08 22:29:10,529 EPOCH 594
2024-02-08 22:29:21,508 Epoch 594: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.91 
2024-02-08 22:29:21,509 EPOCH 595
2024-02-08 22:29:22,006 [Epoch: 595 Step: 00010100] Batch Recognition Loss:   0.000302 => Gls Tokens per Sec:     2586 || Batch Translation Loss:   0.062708 => Txt Tokens per Sec:     7279 || Lr: 0.000100
2024-02-08 22:29:32,288 Epoch 595: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.18 
2024-02-08 22:29:32,288 EPOCH 596
2024-02-08 22:29:42,968 Epoch 596: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.32 
2024-02-08 22:29:42,969 EPOCH 597
2024-02-08 22:29:53,904 Epoch 597: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.74 
2024-02-08 22:29:53,905 EPOCH 598
2024-02-08 22:30:04,907 Epoch 598: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.56 
2024-02-08 22:30:04,908 EPOCH 599
2024-02-08 22:30:15,683 Epoch 599: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.08 
2024-02-08 22:30:15,684 EPOCH 600
2024-02-08 22:30:26,427 [Epoch: 600 Step: 00010200] Batch Recognition Loss:   0.000646 => Gls Tokens per Sec:      989 || Batch Translation Loss:   0.054610 => Txt Tokens per Sec:     2735 || Lr: 0.000100
2024-02-08 22:30:26,427 Epoch 600: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.97 
2024-02-08 22:30:26,428 EPOCH 601
2024-02-08 22:30:37,061 Epoch 601: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.99 
2024-02-08 22:30:37,062 EPOCH 602
2024-02-08 22:30:47,575 Epoch 602: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.88 
2024-02-08 22:30:47,576 EPOCH 603
2024-02-08 22:30:58,162 Epoch 603: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.06 
2024-02-08 22:30:58,163 EPOCH 604
2024-02-08 22:31:09,010 Epoch 604: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.08 
2024-02-08 22:31:09,010 EPOCH 605
2024-02-08 22:31:19,984 Epoch 605: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.21 
2024-02-08 22:31:19,984 EPOCH 606
2024-02-08 22:31:28,657 [Epoch: 606 Step: 00010300] Batch Recognition Loss:   0.000492 => Gls Tokens per Sec:     1077 || Batch Translation Loss:   0.054338 => Txt Tokens per Sec:     2985 || Lr: 0.000100
2024-02-08 22:31:30,649 Epoch 606: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.29 
2024-02-08 22:31:30,649 EPOCH 607
2024-02-08 22:31:41,401 Epoch 607: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.50 
2024-02-08 22:31:41,402 EPOCH 608
2024-02-08 22:31:52,269 Epoch 608: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.27 
2024-02-08 22:31:52,269 EPOCH 609
2024-02-08 22:32:02,832 Epoch 609: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.13 
2024-02-08 22:32:02,832 EPOCH 610
2024-02-08 22:32:13,706 Epoch 610: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.22 
2024-02-08 22:32:13,707 EPOCH 611
2024-02-08 22:32:24,453 Epoch 611: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.11 
2024-02-08 22:32:24,454 EPOCH 612
2024-02-08 22:32:32,622 [Epoch: 612 Step: 00010400] Batch Recognition Loss:   0.000292 => Gls Tokens per Sec:      987 || Batch Translation Loss:   0.110502 => Txt Tokens per Sec:     2717 || Lr: 0.000100
2024-02-08 22:32:35,335 Epoch 612: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.94 
2024-02-08 22:32:35,336 EPOCH 613
2024-02-08 22:32:46,287 Epoch 613: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.92 
2024-02-08 22:32:46,288 EPOCH 614
2024-02-08 22:32:57,244 Epoch 614: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.85 
2024-02-08 22:32:57,245 EPOCH 615
2024-02-08 22:33:08,238 Epoch 615: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.95 
2024-02-08 22:33:08,238 EPOCH 616
2024-02-08 22:33:19,240 Epoch 616: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.71 
2024-02-08 22:33:19,241 EPOCH 617
2024-02-08 22:33:29,886 Epoch 617: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.66 
2024-02-08 22:33:29,887 EPOCH 618
2024-02-08 22:33:37,582 [Epoch: 618 Step: 00010500] Batch Recognition Loss:   0.000652 => Gls Tokens per Sec:      881 || Batch Translation Loss:   0.022055 => Txt Tokens per Sec:     2430 || Lr: 0.000100
2024-02-08 22:33:40,459 Epoch 618: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-08 22:33:40,459 EPOCH 619
2024-02-08 22:33:50,976 Epoch 619: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.63 
2024-02-08 22:33:50,977 EPOCH 620
2024-02-08 22:34:01,761 Epoch 620: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-08 22:34:01,762 EPOCH 621
2024-02-08 22:34:12,438 Epoch 621: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.67 
2024-02-08 22:34:12,439 EPOCH 622
2024-02-08 22:34:23,112 Epoch 622: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.70 
2024-02-08 22:34:23,113 EPOCH 623
2024-02-08 22:34:33,747 Epoch 623: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.00 
2024-02-08 22:34:33,748 EPOCH 624
2024-02-08 22:34:37,367 [Epoch: 624 Step: 00010600] Batch Recognition Loss:   0.001005 => Gls Tokens per Sec:     1592 || Batch Translation Loss:   0.075422 => Txt Tokens per Sec:     4166 || Lr: 0.000100
2024-02-08 22:34:44,408 Epoch 624: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.12 
2024-02-08 22:34:44,408 EPOCH 625
2024-02-08 22:34:55,306 Epoch 625: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.09 
2024-02-08 22:34:55,307 EPOCH 626
2024-02-08 22:35:06,414 Epoch 626: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.46 
2024-02-08 22:35:06,416 EPOCH 627
2024-02-08 22:35:18,352 Epoch 627: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.38 
2024-02-08 22:35:18,352 EPOCH 628
2024-02-08 22:35:31,503 Epoch 628: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.71 
2024-02-08 22:35:31,504 EPOCH 629
2024-02-08 22:35:43,953 Epoch 629: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.68 
2024-02-08 22:35:43,954 EPOCH 630
2024-02-08 22:35:49,275 [Epoch: 630 Step: 00010700] Batch Recognition Loss:   0.000519 => Gls Tokens per Sec:      842 || Batch Translation Loss:   0.290655 => Txt Tokens per Sec:     2415 || Lr: 0.000100
2024-02-08 22:35:56,211 Epoch 630: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.23 
2024-02-08 22:35:56,212 EPOCH 631
2024-02-08 22:36:08,159 Epoch 631: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.45 
2024-02-08 22:36:08,160 EPOCH 632
2024-02-08 22:36:20,837 Epoch 632: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.46 
2024-02-08 22:36:20,838 EPOCH 633
2024-02-08 22:36:33,149 Epoch 633: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.24 
2024-02-08 22:36:33,149 EPOCH 634
2024-02-08 22:36:44,906 Epoch 634: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.30 
2024-02-08 22:36:44,907 EPOCH 635
2024-02-08 22:36:57,167 Epoch 635: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.38 
2024-02-08 22:36:57,168 EPOCH 636
2024-02-08 22:36:58,530 [Epoch: 636 Step: 00010800] Batch Recognition Loss:   0.001517 => Gls Tokens per Sec:     2352 || Batch Translation Loss:   0.066314 => Txt Tokens per Sec:     6514 || Lr: 0.000100
2024-02-08 22:37:10,304 Epoch 636: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.70 
2024-02-08 22:37:10,305 EPOCH 637
2024-02-08 22:37:22,758 Epoch 637: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.90 
2024-02-08 22:37:22,759 EPOCH 638
2024-02-08 22:37:38,179 Epoch 638: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.52 
2024-02-08 22:37:38,179 EPOCH 639
2024-02-08 22:37:51,045 Epoch 639: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.96 
2024-02-08 22:37:51,046 EPOCH 640
2024-02-08 22:38:04,651 Epoch 640: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.37 
2024-02-08 22:38:04,652 EPOCH 641
2024-02-08 22:38:16,763 Epoch 641: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.42 
2024-02-08 22:38:16,763 EPOCH 642
2024-02-08 22:38:19,084 [Epoch: 642 Step: 00010900] Batch Recognition Loss:   0.000794 => Gls Tokens per Sec:      828 || Batch Translation Loss:   0.358549 => Txt Tokens per Sec:     2480 || Lr: 0.000100
2024-02-08 22:38:28,857 Epoch 642: Total Training Recognition Loss 0.05  Total Training Translation Loss 7.90 
2024-02-08 22:38:28,858 EPOCH 643
2024-02-08 22:38:41,313 Epoch 643: Total Training Recognition Loss 0.07  Total Training Translation Loss 5.47 
2024-02-08 22:38:41,314 EPOCH 644
2024-02-08 22:38:53,265 Epoch 644: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.91 
2024-02-08 22:38:53,266 EPOCH 645
2024-02-08 22:39:08,072 Epoch 645: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.33 
2024-02-08 22:39:08,072 EPOCH 646
2024-02-08 22:39:20,628 Epoch 646: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.45 
2024-02-08 22:39:20,629 EPOCH 647
2024-02-08 22:39:35,008 Epoch 647: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.08 
2024-02-08 22:39:35,009 EPOCH 648
2024-02-08 22:39:35,303 [Epoch: 648 Step: 00011000] Batch Recognition Loss:   0.000708 => Gls Tokens per Sec:     2183 || Batch Translation Loss:   0.072069 => Txt Tokens per Sec:     5904 || Lr: 0.000100
2024-02-08 22:39:46,954 Epoch 648: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.94 
2024-02-08 22:39:46,955 EPOCH 649
2024-02-08 22:39:58,371 Epoch 649: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.84 
2024-02-08 22:39:58,372 EPOCH 650
2024-02-08 22:40:11,027 Epoch 650: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.84 
2024-02-08 22:40:11,028 EPOCH 651
2024-02-08 22:40:24,614 Epoch 651: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.72 
2024-02-08 22:40:24,615 EPOCH 652
2024-02-08 22:40:35,940 Epoch 652: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.68 
2024-02-08 22:40:35,941 EPOCH 653
2024-02-08 22:40:47,211 [Epoch: 653 Step: 00011100] Batch Recognition Loss:   0.000620 => Gls Tokens per Sec:      886 || Batch Translation Loss:   0.049630 => Txt Tokens per Sec:     2427 || Lr: 0.000100
2024-02-08 22:40:47,573 Epoch 653: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.59 
2024-02-08 22:40:47,573 EPOCH 654
2024-02-08 22:40:58,890 Epoch 654: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.62 
2024-02-08 22:40:58,890 EPOCH 655
2024-02-08 22:41:10,199 Epoch 655: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.60 
2024-02-08 22:41:10,199 EPOCH 656
2024-02-08 22:41:21,780 Epoch 656: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.58 
2024-02-08 22:41:21,781 EPOCH 657
2024-02-08 22:41:33,626 Epoch 657: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.64 
2024-02-08 22:41:33,627 EPOCH 658
2024-02-08 22:41:45,819 Epoch 658: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.78 
2024-02-08 22:41:45,820 EPOCH 659
2024-02-08 22:41:56,809 [Epoch: 659 Step: 00011200] Batch Recognition Loss:   0.000978 => Gls Tokens per Sec:      792 || Batch Translation Loss:   0.026659 => Txt Tokens per Sec:     2151 || Lr: 0.000100
2024-02-08 22:41:57,787 Epoch 659: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.69 
2024-02-08 22:41:57,788 EPOCH 660
2024-02-08 22:42:09,722 Epoch 660: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.60 
2024-02-08 22:42:09,723 EPOCH 661
2024-02-08 22:42:21,062 Epoch 661: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.57 
2024-02-08 22:42:21,062 EPOCH 662
2024-02-08 22:42:32,654 Epoch 662: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.73 
2024-02-08 22:42:32,655 EPOCH 663
2024-02-08 22:42:44,059 Epoch 663: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.00 
2024-02-08 22:42:44,060 EPOCH 664
2024-02-08 22:42:55,911 Epoch 664: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.10 
2024-02-08 22:42:55,912 EPOCH 665
2024-02-08 22:43:02,435 [Epoch: 665 Step: 00011300] Batch Recognition Loss:   0.000675 => Gls Tokens per Sec:     1177 || Batch Translation Loss:   0.119060 => Txt Tokens per Sec:     3177 || Lr: 0.000100
2024-02-08 22:43:08,204 Epoch 665: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.30 
2024-02-08 22:43:08,205 EPOCH 666
2024-02-08 22:43:20,159 Epoch 666: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.91 
2024-02-08 22:43:20,160 EPOCH 667
2024-02-08 22:43:32,482 Epoch 667: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.79 
2024-02-08 22:43:32,483 EPOCH 668
2024-02-08 22:43:44,516 Epoch 668: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.65 
2024-02-08 22:43:44,517 EPOCH 669
2024-02-08 23:17:41,841 Epoch 669: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-08 23:17:41,843 EPOCH 670
2024-02-09 00:39:05,452 Epoch 670: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-09 00:39:05,454 EPOCH 671
2024-02-09 01:12:15,192 [Epoch: 671 Step: 00011400] Batch Recognition Loss:   0.000375 => Gls Tokens per Sec:        3 || Batch Translation Loss:   0.021933 => Txt Tokens per Sec:        8 || Lr: 0.000100
2024-02-09 01:28:52,431 Epoch 671: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-09 01:28:52,432 EPOCH 672
2024-02-09 02:04:48,668 Epoch 672: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-09 02:04:48,670 EPOCH 673
2024-02-09 02:20:50,254 Epoch 673: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-09 02:20:50,256 EPOCH 674
2024-02-09 02:22:50,978 Epoch 674: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-09 02:22:50,979 EPOCH 675
2024-02-09 02:24:44,335 Epoch 675: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.61 
2024-02-09 02:24:44,336 EPOCH 676
2024-02-09 02:26:30,950 Epoch 676: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.77 
2024-02-09 02:26:30,952 EPOCH 677
2024-02-09 02:27:23,926 [Epoch: 677 Step: 00011500] Batch Recognition Loss:   0.002000 => Gls Tokens per Sec:       97 || Batch Translation Loss:   0.043501 => Txt Tokens per Sec:      256 || Lr: 0.000100
2024-02-09 02:28:30,978 Epoch 677: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.87 
2024-02-09 02:28:30,979 EPOCH 678
2024-02-09 02:30:09,400 Epoch 678: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.12 
2024-02-09 02:30:09,401 EPOCH 679
2024-02-09 02:30:20,957 Epoch 679: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.07 
2024-02-09 02:30:20,957 EPOCH 680
2024-02-09 02:30:32,090 Epoch 680: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.12 
2024-02-09 02:30:32,091 EPOCH 681
2024-02-09 02:30:43,198 Epoch 681: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.86 
2024-02-09 02:30:43,198 EPOCH 682
2024-02-09 02:30:54,237 Epoch 682: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.72 
2024-02-09 02:30:54,237 EPOCH 683
2024-02-09 02:30:59,507 [Epoch: 683 Step: 00011600] Batch Recognition Loss:   0.001595 => Gls Tokens per Sec:      680 || Batch Translation Loss:   0.043649 => Txt Tokens per Sec:     1908 || Lr: 0.000100
2024-02-09 02:31:05,220 Epoch 683: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.65 
2024-02-09 02:31:05,220 EPOCH 684
2024-02-09 02:31:16,347 Epoch 684: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-09 02:31:16,347 EPOCH 685
2024-02-09 02:31:27,299 Epoch 685: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-09 02:31:27,300 EPOCH 686
2024-02-09 02:31:38,243 Epoch 686: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.70 
2024-02-09 02:31:38,245 EPOCH 687
2024-02-09 02:31:49,444 Epoch 687: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.97 
2024-02-09 02:31:49,444 EPOCH 688
2024-02-09 02:32:00,524 Epoch 688: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.32 
2024-02-09 02:32:00,524 EPOCH 689
2024-02-09 02:32:01,291 [Epoch: 689 Step: 00011700] Batch Recognition Loss:   0.000757 => Gls Tokens per Sec:     3351 || Batch Translation Loss:   0.034834 => Txt Tokens per Sec:     8995 || Lr: 0.000100
2024-02-09 02:32:11,377 Epoch 689: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.86 
2024-02-09 02:32:11,379 EPOCH 690
2024-02-09 02:32:22,606 Epoch 690: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.93 
2024-02-09 02:32:22,606 EPOCH 691
2024-02-09 02:32:33,734 Epoch 691: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.07 
2024-02-09 02:32:33,735 EPOCH 692
2024-02-09 02:32:44,760 Epoch 692: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.11 
2024-02-09 02:32:44,760 EPOCH 693
2024-02-09 02:32:55,808 Epoch 693: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.56 
2024-02-09 02:32:55,808 EPOCH 694
2024-02-09 02:33:07,088 Epoch 694: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.27 
2024-02-09 02:33:07,088 EPOCH 695
2024-02-09 02:33:07,548 [Epoch: 695 Step: 00011800] Batch Recognition Loss:   0.000546 => Gls Tokens per Sec:     2795 || Batch Translation Loss:   0.046202 => Txt Tokens per Sec:     8057 || Lr: 0.000100
2024-02-09 02:33:17,920 Epoch 695: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.38 
2024-02-09 02:33:17,920 EPOCH 696
2024-02-09 02:33:28,947 Epoch 696: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.83 
2024-02-09 02:33:28,948 EPOCH 697
2024-02-09 02:33:40,070 Epoch 697: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.66 
2024-02-09 02:33:40,071 EPOCH 698
2024-02-09 02:33:50,969 Epoch 698: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.42 
2024-02-09 02:33:50,969 EPOCH 699
2024-02-09 02:34:01,929 Epoch 699: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.44 
2024-02-09 02:34:01,930 EPOCH 700
2024-02-09 02:34:13,164 [Epoch: 700 Step: 00011900] Batch Recognition Loss:   0.001221 => Gls Tokens per Sec:      946 || Batch Translation Loss:   0.114504 => Txt Tokens per Sec:     2616 || Lr: 0.000100
2024-02-09 02:34:13,164 Epoch 700: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.03 
2024-02-09 02:34:13,164 EPOCH 701
2024-02-09 02:34:24,219 Epoch 701: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.95 
2024-02-09 02:34:24,219 EPOCH 702
2024-02-09 02:34:35,249 Epoch 702: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.18 
2024-02-09 02:34:35,250 EPOCH 703
2024-02-09 02:34:46,307 Epoch 703: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.26 
2024-02-09 02:34:46,308 EPOCH 704
2024-02-09 02:34:57,227 Epoch 704: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.48 
2024-02-09 02:34:57,227 EPOCH 705
2024-02-09 02:35:08,349 Epoch 705: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.59 
2024-02-09 02:35:08,350 EPOCH 706
2024-02-09 02:35:19,246 [Epoch: 706 Step: 00012000] Batch Recognition Loss:   0.001074 => Gls Tokens per Sec:      857 || Batch Translation Loss:   0.054978 => Txt Tokens per Sec:     2411 || Lr: 0.000100
2024-02-09 02:36:00,335 Validation result at epoch 706, step    12000: duration: 41.0894s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.94851	Translation Loss: 91750.09375	PPL: 9547.28320
	Eval Metric: BLEU
	WER 4.59	(DEL: 0.00,	INS: 0.00,	SUB: 4.59)
	BLEU-4 0.49	(BLEU-1: 10.66,	BLEU-2: 3.33,	BLEU-3: 1.26,	BLEU-4: 0.49)
	CHRF 16.76	ROUGE 9.14
2024-02-09 02:36:00,336 Logging Recognition and Translation Outputs
2024-02-09 02:36:00,336 ========================================================================================================================
2024-02-09 02:36:00,337 Logging Sequence: 168_56.00
2024-02-09 02:36:00,337 	Gloss Reference :	A B+C+D+E
2024-02-09 02:36:00,337 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 02:36:00,337 	Gloss Alignment :	         
2024-02-09 02:36:00,337 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 02:36:00,338 	Text Reference  :	fans have    been waiting to          see vamika for      a  long time 
2024-02-09 02:36:00,338 	Text Hypothesis :	**** sameeha is   amazing performance the most   followed by 8    years
2024-02-09 02:36:00,339 	Text Alignment  :	D    S       S    S       S           S   S      S        S  S    S    
2024-02-09 02:36:00,339 ========================================================================================================================
2024-02-09 02:36:00,339 Logging Sequence: 161_74.00
2024-02-09 02:36:00,339 	Gloss Reference :	A B+C+D+E
2024-02-09 02:36:00,339 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 02:36:00,339 	Gloss Alignment :	         
2024-02-09 02:36:00,340 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 02:36:00,340 	Text Reference  :	*** ****** **** **** i   am    proud of  the    indian  team's achievements
2024-02-09 02:36:00,340 	Text Hypothesis :	the second time they are angry at    sky sports stadium in     mumbai      
2024-02-09 02:36:00,341 	Text Alignment  :	I   I      I    I    S   S     S     S   S      S       S      S           
2024-02-09 02:36:00,341 ========================================================================================================================
2024-02-09 02:36:00,341 Logging Sequence: 111_83.00
2024-02-09 02:36:00,341 	Gloss Reference :	A B+C+D+E
2024-02-09 02:36:00,341 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 02:36:00,341 	Gloss Alignment :	         
2024-02-09 02:36:00,342 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 02:36:00,344 	Text Reference  :	and the other 10        team members    are fined 25  of the match ** *** ******* *** ** **** **** fee or   rs   6   lakh
2024-02-09 02:36:00,344 	Text Hypothesis :	*** *** ***** over-rate is   calculated at  the   end of the match by the umpires and if they find the over rate was slow
2024-02-09 02:36:00,344 	Text Alignment  :	D   D   D     S         S    S          S   S     S                I  I   I       I   I  I    I    S   S    S    S   S   
2024-02-09 02:36:00,344 ========================================================================================================================
2024-02-09 02:36:00,344 Logging Sequence: 61_218.00
2024-02-09 02:36:00,344 	Gloss Reference :	A B+C+D+E
2024-02-09 02:36:00,345 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 02:36:00,345 	Gloss Alignment :	         
2024-02-09 02:36:00,345 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 02:36:00,346 	Text Reference  :	in 2020 a    woman had said at     the press conference
2024-02-09 02:36:00,346 	Text Hypothesis :	in **** 2019 shaw  has been become a   very  well      
2024-02-09 02:36:00,346 	Text Alignment  :	   D    S    S     S   S    S      S   S     S         
2024-02-09 02:36:00,346 ========================================================================================================================
2024-02-09 02:36:00,346 Logging Sequence: 94_123.00
2024-02-09 02:36:00,346 	Gloss Reference :	A B+C+D+E
2024-02-09 02:36:00,346 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 02:36:00,347 	Gloss Alignment :	         
2024-02-09 02:36:00,347 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 02:36:00,349 	Text Reference  :	*** the venue narendra modi stadium for the india-pakistan match  has been   kept the ******** *** *** same    people can book flights etc  
2024-02-09 02:36:00,349 	Text Hypothesis :	for the ***** ******** one  day     of  the series         should be  played at   the stadiums and not majorly over   it  does not     known
2024-02-09 02:36:00,349 	Text Alignment  :	I       D     D        S    S       S       S              S      S   S      S        I        I   I   S       S      S   S    S       S    
2024-02-09 02:36:00,350 ========================================================================================================================
2024-02-09 02:36:00,842 Epoch 706: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.40 
2024-02-09 02:36:00,843 EPOCH 707
2024-02-09 02:36:12,637 Epoch 707: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.68 
2024-02-09 02:36:12,638 EPOCH 708
2024-02-09 02:36:23,902 Epoch 708: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.43 
2024-02-09 02:36:23,902 EPOCH 709
2024-02-09 02:36:35,066 Epoch 709: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.30 
2024-02-09 02:36:35,067 EPOCH 710
2024-02-09 02:36:46,068 Epoch 710: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.16 
2024-02-09 02:36:46,069 EPOCH 711
2024-02-09 02:36:57,054 Epoch 711: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.15 
2024-02-09 02:36:57,055 EPOCH 712
2024-02-09 02:37:07,146 [Epoch: 712 Step: 00012100] Batch Recognition Loss:   0.000769 => Gls Tokens per Sec:      799 || Batch Translation Loss:   0.077024 => Txt Tokens per Sec:     2225 || Lr: 0.000100
2024-02-09 02:37:08,101 Epoch 712: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.85 
2024-02-09 02:37:08,101 EPOCH 713
2024-02-09 02:37:19,054 Epoch 713: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.95 
2024-02-09 02:37:19,054 EPOCH 714
2024-02-09 02:37:30,382 Epoch 714: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.90 
2024-02-09 02:37:30,383 EPOCH 715
2024-02-09 02:37:41,304 Epoch 715: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.81 
2024-02-09 02:37:41,304 EPOCH 716
2024-02-09 02:37:52,578 Epoch 716: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.85 
2024-02-09 02:37:52,579 EPOCH 717
2024-02-09 02:38:03,578 Epoch 717: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.03 
2024-02-09 02:38:03,578 EPOCH 718
2024-02-09 02:38:11,882 [Epoch: 718 Step: 00012200] Batch Recognition Loss:   0.000452 => Gls Tokens per Sec:      817 || Batch Translation Loss:   0.027098 => Txt Tokens per Sec:     2364 || Lr: 0.000100
2024-02-09 02:38:14,670 Epoch 718: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.85 
2024-02-09 02:38:14,671 EPOCH 719
2024-02-09 02:38:25,356 Epoch 719: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.84 
2024-02-09 02:38:25,356 EPOCH 720
2024-02-09 02:38:36,224 Epoch 720: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-09 02:38:36,225 EPOCH 721
2024-02-09 02:38:47,240 Epoch 721: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.70 
2024-02-09 02:38:47,241 EPOCH 722
2024-02-09 02:38:58,642 Epoch 722: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.67 
2024-02-09 02:38:58,642 EPOCH 723
2024-02-09 02:39:09,937 Epoch 723: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.60 
2024-02-09 02:39:09,938 EPOCH 724
2024-02-09 02:39:13,871 [Epoch: 724 Step: 00012300] Batch Recognition Loss:   0.000876 => Gls Tokens per Sec:     1465 || Batch Translation Loss:   0.010216 => Txt Tokens per Sec:     3975 || Lr: 0.000100
2024-02-09 02:39:21,006 Epoch 724: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.53 
2024-02-09 02:39:21,006 EPOCH 725
2024-02-09 02:39:32,073 Epoch 725: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-09 02:39:32,073 EPOCH 726
2024-02-09 02:39:42,958 Epoch 726: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-09 02:39:42,959 EPOCH 727
2024-02-09 02:39:53,970 Epoch 727: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-09 02:39:53,971 EPOCH 728
2024-02-09 02:40:04,661 Epoch 728: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.39 
2024-02-09 02:40:04,661 EPOCH 729
2024-02-09 02:40:15,720 Epoch 729: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-09 02:40:15,720 EPOCH 730
2024-02-09 02:40:18,465 [Epoch: 730 Step: 00012400] Batch Recognition Loss:   0.003165 => Gls Tokens per Sec:     1633 || Batch Translation Loss:   0.018688 => Txt Tokens per Sec:     4253 || Lr: 0.000100
2024-02-09 02:40:26,546 Epoch 730: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.32 
2024-02-09 02:40:26,547 EPOCH 731
2024-02-09 02:40:37,518 Epoch 731: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-09 02:40:37,518 EPOCH 732
2024-02-09 02:40:48,229 Epoch 732: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.41 
2024-02-09 02:40:48,230 EPOCH 733
2024-02-09 02:40:59,406 Epoch 733: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.52 
2024-02-09 02:40:59,406 EPOCH 734
2024-02-09 02:41:10,374 Epoch 734: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.70 
2024-02-09 02:41:10,375 EPOCH 735
2024-02-09 02:41:21,414 Epoch 735: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.84 
2024-02-09 02:41:21,414 EPOCH 736
2024-02-09 02:41:26,347 [Epoch: 736 Step: 00012500] Batch Recognition Loss:   0.000410 => Gls Tokens per Sec:      596 || Batch Translation Loss:   0.099937 => Txt Tokens per Sec:     1862 || Lr: 0.000100
2024-02-09 02:41:32,385 Epoch 736: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.68 
2024-02-09 02:41:32,385 EPOCH 737
2024-02-09 02:41:43,608 Epoch 737: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.51 
2024-02-09 02:41:43,609 EPOCH 738
2024-02-09 02:41:54,493 Epoch 738: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.29 
2024-02-09 02:41:54,493 EPOCH 739
2024-02-09 02:42:05,457 Epoch 739: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.10 
2024-02-09 02:42:05,458 EPOCH 740
2024-02-09 02:42:16,428 Epoch 740: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.57 
2024-02-09 02:42:16,429 EPOCH 741
2024-02-09 02:42:27,353 Epoch 741: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.66 
2024-02-09 02:42:27,354 EPOCH 742
2024-02-09 02:42:29,644 [Epoch: 742 Step: 00012600] Batch Recognition Loss:   0.000885 => Gls Tokens per Sec:      838 || Batch Translation Loss:   0.056667 => Txt Tokens per Sec:     2170 || Lr: 0.000100
2024-02-09 02:42:38,186 Epoch 742: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.03 
2024-02-09 02:42:38,187 EPOCH 743
2024-02-09 02:42:49,348 Epoch 743: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.86 
2024-02-09 02:42:49,348 EPOCH 744
2024-02-09 02:43:00,161 Epoch 744: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.46 
2024-02-09 02:43:00,161 EPOCH 745
2024-02-09 02:43:11,223 Epoch 745: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.58 
2024-02-09 02:43:11,224 EPOCH 746
2024-02-09 02:43:22,512 Epoch 746: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.37 
2024-02-09 02:43:22,513 EPOCH 747
2024-02-09 02:43:33,689 Epoch 747: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.12 
2024-02-09 02:43:33,690 EPOCH 748
2024-02-09 02:43:33,986 [Epoch: 748 Step: 00012700] Batch Recognition Loss:   0.001087 => Gls Tokens per Sec:     2170 || Batch Translation Loss:   0.057611 => Txt Tokens per Sec:     6051 || Lr: 0.000100
2024-02-09 02:43:44,967 Epoch 748: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.00 
2024-02-09 02:43:44,967 EPOCH 749
2024-02-09 02:43:55,591 Epoch 749: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.75 
2024-02-09 02:43:55,591 EPOCH 750
2024-02-09 02:44:06,427 Epoch 750: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.69 
2024-02-09 02:44:06,428 EPOCH 751
2024-02-09 02:44:17,407 Epoch 751: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.76 
2024-02-09 02:44:17,408 EPOCH 752
2024-02-09 02:44:28,275 Epoch 752: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.81 
2024-02-09 02:44:28,276 EPOCH 753
2024-02-09 02:44:38,707 [Epoch: 753 Step: 00012800] Batch Recognition Loss:   0.000565 => Gls Tokens per Sec:      957 || Batch Translation Loss:   0.070360 => Txt Tokens per Sec:     2682 || Lr: 0.000100
2024-02-09 02:44:38,835 Epoch 753: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.28 
2024-02-09 02:44:38,835 EPOCH 754
2024-02-09 02:44:49,968 Epoch 754: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.95 
2024-02-09 02:44:49,968 EPOCH 755
2024-02-09 02:45:01,122 Epoch 755: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.16 
2024-02-09 02:45:01,123 EPOCH 756
2024-02-09 02:45:12,267 Epoch 756: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.52 
2024-02-09 02:45:12,268 EPOCH 757
2024-02-09 02:45:22,925 Epoch 757: Total Training Recognition Loss 0.05  Total Training Translation Loss 5.97 
2024-02-09 02:45:22,925 EPOCH 758
2024-02-09 02:45:33,820 Epoch 758: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.36 
2024-02-09 02:45:33,821 EPOCH 759
2024-02-09 02:45:42,600 [Epoch: 759 Step: 00012900] Batch Recognition Loss:   0.001223 => Gls Tokens per Sec:      991 || Batch Translation Loss:   0.091142 => Txt Tokens per Sec:     2656 || Lr: 0.000100
2024-02-09 02:45:45,019 Epoch 759: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.91 
2024-02-09 02:45:45,019 EPOCH 760
2024-02-09 02:45:56,404 Epoch 760: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.28 
2024-02-09 02:45:56,404 EPOCH 761
2024-02-09 02:46:07,306 Epoch 761: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.91 
2024-02-09 02:46:07,307 EPOCH 762
2024-02-09 02:46:18,386 Epoch 762: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.84 
2024-02-09 02:46:18,386 EPOCH 763
2024-02-09 02:46:29,392 Epoch 763: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.82 
2024-02-09 02:46:29,393 EPOCH 764
2024-02-09 02:46:40,328 Epoch 764: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.92 
2024-02-09 02:46:40,328 EPOCH 765
2024-02-09 02:46:48,787 [Epoch: 765 Step: 00013000] Batch Recognition Loss:   0.000706 => Gls Tokens per Sec:      877 || Batch Translation Loss:   0.029580 => Txt Tokens per Sec:     2419 || Lr: 0.000100
2024-02-09 02:46:51,566 Epoch 765: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.74 
2024-02-09 02:46:51,566 EPOCH 766
2024-02-09 02:47:02,632 Epoch 766: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.52 
2024-02-09 02:47:02,633 EPOCH 767
2024-02-09 02:47:13,862 Epoch 767: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.57 
2024-02-09 02:47:13,863 EPOCH 768
2024-02-09 02:47:24,543 Epoch 768: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.67 
2024-02-09 02:47:24,544 EPOCH 769
2024-02-09 02:47:35,623 Epoch 769: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.43 
2024-02-09 02:47:35,624 EPOCH 770
2024-02-09 02:47:46,622 Epoch 770: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.42 
2024-02-09 02:47:46,623 EPOCH 771
2024-02-09 02:47:50,023 [Epoch: 771 Step: 00013100] Batch Recognition Loss:   0.005532 => Gls Tokens per Sec:     1884 || Batch Translation Loss:   0.009374 => Txt Tokens per Sec:     4913 || Lr: 0.000100
2024-02-09 02:47:57,481 Epoch 771: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.48 
2024-02-09 02:47:57,482 EPOCH 772
2024-02-09 02:48:08,549 Epoch 772: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-09 02:48:08,549 EPOCH 773
2024-02-09 02:48:19,614 Epoch 773: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-09 02:48:19,614 EPOCH 774
2024-02-09 02:48:30,653 Epoch 774: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.44 
2024-02-09 02:48:30,653 EPOCH 775
2024-02-09 02:48:41,704 Epoch 775: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-09 02:48:41,704 EPOCH 776
2024-02-09 02:48:52,765 Epoch 776: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-09 02:48:52,765 EPOCH 777
2024-02-09 02:48:55,985 [Epoch: 777 Step: 00013200] Batch Recognition Loss:   0.000350 => Gls Tokens per Sec:     1591 || Batch Translation Loss:   0.018202 => Txt Tokens per Sec:     4276 || Lr: 0.000100
2024-02-09 02:49:03,720 Epoch 777: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-09 02:49:03,721 EPOCH 778
2024-02-09 02:49:15,003 Epoch 778: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-09 02:49:15,003 EPOCH 779
2024-02-09 02:49:26,097 Epoch 779: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-09 02:49:26,097 EPOCH 780
2024-02-09 02:49:37,394 Epoch 780: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-09 02:49:37,394 EPOCH 781
2024-02-09 02:49:48,587 Epoch 781: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-09 02:49:48,587 EPOCH 782
2024-02-09 02:49:59,783 Epoch 782: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.94 
2024-02-09 02:49:59,784 EPOCH 783
2024-02-09 02:50:02,627 [Epoch: 783 Step: 00013300] Batch Recognition Loss:   0.001523 => Gls Tokens per Sec:     1351 || Batch Translation Loss:   0.028318 => Txt Tokens per Sec:     3773 || Lr: 0.000100
2024-02-09 02:50:10,808 Epoch 783: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.78 
2024-02-09 02:50:10,809 EPOCH 784
2024-02-09 02:50:22,125 Epoch 784: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.05 
2024-02-09 02:50:22,126 EPOCH 785
2024-02-09 02:50:32,866 Epoch 785: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.33 
2024-02-09 02:50:32,867 EPOCH 786
2024-02-09 02:50:44,180 Epoch 786: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.75 
2024-02-09 02:50:44,180 EPOCH 787
2024-02-09 02:50:55,163 Epoch 787: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.99 
2024-02-09 02:50:55,164 EPOCH 788
2024-02-09 02:51:06,223 Epoch 788: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.33 
2024-02-09 02:51:06,224 EPOCH 789
2024-02-09 02:51:12,934 [Epoch: 789 Step: 00013400] Batch Recognition Loss:   0.002454 => Gls Tokens per Sec:      343 || Batch Translation Loss:   0.059763 => Txt Tokens per Sec:     1147 || Lr: 0.000100
2024-02-09 02:51:17,317 Epoch 789: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.37 
2024-02-09 02:51:17,317 EPOCH 790
2024-02-09 02:51:28,489 Epoch 790: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.24 
2024-02-09 02:51:28,489 EPOCH 791
2024-02-09 02:51:39,680 Epoch 791: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.08 
2024-02-09 02:51:39,681 EPOCH 792
2024-02-09 02:51:50,886 Epoch 792: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.91 
2024-02-09 02:51:50,886 EPOCH 793
2024-02-09 02:52:02,040 Epoch 793: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.89 
2024-02-09 02:52:02,041 EPOCH 794
2024-02-09 02:52:13,249 Epoch 794: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.89 
2024-02-09 02:52:13,249 EPOCH 795
2024-02-09 02:52:15,145 [Epoch: 795 Step: 00013500] Batch Recognition Loss:   0.001468 => Gls Tokens per Sec:      675 || Batch Translation Loss:   0.045126 => Txt Tokens per Sec:     2122 || Lr: 0.000100
2024-02-09 02:52:24,331 Epoch 795: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.69 
2024-02-09 02:52:24,331 EPOCH 796
2024-02-09 02:52:35,209 Epoch 796: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-09 02:52:35,210 EPOCH 797
2024-02-09 02:52:46,257 Epoch 797: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-09 02:52:46,258 EPOCH 798
2024-02-09 02:52:57,219 Epoch 798: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-09 02:52:57,220 EPOCH 799
2024-02-09 02:53:08,550 Epoch 799: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-09 02:53:08,550 EPOCH 800
2024-02-09 02:53:19,804 [Epoch: 800 Step: 00013600] Batch Recognition Loss:   0.000431 => Gls Tokens per Sec:      944 || Batch Translation Loss:   0.021330 => Txt Tokens per Sec:     2611 || Lr: 0.000100
2024-02-09 02:53:19,805 Epoch 800: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-09 02:53:19,805 EPOCH 801
2024-02-09 02:53:31,002 Epoch 801: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.35 
2024-02-09 02:53:31,002 EPOCH 802
2024-02-09 02:53:42,228 Epoch 802: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.37 
2024-02-09 02:53:42,229 EPOCH 803
2024-02-09 02:53:53,278 Epoch 803: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.33 
2024-02-09 02:53:53,279 EPOCH 804
2024-02-09 02:54:04,274 Epoch 804: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-09 02:54:04,274 EPOCH 805
2024-02-09 02:54:15,715 Epoch 805: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-09 02:54:15,715 EPOCH 806
2024-02-09 02:54:25,078 [Epoch: 806 Step: 00013700] Batch Recognition Loss:   0.001329 => Gls Tokens per Sec:      998 || Batch Translation Loss:   0.023112 => Txt Tokens per Sec:     2713 || Lr: 0.000100
2024-02-09 02:54:26,993 Epoch 806: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-09 02:54:26,993 EPOCH 807
2024-02-09 02:54:38,021 Epoch 807: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-09 02:54:38,021 EPOCH 808
2024-02-09 02:54:48,872 Epoch 808: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-09 02:54:48,872 EPOCH 809
2024-02-09 02:54:59,904 Epoch 809: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-09 02:54:59,905 EPOCH 810
2024-02-09 02:55:11,166 Epoch 810: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-09 02:55:11,167 EPOCH 811
2024-02-09 02:55:22,220 Epoch 811: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.60 
2024-02-09 02:55:22,220 EPOCH 812
2024-02-09 02:55:30,567 [Epoch: 812 Step: 00013800] Batch Recognition Loss:   0.000230 => Gls Tokens per Sec:      966 || Batch Translation Loss:   0.028890 => Txt Tokens per Sec:     2602 || Lr: 0.000100
2024-02-09 02:55:33,438 Epoch 812: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.56 
2024-02-09 02:55:33,438 EPOCH 813
2024-02-09 02:55:44,393 Epoch 813: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-09 02:55:44,394 EPOCH 814
2024-02-09 02:55:55,453 Epoch 814: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-09 02:55:55,454 EPOCH 815
2024-02-09 02:56:06,890 Epoch 815: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.86 
2024-02-09 02:56:06,891 EPOCH 816
2024-02-09 02:56:17,821 Epoch 816: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.91 
2024-02-09 02:56:17,822 EPOCH 817
2024-02-09 02:56:29,149 Epoch 817: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.14 
2024-02-09 02:56:29,149 EPOCH 818
2024-02-09 02:56:37,540 [Epoch: 818 Step: 00013900] Batch Recognition Loss:   0.003384 => Gls Tokens per Sec:      808 || Batch Translation Loss:   0.138317 => Txt Tokens per Sec:     2324 || Lr: 0.000100
2024-02-09 02:56:40,441 Epoch 818: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.71 
2024-02-09 02:56:40,442 EPOCH 819
2024-02-09 02:56:51,475 Epoch 819: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.88 
2024-02-09 02:56:51,476 EPOCH 820
2024-02-09 02:57:02,611 Epoch 820: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.90 
2024-02-09 02:57:02,611 EPOCH 821
2024-02-09 02:57:13,626 Epoch 821: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.34 
2024-02-09 02:57:13,627 EPOCH 822
2024-02-09 02:57:24,823 Epoch 822: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.54 
2024-02-09 02:57:24,824 EPOCH 823
2024-02-09 02:57:35,911 Epoch 823: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.99 
2024-02-09 02:57:35,911 EPOCH 824
2024-02-09 02:57:40,348 [Epoch: 824 Step: 00014000] Batch Recognition Loss:   0.000593 => Gls Tokens per Sec:     1240 || Batch Translation Loss:   0.123088 => Txt Tokens per Sec:     3455 || Lr: 0.000100
2024-02-09 02:58:21,521 Validation result at epoch 824, step    14000: duration: 41.1725s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.90456	Translation Loss: 92802.53906	PPL: 10605.52930
	Eval Metric: BLEU
	WER 4.66	(DEL: 0.00,	INS: 0.00,	SUB: 4.66)
	BLEU-4 0.35	(BLEU-1: 11.06,	BLEU-2: 3.05,	BLEU-3: 1.02,	BLEU-4: 0.35)
	CHRF 17.01	ROUGE 9.14
2024-02-09 02:58:21,524 Logging Recognition and Translation Outputs
2024-02-09 02:58:21,524 ========================================================================================================================
2024-02-09 02:58:21,524 Logging Sequence: 177_50.00
2024-02-09 02:58:21,524 	Gloss Reference :	A B+C+D+E
2024-02-09 02:58:21,524 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 02:58:21,524 	Gloss Alignment :	         
2024-02-09 02:58:21,525 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 02:58:21,526 	Text Reference  :	a similar reward of rs       50000 was   announced for ***** ***** information against his    associate ajay kumar
2024-02-09 02:58:21,526 	Text Hypothesis :	* then    came   in lockdown for   delhi police    for rana' death on          10th    august which     is   delhi
2024-02-09 02:58:21,527 	Text Alignment  :	D S       S      S  S        S     S     S             I     I     S           S       S      S         S    S    
2024-02-09 02:58:21,527 ========================================================================================================================
2024-02-09 02:58:21,527 Logging Sequence: 136_175.00
2024-02-09 02:58:21,527 	Gloss Reference :	A B+C+D+E  
2024-02-09 02:58:21,527 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-09 02:58:21,527 	Gloss Alignment :	  S        
2024-02-09 02:58:21,527 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 02:58:21,528 	Text Reference  :	after 49 years india' hockey team beat  britain and  qualified for the semi-finals
2024-02-09 02:58:21,528 	Text Hypothesis :	***** ** ***** ****** ****** the  first case    will start     in  the tournament 
2024-02-09 02:58:21,529 	Text Alignment  :	D     D  D     D      D      S    S     S       S    S         S       S          
2024-02-09 02:58:21,529 ========================================================================================================================
2024-02-09 02:58:21,529 Logging Sequence: 126_159.00
2024-02-09 02:58:21,529 	Gloss Reference :	A B+C+D+E
2024-02-09 02:58:21,529 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 02:58:21,529 	Gloss Alignment :	         
2024-02-09 02:58:21,529 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 02:58:21,530 	Text Reference  :	despite multiple challenges and injuries  you did not give up   
2024-02-09 02:58:21,530 	Text Hypothesis :	he      was      sad        and overjoyed as  he  was sure about
2024-02-09 02:58:21,530 	Text Alignment  :	S       S        S              S         S   S   S   S    S    
2024-02-09 02:58:21,531 ========================================================================================================================
2024-02-09 02:58:21,531 Logging Sequence: 70_88.00
2024-02-09 02:58:21,531 	Gloss Reference :	A B+C+D+E
2024-02-09 02:58:21,531 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 02:58:21,531 	Gloss Alignment :	         
2024-02-09 02:58:21,531 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 02:58:21,532 	Text Reference  :	******* two coca-cola bottles were placed  on  the  table next     to   the mic    
2024-02-09 02:58:21,532 	Text Hypothesis :	ronaldo has admitted  that    his  fitness but they were  infected with two matches
2024-02-09 02:58:21,533 	Text Alignment  :	I       S   S         S       S    S       S   S    S     S        S    S   S      
2024-02-09 02:58:21,533 ========================================================================================================================
2024-02-09 02:58:21,533 Logging Sequence: 54_201.00
2024-02-09 02:58:21,533 	Gloss Reference :	A B+C+D+E
2024-02-09 02:58:21,533 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 02:58:21,533 	Gloss Alignment :	         
2024-02-09 02:58:21,534 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 02:58:21,537 	Text Reference  :	there is a huge demand       mostly from    non-resident indians nris who  are    excited   to see          the match ******* and they    have booked the       hotel ******* ***** ** rooms
2024-02-09 02:58:21,537 	Text Hypothesis :	***** ** * the  middle-class fans   usually decide       at      the  last moment depending on availability of  match tickets if  tickets are  not    available hotel booking would go waste
2024-02-09 02:58:21,537 	Text Alignment  :	D     D  D S    S            S      S       S            S       S    S    S      S         S  S            S         I       S   S       S    S      S               I       I     I  S    
2024-02-09 02:58:21,537 ========================================================================================================================
2024-02-09 02:58:28,544 Epoch 824: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.76 
2024-02-09 02:58:28,544 EPOCH 825
2024-02-09 02:58:39,940 Epoch 825: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.97 
2024-02-09 02:58:39,940 EPOCH 826
2024-02-09 02:58:51,020 Epoch 826: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.63 
2024-02-09 02:58:51,021 EPOCH 827
2024-02-09 02:59:02,225 Epoch 827: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.33 
2024-02-09 02:59:02,226 EPOCH 828
2024-02-09 02:59:13,410 Epoch 828: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.16 
2024-02-09 02:59:13,411 EPOCH 829
2024-02-09 02:59:24,560 Epoch 829: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.18 
2024-02-09 02:59:24,561 EPOCH 830
2024-02-09 02:59:28,554 [Epoch: 830 Step: 00014100] Batch Recognition Loss:   0.001365 => Gls Tokens per Sec:     1057 || Batch Translation Loss:   0.065957 => Txt Tokens per Sec:     2877 || Lr: 0.000100
2024-02-09 02:59:35,618 Epoch 830: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.47 
2024-02-09 02:59:35,618 EPOCH 831
2024-02-09 02:59:46,671 Epoch 831: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.06 
2024-02-09 02:59:46,671 EPOCH 832
2024-02-09 02:59:57,801 Epoch 832: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.92 
2024-02-09 02:59:57,802 EPOCH 833
2024-02-09 03:00:08,723 Epoch 833: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.96 
2024-02-09 03:00:08,724 EPOCH 834
2024-02-09 03:00:19,710 Epoch 834: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.75 
2024-02-09 03:00:19,710 EPOCH 835
2024-02-09 03:00:30,765 Epoch 835: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.95 
2024-02-09 03:00:30,766 EPOCH 836
2024-02-09 03:00:37,190 [Epoch: 836 Step: 00014200] Batch Recognition Loss:   0.000726 => Gls Tokens per Sec:      458 || Batch Translation Loss:   0.059959 => Txt Tokens per Sec:     1361 || Lr: 0.000100
2024-02-09 03:00:41,802 Epoch 836: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.84 
2024-02-09 03:00:41,802 EPOCH 837
2024-02-09 03:00:52,435 Epoch 837: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-09 03:00:52,435 EPOCH 838
2024-02-09 03:01:03,639 Epoch 838: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.64 
2024-02-09 03:01:03,639 EPOCH 839
2024-02-09 03:01:14,381 Epoch 839: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.54 
2024-02-09 03:01:14,382 EPOCH 840
2024-02-09 03:01:25,460 Epoch 840: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-09 03:01:25,461 EPOCH 841
2024-02-09 03:01:36,835 Epoch 841: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-09 03:01:36,836 EPOCH 842
2024-02-09 03:01:39,034 [Epoch: 842 Step: 00014300] Batch Recognition Loss:   0.000275 => Gls Tokens per Sec:      874 || Batch Translation Loss:   0.040257 => Txt Tokens per Sec:     2415 || Lr: 0.000100
2024-02-09 03:01:47,978 Epoch 842: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-09 03:01:47,979 EPOCH 843
2024-02-09 03:01:58,882 Epoch 843: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-09 03:01:58,883 EPOCH 844
2024-02-09 03:02:09,788 Epoch 844: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-09 03:02:09,789 EPOCH 845
2024-02-09 03:02:20,507 Epoch 845: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.47 
2024-02-09 03:02:20,508 EPOCH 846
2024-02-09 03:02:31,564 Epoch 846: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-09 03:02:31,565 EPOCH 847
2024-02-09 03:02:42,401 Epoch 847: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.59 
2024-02-09 03:02:42,402 EPOCH 848
2024-02-09 03:02:42,547 [Epoch: 848 Step: 00014400] Batch Recognition Loss:   0.000551 => Gls Tokens per Sec:     4444 || Batch Translation Loss:   0.054661 => Txt Tokens per Sec:     9083 || Lr: 0.000100
2024-02-09 03:02:53,185 Epoch 848: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-09 03:02:53,186 EPOCH 849
2024-02-09 03:03:04,430 Epoch 849: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.54 
2024-02-09 03:03:04,431 EPOCH 850
2024-02-09 03:03:15,485 Epoch 850: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.49 
2024-02-09 03:03:15,485 EPOCH 851
2024-02-09 03:03:26,675 Epoch 851: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-09 03:03:26,676 EPOCH 852
2024-02-09 03:03:37,774 Epoch 852: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.92 
2024-02-09 03:03:37,774 EPOCH 853
2024-02-09 03:03:48,670 [Epoch: 853 Step: 00014500] Batch Recognition Loss:   0.000866 => Gls Tokens per Sec:      916 || Batch Translation Loss:   0.020247 => Txt Tokens per Sec:     2579 || Lr: 0.000100
2024-02-09 03:03:48,804 Epoch 853: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.97 
2024-02-09 03:03:48,804 EPOCH 854
2024-02-09 03:03:59,804 Epoch 854: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.25 
2024-02-09 03:03:59,805 EPOCH 855
2024-02-09 03:04:10,886 Epoch 855: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.03 
2024-02-09 03:04:10,886 EPOCH 856
2024-02-09 03:04:22,141 Epoch 856: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.97 
2024-02-09 03:04:22,142 EPOCH 857
2024-02-09 03:04:33,231 Epoch 857: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-09 03:04:33,232 EPOCH 858
2024-02-09 03:04:44,330 Epoch 858: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.94 
2024-02-09 03:04:44,331 EPOCH 859
2024-02-09 03:04:54,728 [Epoch: 859 Step: 00014600] Batch Recognition Loss:   0.002216 => Gls Tokens per Sec:      837 || Batch Translation Loss:   0.047783 => Txt Tokens per Sec:     2340 || Lr: 0.000100
2024-02-09 03:04:55,425 Epoch 859: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.79 
2024-02-09 03:04:55,426 EPOCH 860
2024-02-09 03:05:06,828 Epoch 860: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.73 
2024-02-09 03:05:06,828 EPOCH 861
2024-02-09 03:05:17,894 Epoch 861: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.99 
2024-02-09 03:05:17,895 EPOCH 862
2024-02-09 03:05:28,974 Epoch 862: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.85 
2024-02-09 03:05:28,975 EPOCH 863
2024-02-09 03:05:39,149 Epoch 863: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.08 
2024-02-09 03:05:39,149 EPOCH 864
2024-02-09 03:05:49,964 Epoch 864: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.99 
2024-02-09 03:05:49,965 EPOCH 865
2024-02-09 03:05:54,459 [Epoch: 865 Step: 00014700] Batch Recognition Loss:   0.008881 => Gls Tokens per Sec:     1709 || Batch Translation Loss:   0.205247 => Txt Tokens per Sec:     4588 || Lr: 0.000100
2024-02-09 03:06:01,028 Epoch 865: Total Training Recognition Loss 0.32  Total Training Translation Loss 6.65 
2024-02-09 03:06:01,029 EPOCH 866
2024-02-09 03:06:12,078 Epoch 866: Total Training Recognition Loss 2.43  Total Training Translation Loss 6.58 
2024-02-09 03:06:12,079 EPOCH 867
2024-02-09 03:06:23,430 Epoch 867: Total Training Recognition Loss 2.79  Total Training Translation Loss 11.01 
2024-02-09 03:06:23,430 EPOCH 868
2024-02-09 03:06:34,098 Epoch 868: Total Training Recognition Loss 0.36  Total Training Translation Loss 3.73 
2024-02-09 03:06:34,098 EPOCH 869
2024-02-09 03:06:45,036 Epoch 869: Total Training Recognition Loss 0.18  Total Training Translation Loss 2.18 
2024-02-09 03:06:45,036 EPOCH 870
2024-02-09 03:06:56,232 Epoch 870: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.16 
2024-02-09 03:06:56,232 EPOCH 871
2024-02-09 03:07:04,094 [Epoch: 871 Step: 00014800] Batch Recognition Loss:   0.000730 => Gls Tokens per Sec:      781 || Batch Translation Loss:   0.043014 => Txt Tokens per Sec:     2151 || Lr: 0.000100
2024-02-09 03:07:07,320 Epoch 871: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.79 
2024-02-09 03:07:07,320 EPOCH 872
2024-02-09 03:07:18,132 Epoch 872: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.67 
2024-02-09 03:07:18,133 EPOCH 873
2024-02-09 03:07:29,142 Epoch 873: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.65 
2024-02-09 03:07:29,142 EPOCH 874
2024-02-09 03:07:40,191 Epoch 874: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.52 
2024-02-09 03:07:40,192 EPOCH 875
2024-02-09 03:07:51,213 Epoch 875: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.47 
2024-02-09 03:07:51,213 EPOCH 876
2024-02-09 03:08:02,333 Epoch 876: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.39 
2024-02-09 03:08:02,333 EPOCH 877
2024-02-09 03:08:05,487 [Epoch: 877 Step: 00014900] Batch Recognition Loss:   0.000401 => Gls Tokens per Sec:     1624 || Batch Translation Loss:   0.012882 => Txt Tokens per Sec:     4193 || Lr: 0.000100
2024-02-09 03:08:13,444 Epoch 877: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.39 
2024-02-09 03:08:13,445 EPOCH 878
2024-02-09 03:08:24,590 Epoch 878: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.39 
2024-02-09 03:08:24,590 EPOCH 879
2024-02-09 03:08:35,473 Epoch 879: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.36 
2024-02-09 03:08:35,474 EPOCH 880
2024-02-09 03:08:46,757 Epoch 880: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.36 
2024-02-09 03:08:46,757 EPOCH 881
2024-02-09 03:08:57,740 Epoch 881: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.33 
2024-02-09 03:08:57,741 EPOCH 882
2024-02-09 03:09:08,611 Epoch 882: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.35 
2024-02-09 03:09:08,612 EPOCH 883
2024-02-09 03:09:15,228 [Epoch: 883 Step: 00015000] Batch Recognition Loss:   0.000585 => Gls Tokens per Sec:      541 || Batch Translation Loss:   0.011360 => Txt Tokens per Sec:     1411 || Lr: 0.000100
2024-02-09 03:09:20,102 Epoch 883: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-09 03:09:20,102 EPOCH 884
2024-02-09 03:09:31,169 Epoch 884: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.31 
2024-02-09 03:09:31,169 EPOCH 885
2024-02-09 03:09:42,491 Epoch 885: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 03:09:42,492 EPOCH 886
2024-02-09 03:09:53,678 Epoch 886: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 03:09:53,678 EPOCH 887
2024-02-09 03:10:04,714 Epoch 887: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.26 
2024-02-09 03:10:04,714 EPOCH 888
2024-02-09 03:10:15,764 Epoch 888: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 03:10:15,764 EPOCH 889
2024-02-09 03:10:18,220 [Epoch: 889 Step: 00015100] Batch Recognition Loss:   0.000299 => Gls Tokens per Sec:     1043 || Batch Translation Loss:   0.010585 => Txt Tokens per Sec:     3059 || Lr: 0.000100
2024-02-09 03:10:26,802 Epoch 889: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 03:10:26,803 EPOCH 890
2024-02-09 03:10:37,887 Epoch 890: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-09 03:10:37,887 EPOCH 891
2024-02-09 03:10:48,938 Epoch 891: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-09 03:10:48,938 EPOCH 892
2024-02-09 03:11:00,341 Epoch 892: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 03:11:00,342 EPOCH 893
2024-02-09 03:11:11,518 Epoch 893: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 03:11:11,519 EPOCH 894
2024-02-09 03:11:22,529 Epoch 894: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.26 
2024-02-09 03:11:22,530 EPOCH 895
2024-02-09 03:11:22,968 [Epoch: 895 Step: 00015200] Batch Recognition Loss:   0.001203 => Gls Tokens per Sec:     2929 || Batch Translation Loss:   0.012701 => Txt Tokens per Sec:     8046 || Lr: 0.000100
2024-02-09 03:11:33,597 Epoch 895: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.25 
2024-02-09 03:11:33,597 EPOCH 896
2024-02-09 03:11:44,511 Epoch 896: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 03:11:44,512 EPOCH 897
2024-02-09 03:11:55,608 Epoch 897: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 03:11:55,608 EPOCH 898
2024-02-09 03:12:06,710 Epoch 898: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 03:12:06,711 EPOCH 899
2024-02-09 03:12:17,762 Epoch 899: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 03:12:17,762 EPOCH 900
2024-02-09 03:12:28,864 [Epoch: 900 Step: 00015300] Batch Recognition Loss:   0.000149 => Gls Tokens per Sec:      957 || Batch Translation Loss:   0.014109 => Txt Tokens per Sec:     2647 || Lr: 0.000100
2024-02-09 03:12:28,864 Epoch 900: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 03:12:28,865 EPOCH 901
2024-02-09 03:12:39,912 Epoch 901: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 03:12:39,912 EPOCH 902
2024-02-09 03:12:50,879 Epoch 902: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 03:12:50,879 EPOCH 903
2024-02-09 03:13:02,031 Epoch 903: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 03:13:02,032 EPOCH 904
2024-02-09 03:13:12,817 Epoch 904: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 03:13:12,818 EPOCH 905
2024-02-09 03:13:23,883 Epoch 905: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 03:13:23,884 EPOCH 906
2024-02-09 03:13:31,947 [Epoch: 906 Step: 00015400] Batch Recognition Loss:   0.000110 => Gls Tokens per Sec:     1191 || Batch Translation Loss:   0.015687 => Txt Tokens per Sec:     3230 || Lr: 0.000100
2024-02-09 03:13:34,966 Epoch 906: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 03:13:34,967 EPOCH 907
2024-02-09 03:13:46,230 Epoch 907: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 03:13:46,231 EPOCH 908
2024-02-09 03:13:57,450 Epoch 908: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.24 
2024-02-09 03:13:57,450 EPOCH 909
2024-02-09 03:14:08,496 Epoch 909: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 03:14:08,497 EPOCH 910
2024-02-09 03:14:19,398 Epoch 910: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 03:14:19,398 EPOCH 911
2024-02-09 03:14:30,390 Epoch 911: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 03:14:30,391 EPOCH 912
2024-02-09 03:14:37,380 [Epoch: 912 Step: 00015500] Batch Recognition Loss:   0.000193 => Gls Tokens per Sec:     1153 || Batch Translation Loss:   0.010727 => Txt Tokens per Sec:     3089 || Lr: 0.000100
2024-02-09 03:14:41,344 Epoch 912: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 03:14:41,344 EPOCH 913
2024-02-09 03:14:52,075 Epoch 913: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 03:14:52,076 EPOCH 914
2024-02-09 03:15:02,531 Epoch 914: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.24 
2024-02-09 03:15:02,531 EPOCH 915
2024-02-09 03:15:13,256 Epoch 915: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-09 03:15:13,257 EPOCH 916
2024-02-09 03:15:24,232 Epoch 916: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-09 03:15:24,232 EPOCH 917
2024-02-09 03:15:35,234 Epoch 917: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-09 03:15:35,234 EPOCH 918
2024-02-09 03:15:41,725 [Epoch: 918 Step: 00015600] Batch Recognition Loss:   0.002992 => Gls Tokens per Sec:     1045 || Batch Translation Loss:   0.054893 => Txt Tokens per Sec:     2819 || Lr: 0.000100
2024-02-09 03:15:46,292 Epoch 918: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.10 
2024-02-09 03:15:46,293 EPOCH 919
2024-02-09 03:15:57,101 Epoch 919: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.86 
2024-02-09 03:15:57,102 EPOCH 920
2024-02-09 03:16:08,169 Epoch 920: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.27 
2024-02-09 03:16:08,170 EPOCH 921
2024-02-09 03:16:19,839 Epoch 921: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.11 
2024-02-09 03:16:19,840 EPOCH 922
2024-02-09 03:16:31,718 Epoch 922: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.14 
2024-02-09 03:16:31,718 EPOCH 923
2024-02-09 03:16:42,782 Epoch 923: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.82 
2024-02-09 03:16:42,783 EPOCH 924
2024-02-09 03:16:48,589 [Epoch: 924 Step: 00015700] Batch Recognition Loss:   0.001341 => Gls Tokens per Sec:      948 || Batch Translation Loss:   0.223548 => Txt Tokens per Sec:     2431 || Lr: 0.000100
2024-02-09 03:16:54,044 Epoch 924: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.21 
2024-02-09 03:16:54,044 EPOCH 925
2024-02-09 03:17:05,050 Epoch 925: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.09 
2024-02-09 03:17:05,051 EPOCH 926
2024-02-09 03:17:15,997 Epoch 926: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.54 
2024-02-09 03:17:15,998 EPOCH 927
2024-02-09 03:17:26,986 Epoch 927: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.31 
2024-02-09 03:17:26,986 EPOCH 928
2024-02-09 03:17:38,202 Epoch 928: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.97 
2024-02-09 03:17:38,203 EPOCH 929
2024-02-09 03:17:49,244 Epoch 929: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-09 03:17:49,244 EPOCH 930
2024-02-09 03:17:50,745 [Epoch: 930 Step: 00015800] Batch Recognition Loss:   0.000522 => Gls Tokens per Sec:     2989 || Batch Translation Loss:   0.026605 => Txt Tokens per Sec:     7498 || Lr: 0.000100
2024-02-09 03:18:00,448 Epoch 930: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.67 
2024-02-09 03:18:00,449 EPOCH 931
2024-02-09 03:18:11,579 Epoch 931: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-09 03:18:11,580 EPOCH 932
2024-02-09 03:18:22,640 Epoch 932: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-09 03:18:22,640 EPOCH 933
2024-02-09 03:18:33,830 Epoch 933: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-09 03:18:33,830 EPOCH 934
2024-02-09 03:18:44,775 Epoch 934: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.69 
2024-02-09 03:18:44,775 EPOCH 935
2024-02-09 03:18:55,620 Epoch 935: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-09 03:18:55,620 EPOCH 936
2024-02-09 03:18:59,621 [Epoch: 936 Step: 00015900] Batch Recognition Loss:   0.000484 => Gls Tokens per Sec:      800 || Batch Translation Loss:   0.027572 => Txt Tokens per Sec:     2033 || Lr: 0.000100
2024-02-09 03:19:06,646 Epoch 936: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-09 03:19:06,647 EPOCH 937
2024-02-09 03:19:17,532 Epoch 937: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.59 
2024-02-09 03:19:17,532 EPOCH 938
2024-02-09 03:19:28,675 Epoch 938: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-09 03:19:28,676 EPOCH 939
2024-02-09 03:19:39,663 Epoch 939: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-09 03:19:39,664 EPOCH 940
2024-02-09 03:19:50,793 Epoch 940: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-09 03:19:50,794 EPOCH 941
2024-02-09 03:20:02,004 Epoch 941: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-09 03:20:02,004 EPOCH 942
2024-02-09 03:20:02,732 [Epoch: 942 Step: 00016000] Batch Recognition Loss:   0.000473 => Gls Tokens per Sec:     2645 || Batch Translation Loss:   0.023817 => Txt Tokens per Sec:     6596 || Lr: 0.000100
2024-02-09 03:20:43,825 Validation result at epoch 942, step    16000: duration: 41.0923s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.73278	Translation Loss: 92758.89844	PPL: 10559.39746
	Eval Metric: BLEU
	WER 4.31	(DEL: 0.00,	INS: 0.00,	SUB: 4.31)
	BLEU-4 0.91	(BLEU-1: 11.81,	BLEU-2: 3.94,	BLEU-3: 1.70,	BLEU-4: 0.91)
	CHRF 17.82	ROUGE 9.72
2024-02-09 03:20:43,827 Logging Recognition and Translation Outputs
2024-02-09 03:20:43,827 ========================================================================================================================
2024-02-09 03:20:43,828 Logging Sequence: 163_116.00
2024-02-09 03:20:43,828 	Gloss Reference :	A B+C+D+E
2024-02-09 03:20:43,832 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 03:20:43,832 	Gloss Alignment :	         
2024-02-09 03:20:43,832 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 03:20:43,833 	Text Reference  :	people said that  she looked    similar to   virat   
2024-02-09 03:20:43,833 	Text Hypothesis :	bcci   can  image on  instagram story   with pictures
2024-02-09 03:20:43,833 	Text Alignment  :	S      S    S     S   S         S       S    S       
2024-02-09 03:20:43,833 ========================================================================================================================
2024-02-09 03:20:43,833 Logging Sequence: 53_161.00
2024-02-09 03:20:43,834 	Gloss Reference :	A B+C+D+E
2024-02-09 03:20:43,834 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 03:20:43,834 	Gloss Alignment :	         
2024-02-09 03:20:43,834 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 03:20:43,836 	Text Reference  :	rashid has       also      been      urging people to   donate to   his rashid **** khan foundation and afghanistan cricket     association
2024-02-09 03:20:43,837 	Text Hypothesis :	****** meanwhile sunrisers hyderabad has    said   that both   nabi and rashid will be   available  for the         rescheduled ipl        
2024-02-09 03:20:43,837 	Text Alignment  :	D      S         S         S         S      S      S    S      S    S          I    S    S          S   S           S           S          
2024-02-09 03:20:43,837 ========================================================================================================================
2024-02-09 03:20:43,837 Logging Sequence: 67_73.00
2024-02-09 03:20:43,837 	Gloss Reference :	A B+C+D+E
2024-02-09 03:20:43,838 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 03:20:43,838 	Gloss Alignment :	         
2024-02-09 03:20:43,838 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 03:20:43,839 	Text Reference  :	*** *** ***** in  his tweet     he    also said     
2024-02-09 03:20:43,839 	Text Hypothesis :	the t20 world cup are currently going on   australia
2024-02-09 03:20:43,839 	Text Alignment  :	I   I   I     S   S   S         S     S    S        
2024-02-09 03:20:43,839 ========================================================================================================================
2024-02-09 03:20:43,839 Logging Sequence: 137_44.00
2024-02-09 03:20:43,839 	Gloss Reference :	A B+C+D+E
2024-02-09 03:20:43,839 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 03:20:43,840 	Gloss Alignment :	         
2024-02-09 03:20:43,840 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 03:20:43,841 	Text Reference  :	let me tell you the rules that qatar has announced for the      fans travelling  for       the ******* world cup   
2024-02-09 03:20:43,841 	Text Hypothesis :	*** ** **** *** *** ***** **** they  all posed     for pictures amid celebratory fireworks the stadium in    mumbai
2024-02-09 03:20:43,842 	Text Alignment  :	D   D  D    D   D   D     D    S     S   S             S        S    S           S             I       S     S     
2024-02-09 03:20:43,842 ========================================================================================================================
2024-02-09 03:20:43,842 Logging Sequence: 99_158.00
2024-02-09 03:20:43,842 	Gloss Reference :	A B+C+D+E
2024-02-09 03:20:43,842 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 03:20:43,842 	Gloss Alignment :	         
2024-02-09 03:20:43,842 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 03:20:43,843 	Text Reference  :	****** the incident occured     in   dubai and   it     was  extremely shameful
2024-02-09 03:20:43,844 	Text Hypothesis :	people had high     expectation with each  other famous lost the       match   
2024-02-09 03:20:43,844 	Text Alignment  :	I      S   S        S           S    S     S     S      S    S         S       
2024-02-09 03:20:43,844 ========================================================================================================================
2024-02-09 03:20:54,773 Epoch 942: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-09 03:20:54,774 EPOCH 943
2024-02-09 03:21:06,011 Epoch 943: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.85 
2024-02-09 03:21:06,011 EPOCH 944
2024-02-09 03:21:17,223 Epoch 944: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-09 03:21:17,223 EPOCH 945
2024-02-09 03:21:28,345 Epoch 945: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-09 03:21:28,346 EPOCH 946
2024-02-09 03:21:39,479 Epoch 946: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-09 03:21:39,480 EPOCH 947
2024-02-09 03:21:50,788 Epoch 947: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-09 03:21:50,789 EPOCH 948
2024-02-09 03:21:50,886 [Epoch: 948 Step: 00016100] Batch Recognition Loss:   0.000343 => Gls Tokens per Sec:     6737 || Batch Translation Loss:   0.007908 => Txt Tokens per Sec:    10558 || Lr: 0.000100
2024-02-09 03:22:01,733 Epoch 948: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-09 03:22:01,734 EPOCH 949
2024-02-09 03:22:12,847 Epoch 949: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-09 03:22:12,847 EPOCH 950
2024-02-09 03:22:24,025 Epoch 950: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-09 03:22:24,026 EPOCH 951
2024-02-09 03:22:35,072 Epoch 951: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-09 03:22:35,072 EPOCH 952
2024-02-09 03:22:46,265 Epoch 952: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-09 03:22:46,266 EPOCH 953
2024-02-09 03:22:57,146 [Epoch: 953 Step: 00016200] Batch Recognition Loss:   0.001824 => Gls Tokens per Sec:      917 || Batch Translation Loss:   0.020867 => Txt Tokens per Sec:     2516 || Lr: 0.000100
2024-02-09 03:22:57,517 Epoch 953: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-09 03:22:57,517 EPOCH 954
2024-02-09 03:23:08,529 Epoch 954: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.79 
2024-02-09 03:23:08,529 EPOCH 955
2024-02-09 03:23:19,499 Epoch 955: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.70 
2024-02-09 03:23:19,500 EPOCH 956
2024-02-09 03:23:30,502 Epoch 956: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.75 
2024-02-09 03:23:30,503 EPOCH 957
2024-02-09 03:23:41,597 Epoch 957: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.13 
2024-02-09 03:23:41,598 EPOCH 958
2024-02-09 03:23:52,570 Epoch 958: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.90 
2024-02-09 03:23:52,571 EPOCH 959
2024-02-09 03:24:00,724 [Epoch: 959 Step: 00016300] Batch Recognition Loss:   0.000464 => Gls Tokens per Sec:     1099 || Batch Translation Loss:   0.192295 => Txt Tokens per Sec:     2992 || Lr: 0.000100
2024-02-09 03:24:03,791 Epoch 959: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.18 
2024-02-09 03:24:03,791 EPOCH 960
2024-02-09 03:24:14,863 Epoch 960: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.19 
2024-02-09 03:24:14,863 EPOCH 961
2024-02-09 03:24:25,941 Epoch 961: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.32 
2024-02-09 03:24:25,942 EPOCH 962
2024-02-09 03:24:36,775 Epoch 962: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.18 
2024-02-09 03:24:36,776 EPOCH 963
2024-02-09 03:24:47,699 Epoch 963: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.18 
2024-02-09 03:24:47,700 EPOCH 964
2024-02-09 03:24:58,916 Epoch 964: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.66 
2024-02-09 03:24:58,917 EPOCH 965
2024-02-09 03:25:06,579 [Epoch: 965 Step: 00016400] Batch Recognition Loss:   0.001860 => Gls Tokens per Sec:     1002 || Batch Translation Loss:   0.067413 => Txt Tokens per Sec:     2803 || Lr: 0.000100
2024-02-09 03:25:09,913 Epoch 965: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.86 
2024-02-09 03:25:09,913 EPOCH 966
2024-02-09 03:25:20,939 Epoch 966: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.67 
2024-02-09 03:25:20,940 EPOCH 967
2024-02-09 03:25:31,889 Epoch 967: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.58 
2024-02-09 03:25:31,890 EPOCH 968
2024-02-09 03:25:42,826 Epoch 968: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.82 
2024-02-09 03:25:42,826 EPOCH 969
2024-02-09 03:25:53,968 Epoch 969: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.21 
2024-02-09 03:25:53,969 EPOCH 970
2024-02-09 03:26:05,037 Epoch 970: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.37 
2024-02-09 03:26:05,038 EPOCH 971
2024-02-09 03:26:09,043 [Epoch: 971 Step: 00016500] Batch Recognition Loss:   0.000705 => Gls Tokens per Sec:     1599 || Batch Translation Loss:   0.029969 => Txt Tokens per Sec:     4154 || Lr: 0.000100
2024-02-09 03:26:16,030 Epoch 971: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.92 
2024-02-09 03:26:16,030 EPOCH 972
2024-02-09 03:26:27,146 Epoch 972: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.71 
2024-02-09 03:26:27,147 EPOCH 973
2024-02-09 03:26:38,233 Epoch 973: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.64 
2024-02-09 03:26:38,233 EPOCH 974
2024-02-09 03:26:49,257 Epoch 974: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-09 03:26:49,258 EPOCH 975
2024-02-09 03:27:00,434 Epoch 975: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-09 03:27:00,435 EPOCH 976
2024-02-09 03:27:13,292 Epoch 976: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.55 
2024-02-09 03:27:13,293 EPOCH 977
2024-02-09 03:27:17,176 [Epoch: 977 Step: 00016600] Batch Recognition Loss:   0.000239 => Gls Tokens per Sec:     1252 || Batch Translation Loss:   0.026746 => Txt Tokens per Sec:     3297 || Lr: 0.000100
2024-02-09 03:27:24,213 Epoch 977: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-09 03:27:24,213 EPOCH 978
2024-02-09 03:27:35,511 Epoch 978: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.45 
2024-02-09 03:27:35,511 EPOCH 979
2024-02-09 03:27:46,718 Epoch 979: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.47 
2024-02-09 03:27:46,718 EPOCH 980
2024-02-09 03:27:57,467 Epoch 980: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-09 03:27:57,468 EPOCH 981
2024-02-09 03:28:08,559 Epoch 981: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.37 
2024-02-09 03:28:08,559 EPOCH 982
2024-02-09 03:28:19,739 Epoch 982: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-09 03:28:19,740 EPOCH 983
2024-02-09 03:28:23,021 [Epoch: 983 Step: 00016700] Batch Recognition Loss:   0.000433 => Gls Tokens per Sec:     1171 || Batch Translation Loss:   0.043929 => Txt Tokens per Sec:     3198 || Lr: 0.000100
2024-02-09 03:28:30,866 Epoch 983: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-09 03:28:30,866 EPOCH 984
2024-02-09 03:28:42,301 Epoch 984: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-09 03:28:42,302 EPOCH 985
2024-02-09 03:28:53,541 Epoch 985: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-09 03:28:53,541 EPOCH 986
2024-02-09 03:29:04,570 Epoch 986: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.54 
2024-02-09 03:29:04,570 EPOCH 987
2024-02-09 03:29:15,654 Epoch 987: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-09 03:29:15,654 EPOCH 988
2024-02-09 03:29:26,759 Epoch 988: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-09 03:29:26,759 EPOCH 989
2024-02-09 03:29:29,561 [Epoch: 989 Step: 00016800] Batch Recognition Loss:   0.001489 => Gls Tokens per Sec:      914 || Batch Translation Loss:   0.008112 => Txt Tokens per Sec:     2464 || Lr: 0.000100
2024-02-09 03:29:38,006 Epoch 989: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-09 03:29:38,006 EPOCH 990
2024-02-09 03:29:49,186 Epoch 990: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-09 03:29:49,186 EPOCH 991
2024-02-09 03:30:00,476 Epoch 991: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-09 03:30:00,477 EPOCH 992
2024-02-09 03:30:11,610 Epoch 992: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-09 03:30:11,611 EPOCH 993
2024-02-09 03:30:22,772 Epoch 993: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-09 03:30:22,772 EPOCH 994
2024-02-09 03:30:33,801 Epoch 994: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-09 03:30:33,802 EPOCH 995
2024-02-09 03:30:34,168 [Epoch: 995 Step: 00016900] Batch Recognition Loss:   0.000312 => Gls Tokens per Sec:     3516 || Batch Translation Loss:   0.028633 => Txt Tokens per Sec:     8624 || Lr: 0.000100
2024-02-09 03:30:45,122 Epoch 995: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-09 03:30:45,122 EPOCH 996
2024-02-09 03:30:56,339 Epoch 996: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.75 
2024-02-09 03:30:56,340 EPOCH 997
2024-02-09 03:31:07,619 Epoch 997: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-09 03:31:07,619 EPOCH 998
2024-02-09 03:31:18,806 Epoch 998: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.01 
2024-02-09 03:31:18,807 EPOCH 999
2024-02-09 03:31:29,952 Epoch 999: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-09 03:31:29,953 EPOCH 1000
2024-02-09 03:31:41,109 [Epoch: 1000 Step: 00017000] Batch Recognition Loss:   0.002117 => Gls Tokens per Sec:      952 || Batch Translation Loss:   0.084382 => Txt Tokens per Sec:     2634 || Lr: 0.000100
2024-02-09 03:31:41,110 Epoch 1000: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.14 
2024-02-09 03:31:41,110 EPOCH 1001
2024-02-09 03:31:52,107 Epoch 1001: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.17 
2024-02-09 03:31:52,107 EPOCH 1002
2024-02-09 03:32:03,386 Epoch 1002: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.23 
2024-02-09 03:32:03,387 EPOCH 1003
2024-02-09 03:32:14,622 Epoch 1003: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.88 
2024-02-09 03:32:14,623 EPOCH 1004
2024-02-09 03:32:25,698 Epoch 1004: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.06 
2024-02-09 03:32:25,699 EPOCH 1005
2024-02-09 03:32:36,524 Epoch 1005: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.76 
2024-02-09 03:32:36,524 EPOCH 1006
2024-02-09 03:32:45,275 [Epoch: 1006 Step: 00017100] Batch Recognition Loss:   0.000803 => Gls Tokens per Sec:     1067 || Batch Translation Loss:   0.076669 => Txt Tokens per Sec:     2890 || Lr: 0.000100
2024-02-09 03:32:47,658 Epoch 1006: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.02 
2024-02-09 03:32:47,658 EPOCH 1007
2024-02-09 03:32:58,725 Epoch 1007: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.79 
2024-02-09 03:32:58,725 EPOCH 1008
2024-02-09 03:33:10,033 Epoch 1008: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.50 
2024-02-09 03:33:10,034 EPOCH 1009
2024-02-09 03:33:21,174 Epoch 1009: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.69 
2024-02-09 03:33:21,174 EPOCH 1010
2024-02-09 03:33:32,656 Epoch 1010: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.30 
2024-02-09 03:33:32,657 EPOCH 1011
2024-02-09 03:33:43,756 Epoch 1011: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.92 
2024-02-09 03:33:43,756 EPOCH 1012
2024-02-09 03:33:50,211 [Epoch: 1012 Step: 00017200] Batch Recognition Loss:   0.000975 => Gls Tokens per Sec:     1249 || Batch Translation Loss:   0.135903 => Txt Tokens per Sec:     3301 || Lr: 0.000100
2024-02-09 03:33:54,621 Epoch 1012: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.47 
2024-02-09 03:33:54,622 EPOCH 1013
2024-02-09 03:34:05,915 Epoch 1013: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.45 
2024-02-09 03:34:05,916 EPOCH 1014
2024-02-09 03:34:16,932 Epoch 1014: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.01 
2024-02-09 03:34:16,932 EPOCH 1015
2024-02-09 03:34:28,056 Epoch 1015: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.69 
2024-02-09 03:34:28,056 EPOCH 1016
2024-02-09 03:34:39,060 Epoch 1016: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.54 
2024-02-09 03:34:39,060 EPOCH 1017
2024-02-09 03:34:50,232 Epoch 1017: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.64 
2024-02-09 03:34:50,233 EPOCH 1018
2024-02-09 03:34:56,897 [Epoch: 1018 Step: 00017300] Batch Recognition Loss:   0.000786 => Gls Tokens per Sec:     1018 || Batch Translation Loss:   0.040702 => Txt Tokens per Sec:     2767 || Lr: 0.000100
2024-02-09 03:35:01,296 Epoch 1018: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.46 
2024-02-09 03:35:01,296 EPOCH 1019
2024-02-09 03:35:12,243 Epoch 1019: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.52 
2024-02-09 03:35:12,244 EPOCH 1020
2024-02-09 03:35:22,946 Epoch 1020: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.48 
2024-02-09 03:35:22,946 EPOCH 1021
2024-02-09 03:35:33,987 Epoch 1021: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.61 
2024-02-09 03:35:33,987 EPOCH 1022
2024-02-09 03:35:45,108 Epoch 1022: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-09 03:35:45,108 EPOCH 1023
2024-02-09 03:35:56,369 Epoch 1023: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-09 03:35:56,369 EPOCH 1024
2024-02-09 03:36:03,855 [Epoch: 1024 Step: 00017400] Batch Recognition Loss:   0.000268 => Gls Tokens per Sec:      735 || Batch Translation Loss:   0.019038 => Txt Tokens per Sec:     2096 || Lr: 0.000100
2024-02-09 03:36:07,357 Epoch 1024: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-09 03:36:07,357 EPOCH 1025
2024-02-09 03:36:18,304 Epoch 1025: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-09 03:36:18,305 EPOCH 1026
2024-02-09 03:36:29,516 Epoch 1026: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.53 
2024-02-09 03:36:29,517 EPOCH 1027
2024-02-09 03:36:40,632 Epoch 1027: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-09 03:36:40,632 EPOCH 1028
2024-02-09 03:36:51,791 Epoch 1028: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-09 03:36:51,792 EPOCH 1029
2024-02-09 03:37:02,763 Epoch 1029: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-09 03:37:02,764 EPOCH 1030
2024-02-09 03:37:06,048 [Epoch: 1030 Step: 00017500] Batch Recognition Loss:   0.000310 => Gls Tokens per Sec:     1365 || Batch Translation Loss:   0.032126 => Txt Tokens per Sec:     4005 || Lr: 0.000100
2024-02-09 03:37:13,708 Epoch 1030: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-09 03:37:13,709 EPOCH 1031
2024-02-09 03:37:24,687 Epoch 1031: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-09 03:37:24,687 EPOCH 1032
2024-02-09 03:37:35,488 Epoch 1032: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-09 03:37:35,489 EPOCH 1033
2024-02-09 03:37:46,005 Epoch 1033: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-09 03:37:46,005 EPOCH 1034
2024-02-09 03:37:56,331 Epoch 1034: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-09 03:37:56,332 EPOCH 1035
2024-02-09 03:38:07,598 Epoch 1035: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-09 03:38:07,599 EPOCH 1036
2024-02-09 03:38:10,564 [Epoch: 1036 Step: 00017600] Batch Recognition Loss:   0.000303 => Gls Tokens per Sec:     1080 || Batch Translation Loss:   0.017595 => Txt Tokens per Sec:     2966 || Lr: 0.000100
2024-02-09 03:38:18,879 Epoch 1036: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-09 03:38:18,880 EPOCH 1037
2024-02-09 03:38:29,952 Epoch 1037: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-09 03:38:29,952 EPOCH 1038
2024-02-09 03:38:41,118 Epoch 1038: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-09 03:38:41,119 EPOCH 1039
2024-02-09 03:38:52,405 Epoch 1039: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-09 03:38:52,405 EPOCH 1040
2024-02-09 03:39:03,585 Epoch 1040: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-09 03:39:03,586 EPOCH 1041
2024-02-09 03:39:14,884 Epoch 1041: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-09 03:39:14,885 EPOCH 1042
2024-02-09 03:39:15,404 [Epoch: 1042 Step: 00017700] Batch Recognition Loss:   0.000188 => Gls Tokens per Sec:     3714 || Batch Translation Loss:   0.028009 => Txt Tokens per Sec:     8774 || Lr: 0.000100
2024-02-09 03:39:25,954 Epoch 1042: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.89 
2024-02-09 03:39:25,955 EPOCH 1043
2024-02-09 03:39:37,147 Epoch 1043: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.50 
2024-02-09 03:39:37,148 EPOCH 1044
2024-02-09 03:39:48,436 Epoch 1044: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.59 
2024-02-09 03:39:48,436 EPOCH 1045
2024-02-09 03:39:59,451 Epoch 1045: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.96 
2024-02-09 03:39:59,452 EPOCH 1046
2024-02-09 03:40:10,373 Epoch 1046: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.15 
2024-02-09 03:40:10,374 EPOCH 1047
2024-02-09 03:40:21,427 Epoch 1047: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.78 
2024-02-09 03:40:21,427 EPOCH 1048
2024-02-09 03:40:21,668 [Epoch: 1048 Step: 00017800] Batch Recognition Loss:   0.000711 => Gls Tokens per Sec:     2667 || Batch Translation Loss:   0.133151 => Txt Tokens per Sec:     7071 || Lr: 0.000100
2024-02-09 03:40:32,685 Epoch 1048: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.13 
2024-02-09 03:40:32,685 EPOCH 1049
2024-02-09 03:40:44,075 Epoch 1049: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.10 
2024-02-09 03:40:44,075 EPOCH 1050
2024-02-09 03:40:55,240 Epoch 1050: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.67 
2024-02-09 03:40:55,241 EPOCH 1051
2024-02-09 03:41:06,151 Epoch 1051: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.93 
2024-02-09 03:41:06,151 EPOCH 1052
2024-02-09 03:41:17,014 Epoch 1052: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.45 
2024-02-09 03:41:17,014 EPOCH 1053
2024-02-09 03:41:27,533 [Epoch: 1053 Step: 00017900] Batch Recognition Loss:   0.001013 => Gls Tokens per Sec:      949 || Batch Translation Loss:   0.025551 => Txt Tokens per Sec:     2620 || Lr: 0.000100
2024-02-09 03:41:27,834 Epoch 1053: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.93 
2024-02-09 03:41:27,834 EPOCH 1054
2024-02-09 03:41:38,741 Epoch 1054: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.67 
2024-02-09 03:41:38,742 EPOCH 1055
2024-02-09 03:41:49,787 Epoch 1055: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.54 
2024-02-09 03:41:49,788 EPOCH 1056
2024-02-09 03:42:00,813 Epoch 1056: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-09 03:42:00,814 EPOCH 1057
2024-02-09 03:42:11,823 Epoch 1057: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.41 
2024-02-09 03:42:11,824 EPOCH 1058
2024-02-09 03:42:22,914 Epoch 1058: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.47 
2024-02-09 03:42:22,915 EPOCH 1059
2024-02-09 03:42:33,423 [Epoch: 1059 Step: 00018000] Batch Recognition Loss:   0.000966 => Gls Tokens per Sec:      828 || Batch Translation Loss:   0.023686 => Txt Tokens per Sec:     2380 || Lr: 0.000100
2024-02-09 03:43:14,551 Validation result at epoch 1059, step    18000: duration: 41.1261s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.87539	Translation Loss: 92851.71875	PPL: 10657.75488
	Eval Metric: BLEU
	WER 4.73	(DEL: 0.00,	INS: 0.00,	SUB: 4.73)
	BLEU-4 0.60	(BLEU-1: 11.44,	BLEU-2: 3.50,	BLEU-3: 1.33,	BLEU-4: 0.60)
	CHRF 17.39	ROUGE 9.49
2024-02-09 03:43:14,553 Logging Recognition and Translation Outputs
2024-02-09 03:43:14,553 ========================================================================================================================
2024-02-09 03:43:14,553 Logging Sequence: 179_309.00
2024-02-09 03:43:14,554 	Gloss Reference :	A B+C+D+E
2024-02-09 03:43:14,554 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 03:43:14,555 	Gloss Alignment :	         
2024-02-09 03:43:14,556 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 03:43:14,557 	Text Reference  :	before the ioa could send the notice wfi has  asked phogat      to explain her     indiscipline
2024-02-09 03:43:14,557 	Text Hypothesis :	****** *** *** here  is   one day    she does not   participate in the     penalty shootout    
2024-02-09 03:43:14,558 	Text Alignment  :	D      D   D   S     S    S   S      S   S    S     S           S  S       S       S           
2024-02-09 03:43:14,558 ========================================================================================================================
2024-02-09 03:43:14,558 Logging Sequence: 156_35.00
2024-02-09 03:43:14,558 	Gloss Reference :	A B+C+D+E
2024-02-09 03:43:14,558 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 03:43:14,559 	Gloss Alignment :	         
2024-02-09 03:43:14,559 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 03:43:14,561 	Text Reference  :	the first season of mlc   began    on      13th july   2023    and      ended  on  30th     july    2023 with  six teams
2024-02-09 03:43:14,561 	Text Hypothesis :	*** ***** ****** ** miny' original captain was  kieron pollard nicholas pooran was stand-in captain in   place of  him  
2024-02-09 03:43:14,561 	Text Alignment  :	D   D     D      D  S     S        S       S    S      S       S        S      S   S        S       S    S     S   S    
2024-02-09 03:43:14,561 ========================================================================================================================
2024-02-09 03:43:14,561 Logging Sequence: 129_45.00
2024-02-09 03:43:14,562 	Gloss Reference :	A B+C+D+E
2024-02-09 03:43:14,562 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 03:43:14,562 	Gloss Alignment :	         
2024-02-09 03:43:14,562 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 03:43:14,563 	Text Reference  :	suga then announced that from    5   july onwards japan will  be       in  a      state of   emergency
2024-02-09 03:43:14,564 	Text Hypothesis :	**** **** ********* **** however the end  of      the   covid pandemic the rising cases were postponed
2024-02-09 03:43:14,564 	Text Alignment  :	D    D    D         D    S       S   S    S       S     S     S        S   S      S     S    S        
2024-02-09 03:43:14,564 ========================================================================================================================
2024-02-09 03:43:14,564 Logging Sequence: 56_17.00
2024-02-09 03:43:14,564 	Gloss Reference :	A B+C+D+E  
2024-02-09 03:43:14,565 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-09 03:43:14,565 	Gloss Alignment :	  S        
2024-02-09 03:43:14,565 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 03:43:14,566 	Text Reference  :	****** **** it    was held  at      mumbai's wankhede stadium
2024-02-09 03:43:14,566 	Text Hypothesis :	people were glued to  their screens for      the      match  
2024-02-09 03:43:14,566 	Text Alignment  :	I      I    S     S   S     S       S        S        S      
2024-02-09 03:43:14,566 ========================================================================================================================
2024-02-09 03:43:14,566 Logging Sequence: 152_73.00
2024-02-09 03:43:14,566 	Gloss Reference :	A B+C+D+E
2024-02-09 03:43:14,567 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 03:43:14,567 	Gloss Alignment :	         
2024-02-09 03:43:14,567 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 03:43:14,568 	Text Reference  :	**** **** **** ***** *** eventually he      too got      out    by   shaheen  afridi
2024-02-09 03:43:14,568 	Text Hypothesis :	they will face fines and hearing    friends of  pakistan trophy with pakistan times 
2024-02-09 03:43:14,568 	Text Alignment  :	I    I    I    I     I   S          S       S   S        S      S    S        S     
2024-02-09 03:43:14,568 ========================================================================================================================
2024-02-09 03:43:15,261 Epoch 1059: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.52 
2024-02-09 03:43:15,261 EPOCH 1060
2024-02-09 03:43:27,039 Epoch 1060: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-09 03:43:27,040 EPOCH 1061
2024-02-09 03:43:38,153 Epoch 1061: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-09 03:43:38,153 EPOCH 1062
2024-02-09 03:43:49,062 Epoch 1062: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.35 
2024-02-09 03:43:49,063 EPOCH 1063
2024-02-09 03:44:00,076 Epoch 1063: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-09 03:44:00,076 EPOCH 1064
2024-02-09 03:44:11,417 Epoch 1064: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-09 03:44:11,418 EPOCH 1065
2024-02-09 03:44:19,790 [Epoch: 1065 Step: 00018100] Batch Recognition Loss:   0.000378 => Gls Tokens per Sec:      886 || Batch Translation Loss:   0.016839 => Txt Tokens per Sec:     2590 || Lr: 0.000100
2024-02-09 03:44:22,480 Epoch 1065: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 03:44:22,480 EPOCH 1066
2024-02-09 03:44:33,351 Epoch 1066: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 03:44:33,351 EPOCH 1067
2024-02-09 03:44:44,448 Epoch 1067: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-09 03:44:44,449 EPOCH 1068
2024-02-09 03:44:55,609 Epoch 1068: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-09 03:44:55,610 EPOCH 1069
2024-02-09 03:45:06,874 Epoch 1069: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-09 03:45:06,874 EPOCH 1070
2024-02-09 03:45:17,942 Epoch 1070: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-09 03:45:17,942 EPOCH 1071
2024-02-09 03:45:24,321 [Epoch: 1071 Step: 00018200] Batch Recognition Loss:   0.000262 => Gls Tokens per Sec:      963 || Batch Translation Loss:   0.022726 => Txt Tokens per Sec:     2638 || Lr: 0.000100
2024-02-09 03:45:28,830 Epoch 1071: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-09 03:45:28,830 EPOCH 1072
2024-02-09 03:45:39,887 Epoch 1072: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-09 03:45:39,888 EPOCH 1073
2024-02-09 03:45:50,562 Epoch 1073: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.70 
2024-02-09 03:45:50,562 EPOCH 1074
2024-02-09 03:46:01,657 Epoch 1074: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.84 
2024-02-09 03:46:01,657 EPOCH 1075
2024-02-09 03:46:12,687 Epoch 1075: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.81 
2024-02-09 03:46:12,687 EPOCH 1076
2024-02-09 03:46:23,992 Epoch 1076: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.71 
2024-02-09 03:46:23,993 EPOCH 1077
2024-02-09 03:46:30,161 [Epoch: 1077 Step: 00018300] Batch Recognition Loss:   0.000289 => Gls Tokens per Sec:      788 || Batch Translation Loss:   0.026765 => Txt Tokens per Sec:     2402 || Lr: 0.000100
2024-02-09 03:46:34,937 Epoch 1077: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.70 
2024-02-09 03:46:34,938 EPOCH 1078
2024-02-09 03:46:46,144 Epoch 1078: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-09 03:46:46,145 EPOCH 1079
2024-02-09 03:46:57,175 Epoch 1079: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-09 03:46:57,176 EPOCH 1080
2024-02-09 03:47:08,219 Epoch 1080: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-09 03:47:08,220 EPOCH 1081
2024-02-09 03:47:19,542 Epoch 1081: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.85 
2024-02-09 03:47:19,543 EPOCH 1082
2024-02-09 03:47:30,620 Epoch 1082: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.15 
2024-02-09 03:47:30,621 EPOCH 1083
2024-02-09 03:47:32,001 [Epoch: 1083 Step: 00018400] Batch Recognition Loss:   0.000269 => Gls Tokens per Sec:     2789 || Batch Translation Loss:   0.065519 => Txt Tokens per Sec:     7275 || Lr: 0.000100
2024-02-09 03:47:42,146 Epoch 1083: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.00 
2024-02-09 03:47:42,147 EPOCH 1084
2024-02-09 03:47:53,288 Epoch 1084: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-09 03:47:53,288 EPOCH 1085
2024-02-09 03:48:04,438 Epoch 1085: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.36 
2024-02-09 03:48:04,439 EPOCH 1086
2024-02-09 03:48:15,557 Epoch 1086: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.29 
2024-02-09 03:48:15,557 EPOCH 1087
2024-02-09 03:48:26,868 Epoch 1087: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.24 
2024-02-09 03:48:26,868 EPOCH 1088
2024-02-09 03:48:38,088 Epoch 1088: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.94 
2024-02-09 03:48:38,088 EPOCH 1089
2024-02-09 03:48:43,322 [Epoch: 1089 Step: 00018500] Batch Recognition Loss:   0.000620 => Gls Tokens per Sec:      440 || Batch Translation Loss:   0.054039 => Txt Tokens per Sec:     1360 || Lr: 0.000100
2024-02-09 03:48:49,333 Epoch 1089: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.86 
2024-02-09 03:48:49,333 EPOCH 1090
2024-02-09 03:49:00,483 Epoch 1090: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.82 
2024-02-09 03:49:00,483 EPOCH 1091
2024-02-09 03:49:11,668 Epoch 1091: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.72 
2024-02-09 03:49:11,668 EPOCH 1092
2024-02-09 03:49:22,533 Epoch 1092: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-09 03:49:22,534 EPOCH 1093
2024-02-09 03:49:33,366 Epoch 1093: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.11 
2024-02-09 03:49:33,366 EPOCH 1094
2024-02-09 03:49:44,496 Epoch 1094: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.88 
2024-02-09 03:49:44,496 EPOCH 1095
2024-02-09 03:49:44,996 [Epoch: 1095 Step: 00018600] Batch Recognition Loss:   0.000295 => Gls Tokens per Sec:     2565 || Batch Translation Loss:   0.016823 => Txt Tokens per Sec:     6725 || Lr: 0.000100
2024-02-09 03:49:55,458 Epoch 1095: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.75 
2024-02-09 03:49:55,458 EPOCH 1096
2024-02-09 03:50:06,436 Epoch 1096: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.79 
2024-02-09 03:50:06,436 EPOCH 1097
2024-02-09 03:50:17,361 Epoch 1097: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.02 
2024-02-09 03:50:17,361 EPOCH 1098
2024-02-09 03:50:28,576 Epoch 1098: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-09 03:50:28,577 EPOCH 1099
2024-02-09 03:50:39,874 Epoch 1099: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.48 
2024-02-09 03:50:39,875 EPOCH 1100
2024-02-09 03:50:50,386 [Epoch: 1100 Step: 00018700] Batch Recognition Loss:   0.000260 => Gls Tokens per Sec:     1011 || Batch Translation Loss:   0.029576 => Txt Tokens per Sec:     2796 || Lr: 0.000100
2024-02-09 03:50:50,386 Epoch 1100: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-09 03:50:50,386 EPOCH 1101
2024-02-09 03:51:01,559 Epoch 1101: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.36 
2024-02-09 03:51:01,559 EPOCH 1102
2024-02-09 03:51:12,596 Epoch 1102: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.52 
2024-02-09 03:51:12,597 EPOCH 1103
2024-02-09 03:51:23,671 Epoch 1103: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-09 03:51:23,672 EPOCH 1104
2024-02-09 03:51:34,423 Epoch 1104: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.38 
2024-02-09 03:51:34,424 EPOCH 1105
2024-02-09 03:51:45,293 Epoch 1105: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.49 
2024-02-09 03:51:45,294 EPOCH 1106
2024-02-09 03:51:56,047 [Epoch: 1106 Step: 00018800] Batch Recognition Loss:   0.000761 => Gls Tokens per Sec:      869 || Batch Translation Loss:   0.114718 => Txt Tokens per Sec:     2440 || Lr: 0.000100
2024-02-09 03:51:56,461 Epoch 1106: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-09 03:51:56,461 EPOCH 1107
2024-02-09 03:52:07,637 Epoch 1107: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.74 
2024-02-09 03:52:07,637 EPOCH 1108
2024-02-09 03:52:18,806 Epoch 1108: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.65 
2024-02-09 03:52:18,806 EPOCH 1109
2024-02-09 03:52:29,819 Epoch 1109: Total Training Recognition Loss 0.06  Total Training Translation Loss 0.76 
2024-02-09 03:52:29,819 EPOCH 1110
2024-02-09 03:52:41,069 Epoch 1110: Total Training Recognition Loss 0.14  Total Training Translation Loss 0.93 
2024-02-09 03:52:41,070 EPOCH 1111
2024-02-09 03:52:52,113 Epoch 1111: Total Training Recognition Loss 0.70  Total Training Translation Loss 0.88 
2024-02-09 03:52:52,114 EPOCH 1112
2024-02-09 03:53:02,430 [Epoch: 1112 Step: 00018900] Batch Recognition Loss:   0.003071 => Gls Tokens per Sec:      781 || Batch Translation Loss:   0.044169 => Txt Tokens per Sec:     2195 || Lr: 0.000100
2024-02-09 03:53:03,354 Epoch 1112: Total Training Recognition Loss 2.29  Total Training Translation Loss 0.90 
2024-02-09 03:53:03,354 EPOCH 1113
2024-02-09 03:53:14,449 Epoch 1113: Total Training Recognition Loss 5.88  Total Training Translation Loss 1.63 
2024-02-09 03:53:14,450 EPOCH 1114
2024-02-09 03:53:25,463 Epoch 1114: Total Training Recognition Loss 2.51  Total Training Translation Loss 1.82 
2024-02-09 03:53:25,463 EPOCH 1115
2024-02-09 03:53:36,644 Epoch 1115: Total Training Recognition Loss 6.39  Total Training Translation Loss 15.45 
2024-02-09 03:53:36,645 EPOCH 1116
2024-02-09 03:53:47,759 Epoch 1116: Total Training Recognition Loss 1.66  Total Training Translation Loss 7.43 
2024-02-09 03:53:47,760 EPOCH 1117
2024-02-09 03:53:58,836 Epoch 1117: Total Training Recognition Loss 2.67  Total Training Translation Loss 3.56 
2024-02-09 03:53:58,836 EPOCH 1118
2024-02-09 03:54:06,920 [Epoch: 1118 Step: 00019000] Batch Recognition Loss:   0.018816 => Gls Tokens per Sec:      839 || Batch Translation Loss:   0.124882 => Txt Tokens per Sec:     2408 || Lr: 0.000100
2024-02-09 03:54:09,902 Epoch 1118: Total Training Recognition Loss 3.84  Total Training Translation Loss 2.10 
2024-02-09 03:54:09,902 EPOCH 1119
2024-02-09 03:54:21,056 Epoch 1119: Total Training Recognition Loss 0.65  Total Training Translation Loss 1.54 
2024-02-09 03:54:21,056 EPOCH 1120
2024-02-09 03:54:32,068 Epoch 1120: Total Training Recognition Loss 0.45  Total Training Translation Loss 0.92 
2024-02-09 03:54:32,068 EPOCH 1121
2024-02-09 03:54:43,257 Epoch 1121: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.80 
2024-02-09 03:54:43,258 EPOCH 1122
2024-02-09 03:54:54,239 Epoch 1122: Total Training Recognition Loss 0.09  Total Training Translation Loss 0.64 
2024-02-09 03:54:54,240 EPOCH 1123
2024-02-09 03:55:05,420 Epoch 1123: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.50 
2024-02-09 03:55:05,420 EPOCH 1124
2024-02-09 03:55:13,385 [Epoch: 1124 Step: 00019100] Batch Recognition Loss:   0.000209 => Gls Tokens per Sec:      691 || Batch Translation Loss:   0.031996 => Txt Tokens per Sec:     1924 || Lr: 0.000100
2024-02-09 03:55:16,542 Epoch 1124: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.46 
2024-02-09 03:55:16,542 EPOCH 1125
2024-02-09 03:55:27,408 Epoch 1125: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.41 
2024-02-09 03:55:27,409 EPOCH 1126
2024-02-09 03:55:38,501 Epoch 1126: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.43 
2024-02-09 03:55:38,502 EPOCH 1127
2024-02-09 03:55:49,630 Epoch 1127: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.37 
2024-02-09 03:55:49,630 EPOCH 1128
2024-02-09 03:56:00,489 Epoch 1128: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.37 
2024-02-09 03:56:00,490 EPOCH 1129
2024-02-09 03:56:11,600 Epoch 1129: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.36 
2024-02-09 03:56:11,601 EPOCH 1130
2024-02-09 03:56:14,727 [Epoch: 1130 Step: 00019200] Batch Recognition Loss:   0.000182 => Gls Tokens per Sec:     1434 || Batch Translation Loss:   0.015015 => Txt Tokens per Sec:     3644 || Lr: 0.000100
2024-02-09 03:56:22,945 Epoch 1130: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.32 
2024-02-09 03:56:22,946 EPOCH 1131
2024-02-09 03:56:34,189 Epoch 1131: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.28 
2024-02-09 03:56:34,189 EPOCH 1132
2024-02-09 03:56:45,485 Epoch 1132: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.26 
2024-02-09 03:56:45,485 EPOCH 1133
2024-02-09 03:56:56,485 Epoch 1133: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 03:56:56,485 EPOCH 1134
2024-02-09 03:57:07,421 Epoch 1134: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.29 
2024-02-09 03:57:07,422 EPOCH 1135
2024-02-09 03:57:18,299 Epoch 1135: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.31 
2024-02-09 03:57:18,299 EPOCH 1136
2024-02-09 03:57:23,033 [Epoch: 1136 Step: 00019300] Batch Recognition Loss:   0.000467 => Gls Tokens per Sec:      676 || Batch Translation Loss:   0.020282 => Txt Tokens per Sec:     2016 || Lr: 0.000100
2024-02-09 03:57:29,462 Epoch 1136: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.33 
2024-02-09 03:57:29,463 EPOCH 1137
2024-02-09 03:57:40,499 Epoch 1137: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.27 
2024-02-09 03:57:40,500 EPOCH 1138
2024-02-09 03:57:51,636 Epoch 1138: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-09 03:57:51,637 EPOCH 1139
2024-02-09 03:58:02,720 Epoch 1139: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.57 
2024-02-09 03:58:02,721 EPOCH 1140
2024-02-09 03:58:13,616 Epoch 1140: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.27 
2024-02-09 03:58:13,617 EPOCH 1141
2024-02-09 03:58:24,853 Epoch 1141: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 03:58:24,853 EPOCH 1142
2024-02-09 03:58:25,510 [Epoch: 1142 Step: 00019400] Batch Recognition Loss:   0.000223 => Gls Tokens per Sec:     2931 || Batch Translation Loss:   0.005780 => Txt Tokens per Sec:     7475 || Lr: 0.000100
2024-02-09 03:58:35,767 Epoch 1142: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.27 
2024-02-09 03:58:35,767 EPOCH 1143
2024-02-09 03:58:46,457 Epoch 1143: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.28 
2024-02-09 03:58:46,458 EPOCH 1144
2024-02-09 03:58:57,828 Epoch 1144: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 03:58:57,829 EPOCH 1145
2024-02-09 03:59:08,837 Epoch 1145: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-09 03:59:08,838 EPOCH 1146
2024-02-09 03:59:19,759 Epoch 1146: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-09 03:59:19,760 EPOCH 1147
2024-02-09 03:59:30,846 Epoch 1147: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.35 
2024-02-09 03:59:30,846 EPOCH 1148
2024-02-09 03:59:32,459 [Epoch: 1148 Step: 00019500] Batch Recognition Loss:   0.000736 => Gls Tokens per Sec:      397 || Batch Translation Loss:   0.021132 => Txt Tokens per Sec:     1344 || Lr: 0.000100
2024-02-09 03:59:41,852 Epoch 1148: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.31 
2024-02-09 03:59:41,852 EPOCH 1149
2024-02-09 03:59:53,011 Epoch 1149: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 03:59:53,013 EPOCH 1150
2024-02-09 04:00:04,296 Epoch 1150: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.27 
2024-02-09 04:00:04,296 EPOCH 1151
2024-02-09 04:00:15,542 Epoch 1151: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 04:00:15,543 EPOCH 1152
2024-02-09 04:00:26,686 Epoch 1152: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-09 04:00:26,687 EPOCH 1153
2024-02-09 04:00:37,421 [Epoch: 1153 Step: 00019600] Batch Recognition Loss:   0.000865 => Gls Tokens per Sec:      930 || Batch Translation Loss:   0.020766 => Txt Tokens per Sec:     2548 || Lr: 0.000100
2024-02-09 04:00:37,750 Epoch 1153: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-09 04:00:37,750 EPOCH 1154
2024-02-09 04:00:48,767 Epoch 1154: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-09 04:00:48,768 EPOCH 1155
2024-02-09 04:00:59,837 Epoch 1155: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-09 04:00:59,838 EPOCH 1156
2024-02-09 04:01:10,496 Epoch 1156: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 04:01:10,496 EPOCH 1157
2024-02-09 04:01:21,689 Epoch 1157: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 04:01:21,690 EPOCH 1158
2024-02-09 04:01:32,775 Epoch 1158: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 04:01:32,775 EPOCH 1159
2024-02-09 04:01:42,967 [Epoch: 1159 Step: 00019700] Batch Recognition Loss:   0.000248 => Gls Tokens per Sec:      854 || Batch Translation Loss:   0.006884 => Txt Tokens per Sec:     2368 || Lr: 0.000100
2024-02-09 04:01:43,783 Epoch 1159: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 04:01:43,784 EPOCH 1160
2024-02-09 04:01:54,646 Epoch 1160: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 04:01:54,647 EPOCH 1161
2024-02-09 04:02:05,822 Epoch 1161: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 04:02:05,822 EPOCH 1162
2024-02-09 04:02:16,759 Epoch 1162: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 04:02:16,759 EPOCH 1163
2024-02-09 04:02:27,757 Epoch 1163: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.23 
2024-02-09 04:02:27,758 EPOCH 1164
2024-02-09 04:02:38,774 Epoch 1164: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 04:02:38,775 EPOCH 1165
2024-02-09 04:02:48,564 [Epoch: 1165 Step: 00019800] Batch Recognition Loss:   0.000597 => Gls Tokens per Sec:      758 || Batch Translation Loss:   0.015319 => Txt Tokens per Sec:     2119 || Lr: 0.000100
2024-02-09 04:02:49,739 Epoch 1165: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-09 04:02:49,739 EPOCH 1166
2024-02-09 04:03:00,823 Epoch 1166: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.15 
2024-02-09 04:03:00,824 EPOCH 1167
2024-02-09 04:03:11,810 Epoch 1167: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.40 
2024-02-09 04:03:11,810 EPOCH 1168
2024-02-09 04:03:22,850 Epoch 1168: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.29 
2024-02-09 04:03:22,851 EPOCH 1169
2024-02-09 04:03:33,887 Epoch 1169: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.12 
2024-02-09 04:03:33,887 EPOCH 1170
2024-02-09 04:03:44,850 Epoch 1170: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.06 
2024-02-09 04:03:44,850 EPOCH 1171
2024-02-09 04:03:52,990 [Epoch: 1171 Step: 00019900] Batch Recognition Loss:   0.002655 => Gls Tokens per Sec:      754 || Batch Translation Loss:   0.075659 => Txt Tokens per Sec:     2171 || Lr: 0.000100
2024-02-09 04:03:55,857 Epoch 1171: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.88 
2024-02-09 04:03:55,857 EPOCH 1172
2024-02-09 04:04:06,771 Epoch 1172: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.89 
2024-02-09 04:04:06,771 EPOCH 1173
2024-02-09 04:04:17,720 Epoch 1173: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.60 
2024-02-09 04:04:17,721 EPOCH 1174
2024-02-09 04:04:28,623 Epoch 1174: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.32 
2024-02-09 04:04:28,623 EPOCH 1175
2024-02-09 04:04:39,637 Epoch 1175: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.19 
2024-02-09 04:04:39,637 EPOCH 1176
2024-02-09 04:04:50,638 Epoch 1176: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.75 
2024-02-09 04:04:50,638 EPOCH 1177
2024-02-09 04:04:57,736 [Epoch: 1177 Step: 00020000] Batch Recognition Loss:   0.000842 => Gls Tokens per Sec:      685 || Batch Translation Loss:   0.066685 => Txt Tokens per Sec:     1862 || Lr: 0.000100
2024-02-09 04:05:38,675 Validation result at epoch 1177, step    20000: duration: 40.9363s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.58054	Translation Loss: 92957.46094	PPL: 10770.90723
	Eval Metric: BLEU
	WER 2.75	(DEL: 0.00,	INS: 0.00,	SUB: 2.75)
	BLEU-4 0.62	(BLEU-1: 11.84,	BLEU-2: 3.90,	BLEU-3: 1.52,	BLEU-4: 0.62)
	CHRF 17.57	ROUGE 9.99
2024-02-09 04:05:38,677 Logging Recognition and Translation Outputs
2024-02-09 04:05:38,677 ========================================================================================================================
2024-02-09 04:05:38,677 Logging Sequence: 120_7.00
2024-02-09 04:05:38,677 	Gloss Reference :	A B+C+D+E
2024-02-09 04:05:38,678 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 04:05:38,678 	Gloss Alignment :	         
2024-02-09 04:05:38,678 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 04:05:38,679 	Text Reference  :	he had tested positive for covid-19 on    may 19   
2024-02-09 04:05:38,679 	Text Hypothesis :	he *** ****** was      in  the      state of  india
2024-02-09 04:05:38,679 	Text Alignment  :	   D   D      S        S   S        S     S   S    
2024-02-09 04:05:38,679 ========================================================================================================================
2024-02-09 04:05:38,679 Logging Sequence: 148_186.00
2024-02-09 04:05:38,679 	Gloss Reference :	A B+C+D+E
2024-02-09 04:05:38,679 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 04:05:38,679 	Gloss Alignment :	         
2024-02-09 04:05:38,680 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 04:05:38,682 	Text Reference  :	*** ********* ***** siraj      also took  four wickets in 1 over thus becoming the  record-holder for most wickets in an over in odis
2024-02-09 04:05:38,682 	Text Hypothesis :	the badminton world federation bwf  which is   held    in * uae  with him      from india         to  15   wickets ** ** **** ** ****
2024-02-09 04:05:38,682 	Text Alignment  :	I   I         I     S          S    S     S    S          D S    S    S        S    S             S   S            D  D  D    D  D   
2024-02-09 04:05:38,682 ========================================================================================================================
2024-02-09 04:05:38,682 Logging Sequence: 67_73.00
2024-02-09 04:05:38,683 	Gloss Reference :	A B+C+D+E
2024-02-09 04:05:38,683 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 04:05:38,683 	Gloss Alignment :	         
2024-02-09 04:05:38,683 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 04:05:38,684 	Text Reference  :	*** ****** **** in       his tweet he   also said
2024-02-09 04:05:38,684 	Text Hypothesis :	the finals were supposed to  be    held in   uae 
2024-02-09 04:05:38,684 	Text Alignment  :	I   I      I    S        S   S     S    S    S   
2024-02-09 04:05:38,684 ========================================================================================================================
2024-02-09 04:05:38,684 Logging Sequence: 164_526.00
2024-02-09 04:05:38,685 	Gloss Reference :	A B+C+D+E
2024-02-09 04:05:38,685 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 04:05:38,685 	Gloss Alignment :	         
2024-02-09 04:05:38,685 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 04:05:38,686 	Text Reference  :	you are aware that viacom18 bought the *** **** *** **** broadcast rights of  ipl   
2024-02-09 04:05:38,686 	Text Hypothesis :	in  the first ball bounces  on     the off side and then goes      on     the stumps
2024-02-09 04:05:38,687 	Text Alignment  :	S   S   S     S    S        S          I   I    I   I    S         S      S   S     
2024-02-09 04:05:38,687 ========================================================================================================================
2024-02-09 04:05:38,687 Logging Sequence: 108_28.00
2024-02-09 04:05:38,687 	Gloss Reference :	A B+C+D+E
2024-02-09 04:05:38,687 	Gloss Hypothesis:	A B+C+D  
2024-02-09 04:05:38,687 	Gloss Alignment :	  S      
2024-02-09 04:05:38,688 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 04:05:38,689 	Text Reference  :	the 10 teams bought 204 players including 67      foreign players after spending a  total of     rs 55170 crore   
2024-02-09 04:05:38,689 	Text Hypothesis :	the ** ***** ****** *** ******* ipl       matches will    be      held  only     in pune  mumbai at 25    capacity
2024-02-09 04:05:38,689 	Text Alignment  :	    D  D     D      D   D       S         S       S       S       S     S        S  S     S      S  S     S       
2024-02-09 04:05:38,689 ========================================================================================================================
2024-02-09 04:05:42,686 Epoch 1177: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.64 
2024-02-09 04:05:42,686 EPOCH 1178
2024-02-09 04:05:54,259 Epoch 1178: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-09 04:05:54,259 EPOCH 1179
2024-02-09 04:06:05,450 Epoch 1179: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-09 04:06:05,450 EPOCH 1180
2024-02-09 04:06:16,709 Epoch 1180: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-09 04:06:16,710 EPOCH 1181
2024-02-09 04:06:27,829 Epoch 1181: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-09 04:06:27,829 EPOCH 1182
2024-02-09 04:06:38,993 Epoch 1182: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-09 04:06:38,993 EPOCH 1183
2024-02-09 04:06:43,312 [Epoch: 1183 Step: 00020100] Batch Recognition Loss:   0.000283 => Gls Tokens per Sec:      890 || Batch Translation Loss:   0.015023 => Txt Tokens per Sec:     2296 || Lr: 0.000100
2024-02-09 04:06:50,292 Epoch 1183: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-09 04:06:50,292 EPOCH 1184
2024-02-09 04:07:01,267 Epoch 1184: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-09 04:07:01,267 EPOCH 1185
2024-02-09 04:07:12,429 Epoch 1185: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-09 04:07:12,430 EPOCH 1186
2024-02-09 04:07:23,365 Epoch 1186: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-09 04:07:23,365 EPOCH 1187
2024-02-09 04:07:34,475 Epoch 1187: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.30 
2024-02-09 04:07:34,476 EPOCH 1188
2024-02-09 04:07:45,630 Epoch 1188: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-09 04:07:45,631 EPOCH 1189
2024-02-09 04:07:48,245 [Epoch: 1189 Step: 00020200] Batch Recognition Loss:   0.000156 => Gls Tokens per Sec:      980 || Batch Translation Loss:   0.029765 => Txt Tokens per Sec:     2994 || Lr: 0.000100
2024-02-09 04:07:56,974 Epoch 1189: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-09 04:07:56,975 EPOCH 1190
2024-02-09 04:08:07,950 Epoch 1190: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-09 04:08:07,950 EPOCH 1191
2024-02-09 04:08:19,149 Epoch 1191: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-09 04:08:19,149 EPOCH 1192
2024-02-09 04:08:30,138 Epoch 1192: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-09 04:08:30,138 EPOCH 1193
2024-02-09 04:08:41,054 Epoch 1193: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-09 04:08:41,054 EPOCH 1194
2024-02-09 04:08:51,835 Epoch 1194: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.30 
2024-02-09 04:08:51,836 EPOCH 1195
2024-02-09 04:08:52,272 [Epoch: 1195 Step: 00020300] Batch Recognition Loss:   0.000230 => Gls Tokens per Sec:     2943 || Batch Translation Loss:   0.011104 => Txt Tokens per Sec:     7956 || Lr: 0.000100
2024-02-09 04:09:02,549 Epoch 1195: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.33 
2024-02-09 04:09:02,550 EPOCH 1196
2024-02-09 04:09:13,624 Epoch 1196: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-09 04:09:13,625 EPOCH 1197
2024-02-09 04:09:24,223 Epoch 1197: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-09 04:09:24,224 EPOCH 1198
2024-02-09 04:09:34,831 Epoch 1198: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-09 04:09:34,831 EPOCH 1199
2024-02-09 04:09:45,855 Epoch 1199: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-09 04:09:45,855 EPOCH 1200
2024-02-09 04:09:57,279 [Epoch: 1200 Step: 00020400] Batch Recognition Loss:   0.000243 => Gls Tokens per Sec:      930 || Batch Translation Loss:   0.024913 => Txt Tokens per Sec:     2572 || Lr: 0.000100
2024-02-09 04:09:57,280 Epoch 1200: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.41 
2024-02-09 04:09:57,281 EPOCH 1201
2024-02-09 04:10:08,348 Epoch 1201: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-09 04:10:08,349 EPOCH 1202
2024-02-09 04:10:19,253 Epoch 1202: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-09 04:10:19,253 EPOCH 1203
2024-02-09 04:10:30,472 Epoch 1203: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.96 
2024-02-09 04:10:30,473 EPOCH 1204
2024-02-09 04:10:41,443 Epoch 1204: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.37 
2024-02-09 04:10:41,444 EPOCH 1205
2024-02-09 04:10:52,542 Epoch 1205: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.84 
2024-02-09 04:10:52,543 EPOCH 1206
2024-02-09 04:11:02,970 [Epoch: 1206 Step: 00020500] Batch Recognition Loss:   0.000318 => Gls Tokens per Sec:      896 || Batch Translation Loss:   0.083060 => Txt Tokens per Sec:     2519 || Lr: 0.000100
2024-02-09 04:11:03,363 Epoch 1206: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.07 
2024-02-09 04:11:03,364 EPOCH 1207
2024-02-09 04:11:14,469 Epoch 1207: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.09 
2024-02-09 04:11:14,470 EPOCH 1208
2024-02-09 04:11:25,339 Epoch 1208: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.50 
2024-02-09 04:11:25,339 EPOCH 1209
2024-02-09 04:11:36,329 Epoch 1209: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.46 
2024-02-09 04:11:36,329 EPOCH 1210
2024-02-09 04:11:47,402 Epoch 1210: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.32 
2024-02-09 04:11:47,403 EPOCH 1211
2024-02-09 04:11:58,631 Epoch 1211: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.14 
2024-02-09 04:11:58,632 EPOCH 1212
2024-02-09 04:12:05,534 [Epoch: 1212 Step: 00020600] Batch Recognition Loss:   0.000487 => Gls Tokens per Sec:     1168 || Batch Translation Loss:   0.033169 => Txt Tokens per Sec:     3055 || Lr: 0.000100
2024-02-09 04:12:09,750 Epoch 1212: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.13 
2024-02-09 04:12:09,750 EPOCH 1213
2024-02-09 04:12:21,037 Epoch 1213: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.92 
2024-02-09 04:12:21,038 EPOCH 1214
2024-02-09 04:12:32,380 Epoch 1214: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.85 
2024-02-09 04:12:32,381 EPOCH 1215
2024-02-09 04:12:43,566 Epoch 1215: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-09 04:12:43,567 EPOCH 1216
2024-02-09 04:12:54,773 Epoch 1216: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.47 
2024-02-09 04:12:54,774 EPOCH 1217
2024-02-09 04:13:05,915 Epoch 1217: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-09 04:13:05,915 EPOCH 1218
2024-02-09 04:13:13,879 [Epoch: 1218 Step: 00020700] Batch Recognition Loss:   0.000491 => Gls Tokens per Sec:      851 || Batch Translation Loss:   0.008873 => Txt Tokens per Sec:     2347 || Lr: 0.000100
2024-02-09 04:13:16,996 Epoch 1218: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-09 04:13:16,997 EPOCH 1219
2024-02-09 04:13:27,936 Epoch 1219: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-09 04:13:27,937 EPOCH 1220
2024-02-09 04:13:39,132 Epoch 1220: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 04:13:39,132 EPOCH 1221
2024-02-09 04:13:50,572 Epoch 1221: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 04:13:50,573 EPOCH 1222
2024-02-09 04:14:01,851 Epoch 1222: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-09 04:14:01,851 EPOCH 1223
2024-02-09 04:14:12,989 Epoch 1223: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-09 04:14:12,989 EPOCH 1224
2024-02-09 04:14:21,052 [Epoch: 1224 Step: 00020800] Batch Recognition Loss:   0.000227 => Gls Tokens per Sec:      682 || Batch Translation Loss:   0.101140 => Txt Tokens per Sec:     1957 || Lr: 0.000100
2024-02-09 04:14:24,323 Epoch 1224: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-09 04:14:24,324 EPOCH 1225
2024-02-09 04:14:35,680 Epoch 1225: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-09 04:14:35,680 EPOCH 1226
2024-02-09 04:14:46,770 Epoch 1226: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-09 04:14:46,771 EPOCH 1227
2024-02-09 04:14:57,797 Epoch 1227: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-09 04:14:57,797 EPOCH 1228
2024-02-09 04:15:08,956 Epoch 1228: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-09 04:15:08,956 EPOCH 1229
2024-02-09 04:15:19,989 Epoch 1229: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-09 04:15:19,989 EPOCH 1230
2024-02-09 04:15:25,318 [Epoch: 1230 Step: 00020900] Batch Recognition Loss:   0.000188 => Gls Tokens per Sec:      792 || Batch Translation Loss:   0.092201 => Txt Tokens per Sec:     2159 || Lr: 0.000100
2024-02-09 04:15:31,203 Epoch 1230: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-09 04:15:31,204 EPOCH 1231
2024-02-09 04:15:42,216 Epoch 1231: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-09 04:15:42,217 EPOCH 1232
2024-02-09 04:15:53,342 Epoch 1232: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-09 04:15:53,342 EPOCH 1233
2024-02-09 04:16:04,376 Epoch 1233: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-09 04:16:04,377 EPOCH 1234
2024-02-09 04:16:15,492 Epoch 1234: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-09 04:16:15,493 EPOCH 1235
2024-02-09 04:16:26,715 Epoch 1235: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-09 04:16:26,715 EPOCH 1236
2024-02-09 04:16:32,349 [Epoch: 1236 Step: 00021000] Batch Recognition Loss:   0.000411 => Gls Tokens per Sec:      568 || Batch Translation Loss:   0.094782 => Txt Tokens per Sec:     1725 || Lr: 0.000100
2024-02-09 04:16:39,135 Epoch 1236: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-09 04:16:39,135 EPOCH 1237
2024-02-09 04:16:49,917 Epoch 1237: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-09 04:16:49,918 EPOCH 1238
2024-02-09 04:17:00,935 Epoch 1238: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-09 04:17:00,936 EPOCH 1239
2024-02-09 04:17:12,270 Epoch 1239: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.87 
2024-02-09 04:17:12,271 EPOCH 1240
2024-02-09 04:17:23,272 Epoch 1240: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.73 
2024-02-09 04:17:23,272 EPOCH 1241
2024-02-09 04:17:34,582 Epoch 1241: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-09 04:17:34,582 EPOCH 1242
2024-02-09 04:17:35,137 [Epoch: 1242 Step: 00021100] Batch Recognition Loss:   0.000231 => Gls Tokens per Sec:     3466 || Batch Translation Loss:   0.016904 => Txt Tokens per Sec:     8523 || Lr: 0.000100
2024-02-09 04:17:45,778 Epoch 1242: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.70 
2024-02-09 04:17:45,778 EPOCH 1243
2024-02-09 04:17:56,998 Epoch 1243: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.82 
2024-02-09 04:17:56,999 EPOCH 1244
2024-02-09 04:18:08,184 Epoch 1244: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.73 
2024-02-09 04:18:08,185 EPOCH 1245
2024-02-09 04:18:19,501 Epoch 1245: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-09 04:18:19,502 EPOCH 1246
2024-02-09 04:18:30,167 Epoch 1246: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-09 04:18:30,168 EPOCH 1247
2024-02-09 04:18:41,569 Epoch 1247: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.84 
2024-02-09 04:18:41,570 EPOCH 1248
2024-02-09 04:18:41,788 [Epoch: 1248 Step: 00021200] Batch Recognition Loss:   0.000330 => Gls Tokens per Sec:     2949 || Batch Translation Loss:   0.026613 => Txt Tokens per Sec:     8258 || Lr: 0.000100
2024-02-09 04:18:52,604 Epoch 1248: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.70 
2024-02-09 04:18:52,605 EPOCH 1249
2024-02-09 04:19:03,915 Epoch 1249: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-09 04:19:03,915 EPOCH 1250
2024-02-09 04:19:15,130 Epoch 1250: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.98 
2024-02-09 04:19:15,131 EPOCH 1251
2024-02-09 04:19:26,185 Epoch 1251: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.18 
2024-02-09 04:19:26,185 EPOCH 1252
2024-02-09 04:19:37,351 Epoch 1252: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.57 
2024-02-09 04:19:37,351 EPOCH 1253
2024-02-09 04:19:48,326 [Epoch: 1253 Step: 00021300] Batch Recognition Loss:   0.000652 => Gls Tokens per Sec:      909 || Batch Translation Loss:   0.030489 => Txt Tokens per Sec:     2545 || Lr: 0.000100
2024-02-09 04:19:48,500 Epoch 1253: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.18 
2024-02-09 04:19:48,500 EPOCH 1254
2024-02-09 04:19:59,653 Epoch 1254: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.15 
2024-02-09 04:19:59,654 EPOCH 1255
2024-02-09 04:20:10,660 Epoch 1255: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.10 
2024-02-09 04:20:10,660 EPOCH 1256
2024-02-09 04:20:21,907 Epoch 1256: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.25 
2024-02-09 04:20:21,907 EPOCH 1257
2024-02-09 04:20:32,994 Epoch 1257: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.13 
2024-02-09 04:20:32,995 EPOCH 1258
2024-02-09 04:20:44,330 Epoch 1258: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.18 
2024-02-09 04:20:44,330 EPOCH 1259
2024-02-09 04:20:54,889 [Epoch: 1259 Step: 00021400] Batch Recognition Loss:   0.000286 => Gls Tokens per Sec:      824 || Batch Translation Loss:   0.055692 => Txt Tokens per Sec:     2295 || Lr: 0.000100
2024-02-09 04:20:55,618 Epoch 1259: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.89 
2024-02-09 04:20:55,618 EPOCH 1260
2024-02-09 04:21:06,452 Epoch 1260: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-09 04:21:06,452 EPOCH 1261
2024-02-09 04:21:17,842 Epoch 1261: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-09 04:21:17,843 EPOCH 1262
2024-02-09 04:21:29,016 Epoch 1262: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-09 04:21:29,017 EPOCH 1263
2024-02-09 04:21:40,050 Epoch 1263: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-09 04:21:40,051 EPOCH 1264
2024-02-09 04:21:51,247 Epoch 1264: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.84 
2024-02-09 04:21:51,247 EPOCH 1265
2024-02-09 04:22:01,407 [Epoch: 1265 Step: 00021500] Batch Recognition Loss:   0.000255 => Gls Tokens per Sec:      730 || Batch Translation Loss:   0.041800 => Txt Tokens per Sec:     2165 || Lr: 0.000100
2024-02-09 04:22:02,308 Epoch 1265: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-09 04:22:02,309 EPOCH 1266
2024-02-09 04:22:13,498 Epoch 1266: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.76 
2024-02-09 04:22:13,499 EPOCH 1267
2024-02-09 04:22:24,556 Epoch 1267: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.12 
2024-02-09 04:22:24,557 EPOCH 1268
2024-02-09 04:22:35,588 Epoch 1268: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.47 
2024-02-09 04:22:35,588 EPOCH 1269
2024-02-09 04:22:46,963 Epoch 1269: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.41 
2024-02-09 04:22:46,964 EPOCH 1270
2024-02-09 04:22:58,082 Epoch 1270: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.45 
2024-02-09 04:22:58,083 EPOCH 1271
2024-02-09 04:23:05,713 [Epoch: 1271 Step: 00021600] Batch Recognition Loss:   0.000414 => Gls Tokens per Sec:      805 || Batch Translation Loss:   0.134011 => Txt Tokens per Sec:     2213 || Lr: 0.000100
2024-02-09 04:23:09,006 Epoch 1271: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.42 
2024-02-09 04:23:09,006 EPOCH 1272
2024-02-09 04:23:19,893 Epoch 1272: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.08 
2024-02-09 04:23:19,893 EPOCH 1273
2024-02-09 04:23:31,058 Epoch 1273: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.77 
2024-02-09 04:23:31,059 EPOCH 1274
2024-02-09 04:23:42,022 Epoch 1274: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-09 04:23:42,022 EPOCH 1275
2024-02-09 04:23:53,129 Epoch 1275: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-09 04:23:53,130 EPOCH 1276
2024-02-09 04:24:04,097 Epoch 1276: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-09 04:24:04,097 EPOCH 1277
2024-02-09 04:24:09,992 [Epoch: 1277 Step: 00021700] Batch Recognition Loss:   0.000362 => Gls Tokens per Sec:      825 || Batch Translation Loss:   0.037116 => Txt Tokens per Sec:     2214 || Lr: 0.000100
2024-02-09 04:24:15,167 Epoch 1277: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-09 04:24:15,168 EPOCH 1278
2024-02-09 04:24:26,166 Epoch 1278: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-09 04:24:26,167 EPOCH 1279
2024-02-09 04:24:37,385 Epoch 1279: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-09 04:24:37,385 EPOCH 1280
2024-02-09 04:24:48,634 Epoch 1280: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-09 04:24:48,635 EPOCH 1281
2024-02-09 04:24:59,805 Epoch 1281: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-09 04:24:59,806 EPOCH 1282
2024-02-09 04:25:11,034 Epoch 1282: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.02 
2024-02-09 04:25:11,035 EPOCH 1283
2024-02-09 04:25:17,918 [Epoch: 1283 Step: 00021800] Batch Recognition Loss:   0.000308 => Gls Tokens per Sec:      520 || Batch Translation Loss:   0.057927 => Txt Tokens per Sec:     1565 || Lr: 0.000100
2024-02-09 04:25:22,140 Epoch 1283: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.62 
2024-02-09 04:25:22,140 EPOCH 1284
2024-02-09 04:25:33,267 Epoch 1284: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.32 
2024-02-09 04:25:33,268 EPOCH 1285
2024-02-09 04:25:44,438 Epoch 1285: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.53 
2024-02-09 04:25:44,438 EPOCH 1286
2024-02-09 04:25:55,634 Epoch 1286: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.52 
2024-02-09 04:25:55,635 EPOCH 1287
2024-02-09 04:26:06,800 Epoch 1287: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.13 
2024-02-09 04:26:06,801 EPOCH 1288
2024-02-09 04:26:18,144 Epoch 1288: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.23 
2024-02-09 04:26:18,145 EPOCH 1289
2024-02-09 04:26:23,345 [Epoch: 1289 Step: 00021900] Batch Recognition Loss:   0.001574 => Gls Tokens per Sec:      442 || Batch Translation Loss:   0.053868 => Txt Tokens per Sec:     1403 || Lr: 0.000100
2024-02-09 04:26:29,380 Epoch 1289: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.78 
2024-02-09 04:26:29,381 EPOCH 1290
2024-02-09 04:26:40,358 Epoch 1290: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-09 04:26:40,358 EPOCH 1291
2024-02-09 04:26:51,418 Epoch 1291: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.52 
2024-02-09 04:26:51,418 EPOCH 1292
2024-02-09 04:27:02,502 Epoch 1292: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-09 04:27:02,503 EPOCH 1293
2024-02-09 04:27:13,603 Epoch 1293: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-09 04:27:13,603 EPOCH 1294
2024-02-09 04:27:25,982 Epoch 1294: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-09 04:27:25,983 EPOCH 1295
2024-02-09 04:27:28,089 [Epoch: 1295 Step: 00022000] Batch Recognition Loss:   0.000662 => Gls Tokens per Sec:      608 || Batch Translation Loss:   0.018425 => Txt Tokens per Sec:     1924 || Lr: 0.000100
2024-02-09 04:28:09,271 Validation result at epoch 1295, step    22000: duration: 41.1812s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.71131	Translation Loss: 92772.14844	PPL: 10573.38477
	Eval Metric: BLEU
	WER 3.60	(DEL: 0.00,	INS: 0.00,	SUB: 3.60)
	BLEU-4 0.54	(BLEU-1: 11.38,	BLEU-2: 3.45,	BLEU-3: 1.25,	BLEU-4: 0.54)
	CHRF 17.17	ROUGE 9.47
2024-02-09 04:28:09,273 Logging Recognition and Translation Outputs
2024-02-09 04:28:09,273 ========================================================================================================================
2024-02-09 04:28:09,273 Logging Sequence: 179_2.00
2024-02-09 04:28:09,273 	Gloss Reference :	A B+C+D+E
2024-02-09 04:28:09,274 	Gloss Hypothesis:	A B+C+D  
2024-02-09 04:28:09,274 	Gloss Alignment :	  S      
2024-02-09 04:28:09,274 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 04:28:09,275 	Text Reference  :	*** **** ******** ** *** ** *** vinesh phogat is        a    well   known wrestler
2024-02-09 04:28:09,275 	Text Hypothesis :	the vast majority of the of the family member countries were former and   it      
2024-02-09 04:28:09,275 	Text Alignment  :	I   I    I        I  I   I  I   S      S      S         S    S      S     S       
2024-02-09 04:28:09,275 ========================================================================================================================
2024-02-09 04:28:09,275 Logging Sequence: 55_124.00
2024-02-09 04:28:09,276 	Gloss Reference :	A B+C+D+E
2024-02-09 04:28:09,276 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 04:28:09,276 	Gloss Alignment :	         
2024-02-09 04:28:09,276 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 04:28:09,277 	Text Reference  :	**** ** ***** next   to       him with      the patel jersey was ***** ajaz patel
2024-02-09 04:28:09,277 	Text Hypothesis :	many of these places swimwear is  extremely fit when  it     was named as   well 
2024-02-09 04:28:09,277 	Text Alignment  :	I    I  I     S      S        S   S         S   S     S          I     S    S    
2024-02-09 04:28:09,277 ========================================================================================================================
2024-02-09 04:28:09,278 Logging Sequence: 148_105.00
2024-02-09 04:28:09,278 	Gloss Reference :	A B+C+D+E
2024-02-09 04:28:09,278 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 04:28:09,278 	Gloss Alignment :	         
2024-02-09 04:28:09,278 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 04:28:09,281 	Text Reference  :	later with amazing bowling by hardik pandya  and ******* kuldeep yadav sri    lanka were all out       in ** ******* ******* just     50 runs     
2024-02-09 04:28:09,281 	Text Hypothesis :	***** **** ******* ******* ** star   batsmen and bowlers of      the   indian team  were *** dismissed in an innings against pakistan in bengaluru
2024-02-09 04:28:09,281 	Text Alignment  :	D     D    D       D       D  S      S           I       S       S     S      S          D   S            I  I       I       S        S  S        
2024-02-09 04:28:09,281 ========================================================================================================================
2024-02-09 04:28:09,281 Logging Sequence: 125_165.00
2024-02-09 04:28:09,281 	Gloss Reference :	A B+C+D+E
2024-02-09 04:28:09,282 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 04:28:09,282 	Gloss Alignment :	         
2024-02-09 04:28:09,282 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 04:28:09,283 	Text Reference  :	** please do  not target nadeem we speak to each other       and share a good    bond   
2024-02-09 04:28:09,283 	Text Hypothesis :	so here   are not ****** ****** ** ***** ** **** comfortable and ***** i finally mirabai
2024-02-09 04:28:09,283 	Text Alignment  :	I  S      S       D      D      D  D     D  D    S               D     S S       S      
2024-02-09 04:28:09,283 ========================================================================================================================
2024-02-09 04:28:09,283 Logging Sequence: 77_52.00
2024-02-09 04:28:09,284 	Gloss Reference :	A B+C+D+E
2024-02-09 04:28:09,284 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 04:28:09,284 	Gloss Alignment :	         
2024-02-09 04:28:09,284 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 04:28:09,286 	Text Reference  :	kane williamson held down the fort for   hyderabad by scoring 66    runs and  ended the ********** match in      a tie  
2024-02-09 04:28:09,286 	Text Hypothesis :	**** ********** **** **** *** **** after that      an olympic games was  held in    the tournament while playing 6 balls
2024-02-09 04:28:09,286 	Text Alignment  :	D    D          D    D    D   D    S     S         S  S       S     S    S    S         I          S     S       S S    
2024-02-09 04:28:09,286 ========================================================================================================================
2024-02-09 04:28:18,893 Epoch 1295: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-09 04:28:18,893 EPOCH 1296
2024-02-09 04:28:30,301 Epoch 1296: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 04:28:30,302 EPOCH 1297
2024-02-09 04:28:41,113 Epoch 1297: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 04:28:41,114 EPOCH 1298
2024-02-09 04:28:51,942 Epoch 1298: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-09 04:28:51,943 EPOCH 1299
2024-02-09 04:29:02,970 Epoch 1299: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-09 04:29:02,970 EPOCH 1300
2024-02-09 04:29:13,991 [Epoch: 1300 Step: 00022100] Batch Recognition Loss:   0.000452 => Gls Tokens per Sec:      964 || Batch Translation Loss:   0.030558 => Txt Tokens per Sec:     2666 || Lr: 0.000100
2024-02-09 04:29:13,992 Epoch 1300: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-09 04:29:13,992 EPOCH 1301
2024-02-09 04:29:25,155 Epoch 1301: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-09 04:29:25,156 EPOCH 1302
2024-02-09 04:29:36,305 Epoch 1302: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 04:29:36,306 EPOCH 1303
2024-02-09 04:29:47,417 Epoch 1303: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-09 04:29:47,418 EPOCH 1304
2024-02-09 04:29:58,336 Epoch 1304: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-09 04:29:58,337 EPOCH 1305
2024-02-09 04:30:09,446 Epoch 1305: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.46 
2024-02-09 04:30:09,447 EPOCH 1306
2024-02-09 04:30:19,533 [Epoch: 1306 Step: 00022200] Batch Recognition Loss:   0.001190 => Gls Tokens per Sec:      926 || Batch Translation Loss:   0.021360 => Txt Tokens per Sec:     2577 || Lr: 0.000100
2024-02-09 04:30:20,055 Epoch 1306: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-09 04:30:20,055 EPOCH 1307
2024-02-09 04:30:31,164 Epoch 1307: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-09 04:30:31,164 EPOCH 1308
2024-02-09 04:30:42,334 Epoch 1308: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-09 04:30:42,335 EPOCH 1309
2024-02-09 04:30:53,449 Epoch 1309: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-09 04:30:53,450 EPOCH 1310
2024-02-09 04:31:04,534 Epoch 1310: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-09 04:31:04,535 EPOCH 1311
2024-02-09 04:31:15,238 Epoch 1311: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-09 04:31:15,239 EPOCH 1312
2024-02-09 04:31:23,822 [Epoch: 1312 Step: 00022300] Batch Recognition Loss:   0.000161 => Gls Tokens per Sec:      939 || Batch Translation Loss:   0.016238 => Txt Tokens per Sec:     2575 || Lr: 0.000100
2024-02-09 04:31:26,382 Epoch 1312: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-09 04:31:26,382 EPOCH 1313
2024-02-09 04:31:37,328 Epoch 1313: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-09 04:31:37,329 EPOCH 1314
2024-02-09 04:31:48,208 Epoch 1314: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-09 04:31:48,209 EPOCH 1315
2024-02-09 04:31:59,487 Epoch 1315: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-09 04:31:59,488 EPOCH 1316
2024-02-09 04:32:10,663 Epoch 1316: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-09 04:32:10,664 EPOCH 1317
2024-02-09 04:32:21,743 Epoch 1317: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-09 04:32:21,743 EPOCH 1318
2024-02-09 04:32:29,267 [Epoch: 1318 Step: 00022400] Batch Recognition Loss:   0.000229 => Gls Tokens per Sec:      936 || Batch Translation Loss:   0.022624 => Txt Tokens per Sec:     2771 || Lr: 0.000100
2024-02-09 04:32:32,798 Epoch 1318: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-09 04:32:32,799 EPOCH 1319
2024-02-09 04:32:43,766 Epoch 1319: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-09 04:32:43,767 EPOCH 1320
2024-02-09 04:32:54,857 Epoch 1320: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-09 04:32:54,857 EPOCH 1321
2024-02-09 04:33:05,779 Epoch 1321: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-09 04:33:05,780 EPOCH 1322
2024-02-09 04:33:17,308 Epoch 1322: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-09 04:33:17,309 EPOCH 1323
2024-02-09 04:33:27,987 Epoch 1323: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.40 
2024-02-09 04:33:27,987 EPOCH 1324
2024-02-09 04:33:35,321 [Epoch: 1324 Step: 00022500] Batch Recognition Loss:   0.000275 => Gls Tokens per Sec:      750 || Batch Translation Loss:   0.024270 => Txt Tokens per Sec:     2148 || Lr: 0.000100
2024-02-09 04:33:38,767 Epoch 1324: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.57 
2024-02-09 04:33:38,768 EPOCH 1325
2024-02-09 04:33:49,801 Epoch 1325: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-09 04:33:49,802 EPOCH 1326
2024-02-09 04:34:00,800 Epoch 1326: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-09 04:34:00,801 EPOCH 1327
2024-02-09 04:34:11,773 Epoch 1327: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.89 
2024-02-09 04:34:11,774 EPOCH 1328
2024-02-09 04:34:22,890 Epoch 1328: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-09 04:34:22,891 EPOCH 1329
2024-02-09 04:34:33,930 Epoch 1329: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.90 
2024-02-09 04:34:33,930 EPOCH 1330
2024-02-09 04:34:39,670 [Epoch: 1330 Step: 00022600] Batch Recognition Loss:   0.004712 => Gls Tokens per Sec:      735 || Batch Translation Loss:   0.081550 => Txt Tokens per Sec:     2241 || Lr: 0.000100
2024-02-09 04:34:45,024 Epoch 1330: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.93 
2024-02-09 04:34:45,025 EPOCH 1331
2024-02-09 04:34:55,840 Epoch 1331: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.29 
2024-02-09 04:34:55,841 EPOCH 1332
2024-02-09 04:35:06,903 Epoch 1332: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.76 
2024-02-09 04:35:06,904 EPOCH 1333
2024-02-09 04:35:17,764 Epoch 1333: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.32 
2024-02-09 04:35:17,765 EPOCH 1334
2024-02-09 04:35:29,109 Epoch 1334: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.89 
2024-02-09 04:35:29,109 EPOCH 1335
2024-02-09 04:35:40,952 Epoch 1335: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.10 
2024-02-09 04:35:40,952 EPOCH 1336
2024-02-09 04:35:45,595 [Epoch: 1336 Step: 00022700] Batch Recognition Loss:   0.001809 => Gls Tokens per Sec:      690 || Batch Translation Loss:   0.377175 => Txt Tokens per Sec:     1951 || Lr: 0.000100
2024-02-09 04:35:52,476 Epoch 1336: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.28 
2024-02-09 04:35:52,477 EPOCH 1337
2024-02-09 04:36:08,227 Epoch 1337: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.53 
2024-02-09 04:36:08,227 EPOCH 1338
2024-02-09 04:36:21,429 Epoch 1338: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.79 
2024-02-09 04:36:21,430 EPOCH 1339
2024-02-09 04:36:32,699 Epoch 1339: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.90 
2024-02-09 04:36:32,700 EPOCH 1340
2024-02-09 04:36:44,613 Epoch 1340: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.29 
2024-02-09 04:36:44,614 EPOCH 1341
2024-02-09 04:36:56,061 Epoch 1341: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.79 
2024-02-09 04:36:56,062 EPOCH 1342
2024-02-09 04:36:58,520 [Epoch: 1342 Step: 00022800] Batch Recognition Loss:   0.001119 => Gls Tokens per Sec:      782 || Batch Translation Loss:   0.025380 => Txt Tokens per Sec:     2455 || Lr: 0.000100
2024-02-09 04:37:07,635 Epoch 1342: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.64 
2024-02-09 04:37:07,636 EPOCH 1343
2024-02-09 04:37:19,271 Epoch 1343: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.83 
2024-02-09 04:37:19,272 EPOCH 1344
2024-02-09 04:37:30,779 Epoch 1344: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.75 
2024-02-09 04:37:30,780 EPOCH 1345
2024-02-09 04:37:43,164 Epoch 1345: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.37 
2024-02-09 04:37:43,164 EPOCH 1346
2024-02-09 04:37:57,692 Epoch 1346: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-09 04:37:57,693 EPOCH 1347
2024-02-09 04:38:10,151 Epoch 1347: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-09 04:38:10,152 EPOCH 1348
2024-02-09 04:38:10,377 [Epoch: 1348 Step: 00022900] Batch Recognition Loss:   0.000382 => Gls Tokens per Sec:     2877 || Batch Translation Loss:   0.014819 => Txt Tokens per Sec:     7206 || Lr: 0.000100
2024-02-09 04:38:21,936 Epoch 1348: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-09 04:38:21,936 EPOCH 1349
2024-02-09 04:38:34,441 Epoch 1349: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 04:38:34,442 EPOCH 1350
2024-02-09 04:38:47,054 Epoch 1350: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 04:38:47,054 EPOCH 1351
2024-02-09 04:38:59,739 Epoch 1351: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 04:38:59,739 EPOCH 1352
2024-02-09 04:39:11,224 Epoch 1352: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-09 04:39:11,225 EPOCH 1353
2024-02-09 04:39:20,803 [Epoch: 1353 Step: 00023000] Batch Recognition Loss:   0.000600 => Gls Tokens per Sec:     1042 || Batch Translation Loss:   0.036238 => Txt Tokens per Sec:     2851 || Lr: 0.000100
2024-02-09 04:39:22,748 Epoch 1353: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-09 04:39:22,748 EPOCH 1354
2024-02-09 04:39:34,230 Epoch 1354: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-09 04:39:34,230 EPOCH 1355
2024-02-09 04:39:46,254 Epoch 1355: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-09 04:39:46,255 EPOCH 1356
2024-02-09 04:39:57,652 Epoch 1356: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-09 04:39:57,653 EPOCH 1357
2024-02-09 04:40:09,486 Epoch 1357: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-09 04:40:09,487 EPOCH 1358
2024-02-09 04:40:21,251 Epoch 1358: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-09 04:40:21,252 EPOCH 1359
2024-02-09 04:40:31,960 [Epoch: 1359 Step: 00023100] Batch Recognition Loss:   0.000317 => Gls Tokens per Sec:      813 || Batch Translation Loss:   0.019971 => Txt Tokens per Sec:     2291 || Lr: 0.000100
2024-02-09 04:40:32,816 Epoch 1359: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.61 
2024-02-09 04:40:32,816 EPOCH 1360
2024-02-09 04:40:45,733 Epoch 1360: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.80 
2024-02-09 04:40:45,734 EPOCH 1361
2024-02-09 04:40:57,182 Epoch 1361: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-09 04:40:57,182 EPOCH 1362
2024-02-09 04:41:08,453 Epoch 1362: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-09 04:41:08,453 EPOCH 1363
2024-02-09 04:41:19,864 Epoch 1363: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-09 04:41:19,865 EPOCH 1364
2024-02-09 04:41:33,913 Epoch 1364: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-09 04:41:33,914 EPOCH 1365
2024-02-09 04:41:44,027 [Epoch: 1365 Step: 00023200] Batch Recognition Loss:   0.001080 => Gls Tokens per Sec:      734 || Batch Translation Loss:   0.019924 => Txt Tokens per Sec:     2010 || Lr: 0.000100
2024-02-09 04:41:47,220 Epoch 1365: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-09 04:41:47,220 EPOCH 1366
2024-02-09 04:41:58,946 Epoch 1366: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-09 04:41:58,946 EPOCH 1367
2024-02-09 04:42:10,528 Epoch 1367: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-09 04:42:10,528 EPOCH 1368
2024-02-09 04:42:22,276 Epoch 1368: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-09 04:42:22,276 EPOCH 1369
2024-02-09 04:42:33,960 Epoch 1369: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-09 04:42:33,961 EPOCH 1370
2024-02-09 04:42:46,000 Epoch 1370: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 04:42:46,001 EPOCH 1371
2024-02-09 04:42:54,133 [Epoch: 1371 Step: 00023300] Batch Recognition Loss:   0.000168 => Gls Tokens per Sec:      755 || Batch Translation Loss:   0.012911 => Txt Tokens per Sec:     2153 || Lr: 0.000100
2024-02-09 04:42:57,844 Epoch 1371: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.30 
2024-02-09 04:42:57,844 EPOCH 1372
2024-02-09 04:43:09,720 Epoch 1372: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-09 04:43:09,721 EPOCH 1373
2024-02-09 04:43:22,282 Epoch 1373: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-09 04:43:22,283 EPOCH 1374
2024-02-09 04:43:34,433 Epoch 1374: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 04:43:34,433 EPOCH 1375
2024-02-09 04:43:48,068 Epoch 1375: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 04:43:48,069 EPOCH 1376
2024-02-09 04:44:00,537 Epoch 1376: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-09 04:44:00,537 EPOCH 1377
2024-02-09 04:44:03,924 [Epoch: 1377 Step: 00023400] Batch Recognition Loss:   0.000151 => Gls Tokens per Sec:     1512 || Batch Translation Loss:   0.018468 => Txt Tokens per Sec:     3909 || Lr: 0.000100
2024-02-09 04:44:12,300 Epoch 1377: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-09 04:44:12,301 EPOCH 1378
2024-02-09 04:44:24,138 Epoch 1378: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-09 04:44:24,139 EPOCH 1379
2024-02-09 05:20:36,184 Epoch 1379: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-09 05:20:36,186 EPOCH 1380
2024-02-09 06:19:18,421 Epoch 1380: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-09 06:19:18,424 EPOCH 1381
2024-02-09 07:23:42,253 Epoch 1381: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-09 07:23:42,255 EPOCH 1382
2024-02-09 08:26:21,477 Epoch 1382: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-09 08:26:21,479 EPOCH 1383
2024-02-09 08:26:25,850 [Epoch: 1383 Step: 00023500] Batch Recognition Loss:   0.000170 => Gls Tokens per Sec:      819 || Batch Translation Loss:   0.016925 => Txt Tokens per Sec:     2310 || Lr: 0.000100
2024-02-09 08:26:34,526 Epoch 1383: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-09 08:26:34,527 EPOCH 1384
2024-02-09 08:26:46,736 Epoch 1384: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.86 
2024-02-09 08:26:46,737 EPOCH 1385
2024-02-09 08:26:58,012 Epoch 1385: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.79 
2024-02-09 08:26:58,012 EPOCH 1386
2024-02-09 08:27:09,163 Epoch 1386: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.24 
2024-02-09 08:27:09,163 EPOCH 1387
2024-02-09 08:27:19,918 Epoch 1387: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.01 
2024-02-09 08:27:19,919 EPOCH 1388
2024-02-09 08:27:30,651 Epoch 1388: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.33 
2024-02-09 08:27:30,652 EPOCH 1389
2024-02-09 08:27:33,014 [Epoch: 1389 Step: 00023600] Batch Recognition Loss:   0.002858 => Gls Tokens per Sec:     1084 || Batch Translation Loss:   0.108319 => Txt Tokens per Sec:     2512 || Lr: 0.000100
2024-02-09 08:27:41,515 Epoch 1389: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.25 
2024-02-09 08:27:41,516 EPOCH 1390
2024-02-09 08:27:53,137 Epoch 1390: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.65 
2024-02-09 08:27:53,138 EPOCH 1391
2024-02-09 08:28:04,171 Epoch 1391: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.70 
2024-02-09 08:28:04,171 EPOCH 1392
2024-02-09 08:28:14,882 Epoch 1392: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.93 
2024-02-09 08:28:14,883 EPOCH 1393
2024-02-09 08:28:25,548 Epoch 1393: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.62 
2024-02-09 08:28:25,549 EPOCH 1394
2024-02-09 08:28:36,285 Epoch 1394: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.68 
2024-02-09 08:28:36,285 EPOCH 1395
2024-02-09 08:28:39,111 [Epoch: 1395 Step: 00023700] Batch Recognition Loss:   0.000993 => Gls Tokens per Sec:      361 || Batch Translation Loss:   0.037539 => Txt Tokens per Sec:     1202 || Lr: 0.000100
2024-02-09 08:28:47,220 Epoch 1395: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.87 
2024-02-09 08:28:47,221 EPOCH 1396
2024-02-09 08:28:58,320 Epoch 1396: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.66 
2024-02-09 08:28:58,321 EPOCH 1397
2024-02-09 08:29:09,193 Epoch 1397: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.93 
2024-02-09 08:29:09,194 EPOCH 1398
2024-02-09 08:29:20,261 Epoch 1398: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.52 
2024-02-09 08:29:20,262 EPOCH 1399
2024-02-09 08:29:31,171 Epoch 1399: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.84 
2024-02-09 08:29:31,171 EPOCH 1400
2024-02-09 08:29:42,301 [Epoch: 1400 Step: 00023800] Batch Recognition Loss:   0.001192 => Gls Tokens per Sec:      954 || Batch Translation Loss:   0.046564 => Txt Tokens per Sec:     2640 || Lr: 0.000100
2024-02-09 08:29:42,301 Epoch 1400: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.57 
2024-02-09 08:29:42,301 EPOCH 1401
2024-02-09 08:29:53,298 Epoch 1401: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.04 
2024-02-09 08:29:53,299 EPOCH 1402
2024-02-09 08:30:04,362 Epoch 1402: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.79 
2024-02-09 08:30:04,363 EPOCH 1403
2024-02-09 08:30:15,289 Epoch 1403: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-09 08:30:15,289 EPOCH 1404
2024-02-09 08:30:26,222 Epoch 1404: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.46 
2024-02-09 08:30:26,223 EPOCH 1405
2024-02-09 08:30:37,318 Epoch 1405: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.42 
2024-02-09 08:30:37,319 EPOCH 1406
2024-02-09 08:30:47,897 [Epoch: 1406 Step: 00023900] Batch Recognition Loss:   0.001138 => Gls Tokens per Sec:      883 || Batch Translation Loss:   0.017486 => Txt Tokens per Sec:     2453 || Lr: 0.000100
2024-02-09 08:30:48,360 Epoch 1406: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.38 
2024-02-09 08:30:48,360 EPOCH 1407
2024-02-09 08:30:59,291 Epoch 1407: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-09 08:30:59,292 EPOCH 1408
2024-02-09 08:31:10,374 Epoch 1408: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-09 08:31:10,375 EPOCH 1409
2024-02-09 08:31:21,459 Epoch 1409: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-09 08:31:21,460 EPOCH 1410
2024-02-09 08:31:32,519 Epoch 1410: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-09 08:31:32,520 EPOCH 1411
2024-02-09 08:31:43,556 Epoch 1411: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-09 08:31:43,556 EPOCH 1412
2024-02-09 08:31:52,090 [Epoch: 1412 Step: 00024000] Batch Recognition Loss:   0.000915 => Gls Tokens per Sec:      945 || Batch Translation Loss:   0.010868 => Txt Tokens per Sec:     2679 || Lr: 0.000100
2024-02-09 08:32:33,043 Validation result at epoch 1412, step    24000: duration: 40.9533s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.82587	Translation Loss: 93251.09375	PPL: 11091.47852
	Eval Metric: BLEU
	WER 3.67	(DEL: 0.00,	INS: 0.00,	SUB: 3.67)
	BLEU-4 0.65	(BLEU-1: 11.73,	BLEU-2: 3.81,	BLEU-3: 1.46,	BLEU-4: 0.65)
	CHRF 17.48	ROUGE 9.73
2024-02-09 08:32:33,044 Logging Recognition and Translation Outputs
2024-02-09 08:32:33,045 ========================================================================================================================
2024-02-09 08:32:33,045 Logging Sequence: 171_2.00
2024-02-09 08:32:33,045 	Gloss Reference :	A B+C+D+E
2024-02-09 08:32:33,045 	Gloss Hypothesis:	A B+C+D  
2024-02-09 08:32:33,046 	Gloss Alignment :	  S      
2024-02-09 08:32:33,046 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 08:32:33,047 	Text Reference  :	as you might all know      that the  ipl is   about to      end   the       finals are     on    28th may   
2024-02-09 08:32:33,047 	Text Hypothesis :	** *** ***** *** yesterday on   23rd may 2021 in    javelin throw including an     olympic games in   mumbai
2024-02-09 08:32:33,047 	Text Alignment  :	D  D   D     D   S         S    S    S   S    S     S       S     S         S      S       S     S    S     
2024-02-09 08:32:33,048 ========================================================================================================================
2024-02-09 08:32:33,048 Logging Sequence: 119_33.00
2024-02-09 08:32:33,048 	Gloss Reference :	A B+C+D+E
2024-02-09 08:32:33,048 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 08:32:33,048 	Gloss Alignment :	         
2024-02-09 08:32:33,049 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 08:32:33,050 	Text Reference  :	he   wanted to * gift ********* ** *** *** ******* *** *** ***** 35 people    wow wonderful
2024-02-09 08:32:33,050 	Text Hypothesis :	this led    to a gift something to all the players and the staff to celebrate the moment   
2024-02-09 08:32:33,050 	Text Alignment  :	S    S         I      I         I  I   I   I       I   I   I     S  S         S   S        
2024-02-09 08:32:33,050 ========================================================================================================================
2024-02-09 08:32:33,050 Logging Sequence: 158_131.00
2024-02-09 08:32:33,051 	Gloss Reference :	A B+C+D+E
2024-02-09 08:32:33,051 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 08:32:33,051 	Gloss Alignment :	         
2024-02-09 08:32:33,051 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 08:32:33,052 	Text Reference  :	**** *** on   10th april 2023 there was ******** a  match between rcb and     lsg in *** bengaluru
2024-02-09 08:32:33,053 	Text Hypothesis :	this was when rcb  lost  the  match was escorted by virat kohli   is  india's 5th in the auction  
2024-02-09 08:32:33,053 	Text Alignment  :	I    I   S    S    S     S    S         I        S  S     S       S   S       S      I   S        
2024-02-09 08:32:33,053 ========================================================================================================================
2024-02-09 08:32:33,053 Logging Sequence: 164_412.00
2024-02-09 08:32:33,053 	Gloss Reference :	A B+C+D+E
2024-02-09 08:32:33,053 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 08:32:33,054 	Gloss Alignment :	         
2024-02-09 08:32:33,054 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 08:32:33,055 	Text Reference  :	if you divide these two  figures you will be    shocked to know  that each ball's worth is   rs     50 lakhs
2024-02-09 08:32:33,056 	Text Hypothesis :	** *** ****** ***** will bowl    for 120  balls in      20 overs with each ****** ***** over having to bowl 
2024-02-09 08:32:33,056 	Text Alignment  :	D  D   D      D     S    S       S   S    S     S       S  S     S         D      D     S    S      S  S    
2024-02-09 08:32:33,056 ========================================================================================================================
2024-02-09 08:32:33,056 Logging Sequence: 159_112.00
2024-02-09 08:32:33,056 	Gloss Reference :	A B+C+D+E
2024-02-09 08:32:33,056 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 08:32:33,056 	Gloss Alignment :	         
2024-02-09 08:32:33,056 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 08:32:33,058 	Text Reference  :	******** kohli had revealed that before the tournament he   did  not    touch his bat for a month yes 1  month
2024-02-09 08:32:33,058 	Text Hypothesis :	mohammed shami has said     that ****** the ********** pani puri seller on    his *** *** * ***** *** in odi  
2024-02-09 08:32:33,058 	Text Alignment  :	I        S     S   S             D          D          S    S    S      S         D   D   D D     D   S  S    
2024-02-09 08:32:33,058 ========================================================================================================================
2024-02-09 08:32:35,543 Epoch 1412: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.33 
2024-02-09 08:32:35,543 EPOCH 1413
2024-02-09 08:32:46,570 Epoch 1413: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-09 08:32:46,571 EPOCH 1414
2024-02-09 08:32:57,656 Epoch 1414: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 08:32:57,656 EPOCH 1415
2024-02-09 08:33:08,514 Epoch 1415: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 08:33:08,515 EPOCH 1416
2024-02-09 08:33:19,565 Epoch 1416: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 08:33:19,566 EPOCH 1417
2024-02-09 08:33:30,436 Epoch 1417: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 08:33:30,436 EPOCH 1418
2024-02-09 08:33:37,983 [Epoch: 1418 Step: 00024100] Batch Recognition Loss:   0.001806 => Gls Tokens per Sec:      898 || Batch Translation Loss:   0.005761 => Txt Tokens per Sec:     2412 || Lr: 0.000100
2024-02-09 08:33:41,343 Epoch 1418: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 08:33:41,343 EPOCH 1419
2024-02-09 08:33:52,173 Epoch 1419: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.23 
2024-02-09 08:33:52,173 EPOCH 1420
2024-02-09 08:34:03,000 Epoch 1420: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 08:34:03,000 EPOCH 1421
2024-02-09 08:34:13,713 Epoch 1421: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-09 08:34:13,714 EPOCH 1422
2024-02-09 08:34:24,839 Epoch 1422: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-09 08:34:24,840 EPOCH 1423
2024-02-09 08:34:35,907 Epoch 1423: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-09 08:34:35,908 EPOCH 1424
2024-02-09 08:34:41,390 [Epoch: 1424 Step: 00024200] Batch Recognition Loss:   0.000292 => Gls Tokens per Sec:     1004 || Batch Translation Loss:   0.011781 => Txt Tokens per Sec:     2596 || Lr: 0.000100
2024-02-09 08:34:46,408 Epoch 1424: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-09 08:34:46,408 EPOCH 1425
2024-02-09 08:34:56,912 Epoch 1425: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-09 08:34:56,913 EPOCH 1426
2024-02-09 08:35:07,902 Epoch 1426: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-09 08:35:07,903 EPOCH 1427
2024-02-09 08:35:18,640 Epoch 1427: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-09 08:35:18,640 EPOCH 1428
2024-02-09 08:35:29,169 Epoch 1428: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.98 
2024-02-09 08:35:29,169 EPOCH 1429
2024-02-09 08:35:40,120 Epoch 1429: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-09 08:35:40,121 EPOCH 1430
2024-02-09 08:35:43,886 [Epoch: 1430 Step: 00024300] Batch Recognition Loss:   0.000309 => Gls Tokens per Sec:     1121 || Batch Translation Loss:   0.030625 => Txt Tokens per Sec:     2918 || Lr: 0.000100
2024-02-09 08:35:50,921 Epoch 1430: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.77 
2024-02-09 08:35:50,921 EPOCH 1431
2024-02-09 08:36:01,782 Epoch 1431: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.72 
2024-02-09 08:36:01,782 EPOCH 1432
2024-02-09 08:36:12,632 Epoch 1432: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.66 
2024-02-09 08:36:12,632 EPOCH 1433
2024-02-09 08:36:23,526 Epoch 1433: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.63 
2024-02-09 08:36:23,527 EPOCH 1434
2024-02-09 08:36:34,252 Epoch 1434: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.35 
2024-02-09 08:36:34,253 EPOCH 1435
2024-02-09 08:36:44,942 Epoch 1435: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.29 
2024-02-09 08:36:44,942 EPOCH 1436
2024-02-09 08:36:47,338 [Epoch: 1436 Step: 00024400] Batch Recognition Loss:   0.000267 => Gls Tokens per Sec:     1336 || Batch Translation Loss:   0.017056 => Txt Tokens per Sec:     3306 || Lr: 0.000100
2024-02-09 08:36:55,946 Epoch 1436: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.26 
2024-02-09 08:36:55,947 EPOCH 1437
2024-02-09 08:37:06,795 Epoch 1437: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.28 
2024-02-09 08:37:06,795 EPOCH 1438
2024-02-09 08:37:17,772 Epoch 1438: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.28 
2024-02-09 08:37:17,773 EPOCH 1439
2024-02-09 08:37:28,824 Epoch 1439: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-09 08:37:28,825 EPOCH 1440
2024-02-09 08:37:39,663 Epoch 1440: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 08:37:39,664 EPOCH 1441
2024-02-09 08:37:50,338 Epoch 1441: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 08:37:50,338 EPOCH 1442
2024-02-09 08:37:50,771 [Epoch: 1442 Step: 00024500] Batch Recognition Loss:   0.000123 => Gls Tokens per Sec:     4444 || Batch Translation Loss:   0.012828 => Txt Tokens per Sec:    10169 || Lr: 0.000100
2024-02-09 08:38:00,575 Epoch 1442: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 08:38:00,576 EPOCH 1443
2024-02-09 08:38:11,504 Epoch 1443: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 08:38:11,505 EPOCH 1444
2024-02-09 08:38:22,461 Epoch 1444: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 08:38:22,462 EPOCH 1445
2024-02-09 08:38:33,108 Epoch 1445: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 08:38:33,109 EPOCH 1446
2024-02-09 08:38:44,019 Epoch 1446: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-09 08:38:44,019 EPOCH 1447
2024-02-09 08:38:54,765 Epoch 1447: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-09 08:38:54,766 EPOCH 1448
2024-02-09 08:38:55,096 [Epoch: 1448 Step: 00024600] Batch Recognition Loss:   0.000499 => Gls Tokens per Sec:     1951 || Batch Translation Loss:   0.020770 => Txt Tokens per Sec:     6119 || Lr: 0.000100
2024-02-09 08:39:05,570 Epoch 1448: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.64 
2024-02-09 08:39:05,571 EPOCH 1449
2024-02-09 08:39:16,349 Epoch 1449: Total Training Recognition Loss 0.09  Total Training Translation Loss 0.73 
2024-02-09 08:39:16,349 EPOCH 1450
2024-02-09 08:39:27,417 Epoch 1450: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.10 
2024-02-09 08:39:27,417 EPOCH 1451
2024-02-09 08:39:38,396 Epoch 1451: Total Training Recognition Loss 0.70  Total Training Translation Loss 1.87 
2024-02-09 08:39:38,397 EPOCH 1452
2024-02-09 08:39:49,400 Epoch 1452: Total Training Recognition Loss 1.04  Total Training Translation Loss 3.72 
2024-02-09 08:39:49,401 EPOCH 1453
2024-02-09 08:40:00,181 [Epoch: 1453 Step: 00024700] Batch Recognition Loss:   0.007406 => Gls Tokens per Sec:      926 || Batch Translation Loss:   0.080047 => Txt Tokens per Sec:     2535 || Lr: 0.000100
2024-02-09 08:40:00,572 Epoch 1453: Total Training Recognition Loss 0.80  Total Training Translation Loss 5.13 
2024-02-09 08:40:00,572 EPOCH 1454
2024-02-09 08:40:11,587 Epoch 1454: Total Training Recognition Loss 1.60  Total Training Translation Loss 4.54 
2024-02-09 08:40:11,588 EPOCH 1455
2024-02-09 08:40:22,706 Epoch 1455: Total Training Recognition Loss 0.60  Total Training Translation Loss 3.51 
2024-02-09 08:40:22,707 EPOCH 1456
2024-02-09 08:40:33,341 Epoch 1456: Total Training Recognition Loss 0.09  Total Training Translation Loss 3.65 
2024-02-09 08:40:33,341 EPOCH 1457
2024-02-09 08:40:44,149 Epoch 1457: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.86 
2024-02-09 08:40:44,149 EPOCH 1458
2024-02-09 08:40:55,104 Epoch 1458: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.36 
2024-02-09 08:40:55,105 EPOCH 1459
2024-02-09 08:41:05,303 [Epoch: 1459 Step: 00024800] Batch Recognition Loss:   0.003194 => Gls Tokens per Sec:      853 || Batch Translation Loss:   0.078555 => Txt Tokens per Sec:     2430 || Lr: 0.000100
2024-02-09 08:41:05,870 Epoch 1459: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.92 
2024-02-09 08:41:05,871 EPOCH 1460
2024-02-09 08:41:16,880 Epoch 1460: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.70 
2024-02-09 08:41:16,880 EPOCH 1461
2024-02-09 08:41:27,850 Epoch 1461: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.53 
2024-02-09 08:41:27,851 EPOCH 1462
2024-02-09 08:41:38,906 Epoch 1462: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.45 
2024-02-09 08:41:38,906 EPOCH 1463
2024-02-09 08:41:49,419 Epoch 1463: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-09 08:41:49,419 EPOCH 1464
2024-02-09 08:42:00,175 Epoch 1464: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.34 
2024-02-09 08:42:00,175 EPOCH 1465
2024-02-09 08:42:08,116 [Epoch: 1465 Step: 00024900] Batch Recognition Loss:   0.000646 => Gls Tokens per Sec:      935 || Batch Translation Loss:   0.025540 => Txt Tokens per Sec:     2631 || Lr: 0.000100
2024-02-09 08:42:10,897 Epoch 1465: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-09 08:42:10,897 EPOCH 1466
2024-02-09 08:42:21,588 Epoch 1466: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.36 
2024-02-09 08:42:21,589 EPOCH 1467
2024-02-09 08:42:32,446 Epoch 1467: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-09 08:42:32,447 EPOCH 1468
2024-02-09 08:42:43,294 Epoch 1468: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.28 
2024-02-09 08:42:43,295 EPOCH 1469
2024-02-09 08:42:53,865 Epoch 1469: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-09 08:42:53,866 EPOCH 1470
2024-02-09 08:43:04,988 Epoch 1470: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.47 
2024-02-09 08:43:04,988 EPOCH 1471
2024-02-09 08:43:11,379 [Epoch: 1471 Step: 00025000] Batch Recognition Loss:   0.001242 => Gls Tokens per Sec:      961 || Batch Translation Loss:   0.017879 => Txt Tokens per Sec:     2674 || Lr: 0.000100
2024-02-09 08:43:16,019 Epoch 1471: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-09 08:43:16,019 EPOCH 1472
2024-02-09 08:43:26,871 Epoch 1472: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.75 
2024-02-09 08:43:26,872 EPOCH 1473
2024-02-09 08:43:37,731 Epoch 1473: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-09 08:43:37,732 EPOCH 1474
2024-02-09 08:43:48,753 Epoch 1474: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-09 08:43:48,754 EPOCH 1475
2024-02-09 08:43:59,745 Epoch 1475: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-09 08:43:59,746 EPOCH 1476
2024-02-09 08:44:10,609 Epoch 1476: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-09 08:44:10,610 EPOCH 1477
2024-02-09 08:44:15,785 [Epoch: 1477 Step: 00025100] Batch Recognition Loss:   0.000492 => Gls Tokens per Sec:      990 || Batch Translation Loss:   0.026592 => Txt Tokens per Sec:     2840 || Lr: 0.000100
2024-02-09 08:44:21,329 Epoch 1477: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-09 08:44:21,330 EPOCH 1478
2024-02-09 08:44:31,964 Epoch 1478: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-09 08:44:31,964 EPOCH 1479
2024-02-09 08:44:42,631 Epoch 1479: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-09 08:44:42,631 EPOCH 1480
2024-02-09 08:44:53,136 Epoch 1480: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-09 08:44:53,137 EPOCH 1481
2024-02-09 08:45:04,053 Epoch 1481: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-09 08:45:04,053 EPOCH 1482
2024-02-09 08:45:14,676 Epoch 1482: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.31 
2024-02-09 08:45:14,676 EPOCH 1483
2024-02-09 08:45:18,048 [Epoch: 1483 Step: 00025200] Batch Recognition Loss:   0.000187 => Gls Tokens per Sec:     1139 || Batch Translation Loss:   0.025038 => Txt Tokens per Sec:     3200 || Lr: 0.000100
2024-02-09 08:45:25,621 Epoch 1483: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-09 08:45:25,621 EPOCH 1484
2024-02-09 08:45:36,613 Epoch 1484: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-09 08:45:36,614 EPOCH 1485
2024-02-09 08:45:47,486 Epoch 1485: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-09 08:45:47,487 EPOCH 1486
2024-02-09 08:45:58,319 Epoch 1486: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-09 08:45:58,319 EPOCH 1487
2024-02-09 08:46:08,995 Epoch 1487: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-09 08:46:08,995 EPOCH 1488
2024-02-09 08:46:19,049 Epoch 1488: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-09 08:46:19,049 EPOCH 1489
2024-02-09 08:46:22,060 [Epoch: 1489 Step: 00025300] Batch Recognition Loss:   0.000555 => Gls Tokens per Sec:      764 || Batch Translation Loss:   0.016113 => Txt Tokens per Sec:     2229 || Lr: 0.000100
2024-02-09 08:46:29,132 Epoch 1489: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-09 08:46:29,132 EPOCH 1490
2024-02-09 08:46:39,651 Epoch 1490: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 08:46:39,652 EPOCH 1491
2024-02-09 08:46:50,688 Epoch 1491: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 08:46:50,689 EPOCH 1492
2024-02-09 08:47:01,573 Epoch 1492: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-09 08:47:01,573 EPOCH 1493
2024-02-09 08:47:12,453 Epoch 1493: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-09 08:47:12,454 EPOCH 1494
2024-02-09 08:47:23,122 Epoch 1494: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-09 08:47:23,123 EPOCH 1495
2024-02-09 08:47:23,444 [Epoch: 1495 Step: 00025400] Batch Recognition Loss:   0.000220 => Gls Tokens per Sec:     4000 || Batch Translation Loss:   0.012319 => Txt Tokens per Sec:     9641 || Lr: 0.000100
2024-02-09 08:47:33,998 Epoch 1495: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 08:47:33,999 EPOCH 1496
2024-02-09 08:47:44,730 Epoch 1496: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 08:47:44,731 EPOCH 1497
2024-02-09 08:47:55,458 Epoch 1497: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-09 08:47:55,459 EPOCH 1498
2024-02-09 08:48:06,098 Epoch 1498: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-09 08:48:06,099 EPOCH 1499
2024-02-09 08:48:17,174 Epoch 1499: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-09 08:48:17,175 EPOCH 1500
2024-02-09 08:48:27,669 [Epoch: 1500 Step: 00025500] Batch Recognition Loss:   0.000903 => Gls Tokens per Sec:     1012 || Batch Translation Loss:   0.022489 => Txt Tokens per Sec:     2800 || Lr: 0.000100
2024-02-09 08:48:27,670 Epoch 1500: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-09 08:48:27,670 EPOCH 1501
2024-02-09 08:48:38,675 Epoch 1501: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-09 08:48:38,675 EPOCH 1502
2024-02-09 08:48:49,705 Epoch 1502: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.70 
2024-02-09 08:48:49,705 EPOCH 1503
2024-02-09 08:49:00,569 Epoch 1503: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-09 08:49:00,570 EPOCH 1504
2024-02-09 08:49:11,578 Epoch 1504: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.15 
2024-02-09 08:49:11,578 EPOCH 1505
2024-02-09 08:49:22,484 Epoch 1505: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-09 08:49:22,485 EPOCH 1506
2024-02-09 08:49:31,196 [Epoch: 1506 Step: 00025600] Batch Recognition Loss:   0.000313 => Gls Tokens per Sec:     1072 || Batch Translation Loss:   0.088052 => Txt Tokens per Sec:     2974 || Lr: 0.000100
2024-02-09 08:49:33,088 Epoch 1506: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.40 
2024-02-09 08:49:33,088 EPOCH 1507
2024-02-09 08:49:44,058 Epoch 1507: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.92 
2024-02-09 08:49:44,059 EPOCH 1508
2024-02-09 08:49:55,015 Epoch 1508: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.82 
2024-02-09 08:49:55,016 EPOCH 1509
2024-02-09 08:50:05,886 Epoch 1509: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.28 
2024-02-09 08:50:05,887 EPOCH 1510
2024-02-09 08:50:16,687 Epoch 1510: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.80 
2024-02-09 08:50:16,687 EPOCH 1511
2024-02-09 08:50:27,499 Epoch 1511: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.73 
2024-02-09 08:50:27,499 EPOCH 1512
2024-02-09 08:50:35,832 [Epoch: 1512 Step: 00025700] Batch Recognition Loss:   0.000314 => Gls Tokens per Sec:      967 || Batch Translation Loss:   0.065327 => Txt Tokens per Sec:     2768 || Lr: 0.000100
2024-02-09 08:50:38,366 Epoch 1512: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.73 
2024-02-09 08:50:38,367 EPOCH 1513
2024-02-09 08:50:49,332 Epoch 1513: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.85 
2024-02-09 08:50:49,333 EPOCH 1514
2024-02-09 08:51:00,144 Epoch 1514: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.82 
2024-02-09 08:51:00,144 EPOCH 1515
2024-02-09 08:51:11,165 Epoch 1515: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.17 
2024-02-09 08:51:11,166 EPOCH 1516
2024-02-09 08:51:22,106 Epoch 1516: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.15 
2024-02-09 08:51:22,107 EPOCH 1517
2024-02-09 08:51:33,243 Epoch 1517: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.71 
2024-02-09 08:51:33,244 EPOCH 1518
2024-02-09 08:51:39,675 [Epoch: 1518 Step: 00025800] Batch Recognition Loss:   0.001042 => Gls Tokens per Sec:     1054 || Batch Translation Loss:   0.093505 => Txt Tokens per Sec:     2761 || Lr: 0.000100
2024-02-09 08:51:44,155 Epoch 1518: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.02 
2024-02-09 08:51:44,156 EPOCH 1519
2024-02-09 08:51:55,178 Epoch 1519: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.50 
2024-02-09 08:51:55,179 EPOCH 1520
2024-02-09 08:52:06,184 Epoch 1520: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.96 
2024-02-09 08:52:06,185 EPOCH 1521
2024-02-09 08:52:17,145 Epoch 1521: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.11 
2024-02-09 08:52:17,145 EPOCH 1522
2024-02-09 08:52:28,042 Epoch 1522: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.92 
2024-02-09 08:52:28,042 EPOCH 1523
2024-02-09 08:52:38,590 Epoch 1523: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.32 
2024-02-09 08:52:38,590 EPOCH 1524
2024-02-09 08:52:42,187 [Epoch: 1524 Step: 00025900] Batch Recognition Loss:   0.001119 => Gls Tokens per Sec:     1602 || Batch Translation Loss:   0.030652 => Txt Tokens per Sec:     4371 || Lr: 0.000100
2024-02-09 08:52:49,524 Epoch 1524: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.86 
2024-02-09 08:52:49,525 EPOCH 1525
2024-02-09 08:53:00,495 Epoch 1525: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-09 08:53:00,495 EPOCH 1526
2024-02-09 08:53:11,332 Epoch 1526: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-09 08:53:11,333 EPOCH 1527
2024-02-09 08:53:22,327 Epoch 1527: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-09 08:53:22,328 EPOCH 1528
2024-02-09 08:53:32,851 Epoch 1528: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-09 08:53:32,852 EPOCH 1529
2024-02-09 08:53:43,649 Epoch 1529: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-09 08:53:43,649 EPOCH 1530
2024-02-09 08:53:49,455 [Epoch: 1530 Step: 00026000] Batch Recognition Loss:   0.000277 => Gls Tokens per Sec:      727 || Batch Translation Loss:   0.018459 => Txt Tokens per Sec:     2065 || Lr: 0.000100
2024-02-09 08:54:29,432 Validation result at epoch 1530, step    26000: duration: 39.9769s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.64269	Translation Loss: 94343.39844	PPL: 12370.02930
	Eval Metric: BLEU
	WER 3.25	(DEL: 0.00,	INS: 0.00,	SUB: 3.25)
	BLEU-4 0.70	(BLEU-1: 11.28,	BLEU-2: 3.82,	BLEU-3: 1.52,	BLEU-4: 0.70)
	CHRF 17.17	ROUGE 9.34
2024-02-09 08:54:29,433 Logging Recognition and Translation Outputs
2024-02-09 08:54:29,433 ========================================================================================================================
2024-02-09 08:54:29,433 Logging Sequence: 166_243.00
2024-02-09 08:54:29,434 	Gloss Reference :	A B+C+D+E
2024-02-09 08:54:29,434 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 08:54:29,434 	Gloss Alignment :	         
2024-02-09 08:54:29,434 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 08:54:29,435 	Text Reference  :	*** ********* *********** ********* *** ***** ** icc     worked with members boards like bcci pcb   cricket australia etc 
2024-02-09 08:54:29,435 	Text Hypothesis :	the broadcast advertisers ticketing etc would be decided by     the  board   of     the  2    teams playing the       test
2024-02-09 08:54:29,436 	Text Alignment  :	I   I         I           I         I   I     I  S       S      S    S       S      S    S    S     S       S         S   
2024-02-09 08:54:29,436 ========================================================================================================================
2024-02-09 08:54:29,436 Logging Sequence: 59_152.00
2024-02-09 08:54:29,436 	Gloss Reference :	A B+C+D+E
2024-02-09 08:54:29,436 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 08:54:29,436 	Gloss Alignment :	         
2024-02-09 08:54:29,436 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 08:54:29,437 	Text Reference  :	**** the organisers encouraged athletes to    use      the condoms in their home countries
2024-02-09 08:54:29,437 	Text Hypothesis :	well the ********** ********** ******** tokyo olympics in  2021    on 1st   may  2021     
2024-02-09 08:54:29,438 	Text Alignment  :	I        D          D          D        S     S        S   S       S  S     S    S        
2024-02-09 08:54:29,438 ========================================================================================================================
2024-02-09 08:54:29,438 Logging Sequence: 145_52.00
2024-02-09 08:54:29,438 	Gloss Reference :	A B+C+D+E
2024-02-09 08:54:29,438 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 08:54:29,438 	Gloss Alignment :	         
2024-02-09 08:54:29,439 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 08:54:29,440 	Text Reference  :	her name was dropped despite having qualified as she   was   the only female athlete
2024-02-09 08:54:29,440 	Text Hypothesis :	*** **** *** ******* ******* the    finals    of their child has a    strong athlete
2024-02-09 08:54:29,440 	Text Alignment  :	D   D    D   D       D       S      S         S  S     S     S   S    S             
2024-02-09 08:54:29,440 ========================================================================================================================
2024-02-09 08:54:29,440 Logging Sequence: 172_163.00
2024-02-09 08:54:29,440 	Gloss Reference :	A B+C+D+E
2024-02-09 08:54:29,440 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 08:54:29,441 	Gloss Alignment :	         
2024-02-09 08:54:29,441 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 08:54:29,442 	Text Reference  :	if the match starts anywhere between 730 pm ** to 935   pm     a       full   20-over match can be played 
2024-02-09 08:54:29,443 	Text Hypothesis :	** the match starts ******** at      730 pm if a  major tennis players leaves the     field for 9  minutes
2024-02-09 08:54:29,443 	Text Alignment  :	D                   D        S              I  S  S     S      S       S      S       S     S   S  S      
2024-02-09 08:54:29,443 ========================================================================================================================
2024-02-09 08:54:29,443 Logging Sequence: 150_20.00
2024-02-09 08:54:29,443 	Gloss Reference :	A B+C+D+E
2024-02-09 08:54:29,443 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 08:54:29,443 	Gloss Alignment :	         
2024-02-09 08:54:29,444 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 08:54:29,445 	Text Reference  :	** ** *** after   a    tough         match india won the saff championship 2023  title
2024-02-09 08:54:29,445 	Text Hypothesis :	he is now retired from international team  has   won the **** ************ world cup  
2024-02-09 08:54:29,445 	Text Alignment  :	I  I  I   S       S    S             S     S             D    D            S     S    
2024-02-09 08:54:29,445 ========================================================================================================================
2024-02-09 08:54:34,903 Epoch 1530: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-09 08:54:34,904 EPOCH 1531
2024-02-09 08:54:45,703 Epoch 1531: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-09 08:54:45,703 EPOCH 1532
2024-02-09 08:54:56,800 Epoch 1532: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-09 08:54:56,801 EPOCH 1533
2024-02-09 08:55:07,598 Epoch 1533: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-09 08:55:07,598 EPOCH 1534
2024-02-09 08:55:18,520 Epoch 1534: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-09 08:55:18,520 EPOCH 1535
2024-02-09 08:55:29,295 Epoch 1535: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-09 08:55:29,295 EPOCH 1536
2024-02-09 08:55:32,489 [Epoch: 1536 Step: 00026100] Batch Recognition Loss:   0.000985 => Gls Tokens per Sec:      921 || Batch Translation Loss:   0.010946 => Txt Tokens per Sec:     2351 || Lr: 0.000100
2024-02-09 08:55:40,010 Epoch 1536: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-09 08:55:40,011 EPOCH 1537
2024-02-09 08:55:50,917 Epoch 1537: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-09 08:55:50,918 EPOCH 1538
2024-02-09 08:56:01,807 Epoch 1538: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-09 08:56:01,807 EPOCH 1539
2024-02-09 08:56:12,586 Epoch 1539: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 08:56:12,586 EPOCH 1540
2024-02-09 08:56:23,589 Epoch 1540: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-09 08:56:23,590 EPOCH 1541
2024-02-09 08:56:34,560 Epoch 1541: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-09 08:56:34,560 EPOCH 1542
2024-02-09 08:56:35,403 [Epoch: 1542 Step: 00026200] Batch Recognition Loss:   0.000174 => Gls Tokens per Sec:     2280 || Batch Translation Loss:   0.033962 => Txt Tokens per Sec:     6356 || Lr: 0.000100
2024-02-09 08:56:45,754 Epoch 1542: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-09 08:56:45,754 EPOCH 1543
2024-02-09 08:56:56,780 Epoch 1543: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-09 08:56:56,781 EPOCH 1544
2024-02-09 08:57:07,428 Epoch 1544: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-09 08:57:07,428 EPOCH 1545
2024-02-09 08:57:18,340 Epoch 1545: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-09 08:57:18,340 EPOCH 1546
2024-02-09 08:57:29,264 Epoch 1546: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-09 08:57:29,265 EPOCH 1547
2024-02-09 08:57:39,908 Epoch 1547: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-09 08:57:39,909 EPOCH 1548
2024-02-09 08:57:40,295 [Epoch: 1548 Step: 00026300] Batch Recognition Loss:   0.000536 => Gls Tokens per Sec:     1662 || Batch Translation Loss:   0.017427 => Txt Tokens per Sec:     5249 || Lr: 0.000100
2024-02-09 08:57:50,917 Epoch 1548: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-09 08:57:50,918 EPOCH 1549
2024-02-09 08:58:01,894 Epoch 1549: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-09 08:58:01,895 EPOCH 1550
2024-02-09 08:58:12,956 Epoch 1550: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-09 08:58:12,957 EPOCH 1551
2024-02-09 08:58:23,568 Epoch 1551: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-09 08:58:23,568 EPOCH 1552
2024-02-09 08:58:34,312 Epoch 1552: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.78 
2024-02-09 08:58:34,312 EPOCH 1553
2024-02-09 08:58:43,601 [Epoch: 1553 Step: 00026400] Batch Recognition Loss:   0.000770 => Gls Tokens per Sec:     1074 || Batch Translation Loss:   0.015601 => Txt Tokens per Sec:     2927 || Lr: 0.000100
2024-02-09 08:58:45,251 Epoch 1553: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-09 08:58:45,252 EPOCH 1554
2024-02-09 08:58:56,353 Epoch 1554: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-09 08:58:56,354 EPOCH 1555
2024-02-09 08:59:07,219 Epoch 1555: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-09 08:59:07,220 EPOCH 1556
2024-02-09 08:59:18,268 Epoch 1556: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-09 08:59:18,269 EPOCH 1557
2024-02-09 08:59:29,324 Epoch 1557: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.82 
2024-02-09 08:59:29,324 EPOCH 1558
2024-02-09 08:59:40,401 Epoch 1558: Total Training Recognition Loss 0.05  Total Training Translation Loss 13.32 
2024-02-09 08:59:40,402 EPOCH 1559
2024-02-09 08:59:48,999 [Epoch: 1559 Step: 00026500] Batch Recognition Loss:   0.001618 => Gls Tokens per Sec:     1012 || Batch Translation Loss:   0.125783 => Txt Tokens per Sec:     2840 || Lr: 0.000100
2024-02-09 08:59:51,300 Epoch 1559: Total Training Recognition Loss 0.09  Total Training Translation Loss 5.99 
2024-02-09 08:59:51,301 EPOCH 1560
2024-02-09 09:00:02,107 Epoch 1560: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.67 
2024-02-09 09:00:02,108 EPOCH 1561
2024-02-09 09:00:13,150 Epoch 1561: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.21 
2024-02-09 09:00:13,150 EPOCH 1562
2024-02-09 09:00:24,024 Epoch 1562: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.76 
2024-02-09 09:00:24,025 EPOCH 1563
2024-02-09 09:00:34,930 Epoch 1563: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-09 09:00:34,930 EPOCH 1564
2024-02-09 09:00:45,602 Epoch 1564: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.44 
2024-02-09 09:00:45,602 EPOCH 1565
2024-02-09 09:00:53,557 [Epoch: 1565 Step: 00026600] Batch Recognition Loss:   0.000621 => Gls Tokens per Sec:      933 || Batch Translation Loss:   0.020166 => Txt Tokens per Sec:     2551 || Lr: 0.000100
2024-02-09 09:00:56,483 Epoch 1565: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.41 
2024-02-09 09:00:56,484 EPOCH 1566
2024-02-09 09:01:07,174 Epoch 1566: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-09 09:01:07,175 EPOCH 1567
2024-02-09 09:01:17,911 Epoch 1567: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-09 09:01:17,911 EPOCH 1568
2024-02-09 09:01:28,562 Epoch 1568: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-09 09:01:28,562 EPOCH 1569
2024-02-09 09:01:39,557 Epoch 1569: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-09 09:01:39,558 EPOCH 1570
2024-02-09 09:01:50,399 Epoch 1570: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-09 09:01:50,400 EPOCH 1571
2024-02-09 09:01:56,656 [Epoch: 1571 Step: 00026700] Batch Recognition Loss:   0.000421 => Gls Tokens per Sec:      982 || Batch Translation Loss:   0.066963 => Txt Tokens per Sec:     2761 || Lr: 0.000100
2024-02-09 09:02:01,446 Epoch 1571: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-09 09:02:01,447 EPOCH 1572
2024-02-09 09:02:12,498 Epoch 1572: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-09 09:02:12,499 EPOCH 1573
2024-02-09 09:02:23,472 Epoch 1573: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-09 09:02:23,473 EPOCH 1574
2024-02-09 09:02:34,398 Epoch 1574: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-09 09:02:34,398 EPOCH 1575
2024-02-09 09:02:45,193 Epoch 1575: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-09 09:02:45,193 EPOCH 1576
2024-02-09 09:02:56,172 Epoch 1576: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-09 09:02:56,173 EPOCH 1577
2024-02-09 09:03:03,801 [Epoch: 1577 Step: 00026800] Batch Recognition Loss:   0.000296 => Gls Tokens per Sec:      637 || Batch Translation Loss:   0.022702 => Txt Tokens per Sec:     1911 || Lr: 0.000100
2024-02-09 09:03:07,042 Epoch 1577: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-09 09:03:07,042 EPOCH 1578
2024-02-09 09:03:17,685 Epoch 1578: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-09 09:03:17,685 EPOCH 1579
2024-02-09 09:03:28,409 Epoch 1579: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-09 09:03:28,410 EPOCH 1580
2024-02-09 09:03:39,335 Epoch 1580: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.72 
2024-02-09 09:03:39,335 EPOCH 1581
2024-02-09 09:03:50,204 Epoch 1581: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.64 
2024-02-09 09:03:50,204 EPOCH 1582
2024-02-09 09:04:00,988 Epoch 1582: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-09 09:04:00,988 EPOCH 1583
2024-02-09 09:04:05,376 [Epoch: 1583 Step: 00026900] Batch Recognition Loss:   0.000362 => Gls Tokens per Sec:      875 || Batch Translation Loss:   0.023763 => Txt Tokens per Sec:     2552 || Lr: 0.000100
2024-02-09 09:04:11,768 Epoch 1583: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-09 09:04:11,768 EPOCH 1584
2024-02-09 09:04:22,547 Epoch 1584: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-09 09:04:22,548 EPOCH 1585
2024-02-09 09:04:33,537 Epoch 1585: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-09 09:04:33,538 EPOCH 1586
2024-02-09 09:04:44,459 Epoch 1586: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-09 09:04:44,460 EPOCH 1587
2024-02-09 09:04:55,331 Epoch 1587: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-09 09:04:55,332 EPOCH 1588
2024-02-09 09:05:06,310 Epoch 1588: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-09 09:05:06,310 EPOCH 1589
2024-02-09 09:05:08,680 [Epoch: 1589 Step: 00027000] Batch Recognition Loss:   0.000291 => Gls Tokens per Sec:     1081 || Batch Translation Loss:   0.025999 => Txt Tokens per Sec:     3170 || Lr: 0.000100
2024-02-09 09:05:16,985 Epoch 1589: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-09 09:05:16,985 EPOCH 1590
2024-02-09 09:05:27,936 Epoch 1590: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-09 09:05:27,936 EPOCH 1591
2024-02-09 09:05:38,994 Epoch 1591: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-09 09:05:38,995 EPOCH 1592
2024-02-09 09:05:49,398 Epoch 1592: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-09 09:05:49,398 EPOCH 1593
2024-02-09 09:06:00,132 Epoch 1593: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-09 09:06:00,132 EPOCH 1594
2024-02-09 09:06:10,756 Epoch 1594: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-09 09:06:10,757 EPOCH 1595
2024-02-09 09:06:11,168 [Epoch: 1595 Step: 00027100] Batch Recognition Loss:   0.000199 => Gls Tokens per Sec:     3130 || Batch Translation Loss:   0.018559 => Txt Tokens per Sec:     8506 || Lr: 0.000100
2024-02-09 09:06:21,483 Epoch 1595: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-09 09:06:21,484 EPOCH 1596
2024-02-09 09:06:32,279 Epoch 1596: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-09 09:06:32,280 EPOCH 1597
2024-02-09 09:06:43,105 Epoch 1597: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-09 09:06:43,106 EPOCH 1598
2024-02-09 09:06:54,028 Epoch 1598: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-09 09:06:54,029 EPOCH 1599
2024-02-09 09:07:04,589 Epoch 1599: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-09 09:07:04,589 EPOCH 1600
2024-02-09 09:07:14,871 [Epoch: 1600 Step: 00027200] Batch Recognition Loss:   0.000183 => Gls Tokens per Sec:     1033 || Batch Translation Loss:   0.011992 => Txt Tokens per Sec:     2858 || Lr: 0.000100
2024-02-09 09:07:14,871 Epoch 1600: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-09 09:07:14,872 EPOCH 1601
2024-02-09 09:07:25,796 Epoch 1601: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-09 09:07:25,797 EPOCH 1602
2024-02-09 09:07:36,713 Epoch 1602: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-09 09:07:36,714 EPOCH 1603
2024-02-09 09:07:47,460 Epoch 1603: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-09 09:07:47,460 EPOCH 1604
2024-02-09 09:07:58,438 Epoch 1604: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-09 09:07:58,439 EPOCH 1605
2024-02-09 09:08:09,403 Epoch 1605: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 09:08:09,404 EPOCH 1606
2024-02-09 09:08:15,712 [Epoch: 1606 Step: 00027300] Batch Recognition Loss:   0.000176 => Gls Tokens per Sec:     1522 || Batch Translation Loss:   0.051972 => Txt Tokens per Sec:     4091 || Lr: 0.000100
2024-02-09 09:08:20,141 Epoch 1606: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-09 09:08:20,142 EPOCH 1607
2024-02-09 09:08:31,092 Epoch 1607: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.63 
2024-02-09 09:08:31,093 EPOCH 1608
2024-02-09 09:08:42,122 Epoch 1608: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.73 
2024-02-09 09:08:42,123 EPOCH 1609
2024-02-09 09:08:53,222 Epoch 1609: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-09 09:08:53,223 EPOCH 1610
2024-02-09 09:09:04,288 Epoch 1610: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-09 09:09:04,289 EPOCH 1611
2024-02-09 09:09:15,280 Epoch 1611: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.50 
2024-02-09 09:09:15,280 EPOCH 1612
2024-02-09 09:09:23,886 [Epoch: 1612 Step: 00027400] Batch Recognition Loss:   0.000761 => Gls Tokens per Sec:      937 || Batch Translation Loss:   0.020578 => Txt Tokens per Sec:     2601 || Lr: 0.000100
2024-02-09 09:09:26,297 Epoch 1612: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-09 09:09:26,297 EPOCH 1613
2024-02-09 09:09:37,324 Epoch 1613: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-09 09:09:37,325 EPOCH 1614
2024-02-09 09:09:48,492 Epoch 1614: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-09 09:09:48,493 EPOCH 1615
2024-02-09 09:09:59,343 Epoch 1615: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-09 09:09:59,344 EPOCH 1616
2024-02-09 09:10:10,571 Epoch 1616: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.77 
2024-02-09 09:10:10,571 EPOCH 1617
2024-02-09 09:10:21,426 Epoch 1617: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-09 09:10:21,427 EPOCH 1618
2024-02-09 09:10:29,210 [Epoch: 1618 Step: 00027500] Batch Recognition Loss:   0.000382 => Gls Tokens per Sec:      871 || Batch Translation Loss:   0.033646 => Txt Tokens per Sec:     2348 || Lr: 0.000100
2024-02-09 09:10:32,227 Epoch 1618: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-09 09:10:32,227 EPOCH 1619
2024-02-09 09:10:43,130 Epoch 1619: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.99 
2024-02-09 09:10:43,130 EPOCH 1620
2024-02-09 09:10:54,018 Epoch 1620: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-09 09:10:54,018 EPOCH 1621
2024-02-09 09:11:04,762 Epoch 1621: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-09 09:11:04,762 EPOCH 1622
2024-02-09 09:11:15,689 Epoch 1622: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.82 
2024-02-09 09:11:15,689 EPOCH 1623
2024-02-09 09:11:26,647 Epoch 1623: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.25 
2024-02-09 09:11:26,647 EPOCH 1624
2024-02-09 09:11:34,568 [Epoch: 1624 Step: 00027600] Batch Recognition Loss:   0.000370 => Gls Tokens per Sec:      695 || Batch Translation Loss:   0.077383 => Txt Tokens per Sec:     1970 || Lr: 0.000100
2024-02-09 09:11:37,957 Epoch 1624: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.90 
2024-02-09 09:11:37,958 EPOCH 1625
2024-02-09 09:11:48,905 Epoch 1625: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.05 
2024-02-09 09:11:48,906 EPOCH 1626
2024-02-09 09:11:59,995 Epoch 1626: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.00 
2024-02-09 09:11:59,995 EPOCH 1627
2024-02-09 09:12:11,040 Epoch 1627: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.01 
2024-02-09 09:12:11,040 EPOCH 1628
2024-02-09 09:12:21,919 Epoch 1628: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.96 
2024-02-09 09:12:21,919 EPOCH 1629
2024-02-09 09:12:32,893 Epoch 1629: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.88 
2024-02-09 09:12:32,894 EPOCH 1630
2024-02-09 09:12:33,986 [Epoch: 1630 Step: 00027700] Batch Recognition Loss:   0.001790 => Gls Tokens per Sec:     4110 || Batch Translation Loss:   0.479842 => Txt Tokens per Sec:     9579 || Lr: 0.000100
2024-02-09 09:12:43,815 Epoch 1630: Total Training Recognition Loss 0.05  Total Training Translation Loss 4.26 
2024-02-09 09:12:43,815 EPOCH 1631
2024-02-09 09:12:54,775 Epoch 1631: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.90 
2024-02-09 09:12:54,776 EPOCH 1632
2024-02-09 09:13:05,561 Epoch 1632: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.89 
2024-02-09 09:13:05,561 EPOCH 1633
2024-02-09 09:13:16,417 Epoch 1633: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.25 
2024-02-09 09:13:16,418 EPOCH 1634
2024-02-09 09:13:27,174 Epoch 1634: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.97 
2024-02-09 09:13:27,174 EPOCH 1635
2024-02-09 09:13:37,979 Epoch 1635: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.82 
2024-02-09 09:13:37,980 EPOCH 1636
2024-02-09 09:13:39,123 [Epoch: 1636 Step: 00027800] Batch Recognition Loss:   0.000329 => Gls Tokens per Sec:     2802 || Batch Translation Loss:   0.039005 => Txt Tokens per Sec:     7553 || Lr: 0.000100
2024-02-09 09:13:48,885 Epoch 1636: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.79 
2024-02-09 09:13:48,885 EPOCH 1637
2024-02-09 09:13:59,829 Epoch 1637: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-09 09:13:59,830 EPOCH 1638
2024-02-09 09:14:10,525 Epoch 1638: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.42 
2024-02-09 09:14:10,526 EPOCH 1639
2024-02-09 09:14:21,518 Epoch 1639: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-09 09:14:21,518 EPOCH 1640
2024-02-09 09:14:32,632 Epoch 1640: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-09 09:14:32,633 EPOCH 1641
2024-02-09 09:14:43,462 Epoch 1641: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-09 09:14:43,463 EPOCH 1642
2024-02-09 09:14:47,649 [Epoch: 1642 Step: 00027900] Batch Recognition Loss:   0.000550 => Gls Tokens per Sec:      459 || Batch Translation Loss:   0.117077 => Txt Tokens per Sec:     1476 || Lr: 0.000100
2024-02-09 09:14:54,601 Epoch 1642: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-09 09:14:54,602 EPOCH 1643
2024-02-09 09:15:05,370 Epoch 1643: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-09 09:15:05,370 EPOCH 1644
2024-02-09 09:15:16,371 Epoch 1644: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-09 09:15:16,371 EPOCH 1645
2024-02-09 09:15:27,378 Epoch 1645: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-09 09:15:27,379 EPOCH 1646
2024-02-09 09:15:38,295 Epoch 1646: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-09 09:15:38,295 EPOCH 1647
2024-02-09 09:15:49,393 Epoch 1647: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.34 
2024-02-09 09:15:49,393 EPOCH 1648
2024-02-09 09:15:49,564 [Epoch: 1648 Step: 00028000] Batch Recognition Loss:   0.000331 => Gls Tokens per Sec:     3765 || Batch Translation Loss:   0.013095 => Txt Tokens per Sec:     8441 || Lr: 0.000100
2024-02-09 09:16:29,338 Validation result at epoch 1648, step    28000: duration: 39.7742s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.72721	Translation Loss: 94826.33594	PPL: 12981.33398
	Eval Metric: BLEU
	WER 3.04	(DEL: 0.00,	INS: 0.00,	SUB: 3.04)
	BLEU-4 0.52	(BLEU-1: 10.41,	BLEU-2: 3.31,	BLEU-3: 1.19,	BLEU-4: 0.52)
	CHRF 16.55	ROUGE 8.96
2024-02-09 09:16:29,339 Logging Recognition and Translation Outputs
2024-02-09 09:16:29,339 ========================================================================================================================
2024-02-09 09:16:29,339 Logging Sequence: 156_288.00
2024-02-09 09:16:29,339 	Gloss Reference :	A B+C+D+E
2024-02-09 09:16:29,340 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 09:16:29,340 	Gloss Alignment :	         
2024-02-09 09:16:29,340 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 09:16:29,341 	Text Reference  :	**** *** pooran  led the team to   victory  miny became winners of the  1st season
2024-02-09 09:16:29,341 	Text Hypothesis :	this was because of  the **** most pandemic we   will   have    to wait and see   
2024-02-09 09:16:29,341 	Text Alignment  :	I    I   S       S       D    S    S        S    S      S       S  S    S   S     
2024-02-09 09:16:29,341 ========================================================================================================================
2024-02-09 09:16:29,342 Logging Sequence: 98_135.00
2024-02-09 09:16:29,342 	Gloss Reference :	A B+C+D+E
2024-02-09 09:16:29,342 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 09:16:29,342 	Gloss Alignment :	         
2024-02-09 09:16:29,342 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 09:16:29,343 	Text Reference  :	however due to the ***** rise in   coronavirus cases the tournament was    shifted
2024-02-09 09:16:29,343 	Text Hypothesis :	******* *** ** the girls were very happy       and   the ********** police filed  
2024-02-09 09:16:29,343 	Text Alignment  :	D       D   D      I     S    S    S           S         D          S      S      
2024-02-09 09:16:29,343 ========================================================================================================================
2024-02-09 09:16:29,343 Logging Sequence: 161_47.00
2024-02-09 09:16:29,344 	Gloss Reference :	A B+C+D+E
2024-02-09 09:16:29,344 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 09:16:29,344 	Gloss Alignment :	         
2024-02-09 09:16:29,344 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 09:16:29,345 	Text Reference  :	he requested confidentiality as he      was    planning to  make     an   official announcement
2024-02-09 09:16:29,345 	Text Hypothesis :	** ********* *************** ** anushka sharma and      his stepping down as       well        
2024-02-09 09:16:29,345 	Text Alignment  :	D  D         D               D  S       S      S        S   S        S    S        S           
2024-02-09 09:16:29,345 ========================================================================================================================
2024-02-09 09:16:29,345 Logging Sequence: 131_159.00
2024-02-09 09:16:29,345 	Gloss Reference :	A B+C+D+E
2024-02-09 09:16:29,346 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 09:16:29,346 	Gloss Alignment :	         
2024-02-09 09:16:29,346 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 09:16:29,347 	Text Reference  :	**** chanu also met       biren singh following the meeting singh    described chanu as ** ** **** our   nation' pride
2024-02-09 09:16:29,348 	Text Hypothesis :	fans are   now  wondering why   he    did       not say     anything about     dhoni as he is very close to      work 
2024-02-09 09:16:29,348 	Text Alignment  :	I    S     S    S         S     S     S         S   S       S        S         S        I  I  I    S     S       S    
2024-02-09 09:16:29,348 ========================================================================================================================
2024-02-09 09:16:29,348 Logging Sequence: 137_167.00
2024-02-09 09:16:29,348 	Gloss Reference :	A B+C+D+E
2024-02-09 09:16:29,348 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 09:16:29,348 	Gloss Alignment :	         
2024-02-09 09:16:29,349 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 09:16:29,350 	Text Reference  :	however after 630 pm     there will be     certain fan   zones where    beer will be    available and  nowhere else
2024-02-09 09:16:29,350 	Text Hypothesis :	******* ***** the indian team  was  played between india and   pakistan lost the  match on        11th october 2022
2024-02-09 09:16:29,351 	Text Alignment  :	D       D     S   S      S     S    S      S       S     S     S        S    S    S     S         S    S       S   
2024-02-09 09:16:29,351 ========================================================================================================================
2024-02-09 09:16:40,266 Epoch 1648: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 09:16:40,266 EPOCH 1649
2024-02-09 09:16:50,704 Epoch 1649: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 09:16:50,705 EPOCH 1650
2024-02-09 09:17:01,512 Epoch 1650: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 09:17:01,513 EPOCH 1651
2024-02-09 09:17:12,839 Epoch 1651: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-09 09:17:12,840 EPOCH 1652
2024-02-09 09:17:23,853 Epoch 1652: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-09 09:17:23,854 EPOCH 1653
2024-02-09 09:17:34,526 [Epoch: 1653 Step: 00028100] Batch Recognition Loss:   0.000346 => Gls Tokens per Sec:      935 || Batch Translation Loss:   0.008514 => Txt Tokens per Sec:     2620 || Lr: 0.000100
2024-02-09 09:17:34,679 Epoch 1653: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 09:17:34,679 EPOCH 1654
2024-02-09 09:17:45,498 Epoch 1654: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-09 09:17:45,498 EPOCH 1655
2024-02-09 09:17:56,340 Epoch 1655: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 09:17:56,340 EPOCH 1656
2024-02-09 09:18:07,219 Epoch 1656: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 09:18:07,219 EPOCH 1657
2024-02-09 09:18:18,036 Epoch 1657: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 09:18:18,036 EPOCH 1658
2024-02-09 09:18:28,982 Epoch 1658: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 09:18:28,982 EPOCH 1659
2024-02-09 09:18:39,110 [Epoch: 1659 Step: 00028200] Batch Recognition Loss:   0.000210 => Gls Tokens per Sec:      859 || Batch Translation Loss:   0.015362 => Txt Tokens per Sec:     2402 || Lr: 0.000100
2024-02-09 09:18:39,773 Epoch 1659: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 09:18:39,773 EPOCH 1660
2024-02-09 09:18:50,462 Epoch 1660: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 09:18:50,463 EPOCH 1661
2024-02-09 09:19:01,508 Epoch 1661: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 09:19:01,509 EPOCH 1662
2024-02-09 09:19:12,188 Epoch 1662: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-09 09:19:12,189 EPOCH 1663
2024-02-09 09:19:22,951 Epoch 1663: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-09 09:19:22,952 EPOCH 1664
2024-02-09 09:19:34,069 Epoch 1664: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 09:19:34,069 EPOCH 1665
2024-02-09 09:19:41,764 [Epoch: 1665 Step: 00028300] Batch Recognition Loss:   0.000212 => Gls Tokens per Sec:      998 || Batch Translation Loss:   0.013300 => Txt Tokens per Sec:     2847 || Lr: 0.000100
2024-02-09 09:19:45,105 Epoch 1665: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 09:19:45,105 EPOCH 1666
2024-02-09 09:19:56,165 Epoch 1666: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-09 09:19:56,165 EPOCH 1667
2024-02-09 09:20:07,434 Epoch 1667: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-09 09:20:07,434 EPOCH 1668
2024-02-09 09:20:18,169 Epoch 1668: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 09:20:18,170 EPOCH 1669
2024-02-09 09:20:28,925 Epoch 1669: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 09:20:28,926 EPOCH 1670
2024-02-09 09:20:39,992 Epoch 1670: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-09 09:20:39,993 EPOCH 1671
2024-02-09 09:20:45,881 [Epoch: 1671 Step: 00028400] Batch Recognition Loss:   0.000285 => Gls Tokens per Sec:     1043 || Batch Translation Loss:   0.038495 => Txt Tokens per Sec:     2954 || Lr: 0.000100
2024-02-09 09:20:50,630 Epoch 1671: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-09 09:20:50,630 EPOCH 1672
2024-02-09 09:21:01,595 Epoch 1672: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-09 09:21:01,596 EPOCH 1673
2024-02-09 09:21:12,508 Epoch 1673: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-09 09:21:12,508 EPOCH 1674
2024-02-09 09:21:23,448 Epoch 1674: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.92 
2024-02-09 09:21:23,449 EPOCH 1675
2024-02-09 09:21:34,558 Epoch 1675: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.21 
2024-02-09 09:21:34,559 EPOCH 1676
2024-02-09 09:21:45,318 Epoch 1676: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.17 
2024-02-09 09:21:45,319 EPOCH 1677
2024-02-09 09:21:50,246 [Epoch: 1677 Step: 00028500] Batch Recognition Loss:   0.000270 => Gls Tokens per Sec:     1039 || Batch Translation Loss:   0.215940 => Txt Tokens per Sec:     2948 || Lr: 0.000100
2024-02-09 09:21:56,364 Epoch 1677: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.32 
2024-02-09 09:21:56,364 EPOCH 1678
2024-02-09 09:22:07,449 Epoch 1678: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.15 
2024-02-09 09:22:07,449 EPOCH 1679
2024-02-09 09:22:18,548 Epoch 1679: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.48 
2024-02-09 09:22:18,548 EPOCH 1680
2024-02-09 09:22:29,501 Epoch 1680: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.89 
2024-02-09 09:22:29,502 EPOCH 1681
2024-02-09 09:22:40,248 Epoch 1681: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.10 
2024-02-09 09:22:40,248 EPOCH 1682
2024-02-09 09:22:51,064 Epoch 1682: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.88 
2024-02-09 09:22:51,064 EPOCH 1683
2024-02-09 09:22:52,154 [Epoch: 1683 Step: 00028600] Batch Recognition Loss:   0.000463 => Gls Tokens per Sec:     3526 || Batch Translation Loss:   0.168822 => Txt Tokens per Sec:     8507 || Lr: 0.000100
2024-02-09 09:23:01,729 Epoch 1683: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.96 
2024-02-09 09:23:01,730 EPOCH 1684
2024-02-09 09:23:12,667 Epoch 1684: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.98 
2024-02-09 09:23:12,667 EPOCH 1685
2024-02-09 09:23:23,367 Epoch 1685: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.13 
2024-02-09 09:23:23,368 EPOCH 1686
2024-02-09 09:23:34,237 Epoch 1686: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.39 
2024-02-09 09:23:34,237 EPOCH 1687
2024-02-09 09:23:45,136 Epoch 1687: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.58 
2024-02-09 09:23:45,137 EPOCH 1688
2024-02-09 09:23:56,028 Epoch 1688: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.84 
2024-02-09 09:23:56,028 EPOCH 1689
2024-02-09 09:23:56,726 [Epoch: 1689 Step: 00028700] Batch Recognition Loss:   0.000455 => Gls Tokens per Sec:     3673 || Batch Translation Loss:   0.041137 => Txt Tokens per Sec:     9791 || Lr: 0.000100
2024-02-09 09:24:06,553 Epoch 1689: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.65 
2024-02-09 09:24:06,554 EPOCH 1690
2024-02-09 09:24:17,431 Epoch 1690: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-09 09:24:17,432 EPOCH 1691
2024-02-09 09:24:28,219 Epoch 1691: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.41 
2024-02-09 09:24:28,220 EPOCH 1692
2024-02-09 09:24:39,229 Epoch 1692: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-09 09:24:39,230 EPOCH 1693
2024-02-09 09:24:50,028 Epoch 1693: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-09 09:24:50,029 EPOCH 1694
2024-02-09 09:25:00,770 Epoch 1694: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-09 09:25:00,771 EPOCH 1695
2024-02-09 09:25:02,506 [Epoch: 1695 Step: 00028800] Batch Recognition Loss:   0.000790 => Gls Tokens per Sec:      738 || Batch Translation Loss:   0.009864 => Txt Tokens per Sec:     1852 || Lr: 0.000100
2024-02-09 09:25:11,544 Epoch 1695: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-09 09:25:11,544 EPOCH 1696
2024-02-09 09:25:22,260 Epoch 1696: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 09:25:22,261 EPOCH 1697
2024-02-09 09:25:33,171 Epoch 1697: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 09:25:33,172 EPOCH 1698
2024-02-09 09:25:43,981 Epoch 1698: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-09 09:25:43,982 EPOCH 1699
2024-02-09 09:25:54,851 Epoch 1699: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 09:25:54,851 EPOCH 1700
2024-02-09 09:26:05,572 [Epoch: 1700 Step: 00028900] Batch Recognition Loss:   0.000294 => Gls Tokens per Sec:      991 || Batch Translation Loss:   0.016463 => Txt Tokens per Sec:     2741 || Lr: 0.000100
2024-02-09 09:26:05,573 Epoch 1700: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 09:26:05,573 EPOCH 1701
2024-02-09 09:26:16,252 Epoch 1701: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 09:26:16,253 EPOCH 1702
2024-02-09 09:26:27,179 Epoch 1702: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 09:26:27,179 EPOCH 1703
2024-02-09 09:26:38,148 Epoch 1703: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 09:26:38,149 EPOCH 1704
2024-02-09 09:26:49,018 Epoch 1704: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 09:26:49,018 EPOCH 1705
2024-02-09 09:26:59,863 Epoch 1705: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 09:26:59,864 EPOCH 1706
2024-02-09 09:27:08,816 [Epoch: 1706 Step: 00029000] Batch Recognition Loss:   0.000149 => Gls Tokens per Sec:     1044 || Batch Translation Loss:   0.012727 => Txt Tokens per Sec:     2813 || Lr: 0.000100
2024-02-09 09:27:10,795 Epoch 1706: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 09:27:10,795 EPOCH 1707
2024-02-09 09:27:21,178 Epoch 1707: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-09 09:27:21,179 EPOCH 1708
2024-02-09 09:27:32,271 Epoch 1708: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 09:27:32,272 EPOCH 1709
2024-02-09 09:27:43,383 Epoch 1709: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-09 09:27:43,383 EPOCH 1710
2024-02-09 09:27:54,427 Epoch 1710: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 09:27:54,428 EPOCH 1711
2024-02-09 09:28:05,719 Epoch 1711: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-09 09:28:05,720 EPOCH 1712
2024-02-09 09:28:13,712 [Epoch: 1712 Step: 00029100] Batch Recognition Loss:   0.000273 => Gls Tokens per Sec:     1009 || Batch Translation Loss:   0.017516 => Txt Tokens per Sec:     2759 || Lr: 0.000100
2024-02-09 09:28:16,535 Epoch 1712: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-09 09:28:16,535 EPOCH 1713
2024-02-09 09:28:27,416 Epoch 1713: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 09:28:27,416 EPOCH 1714
2024-02-09 09:28:38,485 Epoch 1714: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-09 09:28:38,486 EPOCH 1715
2024-02-09 09:28:49,479 Epoch 1715: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 09:28:49,480 EPOCH 1716
2024-02-09 09:29:00,533 Epoch 1716: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 09:29:00,534 EPOCH 1717
2024-02-09 09:29:11,393 Epoch 1717: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 09:29:11,393 EPOCH 1718
2024-02-09 09:29:18,002 [Epoch: 1718 Step: 00029200] Batch Recognition Loss:   0.000131 => Gls Tokens per Sec:     1026 || Batch Translation Loss:   0.010096 => Txt Tokens per Sec:     2955 || Lr: 0.000100
2024-02-09 09:29:22,364 Epoch 1718: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.30 
2024-02-09 09:29:22,365 EPOCH 1719
2024-02-09 09:29:33,139 Epoch 1719: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-09 09:29:33,140 EPOCH 1720
2024-02-09 09:29:43,972 Epoch 1720: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-09 09:29:43,973 EPOCH 1721
2024-02-09 09:29:54,965 Epoch 1721: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-09 09:29:54,966 EPOCH 1722
2024-02-09 09:30:05,939 Epoch 1722: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-09 09:30:05,939 EPOCH 1723
2024-02-09 09:30:16,779 Epoch 1723: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-09 09:30:16,780 EPOCH 1724
2024-02-09 09:30:23,719 [Epoch: 1724 Step: 00029300] Batch Recognition Loss:   0.000270 => Gls Tokens per Sec:      830 || Batch Translation Loss:   0.023962 => Txt Tokens per Sec:     2424 || Lr: 0.000100
2024-02-09 09:30:27,634 Epoch 1724: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-09 09:30:27,634 EPOCH 1725
2024-02-09 09:30:38,476 Epoch 1725: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-09 09:30:38,477 EPOCH 1726
2024-02-09 09:30:49,561 Epoch 1726: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-09 09:30:49,562 EPOCH 1727
2024-02-09 09:31:00,319 Epoch 1727: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-09 09:31:00,320 EPOCH 1728
2024-02-09 09:31:10,986 Epoch 1728: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-09 09:31:10,987 EPOCH 1729
2024-02-09 09:31:21,683 Epoch 1729: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-09 09:31:21,684 EPOCH 1730
2024-02-09 09:31:26,654 [Epoch: 1730 Step: 00029400] Batch Recognition Loss:   0.000201 => Gls Tokens per Sec:      902 || Batch Translation Loss:   0.016771 => Txt Tokens per Sec:     2671 || Lr: 0.000100
2024-02-09 09:31:32,493 Epoch 1730: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-09 09:31:32,493 EPOCH 1731
2024-02-09 09:31:43,230 Epoch 1731: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-09 09:31:43,230 EPOCH 1732
2024-02-09 09:31:54,040 Epoch 1732: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.96 
2024-02-09 09:31:54,040 EPOCH 1733
2024-02-09 09:32:04,954 Epoch 1733: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.13 
2024-02-09 09:32:04,954 EPOCH 1734
2024-02-09 09:32:15,739 Epoch 1734: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.40 
2024-02-09 09:32:15,739 EPOCH 1735
2024-02-09 09:32:26,536 Epoch 1735: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.56 
2024-02-09 09:32:26,536 EPOCH 1736
2024-02-09 09:32:29,185 [Epoch: 1736 Step: 00029500] Batch Recognition Loss:   0.000352 => Gls Tokens per Sec:     1209 || Batch Translation Loss:   0.051954 => Txt Tokens per Sec:     3453 || Lr: 0.000100
2024-02-09 09:32:37,252 Epoch 1736: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.52 
2024-02-09 09:32:37,252 EPOCH 1737
2024-02-09 09:32:48,318 Epoch 1737: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.55 
2024-02-09 09:32:48,319 EPOCH 1738
2024-02-09 09:32:58,985 Epoch 1738: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.43 
2024-02-09 09:32:58,986 EPOCH 1739
2024-02-09 09:33:09,831 Epoch 1739: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.49 
2024-02-09 09:33:09,832 EPOCH 1740
2024-02-09 09:33:20,694 Epoch 1740: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.89 
2024-02-09 09:33:20,694 EPOCH 1741
2024-02-09 09:33:31,553 Epoch 1741: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.82 
2024-02-09 09:33:31,553 EPOCH 1742
2024-02-09 09:33:32,388 [Epoch: 1742 Step: 00029600] Batch Recognition Loss:   0.000287 => Gls Tokens per Sec:     2302 || Batch Translation Loss:   0.032823 => Txt Tokens per Sec:     6583 || Lr: 0.000100
2024-02-09 09:33:42,427 Epoch 1742: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-09 09:33:42,428 EPOCH 1743
2024-02-09 09:33:53,490 Epoch 1743: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.83 
2024-02-09 09:33:53,491 EPOCH 1744
2024-02-09 09:34:04,483 Epoch 1744: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.82 
2024-02-09 09:34:04,483 EPOCH 1745
2024-02-09 09:34:15,661 Epoch 1745: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.84 
2024-02-09 09:34:15,661 EPOCH 1746
2024-02-09 09:34:26,822 Epoch 1746: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.80 
2024-02-09 09:34:26,823 EPOCH 1747
2024-02-09 09:34:37,764 Epoch 1747: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-09 09:34:37,764 EPOCH 1748
2024-02-09 09:34:39,648 [Epoch: 1748 Step: 00029700] Batch Recognition Loss:   0.003809 => Gls Tokens per Sec:      340 || Batch Translation Loss:   0.043196 => Txt Tokens per Sec:     1178 || Lr: 0.000100
2024-02-09 09:34:48,508 Epoch 1748: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.52 
2024-02-09 09:34:48,509 EPOCH 1749
2024-02-09 09:34:59,421 Epoch 1749: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-09 09:34:59,422 EPOCH 1750
2024-02-09 09:35:10,138 Epoch 1750: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.55 
2024-02-09 09:35:10,138 EPOCH 1751
2024-02-09 09:35:21,235 Epoch 1751: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.73 
2024-02-09 09:35:21,236 EPOCH 1752
2024-02-09 09:35:32,217 Epoch 1752: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-09 09:35:32,218 EPOCH 1753
2024-02-09 09:35:42,830 [Epoch: 1753 Step: 00029800] Batch Recognition Loss:   0.001035 => Gls Tokens per Sec:      941 || Batch Translation Loss:   0.021182 => Txt Tokens per Sec:     2590 || Lr: 0.000100
2024-02-09 09:35:43,079 Epoch 1753: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.56 
2024-02-09 09:35:43,079 EPOCH 1754
2024-02-09 09:35:53,957 Epoch 1754: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.99 
2024-02-09 09:35:53,958 EPOCH 1755
2024-02-09 09:36:04,630 Epoch 1755: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.83 
2024-02-09 09:36:04,630 EPOCH 1756
2024-02-09 09:36:15,811 Epoch 1756: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.34 
2024-02-09 09:36:15,812 EPOCH 1757
2024-02-09 09:36:26,955 Epoch 1757: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.37 
2024-02-09 09:36:26,955 EPOCH 1758
2024-02-09 09:36:37,572 Epoch 1758: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.97 
2024-02-09 09:36:37,572 EPOCH 1759
2024-02-09 09:36:47,933 [Epoch: 1759 Step: 00029900] Batch Recognition Loss:   0.000707 => Gls Tokens per Sec:      840 || Batch Translation Loss:   0.047613 => Txt Tokens per Sec:     2395 || Lr: 0.000100
2024-02-09 09:36:48,574 Epoch 1759: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.95 
2024-02-09 09:36:48,575 EPOCH 1760
2024-02-09 09:36:59,551 Epoch 1760: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.73 
2024-02-09 09:36:59,552 EPOCH 1761
2024-02-09 09:37:10,467 Epoch 1761: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-09 09:37:10,468 EPOCH 1762
2024-02-09 09:37:21,593 Epoch 1762: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-09 09:37:21,593 EPOCH 1763
2024-02-09 09:37:32,748 Epoch 1763: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-09 09:37:32,748 EPOCH 1764
2024-02-09 09:37:43,776 Epoch 1764: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-09 09:37:43,776 EPOCH 1765
2024-02-09 09:37:51,761 [Epoch: 1765 Step: 00030000] Batch Recognition Loss:   0.000618 => Gls Tokens per Sec:      929 || Batch Translation Loss:   0.026595 => Txt Tokens per Sec:     2525 || Lr: 0.000100
2024-02-09 09:38:31,883 Validation result at epoch 1765, step    30000: duration: 40.1224s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.68213	Translation Loss: 95288.54688	PPL: 13594.67090
	Eval Metric: BLEU
	WER 3.25	(DEL: 0.00,	INS: 0.00,	SUB: 3.25)
	BLEU-4 0.61	(BLEU-1: 9.49,	BLEU-2: 2.78,	BLEU-3: 1.19,	BLEU-4: 0.61)
	CHRF 16.73	ROUGE 7.92
2024-02-09 09:38:31,884 Logging Recognition and Translation Outputs
2024-02-09 09:38:31,884 ========================================================================================================================
2024-02-09 09:38:31,884 Logging Sequence: 146_102.00
2024-02-09 09:38:31,885 	Gloss Reference :	A B+C+D+E
2024-02-09 09:38:31,885 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 09:38:31,885 	Gloss Alignment :	         
2024-02-09 09:38:31,885 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 09:38:31,886 	Text Reference  :	famous indian champion players like   kidambi srikanth and ashwini ponappa have  tested positive for coronavirus
2024-02-09 09:38:31,886 	Text Hypothesis :	****** ****** ******** rohit   sharma also    known    as  the     support staff who    also     an  amazing    
2024-02-09 09:38:31,886 	Text Alignment  :	D      D      D        S       S      S       S        S   S       S       S     S      S        S   S          
2024-02-09 09:38:31,887 ========================================================================================================================
2024-02-09 09:38:31,887 Logging Sequence: 53_178.00
2024-02-09 09:38:31,887 	Gloss Reference :	A B+C+D+E
2024-02-09 09:38:31,887 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 09:38:31,887 	Gloss Alignment :	         
2024-02-09 09:38:31,887 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 09:38:31,888 	Text Reference  :	the money would help all those     affected by the humanitarian crisis in        afghanistan
2024-02-09 09:38:31,888 	Text Hypothesis :	*** ***** ***** i    am  extremely sadened  by *** this         fan    following etc        
2024-02-09 09:38:31,889 	Text Alignment  :	D   D     D     S    S   S         S           D   S            S      S         S          
2024-02-09 09:38:31,889 ========================================================================================================================
2024-02-09 09:38:31,889 Logging Sequence: 129_200.00
2024-02-09 09:38:31,889 	Gloss Reference :	A B+C+D+E      
2024-02-09 09:38:31,889 	Gloss Hypothesis:	A B+C+D+E+D+E+D
2024-02-09 09:38:31,889 	Gloss Alignment :	  S            
2024-02-09 09:38:31,890 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 09:38:31,891 	Text Reference  :	the ioc would lose about 4    billion if the olympics were to   be     cancelled
2024-02-09 09:38:31,891 	Text Hypothesis :	*** *** he    won  a     gold medal   at the ******** **** 2012 london olympics 
2024-02-09 09:38:31,891 	Text Alignment  :	D   D   S     S    S     S    S       S      D        D    S    S      S        
2024-02-09 09:38:31,891 ========================================================================================================================
2024-02-09 09:38:31,891 Logging Sequence: 77_2.00
2024-02-09 09:38:31,891 	Gloss Reference :	A B+C+D+E
2024-02-09 09:38:31,891 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 09:38:31,892 	Gloss Alignment :	         
2024-02-09 09:38:31,892 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 09:38:31,893 	Text Reference  :	on ** **** 25th april the *** ipl   match between sunrisers hyderabad and  delhi   capitals ended in a tie
2024-02-09 09:38:31,894 	Text Hypothesis :	on 16 july 2021 in    the t20 world cup   sri     lanka     had       also against each     other in * uae
2024-02-09 09:38:31,894 	Text Alignment  :	   I  I    S    S         I   S     S     S       S         S         S    S       S        S        D S  
2024-02-09 09:38:31,894 ========================================================================================================================
2024-02-09 09:38:31,894 Logging Sequence: 119_170.00
2024-02-09 09:38:31,894 	Gloss Reference :	A B+C+D+E
2024-02-09 09:38:31,894 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 09:38:31,895 	Gloss Alignment :	         
2024-02-09 09:38:31,895 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 09:38:31,895 	Text Reference  :	they said it was a     proud    moment messi is        a  big hearted man  
2024-02-09 09:38:31,896 	Text Hypothesis :	**** **** ** *** messi intended to     gift  something to all the     match
2024-02-09 09:38:31,896 	Text Alignment  :	D    D    D  D   S     S        S      S     S         S  S   S       S    
2024-02-09 09:38:31,896 ========================================================================================================================
2024-02-09 09:38:34,952 Epoch 1765: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-09 09:38:34,952 EPOCH 1766
2024-02-09 09:38:45,583 Epoch 1766: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-09 09:38:45,583 EPOCH 1767
2024-02-09 09:38:56,625 Epoch 1767: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-09 09:38:56,625 EPOCH 1768
2024-02-09 09:39:07,546 Epoch 1768: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-09 09:39:07,547 EPOCH 1769
2024-02-09 09:39:18,651 Epoch 1769: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.43 
2024-02-09 09:39:18,652 EPOCH 1770
2024-02-09 09:39:29,593 Epoch 1770: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-09 09:39:29,593 EPOCH 1771
2024-02-09 09:39:34,717 [Epoch: 1771 Step: 00030100] Batch Recognition Loss:   0.000455 => Gls Tokens per Sec:     1250 || Batch Translation Loss:   0.026368 => Txt Tokens per Sec:     3397 || Lr: 0.000100
2024-02-09 09:39:40,563 Epoch 1771: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-09 09:39:40,564 EPOCH 1772
2024-02-09 09:39:51,518 Epoch 1772: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-09 09:39:51,519 EPOCH 1773
2024-02-09 09:40:02,527 Epoch 1773: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-09 09:40:02,527 EPOCH 1774
2024-02-09 09:40:13,581 Epoch 1774: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.87 
2024-02-09 09:40:13,582 EPOCH 1775
2024-02-09 09:40:24,497 Epoch 1775: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-09 09:40:24,498 EPOCH 1776
2024-02-09 09:40:35,609 Epoch 1776: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-09 09:40:35,610 EPOCH 1777
2024-02-09 09:40:39,134 [Epoch: 1777 Step: 00030200] Batch Recognition Loss:   0.000241 => Gls Tokens per Sec:     1453 || Batch Translation Loss:   0.084905 => Txt Tokens per Sec:     4241 || Lr: 0.000100
2024-02-09 09:40:46,445 Epoch 1777: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.75 
2024-02-09 09:40:46,446 EPOCH 1778
2024-02-09 09:40:57,103 Epoch 1778: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-09 09:40:57,104 EPOCH 1779
2024-02-09 09:41:08,205 Epoch 1779: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-09 09:41:08,206 EPOCH 1780
2024-02-09 09:41:19,197 Epoch 1780: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.86 
2024-02-09 09:41:19,198 EPOCH 1781
2024-02-09 09:41:30,179 Epoch 1781: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-09 09:41:30,180 EPOCH 1782
2024-02-09 09:41:41,440 Epoch 1782: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.73 
2024-02-09 09:41:41,441 EPOCH 1783
2024-02-09 09:41:42,689 [Epoch: 1783 Step: 00030300] Batch Recognition Loss:   0.000377 => Gls Tokens per Sec:     3079 || Batch Translation Loss:   0.010520 => Txt Tokens per Sec:     7749 || Lr: 0.000100
2024-02-09 09:41:52,374 Epoch 1783: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-09 09:41:52,375 EPOCH 1784
2024-02-09 09:42:03,344 Epoch 1784: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-09 09:42:03,344 EPOCH 1785
2024-02-09 09:42:14,153 Epoch 1785: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-09 09:42:14,153 EPOCH 1786
2024-02-09 09:42:25,033 Epoch 1786: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-09 09:42:25,034 EPOCH 1787
2024-02-09 09:42:35,720 Epoch 1787: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-09 09:42:35,721 EPOCH 1788
2024-02-09 09:42:46,453 Epoch 1788: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-09 09:42:46,454 EPOCH 1789
2024-02-09 09:42:48,510 [Epoch: 1789 Step: 00030400] Batch Recognition Loss:   0.000360 => Gls Tokens per Sec:     1246 || Batch Translation Loss:   0.027168 => Txt Tokens per Sec:     3065 || Lr: 0.000100
2024-02-09 09:42:57,416 Epoch 1789: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-09 09:42:57,416 EPOCH 1790
2024-02-09 09:43:08,088 Epoch 1790: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-09 09:43:08,088 EPOCH 1791
2024-02-09 09:43:18,902 Epoch 1791: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.87 
2024-02-09 09:43:18,903 EPOCH 1792
2024-02-09 09:43:29,573 Epoch 1792: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.54 
2024-02-09 09:43:29,574 EPOCH 1793
2024-02-09 09:43:40,438 Epoch 1793: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.26 
2024-02-09 09:43:40,438 EPOCH 1794
2024-02-09 09:43:51,273 Epoch 1794: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.59 
2024-02-09 09:43:51,274 EPOCH 1795
2024-02-09 09:43:53,240 [Epoch: 1795 Step: 00030500] Batch Recognition Loss:   0.000404 => Gls Tokens per Sec:      651 || Batch Translation Loss:   0.043712 => Txt Tokens per Sec:     1883 || Lr: 0.000100
2024-02-09 09:44:02,207 Epoch 1795: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.60 
2024-02-09 09:44:02,207 EPOCH 1796
2024-02-09 09:44:13,233 Epoch 1796: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.92 
2024-02-09 09:44:13,234 EPOCH 1797
2024-02-09 09:44:24,074 Epoch 1797: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.01 
2024-02-09 09:44:24,075 EPOCH 1798
2024-02-09 09:44:34,884 Epoch 1798: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.82 
2024-02-09 09:44:34,885 EPOCH 1799
2024-02-09 09:44:45,959 Epoch 1799: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-09 09:44:45,959 EPOCH 1800
2024-02-09 09:44:56,806 [Epoch: 1800 Step: 00030600] Batch Recognition Loss:   0.000605 => Gls Tokens per Sec:      979 || Batch Translation Loss:   0.046802 => Txt Tokens per Sec:     2709 || Lr: 0.000100
2024-02-09 09:44:56,807 Epoch 1800: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.73 
2024-02-09 09:44:56,807 EPOCH 1801
2024-02-09 09:45:07,606 Epoch 1801: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.34 
2024-02-09 09:45:07,607 EPOCH 1802
2024-02-09 09:45:18,813 Epoch 1802: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.14 
2024-02-09 09:45:18,814 EPOCH 1803
2024-02-09 09:45:29,886 Epoch 1803: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.64 
2024-02-09 09:45:29,887 EPOCH 1804
2024-02-09 09:45:41,008 Epoch 1804: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.82 
2024-02-09 09:45:41,008 EPOCH 1805
2024-02-09 09:45:51,977 Epoch 1805: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.53 
2024-02-09 09:45:51,978 EPOCH 1806
2024-02-09 09:46:02,315 [Epoch: 1806 Step: 00030700] Batch Recognition Loss:   0.000628 => Gls Tokens per Sec:      904 || Batch Translation Loss:   0.019416 => Txt Tokens per Sec:     2476 || Lr: 0.000100
2024-02-09 09:46:03,001 Epoch 1806: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-09 09:46:03,002 EPOCH 1807
2024-02-09 09:46:13,999 Epoch 1807: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-09 09:46:14,000 EPOCH 1808
2024-02-09 09:46:24,983 Epoch 1808: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-09 09:46:24,984 EPOCH 1809
2024-02-09 09:46:36,232 Epoch 1809: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-09 09:46:36,232 EPOCH 1810
2024-02-09 09:46:47,089 Epoch 1810: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.26 
2024-02-09 09:46:47,089 EPOCH 1811
2024-02-09 09:46:57,802 Epoch 1811: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 09:46:57,803 EPOCH 1812
2024-02-09 09:47:07,835 [Epoch: 1812 Step: 00030800] Batch Recognition Loss:   0.000497 => Gls Tokens per Sec:      804 || Batch Translation Loss:   0.015813 => Txt Tokens per Sec:     2258 || Lr: 0.000100
2024-02-09 09:47:08,759 Epoch 1812: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 09:47:08,759 EPOCH 1813
2024-02-09 09:47:19,638 Epoch 1813: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 09:47:19,639 EPOCH 1814
2024-02-09 09:47:30,340 Epoch 1814: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-09 09:47:30,341 EPOCH 1815
2024-02-09 09:47:41,153 Epoch 1815: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-09 09:47:41,153 EPOCH 1816
2024-02-09 09:47:52,114 Epoch 1816: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 09:47:52,115 EPOCH 1817
2024-02-09 09:48:03,076 Epoch 1817: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 09:48:03,077 EPOCH 1818
2024-02-09 09:48:08,818 [Epoch: 1818 Step: 00030900] Batch Recognition Loss:   0.000193 => Gls Tokens per Sec:     1227 || Batch Translation Loss:   0.011244 => Txt Tokens per Sec:     3262 || Lr: 0.000100
2024-02-09 09:48:14,006 Epoch 1818: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-09 09:48:14,006 EPOCH 1819
2024-02-09 09:48:25,044 Epoch 1819: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-09 09:48:25,044 EPOCH 1820
2024-02-09 09:48:36,227 Epoch 1820: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-09 09:48:36,228 EPOCH 1821
2024-02-09 09:48:47,215 Epoch 1821: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-09 09:48:47,216 EPOCH 1822
2024-02-09 09:48:57,983 Epoch 1822: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-09 09:48:57,983 EPOCH 1823
2024-02-09 09:49:08,807 Epoch 1823: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-09 09:49:08,808 EPOCH 1824
2024-02-09 09:49:15,583 [Epoch: 1824 Step: 00031000] Batch Recognition Loss:   0.000209 => Gls Tokens per Sec:      850 || Batch Translation Loss:   0.044862 => Txt Tokens per Sec:     2526 || Lr: 0.000100
2024-02-09 09:49:19,606 Epoch 1824: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-09 09:49:19,607 EPOCH 1825
2024-02-09 09:49:30,486 Epoch 1825: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-09 09:49:30,487 EPOCH 1826
2024-02-09 09:49:41,490 Epoch 1826: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-09 09:49:41,491 EPOCH 1827
2024-02-09 09:49:52,418 Epoch 1827: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-09 09:49:52,419 EPOCH 1828
2024-02-09 09:50:03,496 Epoch 1828: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-09 09:50:03,497 EPOCH 1829
2024-02-09 09:50:14,403 Epoch 1829: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-09 09:50:14,404 EPOCH 1830
2024-02-09 09:50:20,988 [Epoch: 1830 Step: 00031100] Batch Recognition Loss:   0.001664 => Gls Tokens per Sec:      681 || Batch Translation Loss:   0.044194 => Txt Tokens per Sec:     2124 || Lr: 0.000100
2024-02-09 09:50:24,986 Epoch 1830: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.85 
2024-02-09 09:50:24,987 EPOCH 1831
2024-02-09 09:50:36,056 Epoch 1831: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.77 
2024-02-09 09:50:36,057 EPOCH 1832
2024-02-09 09:50:47,024 Epoch 1832: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.84 
2024-02-09 09:50:47,024 EPOCH 1833
2024-02-09 09:50:57,936 Epoch 1833: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.84 
2024-02-09 09:50:57,937 EPOCH 1834
2024-02-09 09:51:08,818 Epoch 1834: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.80 
2024-02-09 09:51:08,819 EPOCH 1835
2024-02-09 09:51:19,880 Epoch 1835: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.79 
2024-02-09 09:51:19,881 EPOCH 1836
2024-02-09 09:51:21,231 [Epoch: 1836 Step: 00031200] Batch Recognition Loss:   0.000595 => Gls Tokens per Sec:     2372 || Batch Translation Loss:   0.010065 => Txt Tokens per Sec:     6301 || Lr: 0.000100
2024-02-09 09:51:30,866 Epoch 1836: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.78 
2024-02-09 09:51:30,867 EPOCH 1837
2024-02-09 09:51:41,989 Epoch 1837: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-09 09:51:41,990 EPOCH 1838
2024-02-09 09:51:52,849 Epoch 1838: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-09 09:51:52,850 EPOCH 1839
2024-02-09 09:52:03,703 Epoch 1839: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.87 
2024-02-09 09:52:03,704 EPOCH 1840
2024-02-09 09:52:14,463 Epoch 1840: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.14 
2024-02-09 09:52:14,464 EPOCH 1841
2024-02-09 09:52:25,466 Epoch 1841: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-09 09:52:25,467 EPOCH 1842
2024-02-09 09:52:26,161 [Epoch: 1842 Step: 00031300] Batch Recognition Loss:   0.000260 => Gls Tokens per Sec:     2775 || Batch Translation Loss:   0.024658 => Txt Tokens per Sec:     7545 || Lr: 0.000100
2024-02-09 09:52:36,295 Epoch 1842: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-09 09:52:36,296 EPOCH 1843
2024-02-09 09:52:47,045 Epoch 1843: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-09 09:52:47,046 EPOCH 1844
2024-02-09 09:52:57,756 Epoch 1844: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-09 09:52:57,756 EPOCH 1845
2024-02-09 09:53:08,517 Epoch 1845: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-09 09:53:08,518 EPOCH 1846
2024-02-09 09:53:19,476 Epoch 1846: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-09 09:53:19,476 EPOCH 1847
2024-02-09 09:53:30,563 Epoch 1847: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-09 09:53:30,564 EPOCH 1848
2024-02-09 09:53:32,329 [Epoch: 1848 Step: 00031400] Batch Recognition Loss:   0.000349 => Gls Tokens per Sec:      363 || Batch Translation Loss:   0.034488 => Txt Tokens per Sec:     1167 || Lr: 0.000100
2024-02-09 09:53:41,636 Epoch 1848: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-09 09:53:41,636 EPOCH 1849
2024-02-09 09:53:52,477 Epoch 1849: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 09:53:52,478 EPOCH 1850
2024-02-09 09:54:03,294 Epoch 1850: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-09 09:54:03,295 EPOCH 1851
2024-02-09 09:54:14,326 Epoch 1851: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-09 09:54:14,327 EPOCH 1852
2024-02-09 09:54:25,512 Epoch 1852: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-09 09:54:25,513 EPOCH 1853
2024-02-09 09:54:34,450 [Epoch: 1853 Step: 00031500] Batch Recognition Loss:   0.000216 => Gls Tokens per Sec:     1117 || Batch Translation Loss:   0.016454 => Txt Tokens per Sec:     3040 || Lr: 0.000100
2024-02-09 09:54:36,454 Epoch 1853: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-09 09:54:36,454 EPOCH 1854
2024-02-09 09:54:47,369 Epoch 1854: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-09 09:54:47,370 EPOCH 1855
2024-02-09 09:54:58,270 Epoch 1855: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-09 09:54:58,270 EPOCH 1856
2024-02-09 09:55:09,015 Epoch 1856: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-09 09:55:09,016 EPOCH 1857
2024-02-09 09:55:19,826 Epoch 1857: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-09 09:55:19,827 EPOCH 1858
2024-02-09 09:55:30,594 Epoch 1858: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-09 09:55:30,594 EPOCH 1859
2024-02-09 09:55:39,238 [Epoch: 1859 Step: 00031600] Batch Recognition Loss:   0.000352 => Gls Tokens per Sec:     1007 || Batch Translation Loss:   0.019874 => Txt Tokens per Sec:     2757 || Lr: 0.000100
2024-02-09 09:55:41,483 Epoch 1859: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-09 09:55:41,483 EPOCH 1860
2024-02-09 09:55:52,477 Epoch 1860: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-09 09:55:52,478 EPOCH 1861
2024-02-09 09:56:03,423 Epoch 1861: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-09 09:56:03,423 EPOCH 1862
2024-02-09 09:56:14,411 Epoch 1862: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-09 09:56:14,412 EPOCH 1863
2024-02-09 09:56:25,276 Epoch 1863: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-09 09:56:25,277 EPOCH 1864
2024-02-09 09:56:35,522 Epoch 1864: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.51 
2024-02-09 09:56:35,522 EPOCH 1865
2024-02-09 09:56:43,667 [Epoch: 1865 Step: 00031700] Batch Recognition Loss:   0.000820 => Gls Tokens per Sec:      911 || Batch Translation Loss:   0.012665 => Txt Tokens per Sec:     2592 || Lr: 0.000100
2024-02-09 09:56:46,508 Epoch 1865: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.38 
2024-02-09 09:56:46,509 EPOCH 1866
2024-02-09 09:56:57,495 Epoch 1866: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-09 09:56:57,495 EPOCH 1867
2024-02-09 09:57:08,508 Epoch 1867: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-09 09:57:08,508 EPOCH 1868
2024-02-09 09:57:19,513 Epoch 1868: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-09 09:57:19,514 EPOCH 1869
2024-02-09 09:57:30,427 Epoch 1869: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.42 
2024-02-09 09:57:30,427 EPOCH 1870
2024-02-09 09:57:41,467 Epoch 1870: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.34 
2024-02-09 09:57:41,468 EPOCH 1871
2024-02-09 09:57:51,099 [Epoch: 1871 Step: 00031800] Batch Recognition Loss:   0.000263 => Gls Tokens per Sec:      638 || Batch Translation Loss:   0.023406 => Txt Tokens per Sec:     1971 || Lr: 0.000100
2024-02-09 09:57:52,384 Epoch 1871: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-09 09:57:52,384 EPOCH 1872
2024-02-09 09:58:03,174 Epoch 1872: Total Training Recognition Loss 0.20  Total Training Translation Loss 0.31 
2024-02-09 09:58:03,175 EPOCH 1873
2024-02-09 09:58:13,945 Epoch 1873: Total Training Recognition Loss 3.30  Total Training Translation Loss 0.52 
2024-02-09 09:58:13,945 EPOCH 1874
2024-02-09 09:58:24,909 Epoch 1874: Total Training Recognition Loss 1.72  Total Training Translation Loss 0.67 
2024-02-09 09:58:24,910 EPOCH 1875
2024-02-09 09:58:35,866 Epoch 1875: Total Training Recognition Loss 2.88  Total Training Translation Loss 1.28 
2024-02-09 09:58:35,867 EPOCH 1876
2024-02-09 09:58:46,827 Epoch 1876: Total Training Recognition Loss 1.69  Total Training Translation Loss 2.22 
2024-02-09 09:58:46,827 EPOCH 1877
2024-02-09 09:58:53,381 [Epoch: 1877 Step: 00031900] Batch Recognition Loss:   0.220675 => Gls Tokens per Sec:      742 || Batch Translation Loss:   0.199699 => Txt Tokens per Sec:     1953 || Lr: 0.000100
2024-02-09 09:58:57,520 Epoch 1877: Total Training Recognition Loss 1.56  Total Training Translation Loss 2.15 
2024-02-09 09:58:57,520 EPOCH 1878
2024-02-09 09:59:08,095 Epoch 1878: Total Training Recognition Loss 0.44  Total Training Translation Loss 2.21 
2024-02-09 09:59:08,096 EPOCH 1879
2024-02-09 09:59:18,832 Epoch 1879: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.69 
2024-02-09 09:59:18,833 EPOCH 1880
2024-02-09 09:59:29,656 Epoch 1880: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.39 
2024-02-09 09:59:29,656 EPOCH 1881
2024-02-09 09:59:40,362 Epoch 1881: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.29 
2024-02-09 09:59:40,362 EPOCH 1882
2024-02-09 09:59:51,417 Epoch 1882: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.09 
2024-02-09 09:59:51,418 EPOCH 1883
2024-02-09 09:59:54,003 [Epoch: 1883 Step: 00032000] Batch Recognition Loss:   0.000504 => Gls Tokens per Sec:     1486 || Batch Translation Loss:   0.046166 => Txt Tokens per Sec:     3873 || Lr: 0.000100
2024-02-09 10:00:36,421 Validation result at epoch 1883, step    32000: duration: 42.4177s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.49410	Translation Loss: 94886.08594	PPL: 13059.03906
	Eval Metric: BLEU
	WER 3.04	(DEL: 0.00,	INS: 0.00,	SUB: 3.04)
	BLEU-4 0.53	(BLEU-1: 10.48,	BLEU-2: 2.99,	BLEU-3: 1.11,	BLEU-4: 0.53)
	CHRF 17.02	ROUGE 8.67
2024-02-09 10:00:36,422 Logging Recognition and Translation Outputs
2024-02-09 10:00:36,422 ========================================================================================================================
2024-02-09 10:00:36,422 Logging Sequence: 162_133.00
2024-02-09 10:00:36,423 	Gloss Reference :	A B+C+D+E
2024-02-09 10:00:36,423 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 10:00:36,423 	Gloss Alignment :	         
2024-02-09 10:00:36,423 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 10:00:36,424 	Text Reference  :	***** they    also sent rape threats to   his   9-month old  daughter  
2024-02-09 10:00:36,424 	Text Hypothesis :	these attacks said they got  down    well these are     very protective
2024-02-09 10:00:36,424 	Text Alignment  :	I     S       S    S    S    S       S    S     S       S    S         
2024-02-09 10:00:36,424 ========================================================================================================================
2024-02-09 10:00:36,424 Logging Sequence: 134_236.00
2024-02-09 10:00:36,425 	Gloss Reference :	A B+C+D+E
2024-02-09 10:00:36,425 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 10:00:36,425 	Gloss Alignment :	         
2024-02-09 10:00:36,425 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 10:00:36,426 	Text Reference  :	** **** ** ** * after  the interaction modi  tweeted the **** *** images and captioned it   saying
2024-02-09 10:00:36,427 	Text Hypothesis :	pm said it is a matter of  great       pride in      the even his father and uncle     also come  
2024-02-09 10:00:36,427 	Text Alignment  :	I  I    I  I  I S      S   S           S     S           I    I   S          S         S    S     
2024-02-09 10:00:36,427 ========================================================================================================================
2024-02-09 10:00:36,427 Logging Sequence: 145_52.00
2024-02-09 10:00:36,427 	Gloss Reference :	A B+C+D+E
2024-02-09 10:00:36,427 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 10:00:36,427 	Gloss Alignment :	         
2024-02-09 10:00:36,428 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 10:00:36,429 	Text Reference  :	her name was dropped   despite having qualified as    she            was the only female athlete
2024-02-09 10:00:36,429 	Text Hypothesis :	*** **** the wrestlers wanted  to     have      their representative in  the **** ****** panel  
2024-02-09 10:00:36,429 	Text Alignment  :	D   D    S   S         S       S      S         S     S              S       D    D      S      
2024-02-09 10:00:36,429 ========================================================================================================================
2024-02-09 10:00:36,429 Logging Sequence: 175_40.00
2024-02-09 10:00:36,429 	Gloss Reference :	A B+C+D+E
2024-02-09 10:00:36,429 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 10:00:36,430 	Gloss Alignment :	         
2024-02-09 10:00:36,430 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 10:00:36,431 	Text Reference  :	* *** ** ***** ******* **** ******* soumyadeep and shreya bagged three medals each including a            silver medal each
2024-02-09 10:00:36,431 	Text Hypothesis :	a few of these matches were playing well       and ****** losing the   world  deaf badminton championship is     being held
2024-02-09 10:00:36,431 	Text Alignment  :	I I   I  I     I       I    I       S              D      S      S     S      S    S         S            S      S     S   
2024-02-09 10:00:36,432 ========================================================================================================================
2024-02-09 10:00:36,432 Logging Sequence: 156_51.00
2024-02-09 10:00:36,432 	Gloss Reference :	A B+C+D+E
2024-02-09 10:00:36,432 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 10:00:36,432 	Gloss Alignment :	         
2024-02-09 10:00:36,432 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 10:00:36,433 	Text Reference  :	the selection of the players was  similar  to that        of  ipl    
2024-02-09 10:00:36,433 	Text Hypothesis :	*** ********* ** he  has     been selected as commentator and analyst
2024-02-09 10:00:36,433 	Text Alignment  :	D   D         D  S   S       S    S        S  S           S   S      
2024-02-09 10:00:36,433 ========================================================================================================================
2024-02-09 10:00:45,037 Epoch 1883: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.72 
2024-02-09 10:00:45,038 EPOCH 1884
2024-02-09 10:00:56,045 Epoch 1884: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.61 
2024-02-09 10:00:56,045 EPOCH 1885
2024-02-09 10:01:06,984 Epoch 1885: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.70 
2024-02-09 10:01:06,985 EPOCH 1886
2024-02-09 10:01:17,710 Epoch 1886: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.60 
2024-02-09 10:01:17,711 EPOCH 1887
2024-02-09 10:01:28,552 Epoch 1887: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.43 
2024-02-09 10:01:28,553 EPOCH 1888
2024-02-09 10:01:39,398 Epoch 1888: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.57 
2024-02-09 10:01:39,399 EPOCH 1889
2024-02-09 10:01:42,599 [Epoch: 1889 Step: 00032100] Batch Recognition Loss:   0.001760 => Gls Tokens per Sec:      719 || Batch Translation Loss:   0.014672 => Txt Tokens per Sec:     2092 || Lr: 0.000100
2024-02-09 10:01:50,454 Epoch 1889: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.42 
2024-02-09 10:01:50,455 EPOCH 1890
2024-02-09 10:02:01,400 Epoch 1890: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.36 
2024-02-09 10:02:01,401 EPOCH 1891
2024-02-09 10:02:12,270 Epoch 1891: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.53 
2024-02-09 10:02:12,270 EPOCH 1892
2024-02-09 10:02:23,211 Epoch 1892: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.45 
2024-02-09 10:02:23,212 EPOCH 1893
2024-02-09 10:02:34,189 Epoch 1893: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.45 
2024-02-09 10:02:34,189 EPOCH 1894
2024-02-09 10:02:44,998 Epoch 1894: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-09 10:02:44,999 EPOCH 1895
2024-02-09 10:02:45,642 [Epoch: 1895 Step: 00032200] Batch Recognition Loss:   0.002327 => Gls Tokens per Sec:     1991 || Batch Translation Loss:   0.025703 => Txt Tokens per Sec:     5916 || Lr: 0.000100
2024-02-09 10:02:56,048 Epoch 1895: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-09 10:02:56,049 EPOCH 1896
2024-02-09 10:03:06,871 Epoch 1896: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-09 10:03:06,872 EPOCH 1897
2024-02-09 10:03:17,385 Epoch 1897: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-09 10:03:17,385 EPOCH 1898
2024-02-09 10:03:28,406 Epoch 1898: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-09 10:03:28,407 EPOCH 1899
2024-02-09 10:03:39,206 Epoch 1899: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 10:03:39,207 EPOCH 1900
2024-02-09 10:03:49,934 [Epoch: 1900 Step: 00032300] Batch Recognition Loss:   0.000480 => Gls Tokens per Sec:      990 || Batch Translation Loss:   0.017553 => Txt Tokens per Sec:     2740 || Lr: 0.000100
2024-02-09 10:03:49,934 Epoch 1900: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 10:03:49,935 EPOCH 1901
2024-02-09 10:04:00,775 Epoch 1901: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 10:04:00,775 EPOCH 1902
2024-02-09 10:04:11,264 Epoch 1902: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 10:04:11,265 EPOCH 1903
2024-02-09 10:04:22,210 Epoch 1903: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-09 10:04:22,211 EPOCH 1904
2024-02-09 10:04:33,202 Epoch 1904: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 10:04:33,202 EPOCH 1905
2024-02-09 10:04:43,823 Epoch 1905: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 10:04:43,823 EPOCH 1906
2024-02-09 10:04:50,437 [Epoch: 1906 Step: 00032400] Batch Recognition Loss:   0.000536 => Gls Tokens per Sec:     1452 || Batch Translation Loss:   0.007361 => Txt Tokens per Sec:     3908 || Lr: 0.000100
2024-02-09 10:04:54,612 Epoch 1906: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-09 10:04:54,613 EPOCH 1907
2024-02-09 10:05:05,385 Epoch 1907: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-09 10:05:05,386 EPOCH 1908
2024-02-09 10:05:16,373 Epoch 1908: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-09 10:05:16,374 EPOCH 1909
2024-02-09 10:05:27,261 Epoch 1909: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 10:05:27,262 EPOCH 1910
2024-02-09 10:05:38,115 Epoch 1910: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-09 10:05:38,115 EPOCH 1911
2024-02-09 10:05:48,831 Epoch 1911: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 10:05:48,832 EPOCH 1912
2024-02-09 10:05:57,531 [Epoch: 1912 Step: 00032500] Batch Recognition Loss:   0.000127 => Gls Tokens per Sec:      927 || Batch Translation Loss:   0.012956 => Txt Tokens per Sec:     2557 || Lr: 0.000100
2024-02-09 10:05:59,789 Epoch 1912: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 10:05:59,790 EPOCH 1913
2024-02-09 10:06:10,559 Epoch 1913: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 10:06:10,560 EPOCH 1914
2024-02-09 10:06:21,313 Epoch 1914: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-09 10:06:21,314 EPOCH 1915
2024-02-09 10:06:31,870 Epoch 1915: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-09 10:06:31,871 EPOCH 1916
2024-02-09 10:06:42,584 Epoch 1916: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-09 10:06:42,585 EPOCH 1917
2024-02-09 10:06:53,362 Epoch 1917: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-09 10:06:53,362 EPOCH 1918
2024-02-09 10:07:01,066 [Epoch: 1918 Step: 00032600] Batch Recognition Loss:   0.000192 => Gls Tokens per Sec:      914 || Batch Translation Loss:   0.012880 => Txt Tokens per Sec:     2673 || Lr: 0.000100
2024-02-09 10:07:04,345 Epoch 1918: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-09 10:07:04,345 EPOCH 1919
2024-02-09 10:07:15,287 Epoch 1919: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-09 10:07:15,287 EPOCH 1920
2024-02-09 10:07:26,147 Epoch 1920: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-09 10:07:26,147 EPOCH 1921
2024-02-09 10:07:36,945 Epoch 1921: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-09 10:07:36,946 EPOCH 1922
2024-02-09 10:07:48,174 Epoch 1922: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-09 10:07:48,175 EPOCH 1923
2024-02-09 10:07:59,224 Epoch 1923: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-09 10:07:59,225 EPOCH 1924
2024-02-09 10:08:05,258 [Epoch: 1924 Step: 00032700] Batch Recognition Loss:   0.000251 => Gls Tokens per Sec:      912 || Batch Translation Loss:   0.026186 => Txt Tokens per Sec:     2646 || Lr: 0.000100
2024-02-09 10:08:10,160 Epoch 1924: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-09 10:08:10,161 EPOCH 1925
2024-02-09 10:08:21,101 Epoch 1925: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-09 10:08:21,102 EPOCH 1926
2024-02-09 10:08:32,016 Epoch 1926: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-09 10:08:32,017 EPOCH 1927
2024-02-09 10:08:42,970 Epoch 1927: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.70 
2024-02-09 10:08:42,970 EPOCH 1928
2024-02-09 10:08:53,900 Epoch 1928: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.80 
2024-02-09 10:08:53,900 EPOCH 1929
2024-02-09 10:09:04,774 Epoch 1929: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-09 10:09:04,774 EPOCH 1930
2024-02-09 10:09:07,858 [Epoch: 1930 Step: 00032800] Batch Recognition Loss:   0.000308 => Gls Tokens per Sec:     1453 || Batch Translation Loss:   0.086610 => Txt Tokens per Sec:     3960 || Lr: 0.000100
2024-02-09 10:09:15,803 Epoch 1930: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.48 
2024-02-09 10:09:15,804 EPOCH 1931
2024-02-09 10:09:26,765 Epoch 1931: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.68 
2024-02-09 10:09:26,766 EPOCH 1932
2024-02-09 10:09:37,743 Epoch 1932: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.78 
2024-02-09 10:09:37,744 EPOCH 1933
2024-02-09 10:09:48,302 Epoch 1933: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.24 
2024-02-09 10:09:48,302 EPOCH 1934
2024-02-09 10:09:59,469 Epoch 1934: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.45 
2024-02-09 10:09:59,469 EPOCH 1935
2024-02-09 10:10:10,557 Epoch 1935: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.79 
2024-02-09 10:10:10,558 EPOCH 1936
2024-02-09 10:10:13,989 [Epoch: 1936 Step: 00032900] Batch Recognition Loss:   0.000950 => Gls Tokens per Sec:      857 || Batch Translation Loss:   0.058956 => Txt Tokens per Sec:     2280 || Lr: 0.000100
2024-02-09 10:10:21,467 Epoch 1936: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.64 
2024-02-09 10:10:21,467 EPOCH 1937
2024-02-09 10:10:32,494 Epoch 1937: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.25 
2024-02-09 10:10:32,494 EPOCH 1938
2024-02-09 10:10:43,556 Epoch 1938: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.88 
2024-02-09 10:10:43,557 EPOCH 1939
2024-02-09 10:10:54,534 Epoch 1939: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.66 
2024-02-09 10:10:54,534 EPOCH 1940
2024-02-09 10:11:05,454 Epoch 1940: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-09 10:11:05,454 EPOCH 1941
2024-02-09 10:11:16,345 Epoch 1941: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-09 10:11:16,346 EPOCH 1942
2024-02-09 10:11:17,142 [Epoch: 1942 Step: 00033000] Batch Recognition Loss:   0.000474 => Gls Tokens per Sec:     2415 || Batch Translation Loss:   0.018089 => Txt Tokens per Sec:     6799 || Lr: 0.000100
2024-02-09 10:11:27,376 Epoch 1942: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-09 10:11:27,377 EPOCH 1943
2024-02-09 10:11:38,375 Epoch 1943: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-09 10:11:38,375 EPOCH 1944
2024-02-09 10:11:49,161 Epoch 1944: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-09 10:11:49,162 EPOCH 1945
2024-02-09 10:12:00,132 Epoch 1945: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 10:12:00,133 EPOCH 1946
2024-02-09 10:12:11,067 Epoch 1946: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 10:12:11,067 EPOCH 1947
2024-02-09 10:12:22,058 Epoch 1947: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 10:12:22,059 EPOCH 1948
2024-02-09 10:12:22,305 [Epoch: 1948 Step: 00033100] Batch Recognition Loss:   0.000747 => Gls Tokens per Sec:     2602 || Batch Translation Loss:   0.013285 => Txt Tokens per Sec:     7744 || Lr: 0.000100
2024-02-09 10:12:32,823 Epoch 1948: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-09 10:12:32,823 EPOCH 1949
2024-02-09 10:12:43,745 Epoch 1949: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-09 10:12:43,746 EPOCH 1950
2024-02-09 10:12:54,647 Epoch 1950: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-09 10:12:54,648 EPOCH 1951
2024-02-09 10:13:05,452 Epoch 1951: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 10:13:05,452 EPOCH 1952
2024-02-09 10:13:16,636 Epoch 1952: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 10:13:16,637 EPOCH 1953
2024-02-09 10:13:27,066 [Epoch: 1953 Step: 00033200] Batch Recognition Loss:   0.000233 => Gls Tokens per Sec:      957 || Batch Translation Loss:   0.017752 => Txt Tokens per Sec:     2643 || Lr: 0.000100
2024-02-09 10:13:27,426 Epoch 1953: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-09 10:13:27,426 EPOCH 1954
2024-02-09 10:13:38,464 Epoch 1954: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.25 
2024-02-09 10:13:38,464 EPOCH 1955
2024-02-09 10:13:49,561 Epoch 1955: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 10:13:49,562 EPOCH 1956
2024-02-09 10:14:00,423 Epoch 1956: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-09 10:14:00,423 EPOCH 1957
2024-02-09 10:14:11,260 Epoch 1957: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-09 10:14:11,261 EPOCH 1958
2024-02-09 10:14:22,152 Epoch 1958: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-09 10:14:22,153 EPOCH 1959
2024-02-09 10:14:30,623 [Epoch: 1959 Step: 00033300] Batch Recognition Loss:   0.000183 => Gls Tokens per Sec:     1027 || Batch Translation Loss:   0.012188 => Txt Tokens per Sec:     2825 || Lr: 0.000100
2024-02-09 10:14:33,052 Epoch 1959: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 10:14:33,053 EPOCH 1960
2024-02-09 10:14:44,047 Epoch 1960: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 10:14:44,048 EPOCH 1961
2024-02-09 10:14:54,827 Epoch 1961: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 10:14:54,827 EPOCH 1962
2024-02-09 10:15:05,669 Epoch 1962: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-09 10:15:05,670 EPOCH 1963
2024-02-09 10:15:16,449 Epoch 1963: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 10:15:16,449 EPOCH 1964
2024-02-09 10:15:27,244 Epoch 1964: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 10:15:27,244 EPOCH 1965
2024-02-09 10:15:35,503 [Epoch: 1965 Step: 00033400] Batch Recognition Loss:   0.000217 => Gls Tokens per Sec:      899 || Batch Translation Loss:   0.010494 => Txt Tokens per Sec:     2595 || Lr: 0.000100
2024-02-09 10:15:37,909 Epoch 1965: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 10:15:37,910 EPOCH 1966
2024-02-09 10:15:48,765 Epoch 1966: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 10:15:48,765 EPOCH 1967
2024-02-09 10:15:59,465 Epoch 1967: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 10:15:59,466 EPOCH 1968
2024-02-09 10:16:10,001 Epoch 1968: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-09 10:16:10,002 EPOCH 1969
2024-02-09 10:16:20,752 Epoch 1969: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-09 10:16:20,753 EPOCH 1970
2024-02-09 10:16:31,462 Epoch 1970: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.73 
2024-02-09 10:16:31,462 EPOCH 1971
2024-02-09 10:16:39,277 [Epoch: 1971 Step: 00033500] Batch Recognition Loss:   0.000182 => Gls Tokens per Sec:      786 || Batch Translation Loss:   0.033395 => Txt Tokens per Sec:     2142 || Lr: 0.000100
2024-02-09 10:16:42,635 Epoch 1971: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.76 
2024-02-09 10:16:42,635 EPOCH 1972
2024-02-09 10:16:53,475 Epoch 1972: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.73 
2024-02-09 10:16:53,476 EPOCH 1973
2024-02-09 10:17:04,691 Epoch 1973: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.65 
2024-02-09 10:17:04,692 EPOCH 1974
2024-02-09 10:17:16,003 Epoch 1974: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.89 
2024-02-09 10:17:16,003 EPOCH 1975
2024-02-09 10:17:27,180 Epoch 1975: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.09 
2024-02-09 10:17:27,181 EPOCH 1976
2024-02-09 10:17:38,069 Epoch 1976: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.10 
2024-02-09 10:17:38,069 EPOCH 1977
2024-02-09 10:17:41,359 [Epoch: 1977 Step: 00033600] Batch Recognition Loss:   0.001446 => Gls Tokens per Sec:     1556 || Batch Translation Loss:   0.041042 => Txt Tokens per Sec:     4010 || Lr: 0.000100
2024-02-09 10:17:48,848 Epoch 1977: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-09 10:17:48,848 EPOCH 1978
2024-02-09 10:17:59,791 Epoch 1978: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-09 10:17:59,791 EPOCH 1979
2024-02-09 10:18:10,759 Epoch 1979: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.95 
2024-02-09 10:18:10,760 EPOCH 1980
2024-02-09 10:18:21,766 Epoch 1980: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.29 
2024-02-09 10:18:21,767 EPOCH 1981
2024-02-09 10:18:32,838 Epoch 1981: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.39 
2024-02-09 10:18:32,839 EPOCH 1982
2024-02-09 10:18:43,522 Epoch 1982: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.90 
2024-02-09 10:18:43,523 EPOCH 1983
2024-02-09 10:18:49,672 [Epoch: 1983 Step: 00033700] Batch Recognition Loss:   0.000242 => Gls Tokens per Sec:      625 || Batch Translation Loss:   0.027433 => Txt Tokens per Sec:     1823 || Lr: 0.000100
2024-02-09 10:18:54,603 Epoch 1983: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.92 
2024-02-09 10:18:54,604 EPOCH 1984
2024-02-09 10:19:05,369 Epoch 1984: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.88 
2024-02-09 10:19:05,370 EPOCH 1985
2024-02-09 10:19:16,315 Epoch 1985: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.94 
2024-02-09 10:19:16,315 EPOCH 1986
2024-02-09 10:19:27,384 Epoch 1986: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.84 
2024-02-09 10:19:27,385 EPOCH 1987
2024-02-09 10:19:38,285 Epoch 1987: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.73 
2024-02-09 10:19:38,285 EPOCH 1988
2024-02-09 10:19:49,267 Epoch 1988: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.57 
2024-02-09 10:19:49,268 EPOCH 1989
2024-02-09 10:19:51,831 [Epoch: 1989 Step: 00033800] Batch Recognition Loss:   0.000426 => Gls Tokens per Sec:      999 || Batch Translation Loss:   0.027633 => Txt Tokens per Sec:     2754 || Lr: 0.000100
2024-02-09 10:20:00,419 Epoch 1989: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-09 10:20:00,419 EPOCH 1990
2024-02-09 10:20:11,328 Epoch 1990: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.40 
2024-02-09 10:20:11,329 EPOCH 1991
2024-02-09 10:20:22,097 Epoch 1991: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-09 10:20:22,097 EPOCH 1992
2024-02-09 10:20:33,102 Epoch 1992: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-09 10:20:33,103 EPOCH 1993
2024-02-09 10:20:43,834 Epoch 1993: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-09 10:20:43,835 EPOCH 1994
2024-02-09 10:20:54,471 Epoch 1994: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-09 10:20:54,472 EPOCH 1995
2024-02-09 10:20:55,026 [Epoch: 1995 Step: 00033900] Batch Recognition Loss:   0.000133 => Gls Tokens per Sec:     2315 || Batch Translation Loss:   0.013743 => Txt Tokens per Sec:     6787 || Lr: 0.000100
2024-02-09 10:21:05,273 Epoch 1995: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 10:21:05,273 EPOCH 1996
2024-02-09 10:21:16,159 Epoch 1996: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-09 10:21:16,160 EPOCH 1997
2024-02-09 10:21:26,803 Epoch 1997: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-09 10:21:26,803 EPOCH 1998
2024-02-09 10:21:37,752 Epoch 1998: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-09 10:21:37,753 EPOCH 1999
2024-02-09 10:21:48,670 Epoch 1999: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-09 10:21:48,671 EPOCH 2000
2024-02-09 10:21:59,796 [Epoch: 2000 Step: 00034000] Batch Recognition Loss:   0.000272 => Gls Tokens per Sec:      955 || Batch Translation Loss:   0.009872 => Txt Tokens per Sec:     2642 || Lr: 0.000100
2024-02-09 10:22:39,923 Validation result at epoch 2000, step    34000: duration: 40.1258s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.58007	Translation Loss: 95321.49219	PPL: 13639.48633
	Eval Metric: BLEU
	WER 2.90	(DEL: 0.00,	INS: 0.00,	SUB: 2.90)
	BLEU-4 0.52	(BLEU-1: 10.65,	BLEU-2: 3.31,	BLEU-3: 1.19,	BLEU-4: 0.52)
	CHRF 17.11	ROUGE 9.03
2024-02-09 10:22:39,924 Logging Recognition and Translation Outputs
2024-02-09 10:22:39,925 ========================================================================================================================
2024-02-09 10:22:39,925 Logging Sequence: 171_158.00
2024-02-09 10:22:39,925 	Gloss Reference :	A B+C+D+E
2024-02-09 10:22:39,925 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 10:22:39,926 	Gloss Alignment :	         
2024-02-09 10:22:39,926 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 10:22:39,927 	Text Reference  :	with speculations of      dhoni being banned are spreading many say that it is  unlikely to  happen  
2024-02-09 10:22:39,927 	Text Hypothesis :	**** ************ however the   match ended  in  a         tie  the end  of the finals   has happened
2024-02-09 10:22:39,927 	Text Alignment  :	D    D            S       S     S     S      S   S         S    S   S    S  S   S        S   S       
2024-02-09 10:22:39,928 ========================================================================================================================
2024-02-09 10:22:39,928 Logging Sequence: 108_235.00
2024-02-09 10:22:39,928 	Gloss Reference :	A B+C+D+E
2024-02-09 10:22:39,928 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 10:22:39,928 	Gloss Alignment :	         
2024-02-09 10:22:39,928 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 10:22:39,930 	Text Reference  :	he     was  taken to          the hospital and it        was reported that he    is not  in  any    danger
2024-02-09 10:22:39,930 	Text Hypothesis :	people were seen  celebrating the ******** *** cricketer was ******** **** never do with his sudden crore 
2024-02-09 10:22:39,930 	Text Alignment  :	S      S    S     S               D        D   S             D        D    S     S  S    S   S      S     
2024-02-09 10:22:39,930 ========================================================================================================================
2024-02-09 10:22:39,930 Logging Sequence: 153_206.00
2024-02-09 10:22:39,931 	Gloss Reference :	A B+C+D+E
2024-02-09 10:22:39,931 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 10:22:39,931 	Gloss Alignment :	         
2024-02-09 10:22:39,931 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 10:22:39,932 	Text Reference  :	*** now  on    13th november everyone is    hoping pakistan **** *** *** **** **** ***** rewrites history
2024-02-09 10:22:39,932 	Text Hypothesis :	the 2022 final was  played   between  india and    pakistan have won the same time after 7        goals  
2024-02-09 10:22:39,932 	Text Alignment  :	I   S    S     S    S        S        S     S               I    I   I   I    I    I     S        S      
2024-02-09 10:22:39,933 ========================================================================================================================
2024-02-09 10:22:39,933 Logging Sequence: 87_202.00
2024-02-09 10:22:39,933 	Gloss Reference :	A B+C+D+E
2024-02-09 10:22:39,933 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 10:22:39,933 	Gloss Alignment :	         
2024-02-09 10:22:39,933 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 10:22:39,934 	Text Reference  :	*** **** ******** ** *** ** *** ** *** i  love      our  players and         i  love my      country
2024-02-09 10:22:39,934 	Text Hypothesis :	the vast majority of the of the of the 56 countries were former  territories of the  british empire 
2024-02-09 10:22:39,935 	Text Alignment  :	I   I    I        I  I   I  I   I  I   S  S         S    S       S           S  S    S       S      
2024-02-09 10:22:39,935 ========================================================================================================================
2024-02-09 10:22:39,935 Logging Sequence: 84_2.00
2024-02-09 10:22:39,935 	Gloss Reference :	A B+C+D+E
2024-02-09 10:22:39,935 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 10:22:39,935 	Gloss Alignment :	         
2024-02-09 10:22:39,935 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 10:22:39,937 	Text Reference  :	the 2022 fifa football world cup is      going on in qatar   from 20th   november 2022 to 18th december 2022
2024-02-09 10:22:39,937 	Text Hypothesis :	*** **** **** so       what  a   cricket teams on ** winning the  sports be       held at the  world    cup 
2024-02-09 10:22:39,937 	Text Alignment  :	D   D    D    S        S     S   S       S        D  S       S    S      S        S    S  S    S        S   
2024-02-09 10:22:39,938 ========================================================================================================================
2024-02-09 10:22:39,941 Epoch 2000: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 10:22:39,941 EPOCH 2001
2024-02-09 10:22:51,173 Epoch 2001: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 10:22:51,173 EPOCH 2002
2024-02-09 10:23:02,026 Epoch 2002: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-09 10:23:02,026 EPOCH 2003
2024-02-09 10:23:12,655 Epoch 2003: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-09 10:23:12,656 EPOCH 2004
2024-02-09 10:23:23,484 Epoch 2004: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-09 10:23:23,485 EPOCH 2005
2024-02-09 10:23:34,387 Epoch 2005: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-09 10:23:34,388 EPOCH 2006
2024-02-09 10:23:43,445 [Epoch: 2006 Step: 00034100] Batch Recognition Loss:   0.000186 => Gls Tokens per Sec:     1031 || Batch Translation Loss:   0.014217 => Txt Tokens per Sec:     2896 || Lr: 0.000100
2024-02-09 10:23:45,226 Epoch 2006: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-09 10:23:45,226 EPOCH 2007
2024-02-09 10:23:56,048 Epoch 2007: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-09 10:23:56,049 EPOCH 2008
2024-02-09 10:24:06,836 Epoch 2008: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-09 10:24:06,836 EPOCH 2009
2024-02-09 10:24:17,830 Epoch 2009: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-09 10:24:17,831 EPOCH 2010
2024-02-09 10:24:28,658 Epoch 2010: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.80 
2024-02-09 10:24:28,659 EPOCH 2011
2024-02-09 10:24:39,589 Epoch 2011: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.72 
2024-02-09 10:24:39,589 EPOCH 2012
2024-02-09 10:24:47,513 [Epoch: 2012 Step: 00034200] Batch Recognition Loss:   0.000333 => Gls Tokens per Sec:     1017 || Batch Translation Loss:   0.050140 => Txt Tokens per Sec:     2809 || Lr: 0.000100
2024-02-09 10:24:50,264 Epoch 2012: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.87 
2024-02-09 10:24:50,265 EPOCH 2013
2024-02-09 10:25:01,147 Epoch 2013: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.00 
2024-02-09 10:25:01,148 EPOCH 2014
2024-02-09 10:25:11,987 Epoch 2014: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.03 
2024-02-09 10:25:11,987 EPOCH 2015
2024-02-09 10:25:22,692 Epoch 2015: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.99 
2024-02-09 10:25:22,692 EPOCH 2016
2024-02-09 10:25:33,713 Epoch 2016: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-09 10:25:33,714 EPOCH 2017
2024-02-09 10:25:44,383 Epoch 2017: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.83 
2024-02-09 10:25:44,384 EPOCH 2018
2024-02-09 10:25:51,460 [Epoch: 2018 Step: 00034300] Batch Recognition Loss:   0.000404 => Gls Tokens per Sec:      995 || Batch Translation Loss:   0.043944 => Txt Tokens per Sec:     2647 || Lr: 0.000100
2024-02-09 10:25:55,536 Epoch 2018: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.90 
2024-02-09 10:25:55,537 EPOCH 2019
2024-02-09 10:26:06,417 Epoch 2019: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.81 
2024-02-09 10:26:06,418 EPOCH 2020
2024-02-09 10:26:17,123 Epoch 2020: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.82 
2024-02-09 10:26:17,124 EPOCH 2021
2024-02-09 10:26:27,994 Epoch 2021: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.75 
2024-02-09 10:26:27,994 EPOCH 2022
2024-02-09 10:26:39,108 Epoch 2022: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-09 10:26:39,109 EPOCH 2023
2024-02-09 10:26:50,291 Epoch 2023: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-09 10:26:50,292 EPOCH 2024
2024-02-09 10:26:56,379 [Epoch: 2024 Step: 00034400] Batch Recognition Loss:   0.000363 => Gls Tokens per Sec:      904 || Batch Translation Loss:   0.039221 => Txt Tokens per Sec:     2586 || Lr: 0.000100
2024-02-09 10:27:01,257 Epoch 2024: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-09 10:27:01,257 EPOCH 2025
2024-02-09 10:27:12,425 Epoch 2025: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-09 10:27:12,426 EPOCH 2026
2024-02-09 10:27:23,491 Epoch 2026: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-09 10:27:23,491 EPOCH 2027
2024-02-09 10:27:34,677 Epoch 2027: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-09 10:27:34,678 EPOCH 2028
2024-02-09 10:27:45,349 Epoch 2028: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.50 
2024-02-09 10:27:45,350 EPOCH 2029
2024-02-09 10:27:56,554 Epoch 2029: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-09 10:27:56,555 EPOCH 2030
2024-02-09 10:28:01,054 [Epoch: 2030 Step: 00034500] Batch Recognition Loss:   0.000678 => Gls Tokens per Sec:      996 || Batch Translation Loss:   0.048073 => Txt Tokens per Sec:     2578 || Lr: 0.000100
2024-02-09 10:28:07,498 Epoch 2030: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-09 10:28:07,498 EPOCH 2031
2024-02-09 10:28:18,230 Epoch 2031: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.98 
2024-02-09 10:28:18,230 EPOCH 2032
2024-02-09 10:28:29,089 Epoch 2032: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.01 
2024-02-09 10:28:29,089 EPOCH 2033
2024-02-09 10:28:39,862 Epoch 2033: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.32 
2024-02-09 10:28:39,863 EPOCH 2034
2024-02-09 10:28:50,575 Epoch 2034: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.82 
2024-02-09 10:28:50,576 EPOCH 2035
2024-02-09 10:29:01,246 Epoch 2035: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.65 
2024-02-09 10:29:01,247 EPOCH 2036
2024-02-09 10:29:03,736 [Epoch: 2036 Step: 00034600] Batch Recognition Loss:   0.001459 => Gls Tokens per Sec:     1286 || Batch Translation Loss:   0.084228 => Txt Tokens per Sec:     3518 || Lr: 0.000100
2024-02-09 10:29:12,059 Epoch 2036: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.49 
2024-02-09 10:29:12,060 EPOCH 2037
2024-02-09 10:29:22,894 Epoch 2037: Total Training Recognition Loss 0.05  Total Training Translation Loss 7.95 
2024-02-09 10:29:22,895 EPOCH 2038
2024-02-09 10:29:33,443 Epoch 2038: Total Training Recognition Loss 0.11  Total Training Translation Loss 5.59 
2024-02-09 10:29:33,444 EPOCH 2039
2024-02-09 10:29:44,511 Epoch 2039: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.40 
2024-02-09 10:29:44,511 EPOCH 2040
2024-02-09 10:29:55,286 Epoch 2040: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.77 
2024-02-09 10:29:55,286 EPOCH 2041
2024-02-09 10:30:06,326 Epoch 2041: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.53 
2024-02-09 10:30:06,327 EPOCH 2042
2024-02-09 10:30:09,170 [Epoch: 2042 Step: 00034700] Batch Recognition Loss:   0.000760 => Gls Tokens per Sec:      676 || Batch Translation Loss:   0.069521 => Txt Tokens per Sec:     2076 || Lr: 0.000100
2024-02-09 10:30:17,522 Epoch 2042: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-09 10:30:17,522 EPOCH 2043
2024-02-09 10:30:28,341 Epoch 2043: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.42 
2024-02-09 10:30:28,342 EPOCH 2044
2024-02-09 10:30:39,269 Epoch 2044: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.35 
2024-02-09 10:30:39,270 EPOCH 2045
2024-02-09 10:30:50,047 Epoch 2045: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-09 10:30:50,048 EPOCH 2046
2024-02-09 10:31:00,930 Epoch 2046: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 10:31:00,931 EPOCH 2047
2024-02-09 10:31:11,666 Epoch 2047: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 10:31:11,666 EPOCH 2048
2024-02-09 10:31:11,889 [Epoch: 2048 Step: 00034800] Batch Recognition Loss:   0.000236 => Gls Tokens per Sec:     2883 || Batch Translation Loss:   0.012835 => Txt Tokens per Sec:     7144 || Lr: 0.000100
2024-02-09 10:31:22,405 Epoch 2048: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 10:31:22,406 EPOCH 2049
2024-02-09 10:31:32,727 Epoch 2049: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.28 
2024-02-09 10:31:32,727 EPOCH 2050
2024-02-09 10:31:43,638 Epoch 2050: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 10:31:43,639 EPOCH 2051
2024-02-09 10:31:54,604 Epoch 2051: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 10:31:54,604 EPOCH 2052
2024-02-09 10:32:05,284 Epoch 2052: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-09 10:32:05,284 EPOCH 2053
2024-02-09 10:32:15,987 [Epoch: 2053 Step: 00034900] Batch Recognition Loss:   0.000580 => Gls Tokens per Sec:      933 || Batch Translation Loss:   0.025077 => Txt Tokens per Sec:     2624 || Lr: 0.000100
2024-02-09 10:32:16,122 Epoch 2053: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 10:32:16,122 EPOCH 2054
2024-02-09 10:32:27,138 Epoch 2054: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 10:32:27,139 EPOCH 2055
2024-02-09 10:32:38,077 Epoch 2055: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-09 10:32:38,078 EPOCH 2056
2024-02-09 10:32:48,815 Epoch 2056: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-09 10:32:48,815 EPOCH 2057
2024-02-09 10:32:59,511 Epoch 2057: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 10:32:59,512 EPOCH 2058
2024-02-09 10:33:10,442 Epoch 2058: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-09 10:33:10,442 EPOCH 2059
2024-02-09 10:33:20,383 [Epoch: 2059 Step: 00035000] Batch Recognition Loss:   0.000220 => Gls Tokens per Sec:      875 || Batch Translation Loss:   0.010445 => Txt Tokens per Sec:     2377 || Lr: 0.000100
2024-02-09 10:33:21,228 Epoch 2059: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-09 10:33:21,229 EPOCH 2060
2024-02-09 10:33:32,043 Epoch 2060: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-09 10:33:32,044 EPOCH 2061
2024-02-09 10:33:42,862 Epoch 2061: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-09 10:33:42,862 EPOCH 2062
2024-02-09 10:33:53,942 Epoch 2062: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-09 10:33:53,943 EPOCH 2063
2024-02-09 10:34:05,028 Epoch 2063: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-09 10:34:05,028 EPOCH 2064
2024-02-09 10:34:15,919 Epoch 2064: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-09 10:34:15,920 EPOCH 2065
2024-02-09 10:34:24,177 [Epoch: 2065 Step: 00035100] Batch Recognition Loss:   0.000160 => Gls Tokens per Sec:      899 || Batch Translation Loss:   0.012790 => Txt Tokens per Sec:     2442 || Lr: 0.000100
2024-02-09 10:34:26,946 Epoch 2065: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-09 10:34:26,946 EPOCH 2066
2024-02-09 10:34:37,928 Epoch 2066: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-09 10:34:37,929 EPOCH 2067
2024-02-09 10:34:49,058 Epoch 2067: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-09 10:34:49,058 EPOCH 2068
2024-02-09 10:35:00,107 Epoch 2068: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-09 10:35:00,108 EPOCH 2069
2024-02-09 10:35:10,984 Epoch 2069: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-09 10:35:10,985 EPOCH 2070
2024-02-09 10:35:22,192 Epoch 2070: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-09 10:35:22,193 EPOCH 2071
2024-02-09 10:35:31,420 [Epoch: 2071 Step: 00035200] Batch Recognition Loss:   0.000273 => Gls Tokens per Sec:      666 || Batch Translation Loss:   0.012837 => Txt Tokens per Sec:     1832 || Lr: 0.000100
2024-02-09 10:35:33,223 Epoch 2071: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-09 10:35:33,223 EPOCH 2072
2024-02-09 10:35:43,992 Epoch 2072: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-09 10:35:43,993 EPOCH 2073
2024-02-09 10:35:54,945 Epoch 2073: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-09 10:35:54,946 EPOCH 2074
2024-02-09 10:36:06,062 Epoch 2074: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-09 10:36:06,063 EPOCH 2075
2024-02-09 10:36:16,910 Epoch 2075: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-09 10:36:16,911 EPOCH 2076
2024-02-09 10:36:27,795 Epoch 2076: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-09 10:36:27,795 EPOCH 2077
2024-02-09 10:36:34,968 [Epoch: 2077 Step: 00035300] Batch Recognition Loss:   0.000140 => Gls Tokens per Sec:      678 || Batch Translation Loss:   0.012854 => Txt Tokens per Sec:     2021 || Lr: 0.000100
2024-02-09 10:36:38,696 Epoch 2077: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-09 10:36:38,697 EPOCH 2078
2024-02-09 10:36:49,705 Epoch 2078: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-09 10:36:49,706 EPOCH 2079
2024-02-09 10:37:00,720 Epoch 2079: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-09 10:37:00,720 EPOCH 2080
2024-02-09 10:37:11,538 Epoch 2080: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-09 10:37:11,539 EPOCH 2081
2024-02-09 10:37:22,429 Epoch 2081: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-09 10:37:22,429 EPOCH 2082
2024-02-09 10:37:33,313 Epoch 2082: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-09 10:37:33,313 EPOCH 2083
2024-02-09 10:37:38,595 [Epoch: 2083 Step: 00035400] Batch Recognition Loss:   0.000361 => Gls Tokens per Sec:      678 || Batch Translation Loss:   0.042749 => Txt Tokens per Sec:     1842 || Lr: 0.000100
2024-02-09 10:37:44,057 Epoch 2083: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-09 10:37:44,057 EPOCH 2084
2024-02-09 10:37:54,942 Epoch 2084: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-09 10:37:54,943 EPOCH 2085
2024-02-09 10:38:05,848 Epoch 2085: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-09 10:38:05,848 EPOCH 2086
2024-02-09 10:38:16,538 Epoch 2086: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-09 10:38:16,539 EPOCH 2087
2024-02-09 10:38:27,402 Epoch 2087: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-09 10:38:27,402 EPOCH 2088
2024-02-09 10:38:38,191 Epoch 2088: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-09 10:38:38,192 EPOCH 2089
2024-02-09 10:38:41,378 [Epoch: 2089 Step: 00035500] Batch Recognition Loss:   0.000239 => Gls Tokens per Sec:      722 || Batch Translation Loss:   0.034042 => Txt Tokens per Sec:     2147 || Lr: 0.000100
2024-02-09 10:38:49,130 Epoch 2089: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.95 
2024-02-09 10:38:49,131 EPOCH 2090
2024-02-09 10:38:59,809 Epoch 2090: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.68 
2024-02-09 10:38:59,809 EPOCH 2091
2024-02-09 10:39:10,878 Epoch 2091: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.00 
2024-02-09 10:39:10,879 EPOCH 2092
2024-02-09 10:39:21,716 Epoch 2092: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.56 
2024-02-09 10:39:21,717 EPOCH 2093
2024-02-09 10:39:32,635 Epoch 2093: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.26 
2024-02-09 10:39:32,636 EPOCH 2094
2024-02-09 10:39:43,441 Epoch 2094: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.06 
2024-02-09 10:39:43,442 EPOCH 2095
2024-02-09 10:39:45,375 [Epoch: 2095 Step: 00035600] Batch Recognition Loss:   0.000486 => Gls Tokens per Sec:      663 || Batch Translation Loss:   0.085396 => Txt Tokens per Sec:     1807 || Lr: 0.000100
2024-02-09 10:39:54,461 Epoch 2095: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.89 
2024-02-09 10:39:54,462 EPOCH 2096
2024-02-09 10:40:05,367 Epoch 2096: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.12 
2024-02-09 10:40:05,367 EPOCH 2097
2024-02-09 10:40:16,098 Epoch 2097: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.09 
2024-02-09 10:40:16,099 EPOCH 2098
2024-02-09 10:40:27,135 Epoch 2098: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.08 
2024-02-09 10:40:27,136 EPOCH 2099
2024-02-09 10:40:38,289 Epoch 2099: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.99 
2024-02-09 10:40:38,290 EPOCH 2100
2024-02-09 10:40:49,292 [Epoch: 2100 Step: 00035700] Batch Recognition Loss:   0.000523 => Gls Tokens per Sec:      965 || Batch Translation Loss:   0.040400 => Txt Tokens per Sec:     2671 || Lr: 0.000100
2024-02-09 10:40:49,293 Epoch 2100: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.83 
2024-02-09 10:40:49,293 EPOCH 2101
2024-02-09 10:41:00,234 Epoch 2101: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.09 
2024-02-09 10:41:00,235 EPOCH 2102
2024-02-09 10:41:11,191 Epoch 2102: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.03 
2024-02-09 10:41:11,191 EPOCH 2103
2024-02-09 10:41:22,145 Epoch 2103: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.78 
2024-02-09 10:41:22,146 EPOCH 2104
2024-02-09 10:41:33,007 Epoch 2104: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.79 
2024-02-09 10:41:33,008 EPOCH 2105
2024-02-09 10:41:43,884 Epoch 2105: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.44 
2024-02-09 10:41:43,885 EPOCH 2106
2024-02-09 10:41:52,169 [Epoch: 2106 Step: 00035800] Batch Recognition Loss:   0.003231 => Gls Tokens per Sec:     1159 || Batch Translation Loss:   0.020160 => Txt Tokens per Sec:     3264 || Lr: 0.000100
2024-02-09 10:41:54,787 Epoch 2106: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-09 10:41:54,787 EPOCH 2107
2024-02-09 10:42:06,324 Epoch 2107: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-09 10:42:06,325 EPOCH 2108
2024-02-09 10:42:17,201 Epoch 2108: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 10:42:17,201 EPOCH 2109
2024-02-09 10:42:28,049 Epoch 2109: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 10:42:28,049 EPOCH 2110
2024-02-09 10:42:38,922 Epoch 2110: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 10:42:38,923 EPOCH 2111
2024-02-09 10:42:49,800 Epoch 2111: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 10:42:49,801 EPOCH 2112
2024-02-09 10:42:59,960 [Epoch: 2112 Step: 00035900] Batch Recognition Loss:   0.000193 => Gls Tokens per Sec:      794 || Batch Translation Loss:   0.019188 => Txt Tokens per Sec:     2286 || Lr: 0.000100
2024-02-09 10:43:00,780 Epoch 2112: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-09 10:43:00,780 EPOCH 2113
2024-02-09 10:43:11,588 Epoch 2113: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-09 10:43:11,589 EPOCH 2114
2024-02-09 10:43:22,449 Epoch 2114: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-09 10:43:22,450 EPOCH 2115
2024-02-09 10:43:33,395 Epoch 2115: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-09 10:43:33,395 EPOCH 2116
2024-02-09 10:43:44,009 Epoch 2116: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-09 10:43:44,009 EPOCH 2117
2024-02-09 10:43:54,828 Epoch 2117: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-09 10:43:54,828 EPOCH 2118
2024-02-09 10:44:02,735 [Epoch: 2118 Step: 00036000] Batch Recognition Loss:   0.000231 => Gls Tokens per Sec:      858 || Batch Translation Loss:   0.047371 => Txt Tokens per Sec:     2425 || Lr: 0.000100
2024-02-09 10:44:42,469 Validation result at epoch 2118, step    36000: duration: 39.7337s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.66059	Translation Loss: 98032.39844	PPL: 17880.92188
	Eval Metric: BLEU
	WER 2.82	(DEL: 0.00,	INS: 0.00,	SUB: 2.82)
	BLEU-4 0.50	(BLEU-1: 10.25,	BLEU-2: 3.04,	BLEU-3: 1.12,	BLEU-4: 0.50)
	CHRF 16.81	ROUGE 8.91
2024-02-09 10:44:42,470 Logging Recognition and Translation Outputs
2024-02-09 10:44:42,470 ========================================================================================================================
2024-02-09 10:44:42,470 Logging Sequence: 153_36.00
2024-02-09 10:44:42,471 	Gloss Reference :	A B+C+D+E
2024-02-09 10:44:42,471 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 10:44:42,471 	Gloss Alignment :	         
2024-02-09 10:44:42,471 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 10:44:42,472 	Text Reference  :	india **** *** made  a    good  score of 1686    in 20 overs
2024-02-09 10:44:42,472 	Text Hypothesis :	india lost the match when india lost  8  wickets oh my god  
2024-02-09 10:44:42,472 	Text Alignment  :	      I    I   S     S    S     S     S  S       S  S  S    
2024-02-09 10:44:42,472 ========================================================================================================================
2024-02-09 10:44:42,473 Logging Sequence: 163_30.00
2024-02-09 10:44:42,473 	Gloss Reference :	A B+C+D+E
2024-02-09 10:44:42,473 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 10:44:42,473 	Gloss Alignment :	         
2024-02-09 10:44:42,473 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 10:44:42,474 	Text Reference  :	they ****** ** ***** ***** never  permitted anyone to reveal her face
2024-02-09 10:44:42,474 	Text Hypothesis :	they wanted to limit their travel and       i      am so     far etc 
2024-02-09 10:44:42,474 	Text Alignment  :	     I      I  I     I     S      S         S      S  S      S   S   
2024-02-09 10:44:42,474 ========================================================================================================================
2024-02-09 10:44:42,474 Logging Sequence: 167_60.00
2024-02-09 10:44:42,474 	Gloss Reference :	A B+C+D+E
2024-02-09 10:44:42,475 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 10:44:42,475 	Gloss Alignment :	         
2024-02-09 10:44:42,475 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 10:44:42,476 	Text Reference  :	camel flu spreads rapidly when    one   comes in  close contact with  the     infected
2024-02-09 10:44:42,476 	Text Hypothesis :	there was a       stadium between virat kohli and watch the     match between him     
2024-02-09 10:44:42,476 	Text Alignment  :	S     S   S       S       S       S     S     S   S     S       S     S       S       
2024-02-09 10:44:42,477 ========================================================================================================================
2024-02-09 10:44:42,477 Logging Sequence: 84_35.00
2024-02-09 10:44:42,477 	Gloss Reference :	A B+C+D+E
2024-02-09 10:44:42,477 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 10:44:42,477 	Gloss Alignment :	         
2024-02-09 10:44:42,477 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 10:44:42,478 	Text Reference  :	******* here is the reason why they covered their mouth   
2024-02-09 10:44:42,478 	Text Hypothesis :	however he   is *** ****** *** what a       huge  argument
2024-02-09 10:44:42,478 	Text Alignment  :	I       S       D   D      D   S    S       S     S       
2024-02-09 10:44:42,478 ========================================================================================================================
2024-02-09 10:44:42,478 Logging Sequence: 96_2.00
2024-02-09 10:44:42,479 	Gloss Reference :	A B+C+D+E        
2024-02-09 10:44:42,479 	Gloss Hypothesis:	A B+C+D+E+D+E+D+E
2024-02-09 10:44:42,479 	Gloss Alignment :	  S              
2024-02-09 10:44:42,479 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 10:44:42,480 	Text Reference  :	the world is preparing for the      t20     world cup scheduled to start from 16th  october this year
2024-02-09 10:44:42,480 	Text Hypothesis :	the ***** ** ********* icc under-19 cricket world cup ********* ** ***** was  first played  in   1988
2024-02-09 10:44:42,481 	Text Alignment  :	    D     D  D         S   S        S                 D         D  D     S    S     S       S    S   
2024-02-09 10:44:42,481 ========================================================================================================================
2024-02-09 10:44:45,548 Epoch 2118: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-09 10:44:45,549 EPOCH 2119
2024-02-09 10:44:56,428 Epoch 2119: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-09 10:44:56,428 EPOCH 2120
2024-02-09 10:45:07,438 Epoch 2120: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-09 10:45:07,439 EPOCH 2121
2024-02-09 10:45:18,381 Epoch 2121: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-09 10:45:18,381 EPOCH 2122
2024-02-09 10:45:29,425 Epoch 2122: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 10:45:29,426 EPOCH 2123
2024-02-09 10:45:40,226 Epoch 2123: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 10:45:40,226 EPOCH 2124
2024-02-09 10:45:47,825 [Epoch: 2124 Step: 00036100] Batch Recognition Loss:   0.000179 => Gls Tokens per Sec:      724 || Batch Translation Loss:   0.014127 => Txt Tokens per Sec:     1980 || Lr: 0.000050
2024-02-09 10:45:51,137 Epoch 2124: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-09 10:45:51,137 EPOCH 2125
2024-02-09 10:46:01,972 Epoch 2125: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-09 10:46:01,972 EPOCH 2126
2024-02-09 10:46:12,814 Epoch 2126: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-09 10:46:12,815 EPOCH 2127
2024-02-09 10:46:23,825 Epoch 2127: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-09 10:46:23,825 EPOCH 2128
2024-02-09 10:46:34,802 Epoch 2128: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-09 10:46:34,803 EPOCH 2129
2024-02-09 10:46:45,614 Epoch 2129: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 10:46:45,615 EPOCH 2130
2024-02-09 10:46:51,071 [Epoch: 2130 Step: 00036200] Batch Recognition Loss:   0.000370 => Gls Tokens per Sec:      774 || Batch Translation Loss:   0.011725 => Txt Tokens per Sec:     2174 || Lr: 0.000050
2024-02-09 10:46:56,644 Epoch 2130: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-09 10:46:56,644 EPOCH 2131
2024-02-09 10:47:07,545 Epoch 2131: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-09 10:47:07,546 EPOCH 2132
2024-02-09 10:47:18,348 Epoch 2132: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-09 10:47:18,348 EPOCH 2133
2024-02-09 10:47:29,176 Epoch 2133: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-09 10:47:29,177 EPOCH 2134
2024-02-09 10:47:39,986 Epoch 2134: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-09 10:47:39,987 EPOCH 2135
2024-02-09 10:47:51,059 Epoch 2135: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-09 10:47:51,060 EPOCH 2136
2024-02-09 10:47:52,363 [Epoch: 2136 Step: 00036300] Batch Recognition Loss:   0.000186 => Gls Tokens per Sec:     2458 || Batch Translation Loss:   0.026001 => Txt Tokens per Sec:     7280 || Lr: 0.000050
2024-02-09 10:48:01,730 Epoch 2136: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-09 10:48:01,730 EPOCH 2137
2024-02-09 10:48:12,663 Epoch 2137: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 10:48:12,664 EPOCH 2138
2024-02-09 10:48:23,588 Epoch 2138: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-09 10:48:23,588 EPOCH 2139
2024-02-09 10:48:34,375 Epoch 2139: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-09 10:48:34,376 EPOCH 2140
2024-02-09 10:48:45,281 Epoch 2140: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-09 10:48:45,282 EPOCH 2141
2024-02-09 10:48:55,914 Epoch 2141: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-09 10:48:55,915 EPOCH 2142
2024-02-09 10:48:58,490 [Epoch: 2142 Step: 00036400] Batch Recognition Loss:   0.000151 => Gls Tokens per Sec:      746 || Batch Translation Loss:   0.011468 => Txt Tokens per Sec:     2375 || Lr: 0.000050
2024-02-09 10:49:06,720 Epoch 2142: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-09 10:49:06,721 EPOCH 2143
2024-02-09 10:49:17,784 Epoch 2143: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-09 10:49:17,784 EPOCH 2144
2024-02-09 10:49:28,633 Epoch 2144: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-09 10:49:28,634 EPOCH 2145
2024-02-09 10:49:39,513 Epoch 2145: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-09 10:49:39,513 EPOCH 2146
2024-02-09 10:49:50,613 Epoch 2146: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-09 10:49:50,614 EPOCH 2147
2024-02-09 10:50:01,538 Epoch 2147: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-09 10:50:01,539 EPOCH 2148
2024-02-09 10:50:01,709 [Epoch: 2148 Step: 00036500] Batch Recognition Loss:   0.000116 => Gls Tokens per Sec:     3810 || Batch Translation Loss:   0.008523 => Txt Tokens per Sec:     9923 || Lr: 0.000050
2024-02-09 10:50:12,275 Epoch 2148: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-09 10:50:12,275 EPOCH 2149
2024-02-09 10:50:23,306 Epoch 2149: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-09 10:50:23,306 EPOCH 2150
2024-02-09 10:50:34,210 Epoch 2150: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-09 10:50:34,210 EPOCH 2151
2024-02-09 10:50:45,276 Epoch 2151: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-09 10:50:45,277 EPOCH 2152
2024-02-09 10:50:56,129 Epoch 2152: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-09 10:50:56,129 EPOCH 2153
2024-02-09 10:51:04,694 [Epoch: 2153 Step: 00036600] Batch Recognition Loss:   0.000106 => Gls Tokens per Sec:     1196 || Batch Translation Loss:   0.008040 => Txt Tokens per Sec:     3273 || Lr: 0.000050
2024-02-09 10:51:07,160 Epoch 2153: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-09 10:51:07,160 EPOCH 2154
2024-02-09 10:51:18,014 Epoch 2154: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-09 10:51:18,014 EPOCH 2155
2024-02-09 10:51:28,845 Epoch 2155: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-09 10:51:28,845 EPOCH 2156
2024-02-09 10:51:39,467 Epoch 2156: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-09 10:51:39,468 EPOCH 2157
2024-02-09 10:51:50,477 Epoch 2157: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-09 10:51:50,478 EPOCH 2158
2024-02-09 10:52:01,486 Epoch 2158: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-09 10:52:01,487 EPOCH 2159
2024-02-09 10:52:10,283 [Epoch: 2159 Step: 00036700] Batch Recognition Loss:   0.000247 => Gls Tokens per Sec:      989 || Batch Translation Loss:   0.004667 => Txt Tokens per Sec:     2655 || Lr: 0.000050
2024-02-09 10:52:12,538 Epoch 2159: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-09 10:52:12,538 EPOCH 2160
2024-02-09 10:52:23,530 Epoch 2160: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-09 10:52:23,530 EPOCH 2161
2024-02-09 10:52:34,504 Epoch 2161: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-09 10:52:34,505 EPOCH 2162
2024-02-09 10:52:45,431 Epoch 2162: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-09 10:52:45,431 EPOCH 2163
2024-02-09 10:52:56,416 Epoch 2163: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 10:52:56,417 EPOCH 2164
2024-02-09 10:53:07,231 Epoch 2164: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-09 10:53:07,231 EPOCH 2165
2024-02-09 10:53:15,293 [Epoch: 2165 Step: 00036800] Batch Recognition Loss:   0.000277 => Gls Tokens per Sec:      920 || Batch Translation Loss:   0.010941 => Txt Tokens per Sec:     2599 || Lr: 0.000050
2024-02-09 10:53:18,298 Epoch 2165: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-09 10:53:18,299 EPOCH 2166
2024-02-09 10:53:29,425 Epoch 2166: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-09 10:53:29,425 EPOCH 2167
2024-02-09 10:53:40,025 Epoch 2167: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-09 10:53:40,026 EPOCH 2168
2024-02-09 10:53:50,886 Epoch 2168: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-09 10:53:50,886 EPOCH 2169
2024-02-09 10:54:01,868 Epoch 2169: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-09 10:54:01,869 EPOCH 2170
2024-02-09 10:54:12,797 Epoch 2170: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-09 10:54:12,797 EPOCH 2171
2024-02-09 10:54:18,402 [Epoch: 2171 Step: 00036900] Batch Recognition Loss:   0.000221 => Gls Tokens per Sec:     1142 || Batch Translation Loss:   0.015521 => Txt Tokens per Sec:     3136 || Lr: 0.000050
2024-02-09 10:54:23,648 Epoch 2171: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-09 10:54:23,649 EPOCH 2172
2024-02-09 10:54:34,731 Epoch 2172: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-09 10:54:34,732 EPOCH 2173
2024-02-09 10:54:45,607 Epoch 2173: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-09 10:54:45,607 EPOCH 2174
2024-02-09 10:54:56,727 Epoch 2174: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-09 10:54:56,727 EPOCH 2175
2024-02-09 10:55:07,750 Epoch 2175: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-09 10:55:07,751 EPOCH 2176
2024-02-09 10:55:18,628 Epoch 2176: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-09 10:55:18,629 EPOCH 2177
2024-02-09 10:55:25,315 [Epoch: 2177 Step: 00037000] Batch Recognition Loss:   0.000481 => Gls Tokens per Sec:      766 || Batch Translation Loss:   0.009286 => Txt Tokens per Sec:     2108 || Lr: 0.000050
2024-02-09 10:55:29,679 Epoch 2177: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-09 10:55:29,680 EPOCH 2178
2024-02-09 10:55:40,299 Epoch 2178: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 10:55:40,299 EPOCH 2179
2024-02-09 10:55:50,991 Epoch 2179: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 10:55:50,991 EPOCH 2180
2024-02-09 10:56:02,027 Epoch 2180: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-09 10:56:02,027 EPOCH 2181
2024-02-09 10:56:12,906 Epoch 2181: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-09 10:56:12,907 EPOCH 2182
2024-02-09 10:56:23,798 Epoch 2182: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-09 10:56:23,799 EPOCH 2183
2024-02-09 10:56:28,080 [Epoch: 2183 Step: 00037100] Batch Recognition Loss:   0.000132 => Gls Tokens per Sec:      897 || Batch Translation Loss:   0.014618 => Txt Tokens per Sec:     2593 || Lr: 0.000050
2024-02-09 10:56:34,562 Epoch 2183: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-09 10:56:34,562 EPOCH 2184
2024-02-09 10:56:45,335 Epoch 2184: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-09 10:56:45,335 EPOCH 2185
2024-02-09 10:56:56,134 Epoch 2185: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-09 10:56:56,134 EPOCH 2186
2024-02-09 10:57:07,013 Epoch 2186: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 10:57:07,013 EPOCH 2187
2024-02-09 10:57:18,012 Epoch 2187: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 10:57:18,013 EPOCH 2188
2024-02-09 10:57:28,804 Epoch 2188: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 10:57:28,805 EPOCH 2189
2024-02-09 10:57:31,956 [Epoch: 2189 Step: 00037200] Batch Recognition Loss:   0.000176 => Gls Tokens per Sec:      730 || Batch Translation Loss:   0.006571 => Txt Tokens per Sec:     1851 || Lr: 0.000050
2024-02-09 10:57:39,821 Epoch 2189: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 10:57:39,821 EPOCH 2190
2024-02-09 10:57:50,557 Epoch 2190: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-09 10:57:50,558 EPOCH 2191
2024-02-09 10:58:01,097 Epoch 2191: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-09 10:58:01,097 EPOCH 2192
2024-02-09 10:58:12,156 Epoch 2192: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-09 10:58:12,157 EPOCH 2193
2024-02-09 10:58:23,089 Epoch 2193: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-09 10:58:23,090 EPOCH 2194
2024-02-09 10:58:33,851 Epoch 2194: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-09 10:58:33,851 EPOCH 2195
2024-02-09 10:58:34,389 [Epoch: 2195 Step: 00037300] Batch Recognition Loss:   0.000486 => Gls Tokens per Sec:     2384 || Batch Translation Loss:   0.023054 => Txt Tokens per Sec:     6957 || Lr: 0.000050
2024-02-09 10:58:44,682 Epoch 2195: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-09 10:58:44,682 EPOCH 2196
2024-02-09 10:58:55,745 Epoch 2196: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-09 10:58:55,745 EPOCH 2197
2024-02-09 10:59:06,407 Epoch 2197: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-09 10:59:06,408 EPOCH 2198
2024-02-09 10:59:17,416 Epoch 2198: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-09 10:59:17,417 EPOCH 2199
2024-02-09 10:59:28,601 Epoch 2199: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-09 10:59:28,602 EPOCH 2200
2024-02-09 10:59:39,541 [Epoch: 2200 Step: 00037400] Batch Recognition Loss:   0.000435 => Gls Tokens per Sec:      971 || Batch Translation Loss:   0.039674 => Txt Tokens per Sec:     2686 || Lr: 0.000050
2024-02-09 10:59:39,542 Epoch 2200: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.80 
2024-02-09 10:59:39,542 EPOCH 2201
2024-02-09 10:59:50,581 Epoch 2201: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.76 
2024-02-09 10:59:50,582 EPOCH 2202
2024-02-09 11:00:01,339 Epoch 2202: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-09 11:00:01,339 EPOCH 2203
2024-02-09 11:00:12,468 Epoch 2203: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-09 11:00:12,469 EPOCH 2204
2024-02-09 11:00:23,572 Epoch 2204: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-09 11:00:23,573 EPOCH 2205
2024-02-09 11:00:34,627 Epoch 2205: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-09 11:00:34,627 EPOCH 2206
2024-02-09 11:00:44,935 [Epoch: 2206 Step: 00037500] Batch Recognition Loss:   0.000211 => Gls Tokens per Sec:      906 || Batch Translation Loss:   0.011258 => Txt Tokens per Sec:     2588 || Lr: 0.000050
2024-02-09 11:00:45,257 Epoch 2206: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-09 11:00:45,257 EPOCH 2207
2024-02-09 11:00:55,943 Epoch 2207: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-09 11:00:55,944 EPOCH 2208
2024-02-09 11:01:06,432 Epoch 2208: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-09 11:01:06,432 EPOCH 2209
2024-02-09 11:01:17,497 Epoch 2209: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-09 11:01:17,498 EPOCH 2210
2024-02-09 11:01:29,198 Epoch 2210: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-09 11:01:29,198 EPOCH 2211
2024-02-09 11:01:40,243 Epoch 2211: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-09 11:01:40,243 EPOCH 2212
2024-02-09 11:01:47,970 [Epoch: 2212 Step: 00037600] Batch Recognition Loss:   0.000261 => Gls Tokens per Sec:     1077 || Batch Translation Loss:   0.062658 => Txt Tokens per Sec:     2984 || Lr: 0.000050
2024-02-09 11:01:51,118 Epoch 2212: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-09 11:01:51,118 EPOCH 2213
2024-02-09 11:02:02,187 Epoch 2213: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-09 11:02:02,188 EPOCH 2214
2024-02-09 11:02:13,138 Epoch 2214: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-09 11:02:13,139 EPOCH 2215
2024-02-09 11:02:24,137 Epoch 2215: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-09 11:02:24,138 EPOCH 2216
2024-02-09 11:02:34,963 Epoch 2216: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-09 11:02:34,964 EPOCH 2217
2024-02-09 11:02:45,977 Epoch 2217: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-09 11:02:45,977 EPOCH 2218
2024-02-09 11:02:53,304 [Epoch: 2218 Step: 00037700] Batch Recognition Loss:   0.000135 => Gls Tokens per Sec:      961 || Batch Translation Loss:   0.010092 => Txt Tokens per Sec:     2661 || Lr: 0.000050
2024-02-09 11:02:56,881 Epoch 2218: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 11:02:56,881 EPOCH 2219
2024-02-09 11:03:07,925 Epoch 2219: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 11:03:07,926 EPOCH 2220
2024-02-09 11:03:19,123 Epoch 2220: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-09 11:03:19,123 EPOCH 2221
2024-02-09 11:03:30,110 Epoch 2221: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-09 11:03:30,111 EPOCH 2222
2024-02-09 11:03:41,128 Epoch 2222: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-09 11:03:41,128 EPOCH 2223
2024-02-09 11:03:52,105 Epoch 2223: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-09 11:03:52,106 EPOCH 2224
2024-02-09 11:03:57,654 [Epoch: 2224 Step: 00037800] Batch Recognition Loss:   0.000198 => Gls Tokens per Sec:      991 || Batch Translation Loss:   0.007361 => Txt Tokens per Sec:     2573 || Lr: 0.000050
2024-02-09 11:04:02,925 Epoch 2224: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-09 11:04:02,925 EPOCH 2225
2024-02-09 11:04:13,938 Epoch 2225: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-09 11:04:13,939 EPOCH 2226
2024-02-09 11:04:24,921 Epoch 2226: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-09 11:04:24,922 EPOCH 2227
2024-02-09 11:04:36,054 Epoch 2227: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-09 11:04:36,055 EPOCH 2228
2024-02-09 11:04:47,231 Epoch 2228: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-09 11:04:47,232 EPOCH 2229
2024-02-09 11:04:58,311 Epoch 2229: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-09 11:04:58,311 EPOCH 2230
2024-02-09 11:05:02,116 [Epoch: 2230 Step: 00037900] Batch Recognition Loss:   0.000238 => Gls Tokens per Sec:     1109 || Batch Translation Loss:   0.024718 => Txt Tokens per Sec:     3008 || Lr: 0.000050
2024-02-09 11:05:09,323 Epoch 2230: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-09 11:05:09,323 EPOCH 2231
2024-02-09 11:05:20,441 Epoch 2231: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-09 11:05:20,442 EPOCH 2232
2024-02-09 11:05:31,575 Epoch 2232: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-09 11:05:31,575 EPOCH 2233
2024-02-09 11:05:42,558 Epoch 2233: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 11:05:42,559 EPOCH 2234
2024-02-09 11:05:53,772 Epoch 2234: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-09 11:05:53,773 EPOCH 2235
2024-02-09 11:06:04,677 Epoch 2235: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-09 11:06:04,677 EPOCH 2236
2024-02-09 11:06:07,839 [Epoch: 2236 Step: 00038000] Batch Recognition Loss:   0.000142 => Gls Tokens per Sec:      930 || Batch Translation Loss:   0.009223 => Txt Tokens per Sec:     2389 || Lr: 0.000050
2024-02-09 11:06:48,217 Validation result at epoch 2236, step    38000: duration: 40.3780s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.66129	Translation Loss: 95858.27344	PPL: 14390.70801
	Eval Metric: BLEU
	WER 2.90	(DEL: 0.00,	INS: 0.00,	SUB: 2.90)
	BLEU-4 0.43	(BLEU-1: 10.60,	BLEU-2: 3.09,	BLEU-3: 1.07,	BLEU-4: 0.43)
	CHRF 16.93	ROUGE 9.16
2024-02-09 11:06:48,218 Logging Recognition and Translation Outputs
2024-02-09 11:06:48,218 ========================================================================================================================
2024-02-09 11:06:48,218 Logging Sequence: 59_152.00
2024-02-09 11:06:48,219 	Gloss Reference :	A B+C+D+E
2024-02-09 11:06:48,219 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 11:06:48,219 	Gloss Alignment :	         
2024-02-09 11:06:48,219 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 11:06:48,220 	Text Reference  :	**** ** the ***** organisers encouraged athletes to     use    the condoms in their home  countries
2024-02-09 11:06:48,220 	Text Hypothesis :	well is the first time       both       are      always during the ******* ** t20   world cup      
2024-02-09 11:06:48,220 	Text Alignment  :	I    I      I     S          S          S        S      S          D       D  S     S     S        
2024-02-09 11:06:48,221 ========================================================================================================================
2024-02-09 11:06:48,221 Logging Sequence: 155_78.00
2024-02-09 11:06:48,221 	Gloss Reference :	A B+C+D+E
2024-02-09 11:06:48,222 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 11:06:48,222 	Gloss Alignment :	         
2024-02-09 11:06:48,222 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 11:06:48,224 	Text Reference  :	it was difficult for icc   to disqualify the   afghan team   at   the last minute so they included them    as  per    the schedule
2024-02-09 11:06:48,224 	Text Hypothesis :	if you don't     go  ahead of support    staff and    others come the **** ****** ** **** teams    playing the number of  matches 
2024-02-09 11:06:48,224 	Text Alignment  :	S  S   S         S   S     S  S          S     S      S      S        D    D      D  D    S        S       S   S      S   S       
2024-02-09 11:06:48,224 ========================================================================================================================
2024-02-09 11:06:48,225 Logging Sequence: 102_147.00
2024-02-09 11:06:48,225 	Gloss Reference :	A B+C+D+E
2024-02-09 11:06:48,225 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 11:06:48,225 	Gloss Alignment :	         
2024-02-09 11:06:48,225 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 11:06:48,227 	Text Reference  :	despite the muscle cramps this young boy      lifted such a    huge weight and    made   the country proud by securing a gold    medal   
2024-02-09 11:06:48,227 	Text Hypothesis :	as      the ****** ****** goal is    defended shami  them away 6    medal  sheuli lifted the ******* ***** ** ******** * penalty shootout
2024-02-09 11:06:48,227 	Text Alignment  :	S           D      D      S    S     S        S      S    S    S    S      S      S          D       D     D  D        D S       S       
2024-02-09 11:06:48,228 ========================================================================================================================
2024-02-09 11:06:48,228 Logging Sequence: 105_2.00
2024-02-09 11:06:48,228 	Gloss Reference :	A B+C+D+E  
2024-02-09 11:06:48,228 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-09 11:06:48,228 	Gloss Alignment :	  S        
2024-02-09 11:06:48,228 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 11:06:48,229 	Text Reference  :	***** * ********* ** ********* the airthings masters   tournament is an        online chess     tournament
2024-02-09 11:06:48,229 	Text Hypothesis :	group a consisted of australia new zealand   singapore and        13 countries of     caribbean region    
2024-02-09 11:06:48,229 	Text Alignment  :	I     I I         I  I         S   S         S         S          S  S         S      S         S         
2024-02-09 11:06:48,229 ========================================================================================================================
2024-02-09 11:06:48,230 Logging Sequence: 96_31.00
2024-02-09 11:06:48,230 	Gloss Reference :	A B+C+D+E
2024-02-09 11:06:48,230 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 11:06:48,230 	Gloss Alignment :	         
2024-02-09 11:06:48,230 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 11:06:48,231 	Text Reference  :	and then 2 teams will go on    to  play the ***** final
2024-02-09 11:06:48,231 	Text Hypothesis :	*** **** * ***** **** ** india had won  the world cup  
2024-02-09 11:06:48,231 	Text Alignment  :	D   D    D D     D    D  S     S   S        I     S    
2024-02-09 11:06:48,231 ========================================================================================================================
2024-02-09 11:06:56,017 Epoch 2236: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-09 11:06:56,017 EPOCH 2237
2024-02-09 11:07:06,802 Epoch 2237: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-09 11:07:06,802 EPOCH 2238
2024-02-09 11:07:17,935 Epoch 2238: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 11:07:17,936 EPOCH 2239
2024-02-09 11:07:28,914 Epoch 2239: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-09 11:07:28,914 EPOCH 2240
2024-02-09 11:07:39,981 Epoch 2240: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-09 11:07:39,982 EPOCH 2241
2024-02-09 11:07:51,004 Epoch 2241: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-09 11:07:51,004 EPOCH 2242
2024-02-09 11:07:53,419 [Epoch: 2242 Step: 00038100] Batch Recognition Loss:   0.000157 => Gls Tokens per Sec:      795 || Batch Translation Loss:   0.027378 => Txt Tokens per Sec:     2241 || Lr: 0.000050
2024-02-09 11:08:01,930 Epoch 2242: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-09 11:08:01,930 EPOCH 2243
2024-02-09 11:08:12,689 Epoch 2243: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-09 11:08:12,689 EPOCH 2244
2024-02-09 11:08:23,338 Epoch 2244: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-09 11:08:23,339 EPOCH 2245
2024-02-09 11:08:34,196 Epoch 2245: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-09 11:08:34,197 EPOCH 2246
2024-02-09 11:08:45,022 Epoch 2246: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-09 11:08:45,023 EPOCH 2247
2024-02-09 11:08:56,117 Epoch 2247: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-09 11:08:56,118 EPOCH 2248
2024-02-09 11:08:58,146 [Epoch: 2248 Step: 00038200] Batch Recognition Loss:   0.000543 => Gls Tokens per Sec:      316 || Batch Translation Loss:   0.033687 => Txt Tokens per Sec:     1096 || Lr: 0.000050
2024-02-09 11:09:06,851 Epoch 2248: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-09 11:09:06,851 EPOCH 2249
2024-02-09 11:09:17,664 Epoch 2249: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 11:09:17,665 EPOCH 2250
2024-02-09 11:09:28,662 Epoch 2250: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-09 11:09:28,663 EPOCH 2251
2024-02-09 11:09:39,482 Epoch 2251: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-09 11:09:39,482 EPOCH 2252
2024-02-09 11:09:50,428 Epoch 2252: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-09 11:09:50,428 EPOCH 2253
2024-02-09 11:10:01,113 [Epoch: 2253 Step: 00038300] Batch Recognition Loss:   0.000183 => Gls Tokens per Sec:      934 || Batch Translation Loss:   0.028055 => Txt Tokens per Sec:     2559 || Lr: 0.000050
2024-02-09 11:10:01,458 Epoch 2253: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-09 11:10:01,459 EPOCH 2254
2024-02-09 11:10:12,711 Epoch 2254: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-09 11:10:12,712 EPOCH 2255
2024-02-09 11:10:23,683 Epoch 2255: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-09 11:10:23,684 EPOCH 2256
2024-02-09 11:10:34,596 Epoch 2256: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.33 
2024-02-09 11:10:34,597 EPOCH 2257
2024-02-09 11:10:45,657 Epoch 2257: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-09 11:10:45,657 EPOCH 2258
2024-02-09 11:10:56,649 Epoch 2258: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.42 
2024-02-09 11:10:56,649 EPOCH 2259
2024-02-09 11:11:06,967 [Epoch: 2259 Step: 00038400] Batch Recognition Loss:   0.000594 => Gls Tokens per Sec:      843 || Batch Translation Loss:   0.046488 => Txt Tokens per Sec:     2373 || Lr: 0.000050
2024-02-09 11:11:07,651 Epoch 2259: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-09 11:11:07,651 EPOCH 2260
2024-02-09 11:11:18,509 Epoch 2260: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-09 11:11:18,509 EPOCH 2261
2024-02-09 11:11:29,393 Epoch 2261: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.55 
2024-02-09 11:11:29,394 EPOCH 2262
2024-02-09 11:11:40,271 Epoch 2262: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-09 11:11:40,272 EPOCH 2263
2024-02-09 11:11:51,307 Epoch 2263: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-09 11:11:51,308 EPOCH 2264
2024-02-09 11:12:02,473 Epoch 2264: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-09 11:12:02,473 EPOCH 2265
2024-02-09 11:12:10,060 [Epoch: 2265 Step: 00038500] Batch Recognition Loss:   0.000165 => Gls Tokens per Sec:     1013 || Batch Translation Loss:   0.021012 => Txt Tokens per Sec:     2761 || Lr: 0.000050
2024-02-09 11:12:13,709 Epoch 2265: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-09 11:12:13,710 EPOCH 2266
2024-02-09 11:12:24,856 Epoch 2266: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-09 11:12:24,856 EPOCH 2267
2024-02-09 11:12:35,818 Epoch 2267: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-09 11:12:35,819 EPOCH 2268
2024-02-09 11:12:46,797 Epoch 2268: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-09 11:12:46,798 EPOCH 2269
2024-02-09 11:12:57,729 Epoch 2269: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-09 11:12:57,729 EPOCH 2270
2024-02-09 11:13:08,518 Epoch 2270: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 11:13:08,519 EPOCH 2271
2024-02-09 11:13:11,887 [Epoch: 2271 Step: 00038600] Batch Recognition Loss:   0.000206 => Gls Tokens per Sec:     1901 || Batch Translation Loss:   0.013664 => Txt Tokens per Sec:     4987 || Lr: 0.000050
2024-02-09 11:13:19,349 Epoch 2271: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-09 11:13:19,350 EPOCH 2272
2024-02-09 11:13:30,391 Epoch 2272: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-09 11:13:30,391 EPOCH 2273
2024-02-09 11:13:41,178 Epoch 2273: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-09 11:13:41,178 EPOCH 2274
2024-02-09 11:13:52,308 Epoch 2274: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-09 11:13:52,308 EPOCH 2275
2024-02-09 11:14:03,009 Epoch 2275: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-09 11:14:03,009 EPOCH 2276
2024-02-09 11:14:13,746 Epoch 2276: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.35 
2024-02-09 11:14:13,747 EPOCH 2277
2024-02-09 11:14:18,418 [Epoch: 2277 Step: 00038700] Batch Recognition Loss:   0.000436 => Gls Tokens per Sec:     1096 || Batch Translation Loss:   0.014667 => Txt Tokens per Sec:     2955 || Lr: 0.000050
2024-02-09 11:14:24,731 Epoch 2277: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-09 11:14:24,731 EPOCH 2278
2024-02-09 11:14:35,707 Epoch 2278: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-09 11:14:35,708 EPOCH 2279
2024-02-09 11:14:46,550 Epoch 2279: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 11:14:46,551 EPOCH 2280
2024-02-09 11:14:57,775 Epoch 2280: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 11:14:57,776 EPOCH 2281
2024-02-09 11:15:08,796 Epoch 2281: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 11:15:08,797 EPOCH 2282
2024-02-09 11:15:19,746 Epoch 2282: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 11:15:19,746 EPOCH 2283
2024-02-09 11:15:22,574 [Epoch: 2283 Step: 00038800] Batch Recognition Loss:   0.000177 => Gls Tokens per Sec:     1358 || Batch Translation Loss:   0.019166 => Txt Tokens per Sec:     3638 || Lr: 0.000050
2024-02-09 11:15:30,820 Epoch 2283: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 11:15:30,820 EPOCH 2284
2024-02-09 11:15:41,817 Epoch 2284: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-09 11:15:41,817 EPOCH 2285
2024-02-09 11:15:52,875 Epoch 2285: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-09 11:15:52,876 EPOCH 2286
2024-02-09 11:16:03,869 Epoch 2286: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 11:16:03,870 EPOCH 2287
2024-02-09 11:16:14,693 Epoch 2287: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-09 11:16:14,694 EPOCH 2288
2024-02-09 11:16:25,529 Epoch 2288: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 11:16:25,530 EPOCH 2289
2024-02-09 11:16:30,262 [Epoch: 2289 Step: 00038900] Batch Recognition Loss:   0.000215 => Gls Tokens per Sec:      486 || Batch Translation Loss:   0.018004 => Txt Tokens per Sec:     1470 || Lr: 0.000050
2024-02-09 11:16:36,490 Epoch 2289: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 11:16:36,490 EPOCH 2290
2024-02-09 11:16:47,101 Epoch 2290: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 11:16:47,102 EPOCH 2291
2024-02-09 11:16:57,973 Epoch 2291: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 11:16:57,974 EPOCH 2292
2024-02-09 11:17:08,823 Epoch 2292: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 11:17:08,824 EPOCH 2293
2024-02-09 11:17:20,728 Epoch 2293: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 11:17:20,729 EPOCH 2294
2024-02-09 11:17:31,803 Epoch 2294: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 11:17:31,804 EPOCH 2295
2024-02-09 11:17:33,779 [Epoch: 2295 Step: 00039000] Batch Recognition Loss:   0.000109 => Gls Tokens per Sec:      649 || Batch Translation Loss:   0.009743 => Txt Tokens per Sec:     1781 || Lr: 0.000050
2024-02-09 11:17:42,853 Epoch 2295: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 11:17:42,854 EPOCH 2296
2024-02-09 11:17:53,745 Epoch 2296: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 11:17:53,746 EPOCH 2297
2024-02-09 11:18:04,655 Epoch 2297: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-09 11:18:04,655 EPOCH 2298
2024-02-09 11:18:15,717 Epoch 2298: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-09 11:18:15,718 EPOCH 2299
2024-02-09 11:18:26,614 Epoch 2299: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 11:18:26,615 EPOCH 2300
2024-02-09 11:18:37,570 [Epoch: 2300 Step: 00039100] Batch Recognition Loss:   0.000118 => Gls Tokens per Sec:      970 || Batch Translation Loss:   0.009067 => Txt Tokens per Sec:     2682 || Lr: 0.000050
2024-02-09 11:18:37,571 Epoch 2300: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-09 11:18:37,571 EPOCH 2301
2024-02-09 11:18:48,233 Epoch 2301: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-09 11:18:48,234 EPOCH 2302
2024-02-09 11:18:59,062 Epoch 2302: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-09 11:18:59,062 EPOCH 2303
2024-02-09 11:19:10,070 Epoch 2303: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 11:19:10,070 EPOCH 2304
2024-02-09 11:19:20,901 Epoch 2304: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-09 11:19:20,902 EPOCH 2305
2024-02-09 11:19:31,927 Epoch 2305: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-09 11:19:31,927 EPOCH 2306
2024-02-09 11:19:40,807 [Epoch: 2306 Step: 00039200] Batch Recognition Loss:   0.000230 => Gls Tokens per Sec:     1052 || Batch Translation Loss:   0.012008 => Txt Tokens per Sec:     2860 || Lr: 0.000050
2024-02-09 11:19:42,735 Epoch 2306: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-09 11:19:42,735 EPOCH 2307
2024-02-09 11:19:53,624 Epoch 2307: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 11:19:53,625 EPOCH 2308
2024-02-09 11:20:04,684 Epoch 2308: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 11:20:04,684 EPOCH 2309
2024-02-09 11:20:15,325 Epoch 2309: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 11:20:15,326 EPOCH 2310
2024-02-09 11:20:26,122 Epoch 2310: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-09 11:20:26,123 EPOCH 2311
2024-02-09 11:20:36,974 Epoch 2311: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-09 11:20:36,975 EPOCH 2312
2024-02-09 11:20:44,567 [Epoch: 2312 Step: 00039300] Batch Recognition Loss:   0.000239 => Gls Tokens per Sec:     1096 || Batch Translation Loss:   0.022867 => Txt Tokens per Sec:     2953 || Lr: 0.000050
2024-02-09 11:20:47,981 Epoch 2312: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 11:20:47,982 EPOCH 2313
2024-02-09 11:20:58,709 Epoch 2313: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 11:20:58,710 EPOCH 2314
2024-02-09 11:21:09,686 Epoch 2314: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-09 11:21:09,686 EPOCH 2315
2024-02-09 11:21:20,695 Epoch 2315: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.48 
2024-02-09 11:21:20,696 EPOCH 2316
2024-02-09 11:21:31,667 Epoch 2316: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-09 11:21:31,668 EPOCH 2317
2024-02-09 11:21:42,712 Epoch 2317: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-09 11:21:42,712 EPOCH 2318
2024-02-09 11:21:48,283 [Epoch: 2318 Step: 00039400] Batch Recognition Loss:   0.000265 => Gls Tokens per Sec:     1264 || Batch Translation Loss:   0.026701 => Txt Tokens per Sec:     3440 || Lr: 0.000050
2024-02-09 11:21:53,772 Epoch 2318: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-09 11:21:53,772 EPOCH 2319
2024-02-09 11:22:04,869 Epoch 2319: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-09 11:22:04,870 EPOCH 2320
2024-02-09 11:22:15,536 Epoch 2320: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-09 11:22:15,537 EPOCH 2321
2024-02-09 11:22:26,674 Epoch 2321: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-09 11:22:26,675 EPOCH 2322
2024-02-09 11:22:37,513 Epoch 2322: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.75 
2024-02-09 11:22:37,514 EPOCH 2323
2024-02-09 11:22:48,485 Epoch 2323: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.90 
2024-02-09 11:22:48,485 EPOCH 2324
2024-02-09 11:22:54,454 [Epoch: 2324 Step: 00039500] Batch Recognition Loss:   0.000655 => Gls Tokens per Sec:      922 || Batch Translation Loss:   0.035315 => Txt Tokens per Sec:     2425 || Lr: 0.000050
2024-02-09 11:22:59,312 Epoch 2324: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.99 
2024-02-09 11:22:59,312 EPOCH 2325
2024-02-09 11:23:10,140 Epoch 2325: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.26 
2024-02-09 11:23:10,141 EPOCH 2326
2024-02-09 11:23:20,892 Epoch 2326: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.15 
2024-02-09 11:23:20,892 EPOCH 2327
2024-02-09 11:23:31,621 Epoch 2327: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.86 
2024-02-09 11:23:31,622 EPOCH 2328
2024-02-09 11:23:42,513 Epoch 2328: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.72 
2024-02-09 11:23:42,514 EPOCH 2329
2024-02-09 11:23:53,495 Epoch 2329: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-09 11:23:53,496 EPOCH 2330
2024-02-09 11:23:59,877 [Epoch: 2330 Step: 00039600] Batch Recognition Loss:   0.000177 => Gls Tokens per Sec:      702 || Batch Translation Loss:   0.037184 => Txt Tokens per Sec:     1956 || Lr: 0.000050
2024-02-09 11:24:04,469 Epoch 2330: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-09 11:24:04,470 EPOCH 2331
2024-02-09 11:24:15,152 Epoch 2331: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-09 11:24:15,153 EPOCH 2332
2024-02-09 11:24:26,094 Epoch 2332: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-09 11:24:26,094 EPOCH 2333
2024-02-09 11:24:37,017 Epoch 2333: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-09 11:24:37,018 EPOCH 2334
2024-02-09 11:24:47,686 Epoch 2334: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-09 11:24:47,686 EPOCH 2335
2024-02-09 11:24:58,508 Epoch 2335: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-09 11:24:58,508 EPOCH 2336
2024-02-09 11:25:01,584 [Epoch: 2336 Step: 00039700] Batch Recognition Loss:   0.002453 => Gls Tokens per Sec:     1041 || Batch Translation Loss:   0.011907 => Txt Tokens per Sec:     2943 || Lr: 0.000050
2024-02-09 11:25:09,456 Epoch 2336: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-09 11:25:09,457 EPOCH 2337
2024-02-09 11:25:20,533 Epoch 2337: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 11:25:20,534 EPOCH 2338
2024-02-09 11:25:31,220 Epoch 2338: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 11:25:31,220 EPOCH 2339
2024-02-09 11:25:41,715 Epoch 2339: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 11:25:41,715 EPOCH 2340
2024-02-09 11:25:52,686 Epoch 2340: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-09 11:25:52,686 EPOCH 2341
2024-02-09 11:26:03,770 Epoch 2341: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-09 11:26:03,771 EPOCH 2342
2024-02-09 11:26:04,242 [Epoch: 2342 Step: 00039800] Batch Recognition Loss:   0.000154 => Gls Tokens per Sec:     4085 || Batch Translation Loss:   0.012035 => Txt Tokens per Sec:     9051 || Lr: 0.000050
2024-02-09 11:26:14,519 Epoch 2342: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 11:26:14,519 EPOCH 2343
2024-02-09 11:26:25,516 Epoch 2343: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 11:26:25,517 EPOCH 2344
2024-02-09 11:26:36,449 Epoch 2344: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-09 11:26:36,450 EPOCH 2345
2024-02-09 11:26:47,329 Epoch 2345: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-09 11:26:47,329 EPOCH 2346
2024-02-09 11:26:58,133 Epoch 2346: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-09 11:26:58,134 EPOCH 2347
2024-02-09 11:27:09,087 Epoch 2347: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 11:27:09,087 EPOCH 2348
2024-02-09 11:27:09,465 [Epoch: 2348 Step: 00039900] Batch Recognition Loss:   0.000250 => Gls Tokens per Sec:     1702 || Batch Translation Loss:   0.014507 => Txt Tokens per Sec:     5412 || Lr: 0.000050
2024-02-09 11:27:19,939 Epoch 2348: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 11:27:19,940 EPOCH 2349
2024-02-09 11:27:30,901 Epoch 2349: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 11:27:30,901 EPOCH 2350
2024-02-09 11:27:41,663 Epoch 2350: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 11:27:41,664 EPOCH 2351
2024-02-09 11:27:52,358 Epoch 2351: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-09 11:27:52,359 EPOCH 2352
2024-02-09 11:28:04,279 Epoch 2352: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 11:28:04,280 EPOCH 2353
2024-02-09 11:28:15,286 [Epoch: 2353 Step: 00040000] Batch Recognition Loss:   0.000143 => Gls Tokens per Sec:      907 || Batch Translation Loss:   0.019344 => Txt Tokens per Sec:     2514 || Lr: 0.000050
2024-02-09 11:28:56,190 Validation result at epoch 2353, step    40000: duration: 40.9030s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.66600	Translation Loss: 96632.87500	PPL: 15548.27148
	Eval Metric: BLEU
	WER 2.90	(DEL: 0.00,	INS: 0.00,	SUB: 2.90)
	BLEU-4 0.49	(BLEU-1: 11.09,	BLEU-2: 3.17,	BLEU-3: 1.12,	BLEU-4: 0.49)
	CHRF 17.29	ROUGE 9.09
2024-02-09 11:28:56,191 Logging Recognition and Translation Outputs
2024-02-09 11:28:56,191 ========================================================================================================================
2024-02-09 11:28:56,191 Logging Sequence: 86_84.00
2024-02-09 11:28:56,192 	Gloss Reference :	A B+C+D+E
2024-02-09 11:28:56,192 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 11:28:56,192 	Gloss Alignment :	         
2024-02-09 11:28:56,192 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 11:28:56,193 	Text Reference  :	amassing 8933 runs which included 21 centuries with a   highest score of  201    not out 
2024-02-09 11:28:56,193 	Text Hypothesis :	******** **** **** ***** ******** an indian    team was run     by    the scored 260 runs
2024-02-09 11:28:56,193 	Text Alignment  :	D        D    D    D     D        S  S         S    S   S       S     S   S      S   S   
2024-02-09 11:28:56,193 ========================================================================================================================
2024-02-09 11:28:56,194 Logging Sequence: 179_110.00
2024-02-09 11:28:56,194 	Gloss Reference :	A B+C+D+E
2024-02-09 11:28:56,194 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 11:28:56,194 	Gloss Alignment :	         
2024-02-09 11:28:56,194 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 11:28:56,196 	Text Reference  :	**** *** **** ***** phogat refused to    stay     in *** **** the ******* ***** *** **** same      room with other   indian female wrestlers
2024-02-09 11:28:56,196 	Text Hypothesis :	then the gold medal tally  in      tokyo olympics in one over the singlet which had been sponsored by   a    company called shiv   naresh   
2024-02-09 11:28:56,196 	Text Alignment  :	I    I   I    I     S      S       S     S           I   I        I       I     I   I    S         S    S    S       S      S      S        
2024-02-09 11:28:56,196 ========================================================================================================================
2024-02-09 11:28:56,196 Logging Sequence: 102_2.00
2024-02-09 11:28:56,197 	Gloss Reference :	A B+C+D+E    
2024-02-09 11:28:56,197 	Gloss Hypothesis:	A B+C+D+E+D+E
2024-02-09 11:28:56,197 	Gloss Alignment :	  S          
2024-02-09 11:28:56,197 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 11:28:56,198 	Text Reference  :	commonwealth games are among the world's most  recognised gaming championships after  the     olympics
2024-02-09 11:28:56,198 	Text Hypothesis :	************ ***** *** ***** 4   anyone  found this       for    test          series against england 
2024-02-09 11:28:56,198 	Text Alignment  :	D            D     D   D     S   S       S     S          S      S             S      S       S       
2024-02-09 11:28:56,198 ========================================================================================================================
2024-02-09 11:28:56,199 Logging Sequence: 60_195.00
2024-02-09 11:28:56,199 	Gloss Reference :	A B+C+D+E
2024-02-09 11:28:56,199 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 11:28:56,199 	Gloss Alignment :	         
2024-02-09 11:28:56,199 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 11:28:56,200 	Text Reference  :	** **** *** **** ******* *** ** ***** people loved to      watch his aggressive expressions and his  bowling
2024-02-09 11:28:56,200 	Text Hypothesis :	as bcci has been planned say if there are    also  present at    the same       with        the same thing  
2024-02-09 11:28:56,201 	Text Alignment  :	I  I    I   I    I       I   I  I     S      S     S       S     S   S          S           S   S    S      
2024-02-09 11:28:56,201 ========================================================================================================================
2024-02-09 11:28:56,201 Logging Sequence: 70_200.00
2024-02-09 11:28:56,201 	Gloss Reference :	A B+C+D+E
2024-02-09 11:28:56,201 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 11:28:56,201 	Gloss Alignment :	         
2024-02-09 11:28:56,201 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 11:28:56,202 	Text Reference  :	***** *** * ******* ******* ** showing ronaldo whole-heartedly endorsing the ***** brand
2024-02-09 11:28:56,202 	Text Hypothesis :	about the 4 matches started in 15      months  has             won       the world cup  
2024-02-09 11:28:56,202 	Text Alignment  :	I     I   I I       I       I  S       S       S               S             I     S    
2024-02-09 11:28:56,202 ========================================================================================================================
2024-02-09 11:28:56,512 Epoch 2353: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 11:28:56,512 EPOCH 2354
2024-02-09 11:29:07,526 Epoch 2354: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 11:29:07,526 EPOCH 2355
2024-02-09 11:29:18,369 Epoch 2355: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-09 11:29:18,370 EPOCH 2356
2024-02-09 11:29:29,302 Epoch 2356: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 11:29:29,302 EPOCH 2357
2024-02-09 11:29:40,210 Epoch 2357: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-09 11:29:40,211 EPOCH 2358
2024-02-09 11:29:50,682 Epoch 2358: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-09 11:29:50,682 EPOCH 2359
2024-02-09 11:30:00,768 [Epoch: 2359 Step: 00040100] Batch Recognition Loss:   0.000170 => Gls Tokens per Sec:      863 || Batch Translation Loss:   0.014907 => Txt Tokens per Sec:     2371 || Lr: 0.000050
2024-02-09 11:30:01,613 Epoch 2359: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-09 11:30:01,613 EPOCH 2360
2024-02-09 11:30:12,604 Epoch 2360: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-09 11:30:12,604 EPOCH 2361
2024-02-09 11:30:23,359 Epoch 2361: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-09 11:30:23,359 EPOCH 2362
2024-02-09 11:30:34,453 Epoch 2362: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-09 11:30:34,454 EPOCH 2363
2024-02-09 11:30:45,282 Epoch 2363: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-09 11:30:45,282 EPOCH 2364
2024-02-09 11:30:56,379 Epoch 2364: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-09 11:30:56,380 EPOCH 2365
2024-02-09 11:31:02,029 [Epoch: 2365 Step: 00040200] Batch Recognition Loss:   0.000226 => Gls Tokens per Sec:     1360 || Batch Translation Loss:   0.022993 => Txt Tokens per Sec:     3625 || Lr: 0.000050
2024-02-09 11:31:07,474 Epoch 2365: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-09 11:31:07,475 EPOCH 2366
2024-02-09 11:31:18,676 Epoch 2366: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 11:31:18,677 EPOCH 2367
2024-02-09 11:31:29,746 Epoch 2367: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 11:31:29,746 EPOCH 2368
2024-02-09 11:31:40,683 Epoch 2368: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-09 11:31:40,684 EPOCH 2369
2024-02-09 11:31:51,407 Epoch 2369: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 11:31:51,407 EPOCH 2370
2024-02-09 11:32:02,267 Epoch 2370: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-09 11:32:02,267 EPOCH 2371
2024-02-09 11:32:05,860 [Epoch: 2371 Step: 00040300] Batch Recognition Loss:   0.000155 => Gls Tokens per Sec:     1782 || Batch Translation Loss:   0.012245 => Txt Tokens per Sec:     4502 || Lr: 0.000050
2024-02-09 11:32:12,954 Epoch 2371: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 11:32:12,954 EPOCH 2372
2024-02-09 11:32:24,084 Epoch 2372: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-09 11:32:24,084 EPOCH 2373
2024-02-09 11:32:34,955 Epoch 2373: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-09 11:32:34,956 EPOCH 2374
2024-02-09 11:32:45,794 Epoch 2374: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-09 11:32:45,795 EPOCH 2375
2024-02-09 11:32:56,801 Epoch 2375: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-09 11:32:56,801 EPOCH 2376
2024-02-09 11:33:07,486 Epoch 2376: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-09 11:33:07,486 EPOCH 2377
2024-02-09 11:33:12,767 [Epoch: 2377 Step: 00040400] Batch Recognition Loss:   0.000225 => Gls Tokens per Sec:      920 || Batch Translation Loss:   0.021322 => Txt Tokens per Sec:     2623 || Lr: 0.000050
2024-02-09 11:33:17,417 Epoch 2377: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-09 11:33:17,417 EPOCH 2378
2024-02-09 11:33:28,000 Epoch 2378: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-09 11:33:28,001 EPOCH 2379
2024-02-09 11:33:38,822 Epoch 2379: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-09 11:33:38,822 EPOCH 2380
2024-02-09 11:33:49,621 Epoch 2380: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-09 11:33:49,621 EPOCH 2381
2024-02-09 11:34:00,492 Epoch 2381: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-09 11:34:00,493 EPOCH 2382
2024-02-09 11:34:11,154 Epoch 2382: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-09 11:34:11,154 EPOCH 2383
2024-02-09 11:34:14,014 [Epoch: 2383 Step: 00040500] Batch Recognition Loss:   0.000228 => Gls Tokens per Sec:     1343 || Batch Translation Loss:   0.056767 => Txt Tokens per Sec:     3807 || Lr: 0.000050
2024-02-09 11:34:21,994 Epoch 2383: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-09 11:34:21,995 EPOCH 2384
2024-02-09 11:34:32,927 Epoch 2384: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-09 11:34:32,927 EPOCH 2385
2024-02-09 11:34:43,630 Epoch 2385: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-09 11:34:43,631 EPOCH 2386
2024-02-09 11:34:54,623 Epoch 2386: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-09 11:34:54,624 EPOCH 2387
2024-02-09 11:35:05,419 Epoch 2387: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.73 
2024-02-09 11:35:05,420 EPOCH 2388
2024-02-09 11:35:16,421 Epoch 2388: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-09 11:35:16,422 EPOCH 2389
2024-02-09 11:35:20,521 [Epoch: 2389 Step: 00040600] Batch Recognition Loss:   0.000703 => Gls Tokens per Sec:      625 || Batch Translation Loss:   0.034080 => Txt Tokens per Sec:     1988 || Lr: 0.000050
2024-02-09 11:35:27,337 Epoch 2389: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-09 11:35:27,338 EPOCH 2390
2024-02-09 11:35:38,184 Epoch 2390: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-09 11:35:38,185 EPOCH 2391
2024-02-09 11:35:49,209 Epoch 2391: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-09 11:35:49,210 EPOCH 2392
2024-02-09 11:36:00,278 Epoch 2392: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-09 11:36:00,279 EPOCH 2393
2024-02-09 11:36:11,298 Epoch 2393: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-09 11:36:11,299 EPOCH 2394
2024-02-09 11:36:22,328 Epoch 2394: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-09 11:36:22,329 EPOCH 2395
2024-02-09 11:36:24,997 [Epoch: 2395 Step: 00040700] Batch Recognition Loss:   0.000184 => Gls Tokens per Sec:      383 || Batch Translation Loss:   0.008032 => Txt Tokens per Sec:      884 || Lr: 0.000050
2024-02-09 11:36:33,479 Epoch 2395: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 11:36:33,480 EPOCH 2396
2024-02-09 11:36:44,394 Epoch 2396: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 11:36:44,395 EPOCH 2397
2024-02-09 11:36:55,399 Epoch 2397: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 11:36:55,400 EPOCH 2398
2024-02-09 11:37:06,697 Epoch 2398: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 11:37:06,698 EPOCH 2399
2024-02-09 11:37:17,645 Epoch 2399: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-09 11:37:17,645 EPOCH 2400
2024-02-09 11:37:28,404 [Epoch: 2400 Step: 00040800] Batch Recognition Loss:   0.000161 => Gls Tokens per Sec:      987 || Batch Translation Loss:   0.017745 => Txt Tokens per Sec:     2731 || Lr: 0.000050
2024-02-09 11:37:28,404 Epoch 2400: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-09 11:37:28,405 EPOCH 2401
2024-02-09 11:37:39,202 Epoch 2401: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 11:37:39,202 EPOCH 2402
2024-02-09 11:37:49,974 Epoch 2402: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 11:37:49,975 EPOCH 2403
2024-02-09 11:38:01,048 Epoch 2403: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-09 11:38:01,048 EPOCH 2404
2024-02-09 11:38:11,965 Epoch 2404: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 11:38:11,966 EPOCH 2405
2024-02-09 11:38:22,763 Epoch 2405: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 11:38:22,764 EPOCH 2406
2024-02-09 11:38:30,951 [Epoch: 2406 Step: 00040900] Batch Recognition Loss:   0.000167 => Gls Tokens per Sec:     1173 || Batch Translation Loss:   0.024227 => Txt Tokens per Sec:     3213 || Lr: 0.000050
2024-02-09 11:38:33,642 Epoch 2406: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-09 11:38:33,642 EPOCH 2407
2024-02-09 11:38:44,503 Epoch 2407: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-09 11:38:44,503 EPOCH 2408
2024-02-09 11:38:55,506 Epoch 2408: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-09 11:38:55,506 EPOCH 2409
2024-02-09 11:39:06,373 Epoch 2409: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-09 11:39:06,374 EPOCH 2410
2024-02-09 11:39:17,272 Epoch 2410: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-09 11:39:17,273 EPOCH 2411
2024-02-09 11:39:28,079 Epoch 2411: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-09 11:39:28,080 EPOCH 2412
2024-02-09 11:39:36,537 [Epoch: 2412 Step: 00041000] Batch Recognition Loss:   0.000189 => Gls Tokens per Sec:      953 || Batch Translation Loss:   0.039479 => Txt Tokens per Sec:     2668 || Lr: 0.000050
2024-02-09 11:39:38,754 Epoch 2412: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-09 11:39:38,754 EPOCH 2413
2024-02-09 11:39:49,434 Epoch 2413: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-09 11:39:49,435 EPOCH 2414
2024-02-09 11:40:00,296 Epoch 2414: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-09 11:40:00,297 EPOCH 2415
2024-02-09 11:40:11,186 Epoch 2415: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-09 11:40:11,186 EPOCH 2416
2024-02-09 11:40:22,225 Epoch 2416: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-09 11:40:22,226 EPOCH 2417
2024-02-09 11:40:33,106 Epoch 2417: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-09 11:40:33,107 EPOCH 2418
2024-02-09 11:40:42,648 [Epoch: 2418 Step: 00041100] Batch Recognition Loss:   0.000268 => Gls Tokens per Sec:      711 || Batch Translation Loss:   0.019317 => Txt Tokens per Sec:     2036 || Lr: 0.000050
2024-02-09 11:40:43,952 Epoch 2418: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-09 11:40:43,953 EPOCH 2419
2024-02-09 11:40:54,666 Epoch 2419: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-09 11:40:54,666 EPOCH 2420
2024-02-09 11:41:05,489 Epoch 2420: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-09 11:41:05,490 EPOCH 2421
2024-02-09 11:41:16,346 Epoch 2421: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-09 11:41:16,347 EPOCH 2422
2024-02-09 11:41:27,332 Epoch 2422: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-09 11:41:27,332 EPOCH 2423
2024-02-09 11:41:38,297 Epoch 2423: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-09 11:41:38,298 EPOCH 2424
2024-02-09 11:41:44,801 [Epoch: 2424 Step: 00041200] Batch Recognition Loss:   0.000210 => Gls Tokens per Sec:      886 || Batch Translation Loss:   0.009028 => Txt Tokens per Sec:     2529 || Lr: 0.000050
2024-02-09 11:41:48,888 Epoch 2424: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 11:41:48,889 EPOCH 2425
2024-02-09 11:41:59,772 Epoch 2425: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-09 11:41:59,773 EPOCH 2426
2024-02-09 11:42:10,783 Epoch 2426: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 11:42:10,784 EPOCH 2427
2024-02-09 11:42:21,656 Epoch 2427: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-09 11:42:21,657 EPOCH 2428
2024-02-09 11:42:32,882 Epoch 2428: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-09 11:42:32,883 EPOCH 2429
2024-02-09 11:42:43,829 Epoch 2429: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-09 11:42:43,830 EPOCH 2430
2024-02-09 11:42:49,206 [Epoch: 2430 Step: 00041300] Batch Recognition Loss:   0.001494 => Gls Tokens per Sec:      833 || Batch Translation Loss:   0.017545 => Txt Tokens per Sec:     2376 || Lr: 0.000050
2024-02-09 11:42:55,153 Epoch 2430: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 11:42:55,153 EPOCH 2431
2024-02-09 11:43:06,029 Epoch 2431: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 11:43:06,030 EPOCH 2432
2024-02-09 11:43:16,289 Epoch 2432: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-09 11:43:16,290 EPOCH 2433
2024-02-09 11:43:27,311 Epoch 2433: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-09 11:43:27,312 EPOCH 2434
2024-02-09 11:43:38,139 Epoch 2434: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-09 11:43:38,139 EPOCH 2435
2024-02-09 11:43:48,943 Epoch 2435: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 11:43:48,944 EPOCH 2436
2024-02-09 11:43:49,710 [Epoch: 2436 Step: 00041400] Batch Recognition Loss:   0.000123 => Gls Tokens per Sec:     4183 || Batch Translation Loss:   0.014238 => Txt Tokens per Sec:     9982 || Lr: 0.000050
2024-02-09 11:43:59,626 Epoch 2436: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 11:43:59,627 EPOCH 2437
2024-02-09 11:44:10,556 Epoch 2437: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.26 
2024-02-09 11:44:10,557 EPOCH 2438
2024-02-09 11:44:21,518 Epoch 2438: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.24 
2024-02-09 11:44:21,519 EPOCH 2439
2024-02-09 11:44:32,044 Epoch 2439: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.23 
2024-02-09 11:44:32,044 EPOCH 2440
2024-02-09 11:44:42,782 Epoch 2440: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.23 
2024-02-09 11:44:42,782 EPOCH 2441
2024-02-09 11:44:53,600 Epoch 2441: Total Training Recognition Loss 0.10  Total Training Translation Loss 0.23 
2024-02-09 11:44:53,601 EPOCH 2442
2024-02-09 11:44:57,675 [Epoch: 2442 Step: 00041500] Batch Recognition Loss:   0.000199 => Gls Tokens per Sec:      471 || Batch Translation Loss:   0.013139 => Txt Tokens per Sec:     1542 || Lr: 0.000050
2024-02-09 11:45:04,638 Epoch 2442: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.23 
2024-02-09 11:45:04,639 EPOCH 2443
2024-02-09 11:45:15,697 Epoch 2443: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.26 
2024-02-09 11:45:15,697 EPOCH 2444
2024-02-09 11:45:26,636 Epoch 2444: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.24 
2024-02-09 11:45:26,637 EPOCH 2445
2024-02-09 11:45:37,974 Epoch 2445: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 11:45:37,975 EPOCH 2446
2024-02-09 11:45:49,012 Epoch 2446: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 11:45:49,013 EPOCH 2447
2024-02-09 11:46:00,128 Epoch 2447: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 11:46:00,129 EPOCH 2448
2024-02-09 11:46:00,413 [Epoch: 2448 Step: 00041600] Batch Recognition Loss:   0.000149 => Gls Tokens per Sec:     2270 || Batch Translation Loss:   0.013115 => Txt Tokens per Sec:     6713 || Lr: 0.000050
2024-02-09 11:46:11,132 Epoch 2448: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.26 
2024-02-09 11:46:11,133 EPOCH 2449
2024-02-09 11:46:22,101 Epoch 2449: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-09 11:46:22,102 EPOCH 2450
2024-02-09 11:46:33,083 Epoch 2450: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 11:46:33,084 EPOCH 2451
2024-02-09 11:46:44,047 Epoch 2451: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 11:46:44,047 EPOCH 2452
2024-02-09 11:46:54,533 Epoch 2452: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-09 11:46:54,534 EPOCH 2453
2024-02-09 11:47:03,482 [Epoch: 2453 Step: 00041700] Batch Recognition Loss:   0.000510 => Gls Tokens per Sec:     1116 || Batch Translation Loss:   0.053220 => Txt Tokens per Sec:     3051 || Lr: 0.000050
2024-02-09 11:47:05,396 Epoch 2453: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-09 11:47:05,396 EPOCH 2454
2024-02-09 11:47:16,392 Epoch 2454: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-09 11:47:16,393 EPOCH 2455
2024-02-09 11:47:27,143 Epoch 2455: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.14 
2024-02-09 11:47:27,143 EPOCH 2456
2024-02-09 11:47:38,214 Epoch 2456: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.66 
2024-02-09 11:47:38,215 EPOCH 2457
2024-02-09 11:47:49,380 Epoch 2457: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.14 
2024-02-09 11:47:49,380 EPOCH 2458
2024-02-09 11:48:00,278 Epoch 2458: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.96 
2024-02-09 11:48:00,278 EPOCH 2459
2024-02-09 11:48:10,597 [Epoch: 2459 Step: 00041800] Batch Recognition Loss:   0.000823 => Gls Tokens per Sec:      843 || Batch Translation Loss:   0.041973 => Txt Tokens per Sec:     2372 || Lr: 0.000050
2024-02-09 11:48:11,323 Epoch 2459: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.80 
2024-02-09 11:48:11,323 EPOCH 2460
2024-02-09 11:48:22,217 Epoch 2460: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-09 11:48:22,218 EPOCH 2461
2024-02-09 11:48:32,977 Epoch 2461: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-09 11:48:32,977 EPOCH 2462
2024-02-09 11:48:43,772 Epoch 2462: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-09 11:48:43,773 EPOCH 2463
2024-02-09 11:48:54,594 Epoch 2463: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-09 11:48:54,594 EPOCH 2464
2024-02-09 11:49:05,620 Epoch 2464: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-09 11:49:05,621 EPOCH 2465
2024-02-09 11:49:11,230 [Epoch: 2465 Step: 00041900] Batch Recognition Loss:   0.000244 => Gls Tokens per Sec:     1370 || Batch Translation Loss:   0.021941 => Txt Tokens per Sec:     3656 || Lr: 0.000050
2024-02-09 11:49:16,460 Epoch 2465: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-09 11:49:16,460 EPOCH 2466
2024-02-09 11:49:27,549 Epoch 2466: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-09 11:49:27,550 EPOCH 2467
2024-02-09 11:49:38,445 Epoch 2467: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 11:49:38,446 EPOCH 2468
2024-02-09 11:49:49,473 Epoch 2468: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 11:49:49,474 EPOCH 2469
2024-02-09 11:50:00,618 Epoch 2469: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 11:50:00,619 EPOCH 2470
2024-02-09 11:50:11,577 Epoch 2470: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-09 11:50:11,577 EPOCH 2471
2024-02-09 11:50:16,850 [Epoch: 2471 Step: 00042000] Batch Recognition Loss:   0.000227 => Gls Tokens per Sec:     1214 || Batch Translation Loss:   0.019552 => Txt Tokens per Sec:     3508 || Lr: 0.000050
2024-02-09 11:50:56,751 Validation result at epoch 2471, step    42000: duration: 39.9017s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.62329	Translation Loss: 98780.53906	PPL: 19268.24609
	Eval Metric: BLEU
	WER 2.82	(DEL: 0.00,	INS: 0.00,	SUB: 2.82)
	BLEU-4 0.00	(BLEU-1: 10.10,	BLEU-2: 2.95,	BLEU-3: 1.00,	BLEU-4: 0.00)
	CHRF 16.67	ROUGE 8.61
2024-02-09 11:50:56,752 Logging Recognition and Translation Outputs
2024-02-09 11:50:56,752 ========================================================================================================================
2024-02-09 11:50:56,753 Logging Sequence: 154_94.00
2024-02-09 11:50:56,753 	Gloss Reference :	A B+C+D+E
2024-02-09 11:50:56,753 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 11:50:56,753 	Gloss Alignment :	         
2024-02-09 11:50:56,753 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 11:50:56,755 	Text Reference  :	*** the ipl will  also be  held in   uae   from september 19 to  october   15     
2024-02-09 11:50:56,755 	Text Hypothesis :	for the *** first time the high room rates are  because   of his religious beliefs
2024-02-09 11:50:56,755 	Text Alignment  :	I       D   S     S    S   S    S    S     S    S         S  S   S         S      
2024-02-09 11:50:56,755 ========================================================================================================================
2024-02-09 11:50:56,755 Logging Sequence: 118_2.00
2024-02-09 11:50:56,755 	Gloss Reference :	A B+C+D+E
2024-02-09 11:50:56,756 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 11:50:56,756 	Gloss Alignment :	         
2024-02-09 11:50:56,756 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 11:50:56,757 	Text Reference  :	yesterday was a very exciting day   people across   the world were watching 
2024-02-09 11:50:56,757 	Text Hypothesis :	********* *** * **** the      video has    garnered a   huge  fan  following
2024-02-09 11:50:56,757 	Text Alignment  :	D         D   D D    S        S     S      S        S   S     S    S        
2024-02-09 11:50:56,757 ========================================================================================================================
2024-02-09 11:50:56,757 Logging Sequence: 165_453.00
2024-02-09 11:50:56,757 	Gloss Reference :	A B+C+D+E
2024-02-09 11:50:56,757 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 11:50:56,758 	Gloss Alignment :	         
2024-02-09 11:50:56,758 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 11:50:56,758 	Text Reference  :	icc did not  agree to sehwag' decision of wearing a numberless jersey 
2024-02-09 11:50:56,758 	Text Hypothesis :	*** he  came out   to ******* ******** ** ******* * retire     someday
2024-02-09 11:50:56,759 	Text Alignment  :	D   S   S    S        D       D        D  D       D S          S      
2024-02-09 11:50:56,759 ========================================================================================================================
2024-02-09 11:50:56,759 Logging Sequence: 126_163.00
2024-02-09 11:50:56,759 	Gloss Reference :	A B+C+D+E
2024-02-09 11:50:56,759 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 11:50:56,759 	Gloss Alignment :	         
2024-02-09 11:50:56,759 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 11:50:56,760 	Text Reference  :	* your hard work  has helped secure a   medal at  the        tokyo      olympics
2024-02-09 11:50:56,761 	Text Hypothesis :	i am   very grate to  my     fans   and media for constantly supporting me      
2024-02-09 11:50:56,761 	Text Alignment  :	I S    S    S     S   S      S      S   S     S   S          S          S       
2024-02-09 11:50:56,761 ========================================================================================================================
2024-02-09 11:50:56,761 Logging Sequence: 84_2.00
2024-02-09 11:50:56,761 	Gloss Reference :	A B+C+D+E
2024-02-09 11:50:56,761 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 11:50:56,761 	Gloss Alignment :	         
2024-02-09 11:50:56,762 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 11:50:56,763 	Text Reference  :	the 2022 fifa football world         cup  is  going on    in qatar from 20th november 2022 to 18th   december 2022 
2024-02-09 11:50:56,763 	Text Hypothesis :	the **** **** sports   personalities like our 5     times in qatar from **** ******** **** ** around the      world
2024-02-09 11:50:56,763 	Text Alignment  :	    D    D    S        S             S    S   S     S                   D    D        D    D  S      S        S    
2024-02-09 11:50:56,764 ========================================================================================================================
2024-02-09 11:51:02,019 Epoch 2471: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-09 11:51:02,019 EPOCH 2472
2024-02-09 11:51:12,927 Epoch 2472: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-09 11:51:12,927 EPOCH 2473
2024-02-09 11:51:23,727 Epoch 2473: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-09 11:51:23,727 EPOCH 2474
2024-02-09 11:51:34,612 Epoch 2474: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 11:51:34,613 EPOCH 2475
2024-02-09 11:51:45,662 Epoch 2475: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 11:51:45,663 EPOCH 2476
2024-02-09 11:51:56,564 Epoch 2476: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-09 11:51:56,564 EPOCH 2477
2024-02-09 11:52:02,300 [Epoch: 2477 Step: 00042100] Batch Recognition Loss:   0.000168 => Gls Tokens per Sec:      848 || Batch Translation Loss:   0.016558 => Txt Tokens per Sec:     2321 || Lr: 0.000050
2024-02-09 11:52:07,736 Epoch 2477: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-09 11:52:07,736 EPOCH 2478
2024-02-09 11:52:18,553 Epoch 2478: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 11:52:18,553 EPOCH 2479
2024-02-09 11:52:29,277 Epoch 2479: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 11:52:29,278 EPOCH 2480
2024-02-09 11:52:40,154 Epoch 2480: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-09 11:52:40,155 EPOCH 2481
2024-02-09 11:52:51,233 Epoch 2481: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-09 11:52:51,234 EPOCH 2482
2024-02-09 11:53:02,158 Epoch 2482: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-09 11:53:02,159 EPOCH 2483
2024-02-09 11:53:06,145 [Epoch: 2483 Step: 00042200] Batch Recognition Loss:   0.000143 => Gls Tokens per Sec:      899 || Batch Translation Loss:   0.018384 => Txt Tokens per Sec:     2610 || Lr: 0.000050
2024-02-09 11:53:13,209 Epoch 2483: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 11:53:13,210 EPOCH 2484
2024-02-09 11:53:24,264 Epoch 2484: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 11:53:24,264 EPOCH 2485
2024-02-09 11:53:35,023 Epoch 2485: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-09 11:53:35,024 EPOCH 2486
2024-02-09 11:53:45,852 Epoch 2486: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-09 11:53:45,852 EPOCH 2487
2024-02-09 11:53:56,606 Epoch 2487: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-09 11:53:56,606 EPOCH 2488
2024-02-09 11:54:07,381 Epoch 2488: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 11:54:07,381 EPOCH 2489
2024-02-09 11:54:10,486 [Epoch: 2489 Step: 00042300] Batch Recognition Loss:   0.000485 => Gls Tokens per Sec:      825 || Batch Translation Loss:   0.023720 => Txt Tokens per Sec:     2617 || Lr: 0.000050
2024-02-09 11:54:18,264 Epoch 2489: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-09 11:54:18,264 EPOCH 2490
2024-02-09 11:54:29,449 Epoch 2490: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 11:54:29,449 EPOCH 2491
2024-02-09 11:54:40,490 Epoch 2491: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-09 11:54:40,490 EPOCH 2492
2024-02-09 11:54:51,534 Epoch 2492: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-09 11:54:51,534 EPOCH 2493
2024-02-09 11:55:02,500 Epoch 2493: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-09 11:55:02,500 EPOCH 2494
2024-02-09 11:55:13,582 Epoch 2494: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 11:55:13,582 EPOCH 2495
2024-02-09 11:55:15,730 [Epoch: 2495 Step: 00042400] Batch Recognition Loss:   0.000356 => Gls Tokens per Sec:      596 || Batch Translation Loss:   0.021325 => Txt Tokens per Sec:     1810 || Lr: 0.000050
2024-02-09 11:55:24,561 Epoch 2495: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 11:55:24,562 EPOCH 2496
2024-02-09 11:55:35,505 Epoch 2496: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-09 11:55:35,506 EPOCH 2497
2024-02-09 11:55:46,476 Epoch 2497: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 11:55:46,477 EPOCH 2498
2024-02-09 11:55:57,182 Epoch 2498: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 11:55:57,183 EPOCH 2499
2024-02-09 11:56:08,019 Epoch 2499: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-09 11:56:08,020 EPOCH 2500
2024-02-09 11:56:18,869 [Epoch: 2500 Step: 00042500] Batch Recognition Loss:   0.000392 => Gls Tokens per Sec:      979 || Batch Translation Loss:   0.030346 => Txt Tokens per Sec:     2709 || Lr: 0.000050
2024-02-09 11:56:18,869 Epoch 2500: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-09 11:56:18,870 EPOCH 2501
2024-02-09 11:56:29,519 Epoch 2501: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-09 11:56:29,520 EPOCH 2502
2024-02-09 11:56:40,473 Epoch 2502: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-09 11:56:40,473 EPOCH 2503
2024-02-09 11:56:51,307 Epoch 2503: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 11:56:51,307 EPOCH 2504
2024-02-09 11:57:02,253 Epoch 2504: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-09 11:57:02,254 EPOCH 2505
2024-02-09 11:57:13,016 Epoch 2505: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 11:57:13,016 EPOCH 2506
2024-02-09 11:57:23,462 [Epoch: 2506 Step: 00042600] Batch Recognition Loss:   0.000293 => Gls Tokens per Sec:      894 || Batch Translation Loss:   0.022584 => Txt Tokens per Sec:     2541 || Lr: 0.000050
2024-02-09 11:57:23,808 Epoch 2506: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-09 11:57:23,808 EPOCH 2507
2024-02-09 11:57:34,576 Epoch 2507: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-09 11:57:34,577 EPOCH 2508
2024-02-09 11:57:45,369 Epoch 2508: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-09 11:57:45,369 EPOCH 2509
2024-02-09 11:57:56,304 Epoch 2509: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-09 11:57:56,305 EPOCH 2510
2024-02-09 11:58:06,904 Epoch 2510: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-09 11:58:06,905 EPOCH 2511
2024-02-09 11:58:17,900 Epoch 2511: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-09 11:58:17,901 EPOCH 2512
2024-02-09 11:58:27,956 [Epoch: 2512 Step: 00042700] Batch Recognition Loss:   0.000707 => Gls Tokens per Sec:      802 || Batch Translation Loss:   0.069245 => Txt Tokens per Sec:     2265 || Lr: 0.000050
2024-02-09 11:58:28,738 Epoch 2512: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.15 
2024-02-09 11:58:28,739 EPOCH 2513
2024-02-09 11:58:39,477 Epoch 2513: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.08 
2024-02-09 11:58:39,477 EPOCH 2514
2024-02-09 11:58:50,345 Epoch 2514: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.81 
2024-02-09 11:58:50,346 EPOCH 2515
2024-02-09 11:59:01,175 Epoch 2515: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-09 11:59:01,176 EPOCH 2516
2024-02-09 11:59:12,274 Epoch 2516: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-09 11:59:12,275 EPOCH 2517
2024-02-09 11:59:23,259 Epoch 2517: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.39 
2024-02-09 11:59:23,259 EPOCH 2518
2024-02-09 11:59:32,813 [Epoch: 2518 Step: 00042800] Batch Recognition Loss:   0.000484 => Gls Tokens per Sec:      710 || Batch Translation Loss:   0.025912 => Txt Tokens per Sec:     2040 || Lr: 0.000050
2024-02-09 11:59:34,240 Epoch 2518: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-09 11:59:34,240 EPOCH 2519
2024-02-09 11:59:45,062 Epoch 2519: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-09 11:59:45,062 EPOCH 2520
2024-02-09 11:59:55,942 Epoch 2520: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-09 11:59:55,943 EPOCH 2521
2024-02-09 12:00:07,048 Epoch 2521: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-09 12:00:07,049 EPOCH 2522
2024-02-09 12:00:18,231 Epoch 2522: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 12:00:18,232 EPOCH 2523
2024-02-09 12:00:29,051 Epoch 2523: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 12:00:29,051 EPOCH 2524
2024-02-09 12:00:32,892 [Epoch: 2524 Step: 00042900] Batch Recognition Loss:   0.000244 => Gls Tokens per Sec:     1501 || Batch Translation Loss:   0.027607 => Txt Tokens per Sec:     4040 || Lr: 0.000050
2024-02-09 12:00:40,002 Epoch 2524: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 12:00:40,002 EPOCH 2525
2024-02-09 12:00:50,992 Epoch 2525: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 12:00:50,993 EPOCH 2526
2024-02-09 12:01:02,246 Epoch 2526: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 12:01:02,247 EPOCH 2527
2024-02-09 12:01:13,193 Epoch 2527: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 12:01:13,193 EPOCH 2528
2024-02-09 12:01:23,693 Epoch 2528: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 12:01:23,693 EPOCH 2529
2024-02-09 12:01:34,529 Epoch 2529: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 12:01:34,530 EPOCH 2530
2024-02-09 12:01:41,583 [Epoch: 2530 Step: 00043000] Batch Recognition Loss:   0.000260 => Gls Tokens per Sec:      598 || Batch Translation Loss:   0.018126 => Txt Tokens per Sec:     1702 || Lr: 0.000050
2024-02-09 12:01:45,323 Epoch 2530: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 12:01:45,324 EPOCH 2531
2024-02-09 12:01:56,009 Epoch 2531: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-09 12:01:56,009 EPOCH 2532
2024-02-09 12:02:06,865 Epoch 2532: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-09 12:02:06,866 EPOCH 2533
2024-02-09 12:02:17,745 Epoch 2533: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-09 12:02:17,746 EPOCH 2534
2024-02-09 12:02:28,594 Epoch 2534: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-09 12:02:28,595 EPOCH 2535
2024-02-09 12:02:39,185 Epoch 2535: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-09 12:02:39,186 EPOCH 2536
2024-02-09 12:02:42,386 [Epoch: 2536 Step: 00043100] Batch Recognition Loss:   0.000144 => Gls Tokens per Sec:     1000 || Batch Translation Loss:   0.086526 => Txt Tokens per Sec:     2947 || Lr: 0.000050
2024-02-09 12:02:50,275 Epoch 2536: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-09 12:02:50,275 EPOCH 2537
2024-02-09 12:03:01,385 Epoch 2537: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-09 12:03:01,386 EPOCH 2538
2024-02-09 12:03:12,523 Epoch 2538: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-09 12:03:12,524 EPOCH 2539
2024-02-09 12:03:23,631 Epoch 2539: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-09 12:03:23,631 EPOCH 2540
2024-02-09 12:03:34,834 Epoch 2540: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-09 12:03:34,835 EPOCH 2541
2024-02-09 12:03:45,841 Epoch 2541: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.35 
2024-02-09 12:03:45,842 EPOCH 2542
2024-02-09 12:03:46,514 [Epoch: 2542 Step: 00043200] Batch Recognition Loss:   0.000184 => Gls Tokens per Sec:     2861 || Batch Translation Loss:   0.019084 => Txt Tokens per Sec:     7729 || Lr: 0.000050
2024-02-09 12:03:56,856 Epoch 2542: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-09 12:03:56,857 EPOCH 2543
2024-02-09 12:04:07,450 Epoch 2543: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-09 12:04:07,450 EPOCH 2544
2024-02-09 12:04:18,116 Epoch 2544: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-09 12:04:18,116 EPOCH 2545
2024-02-09 12:04:28,918 Epoch 2545: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-09 12:04:28,919 EPOCH 2546
2024-02-09 12:04:40,022 Epoch 2546: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-09 12:04:40,023 EPOCH 2547
2024-02-09 12:04:50,902 Epoch 2547: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-09 12:04:50,903 EPOCH 2548
2024-02-09 12:04:51,190 [Epoch: 2548 Step: 00043300] Batch Recognition Loss:   0.000216 => Gls Tokens per Sec:     2238 || Batch Translation Loss:   0.019859 => Txt Tokens per Sec:     6056 || Lr: 0.000050
2024-02-09 12:05:02,002 Epoch 2548: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-09 12:05:02,003 EPOCH 2549
2024-02-09 12:05:12,994 Epoch 2549: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-09 12:05:12,994 EPOCH 2550
2024-02-09 12:05:23,982 Epoch 2550: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-09 12:05:23,982 EPOCH 2551
2024-02-09 12:05:35,007 Epoch 2551: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-09 12:05:35,008 EPOCH 2552
2024-02-09 12:05:45,976 Epoch 2552: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-09 12:05:45,976 EPOCH 2553
2024-02-09 12:05:55,000 [Epoch: 2553 Step: 00043400] Batch Recognition Loss:   0.000450 => Gls Tokens per Sec:     1106 || Batch Translation Loss:   0.025611 => Txt Tokens per Sec:     3029 || Lr: 0.000050
2024-02-09 12:05:56,734 Epoch 2553: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-09 12:05:56,734 EPOCH 2554
2024-02-09 12:06:07,383 Epoch 2554: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 12:06:07,384 EPOCH 2555
2024-02-09 12:06:18,391 Epoch 2555: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 12:06:18,392 EPOCH 2556
2024-02-09 12:06:29,336 Epoch 2556: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 12:06:29,336 EPOCH 2557
2024-02-09 12:06:40,138 Epoch 2557: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-09 12:06:40,139 EPOCH 2558
2024-02-09 12:06:51,007 Epoch 2558: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 12:06:51,007 EPOCH 2559
2024-02-09 12:06:58,934 [Epoch: 2559 Step: 00043500] Batch Recognition Loss:   0.000626 => Gls Tokens per Sec:     1131 || Batch Translation Loss:   0.022110 => Txt Tokens per Sec:     3156 || Lr: 0.000050
2024-02-09 12:07:01,819 Epoch 2559: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 12:07:01,819 EPOCH 2560
2024-02-09 12:07:12,787 Epoch 2560: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 12:07:12,788 EPOCH 2561
2024-02-09 12:07:23,428 Epoch 2561: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-09 12:07:23,429 EPOCH 2562
2024-02-09 12:07:34,161 Epoch 2562: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-09 12:07:34,161 EPOCH 2563
2024-02-09 12:07:45,109 Epoch 2563: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-09 12:07:45,110 EPOCH 2564
2024-02-09 12:07:55,895 Epoch 2564: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-09 12:07:55,896 EPOCH 2565
2024-02-09 12:08:04,156 [Epoch: 2565 Step: 00043600] Batch Recognition Loss:   0.000275 => Gls Tokens per Sec:      898 || Batch Translation Loss:   0.065316 => Txt Tokens per Sec:     2450 || Lr: 0.000050
2024-02-09 12:08:06,871 Epoch 2565: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-09 12:08:06,872 EPOCH 2566
2024-02-09 12:08:17,724 Epoch 2566: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-09 12:08:17,725 EPOCH 2567
2024-02-09 12:08:28,403 Epoch 2567: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-09 12:08:28,404 EPOCH 2568
2024-02-09 12:08:39,278 Epoch 2568: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-09 12:08:39,278 EPOCH 2569
2024-02-09 12:08:50,123 Epoch 2569: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-09 12:08:50,124 EPOCH 2570
2024-02-09 12:09:01,281 Epoch 2570: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-09 12:09:01,281 EPOCH 2571
2024-02-09 12:09:07,557 [Epoch: 2571 Step: 00043700] Batch Recognition Loss:   0.000163 => Gls Tokens per Sec:      978 || Batch Translation Loss:   0.016745 => Txt Tokens per Sec:     2725 || Lr: 0.000050
2024-02-09 12:09:12,318 Epoch 2571: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-09 12:09:12,319 EPOCH 2572
2024-02-09 12:09:23,129 Epoch 2572: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.36 
2024-02-09 12:09:23,129 EPOCH 2573
2024-02-09 12:09:33,972 Epoch 2573: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-09 12:09:33,972 EPOCH 2574
2024-02-09 12:09:44,916 Epoch 2574: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-09 12:09:44,917 EPOCH 2575
2024-02-09 12:09:55,920 Epoch 2575: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-09 12:09:55,920 EPOCH 2576
2024-02-09 12:10:06,916 Epoch 2576: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-09 12:10:06,917 EPOCH 2577
2024-02-09 12:10:12,641 [Epoch: 2577 Step: 00043800] Batch Recognition Loss:   0.000902 => Gls Tokens per Sec:      849 || Batch Translation Loss:   0.020811 => Txt Tokens per Sec:     2340 || Lr: 0.000050
2024-02-09 12:10:17,880 Epoch 2577: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-09 12:10:17,881 EPOCH 2578
2024-02-09 12:10:28,857 Epoch 2578: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-09 12:10:28,857 EPOCH 2579
2024-02-09 12:10:39,510 Epoch 2579: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-09 12:10:39,511 EPOCH 2580
2024-02-09 12:10:50,430 Epoch 2580: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-09 12:10:50,430 EPOCH 2581
2024-02-09 12:11:01,350 Epoch 2581: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-09 12:11:01,351 EPOCH 2582
2024-02-09 12:11:12,182 Epoch 2582: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-09 12:11:12,182 EPOCH 2583
2024-02-09 12:11:15,086 [Epoch: 2583 Step: 00043900] Batch Recognition Loss:   0.000163 => Gls Tokens per Sec:     1323 || Batch Translation Loss:   0.019983 => Txt Tokens per Sec:     4006 || Lr: 0.000050
2024-02-09 12:11:22,962 Epoch 2583: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-09 12:11:22,963 EPOCH 2584
2024-02-09 12:11:34,173 Epoch 2584: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-09 12:11:34,173 EPOCH 2585
2024-02-09 12:11:45,016 Epoch 2585: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-09 12:11:45,017 EPOCH 2586
2024-02-09 12:11:55,607 Epoch 2586: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-09 12:11:55,608 EPOCH 2587
2024-02-09 12:12:06,664 Epoch 2587: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-09 12:12:06,664 EPOCH 2588
2024-02-09 12:12:17,452 Epoch 2588: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-09 12:12:17,452 EPOCH 2589
2024-02-09 12:12:20,169 [Epoch: 2589 Step: 00044000] Batch Recognition Loss:   0.000204 => Gls Tokens per Sec:      943 || Batch Translation Loss:   0.023316 => Txt Tokens per Sec:     2632 || Lr: 0.000050
2024-02-09 12:13:00,562 Validation result at epoch 2589, step    44000: duration: 40.3935s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.60208	Translation Loss: 98297.53906	PPL: 18360.78125
	Eval Metric: BLEU
	WER 2.97	(DEL: 0.00,	INS: 0.00,	SUB: 2.97)
	BLEU-4 0.50	(BLEU-1: 10.11,	BLEU-2: 3.01,	BLEU-3: 1.15,	BLEU-4: 0.50)
	CHRF 17.00	ROUGE 8.54
2024-02-09 12:13:00,563 Logging Recognition and Translation Outputs
2024-02-09 12:13:00,563 ========================================================================================================================
2024-02-09 12:13:00,563 Logging Sequence: 57_104.00
2024-02-09 12:13:00,564 	Gloss Reference :	A B+C+D+E
2024-02-09 12:13:00,564 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 12:13:00,564 	Gloss Alignment :	         
2024-02-09 12:13:00,564 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 12:13:00,566 	Text Reference  :	the next day kohli and kl rahul continued from where they  had    left and     displayed amazing batting performance without losing their wickets
2024-02-09 12:13:00,566 	Text Hypothesis :	*** **** *** ***** *** ** ***** ********* **** ms    dhoni played 3    players to        wear    their   innings     without losing any   wicket 
2024-02-09 12:13:00,566 	Text Alignment  :	D   D    D   D     D   D  D     D         D    S     S     S      S    S       S         S       S       S                          S     S      
2024-02-09 12:13:00,566 ========================================================================================================================
2024-02-09 12:13:00,566 Logging Sequence: 136_64.00
2024-02-09 12:13:00,566 	Gloss Reference :	A B+C+D+E
2024-02-09 12:13:00,567 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 12:13:00,567 	Gloss Alignment :	         
2024-02-09 12:13:00,567 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 12:13:00,567 	Text Reference  :	* ** in   all        she   has won   2  medals  
2024-02-09 12:13:00,567 	Text Hypothesis :	i am very particular about my  never my feelings
2024-02-09 12:13:00,568 	Text Alignment  :	I I  S    S          S     S   S     S  S       
2024-02-09 12:13:00,568 ========================================================================================================================
2024-02-09 12:13:00,568 Logging Sequence: 54_123.00
2024-02-09 12:13:00,568 	Gloss Reference :	A B+C+D+E
2024-02-09 12:13:00,568 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 12:13:00,569 	Gloss Alignment :	         
2024-02-09 12:13:00,569 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 12:13:00,569 	Text Reference  :	vips sponsors international cricket groups have already booked their     hotel  rooms
2024-02-09 12:13:00,570 	Text Hypothesis :	**** ******** however       he      had    not  given   a      chartered flight wow  
2024-02-09 12:13:00,570 	Text Alignment  :	D    D        S             S       S      S    S       S      S         S      S    
2024-02-09 12:13:00,570 ========================================================================================================================
2024-02-09 12:13:00,570 Logging Sequence: 168_115.00
2024-02-09 12:13:00,570 	Gloss Reference :	A B+C+D+E
2024-02-09 12:13:00,571 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 12:13:00,571 	Gloss Alignment :	         
2024-02-09 12:13:00,571 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 12:13:00,571 	Text Reference  :	**** ***** this   has     sparked a    major discussion on   social media
2024-02-09 12:13:00,572 	Text Hypothesis :	bcci chief sourav ganguly on      28th june  said       that the    team 
2024-02-09 12:13:00,572 	Text Alignment  :	I    I     S      S       S       S    S     S          S    S      S    
2024-02-09 12:13:00,572 ========================================================================================================================
2024-02-09 12:13:00,572 Logging Sequence: 121_132.00
2024-02-09 12:13:00,572 	Gloss Reference :	A B+C+D+E
2024-02-09 12:13:00,572 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 12:13:00,573 	Gloss Alignment :	         
2024-02-09 12:13:00,573 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 12:13:00,574 	Text Reference  :	******* which is   why    they will be retesting her  to check if   she       consumed any stamina enhancing drugs
2024-02-09 12:13:00,574 	Text Hypothesis :	however he    said people set  very be ********* held in a     same melbourne stadium  and may     be        held 
2024-02-09 12:13:00,575 	Text Alignment  :	I       S     S    S      S    S       D         S    S  S     S    S         S        S   S       S         S    
2024-02-09 12:13:00,575 ========================================================================================================================
2024-02-09 12:13:08,965 Epoch 2589: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-09 12:13:08,966 EPOCH 2590
2024-02-09 12:13:20,269 Epoch 2590: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-09 12:13:20,269 EPOCH 2591
2024-02-09 12:13:31,409 Epoch 2591: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-09 12:13:31,409 EPOCH 2592
2024-02-09 12:13:42,285 Epoch 2592: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-09 12:13:42,285 EPOCH 2593
2024-02-09 12:13:53,174 Epoch 2593: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-09 12:13:53,175 EPOCH 2594
2024-02-09 12:14:04,129 Epoch 2594: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-09 12:14:04,130 EPOCH 2595
2024-02-09 12:14:06,163 [Epoch: 2595 Step: 00044100] Batch Recognition Loss:   0.000471 => Gls Tokens per Sec:      630 || Batch Translation Loss:   0.027953 => Txt Tokens per Sec:     2020 || Lr: 0.000050
2024-02-09 12:14:15,004 Epoch 2595: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-09 12:14:15,004 EPOCH 2596
2024-02-09 12:14:25,774 Epoch 2596: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-09 12:14:25,774 EPOCH 2597
2024-02-09 12:14:36,801 Epoch 2597: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-09 12:14:36,802 EPOCH 2598
2024-02-09 12:14:47,553 Epoch 2598: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 12:14:47,553 EPOCH 2599
2024-02-09 12:14:58,754 Epoch 2599: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 12:14:58,755 EPOCH 2600
2024-02-09 12:15:09,603 [Epoch: 2600 Step: 00044200] Batch Recognition Loss:   0.000126 => Gls Tokens per Sec:      979 || Batch Translation Loss:   0.011441 => Txt Tokens per Sec:     2709 || Lr: 0.000050
2024-02-09 12:15:09,604 Epoch 2600: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 12:15:09,604 EPOCH 2601
2024-02-09 12:15:20,367 Epoch 2601: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 12:15:20,368 EPOCH 2602
2024-02-09 12:15:31,328 Epoch 2602: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 12:15:31,329 EPOCH 2603
2024-02-09 12:15:42,294 Epoch 2603: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 12:15:42,295 EPOCH 2604
2024-02-09 12:15:53,196 Epoch 2604: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 12:15:53,197 EPOCH 2605
2024-02-09 12:16:04,153 Epoch 2605: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-09 12:16:04,154 EPOCH 2606
2024-02-09 12:16:13,463 [Epoch: 2606 Step: 00044300] Batch Recognition Loss:   0.000125 => Gls Tokens per Sec:     1004 || Batch Translation Loss:   0.011128 => Txt Tokens per Sec:     2768 || Lr: 0.000050
2024-02-09 12:16:15,324 Epoch 2606: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-09 12:16:15,325 EPOCH 2607
2024-02-09 12:16:26,315 Epoch 2607: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-09 12:16:26,316 EPOCH 2608
2024-02-09 12:16:37,195 Epoch 2608: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-09 12:16:37,196 EPOCH 2609
2024-02-09 12:16:48,085 Epoch 2609: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-09 12:16:48,086 EPOCH 2610
2024-02-09 12:16:59,027 Epoch 2610: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-09 12:16:59,027 EPOCH 2611
2024-02-09 12:17:09,666 Epoch 2611: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 12:17:09,667 EPOCH 2612
2024-02-09 12:17:19,885 [Epoch: 2612 Step: 00044400] Batch Recognition Loss:   0.000178 => Gls Tokens per Sec:      789 || Batch Translation Loss:   0.031543 => Txt Tokens per Sec:     2247 || Lr: 0.000050
2024-02-09 12:17:20,849 Epoch 2612: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 12:17:20,849 EPOCH 2613
2024-02-09 12:17:32,808 Epoch 2613: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-09 12:17:32,809 EPOCH 2614
2024-02-09 12:17:43,584 Epoch 2614: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 12:17:43,585 EPOCH 2615
2024-02-09 12:17:54,424 Epoch 2615: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 12:17:54,425 EPOCH 2616
2024-02-09 12:18:05,413 Epoch 2616: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 12:18:05,414 EPOCH 2617
2024-02-09 12:18:16,442 Epoch 2617: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 12:18:16,442 EPOCH 2618
2024-02-09 12:18:25,901 [Epoch: 2618 Step: 00044500] Batch Recognition Loss:   0.000385 => Gls Tokens per Sec:      717 || Batch Translation Loss:   0.020680 => Txt Tokens per Sec:     1973 || Lr: 0.000050
2024-02-09 12:18:27,400 Epoch 2618: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-09 12:18:27,400 EPOCH 2619
2024-02-09 12:18:38,100 Epoch 2619: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-09 12:18:38,101 EPOCH 2620
2024-02-09 12:18:48,742 Epoch 2620: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-09 12:18:48,742 EPOCH 2621
2024-02-09 12:18:59,734 Epoch 2621: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-09 12:18:59,735 EPOCH 2622
2024-02-09 12:19:10,614 Epoch 2622: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-09 12:19:10,615 EPOCH 2623
2024-02-09 12:19:21,537 Epoch 2623: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-09 12:19:21,538 EPOCH 2624
2024-02-09 12:19:29,080 [Epoch: 2624 Step: 00044600] Batch Recognition Loss:   0.000145 => Gls Tokens per Sec:      729 || Batch Translation Loss:   0.021191 => Txt Tokens per Sec:     1943 || Lr: 0.000050
2024-02-09 12:19:32,517 Epoch 2624: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-09 12:19:32,518 EPOCH 2625
2024-02-09 12:19:42,901 Epoch 2625: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-09 12:19:42,901 EPOCH 2626
2024-02-09 12:19:53,767 Epoch 2626: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-09 12:19:53,768 EPOCH 2627
2024-02-09 12:20:04,401 Epoch 2627: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-09 12:20:04,401 EPOCH 2628
2024-02-09 12:20:15,564 Epoch 2628: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-09 12:20:15,565 EPOCH 2629
2024-02-09 12:20:26,288 Epoch 2629: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-09 12:20:26,288 EPOCH 2630
2024-02-09 12:20:29,782 [Epoch: 2630 Step: 00044700] Batch Recognition Loss:   0.000145 => Gls Tokens per Sec:     1283 || Batch Translation Loss:   0.011841 => Txt Tokens per Sec:     3578 || Lr: 0.000050
2024-02-09 12:20:37,436 Epoch 2630: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-09 12:20:37,437 EPOCH 2631
2024-02-09 12:20:48,231 Epoch 2631: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-09 12:20:48,232 EPOCH 2632
2024-02-09 12:20:59,195 Epoch 2632: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-09 12:20:59,195 EPOCH 2633
2024-02-09 12:21:10,113 Epoch 2633: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-09 12:21:10,113 EPOCH 2634
2024-02-09 12:21:20,945 Epoch 2634: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-09 12:21:20,945 EPOCH 2635
2024-02-09 12:21:31,738 Epoch 2635: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-09 12:21:31,739 EPOCH 2636
2024-02-09 12:21:35,036 [Epoch: 2636 Step: 00044800] Batch Recognition Loss:   0.000224 => Gls Tokens per Sec:      892 || Batch Translation Loss:   0.030843 => Txt Tokens per Sec:     2481 || Lr: 0.000050
2024-02-09 12:21:42,643 Epoch 2636: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-09 12:21:42,643 EPOCH 2637
2024-02-09 12:21:53,524 Epoch 2637: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-09 12:21:53,525 EPOCH 2638
2024-02-09 12:22:04,239 Epoch 2638: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-09 12:22:04,239 EPOCH 2639
2024-02-09 12:22:14,869 Epoch 2639: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 12:22:14,870 EPOCH 2640
2024-02-09 12:22:25,755 Epoch 2640: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-09 12:22:25,756 EPOCH 2641
2024-02-09 12:22:36,646 Epoch 2641: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-09 12:22:36,647 EPOCH 2642
2024-02-09 12:22:39,382 [Epoch: 2642 Step: 00044900] Batch Recognition Loss:   0.000470 => Gls Tokens per Sec:      607 || Batch Translation Loss:   0.014676 => Txt Tokens per Sec:     1583 || Lr: 0.000050
2024-02-09 12:22:47,583 Epoch 2642: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 12:22:47,584 EPOCH 2643
2024-02-09 12:22:58,730 Epoch 2643: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 12:22:58,731 EPOCH 2644
2024-02-09 12:23:09,531 Epoch 2644: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 12:23:09,532 EPOCH 2645
2024-02-09 12:23:20,272 Epoch 2645: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-09 12:23:20,272 EPOCH 2646
2024-02-09 12:23:31,301 Epoch 2646: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-09 12:23:31,302 EPOCH 2647
2024-02-09 12:23:42,029 Epoch 2647: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-09 12:23:42,029 EPOCH 2648
2024-02-09 12:23:44,445 [Epoch: 2648 Step: 00045000] Batch Recognition Loss:   0.000589 => Gls Tokens per Sec:      157 || Batch Translation Loss:   0.013532 => Txt Tokens per Sec:      562 || Lr: 0.000050
2024-02-09 12:23:52,867 Epoch 2648: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-09 12:23:52,868 EPOCH 2649
2024-02-09 12:24:03,771 Epoch 2649: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-09 12:24:03,772 EPOCH 2650
2024-02-09 12:24:14,469 Epoch 2650: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 12:24:14,470 EPOCH 2651
2024-02-09 12:24:25,487 Epoch 2651: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 12:24:25,488 EPOCH 2652
2024-02-09 12:24:36,448 Epoch 2652: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-09 12:24:36,449 EPOCH 2653
2024-02-09 12:24:47,247 [Epoch: 2653 Step: 00045100] Batch Recognition Loss:   0.000158 => Gls Tokens per Sec:      924 || Batch Translation Loss:   0.034679 => Txt Tokens per Sec:     2627 || Lr: 0.000050
2024-02-09 12:24:47,353 Epoch 2653: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-09 12:24:47,353 EPOCH 2654
2024-02-09 12:24:58,073 Epoch 2654: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-09 12:24:58,074 EPOCH 2655
2024-02-09 12:25:08,747 Epoch 2655: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-09 12:25:08,747 EPOCH 2656
2024-02-09 12:25:19,210 Epoch 2656: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-09 12:25:19,210 EPOCH 2657
2024-02-09 12:25:30,088 Epoch 2657: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.20 
2024-02-09 12:25:30,089 EPOCH 2658
2024-02-09 12:25:41,109 Epoch 2658: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.82 
2024-02-09 12:25:41,109 EPOCH 2659
2024-02-09 12:25:47,994 [Epoch: 2659 Step: 00045200] Batch Recognition Loss:   0.001222 => Gls Tokens per Sec:     1264 || Batch Translation Loss:   0.155389 => Txt Tokens per Sec:     3355 || Lr: 0.000050
2024-02-09 12:25:52,295 Epoch 2659: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.68 
2024-02-09 12:25:52,295 EPOCH 2660
2024-02-09 12:26:03,171 Epoch 2660: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.52 
2024-02-09 12:26:03,171 EPOCH 2661
2024-02-09 12:26:14,024 Epoch 2661: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.18 
2024-02-09 12:26:14,025 EPOCH 2662
2024-02-09 12:26:24,927 Epoch 2662: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.48 
2024-02-09 12:26:24,928 EPOCH 2663
2024-02-09 12:26:36,095 Epoch 2663: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.43 
2024-02-09 12:26:36,096 EPOCH 2664
2024-02-09 12:26:47,073 Epoch 2664: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.83 
2024-02-09 12:26:47,073 EPOCH 2665
2024-02-09 12:26:53,535 [Epoch: 2665 Step: 00045300] Batch Recognition Loss:   0.000615 => Gls Tokens per Sec:     1148 || Batch Translation Loss:   0.037501 => Txt Tokens per Sec:     3103 || Lr: 0.000050
2024-02-09 12:26:58,074 Epoch 2665: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-09 12:26:58,075 EPOCH 2666
2024-02-09 12:27:09,160 Epoch 2666: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-09 12:27:09,161 EPOCH 2667
2024-02-09 12:27:20,127 Epoch 2667: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-09 12:27:20,128 EPOCH 2668
2024-02-09 12:27:31,206 Epoch 2668: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-09 12:27:31,206 EPOCH 2669
2024-02-09 12:27:42,121 Epoch 2669: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.28 
2024-02-09 12:27:42,122 EPOCH 2670
2024-02-09 12:27:52,828 Epoch 2670: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-09 12:27:52,828 EPOCH 2671
2024-02-09 12:27:59,963 [Epoch: 2671 Step: 00045400] Batch Recognition Loss:   0.000231 => Gls Tokens per Sec:      897 || Batch Translation Loss:   0.016610 => Txt Tokens per Sec:     2570 || Lr: 0.000050
2024-02-09 12:28:03,821 Epoch 2671: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-09 12:28:03,822 EPOCH 2672
2024-02-09 12:28:14,883 Epoch 2672: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-09 12:28:14,883 EPOCH 2673
2024-02-09 12:28:25,673 Epoch 2673: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-09 12:28:25,674 EPOCH 2674
2024-02-09 12:28:36,643 Epoch 2674: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 12:28:36,643 EPOCH 2675
2024-02-09 12:28:47,846 Epoch 2675: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 12:28:47,847 EPOCH 2676
2024-02-09 12:28:58,761 Epoch 2676: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 12:28:58,762 EPOCH 2677
2024-02-09 12:29:04,042 [Epoch: 2677 Step: 00045500] Batch Recognition Loss:   0.000144 => Gls Tokens per Sec:      970 || Batch Translation Loss:   0.010869 => Txt Tokens per Sec:     2803 || Lr: 0.000050
2024-02-09 12:29:09,789 Epoch 2677: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 12:29:09,790 EPOCH 2678
2024-02-09 12:29:20,622 Epoch 2678: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 12:29:20,623 EPOCH 2679
2024-02-09 12:29:31,560 Epoch 2679: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 12:29:31,561 EPOCH 2680
2024-02-09 12:29:42,335 Epoch 2680: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 12:29:42,335 EPOCH 2681
2024-02-09 12:29:53,387 Epoch 2681: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 12:29:53,388 EPOCH 2682
2024-02-09 12:30:04,260 Epoch 2682: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 12:30:04,260 EPOCH 2683
2024-02-09 12:30:05,594 [Epoch: 2683 Step: 00045600] Batch Recognition Loss:   0.000176 => Gls Tokens per Sec:     2883 || Batch Translation Loss:   0.009964 => Txt Tokens per Sec:     7111 || Lr: 0.000050
2024-02-09 12:30:15,141 Epoch 2683: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 12:30:15,141 EPOCH 2684
2024-02-09 12:30:25,893 Epoch 2684: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 12:30:25,893 EPOCH 2685
2024-02-09 12:30:36,832 Epoch 2685: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 12:30:36,832 EPOCH 2686
2024-02-09 12:30:47,727 Epoch 2686: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-09 12:30:47,727 EPOCH 2687
2024-02-09 12:30:58,724 Epoch 2687: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 12:30:58,725 EPOCH 2688
2024-02-09 12:31:09,594 Epoch 2688: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 12:31:09,595 EPOCH 2689
2024-02-09 12:31:11,839 [Epoch: 2689 Step: 00045700] Batch Recognition Loss:   0.000132 => Gls Tokens per Sec:     1141 || Batch Translation Loss:   0.025339 => Txt Tokens per Sec:     3366 || Lr: 0.000050
2024-02-09 12:31:20,064 Epoch 2689: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 12:31:20,065 EPOCH 2690
2024-02-09 12:31:30,808 Epoch 2690: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 12:31:30,809 EPOCH 2691
2024-02-09 12:31:41,409 Epoch 2691: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 12:31:41,409 EPOCH 2692
2024-02-09 12:31:52,652 Epoch 2692: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-09 12:31:52,653 EPOCH 2693
2024-02-09 12:32:03,734 Epoch 2693: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 12:32:03,735 EPOCH 2694
2024-02-09 12:32:14,656 Epoch 2694: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-09 12:32:14,657 EPOCH 2695
2024-02-09 12:32:16,887 [Epoch: 2695 Step: 00045800] Batch Recognition Loss:   0.000521 => Gls Tokens per Sec:      574 || Batch Translation Loss:   0.019828 => Txt Tokens per Sec:     1718 || Lr: 0.000050
2024-02-09 12:32:25,340 Epoch 2695: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-09 12:32:25,341 EPOCH 2696
2024-02-09 12:32:36,236 Epoch 2696: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 12:32:36,237 EPOCH 2697
2024-02-09 12:32:47,101 Epoch 2697: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 12:32:47,101 EPOCH 2698
2024-02-09 12:32:58,213 Epoch 2698: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-09 12:32:58,213 EPOCH 2699
2024-02-09 12:33:08,931 Epoch 2699: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-09 12:33:08,932 EPOCH 2700
2024-02-09 12:33:19,646 [Epoch: 2700 Step: 00045900] Batch Recognition Loss:   0.000329 => Gls Tokens per Sec:      991 || Batch Translation Loss:   0.006537 => Txt Tokens per Sec:     2743 || Lr: 0.000050
2024-02-09 12:33:19,646 Epoch 2700: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 12:33:19,647 EPOCH 2701
2024-02-09 12:33:30,425 Epoch 2701: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 12:33:30,425 EPOCH 2702
2024-02-09 12:33:41,141 Epoch 2702: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 12:33:41,142 EPOCH 2703
2024-02-09 12:33:51,894 Epoch 2703: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 12:33:51,895 EPOCH 2704
2024-02-09 12:34:02,871 Epoch 2704: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-09 12:34:02,872 EPOCH 2705
2024-02-09 12:34:13,401 Epoch 2705: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-09 12:34:13,402 EPOCH 2706
2024-02-09 12:34:23,436 [Epoch: 2706 Step: 00046000] Batch Recognition Loss:   0.000343 => Gls Tokens per Sec:      931 || Batch Translation Loss:   0.006442 => Txt Tokens per Sec:     2541 || Lr: 0.000050
2024-02-09 12:35:05,494 Validation result at epoch 2706, step    46000: duration: 42.0573s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.63471	Translation Loss: 98773.25000	PPL: 19254.23047
	Eval Metric: BLEU
	WER 3.04	(DEL: 0.00,	INS: 0.00,	SUB: 3.04)
	BLEU-4 0.47	(BLEU-1: 10.83,	BLEU-2: 3.28,	BLEU-3: 1.21,	BLEU-4: 0.47)
	CHRF 16.92	ROUGE 8.74
2024-02-09 12:35:05,496 Logging Recognition and Translation Outputs
2024-02-09 12:35:05,496 ========================================================================================================================
2024-02-09 12:35:05,496 Logging Sequence: 87_207.00
2024-02-09 12:35:05,496 	Gloss Reference :	A B+C+D+E
2024-02-09 12:35:05,496 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 12:35:05,497 	Gloss Alignment :	         
2024-02-09 12:35:05,497 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 12:35:05,498 	Text Reference  :	there were     2-3   pakistanis who were  speaking anti-india things and   things on    kashmir
2024-02-09 12:35:05,498 	Text Hypothesis :	this  amrapali group paid       a   total of       rs         4222   crore to     rhiti sports 
2024-02-09 12:35:05,498 	Text Alignment  :	S     S        S     S          S   S     S        S          S      S     S      S     S      
2024-02-09 12:35:05,498 ========================================================================================================================
2024-02-09 12:35:05,498 Logging Sequence: 67_73.00
2024-02-09 12:35:05,499 	Gloss Reference :	A B+C+D+E
2024-02-09 12:35:05,499 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 12:35:05,499 	Gloss Alignment :	         
2024-02-09 12:35:05,499 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 12:35:05,500 	Text Reference  :	**** *** in    his tweet   he     also  said 
2024-02-09 12:35:05,500 	Text Hypothesis :	shah and india had various awards their first
2024-02-09 12:35:05,500 	Text Alignment  :	I    I   S     S   S       S      S     S    
2024-02-09 12:35:05,500 ========================================================================================================================
2024-02-09 12:35:05,500 Logging Sequence: 172_267.00
2024-02-09 12:35:05,500 	Gloss Reference :	A B+C+D+E
2024-02-09 12:35:05,500 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 12:35:05,501 	Gloss Alignment :	         
2024-02-09 12:35:05,501 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 12:35:05,501 	Text Reference  :	**** such provisions have been made  
2024-02-09 12:35:05,501 	Text Hypothesis :	this is   why        it   was  halted
2024-02-09 12:35:05,501 	Text Alignment  :	I    S    S          S    S    S     
2024-02-09 12:35:05,502 ========================================================================================================================
2024-02-09 12:35:05,502 Logging Sequence: 144_23.00
2024-02-09 12:35:05,502 	Gloss Reference :	A B+C+D+E
2024-02-09 12:35:05,502 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 12:35:05,502 	Gloss Alignment :	         
2024-02-09 12:35:05,502 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 12:35:05,503 	Text Reference  :	the girl is 14-year-old mumal mehar and she       is     from kanasar village of      barmer in rajasthan
2024-02-09 12:35:05,503 	Text Hypothesis :	*** **** ** *********** ***** ***** *** argentina scored 2    goals   however towards the    9  minutes  
2024-02-09 12:35:05,503 	Text Alignment  :	D   D    D  D           D     D     D   S         S      S    S       S       S       S      S  S        
2024-02-09 12:35:05,504 ========================================================================================================================
2024-02-09 12:35:05,504 Logging Sequence: 133_202.00
2024-02-09 12:35:05,504 	Gloss Reference :	A B+C+D+E
2024-02-09 12:35:05,504 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 12:35:05,504 	Gloss Alignment :	         
2024-02-09 12:35:05,504 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 12:35:05,505 	Text Reference  :	australia has already qualified for     the  final   if  india wins it will    face australia
2024-02-09 12:35:05,505 	Text Hypothesis :	********* *** ******* pakistani batsmen kept scoring run but   lost 5  wickets very soon     
2024-02-09 12:35:05,505 	Text Alignment  :	D         D   D       S         S       S    S       S   S     S    S  S       S    S        
2024-02-09 12:35:05,506 ========================================================================================================================
2024-02-09 12:35:06,191 Epoch 2706: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-09 12:35:06,192 EPOCH 2707
2024-02-09 12:35:17,168 Epoch 2707: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-09 12:35:17,168 EPOCH 2708
2024-02-09 12:35:28,076 Epoch 2708: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-09 12:35:28,077 EPOCH 2709
2024-02-09 12:35:38,506 Epoch 2709: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 12:35:38,507 EPOCH 2710
2024-02-09 12:35:49,328 Epoch 2710: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-09 12:35:49,328 EPOCH 2711
2024-02-09 12:36:00,286 Epoch 2711: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 12:36:00,287 EPOCH 2712
2024-02-09 12:36:09,017 [Epoch: 2712 Step: 00046100] Batch Recognition Loss:   0.000159 => Gls Tokens per Sec:      923 || Batch Translation Loss:   0.012735 => Txt Tokens per Sec:     2550 || Lr: 0.000050
2024-02-09 12:36:11,367 Epoch 2712: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 12:36:11,368 EPOCH 2713
2024-02-09 12:36:22,155 Epoch 2713: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-09 12:36:22,156 EPOCH 2714
2024-02-09 12:36:33,199 Epoch 2714: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 12:36:33,200 EPOCH 2715
2024-02-09 12:36:44,313 Epoch 2715: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-09 12:36:44,314 EPOCH 2716
2024-02-09 12:36:55,287 Epoch 2716: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-09 12:36:55,287 EPOCH 2717
2024-02-09 12:37:06,275 Epoch 2717: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.98 
2024-02-09 12:37:06,275 EPOCH 2718
2024-02-09 12:37:13,857 [Epoch: 2718 Step: 00046200] Batch Recognition Loss:   0.000290 => Gls Tokens per Sec:      894 || Batch Translation Loss:   0.029819 => Txt Tokens per Sec:     2407 || Lr: 0.000050
2024-02-09 12:37:17,308 Epoch 2718: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.78 
2024-02-09 12:37:17,309 EPOCH 2719
2024-02-09 12:37:28,365 Epoch 2719: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.90 
2024-02-09 12:37:28,366 EPOCH 2720
2024-02-09 12:37:39,189 Epoch 2720: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-09 12:37:39,190 EPOCH 2721
2024-02-09 12:37:49,970 Epoch 2721: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-09 12:37:49,971 EPOCH 2722
2024-02-09 12:38:00,620 Epoch 2722: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-09 12:38:00,621 EPOCH 2723
2024-02-09 12:38:11,518 Epoch 2723: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-09 12:38:11,518 EPOCH 2724
2024-02-09 12:38:16,630 [Epoch: 2724 Step: 00046300] Batch Recognition Loss:   0.000214 => Gls Tokens per Sec:     1127 || Batch Translation Loss:   0.013377 => Txt Tokens per Sec:     3099 || Lr: 0.000050
2024-02-09 12:38:22,279 Epoch 2724: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 12:38:22,280 EPOCH 2725
2024-02-09 12:38:33,236 Epoch 2725: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 12:38:33,237 EPOCH 2726
2024-02-09 12:38:44,046 Epoch 2726: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 12:38:44,046 EPOCH 2727
2024-02-09 12:38:54,893 Epoch 2727: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 12:38:54,893 EPOCH 2728
2024-02-09 12:39:05,662 Epoch 2728: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-09 12:39:05,663 EPOCH 2729
2024-02-09 12:39:16,311 Epoch 2729: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-09 12:39:16,312 EPOCH 2730
2024-02-09 12:39:21,291 [Epoch: 2730 Step: 00046400] Batch Recognition Loss:   0.000121 => Gls Tokens per Sec:      900 || Batch Translation Loss:   0.009457 => Txt Tokens per Sec:     2655 || Lr: 0.000050
2024-02-09 12:39:27,229 Epoch 2730: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 12:39:27,229 EPOCH 2731
2024-02-09 12:39:38,021 Epoch 2731: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 12:39:38,022 EPOCH 2732
2024-02-09 12:39:48,821 Epoch 2732: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 12:39:48,821 EPOCH 2733
2024-02-09 12:39:59,833 Epoch 2733: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-09 12:39:59,834 EPOCH 2734
2024-02-09 12:40:10,567 Epoch 2734: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-09 12:40:10,568 EPOCH 2735
2024-02-09 12:40:21,716 Epoch 2735: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-09 12:40:21,716 EPOCH 2736
2024-02-09 12:40:22,879 [Epoch: 2736 Step: 00046500] Batch Recognition Loss:   0.000350 => Gls Tokens per Sec:     2754 || Batch Translation Loss:   0.020044 => Txt Tokens per Sec:     7940 || Lr: 0.000050
2024-02-09 12:40:32,476 Epoch 2736: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-09 12:40:32,476 EPOCH 2737
2024-02-09 12:40:43,081 Epoch 2737: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-09 12:40:43,081 EPOCH 2738
2024-02-09 12:40:53,823 Epoch 2738: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-09 12:40:53,823 EPOCH 2739
2024-02-09 12:41:04,895 Epoch 2739: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 12:41:04,895 EPOCH 2740
2024-02-09 12:41:15,865 Epoch 2740: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-09 12:41:15,865 EPOCH 2741
2024-02-09 12:41:25,907 Epoch 2741: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-09 12:41:25,908 EPOCH 2742
2024-02-09 12:41:28,296 [Epoch: 2742 Step: 00046600] Batch Recognition Loss:   0.000159 => Gls Tokens per Sec:      804 || Batch Translation Loss:   0.012106 => Txt Tokens per Sec:     2261 || Lr: 0.000050
2024-02-09 12:41:36,745 Epoch 2742: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 12:41:36,746 EPOCH 2743
2024-02-09 12:41:47,612 Epoch 2743: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 12:41:47,613 EPOCH 2744
2024-02-09 12:41:58,482 Epoch 2744: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-09 12:41:58,482 EPOCH 2745
2024-02-09 12:42:09,437 Epoch 2745: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 12:42:09,437 EPOCH 2746
2024-02-09 12:42:20,165 Epoch 2746: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-09 12:42:20,165 EPOCH 2747
2024-02-09 12:42:30,960 Epoch 2747: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 12:42:30,961 EPOCH 2748
2024-02-09 12:42:31,244 [Epoch: 2748 Step: 00046700] Batch Recognition Loss:   0.000169 => Gls Tokens per Sec:     2269 || Batch Translation Loss:   0.014416 => Txt Tokens per Sec:     7188 || Lr: 0.000050
2024-02-09 12:42:41,872 Epoch 2748: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 12:42:41,872 EPOCH 2749
2024-02-09 12:42:53,068 Epoch 2749: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-09 12:42:53,069 EPOCH 2750
2024-02-09 12:43:03,867 Epoch 2750: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-09 12:43:03,867 EPOCH 2751
2024-02-09 12:43:14,638 Epoch 2751: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-09 12:43:14,638 EPOCH 2752
2024-02-09 12:43:25,640 Epoch 2752: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.02 
2024-02-09 12:43:25,640 EPOCH 2753
2024-02-09 12:43:36,387 [Epoch: 2753 Step: 00046800] Batch Recognition Loss:   0.000259 => Gls Tokens per Sec:      929 || Batch Translation Loss:   0.066057 => Txt Tokens per Sec:     2641 || Lr: 0.000050
2024-02-09 12:43:36,497 Epoch 2753: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.99 
2024-02-09 12:43:36,497 EPOCH 2754
2024-02-09 12:43:47,379 Epoch 2754: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.37 
2024-02-09 12:43:47,380 EPOCH 2755
2024-02-09 12:43:58,261 Epoch 2755: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.83 
2024-02-09 12:43:58,262 EPOCH 2756
2024-02-09 12:44:09,229 Epoch 2756: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-09 12:44:09,230 EPOCH 2757
2024-02-09 12:44:20,126 Epoch 2757: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-09 12:44:20,127 EPOCH 2758
2024-02-09 12:44:31,046 Epoch 2758: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-09 12:44:31,047 EPOCH 2759
2024-02-09 12:44:39,352 [Epoch: 2759 Step: 00046900] Batch Recognition Loss:   0.000200 => Gls Tokens per Sec:     1048 || Batch Translation Loss:   0.015362 => Txt Tokens per Sec:     2820 || Lr: 0.000050
2024-02-09 12:44:41,965 Epoch 2759: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-09 12:44:41,965 EPOCH 2760
2024-02-09 12:44:52,519 Epoch 2760: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-09 12:44:52,520 EPOCH 2761
2024-02-09 12:45:03,244 Epoch 2761: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-09 12:45:03,244 EPOCH 2762
2024-02-09 12:45:14,073 Epoch 2762: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-09 12:45:14,073 EPOCH 2763
2024-02-09 12:45:25,095 Epoch 2763: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-09 12:45:25,096 EPOCH 2764
2024-02-09 12:45:35,888 Epoch 2764: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-09 12:45:35,889 EPOCH 2765
2024-02-09 12:45:44,029 [Epoch: 2765 Step: 00047000] Batch Recognition Loss:   0.000903 => Gls Tokens per Sec:      912 || Batch Translation Loss:   0.014196 => Txt Tokens per Sec:     2521 || Lr: 0.000050
2024-02-09 12:45:46,704 Epoch 2765: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-09 12:45:46,704 EPOCH 2766
2024-02-09 12:45:57,755 Epoch 2766: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 12:45:57,756 EPOCH 2767
2024-02-09 12:46:08,867 Epoch 2767: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 12:46:08,868 EPOCH 2768
2024-02-09 12:46:20,178 Epoch 2768: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-09 12:46:20,179 EPOCH 2769
2024-02-09 12:46:32,834 Epoch 2769: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 12:46:32,835 EPOCH 2770
2024-02-09 12:46:44,784 Epoch 2770: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 12:46:44,785 EPOCH 2771
2024-02-09 12:46:51,437 [Epoch: 2771 Step: 00047100] Batch Recognition Loss:   0.000138 => Gls Tokens per Sec:      923 || Batch Translation Loss:   0.012408 => Txt Tokens per Sec:     2411 || Lr: 0.000050
2024-02-09 12:46:57,154 Epoch 2771: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-09 12:46:57,154 EPOCH 2772
2024-02-09 12:47:09,320 Epoch 2772: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-09 12:47:09,321 EPOCH 2773
2024-02-09 12:47:21,366 Epoch 2773: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 12:47:21,366 EPOCH 2774
2024-02-09 12:47:32,732 Epoch 2774: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 12:47:32,732 EPOCH 2775
2024-02-09 12:47:44,510 Epoch 2775: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 12:47:44,510 EPOCH 2776
2024-02-09 12:47:56,059 Epoch 2776: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 12:47:56,060 EPOCH 2777
2024-02-09 12:47:59,661 [Epoch: 2777 Step: 00047200] Batch Recognition Loss:   0.000121 => Gls Tokens per Sec:     1422 || Batch Translation Loss:   0.011839 => Txt Tokens per Sec:     3782 || Lr: 0.000050
2024-02-09 12:48:07,808 Epoch 2777: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 12:48:07,808 EPOCH 2778
2024-02-09 12:48:20,795 Epoch 2778: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 12:48:20,795 EPOCH 2779
2024-02-09 12:48:33,283 Epoch 2779: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 12:48:33,284 EPOCH 2780
2024-02-09 12:48:44,979 Epoch 2780: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 12:48:44,980 EPOCH 2781
2024-02-09 12:48:56,616 Epoch 2781: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 12:48:56,616 EPOCH 2782
2024-02-09 12:49:08,524 Epoch 2782: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-09 12:49:08,524 EPOCH 2783
2024-02-09 12:52:00,592 [Epoch: 2783 Step: 00047300] Batch Recognition Loss:   0.000942 => Gls Tokens per Sec:       22 || Batch Translation Loss:   0.018555 => Txt Tokens per Sec:       62 || Lr: 0.000050
2024-02-09 13:30:26,676 Epoch 2783: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 13:30:26,677 EPOCH 2784
2024-02-09 13:38:26,318 Epoch 2784: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-09 13:38:26,319 EPOCH 2785
2024-02-09 13:47:13,172 Epoch 2785: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 13:47:13,174 EPOCH 2786
2024-02-09 13:55:57,723 Epoch 2786: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-09 13:55:57,725 EPOCH 2787
2024-02-09 14:06:07,236 Epoch 2787: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 14:06:07,237 EPOCH 2788
2024-02-09 14:15:12,622 Epoch 2788: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 14:15:12,624 EPOCH 2789
2024-02-09 14:15:34,222 [Epoch: 2789 Step: 00047400] Batch Recognition Loss:   0.000168 => Gls Tokens per Sec:      119 || Batch Translation Loss:   0.005658 => Txt Tokens per Sec:      275 || Lr: 0.000050
2024-02-09 14:23:55,143 Epoch 2789: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-09 14:23:55,143 EPOCH 2790
2024-02-09 14:32:28,794 Epoch 2790: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 14:32:28,796 EPOCH 2791
2024-02-09 14:41:03,145 Epoch 2791: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-09 14:41:03,146 EPOCH 2792
2024-02-09 14:48:52,602 Epoch 2792: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-09 14:48:52,603 EPOCH 2793
2024-02-09 14:55:36,991 Epoch 2793: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 14:55:36,992 EPOCH 2794
2024-02-09 15:29:42,314 Epoch 2794: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 15:29:42,315 EPOCH 2795
2024-02-09 15:30:46,609 [Epoch: 2795 Step: 00047500] Batch Recognition Loss:   0.000217 => Gls Tokens per Sec:       20 || Batch Translation Loss:   0.009385 => Txt Tokens per Sec:       52 || Lr: 0.000050
2024-02-09 16:03:36,339 Epoch 2795: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-09 16:03:36,340 EPOCH 2796
2024-02-09 16:04:54,500 Epoch 2796: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-09 16:04:54,502 EPOCH 2797
2024-02-09 16:06:23,935 Epoch 2797: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 16:06:23,937 EPOCH 2798
2024-02-09 16:07:47,124 Epoch 2798: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-09 16:07:47,125 EPOCH 2799
2024-02-09 16:09:14,956 Epoch 2799: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 16:09:14,958 EPOCH 2800
2024-02-09 16:10:38,722 [Epoch: 2800 Step: 00047600] Batch Recognition Loss:   0.001678 => Gls Tokens per Sec:      127 || Batch Translation Loss:   0.064918 => Txt Tokens per Sec:      351 || Lr: 0.000050
2024-02-09 16:10:38,723 Epoch 2800: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.35 
2024-02-09 16:10:38,724 EPOCH 2801
2024-02-09 16:12:20,252 Epoch 2801: Total Training Recognition Loss 0.13  Total Training Translation Loss 0.44 
2024-02-09 16:12:20,254 EPOCH 2802
2024-02-09 16:13:32,827 Epoch 2802: Total Training Recognition Loss 0.91  Total Training Translation Loss 0.77 
2024-02-09 16:13:32,829 EPOCH 2803
2024-02-09 16:14:01,049 Epoch 2803: Total Training Recognition Loss 1.24  Total Training Translation Loss 1.07 
2024-02-09 16:14:01,049 EPOCH 2804
2024-02-09 16:14:12,568 Epoch 2804: Total Training Recognition Loss 0.14  Total Training Translation Loss 0.99 
2024-02-09 16:14:12,569 EPOCH 2805
2024-02-09 16:14:23,686 Epoch 2805: Total Training Recognition Loss 0.07  Total Training Translation Loss 0.89 
2024-02-09 16:14:23,687 EPOCH 2806
2024-02-09 16:14:34,353 [Epoch: 2806 Step: 00047700] Batch Recognition Loss:   0.000533 => Gls Tokens per Sec:      876 || Batch Translation Loss:   0.042089 => Txt Tokens per Sec:     2435 || Lr: 0.000050
2024-02-09 16:14:34,819 Epoch 2806: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.70 
2024-02-09 16:14:34,819 EPOCH 2807
2024-02-09 16:14:45,851 Epoch 2807: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.59 
2024-02-09 16:14:45,852 EPOCH 2808
2024-02-09 16:14:56,841 Epoch 2808: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.50 
2024-02-09 16:14:56,841 EPOCH 2809
2024-02-09 16:15:08,414 Epoch 2809: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.40 
2024-02-09 16:15:08,415 EPOCH 2810
2024-02-09 16:15:19,309 Epoch 2810: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.39 
2024-02-09 16:15:19,310 EPOCH 2811
2024-02-09 16:15:30,448 Epoch 2811: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-09 16:15:30,448 EPOCH 2812
2024-02-09 16:15:37,343 [Epoch: 2812 Step: 00047800] Batch Recognition Loss:   0.000145 => Gls Tokens per Sec:     1169 || Batch Translation Loss:   0.015860 => Txt Tokens per Sec:     3132 || Lr: 0.000050
2024-02-09 16:15:41,463 Epoch 2812: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-09 16:15:41,464 EPOCH 2813
2024-02-09 16:15:52,823 Epoch 2813: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 16:15:52,824 EPOCH 2814
2024-02-09 16:16:03,835 Epoch 2814: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-09 16:16:03,836 EPOCH 2815
2024-02-09 16:16:14,693 Epoch 2815: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 16:16:14,693 EPOCH 2816
2024-02-09 16:16:25,441 Epoch 2816: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 16:16:25,442 EPOCH 2817
2024-02-09 16:16:37,040 Epoch 2817: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-09 16:16:37,041 EPOCH 2818
2024-02-09 16:16:45,305 [Epoch: 2818 Step: 00047900] Batch Recognition Loss:   0.001586 => Gls Tokens per Sec:      821 || Batch Translation Loss:   0.021848 => Txt Tokens per Sec:     2192 || Lr: 0.000050
2024-02-09 16:16:48,519 Epoch 2818: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 16:16:48,519 EPOCH 2819
2024-02-09 16:16:59,792 Epoch 2819: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 16:16:59,793 EPOCH 2820
2024-02-09 16:17:13,219 Epoch 2820: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 16:17:13,219 EPOCH 2821
2024-02-09 16:17:24,494 Epoch 2821: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 16:17:24,495 EPOCH 2822
2024-02-09 16:17:35,512 Epoch 2822: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 16:17:35,513 EPOCH 2823
2024-02-09 16:17:46,447 Epoch 2823: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 16:17:46,447 EPOCH 2824
2024-02-09 16:17:50,431 [Epoch: 2824 Step: 00048000] Batch Recognition Loss:   0.000166 => Gls Tokens per Sec:     1446 || Batch Translation Loss:   0.014351 => Txt Tokens per Sec:     4091 || Lr: 0.000050
2024-02-09 16:18:36,812 Validation result at epoch 2824, step    48000: duration: 46.3807s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.53513	Translation Loss: 99988.21094	PPL: 21738.41797
	Eval Metric: BLEU
	WER 2.68	(DEL: 0.00,	INS: 0.00,	SUB: 2.68)
	BLEU-4 0.00	(BLEU-1: 10.17,	BLEU-2: 2.76,	BLEU-3: 0.88,	BLEU-4: 0.00)
	CHRF 16.56	ROUGE 8.66
2024-02-09 16:18:36,814 Logging Recognition and Translation Outputs
2024-02-09 16:18:36,815 ========================================================================================================================
2024-02-09 16:18:36,815 Logging Sequence: 96_93.00
2024-02-09 16:18:36,815 	Gloss Reference :	A B+C+D+E
2024-02-09 16:18:36,815 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 16:18:36,815 	Gloss Alignment :	         
2024-02-09 16:18:36,816 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 16:18:36,817 	Text Reference  :	bhuvneshwar kumar took 4    wickets and  hardik pandya took  3    wickets wonderful
2024-02-09 16:18:36,817 	Text Hypothesis :	*********** ***** the  next man     sent to     know   about pant and     won      
2024-02-09 16:18:36,817 	Text Alignment  :	D           D     S    S    S       S    S      S      S     S    S       S        
2024-02-09 16:18:36,817 ========================================================================================================================
2024-02-09 16:18:36,817 Logging Sequence: 144_2.00
2024-02-09 16:18:36,818 	Gloss Reference :	A B+C+D+E
2024-02-09 16:18:36,818 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 16:18:36,818 	Gloss Alignment :	         
2024-02-09 16:18:36,818 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 16:18:36,820 	Text Reference  :	a girl posted a video of herself playing cricket  on   a village farm  on     social media    the video has  gone viral  
2024-02-09 16:18:36,820 	Text Hypothesis :	* **** ****** * ***** ** ******* police  detained over a ******* dozen people in     brussels and eight more in   antwerp
2024-02-09 16:18:36,820 	Text Alignment  :	D D    D      D D     D  D       S       S        S      D       S     S      S      S        S   S     S    S    S      
2024-02-09 16:18:36,820 ========================================================================================================================
2024-02-09 16:18:36,820 Logging Sequence: 178_83.00
2024-02-09 16:18:36,821 	Gloss Reference :	A B+C+D+E
2024-02-09 16:18:36,821 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 16:18:36,821 	Gloss Alignment :	         
2024-02-09 16:18:36,821 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 16:18:36,822 	Text Reference  :	and the police still     haven't apprehended the    wrestler
2024-02-09 16:18:36,822 	Text Hypothesis :	and *** named  prominent indian  wrestler    sushil kumar   
2024-02-09 16:18:36,822 	Text Alignment  :	    D   S      S         S       S           S      S       
2024-02-09 16:18:36,822 ========================================================================================================================
2024-02-09 16:18:36,822 Logging Sequence: 169_214.00
2024-02-09 16:18:36,822 	Gloss Reference :	A B+C+D+E
2024-02-09 16:18:36,823 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 16:18:36,823 	Gloss Alignment :	         
2024-02-09 16:18:36,823 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 16:18:36,825 	Text Reference  :	virat kohli said that ***** though arshdeep dropped the catch he is still a       strong part of       the indian       team 
2024-02-09 16:18:36,825 	Text Hypothesis :	***** ***** the  that panel will   be       in      the ***** ** ** ***** british to     keep watching all commonwealth games
2024-02-09 16:18:36,826 	Text Alignment  :	D     D     S         I     S      S        S           D     D  D  D     S       S      S    S        S   S            S    
2024-02-09 16:18:36,826 ========================================================================================================================
2024-02-09 16:18:36,826 Logging Sequence: 147_202.00
2024-02-09 16:18:36,826 	Gloss Reference :	A B+C+D+E
2024-02-09 16:18:36,826 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 16:18:36,826 	Gloss Alignment :	         
2024-02-09 16:18:36,826 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 16:18:36,829 	Text Reference  :	were impressed that she  took the  difficult decision to  withdraw from the ****** ***** ***** olympics and       focus on  her mental health
2024-02-09 16:18:36,829 	Text Hypothesis :	3    if        your love to   tell you       are      now i        want the medals about group is       extremely fit   and are sold   out   
2024-02-09 16:18:36,829 	Text Alignment  :	S    S         S    S    S    S    S         S        S   S        S        I      I     I     S        S         S     S   S   S      S     
2024-02-09 16:18:36,829 ========================================================================================================================
2024-02-09 16:18:44,398 Epoch 2824: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 16:18:44,398 EPOCH 2825
2024-02-09 16:18:55,854 Epoch 2825: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 16:18:55,855 EPOCH 2826
2024-02-09 16:19:06,809 Epoch 2826: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 16:19:06,809 EPOCH 2827
2024-02-09 16:19:17,675 Epoch 2827: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 16:19:17,675 EPOCH 2828
2024-02-09 16:19:28,940 Epoch 2828: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 16:19:28,941 EPOCH 2829
2024-02-09 16:19:40,141 Epoch 2829: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-09 16:19:40,142 EPOCH 2830
2024-02-09 16:19:45,319 [Epoch: 2830 Step: 00048100] Batch Recognition Loss:   0.000387 => Gls Tokens per Sec:      866 || Batch Translation Loss:   0.015241 => Txt Tokens per Sec:     2482 || Lr: 0.000050
2024-02-09 16:19:51,267 Epoch 2830: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 16:19:51,267 EPOCH 2831
2024-02-09 16:20:02,021 Epoch 2831: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-09 16:20:02,021 EPOCH 2832
2024-02-09 16:20:13,141 Epoch 2832: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-09 16:20:13,141 EPOCH 2833
2024-02-09 16:20:24,201 Epoch 2833: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-09 16:20:24,201 EPOCH 2834
2024-02-09 16:20:35,361 Epoch 2834: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-09 16:20:35,362 EPOCH 2835
2024-02-09 16:20:46,596 Epoch 2835: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-09 16:20:46,596 EPOCH 2836
2024-02-09 16:20:50,983 [Epoch: 2836 Step: 00048200] Batch Recognition Loss:   0.000617 => Gls Tokens per Sec:      730 || Batch Translation Loss:   0.022206 => Txt Tokens per Sec:     2217 || Lr: 0.000050
2024-02-09 16:20:57,617 Epoch 2836: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-09 16:20:57,618 EPOCH 2837
2024-02-09 16:21:08,672 Epoch 2837: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 16:21:08,673 EPOCH 2838
2024-02-09 16:21:20,012 Epoch 2838: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 16:21:20,013 EPOCH 2839
2024-02-09 16:21:30,961 Epoch 2839: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 16:21:30,962 EPOCH 2840
2024-02-09 16:21:42,351 Epoch 2840: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-09 16:21:42,352 EPOCH 2841
2024-02-09 16:21:53,470 Epoch 2841: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-09 16:21:53,470 EPOCH 2842
2024-02-09 16:21:55,902 [Epoch: 2842 Step: 00048300] Batch Recognition Loss:   0.000134 => Gls Tokens per Sec:      790 || Batch Translation Loss:   0.012645 => Txt Tokens per Sec:     2437 || Lr: 0.000050
2024-02-09 16:22:04,280 Epoch 2842: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 16:22:04,281 EPOCH 2843
2024-02-09 16:22:15,385 Epoch 2843: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 16:22:15,386 EPOCH 2844
2024-02-09 16:22:26,259 Epoch 2844: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 16:22:26,259 EPOCH 2845
2024-02-09 16:22:37,455 Epoch 2845: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 16:22:37,456 EPOCH 2846
2024-02-09 16:22:48,526 Epoch 2846: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 16:22:48,527 EPOCH 2847
2024-02-09 16:22:59,754 Epoch 2847: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-09 16:22:59,754 EPOCH 2848
2024-02-09 16:23:00,025 [Epoch: 2848 Step: 00048400] Batch Recognition Loss:   0.000213 => Gls Tokens per Sec:     2379 || Batch Translation Loss:   0.022593 => Txt Tokens per Sec:     6874 || Lr: 0.000050
2024-02-09 16:23:10,691 Epoch 2848: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-09 16:23:10,692 EPOCH 2849
2024-02-09 16:23:21,844 Epoch 2849: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-09 16:23:21,845 EPOCH 2850
2024-02-09 16:23:33,132 Epoch 2850: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-09 16:23:33,133 EPOCH 2851
2024-02-09 16:23:44,244 Epoch 2851: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.91 
2024-02-09 16:23:44,244 EPOCH 2852
2024-02-09 16:23:55,224 Epoch 2852: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.11 
2024-02-09 16:23:55,225 EPOCH 2853
2024-02-09 16:24:05,763 [Epoch: 2853 Step: 00048500] Batch Recognition Loss:   0.000325 => Gls Tokens per Sec:      947 || Batch Translation Loss:   0.025830 => Txt Tokens per Sec:     2618 || Lr: 0.000050
2024-02-09 16:24:05,986 Epoch 2853: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-09 16:24:05,987 EPOCH 2854
2024-02-09 16:24:16,811 Epoch 2854: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-09 16:24:16,812 EPOCH 2855
2024-02-09 16:24:27,757 Epoch 2855: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-09 16:24:27,758 EPOCH 2856
2024-02-09 16:24:38,715 Epoch 2856: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-09 16:24:38,716 EPOCH 2857
2024-02-09 16:24:49,589 Epoch 2857: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-09 16:24:49,590 EPOCH 2858
2024-02-09 16:25:00,738 Epoch 2858: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-09 16:25:00,739 EPOCH 2859
2024-02-09 16:25:10,942 [Epoch: 2859 Step: 00048600] Batch Recognition Loss:   0.000131 => Gls Tokens per Sec:      853 || Batch Translation Loss:   0.036415 => Txt Tokens per Sec:     2406 || Lr: 0.000050
2024-02-09 16:25:11,666 Epoch 2859: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-09 16:25:11,666 EPOCH 2860
2024-02-09 16:25:22,762 Epoch 2860: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-09 16:25:22,762 EPOCH 2861
2024-02-09 16:25:33,617 Epoch 2861: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-09 16:25:33,617 EPOCH 2862
2024-02-09 16:25:44,392 Epoch 2862: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-09 16:25:44,393 EPOCH 2863
2024-02-09 16:25:54,734 Epoch 2863: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-09 16:25:54,734 EPOCH 2864
2024-02-09 16:26:05,048 Epoch 2864: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-09 16:26:05,048 EPOCH 2865
2024-02-09 16:26:12,899 [Epoch: 2865 Step: 00048700] Batch Recognition Loss:   0.000197 => Gls Tokens per Sec:      945 || Batch Translation Loss:   0.016368 => Txt Tokens per Sec:     2513 || Lr: 0.000050
2024-02-09 16:26:16,126 Epoch 2865: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-09 16:26:16,126 EPOCH 2866
2024-02-09 16:26:27,156 Epoch 2866: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-09 16:26:27,156 EPOCH 2867
2024-02-09 16:26:38,096 Epoch 2867: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.40 
2024-02-09 16:26:38,096 EPOCH 2868
2024-02-09 16:26:48,977 Epoch 2868: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-09 16:26:48,977 EPOCH 2869
2024-02-09 16:26:59,452 Epoch 2869: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-09 16:26:59,453 EPOCH 2870
2024-02-09 16:27:10,311 Epoch 2870: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-09 16:27:10,312 EPOCH 2871
2024-02-09 16:27:13,772 [Epoch: 2871 Step: 00048800] Batch Recognition Loss:   0.000182 => Gls Tokens per Sec:     1850 || Batch Translation Loss:   0.021078 => Txt Tokens per Sec:     4881 || Lr: 0.000050
2024-02-09 16:27:20,945 Epoch 2871: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-09 16:27:20,946 EPOCH 2872
2024-02-09 16:27:31,918 Epoch 2872: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-09 16:27:31,918 EPOCH 2873
2024-02-09 16:27:42,748 Epoch 2873: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-09 16:27:42,748 EPOCH 2874
2024-02-09 16:27:53,315 Epoch 2874: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-09 16:27:53,316 EPOCH 2875
2024-02-09 16:28:04,050 Epoch 2875: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-09 16:28:04,050 EPOCH 2876
2024-02-09 16:28:15,325 Epoch 2876: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-09 16:28:15,326 EPOCH 2877
2024-02-09 16:28:20,132 [Epoch: 2877 Step: 00048900] Batch Recognition Loss:   0.000167 => Gls Tokens per Sec:     1066 || Batch Translation Loss:   0.017389 => Txt Tokens per Sec:     2827 || Lr: 0.000050
2024-02-09 16:28:26,316 Epoch 2877: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-09 16:28:26,316 EPOCH 2878
2024-02-09 16:28:38,124 Epoch 2878: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 16:28:38,125 EPOCH 2879
2024-02-09 16:28:49,354 Epoch 2879: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 16:28:49,355 EPOCH 2880
2024-02-09 16:29:00,395 Epoch 2880: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 16:29:00,395 EPOCH 2881
2024-02-09 16:29:11,252 Epoch 2881: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 16:29:11,253 EPOCH 2882
2024-02-09 16:29:22,267 Epoch 2882: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 16:29:22,268 EPOCH 2883
2024-02-09 16:29:25,936 [Epoch: 2883 Step: 00049000] Batch Recognition Loss:   0.000401 => Gls Tokens per Sec:      976 || Batch Translation Loss:   0.009055 => Txt Tokens per Sec:     2575 || Lr: 0.000050
2024-02-09 16:29:33,439 Epoch 2883: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 16:29:33,439 EPOCH 2884
2024-02-09 16:29:44,495 Epoch 2884: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 16:29:44,495 EPOCH 2885
2024-02-09 16:29:55,348 Epoch 2885: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 16:29:55,348 EPOCH 2886
2024-02-09 16:30:06,249 Epoch 2886: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 16:30:06,250 EPOCH 2887
2024-02-09 16:30:17,078 Epoch 2887: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 16:30:17,078 EPOCH 2888
2024-02-09 16:30:28,152 Epoch 2888: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-09 16:30:28,153 EPOCH 2889
2024-02-09 16:30:31,807 [Epoch: 2889 Step: 00049100] Batch Recognition Loss:   0.000264 => Gls Tokens per Sec:      701 || Batch Translation Loss:   0.017353 => Txt Tokens per Sec:     2067 || Lr: 0.000050
2024-02-09 16:30:38,827 Epoch 2889: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-09 16:30:38,828 EPOCH 2890
2024-02-09 16:30:49,647 Epoch 2890: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-09 16:30:49,648 EPOCH 2891
2024-02-09 16:31:00,925 Epoch 2891: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-09 16:31:00,925 EPOCH 2892
2024-02-09 16:31:11,902 Epoch 2892: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 16:31:11,902 EPOCH 2893
2024-02-09 16:31:22,865 Epoch 2893: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-09 16:31:22,866 EPOCH 2894
2024-02-09 16:31:34,016 Epoch 2894: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-09 16:31:34,017 EPOCH 2895
2024-02-09 16:31:34,576 [Epoch: 2895 Step: 00049200] Batch Recognition Loss:   0.000148 => Gls Tokens per Sec:     2294 || Batch Translation Loss:   0.054391 => Txt Tokens per Sec:     6901 || Lr: 0.000050
2024-02-09 16:31:45,012 Epoch 2895: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-09 16:31:45,013 EPOCH 2896
2024-02-09 16:31:55,948 Epoch 2896: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-09 16:31:55,948 EPOCH 2897
2024-02-09 16:32:06,769 Epoch 2897: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-09 16:32:06,770 EPOCH 2898
2024-02-09 16:32:17,843 Epoch 2898: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.58 
2024-02-09 16:32:17,844 EPOCH 2899
2024-02-09 16:32:29,026 Epoch 2899: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-09 16:32:29,027 EPOCH 2900
2024-02-09 16:32:40,164 [Epoch: 2900 Step: 00049300] Batch Recognition Loss:   0.000311 => Gls Tokens per Sec:      954 || Batch Translation Loss:   0.018090 => Txt Tokens per Sec:     2639 || Lr: 0.000050
2024-02-09 16:32:40,165 Epoch 2900: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-09 16:32:40,165 EPOCH 2901
2024-02-09 16:32:51,300 Epoch 2901: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-09 16:32:51,300 EPOCH 2902
2024-02-09 16:33:02,382 Epoch 2902: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-09 16:33:02,382 EPOCH 2903
2024-02-09 16:33:13,367 Epoch 2903: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-09 16:33:13,368 EPOCH 2904
2024-02-09 16:33:24,430 Epoch 2904: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-09 16:33:24,430 EPOCH 2905
2024-02-09 16:33:35,205 Epoch 2905: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-09 16:33:35,205 EPOCH 2906
2024-02-09 16:33:45,295 [Epoch: 2906 Step: 00049400] Batch Recognition Loss:   0.001167 => Gls Tokens per Sec:      926 || Batch Translation Loss:   0.029034 => Txt Tokens per Sec:     2523 || Lr: 0.000050
2024-02-09 16:33:45,912 Epoch 2906: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-09 16:33:45,913 EPOCH 2907
2024-02-09 16:33:56,775 Epoch 2907: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-09 16:33:56,776 EPOCH 2908
2024-02-09 16:34:07,549 Epoch 2908: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-09 16:34:07,549 EPOCH 2909
2024-02-09 16:34:18,518 Epoch 2909: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-09 16:34:18,519 EPOCH 2910
2024-02-09 16:34:29,582 Epoch 2910: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-09 16:34:29,582 EPOCH 2911
2024-02-09 16:34:40,356 Epoch 2911: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-09 16:34:40,357 EPOCH 2912
2024-02-09 16:34:48,503 [Epoch: 2912 Step: 00049500] Batch Recognition Loss:   0.000162 => Gls Tokens per Sec:      990 || Batch Translation Loss:   0.015426 => Txt Tokens per Sec:     2729 || Lr: 0.000050
2024-02-09 16:34:51,304 Epoch 2912: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-09 16:34:51,304 EPOCH 2913
2024-02-09 16:35:02,401 Epoch 2913: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 16:35:02,401 EPOCH 2914
2024-02-09 16:35:13,220 Epoch 2914: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 16:35:13,221 EPOCH 2915
2024-02-09 16:35:24,437 Epoch 2915: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-09 16:35:24,438 EPOCH 2916
2024-02-09 16:35:35,321 Epoch 2916: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-09 16:35:35,321 EPOCH 2917
2024-02-09 16:35:46,580 Epoch 2917: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-09 16:35:46,581 EPOCH 2918
2024-02-09 16:35:51,938 [Epoch: 2918 Step: 00049600] Batch Recognition Loss:   0.000718 => Gls Tokens per Sec:     1314 || Batch Translation Loss:   0.010636 => Txt Tokens per Sec:     3458 || Lr: 0.000050
2024-02-09 16:35:57,752 Epoch 2918: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 16:35:57,752 EPOCH 2919
2024-02-09 16:36:08,952 Epoch 2919: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 16:36:08,953 EPOCH 2920
2024-02-09 16:36:20,076 Epoch 2920: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-09 16:36:20,077 EPOCH 2921
2024-02-09 16:36:31,338 Epoch 2921: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-09 16:36:31,339 EPOCH 2922
2024-02-09 16:36:42,471 Epoch 2922: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-09 16:36:42,472 EPOCH 2923
2024-02-09 16:36:53,550 Epoch 2923: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-09 16:36:53,551 EPOCH 2924
2024-02-09 16:36:59,454 [Epoch: 2924 Step: 00049700] Batch Recognition Loss:   0.000381 => Gls Tokens per Sec:      932 || Batch Translation Loss:   0.019809 => Txt Tokens per Sec:     2580 || Lr: 0.000050
2024-02-09 16:37:04,191 Epoch 2924: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-09 16:37:04,192 EPOCH 2925
2024-02-09 16:37:15,221 Epoch 2925: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-09 16:37:15,221 EPOCH 2926
2024-02-09 16:37:25,983 Epoch 2926: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-09 16:37:25,983 EPOCH 2927
2024-02-09 16:37:37,042 Epoch 2927: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-09 16:37:37,043 EPOCH 2928
2024-02-09 16:37:47,453 Epoch 2928: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-09 16:37:47,453 EPOCH 2929
2024-02-09 16:37:58,467 Epoch 2929: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-09 16:37:58,467 EPOCH 2930
2024-02-09 16:38:01,215 [Epoch: 2930 Step: 00049800] Batch Recognition Loss:   0.000213 => Gls Tokens per Sec:     1631 || Batch Translation Loss:   0.023498 => Txt Tokens per Sec:     4379 || Lr: 0.000050
2024-02-09 16:38:09,088 Epoch 2930: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-09 16:38:09,088 EPOCH 2931
2024-02-09 16:38:19,973 Epoch 2931: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-09 16:38:19,973 EPOCH 2932
2024-02-09 16:38:30,950 Epoch 2932: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-09 16:38:30,950 EPOCH 2933
2024-02-09 16:38:41,999 Epoch 2933: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-09 16:38:42,000 EPOCH 2934
2024-02-09 16:38:53,000 Epoch 2934: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-09 16:38:53,000 EPOCH 2935
2024-02-09 16:39:03,843 Epoch 2935: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-09 16:39:03,844 EPOCH 2936
2024-02-09 16:39:06,304 [Epoch: 2936 Step: 00049900] Batch Recognition Loss:   0.000748 => Gls Tokens per Sec:     1301 || Batch Translation Loss:   0.046167 => Txt Tokens per Sec:     3426 || Lr: 0.000050
2024-02-09 16:39:14,157 Epoch 2936: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-09 16:39:14,158 EPOCH 2937
2024-02-09 16:39:25,231 Epoch 2937: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-09 16:39:25,232 EPOCH 2938
2024-02-09 16:39:36,078 Epoch 2938: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-09 16:39:36,078 EPOCH 2939
2024-02-09 16:39:46,903 Epoch 2939: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-09 16:39:46,903 EPOCH 2940
2024-02-09 16:39:58,095 Epoch 2940: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-09 16:39:58,095 EPOCH 2941
2024-02-09 16:40:08,721 Epoch 2941: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-09 16:40:08,722 EPOCH 2942
2024-02-09 16:40:13,123 [Epoch: 2942 Step: 00050000] Batch Recognition Loss:   0.000309 => Gls Tokens per Sec:      377 || Batch Translation Loss:   0.020988 => Txt Tokens per Sec:     1132 || Lr: 0.000050
2024-02-09 16:40:57,207 Validation result at epoch 2942, step    50000: duration: 44.0829s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.58569	Translation Loss: 100497.74219	PPL: 22873.36133
	Eval Metric: BLEU
	WER 2.97	(DEL: 0.00,	INS: 0.00,	SUB: 2.97)
	BLEU-4 0.34	(BLEU-1: 10.31,	BLEU-2: 2.84,	BLEU-3: 0.97,	BLEU-4: 0.34)
	CHRF 16.80	ROUGE 8.46
2024-02-09 16:40:57,209 Logging Recognition and Translation Outputs
2024-02-09 16:40:57,209 ========================================================================================================================
2024-02-09 16:40:57,209 Logging Sequence: 178_157.00
2024-02-09 16:40:57,210 	Gloss Reference :	A B+C+D+E
2024-02-09 16:40:57,210 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 16:40:57,210 	Gloss Alignment :	         
2024-02-09 16:40:57,212 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 16:40:57,213 	Text Reference  :	** *** this is  why sushil     kumar will have   to    be arrested
2024-02-09 16:40:57,214 	Text Hypothesis :	on 5th may  the was questioned by    the  police filed a  few     
2024-02-09 16:40:57,214 	Text Alignment  :	I  I   S    S   S   S          S     S    S      S     S  S       
2024-02-09 16:40:57,214 ========================================================================================================================
2024-02-09 16:40:57,214 Logging Sequence: 118_111.00
2024-02-09 16:40:57,214 	Gloss Reference :	A B+C+D+E
2024-02-09 16:40:57,214 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 16:40:57,214 	Gloss Alignment :	         
2024-02-09 16:40:57,215 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 16:40:57,216 	Text Reference  :	***** and people encourage him  have hope  for the  next world cup 
2024-02-09 16:40:57,216 	Text Hypothesis :	since the match  are       made 4    times i   will be   made  them
2024-02-09 16:40:57,216 	Text Alignment  :	I     S   S      S         S    S    S     S   S    S    S     S   
2024-02-09 16:40:57,216 ========================================================================================================================
2024-02-09 16:40:57,216 Logging Sequence: 148_2.00
2024-02-09 16:40:57,216 	Gloss Reference :	A B+C+D+E
2024-02-09 16:40:57,216 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 16:40:57,217 	Gloss Alignment :	         
2024-02-09 16:40:57,217 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 16:40:57,219 	Text Reference  :	the final of the  asia  cup 2023 cricket   tournament was  played between india   and sri lanka   on 17th september 2023
2024-02-09 16:40:57,219 	Text Hypothesis :	*** ***** ** like india has bcci secretary jay        shah for    1       century and 50  million on 12th january   2022
2024-02-09 16:40:57,219 	Text Alignment  :	D   D     D  S    S     S   S    S         S          S    S      S       S           S   S          S    S         S   
2024-02-09 16:40:57,219 ========================================================================================================================
2024-02-09 16:40:57,219 Logging Sequence: 83_129.00
2024-02-09 16:40:57,219 	Gloss Reference :	A B+C+D+E
2024-02-09 16:40:57,220 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 16:40:57,220 	Gloss Alignment :	         
2024-02-09 16:40:57,220 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 16:40:57,220 	Text Reference  :	** *** later the    denmark football association tweeted
2024-02-09 16:40:57,220 	Text Hypothesis :	he was just  joking around  with     each        other  
2024-02-09 16:40:57,221 	Text Alignment  :	I  I   S     S      S       S        S           S      
2024-02-09 16:40:57,221 ========================================================================================================================
2024-02-09 16:40:57,221 Logging Sequence: 99_158.00
2024-02-09 16:40:57,221 	Gloss Reference :	A B+C+D+E
2024-02-09 16:40:57,221 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 16:40:57,221 	Gloss Alignment :	         
2024-02-09 16:40:57,221 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 16:40:57,222 	Text Reference  :	the    incident occured in dubai and it was extremely shameful
2024-02-09 16:40:57,222 	Text Hypothesis :	people had      met     in ***** *** ** *** ********* home    
2024-02-09 16:40:57,222 	Text Alignment  :	S      S        S          D     D   D  D   D         S       
2024-02-09 16:40:57,222 ========================================================================================================================
2024-02-09 16:41:04,305 Epoch 2942: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 16:41:04,305 EPOCH 2943
2024-02-09 16:41:15,852 Epoch 2943: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-09 16:41:15,852 EPOCH 2944
2024-02-09 16:41:26,821 Epoch 2944: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 16:41:26,822 EPOCH 2945
2024-02-09 16:41:37,917 Epoch 2945: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 16:41:37,917 EPOCH 2946
2024-02-09 16:41:49,021 Epoch 2946: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 16:41:49,022 EPOCH 2947
2024-02-09 16:41:59,784 Epoch 2947: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 16:41:59,785 EPOCH 2948
2024-02-09 16:41:59,931 [Epoch: 2948 Step: 00050100] Batch Recognition Loss:   0.000154 => Gls Tokens per Sec:     4445 || Batch Translation Loss:   0.009572 => Txt Tokens per Sec:     8910 || Lr: 0.000050
2024-02-09 16:42:10,776 Epoch 2948: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-09 16:42:10,777 EPOCH 2949
2024-02-09 16:42:21,634 Epoch 2949: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 16:42:21,635 EPOCH 2950
2024-02-09 16:42:33,928 Epoch 2950: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-09 16:42:33,929 EPOCH 2951
2024-02-09 16:42:45,585 Epoch 2951: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-09 16:42:45,586 EPOCH 2952
2024-02-09 16:42:56,977 Epoch 2952: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-09 16:42:56,979 EPOCH 2953
2024-02-09 16:43:09,474 [Epoch: 2953 Step: 00050200] Batch Recognition Loss:   0.000609 => Gls Tokens per Sec:      799 || Batch Translation Loss:   0.018881 => Txt Tokens per Sec:     2239 || Lr: 0.000050
2024-02-09 16:43:09,673 Epoch 2953: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 16:43:09,673 EPOCH 2954
2024-02-09 16:43:21,282 Epoch 2954: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 16:43:21,282 EPOCH 2955
2024-02-09 16:43:32,766 Epoch 2955: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-09 16:43:32,767 EPOCH 2956
2024-02-09 16:43:44,303 Epoch 2956: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-09 16:43:44,304 EPOCH 2957
2024-02-09 16:43:55,723 Epoch 2957: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 16:43:55,723 EPOCH 2958
2024-02-09 16:44:07,196 Epoch 2958: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 16:44:07,196 EPOCH 2959
2024-02-09 16:44:17,790 [Epoch: 2959 Step: 00050300] Batch Recognition Loss:   0.002080 => Gls Tokens per Sec:      821 || Batch Translation Loss:   0.007696 => Txt Tokens per Sec:     2328 || Lr: 0.000050
2024-02-09 16:44:18,384 Epoch 2959: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-09 16:44:18,384 EPOCH 2960
2024-02-09 16:44:29,389 Epoch 2960: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.81 
2024-02-09 16:44:29,390 EPOCH 2961
2024-02-09 16:44:40,412 Epoch 2961: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.90 
2024-02-09 16:44:40,412 EPOCH 2962
2024-02-09 16:44:51,408 Epoch 2962: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.72 
2024-02-09 16:44:51,409 EPOCH 2963
2024-02-09 16:45:02,529 Epoch 2963: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-09 16:45:02,530 EPOCH 2964
2024-02-09 16:45:13,455 Epoch 2964: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-09 16:45:13,455 EPOCH 2965
2024-02-09 16:45:21,993 [Epoch: 2965 Step: 00050400] Batch Recognition Loss:   0.000418 => Gls Tokens per Sec:      869 || Batch Translation Loss:   0.025573 => Txt Tokens per Sec:     2465 || Lr: 0.000050
2024-02-09 16:45:24,433 Epoch 2965: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.31 
2024-02-09 16:45:24,434 EPOCH 2966
2024-02-09 16:45:36,571 Epoch 2966: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.61 
2024-02-09 16:45:36,571 EPOCH 2967
2024-02-09 16:45:48,157 Epoch 2967: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.52 
2024-02-09 16:45:48,158 EPOCH 2968
2024-02-09 16:45:59,704 Epoch 2968: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.94 
2024-02-09 16:45:59,704 EPOCH 2969
2024-02-09 16:46:11,042 Epoch 2969: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-09 16:46:11,043 EPOCH 2970
2024-02-09 16:46:22,385 Epoch 2970: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-09 16:46:22,386 EPOCH 2971
2024-02-09 16:46:28,655 [Epoch: 2971 Step: 00050500] Batch Recognition Loss:   0.001047 => Gls Tokens per Sec:      980 || Batch Translation Loss:   0.017977 => Txt Tokens per Sec:     2670 || Lr: 0.000050
2024-02-09 16:46:33,881 Epoch 2971: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-09 16:46:33,881 EPOCH 2972
2024-02-09 16:46:45,411 Epoch 2972: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-09 16:46:45,411 EPOCH 2973
2024-02-09 16:46:56,910 Epoch 2973: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 16:46:56,910 EPOCH 2974
2024-02-09 16:47:08,187 Epoch 2974: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 16:47:08,189 EPOCH 2975
2024-02-09 16:47:19,732 Epoch 2975: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 16:47:19,732 EPOCH 2976
2024-02-09 16:47:31,318 Epoch 2976: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 16:47:31,318 EPOCH 2977
2024-02-09 16:47:38,266 [Epoch: 2977 Step: 00050600] Batch Recognition Loss:   0.000212 => Gls Tokens per Sec:      737 || Batch Translation Loss:   0.011362 => Txt Tokens per Sec:     2064 || Lr: 0.000050
2024-02-09 16:47:42,779 Epoch 2977: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 16:47:42,779 EPOCH 2978
2024-02-09 16:47:53,701 Epoch 2978: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 16:47:53,702 EPOCH 2979
2024-02-09 16:48:05,249 Epoch 2979: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 16:48:05,250 EPOCH 2980
2024-02-09 16:48:16,462 Epoch 2980: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 16:48:16,463 EPOCH 2981
2024-02-09 16:48:27,673 Epoch 2981: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-09 16:48:27,674 EPOCH 2982
2024-02-09 16:48:39,049 Epoch 2982: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 16:48:39,050 EPOCH 2983
2024-02-09 16:48:46,506 [Epoch: 2983 Step: 00050700] Batch Recognition Loss:   0.000269 => Gls Tokens per Sec:      480 || Batch Translation Loss:   0.025339 => Txt Tokens per Sec:     1376 || Lr: 0.000050
2024-02-09 16:48:50,669 Epoch 2983: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 16:48:50,670 EPOCH 2984
2024-02-09 16:49:01,821 Epoch 2984: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-09 16:49:01,822 EPOCH 2985
2024-02-09 16:49:13,382 Epoch 2985: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-09 16:49:13,383 EPOCH 2986
2024-02-09 16:49:24,756 Epoch 2986: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-09 16:49:24,756 EPOCH 2987
2024-02-09 16:49:36,047 Epoch 2987: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 16:49:36,048 EPOCH 2988
2024-02-09 16:49:47,521 Epoch 2988: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 16:49:47,522 EPOCH 2989
2024-02-09 16:49:49,937 [Epoch: 2989 Step: 00050800] Batch Recognition Loss:   0.000392 => Gls Tokens per Sec:     1060 || Batch Translation Loss:   0.019796 => Txt Tokens per Sec:     3184 || Lr: 0.000050
2024-02-09 16:49:58,831 Epoch 2989: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 16:49:58,832 EPOCH 2990
2024-02-09 16:50:10,398 Epoch 2990: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 16:50:10,398 EPOCH 2991
2024-02-09 16:50:21,642 Epoch 2991: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-09 16:50:21,642 EPOCH 2992
2024-02-09 16:50:32,931 Epoch 2992: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 16:50:32,932 EPOCH 2993
2024-02-09 16:50:44,269 Epoch 2993: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 16:50:44,269 EPOCH 2994
2024-02-09 16:50:55,640 Epoch 2994: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 16:50:55,641 EPOCH 2995
2024-02-09 16:50:55,964 [Epoch: 2995 Step: 00050900] Batch Recognition Loss:   0.000128 => Gls Tokens per Sec:     3980 || Batch Translation Loss:   0.011091 => Txt Tokens per Sec:     8833 || Lr: 0.000050
2024-02-09 16:51:07,275 Epoch 2995: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 16:51:07,276 EPOCH 2996
2024-02-09 16:51:18,477 Epoch 2996: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-09 16:51:18,478 EPOCH 2997
2024-02-09 16:51:29,470 Epoch 2997: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 16:51:29,471 EPOCH 2998
2024-02-09 16:51:40,786 Epoch 2998: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 16:51:40,786 EPOCH 2999
2024-02-09 16:51:52,241 Epoch 2999: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-09 16:51:52,242 EPOCH 3000
2024-02-09 16:52:03,385 [Epoch: 3000 Step: 00051000] Batch Recognition Loss:   0.000160 => Gls Tokens per Sec:      953 || Batch Translation Loss:   0.012062 => Txt Tokens per Sec:     2637 || Lr: 0.000050
2024-02-09 16:52:03,385 Epoch 3000: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 16:52:03,386 EPOCH 3001
2024-02-09 16:52:14,937 Epoch 3001: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-09 16:52:14,937 EPOCH 3002
2024-02-09 16:52:26,158 Epoch 3002: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-09 16:52:26,158 EPOCH 3003
2024-02-09 16:52:37,612 Epoch 3003: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-09 16:52:37,613 EPOCH 3004
2024-02-09 16:52:48,890 Epoch 3004: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 16:52:48,891 EPOCH 3005
2024-02-09 16:53:00,493 Epoch 3005: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 16:53:00,494 EPOCH 3006
2024-02-09 16:53:11,281 [Epoch: 3006 Step: 00051100] Batch Recognition Loss:   0.000116 => Gls Tokens per Sec:      866 || Batch Translation Loss:   0.010199 => Txt Tokens per Sec:     2366 || Lr: 0.000050
2024-02-09 16:53:11,921 Epoch 3006: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-09 16:53:11,921 EPOCH 3007
2024-02-09 16:53:23,260 Epoch 3007: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-09 16:53:23,261 EPOCH 3008
2024-02-09 16:53:34,639 Epoch 3008: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 16:53:34,640 EPOCH 3009
2024-02-09 16:53:46,163 Epoch 3009: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 16:53:46,164 EPOCH 3010
2024-02-09 16:53:57,738 Epoch 3010: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-09 16:53:57,738 EPOCH 3011
2024-02-09 16:54:09,071 Epoch 3011: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-09 16:54:09,071 EPOCH 3012
2024-02-09 16:54:19,516 [Epoch: 3012 Step: 00051200] Batch Recognition Loss:   0.000161 => Gls Tokens per Sec:      772 || Batch Translation Loss:   0.029112 => Txt Tokens per Sec:     2157 || Lr: 0.000050
2024-02-09 16:54:20,392 Epoch 3012: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-09 16:54:20,392 EPOCH 3013
2024-02-09 16:54:31,698 Epoch 3013: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 16:54:31,698 EPOCH 3014
2024-02-09 16:54:43,227 Epoch 3014: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-09 16:54:43,228 EPOCH 3015
2024-02-09 16:54:54,767 Epoch 3015: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 16:54:54,768 EPOCH 3016
2024-02-09 16:55:06,510 Epoch 3016: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 16:55:06,510 EPOCH 3017
2024-02-09 16:55:17,888 Epoch 3017: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-09 16:55:17,888 EPOCH 3018
2024-02-09 16:55:27,648 [Epoch: 3018 Step: 00051300] Batch Recognition Loss:   0.000212 => Gls Tokens per Sec:      695 || Batch Translation Loss:   0.021748 => Txt Tokens per Sec:     1917 || Lr: 0.000050
2024-02-09 16:55:29,217 Epoch 3018: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-09 16:55:29,218 EPOCH 3019
2024-02-09 16:55:40,476 Epoch 3019: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-09 16:55:40,477 EPOCH 3020
2024-02-09 16:55:51,834 Epoch 3020: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-09 16:55:51,835 EPOCH 3021
2024-02-09 16:56:03,283 Epoch 3021: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-09 16:56:03,284 EPOCH 3022
2024-02-09 16:56:14,427 Epoch 3022: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-09 16:56:14,428 EPOCH 3023
2024-02-09 16:56:25,756 Epoch 3023: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.72 
2024-02-09 16:56:25,757 EPOCH 3024
2024-02-09 16:56:29,228 [Epoch: 3024 Step: 00051400] Batch Recognition Loss:   0.001403 => Gls Tokens per Sec:     1660 || Batch Translation Loss:   0.043755 => Txt Tokens per Sec:     4492 || Lr: 0.000050
2024-02-09 16:56:37,004 Epoch 3024: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-09 16:56:37,004 EPOCH 3025
2024-02-09 16:56:48,525 Epoch 3025: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-09 16:56:48,525 EPOCH 3026
2024-02-09 16:56:59,987 Epoch 3026: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-09 16:56:59,988 EPOCH 3027
2024-02-09 16:57:11,456 Epoch 3027: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-09 16:57:11,457 EPOCH 3028
2024-02-09 16:57:22,625 Epoch 3028: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-09 16:57:22,626 EPOCH 3029
2024-02-09 16:57:34,176 Epoch 3029: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-09 16:57:34,177 EPOCH 3030
2024-02-09 16:57:37,577 [Epoch: 3030 Step: 00051500] Batch Recognition Loss:   0.000257 => Gls Tokens per Sec:     1318 || Batch Translation Loss:   0.019750 => Txt Tokens per Sec:     3527 || Lr: 0.000050
2024-02-09 16:57:45,715 Epoch 3030: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-09 16:57:45,716 EPOCH 3031
2024-02-09 16:57:57,357 Epoch 3031: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-09 16:57:57,358 EPOCH 3032
2024-02-09 16:58:08,666 Epoch 3032: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-09 16:58:08,667 EPOCH 3033
2024-02-09 16:58:19,852 Epoch 3033: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-09 16:58:19,853 EPOCH 3034
2024-02-09 16:58:31,227 Epoch 3034: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-09 16:58:31,228 EPOCH 3035
2024-02-09 16:58:42,571 Epoch 3035: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-09 16:58:42,572 EPOCH 3036
2024-02-09 16:58:47,687 [Epoch: 3036 Step: 00051600] Batch Recognition Loss:   0.000125 => Gls Tokens per Sec:      575 || Batch Translation Loss:   0.018530 => Txt Tokens per Sec:     1610 || Lr: 0.000050
2024-02-09 16:58:54,316 Epoch 3036: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-09 16:58:54,317 EPOCH 3037
2024-02-09 16:59:05,763 Epoch 3037: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 16:59:05,764 EPOCH 3038
2024-02-09 16:59:17,127 Epoch 3038: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 16:59:17,127 EPOCH 3039
2024-02-09 16:59:28,458 Epoch 3039: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 16:59:28,459 EPOCH 3040
2024-02-09 16:59:39,992 Epoch 3040: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 16:59:39,993 EPOCH 3041
2024-02-09 16:59:51,409 Epoch 3041: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 16:59:51,410 EPOCH 3042
2024-02-09 16:59:52,002 [Epoch: 3042 Step: 00051700] Batch Recognition Loss:   0.000177 => Gls Tokens per Sec:     3249 || Batch Translation Loss:   0.013869 => Txt Tokens per Sec:     8523 || Lr: 0.000050
2024-02-09 17:00:02,400 Epoch 3042: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-09 17:00:02,401 EPOCH 3043
2024-02-09 17:00:14,021 Epoch 3043: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-09 17:00:14,021 EPOCH 3044
2024-02-09 17:00:25,641 Epoch 3044: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-09 17:00:25,642 EPOCH 3045
2024-02-09 17:00:37,173 Epoch 3045: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-09 17:00:37,173 EPOCH 3046
2024-02-09 17:00:48,358 Epoch 3046: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-09 17:00:48,358 EPOCH 3047
2024-02-09 17:00:59,981 Epoch 3047: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-09 17:00:59,981 EPOCH 3048
2024-02-09 17:01:00,192 [Epoch: 3048 Step: 00051800] Batch Recognition Loss:   0.000246 => Gls Tokens per Sec:     3040 || Batch Translation Loss:   0.034256 => Txt Tokens per Sec:     7667 || Lr: 0.000050
2024-02-09 17:01:11,181 Epoch 3048: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.98 
2024-02-09 17:01:11,181 EPOCH 3049
2024-02-09 17:01:22,760 Epoch 3049: Total Training Recognition Loss 0.01  Total Training Translation Loss 6.70 
2024-02-09 17:01:22,761 EPOCH 3050
2024-02-09 17:01:34,443 Epoch 3050: Total Training Recognition Loss 0.04  Total Training Translation Loss 6.21 
2024-02-09 17:01:34,444 EPOCH 3051
2024-02-09 17:01:46,085 Epoch 3051: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.71 
2024-02-09 17:01:46,086 EPOCH 3052
2024-02-09 17:01:57,222 Epoch 3052: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.95 
2024-02-09 17:01:57,223 EPOCH 3053
2024-02-09 17:02:06,508 [Epoch: 3053 Step: 00051900] Batch Recognition Loss:   0.001358 => Gls Tokens per Sec:     1075 || Batch Translation Loss:   0.035312 => Txt Tokens per Sec:     2943 || Lr: 0.000050
2024-02-09 17:02:08,430 Epoch 3053: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.55 
2024-02-09 17:02:08,430 EPOCH 3054
2024-02-09 17:02:19,638 Epoch 3054: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-09 17:02:19,638 EPOCH 3055
2024-02-09 17:02:31,047 Epoch 3055: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-09 17:02:31,047 EPOCH 3056
2024-02-09 17:02:42,283 Epoch 3056: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-09 17:02:42,284 EPOCH 3057
2024-02-09 17:02:53,769 Epoch 3057: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 17:02:53,769 EPOCH 3058
2024-02-09 17:03:04,958 Epoch 3058: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-09 17:03:04,959 EPOCH 3059
2024-02-09 17:03:15,856 [Epoch: 3059 Step: 00052000] Batch Recognition Loss:   0.000544 => Gls Tokens per Sec:      799 || Batch Translation Loss:   0.011583 => Txt Tokens per Sec:     2244 || Lr: 0.000050
2024-02-09 17:03:59,115 Validation result at epoch 3059, step    52000: duration: 43.2568s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.52367	Translation Loss: 100813.95312	PPL: 23607.32031
	Eval Metric: BLEU
	WER 2.97	(DEL: 0.00,	INS: 0.00,	SUB: 2.97)
	BLEU-4 0.54	(BLEU-1: 10.10,	BLEU-2: 3.11,	BLEU-3: 1.14,	BLEU-4: 0.54)
	CHRF 16.56	ROUGE 8.58
2024-02-09 17:03:59,117 Logging Recognition and Translation Outputs
2024-02-09 17:03:59,117 ========================================================================================================================
2024-02-09 17:03:59,117 Logging Sequence: 59_101.00
2024-02-09 17:03:59,117 	Gloss Reference :	A B+C+D+E
2024-02-09 17:03:59,117 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 17:03:59,118 	Gloss Alignment :	         
2024-02-09 17:03:59,118 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 17:03:59,120 	Text Reference  :	did you see the     video fox said        she      won her medals  because of a        condom and        is very happy
2024-02-09 17:03:59,120 	Text Hypothesis :	*** a   vow renewal is    a   celebratory ceremony for a   married couple  to reaffirm their  commitment to each other
2024-02-09 17:03:59,120 	Text Alignment  :	D   S   S   S       S     S   S           S        S   S   S       S       S  S        S      S          S  S    S    
2024-02-09 17:03:59,120 ========================================================================================================================
2024-02-09 17:03:59,120 Logging Sequence: 103_112.00
2024-02-09 17:03:59,121 	Gloss Reference :	A B+C+D+E
2024-02-09 17:03:59,121 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 17:03:59,121 	Gloss Alignment :	         
2024-02-09 17:03:59,121 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 17:03:59,122 	Text Reference  :	you are aware that     earlier the britishers had colonized a   lot of     countries in the ***** world  
2024-02-09 17:03:59,123 	Text Hypothesis :	and the vast  majority of      the ********** *** of        the 56  member countries of the delta variant
2024-02-09 17:03:59,123 	Text Alignment  :	S   S   S     S        S           D          D   S         S   S   S                S      I     S      
2024-02-09 17:03:59,123 ========================================================================================================================
2024-02-09 17:03:59,123 Logging Sequence: 143_11.00
2024-02-09 17:03:59,123 	Gloss Reference :	A B+C+D+E
2024-02-09 17:03:59,123 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 17:03:59,124 	Gloss Alignment :	         
2024-02-09 17:03:59,124 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 17:03:59,125 	Text Reference  :	ronaldo has also become the first person to have 500 million followers on      instagram he is the   most    loved    footballer
2024-02-09 17:03:59,125 	Text Hypothesis :	******* *** **** ****** the ***** ****** ** **** ban would   be        applied when      he ** joins another football club      
2024-02-09 17:03:59,125 	Text Alignment  :	D       D   D    D          D     D      D  D    S   S       S         S       S            D  S     S       S        S         
2024-02-09 17:03:59,125 ========================================================================================================================
2024-02-09 17:03:59,126 Logging Sequence: 183_23.00
2024-02-09 17:03:59,126 	Gloss Reference :	A B+C+D+E
2024-02-09 17:03:59,126 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 17:03:59,126 	Gloss Alignment :	         
2024-02-09 17:03:59,126 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 17:03:59,127 	Text Reference  :	however everybody has been waiting for them to   announce  the name of  the child
2024-02-09 17:03:59,127 	Text Hypothesis :	like    india     has **** ******* *** **** bcci secretary jay shah for his wife 
2024-02-09 17:03:59,127 	Text Alignment  :	S       S             D    D       D   D    S    S         S   S    S   S   S    
2024-02-09 17:03:59,127 ========================================================================================================================
2024-02-09 17:03:59,128 Logging Sequence: 169_165.00
2024-02-09 17:03:59,128 	Gloss Reference :	A B+C+D+E
2024-02-09 17:03:59,128 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 17:03:59,128 	Gloss Alignment :	         
2024-02-09 17:03:59,128 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 17:03:59,129 	Text Reference  :	the indian government was outraged  by      the  incident and         these changes were undone by     wikipedia
2024-02-09 17:03:59,130 	Text Hypothesis :	do  you    know       the remaining matches like this     information on    celebs  like their  height age      
2024-02-09 17:03:59,130 	Text Alignment  :	S   S      S          S   S         S       S    S        S           S     S       S    S      S      S        
2024-02-09 17:03:59,130 ========================================================================================================================
2024-02-09 17:04:00,053 Epoch 3059: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-09 17:04:00,054 EPOCH 3060
2024-02-09 17:04:11,825 Epoch 3060: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 17:04:11,826 EPOCH 3061
2024-02-09 17:04:22,764 Epoch 3061: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 17:04:22,765 EPOCH 3062
2024-02-09 17:04:34,104 Epoch 3062: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 17:04:34,104 EPOCH 3063
2024-02-09 17:04:45,068 Epoch 3063: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 17:04:45,069 EPOCH 3064
2024-02-09 17:04:56,359 Epoch 3064: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 17:04:56,360 EPOCH 3065
2024-02-09 17:05:04,939 [Epoch: 3065 Step: 00052100] Batch Recognition Loss:   0.000358 => Gls Tokens per Sec:      865 || Batch Translation Loss:   0.014193 => Txt Tokens per Sec:     2512 || Lr: 0.000050
2024-02-09 17:05:07,455 Epoch 3065: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 17:05:07,455 EPOCH 3066
2024-02-09 17:05:18,743 Epoch 3066: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 17:05:18,743 EPOCH 3067
2024-02-09 17:05:30,110 Epoch 3067: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 17:05:30,111 EPOCH 3068
2024-02-09 17:05:40,884 Epoch 3068: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 17:05:40,885 EPOCH 3069
2024-02-09 17:05:51,723 Epoch 3069: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-09 17:05:51,723 EPOCH 3070
2024-02-09 17:06:02,973 Epoch 3070: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 17:06:02,974 EPOCH 3071
2024-02-09 17:06:06,820 [Epoch: 3071 Step: 00052200] Batch Recognition Loss:   0.000254 => Gls Tokens per Sec:     1665 || Batch Translation Loss:   0.013945 => Txt Tokens per Sec:     4475 || Lr: 0.000050
2024-02-09 17:06:13,972 Epoch 3071: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-09 17:06:13,972 EPOCH 3072
2024-02-09 17:06:25,089 Epoch 3072: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 17:06:25,089 EPOCH 3073
2024-02-09 17:06:36,024 Epoch 3073: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 17:06:36,025 EPOCH 3074
2024-02-09 17:06:47,223 Epoch 3074: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 17:06:47,224 EPOCH 3075
2024-02-09 17:06:58,170 Epoch 3075: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-09 17:06:58,171 EPOCH 3076
2024-02-09 17:07:09,118 Epoch 3076: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-09 17:07:09,118 EPOCH 3077
2024-02-09 17:07:15,736 [Epoch: 3077 Step: 00052300] Batch Recognition Loss:   0.000482 => Gls Tokens per Sec:      774 || Batch Translation Loss:   0.017137 => Txt Tokens per Sec:     2321 || Lr: 0.000050
2024-02-09 17:07:20,018 Epoch 3077: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-09 17:07:20,018 EPOCH 3078
2024-02-09 17:07:30,957 Epoch 3078: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 17:07:30,958 EPOCH 3079
2024-02-09 17:07:41,916 Epoch 3079: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-09 17:07:41,917 EPOCH 3080
2024-02-09 17:07:52,860 Epoch 3080: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 17:07:52,860 EPOCH 3081
2024-02-09 17:08:03,999 Epoch 3081: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 17:08:04,000 EPOCH 3082
2024-02-09 17:08:15,244 Epoch 3082: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-09 17:08:15,244 EPOCH 3083
2024-02-09 17:08:18,348 [Epoch: 3083 Step: 00052400] Batch Recognition Loss:   0.000333 => Gls Tokens per Sec:     1238 || Batch Translation Loss:   0.016517 => Txt Tokens per Sec:     3158 || Lr: 0.000050
2024-02-09 17:08:26,231 Epoch 3083: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-09 17:08:26,231 EPOCH 3084
2024-02-09 17:08:37,263 Epoch 3084: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-09 17:08:37,263 EPOCH 3085
2024-02-09 17:08:48,328 Epoch 3085: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-09 17:08:48,329 EPOCH 3086
2024-02-09 17:08:59,342 Epoch 3086: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 17:08:59,343 EPOCH 3087
2024-02-09 17:09:10,415 Epoch 3087: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-09 17:09:10,416 EPOCH 3088
2024-02-09 17:09:21,401 Epoch 3088: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-09 17:09:21,402 EPOCH 3089
2024-02-09 17:09:22,421 [Epoch: 3089 Step: 00052500] Batch Recognition Loss:   0.000161 => Gls Tokens per Sec:     2515 || Batch Translation Loss:   0.011511 => Txt Tokens per Sec:     7004 || Lr: 0.000050
2024-02-09 17:09:32,400 Epoch 3089: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-09 17:09:32,401 EPOCH 3090
2024-02-09 17:09:43,527 Epoch 3090: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-09 17:09:43,528 EPOCH 3091
2024-02-09 17:09:54,351 Epoch 3091: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-09 17:09:54,351 EPOCH 3092
2024-02-09 17:10:05,291 Epoch 3092: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-09 17:10:05,292 EPOCH 3093
2024-02-09 17:10:16,196 Epoch 3093: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-09 17:10:16,197 EPOCH 3094
2024-02-09 17:10:27,205 Epoch 3094: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 17:10:27,205 EPOCH 3095
2024-02-09 17:10:27,670 [Epoch: 3095 Step: 00052600] Batch Recognition Loss:   0.000193 => Gls Tokens per Sec:     2759 || Batch Translation Loss:   0.012032 => Txt Tokens per Sec:     7043 || Lr: 0.000050
2024-02-09 17:10:38,359 Epoch 3095: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 17:10:38,359 EPOCH 3096
2024-02-09 17:10:49,412 Epoch 3096: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-09 17:10:49,412 EPOCH 3097
2024-02-09 17:11:00,567 Epoch 3097: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-09 17:11:00,568 EPOCH 3098
2024-02-09 17:11:10,973 Epoch 3098: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-09 17:11:10,974 EPOCH 3099
2024-02-09 17:11:22,114 Epoch 3099: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-09 17:11:22,115 EPOCH 3100
2024-02-09 17:11:33,156 [Epoch: 3100 Step: 00052700] Batch Recognition Loss:   0.000515 => Gls Tokens per Sec:      962 || Batch Translation Loss:   0.011826 => Txt Tokens per Sec:     2661 || Lr: 0.000050
2024-02-09 17:11:33,157 Epoch 3100: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-09 17:11:33,157 EPOCH 3101
2024-02-09 17:11:44,200 Epoch 3101: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-09 17:11:44,201 EPOCH 3102
2024-02-09 17:11:55,505 Epoch 3102: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-09 17:11:55,506 EPOCH 3103
2024-02-09 17:12:06,724 Epoch 3103: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-09 17:12:06,725 EPOCH 3104
2024-02-09 17:12:17,514 Epoch 3104: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-09 17:12:17,515 EPOCH 3105
2024-02-09 17:12:28,526 Epoch 3105: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-09 17:12:28,526 EPOCH 3106
2024-02-09 17:12:37,371 [Epoch: 3106 Step: 00052800] Batch Recognition Loss:   0.000132 => Gls Tokens per Sec:     1056 || Batch Translation Loss:   0.015387 => Txt Tokens per Sec:     2883 || Lr: 0.000050
2024-02-09 17:12:39,408 Epoch 3106: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-09 17:12:39,408 EPOCH 3107
2024-02-09 17:12:50,506 Epoch 3107: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 17:12:50,506 EPOCH 3108
2024-02-09 17:13:01,466 Epoch 3108: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 17:13:01,467 EPOCH 3109
2024-02-09 17:13:12,614 Epoch 3109: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-09 17:13:12,615 EPOCH 3110
2024-02-09 17:13:23,400 Epoch 3110: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-09 17:13:23,400 EPOCH 3111
2024-02-09 17:13:34,354 Epoch 3111: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-09 17:13:34,355 EPOCH 3112
2024-02-09 17:13:44,542 [Epoch: 3112 Step: 00052900] Batch Recognition Loss:   0.000143 => Gls Tokens per Sec:      791 || Batch Translation Loss:   0.017575 => Txt Tokens per Sec:     2286 || Lr: 0.000050
2024-02-09 17:13:45,463 Epoch 3112: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-09 17:13:45,463 EPOCH 3113
2024-02-09 17:13:56,294 Epoch 3113: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-09 17:13:56,294 EPOCH 3114
2024-02-09 17:14:07,160 Epoch 3114: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-09 17:14:07,161 EPOCH 3115
2024-02-09 17:14:18,343 Epoch 3115: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-09 17:14:18,343 EPOCH 3116
2024-02-09 17:14:29,765 Epoch 3116: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-09 17:14:29,766 EPOCH 3117
2024-02-09 17:14:40,829 Epoch 3117: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.72 
2024-02-09 17:14:40,830 EPOCH 3118
2024-02-09 17:14:50,688 [Epoch: 3118 Step: 00053000] Batch Recognition Loss:   0.000409 => Gls Tokens per Sec:      688 || Batch Translation Loss:   0.041133 => Txt Tokens per Sec:     1944 || Lr: 0.000050
2024-02-09 17:14:52,023 Epoch 3118: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-09 17:14:52,024 EPOCH 3119
2024-02-09 17:15:02,972 Epoch 3119: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.82 
2024-02-09 17:15:02,973 EPOCH 3120
2024-02-09 17:15:14,030 Epoch 3120: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-09 17:15:14,031 EPOCH 3121
2024-02-09 17:15:25,137 Epoch 3121: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.75 
2024-02-09 17:15:25,138 EPOCH 3122
2024-02-09 17:15:36,152 Epoch 3122: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-09 17:15:36,153 EPOCH 3123
2024-02-09 17:15:47,184 Epoch 3123: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-09 17:15:47,185 EPOCH 3124
2024-02-09 17:15:53,525 [Epoch: 3124 Step: 00053100] Batch Recognition Loss:   0.000304 => Gls Tokens per Sec:      868 || Batch Translation Loss:   0.029538 => Txt Tokens per Sec:     2394 || Lr: 0.000050
2024-02-09 17:15:58,352 Epoch 3124: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-09 17:15:58,352 EPOCH 3125
2024-02-09 17:16:09,277 Epoch 3125: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-09 17:16:09,278 EPOCH 3126
2024-02-09 17:16:20,298 Epoch 3126: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-09 17:16:20,299 EPOCH 3127
2024-02-09 17:16:31,297 Epoch 3127: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 17:16:31,297 EPOCH 3128
2024-02-09 17:16:42,319 Epoch 3128: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-09 17:16:42,320 EPOCH 3129
2024-02-09 17:16:53,121 Epoch 3129: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-09 17:16:53,121 EPOCH 3130
2024-02-09 17:16:59,788 [Epoch: 3130 Step: 00053200] Batch Recognition Loss:   0.000335 => Gls Tokens per Sec:      633 || Batch Translation Loss:   0.021392 => Txt Tokens per Sec:     1812 || Lr: 0.000050
2024-02-09 17:17:03,986 Epoch 3130: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-09 17:17:03,986 EPOCH 3131
2024-02-09 17:17:14,989 Epoch 3131: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-09 17:17:14,989 EPOCH 3132
2024-02-09 17:17:25,815 Epoch 3132: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-09 17:17:25,815 EPOCH 3133
2024-02-09 17:17:36,672 Epoch 3133: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-09 17:17:36,673 EPOCH 3134
2024-02-09 17:17:47,645 Epoch 3134: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-09 17:17:47,645 EPOCH 3135
2024-02-09 17:17:58,833 Epoch 3135: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-09 17:17:58,833 EPOCH 3136
2024-02-09 17:18:02,458 [Epoch: 3136 Step: 00053300] Batch Recognition Loss:   0.002631 => Gls Tokens per Sec:      811 || Batch Translation Loss:   0.060151 => Txt Tokens per Sec:     2220 || Lr: 0.000050
2024-02-09 17:18:10,084 Epoch 3136: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-09 17:18:10,085 EPOCH 3137
2024-02-09 17:18:22,356 Epoch 3137: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-09 17:18:22,356 EPOCH 3138
2024-02-09 17:18:33,414 Epoch 3138: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-09 17:18:33,415 EPOCH 3139
2024-02-09 17:18:44,683 Epoch 3139: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 17:18:44,684 EPOCH 3140
2024-02-09 17:18:55,682 Epoch 3140: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 17:18:55,683 EPOCH 3141
2024-02-09 17:19:06,581 Epoch 3141: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 17:19:06,582 EPOCH 3142
2024-02-09 17:19:07,179 [Epoch: 3142 Step: 00053400] Batch Recognition Loss:   0.000212 => Gls Tokens per Sec:     3221 || Batch Translation Loss:   0.019746 => Txt Tokens per Sec:     8341 || Lr: 0.000050
2024-02-09 17:19:17,340 Epoch 3142: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 17:19:17,341 EPOCH 3143
2024-02-09 17:19:28,313 Epoch 3143: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 17:19:28,314 EPOCH 3144
2024-02-09 17:19:39,295 Epoch 3144: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 17:19:39,296 EPOCH 3145
2024-02-09 17:19:50,397 Epoch 3145: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 17:19:50,397 EPOCH 3146
2024-02-09 17:20:01,138 Epoch 3146: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 17:20:01,139 EPOCH 3147
2024-02-09 17:20:11,998 Epoch 3147: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 17:20:11,999 EPOCH 3148
2024-02-09 17:20:12,288 [Epoch: 3148 Step: 00053500] Batch Recognition Loss:   0.000177 => Gls Tokens per Sec:     2230 || Batch Translation Loss:   0.012741 => Txt Tokens per Sec:     6091 || Lr: 0.000050
2024-02-09 17:20:23,370 Epoch 3148: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 17:20:23,371 EPOCH 3149
2024-02-09 17:20:34,238 Epoch 3149: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 17:20:34,238 EPOCH 3150
2024-02-09 17:20:45,568 Epoch 3150: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-09 17:20:45,568 EPOCH 3151
2024-02-09 17:20:56,395 Epoch 3151: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-09 17:20:56,395 EPOCH 3152
2024-02-09 17:21:07,541 Epoch 3152: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 17:21:07,542 EPOCH 3153
2024-02-09 17:21:18,018 [Epoch: 3153 Step: 00053600] Batch Recognition Loss:   0.000146 => Gls Tokens per Sec:      953 || Batch Translation Loss:   0.028161 => Txt Tokens per Sec:     2613 || Lr: 0.000050
2024-02-09 17:21:18,359 Epoch 3153: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-09 17:21:18,359 EPOCH 3154
2024-02-09 17:21:29,401 Epoch 3154: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 17:21:29,402 EPOCH 3155
2024-02-09 17:21:40,680 Epoch 3155: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-09 17:21:40,680 EPOCH 3156
2024-02-09 17:21:51,537 Epoch 3156: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-09 17:21:51,538 EPOCH 3157
2024-02-09 17:22:02,465 Epoch 3157: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-09 17:22:02,466 EPOCH 3158
2024-02-09 17:22:13,521 Epoch 3158: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-09 17:22:13,522 EPOCH 3159
2024-02-09 17:22:22,512 [Epoch: 3159 Step: 00053700] Batch Recognition Loss:   0.000159 => Gls Tokens per Sec:      968 || Batch Translation Loss:   0.011520 => Txt Tokens per Sec:     2690 || Lr: 0.000050
2024-02-09 17:22:24,695 Epoch 3159: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 17:22:24,696 EPOCH 3160
2024-02-09 17:22:35,803 Epoch 3160: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 17:22:35,804 EPOCH 3161
2024-02-09 17:22:46,919 Epoch 3161: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 17:22:46,920 EPOCH 3162
2024-02-09 17:22:57,842 Epoch 3162: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 17:22:57,843 EPOCH 3163
2024-02-09 17:23:08,712 Epoch 3163: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-09 17:23:08,712 EPOCH 3164
2024-02-09 17:23:19,542 Epoch 3164: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-09 17:23:19,543 EPOCH 3165
2024-02-09 17:23:25,942 [Epoch: 3165 Step: 00053800] Batch Recognition Loss:   0.001108 => Gls Tokens per Sec:     1160 || Batch Translation Loss:   0.055737 => Txt Tokens per Sec:     3120 || Lr: 0.000050
2024-02-09 17:23:30,258 Epoch 3165: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-09 17:23:30,259 EPOCH 3166
2024-02-09 17:23:41,218 Epoch 3166: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.42 
2024-02-09 17:23:41,219 EPOCH 3167
2024-02-09 17:23:51,886 Epoch 3167: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.03 
2024-02-09 17:23:51,886 EPOCH 3168
2024-02-09 17:24:02,614 Epoch 3168: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.84 
2024-02-09 17:24:02,615 EPOCH 3169
2024-02-09 17:24:13,315 Epoch 3169: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.43 
2024-02-09 17:24:13,316 EPOCH 3170
2024-02-09 17:24:24,269 Epoch 3170: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.29 
2024-02-09 17:24:24,269 EPOCH 3171
2024-02-09 17:24:30,025 [Epoch: 3171 Step: 00053900] Batch Recognition Loss:   0.001773 => Gls Tokens per Sec:     1067 || Batch Translation Loss:   0.025383 => Txt Tokens per Sec:     2682 || Lr: 0.000050
2024-02-09 17:24:35,435 Epoch 3171: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.71 
2024-02-09 17:24:35,435 EPOCH 3172
2024-02-09 17:24:46,244 Epoch 3172: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-09 17:24:46,245 EPOCH 3173
2024-02-09 17:24:57,286 Epoch 3173: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-09 17:24:57,286 EPOCH 3174
2024-02-09 17:25:08,267 Epoch 3174: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-09 17:25:08,268 EPOCH 3175
2024-02-09 17:25:18,998 Epoch 3175: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-09 17:25:18,998 EPOCH 3176
2024-02-09 17:25:29,787 Epoch 3176: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-09 17:25:29,787 EPOCH 3177
2024-02-09 17:25:35,456 [Epoch: 3177 Step: 00054000] Batch Recognition Loss:   0.004532 => Gls Tokens per Sec:      858 || Batch Translation Loss:   0.017156 => Txt Tokens per Sec:     2414 || Lr: 0.000050
2024-02-09 17:26:16,643 Validation result at epoch 3177, step    54000: duration: 41.1860s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.57843	Translation Loss: 101027.69531	PPL: 24116.70703
	Eval Metric: BLEU
	WER 2.97	(DEL: 0.00,	INS: 0.00,	SUB: 2.97)
	BLEU-4 0.51	(BLEU-1: 10.40,	BLEU-2: 3.23,	BLEU-3: 1.17,	BLEU-4: 0.51)
	CHRF 16.63	ROUGE 8.83
2024-02-09 17:26:16,646 Logging Recognition and Translation Outputs
2024-02-09 17:26:16,646 ========================================================================================================================
2024-02-09 17:26:16,646 Logging Sequence: 166_243.00
2024-02-09 17:26:16,646 	Gloss Reference :	A B+C+D+E
2024-02-09 17:26:16,647 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 17:26:16,647 	Gloss Alignment :	         
2024-02-09 17:26:16,647 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 17:26:16,648 	Text Reference  :	icc worked with members boards like bcci pcb cricket australia etc 
2024-02-09 17:26:16,648 	Text Hypothesis :	*** ****** the  auction was    now  take for the     first     time
2024-02-09 17:26:16,648 	Text Alignment  :	D   D      S    S       S      S    S    S   S       S         S   
2024-02-09 17:26:16,648 ========================================================================================================================
2024-02-09 17:26:16,648 Logging Sequence: 179_409.00
2024-02-09 17:26:16,649 	Gloss Reference :	A B+C+D+E
2024-02-09 17:26:16,649 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 17:26:16,649 	Gloss Alignment :	         
2024-02-09 17:26:16,649 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 17:26:16,650 	Text Reference  :	*** **** the passport was     at the wfi office   in ***** delhi
2024-02-09 17:26:16,650 	Text Hypothesis :	but then the team     members of the *** olympics in short while
2024-02-09 17:26:16,650 	Text Alignment  :	I   I        S        S       S      D   S           I     S    
2024-02-09 17:26:16,650 ========================================================================================================================
2024-02-09 17:26:16,650 Logging Sequence: 81_407.00
2024-02-09 17:26:16,651 	Gloss Reference :	A B+C+D+E
2024-02-09 17:26:16,651 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 17:26:16,651 	Gloss Alignment :	         
2024-02-09 17:26:16,651 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 17:26:16,652 	Text Reference  :	***** the government company -  national buildings construction corporation and they     will complete them in a time-bound manner    
2024-02-09 17:26:16,652 	Text Hypothesis :	since the ********** start   of the      t20       world        cup         the amrapali will ******** **** ** * also       interested
2024-02-09 17:26:16,653 	Text Alignment  :	I         D          S       S  S        S         S            S           S   S             D        D    D  D S          S         
2024-02-09 17:26:16,653 ========================================================================================================================
2024-02-09 17:26:16,653 Logging Sequence: 96_31.00
2024-02-09 17:26:16,653 	Gloss Reference :	A B+C+D+E
2024-02-09 17:26:16,653 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 17:26:16,653 	Gloss Alignment :	         
2024-02-09 17:26:16,653 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 17:26:16,654 	Text Reference  :	and then    2       teams will go on      to  play   the  final  
2024-02-09 17:26:16,654 	Text Hypothesis :	*** however india's lost  8    of india's win people felt relaxed
2024-02-09 17:26:16,654 	Text Alignment  :	D   S       S       S     S    S  S       S   S      S    S      
2024-02-09 17:26:16,655 ========================================================================================================================
2024-02-09 17:26:16,655 Logging Sequence: 160_87.00
2024-02-09 17:26:16,655 	Gloss Reference :	A B+C+D+E
2024-02-09 17:26:16,655 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 17:26:16,655 	Gloss Alignment :	         
2024-02-09 17:26:16,655 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 17:26:16,656 	Text Reference  :	**** ** kohli held a    press conference and said 
2024-02-09 17:26:16,656 	Text Hypothesis :	when we will  have sent to    the        t20 match
2024-02-09 17:26:16,656 	Text Alignment  :	I    I  S     S    S    S     S          S   S    
2024-02-09 17:26:16,656 ========================================================================================================================
2024-02-09 17:26:22,197 Epoch 3177: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 17:26:22,197 EPOCH 3178
2024-02-09 17:26:33,127 Epoch 3178: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-09 17:26:33,127 EPOCH 3179
2024-02-09 17:26:43,879 Epoch 3179: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-09 17:26:43,880 EPOCH 3180
2024-02-09 17:26:55,088 Epoch 3180: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 17:26:55,089 EPOCH 3181
2024-02-09 17:27:06,194 Epoch 3181: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 17:27:06,194 EPOCH 3182
2024-02-09 17:27:17,115 Epoch 3182: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 17:27:17,115 EPOCH 3183
2024-02-09 17:27:23,729 [Epoch: 3183 Step: 00054100] Batch Recognition Loss:   0.000610 => Gls Tokens per Sec:      541 || Batch Translation Loss:   0.023135 => Txt Tokens per Sec:     1551 || Lr: 0.000050
2024-02-09 17:27:28,179 Epoch 3183: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 17:27:28,180 EPOCH 3184
2024-02-09 17:27:39,006 Epoch 3184: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 17:27:39,007 EPOCH 3185
2024-02-09 17:27:50,004 Epoch 3185: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.23 
2024-02-09 17:27:50,004 EPOCH 3186
2024-02-09 17:28:01,012 Epoch 3186: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 17:28:01,012 EPOCH 3187
2024-02-09 17:28:12,096 Epoch 3187: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 17:28:12,097 EPOCH 3188
2024-02-09 17:28:23,135 Epoch 3188: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 17:28:23,135 EPOCH 3189
2024-02-09 17:28:23,920 [Epoch: 3189 Step: 00054200] Batch Recognition Loss:   0.000119 => Gls Tokens per Sec:     3265 || Batch Translation Loss:   0.009885 => Txt Tokens per Sec:     8045 || Lr: 0.000050
2024-02-09 17:28:34,166 Epoch 3189: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 17:28:34,167 EPOCH 3190
2024-02-09 17:28:45,727 Epoch 3190: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 17:28:45,728 EPOCH 3191
2024-02-09 17:28:57,397 Epoch 3191: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-09 17:28:57,398 EPOCH 3192
2024-02-09 17:29:08,761 Epoch 3192: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 17:29:08,761 EPOCH 3193
2024-02-09 17:29:19,704 Epoch 3193: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-09 17:29:19,705 EPOCH 3194
2024-02-09 17:29:30,804 Epoch 3194: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-09 17:29:30,805 EPOCH 3195
2024-02-09 17:29:33,027 [Epoch: 3195 Step: 00054300] Batch Recognition Loss:   0.000137 => Gls Tokens per Sec:      576 || Batch Translation Loss:   0.012230 => Txt Tokens per Sec:     1766 || Lr: 0.000050
2024-02-09 17:29:41,343 Epoch 3195: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-09 17:29:41,343 EPOCH 3196
2024-02-09 17:29:52,338 Epoch 3196: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-09 17:29:52,339 EPOCH 3197
2024-02-09 17:30:03,285 Epoch 3197: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-09 17:30:03,286 EPOCH 3198
2024-02-09 17:30:14,310 Epoch 3198: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 17:30:14,311 EPOCH 3199
2024-02-09 17:30:25,204 Epoch 3199: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-09 17:30:25,205 EPOCH 3200
2024-02-09 17:30:36,196 [Epoch: 3200 Step: 00054400] Batch Recognition Loss:   0.001090 => Gls Tokens per Sec:      966 || Batch Translation Loss:   0.011571 => Txt Tokens per Sec:     2674 || Lr: 0.000050
2024-02-09 17:30:36,196 Epoch 3200: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-09 17:30:36,196 EPOCH 3201
2024-02-09 17:30:47,106 Epoch 3201: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-09 17:30:47,106 EPOCH 3202
2024-02-09 17:30:58,117 Epoch 3202: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 17:30:58,118 EPOCH 3203
2024-02-09 17:31:09,120 Epoch 3203: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-09 17:31:09,120 EPOCH 3204
2024-02-09 17:31:20,141 Epoch 3204: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 17:31:20,141 EPOCH 3205
2024-02-09 17:31:31,238 Epoch 3205: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 17:31:31,238 EPOCH 3206
2024-02-09 17:31:41,808 [Epoch: 3206 Step: 00054500] Batch Recognition Loss:   0.004282 => Gls Tokens per Sec:      884 || Batch Translation Loss:   0.016069 => Txt Tokens per Sec:     2494 || Lr: 0.000050
2024-02-09 17:31:42,159 Epoch 3206: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 17:31:42,159 EPOCH 3207
2024-02-09 17:31:53,268 Epoch 3207: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-09 17:31:53,269 EPOCH 3208
2024-02-09 17:32:03,977 Epoch 3208: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 17:32:03,978 EPOCH 3209
2024-02-09 17:32:15,083 Epoch 3209: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 17:32:15,084 EPOCH 3210
2024-02-09 17:32:26,171 Epoch 3210: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 17:32:26,172 EPOCH 3211
2024-02-09 17:32:37,421 Epoch 3211: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-09 17:32:37,421 EPOCH 3212
2024-02-09 17:32:43,718 [Epoch: 3212 Step: 00054600] Batch Recognition Loss:   0.000221 => Gls Tokens per Sec:     1322 || Batch Translation Loss:   0.005321 => Txt Tokens per Sec:     3569 || Lr: 0.000050
2024-02-09 17:32:48,471 Epoch 3212: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 17:32:48,471 EPOCH 3213
2024-02-09 17:32:59,207 Epoch 3213: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-09 17:32:59,208 EPOCH 3214
2024-02-09 17:33:10,276 Epoch 3214: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-09 17:33:10,277 EPOCH 3215
2024-02-09 17:33:20,989 Epoch 3215: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-09 17:33:20,990 EPOCH 3216
2024-02-09 17:33:32,028 Epoch 3216: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-09 17:33:32,029 EPOCH 3217
2024-02-09 17:33:42,932 Epoch 3217: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-09 17:33:42,933 EPOCH 3218
2024-02-09 17:33:50,969 [Epoch: 3218 Step: 00054700] Batch Recognition Loss:   0.000337 => Gls Tokens per Sec:      844 || Batch Translation Loss:   0.007163 => Txt Tokens per Sec:     2323 || Lr: 0.000050
2024-02-09 17:33:53,839 Epoch 3218: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-09 17:33:53,839 EPOCH 3219
2024-02-09 17:34:04,603 Epoch 3219: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-09 17:34:04,604 EPOCH 3220
2024-02-09 17:34:15,706 Epoch 3220: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-09 17:34:15,707 EPOCH 3221
2024-02-09 17:34:26,573 Epoch 3221: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-09 17:34:26,574 EPOCH 3222
2024-02-09 17:34:37,668 Epoch 3222: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-09 17:34:37,668 EPOCH 3223
2024-02-09 17:34:48,479 Epoch 3223: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-09 17:34:48,480 EPOCH 3224
2024-02-09 17:34:56,383 [Epoch: 3224 Step: 00054800] Batch Recognition Loss:   0.000335 => Gls Tokens per Sec:      696 || Batch Translation Loss:   0.013004 => Txt Tokens per Sec:     1987 || Lr: 0.000050
2024-02-09 17:34:59,639 Epoch 3224: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-09 17:34:59,639 EPOCH 3225
2024-02-09 17:35:10,700 Epoch 3225: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-09 17:35:10,701 EPOCH 3226
2024-02-09 17:35:21,906 Epoch 3226: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-09 17:35:21,906 EPOCH 3227
2024-02-09 17:35:33,159 Epoch 3227: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-09 17:35:33,160 EPOCH 3228
2024-02-09 17:35:44,202 Epoch 3228: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-09 17:35:44,202 EPOCH 3229
2024-02-09 17:35:55,305 Epoch 3229: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-09 17:35:55,305 EPOCH 3230
2024-02-09 17:35:58,448 [Epoch: 3230 Step: 00054900] Batch Recognition Loss:   0.000177 => Gls Tokens per Sec:     1426 || Batch Translation Loss:   0.011667 => Txt Tokens per Sec:     3798 || Lr: 0.000050
2024-02-09 17:36:06,141 Epoch 3230: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-09 17:36:06,142 EPOCH 3231
2024-02-09 17:36:17,215 Epoch 3231: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-09 17:36:17,215 EPOCH 3232
2024-02-09 17:36:28,217 Epoch 3232: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-09 17:36:28,218 EPOCH 3233
2024-02-09 17:36:38,995 Epoch 3233: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-09 17:36:38,996 EPOCH 3234
2024-02-09 17:36:49,918 Epoch 3234: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-09 17:36:49,919 EPOCH 3235
2024-02-09 17:37:00,901 Epoch 3235: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-09 17:37:00,902 EPOCH 3236
2024-02-09 17:37:07,225 [Epoch: 3236 Step: 00055000] Batch Recognition Loss:   0.000364 => Gls Tokens per Sec:      465 || Batch Translation Loss:   0.024345 => Txt Tokens per Sec:     1478 || Lr: 0.000050
2024-02-09 17:37:11,696 Epoch 3236: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-09 17:37:11,696 EPOCH 3237
2024-02-09 17:37:22,394 Epoch 3237: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-09 17:37:22,395 EPOCH 3238
2024-02-09 17:37:33,118 Epoch 3238: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-09 17:37:33,118 EPOCH 3239
2024-02-09 17:37:44,135 Epoch 3239: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-09 17:37:44,135 EPOCH 3240
2024-02-09 17:37:55,238 Epoch 3240: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-09 17:37:55,238 EPOCH 3241
2024-02-09 17:38:06,048 Epoch 3241: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-09 17:38:06,048 EPOCH 3242
2024-02-09 17:38:06,547 [Epoch: 3242 Step: 00055100] Batch Recognition Loss:   0.000274 => Gls Tokens per Sec:     3863 || Batch Translation Loss:   0.017592 => Txt Tokens per Sec:     9618 || Lr: 0.000050
2024-02-09 17:38:17,053 Epoch 3242: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-09 17:38:17,053 EPOCH 3243
2024-02-09 17:38:28,102 Epoch 3243: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-09 17:38:28,103 EPOCH 3244
2024-02-09 17:38:39,285 Epoch 3244: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-09 17:38:39,285 EPOCH 3245
2024-02-09 17:38:50,064 Epoch 3245: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-09 17:38:50,065 EPOCH 3246
2024-02-09 17:39:00,943 Epoch 3246: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-09 17:39:00,943 EPOCH 3247
2024-02-09 17:39:11,968 Epoch 3247: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-09 17:39:11,969 EPOCH 3248
2024-02-09 17:39:13,762 [Epoch: 3248 Step: 00055200] Batch Recognition Loss:   0.000348 => Gls Tokens per Sec:      357 || Batch Translation Loss:   0.058605 => Txt Tokens per Sec:     1164 || Lr: 0.000050
2024-02-09 17:39:22,976 Epoch 3248: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-09 17:39:22,977 EPOCH 3249
2024-02-09 17:39:34,000 Epoch 3249: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.79 
2024-02-09 17:39:34,001 EPOCH 3250
2024-02-09 17:39:44,950 Epoch 3250: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-09 17:39:44,950 EPOCH 3251
2024-02-09 17:39:55,942 Epoch 3251: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-09 17:39:55,943 EPOCH 3252
2024-02-09 17:40:06,855 Epoch 3252: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-09 17:40:06,855 EPOCH 3253
2024-02-09 17:40:17,669 [Epoch: 3253 Step: 00055300] Batch Recognition Loss:   0.000262 => Gls Tokens per Sec:      923 || Batch Translation Loss:   0.026363 => Txt Tokens per Sec:     2582 || Lr: 0.000050
2024-02-09 17:40:17,849 Epoch 3253: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-09 17:40:17,850 EPOCH 3254
2024-02-09 17:40:28,936 Epoch 3254: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-09 17:40:28,936 EPOCH 3255
2024-02-09 17:40:40,175 Epoch 3255: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-09 17:40:40,176 EPOCH 3256
2024-02-09 17:40:51,257 Epoch 3256: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 17:40:51,257 EPOCH 3257
2024-02-09 17:41:02,304 Epoch 3257: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 17:41:02,305 EPOCH 3258
2024-02-09 17:41:12,635 Epoch 3258: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 17:41:12,636 EPOCH 3259
2024-02-09 17:41:23,140 [Epoch: 3259 Step: 00055400] Batch Recognition Loss:   0.000389 => Gls Tokens per Sec:      828 || Batch Translation Loss:   0.009299 => Txt Tokens per Sec:     2358 || Lr: 0.000050
2024-02-09 17:41:23,740 Epoch 3259: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 17:41:23,740 EPOCH 3260
2024-02-09 17:41:34,809 Epoch 3260: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-09 17:41:34,810 EPOCH 3261
2024-02-09 17:41:46,031 Epoch 3261: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-09 17:41:46,032 EPOCH 3262
2024-02-09 17:41:57,210 Epoch 3262: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-09 17:41:57,211 EPOCH 3263
2024-02-09 17:42:08,316 Epoch 3263: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-09 17:42:08,317 EPOCH 3264
2024-02-09 17:42:19,353 Epoch 3264: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.70 
2024-02-09 17:42:19,354 EPOCH 3265
2024-02-09 17:42:27,924 [Epoch: 3265 Step: 00055500] Batch Recognition Loss:   0.000260 => Gls Tokens per Sec:      866 || Batch Translation Loss:   0.040276 => Txt Tokens per Sec:     2436 || Lr: 0.000050
2024-02-09 17:42:30,446 Epoch 3265: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-09 17:42:30,446 EPOCH 3266
2024-02-09 17:42:41,524 Epoch 3266: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-09 17:42:41,525 EPOCH 3267
2024-02-09 17:42:52,652 Epoch 3267: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-09 17:42:52,653 EPOCH 3268
2024-02-09 17:43:03,617 Epoch 3268: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-09 17:43:03,618 EPOCH 3269
2024-02-09 17:43:14,726 Epoch 3269: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-09 17:43:14,727 EPOCH 3270
2024-02-09 17:43:25,553 Epoch 3270: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-09 17:43:25,554 EPOCH 3271
2024-02-09 17:43:32,562 [Epoch: 3271 Step: 00055600] Batch Recognition Loss:   0.000154 => Gls Tokens per Sec:      913 || Batch Translation Loss:   0.016558 => Txt Tokens per Sec:     2554 || Lr: 0.000050
2024-02-09 17:43:36,511 Epoch 3271: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-09 17:43:36,511 EPOCH 3272
2024-02-09 17:43:47,207 Epoch 3272: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-09 17:43:47,207 EPOCH 3273
2024-02-09 17:43:58,159 Epoch 3273: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-09 17:43:58,159 EPOCH 3274
2024-02-09 17:44:09,115 Epoch 3274: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 17:44:09,115 EPOCH 3275
2024-02-09 17:44:19,965 Epoch 3275: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-09 17:44:19,965 EPOCH 3276
2024-02-09 17:44:31,020 Epoch 3276: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-09 17:44:31,020 EPOCH 3277
2024-02-09 17:44:36,793 [Epoch: 3277 Step: 00055700] Batch Recognition Loss:   0.000618 => Gls Tokens per Sec:      842 || Batch Translation Loss:   0.015773 => Txt Tokens per Sec:     2270 || Lr: 0.000050
2024-02-09 17:44:42,286 Epoch 3277: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-09 17:44:42,286 EPOCH 3278
2024-02-09 17:44:53,543 Epoch 3278: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-09 17:44:53,543 EPOCH 3279
2024-02-09 17:45:04,806 Epoch 3279: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 17:45:04,806 EPOCH 3280
2024-02-09 17:45:16,017 Epoch 3280: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 17:45:16,018 EPOCH 3281
2024-02-09 17:45:26,996 Epoch 3281: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 17:45:26,996 EPOCH 3282
2024-02-09 17:45:38,267 Epoch 3282: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-09 17:45:38,267 EPOCH 3283
2024-02-09 17:45:40,956 [Epoch: 3283 Step: 00055800] Batch Recognition Loss:   0.000119 => Gls Tokens per Sec:     1429 || Batch Translation Loss:   0.008174 => Txt Tokens per Sec:     3590 || Lr: 0.000050
2024-02-09 17:45:49,237 Epoch 3283: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 17:45:49,238 EPOCH 3284
2024-02-09 17:46:00,415 Epoch 3284: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-09 17:46:00,416 EPOCH 3285
2024-02-09 17:46:11,418 Epoch 3285: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 17:46:11,419 EPOCH 3286
2024-02-09 17:46:22,613 Epoch 3286: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-09 17:46:22,614 EPOCH 3287
2024-02-09 17:46:33,727 Epoch 3287: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-09 17:46:33,727 EPOCH 3288
2024-02-09 17:46:44,642 Epoch 3288: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-09 17:46:44,643 EPOCH 3289
2024-02-09 17:46:47,230 [Epoch: 3289 Step: 00055900] Batch Recognition Loss:   0.000431 => Gls Tokens per Sec:      990 || Batch Translation Loss:   0.017366 => Txt Tokens per Sec:     2516 || Lr: 0.000050
2024-02-09 17:46:55,731 Epoch 3289: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 17:46:55,732 EPOCH 3290
2024-02-09 17:47:07,045 Epoch 3290: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 17:47:07,046 EPOCH 3291
2024-02-09 17:47:18,084 Epoch 3291: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-09 17:47:18,085 EPOCH 3292
2024-02-09 17:47:29,171 Epoch 3292: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-09 17:47:29,171 EPOCH 3293
2024-02-09 17:47:40,474 Epoch 3293: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-09 17:47:40,474 EPOCH 3294
2024-02-09 17:47:51,761 Epoch 3294: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-09 17:47:51,761 EPOCH 3295
2024-02-09 17:47:52,241 [Epoch: 3295 Step: 00056000] Batch Recognition Loss:   0.000172 => Gls Tokens per Sec:     2678 || Batch Translation Loss:   0.025627 => Txt Tokens per Sec:     7726 || Lr: 0.000050
2024-02-09 17:48:33,605 Validation result at epoch 3295, step    56000: duration: 41.3616s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.58749	Translation Loss: 101558.12500	PPL: 25428.84766
	Eval Metric: BLEU
	WER 2.90	(DEL: 0.00,	INS: 0.00,	SUB: 2.90)
	BLEU-4 0.00	(BLEU-1: 9.92,	BLEU-2: 2.57,	BLEU-3: 0.75,	BLEU-4: 0.00)
	CHRF 16.30	ROUGE 8.10
2024-02-09 17:48:33,607 Logging Recognition and Translation Outputs
2024-02-09 17:48:33,608 ========================================================================================================================
2024-02-09 17:48:33,608 Logging Sequence: 177_167.00
2024-02-09 17:48:33,608 	Gloss Reference :	A B+C+D+E
2024-02-09 17:48:33,608 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 17:48:33,608 	Gloss Alignment :	         
2024-02-09 17:48:33,609 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 17:48:33,610 	Text Reference  :	******* * ****** this is because    sushil wanted to establish his fear to     ensure no  one   would oppose him  
2024-02-09 17:48:33,611 	Text Hypothesis :	however a police want to interogate sushil ajay   to find      out the  motive behind the brawl which went   viral
2024-02-09 17:48:33,611 	Text Alignment  :	I       I I      S    S  S                 S         S         S   S    S      S      S   S     S     S      S    
2024-02-09 17:48:33,611 ========================================================================================================================
2024-02-09 17:48:33,611 Logging Sequence: 127_140.00
2024-02-09 17:48:33,611 	Gloss Reference :	A B+C+D+E
2024-02-09 17:48:33,611 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 17:48:33,611 	Gloss Alignment :	         
2024-02-09 17:48:33,612 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 17:48:33,614 	Text Reference  :	this is  india'    3rd medal   in the world athletics championships he is very talented and his ******* performance is    highly impressive
2024-02-09 17:48:33,614 	Text Hypothesis :	the  now cricketer is  because of the ***** ********* ************* ** ** same room     for his alleged private     chats leaked in        
2024-02-09 17:48:33,614 	Text Alignment  :	S    S   S         S   S       S      D     D         D             D  D  S    S        S       I       S           S     S      S         
2024-02-09 17:48:33,614 ========================================================================================================================
2024-02-09 17:48:33,614 Logging Sequence: 126_200.00
2024-02-09 17:48:33,614 	Gloss Reference :	A B+C+D+E
2024-02-09 17:48:33,614 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 17:48:33,615 	Gloss Alignment :	         
2024-02-09 17:48:33,615 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 17:48:33,615 	Text Reference  :	let me   tell you  about them
2024-02-09 17:48:33,615 	Text Hypothesis :	we  were in   same in    goa 
2024-02-09 17:48:33,615 	Text Alignment  :	S   S    S    S    S     S   
2024-02-09 17:48:33,616 ========================================================================================================================
2024-02-09 17:48:33,616 Logging Sequence: 104_119.00
2024-02-09 17:48:33,616 	Gloss Reference :	A B+C+D+E    
2024-02-09 17:48:33,616 	Gloss Hypothesis:	A B+C+D+E+D+E
2024-02-09 17:48:33,616 	Gloss Alignment :	  S          
2024-02-09 17:48:33,616 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 17:48:33,618 	Text Reference  :	famous chess players like viswanathan anand  and praggnanandhaa's coach  r      b ramesh congratulated him     for his     impressive performance
2024-02-09 17:48:33,618 	Text Hypothesis :	****** ***** ******* the  third       charge and that             vinesh posted a other  celeb         parents and weekend double     headers    
2024-02-09 17:48:33,618 	Text Alignment  :	D      D     D       S    S           S          S                S      S      S S      S             S       S   S       S          S          
2024-02-09 17:48:33,618 ========================================================================================================================
2024-02-09 17:48:33,619 Logging Sequence: 172_267.00
2024-02-09 17:48:33,619 	Gloss Reference :	A B+C+D+E
2024-02-09 17:48:33,619 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 17:48:33,619 	Gloss Alignment :	         
2024-02-09 17:48:33,619 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 17:48:33,619 	Text Reference  :	such provisions have  been made 
2024-02-09 17:48:33,620 	Text Hypothesis :	**** and        media went viral
2024-02-09 17:48:33,620 	Text Alignment  :	D    S          S     S    S    
2024-02-09 17:48:33,620 ========================================================================================================================
2024-02-09 17:48:44,568 Epoch 3295: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-09 17:48:44,569 EPOCH 3296
2024-02-09 17:48:55,710 Epoch 3296: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-09 17:48:55,711 EPOCH 3297
2024-02-09 17:49:06,952 Epoch 3297: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-09 17:49:06,953 EPOCH 3298
2024-02-09 17:49:17,883 Epoch 3298: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-09 17:49:17,884 EPOCH 3299
2024-02-09 17:49:28,828 Epoch 3299: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-09 17:49:28,829 EPOCH 3300
2024-02-09 17:49:39,516 [Epoch: 3300 Step: 00056100] Batch Recognition Loss:   0.000316 => Gls Tokens per Sec:      994 || Batch Translation Loss:   0.031590 => Txt Tokens per Sec:     2749 || Lr: 0.000050
2024-02-09 17:49:39,517 Epoch 3300: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-09 17:49:39,517 EPOCH 3301
2024-02-09 17:49:50,814 Epoch 3301: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-09 17:49:50,815 EPOCH 3302
2024-02-09 17:50:01,928 Epoch 3302: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-09 17:50:01,928 EPOCH 3303
2024-02-09 17:50:12,970 Epoch 3303: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-09 17:50:12,970 EPOCH 3304
2024-02-09 17:50:24,034 Epoch 3304: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-09 17:50:24,034 EPOCH 3305
2024-02-09 17:50:34,981 Epoch 3305: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-09 17:50:34,981 EPOCH 3306
2024-02-09 17:50:45,703 [Epoch: 3306 Step: 00056200] Batch Recognition Loss:   0.000457 => Gls Tokens per Sec:      871 || Batch Translation Loss:   0.064946 => Txt Tokens per Sec:     2434 || Lr: 0.000050
2024-02-09 17:50:46,113 Epoch 3306: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-09 17:50:46,113 EPOCH 3307
2024-02-09 17:50:57,139 Epoch 3307: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-09 17:50:57,140 EPOCH 3308
2024-02-09 17:51:08,362 Epoch 3308: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-09 17:51:08,363 EPOCH 3309
2024-02-09 17:51:19,322 Epoch 3309: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-09 17:51:19,322 EPOCH 3310
2024-02-09 17:51:30,414 Epoch 3310: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-09 17:51:30,414 EPOCH 3311
2024-02-09 17:51:41,599 Epoch 3311: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-09 17:51:41,599 EPOCH 3312
2024-02-09 17:51:51,903 [Epoch: 3312 Step: 00056300] Batch Recognition Loss:   0.001288 => Gls Tokens per Sec:      782 || Batch Translation Loss:   0.028618 => Txt Tokens per Sec:     2173 || Lr: 0.000050
2024-02-09 17:51:52,845 Epoch 3312: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-09 17:51:52,845 EPOCH 3313
2024-02-09 17:52:03,604 Epoch 3313: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-09 17:52:03,604 EPOCH 3314
2024-02-09 17:52:14,655 Epoch 3314: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-09 17:52:14,656 EPOCH 3315
2024-02-09 17:52:25,530 Epoch 3315: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-09 17:52:25,531 EPOCH 3316
2024-02-09 17:52:36,619 Epoch 3316: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-09 17:52:36,620 EPOCH 3317
2024-02-09 17:52:47,674 Epoch 3317: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 17:52:47,675 EPOCH 3318
2024-02-09 17:52:55,206 [Epoch: 3318 Step: 00056400] Batch Recognition Loss:   0.000201 => Gls Tokens per Sec:      935 || Batch Translation Loss:   0.011399 => Txt Tokens per Sec:     2722 || Lr: 0.000050
2024-02-09 17:52:58,627 Epoch 3318: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-09 17:52:58,627 EPOCH 3319
2024-02-09 17:53:09,613 Epoch 3319: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 17:53:09,614 EPOCH 3320
2024-02-09 17:53:20,549 Epoch 3320: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 17:53:20,550 EPOCH 3321
2024-02-09 17:53:31,297 Epoch 3321: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-09 17:53:31,297 EPOCH 3322
2024-02-09 17:53:42,300 Epoch 3322: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 17:53:42,301 EPOCH 3323
2024-02-09 17:53:53,421 Epoch 3323: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-09 17:53:53,421 EPOCH 3324
2024-02-09 17:54:01,364 [Epoch: 3324 Step: 00056500] Batch Recognition Loss:   0.000149 => Gls Tokens per Sec:      693 || Batch Translation Loss:   0.012364 => Txt Tokens per Sec:     1990 || Lr: 0.000050
2024-02-09 17:54:04,464 Epoch 3324: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-09 17:54:04,464 EPOCH 3325
2024-02-09 17:54:15,366 Epoch 3325: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-09 17:54:15,367 EPOCH 3326
2024-02-09 17:54:26,238 Epoch 3326: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 17:54:26,239 EPOCH 3327
2024-02-09 17:54:37,449 Epoch 3327: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-09 17:54:37,450 EPOCH 3328
2024-02-09 17:54:48,610 Epoch 3328: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 17:54:48,610 EPOCH 3329
2024-02-09 17:54:59,660 Epoch 3329: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-09 17:54:59,661 EPOCH 3330
2024-02-09 17:55:05,131 [Epoch: 3330 Step: 00056600] Batch Recognition Loss:   0.000301 => Gls Tokens per Sec:      772 || Batch Translation Loss:   0.013808 => Txt Tokens per Sec:     2150 || Lr: 0.000050
2024-02-09 17:55:10,835 Epoch 3330: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-09 17:55:10,835 EPOCH 3331
2024-02-09 17:55:21,758 Epoch 3331: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-09 17:55:21,759 EPOCH 3332
2024-02-09 17:55:32,948 Epoch 3332: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 17:55:32,949 EPOCH 3333
2024-02-09 17:55:43,915 Epoch 3333: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-09 17:55:43,916 EPOCH 3334
2024-02-09 17:55:54,916 Epoch 3334: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 17:55:54,916 EPOCH 3335
2024-02-09 17:56:05,773 Epoch 3335: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-09 17:56:05,774 EPOCH 3336
2024-02-09 17:56:09,186 [Epoch: 3336 Step: 00056700] Batch Recognition Loss:   0.000215 => Gls Tokens per Sec:      862 || Batch Translation Loss:   0.020958 => Txt Tokens per Sec:     2306 || Lr: 0.000050
2024-02-09 17:56:16,822 Epoch 3336: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-09 17:56:16,822 EPOCH 3337
2024-02-09 17:56:27,674 Epoch 3337: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 17:56:27,675 EPOCH 3338
2024-02-09 17:56:38,524 Epoch 3338: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 17:56:38,524 EPOCH 3339
2024-02-09 17:56:49,663 Epoch 3339: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 17:56:49,663 EPOCH 3340
2024-02-09 17:57:00,678 Epoch 3340: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 17:57:00,678 EPOCH 3341
2024-02-09 17:57:11,724 Epoch 3341: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-09 17:57:11,724 EPOCH 3342
2024-02-09 17:57:12,335 [Epoch: 3342 Step: 00056800] Batch Recognition Loss:   0.000148 => Gls Tokens per Sec:     3148 || Batch Translation Loss:   0.019147 => Txt Tokens per Sec:     7898 || Lr: 0.000050
2024-02-09 17:57:22,501 Epoch 3342: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-09 17:57:22,501 EPOCH 3343
2024-02-09 17:57:33,497 Epoch 3343: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.09 
2024-02-09 17:57:33,498 EPOCH 3344
2024-02-09 17:57:44,679 Epoch 3344: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.41 
2024-02-09 17:57:44,680 EPOCH 3345
2024-02-09 17:57:56,019 Epoch 3345: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.83 
2024-02-09 17:57:56,020 EPOCH 3346
2024-02-09 17:58:07,352 Epoch 3346: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.27 
2024-02-09 17:58:07,353 EPOCH 3347
2024-02-09 17:58:18,432 Epoch 3347: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-09 17:58:18,433 EPOCH 3348
2024-02-09 17:58:18,743 [Epoch: 3348 Step: 00056900] Batch Recognition Loss:   0.000853 => Gls Tokens per Sec:     2065 || Batch Translation Loss:   0.031458 => Txt Tokens per Sec:     6548 || Lr: 0.000050
2024-02-09 17:58:29,558 Epoch 3348: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-09 17:58:29,558 EPOCH 3349
2024-02-09 17:58:40,553 Epoch 3349: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-09 17:58:40,554 EPOCH 3350
2024-02-09 17:58:51,452 Epoch 3350: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-09 17:58:51,452 EPOCH 3351
2024-02-09 17:59:02,576 Epoch 3351: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.41 
2024-02-09 17:59:02,576 EPOCH 3352
2024-02-09 17:59:13,377 Epoch 3352: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-09 17:59:13,377 EPOCH 3353
2024-02-09 17:59:24,346 [Epoch: 3353 Step: 00057000] Batch Recognition Loss:   0.000278 => Gls Tokens per Sec:      910 || Batch Translation Loss:   0.020218 => Txt Tokens per Sec:     2514 || Lr: 0.000050
2024-02-09 17:59:24,557 Epoch 3353: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.41 
2024-02-09 17:59:24,558 EPOCH 3354
2024-02-09 17:59:35,314 Epoch 3354: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-09 17:59:35,315 EPOCH 3355
2024-02-09 17:59:46,362 Epoch 3355: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-09 17:59:46,362 EPOCH 3356
2024-02-09 17:59:57,571 Epoch 3356: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-09 17:59:57,572 EPOCH 3357
2024-02-09 18:00:08,659 Epoch 3357: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 18:00:08,660 EPOCH 3358
2024-02-09 18:00:19,721 Epoch 3358: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.24 
2024-02-09 18:00:19,722 EPOCH 3359
2024-02-09 18:00:28,402 [Epoch: 3359 Step: 00057100] Batch Recognition Loss:   0.000156 => Gls Tokens per Sec:     1003 || Batch Translation Loss:   0.013393 => Txt Tokens per Sec:     2807 || Lr: 0.000050
2024-02-09 18:00:30,808 Epoch 3359: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.24 
2024-02-09 18:00:30,809 EPOCH 3360
2024-02-09 18:00:42,006 Epoch 3360: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 18:00:42,007 EPOCH 3361
2024-02-09 18:00:52,998 Epoch 3361: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 18:00:52,999 EPOCH 3362
2024-02-09 18:01:03,976 Epoch 3362: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 18:01:03,976 EPOCH 3363
2024-02-09 18:01:14,967 Epoch 3363: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-09 18:01:14,968 EPOCH 3364
2024-02-09 18:01:25,957 Epoch 3364: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-09 18:01:25,958 EPOCH 3365
2024-02-09 18:01:32,758 [Epoch: 3365 Step: 00057200] Batch Recognition Loss:   0.000219 => Gls Tokens per Sec:     1091 || Batch Translation Loss:   0.026533 => Txt Tokens per Sec:     2950 || Lr: 0.000050
2024-02-09 18:01:36,905 Epoch 3365: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 18:01:36,905 EPOCH 3366
2024-02-09 18:01:47,926 Epoch 3366: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-09 18:01:47,927 EPOCH 3367
2024-02-09 18:01:58,958 Epoch 3367: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 18:01:58,958 EPOCH 3368
2024-02-09 18:02:09,892 Epoch 3368: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 18:02:09,893 EPOCH 3369
2024-02-09 18:02:20,899 Epoch 3369: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-09 18:02:20,900 EPOCH 3370
2024-02-09 18:02:31,877 Epoch 3370: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 18:02:31,878 EPOCH 3371
2024-02-09 18:02:41,291 [Epoch: 3371 Step: 00057300] Batch Recognition Loss:   0.000184 => Gls Tokens per Sec:      652 || Batch Translation Loss:   0.013393 => Txt Tokens per Sec:     1829 || Lr: 0.000050
2024-02-09 18:02:43,018 Epoch 3371: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-09 18:02:43,018 EPOCH 3372
2024-02-09 18:02:54,020 Epoch 3372: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 18:02:54,021 EPOCH 3373
2024-02-09 18:03:04,894 Epoch 3373: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 18:03:04,894 EPOCH 3374
2024-02-09 18:03:15,973 Epoch 3374: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 18:03:15,974 EPOCH 3375
2024-02-09 18:03:26,977 Epoch 3375: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 18:03:26,977 EPOCH 3376
2024-02-09 18:03:38,146 Epoch 3376: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 18:03:38,146 EPOCH 3377
2024-02-09 18:03:43,468 [Epoch: 3377 Step: 00057400] Batch Recognition Loss:   0.000313 => Gls Tokens per Sec:      962 || Batch Translation Loss:   0.016216 => Txt Tokens per Sec:     2727 || Lr: 0.000050
2024-02-09 18:03:49,148 Epoch 3377: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 18:03:49,148 EPOCH 3378
2024-02-09 18:04:00,287 Epoch 3378: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 18:04:00,287 EPOCH 3379
2024-02-09 18:04:11,194 Epoch 3379: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 18:04:11,194 EPOCH 3380
2024-02-09 18:04:22,350 Epoch 3380: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-09 18:04:22,351 EPOCH 3381
2024-02-09 18:04:33,347 Epoch 3381: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-09 18:04:33,347 EPOCH 3382
2024-02-09 18:04:44,678 Epoch 3382: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-09 18:04:44,679 EPOCH 3383
2024-02-09 18:04:48,363 [Epoch: 3383 Step: 00057500] Batch Recognition Loss:   0.000177 => Gls Tokens per Sec:      972 || Batch Translation Loss:   0.013759 => Txt Tokens per Sec:     2670 || Lr: 0.000050
2024-02-09 18:04:55,772 Epoch 3383: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 18:04:55,773 EPOCH 3384
2024-02-09 18:05:06,708 Epoch 3384: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 18:05:06,709 EPOCH 3385
2024-02-09 18:05:17,696 Epoch 3385: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-09 18:05:17,696 EPOCH 3386
2024-02-09 18:05:28,909 Epoch 3386: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-09 18:05:28,910 EPOCH 3387
2024-02-09 18:05:39,766 Epoch 3387: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-09 18:05:39,767 EPOCH 3388
2024-02-09 18:05:50,715 Epoch 3388: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-09 18:05:50,715 EPOCH 3389
2024-02-09 18:05:53,537 [Epoch: 3389 Step: 00057600] Batch Recognition Loss:   0.000138 => Gls Tokens per Sec:      908 || Batch Translation Loss:   0.066639 => Txt Tokens per Sec:     2762 || Lr: 0.000050
2024-02-09 18:06:02,000 Epoch 3389: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-09 18:06:02,001 EPOCH 3390
2024-02-09 18:06:13,151 Epoch 3390: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-09 18:06:13,152 EPOCH 3391
2024-02-09 18:06:24,329 Epoch 3391: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-09 18:06:24,330 EPOCH 3392
2024-02-09 18:06:35,369 Epoch 3392: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-09 18:06:35,369 EPOCH 3393
2024-02-09 18:06:46,476 Epoch 3393: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-09 18:06:46,477 EPOCH 3394
2024-02-09 18:06:57,607 Epoch 3394: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-09 18:06:57,608 EPOCH 3395
2024-02-09 18:06:59,549 [Epoch: 3395 Step: 00057700] Batch Recognition Loss:   0.000255 => Gls Tokens per Sec:      660 || Batch Translation Loss:   0.024519 => Txt Tokens per Sec:     1803 || Lr: 0.000050
2024-02-09 18:07:08,752 Epoch 3395: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-09 18:07:08,752 EPOCH 3396
2024-02-09 18:07:20,110 Epoch 3396: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.79 
2024-02-09 18:07:20,111 EPOCH 3397
2024-02-09 18:07:31,094 Epoch 3397: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-09 18:07:31,095 EPOCH 3398
2024-02-09 18:07:41,836 Epoch 3398: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-09 18:07:41,837 EPOCH 3399
2024-02-09 18:07:52,801 Epoch 3399: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-09 18:07:52,801 EPOCH 3400
2024-02-09 18:08:03,891 [Epoch: 3400 Step: 00057800] Batch Recognition Loss:   0.000175 => Gls Tokens per Sec:      958 || Batch Translation Loss:   0.038334 => Txt Tokens per Sec:     2650 || Lr: 0.000050
2024-02-09 18:08:03,891 Epoch 3400: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-09 18:08:03,891 EPOCH 3401
2024-02-09 18:08:15,037 Epoch 3401: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-09 18:08:15,037 EPOCH 3402
2024-02-09 18:08:26,060 Epoch 3402: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-09 18:08:26,061 EPOCH 3403
2024-02-09 18:08:37,492 Epoch 3403: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-09 18:08:37,493 EPOCH 3404
2024-02-09 18:08:48,469 Epoch 3404: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-09 18:08:48,470 EPOCH 3405
2024-02-09 18:08:59,457 Epoch 3405: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-09 18:08:59,457 EPOCH 3406
2024-02-09 18:09:08,357 [Epoch: 3406 Step: 00057900] Batch Recognition Loss:   0.000281 => Gls Tokens per Sec:     1050 || Batch Translation Loss:   0.115679 => Txt Tokens per Sec:     2873 || Lr: 0.000050
2024-02-09 18:09:10,405 Epoch 3406: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.24 
2024-02-09 18:09:10,405 EPOCH 3407
2024-02-09 18:09:21,483 Epoch 3407: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.42 
2024-02-09 18:09:21,484 EPOCH 3408
2024-02-09 18:09:32,130 Epoch 3408: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.85 
2024-02-09 18:09:32,130 EPOCH 3409
2024-02-09 18:09:43,083 Epoch 3409: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-09 18:09:43,084 EPOCH 3410
2024-02-09 18:09:53,907 Epoch 3410: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-09 18:09:53,907 EPOCH 3411
2024-02-09 18:10:04,653 Epoch 3411: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-09 18:10:04,654 EPOCH 3412
2024-02-09 18:10:11,457 [Epoch: 3412 Step: 00058000] Batch Recognition Loss:   0.000441 => Gls Tokens per Sec:     1185 || Batch Translation Loss:   0.021750 => Txt Tokens per Sec:     3182 || Lr: 0.000050
2024-02-09 18:10:52,814 Validation result at epoch 3412, step    58000: duration: 41.3557s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.56330	Translation Loss: 102287.64844	PPL: 27350.90820
	Eval Metric: BLEU
	WER 2.75	(DEL: 0.00,	INS: 0.00,	SUB: 2.75)
	BLEU-4 0.53	(BLEU-1: 10.70,	BLEU-2: 3.09,	BLEU-3: 1.15,	BLEU-4: 0.53)
	CHRF 17.01	ROUGE 8.74
2024-02-09 18:10:52,817 Logging Recognition and Translation Outputs
2024-02-09 18:10:52,817 ========================================================================================================================
2024-02-09 18:10:52,817 Logging Sequence: 60_264.00
2024-02-09 18:10:52,817 	Gloss Reference :	A B+C+D+E
2024-02-09 18:10:52,817 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 18:10:52,818 	Gloss Alignment :	         
2024-02-09 18:10:52,818 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 18:10:52,819 	Text Reference  :	******* plus  do  you     know      that a sex tape of his     with two     women  had gone viral
2024-02-09 18:10:52,819 	Text Hypothesis :	however there was playing instagram as   a *** **** ** message on   twitter handle and the  fifa 
2024-02-09 18:10:52,819 	Text Alignment  :	I       S     S   S       S         S      D   D    D  S       S    S       S      S   S    S    
2024-02-09 18:10:52,819 ========================================================================================================================
2024-02-09 18:10:52,820 Logging Sequence: 100_50.00
2024-02-09 18:10:52,820 	Gloss Reference :	A B+C+D+E
2024-02-09 18:10:52,820 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 18:10:52,820 	Gloss Alignment :	         
2024-02-09 18:10:52,820 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 18:10:52,821 	Text Reference  :	******* with virat kohli as      the captain
2024-02-09 18:10:52,821 	Text Hypothesis :	because of   the   same  reports are delayed
2024-02-09 18:10:52,821 	Text Alignment  :	I       S    S     S     S       S   S      
2024-02-09 18:10:52,821 ========================================================================================================================
2024-02-09 18:10:52,821 Logging Sequence: 137_44.00
2024-02-09 18:10:52,822 	Gloss Reference :	A B+C+D+E
2024-02-09 18:10:52,822 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 18:10:52,822 	Gloss Alignment :	         
2024-02-09 18:10:52,822 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 18:10:52,823 	Text Reference  :	let me tell you the rules that qatar has announced for          the fans travelling for the world cup   
2024-02-09 18:10:52,823 	Text Hypothesis :	*** ** **** *** *** ***** **** raina has been      announcement but what happened   at  the ***** couple
2024-02-09 18:10:52,824 	Text Alignment  :	D   D  D    D   D   D     D    S         S         S            S   S    S          S       D     S     
2024-02-09 18:10:52,824 ========================================================================================================================
2024-02-09 18:10:52,824 Logging Sequence: 58_27.00
2024-02-09 18:10:52,824 	Gloss Reference :	A B+C+D+E
2024-02-09 18:10:52,824 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 18:10:52,824 	Gloss Alignment :	         
2024-02-09 18:10:52,825 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 18:10:52,825 	Text Reference  :	the 19th asian games 2022 were to    be   held in    hangzhou china
2024-02-09 18:10:52,826 	Text Hypothesis :	*** **** ***** india won  the  match with 263  balls they     lose 
2024-02-09 18:10:52,826 	Text Alignment  :	D   D    D     S     S    S    S     S    S    S     S        S    
2024-02-09 18:10:52,826 ========================================================================================================================
2024-02-09 18:10:52,826 Logging Sequence: 75_255.00
2024-02-09 18:10:52,826 	Gloss Reference :	A B+C+D+E  
2024-02-09 18:10:52,826 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-09 18:10:52,827 	Gloss Alignment :	  S        
2024-02-09 18:10:52,827 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 18:10:52,829 	Text Reference  :	we miss our  baby      boy    with    this ronaldo' total     baby count has      reached 5   with 2     boys 3   girls
2024-02-09 18:10:52,829 	Text Hypothesis :	** as   bcci president sourav ganguly and  board    secretary jay  shah  welcomed the     two new  teams to   the ipl  
2024-02-09 18:10:52,829 	Text Alignment  :	D  S    S    S         S      S       S    S        S         S    S     S        S       S   S    S     S    S   S    
2024-02-09 18:10:52,829 ========================================================================================================================
2024-02-09 18:10:56,985 Epoch 3412: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-09 18:10:56,986 EPOCH 3413
2024-02-09 18:11:08,060 Epoch 3413: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-09 18:11:08,060 EPOCH 3414
2024-02-09 18:11:18,840 Epoch 3414: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-09 18:11:18,841 EPOCH 3415
2024-02-09 18:11:29,734 Epoch 3415: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 18:11:29,734 EPOCH 3416
2024-02-09 18:11:40,610 Epoch 3416: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 18:11:40,610 EPOCH 3417
2024-02-09 18:11:51,428 Epoch 3417: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 18:11:51,429 EPOCH 3418
2024-02-09 18:12:00,956 [Epoch: 3418 Step: 00058100] Batch Recognition Loss:   0.000289 => Gls Tokens per Sec:      712 || Batch Translation Loss:   0.024760 => Txt Tokens per Sec:     2054 || Lr: 0.000050
2024-02-09 18:12:02,365 Epoch 3418: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 18:12:02,366 EPOCH 3419
2024-02-09 18:12:13,215 Epoch 3419: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 18:12:13,215 EPOCH 3420
2024-02-09 18:12:24,133 Epoch 3420: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 18:12:24,134 EPOCH 3421
2024-02-09 18:12:35,095 Epoch 3421: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-09 18:12:35,095 EPOCH 3422
2024-02-09 18:12:46,608 Epoch 3422: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 18:12:46,609 EPOCH 3423
2024-02-09 18:12:57,526 Epoch 3423: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 18:12:57,527 EPOCH 3424
2024-02-09 18:13:03,491 [Epoch: 3424 Step: 00058200] Batch Recognition Loss:   0.000128 => Gls Tokens per Sec:      922 || Batch Translation Loss:   0.016226 => Txt Tokens per Sec:     2497 || Lr: 0.000050
2024-02-09 18:13:08,503 Epoch 3424: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 18:13:08,503 EPOCH 3425
2024-02-09 18:13:19,087 Epoch 3425: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-09 18:13:19,087 EPOCH 3426
2024-02-09 18:13:30,292 Epoch 3426: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 18:13:30,293 EPOCH 3427
2024-02-09 18:13:41,304 Epoch 3427: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 18:13:41,305 EPOCH 3428
2024-02-09 18:13:52,269 Epoch 3428: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-09 18:13:52,270 EPOCH 3429
2024-02-09 18:14:03,331 Epoch 3429: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-09 18:14:03,331 EPOCH 3430
2024-02-09 18:14:07,012 [Epoch: 3430 Step: 00058300] Batch Recognition Loss:   0.000193 => Gls Tokens per Sec:     1218 || Batch Translation Loss:   0.013043 => Txt Tokens per Sec:     3384 || Lr: 0.000050
2024-02-09 18:14:14,354 Epoch 3430: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-09 18:14:14,355 EPOCH 3431
2024-02-09 18:14:25,581 Epoch 3431: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-09 18:14:25,582 EPOCH 3432
2024-02-09 18:14:36,652 Epoch 3432: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-09 18:14:36,653 EPOCH 3433
2024-02-09 18:14:47,680 Epoch 3433: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-09 18:14:47,681 EPOCH 3434
2024-02-09 18:14:58,946 Epoch 3434: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-09 18:14:58,947 EPOCH 3435
2024-02-09 18:15:10,076 Epoch 3435: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.20 
2024-02-09 18:15:10,077 EPOCH 3436
2024-02-09 18:15:12,750 [Epoch: 3436 Step: 00058400] Batch Recognition Loss:   0.000237 => Gls Tokens per Sec:     1197 || Batch Translation Loss:   0.007305 => Txt Tokens per Sec:     3382 || Lr: 0.000050
2024-02-09 18:15:21,152 Epoch 3436: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 18:15:21,152 EPOCH 3437
2024-02-09 18:15:32,252 Epoch 3437: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-09 18:15:32,253 EPOCH 3438
2024-02-09 18:15:43,210 Epoch 3438: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-09 18:15:43,210 EPOCH 3439
2024-02-09 18:15:54,199 Epoch 3439: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 18:15:54,199 EPOCH 3440
2024-02-09 18:16:05,025 Epoch 3440: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-09 18:16:05,026 EPOCH 3441
2024-02-09 18:16:15,706 Epoch 3441: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-09 18:16:15,707 EPOCH 3442
2024-02-09 18:16:18,679 [Epoch: 3442 Step: 00058500] Batch Recognition Loss:   0.000135 => Gls Tokens per Sec:      559 || Batch Translation Loss:   0.008351 => Txt Tokens per Sec:     1572 || Lr: 0.000050
2024-02-09 18:16:26,790 Epoch 3442: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 18:16:26,790 EPOCH 3443
2024-02-09 18:16:37,721 Epoch 3443: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-09 18:16:37,722 EPOCH 3444
2024-02-09 18:16:48,832 Epoch 3444: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-09 18:16:48,833 EPOCH 3445
2024-02-09 18:17:00,013 Epoch 3445: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-09 18:17:00,014 EPOCH 3446
2024-02-09 18:17:11,227 Epoch 3446: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.12 
2024-02-09 18:17:11,228 EPOCH 3447
2024-02-09 18:17:22,199 Epoch 3447: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.12 
2024-02-09 18:17:22,200 EPOCH 3448
2024-02-09 18:17:22,466 [Epoch: 3448 Step: 00058600] Batch Recognition Loss:   0.000347 => Gls Tokens per Sec:     2415 || Batch Translation Loss:   0.028867 => Txt Tokens per Sec:     6770 || Lr: 0.000050
2024-02-09 18:17:33,145 Epoch 3448: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-09 18:17:33,145 EPOCH 3449
2024-02-09 18:17:44,130 Epoch 3449: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-09 18:17:44,130 EPOCH 3450
2024-02-09 18:17:55,178 Epoch 3450: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-09 18:17:55,179 EPOCH 3451
2024-02-09 18:18:06,160 Epoch 3451: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-09 18:18:06,161 EPOCH 3452
2024-02-09 18:18:17,067 Epoch 3452: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-09 18:18:17,067 EPOCH 3453
2024-02-09 18:18:29,116 [Epoch: 3453 Step: 00058700] Batch Recognition Loss:   0.000311 => Gls Tokens per Sec:      828 || Batch Translation Loss:   0.025201 => Txt Tokens per Sec:     2319 || Lr: 0.000050
2024-02-09 18:18:29,282 Epoch 3453: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-09 18:18:29,283 EPOCH 3454
2024-02-09 18:18:40,267 Epoch 3454: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-09 18:18:40,267 EPOCH 3455
2024-02-09 18:18:51,194 Epoch 3455: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-09 18:18:51,195 EPOCH 3456
2024-02-09 18:19:02,322 Epoch 3456: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 18:19:02,322 EPOCH 3457
2024-02-09 18:19:13,433 Epoch 3457: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 18:19:13,434 EPOCH 3458
2024-02-09 18:19:24,472 Epoch 3458: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 18:19:24,473 EPOCH 3459
2024-02-09 18:19:33,048 [Epoch: 3459 Step: 00058800] Batch Recognition Loss:   0.000421 => Gls Tokens per Sec:     1015 || Batch Translation Loss:   0.008169 => Txt Tokens per Sec:     2761 || Lr: 0.000050
2024-02-09 18:19:35,589 Epoch 3459: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 18:19:35,589 EPOCH 3460
2024-02-09 18:19:46,652 Epoch 3460: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 18:19:46,653 EPOCH 3461
2024-02-09 18:19:57,735 Epoch 3461: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 18:19:57,736 EPOCH 3462
2024-02-09 18:20:08,750 Epoch 3462: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-09 18:20:08,751 EPOCH 3463
2024-02-09 18:20:19,889 Epoch 3463: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.36 
2024-02-09 18:20:19,890 EPOCH 3464
2024-02-09 18:20:31,006 Epoch 3464: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-09 18:20:31,007 EPOCH 3465
2024-02-09 18:20:39,587 [Epoch: 3465 Step: 00058900] Batch Recognition Loss:   0.000467 => Gls Tokens per Sec:      865 || Batch Translation Loss:   0.301990 => Txt Tokens per Sec:     2437 || Lr: 0.000050
2024-02-09 18:20:42,152 Epoch 3465: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.69 
2024-02-09 18:20:42,152 EPOCH 3466
2024-02-09 18:20:53,165 Epoch 3466: Total Training Recognition Loss 0.01  Total Training Translation Loss 5.62 
2024-02-09 18:20:53,165 EPOCH 3467
2024-02-09 18:21:03,941 Epoch 3467: Total Training Recognition Loss 0.05  Total Training Translation Loss 6.55 
2024-02-09 18:21:03,942 EPOCH 3468
2024-02-09 18:21:15,038 Epoch 3468: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.48 
2024-02-09 18:21:15,038 EPOCH 3469
2024-02-09 18:21:26,087 Epoch 3469: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.40 
2024-02-09 18:21:26,087 EPOCH 3470
2024-02-09 18:21:37,202 Epoch 3470: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.62 
2024-02-09 18:21:37,203 EPOCH 3471
2024-02-09 18:21:43,753 [Epoch: 3471 Step: 00059000] Batch Recognition Loss:   0.000357 => Gls Tokens per Sec:      977 || Batch Translation Loss:   0.017367 => Txt Tokens per Sec:     2580 || Lr: 0.000050
2024-02-09 18:21:48,056 Epoch 3471: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.37 
2024-02-09 18:21:48,056 EPOCH 3472
2024-02-09 18:21:59,070 Epoch 3472: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.34 
2024-02-09 18:21:59,071 EPOCH 3473
2024-02-09 18:22:10,129 Epoch 3473: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-09 18:22:10,130 EPOCH 3474
2024-02-09 18:22:21,168 Epoch 3474: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-09 18:22:21,168 EPOCH 3475
2024-02-09 18:22:32,252 Epoch 3475: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 18:22:32,252 EPOCH 3476
2024-02-09 18:22:43,471 Epoch 3476: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 18:22:43,471 EPOCH 3477
2024-02-09 18:22:46,795 [Epoch: 3477 Step: 00059100] Batch Recognition Loss:   0.000358 => Gls Tokens per Sec:     1541 || Batch Translation Loss:   0.019760 => Txt Tokens per Sec:     3986 || Lr: 0.000050
2024-02-09 18:22:54,405 Epoch 3477: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 18:22:54,405 EPOCH 3478
2024-02-09 18:23:05,362 Epoch 3478: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 18:23:05,362 EPOCH 3479
2024-02-09 18:23:16,367 Epoch 3479: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 18:23:16,367 EPOCH 3480
2024-02-09 18:23:27,339 Epoch 3480: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 18:23:27,340 EPOCH 3481
2024-02-09 18:23:38,279 Epoch 3481: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 18:23:38,279 EPOCH 3482
2024-02-09 18:23:49,456 Epoch 3482: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 18:23:49,457 EPOCH 3483
2024-02-09 18:23:52,620 [Epoch: 3483 Step: 00059200] Batch Recognition Loss:   0.000366 => Gls Tokens per Sec:     1214 || Batch Translation Loss:   0.017623 => Txt Tokens per Sec:     3522 || Lr: 0.000050
2024-02-09 18:24:00,610 Epoch 3483: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 18:24:00,610 EPOCH 3484
2024-02-09 18:24:11,653 Epoch 3484: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 18:24:11,654 EPOCH 3485
2024-02-09 18:24:22,751 Epoch 3485: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 18:24:22,752 EPOCH 3486
2024-02-09 18:24:33,887 Epoch 3486: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-09 18:24:33,887 EPOCH 3487
2024-02-09 18:24:44,750 Epoch 3487: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-09 18:24:44,751 EPOCH 3488
2024-02-09 18:24:55,598 Epoch 3488: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 18:24:55,599 EPOCH 3489
2024-02-09 18:24:56,427 [Epoch: 3489 Step: 00059300] Batch Recognition Loss:   0.000205 => Gls Tokens per Sec:     3096 || Batch Translation Loss:   0.010293 => Txt Tokens per Sec:     8333 || Lr: 0.000050
2024-02-09 18:25:06,341 Epoch 3489: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-09 18:25:06,341 EPOCH 3490
2024-02-09 18:25:17,439 Epoch 3490: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-09 18:25:17,440 EPOCH 3491
2024-02-09 18:25:28,715 Epoch 3491: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-09 18:25:28,716 EPOCH 3492
2024-02-09 18:25:39,793 Epoch 3492: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-09 18:25:39,794 EPOCH 3493
2024-02-09 18:25:51,016 Epoch 3493: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-09 18:25:51,017 EPOCH 3494
2024-02-09 18:26:02,231 Epoch 3494: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 18:26:02,231 EPOCH 3495
2024-02-09 18:26:04,242 [Epoch: 3495 Step: 00059400] Batch Recognition Loss:   0.000147 => Gls Tokens per Sec:      637 || Batch Translation Loss:   0.010942 => Txt Tokens per Sec:     1859 || Lr: 0.000050
2024-02-09 18:26:13,369 Epoch 3495: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-09 18:26:13,370 EPOCH 3496
2024-02-09 18:26:24,434 Epoch 3496: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-09 18:26:24,434 EPOCH 3497
2024-02-09 18:26:35,574 Epoch 3497: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-09 18:26:35,575 EPOCH 3498
2024-02-09 18:26:46,547 Epoch 3498: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-09 18:26:46,547 EPOCH 3499
2024-02-09 18:26:57,623 Epoch 3499: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-09 18:26:57,623 EPOCH 3500
2024-02-09 18:27:08,653 [Epoch: 3500 Step: 00059500] Batch Recognition Loss:   0.000237 => Gls Tokens per Sec:      963 || Batch Translation Loss:   0.014501 => Txt Tokens per Sec:     2664 || Lr: 0.000050
2024-02-09 18:27:08,653 Epoch 3500: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-09 18:27:08,653 EPOCH 3501
2024-02-09 18:27:19,862 Epoch 3501: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-09 18:27:19,862 EPOCH 3502
2024-02-09 18:27:31,022 Epoch 3502: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-09 18:27:31,022 EPOCH 3503
2024-02-09 18:27:41,794 Epoch 3503: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-09 18:27:41,795 EPOCH 3504
2024-02-09 18:27:53,025 Epoch 3504: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 18:27:53,026 EPOCH 3505
2024-02-09 18:28:04,014 Epoch 3505: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 18:28:04,015 EPOCH 3506
2024-02-09 18:28:14,339 [Epoch: 3506 Step: 00059600] Batch Recognition Loss:   0.000252 => Gls Tokens per Sec:      905 || Batch Translation Loss:   0.014120 => Txt Tokens per Sec:     2540 || Lr: 0.000050
2024-02-09 18:28:14,706 Epoch 3506: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-09 18:28:14,706 EPOCH 3507
2024-02-09 18:28:25,477 Epoch 3507: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-09 18:28:25,477 EPOCH 3508
2024-02-09 18:28:36,539 Epoch 3508: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-09 18:28:36,540 EPOCH 3509
2024-02-09 18:28:47,465 Epoch 3509: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-09 18:28:47,465 EPOCH 3510
2024-02-09 18:28:59,803 Epoch 3510: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-09 18:28:59,804 EPOCH 3511
2024-02-09 18:29:10,971 Epoch 3511: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-09 18:29:10,972 EPOCH 3512
2024-02-09 18:29:16,873 [Epoch: 3512 Step: 00059700] Batch Recognition Loss:   0.000138 => Gls Tokens per Sec:     1410 || Batch Translation Loss:   0.018877 => Txt Tokens per Sec:     3813 || Lr: 0.000050
2024-02-09 18:29:21,827 Epoch 3512: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-09 18:29:21,828 EPOCH 3513
2024-02-09 18:29:33,197 Epoch 3513: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-09 18:29:33,198 EPOCH 3514
2024-02-09 18:29:44,468 Epoch 3514: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-09 18:29:44,469 EPOCH 3515
2024-02-09 18:29:55,606 Epoch 3515: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-09 18:29:55,606 EPOCH 3516
2024-02-09 18:30:06,636 Epoch 3516: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-09 18:30:06,637 EPOCH 3517
2024-02-09 18:30:17,543 Epoch 3517: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-09 18:30:17,544 EPOCH 3518
2024-02-09 18:30:24,818 [Epoch: 3518 Step: 00059800] Batch Recognition Loss:   0.000123 => Gls Tokens per Sec:      968 || Batch Translation Loss:   0.007433 => Txt Tokens per Sec:     2629 || Lr: 0.000050
2024-02-09 18:30:28,548 Epoch 3518: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-09 18:30:28,549 EPOCH 3519
2024-02-09 18:30:39,439 Epoch 3519: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-09 18:30:39,439 EPOCH 3520
2024-02-09 18:30:50,490 Epoch 3520: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-09 18:30:50,491 EPOCH 3521
2024-02-09 18:31:01,526 Epoch 3521: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-09 18:31:01,527 EPOCH 3522
2024-02-09 18:31:12,432 Epoch 3522: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-09 18:31:12,433 EPOCH 3523
2024-02-09 18:31:23,418 Epoch 3523: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-09 18:31:23,419 EPOCH 3524
2024-02-09 18:31:31,250 [Epoch: 3524 Step: 00059900] Batch Recognition Loss:   0.000116 => Gls Tokens per Sec:      702 || Batch Translation Loss:   0.021479 => Txt Tokens per Sec:     2151 || Lr: 0.000050
2024-02-09 18:31:33,971 Epoch 3524: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-09 18:31:33,971 EPOCH 3525
2024-02-09 18:31:45,034 Epoch 3525: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-09 18:31:45,035 EPOCH 3526
2024-02-09 18:31:56,207 Epoch 3526: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 18:31:56,208 EPOCH 3527
2024-02-09 18:32:07,497 Epoch 3527: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 18:32:07,498 EPOCH 3528
2024-02-09 18:32:18,490 Epoch 3528: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-09 18:32:18,491 EPOCH 3529
2024-02-09 18:32:29,584 Epoch 3529: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 18:32:29,585 EPOCH 3530
2024-02-09 18:32:35,379 [Epoch: 3530 Step: 00060000] Batch Recognition Loss:   0.000173 => Gls Tokens per Sec:      729 || Batch Translation Loss:   0.014409 => Txt Tokens per Sec:     2111 || Lr: 0.000050
2024-02-09 18:33:16,570 Validation result at epoch 3530, step    60000: duration: 41.1907s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.61630	Translation Loss: 103086.14062	PPL: 29621.56445
	Eval Metric: BLEU
	WER 2.90	(DEL: 0.00,	INS: 0.00,	SUB: 2.90)
	BLEU-4 0.00	(BLEU-1: 10.40,	BLEU-2: 3.14,	BLEU-3: 1.04,	BLEU-4: 0.00)
	CHRF 16.78	ROUGE 8.83
2024-02-09 18:33:16,571 Logging Recognition and Translation Outputs
2024-02-09 18:33:16,572 ========================================================================================================================
2024-02-09 18:33:16,572 Logging Sequence: 75_58.00
2024-02-09 18:33:16,572 	Gloss Reference :	A B+C+D+E
2024-02-09 18:33:16,572 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 18:33:16,572 	Gloss Alignment :	         
2024-02-09 18:33:16,572 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 18:33:16,574 	Text Reference  :	it seems like he like to date women and does    not     want  to get  married people seem  to respect his       choices  
2024-02-09 18:33:16,575 	Text Hypothesis :	it ***** **** ** **** ** **** was   an  intense bidding dhoni as well in      the    start of the     different countries
2024-02-09 18:33:16,575 	Text Alignment  :	   D     D    D  D    D  D    S     S   S       S       S     S  S    S       S      S     S  S       S         S        
2024-02-09 18:33:16,575 ========================================================================================================================
2024-02-09 18:33:16,575 Logging Sequence: 152_113.00
2024-02-09 18:33:16,575 	Gloss Reference :	A B+C+D+E
2024-02-09 18:33:16,575 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 18:33:16,575 	Gloss Alignment :	         
2024-02-09 18:33:16,576 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 18:33:16,577 	Text Reference  :	** ** ** *** **** ** *** indians hoping for   a        victory were distraught at    the defeat    
2024-02-09 18:33:16,577 	Text Hypothesis :	if it is not part of the match   virat  kohli received this    is   important  while the tournament
2024-02-09 18:33:16,577 	Text Alignment  :	I  I  I  I   I    I  I   S       S      S     S        S       S    S          S         S         
2024-02-09 18:33:16,577 ========================================================================================================================
2024-02-09 18:33:16,577 Logging Sequence: 176_41.00
2024-02-09 18:33:16,578 	Gloss Reference :	A B+C+D+E
2024-02-09 18:33:16,578 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 18:33:16,578 	Gloss Alignment :	         
2024-02-09 18:33:16,578 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 18:33:16,579 	Text Reference  :	dahiya did not loose hope and put       up a        strong fight
2024-02-09 18:33:16,579 	Text Hypothesis :	it     was a   close call but coca-cola an official the    game 
2024-02-09 18:33:16,579 	Text Alignment  :	S      S   S   S     S    S   S         S  S        S      S    
2024-02-09 18:33:16,579 ========================================================================================================================
2024-02-09 18:33:16,579 Logging Sequence: 77_190.00
2024-02-09 18:33:16,579 	Gloss Reference :	A B+C+D+E
2024-02-09 18:33:16,580 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 18:33:16,580 	Gloss Alignment :	         
2024-02-09 18:33:16,580 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 18:33:16,581 	Text Reference  :	there are many batsmen who   have scrored 36     runs in 6    balls
2024-02-09 18:33:16,581 	Text Hypothesis :	***** to  know this    began a    large   number of   us know about
2024-02-09 18:33:16,581 	Text Alignment  :	D     S   S    S       S     S    S       S      S    S  S    S    
2024-02-09 18:33:16,581 ========================================================================================================================
2024-02-09 18:33:16,581 Logging Sequence: 155_170.00
2024-02-09 18:33:16,581 	Gloss Reference :	A B+C+D+E
2024-02-09 18:33:16,581 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 18:33:16,582 	Gloss Alignment :	         
2024-02-09 18:33:16,582 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 18:33:16,583 	Text Reference  :	india lost the  matches and    could  not secure  a  place in    the  semi  final
2024-02-09 18:33:16,583 	Text Hypothesis :	***** we   were just    joking around the support of the   media from their life 
2024-02-09 18:33:16,583 	Text Alignment  :	D     S    S    S       S      S      S   S       S  S     S     S    S     S    
2024-02-09 18:33:16,583 ========================================================================================================================
2024-02-09 18:33:22,464 Epoch 3530: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-09 18:33:22,465 EPOCH 3531
2024-02-09 18:33:33,729 Epoch 3531: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-09 18:33:33,730 EPOCH 3532
2024-02-09 18:33:44,622 Epoch 3532: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 18:33:44,623 EPOCH 3533
2024-02-09 18:33:55,769 Epoch 3533: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-09 18:33:55,770 EPOCH 3534
2024-02-09 18:34:06,567 Epoch 3534: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-09 18:34:06,568 EPOCH 3535
2024-02-09 18:34:17,174 Epoch 3535: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-09 18:34:17,175 EPOCH 3536
2024-02-09 18:34:19,609 [Epoch: 3536 Step: 00060100] Batch Recognition Loss:   0.000397 => Gls Tokens per Sec:     1315 || Batch Translation Loss:   0.013539 => Txt Tokens per Sec:     3423 || Lr: 0.000050
2024-02-09 18:34:28,210 Epoch 3536: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-09 18:34:28,211 EPOCH 3537
2024-02-09 18:34:39,042 Epoch 3537: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-09 18:34:39,043 EPOCH 3538
2024-02-09 18:34:49,894 Epoch 3538: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-09 18:34:49,895 EPOCH 3539
2024-02-09 18:35:01,055 Epoch 3539: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 18:35:01,055 EPOCH 3540
2024-02-09 18:35:12,256 Epoch 3540: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-09 18:35:12,256 EPOCH 3541
2024-02-09 18:35:23,496 Epoch 3541: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-09 18:35:23,496 EPOCH 3542
2024-02-09 18:35:25,378 [Epoch: 3542 Step: 00060200] Batch Recognition Loss:   0.000158 => Gls Tokens per Sec:     1020 || Batch Translation Loss:   0.011123 => Txt Tokens per Sec:     2619 || Lr: 0.000050
2024-02-09 18:35:34,433 Epoch 3542: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-09 18:35:34,433 EPOCH 3543
2024-02-09 18:35:45,225 Epoch 3543: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-09 18:35:45,225 EPOCH 3544
2024-02-09 18:35:56,100 Epoch 3544: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-09 18:35:56,101 EPOCH 3545
2024-02-09 18:36:07,367 Epoch 3545: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.77 
2024-02-09 18:36:07,368 EPOCH 3546
2024-02-09 18:36:18,222 Epoch 3546: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.91 
2024-02-09 18:36:18,223 EPOCH 3547
2024-02-09 18:36:29,365 Epoch 3547: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.00 
2024-02-09 18:36:29,366 EPOCH 3548
2024-02-09 18:36:31,418 [Epoch: 3548 Step: 00060300] Batch Recognition Loss:   0.000740 => Gls Tokens per Sec:      312 || Batch Translation Loss:   0.058920 => Txt Tokens per Sec:     1079 || Lr: 0.000050
2024-02-09 18:36:40,547 Epoch 3548: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-09 18:36:40,547 EPOCH 3549
2024-02-09 18:36:51,751 Epoch 3549: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-09 18:36:51,752 EPOCH 3550
2024-02-09 18:37:02,792 Epoch 3550: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-09 18:37:02,792 EPOCH 3551
2024-02-09 18:37:14,006 Epoch 3551: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-09 18:37:14,007 EPOCH 3552
2024-02-09 18:37:25,236 Epoch 3552: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-09 18:37:25,236 EPOCH 3553
2024-02-09 18:37:34,325 [Epoch: 3553 Step: 00060400] Batch Recognition Loss:   0.000310 => Gls Tokens per Sec:     1098 || Batch Translation Loss:   0.009910 => Txt Tokens per Sec:     2989 || Lr: 0.000050
2024-02-09 18:37:36,425 Epoch 3553: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-09 18:37:36,426 EPOCH 3554
2024-02-09 18:37:47,469 Epoch 3554: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-09 18:37:47,470 EPOCH 3555
2024-02-09 18:37:58,258 Epoch 3555: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-09 18:37:58,259 EPOCH 3556
2024-02-09 18:38:09,133 Epoch 3556: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-09 18:38:09,133 EPOCH 3557
2024-02-09 18:38:20,110 Epoch 3557: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-09 18:38:20,111 EPOCH 3558
2024-02-09 18:38:31,191 Epoch 3558: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-09 18:38:31,192 EPOCH 3559
2024-02-09 18:38:41,715 [Epoch: 3559 Step: 00060500] Batch Recognition Loss:   0.000362 => Gls Tokens per Sec:      827 || Batch Translation Loss:   0.016397 => Txt Tokens per Sec:     2313 || Lr: 0.000050
2024-02-09 18:38:42,336 Epoch 3559: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-09 18:38:42,337 EPOCH 3560
2024-02-09 18:38:53,239 Epoch 3560: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-09 18:38:53,240 EPOCH 3561
2024-02-09 18:39:04,069 Epoch 3561: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-09 18:39:04,069 EPOCH 3562
2024-02-09 18:39:14,843 Epoch 3562: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-09 18:39:14,844 EPOCH 3563
2024-02-09 18:39:25,696 Epoch 3563: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-09 18:39:25,697 EPOCH 3564
2024-02-09 18:39:36,684 Epoch 3564: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-09 18:39:36,684 EPOCH 3565
2024-02-09 18:39:44,533 [Epoch: 3565 Step: 00060600] Batch Recognition Loss:   0.000416 => Gls Tokens per Sec:      946 || Batch Translation Loss:   0.024714 => Txt Tokens per Sec:     2596 || Lr: 0.000050
2024-02-09 18:39:47,576 Epoch 3565: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-09 18:39:47,576 EPOCH 3566
2024-02-09 18:39:58,567 Epoch 3566: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.30 
2024-02-09 18:39:58,568 EPOCH 3567
2024-02-09 18:40:09,730 Epoch 3567: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-09 18:40:09,730 EPOCH 3568
2024-02-09 18:40:20,769 Epoch 3568: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-09 18:40:20,770 EPOCH 3569
2024-02-09 18:40:31,800 Epoch 3569: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 18:40:31,801 EPOCH 3570
2024-02-09 18:40:42,916 Epoch 3570: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-09 18:40:42,917 EPOCH 3571
2024-02-09 18:40:48,218 [Epoch: 3571 Step: 00060700] Batch Recognition Loss:   0.000379 => Gls Tokens per Sec:     1207 || Batch Translation Loss:   0.012682 => Txt Tokens per Sec:     3351 || Lr: 0.000050
2024-02-09 18:40:53,876 Epoch 3571: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 18:40:53,876 EPOCH 3572
2024-02-09 18:41:04,961 Epoch 3572: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-09 18:41:04,962 EPOCH 3573
2024-02-09 18:41:15,908 Epoch 3573: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 18:41:15,909 EPOCH 3574
2024-02-09 18:41:27,022 Epoch 3574: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-09 18:41:27,022 EPOCH 3575
2024-02-09 18:41:38,125 Epoch 3575: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 18:41:38,126 EPOCH 3576
2024-02-09 18:41:49,121 Epoch 3576: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 18:41:49,121 EPOCH 3577
2024-02-09 18:41:53,944 [Epoch: 3577 Step: 00060800] Batch Recognition Loss:   0.000185 => Gls Tokens per Sec:     1062 || Batch Translation Loss:   0.009064 => Txt Tokens per Sec:     2806 || Lr: 0.000050
2024-02-09 18:42:00,081 Epoch 3577: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-09 18:42:00,082 EPOCH 3578
2024-02-09 18:42:11,217 Epoch 3578: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 18:42:11,218 EPOCH 3579
2024-02-09 18:42:21,975 Epoch 3579: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-09 18:42:21,975 EPOCH 3580
2024-02-09 18:42:33,027 Epoch 3580: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-09 18:42:33,027 EPOCH 3581
2024-02-09 18:42:44,019 Epoch 3581: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-09 18:42:44,019 EPOCH 3582
2024-02-09 18:42:55,226 Epoch 3582: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-09 18:42:55,226 EPOCH 3583
2024-02-09 18:42:56,538 [Epoch: 3583 Step: 00060900] Batch Recognition Loss:   0.000751 => Gls Tokens per Sec:     2931 || Batch Translation Loss:   0.015007 => Txt Tokens per Sec:     7953 || Lr: 0.000050
2024-02-09 18:43:05,862 Epoch 3583: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-09 18:43:05,862 EPOCH 3584
2024-02-09 18:43:16,866 Epoch 3584: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 18:43:16,866 EPOCH 3585
2024-02-09 18:43:27,917 Epoch 3585: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-09 18:43:27,917 EPOCH 3586
2024-02-09 18:43:38,848 Epoch 3586: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 18:43:38,848 EPOCH 3587
2024-02-09 18:43:49,975 Epoch 3587: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-09 18:43:49,975 EPOCH 3588
2024-02-09 18:44:01,408 Epoch 3588: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.76 
2024-02-09 18:44:01,409 EPOCH 3589
2024-02-09 18:44:02,096 [Epoch: 3589 Step: 00061000] Batch Recognition Loss:   0.000791 => Gls Tokens per Sec:     3732 || Batch Translation Loss:   0.092142 => Txt Tokens per Sec:     8407 || Lr: 0.000050
2024-02-09 18:44:12,199 Epoch 3589: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.73 
2024-02-09 18:44:12,199 EPOCH 3590
2024-02-09 18:44:23,126 Epoch 3590: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-09 18:44:23,127 EPOCH 3591
2024-02-09 18:44:34,248 Epoch 3591: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-09 18:44:34,249 EPOCH 3592
2024-02-09 18:44:45,281 Epoch 3592: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.58 
2024-02-09 18:44:45,282 EPOCH 3593
2024-02-09 18:44:55,983 Epoch 3593: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-09 18:44:55,984 EPOCH 3594
2024-02-09 18:45:06,929 Epoch 3594: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-09 18:45:06,930 EPOCH 3595
2024-02-09 18:45:09,763 [Epoch: 3595 Step: 00061100] Batch Recognition Loss:   0.000287 => Gls Tokens per Sec:      360 || Batch Translation Loss:   0.018656 => Txt Tokens per Sec:     1061 || Lr: 0.000050
2024-02-09 18:45:18,107 Epoch 3595: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-09 18:45:18,107 EPOCH 3596
2024-02-09 18:45:28,967 Epoch 3596: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-09 18:45:28,967 EPOCH 3597
2024-02-09 18:45:39,585 Epoch 3597: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-09 18:45:39,586 EPOCH 3598
2024-02-09 18:45:50,555 Epoch 3598: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-09 18:45:50,555 EPOCH 3599
2024-02-09 18:46:01,592 Epoch 3599: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.31 
2024-02-09 18:46:01,593 EPOCH 3600
2024-02-09 18:46:12,337 [Epoch: 3600 Step: 00061200] Batch Recognition Loss:   0.000126 => Gls Tokens per Sec:      989 || Batch Translation Loss:   0.013719 => Txt Tokens per Sec:     2735 || Lr: 0.000050
2024-02-09 18:46:12,337 Epoch 3600: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-09 18:46:12,338 EPOCH 3601
2024-02-09 18:46:23,119 Epoch 3601: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-09 18:46:23,120 EPOCH 3602
2024-02-09 18:46:34,046 Epoch 3602: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-09 18:46:34,046 EPOCH 3603
2024-02-09 18:46:44,813 Epoch 3603: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-09 18:46:44,813 EPOCH 3604
2024-02-09 18:46:55,734 Epoch 3604: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-09 18:46:55,734 EPOCH 3605
2024-02-09 18:47:06,559 Epoch 3605: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-09 18:47:06,560 EPOCH 3606
2024-02-09 18:47:17,242 [Epoch: 3606 Step: 00061300] Batch Recognition Loss:   0.000185 => Gls Tokens per Sec:      874 || Batch Translation Loss:   0.015074 => Txt Tokens per Sec:     2478 || Lr: 0.000050
2024-02-09 18:47:17,580 Epoch 3606: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-09 18:47:17,581 EPOCH 3607
2024-02-09 18:47:28,602 Epoch 3607: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-09 18:47:28,602 EPOCH 3608
2024-02-09 18:47:39,506 Epoch 3608: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.59 
2024-02-09 18:47:39,507 EPOCH 3609
2024-02-09 18:47:50,561 Epoch 3609: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.74 
2024-02-09 18:47:50,561 EPOCH 3610
2024-02-09 18:48:01,610 Epoch 3610: Total Training Recognition Loss 0.25  Total Training Translation Loss 2.53 
2024-02-09 18:48:01,610 EPOCH 3611
2024-02-09 18:48:12,610 Epoch 3611: Total Training Recognition Loss 0.32  Total Training Translation Loss 2.02 
2024-02-09 18:48:12,610 EPOCH 3612
2024-02-09 18:48:22,455 [Epoch: 3612 Step: 00061400] Batch Recognition Loss:   0.008413 => Gls Tokens per Sec:      819 || Batch Translation Loss:   0.031497 => Txt Tokens per Sec:     2310 || Lr: 0.000050
2024-02-09 18:48:23,450 Epoch 3612: Total Training Recognition Loss 0.51  Total Training Translation Loss 1.20 
2024-02-09 18:48:23,450 EPOCH 3613
2024-02-09 18:48:34,204 Epoch 3613: Total Training Recognition Loss 1.17  Total Training Translation Loss 0.59 
2024-02-09 18:48:34,205 EPOCH 3614
2024-02-09 18:48:45,276 Epoch 3614: Total Training Recognition Loss 0.23  Total Training Translation Loss 0.45 
2024-02-09 18:48:45,276 EPOCH 3615
2024-02-09 18:48:56,322 Epoch 3615: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.36 
2024-02-09 18:48:56,323 EPOCH 3616
2024-02-09 18:49:07,237 Epoch 3616: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.35 
2024-02-09 18:49:07,238 EPOCH 3617
2024-02-09 18:49:18,015 Epoch 3617: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.39 
2024-02-09 18:49:18,015 EPOCH 3618
2024-02-09 18:49:26,462 [Epoch: 3618 Step: 00061500] Batch Recognition Loss:   0.000460 => Gls Tokens per Sec:      803 || Batch Translation Loss:   0.029644 => Txt Tokens per Sec:     2352 || Lr: 0.000050
2024-02-09 18:49:29,246 Epoch 3618: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-09 18:49:29,247 EPOCH 3619
2024-02-09 18:49:40,182 Epoch 3619: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 18:49:40,183 EPOCH 3620
2024-02-09 18:49:50,815 Epoch 3620: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-09 18:49:50,816 EPOCH 3621
2024-02-09 18:50:01,839 Epoch 3621: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-09 18:50:01,839 EPOCH 3622
2024-02-09 18:50:12,739 Epoch 3622: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-09 18:50:12,740 EPOCH 3623
2024-02-09 18:50:23,742 Epoch 3623: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-09 18:50:23,743 EPOCH 3624
2024-02-09 18:50:31,024 [Epoch: 3624 Step: 00061600] Batch Recognition Loss:   0.000160 => Gls Tokens per Sec:      755 || Batch Translation Loss:   0.014160 => Txt Tokens per Sec:     2156 || Lr: 0.000050
2024-02-09 18:50:34,825 Epoch 3624: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-09 18:50:34,826 EPOCH 3625
2024-02-09 18:50:45,797 Epoch 3625: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-09 18:50:45,798 EPOCH 3626
2024-02-09 18:50:56,850 Epoch 3626: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 18:50:56,851 EPOCH 3627
2024-02-09 18:51:07,984 Epoch 3627: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-09 18:51:07,985 EPOCH 3628
2024-02-09 18:51:19,064 Epoch 3628: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-09 18:51:19,064 EPOCH 3629
2024-02-09 18:51:30,009 Epoch 3629: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 18:51:30,010 EPOCH 3630
2024-02-09 18:51:33,644 [Epoch: 3630 Step: 00061700] Batch Recognition Loss:   0.000203 => Gls Tokens per Sec:     1162 || Batch Translation Loss:   0.009983 => Txt Tokens per Sec:     2819 || Lr: 0.000050
2024-02-09 18:51:41,115 Epoch 3630: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 18:51:41,116 EPOCH 3631
2024-02-09 18:51:52,152 Epoch 3631: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 18:51:52,152 EPOCH 3632
2024-02-09 18:52:03,243 Epoch 3632: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-09 18:52:03,244 EPOCH 3633
2024-02-09 18:52:14,302 Epoch 3633: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 18:52:14,303 EPOCH 3634
2024-02-09 18:52:25,293 Epoch 3634: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 18:52:25,294 EPOCH 3635
2024-02-09 18:52:36,188 Epoch 3635: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-09 18:52:36,188 EPOCH 3636
2024-02-09 18:52:37,610 [Epoch: 3636 Step: 00061800] Batch Recognition Loss:   0.000156 => Gls Tokens per Sec:     2254 || Batch Translation Loss:   0.011292 => Txt Tokens per Sec:     6507 || Lr: 0.000050
2024-02-09 18:52:47,243 Epoch 3636: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-09 18:52:47,244 EPOCH 3637
2024-02-09 18:52:58,387 Epoch 3637: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 18:52:58,388 EPOCH 3638
2024-02-09 18:53:09,317 Epoch 3638: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-09 18:53:09,317 EPOCH 3639
2024-02-09 18:53:20,424 Epoch 3639: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-09 18:53:20,424 EPOCH 3640
2024-02-09 18:53:31,251 Epoch 3640: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-09 18:53:31,252 EPOCH 3641
2024-02-09 18:53:42,231 Epoch 3641: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 18:53:42,231 EPOCH 3642
2024-02-09 18:53:42,926 [Epoch: 3642 Step: 00061900] Batch Recognition Loss:   0.000350 => Gls Tokens per Sec:     2767 || Batch Translation Loss:   0.008359 => Txt Tokens per Sec:     6764 || Lr: 0.000050
2024-02-09 18:53:53,191 Epoch 3642: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 18:53:53,192 EPOCH 3643
2024-02-09 18:54:04,207 Epoch 3643: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 18:54:04,207 EPOCH 3644
2024-02-09 18:54:15,229 Epoch 3644: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-09 18:54:15,229 EPOCH 3645
2024-02-09 18:54:26,393 Epoch 3645: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-09 18:54:26,394 EPOCH 3646
2024-02-09 18:54:37,410 Epoch 3646: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-09 18:54:37,411 EPOCH 3647
2024-02-09 18:54:48,402 Epoch 3647: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-09 18:54:48,402 EPOCH 3648
2024-02-09 18:54:48,635 [Epoch: 3648 Step: 00062000] Batch Recognition Loss:   0.000131 => Gls Tokens per Sec:     2759 || Batch Translation Loss:   0.023397 => Txt Tokens per Sec:     7914 || Lr: 0.000050
2024-02-09 18:55:30,189 Validation result at epoch 3648, step    62000: duration: 41.5536s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.49768	Translation Loss: 103802.05469	PPL: 31817.23828
	Eval Metric: BLEU
	WER 2.54	(DEL: 0.00,	INS: 0.00,	SUB: 2.54)
	BLEU-4 0.00	(BLEU-1: 9.77,	BLEU-2: 2.73,	BLEU-3: 0.87,	BLEU-4: 0.00)
	CHRF 16.94	ROUGE 8.11
2024-02-09 18:55:30,191 Logging Recognition and Translation Outputs
2024-02-09 18:55:30,191 ========================================================================================================================
2024-02-09 18:55:30,191 Logging Sequence: 165_523.00
2024-02-09 18:55:30,191 	Gloss Reference :	A B+C+D+E
2024-02-09 18:55:30,192 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 18:55:30,192 	Gloss Alignment :	         
2024-02-09 18:55:30,192 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 18:55:30,193 	Text Reference  :	as he believed that his team might lose if   he takes off his batting pads
2024-02-09 18:55:30,193 	Text Hypothesis :	** ** ******** **** *** **** ***** **** when he ***** was 62  years   old 
2024-02-09 18:55:30,193 	Text Alignment  :	D  D  D        D    D   D    D     D    S       D     S   S   S       S   
2024-02-09 18:55:30,193 ========================================================================================================================
2024-02-09 18:55:30,193 Logging Sequence: 165_233.00
2024-02-09 18:55:30,193 	Gloss Reference :	A B+C+D+E
2024-02-09 18:55:30,194 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 18:55:30,194 	Gloss Alignment :	         
2024-02-09 18:55:30,194 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 18:55:30,196 	Text Reference  :	irrespective of whether he was playing the match or not he always sat    with his     bag   he    was happy when the team won 
2024-02-09 18:55:30,196 	Text Hypothesis :	************ ** ******* it was ******* *** ***** ** *** a  huge   number of   chennai super kings csk has   won  the **** game
2024-02-09 18:55:30,196 	Text Alignment  :	D            D  D       S      D       D   D     D  D   S  S      S      S    S       S     S     S   S     S        D    S   
2024-02-09 18:55:30,196 ========================================================================================================================
2024-02-09 18:55:30,196 Logging Sequence: 169_214.00
2024-02-09 18:55:30,197 	Gloss Reference :	A B+C+D+E
2024-02-09 18:55:30,197 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 18:55:30,197 	Gloss Alignment :	         
2024-02-09 18:55:30,197 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 18:55:30,198 	Text Reference  :	virat kohli said         that  though    arshdeep     dropped the catch he is still a strong  part      of    the    indian      team
2024-02-09 18:55:30,199 	Text Hypothesis :	***** the   commonwealth games encourage independence from    the ***** ** ** ***** * british democracy human rights development etc 
2024-02-09 18:55:30,199 	Text Alignment  :	D     S     S            S     S         S            S           D     D  D  D     D S       S         S     S      S           S   
2024-02-09 18:55:30,199 ========================================================================================================================
2024-02-09 18:55:30,199 Logging Sequence: 88_67.00
2024-02-09 18:55:30,199 	Gloss Reference :	A B+C+D+E
2024-02-09 18:55:30,200 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 18:55:30,200 	Gloss Alignment :	         
2024-02-09 18:55:30,200 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 18:55:30,202 	Text Reference  :	*** ***** pablo javkin the ****** mayor of   rosario  is also  a        drug trafficker so he  won't take     care of     you 
2024-02-09 18:55:30,202 	Text Hypothesis :	the mayor added that   the police never does anything to catch culprits and  that       is why the   culprits have become bold
2024-02-09 18:55:30,202 	Text Alignment  :	I   I     S     S          I      S     S    S        S  S     S        S    S          S  S   S     S        S    S      S   
2024-02-09 18:55:30,202 ========================================================================================================================
2024-02-09 18:55:30,202 Logging Sequence: 69_95.00
2024-02-09 18:55:30,203 	Gloss Reference :	A B+C+D+E
2024-02-09 18:55:30,203 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 18:55:30,203 	Gloss Alignment :	         
2024-02-09 18:55:30,203 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 18:55:30,205 	Text Reference  :	***** a   six    and    a    four  sealed csk's victory and the **** ***** ** **** team      won   the match
2024-02-09 18:55:30,205 	Text Hypothesis :	after the defeat people were glued to     their batting at  the same place at fans including watch the video
2024-02-09 18:55:30,205 	Text Alignment  :	I     S   S      S      S    S     S      S     S       S       I    I     I  I    S         S         S    
2024-02-09 18:55:30,205 ========================================================================================================================
2024-02-09 18:55:30,209 Training ended since there were no improvements inthe last learning rate step: 0.000050
2024-02-09 18:55:30,212 Best validation result at step    10000:   1.00 eval_metric.
2024-02-09 18:55:57,349 ------------------------------------------------------------
2024-02-09 18:55:57,350 [DEV] partition [RECOGNITION] experiment [BW]: 1
2024-02-09 18:56:38,809 finished in 41.4583s 
2024-02-09 18:56:38,812 ************************************************************
2024-02-09 18:56:38,812 [DEV] partition [RECOGNITION] results:
	New Best CTC Decode Beam Size: 1
	WER 4.38	(DEL: 0.00,	INS: 0.00,	SUB: 4.38)
2024-02-09 18:56:38,812 ************************************************************
2024-02-09 18:56:38,812 ------------------------------------------------------------
2024-02-09 18:56:38,812 [DEV] partition [RECOGNITION] experiment [BW]: 2
2024-02-09 18:57:19,808 finished in 40.9963s 
2024-02-09 18:57:19,809 ------------------------------------------------------------
2024-02-09 18:57:19,809 [DEV] partition [RECOGNITION] experiment [BW]: 3
2024-02-09 18:58:01,283 finished in 41.4740s 
2024-02-09 18:58:01,284 ------------------------------------------------------------
2024-02-09 18:58:01,284 [DEV] partition [RECOGNITION] experiment [BW]: 4
2024-02-09 18:58:43,942 finished in 42.6578s 
2024-02-09 18:58:43,943 ------------------------------------------------------------
2024-02-09 18:58:43,943 [DEV] partition [RECOGNITION] experiment [BW]: 5
2024-02-09 18:59:27,563 finished in 43.6198s 
2024-02-09 18:59:27,564 ------------------------------------------------------------
2024-02-09 18:59:27,564 [DEV] partition [RECOGNITION] experiment [BW]: 6
2024-02-09 19:00:12,443 finished in 44.8793s 
2024-02-09 19:00:12,444 ------------------------------------------------------------
2024-02-09 19:00:12,444 [DEV] partition [RECOGNITION] experiment [BW]: 7
2024-02-09 19:00:56,324 finished in 43.8800s 
2024-02-09 19:00:56,324 ------------------------------------------------------------
2024-02-09 19:00:56,325 [DEV] partition [RECOGNITION] experiment [BW]: 8
2024-02-09 19:01:38,666 finished in 42.3402s 
2024-02-09 19:01:38,666 ------------------------------------------------------------
2024-02-09 19:01:38,666 [DEV] partition [RECOGNITION] experiment [BW]: 9
2024-02-09 19:02:22,278 finished in 43.6128s 
2024-02-09 19:02:22,279 ------------------------------------------------------------
2024-02-09 19:02:22,279 [DEV] partition [RECOGNITION] experiment [BW]: 10
2024-02-09 19:03:03,378 finished in 41.0991s 
2024-02-09 19:03:03,379 ============================================================
2024-02-09 19:03:44,489 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 1 and Alpha: -1
	BLEU-4 1.00	(BLEU-1: 11.60,	BLEU-2: 3.96,	BLEU-3: 1.80,	BLEU-4: 1.00)
	CHRF 17.36	ROUGE 9.91
2024-02-09 19:03:44,489 ------------------------------------------------------------
2024-02-09 21:53:11,733 ************************************************************
2024-02-09 21:53:11,737 [DEV] partition [Recognition & Translation] results:
	Best CTC Decode Beam Size: 1
	Best Translation Beam Size: 1 and Alpha: -1
	WER 4.38	(DEL: 0.00,	INS: 0.00,	SUB: 4.38)
	BLEU-4 1.00	(BLEU-1: 11.60,	BLEU-2: 3.96,	BLEU-3: 1.80,	BLEU-4: 1.00)
	CHRF 17.36	ROUGE 9.91
2024-02-09 21:53:11,737 ************************************************************
2024-02-09 21:54:14,366 [TEST] partition [Recognition & Translation] results:
	Best CTC Decode Beam Size: 1
	Best Translation Beam Size: 1 and Alpha: -1
	WER 3.67	(DEL: 0.00,	INS: 0.00,	SUB: 3.67)
	BLEU-4 0.86	(BLEU-1: 10.99,	BLEU-2: 3.29,	BLEU-3: 1.46,	BLEU-4: 0.86)
	CHRF 17.16	ROUGE 9.24
2024-02-09 21:54:14,367 ************************************************************
